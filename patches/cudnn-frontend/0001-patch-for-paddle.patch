From b54dd87c62d3ba45da185c046eca51ff037598a4 Mon Sep 17 00:00:00 2001
From: Tian Zheng <tizheng@nvidia.com>
Date: Mon, 13 Feb 2023 20:59:38 -0800
Subject: [PATCH] patch for paddle

---
 include/cudnn_frontend_ExecutionPlan.h | 4 ++++
 include/cudnn_frontend_Operation.h     | 5 +++--
 include/cudnn_frontend_find_plan.h     | 6 +++---
 3 files changed, 10 insertions(+), 5 deletions(-)

diff --git a/include/cudnn_frontend_ExecutionPlan.h b/include/cudnn_frontend_ExecutionPlan.h
index e361821..26c0a8c 100644
--- a/include/cudnn_frontend_ExecutionPlan.h
+++ b/include/cudnn_frontend_ExecutionPlan.h
@@ -168,6 +168,10 @@ class ExecutionPlan_v8 : public BackendDescriptor {
 #endif
     }
 
+    ManagedOpaqueDescriptor GetEngineConfig() const {
+        return engine_config;
+    }
+
     ExecutionPlan_v8(ExecutionPlan_v8 const &) = default;
     ExecutionPlan_v8 &
     operator=(ExecutionPlan_v8 const &) = default;
diff --git a/include/cudnn_frontend_Operation.h b/include/cudnn_frontend_Operation.h
index 097e970..cf3f4b7 100644
--- a/include/cudnn_frontend_Operation.h
+++ b/include/cudnn_frontend_Operation.h
@@ -2013,8 +2013,9 @@ class OperationBuilder_v8 {
             m_operation.feature_vector.push_back(yTensor_strA[i]); // n, c, (g), d, h , w 
         }
 
-        int64_t alpha_as_int = *reinterpret_cast<int64_t *>(&m_operation.alpha_d);
-        int64_t  beta_as_int = *reinterpret_cast<int64_t *>(&m_operation.beta_d);
+        int64_t alpha_as_int, beta_as_int;
+        memcpy(&alpha_as_int, &m_operation.alpha_d, sizeof(int64_t));
+        memcpy(&beta_as_int, &m_operation.beta_d, sizeof(int64_t));
 
         m_operation.feature_vector.push_back(alpha_as_int);
         m_operation.feature_vector.push_back(beta_as_int);
diff --git a/include/cudnn_frontend_find_plan.h b/include/cudnn_frontend_find_plan.h
index f20432a..c36fc60 100644
--- a/include/cudnn_frontend_find_plan.h
+++ b/include/cudnn_frontend_find_plan.h
@@ -53,7 +53,7 @@ time_sorted_plan(cudnnHandle_t handle, executionPlans_t plans, VariantPack const
     cudaDeviceSynchronize();
 
     cudaStream_t stream = nullptr;
-    ::cudnnGetStream(handle, &stream);
+    cudnnGetStream(handle, &stream);
 
     for (auto &plan : plans) {
         float time_ms       = 0.0f;
@@ -61,7 +61,7 @@ time_sorted_plan(cudnnHandle_t handle, executionPlans_t plans, VariantPack const
         float min_time_ms   = std::numeric_limits<float>::max();
 
         // Warm-up run
-        auto warmup_status = ::cudnnBackendExecute(handle, plan.get_raw_desc(), variantPack.get_raw_desc());
+        auto warmup_status = cudnnBackendExecute(handle, plan.get_raw_desc(), variantPack.get_raw_desc());
         if (warmup_status != CUDNN_STATUS_SUCCESS) {
             getLogger() << "[cudnn_frontend] Plan " << plan.getTag() << " failed with " << to_string(warmup_status) << std::endl;
             continue;
@@ -71,7 +71,7 @@ time_sorted_plan(cudnnHandle_t handle, executionPlans_t plans, VariantPack const
         for (int i = 0; i < maxIterCount; i++) {
             cudaEventRecord(start, stream);
 
-            ::cudnnBackendExecute(handle, plan.get_raw_desc(), variantPack.get_raw_desc());
+            cudnnBackendExecute(handle, plan.get_raw_desc(), variantPack.get_raw_desc());
 
             cudaEventRecord(stop, stream);
             cudaEventSynchronize(stop);
-- 
2.25.1

