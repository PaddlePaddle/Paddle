#   Copyright (c) 2018 PaddlePaddle Authors. All Rights Reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

from paddle.fluid import core
import numpy as np
import math
import os
from paddle.fluid.executor import global_scope
from paddle.fluid import io

__all__ = ['Calibrator']


class Calibrator(object):
    '''
    The calibrator class transforms the program and updates the calculated scale into it.
    This is INT8 v1 calibration tool, mainly for the support of ResNet-50, MobileNet and SSD-MobileNet.
    '''
    # TODO(guomingz): Below op list will be updated once more INT8 op kernels are supported.
    supported_int8_computation_op_type = ['conv2d']
    supported_int8_memory_op_type = [
        'pool2d', 'reshape', 'reshape2', 'concat', 'transpose', 'transpose2'
    ]
    supported_int8_op_type = supported_int8_computation_op_type + \
        supported_int8_memory_op_type
    supported_int8_scale_op_type = supported_int8_computation_op_type + [
        'quantize'
    ]
    # more ops need to do sampling, besides int8 computation ops
    additional_sampling_op_type = supported_int8_memory_op_type

    FP32 = 'fp32'
    INT8 = 'int8'

    u8_max = 255
    s8_max = 127

    def __init__(self, *args, **kwargs):
        self.program = kwargs['program']
        self.pretrained_model = kwargs['pretrained_model']
        self.debug = kwargs['debug'] if 'debug' in kwargs else False
        self.algo = kwargs['algo']
        self.output = kwargs['output']
        self.feed_var_names = kwargs['feed_var_names']
        self.fetch_list = kwargs['fetch_list']
        self.exe = kwargs['exe']
        self.first_conv_int8 = kwargs[
            'first_conv_int8'] if 'first_conv_int8' in kwargs else False

        self._conv_input_var_name = []
        self._conv_output_var_name = []
        self._other_output_var_name = []
        self._weights_var_name = []
        self._residual_input_var_name = []
        self._conv_op_index = [
            index for index, value in enumerate(self.program.global_block().ops)
            if value.type == 'conv2d'
        ]

        self._var_max_value_map = {}
        self._var_max_range = {}
        self._weights_scaling_factor = {}
        self._u8_output_var = []
        self._s8_output_var = []
        self._persistable_vars = []
        self._sampling_data = {}

        self.__init_analysis()
        self.__generate_output_program()

    def save_int8_model(self):
        self.__sampling(self._sampling_data)
        self.__save_scale()
        self.__update_program()
        self.__unify_concat_scale_get_requantize()
        self.__update_output_program_attr()
        self.__display_debug()
        self.__save_offline_model()

    def sample_data(self):
        '''
        Sampling the tensor data of variable.
        '''
        for i in self.sampling_program.list_vars():
            if i.name in self.sampling_vars:
                np_data = np.array(global_scope().find_var(i.name).get_tensor())
                if i.name not in self._sampling_data:
                    self._sampling_data[i.name] = []
                self._sampling_data[i.name].append(np_data)

    def __save_offline_model(self):
        '''
        Save the quantized model to the disk.
        '''
        io.save_inference_model(self.output, self.feed_var_names,
                                self.fetch_list, self.exe,
                                self.sampling_program)

    def __display_debug(self):
        if self.debug:
            self.__dot(self._output_program)
            print(self._output_program)

    def __get_max_range_by_var_name(self, program, var_name):
        """
        Check the specified variable was generated from Relu layer or not.
        If the variable was the output of one of the pool2d/reshape/concat
        /transpose, we keep trace the ancestor of this variable;
        If the variable was the output the conv op, we check it's has_relu
        attr;
        Otherwise, we return the Calibrator.s8 as default value.
        Returns:
            Return Calibrator.u8_max if the variable was generated by Relu,
            otherwise it will returns Calibrator.s8
        """
        search_end_index = -1
        input_index_name = {}
        output_index_name = {}
        ops_type = []

        for index, op in enumerate(program.current_block().ops):
            ops_type.append(op.type)

            input_index_name[index] = op.input_arg_names

            output_index_name[index] = op.output_arg_names
            if var_name in op.output_arg_names:
                search_end_index = index

        # analysis
        while search_end_index >= 0:
            if ops_type[search_end_index] == "relu":
                return Calibrator.u8_max

            input_name = input_index_name[search_end_index][0]

            for i in output_index_name.keys():
                if input_name in output_index_name[i]:
                    search_end_index = i
                    break

            if ops_type[
                    search_end_index] not in Calibrator.supported_int8_op_type:
                return Calibrator.s8_max

            if ops_type[
                    search_end_index] not in Calibrator.supported_int8_computation_op_type:
                continue

            if program.current_block().ops[search_end_index].has_attr(
                    'fuse_relu') and program.current_block().ops[
                        search_end_index].attr('fuse_relu'):
                return Calibrator.u8_max
            else:
                return Calibrator.s8_max

        return Calibrator.s8_max

    def __update_program(self):
        '''
        Update the program with the quantize/dequantize op insertion.
        '''
        quantize_indexes, dequantize_indexes, next_diff_ops_map = self.__get_quantize_dequantize_combination(
            self._output_program)
        calc_max_func = self.__get_optimal_scaling_factor if self.algo == "KL" else np.max
        insert_poses = quantize_indexes + dequantize_indexes
        insert_poses.sort(reverse=True)
        quantize_index = len(quantize_indexes) - 1
        dequantize_index = len(dequantize_indexes) - 1
        for i in insert_poses:
            if i == -1 and self.first_conv_int8:
                current_out = self._output_program.global_block().ops[
                    self._conv_op_index[0]].input_arg_names[2]
            else:
                current_op = self._output_program.global_block().ops[i]
                current_out = current_op.output_arg_names[0]
                if current_op.type == "split":
                    current_out = current_op.output_arg_names[1]
            if i in quantize_indexes:
                if self.__get_max_range_by_var_name(
                        self._output_program, current_out) == Calibrator.s8_max:
                    var_tmp = self._output_program.current_block().create_var(
                        name="quantize_{}_tmp".format(quantize_index),
                        dtype=core.VarDesc.VarType.INT8, )
                else:
                    var_tmp = self._output_program.current_block().create_var(
                        name="quantize_{}_tmp".format(quantize_index),
                        dtype=core.VarDesc.VarType.UINT8, )
                op = self._output_program.current_block()._insert_op(
                    index=i + 1,
                    type="quantize",
                    inputs={"Input": current_out},
                    outputs={"Output": var_tmp}, )
                op._set_attr("data_format", "MKLDNNLAYOUT")
                op._set_attr(
                    "Scale", self._var_max_range[current_out] /
                    calc_max_func(self._var_max_value_map[current_out]))
                op._set_attr("use_mkldnn", 1)
                if self.__get_max_range_by_var_name(
                        self._output_program, current_out) == Calibrator.s8_max:
                    op._set_attr("is_negative_input", 1)
                quantize_index -= 1
            else:
                var_tmp = self._output_program.current_block().create_var(
                    name="dequantize_{}_tmp".format(dequantize_index),
                    dtype="float32", )
                op = self._output_program.current_block()._insert_op(
                    index=i + 1,
                    type="dequantize",
                    inputs={"Input": current_out},
                    outputs={"Output": var_tmp}, )
                op._set_attr("data_format", "MKLDNNLAYOUT")
                op._set_attr(
                    "Scale", self._var_max_range[current_out] /
                    calc_max_func(self._var_max_value_map[current_out]))
                op._set_attr("use_mkldnn", 1)
                dequantize_index -= 1

            for next_op in next_diff_ops_map[i]:
                op_inserted = 1
                for l in insert_poses:
                    if i < l and next_op > l:
                        op_inserted += 1
                op_n = self._output_program.current_block().ops[next_op +
                                                                op_inserted]
                for k in op_n.input_names:
                    op_n_inputs = op_n.input(k)
                    if len(op_n_inputs) > 0 and current_out in op_n_inputs:
                        op_n_new_inputs = [
                            var_tmp.name
                            if op_n_input == current_out else op_n_input
                            for op_n_input in op_n_inputs
                        ]
                        op_n.desc.set_input(k, op_n_new_inputs)

    def __update_output_program_attr(self):
        for i in self._output_program.list_vars():
            if i.name in self._persistable_vars:
                i.persistable = False
                os.system("rm -rf {}/{}".format(self.pretrained_model, i.name))

        for i in self._u8_output_var:
            self._output_program.current_block().var(i).desc.set_dtype(
                core.VarDesc.VarType.UINT8)

        for i in self._s8_output_var:
            self._output_program.current_block().var(i).desc.set_dtype(
                core.VarDesc.VarType.INT8)

    @property
    def sampling_program(self):
        return self._output_program

    @property
    def sampling_vars(self):
        return self._weights_var_name + self._conv_input_var_name + self._conv_output_var_name + \
               self._residual_input_var_name + self._other_output_var_name

    def _is_close(self, a, b, rel_tol=1e-09, abs_tol=0.0):
        return abs(a - b) <= max(rel_tol * max(abs(a), abs(b)), abs_tol)

    def __generate_output_program(self):
        for i in self.program.list_vars():
            if not i.persistable and i.name in self.sampling_vars:
                i.persistable = True
                self._persistable_vars.append(i.name)

        self._output_program = self.program.clone()

    def __save_scale(self):
        '''
        Update the convolution scale information.
        '''
        func = self.__get_optimal_scaling_factor if self.algo == 'KL' else np.max
        start_index = 0 if self.first_conv_int8 else 1
        for i in self._conv_op_index[start_index:]:
            weights_var_name = self.program.current_block().ops[i].input(
                'Filter')[0]
            input_var_name = self.program.current_block().ops[i].input('Input')[
                0]
            output_var_name = self.program.current_block().ops[i].output(
                'Output')[0]
            self._output_program.current_block().ops[i]._set_attr(
                "Scale_weights", self._weights_scaling_factor[weights_var_name])

            self._output_program.current_block().ops[i]._set_attr(
                "Scale_in", self._var_max_range[input_var_name] /
                func(self._var_max_value_map[input_var_name]))
            self._output_program.current_block().ops[i]._set_attr(
                "Scale_out", self._var_max_range[output_var_name] /
                func(self._var_max_value_map[output_var_name]))
            if self._output_program.current_block().ops[i].desc.input(
                    "ResidualData"):
                residual_var_name = self._output_program.current_block().ops[
                    i].desc.input("ResidualData")[0]
                self._output_program.current_block().ops[i]._set_attr(
                    "Scale_in_eltwise", self._var_max_range[residual_var_name] /
                    func(self._var_max_value_map[residual_var_name]))

    def __sampling(self, sampling_data):
        '''
        Sampling the variables data range.
        '''
        for i in self.program.list_vars():
            if i.name not in self.sampling_vars:
                continue

            if i.name in self._weights_var_name:
                scaling_factor_per_channel = []
                data = sampling_data[i.name][0]
                for j in range(data.shape[0]):
                    var_value = float(np.max(np.abs(data[j])))
                    if not self._is_close(var_value, 0.0):
                        scaling_factor_per_channel.append(Calibrator.s8_max /
                                                          var_value)
                    else:
                        scaling_factor_per_channel.append(0.0)
                self._weights_scaling_factor[
                    i.name] = scaling_factor_per_channel
            else:
                if i.name in self._conv_output_var_name:
                    op_pos = self.__get_op_index_by_output_var(self.program,
                                                               i.name)
                    cur_op = self.program.current_block().ops[op_pos]

                    if cur_op.has_attr('fuse_relu') and cur_op.attr(
                            'fuse_relu'):
                        max_range = Calibrator.u8_max
                        self._u8_output_var.append(i.name)
                    else:
                        max_range = Calibrator.s8_max
                        self._s8_output_var.append(i.name)
                else:
                    max_range = self.__get_max_range_by_var_name(self.program,
                                                                 i.name)
                max_value = [[np.abs(np_data)]
                             for np_data in sampling_data[i.name]]

                self._var_max_range[i.name] = max_range
                self._var_max_value_map[i.name] = max_value

    def __get_op_index_by_output_var(self, program, var_name, start_index=0):
        '''
        Check whether the specified input variable is the output of the 
        conv/pool2d op's output or not.

        Returns:
            The index if the variable is the output of any conv/pool2d op's
            output.
            -1 when the variable is not the output of any conv/pool2d op's 
            output.
        '''
        for index, op in enumerate(program.current_block().ops[start_index:]):
            if var_name in op.output_arg_names and op.type in Calibrator.supported_int8_op_type:
                return index
        return -1

    def __get_op_index_by_input_var(self, program, var_name, start_index=0):
        '''
        Get the op index by specified input variable.
        Returns:
            The op index if the variable is the input of this op or -1 if the 
            variable is not the input of any op. 
        '''
        for index, op in enumerate(program.current_block().ops[start_index:]):
            if var_name in op.input_arg_names:
                return index

        return -1

    def __get_op_input_output_info(self, program):
        '''
        Get op input and output information from program
        '''
        op_input_var_names = [
            op.input_arg_names for op in program.global_block().ops
        ]
        op_output_var_names = [
            op.output_arg_names for op in program.global_block().ops
        ]
        op_input_map = {}
        op_output_map = {}

        for index in range(len(program.global_block().ops)):
            output_ops = []
            for op_output in op_output_var_names[index]:
                for j in range(index + 1, len(program.global_block().ops)):
                    if op_output in op_input_var_names[j]:
                        output_ops.append(j)
                        if op_input_var_names[j] == op_output_var_names[j]:
                            break
            op_output_map[index] = output_ops
            reverse_index = len(program.global_block().ops) - 1 - index
            input_ops = []
            for op_input in op_input_var_names[reverse_index]:
                for j in range(reverse_index, -1, -1):
                    if op_input in op_output_var_names[j]:
                        input_ops.append(j)
                        if op_input_var_names[j] == op_output_var_names[j]:
                            break
            op_input_map[reverse_index] = input_ops
        return op_input_var_names, op_output_var_names, op_input_map, op_output_map

    def __get_quantize_dequantize_combination(self, program):
        """
        Get the quantize/dequantize op index for further inserting.
        Args:
            The program desc.
        Returns:
            Two lists contains the quantize op and dequantize op index information.
        """

        op_types = [op.type for op in program.global_block().ops]
        op_support_types = [
            Calibrator.INT8 if x in Calibrator.supported_int8_op_type \
                else Calibrator.FP32 for x in op_types
        ]

        start = self._conv_op_index[0] - \
            1 if self.first_conv_int8 else self._conv_op_index[1] - 1
        end = len(op_types) - 1

        op_input_var_names, op_output_var_names, op_input_map, op_output_map = self.__get_op_input_output_info(
            program)

        combine_indexes = [x for x in op_input_map if len(op_input_map[x]) > 1]
        combine_map = {x: op_input_map[x] for x in combine_indexes}

        split_indexes = [x for x in op_output_map if len(op_output_map[x]) > 1]
        split_map = {x: op_output_map[x] for x in split_indexes}

        # find all op which have different op_type(int8/fp32) with next op
        quantize_indexes = [start]
        dequantize_indexes = []
        int8_to_fp32 = []
        fp32_to_int8 = []

        for index in range(start, end):
            if index not in split_indexes and len(op_output_map[index]) > 0:
                if op_types[
                        index] in Calibrator.supported_int8_op_type and op_types[
                            op_output_map[index][
                                0]] not in Calibrator.supported_int8_op_type:
                    int8_to_fp32.append(index)
                if op_types[
                        index] not in Calibrator.supported_int8_op_type and op_types[
                            op_output_map[index][
                                0]] in Calibrator.supported_int8_op_type:
                    fp32_to_int8.append(index)
            elif index in split_indexes:
                for sub_split in split_map[index]:
                    if op_types[index] in Calibrator.supported_int8_op_type and \
                            op_types[sub_split] not in Calibrator.supported_int8_op_type and \
                            index not in int8_to_fp32:
                        int8_to_fp32.append(index)
                    if op_types[index] not in Calibrator.supported_int8_op_type and \
                            op_types[sub_split] in Calibrator.supported_int8_op_type and \
                            index not in fp32_to_int8:
                        fp32_to_int8.append(index)

        quantize_indexes.extend(
            [x for x in fp32_to_int8 if x not in quantize_indexes])

        # optimize dequantize op positions
        int8_to_fp32_opt = int8_to_fp32
        int8_to_fp32_opt.sort()
        for i in range(len(int8_to_fp32_opt) - 1, -1, -1):
            index = int8_to_fp32_opt[i]
            if index in split_indexes:
                for j in range(i + 1, len(int8_to_fp32_opt) - 1):
                    if int8_to_fp32_opt[j] in split_map[index] and op_types[
                            int8_to_fp32_opt[
                                j]] in Calibrator.supported_int8_memory_op_type:
                        int8_to_fp32_opt.pop(j)

        # optimize fp32 + supported_int8_memory_op_type + fp32
        remove_int8_to_fp32 = []
        for i in range(len(quantize_indexes) - 1, -1, -1):
            quantize_index = quantize_indexes[i]
            if len(op_output_map[quantize_index]) == 1 and op_output_map[
                    quantize_index][0] in int8_to_fp32_opt and op_types[
                        op_output_map[quantize_index][
                            0]] in Calibrator.supported_int8_memory_op_type:
                quantize_indexes.pop(i)
                if op_output_map[quantize_index][0] not in remove_int8_to_fp32:
                    remove_int8_to_fp32.append(op_output_map[quantize_index][0])
        for index in remove_int8_to_fp32:
            int8_to_fp32_opt.pop(int8_to_fp32_opt.index(index))

        # figure out which position to insert
        next_diff_ops_map = {}
        pre_insert_ops = quantize_indexes + int8_to_fp32_opt
        for i in pre_insert_ops:
            next_diff_ops = []
            if i in int8_to_fp32_opt:
                for sub_node in op_output_map[i]:
                    if op_support_types[i] != op_support_types[sub_node]:
                        next_diff_ops.append(sub_node)
                    elif op_support_types[sub_node] != op_support_types[
                            op_output_map[sub_node]
                        [0]] and op_types[
                            sub_node] in Calibrator.supported_int8_memory_op_type:
                        next_diff_ops.append(sub_node)
            else:
                if i == -1 and self.first_conv_int8:
                    next_diff_ops = [self._conv_op_index[0]]
                else:
                    for sub_node in op_output_map[i]:
                        if op_support_types[sub_node] == Calibrator.INT8:
                            next_diff_ops.append(sub_node)
            next_diff_ops.sort(reverse=True)
            next_diff_ops_map[i] = next_diff_ops

        # int8 computation op + dequantize optimization
        int8_conv_fp32_out = []
        for i in int8_to_fp32_opt:
            if op_types[
                    i] not in Calibrator.supported_int8_computation_op_type and i not in dequantize_indexes:
                dequantize_indexes.append(i)
            elif i in split_indexes and len(split_map[i]) != len(
                    next_diff_ops_map[i]) and i not in dequantize_indexes:
                dequantize_indexes.append(i)
            else:
                int8_conv_fp32_out.append(i)

        for i in int8_conv_fp32_out:
            op = program.current_block().ops[i]
            op._set_attr("force_fp32_output", 1)

        return quantize_indexes, dequantize_indexes, next_diff_ops_map

    def __get_parent_op_with_scale(self, op_index, op_types, op_output_map):
        """
        Get a op parent op with scale conv/quantize.
        """
        for i in range(1, op_index):
            if op_index in op_output_map[i] and op_types[
                    i] in Calibrator.supported_int8_scale_op_type:
                return i
            elif op_index in op_output_map[i] and op_types[
                    i] not in Calibrator.supported_int8_scale_op_type:
                return self.__get_parent_op_with_scale(i, op_types,
                                                       op_output_map)

    def __unify_concat_scale_get_requantize(self):
        """
        Unify concat scale and get the requantize op index for further inserting.
        """
        op_types = [op.type for op in self._output_program.global_block().ops]
        concat_op_index = [
            index
            for index, value in enumerate(self._output_program.global_block()
                                          .ops) if value.type == 'concat'
        ]

        op_input_var_names, op_output_var_names, op_input_map, op_output_map = self.__get_op_input_output_info(
            self._output_program)

        fp32_concat = []
        for concat_op in concat_op_index:
            for input_op in op_input_map[concat_op]:
                if op_types[input_op] not in Calibrator.supported_int8_op_type:
                    fp32_concat.append(concat_op)
                    break
        int8_concat = [x for x in concat_op_index if x not in fp32_concat]

        int8_concat.sort(reverse=True)
        for concat_op in int8_concat:
            direct_input = op_input_map[concat_op]
            scale_input = [
                self.__get_parent_op_with_scale(i, op_types, op_output_map)
                for i in direct_input
            ]
            input_scale = [
                self._output_program.current_block().ops[i].attr('Scale_out')
                for i in scale_input
            ]
            scale_min = np.min(input_scale)
            for i in scale_input:
                if self._output_program.current_block().ops[i].attr(
                        'Scale_out') > scale_min:
                    self._output_program.current_block().ops[i]._set_attr(
                        "Scale_out", scale_min)

    def __init_analysis(self):
        '''
        Collect the variable names for sampling.
        '''
        start_index = 0 if self.first_conv_int8 else 1

        for i in self._conv_op_index[start_index:]:
            self._weights_var_name.append(self.program.current_block().ops[i]
                                          .input('Filter')[0])
            self._conv_input_var_name.append(self.program.current_block().ops[i]
                                             .input('Input')[0])
            self._conv_output_var_name.append(self.program.current_block().ops[
                i].output('Output')[0])
            if self.program.current_block().ops[i].desc.input("ResidualData"):
                self._residual_input_var_name.append(self.program.current_block(
                ).ops[i].desc.input("ResidualData")[0])

        for i in range(len(self.program.current_block().ops)):
            if self.program.current_block().ops[i].type in Calibrator.additional_sampling_op_type \
                and 'Out' in self.program.current_block().ops[i].output_names:
                self._other_output_var_name.append(self.program.current_block()
                                                   .ops[i].output('Out')[0])

    def __expand_quantized_bins(self, quantized_bins, reference_bins):
        expanded_quantized_bins = [0] * len(reference_bins)
        num_merged_bins = len(reference_bins) / len(quantized_bins)
        j_start = 0
        j_end = num_merged_bins
        for idx in xrange(len(quantized_bins)):
            zero_count = reference_bins[j_start:j_end].count(0)
            num_merged_bins = j_end - j_start
            if zero_count == num_merged_bins:
                avg_bin_ele = 0
            else:
                avg_bin_ele = quantized_bins[idx] / (
                    num_merged_bins - zero_count + 0.0)
            for idx1 in xrange(j_start, j_end):
                expanded_quantized_bins[idx1] = (0 if reference_bins[idx1] == 0
                                                 else avg_bin_ele)
            j_start += num_merged_bins
            j_end += num_merged_bins
            if (idx + 1) == len(quantized_bins) - 1:
                j_end = len(reference_bins)
        return expanded_quantized_bins

    def __safe_entropy(self, reference_distr_P, P_sum, candidate_distr_Q,
                       Q_sum):
        '''
        Calculate the entropy.
        '''
        assert len(reference_distr_P) == len(candidate_distr_Q)
        tmp_sum1 = 0
        tmp_sum2 = 0
        for idx in range(len(reference_distr_P)):
            p_idx = reference_distr_P[idx]
            q_idx = candidate_distr_Q[idx]
            if p_idx == 0:
                tmp_sum1 += 0
                tmp_sum2 += 0
            else:
                if q_idx == 0:
                    print("Fatal error!, idx = " + str(idx) +
                          " qindex = 0! p_idx = " + str(p_idx))
                tmp_sum1 += p_idx * (math.log(Q_sum * p_idx))
                tmp_sum2 += p_idx * (math.log(P_sum * q_idx))
        return (tmp_sum1 - tmp_sum2) / P_sum

    # Reference: http://on-demand.gputechconf.com/gtc/2017/presentation/s7310-8-bit-inference-with-tensorrt.pdf
    def __get_optimal_scaling_factor(self,
                                     activation_blob,
                                     num_quantized_bins=255):
        '''
        Using the KL-divergenc method to get the more precise scaling factor.
        '''
        max_val = np.max(activation_blob)
        min_val = np.min(activation_blob)
        if min_val >= 0:
            hist, hist_edeges = np.histogram(
                activation_blob, bins=2048, range=(min_val, max_val))
            ending_iter = 2047
            starting_iter = int(ending_iter * 0.7)
        else:
            th = max(abs(max_val), abs(min_val))
            hist, hist_edeges = np.histogram(
                activation_blob, bins=2048, range=(-th, th))
            starting_iter = 0
            ending_iter = 2047
            if abs(max_val) > abs(min_val):
                while starting_iter < ending_iter:
                    if hist[starting_iter] == 0:
                        starting_iter += 1
                        continue
                    else:
                        break
                starting_iter += int((ending_iter - starting_iter) * 0.6)
            else:
                while ending_iter > 0:
                    if hist[ending_iter] == 0:
                        ending_iter -= 1
                        continue
                    else:
                        break
                starting_iter = int(0.6 * ending_iter)
        bin_width = hist_edeges[1] - hist_edeges[0]
        P_sum = len(activation_blob)
        min_kl_divergence = 0
        min_kl_index = 0
        kl_inited = False
        for i in range(starting_iter, ending_iter + 1):
            reference_distr_P = hist[0:i].tolist()
            outliers_count = sum(hist[i:2048])
            if reference_distr_P[i - 1] == 0:
                continue
            reference_distr_P[i - 1] += outliers_count
            reference_distr_bins = reference_distr_P[:]
            candidate_distr_Q = hist[0:i].tolist()
            num_merged_bins = i / num_quantized_bins
            candidate_distr_Q_quantized = [0] * num_quantized_bins
            j_start = 0
            j_end = num_merged_bins
            for idx in xrange(num_quantized_bins):
                candidate_distr_Q_quantized[idx] = sum(candidate_distr_Q[
                    j_start:j_end])
                j_start += num_merged_bins
                j_end += num_merged_bins
                if (idx + 1) == num_quantized_bins - 1:
                    j_end = i
            candidate_distr_Q = self.__expand_quantized_bins(
                candidate_distr_Q_quantized, reference_distr_bins)
            Q_sum = sum(candidate_distr_Q)
            kl_divergence = self.__safe_entropy(reference_distr_P, P_sum,
                                                candidate_distr_Q, Q_sum)
            if not kl_inited:
                min_kl_divergence = kl_divergence
                min_kl_index = i
                kl_inited = True
            elif kl_divergence < min_kl_divergence:
                min_kl_divergence = kl_divergence
                min_kl_index = i
            else:
                pass
        if min_kl_index == 0:
            while starting_iter > 0:
                if hist[starting_iter] == 0:
                    starting_iter -= 1
                    continue
                else:
                    break
            min_kl_index = starting_iter
        return (min_kl_index + 0.5) * bin_width

    @staticmethod
    def __dot(program, output_name="model.dot"):
        '''
        Generate the graphiz dot file for debugging.
        '''
        dot_graph = ""
        dot_nodes = []
        dot_edges = []
        dot_graph += "digraph pm {\n"
        for block in program.blocks:
            ops = list(block.ops)
            for index, op in enumerate(ops):
                op_type = op.type
                op_name = op_type + "_" + op.output_arg_names[0].replace(
                    ".", "_") + "___" + str(index)
                for name in op.input_arg_names:
                    name = name.replace(".", "_")
                    dot_edge = name + " -> " + op_name
                    if dot_edge not in dot_edges:
                        dot_edges.append(dot_edge)
                    dot_node = name + " [shape=oval, style=filled, fillcolor=yellow]"
                    if dot_node not in dot_nodes:
                        dot_nodes.append(dot_node)

                for name in op.output_arg_names:
                    name = name.replace(".", "_")
                    dot_edge = op_name + " -> " + name
                    if dot_edge not in dot_edges:
                        dot_edges.append(dot_edge)
                if op_type in Calibrator.supported_int8_op_type:
                    if op_type == "conv2d" and op.has_attr(
                            'force_fp32_output') and op.attr(
                                "force_fp32_output"):
                        dot_node = op_name + " [shape=box, style=filled, color=deeppink]"
                    else:
                        dot_node = op_name + " [shape=box, style=filled, color=greenyellow]"
                elif op_type in ["quantize", "dequantize"]:
                    dot_node = op_name + " [shape=box, style=filled, color=gold]"
                else:
                    dot_node = op_name + " [shape=box, style=filled, fillcolor=red]"

                if dot_node not in dot_nodes:
                    dot_nodes.append(dot_node)

        for dot_edge in dot_edges:
            dot_graph += dot_edge + "\n"
        for dot_node in dot_nodes:
            dot_graph += dot_node + "\n"
        dot_graph += "}"

        with open(output_name, 'w') as f:
            f.write(dot_graph)
