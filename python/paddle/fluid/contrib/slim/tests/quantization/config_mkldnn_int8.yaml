#int8_model_save_path(str): int8_model_save_path is used to save an int8 ProgramDesc with
#                fp32 weights which is used for MKL-DNN int8 inference. For post training quantization,
#                MKLDNNPostTrainingQuantStrategy only supports converting a fp32 ProgramDesc
#                with fp32 weights to an int8 ProgramDesc with fp32 weights now. The saved
#                int8 ProgramDesc with fp32 weights only can be executed with MKL-DNN enabled.
#                None means it doesn't save int8 ProgramDesc with fp32 weights. default: None.
#
#fp32_model_path(str): fp32_model_path is used to load an original fp32 ProgramDesc with fp32 weights.
#                None means it doesn't have a fp32 ProgramDesc with fp32 weights. default: None.
#
#cpu_math_library_num_threads(int): The number of cpu math library threads which is used on
#                MKLDNNPostTrainingQuantStrategy. 1 means it only uses one cpu math library
#                thread. default: 1
#                Note: Here we set the cpu_math_library_num_threads to 4 which is the maximum number of
#                cpu math library threads on CI machine.
#
version: 1.0
strategies:
    mkldnn_post_training_strategy:
        class: 'MKLDNNPostTrainingQuantStrategy'
        int8_model_save_path: 'OUTPUT_PATH'
        fp32_model_path: 'MODEL_PATH'
        cpu_math_library_num_threads: 4
compressor:
    epoch: 0
    checkpoint_path: ''
    strategies:
        - mkldnn_post_training_strategy
