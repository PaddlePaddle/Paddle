version: 1.0
strategies:
    infer_quant_strategy:
        class: 'InferQuantStrategy'
        start_epoch: 0
        end_epoch: 0
        float_model_save_path: './output/float'
        mobile_model_save_path: './output/mobile'
        int8_model_save_path: './output/int8'
        weight_bits: 8
        activation_bits: 8
        weight_quantize_type: 'abs_max'
        activation_quantize_type: 'abs_max'
compressor:
    epoch: 0
    strategies:
        - infer_quant_strategy

