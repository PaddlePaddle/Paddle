# Memory Optimization


## Motivation
When we train a neural network model, there will be tons of temporary variables besides the model parameters. The temporary variables and model parameters determine the memory cost of the model training job. However, at each forward/backward minibatch iteration, our training routine follow the chain rule, some of the previous temporary variables can be recycled. That give our chance to do the memory optimization.

Memory optimization can reduce the memory consumption, which allow user try larger minibatches, speed up the training process at a limited GPU memory hardware. Besides, memory optimization is also necessary in both online/mobile inference, it will accelerate the memory allocation and halve the memory needs.

## How to
The neural network is representated as a static graph of operators. Each operator is a node, variable is an edge, which generated by the node. We choose graph as intermediate representation and analysis the operator/variable dependencies.
To improve the memory utilization, three strategies are applied. They are explicit backward graph construction, nonlive memory sharing, in-place operations.

### Explicit backward graph construction
A computation graph describes the dataflow dependencies between operations in the neural network. When the user configure a model, only the forward graph is given and backward graph is constructed by the framework. 
In common framework implementations, there are two ways: construct backward graph reversely same as forward graph or explicitly representing a backwards paths, which not isomorphic with forward graph. We choose the later one because two advantages:

First, the explicit backward graph construction empower our ability to have a flexiable backward graph instead of a mirror forward graph. Different with the tightly tied layer-based forward/backward graph, for every forward operator, its backward operator is needed. For example, if we want to do gradient clipping operation, we can insert a op only in backward graph, the path can be totally different with forward graph.

Second, the explicit backward graph can link the dependencies between computations more clearly. The variable generated by the forward graph and used in backward graph will have a tight bounded live range. Our share memory strategy and memory recycle strategy highly depended on it.

To achieve this target, every operator gradient operator should have a explicit OpMaker, which describe the gradient Input/Output instead of reuse forward OpMaker directly.

Use relu op for example, the formula is 
```
forward : y = relu(x) = max(x, 0) 
backward : dx = dy * (y > 0)
```

It's clearly that the derivative of relu do not need x as input. So the backward path is different. We can describe this formulat different in GradOpMaker.

```c++
  class ReluGradOpMaker 
      : public ::paddle::framework::SingleGradOpDescMaker {                  
   public:                                                                   
    using ::paddle::framework::SingleGradOpDescMaker::SingleGradOpDescMaker; 
                                                                             
   protected:                                                                
    std::unique_ptr<::paddle::framework::OpDesc> Apply() const override {    
      auto* op = new ::paddle::framework::OpDesc();                          
      op->SetType("relu_grad");                                     
      op->SetInput("Out", Output("Out"));                                    
      op->SetInput(::paddle::framework::GradVarName("Out"),                  
                   OutputGrad("Out"));                                       
                                                                             
      op->SetAttrMap(Attrs());                                               
                                                                             
      op->SetOutput(::paddle::framework::GradVarName("X"), InputGrad("X"));  
      return std::unique_ptr<::paddle::framework::OpDesc>(op);               
    }                                                                        
  }
```
Need to note that the explicit backward is tight coupled with backward module, please see backward.md if you need more details about the graph construction.

### Nonlive variable memory sharing
In the computations graph, tons of temporary variables are introduced. Some of them are no longer needed in the future computation, mark them as nonlive variables and can used to share memory.

For example, 
```
a = op1(b, c);
d = op2(a)
e = op3(d, f)
```

In this case, variable a is no longer used, and op2 does not support in-place operation. After op2 finishes, we can put the memory of variable a to a memory pool. Then, variable e can share the memory of variable a from the pool. Need to note that our computation graph is staticed, so we can apply the strategy for the whole graph before running. Here comes the question, how do we know the variable is alive or not at specific graph position?

##### Live Variable Analysis
A variable is live if it holds a value that may be needed in the future. The Live Variable Analysis is a kind of data-flow analysis technique, which compute the variables liveness range at data-flow graph level.

Luckily, [live variable analysis](https://en.wikipedia.org/wiki/Live_variable_analysis) is a classic problem in compiler design, it has been well defined and the techique are applied in many stages in compiler, for example, the registers allocation.

To perform Live Variable Analysis, a [control flow graph](https://en.wikipedia.org/wiki/Control_flow_graph) (CFG) is needed. The CFG is different with our computation graph, can be constructed through ProgramDesc. We use it compute the each variable liveness range.

```python
class ControlFlowGraph(object):
    def __init__(self, ops):
        self._ops = ops
        self._sucessors = defaultdict(set)
        self._presucessors = defaultdict(set)
        self._uses = defaultdict(set)
        self._defs = defaultdict(set)
        self._live_in = defaultdict(set)
        self._live_out = defaultdict(set)
    
    def build_graph(self):
        pass
    
    def compute_liveness_range(self):
        pass
        
    def live_in(self, idx):
        pass
    
    def live_out(self, idx):
        pass
```
Here we only focus on the variable liveness computation. For the CFG details please referenced the code and the concept defination.

By defination, in ControlFlowGraph a variable is *live* on an edge if there is a directed path from that edge to a *use* of the variable that does not go through any *def*. A variable is *live-in* at a node if it is live on any of the in-edges of that node; it is *live-out* at a node if it is live on any of the out-edges of the node.

We follow the liveness algorithm in [worklist algorithm](http://www.cs.cornell.edu/courses/cs4120/2013fa/lectures/lec26-fa13.pdf) , after the liveness range computation is finished, then we get each variable's liveness range.
Still take the above one for example, 

```
a = op1(b, c);
d = op2(a)
e = op3(d, f)
```

The dataflow analysis result is:

```
live_in(op1) = {b, c, f}
live_out(op1) = {a, f}

live_in(op2) = {a, f}
live_out(op2) = {d, f}

live_in(op3) = {d, f}
live_out(op3) = {}
```


We maintained a memory pool for the nonlive variable memory sharing. Each operator node will be scanned to determine memory optimization is done or not. If an operator satifies the requirement, following policy will be taken to handle input/output variables.

The pesude code show the algorithm.
```
for op in ops:
    for nonlive var in ops.defs:
        pool <-- var
    for var in ops.defs:
        if can use pool nonlive var:
            pool --> var
            pool.pop()
```

In our memory transpiler, there is a liveness pass to do the nonlive variable memory sharing.

```
class MemoryPlan(object):
    """
    MemoryPlan optimize the memory reuse for each block.
    It takes a fresh Python Block and generate a optimized BlockDesc.
    """
    def build_graph(self):
        pass
       
    def liveness_pass(self):
        pass
```

### Inplace operations

Inplace operations means one operator can reuse its input variable instead of allocate new variables. In neural networks, there are tons of activations, some of them can be inplaced to save memory, reuse input is a inherent property of one operator. 

Why we need treat inplace operator specific? First, the variable liveness can not handle this situation. One operator's input must be alive when the operator is running, so the nonlive memory sharing not work here. Second, if one operator want to apply inplace operation, we must assure there's no other dependencies on the reused variable, otherwise, we will have a data-race problem.

Take relu op as example, we define a reuse tag in OpMaker.

```c++
  class ReluOpMaker                                                
      : public ::paddle::framework::OpProtoAndCheckerMaker {            
   public:                                                              
    void Make() override {                                              
      AddInput("X", "Input of Relu operator");                  
      AddOutput("Out", "Output of Relu operator").Reuse("X");   
      AddAttr<bool>("use_mkldnn",                                       
                    "(bool, default false) Only used in mkldnn kernel") 
          .SetDefault(false);                                           
      AddComment("Relu operator");                                          
    }                                                                   
  }
```

In memory transpiler, we use the tag indicate current operator support inplace, then check the dependency graph, if this dependency edge is the last reference one, then apply inplace.
```
Relu
$y = \max(x, 0)$
```

If the variable x is not used in any other operator, we can make an in-place operation. In other words, the memory block of variable y and variable x will be the same.

```
class MemoryPlan(object):
    """
    MemoryPlan optimize the memory reuse for each block.
    It takes a fresh Python Block and generate a optimized BlockDesc.
    """
    def build_graph(self):
        pass
       
    def inplace_pass(self):
        pass
```

### Memory Transpiler
MemoryTranspiler take a ProgramDesc, apply the memory optimization directly on the ProgramDesc. Consider the fact our ProgramDes contains many blocks, so we treat every block seperately.

In general, we describe the memory optimization strategy as follow steps:

```
1. create memory plan for each Block.
2. for each memory plan:
  2.1 do topology sort, compute the dependency for each var, compute the variable liveness range.
  2.2 create cache pool, put non-lived variable into it, for new created var, if hit cache, then reuse.
  2.3 for each op, if it output marked with reuse and make sure the op is the last referenced op. do inplace.
3. done.
```

pesude code as shows

```python
class MemoryTranspiler(object):
    """
    MemoryTranspiler transpile the program memory.
    It take a instance of Python Program, analysis the non-lived variable then do the re-use,
    also the in-place op computations.

    """

    def __init__(self, program, logging=False):
        self._program = program
        self._plans = []

    def _create_plans(self):
        # create plan for each block
        pass

    def transpile(self):
        self._create_plans()
        for plan in self._plans:
            plan.apply()
```

## Reference

- [Lecture Notes From Artificial Intelligence Is The New Electricity By Andrew Ng](https://manavsehgal.com/lecture-notes-from-artificial-intelligence-is-the-new-electricity-by-andrew-ng-4712dcbf26e5)
- Modern compiler implementation in ML, by Andrew W. Appel
- [Optimizing Memory Consumption in Deep learning](https://mxnet.incubator.apache.org/architecture/note_memory.html)
