# Distributed PaddlePaddle Quickstart

In this article, we'll launch a PaddlePaddle distributed training job with two trainer instances and two pserver instances. Please do some preparation as follows before reading this quick start.

1. prepare a cluster which has four instances at least and ensures the network connectivity. In this article,
we use the hostname `*.paddlepaddle.com` to represent the different node, and feel free to change
them according to the actual situation in your cluster.
1. reading the simple [quickstart](../../getstarted/quickstart_en.rst) and make sure the demo code generally running on each node.

## Transpile Fluid Program by DistributeTranspiler

PaddlePaddle Fluid provide `DistributeTranspiler` module to transpile your Fluid Program automatically and you can 
use the transpiled Program to launch the trainer or pserver instance.

```python
t = fluid.DistributeTranspiler()
```

- get pserver program

```python
t.transpile(
    pservers="ps0.paddlepaddle.com:7164,ps1.paddlepaddle.com:7164",
    trianers=2,
)
```

- get trainer program

```python
t.transpile(
    trainer_id=0,
    pservers="ps0.paddlepaddle.com:7164,ps1.paddlepaddle.com:7164",
    trianers=2,
)
```

the argument description of `transpile` is as follows:

argument | type |description
-- | --
trainer_id | int | the unique and increased number for each trainer node
pservers| str | all pserver instance endpoints
trainers | int | the trainer instance count

## Execute the Program which Generated by DistributeTranspiler

- execute pserver program

```python
pserver_prog = t.get_pserver_program("ps0.paddlepaddle.com:7164")
pserver_startup = t.get_startup_program("ps0.paddlepaddle.com:7164", pserver_prog)
exe = fluid.Executor(fluid.CPUPlace()) # can use fluid.CUDAPlace(0) for GPU device
exe.run(pserver_startup)
exe.run(pserver_prog)
```

- execute trainer program

```python
trainer_prog = t.get_trainer_program()
exe = fluid.Executor(fluid.CPUPlace()) # can use fluid.CUDAPlace(0) for GPU device
exe.run(fluid.default_trainer_program())
for idx, data in enumerate(trainer_reader()):
    exe.run(trainer_prog)
```

## All-In-One Demo Code

```python
import paddle
import paddle.fluid as fluid
import os

x = fluid.layers.data(name='x', shape=[13], dtype='float32')

y_predict = fluid.layers.fc(input=x, size=1, act=None)

y = fluid.layers.data(name='y', shape=[1], dtype='float32')

cost = fluid.layers.square_error_cost(input=y_predict, label=y)
avg_cost = fluid.layers.mean(cost)

sgd_optimizer = fluid.optimizer.SGD(learning_rate=0.001)
sgd_optimizer.minimize(avg_cost)

BATCH_SIZE = 20

train_reader = paddle.batch(
    paddle.reader.shuffle(
        paddle.dataset.uci_housing.train(), buf_size=500),
    batch_size=BATCH_SIZE)

place = fluid.CPUPlace() # use fluid.CUDAPlace(0) for GPU device
exe = fluid.Executor(place)

def train_loop(main_program):
    feeder = fluid.DataFeeder(place=place, feed_list=[x, y])
    exe.run(fluid.default_startup_program())

    PASS_NUM = 100
    for pass_id in range(PASS_NUM):
        for batch_id, data in enumerate(train_reader()):
            avg_loss_value, = exe.run(main_program,
                                      feed=feeder.feed(data),
                                      fetch_list=[avg_cost])
            print(batch_id, avg_loss_value)
            if math.isnan(float(avg_loss_value)):
                sys.exit("got NaN loss, training failed.")

role = os.getenv("PADDLE_TRAINING_ROLE", "")
pserver_endpoints = os.getenv("PADDLE_PSERVER_ENDPOINTS", "")
trianers = int(os.getenv("TRAINERS", 1))
trainer_id = int(os.getenv("TRAINER_ID", 0))
current_endpoint = os.getenv("PADDLE_CURRENT_ENDPOINT", "")
t = fluid.DistributeTranspiler()
t.transpile(
    trainer_id = trainer_id,
    pservers = pserver_endpoints,
    trainers = trainers
)
if role == "TRAINER":
    trainer_loop(t.get_trainer_program())
elif role == "PSERVER":
    pserver_prog = t.get_pserver_program(current_endpoint)
    startup_prog = t.get_startup_program(
        pserver_current_endpoint, pserver_prog)
    exe.run(startup_prog)
    exe.run(pserver_prog)
```

Lunch trainer and pserver instances on different node with specified environment variables

node | command line | description
-- | -- | --
ps0.paddlepaddle.com | PADDLE_TRAINING_ROLE=PSERVER PADDLE_CURRENT_ENDPOINT=ps0.paddlepaddle.com:7164 PADDLE_PSERVER_ENDPOINTS=ps0.paddlepaddle.com:7164, ps1.paddlepaddle.com TRAINERS=2 python fluid_dist.py | lunch pserver instance
ps1.paddlepaddle.com | PADDLE_TRAINING_ROLE=PSERVER PADDLE_CURRENT_ENDPOINT=ps1.paddlepaddle.com:7164 PADDLE_PSERVER_ENDPOINTS=ps0.paddlepaddle.com:7164,ps1.paddlepaddle.com:7164 TRAINERS=2 python fluid_dist.py | lunch pserver instance
trainer0.paddlepaddle.com | PADDLE_TRAINING_ROLE=TRAINER PADDLE_PSERVER_ENDPOINTS=ps0.paddlepaddle.com:7164,ps1.paddlepaddle.com:7164 TRAINERS=2 TRAINER_ID=0 python fluid_dist.py | lunch trainer instance which id=0
trainer1.paddlepaddle.com | PADDLE_TRAINING_ROLE=TRAINER PADDLE_PSERVER_ENDPOINTS=ps0.paddlepaddle.com:7164,ps1.paddlepaddle.com:7164 TRAINERS=2 TRAINER_ID=1 python fluid_dist.py | lunch trainer instance whic id=1
