# 如何写一个Layer (Pure CPP)

这个文档是一个概念性的文档，描述如何使用Pure Cpp重构Paddle的配置解析，在此之上用户如何实现一个Layer。

## 基本目标

用户只需要写Layer的计算信息，不需要写配置解析器，就可以完成Layer的书写。

同时，Layer解析过程尽可能的快。


## 实现总体概览

* 在注册Layer的时候，不只注册Layer的C++类型，同时注册Layer的元信息。元信息是一个C++对象，是简单的 `std::unordered_map<std::string, std::any>`的组合。
* 使用全局静态函数生成Layer的元信息。
* 使用统一的C-API，让用户可以根据元信息，new出来一个layer，并做参数正确性检查
* 网络参数推导在new出一个Layer之后立即进行。

注意: 元信息是指描述某种信息的信息。譬如Layer的元信息是指描述某种Layer可以如何被描述的信息。而不是具体某一个Layer的实际描述。


## 实现细节

### 前置依赖

#### std::any

* [std::any](http://en.cppreference.com/w/cpp/utility/any)由于是 CPP 17的标准库，Paddle支持的最低CPP标准是CPP 11。所以可能需要手写一个简单的`std::any`。
	* std::any是一个可以放置任何类型的对象。

### 元信息

#### AttributeDef

某一个属性的元信息。即描述一个属性是何种类型，可以接受何种参数。其定义如下:


```cpp
struct AttributeDef {
   std::string name;
   std::type_info type;
   std::string description;
   std::any checkCallback;
};
```
其中 checkCallback是一个回调函数，他的类型是`(T* attr, bool setted) => paddle::Error`，因为输入的attr可以是任意类型的泛型，故这里用std::any表示类型。其中`setted`是表示这个参数是不是被用户设置过。

例如，对于dropout_rate的AttributeDef可以是:

```json
{
	"name": "dropout",
	"type": "float",
	"description": "Set drop out rate of layer. 1 means all activations are dropped, 0 means do not drop any activation",
	"checkCallback": function (float* attr, bool setted) => paddle::Error {
		if (!setted) {
			*attr = 0.0;  // default value.
		} else {
			if (0.0 <= *attr <= 1.0) {
				return paddle::Error::OK;
			} else {
				return paddle::Error("Dropout should be in [0.0, 1.0].");
			}
		}
	}
}
```

其中，对于`checkCallback`可以预定义一些常用checkCallback。譬如，同样对于dropout，可以定义为

```json
{
	...
	"checkCallback": paddle::AttributeDef::inRange<float>(name="dropout", min=0.0, max=1.0, default=0.0)
}
```

#### ParameterDef

定义一个输入参数(Parameter)的元信息。即这个参数可以支持哪些属性

```cpp
struct ParameterDef {
	std::vector<AttributeDef> attributes;
};
```

对于常见的ParameterDef为:

```json
[
	{
		"name": "name",
		"description": "The name of this parameter",
		"checkCallback": paddle::AttributeDef::notNull<std::string>()
	},
	{
		"name": "weight_decay",
		"description": "The weight decay rate of parameter, used to implement L2 Norm",
		"checkCallback": paddle::AttributeDef::inRange<float>(name="weight_decay", min=0.0, max=1.0, default=0.0)
	},
	{
		"name": "dims",
		"description": "The dimension of parameter",
		"checkCallback": function (std::vector<size_t>* dims, bool setted) {
			if (!setted) {
				return "Dims must be set".
			}
			if (dims->size() != 2) {
				return "Dims must be 2 in this parameter. They are height * width.";
			}
			return OK;
		}
	}
]

```


#### InputDef

定义一个Layer或者一个Op输入的元信息。即描述可以接受何种输入类型。

```cpp
struct IutputDef {
	std::vector<DataType> dataType;  // 可以接受的输入类型
	std::vector<SeqType> seqType; // 可以接受的sequence type
	bool repeatable;  // 该输入是否可以为多个。例如，对于fc layer，它的输入就是无限多个的。
	std::string name;
	std::string description;
	
	std::unique_ptr<ParameterDef> paramAttr; // 该输入参数的元信息，可以为空。为空表示该输入没有参数
};
```

例如，对于FC Layer的输入，可能定义为:

```json
{
    "name": "input",
    "description": "input fields of fc layer"
    "dataType": [Dense, Sparse, SparseBinary],
    "seqType": [0, 1, 2],
    "repeatable": true,
    "paramAttr" : [
    	{
    		name: "dims",
    		...
    	},
    	{
    		name: "weight_decay",
    		...
    	},
    	{
    		name: "gradient_clipping",
    		...
    	}
    ]
}
```

#### LayerDef

定义一个Layer需要的元信息，即描述这个Layer需要接受的参数有哪些。

```cpp
struct LayerDef {
	std::string type;
	std::string description;
	std::vector<IutputDef> inputs;
	std::unique_ptr<ParameterDef> biasAttr;
	std::vector<AttributeDef> attrs;
};
```

例如 FC Layer的如下表示

```json
{
	"type": "fc",
	"description": "Fully connected layer",
	"inputs": [...],  # just like IutputDef example
	"biasAttr": [{
		"name": "dims",
		...
	},{
		"name": "weight_decay",
		...
	},
		...
	],
	"attrs": [{
		"name": "dropout",
		...
	},
		...
	]
}
```

#### GraphDef

定义一个计算图需要的元信息

```cpp
struct GraphDef {
	std::unordered_map<std::string, AttributeDef> attrs;
};
```

### 具体对象

具体对象表示一个计算图里面里面一个具体层或者OP的描述。

#### ParameterAttributes

表示神经网络中，某一个参数(Parameter)的具体属性。

```cpp
struct ParameterAttributes {
	std::unordered_map<std::string, std::any> attrs;
};
```

可能的取值为:

```json
{
	"name": "fc.w",
	"dims": [784, 200],
	...
}
```
#### OutputAttribute

表示一个Layer/OP的输出属性。这个属性没有元信息。一个Layer或者OP的输出就是其他Layer/OP的输入。

```cpp
struct OutputAttribute {
	std::string name;  // 一个Layer可以有多个输出，但是他们的name不同。
	DataType dataType;
	SeqType seqType;
	size_t size;
	
	std::unordered_map<std::string, std::any> attrs;
}
```

可能的取值为

```json
{
	"name": "", // 空为这个Layer的默认输出
	"DataType": Dense,
	"SeqType": 0,
	"size": 784,
	"attrs" : {
		"num_channels": 1
	}
}
```


#### InputAttribute

表示一个Layer的输入数据属性。

```cpp
struct InputAttribute {
	std::string name;
	std::shared_ptr<LayerAttribute> inputLayer;
	std::string inputName;
	std::shared_ptr<ParameterAttributes> paramAttr;  // same param could be shared by multiple inputs
	std::unordered_map<std::string, std::any> attrs;  // extra input attr.
};
```

可能的取值为

```json
{
	"name": "input",
	"inputLayer": {
		"name": "fc1"
	},
	"inputName": "",
	"paramAttr": {
		"name": "fc.w",
		...
	},
	"attrs" : {
		"num_channels": 1
	}
}
```

#### LayerAttribute

表示一个Layer的属性

```cpp
struct LayerAttribute {
	std::vector<InputAttribute> inputs;
	std::vector<OutputAttribute> outputs;
	std::vector<std::shared_ptr<LayerAttribute>> preDepends;  # most dependencies are written in outputs. But we should add preDepends for some situation. for example, RecurrentLayerGroup.
	std::unordered_map<std::string, std::any> attrs;
};
```

可能的取值为:

```json
{
	"inputs": [{
		name: input,
		inputLayer: {
			name: pixel,
			...
		},
		paramAttr: {
			name: fc.w,
			...
		},
		...
	}],
	"outputs": [{
		name: "",
		size: 200,
		...
	}],
	attrs: {
		"size": 200,
		"activation": tanh,
		...
	},
	...
}
```

#### ComputationGraph

ComputationGraph为当前的计算图

```cpp
struct ComputationGraph {
	std::unordered_map<std::string, std::any> attrs;
	std::vector<std::shared_ptr<OutputAttribute>> outputs;
	std::vector<<std::shared_ptr<LayerAttribute>> extraLayers;  // extra layers are attached to this computation graph, but it is not the outputs.
};
```


## 用户定义一个新的Layer

### 定义Layer接受的参数

用户定义一个新的Layer需要定义这个Layer的元信息 LayerDef。定义方法为:

```cpp
class FCLayer :public Layer {
public:
  ...
  
  static void getLayerDefinition(LayerDef& def) {
    LayerDefinition::supportSize(def);
    LayerDefinition::supportDropout(def);
    LayerDefinition::addInput(def)
        .setRepeatable(True)
        .addSupport({ InputType::Dense, InputType::SparseInt, InputType::Sparse })
        .addSupportSeqLevel({0, 1, 2})
        .addDoc("FC Layer is fully connected. Blah blah blah...");
  }
};
```

### 定义参数推导过程

用户定义参数推导过程，定义方法为:

```cpp
class FCLayer :public Layer {
public:
  ...
  
  static paddle::Error calculateOutputAndParam(LayerAttribute& self) {
    // fill self.outputs by self.inputs.
    // also calculate self.inputs.parameters's size. etc.
  }
};
```

### Layer初始化

经过正确性检查的LayerAttribute会被用来初始化某一个Layer，初始化方法为:

```cpp
class FCLayer :public Layer {
public:
	void init(LayerAttribute& attr, Parameters& params) {
	...
	}
	...
};
```

### 注册Layer

直接注册这个Layer的类型即可

```cpp
REGISTER_LAYER(fc, FCLayer);
```

## 用户配置与解析

对于Layer的配置，只有如下几个接口。他们是

```cpp
extern "C" {

typedef void* Symbol;

Symbol newSymbol(int symbolType);
void setAttribute(void* symbol, const char* path, int type_id, void* value);
void appendAttribute(void* symbol, const char* path, int type_id, void* value);
void destroySymbol(Symbol sym);
}
```
其中Symbol是`ParameterAttribute`, `InputAttribute`, `OutputAttribute`, `LayerAttribute`, `ComputationGraph`的通称

简单的使用样例如下:

```cpp
auto paramAttr = newSymbol(PARAMETER_ATTRIBUTE)
setAttribute(paramAttr, "name", STRING_TYPE, "fc.w");
appendAttribute(paramAttr, "dims", INT_TYPE, 784);
appendAttribute(paramAttr, "dims", INT_TYPE, 200);

auto inputAttr = newSymbol(INPUT_ATTRIBUTE);
setAttribute(inputAttr, "name", STRING_TYPE, "input");
setAttribute(inputAttr, "paramAttr", SYMBOL_TYPE, paramAttr);
setAttribute(inputAttr, "inputLayer", SYMBOL_TYPE, dataLayerAttr);

auto layerAttr = newSymbol(LAYER_ATTRIBUTE);
setAttribute(layerAttr, "type", STRING_TYPE, "fc");
setAttribute(layerAttr, "name", STRING_TYPE, "fc");
setAttribute(layerAttr, "size", INT_TYPE, 200);
setAttribute(layerAttr, "activation", INT_TYPE, 0);  // 0 means sigmoid, etc.
appendAttribute(layerAttr, "inputs", SYMBOL_TYPE, inputAttr);


auto graph = newSymbol(GRAPH_ATTRIBUTE);
appendAttribute(layerAttr, "outputs", SYMBOL_TYPE, layerAttr);

auto engine = newExecuteEngine(graph);
engine.forward()
engine.backward()
parameters.update()
```
