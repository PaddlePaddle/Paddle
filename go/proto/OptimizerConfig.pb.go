// Code generated by protoc-gen-go. DO NOT EDIT.
// source: OptimizerConfig.proto

/*
Package paddle is a generated protocol buffer package.

It is generated from these files:
	OptimizerConfig.proto

It has these top-level messages:
	SGDConfig
	AdadeltaConfig
	AdagradConfig
	AdamConfig
	ConstLrConfig
	LinearLrConfig
	TensorProto
	LrPolicyState
	SGDOptimizerState
	AdadeltaOptimizerState
	AdagradOptimizerState
	AdamOptimizerState
	OptimizerConfig
*/
package paddle

import proto "github.com/golang/protobuf/proto"
import fmt "fmt"
import math "math"

// Reference imports to suppress errors if they are not otherwise used.
var _ = proto.Marshal
var _ = fmt.Errorf
var _ = math.Inf

// This is a compile-time assertion to ensure that this generated file
// is compatible with the proto package it is being compiled against.
// A compilation error at this line likely means your copy of the
// proto package needs to be updated.
const _ = proto.ProtoPackageIsVersion2 // please upgrade the proto package

type TensorProto_DataType int32

const (
	TensorProto_PADDLE_ELEMENT_TYPE_INT32   TensorProto_DataType = 0
	TensorProto_PADDLE_ELEMENT_TYPE_UINT32  TensorProto_DataType = 1
	TensorProto_PADDLE_ELEMENT_TYPE_INT64   TensorProto_DataType = 2
	TensorProto_PADDLE_ELEMENT_TYPE_UINT64  TensorProto_DataType = 3
	TensorProto_PADDLE_ELEMENT_TYPE_FLOAT32 TensorProto_DataType = 4
	TensorProto_PADDLE_ELEMENT_TYPE_FLOAT64 TensorProto_DataType = 5
)

var TensorProto_DataType_name = map[int32]string{
	0: "PADDLE_ELEMENT_TYPE_INT32",
	1: "PADDLE_ELEMENT_TYPE_UINT32",
	2: "PADDLE_ELEMENT_TYPE_INT64",
	3: "PADDLE_ELEMENT_TYPE_UINT64",
	4: "PADDLE_ELEMENT_TYPE_FLOAT32",
	5: "PADDLE_ELEMENT_TYPE_FLOAT64",
}
var TensorProto_DataType_value = map[string]int32{
	"PADDLE_ELEMENT_TYPE_INT32":   0,
	"PADDLE_ELEMENT_TYPE_UINT32":  1,
	"PADDLE_ELEMENT_TYPE_INT64":   2,
	"PADDLE_ELEMENT_TYPE_UINT64":  3,
	"PADDLE_ELEMENT_TYPE_FLOAT32": 4,
	"PADDLE_ELEMENT_TYPE_FLOAT64": 5,
}

func (x TensorProto_DataType) Enum() *TensorProto_DataType {
	p := new(TensorProto_DataType)
	*p = x
	return p
}
func (x TensorProto_DataType) String() string {
	return proto.EnumName(TensorProto_DataType_name, int32(x))
}
func (x *TensorProto_DataType) UnmarshalJSON(data []byte) error {
	value, err := proto.UnmarshalJSONEnum(TensorProto_DataType_value, data, "TensorProto_DataType")
	if err != nil {
		return err
	}
	*x = TensorProto_DataType(value)
	return nil
}
func (TensorProto_DataType) EnumDescriptor() ([]byte, []int) { return fileDescriptor0, []int{6, 0} }

type OptimizerConfig_Optimizer int32

const (
	OptimizerConfig_SGD      OptimizerConfig_Optimizer = 1
	OptimizerConfig_Adadelta OptimizerConfig_Optimizer = 2
	OptimizerConfig_Adagrad  OptimizerConfig_Optimizer = 3
	OptimizerConfig_Adam     OptimizerConfig_Optimizer = 4
)

var OptimizerConfig_Optimizer_name = map[int32]string{
	1: "SGD",
	2: "Adadelta",
	3: "Adagrad",
	4: "Adam",
}
var OptimizerConfig_Optimizer_value = map[string]int32{
	"SGD":      1,
	"Adadelta": 2,
	"Adagrad":  3,
	"Adam":     4,
}

func (x OptimizerConfig_Optimizer) Enum() *OptimizerConfig_Optimizer {
	p := new(OptimizerConfig_Optimizer)
	*p = x
	return p
}
func (x OptimizerConfig_Optimizer) String() string {
	return proto.EnumName(OptimizerConfig_Optimizer_name, int32(x))
}
func (x *OptimizerConfig_Optimizer) UnmarshalJSON(data []byte) error {
	value, err := proto.UnmarshalJSONEnum(OptimizerConfig_Optimizer_value, data, "OptimizerConfig_Optimizer")
	if err != nil {
		return err
	}
	*x = OptimizerConfig_Optimizer(value)
	return nil
}
func (OptimizerConfig_Optimizer) EnumDescriptor() ([]byte, []int) {
	return fileDescriptor0, []int{12, 0}
}

type OptimizerConfig_LrPolicy int32

const (
	OptimizerConfig_Const  OptimizerConfig_LrPolicy = 0
	OptimizerConfig_Linear OptimizerConfig_LrPolicy = 1
)

var OptimizerConfig_LrPolicy_name = map[int32]string{
	0: "Const",
	1: "Linear",
}
var OptimizerConfig_LrPolicy_value = map[string]int32{
	"Const":  0,
	"Linear": 1,
}

func (x OptimizerConfig_LrPolicy) Enum() *OptimizerConfig_LrPolicy {
	p := new(OptimizerConfig_LrPolicy)
	*p = x
	return p
}
func (x OptimizerConfig_LrPolicy) String() string {
	return proto.EnumName(OptimizerConfig_LrPolicy_name, int32(x))
}
func (x *OptimizerConfig_LrPolicy) UnmarshalJSON(data []byte) error {
	value, err := proto.UnmarshalJSONEnum(OptimizerConfig_LrPolicy_value, data, "OptimizerConfig_LrPolicy")
	if err != nil {
		return err
	}
	*x = OptimizerConfig_LrPolicy(value)
	return nil
}
func (OptimizerConfig_LrPolicy) EnumDescriptor() ([]byte, []int) { return fileDescriptor0, []int{12, 1} }

type SGDConfig struct {
	// SGD
	// momentum: float >= 0. Parameter updates momentum.
	// decay: float >= 0. Learning rate decay over each update.
	// nesterov: boolean. Whether to apply Nesterov momentum.
	Momentum         *float64 `protobuf:"fixed64,21,opt,name=momentum,def=0" json:"momentum,omitempty"`
	Decay            *float64 `protobuf:"fixed64,23,opt,name=decay,def=0" json:"decay,omitempty"`
	Nesterov         *bool    `protobuf:"varint,24,opt,name=nesterov,def=0" json:"nesterov,omitempty"`
	XXX_unrecognized []byte   `json:"-"`
}

func (m *SGDConfig) Reset()                    { *m = SGDConfig{} }
func (m *SGDConfig) String() string            { return proto.CompactTextString(m) }
func (*SGDConfig) ProtoMessage()               {}
func (*SGDConfig) Descriptor() ([]byte, []int) { return fileDescriptor0, []int{0} }

const Default_SGDConfig_Momentum float64 = 0
const Default_SGDConfig_Decay float64 = 0
const Default_SGDConfig_Nesterov bool = false

func (m *SGDConfig) GetMomentum() float64 {
	if m != nil && m.Momentum != nil {
		return *m.Momentum
	}
	return Default_SGDConfig_Momentum
}

func (m *SGDConfig) GetDecay() float64 {
	if m != nil && m.Decay != nil {
		return *m.Decay
	}
	return Default_SGDConfig_Decay
}

func (m *SGDConfig) GetNesterov() bool {
	if m != nil && m.Nesterov != nil {
		return *m.Nesterov
	}
	return Default_SGDConfig_Nesterov
}

type AdadeltaConfig struct {
	// reference : [Adadelta - an adaptive learning rate
	// method](http://arxiv.org/abs/1212.5701)
	Rho              *float64 `protobuf:"fixed64,33,opt,name=rho,def=0.9" json:"rho,omitempty"`
	Epsilon          *float64 `protobuf:"fixed64,31,opt,name=epsilon,def=1e-05" json:"epsilon,omitempty"`
	Decay            *float64 `protobuf:"fixed64,32,opt,name=decay,def=0" json:"decay,omitempty"`
	XXX_unrecognized []byte   `json:"-"`
}

func (m *AdadeltaConfig) Reset()                    { *m = AdadeltaConfig{} }
func (m *AdadeltaConfig) String() string            { return proto.CompactTextString(m) }
func (*AdadeltaConfig) ProtoMessage()               {}
func (*AdadeltaConfig) Descriptor() ([]byte, []int) { return fileDescriptor0, []int{1} }

const Default_AdadeltaConfig_Rho float64 = 0.9
const Default_AdadeltaConfig_Epsilon float64 = 1e-05
const Default_AdadeltaConfig_Decay float64 = 0

func (m *AdadeltaConfig) GetRho() float64 {
	if m != nil && m.Rho != nil {
		return *m.Rho
	}
	return Default_AdadeltaConfig_Rho
}

func (m *AdadeltaConfig) GetEpsilon() float64 {
	if m != nil && m.Epsilon != nil {
		return *m.Epsilon
	}
	return Default_AdadeltaConfig_Epsilon
}

func (m *AdadeltaConfig) GetDecay() float64 {
	if m != nil && m.Decay != nil {
		return *m.Decay
	}
	return Default_AdadeltaConfig_Decay
}

type AdagradConfig struct {
	// reference : [Adaptive Subgradient Methods for Online Learning and
	// Stochastic
	// Optimization](http://www.jmlr.org/papers/volume12/duchi11a/duchi11a.pdf)
	Epsilon          *float64 `protobuf:"fixed64,41,opt,name=epsilon,def=1e-05" json:"epsilon,omitempty"`
	Decay            *float64 `protobuf:"fixed64,42,opt,name=decay,def=0" json:"decay,omitempty"`
	XXX_unrecognized []byte   `json:"-"`
}

func (m *AdagradConfig) Reset()                    { *m = AdagradConfig{} }
func (m *AdagradConfig) String() string            { return proto.CompactTextString(m) }
func (*AdagradConfig) ProtoMessage()               {}
func (*AdagradConfig) Descriptor() ([]byte, []int) { return fileDescriptor0, []int{2} }

const Default_AdagradConfig_Epsilon float64 = 1e-05
const Default_AdagradConfig_Decay float64 = 0

func (m *AdagradConfig) GetEpsilon() float64 {
	if m != nil && m.Epsilon != nil {
		return *m.Epsilon
	}
	return Default_AdagradConfig_Epsilon
}

func (m *AdagradConfig) GetDecay() float64 {
	if m != nil && m.Decay != nil {
		return *m.Decay
	}
	return Default_AdagradConfig_Decay
}

type AdamConfig struct {
	// Adaj
	// beta_1: float, 0 < beta < 1. Generally close to 1.
	// beta_2: float, 0 < beta < 1. Generally close to 1.
	// epsilon: float >= 0. Fuzz factor.
	// decay: float >= 0. Learning rate decay over each update.
	// reference : [Adam - A Method for Stochastic
	// Optimization](http://arxiv.org/abs/1412.6980v8)
	Beta_1           *float64 `protobuf:"fixed64,41,opt,name=beta_1,json=beta1" json:"beta_1,omitempty"`
	Beta_2           *float64 `protobuf:"fixed64,42,opt,name=beta_2,json=beta2" json:"beta_2,omitempty"`
	Epsilon          *float64 `protobuf:"fixed64,43,opt,name=epsilon" json:"epsilon,omitempty"`
	Decay            *float64 `protobuf:"fixed64,44,opt,name=decay" json:"decay,omitempty"`
	XXX_unrecognized []byte   `json:"-"`
}

func (m *AdamConfig) Reset()                    { *m = AdamConfig{} }
func (m *AdamConfig) String() string            { return proto.CompactTextString(m) }
func (*AdamConfig) ProtoMessage()               {}
func (*AdamConfig) Descriptor() ([]byte, []int) { return fileDescriptor0, []int{3} }

func (m *AdamConfig) GetBeta_1() float64 {
	if m != nil && m.Beta_1 != nil {
		return *m.Beta_1
	}
	return 0
}

func (m *AdamConfig) GetBeta_2() float64 {
	if m != nil && m.Beta_2 != nil {
		return *m.Beta_2
	}
	return 0
}

func (m *AdamConfig) GetEpsilon() float64 {
	if m != nil && m.Epsilon != nil {
		return *m.Epsilon
	}
	return 0
}

func (m *AdamConfig) GetDecay() float64 {
	if m != nil && m.Decay != nil {
		return *m.Decay
	}
	return 0
}

type ConstLrConfig struct {
	// learninRate Policy
	LearningRate     *float64 `protobuf:"fixed64,1,opt,name=learning_rate,json=learningRate,def=1" json:"learning_rate,omitempty"`
	XXX_unrecognized []byte   `json:"-"`
}

func (m *ConstLrConfig) Reset()                    { *m = ConstLrConfig{} }
func (m *ConstLrConfig) String() string            { return proto.CompactTextString(m) }
func (*ConstLrConfig) ProtoMessage()               {}
func (*ConstLrConfig) Descriptor() ([]byte, []int) { return fileDescriptor0, []int{4} }

const Default_ConstLrConfig_LearningRate float64 = 1

func (m *ConstLrConfig) GetLearningRate() float64 {
	if m != nil && m.LearningRate != nil {
		return *m.LearningRate
	}
	return Default_ConstLrConfig_LearningRate
}

type LinearLrConfig struct {
	// learninRate Policy
	LearningRate     *float64 `protobuf:"fixed64,1,opt,name=learning_rate,json=learningRate,def=1" json:"learning_rate,omitempty"`
	LrDecayA         *float64 `protobuf:"fixed64,2,opt,name=lr_decay_a,json=lrDecayA" json:"lr_decay_a,omitempty"`
	LrDecayB         *float64 `protobuf:"fixed64,3,opt,name=lr_decay_b,json=lrDecayB" json:"lr_decay_b,omitempty"`
	XXX_unrecognized []byte   `json:"-"`
}

func (m *LinearLrConfig) Reset()                    { *m = LinearLrConfig{} }
func (m *LinearLrConfig) String() string            { return proto.CompactTextString(m) }
func (*LinearLrConfig) ProtoMessage()               {}
func (*LinearLrConfig) Descriptor() ([]byte, []int) { return fileDescriptor0, []int{5} }

const Default_LinearLrConfig_LearningRate float64 = 1

func (m *LinearLrConfig) GetLearningRate() float64 {
	if m != nil && m.LearningRate != nil {
		return *m.LearningRate
	}
	return Default_LinearLrConfig_LearningRate
}

func (m *LinearLrConfig) GetLrDecayA() float64 {
	if m != nil && m.LrDecayA != nil {
		return *m.LrDecayA
	}
	return 0
}

func (m *LinearLrConfig) GetLrDecayB() float64 {
	if m != nil && m.LrDecayB != nil {
		return *m.LrDecayB
	}
	return 0
}

type TensorProto struct {
	DataType         *TensorProto_DataType `protobuf:"varint,1,opt,name=data_type,json=dataType,enum=paddle.TensorProto_DataType" json:"data_type,omitempty"`
	Content          [][]byte              `protobuf:"bytes,2,rep,name=content" json:"content,omitempty"`
	XXX_unrecognized []byte                `json:"-"`
}

func (m *TensorProto) Reset()                    { *m = TensorProto{} }
func (m *TensorProto) String() string            { return proto.CompactTextString(m) }
func (*TensorProto) ProtoMessage()               {}
func (*TensorProto) Descriptor() ([]byte, []int) { return fileDescriptor0, []int{6} }

func (m *TensorProto) GetDataType() TensorProto_DataType {
	if m != nil && m.DataType != nil {
		return *m.DataType
	}
	return TensorProto_PADDLE_ELEMENT_TYPE_INT32
}

func (m *TensorProto) GetContent() [][]byte {
	if m != nil {
		return m.Content
	}
	return nil
}

type LrPolicyState struct {
	// learninRate Policy
	LearningRate     *float64 `protobuf:"fixed64,1,opt,name=learning_rate,json=learningRate,def=1" json:"learning_rate,omitempty"`
	LrDecayA         *float64 `protobuf:"fixed64,2,opt,name=lr_decay_a,json=lrDecayA" json:"lr_decay_a,omitempty"`
	LrDecayB         *float64 `protobuf:"fixed64,3,opt,name=lr_decay_b,json=lrDecayB" json:"lr_decay_b,omitempty"`
	XXX_unrecognized []byte   `json:"-"`
}

func (m *LrPolicyState) Reset()                    { *m = LrPolicyState{} }
func (m *LrPolicyState) String() string            { return proto.CompactTextString(m) }
func (*LrPolicyState) ProtoMessage()               {}
func (*LrPolicyState) Descriptor() ([]byte, []int) { return fileDescriptor0, []int{7} }

const Default_LrPolicyState_LearningRate float64 = 1

func (m *LrPolicyState) GetLearningRate() float64 {
	if m != nil && m.LearningRate != nil {
		return *m.LearningRate
	}
	return Default_LrPolicyState_LearningRate
}

func (m *LrPolicyState) GetLrDecayA() float64 {
	if m != nil && m.LrDecayA != nil {
		return *m.LrDecayA
	}
	return 0
}

func (m *LrPolicyState) GetLrDecayB() float64 {
	if m != nil && m.LrDecayB != nil {
		return *m.LrDecayB
	}
	return 0
}

type SGDOptimizerState struct {
	LrState         *LrPolicyState `protobuf:"bytes,101,opt,name=lr_state,json=lrState" json:"lr_state,omitempty"`
	NumSamplePassed *float64       `protobuf:"fixed64,104,opt,name=num_sample_passed,json=numSamplePassed" json:"num_sample_passed,omitempty"`
	// state
	Parameter        *TensorProto `protobuf:"bytes,1,opt,name=parameter" json:"parameter,omitempty"`
	Momentums        *TensorProto `protobuf:"bytes,2,opt,name=momentums" json:"momentums,omitempty"`
	XXX_unrecognized []byte       `json:"-"`
}

func (m *SGDOptimizerState) Reset()                    { *m = SGDOptimizerState{} }
func (m *SGDOptimizerState) String() string            { return proto.CompactTextString(m) }
func (*SGDOptimizerState) ProtoMessage()               {}
func (*SGDOptimizerState) Descriptor() ([]byte, []int) { return fileDescriptor0, []int{8} }

func (m *SGDOptimizerState) GetLrState() *LrPolicyState {
	if m != nil {
		return m.LrState
	}
	return nil
}

func (m *SGDOptimizerState) GetNumSamplePassed() float64 {
	if m != nil && m.NumSamplePassed != nil {
		return *m.NumSamplePassed
	}
	return 0
}

func (m *SGDOptimizerState) GetParameter() *TensorProto {
	if m != nil {
		return m.Parameter
	}
	return nil
}

func (m *SGDOptimizerState) GetMomentums() *TensorProto {
	if m != nil {
		return m.Momentums
	}
	return nil
}

type AdadeltaOptimizerState struct {
	// learning rate policy
	LrState         *LrPolicyState `protobuf:"bytes,101,opt,name=lr_state,json=lrState" json:"lr_state,omitempty"`
	NumSamplePassed *float64       `protobuf:"fixed64,104,opt,name=num_sample_passed,json=numSamplePassed" json:"num_sample_passed,omitempty"`
	// state
	Parameter        *TensorProto `protobuf:"bytes,1,opt,name=parameter" json:"parameter,omitempty"`
	AccumGradient    *TensorProto `protobuf:"bytes,2,opt,name=accum_gradient,json=accumGradient" json:"accum_gradient,omitempty"`
	AccumDelta       *TensorProto `protobuf:"bytes,3,opt,name=accum_delta,json=accumDelta" json:"accum_delta,omitempty"`
	UpdateDelta      *TensorProto `protobuf:"bytes,4,opt,name=update_delta,json=updateDelta" json:"update_delta,omitempty"`
	XXX_unrecognized []byte       `json:"-"`
}

func (m *AdadeltaOptimizerState) Reset()                    { *m = AdadeltaOptimizerState{} }
func (m *AdadeltaOptimizerState) String() string            { return proto.CompactTextString(m) }
func (*AdadeltaOptimizerState) ProtoMessage()               {}
func (*AdadeltaOptimizerState) Descriptor() ([]byte, []int) { return fileDescriptor0, []int{9} }

func (m *AdadeltaOptimizerState) GetLrState() *LrPolicyState {
	if m != nil {
		return m.LrState
	}
	return nil
}

func (m *AdadeltaOptimizerState) GetNumSamplePassed() float64 {
	if m != nil && m.NumSamplePassed != nil {
		return *m.NumSamplePassed
	}
	return 0
}

func (m *AdadeltaOptimizerState) GetParameter() *TensorProto {
	if m != nil {
		return m.Parameter
	}
	return nil
}

func (m *AdadeltaOptimizerState) GetAccumGradient() *TensorProto {
	if m != nil {
		return m.AccumGradient
	}
	return nil
}

func (m *AdadeltaOptimizerState) GetAccumDelta() *TensorProto {
	if m != nil {
		return m.AccumDelta
	}
	return nil
}

func (m *AdadeltaOptimizerState) GetUpdateDelta() *TensorProto {
	if m != nil {
		return m.UpdateDelta
	}
	return nil
}

type AdagradOptimizerState struct {
	LrState         *LrPolicyState `protobuf:"bytes,101,opt,name=lr_state,json=lrState" json:"lr_state,omitempty"`
	NumSamplePassed *float64       `protobuf:"fixed64,104,opt,name=num_sample_passed,json=numSamplePassed" json:"num_sample_passed,omitempty"`
	// state
	Parameter        *TensorProto `protobuf:"bytes,1,opt,name=parameter" json:"parameter,omitempty"`
	AccumGradient    *TensorProto `protobuf:"bytes,2,opt,name=accum_gradient,json=accumGradient" json:"accum_gradient,omitempty"`
	XXX_unrecognized []byte       `json:"-"`
}

func (m *AdagradOptimizerState) Reset()                    { *m = AdagradOptimizerState{} }
func (m *AdagradOptimizerState) String() string            { return proto.CompactTextString(m) }
func (*AdagradOptimizerState) ProtoMessage()               {}
func (*AdagradOptimizerState) Descriptor() ([]byte, []int) { return fileDescriptor0, []int{10} }

func (m *AdagradOptimizerState) GetLrState() *LrPolicyState {
	if m != nil {
		return m.LrState
	}
	return nil
}

func (m *AdagradOptimizerState) GetNumSamplePassed() float64 {
	if m != nil && m.NumSamplePassed != nil {
		return *m.NumSamplePassed
	}
	return 0
}

func (m *AdagradOptimizerState) GetParameter() *TensorProto {
	if m != nil {
		return m.Parameter
	}
	return nil
}

func (m *AdagradOptimizerState) GetAccumGradient() *TensorProto {
	if m != nil {
		return m.AccumGradient
	}
	return nil
}

type AdamOptimizerState struct {
	LrState         *LrPolicyState `protobuf:"bytes,101,opt,name=lr_state,json=lrState" json:"lr_state,omitempty"`
	NumSamplePassed *float64       `protobuf:"fixed64,104,opt,name=num_sample_passed,json=numSamplePassed" json:"num_sample_passed,omitempty"`
	// state
	Parameter        *TensorProto `protobuf:"bytes,1,opt,name=parameter" json:"parameter,omitempty"`
	Momentums        *TensorProto `protobuf:"bytes,2,opt,name=momentums" json:"momentums,omitempty"`
	Velocitys        *TensorProto `protobuf:"bytes,3,opt,name=velocitys" json:"velocitys,omitempty"`
	XXX_unrecognized []byte       `json:"-"`
}

func (m *AdamOptimizerState) Reset()                    { *m = AdamOptimizerState{} }
func (m *AdamOptimizerState) String() string            { return proto.CompactTextString(m) }
func (*AdamOptimizerState) ProtoMessage()               {}
func (*AdamOptimizerState) Descriptor() ([]byte, []int) { return fileDescriptor0, []int{11} }

func (m *AdamOptimizerState) GetLrState() *LrPolicyState {
	if m != nil {
		return m.LrState
	}
	return nil
}

func (m *AdamOptimizerState) GetNumSamplePassed() float64 {
	if m != nil && m.NumSamplePassed != nil {
		return *m.NumSamplePassed
	}
	return 0
}

func (m *AdamOptimizerState) GetParameter() *TensorProto {
	if m != nil {
		return m.Parameter
	}
	return nil
}

func (m *AdamOptimizerState) GetMomentums() *TensorProto {
	if m != nil {
		return m.Momentums
	}
	return nil
}

func (m *AdamOptimizerState) GetVelocitys() *TensorProto {
	if m != nil {
		return m.Velocitys
	}
	return nil
}

type OptimizerConfig struct {
	Optimizer *OptimizerConfig_Optimizer `protobuf:"varint,1,opt,name=optimizer,enum=paddle.OptimizerConfig_Optimizer" json:"optimizer,omitempty"`
	Sgd       *SGDConfig                 `protobuf:"bytes,3,opt,name=sgd" json:"sgd,omitempty"`
	Adadelta  *AdadeltaConfig            `protobuf:"bytes,4,opt,name=adadelta" json:"adadelta,omitempty"`
	Adagrad   *AdagradConfig             `protobuf:"bytes,5,opt,name=adagrad" json:"adagrad,omitempty"`
	Adam      *AdamConfig                `protobuf:"bytes,6,opt,name=adam" json:"adam,omitempty"`
	LrPolicy  *OptimizerConfig_LrPolicy  `protobuf:"varint,11,opt,name=lr_policy,json=lrPolicy,enum=paddle.OptimizerConfig_LrPolicy" json:"lr_policy,omitempty"`
	ConstLr   *ConstLrConfig             `protobuf:"bytes,12,opt,name=const_lr,json=constLr" json:"const_lr,omitempty"`
	LinearLr  *LinearLrConfig            `protobuf:"bytes,13,opt,name=linear_lr,json=linearLr" json:"linear_lr,omitempty"`
	// common config of optimizer
	// gradient clip when L2 exceeding value
	ClipNorm *float64 `protobuf:"fixed64,101,opt,name=clip_norm,json=clipNorm" json:"clip_norm,omitempty"`
	// gradient clip when L1 exceeding value
	ClipValue        *float64 `protobuf:"fixed64,102,opt,name=clip_value,json=clipValue" json:"clip_value,omitempty"`
	XXX_unrecognized []byte   `json:"-"`
}

func (m *OptimizerConfig) Reset()                    { *m = OptimizerConfig{} }
func (m *OptimizerConfig) String() string            { return proto.CompactTextString(m) }
func (*OptimizerConfig) ProtoMessage()               {}
func (*OptimizerConfig) Descriptor() ([]byte, []int) { return fileDescriptor0, []int{12} }

func (m *OptimizerConfig) GetOptimizer() OptimizerConfig_Optimizer {
	if m != nil && m.Optimizer != nil {
		return *m.Optimizer
	}
	return OptimizerConfig_SGD
}

func (m *OptimizerConfig) GetSgd() *SGDConfig {
	if m != nil {
		return m.Sgd
	}
	return nil
}

func (m *OptimizerConfig) GetAdadelta() *AdadeltaConfig {
	if m != nil {
		return m.Adadelta
	}
	return nil
}

func (m *OptimizerConfig) GetAdagrad() *AdagradConfig {
	if m != nil {
		return m.Adagrad
	}
	return nil
}

func (m *OptimizerConfig) GetAdam() *AdamConfig {
	if m != nil {
		return m.Adam
	}
	return nil
}

func (m *OptimizerConfig) GetLrPolicy() OptimizerConfig_LrPolicy {
	if m != nil && m.LrPolicy != nil {
		return *m.LrPolicy
	}
	return OptimizerConfig_Const
}

func (m *OptimizerConfig) GetConstLr() *ConstLrConfig {
	if m != nil {
		return m.ConstLr
	}
	return nil
}

func (m *OptimizerConfig) GetLinearLr() *LinearLrConfig {
	if m != nil {
		return m.LinearLr
	}
	return nil
}

func (m *OptimizerConfig) GetClipNorm() float64 {
	if m != nil && m.ClipNorm != nil {
		return *m.ClipNorm
	}
	return 0
}

func (m *OptimizerConfig) GetClipValue() float64 {
	if m != nil && m.ClipValue != nil {
		return *m.ClipValue
	}
	return 0
}

func init() {
	proto.RegisterType((*SGDConfig)(nil), "paddle.SGDConfig")
	proto.RegisterType((*AdadeltaConfig)(nil), "paddle.AdadeltaConfig")
	proto.RegisterType((*AdagradConfig)(nil), "paddle.AdagradConfig")
	proto.RegisterType((*AdamConfig)(nil), "paddle.AdamConfig")
	proto.RegisterType((*ConstLrConfig)(nil), "paddle.ConstLrConfig")
	proto.RegisterType((*LinearLrConfig)(nil), "paddle.LinearLrConfig")
	proto.RegisterType((*TensorProto)(nil), "paddle.TensorProto")
	proto.RegisterType((*LrPolicyState)(nil), "paddle.LrPolicyState")
	proto.RegisterType((*SGDOptimizerState)(nil), "paddle.SGDOptimizerState")
	proto.RegisterType((*AdadeltaOptimizerState)(nil), "paddle.AdadeltaOptimizerState")
	proto.RegisterType((*AdagradOptimizerState)(nil), "paddle.AdagradOptimizerState")
	proto.RegisterType((*AdamOptimizerState)(nil), "paddle.AdamOptimizerState")
	proto.RegisterType((*OptimizerConfig)(nil), "paddle.OptimizerConfig")
	proto.RegisterEnum("paddle.TensorProto_DataType", TensorProto_DataType_name, TensorProto_DataType_value)
	proto.RegisterEnum("paddle.OptimizerConfig_Optimizer", OptimizerConfig_Optimizer_name, OptimizerConfig_Optimizer_value)
	proto.RegisterEnum("paddle.OptimizerConfig_LrPolicy", OptimizerConfig_LrPolicy_name, OptimizerConfig_LrPolicy_value)
}

func init() { proto.RegisterFile("OptimizerConfig.proto", fileDescriptor0) }

var fileDescriptor0 = []byte{
	// 918 bytes of a gzipped FileDescriptorProto
	0x1f, 0x8b, 0x08, 0x00, 0x00, 0x00, 0x00, 0x00, 0x02, 0xff, 0xdc, 0x56, 0xdd, 0x6e, 0x1b, 0x45,
	0x14, 0xee, 0xfa, 0x27, 0xde, 0x3d, 0xb6, 0xd3, 0xed, 0x80, 0xdb, 0x85, 0x36, 0xc4, 0x59, 0xa4,
	0x2a, 0x14, 0x30, 0xb1, 0x1b, 0x82, 0x12, 0x09, 0x21, 0xb7, 0x36, 0xa1, 0x92, 0x49, 0xad, 0xb1,
	0x41, 0xe2, 0x6a, 0x35, 0xf5, 0x4e, 0xd2, 0x45, 0xb3, 0x3f, 0x9a, 0x1d, 0x47, 0x32, 0xaf, 0xc0,
	0xc3, 0xf0, 0x12, 0x5c, 0x71, 0xcb, 0x05, 0x2f, 0xc0, 0x83, 0xa0, 0x99, 0xd9, 0x5d, 0x67, 0xa3,
	0xd8, 0x12, 0x37, 0x08, 0xf5, 0x6e, 0xcf, 0x39, 0xdf, 0x77, 0x7e, 0x66, 0xce, 0x9e, 0x33, 0xd0,
	0x79, 0x9d, 0x88, 0x20, 0x0c, 0x7e, 0xa1, 0xfc, 0x65, 0x1c, 0x5d, 0x06, 0x57, 0xbd, 0x84, 0xc7,
	0x22, 0x46, 0x3b, 0x09, 0xf1, 0x7d, 0x46, 0xdd, 0x4b, 0xb0, 0x66, 0xe7, 0x23, 0x6d, 0x42, 0x7b,
	0x60, 0x86, 0x71, 0x48, 0x23, 0xb1, 0x0c, 0x9d, 0x4e, 0xd7, 0x38, 0x34, 0xce, 0x8c, 0x23, 0x5c,
	0xa8, 0xd0, 0x23, 0xa8, 0xfb, 0x74, 0x41, 0x56, 0xce, 0xa3, 0xdc, 0xa6, 0x65, 0x74, 0x00, 0x66,
	0x44, 0x53, 0x41, 0x79, 0x7c, 0xed, 0x38, 0x5d, 0xe3, 0xd0, 0x3c, 0xab, 0x5f, 0x12, 0x96, 0x52,
	0x5c, 0xa8, 0x5d, 0x02, 0xbb, 0x43, 0x9f, 0xf8, 0x94, 0x09, 0x92, 0x05, 0xeb, 0x40, 0x95, 0xbf,
	0x8d, 0x9d, 0x03, 0xe5, 0xab, 0x7a, 0xd4, 0x3b, 0xc5, 0x52, 0x46, 0xfb, 0xd0, 0xa0, 0x49, 0x1a,
	0xb0, 0x38, 0x72, 0xf6, 0x95, 0xa9, 0xde, 0xa7, 0x9f, 0x1f, 0x7d, 0x89, 0x73, 0xed, 0x3a, 0x8b,
	0x6e, 0x39, 0x0b, 0xf7, 0x15, 0xb4, 0x87, 0x3e, 0xb9, 0xe2, 0xc4, 0xcf, 0x22, 0xdc, 0x70, 0xf5,
	0xc9, 0x76, 0x57, 0xcf, 0x6e, 0xb9, 0xfa, 0x19, 0x60, 0xe8, 0x93, 0xb0, 0xc8, 0x74, 0xe7, 0x0d,
	0x15, 0xc4, 0xeb, 0x6b, 0x37, 0xb8, 0x2e, 0xa5, 0x7e, 0xa1, 0x1e, 0x68, 0xba, 0x56, 0x0f, 0x90,
	0xb3, 0x8e, 0xfa, 0xa9, 0xd2, 0x17, 0xe1, 0xde, 0xcf, 0xc3, 0x7d, 0xa6, 0xf1, 0x3a, 0xd6, 0x57,
	0xd0, 0x7e, 0x19, 0x47, 0xa9, 0x98, 0x64, 0x17, 0x84, 0x9e, 0x42, 0x9b, 0x51, 0xc2, 0xa3, 0x20,
	0xba, 0xf2, 0x38, 0x11, 0xd4, 0x31, 0x74, 0x76, 0x7d, 0xdc, 0xca, 0xf5, 0x98, 0x08, 0xea, 0x0a,
	0xd8, 0x9d, 0x04, 0x11, 0x25, 0xfc, 0xdf, 0x32, 0xd1, 0x13, 0x00, 0xc6, 0x3d, 0x15, 0xde, 0x23,
	0x4e, 0x45, 0x65, 0x63, 0x32, 0x3e, 0x92, 0x8a, 0x61, 0xc9, 0xfa, 0xc6, 0xa9, 0x96, 0xac, 0x2f,
	0xdc, 0xdf, 0x2a, 0xd0, 0x9c, 0xd3, 0x28, 0x8d, 0xf9, 0x54, 0x35, 0xd2, 0x29, 0x58, 0x3e, 0x11,
	0xc4, 0x13, 0xab, 0x44, 0xc7, 0xdb, 0x1d, 0x3c, 0xe9, 0xe9, 0xe6, 0xea, 0xdd, 0xc0, 0xf5, 0x46,
	0x44, 0x90, 0xf9, 0x2a, 0xa1, 0xd8, 0xf4, 0xb3, 0x2f, 0x79, 0x52, 0x8b, 0x38, 0x12, 0x34, 0x12,
	0x4e, 0xa5, 0x5b, 0x3d, 0x6c, 0xe1, 0x5c, 0x74, 0xff, 0x30, 0xc0, 0xcc, 0x09, 0x68, 0x0f, 0x3e,
	0x98, 0x0e, 0x47, 0xa3, 0xc9, 0xd8, 0x1b, 0x4f, 0xc6, 0xdf, 0x8f, 0x2f, 0xe6, 0xde, 0xfc, 0xa7,
	0xe9, 0xd8, 0x7b, 0x75, 0x31, 0x7f, 0x3e, 0xb0, 0xef, 0xa1, 0x8f, 0xe0, 0xc3, 0xbb, 0xcc, 0x3f,
	0x68, 0xbb, 0xb1, 0x85, 0x7e, 0x72, 0x6c, 0x57, 0xb6, 0xd1, 0x4f, 0x8e, 0xed, 0x2a, 0xda, 0x87,
	0xc7, 0x77, 0xd9, 0xbf, 0x9d, 0xbc, 0x1e, 0x4a, 0xff, 0xb5, 0xad, 0x80, 0x93, 0x63, 0xbb, 0xee,
	0xa6, 0xd0, 0x9e, 0xf0, 0x69, 0xcc, 0x82, 0xc5, 0x6a, 0x26, 0xe4, 0xf1, 0xff, 0x17, 0xd7, 0xf4,
	0x97, 0x01, 0x0f, 0x66, 0xe7, 0xa3, 0xe2, 0xe7, 0xd7, 0x91, 0x8f, 0xc0, 0x64, 0xdc, 0x4b, 0xe5,
	0xb7, 0x43, 0xbb, 0xc6, 0x61, 0x73, 0xd0, 0xc9, 0xef, 0xaa, 0x94, 0x22, 0x6e, 0xb0, 0x8c, 0xf1,
	0x0c, 0x1e, 0x44, 0xcb, 0xd0, 0x4b, 0x49, 0x98, 0x30, 0xea, 0x25, 0x24, 0x4d, 0xa9, 0xef, 0xbc,
	0x55, 0xc1, 0xee, 0x47, 0xcb, 0x70, 0xa6, 0xf4, 0x53, 0xa5, 0x46, 0x7d, 0xb0, 0x12, 0xc2, 0x49,
	0x48, 0x05, 0xe5, 0xaa, 0xa6, 0xe6, 0xe0, 0xbd, 0x3b, 0x5a, 0x01, 0xaf, 0x51, 0x92, 0x92, 0x8f,
	0x97, 0x54, 0x55, 0xb8, 0x89, 0x52, 0xa0, 0xdc, 0x3f, 0x2b, 0xf0, 0x30, 0x1f, 0x25, 0xff, 0xf7,
	0xf2, 0xce, 0x60, 0x97, 0x2c, 0x16, 0xcb, 0xd0, 0x93, 0x53, 0x29, 0xd0, 0x8d, 0xbe, 0x91, 0xd7,
	0x56, 0xd0, 0xf3, 0x0c, 0x89, 0x8e, 0xa1, 0xa9, 0xb9, 0xaa, 0x52, 0x75, 0xc1, 0x1b, 0x88, 0xa0,
	0x70, 0x23, 0x09, 0x43, 0x27, 0xd0, 0x5a, 0x26, 0x3e, 0x11, 0x34, 0xa3, 0xd5, 0x36, 0xd3, 0x9a,
	0x1a, 0xa8, 0x78, 0xee, 0xdf, 0x06, 0x74, 0xb2, 0xe9, 0xf9, 0x0e, 0x1f, 0xaa, 0xfb, 0x6b, 0x05,
	0x90, 0x9c, 0xec, 0xef, 0xde, 0x7f, 0x21, 0x29, 0xd7, 0x94, 0xc5, 0x8b, 0x40, 0xac, 0xd2, 0x6d,
	0xdd, 0xb2, 0x46, 0xb9, 0xbf, 0xd7, 0xe0, 0xfe, 0xad, 0xe7, 0x01, 0xfa, 0x06, 0xac, 0x38, 0x57,
	0x65, 0xf3, 0xfc, 0x20, 0x77, 0x73, 0xfb, 0x29, 0x51, 0xc8, 0x78, 0xcd, 0x41, 0x1f, 0x43, 0x35,
	0xbd, 0xf2, 0xb3, 0x0c, 0x1e, 0xe4, 0xd4, 0xe2, 0x91, 0x81, 0xa5, 0x15, 0x0d, 0xc0, 0x24, 0xd9,
	0x3f, 0x9c, 0xb5, 0xe8, 0xc3, 0x1c, 0x59, 0x7e, 0x26, 0xe0, 0x02, 0x87, 0xbe, 0x80, 0x06, 0xd1,
	0x1d, 0xea, 0xd4, 0xcb, 0x77, 0x54, 0x5a, 0xfb, 0x38, 0x47, 0xa1, 0xa7, 0x50, 0x23, 0x3e, 0x09,
	0x9d, 0x1d, 0x85, 0x46, 0x37, 0xd0, 0xd9, 0x66, 0xc7, 0xca, 0x8e, 0xbe, 0x06, 0x8b, 0x71, 0x2f,
	0x51, 0xd7, 0xec, 0x34, 0x55, 0xc9, 0xdd, 0x4d, 0x25, 0xe7, 0xed, 0x20, 0x47, 0xad, 0xfe, 0x92,
	0xcd, 0xb3, 0x90, 0x0b, 0xdc, 0x63, 0xdc, 0x69, 0x95, 0x13, 0x2b, 0x2d, 0x76, 0xb5, 0xde, 0xa4,
	0x88, 0x9e, 0x83, 0xc5, 0xd4, 0xe6, 0x96, 0x94, 0x76, 0xb9, 0xfc, 0xf2, 0x4a, 0xc7, 0x26, 0xcb,
	0x64, 0xf4, 0x18, 0xac, 0x05, 0x0b, 0x12, 0x2f, 0x8a, 0x79, 0xa8, 0x9a, 0xd4, 0xc0, 0xa6, 0x54,
	0x5c, 0xc4, 0x3c, 0x44, 0x7b, 0x00, 0xca, 0x78, 0x4d, 0xd8, 0x92, 0x3a, 0x97, 0xca, 0xaa, 0xe0,
	0x3f, 0x4a, 0x85, 0x7b, 0x0a, 0x56, 0x51, 0x08, 0x6a, 0x40, 0x75, 0x76, 0x3e, 0xb2, 0x0d, 0xd4,
	0x02, 0x33, 0x3f, 0x6c, 0xbb, 0x82, 0x9a, 0xd0, 0xc8, 0xce, 0xd1, 0xae, 0x22, 0x13, 0x6a, 0xf2,
	0x98, 0xec, 0x9a, 0x7b, 0x00, 0x66, 0x5e, 0x33, 0xb2, 0xa0, 0xae, 0x2a, 0xb2, 0xef, 0x21, 0x80,
	0x1d, 0x9d, 0xa9, 0x6d, 0xbc, 0xa8, 0x7c, 0x57, 0xfd, 0x27, 0x00, 0x00, 0xff, 0xff, 0xfb, 0xf4,
	0xea, 0xcc, 0x67, 0x0a, 0x00, 0x00,
}
