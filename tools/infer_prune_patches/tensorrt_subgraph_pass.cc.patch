diff --git a/paddle/fluid/inference/analysis/ir_passes/tensorrt_subgraph_pass.cc b/paddle/fluid/inference/analysis/ir_passes/tensorrt_subgraph_pass.cc
index bc7dc9704a..9eeddef457 100644
--- a/paddle/fluid/inference/analysis/ir_passes/tensorrt_subgraph_pass.cc
+++ b/paddle/fluid/inference/analysis/ir_passes/tensorrt_subgraph_pass.cc
@@ -379,6 +379,7 @@ void TensorRtSubgraphPass::CreateTensorRTOp(
   trt_engine->SetUseInspector(Get<bool>("use_inspector"));
   trt_engine->SetWithErnie(graph->Has(framework::ir::kMultiheadMatmulPass));
 
+  std::unordered_set<std::string> param_set(params.begin(), params.end());
   if (use_static_engine) {
     trt_engine_serialized_data = GetTrtEngineSerializedData(
         Get<std::string>("model_opt_cache_dir"), engine_key);
@@ -388,6 +389,19 @@ void TensorRtSubgraphPass::CreateTensorRTOp(
       LOG(INFO) << "Load TRT Optimized Info from "
                 << GetTrtEngineSerializedPath(
                        Get<std::string>("model_opt_cache_dir"), engine_key);
+      const auto* root_scope{param_scope()};
+      for (;root_scope->parent();) {
+        root_scope = root_scope->parent();
+      }
+      for (const auto& name: param_set) {
+        LOG(INFO) << " ===== Clear param: " << name;
+        root_scope->FindLocalVar(name)->Clear();
+      }
+      for (int dev_id = 0; dev_id < paddle::platform::GetGPUDeviceCount();
+          ++dev_id) {
+        memory::Release(platform::CUDAPlace(dev_id));
+      }
+      memory::Release(platform::CPUPlace());
       return;
     }
   }
@@ -400,12 +414,25 @@ void TensorRtSubgraphPass::CreateTensorRTOp(
 
   auto *scope = param_scope();
   framework::BlockDesc block_desc_temp(nullptr, block_desc.Proto());
-  std::unordered_set<std::string> param_set(params.begin(), params.end());
   inference::Singleton<inference::tensorrt::OpConverter>::Global()
       .ConvertBlockToTRTEngine(
           &block_desc_temp, *scope,
           std::vector<std::string>(input_names.begin(), input_names.end()),
           param_set, output_mapping, trt_engine);
+  const auto* root_scope{scope};
+  for (;root_scope->parent();) {
+    root_scope = root_scope->parent();
+  }
+  VLOG(4) << "root_scope->LocalVarNames().size: " << root_scope->LocalVarNames().size();
+  for (const auto& name: param_set) {
+    VLOG(4) << "  ===== Clear param: " << name;
+    root_scope->FindLocalVar(name)->Clear();
+  }
+  for (int dev_id = 0; dev_id < paddle::platform::GetGPUDeviceCount();
+       ++dev_id) {
+    memory::Release(platform::CUDAPlace(dev_id));
+  }
+  memory::Release(platform::CPUPlace());
 
   if (use_static_engine) {
     nvinfer1::IHostMemory *serialized_engine_data = trt_engine->Serialize();
@@ -420,6 +447,8 @@ void TensorRtSubgraphPass::CreateTensorRTOp(
               << GetTrtEngineSerializedPath(
                      Get<std::string>("model_opt_cache_dir"), engine_key);
   }
+  trt_engine_serialized_data.clear();
+  trt_engine_serialized_data.shrink_to_fit();
 }
 
 }  // namespace analysis
