WARNING: Logging before InitGoogleLogging() is written to STDERR
I0831 09:07:39.912457 27067 common.cc:284] Call SPMDRuleMap::Insert!
I0831 09:07:39.912518 27067 common.cc:284] Call SPMDRuleMap::Insert!
I0831 09:07:39.912521 27067 common.cc:284] Call SPMDRuleMap::Insert!
I0831 09:07:39.912525 27067 common.cc:284] Call SPMDRuleMap::Insert!
I0831 09:07:39.912528 27067 common.cc:284] Call SPMDRuleMap::Insert!
I0831 09:07:39.912532 27067 common.cc:284] Call SPMDRuleMap::Insert!
I0831 09:07:39.912535 27067 common.cc:284] Call SPMDRuleMap::Insert!
I0831 09:07:39.912539 27067 common.cc:284] Call SPMDRuleMap::Insert!
I0831 09:07:39.912542 27067 common.cc:284] Call SPMDRuleMap::Insert!
I0831 09:07:39.912546 27067 common.cc:284] Call SPMDRuleMap::Insert!
I0831 09:07:39.912549 27067 common.cc:284] Call SPMDRuleMap::Insert!
I0831 09:07:39.912552 27067 common.cc:284] Call SPMDRuleMap::Insert!
I0831 09:07:39.912556 27067 common.cc:284] Call SPMDRuleMap::Insert!
I0831 09:07:39.912559 27067 common.cc:284] Call SPMDRuleMap::Insert!
I0831 09:07:39.912562 27067 common.cc:284] Call SPMDRuleMap::Insert!
I0831 09:07:39.912568 27067 common.cc:284] Call SPMDRuleMap::Insert!
I0831 09:07:39.912571 27067 common.cc:284] Call SPMDRuleMap::Insert!
I0831 09:07:39.912576 27067 common.cc:284] Call SPMDRuleMap::Insert!
I0831 09:07:39.912580 27067 common.cc:284] Call SPMDRuleMap::Insert!
I0831 09:07:39.912582 27067 common.cc:284] Call SPMDRuleMap::Insert!
I0831 09:07:39.912585 27067 common.cc:284] Call SPMDRuleMap::Insert!
I0831 09:07:39.912588 27067 common.cc:284] Call SPMDRuleMap::Insert!
I0831 09:07:39.912591 27067 common.cc:284] Call SPMDRuleMap::Insert!
I0831 09:07:39.912595 27067 common.cc:284] Call SPMDRuleMap::Insert!
I0831 09:07:39.912600 27067 common.cc:284] Call SPMDRuleMap::Insert!
I0831 09:07:39.912602 27067 common.cc:284] Call SPMDRuleMap::Insert!
I0831 09:07:39.912606 27067 common.cc:284] Call SPMDRuleMap::Insert!
I0831 09:07:39.912609 27067 common.cc:284] Call SPMDRuleMap::Insert!
I0831 09:07:39.912612 27067 common.cc:284] Call SPMDRuleMap::Insert!
I0831 09:07:39.912616 27067 common.cc:284] Call SPMDRuleMap::Insert!
I0831 09:07:39.912618 27067 common.cc:284] Call SPMDRuleMap::Insert!
I0831 09:07:39.912621 27067 common.cc:284] Call SPMDRuleMap::Insert!
I0831 09:07:39.912626 27067 common.cc:284] Call SPMDRuleMap::Insert!
I0831 09:07:39.912636 27067 common.cc:284] Call SPMDRuleMap::Insert!
I0831 09:07:39.912639 27067 common.cc:284] Call SPMDRuleMap::Insert!
I0831 09:07:39.912642 27067 common.cc:284] Call SPMDRuleMap::Insert!
I0831 09:07:39.912645 27067 common.cc:284] Call SPMDRuleMap::Insert!
I0831 09:07:39.912648 27067 common.cc:284] Call SPMDRuleMap::Insert!
I0831 09:07:39.912652 27067 common.cc:284] Call SPMDRuleMap::Insert!
I0831 09:07:39.912654 27067 common.cc:284] Call SPMDRuleMap::Insert!
I0831 09:07:39.912657 27067 common.cc:284] Call SPMDRuleMap::Insert!
I0831 09:07:39.912662 27067 common.cc:284] Call SPMDRuleMap::Insert!
I0831 09:07:39.912664 27067 common.cc:284] Call SPMDRuleMap::Insert!
I0831 09:07:39.912667 27067 common.cc:284] Call SPMDRuleMap::Insert!
I0831 09:07:39.912670 27067 common.cc:284] Call SPMDRuleMap::Insert!
I0831 09:07:39.912673 27067 common.cc:284] Call SPMDRuleMap::Insert!
I0831 09:07:39.912676 27067 common.cc:284] Call SPMDRuleMap::Insert!
I0831 09:07:39.912679 27067 common.cc:284] Call SPMDRuleMap::Insert!
I0831 09:07:39.912683 27067 common.cc:284] Call SPMDRuleMap::Insert!
I0831 09:07:39.912686 27067 common.cc:284] Call SPMDRuleMap::Insert!
I0831 09:07:39.912689 27067 common.cc:284] Call SPMDRuleMap::Insert!
I0831 09:07:39.912693 27067 common.cc:284] Call SPMDRuleMap::Insert!
I0831 09:07:39.912695 27067 common.cc:284] Call SPMDRuleMap::Insert!
I0831 09:07:39.912698 27067 common.cc:284] Call SPMDRuleMap::Insert!
I0831 09:07:39.912701 27067 common.cc:284] Call SPMDRuleMap::Insert!
I0831 09:07:39.912704 27067 common.cc:284] Call SPMDRuleMap::Insert!
I0831 09:07:39.912707 27067 common.cc:284] Call SPMDRuleMap::Insert!
I0831 09:07:39.912710 27067 common.cc:284] Call SPMDRuleMap::Insert!
I0831 09:07:39.912714 27067 common.cc:284] Call SPMDRuleMap::Insert!
I0831 09:07:39.912724 27067 common.cc:284] Call SPMDRuleMap::Insert!
I0831 09:07:39.912726 27067 common.cc:284] Call SPMDRuleMap::Insert!
I0831 09:07:39.912729 27067 common.cc:284] Call SPMDRuleMap::Insert!
I0831 09:07:39.912732 27067 common.cc:284] Call SPMDRuleMap::Insert!
I0831 09:07:39.912736 27067 common.cc:284] Call SPMDRuleMap::Insert!
I0831 09:07:39.912739 27067 common.cc:284] Call SPMDRuleMap::Insert!
I0831 09:07:39.912751 27067 common.cc:284] Call SPMDRuleMap::Insert!
I0831 09:07:39.912755 27067 common.cc:284] Call SPMDRuleMap::Insert!
I0831 09:07:39.912757 27067 common.cc:284] Call SPMDRuleMap::Insert!
I0831 09:07:39.912760 27067 common.cc:284] Call SPMDRuleMap::Insert!
I0831 09:07:39.912765 27067 common.cc:284] Call SPMDRuleMap::Insert!
I0831 09:07:39.912767 27067 common.cc:284] Call SPMDRuleMap::Insert!
I0831 09:07:39.912770 27067 common.cc:284] Call SPMDRuleMap::Insert!
I0831 09:07:39.912773 27067 common.cc:284] Call SPMDRuleMap::Insert!
I0831 09:07:39.912776 27067 common.cc:284] Call SPMDRuleMap::Insert!
I0831 09:07:39.912779 27067 common.cc:284] Call SPMDRuleMap::Insert!
I0831 09:07:39.912782 27067 common.cc:284] Call SPMDRuleMap::Insert!
I0831 09:07:39.912786 27067 common.cc:284] Call SPMDRuleMap::Insert!
I0831 09:07:39.912789 27067 common.cc:284] Call SPMDRuleMap::Insert!
I0831 09:07:39.912793 27067 common.cc:284] Call SPMDRuleMap::Insert!
I0831 09:07:39.912796 27067 common.cc:284] Call SPMDRuleMap::Insert!
I0831 09:07:39.912799 27067 common.cc:284] Call SPMDRuleMap::Insert!
I0831 09:07:39.912802 27067 common.cc:284] Call SPMDRuleMap::Insert!
I0831 09:07:39.912806 27067 common.cc:284] Call SPMDRuleMap::Insert!
I0831 09:07:39.912808 27067 common.cc:284] Call SPMDRuleMap::Insert!
I0831 09:07:39.912812 27067 common.cc:284] Call SPMDRuleMap::Insert!
I0831 09:07:39.912815 27067 common.cc:284] Call SPMDRuleMap::Insert!
I0831 09:07:39.912818 27067 common.cc:284] Call SPMDRuleMap::Insert!
I0831 09:07:39.912822 27067 common.cc:284] Call SPMDRuleMap::Insert!
I0831 09:07:39.912824 27067 common.cc:284] Call SPMDRuleMap::Insert!
I0831 09:07:39.912827 27067 common.cc:284] Call SPMDRuleMap::Insert!
I0831 09:07:39.912830 27067 common.cc:284] Call SPMDRuleMap::Insert!
I0831 09:07:39.912833 27067 common.cc:284] Call SPMDRuleMap::Insert!
I0831 09:07:39.912837 27067 common.cc:284] Call SPMDRuleMap::Insert!
I0831 09:07:39.912839 27067 common.cc:284] Call SPMDRuleMap::Insert!
I0831 09:07:39.912843 27067 common.cc:284] Call SPMDRuleMap::Insert!
I0831 09:07:39.912847 27067 common.cc:284] Call SPMDRuleMap::Insert!
I0831 09:07:39.912849 27067 common.cc:284] Call SPMDRuleMap::Insert!
I0831 09:07:39.912853 27067 common.cc:284] Call SPMDRuleMap::Insert!
I0831 09:07:39.912855 27067 common.cc:284] Call SPMDRuleMap::Insert!
I0831 09:07:39.912858 27067 common.cc:284] Call SPMDRuleMap::Insert!
I0831 09:07:39.912863 27067 common.cc:284] Call SPMDRuleMap::Insert!
I0831 09:07:39.912865 27067 common.cc:284] Call SPMDRuleMap::Insert!
I0831 09:07:39.912868 27067 common.cc:284] Call SPMDRuleMap::Insert!
I0831 09:07:39.912871 27067 common.cc:284] Call SPMDRuleMap::Insert!
I0831 09:07:39.912874 27067 common.cc:284] Call SPMDRuleMap::Insert!
I0831 09:07:39.912878 27067 common.cc:284] Call SPMDRuleMap::Insert!
I0831 09:07:39.912881 27067 common.cc:284] Call SPMDRuleMap::Insert!
I0831 09:07:39.912884 27067 common.cc:284] Call SPMDRuleMap::Insert!
I0831 09:07:39.912887 27067 common.cc:284] Call SPMDRuleMap::Insert!
grep: warning: GREP_OPTIONS is deprecated; please use an alias or script
I0831 09:07:39.987120 27067 ir_context.cc:194] BuiltinDialect registered into IrContext. ===>
I0831 09:07:39.987155 27067 ir_context.cc:249] Try to get or register a Dialect of: [name=builtin].
I0831 09:07:39.987160 27067 ir_context.cc:252] Create and register a new Dialect of: [name=builtin].
I0831 09:07:39.987215 27067 ir_context.cc:196] ==============================================
I0831 09:07:39.987231 27067 ir_context.cc:92] Found a cached abstract_attribute of: [TypeId_hash=140104691385512, AbstractAttribute_ptr=0x5566367b2550].
I0831 09:07:39.987262 27067 builtin_op.cc:58] Verifying inputs, outputs and attributes for: ModuleOp.
I0831 09:07:39.987314 27067 ir_context.cc:92] Found a cached abstract_attribute of: [TypeId_hash=140104691385512, AbstractAttribute_ptr=0x5566367b2550].
I0831 09:07:39.987320 27067 builtin_op.cc:58] Verifying inputs, outputs and attributes for: ModuleOp.
I0831 09:07:40.549474 27067 init.cc:96] Before Parse: argc is 2, Init commandline: dummy --tryfromenv=reader_queue_speed_test_mode,graph_metapath_split_opt,enable_exit_when_partial_worker,gpu_allocator_retry_time,selected_gpus,executor_log_deps_every_microseconds,gpugraph_hbm_table_load_factor,fuse_parameter_groups_size,initial_gpu_memory_in_mb,eager_delete_tensor_gb,gpugraph_enable_hbm_table_collision_stat,tracer_mkldnn_ops_off,apply_pass_to_program,graph_get_neighbor_id,use_mkldnn,set_to_1d,sort_sum_gradient,run_kp_kernel,tracer_profile_fname,use_system_allocator,benchmark,communicator_is_sgd_optimizer,use_autotune,new_executor_log_memory_stats,cache_inference_while_scope,check_nan_inf_level,prim_enabled,allocator_strategy,fraction_of_gpu_memory_to_use,new_executor_use_inplace,search_cache_max_number,sync_nccl_allreduce,enable_opt_get_features,enable_unused_var_check,enable_new_ir_in_executor,enable_new_ir_api,enable_all2all_use_fp16,new_executor_use_cuda_graph,gpugraph_enable_segment_merge_grads,use_shm_cache,use_pinned_memory,gpugraph_sparse_table_storage_mode,check_nan_inf,cudnn_deterministic,call_stack_level,cudnn_exhaustive_search,new_executor_use_local_scope,reallocate_gpu_memory_in_mb,add_dependency_for_communication_op,new_executor_static_build,einsum_opt,auto_growth_chunk_size_in_mb,print_sub_graph_dir,memory_fraction_of_eager_deletion,convert_all_blocks,jit_engine_type,multiple_of_cupti_buffer_size,enable_gpu_memory_usage_log_mb,allreduce_record_one_event,low_precision_op_list,free_when_no_cache_hit,cudnn_exhaustive_search_times,dist_threadpool_size,use_fast_math,use_virtual_memory_auto_growth,npu_storage_format,get_host_by_name_time,cublaslt_exhaustive_search_times,enable_auto_detect_gpu_topo,gpugraph_slot_feasign_max_num,enable_rpc_profiler,embedding_deterministic,gpugraph_enable_gpu_direct_access,enable_api_kernel_fallback,fast_eager_deletion_mode,cudnn_batchnorm_spatial_persistent,tracer_mkldnn_ops_on,pe_profile_fname,check_kernel_launch,gpugraph_dedup_pull_push_mode,gpugraph_storage_mode,fuse_parameter_memory_size,max_inplace_grad_add,gpu_memory_limit_mb,use_stream_safe_cuda_allocator,new_executor_sequential_run,init_allocated_mem,eager_delete_scope,use_cuda_managed_memory,conv2d_disable_cudnn,use_stride_kernel,new_executor_serial_run,communicator_max_merge_var_num,host_trace_level,gemm_use_half_precision_compute_type,communicator_send_queue_size,gpugraph_load_node_list_into_hbm,cpu_deterministic,enable_auto_rdma_trans,gpugraph_debug_gpu_memory,static_executor_perfstat_filepath,paddle_num_threads,dygraph_debug,conv_workspace_size_limit,inner_op_parallelism,nccl_blocking_wait,fleet_executor_with_standalone,tensor_operants_mode,fraction_of_cuda_pinned_memory_to_use,free_idle_chunk,enable_cublas_tensor_op_math,gpugraph_merge_grads_segment_size,enable_sparse_inner_gather,graph_load_in_parallel,trt_ibuilder_cache,enable_gpu_memory_usage_log,fraction_of_cpu_memory_to_use,enable_new_ir_in_executor_trace_run,enable_tracker_all2all,local_exe_sub_scope_limit,initial_cpu_memory_in_mb 
I0831 09:07:40.549710 27067 init.cc:104] After Parse: argc is 1
I0831 09:07:40.549757 27067 os_info.cc:117] SetCurrentThreadName MainThread
I0831 09:07:40.719710 27067 pybind.cc:2018] Initialize tensor operants successfully
I0831 09:07:40.801642 27067 amp_auto_cast.cc:106] -- The size of all_ops: 1256 --
I0831 09:07:40.801688 27067 amp_auto_cast.cc:107] -- The size of supported_ops: 193 --
I0831 09:07:40.801693 27067 amp_auto_cast.cc:108] -- The size of unsupported_ops: 1063 --
I0831 09:07:41.067756 27067 imperative.cc:691] Tracer(0x556636a026d0) set expected place Place(gpu:0)
I0831 09:07:41.090521 27067 imperative.cc:691] Tracer(0x556636a026d0) set expected place Place(gpu:0)
I0831 09:07:41.093320 27067 global_value_getter_setter.cc:191] set FLAGS_use_system_allocator to True
I0831 09:07:41.093922 27067 imperative.cc:691] Tracer(0x556636b47f70) set expected place Place(gpu:0)
I0831 09:07:41.093967 27067 pybind.cc:1718] Cannot use get_all_custom_device_type because you have installedCPU/GPU version PaddlePaddle.
If you want to use get_all_custom_device_type, please try to install CustomDevice version PaddlePaddle by: pip install paddlepaddle
I0831 09:07:41.093988 27067 imperative.cc:701] Tracer(0x556636b47f70) set expected place Place(cpu)
I0831 09:07:41.094203 27067 eager.cc:112] Tensor(generated_tensor_0) have not GradNode, add GradNodeAccumulation0x556638159970 for it.
I0831 09:07:41.094242 27067 allocator_facade.cc:842] FLAGS_auto_growth_chunk_size_in_mb is 0
I0831 09:07:41.094357 27067 dynamic_loader.cc:227] Try to find library: libcuda.so from default system path.
I0831 09:07:41.094379 27067 auto_growth_best_fit_allocator.cc:57] chunk_size_:256
I0831 09:07:41.094388 27067 allocator_facade.cc:842] FLAGS_auto_growth_chunk_size_in_mb is 0
I0831 09:07:41.094391 27067 auto_growth_best_fit_allocator.cc:57] chunk_size_:256
I0831 09:07:41.094395 27067 allocator_facade.cc:842] FLAGS_auto_growth_chunk_size_in_mb is 0
I0831 09:07:41.094398 27067 auto_growth_best_fit_allocator.cc:57] chunk_size_:256
I0831 09:07:41.094403 27067 allocator_facade.cc:842] FLAGS_auto_growth_chunk_size_in_mb is 0
I0831 09:07:41.094406 27067 auto_growth_best_fit_allocator.cc:57] chunk_size_:256
I0831 09:07:41.094409 27067 allocator_facade.cc:842] FLAGS_auto_growth_chunk_size_in_mb is 0
I0831 09:07:41.094413 27067 auto_growth_best_fit_allocator.cc:57] chunk_size_:256
I0831 09:07:41.094416 27067 allocator_facade.cc:842] FLAGS_auto_growth_chunk_size_in_mb is 0
I0831 09:07:41.094420 27067 auto_growth_best_fit_allocator.cc:57] chunk_size_:256
I0831 09:07:41.094424 27067 allocator_facade.cc:842] FLAGS_auto_growth_chunk_size_in_mb is 0
I0831 09:07:41.094427 27067 auto_growth_best_fit_allocator.cc:57] chunk_size_:256
I0831 09:07:41.094611 27067 eager.cc:112] Tensor(generated_tensor_1) have not GradNode, add GradNodeAccumulation0x55663815ddd0 for it.
I0831 09:07:41.094729 27067 eager.cc:112] Tensor(generated_tensor_2) have not GradNode, add GradNodeAccumulation0x5566381892d0 for it.
I0831 09:07:41.094887 27067 generator.cc:200] initial seed: 6200909928454850, cpu engine: 0x5566381a61b8
I0831 09:07:41.094964 27067 pybind.cc:1718] Cannot use get_all_custom_device_type because you have installedCPU/GPU version PaddlePaddle.
If you want to use get_all_custom_device_type, please try to install CustomDevice version PaddlePaddle by: pip install paddlepaddle
I0831 09:07:41.094985 27067 imperative.cc:701] Tracer(0x556636b47f70) set expected place Place(cpu)
I0831 09:07:41.095058 27067 eager.cc:112] Tensor(generated_tensor_3) have not GradNode, add GradNodeAccumulation0x5566381892d0 for it.
I0831 09:07:41.095121 27067 eager.cc:112] Tensor(generated_tensor_4) have not GradNode, add GradNodeAccumulation0x55663815ddd0 for it.
I0831 09:07:41.095299 27067 dygraph_functions.cc:52610] Running AD API: divide
I0831 09:07:41.095340 27067 dygraph_functions.cc:52669] { Input: [ 
( x , [{ Name: generated_tensor_3, Initialized: 1, Ptr: 0x556636a8c140 }]),  
( y , [{ Name: generated_tensor_4, Initialized: 1, Ptr: 0x556636a18310 }]), ]} 
I0831 09:07:41.102409 27067 dynamic_loader.cc:227] Try to find library: libmklml_intel.so from default system path.
I0831 09:07:41.156227 27067 grad_node_info.cc:246] Add Edges for slot: 0, the Edge is from DivideGradNode (addr: 0x5566381f2a30)  to GradNodeAccumulation (addr: 0x5566381892d0)
I0831 09:07:41.156292 27067 grad_node_info.cc:246] Add Edges for slot: 1, the Edge is from DivideGradNode (addr: 0x5566381f2a30)  to GradNodeAccumulation (addr: 0x55663815ddd0)
I0831 09:07:41.156311 27067 dygraph_functions.cc:52733] Finish AD API: divide
I0831 09:07:41.156352 27067 dygraph_functions.cc:52750] { Input: [ 
( x , [{ Name: generated_tensor_3, Initialized: 1, Ptr: 0x556636a8c140 }]),  
( y , [{ Name: generated_tensor_4, Initialized: 1, Ptr: 0x556636a18310 }]), ],  
 Output: [ 
( out , [{ Name: None, Initialized: 1, Ptr: 0x556636a189a0 }]), ] } 
I0831 09:07:41.156659 27067 eager.cc:112] Tensor(generated_tensor_5) have not GradNode, add GradNodeAccumulation0x556638159970 for it.
I0831 09:07:41.156769 27067 backward.cc:441] Run in Grad
I0831 09:07:41.156783 27067 backward.cc:112] Start Backward
I0831 09:07:41.156817 27067 general_grad.h:516] Copied Node: DivideGradNode ptr: 0x5566381f2a30 to ptr: 0x55663819c880
I0831 09:07:41.156834 27067 backward.cc:202] Fill grad input tensor 0with give grad tensor
I0831 09:07:41.156852 27067 tensor_method.cc:93] Deep copy Tensor from generated_tensor_5 to 
I0831 09:07:41.156877 27067 tensor_utils.cc:57] TensorCopy 13, 17 from Place(cpu) to Place(cpu)
I0831 09:07:41.156895 27067 tensor_utils.cc:103] src:0x55663819d000, dst:0x5566381d8000
I0831 09:07:41.156906 27067 memcpy.cc:743] memory::Copy 1768 Bytes from Place(cpu) to Place(cpu)
I0831 09:07:41.156917 27067 general_grad.h:563] Copied Node: GradNodeAccumulation ptr: 0x5566381892d0 to ptr: 0x5566380b7a50
I0831 09:07:41.156925 27067 general_grad.h:563] Copied Node: GradNodeAccumulation ptr: 0x55663815ddd0 to ptr: 0x5566381e1450
I0831 09:07:41.156953 27067 backward.cc:248] Preparing GradNode:DivideGradNode addr:0x55663819c880
I0831 09:07:41.156963 27067 nodes.cc:30376] Running AD API GRAD: divide_grad
I0831 09:07:41.157011 27067 nodes.cc:30428] { Input: [ 
( grad_out , [{ Name: None, Initialized: 1, Ptr: 0x556636a074a0 }]),  
( x , [{ Name: None, Initialized: 1, Ptr: 0x556636a8c140 }]),  
( y , [{ Name: None, Initialized: 1, Ptr: 0x556636a18310 }]),  
( out , [{ Name: None, Initialized: 1, Ptr: 0x556636a189a0 }]), ]} 
I0831 09:07:41.157060 27067 nodes.cc:30445] Fused api divide_grad is called 
I0831 09:07:41.157066 27067 nodes.cc:30510] Finish AD API GRAD: divide_grad
I0831 09:07:41.157096 27067 nodes.cc:30536] { Input: [ 
( grad_out , [{ Name: None, Initialized: 1, Ptr: 0x556636a074a0 }]),  
( x , [{ Name: None, Initialized: 1, Ptr: 0x556636a8c140 }]),  
( y , [{ Name: None, Initialized: 1, Ptr: 0x556636a18310 }]),  
( out , [{ Name: None, Initialized: 1, Ptr: 0x556636a189a0 }]), ],  
 Output: [ 
 ( grad_x , [{ Name: None, Initialized: 1, Ptr: 0x5566369bc400 }]),  
 ( grad_y , [{ Name: None, Initialized: 1, Ptr: 0x556636779250 }]), ] } 
I0831 09:07:41.157104 27067 backward.cc:288] retain_graph is false, need to clear the TensorWrapper of nodes.
I0831 09:07:41.157111 27067 backward.cc:317] Node: DivideGradNode addr:0x55663819c880, Found pending node: GradNodeAccumulation addr: 0x5566380b7a50
I0831 09:07:41.157116 27067 backward.cc:358] Sum or Move grad inputs for edge slot: 0, rank: 0
I0831 09:07:41.157125 27067 backward.cc:317] Node: DivideGradNode addr:0x55663819c880, Found pending node: GradNodeAccumulation addr: 0x5566381e1450
I0831 09:07:41.157128 27067 backward.cc:358] Sum or Move grad inputs for edge slot: 0, rank: 0
I0831 09:07:41.157137 27067 backward.cc:248] Preparing GradNode:GradNodeAccumulation addr:0x5566381e1450
I0831 09:07:41.157142 27067 accumulation_node.cc:139] Running AD API Grad: GradNodeAccumulation
I0831 09:07:41.157145 27067 accumulation_node.cc:174] Finish AD API Grad: GradNodeAccumulation
I0831 09:07:41.157156 27067 accumulation_node.cc:187] { Input: [], Output: [(grad_out, [{ Name: None, Initialized: 1, Ptr: 0x556636779250 }]), ] } 
I0831 09:07:41.157163 27067 backward.cc:288] retain_graph is false, need to clear the TensorWrapper of nodes.
I0831 09:07:41.157176 27067 backward.cc:248] Preparing GradNode:GradNodeAccumulation addr:0x5566380b7a50
I0831 09:07:41.157179 27067 accumulation_node.cc:139] Running AD API Grad: GradNodeAccumulation
I0831 09:07:41.157182 27067 accumulation_node.cc:174] Finish AD API Grad: GradNodeAccumulation
I0831 09:07:41.157192 27067 accumulation_node.cc:187] { Input: [], Output: [(grad_out, [{ Name: None, Initialized: 1, Ptr: 0x5566369bc400 }]), ] } 
I0831 09:07:41.157197 27067 backward.cc:288] retain_graph is false, need to clear the TensorWrapper of nodes.
I0831 09:07:41.157200 27067 backward.cc:417] Finish Backward
I0831 09:07:41.157223 27067 eager_functions.cc:177]  in eager_api_run_partial_grad, after runing egr::Grad
I0831 09:07:41.157361 27067 memcpy.cc:119] src: 0x5566381fa000, dst: 0x5566381d8000, num: 1768
I0831 09:07:41.157377 27067 memcpy.cc:119] src: 0x5566381ed000, dst: 0x556638513000, num: 1768
I0831 09:07:41.157449 27067 pybind.cc:1718] Cannot use get_all_custom_device_type because you have installedCPU/GPU version PaddlePaddle.
If you want to use get_all_custom_device_type, please try to install CustomDevice version PaddlePaddle by: pip install paddlepaddle
I0831 09:07:41.157492 27067 imperative.cc:701] Tracer(0x556636b47f70) set expected place Place(cpu)
I0831 09:07:41.157532 27067 pybind.cc:1718] Cannot use get_all_custom_device_type because you have installedCPU/GPU version PaddlePaddle.
If you want to use get_all_custom_device_type, please try to install CustomDevice version PaddlePaddle by: pip install paddlepaddle
I0831 09:07:41.157543 27067 imperative.cc:701] Tracer(0x556636b47f70) set expected place Place(cpu)
I0831 09:07:41.157653 27067 eager.cc:112] Tensor(generated_tensor_6) have not GradNode, add GradNodeAccumulation0x556638159970 for it.
I0831 09:07:41.157729 27067 eager.cc:112] Tensor(generated_tensor_7) have not GradNode, add GradNodeAccumulation0x5566381892d0 for it.
I0831 09:07:41.157924 27067 dygraph_functions.cc:52610] Running AD API: divide
I0831 09:07:41.157946 27067 dygraph_functions.cc:52669] { Input: [ 
( x , [{ Name: generated_tensor_6, Initialized: 1, Ptr: 0x5566369bc400 }]),  
( y , [{ Name: generated_tensor_7, Initialized: 1, Ptr: 0x556636779250 }]), ]} 
I0831 09:07:41.157981 27067 grad_node_info.cc:246] Add Edges for slot: 0, the Edge is from DivideGradNode (addr: 0x55663819c880)  to GradNodeAccumulation (addr: 0x556638159970)
I0831 09:07:41.157987 27067 grad_node_info.cc:246] Add Edges for slot: 1, the Edge is from DivideGradNode (addr: 0x55663819c880)  to GradNodeAccumulation (addr: 0x5566381892d0)
I0831 09:07:41.157992 27067 dygraph_functions.cc:52733] Finish AD API: divide
I0831 09:07:41.158017 27067 dygraph_functions.cc:52750] { Input: [ 
( x , [{ Name: generated_tensor_6, Initialized: 1, Ptr: 0x5566369bc400 }]),  
( y , [{ Name: generated_tensor_7, Initialized: 1, Ptr: 0x556636779250 }]), ],  
 Output: [ 
( out , [{ Name: None, Initialized: 1, Ptr: 0x5566372b9890 }]), ] } 
I0831 09:07:41.158104 27067 eager.cc:112] Tensor(generated_tensor_8) have not GradNode, add GradNodeAccumulation0x55663815ddd0 for it.
I0831 09:07:41.158154 27067 backward.cc:441] Run in Grad
I0831 09:07:41.158160 27067 backward.cc:112] Start Backward
I0831 09:07:41.158177 27067 general_grad.h:516] Copied Node: DivideGradNode ptr: 0x55663819c880 to ptr: 0x5566381f2a30
I0831 09:07:41.158185 27067 backward.cc:202] Fill grad input tensor 0with give grad tensor
I0831 09:07:41.158190 27067 tensor_method.cc:93] Deep copy Tensor from generated_tensor_8 to 
I0831 09:07:41.158200 27067 tensor_utils.cc:57] TensorCopy 13, 17 from Place(cpu) to Place(cpu)
I0831 09:07:41.158207 27067 tensor_utils.cc:103] src:0x5566381ed000, dst:0x5566381bf000
I0831 09:07:41.158211 27067 memcpy.cc:743] memory::Copy 1768 Bytes from Place(cpu) to Place(cpu)
I0831 09:07:41.158219 27067 general_grad.h:563] Copied Node: GradNodeAccumulation ptr: 0x556638159970 to ptr: 0x5566381e1450
I0831 09:07:41.158224 27067 general_grad.h:563] Copied Node: GradNodeAccumulation ptr: 0x5566381892d0 to ptr: 0x5566381beb90
I0831 09:07:41.158241 27067 backward.cc:248] Preparing GradNode:DivideGradNode addr:0x5566381f2a30
I0831 09:07:41.158246 27067 nodes.cc:30376] Running AD API GRAD: divide_grad
I0831 09:07:41.158272 27067 nodes.cc:30428] { Input: [ 
( grad_out , [{ Name: None, Initialized: 1, Ptr: 0x556636b80f80 }]),  
( x , [{ Name: None, Initialized: 1, Ptr: 0x5566369bc400 }]),  
( y , [{ Name: None, Initialized: 1, Ptr: 0x556636779250 }]),  
( out , [{ Name: None, Initialized: 1, Ptr: 0x5566372b9890 }]), ]} 
I0831 09:07:41.158289 27067 operants_manager.cc:753] OperantsManager reusing eager mode API ::pow_ad_func
I0831 09:07:41.158298 27067 dygraph_functions.cc:54851] Running AD API: full_like
I0831 09:07:41.158317 27067 dygraph_functions.cc:54901] { Input: [ 
( x , [{ Name: None, Initialized: 1, Ptr: 0x556636779250 }]), ]} 
I0831 09:07:41.158355 27067 dygraph_functions.cc:54915] Finish AD API: full_like
I0831 09:07:41.158367 27067 dygraph_functions.cc:54932] { Input: [ 
( x , [{ Name: None, Initialized: 1, Ptr: 0x556636779250 }]), ],  
 Output: [ 
( out , [{ Name: None, Initialized: 1, Ptr: 0x556636b153d0 }]), ] } 
I0831 09:07:41.158375 27067 dygraph_functions.cc:53240] Running AD API: elementwise_pow
I0831 09:07:41.158387 27067 dygraph_functions.cc:53299] { Input: [ 
( x , [{ Name: None, Initialized: 1, Ptr: 0x556636779250 }]),  
( y , [{ Name: None, Initialized: 1, Ptr: 0x556636b153d0 }]), ]} 
I0831 09:07:41.158421 27067 dygraph_functions.cc:53363] Finish AD API: elementwise_pow
I0831 09:07:41.158438 27067 dygraph_functions.cc:53380] { Input: [ 
( x , [{ Name: None, Initialized: 1, Ptr: 0x556636779250 }]),  
( y , [{ Name: None, Initialized: 1, Ptr: 0x556636b153d0 }]), ],  
 Output: [ 
( out , [{ Name: None, Initialized: 1, Ptr: 0x5566369f6230 }]), ] } 
I0831 09:07:41.158448 27067 operants_manager.cc:687] OperantsManager reusing eager mode API ::divide_ad_func
I0831 09:07:41.158452 27067 dygraph_functions.cc:52610] Running AD API: divide
I0831 09:07:41.158463 27067 dygraph_functions.cc:52669] { Input: [ 
( x , [{ Name: None, Initialized: 1, Ptr: 0x5566369bc400 }]),  
( y , [{ Name: None, Initialized: 1, Ptr: 0x5566369f6230 }]), ]} 
I0831 09:07:41.158476 27067 dygraph_functions.cc:52733] Finish AD API: divide
I0831 09:07:41.158490 27067 dygraph_functions.cc:52750] { Input: [ 
( x , [{ Name: None, Initialized: 1, Ptr: 0x5566369bc400 }]),  
( y , [{ Name: None, Initialized: 1, Ptr: 0x5566369f6230 }]), ],  
 Output: [ 
( out , [{ Name: None, Initialized: 1, Ptr: 0x556636b153d0 }]), ] } 
I0831 09:07:41.158499 27067 operants_manager.cc:390] OperantsManager reusing eager mode API ::scale_ad_func
I0831 09:07:41.158506 27067 dygraph_functions.cc:38179] Running AD API: scale
I0831 09:07:41.158516 27067 dygraph_functions.cc:38232] { Input: [ 
( x , [{ Name: None, Initialized: 1, Ptr: 0x556636b153d0 }]), ]} 
I0831 09:07:41.158547 27067 dygraph_functions.cc:38294] Finish AD API: scale
I0831 09:07:41.158560 27067 dygraph_functions.cc:38308] { Input: [ 
( x , [{ Name: None, Initialized: 1, Ptr: 0x556636b153d0 }]), ],  
 Output: [ 
( out , [{ Name: None, Initialized: 1, Ptr: 0x5566386be830 }]), ] } 
I0831 09:07:41.158567 27067 operants_manager.cc:1182] OperantsManager reusing eager mode API ::multiply_ad_func
I0831 09:07:41.158576 27067 multiply_fwd_func.cc:31] Running AD API: multiply
I0831 09:07:41.158591 27067 multiply_fwd_func.cc:103] { Input: [ 
( x , [{ Name: None, Initialized: 1, Ptr: 0x5566386be830 }]),  
( y , [{ Name: None, Initialized: 1, Ptr: 0x556636b80f80 }]), ]} 
I0831 09:07:41.158634 27067 multiply_fwd_func.cc:176] Finish AD API: multiply
I0831 09:07:41.158649 27067 multiply_fwd_func.cc:196] { Input: [ 
( x , [{ Name: None, Initialized: 1, Ptr: 0x5566386be830 }]),  
( y , [{ Name: None, Initialized: 1, Ptr: 0x556636b80f80 }]), ],  
 Output: [ 
( out , [{ Name: None, Initialized: 1, Ptr: 0x556638a2f760 }]), ] } 
I0831 09:07:41.158660 27067 eager_prim_api.cc:27] Eager Prim API full_ad_func call
I0831 09:07:41.158666 27067 dygraph_functions.cc:54627] Running AD API: full
I0831 09:07:41.158670 27067 dygraph_functions.cc:54644] { Input: []} 
I0831 09:07:41.158690 27067 dygraph_functions.cc:54658] Finish AD API: full
I0831 09:07:41.158699 27067 dygraph_functions.cc:54672] { Input: [],  
 Output: [ 
( out , [{ Name: None, Initialized: 1, Ptr: 0x5566386be830 }]), ] } 
I0831 09:07:41.158704 27067 operants_manager.cc:687] OperantsManager reusing eager mode API ::divide_ad_func
I0831 09:07:41.158706 27067 dygraph_functions.cc:52610] Running AD API: divide
I0831 09:07:41.158717 27067 dygraph_functions.cc:52669] { Input: [ 
( x , [{ Name: None, Initialized: 1, Ptr: 0x5566386be830 }]),  
( y , [{ Name: None, Initialized: 1, Ptr: 0x556636779250 }]), ]} 
I0831 09:07:41.158726 27067 dygraph_functions.cc:52733] Finish AD API: divide
I0831 09:07:41.158744 27067 dygraph_functions.cc:52750] { Input: [ 
( x , [{ Name: None, Initialized: 1, Ptr: 0x5566386be830 }]),  
( y , [{ Name: None, Initialized: 1, Ptr: 0x556636779250 }]), ],  
 Output: [ 
( out , [{ Name: None, Initialized: 1, Ptr: 0x5566389cd1b0 }]), ] } 
I0831 09:07:41.158749 27067 operants_manager.cc:1182] OperantsManager reusing eager mode API ::multiply_ad_func
I0831 09:07:41.158753 27067 multiply_fwd_func.cc:31] Running AD API: multiply
I0831 09:07:41.158763 27067 multiply_fwd_func.cc:103] { Input: [ 
( x , [{ Name: None, Initialized: 1, Ptr: 0x5566389cd1b0 }]),  
( y , [{ Name: None, Initialized: 1, Ptr: 0x556636b80f80 }]), ]} 
I0831 09:07:41.158772 27067 multiply_fwd_func.cc:176] Finish AD API: multiply
I0831 09:07:41.158787 27067 multiply_fwd_func.cc:196] { Input: [ 
( x , [{ Name: None, Initialized: 1, Ptr: 0x5566389cd1b0 }]),  
( y , [{ Name: None, Initialized: 1, Ptr: 0x556636b80f80 }]), ],  
 Output: [ 
( out , [{ Name: None, Initialized: 1, Ptr: 0x556638987350 }]), ] } 
I0831 09:07:41.158792 27067 nodes.cc:30439] Composite api divide_grad is called 
I0831 09:07:41.158797 27067 nodes.cc:30510] Finish AD API GRAD: divide_grad
I0831 09:07:41.158821 27067 nodes.cc:30536] { Input: [ 
( grad_out , [{ Name: None, Initialized: 1, Ptr: 0x556636b80f80 }]),  
( x , [{ Name: None, Initialized: 1, Ptr: 0x5566369bc400 }]),  
( y , [{ Name: None, Initialized: 1, Ptr: 0x556636779250 }]),  
( out , [{ Name: None, Initialized: 1, Ptr: 0x5566372b9890 }]), ],  
 Output: [ 
 ( grad_x , [{ Name: None, Initialized: 1, Ptr: 0x556638987350 }]),  
 ( grad_y , [{ Name: None, Initialized: 1, Ptr: 0x556638a2f760 }]), ] } 
I0831 09:07:41.158828 27067 backward.cc:288] retain_graph is false, need to clear the TensorWrapper of nodes.
I0831 09:07:41.158833 27067 backward.cc:317] Node: DivideGradNode addr:0x5566381f2a30, Found pending node: GradNodeAccumulation addr: 0x5566381e1450
I0831 09:07:41.158838 27067 backward.cc:358] Sum or Move grad inputs for edge slot: 0, rank: 0
I0831 09:07:41.158843 27067 backward.cc:317] Node: DivideGradNode addr:0x5566381f2a30, Found pending node: GradNodeAccumulation addr: 0x5566381beb90
I0831 09:07:41.158847 27067 backward.cc:358] Sum or Move grad inputs for edge slot: 0, rank: 0
I0831 09:07:41.158852 27067 backward.cc:248] Preparing GradNode:GradNodeAccumulation addr:0x5566381beb90
I0831 09:07:41.158856 27067 accumulation_node.cc:139] Running AD API Grad: GradNodeAccumulation
I0831 09:07:41.158859 27067 accumulation_node.cc:174] Finish AD API Grad: GradNodeAccumulation
I0831 09:07:41.158870 27067 accumulation_node.cc:187] { Input: [], Output: [(grad_out, [{ Name: None, Initialized: 1, Ptr: 0x556638a2f760 }]), ] } 
I0831 09:07:41.158876 27067 backward.cc:288] retain_graph is false, need to clear the TensorWrapper of nodes.
I0831 09:07:41.158879 27067 backward.cc:248] Preparing GradNode:GradNodeAccumulation addr:0x5566381e1450
I0831 09:07:41.158883 27067 accumulation_node.cc:139] Running AD API Grad: GradNodeAccumulation
I0831 09:07:41.158886 27067 accumulation_node.cc:174] Finish AD API Grad: GradNodeAccumulation
I0831 09:07:41.158896 27067 accumulation_node.cc:187] { Input: [], Output: [(grad_out, [{ Name: None, Initialized: 1, Ptr: 0x556638987350 }]), ] } 
I0831 09:07:41.158901 27067 backward.cc:288] retain_graph is false, need to clear the TensorWrapper of nodes.
I0831 09:07:41.158905 27067 backward.cc:417] Finish Backward
I0831 09:07:41.158914 27067 eager_functions.cc:177]  in eager_api_run_partial_grad, after runing egr::Grad
I0831 09:07:41.158957 27067 memcpy.cc:119] src: 0x5566381d4000, dst: 0x5566381bf000, num: 1768
I0831 09:07:41.158968 27067 memcpy.cc:119] src: 0x556638190000, dst: 0x5566381c3000, num: 1768
I0831 09:07:41.175794 27067 op_desc.cc:1097] CompileTime infer shape on elementwise_div
I0831 09:07:41.176970 27067 op_desc.cc:1097] CompileTime infer shape on elementwise_div
I0831 09:07:41.177037 27067 pybind.cc:1459] need skip: 0
I0831 09:07:41.177045 27067 pybind.cc:1462] Prim Flag Open: Runing composite grad fun for elementwise_div
I0831 09:07:41.177068 27067 composite_grad_desc_maker.h:75] Runing Composite Grad func for elementwise_div_grad 
I0831 09:07:41.177129 27067 operants_manager.cc:761] OperantsManager reusing static mode API paddle::prim::pow<DescTensor>
I0831 09:07:41.177240 27067 op_desc.cc:1097] CompileTime infer shape on fill_constant
I0831 09:07:41.177279 27067 op_desc.cc:1097] CompileTime infer shape on elementwise_pow
I0831 09:07:41.177302 27067 infershape_utils.cc:547] BuildInferMetaContext: op kernel signature - Kernel Signature - name: elementwise_pow_raw; inputs: X, Y; attributes: axis; outputs: Out
I0831 09:07:41.177357 27067 operants_manager.cc:695] OperantsManager reusing static mode API paddle::prim::divide<DescTensor>
I0831 09:07:41.177379 27067 op_desc.cc:1097] CompileTime infer shape on elementwise_div
I0831 09:07:41.177397 27067 operants_manager.cc:398] OperantsManager reusing static mode API paddle::prim::scale<DescTensor>
I0831 09:07:41.177423 27067 op_desc.cc:1097] CompileTime infer shape on scale
I0831 09:07:41.177440 27067 infershape_utils.cc:547] BuildInferMetaContext: op kernel signature - Kernel Signature - name: scale; inputs: X; attributes: scale, bias, bias_after_scale; outputs: Out
I0831 09:07:41.177464 27067 operants_manager.cc:1190] OperantsManager reusing static mode API paddle::prim::multiply<DescTensor>
I0831 09:07:41.177487 27067 op_desc.cc:1097] CompileTime infer shape on elementwise_mul
I0831 09:07:41.177523 27067 op_desc.cc:1097] CompileTime infer shape on fill_constant
I0831 09:07:41.177531 27067 operants_manager.cc:695] OperantsManager reusing static mode API paddle::prim::divide<DescTensor>
I0831 09:07:41.177548 27067 op_desc.cc:1097] CompileTime infer shape on elementwise_div
I0831 09:07:41.177560 27067 operants_manager.cc:1190] OperantsManager reusing static mode API paddle::prim::multiply<DescTensor>
I0831 09:07:41.177575 27067 op_desc.cc:1097] CompileTime infer shape on elementwise_mul
I0831 09:07:41.177589 27067 composite_grad_desc_maker.h:509] Recover: composite_tmp_7 To: X@GRAD
I0831 09:07:41.177596 27067 var_desc.cc:416] Flush  composite_tmp_7 1
I0831 09:07:41.177618 27067 composite_grad_desc_maker.h:509] Recover: composite_tmp_4 To: Y@GRAD
I0831 09:07:41.177623 27067 var_desc.cc:416] Flush  composite_tmp_4 1
I0831 09:07:41.179154 27067 op_desc.cc:1097] CompileTime infer shape on fill_constant
I0831 09:07:41.179900 27067 op_desc.cc:1097] CompileTime infer shape on elementwise_pow
I0831 09:07:41.179919 27067 infershape_utils.cc:547] BuildInferMetaContext: op kernel signature - Kernel Signature - name: elementwise_pow_raw; inputs: X, Y; attributes: axis; outputs: Out
I0831 09:07:41.180578 27067 op_desc.cc:1097] CompileTime infer shape on elementwise_div
I0831 09:07:41.181186 27067 op_desc.cc:1097] CompileTime infer shape on scale
I0831 09:07:41.181208 27067 infershape_utils.cc:547] BuildInferMetaContext: op kernel signature - Kernel Signature - name: scale; inputs: X; attributes: scale, bias, bias_after_scale; outputs: Out
I0831 09:07:41.181840 27067 op_desc.cc:1097] CompileTime infer shape on elementwise_mul
I0831 09:07:41.182484 27067 op_desc.cc:1097] CompileTime infer shape on fill_constant
I0831 09:07:41.183102 27067 op_desc.cc:1097] CompileTime infer shape on elementwise_div
I0831 09:07:41.183727 27067 op_desc.cc:1097] CompileTime infer shape on elementwise_mul
I0831 09:07:41.184316 27067 op_desc.cc:1097] CompileTime infer shape on fill_constant
I0831 09:07:41.184404 27067 op_desc.cc:1097] CompileTime infer shape on elementwise_pow
I0831 09:07:41.184414 27067 infershape_utils.cc:547] BuildInferMetaContext: op kernel signature - Kernel Signature - name: elementwise_pow_raw; inputs: X, Y; attributes: axis; outputs: Out
I0831 09:07:41.184478 27067 op_desc.cc:1097] CompileTime infer shape on elementwise_div
I0831 09:07:41.184557 27067 op_desc.cc:1097] CompileTime infer shape on scale
I0831 09:07:41.184568 27067 infershape_utils.cc:547] BuildInferMetaContext: op kernel signature - Kernel Signature - name: scale; inputs: X; attributes: scale, bias, bias_after_scale; outputs: Out
I0831 09:07:41.184636 27067 op_desc.cc:1097] CompileTime infer shape on elementwise_mul
I0831 09:07:41.184725 27067 op_desc.cc:1097] CompileTime infer shape on fill_constant
I0831 09:07:41.184772 27067 op_desc.cc:1097] CompileTime infer shape on elementwise_div
I0831 09:07:41.184821 27067 op_desc.cc:1097] CompileTime infer shape on elementwise_mul
I0831 09:07:41.186347 27067 conditional_block_op_helper.cc:108] Found conditional_block op num: 0, conditional_block_grad op num: 0
I0831 09:07:41.186370 27067 while_op_helper.cc:154] Found while op num: 0, while grad op num: 0
I0831 09:07:41.186381 27067 recurrent_op_helper.cc:259] Found recurrent op num: 0, recurrent grad op num: 0
I0831 09:07:41.186404 27067 block_desc.cc:200] vars in desc 0
I0831 09:07:41.186408 27067 block_desc.cc:212] Flush feed
I0831 09:07:41.186412 27067 var_desc.cc:416] Flush  feed 1
I0831 09:07:41.186419 27067 block_desc.cc:212] Flush fetch
I0831 09:07:41.186422 27067 var_desc.cc:416] Flush  fetch 1
I0831 09:07:41.186635 27067 block_desc.cc:200] vars in desc 0
I0831 09:07:41.186640 27067 block_desc.cc:212] Flush feed
I0831 09:07:41.186643 27067 var_desc.cc:416] Flush  feed 1
I0831 09:07:41.186646 27067 block_desc.cc:212] Flush fetch
I0831 09:07:41.186650 27067 var_desc.cc:416] Flush  fetch 1
I0831 09:07:41.186656 27067 block_desc.cc:200] vars in desc 2
I0831 09:07:41.186708 27067 block_desc.cc:200] vars in desc 0
I0831 09:07:41.186712 27067 block_desc.cc:200] vars in desc 0
I0831 09:07:41.186920 27067 interpretercore.cc:45] InterpreterCore(): 0x5566372767d0 on Place(cpu)
I0831 09:07:41.186952 27067 program_interpreter.cc:49] ProgramInterpreter(): 0x5566381e2c70 on Place(cpu)
I0831 09:07:41.186970 27067 execution_config.cc:106] place:Place(cpu), processor_count:0, device_count:0, serial_run:0, num_host_threads:4, num_device_threads:0
I0831 09:07:41.186980 27067 new_executor_defs.cc:49] Set local scope: 0
I0831 09:07:41.187075 27067 program_interpreter.cc:175] New Executor is Running.
I0831 09:07:41.187084 27067 interpreter_util.cc:1105] Creating Variables
I0831 09:07:41.187098 27067 scope.cc:207] Create variable feed
I0831 09:07:41.187106 27067 interpreter_util.cc:1137] Create Variable feed global, which pointer is 0x556636c182c0 type is 9
I0831 09:07:41.187117 27067 scope.cc:207] Create variable fetch
I0831 09:07:41.187121 27067 interpreter_util.cc:1137] Create Variable fetch global, which pointer is 0x5566389d3490 type is 10
I0831 09:07:41.187132 27067 interpreter_util.cc:567] Static build: 0
I0831 09:07:41.187135 27067 conditional_block_op_helper.cc:108] Found conditional_block op num: 0, conditional_block_grad op num: 0
I0831 09:07:41.187139 27067 while_op_helper.cc:154] Found while op num: 0, while grad op num: 0
I0831 09:07:41.187144 27067 recurrent_op_helper.cc:259] Found recurrent op num: 0, recurrent grad op num: 0
I0831 09:07:41.187150 27067 mkldnn_helper.h:90] RegisterModelLayout for mkldnn
I0831 09:07:41.187155 27067 interpreter_util.cc:283] gc map size:0
I0831 09:07:41.187227 27067 program_interpreter.cc:1475] Analyze the execution order of Trace scheduling mode.
I0831 09:07:41.187239 27067 program_interpreter.cc:1465] Update sync op num, sync op num is: 0
I0831 09:07:41.187826 27067 block_desc.cc:200] vars in desc 0
I0831 09:07:41.187834 27067 block_desc.cc:212] Flush X
I0831 09:07:41.187839 27067 var_desc.cc:416] Flush  X 1
I0831 09:07:41.187845 27067 block_desc.cc:212] Flush X@GRAD
I0831 09:07:41.187848 27067 var_desc.cc:416] Flush  X@GRAD 1
I0831 09:07:41.187852 27067 block_desc.cc:212] Flush Y
I0831 09:07:41.187855 27067 var_desc.cc:416] Flush  Y 1
I0831 09:07:41.187860 27067 block_desc.cc:212] Flush Y@GRAD
I0831 09:07:41.187862 27067 var_desc.cc:416] Flush  Y@GRAD 1
I0831 09:07:41.187866 27067 block_desc.cc:212] Flush composite_tmp_0
I0831 09:07:41.187870 27067 var_desc.cc:416] Flush  composite_tmp_0 1
I0831 09:07:41.187873 27067 block_desc.cc:212] Flush composite_tmp_1
I0831 09:07:41.187876 27067 var_desc.cc:416] Flush  composite_tmp_1 1
I0831 09:07:41.187880 27067 block_desc.cc:212] Flush composite_tmp_2
I0831 09:07:41.187883 27067 var_desc.cc:416] Flush  composite_tmp_2 1
I0831 09:07:41.187894 27067 block_desc.cc:212] Flush composite_tmp_3
I0831 09:07:41.187898 27067 var_desc.cc:416] Flush  composite_tmp_3 1
I0831 09:07:41.187901 27067 block_desc.cc:212] Flush composite_tmp_5
I0831 09:07:41.187904 27067 var_desc.cc:416] Flush  composite_tmp_5 1
I0831 09:07:41.187908 27067 block_desc.cc:212] Flush composite_tmp_6
I0831 09:07:41.187911 27067 var_desc.cc:416] Flush  composite_tmp_6 1
I0831 09:07:41.187914 27067 block_desc.cc:212] Flush elementwise_div_0
I0831 09:07:41.187917 27067 var_desc.cc:416] Flush  elementwise_div_0 1
I0831 09:07:41.187922 27067 block_desc.cc:212] Flush v_0
I0831 09:07:41.187925 27067 var_desc.cc:416] Flush  v_0 1
I0831 09:07:41.190304 27067 op_desc.cc:1097] CompileTime infer shape on fetch_v2
I0831 09:07:41.190690 27067 op_desc.cc:1097] CompileTime infer shape on fetch_v2
I0831 09:07:41.191746 27067 conditional_block_op_helper.cc:108] Found conditional_block op num: 0, conditional_block_grad op num: 0
I0831 09:07:41.191762 27067 while_op_helper.cc:154] Found while op num: 0, while grad op num: 0
I0831 09:07:41.191769 27067 recurrent_op_helper.cc:259] Found recurrent op num: 0, recurrent grad op num: 0
I0831 09:07:41.191959 27067 var_desc.cc:416] Flush  elementwise_div_0 1
I0831 09:07:41.191970 27067 var_desc.cc:416] Flush  composite_tmp_0 1
I0831 09:07:41.191974 27067 var_desc.cc:416] Flush  composite_tmp_0 0
I0831 09:07:41.191982 27067 var_desc.cc:416] Flush  composite_tmp_1 1
I0831 09:07:41.191985 27067 var_desc.cc:416] Flush  composite_tmp_1 0
I0831 09:07:41.191989 27067 var_desc.cc:416] Flush  composite_tmp_2 1
I0831 09:07:41.191992 27067 var_desc.cc:416] Flush  composite_tmp_2 0
I0831 09:07:41.191995 27067 var_desc.cc:416] Flush  composite_tmp_3 1
I0831 09:07:41.191998 27067 var_desc.cc:416] Flush  composite_tmp_3 0
I0831 09:07:41.192003 27067 var_desc.cc:416] Flush  composite_tmp_5 1
I0831 09:07:41.192006 27067 var_desc.cc:416] Flush  composite_tmp_5 0
I0831 09:07:41.192009 27067 var_desc.cc:416] Flush  composite_tmp_6 1
I0831 09:07:41.192013 27067 var_desc.cc:416] Flush  composite_tmp_6 0
I0831 09:07:41.192727 27067 block_desc.cc:200] vars in desc 12
I0831 09:07:41.192739 27067 block_desc.cc:204] Flush X
I0831 09:07:41.192744 27067 var_desc.cc:416] Flush  X 1
I0831 09:07:41.192749 27067 block_desc.cc:204] Flush X@GRAD
I0831 09:07:41.192751 27067 var_desc.cc:416] Flush  X@GRAD 1
I0831 09:07:41.192755 27067 block_desc.cc:204] Flush Y
I0831 09:07:41.192759 27067 var_desc.cc:416] Flush  Y 1
I0831 09:07:41.192761 27067 block_desc.cc:204] Flush Y@GRAD
I0831 09:07:41.192764 27067 var_desc.cc:416] Flush  Y@GRAD 1
I0831 09:07:41.192768 27067 block_desc.cc:204] Flush composite_tmp_0
I0831 09:07:41.192771 27067 var_desc.cc:416] Flush  composite_tmp_0 1
I0831 09:07:41.192775 27067 block_desc.cc:204] Flush composite_tmp_1
I0831 09:07:41.192778 27067 var_desc.cc:416] Flush  composite_tmp_1 1
I0831 09:07:41.192781 27067 block_desc.cc:204] Flush composite_tmp_2
I0831 09:07:41.192785 27067 var_desc.cc:416] Flush  composite_tmp_2 1
I0831 09:07:41.192787 27067 block_desc.cc:204] Flush composite_tmp_3
I0831 09:07:41.192791 27067 var_desc.cc:416] Flush  composite_tmp_3 1
I0831 09:07:41.192795 27067 block_desc.cc:204] Flush composite_tmp_5
I0831 09:07:41.192797 27067 var_desc.cc:416] Flush  composite_tmp_5 1
I0831 09:07:41.192800 27067 block_desc.cc:204] Flush composite_tmp_6
I0831 09:07:41.192803 27067 var_desc.cc:416] Flush  composite_tmp_6 1
I0831 09:07:41.192807 27067 block_desc.cc:204] Flush elementwise_div_0
I0831 09:07:41.192811 27067 var_desc.cc:416] Flush  elementwise_div_0 1
I0831 09:07:41.192813 27067 block_desc.cc:204] Flush v_0
I0831 09:07:41.192816 27067 var_desc.cc:416] Flush  v_0 1
I0831 09:07:41.192822 27067 block_desc.cc:212] Flush feed
I0831 09:07:41.192826 27067 var_desc.cc:416] Flush  feed 1
I0831 09:07:41.192831 27067 block_desc.cc:212] Flush fetch
I0831 09:07:41.192833 27067 var_desc.cc:416] Flush  fetch 1
I0831 09:07:41.194399 27067 block_desc.cc:200] vars in desc 12
I0831 09:07:41.194415 27067 block_desc.cc:204] Flush X
I0831 09:07:41.194428 27067 var_desc.cc:416] Flush  X 1
I0831 09:07:41.194432 27067 block_desc.cc:204] Flush X@GRAD
I0831 09:07:41.194435 27067 var_desc.cc:416] Flush  X@GRAD 1
I0831 09:07:41.194439 27067 block_desc.cc:204] Flush Y
I0831 09:07:41.194442 27067 var_desc.cc:416] Flush  Y 1
I0831 09:07:41.194445 27067 block_desc.cc:204] Flush Y@GRAD
I0831 09:07:41.194449 27067 var_desc.cc:416] Flush  Y@GRAD 1
I0831 09:07:41.194453 27067 block_desc.cc:204] Flush composite_tmp_0
I0831 09:07:41.194455 27067 var_desc.cc:416] Flush  composite_tmp_0 1
I0831 09:07:41.194459 27067 block_desc.cc:204] Flush composite_tmp_1
I0831 09:07:41.194463 27067 var_desc.cc:416] Flush  composite_tmp_1 1
I0831 09:07:41.194466 27067 block_desc.cc:204] Flush composite_tmp_2
I0831 09:07:41.194469 27067 var_desc.cc:416] Flush  composite_tmp_2 1
I0831 09:07:41.194473 27067 block_desc.cc:204] Flush composite_tmp_3
I0831 09:07:41.194475 27067 var_desc.cc:416] Flush  composite_tmp_3 1
I0831 09:07:41.194478 27067 block_desc.cc:204] Flush composite_tmp_5
I0831 09:07:41.194481 27067 var_desc.cc:416] Flush  composite_tmp_5 1
I0831 09:07:41.194485 27067 block_desc.cc:204] Flush composite_tmp_6
I0831 09:07:41.194489 27067 var_desc.cc:416] Flush  composite_tmp_6 1
I0831 09:07:41.194491 27067 block_desc.cc:204] Flush elementwise_div_0
I0831 09:07:41.194494 27067 var_desc.cc:416] Flush  elementwise_div_0 1
I0831 09:07:41.194499 27067 block_desc.cc:204] Flush v_0
I0831 09:07:41.194501 27067 var_desc.cc:416] Flush  v_0 1
I0831 09:07:41.194507 27067 block_desc.cc:212] Flush feed
I0831 09:07:41.194511 27067 var_desc.cc:416] Flush  feed 1
I0831 09:07:41.194514 27067 block_desc.cc:212] Flush fetch
I0831 09:07:41.194517 27067 var_desc.cc:416] Flush  fetch 1
I0831 09:07:41.194656 27067 block_desc.cc:200] vars in desc 14
I0831 09:07:41.195240 27067 block_desc.cc:200] vars in desc 0
I0831 09:07:41.195247 27067 block_desc.cc:200] vars in desc 0
I0831 09:07:41.196794 27067 interpretercore.cc:45] InterpreterCore(): 0x5566386479c0 on Place(cpu)
I0831 09:07:41.196822 27067 program_interpreter.cc:49] ProgramInterpreter(): 0x5566386479f0 on Place(cpu)
I0831 09:07:41.196838 27067 execution_config.cc:106] place:Place(cpu), processor_count:0, device_count:0, serial_run:0, num_host_threads:4, num_device_threads:0
I0831 09:07:41.196849 27067 new_executor_defs.cc:49] Set local scope: 0
I0831 09:07:41.197010 27067 feed_fetch_method.cc:39] SetFeedVariable name=feed index=2
I0831 09:07:41.197074 27067 feed_fetch_method.cc:39] SetFeedVariable name=feed index=1
I0831 09:07:41.197111 27067 feed_fetch_method.cc:39] SetFeedVariable name=feed index=0
I0831 09:07:41.197155 27067 interpreter_util.cc:1105] Creating Variables
I0831 09:07:41.197173 27067 scope.cc:207] Create variable X
I0831 09:07:41.197180 27067 interpreter_util.cc:1142] Create Variable X locally, which pointer is 0x556638648250 type is 7
I0831 09:07:41.197191 27067 scope.cc:207] Create variable X@GRAD
I0831 09:07:41.197196 27067 interpreter_util.cc:1142] Create Variable X@GRAD locally, which pointer is 0x556638647070 type is 7
I0831 09:07:41.197201 27067 scope.cc:207] Create variable Y
I0831 09:07:41.197204 27067 interpreter_util.cc:1142] Create Variable Y locally, which pointer is 0x556638647160 type is 7
I0831 09:07:41.197208 27067 scope.cc:207] Create variable Y@GRAD
I0831 09:07:41.197212 27067 interpreter_util.cc:1142] Create Variable Y@GRAD locally, which pointer is 0x556638646960 type is 7
I0831 09:07:41.197217 27067 scope.cc:207] Create variable composite_tmp_0
I0831 09:07:41.197221 27067 interpreter_util.cc:1142] Create Variable composite_tmp_0 locally, which pointer is 0x55663864aa40 type is 7
I0831 09:07:41.197225 27067 scope.cc:207] Create variable composite_tmp_1
I0831 09:07:41.197229 27067 interpreter_util.cc:1142] Create Variable composite_tmp_1 locally, which pointer is 0x55663864ade0 type is 7
I0831 09:07:41.197234 27067 scope.cc:207] Create variable composite_tmp_2
I0831 09:07:41.197240 27067 interpreter_util.cc:1142] Create Variable composite_tmp_2 locally, which pointer is 0x55663864ca10 type is 7
I0831 09:07:41.197244 27067 scope.cc:207] Create variable composite_tmp_3
I0831 09:07:41.197258 27067 interpreter_util.cc:1142] Create Variable composite_tmp_3 locally, which pointer is 0x55663864ce00 type is 7
I0831 09:07:41.197263 27067 scope.cc:207] Create variable composite_tmp_5
I0831 09:07:41.197266 27067 interpreter_util.cc:1142] Create Variable composite_tmp_5 locally, which pointer is 0x55663864d2f0 type is 7
I0831 09:07:41.197270 27067 scope.cc:207] Create variable composite_tmp_6
I0831 09:07:41.197274 27067 interpreter_util.cc:1142] Create Variable composite_tmp_6 locally, which pointer is 0x55663864d790 type is 7
I0831 09:07:41.197278 27067 scope.cc:207] Create variable elementwise_div_0
I0831 09:07:41.197283 27067 interpreter_util.cc:1142] Create Variable elementwise_div_0 locally, which pointer is 0x55663864dc90 type is 7
I0831 09:07:41.197288 27067 interpreter_util.cc:1137] Create Variable feed global, which pointer is 0x556636c182c0 type is 9
I0831 09:07:41.197292 27067 interpreter_util.cc:1137] Create Variable fetch global, which pointer is 0x5566389d3490 type is 10
I0831 09:07:41.197296 27067 scope.cc:207] Create variable v_0
I0831 09:07:41.197300 27067 interpreter_util.cc:1142] Create Variable v_0 locally, which pointer is 0x55663864dc70 type is 7
I0831 09:07:41.197526 27067 interpreter_util.cc:567] Static build: 0
I0831 09:07:41.197532 27067 conditional_block_op_helper.cc:108] Found conditional_block op num: 0, conditional_block_grad op num: 0
I0831 09:07:41.197537 27067 while_op_helper.cc:154] Found while op num: 0, while grad op num: 0
I0831 09:07:41.197543 27067 recurrent_op_helper.cc:259] Found recurrent op num: 0, recurrent grad op num: 0
I0831 09:07:41.197548 27067 mkldnn_helper.h:90] RegisterModelLayout for mkldnn
I0831 09:07:41.197564 27067 var_desc.cc:416] Flush  v_0 1
I0831 09:07:41.197571 27067 var_desc.cc:416] Flush  Y 1
I0831 09:07:41.197574 27067 var_desc.cc:416] Flush  X 1
I0831 09:07:41.197578 27067 var_desc.cc:416] Flush  X 0
I0831 09:07:41.197582 27067 var_desc.cc:416] Flush  Y 0
I0831 09:07:41.197585 27067 var_desc.cc:416] Flush  elementwise_div_0 1
I0831 09:07:41.197589 27067 var_desc.cc:416] Flush  composite_tmp_0 1
I0831 09:07:41.197593 27067 var_desc.cc:416] Flush  Y 0
I0831 09:07:41.197597 27067 var_desc.cc:416] Flush  composite_tmp_0 0
I0831 09:07:41.197599 27067 var_desc.cc:416] Flush  composite_tmp_1 1
I0831 09:07:41.197603 27067 var_desc.cc:416] Flush  X 0
I0831 09:07:41.197607 27067 var_desc.cc:416] Flush  composite_tmp_1 0
I0831 09:07:41.197609 27067 var_desc.cc:416] Flush  composite_tmp_2 1
I0831 09:07:41.197613 27067 var_desc.cc:416] Flush  composite_tmp_2 0
I0831 09:07:41.197616 27067 var_desc.cc:416] Flush  composite_tmp_3 1
I0831 09:07:41.197619 27067 var_desc.cc:416] Flush  composite_tmp_2 0
I0831 09:07:41.197623 27067 var_desc.cc:416] Flush  composite_tmp_2 0
I0831 09:07:41.197626 27067 var_desc.cc:416] Flush  composite_tmp_3 0
I0831 09:07:41.197629 27067 var_desc.cc:416] Flush  composite_tmp_3 0
I0831 09:07:41.197633 27067 var_desc.cc:416] Flush  v_0 0
I0831 09:07:41.197635 27067 var_desc.cc:416] Flush  Y@GRAD 1
I0831 09:07:41.197639 27067 var_desc.cc:416] Flush  composite_tmp_5 1
I0831 09:07:41.197643 27067 var_desc.cc:416] Flush  composite_tmp_5 0
I0831 09:07:41.197645 27067 var_desc.cc:416] Flush  Y 0
I0831 09:07:41.197649 27067 var_desc.cc:416] Flush  composite_tmp_6 1
I0831 09:07:41.197652 27067 var_desc.cc:416] Flush  composite_tmp_6 0
I0831 09:07:41.197655 27067 var_desc.cc:416] Flush  v_0 0
I0831 09:07:41.197659 27067 var_desc.cc:416] Flush  X@GRAD 1
I0831 09:07:41.197661 27067 var_desc.cc:416] Flush  X@GRAD 0
I0831 09:07:41.197665 27067 var_desc.cc:416] Flush  Y@GRAD 0
I0831 09:07:41.197670 27067 interpreter_util.cc:281] elementwise_mul composite_tmp_6
I0831 09:07:41.197674 27067 interpreter_util.cc:281] fetch_v2 X@GRAD
I0831 09:07:41.197677 27067 interpreter_util.cc:281] elementwise_div composite_tmp_5
I0831 09:07:41.197680 27067 interpreter_util.cc:281] fetch_v2 Y@GRAD
I0831 09:07:41.197683 27067 interpreter_util.cc:281] elementwise_mul composite_tmp_3
I0831 09:07:41.197686 27067 interpreter_util.cc:281] elementwise_div Y
I0831 09:07:41.197695 27067 interpreter_util.cc:281] elementwise_div X
I0831 09:07:41.197698 27067 interpreter_util.cc:281] elementwise_pow composite_tmp_0
I0831 09:07:41.197702 27067 interpreter_util.cc:281] elementwise_div elementwise_div_0
I0831 09:07:41.197705 27067 interpreter_util.cc:281] scale composite_tmp_2
I0831 09:07:41.197708 27067 interpreter_util.cc:281] elementwise_mul v_0
I0831 09:07:41.197711 27067 interpreter_util.cc:281] elementwise_div composite_tmp_1
I0831 09:07:41.197714 27067 interpreter_util.cc:283] gc map size:9
I0831 09:07:41.197753 27067 interpreter_util.cc:677] Start run Place(cpu) Op(feed), inputs:{X[feed:[-1]({{}})()]}, outputs:{Out[v_0:[]({})()]}.
I0831 09:07:41.197790 27067 interpreter_util.cc:687] OP is not null
I0831 09:07:41.197793 27067 interpreter_util.cc:690] get op_with_kernel
I0831 09:07:41.197798 27067 interpreter_util.cc:695] get RuntimeContext
I0831 09:07:41.197822 27067 interpreter_util.cc:724] expected_kernel_key : {data_type[double]; data_layout[Undefined(AnyLayout)]; place[Place(cpu)]; library_type[PLAIN]}
I0831 09:07:41.197854 27067 operator.cc:2207] op type:feed, expected_kernel_key:{data_type[double]; data_layout[Undefined(AnyLayout)]; place[Place(cpu)]; library_type[PLAIN]}
I0831 09:07:41.197862 27067 interpreter_util.cc:772] if run phi kernel? : 1
I0831 09:07:41.197866 27067 interpreter_util.cc:787] feed : finally selected kernel_key: {data_type[double]; data_layout[Undefined(AnyLayout)]; place[Place(cpu)]; library_type[PLAIN]}
I0831 09:07:41.197893 27067 interpreter_util.cc:802] apply data transform done. 
I0831 09:07:41.197897 27067 interpreter_util.cc:806] infer shape
I0831 09:07:41.197940 27067 operator.cc:3182] Done inputs
I0831 09:07:41.197947 27067 operator.cc:3244] Done outputs
I0831 09:07:41.197952 27067 operator.cc:3496] Done attributes
I0831 09:07:41.197959 27067 operator.cc:3546] Done runtime attributes
I0831 09:07:41.197962 27067 operator.cc:3576] Done runtime extra inputs
I0831 09:07:41.197979 27067 interpreter_util.cc:961] End run Place(cpu) Op(feed), inputs:{X[feed:[-1]({{}})()]}, outputs:{Out[v_0:double[13, 17]({})(Place(cpu))]}.
I0831 09:07:41.198001 27067 interpreter_util.cc:677] Start run Place(cpu) Op(feed), inputs:{X[feed:[-1]({{}})()]}, outputs:{Out[Y:[]({})()]}.
I0831 09:07:41.198009 27067 interpreter_util.cc:687] OP is not null
I0831 09:07:41.198012 27067 interpreter_util.cc:690] get op_with_kernel
I0831 09:07:41.198015 27067 interpreter_util.cc:695] get RuntimeContext
I0831 09:07:41.198020 27067 interpreter_util.cc:724] expected_kernel_key : {data_type[double]; data_layout[Undefined(AnyLayout)]; place[Place(cpu)]; library_type[PLAIN]}
I0831 09:07:41.198029 27067 operator.cc:2207] op type:feed, expected_kernel_key:{data_type[double]; data_layout[Undefined(AnyLayout)]; place[Place(cpu)]; library_type[PLAIN]}
I0831 09:07:41.198035 27067 interpreter_util.cc:772] if run phi kernel? : 1
I0831 09:07:41.198037 27067 interpreter_util.cc:787] feed : finally selected kernel_key: {data_type[double]; data_layout[Undefined(AnyLayout)]; place[Place(cpu)]; library_type[PLAIN]}
I0831 09:07:41.198047 27067 interpreter_util.cc:802] apply data transform done. 
I0831 09:07:41.198050 27067 interpreter_util.cc:806] infer shape
I0831 09:07:41.198056 27067 operator.cc:3182] Done inputs
I0831 09:07:41.198060 27067 operator.cc:3244] Done outputs
I0831 09:07:41.198063 27067 operator.cc:3496] Done attributes
I0831 09:07:41.198066 27067 operator.cc:3546] Done runtime attributes
I0831 09:07:41.198069 27067 operator.cc:3576] Done runtime extra inputs
I0831 09:07:41.198076 27067 interpreter_util.cc:961] End run Place(cpu) Op(feed), inputs:{X[feed:[-1]({{}})()]}, outputs:{Out[Y:double[13, 17]({})(Place(cpu))]}.
I0831 09:07:41.198092 27067 interpreter_util.cc:677] Start run Place(cpu) Op(feed), inputs:{X[feed:[-1]({{}})()]}, outputs:{Out[X:[]({})()]}.
I0831 09:07:41.198100 27067 interpreter_util.cc:687] OP is not null
I0831 09:07:41.198103 27067 interpreter_util.cc:690] get op_with_kernel
I0831 09:07:41.198107 27067 interpreter_util.cc:695] get RuntimeContext
I0831 09:07:41.198114 27067 interpreter_util.cc:724] expected_kernel_key : {data_type[double]; data_layout[Undefined(AnyLayout)]; place[Place(cpu)]; library_type[PLAIN]}
I0831 09:07:41.198122 27067 operator.cc:2207] op type:feed, expected_kernel_key:{data_type[double]; data_layout[Undefined(AnyLayout)]; place[Place(cpu)]; library_type[PLAIN]}
I0831 09:07:41.198127 27067 interpreter_util.cc:772] if run phi kernel? : 1
I0831 09:07:41.198130 27067 interpreter_util.cc:787] feed : finally selected kernel_key: {data_type[double]; data_layout[Undefined(AnyLayout)]; place[Place(cpu)]; library_type[PLAIN]}
I0831 09:07:41.198139 27067 interpreter_util.cc:802] apply data transform done. 
I0831 09:07:41.198143 27067 interpreter_util.cc:806] infer shape
I0831 09:07:41.198148 27067 operator.cc:3182] Done inputs
I0831 09:07:41.198150 27067 operator.cc:3244] Done outputs
I0831 09:07:41.198153 27067 operator.cc:3496] Done attributes
I0831 09:07:41.198158 27067 operator.cc:3546] Done runtime attributes
I0831 09:07:41.198160 27067 operator.cc:3576] Done runtime extra inputs
I0831 09:07:41.198174 27067 interpreter_util.cc:961] End run Place(cpu) Op(feed), inputs:{X[feed:[-1]({{}})()]}, outputs:{Out[X:double[13, 17]({})(Place(cpu))]}.
I0831 09:07:41.198194 27067 interpreter_util.cc:677] Start run Place(cpu) Op(elementwise_div), inputs:{X[X:double[13, 17]({})(Place(cpu))], Y[Y:double[13, 17]({})(Place(cpu))]}, outputs:{Out[elementwise_div_0:[]({})()]}.
I0831 09:07:41.198205 27067 interpreter_util.cc:687] OP is not null
I0831 09:07:41.198208 27067 interpreter_util.cc:690] get op_with_kernel
I0831 09:07:41.198211 27067 interpreter_util.cc:695] get RuntimeContext
I0831 09:07:41.198220 27067 interpreter_util.cc:724] expected_kernel_key : {data_type[double]; data_layout[Undefined(AnyLayout)]; place[Place(cpu)]; library_type[PLAIN]}
I0831 09:07:41.198232 27067 operator.cc:2207] op type:elementwise_div, expected_kernel_key:{data_type[double]; data_layout[Undefined(AnyLayout)]; place[Place(cpu)]; library_type[PLAIN]}
I0831 09:07:41.198237 27067 interpreter_util.cc:772] if run phi kernel? : 1
I0831 09:07:41.198241 27067 interpreter_util.cc:787] elementwise_div : finally selected kernel_key: {data_type[double]; data_layout[Undefined(AnyLayout)]; place[Place(cpu)]; library_type[PLAIN]}
I0831 09:07:41.198253 27067 interpreter_util.cc:802] apply data transform done. 
I0831 09:07:41.198256 27067 interpreter_util.cc:806] infer shape
I0831 09:07:41.198271 27067 operator.cc:3182] Done inputs
I0831 09:07:41.198273 27067 operator.cc:3244] Done outputs
I0831 09:07:41.198276 27067 operator.cc:3496] Done attributes
I0831 09:07:41.198280 27067 operator.cc:3031] Runtime attr `mkldnn_data_type` is passed to OneDNNContext.
I0831 09:07:41.198293 27067 operator.cc:3031] Runtime attr `Scale_x` is passed to OneDNNContext.
I0831 09:07:41.198297 27067 operator.cc:3031] Runtime attr `Scale_out` is passed to OneDNNContext.
I0831 09:07:41.198309 27067 operator.cc:3031] Runtime attr `Scale_y` is passed to OneDNNContext.
I0831 09:07:41.198313 27067 operator.cc:3546] Done runtime attributes
I0831 09:07:41.198316 27067 operator.cc:3576] Done runtime extra inputs
I0831 09:07:41.198339 27067 interpreter_util.cc:961] End run Place(cpu) Op(elementwise_div), inputs:{X[X:double[13, 17]({})(Place(cpu))], Y[Y:double[13, 17]({})(Place(cpu))]}, outputs:{Out[elementwise_div_0:double[13, 17]({})(Place(cpu))]}.
I0831 09:07:41.198377 27067 interpreter_util.cc:677] Start run Place(cpu) Op(fill_constant), inputs:{ShapeTensor[], ShapeTensorList[], ValueTensor[]}, outputs:{Out[composite_tmp_0:[]({})()]}.
I0831 09:07:41.198385 27067 interpreter_util.cc:687] OP is not null
I0831 09:07:41.198388 27067 interpreter_util.cc:690] get op_with_kernel
I0831 09:07:41.198391 27067 interpreter_util.cc:695] get RuntimeContext
I0831 09:07:41.198398 27067 interpreter_util.cc:724] expected_kernel_key : {data_type[double]; data_layout[Undefined(AnyLayout)]; place[Place(cpu)]; library_type[PLAIN]}
I0831 09:07:41.198419 27067 operator.cc:2207] op type:fill_constant, expected_kernel_key:{data_type[double]; data_layout[Undefined(AnyLayout)]; place[Place(cpu)]; library_type[PLAIN]}
I0831 09:07:41.198429 27067 interpreter_util.cc:772] if run phi kernel? : 1
I0831 09:07:41.198433 27067 interpreter_util.cc:787] fill_constant : finally selected kernel_key: {data_type[double]; data_layout[Undefined(AnyLayout)]; place[Place(cpu)]; library_type[PLAIN]}
I0831 09:07:41.198444 27067 interpreter_util.cc:802] apply data transform done. 
I0831 09:07:41.198447 27067 interpreter_util.cc:806] infer shape
I0831 09:07:41.198457 27067 operator.cc:3182] Done inputs
I0831 09:07:41.198460 27067 operator.cc:3244] Done outputs
I0831 09:07:41.198498 27067 operator.cc:3496] Done attributes
I0831 09:07:41.198504 27067 operator.cc:3546] Done runtime attributes
I0831 09:07:41.198508 27067 operator.cc:3576] Done runtime extra inputs
I0831 09:07:41.198540 27067 interpreter_util.cc:961] End run Place(cpu) Op(fill_constant), inputs:{ShapeTensor[], ShapeTensorList[], ValueTensor[]}, outputs:{Out[composite_tmp_0:double[13, 17]({})(Place(cpu))]}.
I0831 09:07:41.198558 27067 interpreter_util.cc:677] Start run Place(cpu) Op(elementwise_pow), inputs:{X[Y:double[13, 17]({})(Place(cpu))], Y[composite_tmp_0:double[13, 17]({})(Place(cpu))]}, outputs:{Out[composite_tmp_1:[]({})()]}.
I0831 09:07:41.198571 27067 interpreter_util.cc:687] OP is not null
I0831 09:07:41.198575 27067 interpreter_util.cc:690] get op_with_kernel
I0831 09:07:41.198576 27067 interpreter_util.cc:695] get RuntimeContext
I0831 09:07:41.198585 27067 interpreter_util.cc:724] expected_kernel_key : {data_type[double]; data_layout[Undefined(AnyLayout)]; place[Place(cpu)]; library_type[PLAIN]}
I0831 09:07:41.198596 27067 operator.cc:2207] op type:elementwise_pow, expected_kernel_key:{data_type[double]; data_layout[Undefined(AnyLayout)]; place[Place(cpu)]; library_type[PLAIN]}
I0831 09:07:41.198601 27067 interpreter_util.cc:772] if run phi kernel? : 1
I0831 09:07:41.198604 27067 interpreter_util.cc:787] elementwise_pow : finally selected kernel_key: {data_type[double]; data_layout[Undefined(AnyLayout)]; place[Place(cpu)]; library_type[PLAIN]}
I0831 09:07:41.198616 27067 interpreter_util.cc:802] apply data transform done. 
I0831 09:07:41.198618 27067 interpreter_util.cc:806] infer shape
I0831 09:07:41.198626 27067 infershape_utils.cc:547] BuildInferMetaContext: op kernel signature - Kernel Signature - name: elementwise_pow_raw; inputs: X, Y; attributes: axis; outputs: Out
I0831 09:07:41.198652 27067 operator.cc:3182] Done inputs
I0831 09:07:41.198657 27067 operator.cc:3244] Done outputs
I0831 09:07:41.198659 27067 operator.cc:3496] Done attributes
I0831 09:07:41.198663 27067 operator.cc:3031] Runtime attr `mkldnn_data_type` is passed to OneDNNContext.
I0831 09:07:41.198668 27067 operator.cc:3031] Runtime attr `Scale_x` is passed to OneDNNContext.
I0831 09:07:41.198670 27067 operator.cc:3031] Runtime attr `Scale_out` is passed to OneDNNContext.
I0831 09:07:41.198673 27067 operator.cc:3031] Runtime attr `Scale_y` is passed to OneDNNContext.
I0831 09:07:41.198678 27067 operator.cc:3546] Done runtime attributes
I0831 09:07:41.198680 27067 operator.cc:3576] Done runtime extra inputs
I0831 09:07:41.198693 27067 interpreter_util.cc:961] End run Place(cpu) Op(elementwise_pow), inputs:{X[Y:double[13, 17]({})(Place(cpu))], Y[composite_tmp_0:double[13, 17]({})(Place(cpu))]}, outputs:{Out[composite_tmp_1:double[13, 17]({})(Place(cpu))]}.
I0831 09:07:41.198716 27067 interpreter_util.cc:677] Start run Place(cpu) Op(elementwise_div), inputs:{X[X:double[13, 17]({})(Place(cpu))], Y[composite_tmp_1:double[13, 17]({})(Place(cpu))]}, outputs:{Out[composite_tmp_2:[]({})()]}.
I0831 09:07:41.198727 27067 interpreter_util.cc:687] OP is not null
I0831 09:07:41.198730 27067 interpreter_util.cc:690] get op_with_kernel
I0831 09:07:41.198733 27067 interpreter_util.cc:695] get RuntimeContext
I0831 09:07:41.198738 27067 interpreter_util.cc:724] expected_kernel_key : {data_type[double]; data_layout[Undefined(AnyLayout)]; place[Place(cpu)]; library_type[PLAIN]}
I0831 09:07:41.198746 27067 operator.cc:2207] op type:elementwise_div, expected_kernel_key:{data_type[double]; data_layout[Undefined(AnyLayout)]; place[Place(cpu)]; library_type[PLAIN]}
I0831 09:07:41.198755 27067 interpreter_util.cc:772] if run phi kernel? : 1
I0831 09:07:41.198760 27067 interpreter_util.cc:787] elementwise_div : finally selected kernel_key: {data_type[double]; data_layout[Undefined(AnyLayout)]; place[Place(cpu)]; library_type[PLAIN]}
I0831 09:07:41.198770 27067 interpreter_util.cc:802] apply data transform done. 
I0831 09:07:41.198773 27067 interpreter_util.cc:806] infer shape
I0831 09:07:41.198781 27067 operator.cc:3182] Done inputs
I0831 09:07:41.198784 27067 operator.cc:3244] Done outputs
I0831 09:07:41.198787 27067 operator.cc:3496] Done attributes
I0831 09:07:41.198791 27067 operator.cc:3031] Runtime attr `mkldnn_data_type` is passed to OneDNNContext.
I0831 09:07:41.198794 27067 operator.cc:3031] Runtime attr `Scale_x` is passed to OneDNNContext.
I0831 09:07:41.198797 27067 operator.cc:3031] Runtime attr `Scale_out` is passed to OneDNNContext.
I0831 09:07:41.198801 27067 operator.cc:3031] Runtime attr `Scale_y` is passed to OneDNNContext.
I0831 09:07:41.198803 27067 operator.cc:3546] Done runtime attributes
I0831 09:07:41.198807 27067 operator.cc:3576] Done runtime extra inputs
I0831 09:07:41.198815 27067 interpreter_util.cc:961] End run Place(cpu) Op(elementwise_div), inputs:{X[X:double[13, 17]({})(Place(cpu))], Y[composite_tmp_1:double[13, 17]({})(Place(cpu))]}, outputs:{Out[composite_tmp_2:double[13, 17]({})(Place(cpu))]}.
I0831 09:07:41.198848 27067 interpreter_util.cc:677] Start run Place(cpu) Op(share_buffer), inputs:{X[composite_tmp_2:double[13, 17]({})(Place(cpu))]}, outputs:{Out[composite_tmp_3:[]({})()], XOut[composite_tmp_2:double[13, 17]({})(Place(cpu))]}.
I0831 09:07:41.198861 27067 interpreter_util.cc:687] OP is not null
I0831 09:07:41.198864 27067 interpreter_util.cc:690] get op_with_kernel
I0831 09:07:41.198868 27067 interpreter_util.cc:695] get RuntimeContext
I0831 09:07:41.198874 27067 interpreter_util.cc:724] expected_kernel_key : {data_type[double]; data_layout[Undefined(AnyLayout)]; place[Place(cpu)]; library_type[PLAIN]}
I0831 09:07:41.198891 27067 operator.cc:2207] op type:share_buffer, expected_kernel_key:{data_type[double]; data_layout[Undefined(AnyLayout)]; place[Place(cpu)]; library_type[PLAIN]}
I0831 09:07:41.198897 27067 interpreter_util.cc:772] if run phi kernel? : 1
I0831 09:07:41.198900 27067 interpreter_util.cc:787] share_buffer : finally selected kernel_key: {data_type[double]; data_layout[Undefined(AnyLayout)]; place[Place(cpu)]; library_type[PLAIN]}
I0831 09:07:41.198908 27067 interpreter_util.cc:802] apply data transform done. 
I0831 09:07:41.198912 27067 interpreter_util.cc:806] infer shape
I0831 09:07:41.198918 27067 infershape_utils.cc:547] BuildInferMetaContext: op kernel signature - Kernel Signature - name: share_buffer; inputs: X; attributes: share_dims_and_dtype; outputs: Out, XOut
I0831 09:07:41.198943 27067 operator.cc:3182] Done inputs
I0831 09:07:41.198947 27067 operator.cc:3244] Done outputs
I0831 09:07:41.198951 27067 operator.cc:3496] Done attributes
I0831 09:07:41.198956 27067 operator.cc:3546] Done runtime attributes
I0831 09:07:41.198958 27067 operator.cc:3576] Done runtime extra inputs
I0831 09:07:41.198971 27067 interpreter_util.cc:961] End run Place(cpu) Op(share_buffer), inputs:{X[composite_tmp_2:double[13, 17]({})(Place(cpu))]}, outputs:{Out[composite_tmp_3:double[]({})(Place(cpu))], XOut[composite_tmp_2:double[13, 17]({})(Place(cpu))]}.
I0831 09:07:41.198994 27067 interpreter_util.cc:677] Start run Place(cpu) Op(scale), inputs:{ScaleTensor[], X[composite_tmp_2:double[13, 17]({})(Place(cpu))]}, outputs:{Out[composite_tmp_3:double[]({})(Place(cpu))]}.
I0831 09:07:41.199007 27067 interpreter_util.cc:687] OP is not null
I0831 09:07:41.199009 27067 interpreter_util.cc:690] get op_with_kernel
I0831 09:07:41.199012 27067 interpreter_util.cc:695] get RuntimeContext
I0831 09:07:41.199021 27067 interpreter_util.cc:724] expected_kernel_key : {data_type[double]; data_layout[Undefined(AnyLayout)]; place[Place(cpu)]; library_type[PLAIN]}
I0831 09:07:41.199038 27067 operator.cc:2207] op type:scale, expected_kernel_key:{data_type[double]; data_layout[Undefined(AnyLayout)]; place[Place(cpu)]; library_type[PLAIN]}
I0831 09:07:41.199044 27067 interpreter_util.cc:772] if run phi kernel? : 1
I0831 09:07:41.199048 27067 interpreter_util.cc:787] scale : finally selected kernel_key: {data_type[double]; data_layout[Undefined(AnyLayout)]; place[Place(cpu)]; library_type[PLAIN]}
I0831 09:07:41.199060 27067 interpreter_util.cc:802] apply data transform done. 
I0831 09:07:41.199064 27067 interpreter_util.cc:806] infer shape
I0831 09:07:41.199069 27067 infershape_utils.cc:547] BuildInferMetaContext: op kernel signature - Kernel Signature - name: scale; inputs: X; attributes: scale, bias, bias_after_scale; outputs: Out
I0831 09:07:41.199085 27067 operator.cc:3182] Done inputs
I0831 09:07:41.199090 27067 operator.cc:3244] Done outputs
I0831 09:07:41.199093 27067 operator.cc:3496] Done attributes
I0831 09:07:41.199098 27067 operator.cc:3546] Done runtime attributes
I0831 09:07:41.199101 27067 operator.cc:3576] Done runtime extra inputs
I0831 09:07:41.199126 27067 interpreter_util.cc:961] End run Place(cpu) Op(scale), inputs:{ScaleTensor[], X[composite_tmp_2:double[13, 17]({})(Place(cpu))]}, outputs:{Out[composite_tmp_3:double[13, 17]({})(Place(cpu))]}.
I0831 09:07:41.199148 27067 interpreter_util.cc:677] Start run Place(cpu) Op(elementwise_mul), inputs:{X[composite_tmp_3:double[13, 17]({})(Place(cpu))], Y[v_0:double[13, 17]({})(Place(cpu))]}, outputs:{Out[Y@GRAD:[]({})()]}.
I0831 09:07:41.199162 27067 interpreter_util.cc:687] OP is not null
I0831 09:07:41.199188 27067 interpreter_util.cc:690] get op_with_kernel
I0831 09:07:41.199193 27067 interpreter_util.cc:695] get RuntimeContext
I0831 09:07:41.199198 27067 interpreter_util.cc:724] expected_kernel_key : {data_type[double]; data_layout[Undefined(AnyLayout)]; place[Place(cpu)]; library_type[PLAIN]}
I0831 09:07:41.199211 27067 operator.cc:2207] op type:elementwise_mul, expected_kernel_key:{data_type[double]; data_layout[Undefined(AnyLayout)]; place[Place(cpu)]; library_type[PLAIN]}
I0831 09:07:41.199218 27067 interpreter_util.cc:772] if run phi kernel? : 1
I0831 09:07:41.199220 27067 interpreter_util.cc:787] elementwise_mul : finally selected kernel_key: {data_type[double]; data_layout[Undefined(AnyLayout)]; place[Place(cpu)]; library_type[PLAIN]}
I0831 09:07:41.199234 27067 interpreter_util.cc:802] apply data transform done. 
I0831 09:07:41.199236 27067 interpreter_util.cc:806] infer shape
I0831 09:07:41.199245 27067 operator.cc:3182] Done inputs
I0831 09:07:41.199249 27067 operator.cc:3244] Done outputs
I0831 09:07:41.199252 27067 operator.cc:3496] Done attributes
I0831 09:07:41.199256 27067 operator.cc:3031] Runtime attr `mkldnn_data_type` is passed to OneDNNContext.
I0831 09:07:41.199260 27067 operator.cc:3031] Runtime attr `Scale_x` is passed to OneDNNContext.
I0831 09:07:41.199265 27067 operator.cc:3031] Runtime attr `Scale_out` is passed to OneDNNContext.
I0831 09:07:41.199268 27067 operator.cc:3031] Runtime attr `Scale_y` is passed to OneDNNContext.
I0831 09:07:41.199271 27067 operator.cc:3546] Done runtime attributes
I0831 09:07:41.199275 27067 operator.cc:3576] Done runtime extra inputs
I0831 09:07:41.199287 27067 interpreter_util.cc:961] End run Place(cpu) Op(elementwise_mul), inputs:{X[composite_tmp_3:double[13, 17]({})(Place(cpu))], Y[v_0:double[13, 17]({})(Place(cpu))]}, outputs:{Out[Y@GRAD:double[13, 17]({})(Place(cpu))]}.
I0831 09:07:41.199312 27067 interpreter_util.cc:677] Start run Place(cpu) Op(fill_constant), inputs:{ShapeTensor[], ShapeTensorList[], ValueTensor[]}, outputs:{Out[composite_tmp_5:[]({})()]}.
I0831 09:07:41.199321 27067 interpreter_util.cc:687] OP is not null
I0831 09:07:41.199324 27067 interpreter_util.cc:690] get op_with_kernel
I0831 09:07:41.199327 27067 interpreter_util.cc:695] get RuntimeContext
I0831 09:07:41.199332 27067 interpreter_util.cc:724] expected_kernel_key : {data_type[double]; data_layout[Undefined(AnyLayout)]; place[Place(cpu)]; library_type[PLAIN]}
I0831 09:07:41.199349 27067 operator.cc:2207] op type:fill_constant, expected_kernel_key:{data_type[double]; data_layout[Undefined(AnyLayout)]; place[Place(cpu)]; library_type[PLAIN]}
I0831 09:07:41.199354 27067 interpreter_util.cc:772] if run phi kernel? : 1
I0831 09:07:41.199358 27067 interpreter_util.cc:787] fill_constant : finally selected kernel_key: {data_type[double]; data_layout[Undefined(AnyLayout)]; place[Place(cpu)]; library_type[PLAIN]}
I0831 09:07:41.199370 27067 interpreter_util.cc:802] apply data transform done. 
I0831 09:07:41.199373 27067 interpreter_util.cc:806] infer shape
I0831 09:07:41.199380 27067 operator.cc:3182] Done inputs
I0831 09:07:41.199384 27067 operator.cc:3244] Done outputs
I0831 09:07:41.199393 27067 operator.cc:3496] Done attributes
I0831 09:07:41.199398 27067 operator.cc:3546] Done runtime attributes
I0831 09:07:41.199401 27067 operator.cc:3576] Done runtime extra inputs
I0831 09:07:41.199411 27067 interpreter_util.cc:961] End run Place(cpu) Op(fill_constant), inputs:{ShapeTensor[], ShapeTensorList[], ValueTensor[]}, outputs:{Out[composite_tmp_5:double[13, 17]({})(Place(cpu))]}.
I0831 09:07:41.199429 27067 interpreter_util.cc:677] Start run Place(cpu) Op(elementwise_div), inputs:{X[composite_tmp_5:double[13, 17]({})(Place(cpu))], Y[Y:double[13, 17]({})(Place(cpu))]}, outputs:{Out[composite_tmp_6:[]({})()]}.
I0831 09:07:41.199441 27067 interpreter_util.cc:687] OP is not null
I0831 09:07:41.199445 27067 interpreter_util.cc:690] get op_with_kernel
I0831 09:07:41.199448 27067 interpreter_util.cc:695] get RuntimeContext
I0831 09:07:41.199453 27067 interpreter_util.cc:724] expected_kernel_key : {data_type[double]; data_layout[Undefined(AnyLayout)]; place[Place(cpu)]; library_type[PLAIN]}
I0831 09:07:41.199462 27067 operator.cc:2207] op type:elementwise_div, expected_kernel_key:{data_type[double]; data_layout[Undefined(AnyLayout)]; place[Place(cpu)]; library_type[PLAIN]}
I0831 09:07:41.199468 27067 interpreter_util.cc:772] if run phi kernel? : 1
I0831 09:07:41.199471 27067 interpreter_util.cc:787] elementwise_div : finally selected kernel_key: {data_type[double]; data_layout[Undefined(AnyLayout)]; place[Place(cpu)]; library_type[PLAIN]}
I0831 09:07:41.199482 27067 interpreter_util.cc:802] apply data transform done. 
I0831 09:07:41.199486 27067 interpreter_util.cc:806] infer shape
I0831 09:07:41.199505 27067 operator.cc:3182] Done inputs
I0831 09:07:41.199508 27067 operator.cc:3244] Done outputs
I0831 09:07:41.199512 27067 operator.cc:3496] Done attributes
I0831 09:07:41.199515 27067 operator.cc:3031] Runtime attr `mkldnn_data_type` is passed to OneDNNContext.
I0831 09:07:41.199519 27067 operator.cc:3031] Runtime attr `Scale_x` is passed to OneDNNContext.
I0831 09:07:41.199522 27067 operator.cc:3031] Runtime attr `Scale_out` is passed to OneDNNContext.
I0831 09:07:41.199527 27067 operator.cc:3031] Runtime attr `Scale_y` is passed to OneDNNContext.
I0831 09:07:41.199529 27067 operator.cc:3546] Done runtime attributes
I0831 09:07:41.199532 27067 operator.cc:3576] Done runtime extra inputs
I0831 09:07:41.199541 27067 interpreter_util.cc:961] End run Place(cpu) Op(elementwise_div), inputs:{X[composite_tmp_5:double[13, 17]({})(Place(cpu))], Y[Y:double[13, 17]({})(Place(cpu))]}, outputs:{Out[composite_tmp_6:double[13, 17]({})(Place(cpu))]}.
I0831 09:07:41.199563 27067 interpreter_util.cc:677] Start run Place(cpu) Op(elementwise_mul), inputs:{X[composite_tmp_6:double[13, 17]({})(Place(cpu))], Y[v_0:double[13, 17]({})(Place(cpu))]}, outputs:{Out[X@GRAD:[]({})()]}.
I0831 09:07:41.199585 27067 interpreter_util.cc:687] OP is not null
I0831 09:07:41.199589 27067 interpreter_util.cc:690] get op_with_kernel
I0831 09:07:41.199591 27067 interpreter_util.cc:695] get RuntimeContext
I0831 09:07:41.199595 27067 interpreter_util.cc:724] expected_kernel_key : {data_type[double]; data_layout[Undefined(AnyLayout)]; place[Place(cpu)]; library_type[PLAIN]}
I0831 09:07:41.199604 27067 operator.cc:2207] op type:elementwise_mul, expected_kernel_key:{data_type[double]; data_layout[Undefined(AnyLayout)]; place[Place(cpu)]; library_type[PLAIN]}
I0831 09:07:41.199612 27067 interpreter_util.cc:772] if run phi kernel? : 1
I0831 09:07:41.199616 27067 interpreter_util.cc:787] elementwise_mul : finally selected kernel_key: {data_type[double]; data_layout[Undefined(AnyLayout)]; place[Place(cpu)]; library_type[PLAIN]}
I0831 09:07:41.199627 27067 interpreter_util.cc:802] apply data transform done. 
I0831 09:07:41.199630 27067 interpreter_util.cc:806] infer shape
I0831 09:07:41.199636 27067 operator.cc:3182] Done inputs
I0831 09:07:41.199640 27067 operator.cc:3244] Done outputs
I0831 09:07:41.199642 27067 operator.cc:3496] Done attributes
I0831 09:07:41.199646 27067 operator.cc:3031] Runtime attr `mkldnn_data_type` is passed to OneDNNContext.
I0831 09:07:41.199649 27067 operator.cc:3031] Runtime attr `Scale_x` is passed to OneDNNContext.
I0831 09:07:41.199653 27067 operator.cc:3031] Runtime attr `Scale_out` is passed to OneDNNContext.
I0831 09:07:41.199656 27067 operator.cc:3031] Runtime attr `Scale_y` is passed to OneDNNContext.
I0831 09:07:41.199659 27067 operator.cc:3546] Done runtime attributes
I0831 09:07:41.199662 27067 operator.cc:3576] Done runtime extra inputs
I0831 09:07:41.199671 27067 interpreter_util.cc:961] End run Place(cpu) Op(elementwise_mul), inputs:{X[composite_tmp_6:double[13, 17]({})(Place(cpu))], Y[v_0:double[13, 17]({})(Place(cpu))]}, outputs:{Out[X@GRAD:double[13, 17]({})(Place(cpu))]}.
I0831 09:07:41.199690 27067 interpreter_util.cc:677] Start run Place(cpu) Op(fetch_v2), inputs:{X[X@GRAD:double[13, 17]({})(Place(cpu))]}, outputs:{Out[fetch:[-1]({{}})()]}.
I0831 09:07:41.199702 27067 interpreter_util.cc:687] OP is not null
I0831 09:07:41.199704 27067 interpreter_util.cc:690] get op_with_kernel
I0831 09:07:41.199707 27067 interpreter_util.cc:695] get RuntimeContext
I0831 09:07:41.199713 27067 interpreter_util.cc:724] expected_kernel_key : {data_type[double]; data_layout[Undefined(AnyLayout)]; place[Place(cpu)]; library_type[PLAIN]}
I0831 09:07:41.199723 27067 operator.cc:2207] op type:fetch_v2, expected_kernel_key:{data_type[double]; data_layout[Undefined(AnyLayout)]; place[Place(cpu)]; library_type[PLAIN]}
I0831 09:07:41.199728 27067 interpreter_util.cc:772] if run phi kernel? : 1
I0831 09:07:41.199730 27067 interpreter_util.cc:787] fetch_v2 : finally selected kernel_key: {data_type[double]; data_layout[Undefined(AnyLayout)]; place[Place(cpu)]; library_type[PLAIN]}
I0831 09:07:41.199740 27067 interpreter_util.cc:802] apply data transform done. 
I0831 09:07:41.199743 27067 interpreter_util.cc:806] infer shape
I0831 09:07:41.199755 27067 fetch_v2_op.cc:143] Fetch variable X@GRAD's 0 column.
I0831 09:07:41.199771 27067 tensor_util.cc:309] TensorCopySync 13, 17 from Place(cpu) to Place(cpu)
I0831 09:07:41.199784 27067 tensor_util.cc:322] src:0x556638671000, dst:0x55663866f000
I0831 09:07:41.199790 27067 memcpy.cc:743] memory::Copy 1768 Bytes from Place(cpu) to Place(cpu)
I0831 09:07:41.199795 27067 interpreter_util.cc:961] End run Place(cpu) Op(fetch_v2), inputs:{X[X@GRAD:double[13, 17]({})(Place(cpu))]}, outputs:{Out[fetch:[-1]({{}})()]}.
I0831 09:07:41.199812 27067 interpreter_util.cc:677] Start run Place(cpu) Op(fetch_v2), inputs:{X[Y@GRAD:double[13, 17]({})(Place(cpu))]}, outputs:{Out[fetch:[-1]({{}})()]}.
I0831 09:07:41.199821 27067 interpreter_util.cc:687] OP is not null
I0831 09:07:41.199824 27067 interpreter_util.cc:690] get op_with_kernel
I0831 09:07:41.199827 27067 interpreter_util.cc:695] get RuntimeContext
I0831 09:07:41.199831 27067 interpreter_util.cc:724] expected_kernel_key : {data_type[double]; data_layout[Undefined(AnyLayout)]; place[Place(cpu)]; library_type[PLAIN]}
I0831 09:07:41.199838 27067 operator.cc:2207] op type:fetch_v2, expected_kernel_key:{data_type[double]; data_layout[Undefined(AnyLayout)]; place[Place(cpu)]; library_type[PLAIN]}
I0831 09:07:41.199843 27067 interpreter_util.cc:772] if run phi kernel? : 1
I0831 09:07:41.199846 27067 interpreter_util.cc:787] fetch_v2 : finally selected kernel_key: {data_type[double]; data_layout[Undefined(AnyLayout)]; place[Place(cpu)]; library_type[PLAIN]}
I0831 09:07:41.199859 27067 interpreter_util.cc:802] apply data transform done. 
I0831 09:07:41.199862 27067 interpreter_util.cc:806] infer shape
I0831 09:07:41.199867 27067 fetch_v2_op.cc:143] Fetch variable Y@GRAD's 1 column.
I0831 09:07:41.199877 27067 tensor_util.cc:309] TensorCopySync 13, 17 from Place(cpu) to Place(cpu)
I0831 09:07:41.199882 27067 tensor_util.cc:322] src:0x55663866e000, dst:0x556638670000
I0831 09:07:41.199887 27067 memcpy.cc:743] memory::Copy 1768 Bytes from Place(cpu) to Place(cpu)
I0831 09:07:41.199892 27067 interpreter_util.cc:961] End run Place(cpu) Op(fetch_v2), inputs:{X[Y@GRAD:double[13, 17]({})(Place(cpu))]}, outputs:{Out[fetch:[-1]({{}})()]}.
I0831 09:07:41.200237 27067 program_interpreter.cc:729] not clear feed after feed because its type is N6paddle9framework9PhiVectorINS_7variantIJN3phi11DenseTensorENS1_INSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEEEENS3_15SparseCooTensorEEEEEE
I0831 09:07:41.200309 27067 program_interpreter.cc:729] not clear feed after feed because its type is N6paddle9framework9PhiVectorINS_7variantIJN3phi11DenseTensorENS1_INSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEEEENS3_15SparseCooTensorEEEEEE
I0831 09:07:41.200315 27067 program_interpreter.cc:729] not clear feed after feed because its type is N6paddle9framework9PhiVectorINS_7variantIJN3phi11DenseTensorENS1_INSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEEEENS3_15SparseCooTensorEEEEEE
I0831 09:07:41.200345 27067 program_interpreter.cc:729] not clear fetch after fetch_v2 because its type is St6vectorIN6paddle7variantIJN3phi11DenseTensorENS2_11TensorArrayENS0_9framework5VocabENS2_15SparseCooTensorEEEESaIS8_EE
I0831 09:07:41.200351 27067 program_interpreter.cc:729] not clear fetch after fetch_v2 because its type is St6vectorIN6paddle7variantIJN3phi11DenseTensorENS2_11TensorArrayENS0_9framework5VocabENS2_15SparseCooTensorEEEESaIS8_EE
I0831 09:07:41.200397 27067 program_interpreter.cc:785] Already inplaced, skip inplace now.
I0831 09:07:41.200407 27067 program_interpreter.cc:1475] Analyze the execution order of Trace scheduling mode.
I0831 09:07:41.200415 27067 program_interpreter.cc:1481] op_id: 11, remain deps: 2
I0831 09:07:41.200419 27067 program_interpreter.cc:1481] op_id: 5, remain deps: 2
I0831 09:07:41.200423 27067 program_interpreter.cc:1481] op_id: 3, remain deps: 2
I0831 09:07:41.200426 27067 program_interpreter.cc:1481] op_id: 6, remain deps: 2
I0831 09:07:41.200429 27067 program_interpreter.cc:1481] op_id: 3, remain deps: 1
I0831 09:07:41.200433 27067 program_interpreter.cc:1481] op_id: 5, remain deps: 1
I0831 09:07:41.200435 27067 program_interpreter.cc:1481] op_id: 11, remain deps: 1
I0831 09:07:41.200438 27067 program_interpreter.cc:1481] op_id: 12, remain deps: 2
I0831 09:07:41.200443 27067 program_interpreter.cc:1481] op_id: 6, remain deps: 1
I0831 09:07:41.200445 27067 program_interpreter.cc:1481] op_id: 7, remain deps: 1
I0831 09:07:41.200448 27067 program_interpreter.cc:1481] op_id: 8, remain deps: 1
I0831 09:07:41.200451 27067 program_interpreter.cc:1481] op_id: 9, remain deps: 2
I0831 09:07:41.200455 27067 program_interpreter.cc:1481] op_id: 9, remain deps: 1
I0831 09:07:41.200459 27067 program_interpreter.cc:1481] op_id: 12, remain deps: 1
I0831 09:07:41.200461 27067 program_interpreter.cc:1481] op_id: 13, remain deps: 1
I0831 09:07:41.200464 27067 program_interpreter.cc:1481] op_id: 14, remain deps: 2
I0831 09:07:41.200467 27067 program_interpreter.cc:1481] op_id: 14, remain deps: 1
I0831 09:07:41.200479 27067 program_interpreter.cc:1465] Update sync op num, sync op num is: 15
I0831 09:07:41.210577 27067 interpretercore.cc:62] ~InterpreterCore(): 0x5566372767d0
I0831 09:07:41.210620 27067 program_interpreter.cc:90] ~ProgramInterpreter(): 0x5566381e2c70 on Place(cpu)
I0831 09:07:41.210645 27067 onednn_context.cc:101] 0x5566386479f0 0x5566381e2c70
I0831 09:07:41.210651 27067 onednn_context.cc:104] Clearing DNNL cache.
I0831 09:07:41.210654 27067 onednn_context.cc:122] Resetting Paddle data layout to NCHW.
I0831 09:07:41.210700 27067 interpretercore.cc:62] ~InterpreterCore(): 0x5566386479c0
I0831 09:07:41.210714 27067 program_interpreter.cc:90] ~ProgramInterpreter(): 0x5566386479f0 on Place(cpu)
I0831 09:07:41.210718 27067 onednn_context.cc:101] 0x5566386479f0 0x5566386479f0
I0831 09:07:41.210721 27067 onednn_context.cc:104] Clearing DNNL cache.
I0831 09:07:41.210724 27067 onednn_context.cc:122] Resetting Paddle data layout to NCHW.
I0831 09:07:41.210991 27067 onednn_context.cc:101] 0x5566386479f0 0x556638980f70
I0831 09:07:41.210996 27067 onednn_context.cc:104] Clearing DNNL cache.
I0831 09:07:41.210999 27067 onednn_context.cc:122] Resetting Paddle data layout to NCHW.
I0831 09:07:41.211030 27067 pybind.cc:1718] Cannot use get_all_custom_device_type because you have installedCPU/GPU version PaddlePaddle.
If you want to use get_all_custom_device_type, please try to install CustomDevice version PaddlePaddle by: pip install paddlepaddle
I0831 09:07:41.211074 27067 imperative.cc:701] Tracer(0x556636b47f70) set expected place Place(cpu)
I0831 09:07:41.211287 27067 eager.cc:112] Tensor(generated_tensor_9) have not GradNode, add GradNodeAccumulation0x55663815ddd0 for it.
I0831 09:07:41.211463 27067 eager.cc:112] Tensor(generated_tensor_10) have not GradNode, add GradNodeAccumulation0x556638159970 for it.
I0831 09:07:41.212214 27067 layout_autotune.cc:79] Layout agnostic_ops: rsqrt
I0831 09:07:41.212239 27067 layout_autotune.cc:79] Layout agnostic_ops: multihead_matmul
I0831 09:07:41.212244 27067 layout_autotune.cc:79] Layout agnostic_ops: sparse_transpose
I0831 09:07:41.212250 27067 layout_autotune.cc:79] Layout agnostic_ops: addmm
I0831 09:07:41.212257 27067 layout_autotune.cc:79] Layout agnostic_ops: gru
I0831 09:07:41.212263 27067 layout_autotune.cc:79] Layout agnostic_ops: round
I0831 09:07:41.212270 27067 layout_autotune.cc:79] Layout agnostic_ops: rank_attention
I0831 09:07:41.212275 27067 layout_autotune.cc:79] Layout agnostic_ops: sqrt_p
I0831 09:07:41.212281 27067 layout_autotune.cc:79] Layout agnostic_ops: flash_attn_unpadded
I0831 09:07:41.212286 27067 layout_autotune.cc:79] Layout agnostic_ops: merge_lod_tensor
I0831 09:07:41.212291 27067 layout_autotune.cc:79] Layout agnostic_ops: c_wait_compute
I0831 09:07:41.212299 27067 layout_autotune.cc:79] Layout agnostic_ops: gen_nccl_id
I0831 09:07:41.212307 27067 layout_autotune.cc:79] Layout agnostic_ops: fused_embedding_fc_lstm
I0831 09:07:41.212312 27067 layout_autotune.cc:79] Layout agnostic_ops: where_index
I0831 09:07:41.212317 27067 layout_autotune.cc:79] Layout agnostic_ops: conv2d_transpose_xpu
I0831 09:07:41.212322 27067 layout_autotune.cc:79] Layout agnostic_ops: queue_generator
I0831 09:07:41.212332 27067 layout_autotune.cc:64] Heavily layout sensitive OP: bicubic_interp
I0831 09:07:41.212340 27067 layout_autotune.cc:79] Layout agnostic_ops: flash_attn
I0831 09:07:41.212347 27067 layout_autotune.cc:54] Lightly layout sensitive OP: arg_min
I0831 09:07:41.212352 27067 layout_autotune.cc:79] Layout agnostic_ops: tile
I0831 09:07:41.212356 27067 layout_autotune.cc:79] Layout agnostic_ops: distributed_fused_lamb_init
I0831 09:07:41.212363 27067 layout_autotune.cc:54] Lightly layout sensitive OP: dequantize_linear
I0831 09:07:41.212368 27067 layout_autotune.cc:79] Layout agnostic_ops: bilinear_tensor_product
I0831 09:07:41.212373 27067 layout_autotune.cc:79] Layout agnostic_ops: ctc_align
I0831 09:07:41.212378 27067 layout_autotune.cc:79] Layout agnostic_ops: pow2_decay_with_linear_warmup
I0831 09:07:41.212383 27067 layout_autotune.cc:54] Lightly layout sensitive OP: reduce_amin
I0831 09:07:41.212390 27067 layout_autotune.cc:54] Lightly layout sensitive OP: split
I0831 09:07:41.212401 27067 layout_autotune.cc:54] Lightly layout sensitive OP: fc
I0831 09:07:41.212407 27067 layout_autotune.cc:79] Layout agnostic_ops: clear_float_status
I0831 09:07:41.212414 27067 layout_autotune.cc:79] Layout agnostic_ops: load
I0831 09:07:41.212419 27067 layout_autotune.cc:79] Layout agnostic_ops: matmul_v2
I0831 09:07:41.212426 27067 layout_autotune.cc:54] Lightly layout sensitive OP: elementwise_max
I0831 09:07:41.212431 27067 layout_autotune.cc:54] Lightly layout sensitive OP: c_embedding
I0831 09:07:41.212445 27067 layout_autotune.cc:79] Layout agnostic_ops: select_p
I0831 09:07:41.212452 27067 layout_autotune.cc:79] Layout agnostic_ops: adadelta
I0831 09:07:41.212456 27067 layout_autotune.cc:79] Layout agnostic_ops: check_finite_and_unscale
I0831 09:07:41.212462 27067 layout_autotune.cc:79] Layout agnostic_ops: chunk_eval
I0831 09:07:41.212468 27067 layout_autotune.cc:54] Lightly layout sensitive OP: sparse_momentum
I0831 09:07:41.212472 27067 layout_autotune.cc:79] Layout agnostic_ops: complex
I0831 09:07:41.212477 27067 layout_autotune.cc:79] Layout agnostic_ops: tan
I0831 09:07:41.212482 27067 layout_autotune.cc:79] Layout agnostic_ops: all_gather
I0831 09:07:41.212496 27067 layout_autotune.cc:79] Layout agnostic_ops: adam
I0831 09:07:41.212499 27067 layout_autotune.cc:79] Layout agnostic_ops: fsp
I0831 09:07:41.212503 27067 layout_autotune.cc:79] Layout agnostic_ops: where
I0831 09:07:41.212508 27067 layout_autotune.cc:79] Layout agnostic_ops: logical_xor
I0831 09:07:41.212514 27067 layout_autotune.cc:79] Layout agnostic_ops: multiclass_nms3
I0831 09:07:41.212522 27067 layout_autotune.cc:79] Layout agnostic_ops: one_hot_v2
I0831 09:07:41.212527 27067 layout_autotune.cc:64] Heavily layout sensitive OP: sequence_softmax
I0831 09:07:41.212532 27067 layout_autotune.cc:64] Heavily layout sensitive OP: affine_channel
I0831 09:07:41.212538 27067 layout_autotune.cc:79] Layout agnostic_ops: triangular_solve
I0831 09:07:41.212543 27067 layout_autotune.cc:79] Layout agnostic_ops: sequence_topk_avg_pooling
I0831 09:07:41.212548 27067 layout_autotune.cc:79] Layout agnostic_ops: space_to_depth
I0831 09:07:41.212551 27067 layout_autotune.cc:79] Layout agnostic_ops: transpose_p
I0831 09:07:41.212555 27067 layout_autotune.cc:79] Layout agnostic_ops: reverse
I0831 09:07:41.212559 27067 layout_autotune.cc:79] Layout agnostic_ops: add_act_xpu
I0831 09:07:41.212568 27067 layout_autotune.cc:79] Layout agnostic_ops: expand_v2
I0831 09:07:41.212574 27067 layout_autotune.cc:79] Layout agnostic_ops: p_send
I0831 09:07:41.212579 27067 layout_autotune.cc:54] Lightly layout sensitive OP: repeat_interleave
I0831 09:07:41.212582 27067 layout_autotune.cc:79] Layout agnostic_ops: lgamma
I0831 09:07:41.212586 27067 layout_autotune.cc:79] Layout agnostic_ops: solve
I0831 09:07:41.212590 27067 layout_autotune.cc:79] Layout agnostic_ops: push_sparse_v2
I0831 09:07:41.212594 27067 layout_autotune.cc:79] Layout agnostic_ops: ge_p
I0831 09:07:41.212599 27067 layout_autotune.cc:79] Layout agnostic_ops: deformable_psroi_pooling
I0831 09:07:41.212602 27067 layout_autotune.cc:79] Layout agnostic_ops: conv1d_xpu
I0831 09:07:41.212606 27067 layout_autotune.cc:79] Layout agnostic_ops: sub_p
I0831 09:07:41.212611 27067 layout_autotune.cc:79] Layout agnostic_ops: transfer_layout
I0831 09:07:41.212615 27067 layout_autotune.cc:32] Already exists in Layout OP: instance_norm
I0831 09:07:41.212620 27067 layout_autotune.cc:79] Layout agnostic_ops: decode_jpeg
I0831 09:07:41.212623 27067 layout_autotune.cc:79] Layout agnostic_ops: gather_nd
I0831 09:07:41.212627 27067 layout_autotune.cc:79] Layout agnostic_ops: variable_length_memory_efficient_attention
I0831 09:07:41.212635 27067 layout_autotune.cc:54] Lightly layout sensitive OP: reduce_prod
I0831 09:07:41.212641 27067 layout_autotune.cc:54] Lightly layout sensitive OP: cummin
I0831 09:07:41.212646 27067 layout_autotune.cc:79] Layout agnostic_ops: matrix_rank
I0831 09:07:41.212651 27067 layout_autotune.cc:79] Layout agnostic_ops: asin
I0831 09:07:41.212656 27067 layout_autotune.cc:79] Layout agnostic_ops: max_p
I0831 09:07:41.212662 27067 layout_autotune.cc:79] Layout agnostic_ops: fused_softplus
I0831 09:07:41.212672 27067 layout_autotune.cc:79] Layout agnostic_ops: lstmp
I0831 09:07:41.212677 27067 layout_autotune.cc:79] Layout agnostic_ops: iou_similarity
I0831 09:07:41.212682 27067 layout_autotune.cc:79] Layout agnostic_ops: c_wait_comm
I0831 09:07:41.212687 27067 layout_autotune.cc:79] Layout agnostic_ops: huber_loss
I0831 09:07:41.212692 27067 layout_autotune.cc:79] Layout agnostic_ops: one_hot
I0831 09:07:41.212699 27067 layout_autotune.cc:79] Layout agnostic_ops: sequence_slice
I0831 09:07:41.212703 27067 layout_autotune.cc:79] Layout agnostic_ops: sparse_leaky_relu
I0831 09:07:41.212715 27067 layout_autotune.cc:79] Layout agnostic_ops: lookup_table
I0831 09:07:41.212720 27067 layout_autotune.cc:79] Layout agnostic_ops: erf_p
I0831 09:07:41.212725 27067 layout_autotune.cc:79] Layout agnostic_ops: softplus
I0831 09:07:41.212742 27067 layout_autotune.cc:64] Heavily layout sensitive OP: depthwise_conv2d
I0831 09:07:41.212754 27067 layout_autotune.cc:79] Layout agnostic_ops: sparse_cast
I0831 09:07:41.212759 27067 layout_autotune.cc:79] Layout agnostic_ops: c_allreduce_sum
I0831 09:07:41.212764 27067 layout_autotune.cc:54] Lightly layout sensitive OP: fused_fc_elementwise_layernorm
I0831 09:07:41.212771 27067 layout_autotune.cc:79] Layout agnostic_ops: exp
I0831 09:07:41.212776 27067 layout_autotune.cc:79] Layout agnostic_ops: sigmoid_cross_entropy_with_logits
I0831 09:07:41.212781 27067 layout_autotune.cc:79] Layout agnostic_ops: scatter
I0831 09:07:41.212787 27067 layout_autotune.cc:79] Layout agnostic_ops: self_dp_attention
I0831 09:07:41.212792 27067 layout_autotune.cc:79] Layout agnostic_ops: index_put
I0831 09:07:41.212798 27067 layout_autotune.cc:79] Layout agnostic_ops: c_allreduce_min
I0831 09:07:41.212805 27067 layout_autotune.cc:79] Layout agnostic_ops: equal_all
I0831 09:07:41.212810 27067 layout_autotune.cc:79] Layout agnostic_ops: searchsorted
I0831 09:07:41.212814 27067 layout_autotune.cc:79] Layout agnostic_ops: fusion_squared_mat_sub
I0831 09:07:41.212821 27067 layout_autotune.cc:54] Lightly layout sensitive OP: unique
I0831 09:07:41.212826 27067 layout_autotune.cc:79] Layout agnostic_ops: log
I0831 09:07:41.212832 27067 layout_autotune.cc:79] Layout agnostic_ops: conv_shift
I0831 09:07:41.212836 27067 layout_autotune.cc:79] Layout agnostic_ops: as_complex
I0831 09:07:41.212841 27067 layout_autotune.cc:79] Layout agnostic_ops: smooth_l1_loss
I0831 09:07:41.212849 27067 layout_autotune.cc:64] Heavily layout sensitive OP: linear_interp_v2
I0831 09:07:41.212857 27067 layout_autotune.cc:79] Layout agnostic_ops: momentum
I0831 09:07:41.212860 27067 layout_autotune.cc:64] Heavily layout sensitive OP: temporal_shift
I0831 09:07:41.212867 27067 layout_autotune.cc:79] Layout agnostic_ops: recurrent
I0831 09:07:41.212875 27067 layout_autotune.cc:79] Layout agnostic_ops: nce
I0831 09:07:41.212879 27067 layout_autotune.cc:79] Layout agnostic_ops: mv
I0831 09:07:41.212885 27067 layout_autotune.cc:79] Layout agnostic_ops: global_scatter
I0831 09:07:41.212889 27067 layout_autotune.cc:79] Layout agnostic_ops: delete_var
I0831 09:07:41.212895 27067 layout_autotune.cc:54] Lightly layout sensitive OP: dropout_nd
I0831 09:07:41.212903 27067 layout_autotune.cc:79] Layout agnostic_ops: proximal_gd
I0831 09:07:41.212906 27067 layout_autotune.cc:79] Layout agnostic_ops: memcpy_h2d
I0831 09:07:41.212911 27067 layout_autotune.cc:79] Layout agnostic_ops: add_position_encoding
I0831 09:07:41.212916 27067 layout_autotune.cc:79] Layout agnostic_ops: cosh
I0831 09:07:41.212924 27067 layout_autotune.cc:79] Layout agnostic_ops: hash
I0831 09:07:41.212927 27067 layout_autotune.cc:79] Layout agnostic_ops: sparse_full_like
I0831 09:07:41.212934 27067 layout_autotune.cc:54] Lightly layout sensitive OP: grad_add
I0831 09:07:41.212941 27067 layout_autotune.cc:64] Heavily layout sensitive OP: prelu
I0831 09:07:41.212946 27067 layout_autotune.cc:79] Layout agnostic_ops: sign
I0831 09:07:41.212949 27067 layout_autotune.cc:79] Layout agnostic_ops: linspace
I0831 09:07:41.212955 27067 layout_autotune.cc:79] Layout agnostic_ops: fill_diagonal
I0831 09:07:41.212960 27067 layout_autotune.cc:79] Layout agnostic_ops: sparse_multiply
I0831 09:07:41.212963 27067 layout_autotune.cc:79] Layout agnostic_ops: sparse_atan
I0831 09:07:41.212968 27067 layout_autotune.cc:79] Layout agnostic_ops: logsigmoid
I0831 09:07:41.212975 27067 layout_autotune.cc:79] Layout agnostic_ops: load_combine
I0831 09:07:41.212980 27067 layout_autotune.cc:79] Layout agnostic_ops: fetch_v2
I0831 09:07:41.212989 27067 layout_autotune.cc:79] Layout agnostic_ops: randperm
I0831 09:07:41.212993 27067 layout_autotune.cc:79] Layout agnostic_ops: sequence_scatter
I0831 09:07:41.212997 27067 layout_autotune.cc:79] Layout agnostic_ops: relu6
I0831 09:07:41.213001 27067 layout_autotune.cc:79] Layout agnostic_ops: select_output
I0831 09:07:41.213006 27067 layout_autotune.cc:54] Lightly layout sensitive OP: partial_sum
I0831 09:07:41.213011 27067 layout_autotune.cc:79] Layout agnostic_ops: partial_allgather
I0831 09:07:41.213016 27067 layout_autotune.cc:79] Layout agnostic_ops: abs_p
I0831 09:07:41.213022 27067 layout_autotune.cc:79] Layout agnostic_ops: pylayer
I0831 09:07:41.213027 27067 layout_autotune.cc:79] Layout agnostic_ops: sparse_sinh
I0831 09:07:41.213030 27067 layout_autotune.cc:79] Layout agnostic_ops: broadcast_p
I0831 09:07:41.213035 27067 layout_autotune.cc:79] Layout agnostic_ops: c_scatter
I0831 09:07:41.213040 27067 layout_autotune.cc:79] Layout agnostic_ops: sparse_add
I0831 09:07:41.213053 27067 layout_autotune.cc:64] Heavily layout sensitive OP: fused_conv2d
I0831 09:07:41.213059 27067 layout_autotune.cc:79] Layout agnostic_ops: alltoall
I0831 09:07:41.213073 27067 layout_autotune.cc:64] Heavily layout sensitive OP: conv3d
I0831 09:07:41.213078 27067 layout_autotune.cc:54] Lightly layout sensitive OP: cummax
I0831 09:07:41.213084 27067 layout_autotune.cc:79] Layout agnostic_ops: lu_unpack
I0831 09:07:41.213088 27067 layout_autotune.cc:79] Layout agnostic_ops: lstm_unit
I0831 09:07:41.213093 27067 layout_autotune.cc:54] Lightly layout sensitive OP: not_equal
I0831 09:07:41.213099 27067 layout_autotune.cc:32] Already exists in Layout OP: transpose2
I0831 09:07:41.213102 27067 layout_autotune.cc:79] Layout agnostic_ops: sparse_coalesce
I0831 09:07:41.213106 27067 layout_autotune.cc:79] Layout agnostic_ops: c_sync_comm_stream
I0831 09:07:41.213114 27067 layout_autotune.cc:54] Lightly layout sensitive OP: uniform_random_batch_size_like
I0831 09:07:41.213119 27067 layout_autotune.cc:79] Layout agnostic_ops: yolo_box_head
I0831 09:07:41.213124 27067 layout_autotune.cc:79] Layout agnostic_ops: unfold
I0831 09:07:41.213130 27067 layout_autotune.cc:79] Layout agnostic_ops: while
I0831 09:07:41.213136 27067 layout_autotune.cc:64] Heavily layout sensitive OP: lrn
I0831 09:07:41.213143 27067 layout_autotune.cc:79] Layout agnostic_ops: isclose
I0831 09:07:41.213148 27067 layout_autotune.cc:54] Lightly layout sensitive OP: softmax_with_cross_entropy
I0831 09:07:41.213153 27067 layout_autotune.cc:79] Layout agnostic_ops: isfinite_v2
I0831 09:07:41.213156 27067 layout_autotune.cc:79] Layout agnostic_ops: bernoulli
I0831 09:07:41.213162 27067 layout_autotune.cc:79] Layout agnostic_ops: max_pool3d_with_index
I0831 09:07:41.213177 27067 layout_autotune.cc:79] Layout agnostic_ops: create_py_reader
I0831 09:07:41.213182 27067 layout_autotune.cc:79] Layout agnostic_ops: p_recv
I0831 09:07:41.213189 27067 layout_autotune.cc:79] Layout agnostic_ops: fused_seqpool_cvm
I0831 09:07:41.213196 27067 layout_autotune.cc:79] Layout agnostic_ops: gaussian_random
I0831 09:07:41.213200 27067 layout_autotune.cc:54] Lightly layout sensitive OP: flatten2
I0831 09:07:41.213209 27067 layout_autotune.cc:79] Layout agnostic_ops: matmul
I0831 09:07:41.213214 27067 layout_autotune.cc:79] Layout agnostic_ops: cvm
I0831 09:07:41.213220 27067 layout_autotune.cc:79] Layout agnostic_ops: adamax
I0831 09:07:41.213227 27067 layout_autotune.cc:79] Layout agnostic_ops: recv_v2
I0831 09:07:41.213232 27067 layout_autotune.cc:79] Layout agnostic_ops: requantize
I0831 09:07:41.213235 27067 layout_autotune.cc:79] Layout agnostic_ops: push_gpups_sparse
I0831 09:07:41.213240 27067 layout_autotune.cc:79] Layout agnostic_ops: masked_select
I0831 09:07:41.213244 27067 layout_autotune.cc:79] Layout agnostic_ops: range
I0831 09:07:41.213249 27067 layout_autotune.cc:79] Layout agnostic_ops: bitwise_not
I0831 09:07:41.213253 27067 layout_autotune.cc:79] Layout agnostic_ops: sparse_atanh
I0831 09:07:41.213258 27067 layout_autotune.cc:54] Lightly layout sensitive OP: trace
I0831 09:07:41.213266 27067 layout_autotune.cc:79] Layout agnostic_ops: multinomial
I0831 09:07:41.213270 27067 layout_autotune.cc:79] Layout agnostic_ops: read_from_array
I0831 09:07:41.213277 27067 layout_autotune.cc:54] Lightly layout sensitive OP: fused_elementwise_sub
I0831 09:07:41.213282 27067 layout_autotune.cc:79] Layout agnostic_ops: sparse_conv3d
I0831 09:07:41.213287 27067 layout_autotune.cc:79] Layout agnostic_ops: modified_huber_loss
I0831 09:07:41.213291 27067 layout_autotune.cc:79] Layout agnostic_ops: exp_p
I0831 09:07:41.213296 27067 layout_autotune.cc:79] Layout agnostic_ops: c_reduce_prod
I0831 09:07:41.213301 27067 layout_autotune.cc:54] Lightly layout sensitive OP: roll
I0831 09:07:41.213305 27067 layout_autotune.cc:79] Layout agnostic_ops: squared_l2_distance
I0831 09:07:41.213310 27067 layout_autotune.cc:79] Layout agnostic_ops: slice_select_p
I0831 09:07:41.213320 27067 layout_autotune.cc:64] Heavily layout sensitive OP: conv3d_transpose
I0831 09:07:41.213325 27067 layout_autotune.cc:79] Layout agnostic_ops: sparse_addmm
I0831 09:07:41.213330 27067 layout_autotune.cc:79] Layout agnostic_ops: sparse_divide_scalar
I0831 09:07:41.213333 27067 layout_autotune.cc:79] Layout agnostic_ops: share_data
I0831 09:07:41.213338 27067 layout_autotune.cc:79] Layout agnostic_ops: fake_quantize_abs_max
I0831 09:07:41.213343 27067 layout_autotune.cc:79] Layout agnostic_ops: rrelu
I0831 09:07:41.213348 27067 layout_autotune.cc:79] Layout agnostic_ops: unique_with_counts
I0831 09:07:41.213353 27067 layout_autotune.cc:54] Lightly layout sensitive OP: concat
I0831 09:07:41.213359 27067 layout_autotune.cc:79] Layout agnostic_ops: fill
I0831 09:07:41.213362 27067 layout_autotune.cc:79] Layout agnostic_ops: slice_assign_p
I0831 09:07:41.213367 27067 layout_autotune.cc:79] Layout agnostic_ops: reduce_scatter
I0831 09:07:41.213371 27067 layout_autotune.cc:79] Layout agnostic_ops: fill_zeros_like
I0831 09:07:41.213375 27067 layout_autotune.cc:79] Layout agnostic_ops: i1e
I0831 09:07:41.213382 27067 layout_autotune.cc:79] Layout agnostic_ops: hierarchical_sigmoid
I0831 09:07:41.213387 27067 layout_autotune.cc:79] Layout agnostic_ops: c_comm_init_multitrainer
I0831 09:07:41.213392 27067 layout_autotune.cc:79] Layout agnostic_ops: isinf_v2
I0831 09:07:41.213402 27067 layout_autotune.cc:79] Layout agnostic_ops: masked_multihead_attention
I0831 09:07:41.213405 27067 layout_autotune.cc:54] Lightly layout sensitive OP: squeeze
I0831 09:07:41.213410 27067 layout_autotune.cc:79] Layout agnostic_ops: sparse_slice
I0831 09:07:41.213420 27067 layout_autotune.cc:79] Layout agnostic_ops: multiclass_nms2
I0831 09:07:41.213424 27067 layout_autotune.cc:79] Layout agnostic_ops: bpr_loss
I0831 09:07:41.213428 27067 layout_autotune.cc:79] Layout agnostic_ops: fft_c2c
I0831 09:07:41.213436 27067 layout_autotune.cc:64] Heavily layout sensitive OP: bicubic_interp_v2
I0831 09:07:41.213440 27067 layout_autotune.cc:79] Layout agnostic_ops: angle
I0831 09:07:41.213444 27067 layout_autotune.cc:79] Layout agnostic_ops: sparse_sqrt
I0831 09:07:41.213450 27067 layout_autotune.cc:79] Layout agnostic_ops: reshape
I0831 09:07:41.213459 27067 layout_autotune.cc:79] Layout agnostic_ops: coalesce_tensor
I0831 09:07:41.213464 27067 layout_autotune.cc:79] Layout agnostic_ops: matmul_p
I0831 09:07:41.213469 27067 layout_autotune.cc:79] Layout agnostic_ops: push_box_sparse
I0831 09:07:41.213475 27067 layout_autotune.cc:79] Layout agnostic_ops: roi_align
I0831 09:07:41.213479 27067 layout_autotune.cc:32] Already exists in Layout OP: reshape2
I0831 09:07:41.213485 27067 layout_autotune.cc:54] Lightly layout sensitive OP: reduce_any
I0831 09:07:41.213488 27067 layout_autotune.cc:79] Layout agnostic_ops: limit_by_capacity
I0831 09:07:41.213493 27067 layout_autotune.cc:54] Lightly layout sensitive OP: unstack
I0831 09:07:41.213498 27067 layout_autotune.cc:79] Layout agnostic_ops: scatter_nd_add
I0831 09:07:41.213503 27067 layout_autotune.cc:79] Layout agnostic_ops: sequence_reshape
I0831 09:07:41.213507 27067 layout_autotune.cc:79] Layout agnostic_ops: fill_constant_p
I0831 09:07:41.213515 27067 layout_autotune.cc:79] Layout agnostic_ops: bilateral_slice
I0831 09:07:41.213523 27067 layout_autotune.cc:79] Layout agnostic_ops: print
I0831 09:07:41.213529 27067 layout_autotune.cc:79] Layout agnostic_ops: fill_any_like
I0831 09:07:41.213536 27067 layout_autotune.cc:79] Layout agnostic_ops: partial_recv
I0831 09:07:41.213541 27067 layout_autotune.cc:79] Layout agnostic_ops: empty
I0831 09:07:41.213544 27067 layout_autotune.cc:79] Layout agnostic_ops: add_p
I0831 09:07:41.213549 27067 layout_autotune.cc:79] Layout agnostic_ops: pad_constant_like
I0831 09:07:41.213558 27067 layout_autotune.cc:64] Heavily layout sensitive OP: pool2d
I0831 09:07:41.213563 27067 layout_autotune.cc:79] Layout agnostic_ops: size
I0831 09:07:41.213567 27067 layout_autotune.cc:79] Layout agnostic_ops: imag
I0831 09:07:41.213573 27067 layout_autotune.cc:79] Layout agnostic_ops: pull_gpups_sparse
I0831 09:07:41.213577 27067 layout_autotune.cc:79] Layout agnostic_ops: eigh
I0831 09:07:41.213583 27067 layout_autotune.cc:79] Layout agnostic_ops: sparse_subtract
I0831 09:07:41.213587 27067 layout_autotune.cc:54] Lightly layout sensitive OP: stack
I0831 09:07:41.213596 27067 layout_autotune.cc:79] Layout agnostic_ops: dgc_momentum
I0831 09:07:41.213603 27067 layout_autotune.cc:79] Layout agnostic_ops: lamb
I0831 09:07:41.213609 27067 layout_autotune.cc:79] Layout agnostic_ops: generate_proposals_v2
I0831 09:07:41.213613 27067 layout_autotune.cc:79] Layout agnostic_ops: c_sync_calc_stream
I0831 09:07:41.213618 27067 layout_autotune.cc:79] Layout agnostic_ops: push_sparse
I0831 09:07:41.213621 27067 layout_autotune.cc:79] Layout agnostic_ops: get_places
I0831 09:07:41.213625 27067 layout_autotune.cc:79] Layout agnostic_ops: bitwise_or
I0831 09:07:41.213632 27067 layout_autotune.cc:79] Layout agnostic_ops: gru_unit
I0831 09:07:41.213637 27067 layout_autotune.cc:79] Layout agnostic_ops: push_box_extended_sparse
I0831 09:07:41.213641 27067 layout_autotune.cc:79] Layout agnostic_ops: generate_sequence_xpu
I0831 09:07:41.213645 27067 layout_autotune.cc:54] Lightly layout sensitive OP: reduce_sum_p
I0831 09:07:41.213651 27067 layout_autotune.cc:54] Lightly layout sensitive OP: fake_channel_wise_quantize_dequantize_abs_max
I0831 09:07:41.213657 27067 layout_autotune.cc:79] Layout agnostic_ops: sampling_id
I0831 09:07:41.213661 27067 layout_autotune.cc:54] Lightly layout sensitive OP: unsqueeze2
I0831 09:07:41.213665 27067 layout_autotune.cc:79] Layout agnostic_ops: sparse_relu6
I0831 09:07:41.213670 27067 layout_autotune.cc:79] Layout agnostic_ops: transfer_dtype
I0831 09:07:41.213676 27067 layout_autotune.cc:79] Layout agnostic_ops: create_double_buffer_reader
I0831 09:07:41.213680 27067 layout_autotune.cc:79] Layout agnostic_ops: polygamma
I0831 09:07:41.213685 27067 layout_autotune.cc:79] Layout agnostic_ops: average_accumulates
I0831 09:07:41.213690 27067 layout_autotune.cc:79] Layout agnostic_ops: ne_p
I0831 09:07:41.213696 27067 layout_autotune.cc:79] Layout agnostic_ops: sequence_enumerate
I0831 09:07:41.213701 27067 layout_autotune.cc:79] Layout agnostic_ops: fusion_seqconv_eltadd_relu
I0831 09:07:41.213706 27067 layout_autotune.cc:79] Layout agnostic_ops: bce_loss
I0831 09:07:41.213711 27067 layout_autotune.cc:79] Layout agnostic_ops: generate_proposal_labels
I0831 09:07:41.213716 27067 layout_autotune.cc:79] Layout agnostic_ops: im2sequence
I0831 09:07:41.213721 27067 layout_autotune.cc:79] Layout agnostic_ops: isinf
I0831 09:07:41.213725 27067 layout_autotune.cc:79] Layout agnostic_ops: c_reducescatter
I0831 09:07:41.213733 27067 layout_autotune.cc:54] Lightly layout sensitive OP: logcumsumexp
I0831 09:07:41.213739 27067 layout_autotune.cc:79] Layout agnostic_ops: check_numerics
I0831 09:07:41.213743 27067 layout_autotune.cc:79] Layout agnostic_ops: write_to_array
I0831 09:07:41.213749 27067 layout_autotune.cc:79] Layout agnostic_ops: adagrad
I0831 09:07:41.213753 27067 layout_autotune.cc:79] Layout agnostic_ops: linear_chain_crf
I0831 09:07:41.213759 27067 layout_autotune.cc:79] Layout agnostic_ops: retinanet_target_assign
I0831 09:07:41.213768 27067 layout_autotune.cc:79] Layout agnostic_ops: fusion_group
I0831 09:07:41.213773 27067 layout_autotune.cc:79] Layout agnostic_ops: gather_p
I0831 09:07:41.213780 27067 layout_autotune.cc:79] Layout agnostic_ops: teacher_student_sigmoid_loss
I0831 09:07:41.213785 27067 layout_autotune.cc:79] Layout agnostic_ops: yolo_box_xpu
I0831 09:07:41.213789 27067 layout_autotune.cc:54] Lightly layout sensitive OP: random_crop
I0831 09:07:41.213797 27067 layout_autotune.cc:79] Layout agnostic_ops: lookup_table_v2
I0831 09:07:41.213805 27067 layout_autotune.cc:79] Layout agnostic_ops: elementwise_fmax
I0831 09:07:41.213809 27067 layout_autotune.cc:54] Lightly layout sensitive OP: as_strided
I0831 09:07:41.213814 27067 layout_autotune.cc:54] Lightly layout sensitive OP: index_add
I0831 09:07:41.213819 27067 layout_autotune.cc:79] Layout agnostic_ops: graph_sample_neighbors
I0831 09:07:41.213826 27067 layout_autotune.cc:79] Layout agnostic_ops: detection_map
I0831 09:07:41.213835 27067 layout_autotune.cc:79] Layout agnostic_ops: sqrt
I0831 09:07:41.213840 27067 layout_autotune.cc:79] Layout agnostic_ops: partial_send
I0831 09:07:41.213846 27067 layout_autotune.cc:54] Lightly layout sensitive OP: fused_elemwise_activation
I0831 09:07:41.213851 27067 layout_autotune.cc:79] Layout agnostic_ops: lod_rank_table
I0831 09:07:41.213856 27067 layout_autotune.cc:79] Layout agnostic_ops: all_to_all
I0831 09:07:41.213860 27067 layout_autotune.cc:79] Layout agnostic_ops: slogdeterminant
I0831 09:07:41.213865 27067 layout_autotune.cc:54] Lightly layout sensitive OP: share_buffer
I0831 09:07:41.213869 27067 layout_autotune.cc:79] Layout agnostic_ops: poisson
I0831 09:07:41.213881 27067 layout_autotune.cc:79] Layout agnostic_ops: fused_matmul
I0831 09:07:41.213884 27067 layout_autotune.cc:79] Layout agnostic_ops: bitwise_and
I0831 09:07:41.213891 27067 layout_autotune.cc:79] Layout agnostic_ops: c_comm_init_all
I0831 09:07:41.213896 27067 layout_autotune.cc:54] Lightly layout sensitive OP: diag_embed
I0831 09:07:41.213899 27067 layout_autotune.cc:79] Layout agnostic_ops: check_memory_continue
I0831 09:07:41.213903 27067 layout_autotune.cc:79] Layout agnostic_ops: sparse_masked_matmul
I0831 09:07:41.213907 27067 layout_autotune.cc:79] Layout agnostic_ops: data
I0831 09:07:41.213912 27067 layout_autotune.cc:79] Layout agnostic_ops: eq_p
I0831 09:07:41.213917 27067 layout_autotune.cc:54] Lightly layout sensitive OP: unbind
I0831 09:07:41.213923 27067 layout_autotune.cc:79] Layout agnostic_ops: dropout
I0831 09:07:41.213928 27067 layout_autotune.cc:79] Layout agnostic_ops: beam_search
I0831 09:07:41.213932 27067 layout_autotune.cc:79] Layout agnostic_ops: moving_average_abs_max_scale
I0831 09:07:41.213936 27067 layout_autotune.cc:54] Lightly layout sensitive OP: view_shape
I0831 09:07:41.213941 27067 layout_autotune.cc:54] Lightly layout sensitive OP: greater_than
I0831 09:07:41.213944 27067 layout_autotune.cc:79] Layout agnostic_ops: log_loss
I0831 09:07:41.213949 27067 layout_autotune.cc:79] Layout agnostic_ops: kron
I0831 09:07:41.213954 27067 layout_autotune.cc:79] Layout agnostic_ops: sigmoid_focal_loss
I0831 09:07:41.213961 27067 layout_autotune.cc:79] Layout agnostic_ops: rmsprop
I0831 09:07:41.213970 27067 layout_autotune.cc:64] Heavily layout sensitive OP: conv2d
I0831 09:07:41.213975 27067 layout_autotune.cc:79] Layout agnostic_ops: graph_reindex
I0831 09:07:41.213982 27067 layout_autotune.cc:79] Layout agnostic_ops: uniform_random_inplace
I0831 09:07:41.213985 27067 layout_autotune.cc:54] Lightly layout sensitive OP: maxout
I0831 09:07:41.213990 27067 layout_autotune.cc:79] Layout agnostic_ops: lstsq
I0831 09:07:41.213997 27067 layout_autotune.cc:64] Heavily layout sensitive OP: linear_interp
I0831 09:07:41.214004 27067 layout_autotune.cc:79] Layout agnostic_ops: graph_khop_sampler
I0831 09:07:41.214010 27067 layout_autotune.cc:79] Layout agnostic_ops: put_along_axis
I0831 09:07:41.214015 27067 layout_autotune.cc:79] Layout agnostic_ops: auc
I0831 09:07:41.214020 27067 layout_autotune.cc:79] Layout agnostic_ops: logical_or
I0831 09:07:41.214027 27067 layout_autotune.cc:79] Layout agnostic_ops: sparse_to_sparse_coo
I0831 09:07:41.214030 27067 layout_autotune.cc:32] Already exists in Layout OP: batch_norm
I0831 09:07:41.214035 27067 layout_autotune.cc:79] Layout agnostic_ops: c_reduce_sum
I0831 09:07:41.214042 27067 layout_autotune.cc:54] Lightly layout sensitive OP: elementwise_add
I0831 09:07:41.214046 27067 layout_autotune.cc:79] Layout agnostic_ops: acos
I0831 09:07:41.214052 27067 layout_autotune.cc:64] Heavily layout sensitive OP: unpool
I0831 09:07:41.214056 27067 layout_autotune.cc:79] Layout agnostic_ops: sparse_divide
I0831 09:07:41.214061 27067 layout_autotune.cc:79] Layout agnostic_ops: cumprod
I0831 09:07:41.214066 27067 layout_autotune.cc:79] Layout agnostic_ops: sample_logits
I0831 09:07:41.214071 27067 layout_autotune.cc:54] Lightly layout sensitive OP: pull_box_extended_sparse
I0831 09:07:41.214076 27067 layout_autotune.cc:79] Layout agnostic_ops: sparse_sparse_coo_tensor
I0831 09:07:41.214080 27067 layout_autotune.cc:79] Layout agnostic_ops: sparse_matmul
I0831 09:07:41.214085 27067 layout_autotune.cc:79] Layout agnostic_ops: crop_tensor
I0831 09:07:41.214092 27067 layout_autotune.cc:79] Layout agnostic_ops: deformable_conv
I0831 09:07:41.214099 27067 layout_autotune.cc:79] Layout agnostic_ops: fill_constant
I0831 09:07:41.214103 27067 layout_autotune.cc:79] Layout agnostic_ops: generate_mask_labels
I0831 09:07:41.214109 27067 layout_autotune.cc:79] Layout agnostic_ops: locality_aware_nms
I0831 09:07:41.214114 27067 layout_autotune.cc:79] Layout agnostic_ops: expand_as
I0831 09:07:41.214118 27067 layout_autotune.cc:79] Layout agnostic_ops: matrix_power
I0831 09:07:41.214123 27067 layout_autotune.cc:79] Layout agnostic_ops: lod_array_length
I0831 09:07:41.214128 27067 layout_autotune.cc:54] Lightly layout sensitive OP: greater_equal
I0831 09:07:41.214133 27067 layout_autotune.cc:79] Layout agnostic_ops: generate_proposals
I0831 09:07:41.214141 27067 layout_autotune.cc:64] Heavily layout sensitive OP: conv2d_fusion_cutlass
I0831 09:07:41.214146 27067 layout_autotune.cc:79] Layout agnostic_ops: number_count
I0831 09:07:41.214154 27067 layout_autotune.cc:64] Heavily layout sensitive OP: bilinear_interp
I0831 09:07:41.214159 27067 layout_autotune.cc:79] Layout agnostic_ops: conv2d_xpu
I0831 09:07:41.214172 27067 layout_autotune.cc:79] Layout agnostic_ops: distributed_fused_lamb
I0831 09:07:41.214179 27067 layout_autotune.cc:79] Layout agnostic_ops: sigmoid
I0831 09:07:41.214184 27067 layout_autotune.cc:54] Lightly layout sensitive OP: c_gen_xccl_id
I0831 09:07:41.214192 27067 layout_autotune.cc:64] Heavily layout sensitive OP: inplace_abn
I0831 09:07:41.214197 27067 layout_autotune.cc:79] Layout agnostic_ops: softshrink
I0831 09:07:41.214206 27067 layout_autotune.cc:54] Lightly layout sensitive OP: mul
I0831 09:07:41.214213 27067 layout_autotune.cc:54] Lightly layout sensitive OP: data_norm
I0831 09:07:41.214218 27067 layout_autotune.cc:79] Layout agnostic_ops: sparse_maxpool
I0831 09:07:41.214226 27067 layout_autotune.cc:54] Lightly layout sensitive OP: fused_multi_transformer
I0831 09:07:41.214231 27067 layout_autotune.cc:79] Layout agnostic_ops: asinh
I0831 09:07:41.214236 27067 layout_autotune.cc:79] Layout agnostic_ops: reorder_lod_tensor_by_rank
I0831 09:07:41.214241 27067 layout_autotune.cc:79] Layout agnostic_ops: get_tensor_from_selected_rows
I0831 09:07:41.214246 27067 layout_autotune.cc:79] Layout agnostic_ops: spp
I0831 09:07:41.214251 27067 layout_autotune.cc:79] Layout agnostic_ops: floor
I0831 09:07:41.214254 27067 layout_autotune.cc:79] Layout agnostic_ops: as_real
I0831 09:07:41.214260 27067 layout_autotune.cc:79] Layout agnostic_ops: gelu
I0831 09:07:41.214265 27067 layout_autotune.cc:79] Layout agnostic_ops: retinanet_detection_output
I0831 09:07:41.214269 27067 layout_autotune.cc:79] Layout agnostic_ops: minus
I0831 09:07:41.214274 27067 layout_autotune.cc:79] Layout agnostic_ops: push_dense
I0831 09:07:41.214279 27067 layout_autotune.cc:79] Layout agnostic_ops: silu
I0831 09:07:41.214293 27067 layout_autotune.cc:79] Layout agnostic_ops: enqueue
I0831 09:07:41.214296 27067 layout_autotune.cc:79] Layout agnostic_ops: sequence_erase
I0831 09:07:41.214300 27067 layout_autotune.cc:79] Layout agnostic_ops: unzip
I0831 09:07:41.214305 27067 layout_autotune.cc:79] Layout agnostic_ops: real
I0831 09:07:41.214311 27067 layout_autotune.cc:64] Heavily layout sensitive OP: nearest_interp_v2
I0831 09:07:41.214316 27067 layout_autotune.cc:79] Layout agnostic_ops: dequeue
I0831 09:07:41.214321 27067 layout_autotune.cc:79] Layout agnostic_ops: copy_cross_scope
I0831 09:07:41.214329 27067 layout_autotune.cc:54] Lightly layout sensitive OP: squeeze2
I0831 09:07:41.214335 27067 layout_autotune.cc:79] Layout agnostic_ops: moe
I0831 09:07:41.214339 27067 layout_autotune.cc:79] Layout agnostic_ops: conj
I0831 09:07:41.214344 27067 layout_autotune.cc:54] Lightly layout sensitive OP: strided_slice
I0831 09:07:41.214349 27067 layout_autotune.cc:79] Layout agnostic_ops: full_int_array
I0831 09:07:41.214352 27067 layout_autotune.cc:79] Layout agnostic_ops: sparse_sin
I0831 09:07:41.214357 27067 layout_autotune.cc:79] Layout agnostic_ops: precision_recall
I0831 09:07:41.214361 27067 layout_autotune.cc:79] Layout agnostic_ops: memory_efficient_attention
I0831 09:07:41.214366 27067 layout_autotune.cc:79] Layout agnostic_ops: fusion_seqexpand_concat_fc
I0831 09:07:41.214371 27067 layout_autotune.cc:79] Layout agnostic_ops: save
I0831 09:07:41.214375 27067 layout_autotune.cc:79] Layout agnostic_ops: gt_p
I0831 09:07:41.214387 27067 layout_autotune.cc:64] Heavily layout sensitive OP: depthwise_conv2d_transpose
I0831 09:07:41.214395 27067 layout_autotune.cc:79] Layout agnostic_ops: fake_quantize_range_abs_max
I0831 09:07:41.214399 27067 layout_autotune.cc:79] Layout agnostic_ops: positive_negative_pair
I0831 09:07:41.214406 27067 layout_autotune.cc:79] Layout agnostic_ops: square
I0831 09:07:41.214412 27067 layout_autotune.cc:79] Layout agnostic_ops: var_conv_2d
I0831 09:07:41.214417 27067 layout_autotune.cc:79] Layout agnostic_ops: log1p
I0831 09:07:41.214421 27067 layout_autotune.cc:64] Heavily layout sensitive OP: channel_shuffle
I0831 09:07:41.214426 27067 layout_autotune.cc:79] Layout agnostic_ops: sparse_indices
I0831 09:07:41.214432 27067 layout_autotune.cc:79] Layout agnostic_ops: atan2
I0831 09:07:41.214437 27067 layout_autotune.cc:79] Layout agnostic_ops: fused_softmax_mask_upper_triangle
I0831 09:07:41.214442 27067 layout_autotune.cc:79] Layout agnostic_ops: box_decoder_and_assign
I0831 09:07:41.214449 27067 layout_autotune.cc:79] Layout agnostic_ops: roi_pool
I0831 09:07:41.214452 27067 layout_autotune.cc:79] Layout agnostic_ops: fft_r2c
I0831 09:07:41.214457 27067 layout_autotune.cc:54] Lightly layout sensitive OP: overlap_add
I0831 09:07:41.214462 27067 layout_autotune.cc:54] Lightly layout sensitive OP: fill_constant_batch_size_like
I0831 09:07:41.214468 27067 layout_autotune.cc:79] Layout agnostic_ops: fill_any
I0831 09:07:41.214473 27067 layout_autotune.cc:79] Layout agnostic_ops: dequantize_log
I0831 09:07:41.214479 27067 layout_autotune.cc:79] Layout agnostic_ops: c_split
I0831 09:07:41.214483 27067 layout_autotune.cc:79] Layout agnostic_ops: log_p
I0831 09:07:41.214488 27067 layout_autotune.cc:79] Layout agnostic_ops: barrier
I0831 09:07:41.214494 27067 layout_autotune.cc:79] Layout agnostic_ops: max_pool2d_with_index
I0831 09:07:41.214499 27067 layout_autotune.cc:64] Heavily layout sensitive OP: pad3d
I0831 09:07:41.214506 27067 layout_autotune.cc:79] Layout agnostic_ops: viterbi_decode
I0831 09:07:41.214512 27067 layout_autotune.cc:79] Layout agnostic_ops: mish
I0831 09:07:41.214517 27067 layout_autotune.cc:54] Lightly layout sensitive OP: box_coder
I0831 09:07:41.214522 27067 layout_autotune.cc:54] Lightly layout sensitive OP: flatten
I0831 09:07:41.214529 27067 layout_autotune.cc:54] Lightly layout sensitive OP: elementwise_mod
I0831 09:07:41.214536 27067 layout_autotune.cc:79] Layout agnostic_ops: margin_cross_entropy
I0831 09:07:41.214543 27067 layout_autotune.cc:79] Layout agnostic_ops: pull_sparse
I0831 09:07:41.214553 27067 layout_autotune.cc:79] Layout agnostic_ops: logical_and
I0831 09:07:41.214558 27067 layout_autotune.cc:79] Layout agnostic_ops: pow
I0831 09:07:41.214563 27067 layout_autotune.cc:79] Layout agnostic_ops: dirichlet
I0831 09:07:41.214567 27067 layout_autotune.cc:79] Layout agnostic_ops: stanh
I0831 09:07:41.214571 27067 layout_autotune.cc:79] Layout agnostic_ops: label_smooth
I0831 09:07:41.214576 27067 layout_autotune.cc:79] Layout agnostic_ops: fold
I0831 09:07:41.214582 27067 layout_autotune.cc:79] Layout agnostic_ops: merged_momentum
I0831 09:07:41.214588 27067 layout_autotune.cc:79] Layout agnostic_ops: c_reduce_min
I0831 09:07:41.214596 27067 layout_autotune.cc:79] Layout agnostic_ops: rpn_target_assign
I0831 09:07:41.214607 27067 layout_autotune.cc:79] Layout agnostic_ops: fused_feedforward
I0831 09:07:41.214622 27067 layout_autotune.cc:79] Layout agnostic_ops: roi_perspective_transform
I0831 09:07:41.214627 27067 layout_autotune.cc:79] Layout agnostic_ops: expand
I0831 09:07:41.214632 27067 layout_autotune.cc:79] Layout agnostic_ops: rnn_memory_helper
I0831 09:07:41.214637 27067 layout_autotune.cc:79] Layout agnostic_ops: prroi_pool
I0831 09:07:41.214645 27067 layout_autotune.cc:64] Heavily layout sensitive OP: pool3d
I0831 09:07:41.214649 27067 layout_autotune.cc:79] Layout agnostic_ops: memcpy
I0831 09:07:41.214654 27067 layout_autotune.cc:79] Layout agnostic_ops: distribute_fpn_proposals
I0831 09:07:41.214659 27067 layout_autotune.cc:54] Lightly layout sensitive OP: frame
I0831 09:07:41.214665 27067 layout_autotune.cc:79] Layout agnostic_ops: bincount
I0831 09:07:41.214670 27067 layout_autotune.cc:79] Layout agnostic_ops: tanh_p
I0831 09:07:41.214675 27067 layout_autotune.cc:79] Layout agnostic_ops: shape
I0831 09:07:41.214679 27067 layout_autotune.cc:54] Lightly layout sensitive OP: mode
I0831 09:07:41.214684 27067 layout_autotune.cc:79] Layout agnostic_ops: depend
I0831 09:07:41.214689 27067 layout_autotune.cc:64] Heavily layout sensitive OP: group_norm
I0831 09:07:41.214696 27067 layout_autotune.cc:79] Layout agnostic_ops: c_softmax_with_cross_entropy
I0831 09:07:41.214706 27067 layout_autotune.cc:64] Heavily layout sensitive OP: resnet_unit
I0831 09:07:41.214710 27067 layout_autotune.cc:79] Layout agnostic_ops: sequence_expand_as
I0831 09:07:41.214715 27067 layout_autotune.cc:79] Layout agnostic_ops: assert
I0831 09:07:41.214720 27067 layout_autotune.cc:79] Layout agnostic_ops: weighted_sample_neighbors
I0831 09:07:41.214725 27067 layout_autotune.cc:79] Layout agnostic_ops: cos_sim
I0831 09:07:41.214728 27067 layout_autotune.cc:79] Layout agnostic_ops: eigvals
I0831 09:07:41.214735 27067 layout_autotune.cc:79] Layout agnostic_ops: save_combine
I0831 09:07:41.214740 27067 layout_autotune.cc:79] Layout agnostic_ops: class_center_sample
I0831 09:07:41.214747 27067 layout_autotune.cc:79] Layout agnostic_ops: elementwise_fmin
I0831 09:07:41.214751 27067 layout_autotune.cc:79] Layout agnostic_ops: read_file
I0831 09:07:41.214756 27067 layout_autotune.cc:79] Layout agnostic_ops: isfinite
I0831 09:07:41.214759 27067 layout_autotune.cc:79] Layout agnostic_ops: all_reduce
I0831 09:07:41.214764 27067 layout_autotune.cc:54] Lightly layout sensitive OP: arg_max
I0831 09:07:41.214771 27067 layout_autotune.cc:54] Lightly layout sensitive OP: equal
I0831 09:07:41.214774 27067 layout_autotune.cc:79] Layout agnostic_ops: fake_dequantize_max_abs
I0831 09:07:41.214779 27067 layout_autotune.cc:79] Layout agnostic_ops: qr
I0831 09:07:41.214784 27067 layout_autotune.cc:79] Layout agnostic_ops: anchor_generator
I0831 09:07:41.214792 27067 layout_autotune.cc:54] Lightly layout sensitive OP: layer_norm
I0831 09:07:41.214795 27067 layout_autotune.cc:79] Layout agnostic_ops: merge_selected_rows
I0831 09:07:41.214799 27067 layout_autotune.cc:79] Layout agnostic_ops: select_input
I0831 09:07:41.214804 27067 layout_autotune.cc:79] Layout agnostic_ops: acosh
I0831 09:07:41.214809 27067 layout_autotune.cc:79] Layout agnostic_ops: p_send_array
I0831 09:07:41.214814 27067 layout_autotune.cc:79] Layout agnostic_ops: stft
I0831 09:07:41.214823 27067 layout_autotune.cc:54] Lightly layout sensitive OP: less_equal
I0831 09:07:41.214830 27067 layout_autotune.cc:79] Layout agnostic_ops: rnn
I0831 09:07:41.214843 27067 layout_autotune.cc:79] Layout agnostic_ops: fusion_lstm
I0831 09:07:41.214851 27067 layout_autotune.cc:79] Layout agnostic_ops: lars_momentum
I0831 09:07:41.214857 27067 layout_autotune.cc:79] Layout agnostic_ops: hard_sigmoid
I0831 09:07:41.214860 27067 layout_autotune.cc:79] Layout agnostic_ops: isnan
I0831 09:07:41.214865 27067 layout_autotune.cc:79] Layout agnostic_ops: sparse_mv
I0831 09:07:41.214871 27067 layout_autotune.cc:54] Lightly layout sensitive OP: elementwise_floordiv
I0831 09:07:41.214876 27067 layout_autotune.cc:79] Layout agnostic_ops: mul_p
I0831 09:07:41.214880 27067 layout_autotune.cc:79] Layout agnostic_ops: correlation
I0831 09:07:41.214886 27067 layout_autotune.cc:79] Layout agnostic_ops: conditional_block
I0831 09:07:41.214891 27067 layout_autotune.cc:79] Layout agnostic_ops: gather_tree
I0831 09:07:41.214896 27067 layout_autotune.cc:79] Layout agnostic_ops: histogram
I0831 09:07:41.214900 27067 layout_autotune.cc:79] Layout agnostic_ops: sparse_asin
I0831 09:07:41.214905 27067 layout_autotune.cc:79] Layout agnostic_ops: i0e
I0831 09:07:41.214910 27067 layout_autotune.cc:54] Lightly layout sensitive OP: nanmedian
I0831 09:07:41.214915 27067 layout_autotune.cc:79] Layout agnostic_ops: fused_rotary_position_embedding
I0831 09:07:41.214919 27067 layout_autotune.cc:79] Layout agnostic_ops: segment_pool
I0831 09:07:41.214923 27067 layout_autotune.cc:79] Layout agnostic_ops: fusion_repeated_fc_relu
I0831 09:07:41.214931 27067 layout_autotune.cc:64] Heavily layout sensitive OP: sync_batch_norm
I0831 09:07:41.214936 27067 layout_autotune.cc:79] Layout agnostic_ops: nop
I0831 09:07:41.214948 27067 layout_autotune.cc:79] Layout agnostic_ops: fused_attention
I0831 09:07:41.214953 27067 layout_autotune.cc:79] Layout agnostic_ops: lod_tensor_to_array
I0831 09:07:41.214958 27067 layout_autotune.cc:79] Layout agnostic_ops: filter_by_instag
I0831 09:07:41.214962 27067 layout_autotune.cc:79] Layout agnostic_ops: expand_as_v2
I0831 09:07:41.214967 27067 layout_autotune.cc:79] Layout agnostic_ops: diag_v2
I0831 09:07:41.214972 27067 layout_autotune.cc:79] Layout agnostic_ops: pull_box_sparse
I0831 09:07:41.214977 27067 layout_autotune.cc:79] Layout agnostic_ops: nll_loss
I0831 09:07:41.214982 27067 layout_autotune.cc:79] Layout agnostic_ops: sin_p
I0831 09:07:41.214985 27067 layout_autotune.cc:79] Layout agnostic_ops: sparse_relu
I0831 09:07:41.214989 27067 layout_autotune.cc:79] Layout agnostic_ops: dot
I0831 09:07:41.214995 27067 layout_autotune.cc:79] Layout agnostic_ops: fused_token_prune
I0831 09:07:41.215003 27067 layout_autotune.cc:79] Layout agnostic_ops: scale
I0831 09:07:41.215005 27067 layout_autotune.cc:54] Lightly layout sensitive OP: shuffle_batch
I0831 09:07:41.215010 27067 layout_autotune.cc:79] Layout agnostic_ops: sparse_isnan
I0831 09:07:41.215013 27067 layout_autotune.cc:79] Layout agnostic_ops: diag
I0831 09:07:41.215018 27067 layout_autotune.cc:79] Layout agnostic_ops: leaky_relu
I0831 09:07:41.215023 27067 layout_autotune.cc:79] Layout agnostic_ops: multiplex
I0831 09:07:41.215027 27067 layout_autotune.cc:79] Layout agnostic_ops: allclose
I0831 09:07:41.215037 27067 layout_autotune.cc:79] Layout agnostic_ops: adamw
I0831 09:07:41.215044 27067 layout_autotune.cc:54] Lightly layout sensitive OP: elementwise_pow
I0831 09:07:41.215047 27067 layout_autotune.cc:79] Layout agnostic_ops: fetch
I0831 09:07:41.215056 27067 layout_autotune.cc:79] Layout agnostic_ops: prior_box
I0831 09:07:41.215061 27067 layout_autotune.cc:79] Layout agnostic_ops: shrink_rnn_memory
I0831 09:07:41.215066 27067 layout_autotune.cc:54] Lightly layout sensitive OP: p_norm
I0831 09:07:41.215073 27067 layout_autotune.cc:79] Layout agnostic_ops: c_concat
I0831 09:07:41.215080 27067 layout_autotune.cc:79] Layout agnostic_ops: fused_gate_attention
I0831 09:07:41.215083 27067 layout_autotune.cc:54] Lightly layout sensitive OP: unique_consecutive
I0831 09:07:41.215092 27067 layout_autotune.cc:54] Lightly layout sensitive OP: lod_reset
I0831 09:07:41.215097 27067 layout_autotune.cc:79] Layout agnostic_ops: pad
I0831 09:07:41.215103 27067 layout_autotune.cc:79] Layout agnostic_ops: sequence_conv
I0831 09:07:41.215108 27067 layout_autotune.cc:54] Lightly layout sensitive OP: set_value
I0831 09:07:41.215114 27067 layout_autotune.cc:79] Layout agnostic_ops: log10
I0831 09:07:41.215119 27067 layout_autotune.cc:79] Layout agnostic_ops: nms
I0831 09:07:41.215123 27067 layout_autotune.cc:79] Layout agnostic_ops: bitwise_xor
I0831 09:07:41.215128 27067 layout_autotune.cc:79] Layout agnostic_ops: p_recv_array
I0831 09:07:41.215133 27067 layout_autotune.cc:79] Layout agnostic_ops: center_loss
I0831 09:07:41.215138 27067 layout_autotune.cc:79] Layout agnostic_ops: randint
I0831 09:07:41.215142 27067 layout_autotune.cc:79] Layout agnostic_ops: cos_p
I0831 09:07:41.215147 27067 layout_autotune.cc:54] Lightly layout sensitive OP: sparse_softmax
I0831 09:07:41.215152 27067 layout_autotune.cc:79] Layout agnostic_ops: attention_lstm
I0831 09:07:41.215162 27067 layout_autotune.cc:79] Layout agnostic_ops: uniform_random
I0831 09:07:41.215174 27067 layout_autotune.cc:54] Lightly layout sensitive OP: slice
I0831 09:07:41.215181 27067 layout_autotune.cc:79] Layout agnostic_ops: dequantize
I0831 09:07:41.215184 27067 layout_autotune.cc:79] Layout agnostic_ops: meshgrid
I0831 09:07:41.215190 27067 layout_autotune.cc:79] Layout agnostic_ops: hard_swish
I0831 09:07:41.215195 27067 layout_autotune.cc:79] Layout agnostic_ops: sin
I0831 09:07:41.215199 27067 layout_autotune.cc:79] Layout agnostic_ops: mean_iou
I0831 09:07:41.215204 27067 layout_autotune.cc:79] Layout agnostic_ops: view_dtype
I0831 09:07:41.215209 27067 layout_autotune.cc:64] Heavily layout sensitive OP: pad2d
I0831 09:07:41.215214 27067 layout_autotune.cc:79] Layout agnostic_ops: inverse
I0831 09:07:41.215219 27067 layout_autotune.cc:54] Lightly layout sensitive OP: spectral_norm
I0831 09:07:41.215224 27067 layout_autotune.cc:79] Layout agnostic_ops: shuffle_channel
I0831 09:07:41.215231 27067 layout_autotune.cc:79] Layout agnostic_ops: multi_gru
I0831 09:07:41.215237 27067 layout_autotune.cc:79] Layout agnostic_ops: send_v2
I0831 09:07:41.215243 27067 layout_autotune.cc:79] Layout agnostic_ops: psroi_pool
I0831 09:07:41.215250 27067 layout_autotune.cc:79] Layout agnostic_ops: seed
I0831 09:07:41.215256 27067 layout_autotune.cc:54] Lightly layout sensitive OP: fused_elementwise_div
I0831 09:07:41.215260 27067 layout_autotune.cc:79] Layout agnostic_ops: reshape_p
I0831 09:07:41.215266 27067 layout_autotune.cc:79] Layout agnostic_ops: fused_adam
I0831 09:07:41.215273 27067 layout_autotune.cc:79] Layout agnostic_ops: py_func
I0831 09:07:41.215279 27067 layout_autotune.cc:79] Layout agnostic_ops: dist_concat
I0831 09:07:41.215284 27067 layout_autotune.cc:79] Layout agnostic_ops: ceil
I0831 09:07:41.215288 27067 layout_autotune.cc:79] Layout agnostic_ops: eig
I0831 09:07:41.215293 27067 layout_autotune.cc:54] Lightly layout sensitive OP: reduce_min
I0831 09:07:41.215298 27067 layout_autotune.cc:79] Layout agnostic_ops: cos
I0831 09:07:41.215302 27067 layout_autotune.cc:79] Layout agnostic_ops: tensor_unfold
I0831 09:07:41.215309 27067 layout_autotune.cc:79] Layout agnostic_ops: cudnn_lstm
I0831 09:07:41.215313 27067 layout_autotune.cc:79] Layout agnostic_ops: random_routing
I0831 09:07:41.215319 27067 layout_autotune.cc:54] Lightly layout sensitive OP: reduce_sum
I0831 09:07:41.215323 27067 layout_autotune.cc:79] Layout agnostic_ops: digamma
I0831 09:07:41.215328 27067 layout_autotune.cc:54] Lightly layout sensitive OP: quantize_linear
I0831 09:07:41.215335 27067 layout_autotune.cc:79] Layout agnostic_ops: assign_value
I0831 09:07:41.215340 27067 layout_autotune.cc:79] Layout agnostic_ops: fused_multi_transformer_xpu
I0831 09:07:41.215344 27067 layout_autotune.cc:79] Layout agnostic_ops: increment
I0831 09:07:41.215348 27067 layout_autotune.cc:79] Layout agnostic_ops: logspace
I0831 09:07:41.215358 27067 layout_autotune.cc:79] Layout agnostic_ops: tdm_sampler
I0831 09:07:41.215361 27067 layout_autotune.cc:79] Layout agnostic_ops: fused_softmax_mask
I0831 09:07:41.215365 27067 layout_autotune.cc:79] Layout agnostic_ops: embedding_with_eltwise_add_xpu
I0831 09:07:41.215370 27067 layout_autotune.cc:79] Layout agnostic_ops: sparse_expm1
I0831 09:07:41.215375 27067 layout_autotune.cc:79] Layout agnostic_ops: sequence_reverse
I0831 09:07:41.215379 27067 layout_autotune.cc:79] Layout agnostic_ops: eigvalsh
I0831 09:07:41.215384 27067 layout_autotune.cc:79] Layout agnostic_ops: sparse_values
I0831 09:07:41.215389 27067 layout_autotune.cc:79] Layout agnostic_ops: array_to_lod_tensor
I0831 09:07:41.215394 27067 layout_autotune.cc:54] Lightly layout sensitive OP: diagonal
I0831 09:07:41.215399 27067 layout_autotune.cc:79] Layout agnostic_ops: fused_dropout_add
I0831 09:07:41.215402 27067 layout_autotune.cc:79] Layout agnostic_ops: trunc
I0831 09:07:41.215407 27067 layout_autotune.cc:79] Layout agnostic_ops: log2
I0831 09:07:41.215412 27067 layout_autotune.cc:79] Layout agnostic_ops: marker
I0831 09:07:41.215417 27067 layout_autotune.cc:79] Layout agnostic_ops: scatter_add_p
I0831 09:07:41.215422 27067 layout_autotune.cc:79] Layout agnostic_ops: tanh
I0831 09:07:41.215430 27067 layout_autotune.cc:79] Layout agnostic_ops: yolov3_loss
I0831 09:07:41.215435 27067 layout_autotune.cc:79] Layout agnostic_ops: graph_send_recv
I0831 09:07:41.215437 27067 layout_autotune.cc:79] Layout agnostic_ops: accuracy
I0831 09:07:41.215441 27067 layout_autotune.cc:79] Layout agnostic_ops: atan
I0831 09:07:41.215445 27067 layout_autotune.cc:54] Lightly layout sensitive OP: less_than
I0831 09:07:41.215451 27067 layout_autotune.cc:54] Lightly layout sensitive OP: reduce_amax
I0831 09:07:41.215459 27067 layout_autotune.cc:54] Lightly layout sensitive OP: unsqueeze
I0831 09:07:41.215464 27067 layout_autotune.cc:79] Layout agnostic_ops: npu_identity
I0831 09:07:41.215468 27067 layout_autotune.cc:79] Layout agnostic_ops: crf_decoding
I0831 09:07:41.215473 27067 layout_autotune.cc:79] Layout agnostic_ops: global_gather
I0831 09:07:41.215479 27067 layout_autotune.cc:79] Layout agnostic_ops: merged_adam
I0831 09:07:41.215484 27067 layout_autotune.cc:79] Layout agnostic_ops: lerp
I0831 09:07:41.215490 27067 layout_autotune.cc:79] Layout agnostic_ops: c_allreduce_prod
I0831 09:07:41.215495 27067 layout_autotune.cc:79] Layout agnostic_ops: weight_quantize
I0831 09:07:41.215499 27067 layout_autotune.cc:54] Lightly layout sensitive OP: log_softmax
I0831 09:07:41.215503 27067 layout_autotune.cc:79] Layout agnostic_ops: pow_p
I0831 09:07:41.215509 27067 layout_autotune.cc:79] Layout agnostic_ops: matrix_nms
I0831 09:07:41.215514 27067 layout_autotune.cc:79] Layout agnostic_ops: ftrl
I0831 09:07:41.215519 27067 layout_autotune.cc:54] Lightly layout sensitive OP: top_k_v2
I0831 09:07:41.215524 27067 layout_autotune.cc:79] Layout agnostic_ops: cast
I0831 09:07:41.215528 27067 layout_autotune.cc:79] Layout agnostic_ops: concat_p
I0831 09:07:41.215533 27067 layout_autotune.cc:79] Layout agnostic_ops: max_sequence_len
I0831 09:07:41.215538 27067 layout_autotune.cc:79] Layout agnostic_ops: tanh_shrink
I0831 09:07:41.215541 27067 layout_autotune.cc:79] Layout agnostic_ops: c_comm_init
I0831 09:07:41.215546 27067 layout_autotune.cc:79] Layout agnostic_ops: hard_shrink
I0831 09:07:41.215550 27067 layout_autotune.cc:79] Layout agnostic_ops: sparse_acosh
I0831 09:07:41.215554 27067 layout_autotune.cc:79] Layout agnostic_ops: logit
I0831 09:07:41.215560 27067 layout_autotune.cc:79] Layout agnostic_ops: multiclass_nms
I0831 09:07:41.215565 27067 layout_autotune.cc:79] Layout agnostic_ops: c_broadcast
I0831 09:07:41.215570 27067 layout_autotune.cc:79] Layout agnostic_ops: fusion_transpose_flatten_concat
I0831 09:07:41.215574 27067 layout_autotune.cc:79] Layout agnostic_ops: sequence_unpad
I0831 09:07:41.215579 27067 layout_autotune.cc:54] Lightly layout sensitive OP: fused_elemwise_add_activation
I0831 09:07:41.215585 27067 layout_autotune.cc:79] Layout agnostic_ops: warprnnt
I0831 09:07:41.215595 27067 layout_autotune.cc:79] Layout agnostic_ops: pull_sparse_v2
I0831 09:07:41.215600 27067 layout_autotune.cc:79] Layout agnostic_ops: einsum
I0831 09:07:41.215605 27067 layout_autotune.cc:54] Lightly layout sensitive OP: frobenius_norm
I0831 09:07:41.215612 27067 layout_autotune.cc:79] Layout agnostic_ops: crop
I0831 09:07:41.215617 27067 layout_autotune.cc:79] Layout agnostic_ops: cross_entropy2
I0831 09:07:41.215621 27067 layout_autotune.cc:54] Lightly layout sensitive OP: tensor_array_to_tensor
I0831 09:07:41.215626 27067 layout_autotune.cc:79] Layout agnostic_ops: sparse_fused_attention
I0831 09:07:41.215631 27067 layout_autotune.cc:79] Layout agnostic_ops: tdm_child
I0831 09:07:41.215637 27067 layout_autotune.cc:79] Layout agnostic_ops: fused_bias_act
I0831 09:07:41.215644 27067 layout_autotune.cc:79] Layout agnostic_ops: fused_embedding_seq_pool
I0831 09:07:41.215651 27067 layout_autotune.cc:79] Layout agnostic_ops: weight_only_linear
I0831 09:07:41.215656 27067 layout_autotune.cc:54] Lightly layout sensitive OP: kthvalue
I0831 09:07:41.215660 27067 layout_autotune.cc:79] Layout agnostic_ops: graph_send_uv
I0831 09:07:41.215665 27067 layout_autotune.cc:79] Layout agnostic_ops: erf
I0831 09:07:41.215669 27067 layout_autotune.cc:79] Layout agnostic_ops: yolo_box_post
I0831 09:07:41.215675 27067 layout_autotune.cc:79] Layout agnostic_ops: conv2d_inception_fusion
I0831 09:07:41.215682 27067 layout_autotune.cc:54] Lightly layout sensitive OP: logsumexp
I0831 09:07:41.215689 27067 layout_autotune.cc:64] Heavily layout sensitive OP: trilinear_interp
I0831 09:07:41.215695 27067 layout_autotune.cc:54] Lightly layout sensitive OP: fusion_seqpool_concat
I0831 09:07:41.215699 27067 layout_autotune.cc:79] Layout agnostic_ops: alloc_float_status
I0831 09:07:41.215703 27067 layout_autotune.cc:79] Layout agnostic_ops: sequence_concat
I0831 09:07:41.215708 27067 layout_autotune.cc:79] Layout agnostic_ops: sparse_tanh
I0831 09:07:41.215713 27067 layout_autotune.cc:54] Lightly layout sensitive OP: fusion_seqpool_cvm_concat
I0831 09:07:41.215718 27067 layout_autotune.cc:64] Heavily layout sensitive OP: unpool3d
I0831 09:07:41.215721 27067 layout_autotune.cc:79] Layout agnostic_ops: similarity_focus
I0831 09:07:41.215726 27067 layout_autotune.cc:79] Layout agnostic_ops: shadow_output
I0831 09:07:41.215732 27067 layout_autotune.cc:79] Layout agnostic_ops: c_allreduce_max
I0831 09:07:41.215736 27067 layout_autotune.cc:54] Lightly layout sensitive OP: argsort
I0831 09:07:41.215742 27067 layout_autotune.cc:79] Layout agnostic_ops: sequence_expand
I0831 09:07:41.215749 27067 layout_autotune.cc:79] Layout agnostic_ops: fused_bn_add_activation
I0831 09:07:41.215754 27067 layout_autotune.cc:79] Layout agnostic_ops: sgd
I0831 09:07:41.215759 27067 layout_autotune.cc:79] Layout agnostic_ops: exponential
I0831 09:07:41.215766 27067 layout_autotune.cc:64] Heavily layout sensitive OP: bilinear_interp_v2
I0831 09:07:41.215770 27067 layout_autotune.cc:79] Layout agnostic_ops: sparse_scale
I0831 09:07:41.215775 27067 layout_autotune.cc:79] Layout agnostic_ops: atanh
I0831 09:07:41.215782 27067 layout_autotune.cc:54] Lightly layout sensitive OP: fused_transpose
I0831 09:07:41.215788 27067 layout_autotune.cc:79] Layout agnostic_ops: clip
I0831 09:07:41.215793 27067 layout_autotune.cc:79] Layout agnostic_ops: reduce
I0831 09:07:41.215798 27067 layout_autotune.cc:79] Layout agnostic_ops: conditional_block_infer
I0831 09:07:41.215804 27067 layout_autotune.cc:54] Lightly layout sensitive OP: c_gen_nccl_id
I0831 09:07:41.215808 27067 layout_autotune.cc:79] Layout agnostic_ops: sparse_to_dense
I0831 09:07:41.215816 27067 layout_autotune.cc:79] Layout agnostic_ops: deformable_conv_v1
I0831 09:07:41.215818 27067 layout_autotune.cc:79] Layout agnostic_ops: hinge_loss
I0831 09:07:41.215822 27067 layout_autotune.cc:79] Layout agnostic_ops: determinant
I0831 09:07:41.215826 27067 layout_autotune.cc:79] Layout agnostic_ops: rsqrt_p
I0831 09:07:41.215839 27067 layout_autotune.cc:64] Heavily layout sensitive OP: conv2d_transpose
I0831 09:07:41.215848 27067 layout_autotune.cc:79] Layout agnostic_ops: memcpy_d2h
I0831 09:07:41.215853 27067 layout_autotune.cc:79] Layout agnostic_ops: softsign
I0831 09:07:41.215857 27067 layout_autotune.cc:79] Layout agnostic_ops: sparse_acos
I0831 09:07:41.215862 27067 layout_autotune.cc:79] Layout agnostic_ops: fake_quantize_dequantize_abs_max
I0831 09:07:41.215867 27067 layout_autotune.cc:79] Layout agnostic_ops: broadcast_tensors
I0831 09:07:41.215873 27067 layout_autotune.cc:79] Layout agnostic_ops: cholesky_solve
I0831 09:07:41.215878 27067 layout_autotune.cc:79] Layout agnostic_ops: grid_sampler
I0831 09:07:41.215883 27067 layout_autotune.cc:79] Layout agnostic_ops: mp_allreduce_sum
I0831 09:07:41.215888 27067 layout_autotune.cc:54] Lightly layout sensitive OP: fft_c2r
I0831 09:07:41.215896 27067 layout_autotune.cc:79] Layout agnostic_ops: pyramid_hash
I0831 09:07:41.215902 27067 layout_autotune.cc:79] Layout agnostic_ops: fake_quantize_dequantize_moving_average_abs_max
I0831 09:07:41.215906 27067 layout_autotune.cc:79] Layout agnostic_ops: multi_dot
I0831 09:07:41.215910 27067 layout_autotune.cc:79] Layout agnostic_ops: sparse_abs
I0831 09:07:41.215915 27067 layout_autotune.cc:79] Layout agnostic_ops: sequence_pool
I0831 09:07:41.215919 27067 layout_autotune.cc:79] Layout agnostic_ops: broadcast
I0831 09:07:41.215924 27067 layout_autotune.cc:32] Already exists in Layout OP: transpose
I0831 09:07:41.215927 27067 layout_autotune.cc:79] Layout agnostic_ops: top_k
I0831 09:07:41.215934 27067 layout_autotune.cc:64] Heavily layout sensitive OP: pixel_unshuffle
I0831 09:07:41.215940 27067 layout_autotune.cc:79] Layout agnostic_ops: read
I0831 09:07:41.215945 27067 layout_autotune.cc:79] Layout agnostic_ops: take_along_axis
I0831 09:07:41.215948 27067 layout_autotune.cc:79] Layout agnostic_ops: dist
I0831 09:07:41.215953 27067 layout_autotune.cc:79] Layout agnostic_ops: affine_grid
I0831 09:07:41.215960 27067 layout_autotune.cc:54] Lightly layout sensitive OP: gaussian_random_batch_size_like
I0831 09:07:41.215965 27067 layout_autotune.cc:54] Lightly layout sensitive OP: fake_channel_wise_dequantize_max_abs
I0831 09:07:41.215970 27067 layout_autotune.cc:79] Layout agnostic_ops: div_p
I0831 09:07:41.215973 27067 layout_autotune.cc:79] Layout agnostic_ops: fc_xpu
I0831 09:07:41.215978 27067 layout_autotune.cc:79] Layout agnostic_ops: reciprocal
I0831 09:07:41.215983 27067 layout_autotune.cc:79] Layout agnostic_ops: sequence_mask
I0831 09:07:41.215988 27067 layout_autotune.cc:79] Layout agnostic_ops: prune_gate_by_capacity
I0831 09:07:41.215994 27067 layout_autotune.cc:54] Lightly layout sensitive OP: fill_diagonal_tensor
I0831 09:07:41.215999 27067 layout_autotune.cc:79] Layout agnostic_ops: abs
I0831 09:07:41.216004 27067 layout_autotune.cc:54] Lightly layout sensitive OP: partial_concat
I0831 09:07:41.216009 27067 layout_autotune.cc:79] Layout agnostic_ops: elu
I0831 09:07:41.216014 27067 layout_autotune.cc:54] Lightly layout sensitive OP: index_select
I0831 09:07:41.216018 27067 layout_autotune.cc:79] Layout agnostic_ops: row_conv
I0831 09:07:41.216022 27067 layout_autotune.cc:54] Lightly layout sensitive OP: cross
I0831 09:07:41.216028 27067 layout_autotune.cc:54] Lightly layout sensitive OP: elementwise_mul
I0831 09:07:41.216034 27067 layout_autotune.cc:79] Layout agnostic_ops: decayed_adagrad
I0831 09:07:41.216037 27067 layout_autotune.cc:79] Layout agnostic_ops: sparse_tan
I0831 09:07:41.216043 27067 layout_autotune.cc:79] Layout agnostic_ops: bipartite_match
I0831 09:07:41.216053 27067 layout_autotune.cc:79] Layout agnostic_ops: run_program
I0831 09:07:41.216058 27067 layout_autotune.cc:79] Layout agnostic_ops: fake_quantize_moving_average_abs_max
I0831 09:07:41.216068 27067 layout_autotune.cc:54] Lightly layout sensitive OP: fused_multi_transformer_int8
I0831 09:07:41.216076 27067 layout_autotune.cc:79] Layout agnostic_ops: mine_hard_examples
I0831 09:07:41.216080 27067 layout_autotune.cc:79] Layout agnostic_ops: target_assign
I0831 09:07:41.216089 27067 layout_autotune.cc:79] Layout agnostic_ops: lstm
I0831 09:07:41.216095 27067 layout_autotune.cc:79] Layout agnostic_ops: assign_pos
I0831 09:07:41.216101 27067 layout_autotune.cc:79] Layout agnostic_ops: truncated_gaussian_random
I0831 09:07:41.216105 27067 layout_autotune.cc:79] Layout agnostic_ops: sparse_to_sparse_csr
I0831 09:07:41.216109 27067 layout_autotune.cc:54] Lightly layout sensitive OP: match_matrix_tensor
I0831 09:07:41.216116 27067 layout_autotune.cc:54] Lightly layout sensitive OP: elementwise_div
I0831 09:07:41.216120 27067 layout_autotune.cc:79] Layout agnostic_ops: sparse_square
I0831 09:07:41.216125 27067 layout_autotune.cc:54] Lightly layout sensitive OP: index_select_strided
I0831 09:07:41.216130 27067 layout_autotune.cc:79] Layout agnostic_ops: kldiv_loss
I0831 09:07:41.216132 27067 layout_autotune.cc:79] Layout agnostic_ops: split_p
I0831 09:07:41.216138 27067 layout_autotune.cc:54] Lightly layout sensitive OP: cumsum
I0831 09:07:41.216146 27067 layout_autotune.cc:79] Layout agnostic_ops: sum
I0831 09:07:41.216151 27067 layout_autotune.cc:79] Layout agnostic_ops: triu_indices
I0831 09:07:41.216156 27067 layout_autotune.cc:79] Layout agnostic_ops: proximal_adagrad
I0831 09:07:41.216161 27067 layout_autotune.cc:79] Layout agnostic_ops: i1
I0831 09:07:41.216171 27067 layout_autotune.cc:79] Layout agnostic_ops: update_loss_scaling
I0831 09:07:41.216176 27067 layout_autotune.cc:79] Layout agnostic_ops: fast_where_xpu
I0831 09:07:41.216181 27067 layout_autotune.cc:79] Layout agnostic_ops: shard_index
I0831 09:07:41.216190 27067 layout_autotune.cc:64] Heavily layout sensitive OP: fused_conv3d
I0831 09:07:41.216197 27067 layout_autotune.cc:79] Layout agnostic_ops: sparse_asinh
I0831 09:07:41.216202 27067 layout_autotune.cc:79] Layout agnostic_ops: selu
I0831 09:07:41.216207 27067 layout_autotune.cc:54] Lightly layout sensitive OP: gumbel_softmax
I0831 09:07:41.216212 27067 layout_autotune.cc:79] Layout agnostic_ops: mean
I0831 09:07:41.216217 27067 layout_autotune.cc:79] Layout agnostic_ops: sequence_pad
I0831 09:07:41.216221 27067 layout_autotune.cc:79] Layout agnostic_ops: uniform_random_p
I0831 09:07:41.216225 27067 layout_autotune.cc:79] Layout agnostic_ops: tree_conv
I0831 09:07:41.216229 27067 layout_autotune.cc:79] Layout agnostic_ops: assign
I0831 09:07:41.216235 27067 layout_autotune.cc:54] Lightly layout sensitive OP: flatten_contiguous_range
I0831 09:07:41.216240 27067 layout_autotune.cc:79] Layout agnostic_ops: tril_triu
I0831 09:07:41.216244 27067 layout_autotune.cc:79] Layout agnostic_ops: llm_int8_linear
I0831 09:07:41.216248 27067 layout_autotune.cc:79] Layout agnostic_ops: multi_encoder_xpu
I0831 09:07:41.216253 27067 layout_autotune.cc:79] Layout agnostic_ops: celu
I0831 09:07:41.216259 27067 layout_autotune.cc:54] Lightly layout sensitive OP: reduce_mean
I0831 09:07:41.216264 27067 layout_autotune.cc:79] Layout agnostic_ops: brelu
I0831 09:07:41.216269 27067 layout_autotune.cc:79] Layout agnostic_ops: sinh
I0831 09:07:41.216274 27067 layout_autotune.cc:79] Layout agnostic_ops: nextafter
I0831 09:07:41.216277 27067 layout_autotune.cc:79] Layout agnostic_ops: rank_loss
I0831 09:07:41.216281 27067 layout_autotune.cc:79] Layout agnostic_ops: sparse_pow
I0831 09:07:41.216287 27067 layout_autotune.cc:54] Lightly layout sensitive OP: reduce_max
I0831 09:07:41.216297 27067 layout_autotune.cc:79] Layout agnostic_ops: fusion_gru
I0831 09:07:41.216301 27067 layout_autotune.cc:79] Layout agnostic_ops: fill_zeros_like2
I0831 09:07:41.216305 27067 layout_autotune.cc:79] Layout agnostic_ops: expm1
I0831 09:07:41.216315 27067 layout_autotune.cc:54] Lightly layout sensitive OP: elementwise_sub
I0831 09:07:41.216320 27067 layout_autotune.cc:79] Layout agnostic_ops: margin_rank_loss
I0831 09:07:41.216325 27067 layout_autotune.cc:79] Layout agnostic_ops: merge_lod_tensor_infer
I0831 09:07:41.216332 27067 layout_autotune.cc:79] Layout agnostic_ops: faster_tokenizer
I0831 09:07:41.216337 27067 layout_autotune.cc:79] Layout agnostic_ops: c_reduce_max
I0831 09:07:41.216343 27067 layout_autotune.cc:79] Layout agnostic_ops: c_identity
I0831 09:07:41.216347 27067 layout_autotune.cc:79] Layout agnostic_ops: relu
I0831 09:07:41.216354 27067 layout_autotune.cc:79] Layout agnostic_ops: sparse_reshape
I0831 09:07:41.216358 27067 layout_autotune.cc:79] Layout agnostic_ops: bernoulli_p
I0831 09:07:41.216361 27067 layout_autotune.cc:79] Layout agnostic_ops: is_empty
I0831 09:07:41.216367 27067 layout_autotune.cc:54] Lightly layout sensitive OP: reduce_all
I0831 09:07:41.216372 27067 layout_autotune.cc:79] Layout agnostic_ops: edit_distance
I0831 09:07:41.216377 27067 layout_autotune.cc:79] Layout agnostic_ops: tril_indices
I0831 09:07:41.216382 27067 layout_autotune.cc:79] Layout agnostic_ops: bmm
I0831 09:07:41.216389 27067 layout_autotune.cc:79] Layout agnostic_ops: yolo_box
I0831 09:07:41.216394 27067 layout_autotune.cc:79] Layout agnostic_ops: soft_relu
I0831 09:07:41.216399 27067 layout_autotune.cc:79] Layout agnostic_ops: split_lod_tensor
I0831 09:07:41.216406 27067 layout_autotune.cc:79] Layout agnostic_ops: density_prior_box
I0831 09:07:41.216411 27067 layout_autotune.cc:79] Layout agnostic_ops: swish
I0831 09:07:41.216416 27067 layout_autotune.cc:79] Layout agnostic_ops: eye
I0831 09:07:41.216423 27067 layout_autotune.cc:79] Layout agnostic_ops: cross_entropy
I0831 09:07:41.216426 27067 layout_autotune.cc:79] Layout agnostic_ops: beam_search_decode
I0831 09:07:41.216434 27067 layout_autotune.cc:54] Lightly layout sensitive OP: fused_elementwise_mul
I0831 09:07:41.216439 27067 layout_autotune.cc:79] Layout agnostic_ops: dpsgd
I0831 09:07:41.216446 27067 layout_autotune.cc:79] Layout agnostic_ops: cholesky
I0831 09:07:41.216450 27067 layout_autotune.cc:79] Layout agnostic_ops: identity_loss
I0831 09:07:41.216454 27067 layout_autotune.cc:79] Layout agnostic_ops: batch_fc
I0831 09:07:41.216461 27067 layout_autotune.cc:64] Heavily layout sensitive OP: nearest_interp
I0831 09:07:41.216466 27067 layout_autotune.cc:79] Layout agnostic_ops: sparse_log1p
I0831 09:07:41.216471 27067 layout_autotune.cc:54] Lightly layout sensitive OP: gather
I0831 09:07:41.216476 27067 layout_autotune.cc:54] Lightly layout sensitive OP: sparse_sum
I0831 09:07:41.216485 27067 layout_autotune.cc:64] Heavily layout sensitive OP: trilinear_interp_v2
I0831 09:07:41.216490 27067 layout_autotune.cc:79] Layout agnostic_ops: box_clip
I0831 09:07:41.216495 27067 layout_autotune.cc:79] Layout agnostic_ops: c_allgather
I0831 09:07:41.216498 27067 layout_autotune.cc:79] Layout agnostic_ops: i0
I0831 09:07:41.216502 27067 layout_autotune.cc:79] Layout agnostic_ops: isnan_v2
I0831 09:07:41.216506 27067 layout_autotune.cc:79] Layout agnostic_ops: lu
I0831 09:07:41.216509 27067 layout_autotune.cc:32] Already exists in Layout OP: softmax
I0831 09:07:41.216526 27067 layout_autotune.cc:64] Heavily layout sensitive OP: conv2d_fusion
I0831 09:07:41.216536 27067 layout_autotune.cc:79] Layout agnostic_ops: get_float_status
I0831 09:07:41.216540 27067 layout_autotune.cc:79] Layout agnostic_ops: index_sample
I0831 09:07:41.216544 27067 layout_autotune.cc:79] Layout agnostic_ops: logical_not
I0831 09:07:41.216552 27067 layout_autotune.cc:54] Lightly layout sensitive OP: elementwise_min
I0831 09:07:41.216555 27067 layout_autotune.cc:79] Layout agnostic_ops: cast_p
I0831 09:07:41.216559 27067 layout_autotune.cc:79] Layout agnostic_ops: erfinv
I0831 09:07:41.216563 27067 layout_autotune.cc:79] Layout agnostic_ops: collect_fpn_proposals
I0831 09:07:41.216568 27067 layout_autotune.cc:64] Heavily layout sensitive OP: pixel_shuffle
I0831 09:07:41.216573 27067 layout_autotune.cc:79] Layout agnostic_ops: thresholded_relu
I0831 09:07:41.216578 27067 layout_autotune.cc:79] Layout agnostic_ops: polygon_box_transform
I0831 09:07:41.216581 27067 layout_autotune.cc:79] Layout agnostic_ops: lookup_table_dequant
I0831 09:07:41.216586 27067 layout_autotune.cc:79] Layout agnostic_ops: warpctc
I0831 09:07:41.216593 27067 layout_autotune.cc:79] Layout agnostic_ops: elementwise_heaviside
I0831 09:07:41.216598 27067 layout_autotune.cc:54] Lightly layout sensitive OP: fake_channel_wise_quantize_abs_max
I0831 09:07:41.216605 27067 layout_autotune.cc:54] Lightly layout sensitive OP: fused_elementwise_add
I0831 09:07:41.216614 27067 layout_autotune.cc:79] Layout agnostic_ops: dequantize_abs_max
I0831 09:07:41.216617 27067 layout_autotune.cc:79] Layout agnostic_ops: svd
I0831 09:07:41.216622 27067 layout_autotune.cc:79] Layout agnostic_ops: flip
I0831 09:07:41.216629 27067 layout_autotune.cc:79] Layout agnostic_ops: quantize
I0831 09:07:41.216634 27067 layout_autotune.cc:79] Layout agnostic_ops: graph_send_ue_recv
I0831 09:07:41.216639 27067 layout_autotune.cc:79] Layout agnostic_ops: feed
I0831 09:07:41.216643 27067 layout_autotune.cc:84] The number of layout agnostic OPs: 633, heavily layout sensitive OPs: 39, lightly layout sensitive OPs: 140
I0831 09:07:41.241689 27067 op_desc.cc:1097] CompileTime infer shape on elementwise_div
I0831 09:07:41.242287 27067 eager.cc:112] Tensor(cuda_graph) have not GradNode, add GradNodeAccumulation0x5566381892d0 for it.
I0831 09:07:41.260442 27067 amp_auto_cast.cc:106] -- The size of all_ops: 1256 --
I0831 09:07:41.260489 27067 amp_auto_cast.cc:107] -- The size of supported_ops: 621 --
I0831 09:07:41.260493 27067 amp_auto_cast.cc:108] -- The size of unsupported_ops: 635 --
I0831 09:07:41.269408 27067 amp_auto_cast.cc:106] -- The size of all_ops: 1256 --
I0831 09:07:41.269452 27067 amp_auto_cast.cc:107] -- The size of supported_ops: 474 --
I0831 09:07:41.269456 27067 amp_auto_cast.cc:108] -- The size of unsupported_ops: 782 --
I0831 09:07:41.270643 27067 amp_auto_cast.cc:152] 0 0 635 782
I0831 09:07:41.273265 27067 op_desc.cc:1097] CompileTime infer shape on elementwise_div
I0831 09:07:41.273363 27067 op_desc.cc:1097] CompileTime infer shape on fill_any_like
I0831 09:07:41.273394 27067 infershape_utils.cc:547] BuildInferMetaContext: op kernel signature - Kernel Signature - name: full_like; inputs: X; attributes: value, dtype; outputs: Out
I0831 09:07:41.273464 27067 pybind.cc:1459] need skip: 0
I0831 09:07:41.273470 27067 pybind.cc:1462] Prim Flag Open: Runing composite grad fun for elementwise_div
I0831 09:07:41.273478 27067 composite_grad_desc_maker.h:75] Runing Composite Grad func for elementwise_div_grad 
I0831 09:07:41.273519 27067 operants_manager.cc:761] OperantsManager reusing static mode API paddle::prim::pow<DescTensor>
I0831 09:07:41.273578 27067 op_desc.cc:1097] CompileTime infer shape on fill_constant
I0831 09:07:41.273604 27067 op_desc.cc:1097] CompileTime infer shape on elementwise_pow
I0831 09:07:41.273613 27067 infershape_utils.cc:547] BuildInferMetaContext: op kernel signature - Kernel Signature - name: elementwise_pow_raw; inputs: X, Y; attributes: axis; outputs: Out
I0831 09:07:41.273634 27067 operants_manager.cc:695] OperantsManager reusing static mode API paddle::prim::divide<DescTensor>
I0831 09:07:41.273655 27067 op_desc.cc:1097] CompileTime infer shape on elementwise_div
I0831 09:07:41.273670 27067 operants_manager.cc:398] OperantsManager reusing static mode API paddle::prim::scale<DescTensor>
I0831 09:07:41.273691 27067 op_desc.cc:1097] CompileTime infer shape on scale
I0831 09:07:41.273701 27067 infershape_utils.cc:547] BuildInferMetaContext: op kernel signature - Kernel Signature - name: scale; inputs: X; attributes: scale, bias, bias_after_scale; outputs: Out
I0831 09:07:41.273718 27067 operants_manager.cc:1190] OperantsManager reusing static mode API paddle::prim::multiply<DescTensor>
I0831 09:07:41.273738 27067 op_desc.cc:1097] CompileTime infer shape on elementwise_mul
I0831 09:07:41.273769 27067 op_desc.cc:1097] CompileTime infer shape on fill_constant
I0831 09:07:41.273777 27067 operants_manager.cc:695] OperantsManager reusing static mode API paddle::prim::divide<DescTensor>
I0831 09:07:41.273790 27067 op_desc.cc:1097] CompileTime infer shape on elementwise_div
I0831 09:07:41.273803 27067 operants_manager.cc:1190] OperantsManager reusing static mode API paddle::prim::multiply<DescTensor>
I0831 09:07:41.273815 27067 op_desc.cc:1097] CompileTime infer shape on elementwise_mul
I0831 09:07:41.273828 27067 composite_grad_desc_maker.h:509] Recover: composite_tmp_15 To: _jst.0.args.0@GRAD
I0831 09:07:41.273834 27067 var_desc.cc:416] Flush  composite_tmp_15 1
I0831 09:07:41.273861 27067 composite_grad_desc_maker.h:509] Recover: composite_tmp_12 To: _jst.0.args.1@GRAD
I0831 09:07:41.273865 27067 var_desc.cc:416] Flush  composite_tmp_12 1
I0831 09:07:41.275274 27067 op_desc.cc:1097] CompileTime infer shape on fill_constant
I0831 09:07:41.276065 27067 op_desc.cc:1097] CompileTime infer shape on elementwise_pow
I0831 09:07:41.276084 27067 infershape_utils.cc:547] BuildInferMetaContext: op kernel signature - Kernel Signature - name: elementwise_pow_raw; inputs: X, Y; attributes: axis; outputs: Out
I0831 09:07:41.276814 27067 op_desc.cc:1097] CompileTime infer shape on elementwise_div
I0831 09:07:41.277487 27067 op_desc.cc:1097] CompileTime infer shape on scale
I0831 09:07:41.277509 27067 infershape_utils.cc:547] BuildInferMetaContext: op kernel signature - Kernel Signature - name: scale; inputs: X; attributes: scale, bias, bias_after_scale; outputs: Out
I0831 09:07:41.278213 27067 op_desc.cc:1097] CompileTime infer shape on elementwise_mul
I0831 09:07:41.278921 27067 op_desc.cc:1097] CompileTime infer shape on fill_constant
I0831 09:07:41.279672 27067 op_desc.cc:1097] CompileTime infer shape on elementwise_div
I0831 09:07:41.280382 27067 op_desc.cc:1097] CompileTime infer shape on elementwise_mul
I0831 09:07:41.280866 27067 op_desc.cc:1097] CompileTime infer shape on fill_any_like
I0831 09:07:41.280884 27067 infershape_utils.cc:547] BuildInferMetaContext: op kernel signature - Kernel Signature - name: full_like; inputs: X; attributes: value, dtype; outputs: Out
I0831 09:07:41.280990 27067 op_desc.cc:1097] CompileTime infer shape on fill_constant
I0831 09:07:41.281044 27067 op_desc.cc:1097] CompileTime infer shape on elementwise_pow
I0831 09:07:41.281051 27067 infershape_utils.cc:547] BuildInferMetaContext: op kernel signature - Kernel Signature - name: elementwise_pow_raw; inputs: X, Y; attributes: axis; outputs: Out
I0831 09:07:41.281109 27067 op_desc.cc:1097] CompileTime infer shape on elementwise_div
I0831 09:07:41.281160 27067 op_desc.cc:1097] CompileTime infer shape on scale
I0831 09:07:41.281198 27067 infershape_utils.cc:547] BuildInferMetaContext: op kernel signature - Kernel Signature - name: scale; inputs: X; attributes: scale, bias, bias_after_scale; outputs: Out
I0831 09:07:41.281260 27067 op_desc.cc:1097] CompileTime infer shape on elementwise_mul
I0831 09:07:41.281316 27067 op_desc.cc:1097] CompileTime infer shape on fill_constant
I0831 09:07:41.281363 27067 op_desc.cc:1097] CompileTime infer shape on elementwise_div
I0831 09:07:41.281414 27067 op_desc.cc:1097] CompileTime infer shape on elementwise_mul
I0831 09:07:41.282398 27067 graph.cc:144] create OpNode by fill_constant
I0831 09:07:41.282450 27067 graph.cc:144] create OpNode by elementwise_pow
I0831 09:07:41.282477 27067 graph.cc:144] create OpNode by elementwise_div
I0831 09:07:41.282500 27067 graph.cc:144] create OpNode by scale
I0831 09:07:41.282522 27067 graph.cc:144] create OpNode by elementwise_mul
I0831 09:07:41.282543 27067 graph.cc:144] create OpNode by fill_constant
I0831 09:07:41.282562 27067 graph.cc:144] create OpNode by elementwise_div
I0831 09:07:41.282582 27067 graph.cc:144] create OpNode by elementwise_mul
I0831 09:07:41.282613 27067 graph.cc:219] kStaleProgramOpDescs.size: 8
I0831 09:07:41.282892 27067 parallel_executor.cc:1323] The Program will be executed on CPU using ParallelExecutor, 1 cards are used, so 1 programs are executed in parallel.
I0831 09:07:41.282920 27067 build_strategy.cc:360] apply all passes
I0831 09:07:41.282935 27067 pass_builder.cc:29] Append fuse_bn_add_act_pass
I0831 09:07:41.282945 27067 pass_builder.cc:29] Append coalesce_grad_tensor_pass
I0831 09:07:41.282956 27067 pass_builder.cc:29] Append add_reader_dependency_pass
I0831 09:07:41.282963 27067 pass_builder.cc:29] Append all_reduce_mode_multi_devices_pass
I0831 09:07:41.282971 27067 pass_builder.cc:29] Append fuse_all_reduce_op_pass
I0831 09:07:41.282979 27067 pass_builder.cc:29] Append all_reduce_deps_pass
I0831 09:07:41.282987 27067 pass_builder.cc:29] Append modify_op_lock_and_record_event_pass
I0831 09:07:41.283001 27067 pass_builder.cc:29] Append multi_devices_check_pass
I0831 09:07:41.283011 27067 build_strategy.cc:247] CollectiveContext:endpoints_:trainer_id_:0
I0831 09:07:41.283017 27067 build_strategy.cc:371] BuildStrategy::Apply pass:fuse_bn_add_act_pass
I0831 09:07:41.283022 27067 build_strategy.cc:474] fuse_bn_add_act_pass is only supported on GPU, skipped.
I0831 09:07:41.283025 27067 build_strategy.cc:371] BuildStrategy::Apply pass:coalesce_grad_tensor_pass
I0831 09:07:41.283035 27067 build_strategy.cc:488] Start Apply Pass coalesce_grad_tensor_pass
I0831 09:07:41.283039 27067 build_strategy.cc:491] Apply Pass coalesce_grad_tensor_passto SubGraph 0
I0831 09:07:41.283053 27067 graph_helper.h:104] adj elementwise_pow0x556636c54db0 -> elementwise_div0x556636c53920  via composite_tmp_90x5566371f4440
I0831 09:07:41.283059 27067 graph_helper.h:104] adj elementwise_div0x556638579a90 -> elementwise_mul0x5566381791e0  via composite_tmp_140x556638178f90
I0831 09:07:41.283063 27067 graph_helper.h:104] adj fill_constant0x5566372dd660 -> elementwise_pow0x556636c54db0  via composite_tmp_80x5566372fb3c0
I0831 09:07:41.283067 27067 graph_helper.h:104] adj elementwise_div0x556636c53920 -> scale0x55663817e360  via composite_tmp_100x55663817e0f0
I0831 09:07:41.283071 27067 graph_helper.h:104] adj fill_constant0x5566386743a0 -> elementwise_div0x556638579a90  via composite_tmp_130x556638579880
I0831 09:07:41.283075 27067 graph_helper.h:104] adj scale0x55663817e360 -> elementwise_mul0x556638205180  via composite_tmp_110x556638204f30
I0831 09:07:41.283115 27067 onednn_context.cc:101] 0x5566386479f0 0
I0831 09:07:41.283121 27067 onednn_context.cc:104] Clearing DNNL cache.
I0831 09:07:41.283125 27067 onednn_context.cc:122] Resetting Paddle data layout to NCHW.
I0831 09:07:41.283129 27067 build_strategy.cc:497] Finish Apply Pass coalesce_grad_tensor_pass
I0831 09:07:41.283133 27067 build_strategy.cc:371] BuildStrategy::Apply pass:add_reader_dependency_pass
I0831 09:07:41.283138 27067 build_strategy.cc:488] Start Apply Pass add_reader_dependency_pass
I0831 09:07:41.283140 27067 build_strategy.cc:491] Apply Pass add_reader_dependency_passto SubGraph 0
I0831 09:07:41.283149 27067 graph_helper.h:104] adj elementwise_pow0x556636c54db0 -> elementwise_div0x556636c53920  via composite_tmp_90x5566371f4440
I0831 09:07:41.283152 27067 graph_helper.h:104] adj elementwise_div0x556638579a90 -> elementwise_mul0x5566381791e0  via composite_tmp_140x556638178f90
I0831 09:07:41.283156 27067 graph_helper.h:104] adj fill_constant0x5566372dd660 -> elementwise_pow0x556636c54db0  via composite_tmp_80x5566372fb3c0
I0831 09:07:41.283160 27067 graph_helper.h:104] adj elementwise_div0x556636c53920 -> scale0x55663817e360  via composite_tmp_100x55663817e0f0
I0831 09:07:41.283164 27067 graph_helper.h:104] adj fill_constant0x5566386743a0 -> elementwise_div0x556638579a90  via composite_tmp_130x556638579880
I0831 09:07:41.283591 27067 graph_helper.h:104] adj scale0x55663817e360 -> elementwise_mul0x556638205180  via composite_tmp_110x556638204f30
I0831 09:07:41.283617 27067 onednn_context.cc:101] 0x5566386479f0 0
I0831 09:07:41.283622 27067 onednn_context.cc:104] Clearing DNNL cache.
I0831 09:07:41.283624 27067 onednn_context.cc:122] Resetting Paddle data layout to NCHW.
I0831 09:07:41.283632 27067 build_strategy.cc:497] Finish Apply Pass add_reader_dependency_pass
I0831 09:07:41.283636 27067 build_strategy.cc:371] BuildStrategy::Apply pass:all_reduce_mode_multi_devices_pass
I0831 09:07:41.283648 27067 build_strategy.cc:488] Start Apply Pass all_reduce_mode_multi_devices_pass
I0831 09:07:41.283650 27067 build_strategy.cc:491] Apply Pass all_reduce_mode_multi_devices_passto SubGraph 0
I0831 09:07:41.283676 27067 graph_helper.h:104] adj elementwise_pow0x556636c54db0 -> elementwise_div0x556636c53920  via composite_tmp_90x5566371f4440
I0831 09:07:41.283680 27067 graph_helper.h:104] adj elementwise_div0x556638579a90 -> elementwise_mul0x5566381791e0  via composite_tmp_140x556638178f90
I0831 09:07:41.283684 27067 graph_helper.h:104] adj fill_constant0x5566372dd660 -> elementwise_pow0x556636c54db0  via composite_tmp_80x5566372fb3c0
I0831 09:07:41.283696 27067 graph_helper.h:104] adj elementwise_div0x556636c53920 -> scale0x55663817e360  via composite_tmp_100x55663817e0f0
I0831 09:07:41.283700 27067 graph_helper.h:104] adj fill_constant0x5566386743a0 -> elementwise_div0x556638579a90  via composite_tmp_130x556638579880
I0831 09:07:41.283704 27067 graph_helper.h:104] adj scale0x55663817e360 -> elementwise_mul0x556638205180  via composite_tmp_110x556638204f30
I0831 09:07:41.284090 27067 graph.h:183] deleting ops
I0831 09:07:41.284126 27067 graph_helper.h:104] adj elementwise_div0x55663814b590 -> elementwise_mul0x55663860f760  via composite_tmp_140x55663860f400
I0831 09:07:41.284132 27067 graph_helper.h:104] adj scale0x5566381414e0 -> elementwise_mul0x5566385dcc40  via composite_tmp_110x5566385db470
I0831 09:07:41.284134 27067 graph_helper.h:104] adj elementwise_div0x556638167cb0 -> scale0x5566381414e0  via composite_tmp_100x556638141160
I0831 09:07:41.284138 27067 graph_helper.h:104] adj fill_constant0x5566385eacf0 -> elementwise_div0x55663814b590  via composite_tmp_130x55663814b290
I0831 09:07:41.284142 27067 graph_helper.h:104] adj elementwise_pow0x5566386450f0 -> elementwise_div0x556638167cb0  via composite_tmp_90x5566381679d0
I0831 09:07:41.284145 27067 graph_helper.h:104] adj fill_constant0x5566385a2b00 -> elementwise_pow0x5566386450f0  via composite_tmp_80x556638644e90
I0831 09:07:41.284161 27067 onednn_context.cc:101] 0x5566386479f0 0
I0831 09:07:41.284173 27067 onednn_context.cc:104] Clearing DNNL cache.
I0831 09:07:41.284176 27067 onednn_context.cc:122] Resetting Paddle data layout to NCHW.
I0831 09:07:41.284179 27067 build_strategy.cc:497] Finish Apply Pass all_reduce_mode_multi_devices_pass
I0831 09:07:41.284183 27067 build_strategy.cc:371] BuildStrategy::Apply pass:fuse_all_reduce_op_pass
I0831 09:07:41.284193 27067 build_strategy.cc:488] Start Apply Pass fuse_all_reduce_op_pass
I0831 09:07:41.284195 27067 build_strategy.cc:491] Apply Pass fuse_all_reduce_op_passto SubGraph 0
I0831 09:07:41.284206 27067 graph_helper.h:104] adj elementwise_div0x55663814b590 -> elementwise_mul0x55663860f760  via composite_tmp_140x55663860f400
I0831 09:07:41.284210 27067 graph_helper.h:104] adj scale0x5566381414e0 -> elementwise_mul0x5566385dcc40  via composite_tmp_110x5566385db470
I0831 09:07:41.284214 27067 graph_helper.h:104] adj elementwise_div0x556638167cb0 -> scale0x5566381414e0  via composite_tmp_100x556638141160
I0831 09:07:41.284217 27067 graph_helper.h:104] adj fill_constant0x5566385eacf0 -> elementwise_div0x55663814b590  via composite_tmp_130x55663814b290
I0831 09:07:41.284220 27067 graph_helper.h:104] adj elementwise_pow0x5566386450f0 -> elementwise_div0x556638167cb0  via composite_tmp_90x5566381679d0
I0831 09:07:41.284224 27067 graph_helper.h:104] adj fill_constant0x5566385a2b00 -> elementwise_pow0x5566386450f0  via composite_tmp_80x556638644e90
I0831 09:07:41.284236 27067 onednn_context.cc:101] 0x5566386479f0 0
I0831 09:07:41.284240 27067 onednn_context.cc:104] Clearing DNNL cache.
I0831 09:07:41.284242 27067 onednn_context.cc:122] Resetting Paddle data layout to NCHW.
I0831 09:07:41.284245 27067 build_strategy.cc:497] Finish Apply Pass fuse_all_reduce_op_pass
I0831 09:07:41.284248 27067 build_strategy.cc:371] BuildStrategy::Apply pass:all_reduce_deps_pass
I0831 09:07:41.284255 27067 build_strategy.cc:452] SeqOnlyAllReduceOps:0, num_trainers:1
I0831 09:07:41.284257 27067 build_strategy.cc:488] Start Apply Pass all_reduce_deps_pass
I0831 09:07:41.284260 27067 build_strategy.cc:491] Apply Pass all_reduce_deps_passto SubGraph 0
I0831 09:07:41.284282 27067 graph_helper.h:104] adj elementwise_div0x55663814b590 -> elementwise_mul0x55663860f760  via composite_tmp_140x55663860f400
I0831 09:07:41.284286 27067 graph_helper.h:104] adj scale0x5566381414e0 -> elementwise_mul0x5566385dcc40  via composite_tmp_110x5566385db470
I0831 09:07:41.284289 27067 graph_helper.h:104] adj elementwise_div0x556638167cb0 -> scale0x5566381414e0  via composite_tmp_100x556638141160
I0831 09:07:41.284298 27067 graph_helper.h:104] adj fill_constant0x5566385eacf0 -> elementwise_div0x55663814b590  via composite_tmp_130x55663814b290
I0831 09:07:41.284301 27067 graph_helper.h:104] adj elementwise_pow0x5566386450f0 -> elementwise_div0x556638167cb0  via composite_tmp_90x5566381679d0
I0831 09:07:41.284305 27067 graph_helper.h:104] adj fill_constant0x5566385a2b00 -> elementwise_pow0x5566386450f0  via composite_tmp_80x556638644e90
I0831 09:07:41.284317 27067 onednn_context.cc:101] 0x5566386479f0 0
I0831 09:07:41.284320 27067 onednn_context.cc:104] Clearing DNNL cache.
I0831 09:07:41.284323 27067 onednn_context.cc:122] Resetting Paddle data layout to NCHW.
I0831 09:07:41.284327 27067 build_strategy.cc:497] Finish Apply Pass all_reduce_deps_pass
I0831 09:07:41.284329 27067 build_strategy.cc:371] BuildStrategy::Apply pass:modify_op_lock_and_record_event_pass
I0831 09:07:41.284333 27067 build_strategy.cc:488] Start Apply Pass modify_op_lock_and_record_event_pass
I0831 09:07:41.284336 27067 build_strategy.cc:491] Apply Pass modify_op_lock_and_record_event_passto SubGraph 0
I0831 09:07:41.284351 27067 graph_helper.h:104] adj elementwise_div0x55663814b590 -> elementwise_mul0x55663860f760  via composite_tmp_140x55663860f400
I0831 09:07:41.284355 27067 graph_helper.h:104] adj scale0x5566381414e0 -> elementwise_mul0x5566385dcc40  via composite_tmp_110x5566385db470
I0831 09:07:41.284358 27067 graph_helper.h:104] adj elementwise_div0x556638167cb0 -> scale0x5566381414e0  via composite_tmp_100x556638141160
I0831 09:07:41.284363 27067 graph_helper.h:104] adj fill_constant0x5566385eacf0 -> elementwise_div0x55663814b590  via composite_tmp_130x55663814b290
I0831 09:07:41.284365 27067 graph_helper.h:104] adj elementwise_pow0x5566386450f0 -> elementwise_div0x556638167cb0  via composite_tmp_90x5566381679d0
I0831 09:07:41.284369 27067 graph_helper.h:104] adj fill_constant0x5566385a2b00 -> elementwise_pow0x5566386450f0  via composite_tmp_80x556638644e90
I0831 09:07:41.284380 27067 onednn_context.cc:101] 0x5566386479f0 0
I0831 09:07:41.284384 27067 onednn_context.cc:104] Clearing DNNL cache.
I0831 09:07:41.284386 27067 onednn_context.cc:122] Resetting Paddle data layout to NCHW.
I0831 09:07:41.284389 27067 build_strategy.cc:497] Finish Apply Pass modify_op_lock_and_record_event_pass
I0831 09:07:41.284392 27067 build_strategy.cc:371] BuildStrategy::Apply pass:multi_devices_check_pass
I0831 09:07:41.284396 27067 build_strategy.cc:488] Start Apply Pass multi_devices_check_pass
I0831 09:07:41.284399 27067 build_strategy.cc:491] Apply Pass multi_devices_check_passto SubGraph 0
I0831 09:07:41.284416 27067 graph_helper.h:104] adj elementwise_div0x55663814b590 -> elementwise_mul0x55663860f760  via composite_tmp_140x55663860f400
I0831 09:07:41.284420 27067 graph_helper.h:104] adj scale0x5566381414e0 -> elementwise_mul0x5566385dcc40  via composite_tmp_110x5566385db470
I0831 09:07:41.284423 27067 graph_helper.h:104] adj elementwise_div0x556638167cb0 -> scale0x5566381414e0  via composite_tmp_100x556638141160
I0831 09:07:41.284427 27067 graph_helper.h:104] adj fill_constant0x5566385eacf0 -> elementwise_div0x55663814b590  via composite_tmp_130x55663814b290
I0831 09:07:41.284430 27067 graph_helper.h:104] adj elementwise_pow0x5566386450f0 -> elementwise_div0x556638167cb0  via composite_tmp_90x5566381679d0
I0831 09:07:41.284435 27067 graph_helper.h:104] adj fill_constant0x5566385a2b00 -> elementwise_pow0x5566386450f0  via composite_tmp_80x556638644e90
I0831 09:07:41.284446 27067 onednn_context.cc:101] 0x5566386479f0 0
I0831 09:07:41.284448 27067 onednn_context.cc:104] Clearing DNNL cache.
I0831 09:07:41.284451 27067 onednn_context.cc:122] Resetting Paddle data layout to NCHW.
I0831 09:07:41.284454 27067 build_strategy.cc:497] Finish Apply Pass multi_devices_check_pass
I0831 09:07:41.284458 27067 build_strategy.cc:499] All Passes Applied
I0831 09:07:41.284494 27067 reference_count_pass.cc:289] Place number: 1
I0831 09:07:41.284499 27067 var_desc.cc:416] Flush  composite_tmp_14 1
I0831 09:07:41.284520 27067 var_desc.cc:416] Flush  _jst.0.args.0@GRAD 1
I0831 09:07:41.284528 27067 var_desc.cc:416] Flush  composite_tmp_13 1
I0831 09:07:41.284535 27067 var_desc.cc:416] Flush  _jst.0.args.1@GRAD 1
I0831 09:07:41.284540 27067 var_desc.cc:416] Flush  _jst.0.args.1 1
I0831 09:07:41.284547 27067 var_desc.cc:416] Flush  composite_tmp_8 1
I0831 09:07:41.284554 27067 var_desc.cc:416] Flush  elementwise_div_0@GRAD 1
I0831 09:07:41.284560 27067 var_desc.cc:416] Flush  _jst.0.args.0 1
I0831 09:07:41.284564 27067 var_desc.cc:416] Flush  composite_tmp_9 1
I0831 09:07:41.284570 27067 var_desc.cc:416] Flush  composite_tmp_10 1
I0831 09:07:41.284575 27067 var_desc.cc:416] Flush  composite_tmp_11 1
I0831 09:07:41.284582 27067 graph_helper.h:104] adj elementwise_div0x55663814b590 -> elementwise_mul0x55663860f760  via composite_tmp_140x55663860f400
I0831 09:07:41.284586 27067 graph_helper.h:104] adj scale0x5566381414e0 -> elementwise_mul0x5566385dcc40  via composite_tmp_110x5566385db470
I0831 09:07:41.284590 27067 graph_helper.h:104] adj elementwise_div0x556638167cb0 -> scale0x5566381414e0  via composite_tmp_100x556638141160
I0831 09:07:41.284593 27067 graph_helper.h:104] adj fill_constant0x5566385eacf0 -> elementwise_div0x55663814b590  via composite_tmp_130x55663814b290
I0831 09:07:41.284597 27067 graph_helper.h:104] adj elementwise_pow0x5566386450f0 -> elementwise_div0x556638167cb0  via composite_tmp_90x5566381679d0
I0831 09:07:41.284600 27067 graph_helper.h:104] adj fill_constant0x5566385a2b00 -> elementwise_pow0x5566386450f0  via composite_tmp_80x556638644e90
I0831 09:07:41.284615 27067 onednn_context.cc:101] 0x5566386479f0 0
I0831 09:07:41.284618 27067 onednn_context.cc:104] Clearing DNNL cache.
I0831 09:07:41.284683 27067 onednn_context.cc:122] Resetting Paddle data layout to NCHW.
I0831 09:07:41.284760 27067 buffer_shared_inplace_op_pass.cc:142] Inplace performed in op scale: composite_tmp_10 -> composite_tmp_11. Debug String is: Op(scale), inputs:{ScaleTensor[], X[composite_tmp_10]}, outputs:{Out[composite_tmp_11]}.. ReuseType: inplace
I0831 09:07:41.284775 27067 memory_reuse_pass.cc:63] Create 1 ShareTensorBufferOpHandles in Scope 0
I0831 09:07:41.284778 27067 graph_helper.h:104] adj elementwise_div0x556638167cb0 -> buffer_share0x5566372fb3c0  via composite_tmp_100x556638141160
I0831 09:07:41.284782 27067 graph_helper.h:104] adj elementwise_div0x55663814b590 -> elementwise_mul0x55663860f760  via composite_tmp_140x55663860f400
I0831 09:07:41.284786 27067 graph_helper.h:104] adj scale0x5566381414e0 -> elementwise_mul0x5566385dcc40  via composite_tmp_110x5566385db470
I0831 09:07:41.284790 27067 graph_helper.h:104] adj elementwise_div0x556638167cb0 -> scale0x5566381414e0  via composite_tmp_100x556638141160
I0831 09:07:41.284793 27067 graph_helper.h:104] adj buffer_share0x5566372fb3c0 -> scale0x5566381414e0  via __control_var@390x5566372dd660
I0831 09:07:41.284797 27067 graph_helper.h:104] adj fill_constant0x5566385eacf0 -> elementwise_div0x55663814b590  via composite_tmp_130x55663814b290
I0831 09:07:41.284801 27067 graph_helper.h:104] adj elementwise_pow0x5566386450f0 -> elementwise_div0x556638167cb0  via composite_tmp_90x5566381679d0
I0831 09:07:41.284804 27067 graph_helper.h:104] adj fill_constant0x5566385a2b00 -> elementwise_pow0x5566386450f0  via composite_tmp_80x556638644e90
I0831 09:07:41.284821 27067 onednn_context.cc:101] 0x5566386479f0 0
I0831 09:07:41.284824 27067 onednn_context.cc:104] Clearing DNNL cache.
I0831 09:07:41.284827 27067 onednn_context.cc:122] Resetting Paddle data layout to NCHW.
I0831 09:07:41.284830 27067 parallel_executor.cc:482] Inplace strategy is enabled, when build_strategy.enable_inplace = True
I0831 09:07:41.284864 27067 var_desc.cc:416] Flush  composite_tmp_8 0
I0831 09:07:41.284869 27067 var_desc.cc:416] Flush  _jst.0.args.1 0
I0831 09:07:41.284873 27067 var_desc.cc:416] Flush  _jst.0.args.0@GRAD 0
I0831 09:07:41.284876 27067 var_desc.cc:416] Flush  elementwise_div_0@GRAD 0
I0831 09:07:41.284880 27067 var_desc.cc:416] Flush  composite_tmp_14 0
I0831 09:07:41.284883 27067 var_desc.cc:416] Flush  composite_tmp_13 0
I0831 09:07:41.284888 27067 var_desc.cc:416] Flush  _jst.0.args.1 0
I0831 09:07:41.284895 27067 var_desc.cc:416] Flush  _jst.0.args.1@GRAD 0
I0831 09:07:41.284899 27067 var_desc.cc:416] Flush  composite_tmp_11 0
I0831 09:07:41.284902 27067 var_desc.cc:416] Flush  elementwise_div_0@GRAD 0
I0831 09:07:41.284906 27067 var_desc.cc:416] Flush  composite_tmp_10 0
I0831 09:07:41.284909 27067 var_desc.cc:416] Flush  _jst.0.args.0 0
I0831 09:07:41.284914 27067 var_desc.cc:416] Flush  composite_tmp_9 0
I0831 09:07:41.284974 27067 conditional_block_op_helper.cc:108] Found conditional_block op num: 0, conditional_block_grad op num: 0
I0831 09:07:41.284981 27067 graph_helper.h:104] adj elementwise_mul0x5566385dcc40 -> eager_deletion0x556638204f30  via __control_var@500x55663817e360
I0831 09:07:41.284986 27067 graph_helper.h:104] adj elementwise_pow0x5566386450f0 -> eager_deletion0x5566371f4440  via __control_var@410x5566371de810
I0831 09:07:41.284989 27067 graph_helper.h:104] adj elementwise_div0x556638167cb0 -> buffer_share0x5566372fb3c0  via composite_tmp_100x556638141160
I0831 09:07:41.284992 27067 graph_helper.h:104] adj elementwise_div0x55663814b590 -> elementwise_mul0x55663860f760  via composite_tmp_140x55663860f400
I0831 09:07:41.284996 27067 graph_helper.h:104] adj scale0x5566381414e0 -> elementwise_mul0x5566385dcc40  via composite_tmp_110x5566385db470
I0831 09:07:41.284999 27067 graph_helper.h:104] adj elementwise_div0x556638167cb0 -> scale0x5566381414e0  via composite_tmp_100x556638141160
I0831 09:07:41.285003 27067 graph_helper.h:104] adj buffer_share0x5566372fb3c0 -> scale0x5566381414e0  via __control_var@390x5566372dd660
I0831 09:07:41.285007 27067 graph_helper.h:104] adj elementwise_div0x55663814b590 -> eager_deletion0x556638674130  via __control_var@470x556638579a90
I0831 09:07:41.285010 27067 graph_helper.h:104] adj fill_constant0x5566385eacf0 -> elementwise_div0x55663814b590  via composite_tmp_130x55663814b290
I0831 09:07:41.285014 27067 graph_helper.h:104] adj elementwise_pow0x5566386450f0 -> elementwise_div0x556638167cb0  via composite_tmp_90x5566381679d0
I0831 09:07:41.285017 27067 graph_helper.h:104] adj fill_constant0x5566385a2b00 -> elementwise_pow0x5566386450f0  via composite_tmp_80x556638644e90
I0831 09:07:41.285022 27067 graph_helper.h:104] adj elementwise_div0x556638167cb0 -> eager_deletion0x5566381791e0  via __control_var@530x5566385e25e0
I0831 09:07:41.285024 27067 graph_helper.h:104] adj elementwise_mul0x55663860f760 -> eager_deletion0x556636c54db0  via __control_var@440x556636c53920
I0831 09:07:41.285043 27067 onednn_context.cc:101] 0x5566386479f0 0
I0831 09:07:41.285046 27067 onednn_context.cc:104] Clearing DNNL cache.
I0831 09:07:41.285049 27067 onednn_context.cc:122] Resetting Paddle data layout to NCHW.
I0831 09:07:41.285058 27067 while_op_eager_deletion_pass.cc:58] Is Paritial Program
I0831 09:07:41.285063 27067 while_op_eager_deletion_pass.cc:82] Scope Idx = 0
I0831 09:07:41.285066 27067 while_op_eager_deletion_pass.cc:84] while_ops.size() = 0
I0831 09:07:41.285069 27067 while_op_eager_deletion_pass.cc:86] while_grad_ops.size() = 0
I0831 09:07:41.285073 27067 while_op_helper.cc:154] Found while op num: 0, while grad op num: 0
I0831 09:07:41.285079 27067 graph_helper.h:104] adj elementwise_mul0x5566385dcc40 -> eager_deletion0x556638204f30  via __control_var@500x55663817e360
I0831 09:07:41.285082 27067 graph_helper.h:104] adj elementwise_pow0x5566386450f0 -> eager_deletion0x5566371f4440  via __control_var@410x5566371de810
I0831 09:07:41.285086 27067 graph_helper.h:104] adj elementwise_div0x556638167cb0 -> buffer_share0x5566372fb3c0  via composite_tmp_100x556638141160
I0831 09:07:41.285089 27067 graph_helper.h:104] adj elementwise_div0x55663814b590 -> elementwise_mul0x55663860f760  via composite_tmp_140x55663860f400
I0831 09:07:41.285094 27067 graph_helper.h:104] adj scale0x5566381414e0 -> elementwise_mul0x5566385dcc40  via composite_tmp_110x5566385db470
I0831 09:07:41.285096 27067 graph_helper.h:104] adj elementwise_div0x556638167cb0 -> scale0x5566381414e0  via composite_tmp_100x556638141160
I0831 09:07:41.285100 27067 graph_helper.h:104] adj buffer_share0x5566372fb3c0 -> scale0x5566381414e0  via __control_var@390x5566372dd660
I0831 09:07:41.285106 27067 graph_helper.h:104] adj elementwise_div0x55663814b590 -> eager_deletion0x556638674130  via __control_var@470x556638579a90
I0831 09:07:41.285110 27067 graph_helper.h:104] adj fill_constant0x5566385eacf0 -> elementwise_div0x55663814b590  via composite_tmp_130x55663814b290
I0831 09:07:41.285115 27067 graph_helper.h:104] adj elementwise_pow0x5566386450f0 -> elementwise_div0x556638167cb0  via composite_tmp_90x5566381679d0
I0831 09:07:41.285117 27067 graph_helper.h:104] adj fill_constant0x5566385a2b00 -> elementwise_pow0x5566386450f0  via composite_tmp_80x556638644e90
I0831 09:07:41.285121 27067 graph_helper.h:104] adj elementwise_div0x556638167cb0 -> eager_deletion0x5566381791e0  via __control_var@530x5566385e25e0
I0831 09:07:41.285125 27067 graph_helper.h:104] adj elementwise_mul0x55663860f760 -> eager_deletion0x556636c54db0  via __control_var@440x556636c53920
I0831 09:07:41.285140 27067 onednn_context.cc:101] 0x5566386479f0 0
I0831 09:07:41.285143 27067 onednn_context.cc:104] Clearing DNNL cache.
I0831 09:07:41.285146 27067 onednn_context.cc:122] Resetting Paddle data layout to NCHW.
I0831 09:07:41.285161 27067 recurrent_op_helper.cc:259] Found recurrent op num: 0, recurrent grad op num: 0
I0831 09:07:41.285173 27067 graph_helper.h:104] adj elementwise_mul0x5566385dcc40 -> eager_deletion0x556638204f30  via __control_var@500x55663817e360
I0831 09:07:41.285177 27067 graph_helper.h:104] adj elementwise_pow0x5566386450f0 -> eager_deletion0x5566371f4440  via __control_var@410x5566371de810
I0831 09:07:41.285181 27067 graph_helper.h:104] adj elementwise_div0x556638167cb0 -> buffer_share0x5566372fb3c0  via composite_tmp_100x556638141160
I0831 09:07:41.285184 27067 graph_helper.h:104] adj elementwise_div0x55663814b590 -> elementwise_mul0x55663860f760  via composite_tmp_140x55663860f400
I0831 09:07:41.285187 27067 graph_helper.h:104] adj scale0x5566381414e0 -> elementwise_mul0x5566385dcc40  via composite_tmp_110x5566385db470
I0831 09:07:41.285192 27067 graph_helper.h:104] adj elementwise_div0x556638167cb0 -> scale0x5566381414e0  via composite_tmp_100x556638141160
I0831 09:07:41.285194 27067 graph_helper.h:104] adj buffer_share0x5566372fb3c0 -> scale0x5566381414e0  via __control_var@390x5566372dd660
I0831 09:07:41.285198 27067 graph_helper.h:104] adj elementwise_div0x55663814b590 -> eager_deletion0x556638674130  via __control_var@470x556638579a90
I0831 09:07:41.285202 27067 graph_helper.h:104] adj fill_constant0x5566385eacf0 -> elementwise_div0x55663814b590  via composite_tmp_130x55663814b290
I0831 09:07:41.285205 27067 graph_helper.h:104] adj elementwise_pow0x5566386450f0 -> elementwise_div0x556638167cb0  via composite_tmp_90x5566381679d0
I0831 09:07:41.285208 27067 graph_helper.h:104] adj fill_constant0x5566385a2b00 -> elementwise_pow0x5566386450f0  via composite_tmp_80x556638644e90
I0831 09:07:41.285212 27067 graph_helper.h:104] adj elementwise_div0x556638167cb0 -> eager_deletion0x5566381791e0  via __control_var@530x5566385e25e0
I0831 09:07:41.285215 27067 graph_helper.h:104] adj elementwise_mul0x55663860f760 -> eager_deletion0x556636c54db0  via __control_var@440x556636c53920
I0831 09:07:41.285229 27067 onednn_context.cc:101] 0x5566386479f0 0
I0831 09:07:41.285233 27067 onednn_context.cc:104] Clearing DNNL cache.
I0831 09:07:41.285235 27067 onednn_context.cc:122] Resetting Paddle data layout to NCHW.
I0831 09:07:41.285240 27067 graph_helper.h:104] adj elementwise_mul0x5566385dcc40 -> eager_deletion0x556638204f30  via __control_var@500x55663817e360
I0831 09:07:41.285244 27067 graph_helper.h:104] adj elementwise_pow0x5566386450f0 -> eager_deletion0x5566371f4440  via __control_var@410x5566371de810
I0831 09:07:41.285248 27067 graph_helper.h:104] adj elementwise_div0x556638167cb0 -> buffer_share0x5566372fb3c0  via composite_tmp_100x556638141160
I0831 09:07:41.285251 27067 graph_helper.h:104] adj elementwise_div0x55663814b590 -> elementwise_mul0x55663860f760  via composite_tmp_140x55663860f400
I0831 09:07:41.285254 27067 graph_helper.h:104] adj scale0x5566381414e0 -> elementwise_mul0x5566385dcc40  via composite_tmp_110x5566385db470
I0831 09:07:41.285261 27067 graph_helper.h:104] adj elementwise_div0x556638167cb0 -> scale0x5566381414e0  via composite_tmp_100x556638141160
I0831 09:07:41.285264 27067 graph_helper.h:104] adj buffer_share0x5566372fb3c0 -> scale0x5566381414e0  via __control_var@390x5566372dd660
I0831 09:07:41.285269 27067 graph_helper.h:104] adj elementwise_div0x55663814b590 -> eager_deletion0x556638674130  via __control_var@470x556638579a90
I0831 09:07:41.285271 27067 graph_helper.h:104] adj fill_constant0x5566385eacf0 -> elementwise_div0x55663814b590  via composite_tmp_130x55663814b290
I0831 09:07:41.285275 27067 graph_helper.h:104] adj elementwise_pow0x5566386450f0 -> elementwise_div0x556638167cb0  via composite_tmp_90x5566381679d0
I0831 09:07:41.285279 27067 graph_helper.h:104] adj fill_constant0x5566385a2b00 -> elementwise_pow0x5566386450f0  via composite_tmp_80x556638644e90
I0831 09:07:41.285282 27067 graph_helper.h:104] adj elementwise_div0x556638167cb0 -> eager_deletion0x5566381791e0  via __control_var@530x5566385e25e0
I0831 09:07:41.285285 27067 graph_helper.h:104] adj elementwise_mul0x55663860f760 -> eager_deletion0x556636c54db0  via __control_var@440x556636c53920
I0831 09:07:41.285298 27067 onednn_context.cc:101] 0x5566386479f0 0
I0831 09:07:41.285301 27067 onednn_context.cc:104] Clearing DNNL cache.
I0831 09:07:41.285305 27067 onednn_context.cc:122] Resetting Paddle data layout to NCHW.
I0831 09:07:41.285307 27067 parallel_executor.cc:583] Garbage collection strategy is enabled, when FLAGS_eager_delete_tensor_gb = 0
I0831 09:07:41.285363 27067 parallel_executor.cc:1743] use FastThreadedSSAGraphExecutor
I0831 09:07:41.285441 27067 graph_helper.h:104] adj elementwise_mul0x5566385dcc40 -> eager_deletion0x556638204f30  via __control_var@500x55663817e360
I0831 09:07:41.285446 27067 graph_helper.h:104] adj elementwise_pow0x5566386450f0 -> eager_deletion0x5566371f4440  via __control_var@410x5566371de810
I0831 09:07:41.285450 27067 graph_helper.h:104] adj elementwise_div0x556638167cb0 -> buffer_share0x5566372fb3c0  via composite_tmp_100x556638141160
I0831 09:07:41.285454 27067 graph_helper.h:104] adj elementwise_div0x55663814b590 -> elementwise_mul0x55663860f760  via composite_tmp_140x55663860f400
I0831 09:07:41.285457 27067 graph_helper.h:104] adj scale0x5566381414e0 -> elementwise_mul0x5566385dcc40  via composite_tmp_110x5566385db470
I0831 09:07:41.285461 27067 graph_helper.h:104] adj elementwise_div0x556638167cb0 -> scale0x5566381414e0  via composite_tmp_100x556638141160
I0831 09:07:41.285465 27067 graph_helper.h:104] adj buffer_share0x5566372fb3c0 -> scale0x5566381414e0  via __control_var@390x5566372dd660
I0831 09:07:41.285468 27067 graph_helper.h:104] adj elementwise_div0x55663814b590 -> eager_deletion0x556638674130  via __control_var@470x556638579a90
I0831 09:07:41.285471 27067 graph_helper.h:104] adj fill_constant0x5566385eacf0 -> elementwise_div0x55663814b590  via composite_tmp_130x55663814b290
I0831 09:07:41.285475 27067 graph_helper.h:104] adj elementwise_pow0x5566386450f0 -> elementwise_div0x556638167cb0  via composite_tmp_90x5566381679d0
I0831 09:07:41.285478 27067 graph_helper.h:104] adj fill_constant0x5566385a2b00 -> elementwise_pow0x5566386450f0  via composite_tmp_80x556638644e90
I0831 09:07:41.285482 27067 graph_helper.h:104] adj elementwise_div0x556638167cb0 -> eager_deletion0x5566381791e0  via __control_var@530x5566385e25e0
I0831 09:07:41.285485 27067 graph_helper.h:104] adj elementwise_mul0x55663860f760 -> eager_deletion0x556636c54db0  via __control_var@440x556636c53920
I0831 09:07:41.285588 27067 parallel_executor.cc:729] use ScopeBufferedSSAGraphExecutor
I0831 09:07:41.285674 27067 scope.cc:207] Create variable composite_tmp_14
I0831 09:07:41.285703 27067 scope.cc:207] Create variable elementwise_div_0@GRAD
I0831 09:07:41.285709 27067 scope.cc:207] Create variable composite_tmp_10
I0831 09:07:41.285713 27067 scope.cc:207] Create variable _jst.0.args.1@GRAD
I0831 09:07:41.285717 27067 scope.cc:207] Create variable _jst.0.args.0
I0831 09:07:41.285730 27067 scope.cc:207] Create variable composite_tmp_11
I0831 09:07:41.285735 27067 scope.cc:207] Create variable composite_tmp_9
I0831 09:07:41.285739 27067 scope.cc:207] Create variable _jst.0.args.0@GRAD
I0831 09:07:41.285743 27067 scope.cc:207] Create variable _jst.0.args.1
I0831 09:07:41.285748 27067 scope.cc:207] Create variable composite_tmp_13
I0831 09:07:41.285750 27067 scope.cc:207] Create variable composite_tmp_8
I0831 09:07:41.285854 27067 var_desc.cc:416] Flush  composite_tmp_14 0
I0831 09:07:41.285864 27067 var_desc.cc:416] Flush  elementwise_div_0@GRAD 0
I0831 09:07:41.285869 27067 var_desc.cc:416] Flush  composite_tmp_10 0
I0831 09:07:41.285874 27067 var_desc.cc:416] Flush  _jst.0.args.1@GRAD 0
I0831 09:07:41.285878 27067 var_desc.cc:416] Flush  _jst.0.args.0 0
I0831 09:07:41.285883 27067 var_desc.cc:416] Flush  composite_tmp_11 0
I0831 09:07:41.285887 27067 var_desc.cc:416] Flush  composite_tmp_9 0
I0831 09:07:41.285892 27067 var_desc.cc:416] Flush  _jst.0.args.0@GRAD 0
I0831 09:07:41.285895 27067 var_desc.cc:416] Flush  _jst.0.args.1 0
I0831 09:07:41.285916 27067 var_desc.cc:416] Flush  composite_tmp_13 0
I0831 09:07:41.285920 27067 var_desc.cc:416] Flush  composite_tmp_8 0
I0831 09:07:41.285949 27067 graph_helper.h:104] adj elementwise_mul0x5566385dcc40 -> eager_deletion0x556638204f30  via __control_var@500x55663817e360
I0831 09:07:41.285975 27067 graph_helper.h:104] adj elementwise_pow0x5566386450f0 -> eager_deletion0x5566371f4440  via __control_var@410x5566371de810
I0831 09:07:41.285982 27067 graph_helper.h:104] adj elementwise_div0x556638167cb0 -> buffer_share0x5566372fb3c0  via composite_tmp_100x556638141160
I0831 09:07:41.285988 27067 graph_helper.h:104] adj elementwise_div0x55663814b590 -> elementwise_mul0x55663860f760  via composite_tmp_140x55663860f400
I0831 09:07:41.286005 27067 graph_helper.h:104] adj scale0x5566381414e0 -> elementwise_mul0x5566385dcc40  via composite_tmp_110x5566385db470
I0831 09:07:41.286013 27067 graph_helper.h:104] adj elementwise_div0x556638167cb0 -> scale0x5566381414e0  via composite_tmp_100x556638141160
I0831 09:07:41.286022 27067 graph_helper.h:104] adj buffer_share0x5566372fb3c0 -> scale0x5566381414e0  via __control_var@390x5566372dd660
I0831 09:07:41.286031 27067 graph_helper.h:104] adj elementwise_div0x55663814b590 -> eager_deletion0x556638674130  via __control_var@470x556638579a90
I0831 09:07:41.286037 27067 graph_helper.h:104] adj fill_constant0x5566385eacf0 -> elementwise_div0x55663814b590  via composite_tmp_130x55663814b290
I0831 09:07:41.286046 27067 graph_helper.h:104] adj elementwise_pow0x5566386450f0 -> elementwise_div0x556638167cb0  via composite_tmp_90x5566381679d0
I0831 09:07:41.286054 27067 graph_helper.h:104] adj fill_constant0x5566385a2b00 -> elementwise_pow0x5566386450f0  via composite_tmp_80x556638644e90
I0831 09:07:41.286062 27067 graph_helper.h:104] adj elementwise_div0x556638167cb0 -> eager_deletion0x5566381791e0  via __control_var@530x5566385e25e0
I0831 09:07:41.286069 27067 graph_helper.h:104] adj elementwise_mul0x55663860f760 -> eager_deletion0x556636c54db0  via __control_var@440x556636c53920
I0831 09:07:41.286183 27067 graph_helper.cc:688] convert op node to desc fill_constant
I0831 09:07:41.286190 27067 graph_helper.cc:666] 0 0
I0831 09:07:41.286206 27067 graph_helper.cc:688] convert op node to desc elementwise_pow
I0831 09:07:41.286211 27067 graph_helper.cc:666] 0 0
I0831 09:07:41.286234 27067 graph_helper.cc:688] convert op node to desc elementwise_div
I0831 09:07:41.286238 27067 graph_helper.cc:666] 0 0
I0831 09:07:41.286267 27067 graph_helper.cc:688] convert op node to desc scale
I0831 09:07:41.286271 27067 graph_helper.cc:666] 0 0
I0831 09:07:41.286283 27067 graph_helper.cc:688] convert op node to desc elementwise_mul
I0831 09:07:41.286288 27067 graph_helper.cc:666] 0 0
I0831 09:07:41.286334 27067 graph_helper.cc:688] convert op node to desc fill_constant
I0831 09:07:41.286337 27067 graph_helper.cc:666] 0 0
I0831 09:07:41.286350 27067 graph_helper.cc:688] convert op node to desc elementwise_div
I0831 09:07:41.286360 27067 graph_helper.cc:666] 0 0
I0831 09:07:41.286370 27067 graph_helper.cc:688] convert op node to desc elementwise_mul
I0831 09:07:41.286375 27067 graph_helper.cc:666] 0 0
I0831 09:07:41.286926 27067 graph_helper.cc:811] Graph to program need convert 1 sub graph
I0831 09:07:41.287251 27067 graph_helper.h:104] adj elementwise_mul0x5566385dcc40 -> eager_deletion0x556638204f30  via __control_var@500x55663817e360
I0831 09:07:41.287261 27067 graph_helper.h:104] adj elementwise_pow0x5566386450f0 -> eager_deletion0x5566371f4440  via __control_var@410x5566371de810
I0831 09:07:41.287266 27067 graph_helper.h:104] adj elementwise_div0x556638167cb0 -> buffer_share0x5566372fb3c0  via composite_tmp_100x556638141160
I0831 09:07:41.287269 27067 graph_helper.h:104] adj elementwise_div0x55663814b590 -> elementwise_mul0x55663860f760  via composite_tmp_140x55663860f400
I0831 09:07:41.287273 27067 graph_helper.h:104] adj scale0x5566381414e0 -> elementwise_mul0x5566385dcc40  via composite_tmp_110x5566385db470
I0831 09:07:41.287276 27067 graph_helper.h:104] adj elementwise_div0x556638167cb0 -> scale0x5566381414e0  via composite_tmp_100x556638141160
I0831 09:07:41.287281 27067 graph_helper.h:104] adj buffer_share0x5566372fb3c0 -> scale0x5566381414e0  via __control_var@390x5566372dd660
I0831 09:07:41.287284 27067 graph_helper.h:104] adj elementwise_div0x55663814b590 -> eager_deletion0x556638674130  via __control_var@470x556638579a90
I0831 09:07:41.287288 27067 graph_helper.h:104] adj fill_constant0x5566385eacf0 -> elementwise_div0x55663814b590  via composite_tmp_130x55663814b290
I0831 09:07:41.287292 27067 graph_helper.h:104] adj elementwise_pow0x5566386450f0 -> elementwise_div0x556638167cb0  via composite_tmp_90x5566381679d0
I0831 09:07:41.287297 27067 graph_helper.h:104] adj fill_constant0x5566385a2b00 -> elementwise_pow0x5566386450f0  via composite_tmp_80x556638644e90
I0831 09:07:41.287302 27067 graph_helper.h:104] adj elementwise_div0x556638167cb0 -> eager_deletion0x5566381791e0  via __control_var@530x5566385e25e0
I0831 09:07:41.287305 27067 graph_helper.h:104] adj elementwise_mul0x55663860f760 -> eager_deletion0x556636c54db0  via __control_var@440x556636c53920
I0831 09:07:41.287333 27067 onednn_context.cc:101] 0x5566386479f0 0
I0831 09:07:41.287338 27067 onednn_context.cc:104] Clearing DNNL cache.
I0831 09:07:41.287340 27067 onednn_context.cc:122] Resetting Paddle data layout to NCHW.
I0831 09:07:41.287983 27067 graph.h:183] deleting dep_vars
I0831 09:07:41.288024 27067 graph.h:183] deleting pass_recorder
I0831 09:07:41.288033 27067 graph.h:183] deleting skip_gc_vars
I0831 09:07:41.288038 27067 graph.h:183] deleting stale_program_op_descs
I0831 09:07:41.288043 27067 graph.h:183] deleting vars
I0831 09:07:41.288092 27067 var_handle.cc:23] deleting var handle name:composite_tmp_8, place:Place(cpu), version:0, scope_idx:0
I0831 09:07:41.288129 27067 var_handle.cc:23] deleting var handle name:composite_tmp_14, place:Place(cpu), version:0, scope_idx:0
I0831 09:07:41.288136 27067 var_handle.cc:23] deleting var handle name:_jst.0.args.0@GRAD, place:Place(cpu), version:0, scope_idx:0
I0831 09:07:41.288142 27067 var_handle.cc:23] deleting var handle name:_jst.0.args.1@GRAD, place:Place(cpu), version:0, scope_idx:0
I0831 09:07:41.288148 27067 var_handle.cc:23] deleting var handle name:elementwise_div_0@GRAD, place:Place(cpu), version:0, scope_idx:0
I0831 09:07:41.288177 27067 var_handle.cc:35] deleting dummy var handle __control_var@53
I0831 09:07:41.288198 27067 var_handle.cc:23] deleting var handle name:composite_tmp_11, place:Place(cpu), version:0, scope_idx:0
I0831 09:07:41.288218 27067 var_handle.cc:35] deleting dummy var handle __control_var@47
I0831 09:07:41.288223 27067 var_handle.cc:35] deleting dummy var handle __control_var@48
I0831 09:07:41.288228 27067 var_handle.cc:35] deleting dummy var handle __control_var@54
I0831 09:07:41.288231 27067 var_handle.cc:35] deleting dummy var handle __control_var@50
I0831 09:07:41.288235 27067 var_handle.cc:35] deleting dummy var handle __control_var@51
I0831 09:07:41.288239 27067 var_handle.cc:35] deleting dummy var handle __control_var@45
I0831 09:07:41.288271 27067 var_handle.cc:23] deleting var handle name:composite_tmp_9, place:Place(cpu), version:0, scope_idx:0
I0831 09:07:41.288277 27067 var_handle.cc:23] deleting var handle name:_jst.0.args.1, place:Place(cpu), version:0, scope_idx:0
I0831 09:07:41.288295 27067 var_handle.cc:23] deleting var handle name:composite_tmp_13, place:Place(cpu), version:0, scope_idx:0
I0831 09:07:41.288312 27067 var_handle.cc:23] deleting var handle name:composite_tmp_10, place:Place(cpu), version:0, scope_idx:0
I0831 09:07:41.288318 27067 var_handle.cc:23] deleting var handle name:_jst.0.args.0, place:Place(cpu), version:0, scope_idx:0
I0831 09:07:41.288326 27067 var_handle.cc:35] deleting dummy var handle __control_var@39
I0831 09:07:41.288331 27067 var_handle.cc:35] deleting dummy var handle __control_var@41
I0831 09:07:41.288334 27067 var_handle.cc:35] deleting dummy var handle __control_var@42
I0831 09:07:41.288339 27067 var_handle.cc:35] deleting dummy var handle __control_var@44
I0831 09:07:41.288561 27067 executor_cache.cc:160] parse op type: fill_constant
I0831 09:07:41.288574 27067 executor_cache.cc:160] parse op type: elementwise_pow
I0831 09:07:41.288579 27067 executor_cache.cc:160] parse op type: elementwise_div
I0831 09:07:41.288585 27067 executor_cache.cc:160] parse op type: scale
I0831 09:07:41.288589 27067 executor_cache.cc:160] parse op type: elementwise_mul
I0831 09:07:41.288595 27067 executor_cache.cc:160] parse op type: fill_constant
I0831 09:07:41.288599 27067 executor_cache.cc:160] parse op type: elementwise_div
I0831 09:07:41.288604 27067 executor_cache.cc:160] parse op type: elementwise_mul
I0831 09:07:41.288609 27067 executor_cache.cc:190] parse op.input: composite_tmp_14
I0831 09:07:41.288614 27067 executor_cache.cc:190] parse op.input: composite_tmp_13
I0831 09:07:41.288616 27067 executor_cache.cc:190] parse op.input: elementwise_div_0@GRAD
I0831 09:07:41.288619 27067 executor_cache.cc:192] skip eager var: elementwise_div_0@GRAD
I0831 09:07:41.288623 27067 executor_cache.cc:190] parse op.input: _jst.0.args.0
I0831 09:07:41.288626 27067 executor_cache.cc:192] skip eager var: _jst.0.args.0
I0831 09:07:41.288630 27067 executor_cache.cc:190] parse op.input: composite_tmp_8
I0831 09:07:41.288632 27067 executor_cache.cc:190] parse op.input: _jst.0.args.1
I0831 09:07:41.288635 27067 executor_cache.cc:192] skip eager var: _jst.0.args.1
I0831 09:07:41.288638 27067 executor_cache.cc:190] parse op.input: composite_tmp_9
I0831 09:07:41.288641 27067 executor_cache.cc:190] parse op.input: composite_tmp_10
I0831 09:07:41.288645 27067 executor_cache.cc:190] parse op.input: composite_tmp_11
I0831 09:07:41.288647 27067 executor_cache.cc:196] Found skip_eager_delete_vars: 3
I0831 09:07:41.289254 27067 graph.cc:144] create OpNode by elementwise_div
I0831 09:07:41.289300 27067 graph.cc:219] kStaleProgramOpDescs.size: 1
I0831 09:07:41.289515 27067 parallel_executor.cc:1323] The Program will be executed on CPU using ParallelExecutor, 1 cards are used, so 1 programs are executed in parallel.
I0831 09:07:41.289532 27067 build_strategy.cc:360] apply all passes
I0831 09:07:41.289541 27067 pass_builder.cc:29] Append fuse_bn_add_act_pass
I0831 09:07:41.289549 27067 pass_builder.cc:29] Append coalesce_grad_tensor_pass
I0831 09:07:41.289556 27067 pass_builder.cc:29] Append add_reader_dependency_pass
I0831 09:07:41.289561 27067 pass_builder.cc:29] Append all_reduce_mode_multi_devices_pass
I0831 09:07:41.289572 27067 pass_builder.cc:29] Append fuse_all_reduce_op_pass
I0831 09:07:41.289577 27067 pass_builder.cc:29] Append all_reduce_deps_pass
I0831 09:07:41.289585 27067 pass_builder.cc:29] Append modify_op_lock_and_record_event_pass
I0831 09:07:41.289590 27067 pass_builder.cc:29] Append multi_devices_check_pass
I0831 09:07:41.289597 27067 build_strategy.cc:247] CollectiveContext:endpoints_:trainer_id_:0
I0831 09:07:41.289602 27067 build_strategy.cc:371] BuildStrategy::Apply pass:fuse_bn_add_act_pass
I0831 09:07:41.289608 27067 build_strategy.cc:474] fuse_bn_add_act_pass is only supported on GPU, skipped.
I0831 09:07:41.289619 27067 build_strategy.cc:371] BuildStrategy::Apply pass:coalesce_grad_tensor_pass
I0831 09:07:41.289626 27067 build_strategy.cc:488] Start Apply Pass coalesce_grad_tensor_pass
I0831 09:07:41.289629 27067 build_strategy.cc:491] Apply Pass coalesce_grad_tensor_passto SubGraph 0
I0831 09:07:41.289657 27067 onednn_context.cc:101] 0x5566386479f0 0
I0831 09:07:41.289664 27067 onednn_context.cc:104] Clearing DNNL cache.
I0831 09:07:41.289667 27067 onednn_context.cc:122] Resetting Paddle data layout to NCHW.
I0831 09:07:41.289671 27067 build_strategy.cc:497] Finish Apply Pass coalesce_grad_tensor_pass
I0831 09:07:41.289674 27067 build_strategy.cc:371] BuildStrategy::Apply pass:add_reader_dependency_pass
I0831 09:07:41.289678 27067 build_strategy.cc:488] Start Apply Pass add_reader_dependency_pass
I0831 09:07:41.289681 27067 build_strategy.cc:491] Apply Pass add_reader_dependency_passto SubGraph 0
I0831 09:07:41.289690 27067 onednn_context.cc:101] 0x5566386479f0 0
I0831 09:07:41.289692 27067 onednn_context.cc:104] Clearing DNNL cache.
I0831 09:07:41.289695 27067 onednn_context.cc:122] Resetting Paddle data layout to NCHW.
I0831 09:07:41.289698 27067 build_strategy.cc:497] Finish Apply Pass add_reader_dependency_pass
I0831 09:07:41.289701 27067 build_strategy.cc:371] BuildStrategy::Apply pass:all_reduce_mode_multi_devices_pass
I0831 09:07:41.289710 27067 build_strategy.cc:488] Start Apply Pass all_reduce_mode_multi_devices_pass
I0831 09:07:41.289713 27067 build_strategy.cc:491] Apply Pass all_reduce_mode_multi_devices_passto SubGraph 0
I0831 09:07:41.289821 27067 graph.h:183] deleting ops
I0831 09:07:41.289839 27067 onednn_context.cc:101] 0x5566386479f0 0
I0831 09:07:41.289842 27067 onednn_context.cc:104] Clearing DNNL cache.
I0831 09:07:41.289845 27067 onednn_context.cc:122] Resetting Paddle data layout to NCHW.
I0831 09:07:41.289848 27067 build_strategy.cc:497] Finish Apply Pass all_reduce_mode_multi_devices_pass
I0831 09:07:41.289852 27067 build_strategy.cc:371] BuildStrategy::Apply pass:fuse_all_reduce_op_pass
I0831 09:07:41.289860 27067 build_strategy.cc:488] Start Apply Pass fuse_all_reduce_op_pass
I0831 09:07:41.289863 27067 build_strategy.cc:491] Apply Pass fuse_all_reduce_op_passto SubGraph 0
I0831 09:07:41.289870 27067 onednn_context.cc:101] 0x5566386479f0 0
I0831 09:07:41.289873 27067 onednn_context.cc:104] Clearing DNNL cache.
I0831 09:07:41.289876 27067 onednn_context.cc:122] Resetting Paddle data layout to NCHW.
I0831 09:07:41.289880 27067 build_strategy.cc:497] Finish Apply Pass fuse_all_reduce_op_pass
I0831 09:07:41.289882 27067 build_strategy.cc:371] BuildStrategy::Apply pass:all_reduce_deps_pass
I0831 09:07:41.289887 27067 build_strategy.cc:452] SeqOnlyAllReduceOps:0, num_trainers:1
I0831 09:07:41.289891 27067 build_strategy.cc:488] Start Apply Pass all_reduce_deps_pass
I0831 09:07:41.289894 27067 build_strategy.cc:491] Apply Pass all_reduce_deps_passto SubGraph 0
I0831 09:07:41.289908 27067 onednn_context.cc:101] 0x5566386479f0 0
I0831 09:07:41.289912 27067 onednn_context.cc:104] Clearing DNNL cache.
I0831 09:07:41.289914 27067 onednn_context.cc:122] Resetting Paddle data layout to NCHW.
I0831 09:07:41.289918 27067 build_strategy.cc:497] Finish Apply Pass all_reduce_deps_pass
I0831 09:07:41.289921 27067 build_strategy.cc:371] BuildStrategy::Apply pass:modify_op_lock_and_record_event_pass
I0831 09:07:41.289925 27067 build_strategy.cc:488] Start Apply Pass modify_op_lock_and_record_event_pass
I0831 09:07:41.289927 27067 build_strategy.cc:491] Apply Pass modify_op_lock_and_record_event_passto SubGraph 0
I0831 09:07:41.289938 27067 onednn_context.cc:101] 0x5566386479f0 0
I0831 09:07:41.289942 27067 onednn_context.cc:104] Clearing DNNL cache.
I0831 09:07:41.289944 27067 onednn_context.cc:122] Resetting Paddle data layout to NCHW.
I0831 09:07:41.289948 27067 build_strategy.cc:497] Finish Apply Pass modify_op_lock_and_record_event_pass
I0831 09:07:41.289950 27067 build_strategy.cc:371] BuildStrategy::Apply pass:multi_devices_check_pass
I0831 09:07:41.289954 27067 build_strategy.cc:488] Start Apply Pass multi_devices_check_pass
I0831 09:07:41.289963 27067 build_strategy.cc:491] Apply Pass multi_devices_check_passto SubGraph 0
I0831 09:07:41.289975 27067 onednn_context.cc:101] 0x5566386479f0 0
I0831 09:07:41.289979 27067 onednn_context.cc:104] Clearing DNNL cache.
I0831 09:07:41.289981 27067 onednn_context.cc:122] Resetting Paddle data layout to NCHW.
I0831 09:07:41.289985 27067 build_strategy.cc:497] Finish Apply Pass multi_devices_check_pass
I0831 09:07:41.289988 27067 build_strategy.cc:499] All Passes Applied
I0831 09:07:41.290004 27067 reference_count_pass.cc:289] Place number: 1
I0831 09:07:41.290010 27067 var_desc.cc:416] Flush  elementwise_div_0 1
I0831 09:07:41.290022 27067 var_desc.cc:416] Flush  _jst.0.args.0 1
I0831 09:07:41.290027 27067 var_desc.cc:416] Flush  _jst.0.args.1 1
I0831 09:07:41.290036 27067 onednn_context.cc:101] 0x5566386479f0 0
I0831 09:07:41.290040 27067 onednn_context.cc:104] Clearing DNNL cache.
I0831 09:07:41.290043 27067 onednn_context.cc:122] Resetting Paddle data layout to NCHW.
I0831 09:07:41.290063 27067 onednn_context.cc:101] 0x5566386479f0 0
I0831 09:07:41.290067 27067 onednn_context.cc:104] Clearing DNNL cache.
I0831 09:07:41.290069 27067 onednn_context.cc:122] Resetting Paddle data layout to NCHW.
I0831 09:07:41.290072 27067 parallel_executor.cc:482] Inplace strategy is enabled, when build_strategy.enable_inplace = True
I0831 09:07:41.290096 27067 var_desc.cc:416] Flush  _jst.0.args.0 0
I0831 09:07:41.290100 27067 var_desc.cc:416] Flush  _jst.0.args.1 0
I0831 09:07:41.290103 27067 var_desc.cc:416] Flush  elementwise_div_0 0
I0831 09:07:41.290132 27067 conditional_block_op_helper.cc:108] Found conditional_block op num: 0, conditional_block_grad op num: 0
I0831 09:07:41.290138 27067 graph_helper.h:104] adj elementwise_div0x556638644e90 -> eager_deletion0x55663860f760  via __control_var@90x55663860f400
I0831 09:07:41.290148 27067 onednn_context.cc:101] 0x5566386479f0 0
I0831 09:07:41.290151 27067 onednn_context.cc:104] Clearing DNNL cache.
I0831 09:07:41.290154 27067 onednn_context.cc:122] Resetting Paddle data layout to NCHW.
I0831 09:07:41.290160 27067 while_op_eager_deletion_pass.cc:58] Is Paritial Program
I0831 09:07:41.290174 27067 while_op_eager_deletion_pass.cc:82] Scope Idx = 0
I0831 09:07:41.290179 27067 while_op_eager_deletion_pass.cc:84] while_ops.size() = 0
I0831 09:07:41.290181 27067 while_op_eager_deletion_pass.cc:86] while_grad_ops.size() = 0
I0831 09:07:41.290185 27067 while_op_helper.cc:154] Found while op num: 0, while grad op num: 0
I0831 09:07:41.290190 27067 graph_helper.h:104] adj elementwise_div0x556638644e90 -> eager_deletion0x55663860f760  via __control_var@90x55663860f400
I0831 09:07:41.290197 27067 onednn_context.cc:101] 0x5566386479f0 0
I0831 09:07:41.290201 27067 onednn_context.cc:104] Clearing DNNL cache.
I0831 09:07:41.290203 27067 onednn_context.cc:122] Resetting Paddle data layout to NCHW.
I0831 09:07:41.290215 27067 recurrent_op_helper.cc:259] Found recurrent op num: 0, recurrent grad op num: 0
I0831 09:07:41.290220 27067 graph_helper.h:104] adj elementwise_div0x556638644e90 -> eager_deletion0x55663860f760  via __control_var@90x55663860f400
I0831 09:07:41.290226 27067 onednn_context.cc:101] 0x5566386479f0 0
I0831 09:07:41.290230 27067 onednn_context.cc:104] Clearing DNNL cache.
I0831 09:07:41.290233 27067 onednn_context.cc:122] Resetting Paddle data layout to NCHW.
I0831 09:07:41.290237 27067 graph_helper.h:104] adj elementwise_div0x556638644e90 -> eager_deletion0x55663860f760  via __control_var@90x55663860f400
I0831 09:07:41.290244 27067 onednn_context.cc:101] 0x5566386479f0 0
I0831 09:07:41.290247 27067 onednn_context.cc:104] Clearing DNNL cache.
I0831 09:07:41.290251 27067 onednn_context.cc:122] Resetting Paddle data layout to NCHW.
I0831 09:07:41.290253 27067 parallel_executor.cc:583] Garbage collection strategy is enabled, when FLAGS_eager_delete_tensor_gb = 0
I0831 09:07:41.290287 27067 parallel_executor.cc:1743] use FastThreadedSSAGraphExecutor
I0831 09:07:41.290338 27067 graph_helper.h:104] adj elementwise_div0x556638644e90 -> eager_deletion0x55663860f760  via __control_var@90x55663860f400
I0831 09:07:41.290354 27067 graph_helper.h:104] adj elementwise_div0x556638644e90 -> eager_deletion0x55663860f760  via __control_var@90x55663860f400
I0831 09:07:41.290414 27067 parallel_executor.cc:729] use ScopeBufferedSSAGraphExecutor
I0831 09:07:41.290426 27067 scope.cc:207] Create variable _jst.0.args.0
I0831 09:07:41.290431 27067 scope.cc:207] Create variable _jst.0.args.1
I0831 09:07:41.290436 27067 scope.cc:207] Create variable elementwise_div_0
I0831 09:07:41.290495 27067 var_desc.cc:416] Flush  _jst.0.args.0 0
I0831 09:07:41.290503 27067 var_desc.cc:416] Flush  _jst.0.args.1 0
I0831 09:07:41.290508 27067 var_desc.cc:416] Flush  elementwise_div_0 0
I0831 09:07:41.290524 27067 graph_helper.h:104] adj elementwise_div0x556638644e90 -> eager_deletion0x55663860f760  via __control_var@90x55663860f400
I0831 09:07:41.290560 27067 graph_helper.cc:688] convert op node to desc elementwise_div
I0831 09:07:41.290565 27067 graph_helper.cc:666] 0 0
I0831 09:07:41.290660 27067 graph_helper.cc:811] Graph to program need convert 1 sub graph
I0831 09:07:41.290724 27067 graph_helper.h:104] adj elementwise_div0x556638644e90 -> eager_deletion0x55663860f760  via __control_var@90x55663860f400
I0831 09:07:41.290735 27067 onednn_context.cc:101] 0x5566386479f0 0
I0831 09:07:41.290737 27067 onednn_context.cc:104] Clearing DNNL cache.
I0831 09:07:41.290740 27067 onednn_context.cc:122] Resetting Paddle data layout to NCHW.
I0831 09:07:41.291090 27067 graph.h:183] deleting dep_vars
I0831 09:07:41.291100 27067 graph.h:183] deleting pass_recorder
I0831 09:07:41.291105 27067 graph.h:183] deleting skip_gc_vars
I0831 09:07:41.291108 27067 graph.h:183] deleting stale_program_op_descs
I0831 09:07:41.291112 27067 graph.h:183] deleting vars
I0831 09:07:41.291117 27067 var_handle.cc:23] deleting var handle name:_jst.0.args.1, place:Place(cpu), version:0, scope_idx:0
I0831 09:07:41.291127 27067 var_handle.cc:23] deleting var handle name:_jst.0.args.0, place:Place(cpu), version:0, scope_idx:0
I0831 09:07:41.291144 27067 var_handle.cc:35] deleting dummy var handle __control_var@9
I0831 09:07:41.291149 27067 var_handle.cc:35] deleting dummy var handle __control_var@10
I0831 09:07:41.291152 27067 var_handle.cc:23] deleting var handle name:elementwise_div_0, place:Place(cpu), version:0, scope_idx:0
I0831 09:07:41.291316 27067 executor_cache.cc:160] parse op type: fill_constant
I0831 09:07:41.291326 27067 executor_cache.cc:160] parse op type: elementwise_pow
I0831 09:07:41.291330 27067 executor_cache.cc:160] parse op type: elementwise_div
I0831 09:07:41.291335 27067 executor_cache.cc:160] parse op type: scale
I0831 09:07:41.291342 27067 executor_cache.cc:160] parse op type: elementwise_mul
I0831 09:07:41.291347 27067 executor_cache.cc:160] parse op type: fill_constant
I0831 09:07:41.291350 27067 executor_cache.cc:160] parse op type: elementwise_div
I0831 09:07:41.291355 27067 executor_cache.cc:160] parse op type: elementwise_mul
I0831 09:07:41.291359 27067 executor_cache.cc:190] parse op.input: composite_tmp_14
I0831 09:07:41.291363 27067 executor_cache.cc:190] parse op.input: composite_tmp_13
I0831 09:07:41.291366 27067 executor_cache.cc:190] parse op.input: elementwise_div_0@GRAD
I0831 09:07:41.291369 27067 executor_cache.cc:192] skip eager var: elementwise_div_0@GRAD
I0831 09:07:41.291373 27067 executor_cache.cc:190] parse op.input: _jst.0.args.0
I0831 09:07:41.291375 27067 executor_cache.cc:192] skip eager var: _jst.0.args.0
I0831 09:07:41.291378 27067 executor_cache.cc:190] parse op.input: composite_tmp_8
I0831 09:07:41.291381 27067 executor_cache.cc:190] parse op.input: _jst.0.args.1
I0831 09:07:41.291384 27067 executor_cache.cc:192] skip eager var: _jst.0.args.1
I0831 09:07:41.291388 27067 executor_cache.cc:190] parse op.input: composite_tmp_9
I0831 09:07:41.291390 27067 executor_cache.cc:190] parse op.input: composite_tmp_10
I0831 09:07:41.291393 27067 executor_cache.cc:190] parse op.input: composite_tmp_11
I0831 09:07:41.291396 27067 executor_cache.cc:196] Found skip_eager_delete_vars: 3
I0831 09:07:41.291710 27067 block_desc.cc:200] vars in desc 3
I0831 09:07:41.291721 27067 block_desc.cc:204] Flush _jst.0.args.0
I0831 09:07:41.291725 27067 var_desc.cc:416] Flush  _jst.0.args.0 1
I0831 09:07:41.291730 27067 block_desc.cc:204] Flush _jst.0.args.1
I0831 09:07:41.291733 27067 var_desc.cc:416] Flush  _jst.0.args.1 1
I0831 09:07:41.291736 27067 block_desc.cc:204] Flush elementwise_div_0
I0831 09:07:41.291739 27067 var_desc.cc:416] Flush  elementwise_div_0 1
I0831 09:07:41.292120 27067 block_desc.cc:200] vars in desc 3
I0831 09:07:41.292130 27067 block_desc.cc:204] Flush _jst.0.args.0
I0831 09:07:41.292133 27067 var_desc.cc:416] Flush  _jst.0.args.0 1
I0831 09:07:41.292137 27067 block_desc.cc:204] Flush _jst.0.args.1
I0831 09:07:41.292140 27067 var_desc.cc:416] Flush  _jst.0.args.1 1
I0831 09:07:41.292143 27067 block_desc.cc:204] Flush elementwise_div_0
I0831 09:07:41.292146 27067 var_desc.cc:416] Flush  elementwise_div_0 1
I0831 09:07:41.292161 27067 block_desc.cc:200] vars in desc 3
I0831 09:07:41.292320 27067 block_desc.cc:200] vars in desc 0
I0831 09:07:41.292326 27067 block_desc.cc:200] vars in desc 0
I0831 09:07:41.293387 27067 var_desc.cc:416] Flush  composite_tmp_8 1
I0831 09:07:41.293406 27067 var_desc.cc:416] Flush  composite_tmp_8 0
I0831 09:07:41.293411 27067 var_desc.cc:416] Flush  composite_tmp_9 1
I0831 09:07:41.293414 27067 var_desc.cc:416] Flush  composite_tmp_9 0
I0831 09:07:41.293417 27067 var_desc.cc:416] Flush  composite_tmp_10 1
I0831 09:07:41.293421 27067 var_desc.cc:416] Flush  composite_tmp_10 0
I0831 09:07:41.293426 27067 var_desc.cc:416] Flush  composite_tmp_11 1
I0831 09:07:41.293428 27067 var_desc.cc:416] Flush  composite_tmp_11 0
I0831 09:07:41.293432 27067 var_desc.cc:416] Flush  elementwise_div_0@GRAD 1
I0831 09:07:41.293435 27067 var_desc.cc:416] Flush  _jst.0.args.1@GRAD 1
I0831 09:07:41.293439 27067 var_desc.cc:416] Flush  composite_tmp_13 1
I0831 09:07:41.293443 27067 var_desc.cc:416] Flush  composite_tmp_13 0
I0831 09:07:41.293447 27067 var_desc.cc:416] Flush  composite_tmp_14 1
I0831 09:07:41.293449 27067 var_desc.cc:416] Flush  composite_tmp_14 0
I0831 09:07:41.293453 27067 var_desc.cc:416] Flush  elementwise_div_0@GRAD 0
I0831 09:07:41.293457 27067 var_desc.cc:416] Flush  _jst.0.args.0@GRAD 1
I0831 09:07:41.294067 27067 block_desc.cc:200] vars in desc 11
I0831 09:07:41.294075 27067 block_desc.cc:204] Flush composite_tmp_14
I0831 09:07:41.294080 27067 var_desc.cc:416] Flush  composite_tmp_14 1
I0831 09:07:41.294083 27067 block_desc.cc:204] Flush elementwise_div_0@GRAD
I0831 09:07:41.294087 27067 var_desc.cc:416] Flush  elementwise_div_0@GRAD 1
I0831 09:07:41.294090 27067 block_desc.cc:204] Flush composite_tmp_10
I0831 09:07:41.294093 27067 var_desc.cc:416] Flush  composite_tmp_10 1
I0831 09:07:41.294097 27067 block_desc.cc:204] Flush _jst.0.args.1@GRAD
I0831 09:07:41.294100 27067 var_desc.cc:416] Flush  _jst.0.args.1@GRAD 1
I0831 09:07:41.294103 27067 block_desc.cc:204] Flush _jst.0.args.0
I0831 09:07:41.294106 27067 var_desc.cc:416] Flush  _jst.0.args.0 1
I0831 09:07:41.294111 27067 block_desc.cc:204] Flush composite_tmp_11
I0831 09:07:41.294113 27067 var_desc.cc:416] Flush  composite_tmp_11 1
I0831 09:07:41.294116 27067 block_desc.cc:204] Flush composite_tmp_9
I0831 09:07:41.294119 27067 var_desc.cc:416] Flush  composite_tmp_9 1
I0831 09:07:41.294123 27067 block_desc.cc:204] Flush _jst.0.args.0@GRAD
I0831 09:07:41.294126 27067 var_desc.cc:416] Flush  _jst.0.args.0@GRAD 1
I0831 09:07:41.294129 27067 block_desc.cc:204] Flush _jst.0.args.1
I0831 09:07:41.294132 27067 var_desc.cc:416] Flush  _jst.0.args.1 1
I0831 09:07:41.294137 27067 block_desc.cc:204] Flush composite_tmp_13
I0831 09:07:41.294139 27067 var_desc.cc:416] Flush  composite_tmp_13 1
I0831 09:07:41.294143 27067 block_desc.cc:204] Flush composite_tmp_8
I0831 09:07:41.294147 27067 var_desc.cc:416] Flush  composite_tmp_8 1
I0831 09:07:41.295603 27067 block_desc.cc:200] vars in desc 11
I0831 09:07:41.295619 27067 block_desc.cc:204] Flush composite_tmp_14
I0831 09:07:41.295624 27067 var_desc.cc:416] Flush  composite_tmp_14 1
I0831 09:07:41.295637 27067 block_desc.cc:204] Flush elementwise_div_0@GRAD
I0831 09:07:41.295640 27067 var_desc.cc:416] Flush  elementwise_div_0@GRAD 1
I0831 09:07:41.295644 27067 block_desc.cc:204] Flush composite_tmp_10
I0831 09:07:41.295647 27067 var_desc.cc:416] Flush  composite_tmp_10 1
I0831 09:07:41.295650 27067 block_desc.cc:204] Flush _jst.0.args.1@GRAD
I0831 09:07:41.295653 27067 var_desc.cc:416] Flush  _jst.0.args.1@GRAD 1
I0831 09:07:41.295657 27067 block_desc.cc:204] Flush _jst.0.args.0
I0831 09:07:41.295660 27067 var_desc.cc:416] Flush  _jst.0.args.0 1
I0831 09:07:41.295663 27067 block_desc.cc:204] Flush composite_tmp_11
I0831 09:07:41.295666 27067 var_desc.cc:416] Flush  composite_tmp_11 1
I0831 09:07:41.295670 27067 block_desc.cc:204] Flush composite_tmp_9
I0831 09:07:41.295673 27067 var_desc.cc:416] Flush  composite_tmp_9 1
I0831 09:07:41.295677 27067 block_desc.cc:204] Flush _jst.0.args.0@GRAD
I0831 09:07:41.295680 27067 var_desc.cc:416] Flush  _jst.0.args.0@GRAD 1
I0831 09:07:41.295684 27067 block_desc.cc:204] Flush _jst.0.args.1
I0831 09:07:41.295687 27067 var_desc.cc:416] Flush  _jst.0.args.1 1
I0831 09:07:41.295691 27067 block_desc.cc:204] Flush composite_tmp_13
I0831 09:07:41.295693 27067 var_desc.cc:416] Flush  composite_tmp_13 1
I0831 09:07:41.295697 27067 block_desc.cc:204] Flush composite_tmp_8
I0831 09:07:41.295701 27067 var_desc.cc:416] Flush  composite_tmp_8 1
I0831 09:07:41.295804 27067 block_desc.cc:200] vars in desc 11
I0831 09:07:41.296244 27067 block_desc.cc:200] vars in desc 0
I0831 09:07:41.296252 27067 block_desc.cc:200] vars in desc 0
I0831 09:07:41.296787 27067 eager.cc:112] Tensor(elementwise_div_0) have not GradNode, add GradNodeAccumulation0x5566381b3740 for it.
I0831 09:07:41.296993 27067 run_program_op_func.h:113] start run run_program with require_any_grad = 1
I0831 09:07:41.297001 27067 run_program_op_node.h:322] RunProgramOpKernel Compute
I0831 09:07:41.297006 27067 run_program_op_node.h:342] RunProgramOp use interpretercore to execute program.
I0831 09:07:41.297010 27067 run_program_op_node.h:346] global_inner_scope:0x556636a2e190
I0831 09:07:41.297020 27067 run_program_op_node.h:396] No interpretercore cahce, so create a new interpretercore for program: -7991629131379433153
I0831 09:07:41.297027 27067 scope.cc:207] Create variable _jst.0.args.0
I0831 09:07:41.297040 27067 scope.cc:207] Create variable _jst.0.args.1
I0831 09:07:41.297048 27067 interpretercore.cc:45] InterpreterCore(): 0x55663864fcf0 on Place(cpu)
I0831 09:07:41.297192 27067 program_interpreter.cc:49] ProgramInterpreter(): 0x5566386ae7e0 on Place(cpu)
I0831 09:07:41.297209 27067 execution_config.cc:106] place:Place(cpu), processor_count:0, device_count:0, serial_run:0, num_host_threads:4, num_device_threads:0
I0831 09:07:41.297219 27067 new_executor_defs.cc:49] Set local scope: 0
I0831 09:07:41.297225 27067 executor_cache.cc:160] parse op type: fill_constant
I0831 09:07:41.297232 27067 executor_cache.cc:160] parse op type: elementwise_pow
I0831 09:07:41.297236 27067 executor_cache.cc:160] parse op type: elementwise_div
I0831 09:07:41.297241 27067 executor_cache.cc:160] parse op type: share_buffer
I0831 09:07:41.297245 27067 executor_cache.cc:162] skip share_buffer op
I0831 09:07:41.297247 27067 executor_cache.cc:160] parse op type: scale
I0831 09:07:41.297251 27067 executor_cache.cc:160] parse op type: elementwise_mul
I0831 09:07:41.297255 27067 executor_cache.cc:160] parse op type: fill_constant
I0831 09:07:41.297259 27067 executor_cache.cc:160] parse op type: elementwise_div
I0831 09:07:41.297264 27067 executor_cache.cc:160] parse op type: elementwise_mul
I0831 09:07:41.297268 27067 executor_cache.cc:190] parse op.input: composite_tmp_14
I0831 09:07:41.297272 27067 executor_cache.cc:190] parse op.input: composite_tmp_13
I0831 09:07:41.297274 27067 executor_cache.cc:190] parse op.input: elementwise_div_0@GRAD
I0831 09:07:41.297277 27067 executor_cache.cc:192] skip eager var: elementwise_div_0@GRAD
I0831 09:07:41.297281 27067 executor_cache.cc:190] parse op.input: _jst.0.args.0
I0831 09:07:41.297291 27067 executor_cache.cc:192] skip eager var: _jst.0.args.0
I0831 09:07:41.297295 27067 executor_cache.cc:190] parse op.input: composite_tmp_8
I0831 09:07:41.297298 27067 executor_cache.cc:190] parse op.input: _jst.0.args.1
I0831 09:07:41.297302 27067 executor_cache.cc:192] skip eager var: _jst.0.args.1
I0831 09:07:41.297304 27067 executor_cache.cc:190] parse op.input: composite_tmp_9
I0831 09:07:41.297307 27067 executor_cache.cc:190] parse op.input: composite_tmp_10
I0831 09:07:41.297309 27067 executor_cache.cc:190] parse op.input: composite_tmp_11
I0831 09:07:41.297312 27067 executor_cache.cc:196] Found skip_eager_delete_vars: 3
I0831 09:07:41.297324 27067 run_program_op_node.h:450] Get skip GC vars size is: 4
I0831 09:07:41.297335 27067 interpreter_util.cc:1105] Creating Variables
I0831 09:07:41.297341 27067 interpreter_util.cc:1142] Create Variable _jst.0.args.0 locally, which pointer is 0x5566385a21d0 type is 7
I0831 09:07:41.297350 27067 interpreter_util.cc:1142] Create Variable _jst.0.args.1 locally, which pointer is 0x55663863cca0 type is 7
I0831 09:07:41.297355 27067 scope.cc:207] Create variable elementwise_div_0
I0831 09:07:41.297359 27067 interpreter_util.cc:1142] Create Variable elementwise_div_0 locally, which pointer is 0x556638674e40 type is 7
I0831 09:07:41.297395 27067 interpreter_util.cc:567] Static build: 0
I0831 09:07:41.297400 27067 mkldnn_helper.h:90] RegisterModelLayout for mkldnn
I0831 09:07:41.297407 27067 var_desc.cc:416] Flush  _jst.0.args.0 1
I0831 09:07:41.297412 27067 var_desc.cc:416] Flush  _jst.0.args.1 1
I0831 09:07:41.297416 27067 var_desc.cc:416] Flush  elementwise_div_0 1
I0831 09:07:41.297421 27067 interpreter_util.cc:281] elementwise_div elementwise_div_0
I0831 09:07:41.297425 27067 interpreter_util.cc:281] elementwise_div _jst.0.args.0
I0831 09:07:41.297427 27067 interpreter_util.cc:281] elementwise_div _jst.0.args.1
I0831 09:07:41.297430 27067 interpreter_util.cc:283] gc map size:1
I0831 09:07:41.297456 27067 interpreter_util.cc:677] Start run Place(cpu) Op(elementwise_div), inputs:{X[_jst.0.args.0:double[13, 17]({})(Place(cpu))], Y[_jst.0.args.1:double[13, 17]({})(Place(cpu))]}, outputs:{Out[elementwise_div_0:[]({})()]}.
I0831 09:07:41.297479 27067 interpreter_util.cc:687] OP is not null
I0831 09:07:41.297483 27067 interpreter_util.cc:690] get op_with_kernel
I0831 09:07:41.297487 27067 interpreter_util.cc:695] get RuntimeContext
I0831 09:07:41.297502 27067 interpreter_util.cc:724] expected_kernel_key : {data_type[double]; data_layout[Undefined(AnyLayout)]; place[Place(cpu)]; library_type[PLAIN]}
I0831 09:07:41.297523 27067 operator.cc:2207] op type:elementwise_div, expected_kernel_key:{data_type[double]; data_layout[Undefined(AnyLayout)]; place[Place(cpu)]; library_type[PLAIN]}
I0831 09:07:41.297530 27067 interpreter_util.cc:772] if run phi kernel? : 1
I0831 09:07:41.297534 27067 interpreter_util.cc:787] elementwise_div : finally selected kernel_key: {data_type[double]; data_layout[Undefined(AnyLayout)]; place[Place(cpu)]; library_type[PLAIN]}
I0831 09:07:41.297552 27067 interpreter_util.cc:802] apply data transform done. 
I0831 09:07:41.297556 27067 interpreter_util.cc:806] infer shape
I0831 09:07:41.297572 27067 operator.cc:3182] Done inputs
I0831 09:07:41.297577 27067 operator.cc:3244] Done outputs
I0831 09:07:41.297580 27067 operator.cc:3496] Done attributes
I0831 09:07:41.297586 27067 operator.cc:3031] Runtime attr `Scale_x` is passed to OneDNNContext.
I0831 09:07:41.297592 27067 operator.cc:3031] Runtime attr `mkldnn_data_type` is passed to OneDNNContext.
I0831 09:07:41.297597 27067 operator.cc:3031] Runtime attr `Scale_y` is passed to OneDNNContext.
I0831 09:07:41.297600 27067 operator.cc:3031] Runtime attr `Scale_out` is passed to OneDNNContext.
I0831 09:07:41.297605 27067 operator.cc:3546] Done runtime attributes
I0831 09:07:41.297608 27067 operator.cc:3576] Done runtime extra inputs
I0831 09:07:41.297641 27067 interpreter_util.cc:961] End run Place(cpu) Op(elementwise_div), inputs:{X[_jst.0.args.0:double[13, 17]({})(Place(cpu))], Y[_jst.0.args.1:double[13, 17]({})(Place(cpu))]}, outputs:{Out[elementwise_div_0:double[13, 17]({})(Place(cpu))]}.
I0831 09:07:41.297739 27067 program_interpreter.cc:1475] Analyze the execution order of Trace scheduling mode.
I0831 09:07:41.297749 27067 program_interpreter.cc:1465] Update sync op num, sync op num is: 1
I0831 09:07:41.297757 27067 run_program_op_node.h:246] share elementwise_div_0 from scope
I0831 09:07:41.297763 27067 run_program_op_node.h:492] 0x556636a2e190 
------------------------------------------

Details:

====
0x556636a2e190:
  - elementwise_div_0
  - _jst.0.args.0
  - _jst.0.args.1
I0831 09:07:41.297775 27067 run_program_op_node.h:501] not test, set this scope can not reused
I0831 09:07:41.297778 27067 run_program_op_func.h:118] start run run_program grad
I0831 09:07:41.297793 27067 grad_node_info.cc:445] Add Edges for slot: 0, the Edge is from GradNodeBase (addr: 0x5566385d35c0)  to GradNodeAccumulation (addr: 0x55663815ddd0)
I0831 09:07:41.297801 27067 grad_node_info.cc:445] Add Edges for slot: 0, the Edge is from GradNodeBase (addr: 0x5566385d35c0)  to GradNodeAccumulation (addr: 0x556638159970)
I0831 09:07:41.297807 27067 run_program_op_func.h:162] clear_no_grad_edges.
I0831 09:07:41.298070 27067 eager.cc:112] Tensor(generated_tensor_11) have not GradNode, add GradNodeAccumulation0x5566381b3740 for it.
I0831 09:07:41.298143 27067 backward.cc:441] Run in Grad
I0831 09:07:41.298151 27067 backward.cc:112] Start Backward
I0831 09:07:41.298163 27067 general_grad.h:516] Copied Node: GradNodeBase ptr: 0x5566385d35c0 to ptr: 0x556638150250
I0831 09:07:41.298180 27067 backward.cc:202] Fill grad input tensor 0with give grad tensor
I0831 09:07:41.298187 27067 tensor_method.cc:93] Deep copy Tensor from generated_tensor_11 to 
I0831 09:07:41.298202 27067 tensor_utils.cc:57] TensorCopy 13, 17 from Place(cpu) to Place(cpu)
I0831 09:07:41.298210 27067 tensor_utils.cc:103] src:0x55663858f000, dst:0x556638585000
I0831 09:07:41.298215 27067 memcpy.cc:743] memory::Copy 1768 Bytes from Place(cpu) to Place(cpu)
I0831 09:07:41.298224 27067 general_grad.h:563] Copied Node: GradNodeAccumulation ptr: 0x55663815ddd0 to ptr: 0x5566385136f0
I0831 09:07:41.298229 27067 general_grad.h:563] Copied Node: GradNodeAccumulation ptr: 0x556638159970 to ptr: 0x556638608c70
I0831 09:07:41.298249 27067 backward.cc:248] Preparing GradNode:GradNodeBase addr:0x556638150250
I0831 09:07:41.298257 27067 run_program_op_node.h:688] Running Eager Backward Node: GradNodeRunProgram
I0831 09:07:41.298264 27067 run_program_op_node.h:709] hooked_grads[0].size() : 1
I0831 09:07:41.298275 27067 run_program_op_node.h:531] RunProgramGradOp use interpretercore to execute program.
I0831 09:07:41.298278 27067 run_program_op_node.h:533] global_inner_scope:0x556636a2e190
I0831 09:07:41.298283 27067 run_program_op_node.h:552] No interpretercore cahce, so create a new interpretercore
I0831 09:07:41.298288 27067 scope.cc:207] Create variable elementwise_div_0@GRAD
I0831 09:07:41.298295 27067 interpretercore.cc:45] InterpreterCore(): 0x5566381aa110 on Place(cpu)
I0831 09:07:41.298302 27067 program_interpreter.cc:49] ProgramInterpreter(): 0x55663860f990 on Place(cpu)
I0831 09:07:41.298310 27067 execution_config.cc:106] place:Place(cpu), processor_count:0, device_count:0, serial_run:0, num_host_threads:4, num_device_threads:0
I0831 09:07:41.298317 27067 new_executor_defs.cc:49] Set local scope: 0
I0831 09:07:41.298529 27067 run_program_op_node.h:589] Share workqueue from 0x55663864fcf0 to 0x5566381aa110
I0831 09:07:41.298539 27067 run_program_op_node.h:614] Get skip GC vars size is: 2
I0831 09:07:41.298544 27067 run_program_op_node.h:641] 0x556636a2e190 
------------------------------------------

Details:

====
0x556636a2e190:
  - elementwise_div_0
  - _jst.0.args.0
  - elementwise_div_0@GRAD
  - _jst.0.args.1
I0831 09:07:41.298557 27067 interpreter_util.cc:1105] Creating Variables
I0831 09:07:41.298565 27067 interpreter_util.cc:1142] Create Variable _jst.0.args.0 locally, which pointer is 0x5566385a21d0 type is 7
I0831 09:07:41.298573 27067 scope.cc:207] Create variable _jst.0.args.0@GRAD
I0831 09:07:41.298564 27135 nonblocking_threadpool.h:258] HostTasks_thread_0 started 
I0831 09:07:41.298588 27067 interpreter_util.cc:1142] Create Variable _jst.0.args.0@GRAD locally, which pointer is 0x556638204340 type is 7
I0831 09:07:41.298599 27136 nonblocking_threadpool.h:258] HostTasks_thread_1 started 
I0831 09:07:41.298607 27138 nonblocking_threadpool.h:258] HostTasks_thread_3 started 
I0831 09:07:41.298621 27135 os_info.cc:117] SetCurrentThreadName HostTasks_thread_0
I0831 09:07:41.298647 27138 os_info.cc:117] SetCurrentThreadName HostTasks_thread_3
I0831 09:07:41.298607 27137 nonblocking_threadpool.h:258] HostTasks_thread_2 started 
I0831 09:07:41.298641 27136 os_info.cc:117] SetCurrentThreadName HostTasks_thread_1
I0831 09:07:41.298671 27137 os_info.cc:117] SetCurrentThreadName HostTasks_thread_2
I0831 09:07:41.298617 27067 interpreter_util.cc:1142] Create Variable _jst.0.args.1 locally, which pointer is 0x55663863cca0 type is 7
I0831 09:07:41.298687 27067 scope.cc:207] Create variable _jst.0.args.1@GRAD
I0831 09:07:41.298692 27067 interpreter_util.cc:1142] Create Variable _jst.0.args.1@GRAD locally, which pointer is 0x5566380422c0 type is 7
I0831 09:07:41.298697 27067 scope.cc:207] Create variable composite_tmp_10
I0831 09:07:41.298702 27067 interpreter_util.cc:1142] Create Variable composite_tmp_10 locally, which pointer is 0x55663818c850 type is 7
I0831 09:07:41.298707 27067 scope.cc:207] Create variable composite_tmp_11
I0831 09:07:41.298712 27067 interpreter_util.cc:1142] Create Variable composite_tmp_11 locally, which pointer is 0x556638658c10 type is 7
I0831 09:07:41.298717 27067 scope.cc:207] Create variable composite_tmp_13
I0831 09:07:41.298720 27067 interpreter_util.cc:1142] Create Variable composite_tmp_13 locally, which pointer is 0x55663858aa90 type is 7
I0831 09:07:41.298725 27067 scope.cc:207] Create variable composite_tmp_14
I0831 09:07:41.298729 27067 interpreter_util.cc:1142] Create Variable composite_tmp_14 locally, which pointer is 0x5566385a3ab0 type is 7
I0831 09:07:41.298734 27067 scope.cc:207] Create variable composite_tmp_8
I0831 09:07:41.298738 27067 interpreter_util.cc:1142] Create Variable composite_tmp_8 locally, which pointer is 0x55663863f1e0 type is 7
I0831 09:07:41.298743 27067 scope.cc:207] Create variable composite_tmp_9
I0831 09:07:41.298748 27067 interpreter_util.cc:1142] Create Variable composite_tmp_9 locally, which pointer is 0x5566381a1980 type is 7
I0831 09:07:41.298753 27067 interpreter_util.cc:1142] Create Variable elementwise_div_0@GRAD locally, which pointer is 0x5566381ed4c0 type is 7
I0831 09:07:41.298970 27067 interpreter_util.cc:567] Static build: 0
I0831 09:07:41.298976 27067 mkldnn_helper.h:90] RegisterModelLayout for mkldnn
I0831 09:07:41.298986 27067 var_desc.cc:416] Flush  composite_tmp_8 1
I0831 09:07:41.298991 27067 var_desc.cc:416] Flush  _jst.0.args.1 1
I0831 09:07:41.298995 27067 var_desc.cc:416] Flush  composite_tmp_8 0
I0831 09:07:41.298998 27067 var_desc.cc:416] Flush  composite_tmp_9 1
I0831 09:07:41.299002 27067 var_desc.cc:416] Flush  _jst.0.args.0 1
I0831 09:07:41.299005 27067 var_desc.cc:416] Flush  composite_tmp_9 0
I0831 09:07:41.299010 27067 var_desc.cc:416] Flush  composite_tmp_10 1
I0831 09:07:41.299012 27067 var_desc.cc:416] Flush  composite_tmp_10 0
I0831 09:07:41.299017 27067 var_desc.cc:416] Flush  composite_tmp_11 1
I0831 09:07:41.299021 27067 var_desc.cc:416] Flush  composite_tmp_10 0
I0831 09:07:41.299024 27067 var_desc.cc:416] Flush  composite_tmp_10 0
I0831 09:07:41.299027 27067 var_desc.cc:416] Flush  composite_tmp_11 0
I0831 09:07:41.299031 27067 var_desc.cc:416] Flush  composite_tmp_11 0
I0831 09:07:41.299034 27067 var_desc.cc:416] Flush  elementwise_div_0@GRAD 1
I0831 09:07:41.299038 27067 var_desc.cc:416] Flush  _jst.0.args.1@GRAD 1
I0831 09:07:41.299042 27067 var_desc.cc:416] Flush  composite_tmp_13 1
I0831 09:07:41.299046 27067 var_desc.cc:416] Flush  composite_tmp_13 0
I0831 09:07:41.299050 27067 var_desc.cc:416] Flush  _jst.0.args.1 0
I0831 09:07:41.299053 27067 var_desc.cc:416] Flush  composite_tmp_14 1
I0831 09:07:41.299062 27067 var_desc.cc:416] Flush  composite_tmp_14 0
I0831 09:07:41.299064 27067 var_desc.cc:416] Flush  elementwise_div_0@GRAD 0
I0831 09:07:41.299068 27067 var_desc.cc:416] Flush  _jst.0.args.0@GRAD 1
I0831 09:07:41.299073 27067 interpreter_util.cc:281] elementwise_mul composite_tmp_14
I0831 09:07:41.299077 27067 interpreter_util.cc:281] elementwise_mul _jst.0.args.0@GRAD
I0831 09:07:41.299080 27067 interpreter_util.cc:281] elementwise_div composite_tmp_13
I0831 09:07:41.299083 27067 interpreter_util.cc:281] elementwise_mul _jst.0.args.1@GRAD
I0831 09:07:41.299086 27067 interpreter_util.cc:281] elementwise_div _jst.0.args.1
I0831 09:07:41.299090 27067 interpreter_util.cc:281] elementwise_pow composite_tmp_8
I0831 09:07:41.299093 27067 interpreter_util.cc:281] elementwise_mul elementwise_div_0@GRAD
I0831 09:07:41.299096 27067 interpreter_util.cc:281] elementwise_div _jst.0.args.0
I0831 09:07:41.299099 27067 interpreter_util.cc:281] elementwise_div composite_tmp_9
I0831 09:07:41.299103 27067 interpreter_util.cc:281] scale composite_tmp_10
I0831 09:07:41.299106 27067 interpreter_util.cc:281] elementwise_mul composite_tmp_11
I0831 09:07:41.299109 27067 interpreter_util.cc:283] gc map size:6
I0831 09:07:41.299129 27067 interpreter_util.cc:677] Start run Place(cpu) Op(fill_constant), inputs:{ShapeTensor[], ShapeTensorList[], ValueTensor[]}, outputs:{Out[composite_tmp_8:[]({})()]}.
I0831 09:07:41.299142 27067 interpreter_util.cc:687] OP is not null
I0831 09:07:41.299145 27067 interpreter_util.cc:690] get op_with_kernel
I0831 09:07:41.299149 27067 interpreter_util.cc:695] get RuntimeContext
I0831 09:07:41.299158 27067 interpreter_util.cc:724] expected_kernel_key : {data_type[double]; data_layout[Undefined(AnyLayout)]; place[Place(cpu)]; library_type[PLAIN]}
I0831 09:07:41.299190 27067 operator.cc:2207] op type:fill_constant, expected_kernel_key:{data_type[double]; data_layout[Undefined(AnyLayout)]; place[Place(cpu)]; library_type[PLAIN]}
I0831 09:07:41.299197 27067 interpreter_util.cc:772] if run phi kernel? : 1
I0831 09:07:41.299201 27067 interpreter_util.cc:787] fill_constant : finally selected kernel_key: {data_type[double]; data_layout[Undefined(AnyLayout)]; place[Place(cpu)]; library_type[PLAIN]}
I0831 09:07:41.299216 27067 interpreter_util.cc:802] apply data transform done. 
I0831 09:07:41.299221 27067 interpreter_util.cc:806] infer shape
I0831 09:07:41.299232 27067 operator.cc:3182] Done inputs
I0831 09:07:41.299237 27067 operator.cc:3244] Done outputs
I0831 09:07:41.299250 27067 operator.cc:3496] Done attributes
I0831 09:07:41.299257 27067 operator.cc:3546] Done runtime attributes
I0831 09:07:41.299261 27067 operator.cc:3576] Done runtime extra inputs
I0831 09:07:41.299281 27067 interpreter_util.cc:961] End run Place(cpu) Op(fill_constant), inputs:{ShapeTensor[], ShapeTensorList[], ValueTensor[]}, outputs:{Out[composite_tmp_8:double[13, 17]({})(Place(cpu))]}.
I0831 09:07:41.299304 27067 interpreter_util.cc:677] Start run Place(cpu) Op(elementwise_pow), inputs:{X[_jst.0.args.1:double[13, 17]({})(Place(cpu))], Y[composite_tmp_8:double[13, 17]({})(Place(cpu))]}, outputs:{Out[composite_tmp_9:[]({})()]}.
I0831 09:07:41.299317 27067 interpreter_util.cc:687] OP is not null
I0831 09:07:41.299320 27067 interpreter_util.cc:690] get op_with_kernel
I0831 09:07:41.299324 27067 interpreter_util.cc:695] get RuntimeContext
I0831 09:07:41.299331 27067 interpreter_util.cc:724] expected_kernel_key : {data_type[double]; data_layout[Undefined(AnyLayout)]; place[Place(cpu)]; library_type[PLAIN]}
I0831 09:07:41.299345 27067 operator.cc:2207] op type:elementwise_pow, expected_kernel_key:{data_type[double]; data_layout[Undefined(AnyLayout)]; place[Place(cpu)]; library_type[PLAIN]}
I0831 09:07:41.299350 27067 interpreter_util.cc:772] if run phi kernel? : 1
I0831 09:07:41.299353 27067 interpreter_util.cc:787] elementwise_pow : finally selected kernel_key: {data_type[double]; data_layout[Undefined(AnyLayout)]; place[Place(cpu)]; library_type[PLAIN]}
I0831 09:07:41.299368 27067 interpreter_util.cc:802] apply data transform done. 
I0831 09:07:41.299377 27067 interpreter_util.cc:806] infer shape
I0831 09:07:41.299384 27067 infershape_utils.cc:547] BuildInferMetaContext: op kernel signature - Kernel Signature - name: elementwise_pow_raw; inputs: X, Y; attributes: axis; outputs: Out
I0831 09:07:41.299412 27067 operator.cc:3182] Done inputs
I0831 09:07:41.299417 27067 operator.cc:3244] Done outputs
I0831 09:07:41.299419 27067 operator.cc:3496] Done attributes
I0831 09:07:41.299423 27067 operator.cc:3031] Runtime attr `Scale_x` is passed to OneDNNContext.
I0831 09:07:41.299428 27067 operator.cc:3031] Runtime attr `mkldnn_data_type` is passed to OneDNNContext.
I0831 09:07:41.299432 27067 operator.cc:3031] Runtime attr `Scale_y` is passed to OneDNNContext.
I0831 09:07:41.299435 27067 operator.cc:3031] Runtime attr `Scale_out` is passed to OneDNNContext.
I0831 09:07:41.299439 27067 operator.cc:3546] Done runtime attributes
I0831 09:07:41.299443 27067 operator.cc:3576] Done runtime extra inputs
I0831 09:07:41.299461 27067 interpreter_util.cc:961] End run Place(cpu) Op(elementwise_pow), inputs:{X[_jst.0.args.1:double[13, 17]({})(Place(cpu))], Y[composite_tmp_8:double[13, 17]({})(Place(cpu))]}, outputs:{Out[composite_tmp_9:double[13, 17]({})(Place(cpu))]}.
I0831 09:07:41.299489 27067 interpreter_util.cc:677] Start run Place(cpu) Op(elementwise_div), inputs:{X[_jst.0.args.0:double[13, 17]({})(Place(cpu))], Y[composite_tmp_9:double[13, 17]({})(Place(cpu))]}, outputs:{Out[composite_tmp_10:[]({})()]}.
I0831 09:07:41.299500 27067 interpreter_util.cc:687] OP is not null
I0831 09:07:41.299504 27067 interpreter_util.cc:690] get op_with_kernel
I0831 09:07:41.299506 27067 interpreter_util.cc:695] get RuntimeContext
I0831 09:07:41.299512 27067 interpreter_util.cc:724] expected_kernel_key : {data_type[double]; data_layout[Undefined(AnyLayout)]; place[Place(cpu)]; library_type[PLAIN]}
I0831 09:07:41.299522 27067 operator.cc:2207] op type:elementwise_div, expected_kernel_key:{data_type[double]; data_layout[Undefined(AnyLayout)]; place[Place(cpu)]; library_type[PLAIN]}
I0831 09:07:41.299527 27067 interpreter_util.cc:772] if run phi kernel? : 1
I0831 09:07:41.299530 27067 interpreter_util.cc:787] elementwise_div : finally selected kernel_key: {data_type[double]; data_layout[Undefined(AnyLayout)]; place[Place(cpu)]; library_type[PLAIN]}
I0831 09:07:41.299544 27067 interpreter_util.cc:802] apply data transform done. 
I0831 09:07:41.299547 27067 interpreter_util.cc:806] infer shape
I0831 09:07:41.299557 27067 operator.cc:3182] Done inputs
I0831 09:07:41.299561 27067 operator.cc:3244] Done outputs
I0831 09:07:41.299564 27067 operator.cc:3496] Done attributes
I0831 09:07:41.299567 27067 operator.cc:3031] Runtime attr `Scale_x` is passed to OneDNNContext.
I0831 09:07:41.299571 27067 operator.cc:3031] Runtime attr `mkldnn_data_type` is passed to OneDNNContext.
I0831 09:07:41.299575 27067 operator.cc:3031] Runtime attr `Scale_y` is passed to OneDNNContext.
I0831 09:07:41.299578 27067 operator.cc:3031] Runtime attr `Scale_out` is passed to OneDNNContext.
I0831 09:07:41.299582 27067 operator.cc:3546] Done runtime attributes
I0831 09:07:41.299585 27067 operator.cc:3576] Done runtime extra inputs
I0831 09:07:41.299602 27067 interpreter_util.cc:961] End run Place(cpu) Op(elementwise_div), inputs:{X[_jst.0.args.0:double[13, 17]({})(Place(cpu))], Y[composite_tmp_9:double[13, 17]({})(Place(cpu))]}, outputs:{Out[composite_tmp_10:double[13, 17]({})(Place(cpu))]}.
I0831 09:07:41.299636 27067 interpreter_util.cc:677] Start run Place(cpu) Op(share_buffer), inputs:{X[composite_tmp_10:double[13, 17]({})(Place(cpu))]}, outputs:{Out[composite_tmp_11:[]({})()], XOut[composite_tmp_10:double[13, 17]({})(Place(cpu))]}.
I0831 09:07:41.299649 27067 interpreter_util.cc:687] OP is not null
I0831 09:07:41.299654 27067 interpreter_util.cc:690] get op_with_kernel
I0831 09:07:41.299656 27067 interpreter_util.cc:695] get RuntimeContext
I0831 09:07:41.299664 27067 interpreter_util.cc:724] expected_kernel_key : {data_type[double]; data_layout[Undefined(AnyLayout)]; place[Place(cpu)]; library_type[PLAIN]}
I0831 09:07:41.299680 27067 operator.cc:2207] op type:share_buffer, expected_kernel_key:{data_type[double]; data_layout[Undefined(AnyLayout)]; place[Place(cpu)]; library_type[PLAIN]}
I0831 09:07:41.299687 27067 interpreter_util.cc:772] if run phi kernel? : 1
I0831 09:07:41.299690 27067 interpreter_util.cc:787] share_buffer : finally selected kernel_key: {data_type[double]; data_layout[Undefined(AnyLayout)]; place[Place(cpu)]; library_type[PLAIN]}
I0831 09:07:41.299700 27067 interpreter_util.cc:802] apply data transform done. 
I0831 09:07:41.299703 27067 interpreter_util.cc:806] infer shape
I0831 09:07:41.299708 27067 infershape_utils.cc:547] BuildInferMetaContext: op kernel signature - Kernel Signature - name: share_buffer; inputs: X; attributes: share_dims_and_dtype; outputs: Out, XOut
I0831 09:07:41.299726 27067 operator.cc:3182] Done inputs
I0831 09:07:41.299729 27067 operator.cc:3244] Done outputs
I0831 09:07:41.299734 27067 operator.cc:3496] Done attributes
I0831 09:07:41.299738 27067 operator.cc:3546] Done runtime attributes
I0831 09:07:41.299742 27067 operator.cc:3576] Done runtime extra inputs
I0831 09:07:41.299749 27067 interpreter_util.cc:961] End run Place(cpu) Op(share_buffer), inputs:{X[composite_tmp_10:double[13, 17]({})(Place(cpu))]}, outputs:{Out[composite_tmp_11:double[]({})(Place(cpu))], XOut[composite_tmp_10:double[13, 17]({})(Place(cpu))]}.
I0831 09:07:41.299772 27067 interpreter_util.cc:677] Start run Place(cpu) Op(scale), inputs:{ScaleTensor[], X[composite_tmp_10:double[13, 17]({})(Place(cpu))]}, outputs:{Out[composite_tmp_11:double[]({})(Place(cpu))]}.
I0831 09:07:41.299784 27067 interpreter_util.cc:687] OP is not null
I0831 09:07:41.299787 27067 interpreter_util.cc:690] get op_with_kernel
I0831 09:07:41.299791 27067 interpreter_util.cc:695] get RuntimeContext
I0831 09:07:41.299798 27067 interpreter_util.cc:724] expected_kernel_key : {data_type[double]; data_layout[Undefined(AnyLayout)]; place[Place(cpu)]; library_type[PLAIN]}
I0831 09:07:41.299811 27067 operator.cc:2207] op type:scale, expected_kernel_key:{data_type[double]; data_layout[Undefined(AnyLayout)]; place[Place(cpu)]; library_type[PLAIN]}
I0831 09:07:41.299818 27067 interpreter_util.cc:772] if run phi kernel? : 1
I0831 09:07:41.299821 27067 interpreter_util.cc:787] scale : finally selected kernel_key: {data_type[double]; data_layout[Undefined(AnyLayout)]; place[Place(cpu)]; library_type[PLAIN]}
I0831 09:07:41.299835 27067 interpreter_util.cc:802] apply data transform done. 
I0831 09:07:41.299839 27067 interpreter_util.cc:806] infer shape
I0831 09:07:41.299845 27067 infershape_utils.cc:547] BuildInferMetaContext: op kernel signature - Kernel Signature - name: scale; inputs: X; attributes: scale, bias, bias_after_scale; outputs: Out
I0831 09:07:41.299861 27067 operator.cc:3182] Done inputs
I0831 09:07:41.299865 27067 operator.cc:3244] Done outputs
I0831 09:07:41.299870 27067 operator.cc:3496] Done attributes
I0831 09:07:41.299875 27067 operator.cc:3546] Done runtime attributes
I0831 09:07:41.299878 27067 operator.cc:3576] Done runtime extra inputs
I0831 09:07:41.299896 27067 interpreter_util.cc:961] End run Place(cpu) Op(scale), inputs:{ScaleTensor[], X[composite_tmp_10:double[13, 17]({})(Place(cpu))]}, outputs:{Out[composite_tmp_11:double[13, 17]({})(Place(cpu))]}.
I0831 09:07:41.299917 27067 interpreter_util.cc:677] Start run Place(cpu) Op(elementwise_mul), inputs:{X[composite_tmp_11:double[13, 17]({})(Place(cpu))], Y[elementwise_div_0@GRAD:double[13, 17]({})(Place(cpu))]}, outputs:{Out[_jst.0.args.1@GRAD:[]({})()]}.
I0831 09:07:41.299930 27067 interpreter_util.cc:687] OP is not null
I0831 09:07:41.299934 27067 interpreter_util.cc:690] get op_with_kernel
I0831 09:07:41.299937 27067 interpreter_util.cc:695] get RuntimeContext
I0831 09:07:41.299943 27067 interpreter_util.cc:724] expected_kernel_key : {data_type[double]; data_layout[Undefined(AnyLayout)]; place[Place(cpu)]; library_type[PLAIN]}
I0831 09:07:41.299957 27067 operator.cc:2207] op type:elementwise_mul, expected_kernel_key:{data_type[double]; data_layout[Undefined(AnyLayout)]; place[Place(cpu)]; library_type[PLAIN]}
I0831 09:07:41.299966 27067 interpreter_util.cc:772] if run phi kernel? : 1
I0831 09:07:41.299969 27067 interpreter_util.cc:787] elementwise_mul : finally selected kernel_key: {data_type[double]; data_layout[Undefined(AnyLayout)]; place[Place(cpu)]; library_type[PLAIN]}
I0831 09:07:41.299984 27067 interpreter_util.cc:802] apply data transform done. 
I0831 09:07:41.299988 27067 interpreter_util.cc:806] infer shape
I0831 09:07:41.299997 27067 operator.cc:3182] Done inputs
I0831 09:07:41.300000 27067 operator.cc:3244] Done outputs
I0831 09:07:41.300004 27067 operator.cc:3496] Done attributes
I0831 09:07:41.300009 27067 operator.cc:3031] Runtime attr `Scale_x` is passed to OneDNNContext.
I0831 09:07:41.300012 27067 operator.cc:3031] Runtime attr `mkldnn_data_type` is passed to OneDNNContext.
I0831 09:07:41.300016 27067 operator.cc:3031] Runtime attr `Scale_y` is passed to OneDNNContext.
I0831 09:07:41.300020 27067 operator.cc:3031] Runtime attr `Scale_out` is passed to OneDNNContext.
I0831 09:07:41.300024 27067 operator.cc:3546] Done runtime attributes
I0831 09:07:41.300029 27067 operator.cc:3576] Done runtime extra inputs
I0831 09:07:41.300041 27067 interpreter_util.cc:961] End run Place(cpu) Op(elementwise_mul), inputs:{X[composite_tmp_11:double[13, 17]({})(Place(cpu))], Y[elementwise_div_0@GRAD:double[13, 17]({})(Place(cpu))]}, outputs:{Out[_jst.0.args.1@GRAD:double[13, 17]({})(Place(cpu))]}.
I0831 09:07:41.300066 27067 interpreter_util.cc:677] Start run Place(cpu) Op(fill_constant), inputs:{ShapeTensor[], ShapeTensorList[], ValueTensor[]}, outputs:{Out[composite_tmp_13:[]({})()]}.
I0831 09:07:41.300074 27067 interpreter_util.cc:687] OP is not null
I0831 09:07:41.300078 27067 interpreter_util.cc:690] get op_with_kernel
I0831 09:07:41.300081 27067 interpreter_util.cc:695] get RuntimeContext
I0831 09:07:41.300087 27067 interpreter_util.cc:724] expected_kernel_key : {data_type[double]; data_layout[Undefined(AnyLayout)]; place[Place(cpu)]; library_type[PLAIN]}
I0831 09:07:41.300098 27067 operator.cc:2207] op type:fill_constant, expected_kernel_key:{data_type[double]; data_layout[Undefined(AnyLayout)]; place[Place(cpu)]; library_type[PLAIN]}
I0831 09:07:41.300104 27067 interpreter_util.cc:772] if run phi kernel? : 1
I0831 09:07:41.300108 27067 interpreter_util.cc:787] fill_constant : finally selected kernel_key: {data_type[double]; data_layout[Undefined(AnyLayout)]; place[Place(cpu)]; library_type[PLAIN]}
I0831 09:07:41.300123 27067 interpreter_util.cc:802] apply data transform done. 
I0831 09:07:41.300127 27067 interpreter_util.cc:806] infer shape
I0831 09:07:41.300133 27067 operator.cc:3182] Done inputs
I0831 09:07:41.300138 27067 operator.cc:3244] Done outputs
I0831 09:07:41.300148 27067 operator.cc:3496] Done attributes
I0831 09:07:41.300153 27067 operator.cc:3546] Done runtime attributes
I0831 09:07:41.300156 27067 operator.cc:3576] Done runtime extra inputs
I0831 09:07:41.300172 27067 interpreter_util.cc:961] End run Place(cpu) Op(fill_constant), inputs:{ShapeTensor[], ShapeTensorList[], ValueTensor[]}, outputs:{Out[composite_tmp_13:double[13, 17]({})(Place(cpu))]}.
I0831 09:07:41.300192 27067 interpreter_util.cc:677] Start run Place(cpu) Op(elementwise_div), inputs:{X[composite_tmp_13:double[13, 17]({})(Place(cpu))], Y[_jst.0.args.1:double[13, 17]({})(Place(cpu))]}, outputs:{Out[composite_tmp_14:[]({})()]}.
I0831 09:07:41.300205 27067 interpreter_util.cc:687] OP is not null
I0831 09:07:41.300209 27067 interpreter_util.cc:690] get op_with_kernel
I0831 09:07:41.300212 27067 interpreter_util.cc:695] get RuntimeContext
I0831 09:07:41.300217 27067 interpreter_util.cc:724] expected_kernel_key : {data_type[double]; data_layout[Undefined(AnyLayout)]; place[Place(cpu)]; library_type[PLAIN]}
I0831 09:07:41.300228 27067 operator.cc:2207] op type:elementwise_div, expected_kernel_key:{data_type[double]; data_layout[Undefined(AnyLayout)]; place[Place(cpu)]; library_type[PLAIN]}
I0831 09:07:41.300233 27067 interpreter_util.cc:772] if run phi kernel? : 1
I0831 09:07:41.300237 27067 interpreter_util.cc:787] elementwise_div : finally selected kernel_key: {data_type[double]; data_layout[Undefined(AnyLayout)]; place[Place(cpu)]; library_type[PLAIN]}
I0831 09:07:41.300257 27067 interpreter_util.cc:802] apply data transform done. 
I0831 09:07:41.300261 27067 interpreter_util.cc:806] infer shape
I0831 09:07:41.300269 27067 operator.cc:3182] Done inputs
I0831 09:07:41.300273 27067 operator.cc:3244] Done outputs
I0831 09:07:41.300285 27067 operator.cc:3496] Done attributes
I0831 09:07:41.300289 27067 operator.cc:3031] Runtime attr `Scale_x` is passed to OneDNNContext.
I0831 09:07:41.300293 27067 operator.cc:3031] Runtime attr `mkldnn_data_type` is passed to OneDNNContext.
I0831 09:07:41.300297 27067 operator.cc:3031] Runtime attr `Scale_y` is passed to OneDNNContext.
I0831 09:07:41.300300 27067 operator.cc:3031] Runtime attr `Scale_out` is passed to OneDNNContext.
I0831 09:07:41.300304 27067 operator.cc:3546] Done runtime attributes
I0831 09:07:41.300307 27067 operator.cc:3576] Done runtime extra inputs
I0831 09:07:41.300318 27067 interpreter_util.cc:961] End run Place(cpu) Op(elementwise_div), inputs:{X[composite_tmp_13:double[13, 17]({})(Place(cpu))], Y[_jst.0.args.1:double[13, 17]({})(Place(cpu))]}, outputs:{Out[composite_tmp_14:double[13, 17]({})(Place(cpu))]}.
I0831 09:07:41.300340 27067 interpreter_util.cc:677] Start run Place(cpu) Op(elementwise_mul), inputs:{X[composite_tmp_14:double[13, 17]({})(Place(cpu))], Y[elementwise_div_0@GRAD:double[13, 17]({})(Place(cpu))]}, outputs:{Out[_jst.0.args.0@GRAD:[]({})()]}.
I0831 09:07:41.300352 27067 interpreter_util.cc:687] OP is not null
I0831 09:07:41.300355 27067 interpreter_util.cc:690] get op_with_kernel
I0831 09:07:41.300359 27067 interpreter_util.cc:695] get RuntimeContext
I0831 09:07:41.300364 27067 interpreter_util.cc:724] expected_kernel_key : {data_type[double]; data_layout[Undefined(AnyLayout)]; place[Place(cpu)]; library_type[PLAIN]}
I0831 09:07:41.300374 27067 operator.cc:2207] op type:elementwise_mul, expected_kernel_key:{data_type[double]; data_layout[Undefined(AnyLayout)]; place[Place(cpu)]; library_type[PLAIN]}
I0831 09:07:41.300379 27067 interpreter_util.cc:772] if run phi kernel? : 1
I0831 09:07:41.300381 27067 interpreter_util.cc:787] elementwise_mul : finally selected kernel_key: {data_type[double]; data_layout[Undefined(AnyLayout)]; place[Place(cpu)]; library_type[PLAIN]}
I0831 09:07:41.300395 27067 interpreter_util.cc:802] apply data transform done. 
I0831 09:07:41.300398 27067 interpreter_util.cc:806] infer shape
I0831 09:07:41.300406 27067 operator.cc:3182] Done inputs
I0831 09:07:41.300410 27067 operator.cc:3244] Done outputs
I0831 09:07:41.300412 27067 operator.cc:3496] Done attributes
I0831 09:07:41.300416 27067 operator.cc:3031] Runtime attr `Scale_x` is passed to OneDNNContext.
I0831 09:07:41.300420 27067 operator.cc:3031] Runtime attr `mkldnn_data_type` is passed to OneDNNContext.
I0831 09:07:41.300423 27067 operator.cc:3031] Runtime attr `Scale_y` is passed to OneDNNContext.
I0831 09:07:41.300427 27067 operator.cc:3031] Runtime attr `Scale_out` is passed to OneDNNContext.
I0831 09:07:41.300431 27067 operator.cc:3546] Done runtime attributes
I0831 09:07:41.300434 27067 operator.cc:3576] Done runtime extra inputs
I0831 09:07:41.300453 27067 interpreter_util.cc:961] End run Place(cpu) Op(elementwise_mul), inputs:{X[composite_tmp_14:double[13, 17]({})(Place(cpu))], Y[elementwise_div_0@GRAD:double[13, 17]({})(Place(cpu))]}, outputs:{Out[_jst.0.args.0@GRAD:double[13, 17]({})(Place(cpu))]}.
I0831 09:07:41.300711 27067 program_interpreter.cc:785] Already inplaced, skip inplace now.
I0831 09:07:41.300722 27067 program_interpreter.cc:1475] Analyze the execution order of Trace scheduling mode.
I0831 09:07:41.300729 27067 program_interpreter.cc:1481] op_id: 7, remain deps: 1
I0831 09:07:41.300732 27067 program_interpreter.cc:1481] op_id: 8, remain deps: 1
I0831 09:07:41.300736 27067 program_interpreter.cc:1481] op_id: 1, remain deps: 1
I0831 09:07:41.300740 27067 program_interpreter.cc:1481] op_id: 2, remain deps: 1
I0831 09:07:41.300743 27067 program_interpreter.cc:1481] op_id: 3, remain deps: 1
I0831 09:07:41.300751 27067 program_interpreter.cc:1481] op_id: 4, remain deps: 1
I0831 09:07:41.300755 27067 program_interpreter.cc:1481] op_id: 5, remain deps: 1
I0831 09:07:41.300762 27067 program_interpreter.cc:1465] Update sync op num, sync op num is: 9
I0831 09:07:41.300773 27067 run_program_op_node.h:246] share _jst.0.args.0@GRAD from scope
I0831 09:07:41.300781 27067 run_program_op_node.h:246] share _jst.0.args.1@GRAD from scope
I0831 09:07:41.300784 27067 run_program_op_node.h:657] after backward gc all vars
I0831 09:07:41.300791 27067 run_program_op_node.h:734] End Eager Backward Node: GradNodeRunProgram
I0831 09:07:41.300799 27067 backward.cc:288] retain_graph is false, need to clear the TensorWrapper of nodes.
I0831 09:07:41.300805 27067 backward.cc:317] Node: GradNodeBase addr:0x556638150250, Found pending node: GradNodeAccumulation addr: 0x5566385136f0
I0831 09:07:41.300812 27067 backward.cc:358] Sum or Move grad inputs for edge slot: 0, rank: 0
I0831 09:07:41.300818 27067 backward.cc:317] Node: GradNodeBase addr:0x556638150250, Found pending node: GradNodeAccumulation addr: 0x556638608c70
I0831 09:07:41.300822 27067 backward.cc:358] Sum or Move grad inputs for edge slot: 0, rank: 0
I0831 09:07:41.300828 27067 backward.cc:248] Preparing GradNode:GradNodeAccumulation addr:0x556638608c70
I0831 09:07:41.300832 27067 accumulation_node.cc:139] Running AD API Grad: GradNodeAccumulation
I0831 09:07:41.300837 27067 accumulation_node.cc:174] Finish AD API Grad: GradNodeAccumulation
I0831 09:07:41.300856 27067 accumulation_node.cc:187] { Input: [], Output: [(grad_out, [{ Name: _jst.0.args.1@GRAD, Initialized: 1, Ptr: 0x55663857a820 }]), ] } 
I0831 09:07:41.300864 27067 backward.cc:288] retain_graph is false, need to clear the TensorWrapper of nodes.
I0831 09:07:41.300868 27067 backward.cc:248] Preparing GradNode:GradNodeAccumulation addr:0x5566385136f0
I0831 09:07:41.300873 27067 accumulation_node.cc:139] Running AD API Grad: GradNodeAccumulation
I0831 09:07:41.300875 27067 accumulation_node.cc:174] Finish AD API Grad: GradNodeAccumulation
I0831 09:07:41.300885 27067 accumulation_node.cc:187] { Input: [], Output: [(grad_out, [{ Name: _jst.0.args.0@GRAD, Initialized: 1, Ptr: 0x5566386404f0 }]), ] } 
I0831 09:07:41.300890 27067 backward.cc:288] retain_graph is false, need to clear the TensorWrapper of nodes.
I0831 09:07:41.300894 27067 backward.cc:417] Finish Backward
I0831 09:07:41.300911 27067 eager_functions.cc:177]  in eager_api_run_partial_grad, after runing egr::Grad
I0831 09:07:41.300973 27067 memcpy.cc:119] src: 0x55663860e000, dst: 0x556638591000, num: 1768
I0831 09:07:41.300989 27067 memcpy.cc:119] src: 0x556638615000, dst: 0x5566386af000, num: 1768
I0831 09:07:41.310050 27067 run_program_op_node.h:671] ~GradNodeRunProgram
I0831 09:07:41.310088 27067 run_program_op_node.h:677] global_inner_scope SetCanReused
I0831 09:07:41.310133 27067 pybind.cc:1718] Cannot use get_all_custom_device_type because you have installedCPU/GPU version PaddlePaddle.
If you want to use get_all_custom_device_type, please try to install CustomDevice version PaddlePaddle by: pip install paddlepaddle
I0831 09:07:41.310173 27067 imperative.cc:701] Tracer(0x556636b47f70) set expected place Place(cpu)
I0831 09:07:41.310321 27067 eager.cc:112] Tensor(generated_tensor_12) have not GradNode, add GradNodeAccumulation0x55663815ddd0 for it.
I0831 09:07:41.310405 27067 eager.cc:112] Tensor(generated_tensor_13) have not GradNode, add GradNodeAccumulation0x556638159970 for it.
I0831 09:07:41.317013 27067 op_desc.cc:1097] CompileTime infer shape on elementwise_div
I0831 09:07:41.317484 27067 eager.cc:112] Tensor(cuda_graph) have not GradNode, add GradNodeAccumulation0x5566381892d0 for it.
I0831 09:07:41.318410 27067 op_desc.cc:1097] CompileTime infer shape on elementwise_div
I0831 09:07:41.318473 27067 op_desc.cc:1097] CompileTime infer shape on fill_any_like
I0831 09:07:41.318491 27067 infershape_utils.cc:547] BuildInferMetaContext: op kernel signature - Kernel Signature - name: full_like; inputs: X; attributes: value, dtype; outputs: Out
I0831 09:07:41.318550 27067 pybind.cc:1459] need skip: 0
I0831 09:07:41.318557 27067 pybind.cc:1462] Prim Flag Open: Runing composite grad fun for elementwise_div
I0831 09:07:41.318563 27067 composite_grad_desc_maker.h:75] Runing Composite Grad func for elementwise_div_grad 
I0831 09:07:41.318599 27067 operants_manager.cc:761] OperantsManager reusing static mode API paddle::prim::pow<DescTensor>
I0831 09:07:41.318647 27067 op_desc.cc:1097] CompileTime infer shape on fill_constant
I0831 09:07:41.318675 27067 op_desc.cc:1097] CompileTime infer shape on elementwise_pow
I0831 09:07:41.318683 27067 infershape_utils.cc:547] BuildInferMetaContext: op kernel signature - Kernel Signature - name: elementwise_pow_raw; inputs: X, Y; attributes: axis; outputs: Out
I0831 09:07:41.318703 27067 operants_manager.cc:695] OperantsManager reusing static mode API paddle::prim::divide<DescTensor>
I0831 09:07:41.318722 27067 op_desc.cc:1097] CompileTime infer shape on elementwise_div
I0831 09:07:41.318738 27067 operants_manager.cc:398] OperantsManager reusing static mode API paddle::prim::scale<DescTensor>
I0831 09:07:41.318758 27067 op_desc.cc:1097] CompileTime infer shape on scale
I0831 09:07:41.318766 27067 infershape_utils.cc:547] BuildInferMetaContext: op kernel signature - Kernel Signature - name: scale; inputs: X; attributes: scale, bias, bias_after_scale; outputs: Out
I0831 09:07:41.318782 27067 operants_manager.cc:1190] OperantsManager reusing static mode API paddle::prim::multiply<DescTensor>
I0831 09:07:41.318802 27067 op_desc.cc:1097] CompileTime infer shape on elementwise_mul
I0831 09:07:41.318835 27067 op_desc.cc:1097] CompileTime infer shape on fill_constant
I0831 09:07:41.318843 27067 operants_manager.cc:695] OperantsManager reusing static mode API paddle::prim::divide<DescTensor>
I0831 09:07:41.318858 27067 op_desc.cc:1097] CompileTime infer shape on elementwise_div
I0831 09:07:41.318872 27067 operants_manager.cc:1190] OperantsManager reusing static mode API paddle::prim::multiply<DescTensor>
I0831 09:07:41.318888 27067 op_desc.cc:1097] CompileTime infer shape on elementwise_mul
I0831 09:07:41.318902 27067 composite_grad_desc_maker.h:509] Recover: composite_tmp_23 To: _jst.0.args.0@GRAD
I0831 09:07:41.318907 27067 var_desc.cc:416] Flush  composite_tmp_23 1
I0831 09:07:41.318920 27067 composite_grad_desc_maker.h:509] Recover: composite_tmp_20 To: _jst.0.args.1@GRAD
I0831 09:07:41.318924 27067 var_desc.cc:416] Flush  composite_tmp_20 1
I0831 09:07:41.319785 27067 op_desc.cc:1097] CompileTime infer shape on fill_constant
I0831 09:07:41.320509 27067 op_desc.cc:1097] CompileTime infer shape on elementwise_pow
I0831 09:07:41.320528 27067 infershape_utils.cc:547] BuildInferMetaContext: op kernel signature - Kernel Signature - name: elementwise_pow_raw; inputs: X, Y; attributes: axis; outputs: Out
I0831 09:07:41.321241 27067 op_desc.cc:1097] CompileTime infer shape on elementwise_div
I0831 09:07:41.321905 27067 op_desc.cc:1097] CompileTime infer shape on scale
I0831 09:07:41.321926 27067 infershape_utils.cc:547] BuildInferMetaContext: op kernel signature - Kernel Signature - name: scale; inputs: X; attributes: scale, bias, bias_after_scale; outputs: Out
I0831 09:07:41.322631 27067 op_desc.cc:1097] CompileTime infer shape on elementwise_mul
I0831 09:07:41.323339 27067 op_desc.cc:1097] CompileTime infer shape on fill_constant
I0831 09:07:41.324023 27067 op_desc.cc:1097] CompileTime infer shape on elementwise_div
I0831 09:07:41.324714 27067 op_desc.cc:1097] CompileTime infer shape on elementwise_mul
I0831 09:07:41.325208 27067 op_desc.cc:1097] CompileTime infer shape on fill_any_like
I0831 09:07:41.325227 27067 infershape_utils.cc:547] BuildInferMetaContext: op kernel signature - Kernel Signature - name: full_like; inputs: X; attributes: value, dtype; outputs: Out
I0831 09:07:41.325332 27067 op_desc.cc:1097] CompileTime infer shape on fill_constant
I0831 09:07:41.325384 27067 op_desc.cc:1097] CompileTime infer shape on elementwise_pow
I0831 09:07:41.325393 27067 infershape_utils.cc:547] BuildInferMetaContext: op kernel signature - Kernel Signature - name: elementwise_pow_raw; inputs: X, Y; attributes: axis; outputs: Out
I0831 09:07:41.325464 27067 op_desc.cc:1097] CompileTime infer shape on elementwise_div
I0831 09:07:41.325515 27067 op_desc.cc:1097] CompileTime infer shape on scale
I0831 09:07:41.325525 27067 infershape_utils.cc:547] BuildInferMetaContext: op kernel signature - Kernel Signature - name: scale; inputs: X; attributes: scale, bias, bias_after_scale; outputs: Out
I0831 09:07:41.325583 27067 op_desc.cc:1097] CompileTime infer shape on elementwise_mul
I0831 09:07:41.325637 27067 op_desc.cc:1097] CompileTime infer shape on fill_constant
I0831 09:07:41.325682 27067 op_desc.cc:1097] CompileTime infer shape on elementwise_div
I0831 09:07:41.325734 27067 op_desc.cc:1097] CompileTime infer shape on elementwise_mul
I0831 09:07:41.326653 27067 graph.cc:144] create OpNode by fill_constant
I0831 09:07:41.326695 27067 graph.cc:144] create OpNode by elementwise_pow
I0831 09:07:41.326719 27067 graph.cc:144] create OpNode by elementwise_div
I0831 09:07:41.326745 27067 graph.cc:144] create OpNode by scale
I0831 09:07:41.326769 27067 graph.cc:144] create OpNode by elementwise_mul
I0831 09:07:41.326793 27067 graph.cc:144] create OpNode by fill_constant
I0831 09:07:41.326815 27067 graph.cc:144] create OpNode by elementwise_div
I0831 09:07:41.326834 27067 graph.cc:144] create OpNode by elementwise_mul
I0831 09:07:41.326858 27067 graph.cc:219] kStaleProgramOpDescs.size: 8
I0831 09:07:41.327040 27067 parallel_executor.cc:1323] The Program will be executed on CPU using ParallelExecutor, 1 cards are used, so 1 programs are executed in parallel.
I0831 09:07:41.327055 27067 build_strategy.cc:360] apply all passes
I0831 09:07:41.327064 27067 pass_builder.cc:29] Append fuse_bn_add_act_pass
I0831 09:07:41.327071 27067 pass_builder.cc:29] Append coalesce_grad_tensor_pass
I0831 09:07:41.327077 27067 pass_builder.cc:29] Append add_reader_dependency_pass
I0831 09:07:41.327083 27067 pass_builder.cc:29] Append all_reduce_mode_multi_devices_pass
I0831 09:07:41.327090 27067 pass_builder.cc:29] Append fuse_all_reduce_op_pass
I0831 09:07:41.327096 27067 pass_builder.cc:29] Append all_reduce_deps_pass
I0831 09:07:41.327102 27067 pass_builder.cc:29] Append modify_op_lock_and_record_event_pass
I0831 09:07:41.327106 27067 pass_builder.cc:29] Append multi_devices_check_pass
I0831 09:07:41.327113 27067 build_strategy.cc:247] CollectiveContext:endpoints_:trainer_id_:0
I0831 09:07:41.327118 27067 build_strategy.cc:371] BuildStrategy::Apply pass:fuse_bn_add_act_pass
I0831 09:07:41.327123 27067 build_strategy.cc:474] fuse_bn_add_act_pass is only supported on GPU, skipped.
I0831 09:07:41.327126 27067 build_strategy.cc:371] BuildStrategy::Apply pass:coalesce_grad_tensor_pass
I0831 09:07:41.327132 27067 build_strategy.cc:488] Start Apply Pass coalesce_grad_tensor_pass
I0831 09:07:41.327136 27067 build_strategy.cc:491] Apply Pass coalesce_grad_tensor_passto SubGraph 0
I0831 09:07:41.327144 27067 graph_helper.h:104] adj elementwise_div0x5566385c41b0 -> elementwise_mul0x5566385c7310  via composite_tmp_220x5566385c70c0
I0831 09:07:41.327149 27067 graph_helper.h:104] adj elementwise_pow0x55663860f760 -> elementwise_div0x556638674130  via composite_tmp_170x5566386450f0
I0831 09:07:41.327153 27067 graph_helper.h:104] adj fill_constant0x5566385f6a70 -> elementwise_pow0x55663860f760  via composite_tmp_160x55663860f400
I0831 09:07:41.327157 27067 graph_helper.h:104] adj elementwise_div0x556638674130 -> scale0x5566385afc20  via composite_tmp_180x5566385af9b0
I0831 09:07:41.327162 27067 graph_helper.h:104] adj scale0x5566385afc20 -> elementwise_mul0x5566385b4d80  via composite_tmp_190x5566385b4b30
I0831 09:07:41.327174 27067 graph_helper.h:104] adj fill_constant0x5566385c0f50 -> elementwise_div0x5566385c41b0  via composite_tmp_210x5566385c3fa0
I0831 09:07:41.327203 27067 onednn_context.cc:101] 0x55663860f990 0
I0831 09:07:41.327208 27067 onednn_context.cc:104] Clearing DNNL cache.
I0831 09:07:41.327211 27067 onednn_context.cc:122] Resetting Paddle data layout to NCHW.
I0831 09:07:41.327225 27067 build_strategy.cc:497] Finish Apply Pass coalesce_grad_tensor_pass
I0831 09:07:41.327229 27067 build_strategy.cc:371] BuildStrategy::Apply pass:add_reader_dependency_pass
I0831 09:07:41.327234 27067 build_strategy.cc:488] Start Apply Pass add_reader_dependency_pass
I0831 09:07:41.327236 27067 build_strategy.cc:491] Apply Pass add_reader_dependency_passto SubGraph 0
I0831 09:07:41.327243 27067 graph_helper.h:104] adj elementwise_div0x5566385c41b0 -> elementwise_mul0x5566385c7310  via composite_tmp_220x5566385c70c0
I0831 09:07:41.327246 27067 graph_helper.h:104] adj elementwise_pow0x55663860f760 -> elementwise_div0x556638674130  via composite_tmp_170x5566386450f0
I0831 09:07:41.327250 27067 graph_helper.h:104] adj fill_constant0x5566385f6a70 -> elementwise_pow0x55663860f760  via composite_tmp_160x55663860f400
I0831 09:07:41.327253 27067 graph_helper.h:104] adj elementwise_div0x556638674130 -> scale0x5566385afc20  via composite_tmp_180x5566385af9b0
I0831 09:07:41.327257 27067 graph_helper.h:104] adj scale0x5566385afc20 -> elementwise_mul0x5566385b4d80  via composite_tmp_190x5566385b4b30
I0831 09:07:41.327261 27067 graph_helper.h:104] adj fill_constant0x5566385c0f50 -> elementwise_div0x5566385c41b0  via composite_tmp_210x5566385c3fa0
I0831 09:07:41.327275 27067 onednn_context.cc:101] 0x55663860f990 0
I0831 09:07:41.327278 27067 onednn_context.cc:104] Clearing DNNL cache.
I0831 09:07:41.327281 27067 onednn_context.cc:122] Resetting Paddle data layout to NCHW.
I0831 09:07:41.327284 27067 build_strategy.cc:497] Finish Apply Pass add_reader_dependency_pass
I0831 09:07:41.327287 27067 build_strategy.cc:371] BuildStrategy::Apply pass:all_reduce_mode_multi_devices_pass
I0831 09:07:41.327296 27067 build_strategy.cc:488] Start Apply Pass all_reduce_mode_multi_devices_pass
I0831 09:07:41.327298 27067 build_strategy.cc:491] Apply Pass all_reduce_mode_multi_devices_passto SubGraph 0
I0831 09:07:41.327309 27067 graph_helper.h:104] adj elementwise_div0x5566385c41b0 -> elementwise_mul0x5566385c7310  via composite_tmp_220x5566385c70c0
I0831 09:07:41.327313 27067 graph_helper.h:104] adj elementwise_pow0x55663860f760 -> elementwise_div0x556638674130  via composite_tmp_170x5566386450f0
I0831 09:07:41.327317 27067 graph_helper.h:104] adj fill_constant0x5566385f6a70 -> elementwise_pow0x55663860f760  via composite_tmp_160x55663860f400
I0831 09:07:41.327320 27067 graph_helper.h:104] adj elementwise_div0x556638674130 -> scale0x5566385afc20  via composite_tmp_180x5566385af9b0
I0831 09:07:41.327324 27067 graph_helper.h:104] adj scale0x5566385afc20 -> elementwise_mul0x5566385b4d80  via composite_tmp_190x5566385b4b30
I0831 09:07:41.327328 27067 graph_helper.h:104] adj fill_constant0x5566385c0f50 -> elementwise_div0x5566385c41b0  via composite_tmp_210x5566385c3fa0
I0831 09:07:41.327675 27067 graph.h:183] deleting ops
I0831 09:07:41.327708 27067 graph_helper.h:104] adj fill_constant0x556638635ab0 -> elementwise_div0x55663867c860  via composite_tmp_210x55663867c560
I0831 09:07:41.327713 27067 graph_helper.h:104] adj scale0x556638629980 -> elementwise_mul0x55663862f6b0  via composite_tmp_190x55663862d7a0
I0831 09:07:41.327716 27067 graph_helper.h:104] adj elementwise_div0x5566386234e0 -> scale0x556638629980  via composite_tmp_180x556638629600
I0831 09:07:41.327720 27067 graph_helper.h:104] adj elementwise_pow0x5566385d0b70 -> elementwise_div0x5566386234e0  via composite_tmp_170x556638623180
I0831 09:07:41.327723 27067 graph_helper.h:104] adj elementwise_div0x55663867c860 -> elementwise_mul0x556638682480  via composite_tmp_220x556638682120
I0831 09:07:41.327728 27067 graph_helper.h:104] adj fill_constant0x5566385a64c0 -> elementwise_pow0x5566385d0b70  via composite_tmp_160x5566385d0870
I0831 09:07:41.327742 27067 onednn_context.cc:101] 0x55663860f990 0
I0831 09:07:41.327746 27067 onednn_context.cc:104] Clearing DNNL cache.
I0831 09:07:41.327749 27067 onednn_context.cc:122] Resetting Paddle data layout to NCHW.
I0831 09:07:41.327754 27067 build_strategy.cc:497] Finish Apply Pass all_reduce_mode_multi_devices_pass
I0831 09:07:41.327756 27067 build_strategy.cc:371] BuildStrategy::Apply pass:fuse_all_reduce_op_pass
I0831 09:07:41.327773 27067 build_strategy.cc:488] Start Apply Pass fuse_all_reduce_op_pass
I0831 09:07:41.327777 27067 build_strategy.cc:491] Apply Pass fuse_all_reduce_op_passto SubGraph 0
I0831 09:07:41.327782 27067 graph_helper.h:104] adj fill_constant0x556638635ab0 -> elementwise_div0x55663867c860  via composite_tmp_210x55663867c560
I0831 09:07:41.327786 27067 graph_helper.h:104] adj scale0x556638629980 -> elementwise_mul0x55663862f6b0  via composite_tmp_190x55663862d7a0
I0831 09:07:41.327790 27067 graph_helper.h:104] adj elementwise_div0x5566386234e0 -> scale0x556638629980  via composite_tmp_180x556638629600
I0831 09:07:41.327793 27067 graph_helper.h:104] adj elementwise_pow0x5566385d0b70 -> elementwise_div0x5566386234e0  via composite_tmp_170x556638623180
I0831 09:07:41.327797 27067 graph_helper.h:104] adj elementwise_div0x55663867c860 -> elementwise_mul0x556638682480  via composite_tmp_220x556638682120
I0831 09:07:41.327800 27067 graph_helper.h:104] adj fill_constant0x5566385a64c0 -> elementwise_pow0x5566385d0b70  via composite_tmp_160x5566385d0870
I0831 09:07:41.327813 27067 onednn_context.cc:101] 0x55663860f990 0
I0831 09:07:41.327816 27067 onednn_context.cc:104] Clearing DNNL cache.
I0831 09:07:41.327819 27067 onednn_context.cc:122] Resetting Paddle data layout to NCHW.
I0831 09:07:41.327823 27067 build_strategy.cc:497] Finish Apply Pass fuse_all_reduce_op_pass
I0831 09:07:41.327826 27067 build_strategy.cc:371] BuildStrategy::Apply pass:all_reduce_deps_pass
I0831 09:07:41.327831 27067 build_strategy.cc:452] SeqOnlyAllReduceOps:0, num_trainers:1
I0831 09:07:41.327836 27067 build_strategy.cc:488] Start Apply Pass all_reduce_deps_pass
I0831 09:07:41.327838 27067 build_strategy.cc:491] Apply Pass all_reduce_deps_passto SubGraph 0
I0831 09:07:41.327852 27067 graph_helper.h:104] adj fill_constant0x556638635ab0 -> elementwise_div0x55663867c860  via composite_tmp_210x55663867c560
I0831 09:07:41.327857 27067 graph_helper.h:104] adj scale0x556638629980 -> elementwise_mul0x55663862f6b0  via composite_tmp_190x55663862d7a0
I0831 09:07:41.327860 27067 graph_helper.h:104] adj elementwise_div0x5566386234e0 -> scale0x556638629980  via composite_tmp_180x556638629600
I0831 09:07:41.327863 27067 graph_helper.h:104] adj elementwise_pow0x5566385d0b70 -> elementwise_div0x5566386234e0  via composite_tmp_170x556638623180
I0831 09:07:41.327867 27067 graph_helper.h:104] adj elementwise_div0x55663867c860 -> elementwise_mul0x556638682480  via composite_tmp_220x556638682120
I0831 09:07:41.327872 27067 graph_helper.h:104] adj fill_constant0x5566385a64c0 -> elementwise_pow0x5566385d0b70  via composite_tmp_160x5566385d0870
I0831 09:07:41.327883 27067 onednn_context.cc:101] 0x55663860f990 0
I0831 09:07:41.327886 27067 onednn_context.cc:104] Clearing DNNL cache.
I0831 09:07:41.327889 27067 onednn_context.cc:122] Resetting Paddle data layout to NCHW.
I0831 09:07:41.327893 27067 build_strategy.cc:497] Finish Apply Pass all_reduce_deps_pass
I0831 09:07:41.327895 27067 build_strategy.cc:371] BuildStrategy::Apply pass:modify_op_lock_and_record_event_pass
I0831 09:07:41.327899 27067 build_strategy.cc:488] Start Apply Pass modify_op_lock_and_record_event_pass
I0831 09:07:41.327903 27067 build_strategy.cc:491] Apply Pass modify_op_lock_and_record_event_passto SubGraph 0
I0831 09:07:41.327915 27067 graph_helper.h:104] adj fill_constant0x556638635ab0 -> elementwise_div0x55663867c860  via composite_tmp_210x55663867c560
I0831 09:07:41.327919 27067 graph_helper.h:104] adj scale0x556638629980 -> elementwise_mul0x55663862f6b0  via composite_tmp_190x55663862d7a0
I0831 09:07:41.327922 27067 graph_helper.h:104] adj elementwise_div0x5566386234e0 -> scale0x556638629980  via composite_tmp_180x556638629600
I0831 09:07:41.327926 27067 graph_helper.h:104] adj elementwise_pow0x5566385d0b70 -> elementwise_div0x5566386234e0  via composite_tmp_170x556638623180
I0831 09:07:41.327929 27067 graph_helper.h:104] adj elementwise_div0x55663867c860 -> elementwise_mul0x556638682480  via composite_tmp_220x556638682120
I0831 09:07:41.327936 27067 graph_helper.h:104] adj fill_constant0x5566385a64c0 -> elementwise_pow0x5566385d0b70  via composite_tmp_160x5566385d0870
I0831 09:07:41.327947 27067 onednn_context.cc:101] 0x55663860f990 0
I0831 09:07:41.327951 27067 onednn_context.cc:104] Clearing DNNL cache.
I0831 09:07:41.327955 27067 onednn_context.cc:122] Resetting Paddle data layout to NCHW.
I0831 09:07:41.327957 27067 build_strategy.cc:497] Finish Apply Pass modify_op_lock_and_record_event_pass
I0831 09:07:41.327961 27067 build_strategy.cc:371] BuildStrategy::Apply pass:multi_devices_check_pass
I0831 09:07:41.327965 27067 build_strategy.cc:488] Start Apply Pass multi_devices_check_pass
I0831 09:07:41.327968 27067 build_strategy.cc:491] Apply Pass multi_devices_check_passto SubGraph 0
I0831 09:07:41.327983 27067 graph_helper.h:104] adj fill_constant0x556638635ab0 -> elementwise_div0x55663867c860  via composite_tmp_210x55663867c560
I0831 09:07:41.327986 27067 graph_helper.h:104] adj scale0x556638629980 -> elementwise_mul0x55663862f6b0  via composite_tmp_190x55663862d7a0
I0831 09:07:41.327991 27067 graph_helper.h:104] adj elementwise_div0x5566386234e0 -> scale0x556638629980  via composite_tmp_180x556638629600
I0831 09:07:41.327993 27067 graph_helper.h:104] adj elementwise_pow0x5566385d0b70 -> elementwise_div0x5566386234e0  via composite_tmp_170x556638623180
I0831 09:07:41.327997 27067 graph_helper.h:104] adj elementwise_div0x55663867c860 -> elementwise_mul0x556638682480  via composite_tmp_220x556638682120
I0831 09:07:41.328001 27067 graph_helper.h:104] adj fill_constant0x5566385a64c0 -> elementwise_pow0x5566385d0b70  via composite_tmp_160x5566385d0870
I0831 09:07:41.328011 27067 onednn_context.cc:101] 0x55663860f990 0
I0831 09:07:41.328015 27067 onednn_context.cc:104] Clearing DNNL cache.
I0831 09:07:41.328018 27067 onednn_context.cc:122] Resetting Paddle data layout to NCHW.
I0831 09:07:41.328022 27067 build_strategy.cc:497] Finish Apply Pass multi_devices_check_pass
I0831 09:07:41.328025 27067 build_strategy.cc:499] All Passes Applied
I0831 09:07:41.328045 27067 reference_count_pass.cc:289] Place number: 1
I0831 09:07:41.328050 27067 var_desc.cc:416] Flush  _jst.0.args.0@GRAD 1
I0831 09:07:41.328060 27067 var_desc.cc:416] Flush  composite_tmp_22 1
I0831 09:07:41.328070 27067 var_desc.cc:416] Flush  composite_tmp_21 1
I0831 09:07:41.328076 27067 var_desc.cc:416] Flush  _jst.0.args.1@GRAD 1
I0831 09:07:41.328081 27067 var_desc.cc:416] Flush  _jst.0.args.1 1
I0831 09:07:41.328089 27067 var_desc.cc:416] Flush  elementwise_div_0@GRAD 1
I0831 09:07:41.328095 27067 var_desc.cc:416] Flush  _jst.0.args.0 1
I0831 09:07:41.328100 27067 var_desc.cc:416] Flush  composite_tmp_16 1
I0831 09:07:41.328106 27067 var_desc.cc:416] Flush  composite_tmp_18 1
I0831 09:07:41.328112 27067 var_desc.cc:416] Flush  composite_tmp_17 1
I0831 09:07:41.328117 27067 var_desc.cc:416] Flush  composite_tmp_19 1
I0831 09:07:41.328125 27067 graph_helper.h:104] adj fill_constant0x556638635ab0 -> elementwise_div0x55663867c860  via composite_tmp_210x55663867c560
I0831 09:07:41.328128 27067 graph_helper.h:104] adj scale0x556638629980 -> elementwise_mul0x55663862f6b0  via composite_tmp_190x55663862d7a0
I0831 09:07:41.328132 27067 graph_helper.h:104] adj elementwise_div0x5566386234e0 -> scale0x556638629980  via composite_tmp_180x556638629600
I0831 09:07:41.328135 27067 graph_helper.h:104] adj elementwise_pow0x5566385d0b70 -> elementwise_div0x5566386234e0  via composite_tmp_170x556638623180
I0831 09:07:41.328140 27067 graph_helper.h:104] adj elementwise_div0x55663867c860 -> elementwise_mul0x556638682480  via composite_tmp_220x556638682120
I0831 09:07:41.328142 27067 graph_helper.h:104] adj fill_constant0x5566385a64c0 -> elementwise_pow0x5566385d0b70  via composite_tmp_160x5566385d0870
I0831 09:07:41.328155 27067 onednn_context.cc:101] 0x55663860f990 0
I0831 09:07:41.328158 27067 onednn_context.cc:104] Clearing DNNL cache.
I0831 09:07:41.328161 27067 onednn_context.cc:122] Resetting Paddle data layout to NCHW.
I0831 09:07:41.328212 27067 buffer_shared_inplace_op_pass.cc:142] Inplace performed in op scale: composite_tmp_18 -> composite_tmp_19. Debug String is: Op(scale), inputs:{ScaleTensor[], X[composite_tmp_18]}, outputs:{Out[composite_tmp_19]}.. ReuseType: inplace
I0831 09:07:41.328230 27067 memory_reuse_pass.cc:63] Create 1 ShareTensorBufferOpHandles in Scope 0
I0831 09:07:41.328234 27067 graph_helper.h:104] adj fill_constant0x556638635ab0 -> elementwise_div0x55663867c860  via composite_tmp_210x55663867c560
I0831 09:07:41.328238 27067 graph_helper.h:104] adj scale0x556638629980 -> elementwise_mul0x55663862f6b0  via composite_tmp_190x55663862d7a0
I0831 09:07:41.328243 27067 graph_helper.h:104] adj elementwise_div0x5566386234e0 -> scale0x556638629980  via composite_tmp_180x556638629600
I0831 09:07:41.328246 27067 graph_helper.h:104] adj buffer_share0x5566385c0f50 -> scale0x556638629980  via __control_var@390x5566385c0ce0
I0831 09:07:41.328250 27067 graph_helper.h:104] adj elementwise_pow0x5566385d0b70 -> elementwise_div0x5566386234e0  via composite_tmp_170x556638623180
I0831 09:07:41.328253 27067 graph_helper.h:104] adj elementwise_div0x5566386234e0 -> buffer_share0x5566385c0f50  via composite_tmp_180x556638629600
I0831 09:07:41.328258 27067 graph_helper.h:104] adj elementwise_div0x55663867c860 -> elementwise_mul0x556638682480  via composite_tmp_220x556638682120
I0831 09:07:41.328261 27067 graph_helper.h:104] adj fill_constant0x5566385a64c0 -> elementwise_pow0x5566385d0b70  via composite_tmp_160x5566385d0870
I0831 09:07:41.328276 27067 onednn_context.cc:101] 0x55663860f990 0
I0831 09:07:41.328280 27067 onednn_context.cc:104] Clearing DNNL cache.
I0831 09:07:41.328284 27067 onednn_context.cc:122] Resetting Paddle data layout to NCHW.
I0831 09:07:41.328286 27067 parallel_executor.cc:482] Inplace strategy is enabled, when build_strategy.enable_inplace = True
I0831 09:07:41.328310 27067 var_desc.cc:416] Flush  _jst.0.args.1 0
I0831 09:07:41.328315 27067 var_desc.cc:416] Flush  composite_tmp_21 0
I0831 09:07:41.328320 27067 var_desc.cc:416] Flush  elementwise_div_0@GRAD 0
I0831 09:07:41.328322 27067 var_desc.cc:416] Flush  composite_tmp_22 0
I0831 09:07:41.328326 27067 var_desc.cc:416] Flush  _jst.0.args.0@GRAD 0
I0831 09:07:41.328330 27067 var_desc.cc:416] Flush  _jst.0.args.1 0
I0831 09:07:41.328333 27067 var_desc.cc:416] Flush  composite_tmp_16 0
I0831 09:07:41.328336 27067 var_desc.cc:416] Flush  elementwise_div_0@GRAD 0
I0831 09:07:41.328341 27067 var_desc.cc:416] Flush  _jst.0.args.1@GRAD 0
I0831 09:07:41.328344 27067 var_desc.cc:416] Flush  composite_tmp_19 0
I0831 09:07:41.328347 27067 var_desc.cc:416] Flush  composite_tmp_18 0
I0831 09:07:41.328351 27067 var_desc.cc:416] Flush  _jst.0.args.0 0
I0831 09:07:41.328354 27067 var_desc.cc:416] Flush  composite_tmp_17 0
I0831 09:07:41.328404 27067 conditional_block_op_helper.cc:108] Found conditional_block op num: 0, conditional_block_grad op num: 0
I0831 09:07:41.328410 27067 graph_helper.h:104] adj elementwise_div0x5566386234e0 -> eager_deletion0x5566385c41b0  via __control_var@530x55663860f760
I0831 09:07:41.328414 27067 graph_helper.h:104] adj elementwise_mul0x55663862f6b0 -> eager_deletion0x5566385ca1d0  via __control_var@500x5566385c7310
I0831 09:07:41.328418 27067 graph_helper.h:104] adj elementwise_div0x55663867c860 -> eager_deletion0x5566385c0ad0  via __control_var@410x5566385b4d80
I0831 09:07:41.328421 27067 graph_helper.h:104] adj elementwise_pow0x5566385d0b70 -> eager_deletion0x55663860f400  via __control_var@470x5566385f6a70
I0831 09:07:41.328424 27067 graph_helper.h:104] adj fill_constant0x556638635ab0 -> elementwise_div0x55663867c860  via composite_tmp_210x55663867c560
I0831 09:07:41.328428 27067 graph_helper.h:104] adj scale0x556638629980 -> elementwise_mul0x55663862f6b0  via composite_tmp_190x55663862d7a0
I0831 09:07:41.328433 27067 graph_helper.h:104] adj elementwise_div0x5566386234e0 -> scale0x556638629980  via composite_tmp_180x556638629600
I0831 09:07:41.328435 27067 graph_helper.h:104] adj buffer_share0x5566385c0f50 -> scale0x556638629980  via __control_var@390x5566385c0ce0
I0831 09:07:41.328439 27067 graph_helper.h:104] adj elementwise_pow0x5566385d0b70 -> elementwise_div0x5566386234e0  via composite_tmp_170x556638623180
I0831 09:07:41.328446 27067 graph_helper.h:104] adj elementwise_mul0x556638682480 -> eager_deletion0x5566385afc20  via __control_var@440x5566385af9b0
I0831 09:07:41.328450 27067 graph_helper.h:104] adj elementwise_div0x5566386234e0 -> buffer_share0x5566385c0f50  via composite_tmp_180x556638629600
I0831 09:07:41.328454 27067 graph_helper.h:104] adj elementwise_div0x55663867c860 -> elementwise_mul0x556638682480  via composite_tmp_220x556638682120
I0831 09:07:41.328457 27067 graph_helper.h:104] adj fill_constant0x5566385a64c0 -> elementwise_pow0x5566385d0b70  via composite_tmp_160x5566385d0870
I0831 09:07:41.328474 27067 onednn_context.cc:101] 0x55663860f990 0
I0831 09:07:41.328478 27067 onednn_context.cc:104] Clearing DNNL cache.
I0831 09:07:41.328481 27067 onednn_context.cc:122] Resetting Paddle data layout to NCHW.
I0831 09:07:41.328490 27067 while_op_eager_deletion_pass.cc:58] Is Paritial Program
I0831 09:07:41.328493 27067 while_op_eager_deletion_pass.cc:82] Scope Idx = 0
I0831 09:07:41.328496 27067 while_op_eager_deletion_pass.cc:84] while_ops.size() = 0
I0831 09:07:41.328500 27067 while_op_eager_deletion_pass.cc:86] while_grad_ops.size() = 0
I0831 09:07:41.328503 27067 while_op_helper.cc:154] Found while op num: 0, while grad op num: 0
I0831 09:07:41.328509 27067 graph_helper.h:104] adj elementwise_div0x5566386234e0 -> eager_deletion0x5566385c41b0  via __control_var@530x55663860f760
I0831 09:07:41.328512 27067 graph_helper.h:104] adj elementwise_mul0x55663862f6b0 -> eager_deletion0x5566385ca1d0  via __control_var@500x5566385c7310
I0831 09:07:41.328516 27067 graph_helper.h:104] adj elementwise_div0x55663867c860 -> eager_deletion0x5566385c0ad0  via __control_var@410x5566385b4d80
I0831 09:07:41.328519 27067 graph_helper.h:104] adj elementwise_pow0x5566385d0b70 -> eager_deletion0x55663860f400  via __control_var@470x5566385f6a70
I0831 09:07:41.328523 27067 graph_helper.h:104] adj fill_constant0x556638635ab0 -> elementwise_div0x55663867c860  via composite_tmp_210x55663867c560
I0831 09:07:41.328526 27067 graph_helper.h:104] adj scale0x556638629980 -> elementwise_mul0x55663862f6b0  via composite_tmp_190x55663862d7a0
I0831 09:07:41.328531 27067 graph_helper.h:104] adj elementwise_div0x5566386234e0 -> scale0x556638629980  via composite_tmp_180x556638629600
I0831 09:07:41.328533 27067 graph_helper.h:104] adj buffer_share0x5566385c0f50 -> scale0x556638629980  via __control_var@390x5566385c0ce0
I0831 09:07:41.328537 27067 graph_helper.h:104] adj elementwise_pow0x5566385d0b70 -> elementwise_div0x5566386234e0  via composite_tmp_170x556638623180
I0831 09:07:41.328541 27067 graph_helper.h:104] adj elementwise_mul0x556638682480 -> eager_deletion0x5566385afc20  via __control_var@440x5566385af9b0
I0831 09:07:41.328544 27067 graph_helper.h:104] adj elementwise_div0x5566386234e0 -> buffer_share0x5566385c0f50  via composite_tmp_180x556638629600
I0831 09:07:41.328548 27067 graph_helper.h:104] adj elementwise_div0x55663867c860 -> elementwise_mul0x556638682480  via composite_tmp_220x556638682120
I0831 09:07:41.328552 27067 graph_helper.h:104] adj fill_constant0x5566385a64c0 -> elementwise_pow0x5566385d0b70  via composite_tmp_160x5566385d0870
I0831 09:07:41.328567 27067 onednn_context.cc:101] 0x55663860f990 0
I0831 09:07:41.328570 27067 onednn_context.cc:104] Clearing DNNL cache.
I0831 09:07:41.328573 27067 onednn_context.cc:122] Resetting Paddle data layout to NCHW.
I0831 09:07:41.328583 27067 recurrent_op_helper.cc:259] Found recurrent op num: 0, recurrent grad op num: 0
I0831 09:07:41.328589 27067 graph_helper.h:104] adj elementwise_div0x5566386234e0 -> eager_deletion0x5566385c41b0  via __control_var@530x55663860f760
I0831 09:07:41.328593 27067 graph_helper.h:104] adj elementwise_mul0x55663862f6b0 -> eager_deletion0x5566385ca1d0  via __control_var@500x5566385c7310
I0831 09:07:41.328598 27067 graph_helper.h:104] adj elementwise_div0x55663867c860 -> eager_deletion0x5566385c0ad0  via __control_var@410x5566385b4d80
I0831 09:07:41.328601 27067 graph_helper.h:104] adj elementwise_pow0x5566385d0b70 -> eager_deletion0x55663860f400  via __control_var@470x5566385f6a70
I0831 09:07:41.328608 27067 graph_helper.h:104] adj fill_constant0x556638635ab0 -> elementwise_div0x55663867c860  via composite_tmp_210x55663867c560
I0831 09:07:41.328612 27067 graph_helper.h:104] adj scale0x556638629980 -> elementwise_mul0x55663862f6b0  via composite_tmp_190x55663862d7a0
I0831 09:07:41.328615 27067 graph_helper.h:104] adj elementwise_div0x5566386234e0 -> scale0x556638629980  via composite_tmp_180x556638629600
I0831 09:07:41.328619 27067 graph_helper.h:104] adj buffer_share0x5566385c0f50 -> scale0x556638629980  via __control_var@390x5566385c0ce0
I0831 09:07:41.328622 27067 graph_helper.h:104] adj elementwise_pow0x5566385d0b70 -> elementwise_div0x5566386234e0  via composite_tmp_170x556638623180
I0831 09:07:41.328626 27067 graph_helper.h:104] adj elementwise_mul0x556638682480 -> eager_deletion0x5566385afc20  via __control_var@440x5566385af9b0
I0831 09:07:41.328629 27067 graph_helper.h:104] adj elementwise_div0x5566386234e0 -> buffer_share0x5566385c0f50  via composite_tmp_180x556638629600
I0831 09:07:41.328634 27067 graph_helper.h:104] adj elementwise_div0x55663867c860 -> elementwise_mul0x556638682480  via composite_tmp_220x556638682120
I0831 09:07:41.328637 27067 graph_helper.h:104] adj fill_constant0x5566385a64c0 -> elementwise_pow0x5566385d0b70  via composite_tmp_160x5566385d0870
I0831 09:07:41.328650 27067 onednn_context.cc:101] 0x55663860f990 0
I0831 09:07:41.328653 27067 onednn_context.cc:104] Clearing DNNL cache.
I0831 09:07:41.328656 27067 onednn_context.cc:122] Resetting Paddle data layout to NCHW.
I0831 09:07:41.328662 27067 graph_helper.h:104] adj elementwise_div0x5566386234e0 -> eager_deletion0x5566385c41b0  via __control_var@530x55663860f760
I0831 09:07:41.328666 27067 graph_helper.h:104] adj elementwise_mul0x55663862f6b0 -> eager_deletion0x5566385ca1d0  via __control_var@500x5566385c7310
I0831 09:07:41.328670 27067 graph_helper.h:104] adj elementwise_div0x55663867c860 -> eager_deletion0x5566385c0ad0  via __control_var@410x5566385b4d80
I0831 09:07:41.328673 27067 graph_helper.h:104] adj elementwise_pow0x5566385d0b70 -> eager_deletion0x55663860f400  via __control_var@470x5566385f6a70
I0831 09:07:41.328676 27067 graph_helper.h:104] adj fill_constant0x556638635ab0 -> elementwise_div0x55663867c860  via composite_tmp_210x55663867c560
I0831 09:07:41.328680 27067 graph_helper.h:104] adj scale0x556638629980 -> elementwise_mul0x55663862f6b0  via composite_tmp_190x55663862d7a0
I0831 09:07:41.328683 27067 graph_helper.h:104] adj elementwise_div0x5566386234e0 -> scale0x556638629980  via composite_tmp_180x556638629600
I0831 09:07:41.328687 27067 graph_helper.h:104] adj buffer_share0x5566385c0f50 -> scale0x556638629980  via __control_var@390x5566385c0ce0
I0831 09:07:41.328691 27067 graph_helper.h:104] adj elementwise_pow0x5566385d0b70 -> elementwise_div0x5566386234e0  via composite_tmp_170x556638623180
I0831 09:07:41.328694 27067 graph_helper.h:104] adj elementwise_mul0x556638682480 -> eager_deletion0x5566385afc20  via __control_var@440x5566385af9b0
I0831 09:07:41.328697 27067 graph_helper.h:104] adj elementwise_div0x5566386234e0 -> buffer_share0x5566385c0f50  via composite_tmp_180x556638629600
I0831 09:07:41.328701 27067 graph_helper.h:104] adj elementwise_div0x55663867c860 -> elementwise_mul0x556638682480  via composite_tmp_220x556638682120
I0831 09:07:41.328704 27067 graph_helper.h:104] adj fill_constant0x5566385a64c0 -> elementwise_pow0x5566385d0b70  via composite_tmp_160x5566385d0870
I0831 09:07:41.328717 27067 onednn_context.cc:101] 0x55663860f990 0
I0831 09:07:41.328720 27067 onednn_context.cc:104] Clearing DNNL cache.
I0831 09:07:41.328723 27067 onednn_context.cc:122] Resetting Paddle data layout to NCHW.
I0831 09:07:41.328727 27067 parallel_executor.cc:583] Garbage collection strategy is enabled, when FLAGS_eager_delete_tensor_gb = 0
I0831 09:07:41.328768 27067 parallel_executor.cc:1743] use FastThreadedSSAGraphExecutor
I0831 09:07:41.328855 27067 graph_helper.h:104] adj elementwise_div0x5566386234e0 -> eager_deletion0x5566385c41b0  via __control_var@530x55663860f760
I0831 09:07:41.328864 27067 graph_helper.h:104] adj elementwise_mul0x55663862f6b0 -> eager_deletion0x5566385ca1d0  via __control_var@500x5566385c7310
I0831 09:07:41.328867 27067 graph_helper.h:104] adj elementwise_div0x55663867c860 -> eager_deletion0x5566385c0ad0  via __control_var@410x5566385b4d80
I0831 09:07:41.328871 27067 graph_helper.h:104] adj elementwise_pow0x5566385d0b70 -> eager_deletion0x55663860f400  via __control_var@470x5566385f6a70
I0831 09:07:41.328876 27067 graph_helper.h:104] adj fill_constant0x556638635ab0 -> elementwise_div0x55663867c860  via composite_tmp_210x55663867c560
I0831 09:07:41.328879 27067 graph_helper.h:104] adj scale0x556638629980 -> elementwise_mul0x55663862f6b0  via composite_tmp_190x55663862d7a0
I0831 09:07:41.328882 27067 graph_helper.h:104] adj elementwise_div0x5566386234e0 -> scale0x556638629980  via composite_tmp_180x556638629600
I0831 09:07:41.328886 27067 graph_helper.h:104] adj buffer_share0x5566385c0f50 -> scale0x556638629980  via __control_var@390x5566385c0ce0
I0831 09:07:41.328889 27067 graph_helper.h:104] adj elementwise_pow0x5566385d0b70 -> elementwise_div0x5566386234e0  via composite_tmp_170x556638623180
I0831 09:07:41.328893 27067 graph_helper.h:104] adj elementwise_mul0x556638682480 -> eager_deletion0x5566385afc20  via __control_var@440x5566385af9b0
I0831 09:07:41.328897 27067 graph_helper.h:104] adj elementwise_div0x5566386234e0 -> buffer_share0x5566385c0f50  via composite_tmp_180x556638629600
I0831 09:07:41.328900 27067 graph_helper.h:104] adj elementwise_div0x55663867c860 -> elementwise_mul0x556638682480  via composite_tmp_220x556638682120
I0831 09:07:41.328904 27067 graph_helper.h:104] adj fill_constant0x5566385a64c0 -> elementwise_pow0x5566385d0b70  via composite_tmp_160x5566385d0870
I0831 09:07:41.328992 27067 parallel_executor.cc:729] use ScopeBufferedSSAGraphExecutor
I0831 09:07:41.329005 27067 scope.cc:207] Create variable _jst.0.args.0@GRAD
I0831 09:07:41.329010 27067 scope.cc:207] Create variable composite_tmp_22
I0831 09:07:41.329013 27067 scope.cc:207] Create variable composite_tmp_19
I0831 09:07:41.329017 27067 scope.cc:207] Create variable _jst.0.args.0
I0831 09:07:41.329021 27067 scope.cc:207] Create variable composite_tmp_21
I0831 09:07:41.329025 27067 scope.cc:207] Create variable composite_tmp_18
I0831 09:07:41.329030 27067 scope.cc:207] Create variable _jst.0.args.1@GRAD
I0831 09:07:41.329033 27067 scope.cc:207] Create variable composite_tmp_17
I0831 09:07:41.329037 27067 scope.cc:207] Create variable _jst.0.args.1
I0831 09:07:41.329041 27067 scope.cc:207] Create variable composite_tmp_16
I0831 09:07:41.329044 27067 scope.cc:207] Create variable elementwise_div_0@GRAD
I0831 09:07:41.329103 27067 var_desc.cc:416] Flush  _jst.0.args.0@GRAD 0
I0831 09:07:41.329111 27067 var_desc.cc:416] Flush  composite_tmp_22 0
I0831 09:07:41.329116 27067 var_desc.cc:416] Flush  composite_tmp_19 0
I0831 09:07:41.329120 27067 var_desc.cc:416] Flush  _jst.0.args.0 0
I0831 09:07:41.329125 27067 var_desc.cc:416] Flush  composite_tmp_21 0
I0831 09:07:41.329129 27067 var_desc.cc:416] Flush  composite_tmp_18 0
I0831 09:07:41.329133 27067 var_desc.cc:416] Flush  _jst.0.args.1@GRAD 0
I0831 09:07:41.329138 27067 var_desc.cc:416] Flush  composite_tmp_17 0
I0831 09:07:41.329141 27067 var_desc.cc:416] Flush  _jst.0.args.1 0
I0831 09:07:41.329174 27067 var_desc.cc:416] Flush  composite_tmp_16 0
I0831 09:07:41.329180 27067 var_desc.cc:416] Flush  elementwise_div_0@GRAD 0
I0831 09:07:41.329200 27067 graph_helper.h:104] adj elementwise_div0x5566386234e0 -> eager_deletion0x5566385c41b0  via __control_var@530x55663860f760
I0831 09:07:41.329214 27067 graph_helper.h:104] adj elementwise_mul0x55663862f6b0 -> eager_deletion0x5566385ca1d0  via __control_var@500x5566385c7310
I0831 09:07:41.329221 27067 graph_helper.h:104] adj elementwise_div0x55663867c860 -> eager_deletion0x5566385c0ad0  via __control_var@410x5566385b4d80
I0831 09:07:41.329227 27067 graph_helper.h:104] adj elementwise_pow0x5566385d0b70 -> eager_deletion0x55663860f400  via __control_var@470x5566385f6a70
I0831 09:07:41.329238 27067 graph_helper.h:104] adj fill_constant0x556638635ab0 -> elementwise_div0x55663867c860  via composite_tmp_210x55663867c560
I0831 09:07:41.329252 27067 graph_helper.h:104] adj scale0x556638629980 -> elementwise_mul0x55663862f6b0  via composite_tmp_190x55663862d7a0
I0831 09:07:41.329262 27067 graph_helper.h:104] adj elementwise_div0x5566386234e0 -> scale0x556638629980  via composite_tmp_180x556638629600
I0831 09:07:41.329270 27067 graph_helper.h:104] adj buffer_share0x5566385c0f50 -> scale0x556638629980  via __control_var@390x5566385c0ce0
I0831 09:07:41.329278 27067 graph_helper.h:104] adj elementwise_pow0x5566385d0b70 -> elementwise_div0x5566386234e0  via composite_tmp_170x556638623180
I0831 09:07:41.329288 27067 graph_helper.h:104] adj elementwise_mul0x556638682480 -> eager_deletion0x5566385afc20  via __control_var@440x5566385af9b0
I0831 09:07:41.329294 27067 graph_helper.h:104] adj elementwise_div0x5566386234e0 -> buffer_share0x5566385c0f50  via composite_tmp_180x556638629600
I0831 09:07:41.329301 27067 graph_helper.h:104] adj elementwise_div0x55663867c860 -> elementwise_mul0x556638682480  via composite_tmp_220x556638682120
I0831 09:07:41.329309 27067 graph_helper.h:104] adj fill_constant0x5566385a64c0 -> elementwise_pow0x5566385d0b70  via composite_tmp_160x5566385d0870
I0831 09:07:41.329421 27067 graph_helper.cc:688] convert op node to desc fill_constant
I0831 09:07:41.329427 27067 graph_helper.cc:666] 0 0
I0831 09:07:41.329443 27067 graph_helper.cc:688] convert op node to desc elementwise_pow
I0831 09:07:41.329448 27067 graph_helper.cc:666] 0 0
I0831 09:07:41.329474 27067 graph_helper.cc:688] convert op node to desc elementwise_div
I0831 09:07:41.329478 27067 graph_helper.cc:666] 0 0
I0831 09:07:41.329509 27067 graph_helper.cc:688] convert op node to desc scale
I0831 09:07:41.329514 27067 graph_helper.cc:666] 0 0
I0831 09:07:41.329524 27067 graph_helper.cc:688] convert op node to desc elementwise_mul
I0831 09:07:41.329530 27067 graph_helper.cc:666] 0 0
I0831 09:07:41.329576 27067 graph_helper.cc:688] convert op node to desc fill_constant
I0831 09:07:41.329579 27067 graph_helper.cc:666] 0 0
I0831 09:07:41.329593 27067 graph_helper.cc:688] convert op node to desc elementwise_div
I0831 09:07:41.329598 27067 graph_helper.cc:666] 0 0
I0831 09:07:41.329610 27067 graph_helper.cc:688] convert op node to desc elementwise_mul
I0831 09:07:41.329615 27067 graph_helper.cc:666] 0 0
I0831 09:07:41.330175 27067 graph_helper.cc:811] Graph to program need convert 1 sub graph
I0831 09:07:41.330494 27067 graph_helper.h:104] adj elementwise_div0x5566386234e0 -> eager_deletion0x5566385c41b0  via __control_var@530x55663860f760
I0831 09:07:41.330502 27067 graph_helper.h:104] adj elementwise_mul0x55663862f6b0 -> eager_deletion0x5566385ca1d0  via __control_var@500x5566385c7310
I0831 09:07:41.330505 27067 graph_helper.h:104] adj elementwise_div0x55663867c860 -> eager_deletion0x5566385c0ad0  via __control_var@410x5566385b4d80
I0831 09:07:41.330509 27067 graph_helper.h:104] adj elementwise_pow0x5566385d0b70 -> eager_deletion0x55663860f400  via __control_var@470x5566385f6a70
I0831 09:07:41.330513 27067 graph_helper.h:104] adj fill_constant0x556638635ab0 -> elementwise_div0x55663867c860  via composite_tmp_210x55663867c560
I0831 09:07:41.330518 27067 graph_helper.h:104] adj scale0x556638629980 -> elementwise_mul0x55663862f6b0  via composite_tmp_190x55663862d7a0
I0831 09:07:41.330521 27067 graph_helper.h:104] adj elementwise_div0x5566386234e0 -> scale0x556638629980  via composite_tmp_180x556638629600
I0831 09:07:41.330525 27067 graph_helper.h:104] adj buffer_share0x5566385c0f50 -> scale0x556638629980  via __control_var@390x5566385c0ce0
I0831 09:07:41.330529 27067 graph_helper.h:104] adj elementwise_pow0x5566385d0b70 -> elementwise_div0x5566386234e0  via composite_tmp_170x556638623180
I0831 09:07:41.330533 27067 graph_helper.h:104] adj elementwise_mul0x556638682480 -> eager_deletion0x5566385afc20  via __control_var@440x5566385af9b0
I0831 09:07:41.330538 27067 graph_helper.h:104] adj elementwise_div0x5566386234e0 -> buffer_share0x5566385c0f50  via composite_tmp_180x556638629600
I0831 09:07:41.330554 27067 graph_helper.h:104] adj elementwise_div0x55663867c860 -> elementwise_mul0x556638682480  via composite_tmp_220x556638682120
I0831 09:07:41.330557 27067 graph_helper.h:104] adj fill_constant0x5566385a64c0 -> elementwise_pow0x5566385d0b70  via composite_tmp_160x5566385d0870
I0831 09:07:41.330581 27067 onednn_context.cc:101] 0x55663860f990 0
I0831 09:07:41.330586 27067 onednn_context.cc:104] Clearing DNNL cache.
I0831 09:07:41.330590 27067 onednn_context.cc:122] Resetting Paddle data layout to NCHW.
I0831 09:07:41.331125 27067 graph.h:183] deleting dep_vars
I0831 09:07:41.331136 27067 graph.h:183] deleting pass_recorder
I0831 09:07:41.331141 27067 graph.h:183] deleting skip_gc_vars
I0831 09:07:41.331146 27067 graph.h:183] deleting stale_program_op_descs
I0831 09:07:41.331149 27067 graph.h:183] deleting vars
I0831 09:07:41.331161 27067 var_handle.cc:23] deleting var handle name:_jst.0.args.0@GRAD, place:Place(cpu), version:0, scope_idx:0
I0831 09:07:41.331210 27067 var_handle.cc:23] deleting var handle name:composite_tmp_22, place:Place(cpu), version:0, scope_idx:0
I0831 09:07:41.331225 27067 var_handle.cc:23] deleting var handle name:composite_tmp_21, place:Place(cpu), version:0, scope_idx:0
I0831 09:07:41.331231 27067 var_handle.cc:35] deleting dummy var handle __control_var@54
I0831 09:07:41.331245 27067 var_handle.cc:23] deleting var handle name:_jst.0.args.1@GRAD, place:Place(cpu), version:0, scope_idx:0
I0831 09:07:41.331250 27067 var_handle.cc:23] deleting var handle name:elementwise_div_0@GRAD, place:Place(cpu), version:0, scope_idx:0
I0831 09:07:41.331265 27067 var_handle.cc:23] deleting var handle name:composite_tmp_19, place:Place(cpu), version:0, scope_idx:0
I0831 09:07:41.331279 27067 var_handle.cc:23] deleting var handle name:composite_tmp_18, place:Place(cpu), version:0, scope_idx:0
I0831 09:07:41.331283 27067 var_handle.cc:23] deleting var handle name:_jst.0.args.0, place:Place(cpu), version:0, scope_idx:0
I0831 09:07:41.331296 27067 var_handle.cc:23] deleting var handle name:composite_tmp_17, place:Place(cpu), version:0, scope_idx:0
I0831 09:07:41.331301 27067 var_handle.cc:23] deleting var handle name:_jst.0.args.1, place:Place(cpu), version:0, scope_idx:0
I0831 09:07:41.331306 27067 var_handle.cc:35] deleting dummy var handle __control_var@53
I0831 09:07:41.331311 27067 var_handle.cc:35] deleting dummy var handle __control_var@47
I0831 09:07:41.331315 27067 var_handle.cc:35] deleting dummy var handle __control_var@48
I0831 09:07:41.331326 27067 var_handle.cc:23] deleting var handle name:composite_tmp_16, place:Place(cpu), version:0, scope_idx:0
I0831 09:07:41.331331 27067 var_handle.cc:35] deleting dummy var handle __control_var@50
I0831 09:07:41.331336 27067 var_handle.cc:35] deleting dummy var handle __control_var@51
I0831 09:07:41.331339 27067 var_handle.cc:35] deleting dummy var handle __control_var@45
I0831 09:07:41.331343 27067 var_handle.cc:35] deleting dummy var handle __control_var@39
I0831 09:07:41.331347 27067 var_handle.cc:35] deleting dummy var handle __control_var@41
I0831 09:07:41.331351 27067 var_handle.cc:35] deleting dummy var handle __control_var@42
I0831 09:07:41.331355 27067 var_handle.cc:35] deleting dummy var handle __control_var@44
I0831 09:07:41.331490 27067 executor_cache.cc:160] parse op type: fill_constant
I0831 09:07:41.331498 27067 executor_cache.cc:160] parse op type: elementwise_pow
I0831 09:07:41.331502 27067 executor_cache.cc:160] parse op type: elementwise_div
I0831 09:07:41.331507 27067 executor_cache.cc:160] parse op type: scale
I0831 09:07:41.331511 27067 executor_cache.cc:160] parse op type: elementwise_mul
I0831 09:07:41.331516 27067 executor_cache.cc:160] parse op type: fill_constant
I0831 09:07:41.331519 27067 executor_cache.cc:160] parse op type: elementwise_div
I0831 09:07:41.331523 27067 executor_cache.cc:160] parse op type: elementwise_mul
I0831 09:07:41.331528 27067 executor_cache.cc:190] parse op.input: composite_tmp_22
I0831 09:07:41.331542 27067 executor_cache.cc:190] parse op.input: composite_tmp_21
I0831 09:07:41.331544 27067 executor_cache.cc:190] parse op.input: elementwise_div_0@GRAD
I0831 09:07:41.331547 27067 executor_cache.cc:192] skip eager var: elementwise_div_0@GRAD
I0831 09:07:41.331550 27067 executor_cache.cc:190] parse op.input: _jst.0.args.0
I0831 09:07:41.331553 27067 executor_cache.cc:192] skip eager var: _jst.0.args.0
I0831 09:07:41.331557 27067 executor_cache.cc:190] parse op.input: composite_tmp_16
I0831 09:07:41.331560 27067 executor_cache.cc:190] parse op.input: composite_tmp_18
I0831 09:07:41.331562 27067 executor_cache.cc:190] parse op.input: _jst.0.args.1
I0831 09:07:41.331565 27067 executor_cache.cc:192] skip eager var: _jst.0.args.1
I0831 09:07:41.331569 27067 executor_cache.cc:190] parse op.input: composite_tmp_17
I0831 09:07:41.331571 27067 executor_cache.cc:190] parse op.input: composite_tmp_19
I0831 09:07:41.331574 27067 executor_cache.cc:196] Found skip_eager_delete_vars: 3
I0831 09:07:41.332041 27067 graph.cc:144] create OpNode by elementwise_div
I0831 09:07:41.332073 27067 graph.cc:219] kStaleProgramOpDescs.size: 1
I0831 09:07:41.332248 27067 parallel_executor.cc:1323] The Program will be executed on CPU using ParallelExecutor, 1 cards are used, so 1 programs are executed in parallel.
I0831 09:07:41.332262 27067 build_strategy.cc:360] apply all passes
I0831 09:07:41.332268 27067 pass_builder.cc:29] Append fuse_bn_add_act_pass
I0831 09:07:41.332275 27067 pass_builder.cc:29] Append coalesce_grad_tensor_pass
I0831 09:07:41.332283 27067 pass_builder.cc:29] Append add_reader_dependency_pass
I0831 09:07:41.332286 27067 pass_builder.cc:29] Append all_reduce_mode_multi_devices_pass
I0831 09:07:41.332294 27067 pass_builder.cc:29] Append fuse_all_reduce_op_pass
I0831 09:07:41.332299 27067 pass_builder.cc:29] Append all_reduce_deps_pass
I0831 09:07:41.332304 27067 pass_builder.cc:29] Append modify_op_lock_and_record_event_pass
I0831 09:07:41.332309 27067 pass_builder.cc:29] Append multi_devices_check_pass
I0831 09:07:41.332314 27067 build_strategy.cc:247] CollectiveContext:endpoints_:trainer_id_:0
I0831 09:07:41.332319 27067 build_strategy.cc:371] BuildStrategy::Apply pass:fuse_bn_add_act_pass
I0831 09:07:41.332324 27067 build_strategy.cc:474] fuse_bn_add_act_pass is only supported on GPU, skipped.
I0831 09:07:41.332327 27067 build_strategy.cc:371] BuildStrategy::Apply pass:coalesce_grad_tensor_pass
I0831 09:07:41.332332 27067 build_strategy.cc:488] Start Apply Pass coalesce_grad_tensor_pass
I0831 09:07:41.332336 27067 build_strategy.cc:491] Apply Pass coalesce_grad_tensor_passto SubGraph 0
I0831 09:07:41.332352 27067 onednn_context.cc:101] 0x55663860f990 0
I0831 09:07:41.332357 27067 onednn_context.cc:104] Clearing DNNL cache.
I0831 09:07:41.332360 27067 onednn_context.cc:122] Resetting Paddle data layout to NCHW.
I0831 09:07:41.332363 27067 build_strategy.cc:497] Finish Apply Pass coalesce_grad_tensor_pass
I0831 09:07:41.332366 27067 build_strategy.cc:371] BuildStrategy::Apply pass:add_reader_dependency_pass
I0831 09:07:41.332370 27067 build_strategy.cc:488] Start Apply Pass add_reader_dependency_pass
I0831 09:07:41.332373 27067 build_strategy.cc:491] Apply Pass add_reader_dependency_passto SubGraph 0
I0831 09:07:41.332381 27067 onednn_context.cc:101] 0x55663860f990 0
I0831 09:07:41.332384 27067 onednn_context.cc:104] Clearing DNNL cache.
I0831 09:07:41.332387 27067 onednn_context.cc:122] Resetting Paddle data layout to NCHW.
I0831 09:07:41.332391 27067 build_strategy.cc:497] Finish Apply Pass add_reader_dependency_pass
I0831 09:07:41.332393 27067 build_strategy.cc:371] BuildStrategy::Apply pass:all_reduce_mode_multi_devices_pass
I0831 09:07:41.332401 27067 build_strategy.cc:488] Start Apply Pass all_reduce_mode_multi_devices_pass
I0831 09:07:41.332404 27067 build_strategy.cc:491] Apply Pass all_reduce_mode_multi_devices_passto SubGraph 0
I0831 09:07:41.332484 27067 graph.h:183] deleting ops
I0831 09:07:41.332500 27067 onednn_context.cc:101] 0x55663860f990 0
I0831 09:07:41.332504 27067 onednn_context.cc:104] Clearing DNNL cache.
I0831 09:07:41.332513 27067 onednn_context.cc:122] Resetting Paddle data layout to NCHW.
I0831 09:07:41.332516 27067 build_strategy.cc:497] Finish Apply Pass all_reduce_mode_multi_devices_pass
I0831 09:07:41.332520 27067 build_strategy.cc:371] BuildStrategy::Apply pass:fuse_all_reduce_op_pass
I0831 09:07:41.332527 27067 build_strategy.cc:488] Start Apply Pass fuse_all_reduce_op_pass
I0831 09:07:41.332530 27067 build_strategy.cc:491] Apply Pass fuse_all_reduce_op_passto SubGraph 0
I0831 09:07:41.332537 27067 onednn_context.cc:101] 0x55663860f990 0
I0831 09:07:41.332541 27067 onednn_context.cc:104] Clearing DNNL cache.
I0831 09:07:41.332545 27067 onednn_context.cc:122] Resetting Paddle data layout to NCHW.
I0831 09:07:41.332547 27067 build_strategy.cc:497] Finish Apply Pass fuse_all_reduce_op_pass
I0831 09:07:41.332551 27067 build_strategy.cc:371] BuildStrategy::Apply pass:all_reduce_deps_pass
I0831 09:07:41.332554 27067 build_strategy.cc:452] SeqOnlyAllReduceOps:0, num_trainers:1
I0831 09:07:41.332558 27067 build_strategy.cc:488] Start Apply Pass all_reduce_deps_pass
I0831 09:07:41.332561 27067 build_strategy.cc:491] Apply Pass all_reduce_deps_passto SubGraph 0
I0831 09:07:41.332572 27067 onednn_context.cc:101] 0x55663860f990 0
I0831 09:07:41.332576 27067 onednn_context.cc:104] Clearing DNNL cache.
I0831 09:07:41.332579 27067 onednn_context.cc:122] Resetting Paddle data layout to NCHW.
I0831 09:07:41.332582 27067 build_strategy.cc:497] Finish Apply Pass all_reduce_deps_pass
I0831 09:07:41.332585 27067 build_strategy.cc:371] BuildStrategy::Apply pass:modify_op_lock_and_record_event_pass
I0831 09:07:41.332589 27067 build_strategy.cc:488] Start Apply Pass modify_op_lock_and_record_event_pass
I0831 09:07:41.332592 27067 build_strategy.cc:491] Apply Pass modify_op_lock_and_record_event_passto SubGraph 0
I0831 09:07:41.332602 27067 onednn_context.cc:101] 0x55663860f990 0
I0831 09:07:41.332604 27067 onednn_context.cc:104] Clearing DNNL cache.
I0831 09:07:41.332607 27067 onednn_context.cc:122] Resetting Paddle data layout to NCHW.
I0831 09:07:41.332610 27067 build_strategy.cc:497] Finish Apply Pass modify_op_lock_and_record_event_pass
I0831 09:07:41.332613 27067 build_strategy.cc:371] BuildStrategy::Apply pass:multi_devices_check_pass
I0831 09:07:41.332617 27067 build_strategy.cc:488] Start Apply Pass multi_devices_check_pass
I0831 09:07:41.332620 27067 build_strategy.cc:491] Apply Pass multi_devices_check_passto SubGraph 0
I0831 09:07:41.332633 27067 onednn_context.cc:101] 0x55663860f990 0
I0831 09:07:41.332635 27067 onednn_context.cc:104] Clearing DNNL cache.
I0831 09:07:41.332638 27067 onednn_context.cc:122] Resetting Paddle data layout to NCHW.
I0831 09:07:41.332641 27067 build_strategy.cc:497] Finish Apply Pass multi_devices_check_pass
I0831 09:07:41.332645 27067 build_strategy.cc:499] All Passes Applied
I0831 09:07:41.332657 27067 reference_count_pass.cc:289] Place number: 1
I0831 09:07:41.332662 27067 var_desc.cc:416] Flush  elementwise_div_0 1
I0831 09:07:41.332671 27067 var_desc.cc:416] Flush  _jst.0.args.0 1
I0831 09:07:41.332676 27067 var_desc.cc:416] Flush  _jst.0.args.1 1
I0831 09:07:41.332685 27067 onednn_context.cc:101] 0x55663860f990 0
I0831 09:07:41.332688 27067 onednn_context.cc:104] Clearing DNNL cache.
I0831 09:07:41.332691 27067 onednn_context.cc:122] Resetting Paddle data layout to NCHW.
I0831 09:07:41.332710 27067 onednn_context.cc:101] 0x55663860f990 0
I0831 09:07:41.332712 27067 onednn_context.cc:104] Clearing DNNL cache.
I0831 09:07:41.332715 27067 onednn_context.cc:122] Resetting Paddle data layout to NCHW.
I0831 09:07:41.332718 27067 parallel_executor.cc:482] Inplace strategy is enabled, when build_strategy.enable_inplace = True
I0831 09:07:41.332736 27067 var_desc.cc:416] Flush  _jst.0.args.0 0
I0831 09:07:41.332741 27067 var_desc.cc:416] Flush  _jst.0.args.1 0
I0831 09:07:41.332744 27067 var_desc.cc:416] Flush  elementwise_div_0 0
I0831 09:07:41.332768 27067 conditional_block_op_helper.cc:108] Found conditional_block op num: 0, conditional_block_grad op num: 0
I0831 09:07:41.332777 27067 graph_helper.h:104] adj elementwise_div0x556638688140 -> eager_deletion0x556638682480  via __control_var@90x556638682120
I0831 09:07:41.332787 27067 onednn_context.cc:101] 0x55663860f990 0
I0831 09:07:41.332790 27067 onednn_context.cc:104] Clearing DNNL cache.
I0831 09:07:41.332792 27067 onednn_context.cc:122] Resetting Paddle data layout to NCHW.
I0831 09:07:41.332798 27067 while_op_eager_deletion_pass.cc:58] Is Paritial Program
I0831 09:07:41.332803 27067 while_op_eager_deletion_pass.cc:82] Scope Idx = 0
I0831 09:07:41.332805 27067 while_op_eager_deletion_pass.cc:84] while_ops.size() = 0
I0831 09:07:41.332808 27067 while_op_eager_deletion_pass.cc:86] while_grad_ops.size() = 0
I0831 09:07:41.332813 27067 while_op_helper.cc:154] Found while op num: 0, while grad op num: 0
I0831 09:07:41.332816 27067 graph_helper.h:104] adj elementwise_div0x556638688140 -> eager_deletion0x556638682480  via __control_var@90x556638682120
I0831 09:07:41.332823 27067 onednn_context.cc:101] 0x55663860f990 0
I0831 09:07:41.332827 27067 onednn_context.cc:104] Clearing DNNL cache.
I0831 09:07:41.332829 27067 onednn_context.cc:122] Resetting Paddle data layout to NCHW.
I0831 09:07:41.332839 27067 recurrent_op_helper.cc:259] Found recurrent op num: 0, recurrent grad op num: 0
I0831 09:07:41.332844 27067 graph_helper.h:104] adj elementwise_div0x556638688140 -> eager_deletion0x556638682480  via __control_var@90x556638682120
I0831 09:07:41.332850 27067 onednn_context.cc:101] 0x55663860f990 0
I0831 09:07:41.332854 27067 onednn_context.cc:104] Clearing DNNL cache.
I0831 09:07:41.332856 27067 onednn_context.cc:122] Resetting Paddle data layout to NCHW.
I0831 09:07:41.332860 27067 graph_helper.h:104] adj elementwise_div0x556638688140 -> eager_deletion0x556638682480  via __control_var@90x556638682120
I0831 09:07:41.332867 27067 onednn_context.cc:101] 0x55663860f990 0
I0831 09:07:41.332870 27067 onednn_context.cc:104] Clearing DNNL cache.
I0831 09:07:41.332873 27067 onednn_context.cc:122] Resetting Paddle data layout to NCHW.
I0831 09:07:41.332876 27067 parallel_executor.cc:583] Garbage collection strategy is enabled, when FLAGS_eager_delete_tensor_gb = 0
I0831 09:07:41.332903 27067 parallel_executor.cc:1743] use FastThreadedSSAGraphExecutor
I0831 09:07:41.332952 27067 graph_helper.h:104] adj elementwise_div0x556638688140 -> eager_deletion0x556638682480  via __control_var@90x556638682120
I0831 09:07:41.332958 27067 graph_helper.h:104] adj elementwise_div0x556638688140 -> eager_deletion0x556638682480  via __control_var@90x556638682120
I0831 09:07:41.333014 27067 parallel_executor.cc:729] use ScopeBufferedSSAGraphExecutor
I0831 09:07:41.333024 27067 scope.cc:207] Create variable _jst.0.args.0
I0831 09:07:41.333029 27067 scope.cc:207] Create variable elementwise_div_0
I0831 09:07:41.333032 27067 scope.cc:207] Create variable _jst.0.args.1
I0831 09:07:41.333071 27067 var_desc.cc:416] Flush  _jst.0.args.0 0
I0831 09:07:41.333078 27067 var_desc.cc:416] Flush  elementwise_div_0 0
I0831 09:07:41.333083 27067 var_desc.cc:416] Flush  _jst.0.args.1 0
I0831 09:07:41.333094 27067 graph_helper.h:104] adj elementwise_div0x556638688140 -> eager_deletion0x556638682480  via __control_var@90x556638682120
I0831 09:07:41.333125 27067 graph_helper.cc:688] convert op node to desc elementwise_div
I0831 09:07:41.333132 27067 graph_helper.cc:666] 0 0
I0831 09:07:41.333218 27067 graph_helper.cc:811] Graph to program need convert 1 sub graph
I0831 09:07:41.333274 27067 graph_helper.h:104] adj elementwise_div0x556638688140 -> eager_deletion0x556638682480  via __control_var@90x556638682120
I0831 09:07:41.333285 27067 onednn_context.cc:101] 0x55663860f990 0
I0831 09:07:41.333288 27067 onednn_context.cc:104] Clearing DNNL cache.
I0831 09:07:41.333292 27067 onednn_context.cc:122] Resetting Paddle data layout to NCHW.
I0831 09:07:41.333536 27067 graph.h:183] deleting dep_vars
I0831 09:07:41.333545 27067 graph.h:183] deleting pass_recorder
I0831 09:07:41.333550 27067 graph.h:183] deleting skip_gc_vars
I0831 09:07:41.333554 27067 graph.h:183] deleting stale_program_op_descs
I0831 09:07:41.333564 27067 graph.h:183] deleting vars
I0831 09:07:41.333580 27067 var_handle.cc:35] deleting dummy var handle __control_var@9
I0831 09:07:41.333585 27067 var_handle.cc:35] deleting dummy var handle __control_var@10
I0831 09:07:41.333590 27067 var_handle.cc:23] deleting var handle name:_jst.0.args.0, place:Place(cpu), version:0, scope_idx:0
I0831 09:07:41.333597 27067 var_handle.cc:23] deleting var handle name:_jst.0.args.1, place:Place(cpu), version:0, scope_idx:0
I0831 09:07:41.333603 27067 var_handle.cc:23] deleting var handle name:elementwise_div_0, place:Place(cpu), version:0, scope_idx:0
I0831 09:07:41.333727 27067 executor_cache.cc:160] parse op type: fill_constant
I0831 09:07:41.333735 27067 executor_cache.cc:160] parse op type: elementwise_pow
I0831 09:07:41.333740 27067 executor_cache.cc:160] parse op type: elementwise_div
I0831 09:07:41.333745 27067 executor_cache.cc:160] parse op type: scale
I0831 09:07:41.333750 27067 executor_cache.cc:160] parse op type: elementwise_mul
I0831 09:07:41.333756 27067 executor_cache.cc:160] parse op type: fill_constant
I0831 09:07:41.333760 27067 executor_cache.cc:160] parse op type: elementwise_div
I0831 09:07:41.333765 27067 executor_cache.cc:160] parse op type: elementwise_mul
I0831 09:07:41.333770 27067 executor_cache.cc:190] parse op.input: composite_tmp_22
I0831 09:07:41.333773 27067 executor_cache.cc:190] parse op.input: composite_tmp_21
I0831 09:07:41.333776 27067 executor_cache.cc:190] parse op.input: elementwise_div_0@GRAD
I0831 09:07:41.333779 27067 executor_cache.cc:192] skip eager var: elementwise_div_0@GRAD
I0831 09:07:41.333783 27067 executor_cache.cc:190] parse op.input: _jst.0.args.0
I0831 09:07:41.333786 27067 executor_cache.cc:192] skip eager var: _jst.0.args.0
I0831 09:07:41.333789 27067 executor_cache.cc:190] parse op.input: composite_tmp_16
I0831 09:07:41.333792 27067 executor_cache.cc:190] parse op.input: composite_tmp_18
I0831 09:07:41.333796 27067 executor_cache.cc:190] parse op.input: _jst.0.args.1
I0831 09:07:41.333798 27067 executor_cache.cc:192] skip eager var: _jst.0.args.1
I0831 09:07:41.333802 27067 executor_cache.cc:190] parse op.input: composite_tmp_17
I0831 09:07:41.333806 27067 executor_cache.cc:190] parse op.input: composite_tmp_19
I0831 09:07:41.333808 27067 executor_cache.cc:196] Found skip_eager_delete_vars: 3
I0831 09:07:41.334093 27067 block_desc.cc:200] vars in desc 3
I0831 09:07:41.334103 27067 block_desc.cc:204] Flush _jst.0.args.0
I0831 09:07:41.334107 27067 var_desc.cc:416] Flush  _jst.0.args.0 1
I0831 09:07:41.334112 27067 block_desc.cc:204] Flush elementwise_div_0
I0831 09:07:41.334115 27067 var_desc.cc:416] Flush  elementwise_div_0 1
I0831 09:07:41.334120 27067 block_desc.cc:204] Flush _jst.0.args.1
I0831 09:07:41.334122 27067 var_desc.cc:416] Flush  _jst.0.args.1 1
I0831 09:07:41.334488 27067 block_desc.cc:200] vars in desc 3
I0831 09:07:41.334498 27067 block_desc.cc:204] Flush _jst.0.args.0
I0831 09:07:41.334501 27067 var_desc.cc:416] Flush  _jst.0.args.0 1
I0831 09:07:41.334506 27067 block_desc.cc:204] Flush elementwise_div_0
I0831 09:07:41.334508 27067 var_desc.cc:416] Flush  elementwise_div_0 1
I0831 09:07:41.334512 27067 block_desc.cc:204] Flush _jst.0.args.1
I0831 09:07:41.334515 27067 var_desc.cc:416] Flush  _jst.0.args.1 1
I0831 09:07:41.334532 27067 block_desc.cc:200] vars in desc 3
I0831 09:07:41.334669 27067 block_desc.cc:200] vars in desc 0
I0831 09:07:41.334676 27067 block_desc.cc:200] vars in desc 0
I0831 09:07:41.335680 27067 var_desc.cc:416] Flush  composite_tmp_16 1
I0831 09:07:41.335697 27067 var_desc.cc:416] Flush  composite_tmp_16 0
I0831 09:07:41.335701 27067 var_desc.cc:416] Flush  composite_tmp_17 1
I0831 09:07:41.335705 27067 var_desc.cc:416] Flush  composite_tmp_17 0
I0831 09:07:41.335709 27067 var_desc.cc:416] Flush  composite_tmp_18 1
I0831 09:07:41.335713 27067 var_desc.cc:416] Flush  composite_tmp_18 0
I0831 09:07:41.335716 27067 var_desc.cc:416] Flush  composite_tmp_19 1
I0831 09:07:41.335721 27067 var_desc.cc:416] Flush  composite_tmp_19 0
I0831 09:07:41.335724 27067 var_desc.cc:416] Flush  elementwise_div_0@GRAD 1
I0831 09:07:41.335736 27067 var_desc.cc:416] Flush  _jst.0.args.1@GRAD 1
I0831 09:07:41.335740 27067 var_desc.cc:416] Flush  composite_tmp_21 1
I0831 09:07:41.335744 27067 var_desc.cc:416] Flush  composite_tmp_21 0
I0831 09:07:41.335748 27067 var_desc.cc:416] Flush  composite_tmp_22 1
I0831 09:07:41.335752 27067 var_desc.cc:416] Flush  composite_tmp_22 0
I0831 09:07:41.335754 27067 var_desc.cc:416] Flush  elementwise_div_0@GRAD 0
I0831 09:07:41.335758 27067 var_desc.cc:416] Flush  _jst.0.args.0@GRAD 1
I0831 09:07:41.336374 27067 block_desc.cc:200] vars in desc 11
I0831 09:07:41.336383 27067 block_desc.cc:204] Flush _jst.0.args.0@GRAD
I0831 09:07:41.336387 27067 var_desc.cc:416] Flush  _jst.0.args.0@GRAD 1
I0831 09:07:41.336392 27067 block_desc.cc:204] Flush composite_tmp_22
I0831 09:07:41.336396 27067 var_desc.cc:416] Flush  composite_tmp_22 1
I0831 09:07:41.336400 27067 block_desc.cc:204] Flush composite_tmp_19
I0831 09:07:41.336403 27067 var_desc.cc:416] Flush  composite_tmp_19 1
I0831 09:07:41.336406 27067 block_desc.cc:204] Flush _jst.0.args.0
I0831 09:07:41.336409 27067 var_desc.cc:416] Flush  _jst.0.args.0 1
I0831 09:07:41.336413 27067 block_desc.cc:204] Flush composite_tmp_21
I0831 09:07:41.336416 27067 var_desc.cc:416] Flush  composite_tmp_21 1
I0831 09:07:41.336421 27067 block_desc.cc:204] Flush composite_tmp_18
I0831 09:07:41.336423 27067 var_desc.cc:416] Flush  composite_tmp_18 1
I0831 09:07:41.336427 27067 block_desc.cc:204] Flush _jst.0.args.1@GRAD
I0831 09:07:41.336431 27067 var_desc.cc:416] Flush  _jst.0.args.1@GRAD 1
I0831 09:07:41.336433 27067 block_desc.cc:204] Flush composite_tmp_17
I0831 09:07:41.336437 27067 var_desc.cc:416] Flush  composite_tmp_17 1
I0831 09:07:41.336441 27067 block_desc.cc:204] Flush _jst.0.args.1
I0831 09:07:41.336444 27067 var_desc.cc:416] Flush  _jst.0.args.1 1
I0831 09:07:41.336447 27067 block_desc.cc:204] Flush composite_tmp_16
I0831 09:07:41.336450 27067 var_desc.cc:416] Flush  composite_tmp_16 1
I0831 09:07:41.336454 27067 block_desc.cc:204] Flush elementwise_div_0@GRAD
I0831 09:07:41.336457 27067 var_desc.cc:416] Flush  elementwise_div_0@GRAD 1
I0831 09:07:41.337844 27067 block_desc.cc:200] vars in desc 11
I0831 09:07:41.337860 27067 block_desc.cc:204] Flush _jst.0.args.0@GRAD
I0831 09:07:41.337863 27067 var_desc.cc:416] Flush  _jst.0.args.0@GRAD 1
I0831 09:07:41.337869 27067 block_desc.cc:204] Flush composite_tmp_22
I0831 09:07:41.337872 27067 var_desc.cc:416] Flush  composite_tmp_22 1
I0831 09:07:41.337877 27067 block_desc.cc:204] Flush composite_tmp_19
I0831 09:07:41.337879 27067 var_desc.cc:416] Flush  composite_tmp_19 1
I0831 09:07:41.337882 27067 block_desc.cc:204] Flush _jst.0.args.0
I0831 09:07:41.337886 27067 var_desc.cc:416] Flush  _jst.0.args.0 1
I0831 09:07:41.337889 27067 block_desc.cc:204] Flush composite_tmp_21
I0831 09:07:41.337893 27067 var_desc.cc:416] Flush  composite_tmp_21 1
I0831 09:07:41.337896 27067 block_desc.cc:204] Flush composite_tmp_18
I0831 09:07:41.337899 27067 var_desc.cc:416] Flush  composite_tmp_18 1
I0831 09:07:41.337903 27067 block_desc.cc:204] Flush _jst.0.args.1@GRAD
I0831 09:07:41.337906 27067 var_desc.cc:416] Flush  _jst.0.args.1@GRAD 1
I0831 09:07:41.337910 27067 block_desc.cc:204] Flush composite_tmp_17
I0831 09:07:41.337913 27067 var_desc.cc:416] Flush  composite_tmp_17 1
I0831 09:07:41.337916 27067 block_desc.cc:204] Flush _jst.0.args.1
I0831 09:07:41.337920 27067 var_desc.cc:416] Flush  _jst.0.args.1 1
I0831 09:07:41.337924 27067 block_desc.cc:204] Flush composite_tmp_16
I0831 09:07:41.337926 27067 var_desc.cc:416] Flush  composite_tmp_16 1
I0831 09:07:41.337930 27067 block_desc.cc:204] Flush elementwise_div_0@GRAD
I0831 09:07:41.337934 27067 var_desc.cc:416] Flush  elementwise_div_0@GRAD 1
I0831 09:07:41.338029 27067 block_desc.cc:200] vars in desc 11
I0831 09:07:41.338464 27067 block_desc.cc:200] vars in desc 0
I0831 09:07:41.338472 27067 block_desc.cc:200] vars in desc 0
I0831 09:07:41.338933 27067 eager.cc:112] Tensor(elementwise_div_0) have not GradNode, add GradNodeAccumulation0x556638608c80 for it.
I0831 09:07:41.339099 27067 run_program_op_func.h:113] start run run_program with require_any_grad = 1
I0831 09:07:41.339107 27067 run_program_op_node.h:322] RunProgramOpKernel Compute
I0831 09:07:41.339112 27067 run_program_op_node.h:342] RunProgramOp use interpretercore to execute program.
I0831 09:07:41.339115 27067 run_program_op_node.h:346] global_inner_scope:0x556638ab9430
I0831 09:07:41.339123 27067 run_program_op_node.h:396] No interpretercore cahce, so create a new interpretercore for program: 7060865986609392811
I0831 09:07:41.339128 27067 scope.cc:207] Create variable _jst.0.args.0
I0831 09:07:41.339138 27067 scope.cc:207] Create variable _jst.0.args.1
I0831 09:07:41.339145 27067 interpretercore.cc:45] InterpreterCore(): 0x5566385cc5d0 on Place(cpu)
I0831 09:07:41.339277 27067 program_interpreter.cc:49] ProgramInterpreter(): 0x5566385e5670 on Place(cpu)
I0831 09:07:41.339293 27067 execution_config.cc:106] place:Place(cpu), processor_count:0, device_count:0, serial_run:0, num_host_threads:4, num_device_threads:0
I0831 09:07:41.339301 27067 new_executor_defs.cc:49] Set local scope: 0
I0831 09:07:41.339308 27067 executor_cache.cc:160] parse op type: fill_constant
I0831 09:07:41.339314 27067 executor_cache.cc:160] parse op type: elementwise_pow
I0831 09:07:41.339318 27067 executor_cache.cc:160] parse op type: elementwise_div
I0831 09:07:41.339323 27067 executor_cache.cc:160] parse op type: share_buffer
I0831 09:07:41.339326 27067 executor_cache.cc:162] skip share_buffer op
I0831 09:07:41.339329 27067 executor_cache.cc:160] parse op type: scale
I0831 09:07:41.339334 27067 executor_cache.cc:160] parse op type: elementwise_mul
I0831 09:07:41.339339 27067 executor_cache.cc:160] parse op type: fill_constant
I0831 09:07:41.339342 27067 executor_cache.cc:160] parse op type: elementwise_div
I0831 09:07:41.339347 27067 executor_cache.cc:160] parse op type: elementwise_mul
I0831 09:07:41.339351 27067 executor_cache.cc:190] parse op.input: composite_tmp_22
I0831 09:07:41.339355 27067 executor_cache.cc:190] parse op.input: composite_tmp_21
I0831 09:07:41.339358 27067 executor_cache.cc:190] parse op.input: elementwise_div_0@GRAD
I0831 09:07:41.339361 27067 executor_cache.cc:192] skip eager var: elementwise_div_0@GRAD
I0831 09:07:41.339365 27067 executor_cache.cc:190] parse op.input: _jst.0.args.0
I0831 09:07:41.339368 27067 executor_cache.cc:192] skip eager var: _jst.0.args.0
I0831 09:07:41.339371 27067 executor_cache.cc:190] parse op.input: composite_tmp_16
I0831 09:07:41.339375 27067 executor_cache.cc:190] parse op.input: composite_tmp_18
I0831 09:07:41.339377 27067 executor_cache.cc:190] parse op.input: _jst.0.args.1
I0831 09:07:41.339380 27067 executor_cache.cc:192] skip eager var: _jst.0.args.1
I0831 09:07:41.339383 27067 executor_cache.cc:190] parse op.input: composite_tmp_17
I0831 09:07:41.339386 27067 executor_cache.cc:190] parse op.input: composite_tmp_19
I0831 09:07:41.339390 27067 executor_cache.cc:196] Found skip_eager_delete_vars: 3
I0831 09:07:41.339399 27067 run_program_op_node.h:450] Get skip GC vars size is: 4
I0831 09:07:41.339407 27067 interpreter_util.cc:1105] Creating Variables
I0831 09:07:41.339414 27067 interpreter_util.cc:1142] Create Variable _jst.0.args.0 locally, which pointer is 0x556638644810 type is 7
I0831 09:07:41.339421 27067 interpreter_util.cc:1142] Create Variable _jst.0.args.1 locally, which pointer is 0x5566385c2180 type is 7
I0831 09:07:41.339426 27067 scope.cc:207] Create variable elementwise_div_0
I0831 09:07:41.339434 27067 interpreter_util.cc:1142] Create Variable elementwise_div_0 locally, which pointer is 0x556638a8ee70 type is 7
I0831 09:07:41.339469 27067 interpreter_util.cc:567] Static build: 0
I0831 09:07:41.339473 27067 mkldnn_helper.h:90] RegisterModelLayout for mkldnn
I0831 09:07:41.339480 27067 var_desc.cc:416] Flush  _jst.0.args.0 1
I0831 09:07:41.339485 27067 var_desc.cc:416] Flush  _jst.0.args.1 1
I0831 09:07:41.339489 27067 var_desc.cc:416] Flush  elementwise_div_0 1
I0831 09:07:41.339493 27067 interpreter_util.cc:281] elementwise_div elementwise_div_0
I0831 09:07:41.339504 27067 interpreter_util.cc:281] elementwise_div _jst.0.args.0
I0831 09:07:41.339507 27067 interpreter_util.cc:281] elementwise_div _jst.0.args.1
I0831 09:07:41.339510 27067 interpreter_util.cc:283] gc map size:1
I0831 09:07:41.339532 27067 interpreter_util.cc:677] Start run Place(cpu) Op(elementwise_div), inputs:{X[_jst.0.args.0:double[13, 17]({})(Place(cpu))], Y[_jst.0.args.1:double[13, 17]({})(Place(cpu))]}, outputs:{Out[elementwise_div_0:[]({})()]}.
I0831 09:07:41.339553 27067 interpreter_util.cc:687] OP is not null
I0831 09:07:41.339556 27067 interpreter_util.cc:690] get op_with_kernel
I0831 09:07:41.339560 27067 interpreter_util.cc:695] get RuntimeContext
I0831 09:07:41.339572 27067 interpreter_util.cc:724] expected_kernel_key : {data_type[double]; data_layout[Undefined(AnyLayout)]; place[Place(cpu)]; library_type[PLAIN]}
I0831 09:07:41.339591 27067 operator.cc:2207] op type:elementwise_div, expected_kernel_key:{data_type[double]; data_layout[Undefined(AnyLayout)]; place[Place(cpu)]; library_type[PLAIN]}
I0831 09:07:41.339597 27067 interpreter_util.cc:772] if run phi kernel? : 1
I0831 09:07:41.339601 27067 interpreter_util.cc:787] elementwise_div : finally selected kernel_key: {data_type[double]; data_layout[Undefined(AnyLayout)]; place[Place(cpu)]; library_type[PLAIN]}
I0831 09:07:41.339618 27067 interpreter_util.cc:802] apply data transform done. 
I0831 09:07:41.339622 27067 interpreter_util.cc:806] infer shape
I0831 09:07:41.339637 27067 operator.cc:3182] Done inputs
I0831 09:07:41.339641 27067 operator.cc:3244] Done outputs
I0831 09:07:41.339645 27067 operator.cc:3496] Done attributes
I0831 09:07:41.339651 27067 operator.cc:3031] Runtime attr `Scale_x` is passed to OneDNNContext.
I0831 09:07:41.339656 27067 operator.cc:3031] Runtime attr `mkldnn_data_type` is passed to OneDNNContext.
I0831 09:07:41.339660 27067 operator.cc:3031] Runtime attr `Scale_y` is passed to OneDNNContext.
I0831 09:07:41.339663 27067 operator.cc:3031] Runtime attr `Scale_out` is passed to OneDNNContext.
I0831 09:07:41.339668 27067 operator.cc:3546] Done runtime attributes
I0831 09:07:41.339671 27067 operator.cc:3576] Done runtime extra inputs
I0831 09:07:41.339696 27067 interpreter_util.cc:961] End run Place(cpu) Op(elementwise_div), inputs:{X[_jst.0.args.0:double[13, 17]({})(Place(cpu))], Y[_jst.0.args.1:double[13, 17]({})(Place(cpu))]}, outputs:{Out[elementwise_div_0:double[13, 17]({})(Place(cpu))]}.
I0831 09:07:41.339788 27067 program_interpreter.cc:1475] Analyze the execution order of Trace scheduling mode.
I0831 09:07:41.339797 27067 program_interpreter.cc:1465] Update sync op num, sync op num is: 1
I0831 09:07:41.339804 27067 run_program_op_node.h:246] share elementwise_div_0 from scope
I0831 09:07:41.339812 27067 run_program_op_node.h:492] 0x556638ab9430 
------------------------------------------

Details:

====
0x556638ab9430:
  - elementwise_div_0
  - _jst.0.args.0
  - _jst.0.args.1
I0831 09:07:41.339819 27067 run_program_op_node.h:501] not test, set this scope can not reused
I0831 09:07:41.339823 27067 run_program_op_func.h:118] start run run_program grad
I0831 09:07:41.339838 27067 grad_node_info.cc:445] Add Edges for slot: 0, the Edge is from GradNodeBase (addr: 0x5566385d35c0)  to GradNodeAccumulation (addr: 0x55663815ddd0)
I0831 09:07:41.339845 27067 grad_node_info.cc:445] Add Edges for slot: 0, the Edge is from GradNodeBase (addr: 0x5566385d35c0)  to GradNodeAccumulation (addr: 0x556638159970)
I0831 09:07:41.339850 27067 run_program_op_func.h:162] clear_no_grad_edges.
I0831 09:07:41.340082 27067 eager.cc:112] Tensor(generated_tensor_14) have not GradNode, add GradNodeAccumulation0x556638608c80 for it.
I0831 09:07:41.340147 27067 backward.cc:441] Run in Grad
I0831 09:07:41.340157 27067 backward.cc:112] Start Backward
I0831 09:07:41.340193 27067 general_grad.h:516] Copied Node: GradNodeBase ptr: 0x5566385d35c0 to ptr: 0x556638150250
I0831 09:07:41.340202 27067 backward.cc:202] Fill grad input tensor 0with give grad tensor
I0831 09:07:41.340210 27067 tensor_method.cc:93] Deep copy Tensor from generated_tensor_14 to 
I0831 09:07:41.340231 27067 tensor_utils.cc:57] TensorCopy 13, 17 from Place(cpu) to Place(cpu)
I0831 09:07:41.340242 27067 tensor_utils.cc:103] src:0x556638622000, dst:0x55663867f000
I0831 09:07:41.340246 27067 memcpy.cc:743] memory::Copy 1768 Bytes from Place(cpu) to Place(cpu)
I0831 09:07:41.340256 27067 general_grad.h:563] Copied Node: GradNodeAccumulation ptr: 0x55663815ddd0 to ptr: 0x5566386502d0
I0831 09:07:41.340261 27067 general_grad.h:563] Copied Node: GradNodeAccumulation ptr: 0x556638159970 to ptr: 0x55663864f170
I0831 09:07:41.340279 27067 backward.cc:248] Preparing GradNode:GradNodeBase addr:0x556638150250
I0831 09:07:41.340284 27067 run_program_op_node.h:688] Running Eager Backward Node: GradNodeRunProgram
I0831 09:07:41.340291 27067 run_program_op_node.h:709] hooked_grads[0].size() : 1
I0831 09:07:41.340301 27067 run_program_op_node.h:531] RunProgramGradOp use interpretercore to execute program.
I0831 09:07:41.340305 27067 run_program_op_node.h:533] global_inner_scope:0x556638ab9430
I0831 09:07:41.340309 27067 run_program_op_node.h:552] No interpretercore cahce, so create a new interpretercore
I0831 09:07:41.340315 27067 scope.cc:207] Create variable elementwise_div_0@GRAD
I0831 09:07:41.340323 27067 interpretercore.cc:45] InterpreterCore(): 0x5566385c7150 on Place(cpu)
I0831 09:07:41.340332 27067 program_interpreter.cc:49] ProgramInterpreter(): 0x556638192130 on Place(cpu)
I0831 09:07:41.340341 27067 execution_config.cc:106] place:Place(cpu), processor_count:0, device_count:0, serial_run:0, num_host_threads:4, num_device_threads:0
I0831 09:07:41.340349 27067 new_executor_defs.cc:49] Set local scope: 0
I0831 09:07:41.340601 27067 run_program_op_node.h:589] Share workqueue from 0x5566385cc5d0 to 0x5566385c7150
I0831 09:07:41.340602 27145 nonblocking_threadpool.h:258] HostTasks_thread_0 started 
I0831 09:07:41.340612 27067 run_program_op_node.h:614] Get skip GC vars size is: 2
I0831 09:07:41.340627 27146 nonblocking_threadpool.h:258] HostTasks_thread_1 started 
I0831 09:07:41.340633 27147 nonblocking_threadpool.h:258] HostTasks_thread_2 started 
I0831 09:07:41.340656 27146 os_info.cc:117] SetCurrentThreadName HostTasks_thread_1
I0831 09:07:41.340632 27145 os_info.cc:117] SetCurrentThreadName HostTasks_thread_0
I0831 09:07:41.340631 27067 run_program_op_node.h:641] 0x556638ab9430 
------------------------------------------

Details:

====
0x556638ab9430:
  - elementwise_div_0
  - _jst.0.args.0
  - elementwise_div_0@GRAD
  - _jst.0.args.1
I0831 09:07:41.340662 27147 os_info.cc:117] SetCurrentThreadName HostTasks_thread_2
I0831 09:07:41.340684 27067 interpreter_util.cc:1105] Creating Variables
I0831 09:07:41.340692 27067 interpreter_util.cc:1142] Create Variable _jst.0.args.0 locally, which pointer is 0x556638644810 type is 7
I0831 09:07:41.340699 27067 scope.cc:207] Create variable _jst.0.args.0@GRAD
I0831 09:07:41.340704 27067 interpreter_util.cc:1142] Create Variable _jst.0.args.0@GRAD locally, which pointer is 0x5566385a7950 type is 7
I0831 09:07:41.340709 27067 interpreter_util.cc:1142] Create Variable _jst.0.args.1 locally, which pointer is 0x5566385c2180 type is 7
I0831 09:07:41.340713 27067 scope.cc:207] Create variable _jst.0.args.1@GRAD
I0831 09:07:41.340718 27067 interpreter_util.cc:1142] Create Variable _jst.0.args.1@GRAD locally, which pointer is 0x5566381925a0 type is 7
I0831 09:07:41.340724 27067 scope.cc:207] Create variable composite_tmp_16
I0831 09:07:41.340728 27067 interpreter_util.cc:1142] Create Variable composite_tmp_16 locally, which pointer is 0x55663864f130 type is 7
I0831 09:07:41.340734 27067 scope.cc:207] Create variable composite_tmp_17
I0831 09:07:41.340739 27067 interpreter_util.cc:1142] Create Variable composite_tmp_17 locally, which pointer is 0x556638650260 type is 7
I0831 09:07:41.340742 27067 scope.cc:207] Create variable composite_tmp_18
I0831 09:07:41.340747 27067 interpreter_util.cc:1142] Create Variable composite_tmp_18 locally, which pointer is 0x556638169c80 type is 7
I0831 09:07:41.340751 27067 scope.cc:207] Create variable composite_tmp_19
I0831 09:07:41.340754 27148 nonblocking_threadpool.h:258] HostTasks_thread_3 started 
I0831 09:07:41.340757 27067 interpreter_util.cc:1142] Create Variable composite_tmp_19 locally, which pointer is 0x556638598330 type is 7
I0831 09:07:41.340770 27148 os_info.cc:117] SetCurrentThreadName HostTasks_thread_3
I0831 09:07:41.340778 27067 scope.cc:207] Create variable composite_tmp_21
I0831 09:07:41.340783 27067 interpreter_util.cc:1142] Create Variable composite_tmp_21 locally, which pointer is 0x556638623aa0 type is 7
I0831 09:07:41.340788 27067 scope.cc:207] Create variable composite_tmp_22
I0831 09:07:41.340793 27067 interpreter_util.cc:1142] Create Variable composite_tmp_22 locally, which pointer is 0x55663818fcb0 type is 7
I0831 09:07:41.340798 27067 interpreter_util.cc:1142] Create Variable elementwise_div_0@GRAD locally, which pointer is 0x5566385c5460 type is 7
I0831 09:07:41.341001 27067 interpreter_util.cc:567] Static build: 0
I0831 09:07:41.341006 27067 mkldnn_helper.h:90] RegisterModelLayout for mkldnn
I0831 09:07:41.341015 27067 var_desc.cc:416] Flush  composite_tmp_16 1
I0831 09:07:41.341020 27067 var_desc.cc:416] Flush  _jst.0.args.1 1
I0831 09:07:41.341024 27067 var_desc.cc:416] Flush  composite_tmp_16 0
I0831 09:07:41.341028 27067 var_desc.cc:416] Flush  composite_tmp_17 1
I0831 09:07:41.341032 27067 var_desc.cc:416] Flush  _jst.0.args.0 1
I0831 09:07:41.341035 27067 var_desc.cc:416] Flush  composite_tmp_17 0
I0831 09:07:41.341039 27067 var_desc.cc:416] Flush  composite_tmp_18 1
I0831 09:07:41.341042 27067 var_desc.cc:416] Flush  composite_tmp_18 0
I0831 09:07:41.341046 27067 var_desc.cc:416] Flush  composite_tmp_19 1
I0831 09:07:41.341050 27067 var_desc.cc:416] Flush  composite_tmp_18 0
I0831 09:07:41.341053 27067 var_desc.cc:416] Flush  composite_tmp_18 0
I0831 09:07:41.341058 27067 var_desc.cc:416] Flush  composite_tmp_19 0
I0831 09:07:41.341060 27067 var_desc.cc:416] Flush  composite_tmp_19 0
I0831 09:07:41.341064 27067 var_desc.cc:416] Flush  elementwise_div_0@GRAD 1
I0831 09:07:41.341068 27067 var_desc.cc:416] Flush  _jst.0.args.1@GRAD 1
I0831 09:07:41.341073 27067 var_desc.cc:416] Flush  composite_tmp_21 1
I0831 09:07:41.341075 27067 var_desc.cc:416] Flush  composite_tmp_21 0
I0831 09:07:41.341079 27067 var_desc.cc:416] Flush  _jst.0.args.1 0
I0831 09:07:41.341082 27067 var_desc.cc:416] Flush  composite_tmp_22 1
I0831 09:07:41.341085 27067 var_desc.cc:416] Flush  composite_tmp_22 0
I0831 09:07:41.341089 27067 var_desc.cc:416] Flush  elementwise_div_0@GRAD 0
I0831 09:07:41.341094 27067 var_desc.cc:416] Flush  _jst.0.args.0@GRAD 1
I0831 09:07:41.341097 27067 interpreter_util.cc:281] elementwise_mul _jst.0.args.0@GRAD
I0831 09:07:41.341101 27067 interpreter_util.cc:281] elementwise_mul composite_tmp_22
I0831 09:07:41.341104 27067 interpreter_util.cc:281] elementwise_div composite_tmp_21
I0831 09:07:41.341107 27067 interpreter_util.cc:281] elementwise_mul _jst.0.args.1@GRAD
I0831 09:07:41.341111 27067 interpreter_util.cc:281] elementwise_div _jst.0.args.1
I0831 09:07:41.341114 27067 interpreter_util.cc:281] elementwise_mul elementwise_div_0@GRAD
I0831 09:07:41.341117 27067 interpreter_util.cc:281] elementwise_div _jst.0.args.0
I0831 09:07:41.341121 27067 interpreter_util.cc:281] elementwise_pow composite_tmp_16
I0831 09:07:41.341125 27067 interpreter_util.cc:281] scale composite_tmp_18
I0831 09:07:41.341127 27067 interpreter_util.cc:281] elementwise_div composite_tmp_17
I0831 09:07:41.341131 27067 interpreter_util.cc:281] elementwise_mul composite_tmp_19
I0831 09:07:41.341135 27067 interpreter_util.cc:283] gc map size:6
I0831 09:07:41.341154 27067 interpreter_util.cc:677] Start run Place(cpu) Op(fill_constant), inputs:{ShapeTensor[], ShapeTensorList[], ValueTensor[]}, outputs:{Out[composite_tmp_16:[]({})()]}.
I0831 09:07:41.341187 27067 interpreter_util.cc:687] OP is not null
I0831 09:07:41.341190 27067 interpreter_util.cc:690] get op_with_kernel
I0831 09:07:41.341194 27067 interpreter_util.cc:695] get RuntimeContext
I0831 09:07:41.341204 27067 interpreter_util.cc:724] expected_kernel_key : {data_type[double]; data_layout[Undefined(AnyLayout)]; place[Place(cpu)]; library_type[PLAIN]}
I0831 09:07:41.341231 27067 operator.cc:2207] op type:fill_constant, expected_kernel_key:{data_type[double]; data_layout[Undefined(AnyLayout)]; place[Place(cpu)]; library_type[PLAIN]}
I0831 09:07:41.341238 27067 interpreter_util.cc:772] if run phi kernel? : 1
I0831 09:07:41.341241 27067 interpreter_util.cc:787] fill_constant : finally selected kernel_key: {data_type[double]; data_layout[Undefined(AnyLayout)]; place[Place(cpu)]; library_type[PLAIN]}
I0831 09:07:41.341259 27067 interpreter_util.cc:802] apply data transform done. 
I0831 09:07:41.341261 27067 interpreter_util.cc:806] infer shape
I0831 09:07:41.341272 27067 operator.cc:3182] Done inputs
I0831 09:07:41.341277 27067 operator.cc:3244] Done outputs
I0831 09:07:41.341290 27067 operator.cc:3496] Done attributes
I0831 09:07:41.341298 27067 operator.cc:3546] Done runtime attributes
I0831 09:07:41.341301 27067 operator.cc:3576] Done runtime extra inputs
I0831 09:07:41.341320 27067 interpreter_util.cc:961] End run Place(cpu) Op(fill_constant), inputs:{ShapeTensor[], ShapeTensorList[], ValueTensor[]}, outputs:{Out[composite_tmp_16:double[13, 17]({})(Place(cpu))]}.
I0831 09:07:41.341341 27067 interpreter_util.cc:677] Start run Place(cpu) Op(elementwise_pow), inputs:{X[_jst.0.args.1:double[13, 17]({})(Place(cpu))], Y[composite_tmp_16:double[13, 17]({})(Place(cpu))]}, outputs:{Out[composite_tmp_17:[]({})()]}.
I0831 09:07:41.341353 27067 interpreter_util.cc:687] OP is not null
I0831 09:07:41.341356 27067 interpreter_util.cc:690] get op_with_kernel
I0831 09:07:41.341360 27067 interpreter_util.cc:695] get RuntimeContext
I0831 09:07:41.341367 27067 interpreter_util.cc:724] expected_kernel_key : {data_type[double]; data_layout[Undefined(AnyLayout)]; place[Place(cpu)]; library_type[PLAIN]}
I0831 09:07:41.341378 27067 operator.cc:2207] op type:elementwise_pow, expected_kernel_key:{data_type[double]; data_layout[Undefined(AnyLayout)]; place[Place(cpu)]; library_type[PLAIN]}
I0831 09:07:41.341384 27067 interpreter_util.cc:772] if run phi kernel? : 1
I0831 09:07:41.341387 27067 interpreter_util.cc:787] elementwise_pow : finally selected kernel_key: {data_type[double]; data_layout[Undefined(AnyLayout)]; place[Place(cpu)]; library_type[PLAIN]}
I0831 09:07:41.341403 27067 interpreter_util.cc:802] apply data transform done. 
I0831 09:07:41.341406 27067 interpreter_util.cc:806] infer shape
I0831 09:07:41.341411 27067 infershape_utils.cc:547] BuildInferMetaContext: op kernel signature - Kernel Signature - name: elementwise_pow_raw; inputs: X, Y; attributes: axis; outputs: Out
I0831 09:07:41.341434 27067 operator.cc:3182] Done inputs
I0831 09:07:41.341437 27067 operator.cc:3244] Done outputs
I0831 09:07:41.341441 27067 operator.cc:3496] Done attributes
I0831 09:07:41.341445 27067 operator.cc:3031] Runtime attr `Scale_x` is passed to OneDNNContext.
I0831 09:07:41.341449 27067 operator.cc:3031] Runtime attr `mkldnn_data_type` is passed to OneDNNContext.
I0831 09:07:41.341454 27067 operator.cc:3031] Runtime attr `Scale_y` is passed to OneDNNContext.
I0831 09:07:41.341457 27067 operator.cc:3031] Runtime attr `Scale_out` is passed to OneDNNContext.
I0831 09:07:41.341461 27067 operator.cc:3546] Done runtime attributes
I0831 09:07:41.341464 27067 operator.cc:3576] Done runtime extra inputs
I0831 09:07:41.341480 27067 interpreter_util.cc:961] End run Place(cpu) Op(elementwise_pow), inputs:{X[_jst.0.args.1:double[13, 17]({})(Place(cpu))], Y[composite_tmp_16:double[13, 17]({})(Place(cpu))]}, outputs:{Out[composite_tmp_17:double[13, 17]({})(Place(cpu))]}.
I0831 09:07:41.341504 27067 interpreter_util.cc:677] Start run Place(cpu) Op(elementwise_div), inputs:{X[_jst.0.args.0:double[13, 17]({})(Place(cpu))], Y[composite_tmp_17:double[13, 17]({})(Place(cpu))]}, outputs:{Out[composite_tmp_18:[]({})()]}.
I0831 09:07:41.341517 27067 interpreter_util.cc:687] OP is not null
I0831 09:07:41.341521 27067 interpreter_util.cc:690] get op_with_kernel
I0831 09:07:41.341523 27067 interpreter_util.cc:695] get RuntimeContext
I0831 09:07:41.341529 27067 interpreter_util.cc:724] expected_kernel_key : {data_type[double]; data_layout[Undefined(AnyLayout)]; place[Place(cpu)]; library_type[PLAIN]}
I0831 09:07:41.341544 27067 operator.cc:2207] op type:elementwise_div, expected_kernel_key:{data_type[double]; data_layout[Undefined(AnyLayout)]; place[Place(cpu)]; library_type[PLAIN]}
I0831 09:07:41.341550 27067 interpreter_util.cc:772] if run phi kernel? : 1
I0831 09:07:41.341553 27067 interpreter_util.cc:787] elementwise_div : finally selected kernel_key: {data_type[double]; data_layout[Undefined(AnyLayout)]; place[Place(cpu)]; library_type[PLAIN]}
I0831 09:07:41.341567 27067 interpreter_util.cc:802] apply data transform done. 
I0831 09:07:41.341571 27067 interpreter_util.cc:806] infer shape
I0831 09:07:41.341580 27067 operator.cc:3182] Done inputs
I0831 09:07:41.341584 27067 operator.cc:3244] Done outputs
I0831 09:07:41.341588 27067 operator.cc:3496] Done attributes
I0831 09:07:41.341590 27067 operator.cc:3031] Runtime attr `Scale_x` is passed to OneDNNContext.
I0831 09:07:41.341594 27067 operator.cc:3031] Runtime attr `mkldnn_data_type` is passed to OneDNNContext.
I0831 09:07:41.341598 27067 operator.cc:3031] Runtime attr `Scale_y` is passed to OneDNNContext.
I0831 09:07:41.341601 27067 operator.cc:3031] Runtime attr `Scale_out` is passed to OneDNNContext.
I0831 09:07:41.341605 27067 operator.cc:3546] Done runtime attributes
I0831 09:07:41.341609 27067 operator.cc:3576] Done runtime extra inputs
I0831 09:07:41.341625 27067 interpreter_util.cc:961] End run Place(cpu) Op(elementwise_div), inputs:{X[_jst.0.args.0:double[13, 17]({})(Place(cpu))], Y[composite_tmp_17:double[13, 17]({})(Place(cpu))]}, outputs:{Out[composite_tmp_18:double[13, 17]({})(Place(cpu))]}.
I0831 09:07:41.341648 27067 interpreter_util.cc:677] Start run Place(cpu) Op(share_buffer), inputs:{X[composite_tmp_18:double[13, 17]({})(Place(cpu))]}, outputs:{Out[composite_tmp_19:[]({})()], XOut[composite_tmp_18:double[13, 17]({})(Place(cpu))]}.
I0831 09:07:41.341660 27067 interpreter_util.cc:687] OP is not null
I0831 09:07:41.341665 27067 interpreter_util.cc:690] get op_with_kernel
I0831 09:07:41.341667 27067 interpreter_util.cc:695] get RuntimeContext
I0831 09:07:41.341673 27067 interpreter_util.cc:724] expected_kernel_key : {data_type[double]; data_layout[Undefined(AnyLayout)]; place[Place(cpu)]; library_type[PLAIN]}
I0831 09:07:41.341684 27067 operator.cc:2207] op type:share_buffer, expected_kernel_key:{data_type[double]; data_layout[Undefined(AnyLayout)]; place[Place(cpu)]; library_type[PLAIN]}
I0831 09:07:41.341689 27067 interpreter_util.cc:772] if run phi kernel? : 1
I0831 09:07:41.341692 27067 interpreter_util.cc:787] share_buffer : finally selected kernel_key: {data_type[double]; data_layout[Undefined(AnyLayout)]; place[Place(cpu)]; library_type[PLAIN]}
I0831 09:07:41.341701 27067 interpreter_util.cc:802] apply data transform done. 
I0831 09:07:41.341704 27067 interpreter_util.cc:806] infer shape
I0831 09:07:41.341708 27067 infershape_utils.cc:547] BuildInferMetaContext: op kernel signature - Kernel Signature - name: share_buffer; inputs: X; attributes: share_dims_and_dtype; outputs: Out, XOut
I0831 09:07:41.341723 27067 operator.cc:3182] Done inputs
I0831 09:07:41.341727 27067 operator.cc:3244] Done outputs
I0831 09:07:41.341732 27067 operator.cc:3496] Done attributes
I0831 09:07:41.341735 27067 operator.cc:3546] Done runtime attributes
I0831 09:07:41.341739 27067 operator.cc:3576] Done runtime extra inputs
I0831 09:07:41.341745 27067 interpreter_util.cc:961] End run Place(cpu) Op(share_buffer), inputs:{X[composite_tmp_18:double[13, 17]({})(Place(cpu))]}, outputs:{Out[composite_tmp_19:double[]({})(Place(cpu))], XOut[composite_tmp_18:double[13, 17]({})(Place(cpu))]}.
I0831 09:07:41.341765 27067 interpreter_util.cc:677] Start run Place(cpu) Op(scale), inputs:{ScaleTensor[], X[composite_tmp_18:double[13, 17]({})(Place(cpu))]}, outputs:{Out[composite_tmp_19:double[]({})(Place(cpu))]}.
I0831 09:07:41.341776 27067 interpreter_util.cc:687] OP is not null
I0831 09:07:41.341779 27067 interpreter_util.cc:690] get op_with_kernel
I0831 09:07:41.341787 27067 interpreter_util.cc:695] get RuntimeContext
I0831 09:07:41.341794 27067 interpreter_util.cc:724] expected_kernel_key : {data_type[double]; data_layout[Undefined(AnyLayout)]; place[Place(cpu)]; library_type[PLAIN]}
I0831 09:07:41.341804 27067 operator.cc:2207] op type:scale, expected_kernel_key:{data_type[double]; data_layout[Undefined(AnyLayout)]; place[Place(cpu)]; library_type[PLAIN]}
I0831 09:07:41.341810 27067 interpreter_util.cc:772] if run phi kernel? : 1
I0831 09:07:41.341814 27067 interpreter_util.cc:787] scale : finally selected kernel_key: {data_type[double]; data_layout[Undefined(AnyLayout)]; place[Place(cpu)]; library_type[PLAIN]}
I0831 09:07:41.341827 27067 interpreter_util.cc:802] apply data transform done. 
I0831 09:07:41.341830 27067 interpreter_util.cc:806] infer shape
I0831 09:07:41.341836 27067 infershape_utils.cc:547] BuildInferMetaContext: op kernel signature - Kernel Signature - name: scale; inputs: X; attributes: scale, bias, bias_after_scale; outputs: Out
I0831 09:07:41.341850 27067 operator.cc:3182] Done inputs
I0831 09:07:41.341854 27067 operator.cc:3244] Done outputs
I0831 09:07:41.341858 27067 operator.cc:3496] Done attributes
I0831 09:07:41.341862 27067 operator.cc:3546] Done runtime attributes
I0831 09:07:41.341866 27067 operator.cc:3576] Done runtime extra inputs
I0831 09:07:41.341881 27067 interpreter_util.cc:961] End run Place(cpu) Op(scale), inputs:{ScaleTensor[], X[composite_tmp_18:double[13, 17]({})(Place(cpu))]}, outputs:{Out[composite_tmp_19:double[13, 17]({})(Place(cpu))]}.
I0831 09:07:41.341902 27067 interpreter_util.cc:677] Start run Place(cpu) Op(elementwise_mul), inputs:{X[composite_tmp_19:double[13, 17]({})(Place(cpu))], Y[elementwise_div_0@GRAD:double[13, 17]({})(Place(cpu))]}, outputs:{Out[_jst.0.args.1@GRAD:[]({})()]}.
I0831 09:07:41.341913 27067 interpreter_util.cc:687] OP is not null
I0831 09:07:41.341917 27067 interpreter_util.cc:690] get op_with_kernel
I0831 09:07:41.341919 27067 interpreter_util.cc:695] get RuntimeContext
I0831 09:07:41.341924 27067 interpreter_util.cc:724] expected_kernel_key : {data_type[double]; data_layout[Undefined(AnyLayout)]; place[Place(cpu)]; library_type[PLAIN]}
I0831 09:07:41.341935 27067 operator.cc:2207] op type:elementwise_mul, expected_kernel_key:{data_type[double]; data_layout[Undefined(AnyLayout)]; place[Place(cpu)]; library_type[PLAIN]}
I0831 09:07:41.341940 27067 interpreter_util.cc:772] if run phi kernel? : 1
I0831 09:07:41.341944 27067 interpreter_util.cc:787] elementwise_mul : finally selected kernel_key: {data_type[double]; data_layout[Undefined(AnyLayout)]; place[Place(cpu)]; library_type[PLAIN]}
I0831 09:07:41.341959 27067 interpreter_util.cc:802] apply data transform done. 
I0831 09:07:41.341962 27067 interpreter_util.cc:806] infer shape
I0831 09:07:41.341970 27067 operator.cc:3182] Done inputs
I0831 09:07:41.341974 27067 operator.cc:3244] Done outputs
I0831 09:07:41.341976 27067 operator.cc:3496] Done attributes
I0831 09:07:41.341980 27067 operator.cc:3031] Runtime attr `Scale_x` is passed to OneDNNContext.
I0831 09:07:41.341984 27067 operator.cc:3031] Runtime attr `mkldnn_data_type` is passed to OneDNNContext.
I0831 09:07:41.341989 27067 operator.cc:3031] Runtime attr `Scale_y` is passed to OneDNNContext.
I0831 09:07:41.341991 27067 operator.cc:3031] Runtime attr `Scale_out` is passed to OneDNNContext.
I0831 09:07:41.341995 27067 operator.cc:3546] Done runtime attributes
I0831 09:07:41.341998 27067 operator.cc:3576] Done runtime extra inputs
I0831 09:07:41.342010 27067 interpreter_util.cc:961] End run Place(cpu) Op(elementwise_mul), inputs:{X[composite_tmp_19:double[13, 17]({})(Place(cpu))], Y[elementwise_div_0@GRAD:double[13, 17]({})(Place(cpu))]}, outputs:{Out[_jst.0.args.1@GRAD:double[13, 17]({})(Place(cpu))]}.
I0831 09:07:41.342032 27067 interpreter_util.cc:677] Start run Place(cpu) Op(fill_constant), inputs:{ShapeTensor[], ShapeTensorList[], ValueTensor[]}, outputs:{Out[composite_tmp_21:[]({})()]}.
I0831 09:07:41.342041 27067 interpreter_util.cc:687] OP is not null
I0831 09:07:41.342043 27067 interpreter_util.cc:690] get op_with_kernel
I0831 09:07:41.342051 27067 interpreter_util.cc:695] get RuntimeContext
I0831 09:07:41.342056 27067 interpreter_util.cc:724] expected_kernel_key : {data_type[double]; data_layout[Undefined(AnyLayout)]; place[Place(cpu)]; library_type[PLAIN]}
I0831 09:07:41.342068 27067 operator.cc:2207] op type:fill_constant, expected_kernel_key:{data_type[double]; data_layout[Undefined(AnyLayout)]; place[Place(cpu)]; library_type[PLAIN]}
I0831 09:07:41.342074 27067 interpreter_util.cc:772] if run phi kernel? : 1
I0831 09:07:41.342077 27067 interpreter_util.cc:787] fill_constant : finally selected kernel_key: {data_type[double]; data_layout[Undefined(AnyLayout)]; place[Place(cpu)]; library_type[PLAIN]}
I0831 09:07:41.342092 27067 interpreter_util.cc:802] apply data transform done. 
I0831 09:07:41.342096 27067 interpreter_util.cc:806] infer shape
I0831 09:07:41.342103 27067 operator.cc:3182] Done inputs
I0831 09:07:41.342106 27067 operator.cc:3244] Done outputs
I0831 09:07:41.342114 27067 operator.cc:3496] Done attributes
I0831 09:07:41.342119 27067 operator.cc:3546] Done runtime attributes
I0831 09:07:41.342123 27067 operator.cc:3576] Done runtime extra inputs
I0831 09:07:41.342133 27067 interpreter_util.cc:961] End run Place(cpu) Op(fill_constant), inputs:{ShapeTensor[], ShapeTensorList[], ValueTensor[]}, outputs:{Out[composite_tmp_21:double[13, 17]({})(Place(cpu))]}.
I0831 09:07:41.342149 27067 interpreter_util.cc:677] Start run Place(cpu) Op(elementwise_div), inputs:{X[composite_tmp_21:double[13, 17]({})(Place(cpu))], Y[_jst.0.args.1:double[13, 17]({})(Place(cpu))]}, outputs:{Out[composite_tmp_22:[]({})()]}.
I0831 09:07:41.342161 27067 interpreter_util.cc:687] OP is not null
I0831 09:07:41.342171 27067 interpreter_util.cc:690] get op_with_kernel
I0831 09:07:41.342175 27067 interpreter_util.cc:695] get RuntimeContext
I0831 09:07:41.342180 27067 interpreter_util.cc:724] expected_kernel_key : {data_type[double]; data_layout[Undefined(AnyLayout)]; place[Place(cpu)]; library_type[PLAIN]}
I0831 09:07:41.342188 27067 operator.cc:2207] op type:elementwise_div, expected_kernel_key:{data_type[double]; data_layout[Undefined(AnyLayout)]; place[Place(cpu)]; library_type[PLAIN]}
I0831 09:07:41.342193 27067 interpreter_util.cc:772] if run phi kernel? : 1
I0831 09:07:41.342197 27067 interpreter_util.cc:787] elementwise_div : finally selected kernel_key: {data_type[double]; data_layout[Undefined(AnyLayout)]; place[Place(cpu)]; library_type[PLAIN]}
I0831 09:07:41.342211 27067 interpreter_util.cc:802] apply data transform done. 
I0831 09:07:41.342213 27067 interpreter_util.cc:806] infer shape
I0831 09:07:41.342221 27067 operator.cc:3182] Done inputs
I0831 09:07:41.342226 27067 operator.cc:3244] Done outputs
I0831 09:07:41.342228 27067 operator.cc:3496] Done attributes
I0831 09:07:41.342232 27067 operator.cc:3031] Runtime attr `Scale_x` is passed to OneDNNContext.
I0831 09:07:41.342236 27067 operator.cc:3031] Runtime attr `mkldnn_data_type` is passed to OneDNNContext.
I0831 09:07:41.342240 27067 operator.cc:3031] Runtime attr `Scale_y` is passed to OneDNNContext.
I0831 09:07:41.342243 27067 operator.cc:3031] Runtime attr `Scale_out` is passed to OneDNNContext.
I0831 09:07:41.342247 27067 operator.cc:3546] Done runtime attributes
I0831 09:07:41.342250 27067 operator.cc:3576] Done runtime extra inputs
I0831 09:07:41.342259 27067 interpreter_util.cc:961] End run Place(cpu) Op(elementwise_div), inputs:{X[composite_tmp_21:double[13, 17]({})(Place(cpu))], Y[_jst.0.args.1:double[13, 17]({})(Place(cpu))]}, outputs:{Out[composite_tmp_22:double[13, 17]({})(Place(cpu))]}.
I0831 09:07:41.342291 27067 interpreter_util.cc:677] Start run Place(cpu) Op(elementwise_mul), inputs:{X[composite_tmp_22:double[13, 17]({})(Place(cpu))], Y[elementwise_div_0@GRAD:double[13, 17]({})(Place(cpu))]}, outputs:{Out[_jst.0.args.0@GRAD:[]({})()]}.
I0831 09:07:41.342304 27067 interpreter_util.cc:687] OP is not null
I0831 09:07:41.342308 27067 interpreter_util.cc:690] get op_with_kernel
I0831 09:07:41.342311 27067 interpreter_util.cc:695] get RuntimeContext
I0831 09:07:41.342324 27067 interpreter_util.cc:724] expected_kernel_key : {data_type[double]; data_layout[Undefined(AnyLayout)]; place[Place(cpu)]; library_type[PLAIN]}
I0831 09:07:41.342334 27067 operator.cc:2207] op type:elementwise_mul, expected_kernel_key:{data_type[double]; data_layout[Undefined(AnyLayout)]; place[Place(cpu)]; library_type[PLAIN]}
I0831 09:07:41.342339 27067 interpreter_util.cc:772] if run phi kernel? : 1
I0831 09:07:41.342342 27067 interpreter_util.cc:787] elementwise_mul : finally selected kernel_key: {data_type[double]; data_layout[Undefined(AnyLayout)]; place[Place(cpu)]; library_type[PLAIN]}
I0831 09:07:41.342358 27067 interpreter_util.cc:802] apply data transform done. 
I0831 09:07:41.342362 27067 interpreter_util.cc:806] infer shape
I0831 09:07:41.342370 27067 operator.cc:3182] Done inputs
I0831 09:07:41.342373 27067 operator.cc:3244] Done outputs
I0831 09:07:41.342377 27067 operator.cc:3496] Done attributes
I0831 09:07:41.342381 27067 operator.cc:3031] Runtime attr `Scale_x` is passed to OneDNNContext.
I0831 09:07:41.342386 27067 operator.cc:3031] Runtime attr `mkldnn_data_type` is passed to OneDNNContext.
I0831 09:07:41.342389 27067 operator.cc:3031] Runtime attr `Scale_y` is passed to OneDNNContext.
I0831 09:07:41.342392 27067 operator.cc:3031] Runtime attr `Scale_out` is passed to OneDNNContext.
I0831 09:07:41.342396 27067 operator.cc:3546] Done runtime attributes
I0831 09:07:41.342401 27067 operator.cc:3576] Done runtime extra inputs
I0831 09:07:41.342422 27067 interpreter_util.cc:961] End run Place(cpu) Op(elementwise_mul), inputs:{X[composite_tmp_22:double[13, 17]({})(Place(cpu))], Y[elementwise_div_0@GRAD:double[13, 17]({})(Place(cpu))]}, outputs:{Out[_jst.0.args.0@GRAD:double[13, 17]({})(Place(cpu))]}.
I0831 09:07:41.342693 27067 program_interpreter.cc:785] Already inplaced, skip inplace now.
I0831 09:07:41.342705 27067 program_interpreter.cc:1475] Analyze the execution order of Trace scheduling mode.
I0831 09:07:41.342712 27067 program_interpreter.cc:1481] op_id: 7, remain deps: 1
I0831 09:07:41.342717 27067 program_interpreter.cc:1481] op_id: 8, remain deps: 1
I0831 09:07:41.342721 27067 program_interpreter.cc:1481] op_id: 1, remain deps: 1
I0831 09:07:41.342725 27067 program_interpreter.cc:1481] op_id: 2, remain deps: 1
I0831 09:07:41.342728 27067 program_interpreter.cc:1481] op_id: 3, remain deps: 1
I0831 09:07:41.342731 27067 program_interpreter.cc:1481] op_id: 4, remain deps: 1
I0831 09:07:41.342736 27067 program_interpreter.cc:1481] op_id: 5, remain deps: 1
I0831 09:07:41.342743 27067 program_interpreter.cc:1465] Update sync op num, sync op num is: 9
I0831 09:07:41.342752 27067 run_program_op_node.h:246] share _jst.0.args.0@GRAD from scope
I0831 09:07:41.342761 27067 run_program_op_node.h:246] share _jst.0.args.1@GRAD from scope
I0831 09:07:41.342764 27067 run_program_op_node.h:657] after backward gc all vars
I0831 09:07:41.342772 27067 run_program_op_node.h:734] End Eager Backward Node: GradNodeRunProgram
I0831 09:07:41.342778 27067 backward.cc:288] retain_graph is false, need to clear the TensorWrapper of nodes.
I0831 09:07:41.342783 27067 backward.cc:317] Node: GradNodeBase addr:0x556638150250, Found pending node: GradNodeAccumulation addr: 0x5566386502d0
I0831 09:07:41.342792 27067 backward.cc:358] Sum or Move grad inputs for edge slot: 0, rank: 0
I0831 09:07:41.342797 27067 backward.cc:317] Node: GradNodeBase addr:0x556638150250, Found pending node: GradNodeAccumulation addr: 0x55663864f170
I0831 09:07:41.342800 27067 backward.cc:358] Sum or Move grad inputs for edge slot: 0, rank: 0
I0831 09:07:41.342808 27067 backward.cc:248] Preparing GradNode:GradNodeAccumulation addr:0x55663864f170
I0831 09:07:41.342811 27067 accumulation_node.cc:139] Running AD API Grad: GradNodeAccumulation
I0831 09:07:41.342815 27067 accumulation_node.cc:174] Finish AD API Grad: GradNodeAccumulation
I0831 09:07:41.342833 27067 accumulation_node.cc:187] { Input: [], Output: [(grad_out, [{ Name: _jst.0.args.1@GRAD, Initialized: 1, Ptr: 0x55663857a820 }]), ] } 
I0831 09:07:41.342842 27067 backward.cc:288] retain_graph is false, need to clear the TensorWrapper of nodes.
I0831 09:07:41.342852 27067 backward.cc:248] Preparing GradNode:GradNodeAccumulation addr:0x5566386502d0
I0831 09:07:41.342856 27067 accumulation_node.cc:139] Running AD API Grad: GradNodeAccumulation
I0831 09:07:41.342860 27067 accumulation_node.cc:174] Finish AD API Grad: GradNodeAccumulation
I0831 09:07:41.342871 27067 accumulation_node.cc:187] { Input: [], Output: [(grad_out, [{ Name: _jst.0.args.0@GRAD, Initialized: 1, Ptr: 0x5566386404f0 }]), ] } 
I0831 09:07:41.342876 27067 backward.cc:288] retain_graph is false, need to clear the TensorWrapper of nodes.
I0831 09:07:41.342881 27067 backward.cc:417] Finish Backward
I0831 09:07:41.342898 27067 eager_functions.cc:177]  in eager_api_run_partial_grad, after runing egr::Grad
I0831 09:07:41.342965 27067 memcpy.cc:119] src: 0x556638aab000, dst: 0x55663867f000, num: 1768
I0831 09:07:41.342979 27067 memcpy.cc:119] src: 0x556638a9c000, dst: 0x556638adb000, num: 1768
I0831 09:07:41.351866 27067 run_program_op_node.h:671] ~GradNodeRunProgram
I0831 09:07:41.351905 27067 run_program_op_node.h:677] global_inner_scope SetCanReused
I0831 09:07:41.351960 27067 imperative.cc:691] Tracer(0x556636b47f70) set expected place Place(gpu:0)
I0831 09:07:41.352097 27067 scope.cc:207] Create variable X
I0831 09:07:41.352125 27067 scope.cc:207] Create variable Y
I0831 09:07:41.352139 27067 scope.cc:207] Create variable Out
I0831 09:07:41.352263 27067 op_registry.cc:112] CreateOp directly from OpDesc is deprecated. It should only beused in unit tests. Use CreateOp(const OpDesc& op_desc) instead.
I0831 09:07:41.352453 27067 global_value_getter_setter.cc:191] set FLAGS_enable_new_ir_api to False
I0831 09:07:41.352478 27067 global_value_getter_setter.cc:191] set FLAGS_enable_new_ir_api to False
I0831 09:07:41.352485 27067 global_value_getter_setter.cc:191] set FLAGS_enable_new_ir_api to True
I0831 09:07:41.352497 27067 global_value_getter_setter.cc:191] set FLAGS_enable_new_ir_in_executor to True
I0831 09:07:41.352504 27067 ir_context.cc:249] Try to get or register a Dialect of: [name=pd].
I0831 09:07:41.352509 27067 ir_context.cc:252] Create and register a new Dialect of: [name=pd].
I0831 09:07:41.354126 27067 dialect.cc:24] Register interface into dialect
I0831 09:07:41.354373 27067 imperative.cc:701] Tracer(0x556638a9b690) set expected place Place(cpu)
I0831 09:07:41.354471 27067 eager.cc:112] Tensor(generated_tensor_0) have not GradNode, add GradNodeAccumulation0x556638608c80 for it.
I0831 09:07:41.354529 27067 eager.cc:112] Tensor(generated_tensor_1) have not GradNode, add GradNodeAccumulation0x55663815ddd0 for it.
I0831 09:07:41.354578 27067 eager.cc:112] Tensor(Out_out_0) have not GradNode, add GradNodeAccumulation0x556638159970 for it.
I0831 09:07:41.354674 27067 imperative.cc:691] Tracer(0x556638a9b690) set expected place Place(gpu:0)
I0831 09:07:41.354715 27067 ir_context.cc:92] Found a cached abstract_attribute of: [TypeId_hash=140104691385512, AbstractAttribute_ptr=0x5566367b2550].
I0831 09:07:41.354725 27067 builtin_op.cc:58] Verifying inputs, outputs and attributes for: ModuleOp.
I0831 09:07:41.354842 27067 pd_op.cc:11566] Builder construction inputs
I0831 09:07:41.354848 27067 pd_op.cc:11568] Builder construction attributes
I0831 09:07:41.354856 27067 ir_context.cc:92] Found a cached abstract_attribute of: [TypeId_hash=140104691385568, AbstractAttribute_ptr=0x5566367b2500].
I0831 09:07:41.354866 27067 ir_context.cc:92] Found a cached abstract_attribute of: [TypeId_hash=140104704071424, AbstractAttribute_ptr=0x5566367b24e0].
I0831 09:07:41.354873 27067 ir_context.cc:92] Found a cached abstract_attribute of: [TypeId_hash=140104704071408, AbstractAttribute_ptr=0x55663866a120].
I0831 09:07:41.354879 27067 ir_context.cc:92] Found a cached abstract_attribute of: [TypeId_hash=140104704071400, AbstractAttribute_ptr=0x556638585060].
I0831 09:07:41.354883 27067 pd_op.cc:11578] Builder construction outputs
I0831 09:07:41.354904 27067 value_impl.h:118] Construct a ValueImpl whose's index is 0. The offset first_use address is: 0
I0831 09:07:41.354930 27067 pd_op.cc:11628] Start Verifying inputs, outputs and attributes for: DataOp.
I0831 09:07:41.354939 27067 pd_op.cc:11629] Verifying inputs:
I0831 09:07:41.354944 27067 pd_op.cc:11636] Verifying attributes:
I0831 09:07:41.354949 27067 pd_op.cc:11648] Verifying outputs:
I0831 09:07:41.354954 27067 pd_op.cc:11656] End Verifying for: DataOp.
I0831 09:07:41.354974 27067 ir_context.cc:92] Found a cached abstract_attribute of: [TypeId_hash=140104691385560, AbstractAttribute_ptr=0x5566367b2620].
I0831 09:07:41.354980 27067 ir_context.cc:92] Found a cached abstract_attribute of: [TypeId_hash=140104691385520, AbstractAttribute_ptr=0x5566367b2c40].
I0831 09:07:41.355005 27067 pd_op.cc:11566] Builder construction inputs
I0831 09:07:41.355010 27067 pd_op.cc:11568] Builder construction attributes
I0831 09:07:41.355012 27067 ir_context.cc:92] Found a cached abstract_attribute of: [TypeId_hash=140104691385568, AbstractAttribute_ptr=0x5566367b2500].
I0831 09:07:41.355018 27067 pd_op.cc:11578] Builder construction outputs
I0831 09:07:41.355027 27067 value_impl.h:118] Construct a ValueImpl whose's index is 0. The offset first_use address is: 0
I0831 09:07:41.355031 27067 pd_op.cc:11628] Start Verifying inputs, outputs and attributes for: DataOp.
I0831 09:07:41.355034 27067 pd_op.cc:11629] Verifying inputs:
I0831 09:07:41.355037 27067 pd_op.cc:11636] Verifying attributes:
I0831 09:07:41.355041 27067 pd_op.cc:11648] Verifying outputs:
I0831 09:07:41.355043 27067 pd_op.cc:11656] End Verifying for: DataOp.
I0831 09:07:41.355232 27067 pd_op.cc:50127] Builder construction inputs
I0831 09:07:41.355240 27067 pd_op.cc:50131] Builder construction attributes
I0831 09:07:41.355244 27067 pd_op.cc:50133] Builder construction outputs
I0831 09:07:41.355247 27067 pd_op.cc:50137] Builder construction  dense_x
I0831 09:07:41.355262 27067 pd_op.cc:50144] Builder construction  meta_x
I0831 09:07:41.355266 27067 pd_op.cc:50147] Builder construction  dense_y
I0831 09:07:41.355271 27067 pd_op.cc:50154] Builder construction  meta_y
I0831 09:07:41.355283 27067 value_impl.h:118] Construct a ValueImpl whose's index is 0. The offset first_use address is: 0
I0831 09:07:41.355288 27067 value_impl.h:92] The index of this value is 0. Offset and set first use: 0x556638652cc0 -> 0x556638652cc0.
I0831 09:07:41.355293 27067 value_impl.h:92] The index of this value is 0. Offset and set first use: 0x556638652ce0 -> 0x556638652ce0.
I0831 09:07:41.355301 27067 pd_op.cc:50169] Start Verifying inputs, outputs and attributes for: DivideOp.
I0831 09:07:41.355306 27067 pd_op.cc:50170] Verifying inputs:
I0831 09:07:41.355311 27067 pd_op.cc:50180] Verifying attributes:
I0831 09:07:41.355314 27067 pd_op.cc:50184] Verifying outputs:
I0831 09:07:41.355317 27067 pd_op.cc:50192] End Verifying for: DivideOp.
I0831 09:07:41.355357 27067 pd_op.cc:11566] Builder construction inputs
I0831 09:07:41.355361 27067 pd_op.cc:11568] Builder construction attributes
I0831 09:07:41.355365 27067 ir_context.cc:92] Found a cached abstract_attribute of: [TypeId_hash=140104691385568, AbstractAttribute_ptr=0x5566367b2500].
I0831 09:07:41.355372 27067 pd_op.cc:11578] Builder construction outputs
I0831 09:07:41.355378 27067 value_impl.h:118] Construct a ValueImpl whose's index is 0. The offset first_use address is: 0
I0831 09:07:41.355382 27067 pd_op.cc:11628] Start Verifying inputs, outputs and attributes for: DataOp.
I0831 09:07:41.355386 27067 pd_op.cc:11629] Verifying inputs:
I0831 09:07:41.355388 27067 pd_op.cc:11636] Verifying attributes:
I0831 09:07:41.355392 27067 pd_op.cc:11648] Verifying outputs:
I0831 09:07:41.355396 27067 pd_op.cc:11656] End Verifying for: DataOp.
I0831 09:07:41.355830 27067 pd_op_vjp.cc:194] Vjp prepare call divide's vjp inteface
I0831 09:07:41.355851 27067 pd_op.cc:93309] Builder construction inputs
I0831 09:07:41.355855 27067 pd_op.cc:93313] Builder construction attributes
I0831 09:07:41.355860 27067 ir_context.cc:92] Found a cached abstract_attribute of: [TypeId_hash=140104691385536, AbstractAttribute_ptr=0x5566367b2960].
I0831 09:07:41.355865 27067 pd_op.cc:93317] Builder construction outputs
I0831 09:07:41.355877 27067 pd_op.cc:93323] Builder construction  dense_x
I0831 09:07:41.355886 27067 pd_op.cc:93330] Builder construction  meta_x
I0831 09:07:41.355890 27067 pd_op.cc:93333] Builder construction  dense_y
I0831 09:07:41.355893 27067 pd_op.cc:93340] Builder construction  meta_y
I0831 09:07:41.355906 27067 value_impl.h:118] Construct a ValueImpl whose's index is 1. The offset first_use address is: 0x1
I0831 09:07:41.355909 27067 value_impl.h:118] Construct a ValueImpl whose's index is 0. The offset first_use address is: 0
I0831 09:07:41.355913 27067 value_impl.h:92] The index of this value is 0. Offset and set first use: 0x55663339ce80 -> 0x55663339ce80.
I0831 09:07:41.355916 27067 value_impl.h:92] The index of this value is 0. Offset and set first use: 0x55663339cea0 -> 0x55663339cea0.
I0831 09:07:41.355921 27067 value_impl.h:92] The index of this value is 0. Offset and set first use: 0x55663339cec0 -> 0x55663339cec0.
I0831 09:07:41.355923 27067 value_impl.h:92] The index of this value is 0. Offset and set first use: 0x55663339cee0 -> 0x55663339cee0.
I0831 09:07:41.355937 27067 pd_op_vjp.cc:199] Vjp prepare stop gradient of divide_grad
I0831 09:07:41.356282 27067 pd_op.cc:105153] Builder construction inputs
I0831 09:07:41.356292 27067 pd_op.cc:105157] Builder construction attributes
I0831 09:07:41.356295 27067 ir_context.cc:92] Found a cached abstract_attribute of: [TypeId_hash=140104691385568, AbstractAttribute_ptr=0x5566367b2500].
I0831 09:07:41.356300 27067 ir_context.cc:92] Found a cached abstract_attribute of: [TypeId_hash=140104691385536, AbstractAttribute_ptr=0x5566367b2960].
I0831 09:07:41.356304 27067 pd_op.cc:105163] Builder construction outputs
I0831 09:07:41.356307 27067 pd_op.cc:105166] Builder construction  dense_x
I0831 09:07:41.356315 27067 pd_op.cc:105173] Builder construction  meta_x
I0831 09:07:41.356325 27067 value_impl.h:118] Construct a ValueImpl whose's index is 0. The offset first_use address is: 0
I0831 09:07:41.356329 27067 value_impl.h:92] The index of this value is 0. Offset and set first use: 0x556638630d60 -> 0x556638630d60.
I0831 09:07:41.356338 27067 pd_op.cc:105229] Start Verifying inputs, outputs and attributes for: FetchOp.
I0831 09:07:41.356343 27067 pd_op.cc:105230] Verifying inputs:
I0831 09:07:41.356346 27067 pd_op.cc:105238] Verifying attributes:
I0831 09:07:41.356350 27067 pd_op.cc:105246] Verifying outputs:
I0831 09:07:41.356353 27067 pd_op.cc:105254] End Verifying for: FetchOp.
I0831 09:07:41.356362 27067 pd_op.cc:105153] Builder construction inputs
I0831 09:07:41.356365 27067 pd_op.cc:105157] Builder construction attributes
I0831 09:07:41.356369 27067 ir_context.cc:92] Found a cached abstract_attribute of: [TypeId_hash=140104691385568, AbstractAttribute_ptr=0x5566367b2500].
I0831 09:07:41.356372 27067 ir_context.cc:92] Found a cached abstract_attribute of: [TypeId_hash=140104691385536, AbstractAttribute_ptr=0x5566367b2960].
I0831 09:07:41.356376 27067 pd_op.cc:105163] Builder construction outputs
I0831 09:07:41.356379 27067 pd_op.cc:105166] Builder construction  dense_x
I0831 09:07:41.356384 27067 pd_op.cc:105173] Builder construction  meta_x
I0831 09:07:41.356390 27067 value_impl.h:118] Construct a ValueImpl whose's index is 0. The offset first_use address is: 0
I0831 09:07:41.356395 27067 value_impl.h:92] The index of this value is 1. Offset and set first use: 0x556638630fe0 -> 0x556638630fe1.
I0831 09:07:41.356398 27067 pd_op.cc:105229] Start Verifying inputs, outputs and attributes for: FetchOp.
I0831 09:07:41.356401 27067 pd_op.cc:105230] Verifying inputs:
I0831 09:07:41.356405 27067 pd_op.cc:105238] Verifying attributes:
I0831 09:07:41.356407 27067 pd_op.cc:105246] Verifying outputs:
I0831 09:07:41.356410 27067 pd_op.cc:105254] End Verifying for: FetchOp.
I0831 09:07:41.356469 27067 ir_context.cc:92] Found a cached abstract_attribute of: [TypeId_hash=140104691385512, AbstractAttribute_ptr=0x5566367b2550].
I0831 09:07:41.356475 27067 builtin_op.cc:58] Verifying inputs, outputs and attributes for: ModuleOp.
I0831 09:07:41.356485 27067 ir_context.cc:249] Try to get or register a Dialect of: [name=pd].
I0831 09:07:41.356490 27067 ir_context.cc:249] Try to get or register a Dialect of: [name=pd_kernel].
I0831 09:07:41.356493 27067 ir_context.cc:252] Create and register a new Dialect of: [name=pd_kernel].
I0831 09:07:41.356572 27067 ir_context.cc:92] Found a cached abstract_attribute of: [TypeId_hash=140104691385568, AbstractAttribute_ptr=0x5566367b2500].
I0831 09:07:41.356577 27067 ir_context.cc:92] Found a cached abstract_attribute of: [TypeId_hash=140104691385568, AbstractAttribute_ptr=0x5566367b2500].
I0831 09:07:41.356583 27067 ir_context.cc:92] Found a cached abstract_attribute of: [TypeId_hash=140104704071168, AbstractAttribute_ptr=0x556638628d60].
I0831 09:07:41.356592 27067 value_impl.h:118] Construct a ValueImpl whose's index is 0. The offset first_use address is: 0
I0831 09:07:41.356600 27067 kernel_op.cc:29] Verifying inputs, outputs and attributes for: PhiKernelOp.
I0831 09:07:41.356608 27067 ir_context.cc:92] Found a cached abstract_attribute of: [TypeId_hash=140104691385568, AbstractAttribute_ptr=0x5566367b2500].
I0831 09:07:41.356612 27067 ir_context.cc:92] Found a cached abstract_attribute of: [TypeId_hash=140104691385568, AbstractAttribute_ptr=0x5566367b2500].
I0831 09:07:41.356616 27067 ir_context.cc:92] Found a cached abstract_attribute of: [TypeId_hash=140104704071168, AbstractAttribute_ptr=0x556638628d60].
I0831 09:07:41.356624 27067 value_impl.h:118] Construct a ValueImpl whose's index is 0. The offset first_use address is: 0
I0831 09:07:41.356628 27067 value_impl.h:92] The index of this value is 0. Offset and set first use: 0x556638631c60 -> 0x556638631c60.
I0831 09:07:41.356632 27067 kernel_op.cc:29] Verifying inputs, outputs and attributes for: PhiKernelOp.
I0831 09:07:41.356655 27067 value_impl.h:118] Construct a ValueImpl whose's index is 0. The offset first_use address is: 0
I0831 09:07:41.356660 27067 kernel_op.cc:29] Verifying inputs, outputs and attributes for: PhiKernelOp.
I0831 09:07:41.356667 27067 value_impl.h:118] Construct a ValueImpl whose's index is 0. The offset first_use address is: 0
I0831 09:07:41.356671 27067 value_impl.h:92] The index of this value is 0. Offset and set first use: 0x556638a8dbd0 -> 0x556638a8dbd0.
I0831 09:07:41.356674 27067 kernel_op.cc:29] Verifying inputs, outputs and attributes for: PhiKernelOp.
I0831 09:07:41.356724 27067 ir_context.cc:92] Found a cached abstract_attribute of: [TypeId_hash=140104691385568, AbstractAttribute_ptr=0x5566367b2500].
I0831 09:07:41.356729 27067 ir_context.cc:92] Found a cached abstract_attribute of: [TypeId_hash=140104691385568, AbstractAttribute_ptr=0x5566367b2500].
I0831 09:07:41.356732 27067 ir_context.cc:92] Found a cached abstract_attribute of: [TypeId_hash=140104704071168, AbstractAttribute_ptr=0x556638628d60].
I0831 09:07:41.356737 27067 value_impl.h:118] Construct a ValueImpl whose's index is 0. The offset first_use address is: 0
I0831 09:07:41.356740 27067 value_impl.h:92] The index of this value is 0. Offset and set first use: 0x556638adab80 -> 0x556638adab80.
I0831 09:07:41.356745 27067 value_impl.h:92] The index of this value is 0. Offset and set first use: 0x556638adaba0 -> 0x556638adaba0.
I0831 09:07:41.356747 27067 kernel_op.cc:29] Verifying inputs, outputs and attributes for: PhiKernelOp.
I0831 09:07:41.356767 27067 value_impl.h:118] Construct a ValueImpl whose's index is 0. The offset first_use address is: 0
I0831 09:07:41.356772 27067 kernel_op.cc:29] Verifying inputs, outputs and attributes for: PhiKernelOp.
I0831 09:07:41.356778 27067 value_impl.h:118] Construct a ValueImpl whose's index is 0. The offset first_use address is: 0
I0831 09:07:41.356782 27067 value_impl.h:92] The index of this value is 0. Offset and set first use: 0x556638adb2a0 -> 0x556638adb2a0.
I0831 09:07:41.356786 27067 kernel_op.cc:29] Verifying inputs, outputs and attributes for: PhiKernelOp.
I0831 09:07:41.356829 27067 ir_context.cc:92] Found a cached abstract_attribute of: [TypeId_hash=140104691385568, AbstractAttribute_ptr=0x5566367b2500].
I0831 09:07:41.356838 27067 ir_context.cc:92] Found a cached abstract_attribute of: [TypeId_hash=140104691385568, AbstractAttribute_ptr=0x5566367b2500].
I0831 09:07:41.356843 27067 value_impl.h:118] Construct a ValueImpl whose's index is 1. The offset first_use address is: 0x1
I0831 09:07:41.356846 27067 value_impl.h:118] Construct a ValueImpl whose's index is 0. The offset first_use address is: 0
I0831 09:07:41.356850 27067 value_impl.h:92] The index of this value is 0. Offset and set first use: 0x556636ae0510 -> 0x556636ae0510.
I0831 09:07:41.356853 27067 value_impl.h:92] The index of this value is 0. Offset and set first use: 0x556636ae0530 -> 0x556636ae0530.
I0831 09:07:41.356856 27067 value_impl.h:92] The index of this value is 0. Offset and set first use: 0x556636ae0550 -> 0x556636ae0550.
I0831 09:07:41.356859 27067 value_impl.h:92] The index of this value is 0. Offset and set first use: 0x556636ae0570 -> 0x556636ae0570.
I0831 09:07:41.356863 27067 kernel_op.cc:29] Verifying inputs, outputs and attributes for: PhiKernelOp.
I0831 09:07:41.356890 27067 ir_context.cc:92] Found a cached abstract_attribute of: [TypeId_hash=140104691385568, AbstractAttribute_ptr=0x5566367b2500].
I0831 09:07:41.356895 27067 ir_context.cc:92] Found a cached abstract_attribute of: [TypeId_hash=140104691385568, AbstractAttribute_ptr=0x5566367b2500].
I0831 09:07:41.356900 27067 value_impl.h:118] Construct a ValueImpl whose's index is 0. The offset first_use address is: 0
I0831 09:07:41.356904 27067 value_impl.h:92] The index of this value is 0. Offset and set first use: 0x5566386314e0 -> 0x5566386314e0.
I0831 09:07:41.356907 27067 kernel_op.cc:29] Verifying inputs, outputs and attributes for: PhiKernelOp.
I0831 09:07:41.356927 27067 value_impl.h:118] Construct a ValueImpl whose's index is 0. The offset first_use address is: 0
I0831 09:07:41.356932 27067 value_impl.h:92] The index of this value is 1. Offset and set first use: 0x556638631760 -> 0x556638631761.
I0831 09:07:41.356936 27067 kernel_op.cc:29] Verifying inputs, outputs and attributes for: PhiKernelOp.
I0831 09:07:41.356942 27067 interpretercore.cc:56] InterpreterCore(): 0x556638a8c850 on Place(gpu:0)
I0831 09:07:41.356961 27067 new_ir_interpreter.cc:68] NewIRInterpreter(): 0x556638697b80 on Place(gpu:0)
I0831 09:07:41.356969 27067 new_executor_defs.cc:49] Set local scope: 0
I0831 09:07:41.356974 27067 execution_config.cc:106] place:Place(gpu:0), processor_count:40, device_count:7, serial_run:0, num_host_threads:1, num_device_threads:1
I0831 09:07:41.357074 27067 cuda_info.cc:257] SetDeviceId 0
I0831 09:07:43.933660 27067 feed_fetch_method.cc:39] SetFeedVariable name=X index=0
I0831 09:07:43.933732 27067 scope.cc:207] Create variable X
I0831 09:07:43.933737 27067 feed_fetch_method.cc:44] Reset X to phi::DenseTensor
I0831 09:07:43.934072 27067 feed_fetch_method.cc:39] SetFeedVariable name=Y index=0
I0831 09:07:43.934083 27067 scope.cc:207] Create variable Y
I0831 09:07:43.934087 27067 feed_fetch_method.cc:44] Reset Y to phi::DenseTensor
I0831 09:07:43.934355 27067 cuda_info.cc:257] SetDeviceId 0
I0831 09:07:43.934366 27067 new_ir_interpreter.cc:912] New Executor is BetaRunning.
I0831 09:07:43.934378 27067 phi_kernel_util.cc:516] ***** [before build] scope(0x556636775c60) ******
0x556636775c60 
------------------------------------------
0x5566381ec340 0x556636a1cca0 0x5566385e6340 
------------------------------------------

Details:

====
0x556636775c60:
  - X
  - feed
  - Y
  - fetch
====
0x5566381ec340:
====
0x556636a1cca0:
  - v_0
  - elementwise_div_0
  - composite_tmp_2
  - composite_tmp_3
  - X
  - X@GRAD
  - Y
  - Y@GRAD
  - composite_tmp_6
  - composite_tmp_0
  - composite_tmp_5
  - composite_tmp_1
====
0x5566385e6340:
I0831 09:07:43.934441 27067 phi_kernel_util.cc:529] build op:pd.data
I0831 09:07:43.934469 27067 phi_kernel_util.cc:529] build op:pd.shadow_feed
I0831 09:07:43.934481 27067 scope.cc:207] Create variable 0x556638697b80_inner_var_1
I0831 09:07:43.934489 27067 phi_kernel_util.cc:529] build op:pd.data
I0831 09:07:43.934496 27067 phi_kernel_util.cc:529] build op:pd.shadow_feed
I0831 09:07:43.934514 27067 scope.cc:207] Create variable 0x556638697b80_inner_var_3
I0831 09:07:43.934521 27067 phi_kernel_util.cc:529] build op:pd.divide
I0831 09:07:43.934526 27067 scope.cc:207] Create variable 0x556638697b80_inner_var_4
I0831 09:07:43.934532 27067 phi_kernel_util.cc:529] build op:pd.data
I0831 09:07:43.966882 27067 global_value_getter_setter.cc:191] set FLAGS_enable_new_ir_api to False
I0831 09:07:43.966981 27067 global_value_getter_setter.cc:191] set FLAGS_enable_new_ir_in_executor to False
/home/chenzhiyang02/work/Paddle/build/python/paddle/fluid/executor.py:1731: UserWarning: There are no operators in the program to be executed. If you pass Program manually, please use fluid.program_guard to ensure the current Program is being used.
  warnings.warn(error_info)
EI0831 09:07:43.967464 27067 interpretercore.cc:62] ~InterpreterCore(): 0x556638a8c850
I0831 09:07:43.967478 27067 new_ir_interpreter.cc:114] ~NewIRInterpreter(): 0x556638697b80 on Place(gpu:0)
I0831 09:07:43.967505 27067 value_impl.h:92] The index of this value is 1. Offset and set first use: 0 -> 0x1.
I0831 09:07:43.967514 27067 value_impl.h:92] The index of this value is 0. Offset and set first use: 0 -> 0.
I0831 09:07:43.967517 27067 value_impl.h:92] The index of this value is 0. Offset and set first use: 0x556638adab80 -> 0x556638adab80.
I0831 09:07:43.967520 27067 value_impl.h:92] The index of this value is 0. Offset and set first use: 0x556638adaba0 -> 0x556638adaba0.
I0831 09:07:43.967525 27067 value_impl.h:92] The index of this value is 0. Offset and set first use: 0 -> 0.
I0831 09:07:43.967527 27067 value_impl.h:92] The index of this value is 0. Offset and set first use: 0 -> 0.
I0831 09:07:43.967530 27067 value_impl.h:92] The index of this value is 0. Offset and set first use: 0 -> 0.
I0831 09:07:43.967535 27067 value_impl.h:92] The index of this value is 0. Offset and set first use: 0 -> 0.
I0831 09:07:43.967538 27067 value_impl.h:92] The index of this value is 0. Offset and set first use: 0 -> 0.
I0831 09:07:43.967541 27067 value_impl.h:92] The index of this value is 0. Offset and set first use: 0 -> 0.
I0831 09:07:43.967546 27067 value_impl.h:92] The index of this value is 0. Offset and set first use: 0 -> 0.
I0831 09:07:43.967947 27067 global_value_getter_setter.cc:191] set FLAGS_use_system_allocator to False
I0831 09:07:44.019773 27067 value_impl.h:92] The index of this value is 1. Offset and set first use: 0 -> 0x1.
I0831 09:07:44.019816 27067 value_impl.h:92] The index of this value is 0. Offset and set first use: 0 -> 0.
I0831 09:07:44.019820 27067 value_impl.h:92] The index of this value is 0. Offset and set first use: 0x556638652cc0 -> 0x556638652cc0.
I0831 09:07:44.019824 27067 value_impl.h:92] The index of this value is 0. Offset and set first use: 0x556638652ce0 -> 0x556638652ce0.
I0831 09:07:44.019827 27067 value_impl.h:92] The index of this value is 0. Offset and set first use: 0 -> 0.
I0831 09:07:44.019831 27067 value_impl.h:92] The index of this value is 0. Offset and set first use: 0 -> 0.
I0831 09:07:44.019835 27067 value_impl.h:92] The index of this value is 0. Offset and set first use: 0 -> 0.
I0831 09:07:44.019838 27067 value_impl.h:92] The index of this value is 0. Offset and set first use: 0 -> 0.

======================================================================
ERROR: test_check_gradient (__main__.ElementwiseDivOp)
----------------------------------------------------------------------
Traceback (most recent call last):
  File "test_elementwise_div_op.py", line 129, in test_check_gradient
    self.check_grad(*check_args, **check_kwargs)
  File "/home/chenzhiyang02/work/Paddle/test/legacy_test/eager_op_test.py", line 2790, in check_grad
    check_cinn=check_cinn,
  File "/home/chenzhiyang02/work/Paddle/test/legacy_test/eager_op_test.py", line 3023, in check_grad_with_place
    no_grad_set,
  File "/home/chenzhiyang02/work/Paddle/test/legacy_test/eager_op_test.py", line 3476, in _get_ir_gradient
    fetch_list=fetch_list,
  File "/home/chenzhiyang02/work/Paddle/build/python/paddle/fluid/executor.py", line 1633, in run
    return_numpy=return_numpy,
  File "/home/chenzhiyang02/work/Paddle/build/python/paddle/fluid/executor.py", line 1950, in _run_new_ir_impl
    ret = new_exe.run(list(feed.keys()), return_numpy)
  File "/home/chenzhiyang02/work/Paddle/build/python/paddle/fluid/executor.py", line 800, in run
    tensors = self._new_exe.run(feed_names)._move_to_list()
ValueError: 

--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   paddle::framework::StandaloneExecutor::Run(std::vector<std::string, std::allocator<std::string > > const&)
1   paddle::framework::InterpreterCore::Run(std::vector<std::string, std::allocator<std::string > > const&, bool)
2   paddle::framework::NewIRInterpreter::Run(std::vector<std::string, std::allocator<std::string > > const&, bool)
3   ir::BuildScope(ir::Block const&, paddle::framework::Scope*, std::string const&, std::unordered_map<ir::Value, std::string, std::hash<ir::Value>, std::equal_to<ir::Value>, std::allocator<std::pair<ir::Value const, std::string > > >*, std::unordered_map<paddle::framework::Variable const*, std::string, std::hash<paddle::framework::Variable const*>, std::equal_to<paddle::framework::Variable const*>, std::allocator<std::pair<paddle::framework::Variable const* const, std::string > > >*, std::map<std::string, int, std::less<std::string >, std::allocator<std::pair<std::string const, int> > >*, std::vector<paddle::framework::Variable*, std::allocator<paddle::framework::Variable*> >*)
4   ir::HandleForSpecialOp(ir::Operation*, paddle::framework::Scope*, std::string const&, std::unordered_map<ir::Value, std::string, std::hash<ir::Value>, std::equal_to<ir::Value>, std::allocator<std::pair<ir::Value const, std::string > > >*, std::unordered_map<paddle::framework::Variable const*, std::string, std::hash<paddle::framework::Variable const*>, std::equal_to<paddle::framework::Variable const*>, std::allocator<std::pair<paddle::framework::Variable const* const, std::string > > >*, std::map<std::string, int, std::less<std::string >, std::allocator<std::pair<std::string const, int> > >*, std::vector<paddle::framework::Variable*, std::allocator<paddle::framework::Variable*> >*)
5   phi::enforce::EnforceNotMet::EnforceNotMet(phi::ErrorSummary const&, char const*, int)
6   phi::enforce::GetCurrentTraceBackString[abi:cxx11](bool)

----------------------
Error Message Summary:
----------------------
InvalidArgumentError: The variable val_grad_0 shoud exist (at /home/chenzhiyang02/work/Paddle/paddle/fluid/ir/phi_kernel_adaptor/phi_kernel_util.cc:286)


----------------------------------------------------------------------
Ran 1 test in 2.929s

FAILED (errors=1)
fffffffffffffff: []
fffffffffffffff: ['X', 'Y', 'v_0']
New IR gradient begins...........
ppppppppppppp: {
 (%0) = "pd.data" () {dtype:float64,name:X,place:Place(undefined:0),shape:IntArray[13,17],stop_gradient:array[0]} : () -> pd.tensor<13x17xf64>
 (%1) = "pd.data" () {dtype:float64,name:Y,place:Place(undefined:0),shape:IntArray[13,17],stop_gradient:array[0]} : () -> pd.tensor<13x17xf64>
 (%2) = "pd.divide" (%0, %1) {} : (pd.tensor<13x17xf64>, pd.tensor<13x17xf64>) -> pd.tensor<13x17xf64>
 (%3) = "pd.data" () {dtype:float64,name:val_grad_0,place:Place(undefined:0),shape:IntArray[13,17]} : () -> pd.tensor<13x17xf64>
 (%4, %5) = "pd.divide_grad" (%0, %1, %2, %3) {axis:-1} : (pd.tensor<13x17xf64>, pd.tensor<13x17xf64>, pd.tensor<13x17xf64>, pd.tensor<13x17xf64>) -> pd.tensor<13x17xf64>, pd.tensor<13x17xf64>
 (%6) = "pd.fetch" (%4) {col:0,name:fetch0} : (pd.tensor<13x17xf64>) -> pd.tensor<13x17xf64>
 (%7) = "pd.fetch" (%5) {col:1,name:fetch1} : (pd.tensor<13x17xf64>) -> pd.tensor<13x17xf64>
}

fffffffffffffff: ['X', 'Y', 'val_grad_0']
I0831 09:07:44.023216 27067 mmap_allocator.cc:321] PID: 27067, MemoryMapFdSet: set size - 0
I0831 09:07:44.030601 27067 interpretercore.cc:62] ~InterpreterCore(): 0x5566385c7150
I0831 09:07:44.030647 27067 program_interpreter.cc:90] ~ProgramInterpreter(): 0x556638192130 on Place(cpu)
I0831 09:07:44.030668 27067 onednn_context.cc:101] 0x556638192130 0x556638192130
I0831 09:07:44.030696 27067 onednn_context.cc:104] Clearing DNNL cache.
I0831 09:07:44.030705 27067 onednn_context.cc:122] Resetting Paddle data layout to NCHW.
I0831 09:07:44.031301 27067 interpretercore.cc:62] ~InterpreterCore(): 0x5566385cc5d0
I0831 09:07:44.031905 27067 program_interpreter.cc:90] ~ProgramInterpreter(): 0x5566385e5670 on Place(cpu)
I0831 09:07:44.031917 27067 onednn_context.cc:101] 0x556638192130 0x5566385e5670
I0831 09:07:44.031921 27067 onednn_context.cc:104] Clearing DNNL cache.
I0831 09:07:44.031924 27067 onednn_context.cc:122] Resetting Paddle data layout to NCHW.
I0831 09:07:44.032052 27067 interpretercore.cc:62] ~InterpreterCore(): 0x5566381aa110
I0831 09:07:44.032064 27067 program_interpreter.cc:90] ~ProgramInterpreter(): 0x55663860f990 on Place(cpu)
I0831 09:07:44.032068 27067 onednn_context.cc:101] 0x556638192130 0x55663860f990
I0831 09:07:44.032071 27067 onednn_context.cc:104] Clearing DNNL cache.
I0831 09:07:44.032074 27067 onednn_context.cc:122] Resetting Paddle data layout to NCHW.
I0831 09:07:44.032536 27067 interpretercore.cc:62] ~InterpreterCore(): 0x55663864fcf0
I0831 09:07:44.033133 27067 program_interpreter.cc:90] ~ProgramInterpreter(): 0x5566386ae7e0 on Place(cpu)
I0831 09:07:44.033143 27067 onednn_context.cc:101] 0x556638192130 0x5566386ae7e0
I0831 09:07:44.033147 27067 onednn_context.cc:104] Clearing DNNL cache.
I0831 09:07:44.033150 27067 onednn_context.cc:122] Resetting Paddle data layout to NCHW.
I0831 09:07:44.033249 27067 mmap_allocator.cc:321] PID: 27067, MemoryMapFdSet: set size - 0
I0831 09:07:44.250406 27067 onednn_context.cc:101] 0x556638192130 0x556638192130
I0831 09:07:44.250450 27067 onednn_context.cc:104] Clearing DNNL cache.
I0831 09:07:44.250454 27067 onednn_context.cc:122] Resetting Paddle data layout to NCHW.
I0831 09:07:44.250470 27067 mmap_allocator.cc:321] PID: 27067, MemoryMapFdSet: set size - 0
