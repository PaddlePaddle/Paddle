/*===- TableGen'source file -----------------------------------------------===*\
|*                                                                            *|
|* Op Definitions                                                             *|
|*                                                                            *|
|* Automatically generated file, do not edit!                                 *|
|* Generated by tools/infrt/generate_pd_op_dialect_from_paddle_op_maker.py    *|
|*                                                                            *|
\*===----------------------------------------------------------------------===*/
#ifndef PD_OPS
#define PD_OPS
include "mlir/Interfaces/InferTypeOpInterface.td"
include "mlir/Interfaces/LoopLikeInterface.td"
include "mlir/IR/OpBase.td"
include "paddle/infrt/dialect/pd_op_base.td"

def PD_RsqrtOp : PD_Op<"rsqrt", [NoSideEffect]> {
  let summary = "rsqrt op";
  let description = [{
    
    Rsqrt Activation Operator.
    
    Please make sure input is legal in case of numeric errors.
    
    $$out = \\frac{1}{\\sqrt{x}}$$
    
    
  }];
  let arguments = (ins  PD_Tensor:$X);

  let results = (outs PD_Tensor:$Out);
}
def PD_AddmmOp : PD_Op<"addmm", [NoSideEffect]> {
  let summary = "addmm op";
  let description = [{
    
    AddMM Operator.
    This operator is used to perform matrix multiplication for input $x$ and $y$ with coefficient $alpha$.
    $input$ with coefficient $beta$ is added to the final result. 
    The equation is:
    
    $$Out = alpha * x * y + beta * input$$
    
    $x$ and $y$ must be two-dimensional, and $input$ can be broadcastable.
    
  }];
  let arguments = (ins  PD_Tensor:$Input, PD_Tensor:$X, PD_Tensor:$Y, DefaultValuedAttr<F32Attr, "1.0">:$Alpha, DefaultValuedAttr<F32Attr, "1.0">:$Beta);

  let results = (outs PD_Tensor:$Out);
}
def PD_GruOp : PD_Op<"gru", [NoSideEffect]> {
  let summary = "gru op";
  let description = [{
    
    GRU Operator implements part calculations of the complete GRU as following:
    
    $$
    update\_gate: u_t = actGate(xu_t + W_u * h_{t-1} + b_u) \\
    reset\_gate: r_t = actGate(xr_t + W_r * h_{t-1} + b_r)  \\
    output\_candidate: {h}_t = actNode(xc_t + W_c * dot(r_t, h_{t-1}) + b_c) \\
    output: h_t = dot((1 - u_t), h_{t-1}) + dot(u_t, {h}_t)
    $$
    
    @note To implement the complete GRU, fully-connected operator must be used
    before to feed xu, xr and xc as the Input of GRU operator.
    
  }];
  let arguments = (ins  PD_Tensor:$Input, PD_Tensor:$H0, PD_Tensor:$Weight, PD_Tensor:$Bias, DefaultValuedAttr<StrAttr, "tanh">:$activation, DefaultValuedAttr<StrAttr, "sigmoid">:$gate_activation, DefaultValuedAttr<BoolAttr, "false">:$is_reverse, DefaultValuedAttr<BoolAttr, "false">:$origin_mode);

  let results = (outs PD_Tensor:$Hidden);
}
def PD_RoundOp : PD_Op<"round", [NoSideEffect]> {
  let summary = "round op";
  let description = [{
    
    The OP rounds the values in the input to the nearest integer value.
    
    .. code-block:: text
    
      input:
        x.shape = [4]
        x.data = [1.2, -0.9, 3.4, 0.9]
    
      output:
        out.shape = [4]
        out.data = [1., -1., 3., 1.]
    
    
  }];
  let arguments = (ins  PD_Tensor:$X);

  let results = (outs PD_Tensor:$Out);
}
def PD_Rank_attentionOp : PD_Op<"rank_attention", [NoSideEffect]> {
  let summary = "rank_attention op";
  let description = [{
    
    RankAttention Operator.
    This Op can calculate rank attention between input and rank_param, 
    and rank_param gives the organization of data. Notice: It currently supports GPU device.
    This Op exists in contrib, which means that it is not shown to the public.
    
  }];
  let arguments = (ins  PD_Tensor:$X, PD_Tensor:$RankOffset, PD_Tensor:$RankParam, DefaultValuedAttr<SI32Attr, "3">:$MaxRank, DefaultValuedAttr<SI32Attr, "0">:$MaxSize);

  let results = (outs PD_Tensor:$InputHelp,PD_Tensor:$Out,PD_Tensor:$InsRank);
}
def PD_Bicubic_interpOp : PD_Op<"bicubic_interp", [NoSideEffect]> {
  let summary = "bicubic_interp op";
  let description = [{
    
              This operator samples input X to given output shape by using specified
              interpolation method, the interpolation methods can be \"nearest\"
              for nearest neighbor interpolation and \"bilinear\" for bilinear 
              interpolation and \"linear\" for linear interpolation..
    
              Nearest neighbor interpolation is to perform nearest neighbor interpolation
              in both the 3rd dimension(in height direction) and the 4th dimension(in width 
              direction) on input tensor.
               
              Linear interpolation is the method of using a line connecting two known quantities 
              to determine the value of an unknown quantity between the two known quantities. 
              
              Bilinear interpolation is an extension of linear interpolation for 
              interpolating functions of two variables (e.g. H-direction and 
              W-direction in this op) on a rectilinear 2D grid. The key idea is 
              to perform linear interpolation first in one direction, and then 
              again in the other direction.
    
              Trilinear interpolation is an extension of linear interpolation for 
              interpolating functions of three variables (e.g. D-direction, 
              H-direction and W-direction in this op) on a rectilinear 3D grid. 
              The linear interpolation is performed on three directions.
    
              Bicubic interpolation is an extension of cubic interpolation for interpolating
              data points on a two-dimensional regular grid. The interpolated surface is
              smoother than corresponding surfaces obtained by bilinear interpolation or
              nearest-neighbor interpolation.
    
              Align_corners and align_mode are optional parameters,the calculation method 
              of interpolation can be selected by them.
              
              Example:
    
              For scale:
              
                if align_corners = True and out_{size}>1 :
    
                  scale_{factor} = (in_{size}-1.0)/(out_{size}-1.0)
                
                else:
                  
                  scale_{factor} = float(in_{size}/out_{size})
                
              
              Nearest neighbor interpolation:
              
              if:
                  align_corners = False
    
                  input : (N,C,H_in,W_in)
                  output: (N,C,H_out,W_out) where:
    
                  H_out = \left \lfloor {H_{in} * scale_{}factor}} \right \rfloor
                  W_out = \left \lfloor {W_{in} * scale_{}factor}} \right \rfloor
    
              else:
                  align_corners = True
    
                  input : (N,C,H_in,W_in)
                  output: (N,C,H_out,W_out) where:
    
                  H_out = round(H_{in} * scale_{factor})
                  W_out = round(W_{in} * scale_{factor})
    
              Bilinear interpolation:
    
              if:
                  align_corners = False , align_mode = 0
                  
                  input : (N,C,H_in,W_in)
                  output: (N,C,H_out,W_out) where:
                  
                  H_out = (H_{in}+0.5) * scale_{factor} - 0.5
                  W_out = (W_{in}+0.5) * scale_{factor} - 0.5
    
    
              else:
               
                  input : (N,C,H_in,W_in)
                  output: (N,C,H_out,W_out) where:
    
                  H_out = H_{in} * scale_{factor}
                  W_out = W_{in} * scale_{factor}
    
              Trilinear interpolation:
    
              if:
                  align_corners = False , align_mode = 0
                  
                  input : (N,C,D_in,H_in,W_in)
                  output: (N,C,D_out,H_out,W_out) where:
                  
                  D_out = (D_{in}+0.5) * scale_{factor} - 0.5
                  H_out = (H_{in}+0.5) * scale_{factor} - 0.5
                  W_out = (W_{in}+0.5) * scale_{factor} - 0.5
    
    
              else:
               
                  input : (N,C,D_in,H_in,W_in)
                  output: (N,C,D_out,H_out,W_out) where:
    
                  D_out = D_{in} * scale_{factor}
                  H_out = H_{in} * scale_{factor}
                  W_out = W_{in} * scale_{factor}
    
              Bicubic interpolation:
    
              if:
                  align_corners = False
                  input : (N,C,H_in,W_in)
                  output: (N,C,H_out,W_out) where:
                  H_out = (H_{in}+0.5) * scale_{factor} - 0.5
                  W_out = (W_{in}+0.5) * scale_{factor} - 0.5
              else:
                  input : (N,C,H_in,W_in)
                  output: (N,C,H_out,W_out) where:
                  H_out = H_{in} * scale_{factor}
                  W_out = W_{in} * scale_{factor}
    
              For details of nearest neighbor interpolation, please refer to Wikipedia: 
              https://en.wikipedia.org/wiki/Nearest-neighbor_interpolation
    
              For details of bilinear interpolation, please refer to Wikipedia: 
              https://en.wikipedia.org/wiki/Bilinear_interpolation
    
              For details of trilinear interpolation, please refer to Wikipedia: 
              https://en.wikipedia.org/wiki/Trilinear_interpolation
    
              For details of bicubic interpolation, please refer to Wikipedia:
              https://en.wikipedia.org/wiki/Bicubic_interpolation
             
  }];
  let arguments = (ins  PD_Tensor:$X, PD_Tensor:$OutSize, PD_Tensor:$SizeTensor, PD_Tensor:$Scale, DefaultValuedAttr<StrAttr, "NCHW">:$data_layout, DefaultValuedAttr<SI32Attr, "0">:$out_d, DefaultValuedAttr<SI32Attr, "0">:$out_h, DefaultValuedAttr<SI32Attr, "0">:$out_w, DefaultValuedAttr<F32Attr, "0.0">:$scale, DefaultValuedAttr<StrAttr, "bilinear">:$interp_method, DefaultValuedAttr<BoolAttr, "true">:$align_corners, DefaultValuedAttr<SI32Attr, "1">:$align_mode);

  let results = (outs PD_Tensor:$Out);
}
def PD_TileOp : PD_Op<"tile", [NoSideEffect]> {
  let summary = "tile op";
  let description = [{
    
    Tile operator repeats the input by given times number. You should set times
    number for each dimension by providing attribute 'repeat_times'. The rank of X
    should be in [1, 6]. Please note that size of 'repeat_times' must be the same
    with X's rank. Following is a using case:
    
    Input(X) is a 3-D tensor with shape [2, 3, 1]:
    
            [
               [[1], [2], [3]],
               [[4], [5], [6]]
            ]
    
    Attr(repeat_times):  [1, 2, 2]
    
    Output(Out) is a 3-D tensor with shape [2, 6, 2]:
    
            [
                [[1, 1], [2, 2], [3, 3], [1, 1], [2, 2], [3, 3]],
                [[4, 4], [5, 5], [6, 6], [4, 4], [5, 5], [6, 6]]
            ]
    
    
  }];
  let arguments = (ins  PD_Tensor:$X, PD_Tensor:$RepeatTimes, PD_Tensor:$repeat_times_tensor, DefaultValuedAttr<I32ArrayAttr, "{}">:$repeat_times);

  let results = (outs PD_Tensor:$Out);
}
def PD_Bilinear_tensor_productOp : PD_Op<"bilinear_tensor_product", [NoSideEffect]> {
  let summary = "bilinear_tensor_product op";
  let description = [{
    
    Bilinear Tensor Product operator.
    Given input X and Y, a 3D tensor Weight and a Bias. Each column of the
    Output is computed by one slice $i = 1, . . . , k$ of the tensor:
    
    $$
    M =  (X W_i) * Y \\
    Out_i = \sum_j {M_j} + Bias_i
    $$
    
    Where $W_i$ is the $i$-th slice of Input(Weight);
          $M_j$ is the $j$-th column of $M$;
          $Out_i$ is the $i$-th column of Output(Out);
          $Bias_i$ is a column vector, each element of it is equal to
            the $i$-th element of $Bias$;
    
    
  }];
  let arguments = (ins  PD_Tensor:$X, PD_Tensor:$Y, PD_Tensor:$Weight, PD_Tensor:$Bias);

  let results = (outs PD_Tensor:$Out);
}
def PD_Matmul_v2Op : PD_Op<"matmul_v2", [NoSideEffect]> {
  let summary = "matmul_v2 op";
  let description = [{
    Matrix multiplication Out = X * Y. A has shape (d0, d1 ... M, K), 
            B has shape (d0, d1 ... K, N), Out has shape ((d0, d1 ... M, N)). 
            In addition, it also follows the broadcast rule which is similar as
            numpy.matmul.
    
  }];
  let arguments = (ins  PD_Tensor:$X, PD_Tensor:$Y, DefaultValuedAttr<BoolAttr, "false">:$trans_x, DefaultValuedAttr<BoolAttr, "false">:$trans_y);

  let results = (outs PD_Tensor:$Out);
}
def PD_C_embeddingOp : PD_Op<"c_embedding", [NoSideEffect]> {
  let summary = "c_embedding op";
  let description = [{
    
    c_embedding Operator.
    
    This operator is used to perform lookups on the parameter W,
    then concatenated into a dense tensor.
    
    The input Ids can carry the LoD (Level of Details) information,
    or not. And the output only shares the LoD information with input Ids.
    
    
  }];
  let arguments = (ins  PD_Tensor:$W, PD_Tensor:$Ids, DefaultValuedAttr<SI64Attr, "0">:$start_index);

  let results = (outs PD_Tensor:$Out);
}
def PD_Elementwise_maxOp : PD_Op<"elementwise_max", [NoSideEffect]> {
  let summary = "elementwise_max op";
  let description = [{
    
    Elementwise Max Operator.
    
    Compare two tensors and returns a new tensor containing the element-wise maxima.
    
    The equation is:
    
    $$Out = max(X, Y)$$
    
    - $X$: a tensor of any dimension.
    - $Y$: a tensor whose dimensions must be less than or equal to the dimensions of $X$.
    
    There are two cases for this operator:
    
    1. The shape of $Y$ is the same with $X$.
    2. The shape of $Y$ is a continuous subsequence of $X$.
    
    For case 2:
    
    1. Broadcast $Y$ to match the shape of $X$, where $axis$ is the start dimension index
       for broadcasting $Y$ onto $X$.
    2. If $axis$ is -1 (default), $axis = rank(X) - rank(Y)$.
    3. The trailing dimensions of size 1 for $Y$ will be ignored for the consideration of
       subsequence, such as shape(Y) = (2, 1) => (2).
    
    For example:
    
      .. code-block:: text
    
        shape(X) = (2, 3, 4, 5), shape(Y) = (,)
        shape(X) = (2, 3, 4, 5), shape(Y) = (5,)
        shape(X) = (2, 3, 4, 5), shape(Y) = (4, 5), with axis=-1(default) or axis=2
        shape(X) = (2, 3, 4, 5), shape(Y) = (3, 4), with axis=1
        shape(X) = (2, 3, 4, 5), shape(Y) = (2), with axis=0
        shape(X) = (2, 3, 4, 5), shape(Y) = (2, 1), with axis=0
    
    
  }];
  let arguments = (ins  PD_Tensor:$X, PD_Tensor:$Y, DefaultValuedAttr<SI32Attr, "-1">:$axis);

  let results = (outs PD_Tensor:$Out);
}
def PD_TanOp : PD_Op<"tan", [NoSideEffect]> {
  let summary = "tan op";
  let description = [{
    
    Tangent Operator. Computes tangent of x element-wise.
    
    Input range is `(k*pi-pi/2, k*pi+pi/2)` and output range is `(-inf, inf)`.
    
    $$out = tan(x)$$
    
    
  }];
  let arguments = (ins  PD_Tensor:$X);

  let results = (outs PD_Tensor:$Out);
}
def PD_FspOp : PD_Op<"fsp", [NoSideEffect]> {
  let summary = "fsp op";
  let description = [{
    
        This op is used to calculate the flow of solution procedure (FSP) matrix of two feature maps.
        Given feature map x with shape [x_channel, h, w] and feature map y with shape
        [y_channel, h, w], we can get the fsp matrix of x and y in two steps:
    
            step 1: reshape x into matrix with shape [x_channel, h * w] and reshape and
                    transpose y into matrix with shape [h * w, y_channel]
            step 2: multiply x and y to get fsp matrix with shape [x_channel, y_channel]
    
        The output is a batch of fsp matrices.
        
  }];
  let arguments = (ins  PD_Tensor:$X, PD_Tensor:$Y);

  let results = (outs PD_Tensor:$Out);
}
def PD_WhereOp : PD_Op<"where", [NoSideEffect]> {
  let summary = "where op";
  let description = [{
    
          Where Operator.
          Return a tensor of elements selected from either $X$ or $Y$, depending on condition.
          The equation is:
          $$
          Out_i =
          \begin{cases}
          \X_i, \quad  \text{if} \ cond_i is True \\
          \Y_i, \quad  \text{if} \ cond_i is False \\
          \end{cases}
          $$
    
  }];
  let arguments = (ins  PD_Tensor:$Condition, PD_Tensor:$X, PD_Tensor:$Y);

  let results = (outs PD_Tensor:$Out);
}
def PD_Sequence_softmaxOp : PD_Op<"sequence_softmax", [NoSideEffect]> {
  let summary = "sequence_softmax op";
  let description = [{
    
    Sequence Softmax Operator.
    
    SequenceSoftmaxOp computes the softmax activation among all time-steps for each
    sequence. The dimension of each time-step should be 1. Thus, the shape of
    input Tensor can be either [N, 1] or [N], where N is the sum of the length
    of all sequences.
    
    The algorithm works as follows:
    
        for i-th sequence in a mini-batch:
    
    $$
    Out(X[lod[i]:lod[i+1]], :) = \
    \frac{\exp(X[lod[i]:lod[i+1], :])} \
    {\sum(\exp(X[lod[i]:lod[i+1], :]))}
    $$
    
    For example, for a mini-batch of 3 sequences with variable-length,
    each containing 2, 3, 2 time-steps, the lod of which is [0, 2, 5, 7],
    then softmax will be computed among X[0:2, :], X[2:5, :], X[5:7, :]
    and N turns out to be 7.
    
    
  }];
  let arguments = (ins  PD_Tensor:$X);

  let results = (outs PD_Tensor:$Out);
}
def PD_Affine_channelOp : PD_Op<"affine_channel", [NoSideEffect]> {
  let summary = "affine_channel op";
  let description = [{
    
    
    Applies a separate affine transformation to each channel of the input. Useful
    for replacing spatial batch norm with its equivalent fixed transformation.
    The input also can be 2D tensor and applies a affine transformation in second
    dimension.
    
    $$Out = Scale*X + Bias$$
    
    
  }];
  let arguments = (ins  PD_Tensor:$X, PD_Tensor:$Scale, PD_Tensor:$Bias, DefaultValuedAttr<StrAttr, "AnyLayout">:$data_layout);

  let results = (outs PD_Tensor:$Out);
}
def PD_Triangular_solveOp : PD_Op<"triangular_solve", [NoSideEffect]> {
  let summary = "triangular_solve op";
  let description = [{
    
              Triangular Solve Operator.
              This operator is used to computes the solution of equations with a triangular coefficient matrix.
    
              The equation is:
              $$Out = X^-1 * Y$$
    
  }];
  let arguments = (ins  PD_Tensor:$X, PD_Tensor:$Y, DefaultValuedAttr<BoolAttr, "true">:$upper, DefaultValuedAttr<BoolAttr, "false">:$transpose, DefaultValuedAttr<BoolAttr, "false">:$unitriangular);

  let results = (outs PD_Tensor:$Out);
}
def PD_Sequence_topk_avg_poolingOp : PD_Op<"sequence_topk_avg_pooling", [NoSideEffect]> {
  let summary = "sequence_topk_avg_pooling op";
  let description = [{
    
        sequecen topk average pooling op
        
  }];
  let arguments = (ins  PD_Tensor:$X, PD_Tensor:$ROW, PD_Tensor:$COLUMN,I32ArrayAttr:$topks);

  let results = (outs PD_Tensor:$Out);
}
def PD_Space_to_depthOp : PD_Op<"space_to_depth", [NoSideEffect]> {
  let summary = "space_to_depth op";
  let description = [{
    
            reorg operator used in Yolo v2.
            The equation is: C2 = C1/blocksize * blocksize, W2 = W1 * blocksize + offset % blocksize, H2 = H1 * blocksize + offset / blocksize,
    
            Reshape Input(X) into the shape according to Attr(blocksize). The
            data in Input(X) are unchanged.
    
            Examples:
    
                1. Given a 4-D tensor Input(X) with a shape [128, 2048, 26, 26], and the blocksize is 2, the reorg operator will transform Input(X)
                into a 4-D tensor with shape [128, 2048, 13, 13] and leaving Input(X)'s data unchanged.
    
        
  }];
  let arguments = (ins  PD_Tensor:$X, DefaultValuedAttr<SI64Attr, "2">:$blocksize);

  let results = (outs PD_Tensor:$Out);
}
def PD_ReverseOp : PD_Op<"reverse", [NoSideEffect]> {
  let summary = "reverse op";
  let description = [{
    
          Reverse Operator.
    
          Reverse the order of elements in the input LoDTensor along given axises.
    
          Case 1:
            Given
                X = [[1, 2, 3, 4, 5]
                     [6, 7, 8, 9, 10]
                     [11, 12, 13, 14, 15]],
            and
                axis = [0],
            we get:
                Out = [[11, 12, 13, 14, 15]
                       [6, 7, 8, 9, 10]
                       [1, 2, 3, 4, 5]].
            
          Case 2:
            Given
                X = [[[1, 2, 3, 4]
                      [5, 6, 7, 8]]
                     [[9, 10, 11, 12]
                      [13, 14, 15, 16]]],
            and
                axis = [0, 2],
            we get:
                Out = [[[12, 11, 10, 9]
                        [16, 15, 14, 13]]
                       [[4, 3, 2, 1]
                        [8, 7, 6, 5]]],
        
  }];
  let arguments = (ins  PD_Tensor:$X,I32ArrayAttr:$axis);

  let results = (outs PD_Tensor:$Out);
}
def PD_Expand_v2Op : PD_Op<"expand_v2", [NoSideEffect]> {
  let summary = "expand_v2 op";
  let description = [{
    
    Expand the input to the given shape. The rank of X
    should be in [1, 6] and size of 'shape' must be in [1, 6] also.
    Following is a using case:
    
    Input(X) is a 3-D tensor with shape [2, 3, 1]:
    
            [
               [[1], [2], [3]],
               [[4], [5], [6]]
            ]
    
    Attr(shape):  [2, 6, 2]
    
    Output(Out) is a 3-D tensor with shape [2, 6, 2]:
    
            [
                [[1, 1], [2, 2], [3, 3], [1, 1], [2, 2], [3, 3]],
                [[4, 4], [5, 5], [6, 6], [4, 4], [5, 5], [6, 6]]
            ]
    
    
  }];
  let arguments = (ins  PD_Tensor:$X, PD_Tensor:$Shape, PD_Tensor:$expand_shapes_tensor, DefaultValuedAttr<I32ArrayAttr, "{}">:$shape);

  let results = (outs PD_Tensor:$Out);
}
def PD_LgammaOp : PD_Op<"lgamma", [NoSideEffect]> {
  let summary = "lgamma op";
  let description = [{
    
    Lgamma Operator.
    
    This operator performs elementwise lgamma for input $X$.
    $$out = log\Gamma(x)$$
    
    
  }];
  let arguments = (ins  PD_Tensor:$X);

  let results = (outs PD_Tensor:$Out);
}
def PD_SolveOp : PD_Op<"solve", [NoSideEffect]> {
  let summary = "solve op";
  let description = [{
    
              Solve Operator.
              This operator is used to computes the solution of a square system of 
              linear equations with a unique solution for input $X$ and $Y$.
    
              The equation is:
              $$Out = X^-1 * Y$$
    
  }];
  let arguments = (ins  PD_Tensor:$X, PD_Tensor:$Y);

  let results = (outs PD_Tensor:$Out);
}
def PD_Deformable_psroi_poolingOp : PD_Op<"deformable_psroi_pooling", [NoSideEffect]> {
  let summary = "deformable_psroi_pooling op";
  let description = [{
    
    **DeformablePSROIPooling Operator**
    DeformablePSROIPooling is a new method based Region of interest pooling 
    (also known as RoI pooling).
    The operator has four steps:
    
    1. Dividing each region proposal into equal-sized sections with
       the pooled_width and pooled_height.
    
    2. Add offset to pixel in ROI to get new location and the new value which are
       computed directly through bilinear interpolation with four nearest pixel.
    
    3. Sample several points to get average values in each bin.
    
    4. Copying these average values to the output buffer.
    
    DeformablePSROIPooling is part of Deformable Convolutional Networks,
    please refer to https://arxiv.org/abs/1703.06211 for more details.
        
  }];
  let arguments = (ins  PD_Tensor:$Input, PD_Tensor:$ROIs, PD_Tensor:$Trans,I32ArrayAttr:$group_size,I32ArrayAttr:$part_size);

  let results = (outs PD_Tensor:$TopCount,PD_Tensor:$Output);
}
def PD_Instance_normOp : PD_Op<"instance_norm", [NoSideEffect]> {
  let summary = "instance_norm op";
  let description = [{
    
    Instance Normalization.
    
    Instance Norm has been implemented as disscussed in the paper:
    https://arxiv.org/pdf/1607.08022.pdf
    Can be used as a normalizer function for conv2d and fully_connected operations.
    The required data format for this layer is as following:
    NCHW `[batch, in_channels, in_height, in_width]`
    
    
  }];
  let arguments = (ins  PD_Tensor:$X, PD_Tensor:$Scale, PD_Tensor:$Bias, DefaultValuedAttr<F32Attr, "9.999999747378752e-06">:$epsilon);

  let results = (outs PD_Tensor:$Y);
}
def PD_Gather_ndOp : PD_Op<"gather_nd", [NoSideEffect]> {
  let summary = "gather_nd op";
  let description = [{
    
        Gather_Nd Operator.
    
        This function is actually a high-dimensional extension of gather 
        and supports for simultaneous indexing by multiple axes. Out is 
        obtained by gathering slices from X into a tensor with shape 
        Index.shape[:-1] + X.shape[Index.shape[-1]:].
    
        Example:
       
        Given:
             X = [[[ 0,  1,  2,  3],
                   [ 4,  5,  6,  7],
                   [ 8,  9, 10, 11]],
                  [[12, 13, 14, 15],
                   [16, 17, 18, 19],
                   [20, 21, 22, 23]]]
           
             X.shape = (2, 3, 4)
    
       *Case 1:
    
           Index = [[1]]
    
        we get:
           Out = 
                [[12, 13, 14, 15],
                 [16, 17, 18, 19],
                 [20, 21, 22, 23]]
    
       *Case 2:
    
           Index = [[0,2]]
    
        we get:
            
           Out =  [8, 9, 10, 11]
    
       *Case 3:
    
           Index = [[1, 2, 3]]
    
        we get:
    
           Out = [23]
    
    
  }];
  let arguments = (ins  PD_Tensor:$X, PD_Tensor:$Index);

  let results = (outs PD_Tensor:$Out);
}
def PD_Reduce_prodOp : PD_Op<"reduce_prod", [NoSideEffect]> {
  let summary = "reduce_prod op";
  let description = [{
    
    Reduce reduce_prod Operator.
    
    This operator computes the reduce_prod of input tensor along the given dimension.
    The result tensor has 1 fewer dimension than the input unless keep_dim is true.
    If reduce_all is true, just reduce along all dimensions and output a scalar.
    
    
  }];
  let arguments = (ins  PD_Tensor:$X, DefaultValuedAttr<I32ArrayAttr, "{0}">:$dim, DefaultValuedAttr<BoolAttr, "false">:$keep_dim, DefaultValuedAttr<BoolAttr, "false">:$reduce_all, DefaultValuedAttr<SI32Attr, "-1">:$in_dtype, DefaultValuedAttr<SI32Attr, "-1">:$out_dtype);

  let results = (outs PD_Tensor:$Out);
}
def PD_AsinOp : PD_Op<"asin", [NoSideEffect]> {
  let summary = "asin op";
  let description = [{
    
    Arcsine Operator.
    
    $$out = \sin^{-1}(x)$$
    
    
  }];
  let arguments = (ins  PD_Tensor:$X);

  let results = (outs PD_Tensor:$Out);
}
def PD_LstmpOp : PD_Op<"lstmp", [NoSideEffect]> {
  let summary = "lstmp op";
  let description = [{
    
    Long-Short Term Memory with recurrent Projection layer (LSTMP) Operator.
    
    LSTMP has a separate projection layer after the LSTM layer, projecting the 
    original hidden state to a lower-dimensional one, which is proposed to reduce 
    the number of total parameters and furthermore computational complexity for 
    the LSTM, espeacially for the case that the size of output units is relative 
    large (https://research.google.com/pubs/archive/43905.pdf). 
    
    The formula is as follows:
    
    $$
    i_t = \sigma(W_{ix}x_{t} + W_{ir}r_{t-1} + W_{ic}c_{t-1} + b_i) \\
    
    f_t = \sigma(W_{fx}x_{t} + W_{fr}r_{t-1} + W_{fc}c_{t-1} + b_f) \\
    
    \tilde{c_t} = act_g(W_{cx}x_t + W_{cr}r_{t-1} + b_c) \\
    
    o_t = \sigma(W_{ox}x_{t} + W_{or}r_{t-1} + W_{oc}c_t + b_o) \\
    
    c_t = f_t \odot c_{t-1} + i_t \odot \tilde{c_t} \\
    
    h_t = o_t \odot act_h(c_t) \\
    
    r_t = \overline{act_h}(W_{rh}h_t)
    $$
    
    where the W terms denote weight matrices (e.g. $W_{xi}$ is the matrix
    of weights from the input gate to the input), $W_{ic}, W_{fc}, W_{oc}$
    are diagonal weight matrices for peephole connections. In our implementation,
    we use vectors to represent these diagonal weight matrices. The b terms
    denote bias vectors ($b_i$ is the input gate bias vector), $\sigma$
    is the activation, such as logistic sigmoid function, and
    $i, f, o$ and $c$ are the input gate, forget gate, output gate,
    and cell activation vectors, respectively, all of which have the same size as
    the cell output activation vector $h$. Here $h$ is usually called the hidden 
    state and $r$ denotes its recurrent projection. And $\tilde{c_t}$ is also 
    called the candidate hidden state, whose computation is based on the current 
    input and previous hidden state.
    
    The $\odot$ is the element-wise product of the vectors. $act_g$ and $act_h$
    are the cell input and cell output activation functions and `tanh` is usually
    used for them. $\overline{act_h}$ is the activation function for the 
    projection output, usually using `identity` or same as $act_h$.
    
    Note that these $W_{xi}x_{t}, W_{xf}x_{t}, W_{xc}x_{t}, W_{xo}x_{t}$
    operations on the input $x_{t}$ are NOT included in this operator.
    Users can choose to use fully-connected operator before LSTMP operator.
    
    
  }];
  let arguments = (ins  PD_Tensor:$Input, PD_Tensor:$H0, PD_Tensor:$C0, PD_Tensor:$Weight, PD_Tensor:$ProjWeight, PD_Tensor:$Bias, DefaultValuedAttr<BoolAttr, "true">:$use_peepholes, DefaultValuedAttr<BoolAttr, "false">:$is_reverse, DefaultValuedAttr<F32Attr, "0.0">:$cell_clip, DefaultValuedAttr<F32Attr, "0.0">:$proj_clip, DefaultValuedAttr<StrAttr, "sigmoid">:$gate_activation, DefaultValuedAttr<StrAttr, "tanh">:$cell_activation, DefaultValuedAttr<StrAttr, "tanh">:$candidate_activation, DefaultValuedAttr<StrAttr, "tanh">:$proj_activation);

  let results = (outs PD_Tensor:$Projection,PD_Tensor:$Cell);
}
def PD_Huber_lossOp : PD_Op<"huber_loss", [NoSideEffect]> {
  let summary = "huber_loss op";
  let description = [{
    
    HuberLoss Operator.
    
    Huber loss is a loss function used in robust regression. We define X as the
    input value and Y as the target value. Huber loss can evaluate the fitness of
    X to Y. Different from MSE loss, Huber loss is more robust for outliers. If the
    shape of X and Y are [batch_size, 1]. The equation is:
    
    $$
    Out_{\delta}(X, Y)_i =
    \begin{cases}
    0.5 * (Y_i - X_i)^2,
    \quad |Y_i - X_i| \leq \delta \\
    \delta * (|Y_i - X_i| - 0.5 * \delta),
    \quad otherwise
    \end{cases}
    $$
    
    In the above equation, $Out_\delta(X, Y)_i$, $X_i$ and $Y_i$ represent the ith
    element of Out, X and Y.
    
    
  }];
  let arguments = (ins  PD_Tensor:$X, PD_Tensor:$Y);

  let results = (outs PD_Tensor:$Out);
}
def PD_Sequence_sliceOp : PD_Op<"sequence_slice", [NoSideEffect]> {
  let summary = "sequence_slice op";
  let description = [{
    
    Sequence slice operator
    
    The operator crops a subsequence from given sequence with given start offset and subsequence length.
    It only supports sequence (LoD Tensor with level number is 1).
    - Case:
        X = [[a1, a2;
            b1, b2;
            c1, c2]
           [d1, d2;
            e1, e2]]
        LoD(X) = {{0, 3, 5}}; Dims(X) = (5, 2)
        Offset = [[0], [1]]; Length = [[2], [1]]
    
        Out = [[a1, a2;
                b1, b2]
                [e1, e2]]
        LoD(Out) = {{0, 2, 3}}; Dims(Out) = (3, 2)
    NOTE: The first dimension size of input, the size of offset and Length, should be equal. The offset start from 0.
        
  }];
  let arguments = (ins  PD_Tensor:$X, PD_Tensor:$Offset, PD_Tensor:$Length);

  let results = (outs PD_Tensor:$Out);
}
def PD_Lookup_tableOp : PD_Op<"lookup_table", [NoSideEffect]> {
  let summary = "lookup_table op";
  let description = [{
    
    Lookup Table Operator.
    
    This operator is used to perform lookups on the parameter W,
    then concatenated into a dense tensor.
    
    The input Ids can carry the LoD (Level of Details) information,
    or not. And the output only shares the LoD information with input Ids.
    
    
  }];
  let arguments = (ins  PD_Tensor:$W, PD_Tensor:$Ids, DefaultValuedAttr<BoolAttr, "false">:$is_sparse, DefaultValuedAttr<BoolAttr, "false">:$is_distributed, DefaultValuedAttr<SI64Attr, "-1">:$padding_idx, DefaultValuedAttr<BoolAttr, "false">:$remote_prefetch, DefaultValuedAttr<StrAttr, "">:$entry_config, DefaultValuedAttr<StrAttr, "none">:$entry, DefaultValuedAttr<StrAttr, "none">:$table_class, DefaultValuedAttr<StrArrayAttr, "{}">:$table_names, DefaultValuedAttr<SI32Attr, "0">:$trainer_id, DefaultValuedAttr<BoolAttr, "false">:$grad_inplace, DefaultValuedAttr<StrArrayAttr, "{}">:$epmap, DefaultValuedAttr<I64ArrayAttr, "{}">:$height_sections);

  let results = (outs PD_Tensor:$Out);
}
def PD_SoftplusOp : PD_Op<"softplus", [NoSideEffect]> {
  let summary = "softplus op";
  let description = [{
    
    :strong:`Softplus Activation Operator`
    
    ..  math::
        out = \frac{1}{\beta} * \log(1 + \exp(\beta * x)) \\
        \text{For numerical stability, the implementation reverts to the linear function when :}\,x \times \beta > threshold.
    
    
  }];
  let arguments = (ins  PD_Tensor:$X, DefaultValuedAttr<F32Attr, "1.0">:$beta, DefaultValuedAttr<F32Attr, "20.0">:$threshold);

  let results = (outs PD_Tensor:$Out);
}
def PD_Depthwise_conv2dOp : PD_Op<"depthwise_conv2d", [NoSideEffect]> {
  let summary = "depthwise_conv2d op";
  let description = [{
    
    Convolution Operator.
    
    The convolution operation calculates the output based on the input, filter
    and strides, paddings, dilations, groups parameters. The size of each dimension of the
    parameters is checked in the infer-shape.
    Input(Input) and Output(Output) are in NCHW or NHWC format. Where N is batch
    size, C is the number of channels, H is the height of the feature, and W is
    the width of the feature.
    Filters(Input) is MCHW format format. Where M is the number of output image channels, C is
    the number of input image channels, H is the height of the filter, and W
    is the width of the filter.
    Parameters(strides, paddings, dilations) are two elements. These two elements represent
    height and width, respectively.
    The input(X) size and output(Out) size may be different.
    
    Example:
      Input:
           Input shape: $(N, C_{in}, H_{in}, W_{in})$
           Filter shape: $(C_{out}, C_{in}, H_f, W_f)$
      Output:
           Output shape: $(N, C_{out}, H_{out}, W_{out})$
      Where
    $$
           H_{out}= \frac{(H_{in} + pad_height_top + pad_height_bottom - (dilations[0] * (H_f - 1) + 1))}{strides[0]}+ 1 \\
           W_{out}= \frac{(W_{in} + pad_width_left + pad_width_right - (dilations[1] * (W_f - 1) + 1))}{strides[1]}+ 1
    $$
    
  }];
  let arguments = (ins  PD_Tensor:$Input, PD_Tensor:$Filter, DefaultValuedAttr<I32ArrayAttr, "{1, 1}">:$strides, DefaultValuedAttr<I32ArrayAttr, "{0, 0}">:$paddings, DefaultValuedAttr<StrAttr, "EXPLICIT">:$padding_algorithm, DefaultValuedAttr<SI32Attr, "1">:$groups, DefaultValuedAttr<I32ArrayAttr, "{1, 1}">:$dilations, DefaultValuedAttr<StrAttr, "NCHW">:$data_format);

  let results = (outs PD_Tensor:$Output);
}
def PD_Sigmoid_cross_entropy_with_logitsOp : PD_Op<"sigmoid_cross_entropy_with_logits", [NoSideEffect]> {
  let summary = "sigmoid_cross_entropy_with_logits op";
  let description = [{
    
    SigmoidCrossEntropyWithLogits Operator.
    
    This measures the element-wise probability error in classification tasks
    in which each class is independent. This can be thought of as predicting labels
    for a data-point, where labels are not mutually exclusive.
    For example, a news article can be about politics, technology or sports
    at the same time or none of these.
    
    The logistic loss is given as follows:
    
           $$loss = -Labels * \log(\sigma(X)) - (1 - Labels) * \log(1 - \sigma(X))$$
    
    We know that $$\sigma(X) = \\frac{1}{1 + \exp(-X)}$$. By substituting this we get:
    
           $$loss = X - X * Labels + \log(1 + \exp(-X))$$
    
    For stability and to prevent overflow of $$\exp(-X)$$ when X < 0,
    we reformulate the loss as follows:
    
           $$loss = \max(X, 0) - X * Labels + \log(1 + \exp(-\|X\|))$$
    
    Both the input `X` and `Labels` can carry the LoD (Level of Details) information.
    However the output only shares the LoD with input `X`.
    
    
  }];
  let arguments = (ins  PD_Tensor:$X, PD_Tensor:$Label, DefaultValuedAttr<BoolAttr, "false">:$normalize, DefaultValuedAttr<SI32Attr, "-100">:$ignore_index);

  let results = (outs PD_Tensor:$Out);
}
def PD_ExpOp : PD_Op<"exp", [NoSideEffect]> {
  let summary = "exp op";
  let description = [{
    
    Exp Operator. Computes exp of x element-wise with a natural number :math:`e` as the base.
    
    $$out = e^x$$
    
    
  }];
  let arguments = (ins  PD_Tensor:$X);

  let results = (outs PD_Tensor:$Out);
}
def PD_ScatterOp : PD_Op<"scatter", [NoSideEffect]> {
  let summary = "scatter op";
  let description = [{
    
    Scatter Operator.
    
    This operator obtains output by updating the input on selected indices on the first axis:
    
    $$
    Out = X \\
    Out[Ids] = Updates
    $$
    
    
  }];
  let arguments = (ins  PD_Tensor:$X, PD_Tensor:$Ids, PD_Tensor:$Updates, DefaultValuedAttr<BoolAttr, "true">:$overwrite);

  let results = (outs PD_Tensor:$Out);
}
def PD_LogOp : PD_Op<"log", [NoSideEffect]> {
  let summary = "log op";
  let description = [{
    
    Log Activation Operator.
    
    $$out = \ln(x)$$
    
    Natural logarithm of x.
    
    
  }];
  let arguments = (ins  PD_Tensor:$X);

  let results = (outs PD_Tensor:$Out);
}
def PD_Conv_shiftOp : PD_Op<"conv_shift", [NoSideEffect]> {
  let summary = "conv_shift op";
  let description = [{
    
    ConvShift Operator.
    
    A layer for circular convolution of two vectors,
    as used in the Neural Turing Machine: https://arxiv.org/abs/1410.5401
    
    The equation is:
    
    $$Out[i] = \sum_{j=-(N-1)/2}^{(N-1)/2} X_{i+j} * Y_{j}$$
    
    where X's index is computed modulo M, and Y's index is computed modulo N.
    
    Both inputs X and Y can carry LoD (Level of Details) information.
    However, the output only shares the LoD information with input X.
    
    
  }];
  let arguments = (ins  PD_Tensor:$X, PD_Tensor:$Y);

  let results = (outs PD_Tensor:$Out);
}
def PD_Smooth_l1_lossOp : PD_Op<"smooth_l1_loss", [NoSideEffect]> {
  let summary = "smooth_l1_loss op";
  let description = [{
    
    Smooth L1 Loss Operator.
    
    This operator computes the smooth l1 loss for X and Y.
    The operator takes the first dimension of X and Y as batch size.
    For each instance, it computes the smooth l1 loss element by element first
    and then sums all the losses. So the shape of Out is [batch_size, 1].
    
    The equation is:
    $$
    Out_{\sigma}(X, Y)_i = \begin{cases}
    0.5 * (\sigma * (X_i - Y_i)) ^ 2
    \quad |X_i - Y_i| \lt \frac{1} {{\sigma} ^ 2} \\
    \frac{|X_i - Y_i| - 0.5}{{\sigma}^2},
    \quad otherwise
    \end{cases}
    $$
    
    In the above equation, $Out_{\sigma}(X, Y)_i$, $X_i$ and $Y_i$ represent the ith
    element of Out, X and Y.
    
    
  }];
  let arguments = (ins  PD_Tensor:$X, PD_Tensor:$Y, PD_Tensor:$InsideWeight, PD_Tensor:$OutsideWeight, DefaultValuedAttr<F32Attr, "1.0">:$sigma);

  let results = (outs PD_Tensor:$Out);
}
def PD_Linear_interp_v2Op : PD_Op<"linear_interp_v2", [NoSideEffect]> {
  let summary = "linear_interp_v2 op";
  let description = [{
    
              This operator samples input X to given output shape by using specified
              interpolation method, the interpolation methods can be \"nearest\"
              for nearest neighbor interpolation and \"bilinear\" for bilinear 
              interpolation and \"linear\" for linear interpolation..
    
              Nearest neighbor interpolation is to perform nearest neighbor interpolation
              in both the 3rd dimension(in height direction) and the 4th dimension(in width 
              direction) on input tensor.
               
              Linear interpolation is the method of using a line connecting two known quantities 
              to determine the value of an unknown quantity between the two known quantities. 
              
              Bilinear interpolation is an extension of linear interpolation for 
              interpolating functions of two variables (e.g. H-direction and 
              W-direction in this op) on a rectilinear 2D grid. The key idea is 
              to perform linear interpolation first in one direction, and then 
              again in the other direction.
    
              Trilinear interpolation is an extension of linear interpolation for 
              interpolating functions of three variables (e.g. D-direction, 
              H-direction and W-direction in this op) on a rectilinear 3D grid. 
              The linear interpolation is performed on three directions.
    
              Bicubic interpolation is an extension of cubic interpolation for interpolating
              data points on a two-dimensional regular grid. The interpolated surface is
              smoother than corresponding surfaces obtained by bilinear interpolation or
              nearest-neighbor interpolation.
    
              Align_corners and align_mode are optional parameters,the calculation method 
              of interpolation can be selected by them.
              
              Example:
    
              For scale:
              
                if align_corners = True and out_{size}>1 :
    
                  scale_{factor} = (in_{size}-1.0)/(out_{size}-1.0)
                
                else:
                  
                  scale_{factor} = float(in_{size}/out_{size})
                
              
              Nearest neighbor interpolation:
              
              if:
                  align_corners = False
    
                  input : (N,C,H_in,W_in)
                  output: (N,C,H_out,W_out) where:
    
                  H_out = \left \lfloor {H_{in} * scale_{}factor}} \right \rfloor
                  W_out = \left \lfloor {W_{in} * scale_{}factor}} \right \rfloor
    
              else:
                  align_corners = True
    
                  input : (N,C,H_in,W_in)
                  output: (N,C,H_out,W_out) where:
    
                  H_out = round(H_{in} * scale_{factor})
                  W_out = round(W_{in} * scale_{factor})
    
              Bilinear interpolation:
    
              if:
                  align_corners = False , align_mode = 0
                  
                  input : (N,C,H_in,W_in)
                  output: (N,C,H_out,W_out) where:
                  
                  H_out = (H_{in}+0.5) * scale_{factor} - 0.5
                  W_out = (W_{in}+0.5) * scale_{factor} - 0.5
    
    
              else:
               
                  input : (N,C,H_in,W_in)
                  output: (N,C,H_out,W_out) where:
    
                  H_out = H_{in} * scale_{factor}
                  W_out = W_{in} * scale_{factor}
    
              Trilinear interpolation:
    
              if:
                  align_corners = False , align_mode = 0
                  
                  input : (N,C,D_in,H_in,W_in)
                  output: (N,C,D_out,H_out,W_out) where:
                  
                  D_out = (D_{in}+0.5) * scale_{factor} - 0.5
                  H_out = (H_{in}+0.5) * scale_{factor} - 0.5
                  W_out = (W_{in}+0.5) * scale_{factor} - 0.5
    
    
              else:
               
                  input : (N,C,D_in,H_in,W_in)
                  output: (N,C,D_out,H_out,W_out) where:
    
                  D_out = D_{in} * scale_{factor}
                  H_out = H_{in} * scale_{factor}
                  W_out = W_{in} * scale_{factor}
    
              Bicubic interpolation:
    
              if:
                  align_corners = False
                  input : (N,C,H_in,W_in)
                  output: (N,C,H_out,W_out) where:
                  H_out = (H_{in}+0.5) * scale_{factor} - 0.5
                  W_out = (W_{in}+0.5) * scale_{factor} - 0.5
              else:
                  input : (N,C,H_in,W_in)
                  output: (N,C,H_out,W_out) where:
                  H_out = H_{in} * scale_{factor}
                  W_out = W_{in} * scale_{factor}
    
              For details of nearest neighbor interpolation, please refer to Wikipedia: 
              https://en.wikipedia.org/wiki/Nearest-neighbor_interpolation
    
              For details of bilinear interpolation, please refer to Wikipedia: 
              https://en.wikipedia.org/wiki/Bilinear_interp_v2olation
    
              For details of trilinear interpolation, please refer to Wikipedia: 
              https://en.wikipedia.org/wiki/Trilinear_interp_v2olation
    
              For details of bicubic interpolation, please refer to Wikipedia:
              https://en.wikipedia.org/wiki/Bicubic_interpolation
             
  }];
  let arguments = (ins  PD_Tensor:$X, PD_Tensor:$OutSize, PD_Tensor:$SizeTensor, PD_Tensor:$Scale, DefaultValuedAttr<StrAttr, "NCHW">:$data_layout, DefaultValuedAttr<SI32Attr, "0">:$out_d, DefaultValuedAttr<SI32Attr, "0">:$out_h, DefaultValuedAttr<SI32Attr, "0">:$out_w, DefaultValuedAttr<F32ArrayAttr, "{}">:$scale, DefaultValuedAttr<StrAttr, "bilinear">:$interp_method, DefaultValuedAttr<BoolAttr, "true">:$align_corners, DefaultValuedAttr<SI32Attr, "1">:$align_mode);

  let results = (outs PD_Tensor:$Out);
}
def PD_Temporal_shiftOp : PD_Op<"temporal_shift", [NoSideEffect]> {
  let summary = "temporal_shift op";
  let description = [{
    
              This operator calculates the temporal shifting features for Input(X).
    
              Input(X) should be in shape of [N*T, C, H, W] or [N*T, H, W, C], while 
              N is the batch size, T is the temporal segment number specified by 
              :attr:`seg_num`, C is the channel number, H and W is the height and 
              width of features.
    
              Temporal Shifting is calculated as follows when data format is NCHW:
              
              Step 1: Reshape Input(X) to [N, T, C, H, W].
    
              Step 2: Pad 0 to reshaping result in the 2nd(T) dimension with 
              padding width as 1 on each side, padding result will be in shape 
              of [N, T+2, C, H, W].
    
              Step 3: Assume :attr:`shift_ratio` is :math:`1/4`, slice padding 
              result as follows:
    
              $$
              slice1 = x[:, :T, :C/4, :, :]
              $$
              $$
              slice2 = x[:, 2:T+2, C/4:C/2, :, :]
              $$
              $$
              slice3 = x[:, 1:T+1, C/2:, :, :]
              $$
    
              Step 4: Concatenate three slices along the 3rd(C) dimension and 
              reshape result to [N*T, C, H, W].
    
              For details of temporal shifting, please refer to paper: 
              `Temporal Shift Module <http://arxiv.org/abs/1811.08383>`_ .
    
             
  }];
  let arguments = (ins  PD_Tensor:$X, DefaultValuedAttr<F32Attr, "0.25">:$shift_ratio, DefaultValuedAttr<StrAttr, "NCHW">:$data_format);

  let results = (outs PD_Tensor:$Out);
}
def PD_NceOp : PD_Op<"nce", [NoSideEffect]> {
  let summary = "nce op";
  let description = [{
    
    Compute and return the noise-contrastive estimation training loss. See
    `Noise-contrastive estimation: A new estimation principle for unnormalized
    statistical models
     <http://www.jmlr.org/proceedings/papers/v9/gutmann10a/gutmann10a.pdf>`_.
    By default this operator uses a uniform distribution for sampling.
    
  }];
  let arguments = (ins  PD_Tensor:$Input, PD_Tensor:$Label, PD_Tensor:$Weight, PD_Tensor:$Bias, PD_Tensor:$SampleWeight, PD_Tensor:$CustomDistProbs, PD_Tensor:$CustomDistAlias, PD_Tensor:$CustomDistAliasProbs, DefaultValuedAttr<SI32Attr, "10">:$num_neg_samples, DefaultValuedAttr<SI32Attr, "0">:$sampler, DefaultValuedAttr<SI32Attr, "0">:$seed, DefaultValuedAttr<BoolAttr, "false">:$is_sparse, DefaultValuedAttr<BoolAttr, "false">:$remote_prefetch);

  let results = (outs PD_Tensor:$Cost);
}
def PD_MvOp : PD_Op<"mv", [NoSideEffect]> {
  let summary = "mv op";
  let description = [{
    
    MV Operator.
    
    This operator is used to perform matrix vector multiplication
    of the input tensors `X` and `Vec`.
    
  }];
  let arguments = (ins  PD_Tensor:$X, PD_Tensor:$Vec);

  let results = (outs PD_Tensor:$Out);
}
def PD_Add_position_encodingOp : PD_Op<"add_position_encoding", [NoSideEffect]> {
  let summary = "add_position_encoding op";
  let description = [{
    
        Add Position Encoding Operator.
        
        The add position encoding calculates the output based on the input, alpha, beta.
        The size of each dimension of the parameters checked in the infer-shape.
      
  }];
  let arguments = (ins  PD_Tensor:$X, DefaultValuedAttr<F32Attr, "1.0">:$alpha, DefaultValuedAttr<F32Attr, "1.0">:$beta);

  let results = (outs PD_Tensor:$Out);
}
def PD_PreluOp : PD_Op<"prelu", [NoSideEffect]> {
  let summary = "prelu op";
  let description = [{
    
    PRelu Operator.
    The equation is:
    $$
    f(x) =
    \begin{cases}
    \alpha * x, \quad  \text{if} \ x < 0 \\
    x,         \qquad  \text{if} \ x >= 0
    \end{cases}
    $$
    The input `X` can carry the LoD (Level of Details) information,
    or not. And the output shares the LoD information with input `X`.
    There are modes:
      all: all elements share same weight
      channel: elements in a channel share same weight
      element: each element has a weight
    
  }];
  let arguments = (ins  PD_Tensor:$X, PD_Tensor:$Alpha, DefaultValuedAttr<StrAttr, "all">:$mode);

  let results = (outs PD_Tensor:$Out);
}
def PD_Fill_diagonalOp : PD_Op<"fill_diagonal", [NoSideEffect]> {
  let summary = "fill_diagonal op";
  let description = [{
    Fill replace operator
                    Fill the diagonal of an tensor with 'value'.
                    
  }];
  let arguments = (ins  PD_Tensor:$X, DefaultValuedAttr<F32Attr, "0.0">:$value, DefaultValuedAttr<BoolAttr, "false">:$wrap, DefaultValuedAttr<SI32Attr, "0">:$offset);

  let results = (outs PD_Tensor:$Out);
}
def PD_LogsigmoidOp : PD_Op<"logsigmoid", [NoSideEffect]> {
  let summary = "logsigmoid op";
  let description = [{
    
    Logsigmoid Activation Operator
    
    $$out = \\log \\frac{1}{1 + e^{-x}}$$
    
    
  }];
  let arguments = (ins  PD_Tensor:$X);

  let results = (outs PD_Tensor:$Out);
}
def PD_Sequence_scatterOp : PD_Op<"sequence_scatter", [NoSideEffect]> {
  let summary = "sequence_scatter op";
  let description = [{
    
    Sequence Scatter Operator.
    
    This operator scatters the Updates tensor to the input X. It uses the LoD
    information of Ids to select the rows to update, and use the values in Ids as
    the columns to update in each row of X.
    
    Following are cases to better explain how this works:
    
    Example 1:
    Given an all-ones Tensor input(X)
        X.data = [[1.0, 1.0, 1.0, 1.0, 1.0, 1.0],
                  [1.0, 1.0, 1.0, 1.0, 1.0, 1.0],
                  [1.0, 1.0, 1.0, 1.0, 1.0, 1.0]]
        X.dims = [3, 6]
    a LoDTensor input(Ids)
        Ids.data = [[0], [1], [2], [5], [4], [3], [2], [1], [3], [2], [5], [4]]
        Ids.lod =  [[0,        3,                       8,                 12]]
    and a Tensor input(Updates)
        Updates.data = [[0.3], [0.3], [0.4], [0.1], [0.2], [0.3], [0.4], [0.0], [0.2], [0.3], [0.1], [0.4]]
        Updates.lod =  [[  0,            3,                                 8,                         12]]
    then we get an output Tensor
        Out.data = [[1.3, 1.3, 1.4, 1.0, 1.0, 1.0],
                    [1.0, 1.0, 1.4, 1.3, 1.2, 1.1],
                    [1.0, 1.0, 1.3, 1.2, 1.4, 1.1]]
        Out.dims = X.dims = [3, 6]
    
  }];
  let arguments = (ins  PD_Tensor:$X, PD_Tensor:$Ids, PD_Tensor:$Updates);

  let results = (outs PD_Tensor:$Out);
}
def PD_Partial_sumOp : PD_Op<"partial_sum", [NoSideEffect]> {
  let summary = "partial_sum op";
  let description = [{
    
    PartialSum Operator.
    This Op can sum the vars by specifying the initial position(start_index) and length(length). 
    This OP exists in contrib, which means that it is not shown to the public.
    Only 2-D Tensor or LodTensor input is supported. Slice and concat can only be 
    performed along the second dimension.
    
    Examples:
      Input[0] = [[1,2,3],[3,4,5]]
      Input[1] = [[5,6,7],[7,8,9]]
      start_index = 0
      length = 2
      Output = [[6,8],
                [10,12]]
    
  }];
  let arguments = (ins  PD_Tensor:$X, DefaultValuedAttr<SI32Attr, "0">:$start_index, DefaultValuedAttr<SI32Attr, "-1">:$length);

  let results = (outs PD_Tensor:$Out);
}
def PD_Relu6Op : PD_Op<"relu6", [NoSideEffect]> {
  let summary = "relu6 op";
  let description = [{
    
    Relu6 Activation Operator.
    
    $$out = \min(\max(0, x), threshold)$$
    
    
  }];
  let arguments = (ins  PD_Tensor:$X, DefaultValuedAttr<F32Attr, "6.0">:$threshold);

  let results = (outs PD_Tensor:$Out);
}
def PD_Conv3dOp : PD_Op<"conv3d", [NoSideEffect]> {
  let summary = "conv3d op";
  let description = [{
    
    Convolution3D Operator.
    
    The convolution operation calculates the output based on the input, filter
    and strides, paddings, dilations, groups parameters. The size of each dimension of the
    parameters is checked in the infer-shape.
    Input(Input) and output(Output) are in NCDHW or NDHWC format, where N is batch
    size, C is the number of channels,D is the depth of the feature, H is the height of
    the feature, and W is the width of the feature.
    Filters(Input) is MCDHW format, where M is the number of output image channels,
    C is the number of input image channels, D is the depth of the filter,
    H is the height of the filter, and W is the width of the filter.
    Parameters(strides, paddings, dilations) are three elements. These three elements
    represent depth, height and width, respectively.
    The input(X) size and output(Out) size may be different.
    
    Example:
      Input:
           Input shape: $(N, C_{in}, D_{in}, H_{in}, W_{in})$
           Filter shape: $(C_{out}, C_{in}, D_f, H_f, W_f)$
      Output:
           Output shape: $(N, C_{out}, D_{out}, H_{out}, W_{out})$
      Where
      $$
           D_{out}= \frac{(D_{in} + pad_depth_front + pad_depth_back - (dilations[0] * (D_f - 1) + 1))}{ strides[0]}+ 1 \\
           H_{out}= \frac{(H_{in} + pad_height_top + pad_height_bottom - (dilations[1] * (H_f - 1) + 1))}{ strides[1]}+ 1 \\
           W_{out}= \frac{(W_{in} + pad_width_left + pad_width_right - (dilations[2] * (W_f - 1) + 1))}{ strides[2]}+ 1
      $$
    
  }];
  let arguments = (ins  PD_Tensor:$Input, PD_Tensor:$Filter, DefaultValuedAttr<I32ArrayAttr, "{1, 1, 1}">:$strides, DefaultValuedAttr<I32ArrayAttr, "{0, 0, 0}">:$paddings, DefaultValuedAttr<StrAttr, "EXPLICIT">:$padding_algorithm, DefaultValuedAttr<SI32Attr, "1">:$groups, DefaultValuedAttr<I32ArrayAttr, "{1, 1, 1}">:$dilations, DefaultValuedAttr<StrAttr, "NCDHW">:$data_format);

  let results = (outs PD_Tensor:$Output);
}
def PD_Lstm_unitOp : PD_Op<"lstm_unit", [NoSideEffect]> {
  let summary = "lstm_unit op";
  let description = [{
    
    Lstm Unit Operator
    
    Equation:
    
    $$
    i, f, o, j = split(X) \\
    C = C_{prev} * sigm(f + forget\_bias) + sigm(i) * tanh(j) \\
    H = C * sigm(o)
    $$
    
    
  }];
  let arguments = (ins  PD_Tensor:$X, PD_Tensor:$C_prev, DefaultValuedAttr<F32Attr, "0.0">:$forget_bias);

  let results = (outs PD_Tensor:$C,PD_Tensor:$H);
}
def PD_Transpose2Op : PD_Op<"transpose2", [NoSideEffect]> {
  let summary = "transpose2 op";
  let description = [{
    
    Transpose Operator.
    
    The input tensor will be permuted according to the axes given.
    The behavior of this operator is similar to how `numpy.transpose` works.
    
    - suppose the input `X` is a 2-D tensor:
        $$
        X = \begin{pmatrix}
        0 &1 &2 \\
        3 &4 &5
        \end{pmatrix}$$
    
        the given `axes` is: $[1, 0]$, and $Y$ = transpose($X$, axis)
    
        then the output $Y$ is:
    
        $$
        Y = \begin{pmatrix}
             0 &3 \\
             1 &4  \\
             2 &5
        \end{pmatrix}$$
    
    - Given a input tensor with shape $(N, C, H, W)$ and the `axes` is
    $[0, 2, 3, 1]$, then shape of the output tensor will be: $(N, H, W, C)$.
    
    
  }];
  let arguments = (ins  PD_Tensor:$X,I32ArrayAttr:$axis);

  let results = (outs PD_Tensor:$Out);
}
def PD_UnfoldOp : PD_Op<"unfold", [NoSideEffect]> {
  let summary = "unfold op";
  let description = [{
    
    **Unfold Operator**
    
    This Operator is used to extract sliding local blocks from a batched input tensor, also known
    as im2col when operated on batched 2D image tensor. For each block under the convolution filter,
    all element will be rearranged as a column. While the convolution filter sliding over the input
    feature map, a series of such columns will be formed. 
        
  }];
  let arguments = (ins  PD_Tensor:$X,I32ArrayAttr:$kernel_sizes,I32ArrayAttr:$strides,I32ArrayAttr:$paddings,I32ArrayAttr:$dilations);

  let results = (outs PD_Tensor:$Y);
}
def PD_LrnOp : PD_Op<"lrn", [NoSideEffect]> {
  let summary = "lrn op";
  let description = [{
    
    Local Response Normalization Operator.
    
    This operator comes from the paper:
    <<ImageNet Classification with Deep Convolutional Neural Networks>>.
    
    The original formula is:
    
    $$
    Output(i, x, y) = Input(i, x, y) / \left(
    k + \alpha \sum\limits^{\min(C-1, i + n/2)}_{j = \max(0, i - n/2)}
    (Input(j, x, y))^2
    \right)^{\beta}
    $$
    
    Function implementation:
    
    Inputs and outputs are in NCHW or NHWC format, while input.shape.ndims() equals 4.
    If NCHW, the dimensions 0 ~ 3 represent batch size, feature maps, rows,
    and columns, respectively.
    
    Input and Output in the formula above is for each map(i) of one image, and
    Input(i, x, y), Output(i, x, y) represents an element in an image.
    
    C is the number of feature maps of one image. n is a hyper-parameter
    configured when operator is initialized. The sum in the denominator
    is the sum of the same positions in the neighboring maps.
    
    
  }];
  let arguments = (ins  PD_Tensor:$X, DefaultValuedAttr<SI32Attr, "5">:$n, DefaultValuedAttr<F32Attr, "2.0">:$k, DefaultValuedAttr<F32Attr, "9.999999747378752e-05">:$alpha, DefaultValuedAttr<F32Attr, "0.75">:$beta, DefaultValuedAttr<StrAttr, "AnyLayout">:$data_format);

  let results = (outs PD_Tensor:$Out,PD_Tensor:$MidOut);
}
def PD_Softmax_with_cross_entropyOp : PD_Op<"softmax_with_cross_entropy", [NoSideEffect]> {
  let summary = "softmax_with_cross_entropy op";
  let description = [{
    
    Softmax With Cross Entropy Operator.
    
    Cross entropy loss with softmax is used as the output layer extensively. This
    operator computes the softmax normalized values for each row of the input
    tensor, after which cross-entropy loss is computed. This provides a more
    numerically stable gradient.
    
    Because this operator performs a softmax on logits internally, it expects
    unscaled logits. This operator should not be used with the output of
    softmax operator since that would produce incorrect results.
    
    When the attribute soft_label is set false, this operators expects mutually
    exclusive hard labels, each sample in a batch is in exactly one class with a
    probability of 1.0. Each sample in the batch will have a single label.
    
    The equation is as follows:
    
    1) Hard label (one-hot label, so every sample has exactly one class)
    
    $$Loss_j =  -\text{Logit}_{Label_j} +
    \log\left(\sum_{i=0}^{K}\exp(\text{Logit}_i)\right),
    j = 1,..., K$$
    
    2) Soft label (each sample can have a distribution over all classes)
    
    $$Loss_j =  -\sum_{i=0}^{K}\text{Label}_i \left(\text{Logit}_i -
    \log\left(\sum_{i=0}^{K}\exp(\text{Logit}_i)\right)\right),
    j = 1,...,K$$
    
    
  }];
  let arguments = (ins  PD_Tensor:$Logits, PD_Tensor:$Label, DefaultValuedAttr<BoolAttr, "false">:$soft_label, DefaultValuedAttr<BoolAttr, "true">:$use_softmax, DefaultValuedAttr<BoolAttr, "true">:$numeric_stable_mode, DefaultValuedAttr<SI32Attr, "-100">:$ignore_index, DefaultValuedAttr<SI32Attr, "-1">:$axis);

  let results = (outs PD_Tensor:$Loss);
}
def PD_Max_pool3d_with_indexOp : PD_Op<"max_pool3d_with_index", [NoSideEffect]> {
  let summary = "max_pool3d_with_index op";
  let description = [{
    
    MaxPool3d Operator.
    
    The maxpooling3d with index operation calculates the output and the mask
    based on the input and ksize, strides, paddings parameters.
    Input(X) and output(Out, Mask) are in NCDHW format, where N is batch
    size, C is the number of channels, and D, H and W are the depth, height and
    width of the feature, respectively. 
    Parameters(ksize, strides, paddings) are three elements.
    These three elements represent depth, height and width, respectively.
    The input(X) size and output(Out, Mask) size may be different.
    
    Example:
      Input:
           X shape: $(N, C, D_{in}, H_{in}, W_{in})$
      Output:
           Out shape: $(N, C, D_{out}, H_{out}, W_{out})$
           Mask shape: $(N, C, D_{out}, H_{out}, W_{out})$
      Where
           $$
           D_{out} = \frac{(D_{in} - ksize[0] + 2 * paddings[0])}{strides[0]} + 1 \\
           H_{out} = \frac{(H_{in} - ksize[1] + 2 * paddings[1])}{strides[1]} + 1 \\
           W_{out} = \frac{(W_{in} - ksize[2] + 2 * paddings[2])}{strides[2]} + 1
           $$
      
      For adaptive = true:
           $$
           D_{out} = ksize[0]   H_{out} = ksize[1]   W_{out} = ksize[2]
           $$
    
    
  }];
  let arguments = (ins  PD_Tensor:$X,I32ArrayAttr:$ksize, DefaultValuedAttr<BoolAttr, "false">:$global_pooling, DefaultValuedAttr<BoolAttr, "false">:$adaptive, DefaultValuedAttr<I32ArrayAttr, "{1, 1, 1}">:$strides, DefaultValuedAttr<I32ArrayAttr, "{0, 0, 0}">:$paddings);

  let results = (outs PD_Tensor:$Out,PD_Tensor:$Mask);
}
def PD_Flatten2Op : PD_Op<"flatten2", [NoSideEffect]> {
  let summary = "flatten2 op";
  let description = [{
    
    Flatten Operator
    
    Flattens the input tensor into a 2D matrix.
    
    Examples:
    Case 1:
      Given
        X.shape = (3, 100, 100, 4)
      and
        axis = 2
      We get:
        Out.shape = (3 * 100, 4 * 100)
    
    Case 2:
      Given
        X.shape = (3, 100, 100, 4)
      and
        axis = 0
      We get:
        Out.shape = (1, 3 * 100 * 100 * 4)
    
  }];
  let arguments = (ins  PD_Tensor:$X, DefaultValuedAttr<SI32Attr, "1">:$axis);

  let results = (outs PD_Tensor:$Out);
}
def PD_MatmulOp : PD_Op<"matmul", [NoSideEffect]> {
  let summary = "matmul op";
  let description = [{
    
    MatMul Operator.
    
    
    This operator is used to perform (batched) matrix multiplication
    over the last two dimensions of the input tensors `X` and `Y`.
    
    If a transpose flag is specified, the last two dimensions of the
    tensor are transposed. If the tensor is rank-1 of shape [D], then
    for `X` it is treated as [1, D] in nontransposed form and as [D, 1]
    in transposed form, whereas for `Y` it is the opposite: It is treated
    as [D, 1] in nontransposed form and as [1, D] in transposed form.
    
    Examples without transpose:
    - X: [K], Y: [K] => Out: [1]
    - X: [K], Y: [K, N] => Out: [N]
    - X: [B, M, K], Y: [K] => Out: [B, M]
    - X: [M, K], Y: [B, K, N] => Out: [B, M, N]
    - X: [B, M, K], Y: [B, K, N] => Out: [B, M, N]
    - X: [B, ..., M, K], Y: [B, ..., K, N] => Out: [B, ..., M, N]
    
    Example of matrix multiplication with head_number of H
    - X: [B, M, K], Y: [B, K, N] => Out: [B, M, H * N]
    
    The behavior is designed to be similar to the `numpy.matmul` function.
    The differences are:
    - When the rank of the input data is less than or equal to 3, it
      is similar to the `numpy.matmul` function.
    - When the rank of the input is greater than 3, the rank of X and
      Y must be equal, and the first `rank - 2` dimensions must be equal.
    - We add `transpose_X` and `transpose_Y` flags.
    - We add `head_number` attribute, which is used to multiple two matrixes head
      by head, and eventually concatenates the output of several (head_number)
      small matrixes multiplication.
    
    Both the input `X` and `Y` can carry the LoD (Level of Details) information,
    or not. But the output only shares the LoD information with input `X`.
    
    
  }];
  let arguments = (ins  PD_Tensor:$X, PD_Tensor:$Y, DefaultValuedAttr<BoolAttr, "false">:$transpose_X, DefaultValuedAttr<BoolAttr, "false">:$transpose_Y, DefaultValuedAttr<F32Attr, "1.0">:$alpha, DefaultValuedAttr<SI32Attr, "1">:$head_number);

  let results = (outs PD_Tensor:$Out);
}
def PD_CvmOp : PD_Op<"cvm", [NoSideEffect]> {
  let summary = "cvm op";
  let description = [{
    
    CVM Operator.
    
          We assume that input X is a embedding vector with cvm_feature(show and click), which shape is [N * D] (D is 2(cvm_feature) + embedding dim, N is batch_size)
          if use_cvm is True, we will log(cvm_feature), and output shape is [N * D].
          if use_cvm is False, we will remove cvm_feature from input, and output shape is [N * (D - 2)].
    
    
  }];
  let arguments = (ins  PD_Tensor:$X, PD_Tensor:$CVM, DefaultValuedAttr<BoolAttr, "true">:$use_cvm);

  let results = (outs PD_Tensor:$Y);
}
def PD_Masked_selectOp : PD_Op<"masked_select", [NoSideEffect]> {
  let summary = "masked_select op";
  let description = [{
    
    Size Operator.
    
    Return a new 0-D tensor which indexes the indexed tensor according
    the mask which is a tensor withe data type bool.
    
  }];
  let arguments = (ins  PD_Tensor:$X, PD_Tensor:$Mask);

  let results = (outs PD_Tensor:$Y);
}
def PD_TraceOp : PD_Op<"trace", [NoSideEffect]> {
  let summary = "trace op";
  let description = [{
    
    Trace Operator.
    Return the sum along diagonals of the input tensor.
    The behavior of this operator is similar to how `numpy.trace` works.
    
    If Input is 2-D, returns the sum of diagonal. 
    If Input has larger dimensions, then returns an tensor of diagonals sum, diagonals be taken from
    the 2-D planes specified by dim1 and dim2.
    
    
  }];
  let arguments = (ins  PD_Tensor:$Input, DefaultValuedAttr<SI32Attr, "0">:$offset, DefaultValuedAttr<SI32Attr, "0">:$axis1, DefaultValuedAttr<SI32Attr, "1">:$axis2);

  let results = (outs PD_Tensor:$Out);
}
def PD_Modified_huber_lossOp : PD_Op<"modified_huber_loss", [NoSideEffect]> {
  let summary = "modified_huber_loss op";
  let description = [{
    
    Modified Huber Loss Operator.
    
    This operator is used in binary classification problem. The shape of
    input X and target Y are both [N, 1] and so is the shape of the output loss.
    Since target Y is not differentiable, calculating gradient for Y is illegal.
    The formula of modified huber loss is:
    
    $$
    L(y, f(x)) = 
    \begin{cases}
    (\max(0, 1 - yf(x)))^2,  \text{if} \  yf(x) >= -1    \\
                 -4yf(x),    \quad \text{otherwise}
    \end{cases}
    $$
    
    Make sure the values of target label Y are in {0, 1} here. This operator will
    scale values of Y to {-1, +1} when computing losses and gradients.
    
    
  }];
  let arguments = (ins  PD_Tensor:$X, PD_Tensor:$Y);

  let results = (outs PD_Tensor:$Out);
}
def PD_RollOp : PD_Op<"roll", [NoSideEffect]> {
  let summary = "roll op";
  let description = [{
    
        Roll the tensor along the given dimension(s). 
        Elements that are shifted beyond the last position
        are re-introduced at the first position. If a dimension
        is not specified, the tensor will be flattened before 
        rolling and then restored to the original shape.
        
  }];
  let arguments = (ins  PD_Tensor:$X, PD_Tensor:$ShiftsTensor, DefaultValuedAttr<I64ArrayAttr, "{}">:$shifts, DefaultValuedAttr<I64ArrayAttr, "{}">:$axis);

  let results = (outs PD_Tensor:$Out);
}
def PD_Squared_l2_distanceOp : PD_Op<"squared_l2_distance", [NoSideEffect]> {
  let summary = "squared_l2_distance op";
  let description = [{
    
    SquaredL2Distance operator
    
    This operator will cacluate the squared L2 distance for the input and 
    the target. Number of distance value will be equal to the first dimension 
    of input. First dimension of the target could be equal to the input or to 1. 
    If the first dimension of target is 1, the operator will broadcast target's 
    first dimension to input's first dimension. During backward propagation, 
    the user can decide whether to calculate the gradient of the input or 
    the target or both.
    
    Both the input X and Y can carry the LoD (Level of Details) information. 
    However, the output only shares the LoD information with input X.
        
  }];
  let arguments = (ins  PD_Tensor:$X, PD_Tensor:$Y);

  let results = (outs PD_Tensor:$Out);
}
def PD_Conv3d_transposeOp : PD_Op<"conv3d_transpose", [NoSideEffect]> {
  let summary = "conv3d_transpose op";
  let description = [{
    
    Convolution3D Transpose Operator.
    
    The convolution transpose operation calculates the output based on the input, filter
    and dilations, strides, paddings, groups parameters. The size of each dimension of the
    parameters is checked in the infer-shape.
    Input(Input) and output(Output) are in NCDHW or NDHWC format. Where N is batch size, C is the
    number of channels, D is the depth of the feature, H is the height of the feature,
    and W is the width of the feature.
    Filter(Input) is in MCDHW format. Where M is the number of input feature channels,
    C is the number of output feature channels, D is the depth of the filter,H is the
    height of the filter, and W is the width of the filter.
    Parameters(strides, paddings) are three elements. These three elements represent
    depth, height and width, respectively.
    The input(X) size and output(Out) size may be different.
    
    Example:
      Input:
           Input shape: $(N, C_{in}, D_{in}, H_{in}, W_{in})$
           Filter shape: $(C_{in}, C_{out}, D_f, H_f, W_f)$
      Output:
           Output shape: $(N, C_{out}, D_{out}, H_{out}, W_{out})$
      Where
      $$
           D_{out} = (D_{in} - 1) * strides[0] - pad_depth_front - pad_depth_back + dilations[0] * (D_f - 1) + 1 \\
           H_{out} = (H_{in} - 1) * strides[1] - pad_height_top  - pad_height_bottom + dilations[1] * (H_f - 1) + 1 \\
           W_{out} = (W_{in} - 1) * strides[2] - pad_width_left - pad_width_right + dilations[2] * (W_f - 1) + 1
      $$
    
  }];
  let arguments = (ins  PD_Tensor:$Input, PD_Tensor:$Filter, DefaultValuedAttr<I32ArrayAttr, "{}">:$output_padding, DefaultValuedAttr<I32ArrayAttr, "{}">:$output_size, DefaultValuedAttr<I32ArrayAttr, "{1, 1, 1}">:$dilations, DefaultValuedAttr<I32ArrayAttr, "{1, 1, 1}">:$strides, DefaultValuedAttr<I32ArrayAttr, "{0, 0, 0}">:$paddings, DefaultValuedAttr<SI32Attr, "1">:$groups, DefaultValuedAttr<StrAttr, "NCHW">:$data_format, DefaultValuedAttr<StrAttr, "EXPLICIT">:$padding_algorithm);

  let results = (outs PD_Tensor:$Output);
}
def PD_ConcatOp : PD_Op<"concat", [NoSideEffect]> {
  let summary = "concat op";
  let description = [{
    
    Concat Operator.
    
    Concatenate the input tensors along dimension axis.
    Examples:
      Input[0] = [[1,2],[3,4]]
      Input[1] = [[5,6]]
      axis = 0
      Output = [[1,2],
                [3,4],
                [5,6]]
    
    
  }];
  let arguments = (ins  PD_Tensor:$X, PD_Tensor:$AxisTensor, DefaultValuedAttr<SI32Attr, "0">:$axis);

  let results = (outs PD_Tensor:$Out);
}
def PD_Hierarchical_sigmoidOp : PD_Op<"hierarchical_sigmoid", [NoSideEffect]> {
  let summary = "hierarchical_sigmoid op";
  let description = [{
    
    The hierarchical sigmoid operator organize the classes into a binary tree.
    At each node, a sigmoid function is used to calculate the probability of
    belonging to the right branch. This idea is from
    "F. Morin, Y. Bengio (AISTATS 05):
    Hierarchical Probabilistic Neural Network Language Model."
          
  }];
  let arguments = (ins  PD_Tensor:$X, PD_Tensor:$W, PD_Tensor:$Label, PD_Tensor:$PathTable, PD_Tensor:$PathCode, PD_Tensor:$Bias, DefaultValuedAttr<SI32Attr, "2">:$num_classes, DefaultValuedAttr<BoolAttr, "false">:$remote_prefetch, DefaultValuedAttr<SI32Attr, "0">:$trainer_id, DefaultValuedAttr<I64ArrayAttr, "{}">:$height_sections, DefaultValuedAttr<StrArrayAttr, "{}">:$epmap, DefaultValuedAttr<StrArrayAttr, "{}">:$table_names, DefaultValuedAttr<BoolAttr, "false">:$is_sparse);

  let results = (outs PD_Tensor:$Out);
}
def PD_SqueezeOp : PD_Op<"squeeze", [NoSideEffect]> {
  let summary = "squeeze op";
  let description = [{
    
            Squeeze Operator.
    
            Remove single-dimensional entries from the shape of a tensor.
            Takes a parameter axes with a list of axes to squeeze.
            If axes is not provided, all the single dimensions will be removed from the shape.
            If an axis is selected with shape entry not equal to one, an error is raised.
    
            Examples:
            Case 1:
              Given
                X.shape = (1, 3, 1, 5)
              and
                axes = [0]
              we get:
                Out.shape = (3, 1, 5)
    
            Case 2:
              Given
                X.shape = (1, 3, 1, 5)
              and
                axes = []
              we get:
                Out.shape = (3, 5)
        
  }];
  let arguments = (ins  PD_Tensor:$X, DefaultValuedAttr<I32ArrayAttr, "{}">:$axes);

  let results = (outs PD_Tensor:$Out);
}
def PD_Bpr_lossOp : PD_Op<"bpr_loss", [NoSideEffect]> {
  let summary = "bpr_loss op";
  let description = [{
    
    Bayesian Personalized Ranking Loss Operator.
    
    This operator belongs to pairwise ranking loss. Label is the desired item.
    The loss at a given point in one session is defined as:
    $Y[i] = -\frac{1}{N_{i}} * \sum_{j=0}^{N_{i}}\log(\sigma(X[i, Label[i]]-X[i, j]))$
    
    Learn more details by reading paper <session-based recommendations with recurrent
    neural networks>(https://arxiv.org/abs/1511.06939)
    
    
  }];
  let arguments = (ins  PD_Tensor:$X, PD_Tensor:$Label);

  let results = (outs PD_Tensor:$Y);
}
def PD_Fft_c2cOp : PD_Op<"fft_c2c", [NoSideEffect]> {
  let summary = "fft_c2c op";
  let description = [{
    
          Compute complex to complex FFT.
        
  }];
  let arguments = (ins  PD_Tensor:$X,I64ArrayAttr:$axes);

  let results = (outs PD_Tensor:$Out);
}
def PD_Bicubic_interp_v2Op : PD_Op<"bicubic_interp_v2", [NoSideEffect]> {
  let summary = "bicubic_interp_v2 op";
  let description = [{
    
              This operator samples input X to given output shape by using specified
              interpolation method, the interpolation methods can be \"nearest\"
              for nearest neighbor interpolation and \"bilinear\" for bilinear 
              interpolation and \"linear\" for linear interpolation..
    
              Nearest neighbor interpolation is to perform nearest neighbor interpolation
              in both the 3rd dimension(in height direction) and the 4th dimension(in width 
              direction) on input tensor.
               
              Linear interpolation is the method of using a line connecting two known quantities 
              to determine the value of an unknown quantity between the two known quantities. 
              
              Bilinear interpolation is an extension of linear interpolation for 
              interpolating functions of two variables (e.g. H-direction and 
              W-direction in this op) on a rectilinear 2D grid. The key idea is 
              to perform linear interpolation first in one direction, and then 
              again in the other direction.
    
              Trilinear interpolation is an extension of linear interpolation for 
              interpolating functions of three variables (e.g. D-direction, 
              H-direction and W-direction in this op) on a rectilinear 3D grid. 
              The linear interpolation is performed on three directions.
    
              Bicubic interpolation is an extension of cubic interpolation for interpolating
              data points on a two-dimensional regular grid. The interpolated surface is
              smoother than corresponding surfaces obtained by bilinear interpolation or
              nearest-neighbor interpolation.
    
              Align_corners and align_mode are optional parameters,the calculation method 
              of interpolation can be selected by them.
              
              Example:
    
              For scale:
              
                if align_corners = True and out_{size}>1 :
    
                  scale_{factor} = (in_{size}-1.0)/(out_{size}-1.0)
                
                else:
                  
                  scale_{factor} = float(in_{size}/out_{size})
                
              
              Nearest neighbor interpolation:
              
              if:
                  align_corners = False
    
                  input : (N,C,H_in,W_in)
                  output: (N,C,H_out,W_out) where:
    
                  H_out = \left \lfloor {H_{in} * scale_{}factor}} \right \rfloor
                  W_out = \left \lfloor {W_{in} * scale_{}factor}} \right \rfloor
    
              else:
                  align_corners = True
    
                  input : (N,C,H_in,W_in)
                  output: (N,C,H_out,W_out) where:
    
                  H_out = round(H_{in} * scale_{factor})
                  W_out = round(W_{in} * scale_{factor})
    
              Bilinear interpolation:
    
              if:
                  align_corners = False , align_mode = 0
                  
                  input : (N,C,H_in,W_in)
                  output: (N,C,H_out,W_out) where:
                  
                  H_out = (H_{in}+0.5) * scale_{factor} - 0.5
                  W_out = (W_{in}+0.5) * scale_{factor} - 0.5
    
    
              else:
               
                  input : (N,C,H_in,W_in)
                  output: (N,C,H_out,W_out) where:
    
                  H_out = H_{in} * scale_{factor}
                  W_out = W_{in} * scale_{factor}
    
              Trilinear interpolation:
    
              if:
                  align_corners = False , align_mode = 0
                  
                  input : (N,C,D_in,H_in,W_in)
                  output: (N,C,D_out,H_out,W_out) where:
                  
                  D_out = (D_{in}+0.5) * scale_{factor} - 0.5
                  H_out = (H_{in}+0.5) * scale_{factor} - 0.5
                  W_out = (W_{in}+0.5) * scale_{factor} - 0.5
    
    
              else:
               
                  input : (N,C,D_in,H_in,W_in)
                  output: (N,C,D_out,H_out,W_out) where:
    
                  D_out = D_{in} * scale_{factor}
                  H_out = H_{in} * scale_{factor}
                  W_out = W_{in} * scale_{factor}
    
              Bicubic interpolation:
    
              if:
                  align_corners = False
                  input : (N,C,H_in,W_in)
                  output: (N,C,H_out,W_out) where:
                  H_out = (H_{in}+0.5) * scale_{factor} - 0.5
                  W_out = (W_{in}+0.5) * scale_{factor} - 0.5
              else:
                  input : (N,C,H_in,W_in)
                  output: (N,C,H_out,W_out) where:
                  H_out = H_{in} * scale_{factor}
                  W_out = W_{in} * scale_{factor}
    
              For details of nearest neighbor interpolation, please refer to Wikipedia: 
              https://en.wikipedia.org/wiki/Nearest-neighbor_interpolation
    
              For details of bilinear interpolation, please refer to Wikipedia: 
              https://en.wikipedia.org/wiki/Bilinear_interp_v2olation
    
              For details of trilinear interpolation, please refer to Wikipedia: 
              https://en.wikipedia.org/wiki/Trilinear_interp_v2olation
    
              For details of bicubic interpolation, please refer to Wikipedia:
              https://en.wikipedia.org/wiki/Bicubic_interpolation
             
  }];
  let arguments = (ins  PD_Tensor:$X, PD_Tensor:$OutSize, PD_Tensor:$SizeTensor, PD_Tensor:$Scale, DefaultValuedAttr<StrAttr, "NCHW">:$data_layout, DefaultValuedAttr<SI32Attr, "0">:$out_d, DefaultValuedAttr<SI32Attr, "0">:$out_h, DefaultValuedAttr<SI32Attr, "0">:$out_w, DefaultValuedAttr<F32ArrayAttr, "{}">:$scale, DefaultValuedAttr<StrAttr, "bilinear">:$interp_method, DefaultValuedAttr<BoolAttr, "true">:$align_corners, DefaultValuedAttr<SI32Attr, "1">:$align_mode);

  let results = (outs PD_Tensor:$Out);
}
def PD_ReshapeOp : PD_Op<"reshape", [NoSideEffect]> {
  let summary = "reshape op";
  let description = [{
    
    Reshape Operator.
    
    Reshape Input(X) into the shape specified by Attr(shape) or Input(Shape). The
    data in Input(X) are unchanged.
    
    Examples:
    
    1. Given a 3-D tensor Input(X) with a shape [2, 4, 6], and the target shape
    specified by Attr(shape) is [6, 8], the reshape operator will transform Input(X)
    into a 2-D tensor with shape [6, 8] and leaving Input(X)'s data unchanged.
    
    2. Given a 3-D tensor Input(X) with a shape [2, 4, 6], and the target shape
    specified by Attr(shape) is [2, 3, -1, 2], the reshape operator will transform
    Input(X) into a 4-D tensor with shape [2, 3, 4, 2] and leaving Input(X)'s data
    unchanged. In this case, one and only dimension of Attr(shape) can be set to -1,
    the value of this dimension is inferred from the total element number of
    Input(X) and remaining dimensions.
    
    3. Given a 3-D tensor Input(X) with a shape [2, 4, 6], and the target shape
    specified by Attr(shape) is [-1, 0, 3, 2], the reshape operator will transform
    Input(X) into a 4-D tensor with shape [2, 4, 3, 2] and leaving Input(X)'s data
    unchanged. In this case, besides -1, 0 means the actual dimension value is going
    to be copied from the corresponding dimension of Input(X).
    
    Note:
    
    1. One and only one dimension in Attr(shape) can be set -1. In this case,
    the actual dimension value will be infered from the total element number of
    Input(X) and remaining dimensions.
    
    2. More than one dimensions in Attr(shape) can be set to 0, which means the real
    dimension value will be copied from Input(X) at runtime. Note that the index of
    0 can not exceed Rank(X). For example, Input(X) is a 3-D tensor with shape
    [2, 3, 4], Attr(shape) = [2, 3, 2, 0] is an invalid input.
    
    3. Input(Shape) has a higher priority than Attr(shape) if it is provided, while
    Attr(shape) still should be set correctly to guarantee shape inference in
    compile-time.
    
    
  }];
  let arguments = (ins  PD_Tensor:$X, PD_Tensor:$Shape, PD_Tensor:$ShapeTensor, DefaultValuedAttr<I32ArrayAttr, "{}">:$shape);

  let results = (outs PD_Tensor:$Out);
}
def PD_Roi_alignOp : PD_Op<"roi_align", [NoSideEffect]> {
  let summary = "roi_align op";
  let description = [{
    
    **RoIAlign Operator**
    
    Region of interest align (also known as RoI align) is to perform
    bilinear interpolation on inputs of nonuniform sizes to obtain 
    fixed-size feature maps (e.g. 7*7)
    
    Dividing each region proposal into equal-sized sections with
    the pooled_width and pooled_height. Location remains the origin
    result.
    
    In each ROI bin, the value of the four regularly sampled locations 
    are computed directly through bilinear interpolation. The output is
    the mean of four locations.
    Thus avoid the misaligned problem.   
        
  }];
  let arguments = (ins  PD_Tensor:$X, PD_Tensor:$ROIs, PD_Tensor:$RoisNum, DefaultValuedAttr<F32Attr, "1.0">:$spatial_scale, DefaultValuedAttr<SI32Attr, "1">:$pooled_height, DefaultValuedAttr<SI32Attr, "1">:$pooled_width, DefaultValuedAttr<SI32Attr, "-1">:$sampling_ratio, DefaultValuedAttr<BoolAttr, "false">:$aligned);

  let results = (outs PD_Tensor:$Out);
}
def PD_Reshape2Op : PD_Op<"reshape2", [NoSideEffect]> {
  let summary = "reshape2 op";
  let description = [{
    
    Reshape Operator.
    
    Reshape Input(X) into the shape specified by Attr(shape) or Input(Shape). The
    data in Input(X) are unchanged.
    
    Examples:
    
    1. Given a 3-D tensor Input(X) with a shape [2, 4, 6], and the target shape
    specified by Attr(shape) is [6, 8], the reshape operator will transform Input(X)
    into a 2-D tensor with shape [6, 8] and leaving Input(X)'s data unchanged.
    
    2. Given a 3-D tensor Input(X) with a shape [2, 4, 6], and the target shape
    specified by Attr(shape) is [2, 3, -1, 2], the reshape operator will transform
    Input(X) into a 4-D tensor with shape [2, 3, 4, 2] and leaving Input(X)'s data
    unchanged. In this case, one and only dimension of Attr(shape) can be set to -1,
    the value of this dimension is inferred from the total element number of
    Input(X) and remaining dimensions.
    
    3. Given a 3-D tensor Input(X) with a shape [2, 4, 6], and the target shape
    specified by Attr(shape) is [-1, 0, 3, 2], the reshape operator will transform
    Input(X) into a 4-D tensor with shape [2, 4, 3, 2] and leaving Input(X)'s data
    unchanged. In this case, besides -1, 0 means the actual dimension value is going
    to be copied from the corresponding dimension of Input(X).
    
    Note:
    
    1. One and only one dimension in Attr(shape) can be set -1. In this case,
    the actual dimension value will be infered from the total element number of
    Input(X) and remaining dimensions.
    
    2. More than one dimensions in Attr(shape) can be set to 0, which means the real
    dimension value will be copied from Input(X) at runtime. Note that the index of
    0 can not exceed Rank(X). For example, Input(X) is a 3-D tensor with shape
    [2, 3, 4], Attr(shape) = [2, 3, 2, 0] is an invalid input.
    
    3. Input(Shape) has a higher priority than Attr(shape) if it is provided, while
    Attr(shape) still should be set correctly to guarantee shape inference in
    compile-time.
    
    
  }];
  let arguments = (ins  PD_Tensor:$X, PD_Tensor:$Shape, PD_Tensor:$ShapeTensor, DefaultValuedAttr<I32ArrayAttr, "{}">:$shape, DefaultValuedAttr<BoolAttr, "false">:$use_quantizer, DefaultValuedAttr<StrAttr, "float32">:$mkldnn_data_type);

  let results = (outs PD_Tensor:$Out);
}
def PD_UnstackOp : PD_Op<"unstack", [NoSideEffect]> {
  let summary = "unstack op";
  let description = [{
    
          UnStack Operator.
    
          UnStack Input(X) into several tensors along Attr(axis).
        
  }];
  let arguments = (ins  PD_Tensor:$X, DefaultValuedAttr<SI32Attr, "0">:$axis);

  let results = (outs PD_Tensor:$Y);
}
def PD_Scatter_nd_addOp : PD_Op<"scatter_nd_add", [NoSideEffect]> {
  let summary = "scatter_nd_add op";
  let description = [{
    
    Scatter_nd_add Operator.
    
    Output is obtained by applying sparse addition to a single value or slice in a Variable.
    
          Given:
            * Case 1:
                ref = [0, 1, 2, 3, 4, 5]
                index = [[1], [2], [3], [1]]
                updates = [9, 10, 11, 12]
    
              we get:
    
                output = [0, 22, 12, 14, 4, 5]
    
            * Case 2:
                ref = [[65, 17], [-14, -25]]
                index = [[], []]
                updates = [[[-1, -2], [1, 2]],
                           [[3, 4], [-3, -4]]]
                ref.shape = (2, 2)
                index.shape = (2, 0)
                updates.shape = (2, 2, 2)
    
              we get:
    
                output = [[67, 19], [-16, -27]]
    
  }];
  let arguments = (ins  PD_Tensor:$X, PD_Tensor:$Index, PD_Tensor:$Updates);

  let results = (outs PD_Tensor:$Out);
}
def PD_Sequence_reshapeOp : PD_Op<"sequence_reshape", [NoSideEffect]> {
  let summary = "sequence_reshape op";
  let description = [{
    
    Sequence Reshape Operator.
    
    This operator will rearrange the input sequences. The new dimension is set by
    attribute and length of each sequence may change longer or shorter which is
    decided by original length, original dimension and new dimension. The following
    example will help to illustrate the function of this operator:
    
    x is a LoDTensor:
        x.lod  = [[0, 2, 6]]
        x.data = [[1, 2], [3, 4],
                  [5, 6], [7, 8], [9, 10], [11, 12]]
        x.dims = [6, 2]
    
    set new_dim = 4
    
    then out is a LoDTensor:
        out.lod  = [[0, 1, 3]]
        out.data = [[1, 2, 3, 4],
                    [5, 6, 7, 8], [9, 10, 11, 12]]
        out.dims = [3, 4]
    
    Currently, only 1-level LoDTensor is supported and please make sure (original
    length * original dimension) can be divided by new_dim with no remainder for
    each sequence.
    
    
  }];
  let arguments = (ins  PD_Tensor:$X);

  let results = (outs PD_Tensor:$Out);
}
def PD_Bilateral_sliceOp : PD_Op<"bilateral_slice", [NoSideEffect]> {
  let summary = "bilateral_slice op";
  let description = [{
    
              This operator enhance input X according guide and grid
              For details of bilateral slice, please refer to paper:
              https://groups.csail.mit.edu/graphics/hdrnet/
             
  }];
  let arguments = (ins  PD_Tensor:$X, PD_Tensor:$Grid, PD_Tensor:$Guide, DefaultValuedAttr<BoolAttr, "false">:$has_offset);

  let results = (outs PD_Tensor:$Out);
}
def PD_Pad_constant_likeOp : PD_Op<"pad_constant_like", [NoSideEffect]> {
  let summary = "pad_constant_like op";
  let description = [{
    
    PadConstantLikeOp Operator.
    
    Pad input(Y) with a pad_value, the number of values padded to the edges of each
    axis is specified by the difference of the shape of X and Y.
    ((0, shape_x_0 - shape_y_0), ... (0, shape_x_n - shape_y_n)) unique pad widths for
    each axis.
    The input should be a k-D tensor(k > 0 and k < 7). As an example:
    
    case1:
        Given:
            X = [[1, 2],
                 [3, 4],
                 [1, 2],
                 [3, 4]]],
            X.shape = (4, 2)
    
            Y = [[5, 6],
                [7, 8]],
            Y.shape = (2, 2)
    
        And
            pad_value = 0,
    
        Return:
            Out = [[5, 6],
                   [7, 8],
                   [0, 0],
                   [0, 0]]
            Out.shape = (4, 2)
    
    case2:
        Given:
            X = [[[[ 0,  1,  2],
                   [ 3,  4,  5]],
                  [[ 6,  7,  8],
                   [ 9, 10, 11]],
                  [[12, 13, 14],
                   [15, 16, 17]]],
                 [[[18, 19, 20],
                   [21, 22, 23]],
                  [[24, 25, 26],
                   [27, 28, 29]],
                  [[30, 31, 32],
                   [33, 34, 35]]]]
            X.shape = (2, 3, 2, 3)
    
            Y = [[[[35, 36, 37]],
                  [[38, 39, 40]],
                  [[41, 42, 43]]]]
            Y.shape = (1, 3, 1, 3)
    
        And
            pad_value = -1,
    
        Return:
    
            Out = [[[[35, 36, 37],
                     [-1, -1, -1]],
                    [[38, 39, 40],
                     [-1, -1, -1]],
                    [[41, 42, 43],
                     [-1, -1, -1]]],
                   [[[-1, -1, -1],
                     [-1, -1, -1]],
                    [[-1, -1, -1],
                     [-1, -1, -1]],
                    [[-1, -1, -1],
                     [-1, -1, -1]]]]
            Out.shape = (2, 3, 2, 3)
    
  }];
  let arguments = (ins  PD_Tensor:$X, PD_Tensor:$Y, DefaultValuedAttr<F32Attr, "0.0">:$pad_value);

  let results = (outs PD_Tensor:$Out);
}
def PD_Pool2dOp : PD_Op<"pool2d", [NoSideEffect]> {
  let summary = "pool2d op";
  let description = [{
    
    This operation calculates the pooling output based on
    the input, pooling_type and pool_size, pool_stride, pool_padding parameters.
    Input(X) and Output(Out) are in NCHW or NHWC format, where N is batch size, C is the
    number of channels, H is the height of the feature, and W is the width of the feature.
    Parameters(pool_size, pool_stride, pool_padding) hold two integer elements.
    These two elements represent height and width, respectively.
    The input(X) size and output(Out) size may be different.
    
    Example:
    
      Input:
    
           X shape: $(N, C, H_{in}, W_{in})$
    
      Output:
    
           Out shape: $(N, C, H_{out}, W_{out})$
    
      For pool_padding = "SAME":
           $$
           H_{out} = \\frac{(H_{in} + strides[0] - 1)}{strides[0]}
           $$
           $$
           W_{out} = \\frac{(W_{in} + strides[1] - 1)}{strides[1]}
           $$
    
      For pool_padding = "VALID":
           $$
           H_{out} = \\frac{(H_{in} - ksize[0] + strides[0])}{strides[0]}
           $$
           $$
           W_{out} = \\frac{(W_{in} - ksize[1] + strides[1])}{strides[1]}
           $$
    
      For ceil_mode = false:
           $$
           H_{out} = \\frac{(H_{in} - ksize[0] + pad_height_top + pad_height_bottom}{strides[0]} + 1
           $$
           $$
           W_{out} = \\frac{(W_{in} - ksize[1] + pad_width_left + pad_width_right}{strides[1]} + 1
           $$
    
      For ceil_mode = true:
           $$
           H_{out} = \\frac{(H_{in} - ksize[0] + pad_height_top + pad_height_bottom + strides[0] - 1)}{strides[0]} + 1
           $$
           $$
           W_{out} = \\frac{(W_{in} - ksize[1] + pad_width_left + pad_width_right + strides[1] - 1)}{strides[1]} + 1
           $$
    
      For exclusive = false:
           $$
           hstart = i * strides[0] - pad_height_top
           $$
           $$
           hend = hstart + ksize[0]
           $$
           $$
           wstart = j * strides[1] - pad_width_left
           $$
           $$
           wend = wstart + ksize[1]
           $$
           $$
           Output(i ,j) = \\frac{sum(Input[hstart:hend, wstart:wend])}{ksize[0] * ksize[1]}
           $$
    
      For exclusive = true:
           $$
           hstart = max(0, i * strides[0] - pad_height_top)
           $$
           $$
           hend = min(H, hstart + ksize[0])
           $$
           $$
           wstart = max(0, j * strides[1] - pad_width_left)
           $$
           $$
           wend = min(W, wstart + ksize[1])
           $$
           $$
           Output(i ,j) = \\frac{sum(Input[hstart:hend, wstart:wend])}{(hend - hstart) * (wend - wstart)}
           $$
    
    
  }];
  let arguments = (ins  PD_Tensor:$X,I32ArrayAttr:$ksize, DefaultValuedAttr<BoolAttr, "false">:$global_pooling, DefaultValuedAttr<I32ArrayAttr, "{1, 1}">:$strides, DefaultValuedAttr<I32ArrayAttr, "{0, 0}">:$paddings, DefaultValuedAttr<BoolAttr, "true">:$exclusive, DefaultValuedAttr<BoolAttr, "false">:$adaptive, DefaultValuedAttr<BoolAttr, "false">:$ceil_mode, DefaultValuedAttr<StrAttr, "NCHW">:$data_format, DefaultValuedAttr<StrAttr, "EXPLICIT">:$padding_algorithm);

  let results = (outs PD_Tensor:$Out);
}
def PD_ImagOp : PD_Op<"imag", [NoSideEffect]> {
  let summary = "imag op";
  let description = [{
    
    Imag Operator.
    
    This operator is used to get a new tensor containing imaginary values
    from a tensor with complex data type.
    
    
  }];
  let arguments = (ins  PD_Tensor:$X);

  let results = (outs PD_Tensor:$Out);
}
def PD_EighOp : PD_Op<"eigh", [NoSideEffect]> {
  let summary = "eigh op";
  let description = [{
    
    Eigh Operator.
    
    Computes the eigenvalues and eigenvectors of a complex Hermitian
     (conjugate symmetric) or a real symmetric matrix.
    
    
  }];
  let arguments = (ins  PD_Tensor:$X, DefaultValuedAttr<StrAttr, "L">:$UPLO);

  let results = (outs PD_Tensor:$Eigenvalues,PD_Tensor:$Eigenvectors);
}
def PD_StackOp : PD_Op<"stack", [NoSideEffect]> {
  let summary = "stack op";
  let description = [{
    
    Stack Operator.
    Stack all of the Inputs(X) into one tensor along Attr(axis). The dims of all Inputs(X) must be the same.
    
  }];
  let arguments = (ins  PD_Tensor:$X, DefaultValuedAttr<SI32Attr, "0">:$axis);

  let results = (outs PD_Tensor:$Y);
}
def PD_Gru_unitOp : PD_Op<"gru_unit", [NoSideEffect]> {
  let summary = "gru_unit op";
  let description = [{
    
    GRUUnit Operator implements partial calculations of the GRU unit as following:
    
    $$
    update \ gate: u_t = actGate(xu_t + W_u * h_{t-1} + b_u) \\
    reset \ gate: r_t = actGate(xr_t + W_r * h_{t-1} + b_r)  \\
    output \ candidate: {h}_t = actNode(xc_t + W_c * dot(r_t, h_{t-1}) + b_c) \\
    output: h_t = dot((1 - u_t), h_{t-1}) + dot(u_t, {h}_t)
    $$
    
    which is same as one time step of GRU Operator.
    
    @note To implement the complete GRU unit, fully-connected operator must be
    used before to feed xu, xr and xc as the Input of GRUUnit operator.
    
    
  }];
  let arguments = (ins  PD_Tensor:$Input, PD_Tensor:$HiddenPrev, PD_Tensor:$Weight, PD_Tensor:$Bias, DefaultValuedAttr<SI32Attr, "2">:$activation, DefaultValuedAttr<SI32Attr, "1">:$gate_activation, DefaultValuedAttr<BoolAttr, "false">:$origin_mode);

  let results = (outs PD_Tensor:$Hidden);
}
def PD_Unsqueeze2Op : PD_Op<"unsqueeze2", [NoSideEffect]> {
  let summary = "unsqueeze2 op";
  let description = [{
    
        Unsqueeze Operator.
    
        Insert single-dimensional entries to the shape of a tensor.
        Takes one required argument axes, a list of dimensions that will be inserted.
        Dimension indices in axes are as seen in the output tensor.
    
        For example:
          Given a tensor such that tensor with shape [3, 4, 5],
          then Unsqueeze(tensor, axes=[0, 4]) has shape [1, 3, 4, 5, 1]
        
  }];
  let arguments = (ins  PD_Tensor:$X, PD_Tensor:$AxesTensor, PD_Tensor:$AxesTensorList, DefaultValuedAttr<I32ArrayAttr, "{}">:$axes);

  let results = (outs PD_Tensor:$Out);
}
def PD_Bce_lossOp : PD_Op<"bce_loss", [NoSideEffect]> {
  let summary = "bce_loss op";
  let description = [{
    
    BinaryCrossEntropy operator.
    
    This measures the element-wise probability error in classification tasks
    in which each class is independent.
    
    The logitstic loss is given as follows:
          $$loss = -Label * \log(X) - (1 - Label) * \log(1 - X)$$
    
  }];
  let arguments = (ins  PD_Tensor:$X, PD_Tensor:$Label);

  let results = (outs PD_Tensor:$Out);
}
def PD_Im2sequenceOp : PD_Op<"im2sequence", [NoSideEffect]> {
  let summary = "im2sequence op";
  let description = [{
    
    This op uses kernels to scan images and converts these images to sequences.
    After expanding, The number of time steps are output_height * output_width
    and the dimension of each time step is kernel_height * kernel_width * channels,
    in which:
    
    output_height =
        1 + (padding_height + padding_down + img_height - kernel_height + stride_height - 1) /
                stride_height;
    output_width =
        1 + (padding_left + padding+right + img_width - kernel_width + stride_width - 1) /
                stride_width;
    
    This op can be used after convolution neural network, and before recurrent neural network.
    
    Given:
    
    x = [[[[ 6.  2.  1.]
           [ 8.  3.  5.]
           [ 0.  2.  6.]]
    
          [[ 2.  4.  4.]
           [ 6.  3.  0.]
           [ 6.  4.  7.]]]
    
         [[[ 6.  7.  1.]
           [ 5.  7.  9.]
           [ 2.  4.  8.]]
    
          [[ 1.  2.  1.]
           [ 1.  3.  5.]
           [ 9.  0.  8.]]]]
    x.dims = {2, 2, 3, 3}
    
    And:
    
    kernels = [2, 2]
    strides = [1, 1]
    paddings = [0, 0, 0, 0]
    
    Then:
    
    output.data = [[ 6.  2.  8.  3.  2.  4.  6.  3.]
                   [ 2.  1.  3.  5.  4.  4.  3.  0.]
                   [ 8.  3.  0.  2.  6.  3.  6.  4.]
                   [ 3.  5.  2.  6.  3.  0.  4.  7.]
                   [ 6.  7.  5.  7.  1.  2.  1.  3.]
                   [ 7.  1.  7.  9.  2.  1.  3.  5.]
                   [ 5.  7.  2.  4.  1.  3.  9.  0.]
                   [ 7.  9.  4.  8.  3.  5.  0.  8.]]
    output.dims = {8, 8}
    output.lod = [[0, 4, 8]]
    
    
  }];
  let arguments = (ins  PD_Tensor:$X, PD_Tensor:$Y,I32ArrayAttr:$kernels, DefaultValuedAttr<I32ArrayAttr, "{1, 1}">:$strides, DefaultValuedAttr<I32ArrayAttr, "{0, 0, 0, 0}">:$paddings, DefaultValuedAttr<I32ArrayAttr, "{1, 1}">:$out_stride);

  let results = (outs PD_Tensor:$Out);
}
def PD_Linear_chain_crfOp : PD_Op<"linear_chain_crf", [NoSideEffect]> {
  let summary = "linear_chain_crf op";
  let description = [{
    
    Conditional Random Field defines an undirected probabilistic graph with nodes
    denoting random variables and edges denoting dependencies between these
    variables. CRF learns the conditional probability $P(Y|X)$, where
    $X = (x_1, x_2, ... , x_n)$ are structured inputs and
    $Y = (y_1, y_2, ... , y_n)$ are labels for the inputs.
    
    Linear chain CRF is a special case of CRF that is useful for sequence labeling
    task. Sequence labeling tasks do not assume a lot of conditional
    independences among inputs. The only constraint they impose is that the input
    and output must be linear sequences. Thus, the graph of such a CRF is a simple
    chain or a line, which results in the linear chain CRF.
    
    This operator implements the Forward-Backward algorithm for the linear chain
    CRF. Please refer to http://www.cs.columbia.edu/~mcollins/fb.pdf and
    http://cseweb.ucsd.edu/~elkan/250Bwinter2012/loglinearCRFs.pdf for details.
    
    Equation:
    
    1. Denote Input(Emission) to this operator as $x$ here.
    2. The first D values of Input(Transition) to this operator are for starting
    weights, denoted as $a$ here.
    3. The next D values of Input(Transition) of this operator are for ending
    weights, denoted as $b$ here.
    4. The remaning values of Input(Transition) are for transition weights,
    denoted as $w$ here.
    5. Denote Input(Label) as $s$ here.
    
    The probability of a sequence $s$ of length $L$ is defined as:
    $$P(s) = (1/Z) \exp(a_{s_1} + b_{s_L}
                    + \sum_{l=1}^L x_{s_l}
                    + \sum_{l=2}^L w_{s_{l-1},s_l})$$
    
    where $Z$ is a normalization value so that the sum of $P(s)$ over
    all possible sequences is 1, and $x$ is the emission feature weight
    to the linear chain CRF.
    
    Finally, the linear chain CRF operator outputs the logarithm of the conditional
    likelihood of each training sample in a mini-batch.
    
    NOTE:
    
    1. The feature function for a CRF is made up of the emission features and the
    transition features. The emission feature weights are NOT computed in
    this operator. They MUST be computed first before this operator is called.
    
    2. Because this operator performs global normalization over all possible
    sequences internally, it expects UNSCALED emission feature weights.
    Please do not call this op with the emission feature being output of any
    nonlinear activation.
    
    3. The 2nd dimension of Input(Emission) MUST be equal to the tag number.
    
    
  }];
  let arguments = (ins  PD_Tensor:$Emission, PD_Tensor:$Transition, PD_Tensor:$Label, PD_Tensor:$Length);

  let results = (outs PD_Tensor:$LogLikelihood);
}
def PD_Teacher_student_sigmoid_lossOp : PD_Op<"teacher_student_sigmoid_loss", [NoSideEffect]> {
  let summary = "teacher_student_sigmoid_loss op";
  let description = [{
    
    TeacherStudentSigmoidLoss Operator.
    
    It's similarity to SigmoidCrossEntropyWithLogits Operator. The difference is that
    we add another label(z') to original.
            loss = max(x, 0) - x * z + log(1 + exp(-abs(x))) + max(x, 0) - x * z' + log(1 + exp(-abs(x)))
            z is click or not
            z' is teacher value 
            label = {-2, -1, [0, 2]}
            when z' is not exist, clk = 0 : label = -2;
            when z' is not exist, clk = 1 : label = -1;
            when z' is exist , clk = 0 : label = 0 + z';
            when z' is exist    , clk = 1 : label = 1 + z';
    
    
  }];
  let arguments = (ins  PD_Tensor:$X, PD_Tensor:$Label, DefaultValuedAttr<F32Attr, "15.0">:$soft_max_up_bound, DefaultValuedAttr<F32Attr, "-15.0">:$soft_max_lower_bound);

  let results = (outs PD_Tensor:$Y);
}
def PD_Lookup_table_v2Op : PD_Op<"lookup_table_v2", [NoSideEffect]> {
  let summary = "lookup_table_v2 op";
  let description = [{
    
    Lookup Table V2 Operator.
    
    This operator is used to perform lookups on the parameter W,
    then concatenated into a dense tensor.
    
    The input Ids can carry the LoD (Level of Details) information,
    or not. And the output only shares the LoD information with input Ids.
    
    
  }];
  let arguments = (ins  PD_Tensor:$W, PD_Tensor:$Ids, DefaultValuedAttr<SI64Attr, "-1">:$padding_idx);

  let results = (outs PD_Tensor:$Out);
}
def PD_L1_normOp : PD_Op<"l1_norm", [NoSideEffect]> {
  let summary = "l1_norm op";
  let description = [{
    
    L1 Norm Operator.
    
    Computes the L1 norm of a tensor.
    
    $$Out = \sum{|X|}$$
    
    
  }];
  let arguments = (ins  PD_Tensor:$X);

  let results = (outs PD_Tensor:$Out);
}
def PD_SqrtOp : PD_Op<"sqrt", [NoSideEffect]> {
  let summary = "sqrt op";
  let description = [{
    
    Sqrt Activation Operator.
    
    $$out=\\sqrt{x}=x^{1/2}$$
    
    **Note**:
      input value must be greater than or equal to zero.
    
    
  }];
  let arguments = (ins  PD_Tensor:$X);

  let results = (outs PD_Tensor:$Out);
}
def PD_Fused_elemwise_activationOp : PD_Op<"fused_elemwise_activation", [NoSideEffect]> {
  let summary = "fused_elemwise_activation op";
  let description = [{
    
    FusedElemwiseActivation Operator.
    
    At present, FusedElemwiseActivation only supports Two kinds of compound
    operators (elementwise_op and activation_op):
    
        Z = Binary(X, Unary(Y))
        Z = Unary(Binary(X, Y))
    
    There are two cases for this operator:
    
    1. The shape of $Y$ and $X$ is the same.
    2. The shape of $Y$ is a continuous subsequence of $X$ or the shape of $X$ is a continuous subsequence of $Y$.
    
    For case 2 (assume that the shape of $Y$ is a continuous subsequence of $X$ ):
    
    1. Broadcast $Y$ to match the shape of $X$, where $axis$ is the start dimension index
       for broadcasting $Y$ onto $X$.
    2. If $axis$ is -1 (default), $axis = rank(X) - rank(Y)$.
    3. The trailing dimensions of size 1 for $Y$ will be ignored for the consideration of
       subsequence, such as shape(Y) = (2, 1) => (2).
    
    For example:
    
      .. code-block:: python
    
        shape(X) = (2, 3, 4, 5), shape(Y) = (,)
        shape(X) = (2, 3, 4, 5), shape(Y) = (5,)
        shape(X) = (2, 3, 4, 5), shape(Y) = (4, 5), with axis=-1(default) or axis=2
        shape(X) = (2, 3, 4, 5), shape(Y) = (3, 4), with axis=1
        shape(X) = (2, 3, 4, 5), shape(Y) = (2), with axis=0
        shape(X) = (2, 3, 4, 5), shape(Y) = (2, 1), with axis=0
    
    
    The inputs $X$ and $Y$ can carry the different LoD information.
    But the output only shares the LoD information with the one whose shape is the same with Out.
    The attributions of activation_op can be get from fused_elemwise_activation_op's.
    The functor_list records the functions to be fused, for example
    ["scale", "elementwise_add"].
    
    
  }];
  let arguments = (ins  PD_Tensor:$X, PD_Tensor:$Y, DefaultValuedAttr<SI32Attr, "-1">:$axis, DefaultValuedAttr<F32Attr, "0.0">:$scale, DefaultValuedAttr<BoolAttr, "false">:$save_intermediate_out,StrArrayAttr:$functor_list);

  let results = (outs PD_Tensor:$Out);
}
def PD_SlogdeterminantOp : PD_Op<"slogdeterminant", [NoSideEffect]> {
  let summary = "slogdeterminant op";
  let description = [{
    
    SlogDeterminant Operator.
  }];
  let arguments = (ins  PD_Tensor:$Input);

  let results = (outs PD_Tensor:$Out);
}
def PD_DropoutOp : PD_Op<"dropout", [NoSideEffect]> {
  let summary = "dropout op";
  let description = [{
    
    Dropout Operator.
    
    Dropout refers to randomly dropping out units in a nerual network. It is a
    regularization technique for reducing overfitting by preventing neuron
    co-adaption during training. The dropout operator randomly set (according to
    the given dropout probability) the outputs of some units to zero, while others
    are set equal to their corresponding inputs.
    
    
  }];
  let arguments = (ins  PD_Tensor:$X, DefaultValuedAttr<F32Attr, "0.5">:$dropout_prob, DefaultValuedAttr<StrAttr, "downgrade_in_infer">:$dropout_implementation);

  let results = (outs PD_Tensor:$Out);
}
def PD_Log_lossOp : PD_Op<"log_loss", [NoSideEffect]> {
  let summary = "log_loss op";
  let description = [{
    
    LogLoss Operator.
    
    Log loss is a loss function used for binary classification. Log Loss quantifies
    the accuracy of a classifier by penalising false classifications. Minimising the
    Log Loss is equivalent to maximising the accuracy of the classifier. We define
    Predicted as the values predicted by our model and Labels as the target ground
    truth value. Log loss can evaluate how close the predicted values are to the
    target. The shapes of Predicted and Labels are both [batch_size, 1].
    The equation is:
    
    $$
    Loss = - Labels * log(Predicted + \epsilon) -
            (1 - Labels) * log(1 - Predicted + \epsilon)
    $$
    
    
  }];
  let arguments = (ins  PD_Tensor:$Predicted, PD_Tensor:$Labels);

  let results = (outs PD_Tensor:$Loss);
}
def PD_Sigmoid_focal_lossOp : PD_Op<"sigmoid_focal_loss", [NoSideEffect]> {
  let summary = "sigmoid_focal_loss op";
  let description = [{
    
    Sigmoid Focal Loss Operator.
    
    Focal loss is used to address the foreground-background class imbalance existed
    on the training phase of one-stage detectors. This operator computes the sigmoid
    value for each element in the input tensor, after which focal loss is measured.
    
    The focal loss is given as follows:
    
    $$Loss_j = (-Label_j * alpha * \pow(1 - \sigma(X_j), gamma) * \log(\sigma(X_j)) -
    (1 - Labels_j) * (1 - alpha) * \pow(\sigma(X_j), gamma) * \log(1 - \sigma(X_j)))
    / FgNum, j = 1,...,K$$
    
    We know that $$\sigma(X_j) = \\frac{1}{1 + \exp(-X_j)}$$.
    
    
  }];
  let arguments = (ins  PD_Tensor:$X, PD_Tensor:$Label, PD_Tensor:$FgNum, DefaultValuedAttr<F32Attr, "2.0">:$gamma, DefaultValuedAttr<F32Attr, "0.25">:$alpha);

  let results = (outs PD_Tensor:$Out);
}
def PD_Conv2dOp : PD_Op<"conv2d", [NoSideEffect]> {
  let summary = "conv2d op";
  let description = [{
    
    Convolution Operator.
    
    The convolution operation calculates the output based on the input, filter
    and strides, paddings, dilations, groups parameters. The size of each dimension of the
    parameters is checked in the infer-shape.
    Input(Input) and Output(Output) are in NCHW or NHWC format. Where N is batch
    size, C is the number of channels, H is the height of the feature, and W is
    the width of the feature.
    Filters(Input) is MCHW format format. Where M is the number of output image channels, C is
    the number of input image channels, H is the height of the filter, and W
    is the width of the filter.
    Parameters(strides, paddings, dilations) are two elements. These two elements represent
    height and width, respectively.
    The input(X) size and output(Out) size may be different.
    
    Example:
      Input:
           Input shape: $(N, C_{in}, H_{in}, W_{in})$
           Filter shape: $(C_{out}, C_{in}, H_f, W_f)$
      Output:
           Output shape: $(N, C_{out}, H_{out}, W_{out})$
      Where
    $$
           H_{out}= \frac{(H_{in} + pad_height_top + pad_height_bottom - (dilations[0] * (H_f - 1) + 1))}{strides[0]}+ 1 \\
           W_{out}= \frac{(W_{in} + pad_width_left + pad_width_right - (dilations[1] * (W_f - 1) + 1))}{strides[1]}+ 1
    $$
    
  }];
  let arguments = (ins  PD_Tensor:$Input, PD_Tensor:$Filter, DefaultValuedAttr<I32ArrayAttr, "{1, 1}">:$strides, DefaultValuedAttr<I32ArrayAttr, "{0, 0}">:$paddings, DefaultValuedAttr<StrAttr, "EXPLICIT">:$padding_algorithm, DefaultValuedAttr<SI32Attr, "1">:$groups, DefaultValuedAttr<I32ArrayAttr, "{1, 1}">:$dilations, DefaultValuedAttr<StrAttr, "NCHW">:$data_format);

  let results = (outs PD_Tensor:$Output);
}
def PD_Uniform_random_inplaceOp : PD_Op<"uniform_random_inplace", [NoSideEffect]> {
  let summary = "uniform_random_inplace op";
  let description = [{
    
    This operator fills self tensor with random values sampled from a
    uniform distribution. The random result is in a range of [min, max).
    
  }];
  let arguments = (ins  PD_Tensor:$X, DefaultValuedAttr<F32Attr, "-1.0">:$min, DefaultValuedAttr<F32Attr, "1.0">:$max, DefaultValuedAttr<SI32Attr, "0">:$seed, DefaultValuedAttr<SI32Attr, "0">:$diag_num, DefaultValuedAttr<SI32Attr, "0">:$diag_step, DefaultValuedAttr<F32Attr, "1.0">:$diag_val);

  let results = (outs PD_Tensor:$Out);
}
def PD_MaxoutOp : PD_Op<"maxout", [NoSideEffect]> {
  let summary = "maxout op";
  let description = [{
    
    MaxOut Operator.
    
    Assumed the input shape is (N, Ci, H, W).
    The output shape is (N, Co, H, W).
    Then $Co = Ci / groups$ and the operator formula is as follows:
    
    $$ y_{si+j} = \max_{k} x_{gsi + sk + j} $$
    $$ g = groups $$
    $$ s = \\frac{input.size}{num\\_channels} $$
    $$ 0 \\le i < \\frac{num\\_channels}{groups} $$
    $$ 0 \\le j < s $$
    $$ 0 \\le k < groups $$
    
    Please refer to Paper:
      - Maxout Networks: http://www.jmlr.org/proceedings/papers/v28/goodfellow13.pdf
      - Multi-digit Number Recognition from Street View \
        Imagery using Deep Convolutional Neural Networks: \
        https://arxiv.org/pdf/1312.6082v4.pdf
    
    
  }];
  let arguments = (ins  PD_Tensor:$X, DefaultValuedAttr<SI32Attr, "1">:$axis);

  let results = (outs PD_Tensor:$Out);
}
def PD_Linear_interpOp : PD_Op<"linear_interp", [NoSideEffect]> {
  let summary = "linear_interp op";
  let description = [{
    
              This operator samples input X to given output shape by using specified
              interpolation method, the interpolation methods can be \"nearest\"
              for nearest neighbor interpolation and \"bilinear\" for bilinear 
              interpolation and \"linear\" for linear interpolation..
    
              Nearest neighbor interpolation is to perform nearest neighbor interpolation
              in both the 3rd dimension(in height direction) and the 4th dimension(in width 
              direction) on input tensor.
               
              Linear interpolation is the method of using a line connecting two known quantities 
              to determine the value of an unknown quantity between the two known quantities. 
              
              Bilinear interpolation is an extension of linear interpolation for 
              interpolating functions of two variables (e.g. H-direction and 
              W-direction in this op) on a rectilinear 2D grid. The key idea is 
              to perform linear interpolation first in one direction, and then 
              again in the other direction.
    
              Trilinear interpolation is an extension of linear interpolation for 
              interpolating functions of three variables (e.g. D-direction, 
              H-direction and W-direction in this op) on a rectilinear 3D grid. 
              The linear interpolation is performed on three directions.
    
              Bicubic interpolation is an extension of cubic interpolation for interpolating
              data points on a two-dimensional regular grid. The interpolated surface is
              smoother than corresponding surfaces obtained by bilinear interpolation or
              nearest-neighbor interpolation.
    
              Align_corners and align_mode are optional parameters,the calculation method 
              of interpolation can be selected by them.
              
              Example:
    
              For scale:
              
                if align_corners = True and out_{size}>1 :
    
                  scale_{factor} = (in_{size}-1.0)/(out_{size}-1.0)
                
                else:
                  
                  scale_{factor} = float(in_{size}/out_{size})
                
              
              Nearest neighbor interpolation:
              
              if:
                  align_corners = False
    
                  input : (N,C,H_in,W_in)
                  output: (N,C,H_out,W_out) where:
    
                  H_out = \left \lfloor {H_{in} * scale_{}factor}} \right \rfloor
                  W_out = \left \lfloor {W_{in} * scale_{}factor}} \right \rfloor
    
              else:
                  align_corners = True
    
                  input : (N,C,H_in,W_in)
                  output: (N,C,H_out,W_out) where:
    
                  H_out = round(H_{in} * scale_{factor})
                  W_out = round(W_{in} * scale_{factor})
    
              Bilinear interpolation:
    
              if:
                  align_corners = False , align_mode = 0
                  
                  input : (N,C,H_in,W_in)
                  output: (N,C,H_out,W_out) where:
                  
                  H_out = (H_{in}+0.5) * scale_{factor} - 0.5
                  W_out = (W_{in}+0.5) * scale_{factor} - 0.5
    
    
              else:
               
                  input : (N,C,H_in,W_in)
                  output: (N,C,H_out,W_out) where:
    
                  H_out = H_{in} * scale_{factor}
                  W_out = W_{in} * scale_{factor}
    
              Trilinear interpolation:
    
              if:
                  align_corners = False , align_mode = 0
                  
                  input : (N,C,D_in,H_in,W_in)
                  output: (N,C,D_out,H_out,W_out) where:
                  
                  D_out = (D_{in}+0.5) * scale_{factor} - 0.5
                  H_out = (H_{in}+0.5) * scale_{factor} - 0.5
                  W_out = (W_{in}+0.5) * scale_{factor} - 0.5
    
    
              else:
               
                  input : (N,C,D_in,H_in,W_in)
                  output: (N,C,D_out,H_out,W_out) where:
    
                  D_out = D_{in} * scale_{factor}
                  H_out = H_{in} * scale_{factor}
                  W_out = W_{in} * scale_{factor}
    
              Bicubic interpolation:
    
              if:
                  align_corners = False
                  input : (N,C,H_in,W_in)
                  output: (N,C,H_out,W_out) where:
                  H_out = (H_{in}+0.5) * scale_{factor} - 0.5
                  W_out = (W_{in}+0.5) * scale_{factor} - 0.5
              else:
                  input : (N,C,H_in,W_in)
                  output: (N,C,H_out,W_out) where:
                  H_out = H_{in} * scale_{factor}
                  W_out = W_{in} * scale_{factor}
    
              For details of nearest neighbor interpolation, please refer to Wikipedia: 
              https://en.wikipedia.org/wiki/Nearest-neighbor_interpolation
    
              For details of bilinear interpolation, please refer to Wikipedia: 
              https://en.wikipedia.org/wiki/Bilinear_interpolation
    
              For details of trilinear interpolation, please refer to Wikipedia: 
              https://en.wikipedia.org/wiki/Trilinear_interpolation
    
              For details of bicubic interpolation, please refer to Wikipedia:
              https://en.wikipedia.org/wiki/Bicubic_interpolation
             
  }];
  let arguments = (ins  PD_Tensor:$X, PD_Tensor:$OutSize, PD_Tensor:$SizeTensor, PD_Tensor:$Scale, DefaultValuedAttr<StrAttr, "NCHW">:$data_layout, DefaultValuedAttr<SI32Attr, "0">:$out_d, DefaultValuedAttr<SI32Attr, "0">:$out_h, DefaultValuedAttr<SI32Attr, "0">:$out_w, DefaultValuedAttr<F32Attr, "0.0">:$scale, DefaultValuedAttr<StrAttr, "bilinear">:$interp_method, DefaultValuedAttr<BoolAttr, "true">:$align_corners, DefaultValuedAttr<SI32Attr, "1">:$align_mode);

  let results = (outs PD_Tensor:$Out);
}
def PD_Batch_normOp : PD_Op<"batch_norm", [NoSideEffect]> {
  let summary = "batch_norm op";
  let description = [{
    
    Batch Normalization.
    
    Batch Norm has been implemented as discussed in the paper:
    https://arxiv.org/pdf/1502.03167.pdf
    Can be used as a normalizer function for conv2d and fully_connected operations.
    The required data format for this layer is one of the following:
    1. NHWC `[batch, in_height, in_width, in_channels]`
    2. NCHW `[batch, in_channels, in_height, in_width]`
    
    
  }];
  let arguments = (ins  PD_Tensor:$X, PD_Tensor:$Scale, PD_Tensor:$Bias, PD_Tensor:$Mean, PD_Tensor:$Variance, PD_Tensor:$MomentumTensor, DefaultValuedAttr<F32Attr, "0.8999999761581421">:$momentum, DefaultValuedAttr<F32Attr, "9.999999747378752e-06">:$epsilon, DefaultValuedAttr<StrAttr, "NCHW">:$data_layout);

  let results = (outs PD_Tensor:$Y,PD_Tensor:$MeanOut,PD_Tensor:$VarianceOut);
}
def PD_Elementwise_addOp : PD_Op<"elementwise_add", [NoSideEffect]> {
  let summary = "elementwise_add op";
  let description = [{
    
    Elementwise Add Operator.
    
    Add two tensors element-wise
    
    The equation is:
    
    $$Out = X + Y$$
    
    - $X$: a tensor of any dimension.
    - $Y$: a tensor whose dimensions must be less than or equal to the dimensions of $X$.
    
    There are two cases for this operator:
    
    1. The shape of $Y$ is the same with $X$.
    2. The shape of $Y$ is a continuous subsequence of $X$.
    
    For case 2:
    
    1. Broadcast $Y$ to match the shape of $X$, where $axis$ is the start dimension index
       for broadcasting $Y$ onto $X$.
    2. If $axis$ is -1 (default), $axis = rank(X) - rank(Y)$.
    3. The trailing dimensions of size 1 for $Y$ will be ignored for the consideration of
       subsequence, such as shape(Y) = (2, 1) => (2).
    
    For example:
    
      .. code-block:: text
    
        shape(X) = (2, 3, 4, 5), shape(Y) = (,)
        shape(X) = (2, 3, 4, 5), shape(Y) = (5,)
        shape(X) = (2, 3, 4, 5), shape(Y) = (4, 5), with axis=-1(default) or axis=2
        shape(X) = (2, 3, 4, 5), shape(Y) = (3, 4), with axis=1
        shape(X) = (2, 3, 4, 5), shape(Y) = (2), with axis=0
        shape(X) = (2, 3, 4, 5), shape(Y) = (2, 1), with axis=0
    
    
  }];
  let arguments = (ins  PD_Tensor:$X, PD_Tensor:$Y, DefaultValuedAttr<SI32Attr, "-1">:$axis);

  let results = (outs PD_Tensor:$Out);
}
def PD_AcosOp : PD_Op<"acos", [NoSideEffect]> {
  let summary = "acos op";
  let description = [{
    
    Arccosine Operator.
    
    $$out = \cos^{-1}(x)$$
    
    
  }];
  let arguments = (ins  PD_Tensor:$X);

  let results = (outs PD_Tensor:$Out);
}
def PD_UnpoolOp : PD_Op<"unpool", [NoSideEffect]> {
  let summary = "unpool op";
  let description = [{
    
    Input shape is: $(N, C_{in}, H_{in}, W_{in})$, Output shape is:
    $(N, C_{out}, H_{out}, W_{out})$, where
    $$
    H_{out} = (H_{in}-1) * strides[0] - 2 * paddings[0] + ksize[0] \\
    W_{out} = (W_{in}-1) * strides[1] - 2 * paddings[1] + ksize[1]
    $$
    Paper: http://www.matthewzeiler.com/wp-content/uploads/2017/07/iccv2011.pdf
    
  }];
  let arguments = (ins  PD_Tensor:$X, PD_Tensor:$Indices,I32ArrayAttr:$ksize, DefaultValuedAttr<I32ArrayAttr, "{1, 1}">:$strides, DefaultValuedAttr<I32ArrayAttr, "{0, 0}">:$paddings, DefaultValuedAttr<I32ArrayAttr, "{0, 0}">:$output_size, DefaultValuedAttr<StrAttr, "NCHW">:$data_format);

  let results = (outs PD_Tensor:$Out);
}
def PD_CumprodOp : PD_Op<"cumprod", [NoSideEffect]> {
  let summary = "cumprod op";
  let description = [{
    Cumprod operator. Return the cumprod results of the input elements along the dim.
                  For example, if input X is a tensor with rank 1 and N elements, the output will also be a tensor 
                  with rank 1 and N elements, and elements y[i] = x[0] * x[1] * x[2] *...* x[i] (0<=i<N)
  }];
  let arguments = (ins  PD_Tensor:$X);

  let results = (outs PD_Tensor:$Out);
}
def PD_Sample_logitsOp : PD_Op<"sample_logits", [NoSideEffect]> {
  let summary = "sample_logits op";
  let description = [{
    
      """
      Computes sampled output training logits and labels suitable for implementing
      sampled softmax.        
      """
    
    
  }];
  let arguments = (ins  PD_Tensor:$Logits, PD_Tensor:$Labels, PD_Tensor:$CustomizedSamples, PD_Tensor:$CustomizedProbabilities, DefaultValuedAttr<BoolAttr, "false">:$use_customized_samples, DefaultValuedAttr<BoolAttr, "true">:$uniq, DefaultValuedAttr<BoolAttr, "true">:$remove_accidental_hits, DefaultValuedAttr<SI32Attr, "0">:$seed);

  let results = (outs PD_Tensor:$SampledLabels);
}
def PD_Crop_tensorOp : PD_Op<"crop_tensor", [NoSideEffect]> {
  let summary = "crop_tensor op";
  let description = [{
    
    CropTensor Operator.
    
    Crop input into output, as specified by offsets and shape.
    
    There are three ways to set the offsets:
    1. Input 'OffsetsTensor: It is a tensor list. It should be set as a list that 
                             contains tensor variable in python configure script. 
                             This way is suitable for dynamic offsets.
    2. Input 'Offsets': It is a variable and can be output of other operators. 
                        This way is suitable for dynamic offsets.
    3. Attribute 'offsets': It will be set in python configure script. This way 
                            is suitable for fixed offsets.
    
    You CANNOT use these three ways at the same time. An exception will be raised 
    if input 'OffsetsTensor' or 'Offset' is configured and meanwhile the attribute 'offsets' is 
    not empty.
    
    There are three ways to set shape:
    1. Input 'ShapeTensor': It is a tensor list. It should be set as a list that contains
                            tensor variable in python configure script. This way is suitable 
                            for dynamic shape.
    2. Input 'Shape': It is a Variable and can be output of other operators. This way is suitable 
                      for dynamic shape.
    2. Attribute 'shape': crop input X into the shape described by a list<int>. The size of shape 
                          list should be the same as the dimension size of input X. This way is 
                          suitable for fixed shape.
    
    The input should be a k-D tensor(k > 0 and k < 7). As an example:
    
    Case 1:
    Given
    
        X = [[0, 1, 2, 0, 0]
             [0, 3, 4, 0, 0]
             [0, 0, 0, 0, 0]],
    
    and
    
        offsets = [0, 1],
    
    and
    
        shape = [2, 2],
    
    we get:
    
        Out = [[1, 2],
               [3, 4]].
    
    
    Case 2:
    Given
    
        X = [[0, 1, 2, 5, 0]
             [0, 3, 4, 6, 0]
             [0, 0, 0, 0, 0]],
    
    and offsets is a list that contains tensor variable,
    in runtime offses_var' s value is 1.
    
        offsets = [0, offsets_var],
    
    and shape is a list that contains tensor variable,
    in runtime dim's value is 2.
    
        shape = [dim, 3]
    
    we get:
    
        Out = [[1, 2, 5],
               [3, 4, 6]].
    
  }];
  let arguments = (ins  PD_Tensor:$X, PD_Tensor:$Shape, PD_Tensor:$Offsets, PD_Tensor:$ShapeTensor, PD_Tensor:$OffsetsTensor, DefaultValuedAttr<I32ArrayAttr, "{}">:$offsets, DefaultValuedAttr<I32ArrayAttr, "{}">:$shape);

  let results = (outs PD_Tensor:$Out);
}
def PD_Deformable_convOp : PD_Op<"deformable_conv", [NoSideEffect]> {
  let summary = "deformable_conv op";
  let description = [{
    
    **Deformable Convolution Operator**
    
    Compute 2-D deformable convolution on 4-D input.
    
    Given input image x, output feature map y, the deformable convolution operation can be expressed as follow:
    
    $$
    y(p) = \\sum_{k=1}^{K}{w_k * x(p + p_k + \\Delta p_k) * \\Delta m_k}
    $$
    
    Where $$\\Delta p_k$$ and $$\Delta m_k$$ are the learnable offset and modulation scalar for the k-th location, respectively.
    
    Refer to 'Deformable ConvNets v2: More Deformable, Better Results
    '<https://arxiv.org/abs/1811.11168v2>
    
    Example:
      Input:
           Input shape: $(N, C_{in}, H_{in}, W_{in})$
           Filter shape: $(C_{out}, C_{in}, H_f, W_f)$
           Offset shape: $(N, 2 * deformable_groups, * H_f * W_f, H_{out}, W_{out})$
           Mask shape: $(N, deformable_groups * H_f * W_f, H_{out}, W_{out})$
      Output:
           Output shape: $(N, C_{out}, H_{out}, W_{out})$
                         where $H_{out}, W_{out}$ must be equal to $H_{in}, W_{in}$ respectively.
      Where
    $$
           H_{out}= \frac{(H_{in} + 2 * paddings[0] - (dilations[0] * (H_f - 1) + 1))}{strides[0]}+ 1 \\
           W_{out}= \frac{(W_{in} + 2 * paddings[1] - (dilations[1] * (W_f - 1) + 1))}{strides[1]}+ 1
    $$
    
  }];
  let arguments = (ins  PD_Tensor:$Input, PD_Tensor:$Offset, PD_Tensor:$Mask, PD_Tensor:$Filter, DefaultValuedAttr<I32ArrayAttr, "{1, 1}">:$strides, DefaultValuedAttr<I32ArrayAttr, "{0, 0}">:$paddings, DefaultValuedAttr<I32ArrayAttr, "{1, 1}">:$dilations, DefaultValuedAttr<SI32Attr, "1">:$groups, DefaultValuedAttr<SI32Attr, "1">:$deformable_groups, DefaultValuedAttr<SI32Attr, "64">:$im2col_step);

  let results = (outs PD_Tensor:$Output);
}
def PD_Expand_asOp : PD_Op<"expand_as", [NoSideEffect]> {
  let summary = "expand_as op";
  let description = [{
    
    Expand as operator tiles the input by given times number. You should set times
    number for each dimension by providing tensor 'expend_tensor'. The rank of X
    should be in [1, 6]. Please note that size of 'expend_tensor' must be the same
    with X's rank. Following is a using case:
    Input(X) is a 3-D tensor with shape [2, 3, 1]:
            [
               [[1], [2], [3]],
               [[4], [5], [6]]
            ]
    target_tensors'shape:  [2, 6, 2]
    Output(Out) is a 3-D tensor with shape [2, 6, 2]:
            [
                [[1, 1], [2, 2], [3, 3], [1, 1], [2, 2], [3, 3]],
                [[4, 4], [5, 5], [6, 6], [4, 4], [5, 5], [6, 6]]
            ]
    
  }];
  let arguments = (ins  PD_Tensor:$X, PD_Tensor:$target_tensor);

  let results = (outs PD_Tensor:$Out);
}
def PD_Matrix_powerOp : PD_Op<"matrix_power", [NoSideEffect]> {
  let summary = "matrix_power op";
  let description = [{
    
    Matrix Power Operator.
    
    Computes the n-th power of a square matrix or a batch of square matrices.
    
    
  }];
  let arguments = (ins  PD_Tensor:$X);

  let results = (outs PD_Tensor:$Out);
}
def PD_Bilinear_interpOp : PD_Op<"bilinear_interp", [NoSideEffect]> {
  let summary = "bilinear_interp op";
  let description = [{
    
              This operator samples input X to given output shape by using specified
              interpolation method, the interpolation methods can be \"nearest\"
              for nearest neighbor interpolation and \"bilinear\" for bilinear 
              interpolation and \"linear\" for linear interpolation..
    
              Nearest neighbor interpolation is to perform nearest neighbor interpolation
              in both the 3rd dimension(in height direction) and the 4th dimension(in width 
              direction) on input tensor.
               
              Linear interpolation is the method of using a line connecting two known quantities 
              to determine the value of an unknown quantity between the two known quantities. 
              
              Bilinear interpolation is an extension of linear interpolation for 
              interpolating functions of two variables (e.g. H-direction and 
              W-direction in this op) on a rectilinear 2D grid. The key idea is 
              to perform linear interpolation first in one direction, and then 
              again in the other direction.
    
              Trilinear interpolation is an extension of linear interpolation for 
              interpolating functions of three variables (e.g. D-direction, 
              H-direction and W-direction in this op) on a rectilinear 3D grid. 
              The linear interpolation is performed on three directions.
    
              Bicubic interpolation is an extension of cubic interpolation for interpolating
              data points on a two-dimensional regular grid. The interpolated surface is
              smoother than corresponding surfaces obtained by bilinear interpolation or
              nearest-neighbor interpolation.
    
              Align_corners and align_mode are optional parameters,the calculation method 
              of interpolation can be selected by them.
              
              Example:
    
              For scale:
              
                if align_corners = True and out_{size}>1 :
    
                  scale_{factor} = (in_{size}-1.0)/(out_{size}-1.0)
                
                else:
                  
                  scale_{factor} = float(in_{size}/out_{size})
                
              
              Nearest neighbor interpolation:
              
              if:
                  align_corners = False
    
                  input : (N,C,H_in,W_in)
                  output: (N,C,H_out,W_out) where:
    
                  H_out = \left \lfloor {H_{in} * scale_{}factor}} \right \rfloor
                  W_out = \left \lfloor {W_{in} * scale_{}factor}} \right \rfloor
    
              else:
                  align_corners = True
    
                  input : (N,C,H_in,W_in)
                  output: (N,C,H_out,W_out) where:
    
                  H_out = round(H_{in} * scale_{factor})
                  W_out = round(W_{in} * scale_{factor})
    
              Bilinear interpolation:
    
              if:
                  align_corners = False , align_mode = 0
                  
                  input : (N,C,H_in,W_in)
                  output: (N,C,H_out,W_out) where:
                  
                  H_out = (H_{in}+0.5) * scale_{factor} - 0.5
                  W_out = (W_{in}+0.5) * scale_{factor} - 0.5
    
    
              else:
               
                  input : (N,C,H_in,W_in)
                  output: (N,C,H_out,W_out) where:
    
                  H_out = H_{in} * scale_{factor}
                  W_out = W_{in} * scale_{factor}
    
              Trilinear interpolation:
    
              if:
                  align_corners = False , align_mode = 0
                  
                  input : (N,C,D_in,H_in,W_in)
                  output: (N,C,D_out,H_out,W_out) where:
                  
                  D_out = (D_{in}+0.5) * scale_{factor} - 0.5
                  H_out = (H_{in}+0.5) * scale_{factor} - 0.5
                  W_out = (W_{in}+0.5) * scale_{factor} - 0.5
    
    
              else:
               
                  input : (N,C,D_in,H_in,W_in)
                  output: (N,C,D_out,H_out,W_out) where:
    
                  D_out = D_{in} * scale_{factor}
                  H_out = H_{in} * scale_{factor}
                  W_out = W_{in} * scale_{factor}
    
              Bicubic interpolation:
    
              if:
                  align_corners = False
                  input : (N,C,H_in,W_in)
                  output: (N,C,H_out,W_out) where:
                  H_out = (H_{in}+0.5) * scale_{factor} - 0.5
                  W_out = (W_{in}+0.5) * scale_{factor} - 0.5
              else:
                  input : (N,C,H_in,W_in)
                  output: (N,C,H_out,W_out) where:
                  H_out = H_{in} * scale_{factor}
                  W_out = W_{in} * scale_{factor}
    
              For details of nearest neighbor interpolation, please refer to Wikipedia: 
              https://en.wikipedia.org/wiki/Nearest-neighbor_interpolation
    
              For details of bilinear interpolation, please refer to Wikipedia: 
              https://en.wikipedia.org/wiki/Bilinear_interpolation
    
              For details of trilinear interpolation, please refer to Wikipedia: 
              https://en.wikipedia.org/wiki/Trilinear_interpolation
    
              For details of bicubic interpolation, please refer to Wikipedia:
              https://en.wikipedia.org/wiki/Bicubic_interpolation
             
  }];
  let arguments = (ins  PD_Tensor:$X, PD_Tensor:$OutSize, PD_Tensor:$SizeTensor, PD_Tensor:$Scale, DefaultValuedAttr<StrAttr, "NCHW">:$data_layout, DefaultValuedAttr<SI32Attr, "0">:$out_d, DefaultValuedAttr<SI32Attr, "0">:$out_h, DefaultValuedAttr<SI32Attr, "0">:$out_w, DefaultValuedAttr<F32Attr, "0.0">:$scale, DefaultValuedAttr<StrAttr, "bilinear">:$interp_method, DefaultValuedAttr<BoolAttr, "true">:$align_corners, DefaultValuedAttr<SI32Attr, "1">:$align_mode);

  let results = (outs PD_Tensor:$Out);
}
def PD_SigmoidOp : PD_Op<"sigmoid", [NoSideEffect]> {
  let summary = "sigmoid op";
  let description = [{
    
    Sigmoid Activation Operator
    
    $$out = \\frac{1}{1 + e^{-x}}$$
    
    
  }];
  let arguments = (ins  PD_Tensor:$X);

  let results = (outs PD_Tensor:$Out);
}
def PD_Inplace_abnOp : PD_Op<"inplace_abn", [NoSideEffect]> {
  let summary = "inplace_abn op";
  let description = [{
    
    Batch Normalization.
    
    Batch Norm has been implemented as discussed in the paper:
    https://arxiv.org/pdf/1502.03167.pdf
    Can be used as a normalizer function for conv2d and fully_connected operations.
    The required data format for this layer is one of the following:
    1. NHWC `[batch, in_height, in_width, in_channels]`
    2. NCHW `[batch, in_channels, in_height, in_width]`
    
    
  }];
  let arguments = (ins  PD_Tensor:$X, PD_Tensor:$Scale, PD_Tensor:$Bias, PD_Tensor:$Mean, PD_Tensor:$Variance, PD_Tensor:$MomentumTensor, DefaultValuedAttr<F32Attr, "0.8999999761581421">:$momentum, DefaultValuedAttr<F32Attr, "9.999999747378752e-06">:$epsilon, DefaultValuedAttr<StrAttr, "NCHW">:$data_layout, DefaultValuedAttr<StrAttr, "">:$activation, DefaultValuedAttr<F32Attr, "0.10000000149011612">:$alpha, DefaultValuedAttr<BoolAttr, "false">:$use_sync_bn);

  let results = (outs PD_Tensor:$Y,PD_Tensor:$MeanOut,PD_Tensor:$VarianceOut);
}
def PD_SoftshrinkOp : PD_Op<"softshrink", [NoSideEffect]> {
  let summary = "softshrink op";
  let description = [{
    
    :strong:`Softshrink Activation Operator`
    
    ..  math::
        out = \begin{cases}
             x - \lambda, \text{if } x > \lambda \\
             x + \lambda, \text{if } x < -\lambda \\
             0,  \text{otherwise}
             \end{cases}
    
    
  }];
  let arguments = (ins  PD_Tensor:$X, DefaultValuedAttr<F32Attr, "0.5">:$lambda);

  let results = (outs PD_Tensor:$Out);
}
def PD_MulOp : PD_Op<"mul", [NoSideEffect]> {
  let summary = "mul op";
  let description = [{
    
    Mul Operator.
    
    This operator is used to perform matrix multiplication for input $X$ and $Y$.
    
    The equation is:
    
    $$Out = X * Y$$
    
    Both the input $X$ and $Y$ can carry the LoD (Level of Details) information,
    or not. But the output only shares the LoD information with input $X$.
    
    
  }];
  let arguments = (ins  PD_Tensor:$X, PD_Tensor:$Y, DefaultValuedAttr<SI32Attr, "1">:$x_num_col_dims, DefaultValuedAttr<SI32Attr, "1">:$y_num_col_dims);

  let results = (outs PD_Tensor:$Out);
}
def PD_Data_normOp : PD_Op<"data_norm", [NoSideEffect]> {
  let summary = "data_norm op";
  let description = [{
    
    Data Normalization.
    
    Can be used as a normalizer function for data
    The required data format for this layer is one of the following:
    1. NHWC `[batch, in_height, in_width, in_channels]`
    2. NCHW `[batch, in_channels, in_height, in_width]`
    
    
  }];
  let arguments = (ins  PD_Tensor:$scale_w, PD_Tensor:$bias, PD_Tensor:$X, PD_Tensor:$BatchSize, PD_Tensor:$BatchSum, PD_Tensor:$BatchSquareSum, DefaultValuedAttr<F32Attr, "9.999999747378752e-05">:$epsilon, DefaultValuedAttr<SI32Attr, "-1">:$slot_dim, DefaultValuedAttr<F32Attr, "0.9999998807907104">:$summary_decay_rate, DefaultValuedAttr<BoolAttr, "false">:$enable_scale_and_shift, DefaultValuedAttr<StrAttr, "NCHW">:$data_layout, DefaultValuedAttr<BoolAttr, "false">:$sync_stats);

  let results = (outs PD_Tensor:$Y);
}
def PD_Reorder_lod_tensor_by_rankOp : PD_Op<"reorder_lod_tensor_by_rank", [NoSideEffect]> {
  let summary = "reorder_lod_tensor_by_rank op";
  let description = [{
    ReorderLoDTensorByRankTable operator.
    
    Input(X) is a batch of sequences. Input(RankTable) stores new orders of the
    input sequence batch. The reorder_lod_tensor_by_rank operator reorders the
    Input(X) according to the information provided by Input(RankTable).
    
    For example:
    
    If the indices stored in the Input(RankTable) are [3, 0, 2, 1], the
    Input(X) will be reordered that the fourth sequence in Input(X) will become the
    first one, and then followed by the original first, third, and the second one.
    
    This is:
    X = [Seq0, Seq1, Seq2, Seq3]. The indices in RankTable are [3, 0, 2, 1].
    Out =  [Seq3, Seq0, Seq2, Seq1] with a new LoD information.
    
    If the LoD information of Input(X) is empty, this means Input(X) is not sequence
    data. This is also identical to a batch of sequences where each sequence has a
    fixed length 1. In this case, the reorder_lod_tensor_by_rank operator reorders
    each slice of Input(X) along the first axis according to Input(RankTable).
    
    This is:
    X = [Slice0, Slice1, Slice2, Slice3] and its LoD information is empty. The
    indices in RankTable are [3, 0, 2, 1].
    Out = [Slice3, Slice0, Slice2, Slice1] with no LoD information is appended.
    
    **NOTE**: 
    This operator sorts Input(X) according to a given LoDRankTable which does
    not need to be calculated according to Input(X). It can be calculated according
    to another different sequence, and then this operator sorts Input(X) according
    to the given LoDRankTable.
    
    
  }];
  let arguments = (ins  PD_Tensor:$X, PD_Tensor:$RankTable);

  let results = (outs PD_Tensor:$Out);
}
def PD_SppOp : PD_Op<"spp", [NoSideEffect]> {
  let summary = "spp op";
  let description = [{
    
            "With spatial pyramid pooling, the input image can
            be of any sizes. This not only allows arbitrary aspect
            ratios, but also allows arbitrary scales. We can resize
            the input image to any scale (e.g., min(w, h)=180, 224,
            ...) and apply the same deep network. When the
            input image is at different scales, the network (with
            the same filter sizes) will extract features at different
            scales. The scales play important roles in traditional
            methods.
            Input shape: $(N, C_{in}, H_{in}, W_{in})$
            Output shape: $(H_{out}, W_{out})$
            Where
              $$
                H_{out} = N \\
                W_{out} = (((4^pyramid_height) - 1) / (4 - 1))$ * C_{in}
              $$
            paper https://arxiv.org/pdf/1406.4729v4.pdf
            
  }];
  let arguments = (ins  PD_Tensor:$X);

  let results = (outs PD_Tensor:$Out);
}
def PD_FloorOp : PD_Op<"floor", [NoSideEffect]> {
  let summary = "floor op";
  let description = [{
    
    Floor Activation Operator. Computes floor of x element-wise.
    
    $$out = \\lfloor x \\rfloor$$
    
    
  }];
  let arguments = (ins  PD_Tensor:$X);

  let results = (outs PD_Tensor:$Out);
}
def PD_GeluOp : PD_Op<"gelu", [NoSideEffect]> {
  let summary = "gelu op";
  let description = [{
    
    Gelu Activation Operator. 
    
    For more details, please refer to [Gaussian Error Linear Units](https://arxiv.org/pdf/1606.08415.pdf).
    
    when using approximation
    $out = \\frac{1}{2}x(1+tanh(\\sqrt{\\frac{2}{\\pi}}(x+0.044715x^{3}))$
    
    or else
    $out = \\frac{1 + erf(\\frac{x}{\\sqrt{2}})}{2} x$
    
    
  }];
  let arguments = (ins  PD_Tensor:$X, DefaultValuedAttr<BoolAttr, "false">:$approximate);

  let results = (outs PD_Tensor:$Out);
}
def PD_SiluOp : PD_Op<"silu", [NoSideEffect]> {
  let summary = "silu op";
  let description = [{
    
    Silu Activation Operator
    
    $$out = x * \\frac{1}{1 + e^{-x}}$$
    
  }];
  let arguments = (ins  PD_Tensor:$X);

  let results = (outs PD_Tensor:$Out);
}
def PD_RealOp : PD_Op<"real", [NoSideEffect]> {
  let summary = "real op";
  let description = [{
     
    Real Operator. 
    
    This operator is used to get a new tensor containing real values 
    from a tensor with complex data type.
    
    
  }];
  let arguments = (ins  PD_Tensor:$X);

  let results = (outs PD_Tensor:$Out);
}
def PD_Nearest_interp_v2Op : PD_Op<"nearest_interp_v2", [NoSideEffect]> {
  let summary = "nearest_interp_v2 op";
  let description = [{
    
              This operator samples input X to given output shape by using specified
              interpolation method, the interpolation methods can be \"nearest\"
              for nearest neighbor interpolation and \"bilinear\" for bilinear 
              interpolation and \"linear\" for linear interpolation..
    
              Nearest neighbor interpolation is to perform nearest neighbor interpolation
              in both the 3rd dimension(in height direction) and the 4th dimension(in width 
              direction) on input tensor.
               
              Linear interpolation is the method of using a line connecting two known quantities 
              to determine the value of an unknown quantity between the two known quantities. 
              
              Bilinear interpolation is an extension of linear interpolation for 
              interpolating functions of two variables (e.g. H-direction and 
              W-direction in this op) on a rectilinear 2D grid. The key idea is 
              to perform linear interpolation first in one direction, and then 
              again in the other direction.
    
              Trilinear interpolation is an extension of linear interpolation for 
              interpolating functions of three variables (e.g. D-direction, 
              H-direction and W-direction in this op) on a rectilinear 3D grid. 
              The linear interpolation is performed on three directions.
    
              Bicubic interpolation is an extension of cubic interpolation for interpolating
              data points on a two-dimensional regular grid. The interpolated surface is
              smoother than corresponding surfaces obtained by bilinear interpolation or
              nearest-neighbor interpolation.
    
              Align_corners and align_mode are optional parameters,the calculation method 
              of interpolation can be selected by them.
              
              Example:
    
              For scale:
              
                if align_corners = True and out_{size}>1 :
    
                  scale_{factor} = (in_{size}-1.0)/(out_{size}-1.0)
                
                else:
                  
                  scale_{factor} = float(in_{size}/out_{size})
                
              
              Nearest neighbor interpolation:
              
              if:
                  align_corners = False
    
                  input : (N,C,H_in,W_in)
                  output: (N,C,H_out,W_out) where:
    
                  H_out = \left \lfloor {H_{in} * scale_{}factor}} \right \rfloor
                  W_out = \left \lfloor {W_{in} * scale_{}factor}} \right \rfloor
    
              else:
                  align_corners = True
    
                  input : (N,C,H_in,W_in)
                  output: (N,C,H_out,W_out) where:
    
                  H_out = round(H_{in} * scale_{factor})
                  W_out = round(W_{in} * scale_{factor})
    
              Bilinear interpolation:
    
              if:
                  align_corners = False , align_mode = 0
                  
                  input : (N,C,H_in,W_in)
                  output: (N,C,H_out,W_out) where:
                  
                  H_out = (H_{in}+0.5) * scale_{factor} - 0.5
                  W_out = (W_{in}+0.5) * scale_{factor} - 0.5
    
    
              else:
               
                  input : (N,C,H_in,W_in)
                  output: (N,C,H_out,W_out) where:
    
                  H_out = H_{in} * scale_{factor}
                  W_out = W_{in} * scale_{factor}
    
              Trilinear interpolation:
    
              if:
                  align_corners = False , align_mode = 0
                  
                  input : (N,C,D_in,H_in,W_in)
                  output: (N,C,D_out,H_out,W_out) where:
                  
                  D_out = (D_{in}+0.5) * scale_{factor} - 0.5
                  H_out = (H_{in}+0.5) * scale_{factor} - 0.5
                  W_out = (W_{in}+0.5) * scale_{factor} - 0.5
    
    
              else:
               
                  input : (N,C,D_in,H_in,W_in)
                  output: (N,C,D_out,H_out,W_out) where:
    
                  D_out = D_{in} * scale_{factor}
                  H_out = H_{in} * scale_{factor}
                  W_out = W_{in} * scale_{factor}
    
              Bicubic interpolation:
    
              if:
                  align_corners = False
                  input : (N,C,H_in,W_in)
                  output: (N,C,H_out,W_out) where:
                  H_out = (H_{in}+0.5) * scale_{factor} - 0.5
                  W_out = (W_{in}+0.5) * scale_{factor} - 0.5
              else:
                  input : (N,C,H_in,W_in)
                  output: (N,C,H_out,W_out) where:
                  H_out = H_{in} * scale_{factor}
                  W_out = W_{in} * scale_{factor}
    
              For details of nearest neighbor interpolation, please refer to Wikipedia: 
              https://en.wikipedia.org/wiki/Nearest-neighbor_interpolation
    
              For details of bilinear interpolation, please refer to Wikipedia: 
              https://en.wikipedia.org/wiki/Bilinear_interp_v2olation
    
              For details of trilinear interpolation, please refer to Wikipedia: 
              https://en.wikipedia.org/wiki/Trilinear_interp_v2olation
    
              For details of bicubic interpolation, please refer to Wikipedia:
              https://en.wikipedia.org/wiki/Bicubic_interpolation
             
  }];
  let arguments = (ins  PD_Tensor:$X, PD_Tensor:$OutSize, PD_Tensor:$SizeTensor, PD_Tensor:$Scale, DefaultValuedAttr<StrAttr, "NCHW">:$data_layout, DefaultValuedAttr<SI32Attr, "0">:$out_d, DefaultValuedAttr<SI32Attr, "0">:$out_h, DefaultValuedAttr<SI32Attr, "0">:$out_w, DefaultValuedAttr<F32ArrayAttr, "{}">:$scale, DefaultValuedAttr<StrAttr, "bilinear">:$interp_method, DefaultValuedAttr<BoolAttr, "true">:$align_corners, DefaultValuedAttr<SI32Attr, "1">:$align_mode);

  let results = (outs PD_Tensor:$Out);
}
def PD_Squeeze2Op : PD_Op<"squeeze2", [NoSideEffect]> {
  let summary = "squeeze2 op";
  let description = [{
    
            Squeeze Operator.
    
            Remove single-dimensional entries from the shape of a tensor.
            Takes a parameter axes with a list of axes to squeeze.
            If axes is not provided, all the single dimensions will be removed from the shape.
            If an axis is selected with shape entry not equal to one, an error is raised.
    
            Examples:
            Case 1:
              Given
                X.shape = (1, 3, 1, 5)
              and
                axes = [0]
              we get:
                Out.shape = (3, 1, 5)
    
            Case 2:
              Given
                X.shape = (1, 3, 1, 5)
              and
                axes = []
              we get:
                Out.shape = (3, 5)
        
  }];
  let arguments = (ins  PD_Tensor:$X, DefaultValuedAttr<I32ArrayAttr, "{}">:$axes);

  let results = (outs PD_Tensor:$Out);
}
def PD_Strided_sliceOp : PD_Op<"strided_slice", [NoSideEffect]> {
  let summary = "strided_slice op";
  let description = [{
    
    Strided Slice Operator.
    Instead of calling this op directly most users will want to use the
    NumPy-style slicing syntax.
    For Example:
    data = fluid.layers.fill_constant(shape=[3, 3], value=0, dtype='int64')
    y = fluid.layers.strided_slice(data, [0, 1], [1,0], [2, 3], [1, 1])
    
  }];
  let arguments = (ins  PD_Tensor:$Input, PD_Tensor:$StartsTensor, PD_Tensor:$EndsTensor, PD_Tensor:$StridesTensor, PD_Tensor:$StartsTensorList, PD_Tensor:$EndsTensorList, PD_Tensor:$StridesTensorList,I32ArrayAttr:$axes, DefaultValuedAttr<I32ArrayAttr, "{}">:$starts, DefaultValuedAttr<I32ArrayAttr, "{}">:$ends, DefaultValuedAttr<I32ArrayAttr, "{}">:$strides, DefaultValuedAttr<I32ArrayAttr, "{}">:$infer_flags, DefaultValuedAttr<I32ArrayAttr, "{}">:$decrease_axis);

  let results = (outs PD_Tensor:$Out);
}
def PD_Depthwise_conv2d_transposeOp : PD_Op<"depthwise_conv2d_transpose", [NoSideEffect]> {
  let summary = "depthwise_conv2d_transpose op";
  let description = [{
    
    Convolution2D Transpose Operator.
    
    The convolution transpose operation calculates the output based on the input, filter
    and dilations, strides, paddings, groups parameters. The size of each dimension of the
    parameters is checked in the infer-shape.
    Input(Input) and output(Output) are in NCHW or NHWC format. Where N is batchsize, C is the
    number of channels, H is the height of the feature, and W is the width of the feature.
    Filter(Input) is in MCHW format. Where M is the number of input feature channels,
    C is the number of output feature channels, H is the height of the filter,
    and W is the width of the filter.
    Parameters(strides, paddings) are two elements. These two elements represent height
    and width, respectively.
    The input(X) size and output(Out) size may be different.
    
    For an example:
      Input:
           Input shape: $(N, C_{in}, H_{in}, W_{in})$
           Filter shape: $(C_{in}, C_{out}, H_f, W_f)$
      Output:
           Output shape: $(N, C_{out}, H_{out}, W_{out})$
      Where
      $$
           H_{out} = (H_{in} - 1) * strides[0] - pad_height_top - pad_height_bottom  + dilations[0] * (H_f - 1) + 1 \\
           W_{out} = (W_{in} - 1) * strides[1] - pad_width_left  - pad_width_right + dilations[1] * (W_f - 1) + 1
      $$
    
  }];
  let arguments = (ins  PD_Tensor:$Input, PD_Tensor:$Filter, DefaultValuedAttr<I32ArrayAttr, "{}">:$output_padding, DefaultValuedAttr<I32ArrayAttr, "{}">:$output_size, DefaultValuedAttr<SI32Attr, "1">:$groups, DefaultValuedAttr<I32ArrayAttr, "{1, 1}">:$dilations, DefaultValuedAttr<I32ArrayAttr, "{1, 1}">:$strides, DefaultValuedAttr<I32ArrayAttr, "{0, 0}">:$paddings, DefaultValuedAttr<StrAttr, "NCHW">:$data_format, DefaultValuedAttr<StrAttr, "EXPLICIT">:$padding_algorithm);

  let results = (outs PD_Tensor:$Output);
}
def PD_SquareOp : PD_Op<"square", [NoSideEffect]> {
  let summary = "square op";
  let description = [{
    
    The OP square each elements of the inputs.
    
    $$out = x^2$$
    
    
  }];
  let arguments = (ins  PD_Tensor:$X);

  let results = (outs PD_Tensor:$Out);
}
def PD_Var_conv_2dOp : PD_Op<"var_conv_2d", [NoSideEffect]> {
  let summary = "var_conv_2d op";
  let description = [{
    
        Var Size Conv Operator
    
        This operator calculate Out = \sigma \left ( W * X + b \right ), 
        only support 2-D for X.
        
        NOTE: only support 'float32' data type now.
    
      
  }];
  let arguments = (ins  PD_Tensor:$X, PD_Tensor:$ROW, PD_Tensor:$COLUMN, PD_Tensor:$W, DefaultValuedAttr<SI32Attr, "1">:$InputChannel, DefaultValuedAttr<SI32Attr, "1">:$OutputChannel, DefaultValuedAttr<SI32Attr, "1">:$StrideH, DefaultValuedAttr<SI32Attr, "1">:$StrideW, DefaultValuedAttr<SI32Attr, "1">:$KernelH, DefaultValuedAttr<SI32Attr, "1">:$KernelW);

  let results = (outs PD_Tensor:$Out,PD_Tensor:$Col);
}
def PD_Log1pOp : PD_Op<"log1p", [NoSideEffect]> {
  let summary = "log1p op";
  let description = [{
    
    Log Activation Operator.
    
    $out = \ln(x+1)$
    
    Natural logarithm of x.
    
    
  }];
  let arguments = (ins  PD_Tensor:$X);

  let results = (outs PD_Tensor:$Out);
}
def PD_Fused_softmax_mask_upper_triangleOp : PD_Op<"fused_softmax_mask_upper_triangle", [NoSideEffect]> {
  let summary = "fused_softmax_mask_upper_triangle op";
  let description = [{
    
    Softmax Mask Fuse Operator.
    product = matmul(QK)/sqrt(dk)
    output = softmax_mask_fuse_upper_triangle(product)
    to get the final output.
    
  }];
  let arguments = (ins  PD_Tensor:$X);

  let results = (outs PD_Tensor:$Out);
}
def PD_Atan2Op : PD_Op<"atan2", [NoSideEffect]> {
  let summary = "atan2 op";
  let description = [{
    
    Atan2 Operator.
    
    This operator is used to perform elementwise atan2 for input $X1$, $X2$.
    $$out = atan2(x1, x2)$$
    
    
  }];
  let arguments = (ins  PD_Tensor:$X1, PD_Tensor:$X2);

  let results = (outs PD_Tensor:$Out);
}
def PD_Fft_r2cOp : PD_Op<"fft_r2c", [NoSideEffect]> {
  let summary = "fft_r2c op";
  let description = [{
    
          Compute real to complex FFT.
        
  }];
  let arguments = (ins  PD_Tensor:$X,I64ArrayAttr:$axes);

  let results = (outs PD_Tensor:$Out);
}
def PD_Roi_poolOp : PD_Op<"roi_pool", [NoSideEffect]> {
  let summary = "roi_pool op";
  let description = [{
    
    **ROIPool Operator**
    
    Region of interest pooling (also known as RoI pooling) is to perform
    is to perform max pooling on inputs of nonuniform sizes to obtain
    fixed-size feature maps (e.g. 7*7).
    
    The operator has three steps:
    
    1. Dividing each region proposal into equal-sized sections with
       the pooled_width and pooled_height
    
    2. Finding the largest value in each section
    
    3. Copying these max values to the output buffer
    
    ROI Pooling for Faster-RCNN. The link below is a further introduction: 
    https://stackoverflow.com/questions/43430056/what-is-roi-layer-in-fast-rcnn
        
  }];
  let arguments = (ins  PD_Tensor:$X, PD_Tensor:$ROIs, PD_Tensor:$RoisNum, DefaultValuedAttr<F32Attr, "1.0">:$spatial_scale, DefaultValuedAttr<SI32Attr, "1">:$pooled_height, DefaultValuedAttr<SI32Attr, "1">:$pooled_width);

  let results = (outs PD_Tensor:$Out);
}
def PD_Overlap_addOp : PD_Op<"overlap_add", [NoSideEffect]> {
  let summary = "overlap_add op";
  let description = [{
    
          Reconstructs a tensor consisted of overlap added sequences from input frames.
        
  }];
  let arguments = (ins  PD_Tensor:$X, DefaultValuedAttr<SI32Attr, "-1">:$axis);

  let results = (outs PD_Tensor:$Out);
}
def PD_Fill_anyOp : PD_Op<"fill_any", [NoSideEffect]> {
  let summary = "fill_any op";
  let description = [{
    Fill operator with backward;
                    Fill an tensor with `value`. 
                    
  }];
  let arguments = (ins  PD_Tensor:$X, DefaultValuedAttr<F32Attr, "0.0">:$value_float, DefaultValuedAttr<SI32Attr, "0">:$value_int);

  let results = (outs PD_Tensor:$Out);
}
def PD_Max_pool2d_with_indexOp : PD_Op<"max_pool2d_with_index", [NoSideEffect]> {
  let summary = "max_pool2d_with_index op";
  let description = [{
    
    MaxPool2d Operator.
    
    The maxPooling2d with index operation calculates the output and the mask
    based on the input, ksize, strides, and paddings parameters. Input(X) and
    output(Out, Mask) are in NCHW format, where N is batch size, C is the
    number of channels, H is the height of the feature, 
    and W is the width of the feature.
    Parameters(ksize, strides, paddings) are two elements.
    These two elements represent height and width, respectively.
    The input(X) size and output(Out, Mask) size may be different.
    
    Example:
      Input:
           X shape: $(N, C, H_{in}, W_{in})$
      Output:
           Out shape: $(N, C, H_{out}, W_{out})$
           Mask shape: $(N, C, H_{out}, W_{out})$
      Where
           $$
           H_{out} = \frac{(H_{in} - ksize[0] + 2 * paddings[0])}{strides[0]} + 1 \\
           W_{out} = \frac{(W_{in} - ksize[1] + 2 * paddings[1])}{strides[1]} + 1
           $$
      
      For adaptive = true:
           $$
           H_{out} = ksize[0]   W_{out} = ksize[1]
           $$
          
    
    
  }];
  let arguments = (ins  PD_Tensor:$X,I32ArrayAttr:$ksize, DefaultValuedAttr<BoolAttr, "false">:$global_pooling, DefaultValuedAttr<BoolAttr, "false">:$adaptive, DefaultValuedAttr<I32ArrayAttr, "{1, 1}">:$strides, DefaultValuedAttr<I32ArrayAttr, "{0, 0}">:$paddings);

  let results = (outs PD_Tensor:$Out,PD_Tensor:$Mask);
}
def PD_Pad3dOp : PD_Op<"pad3d", [NoSideEffect]> {
  let summary = "pad3d op";
  let description = [{
    
    Pad3d Operator.
    Pad 3-d images according to 'paddings' and 'mode'. 
    If mode is 'reflect', paddings[0] and paddings[1] must be no greater
    than width-1. The height and depth dimension have the same condition.
    
    Given that X is a channel of image from input:
    
    X = [[[[[1, 2, 3],
         [4, 5, 6]]]]]
    
    Case 0:
    
    paddings = [2, 2, 1, 1, 0, 0],
    mode = 'constant'
    pad_value = 0
    
    Out = [[[[[0. 0. 0. 0. 0. 0. 0.]
              [0. 0. 1. 2. 3. 0. 0.]
              [0. 0. 4. 5. 6. 0. 0.]
              [0. 0. 0. 0. 0. 0. 0.]]]]]
    
    Case 1:
    
    paddings = [2, 2, 1, 1, 0, 0],
    mode = 'reflect'
    
    Out = [[[[[6. 5. 4. 5. 6. 5. 4.]
              [3. 2. 1. 2. 3. 2. 1.]
              [6. 5. 4. 5. 6. 5. 4.]
              [3. 2. 1. 2. 3. 2. 1.]]]]]
    
    Case 2:
    
    paddings = [2, 2, 1, 1, 0, 0],
    mode = 'replicate'
    
    Out = [[[[[1. 1. 1. 2. 3. 3. 3.]
              [1. 1. 1. 2. 3. 3. 3.]
              [4. 4. 4. 5. 6. 6. 6.]
              [4. 4. 4. 5. 6. 6. 6.]]]]]
    
    Case 3:
    
    paddings = [2, 2, 1, 1, 0, 0],
    mode = 'circular'
    
    Out = [[[[[5. 6. 4. 5. 6. 4. 5.]
              [2. 3. 1. 2. 3. 1. 2.]
              [5. 6. 4. 5. 6. 4. 5.]
              [2. 3. 1. 2. 3. 1. 2.]]]]]
    
    
  }];
  let arguments = (ins  PD_Tensor:$X, PD_Tensor:$Paddings,I32ArrayAttr:$paddings, DefaultValuedAttr<F32Attr, "0.0">:$value, DefaultValuedAttr<StrAttr, "constant">:$mode, DefaultValuedAttr<StrAttr, "NCDHW">:$data_format);

  let results = (outs PD_Tensor:$Out);
}
def PD_NormOp : PD_Op<"norm", [NoSideEffect]> {
  let summary = "norm op";
  let description = [{
    
    
    Given a tensor, apply 2-normalization along the provided axis.
    
    $$
    y = \frac{x}{ \sqrt{\sum {x^2} + epsion }}
    $$
    
    where, $\sum {x^2}$ is calculated along the `axis` dimension.
            
    
  }];
  let arguments = (ins  PD_Tensor:$X, DefaultValuedAttr<F32Attr, "1.000000013351432e-10">:$epsilon);

  let results = (outs PD_Tensor:$Out);
}
def PD_MishOp : PD_Op<"mish", [NoSideEffect]> {
  let summary = "mish op";
  let description = [{
    
    Mish Activation Operator.
    
    ..  math::
        softplus = \begin{cases}
                x, \text{if } x > \text{threshold} \\
                e^{x}, \text{if } x < -\text{threshold} \\
                \ln(1 + e^{x}),  \text{otherwise}
              \end{cases}
    
        out = x * \tanh(softplus)
    
    
  }];
  let arguments = (ins  PD_Tensor:$X, DefaultValuedAttr<F32Attr, "20.0">:$threshold);

  let results = (outs PD_Tensor:$Out);
}
def PD_FlattenOp : PD_Op<"flatten", [NoSideEffect]> {
  let summary = "flatten op";
  let description = [{
    
    Flatten Operator
    
    Flattens the input tensor into a 2D matrix.
    
    Examples:
    Case 1:
      Given
        X.shape = (3, 100, 100, 4)
      and
        axis = 2
      We get:
        Out.shape = (3 * 100, 4 * 100)
    
    Case 2:
      Given
        X.shape = (3, 100, 100, 4)
      and
        axis = 0
      We get:
        Out.shape = (1, 3 * 100 * 100 * 4)
    
  }];
  let arguments = (ins  PD_Tensor:$X, DefaultValuedAttr<SI32Attr, "1">:$axis);

  let results = (outs PD_Tensor:$Out);
}
def PD_Margin_cross_entropyOp : PD_Op<"margin_cross_entropy", [NoSideEffect]> {
  let summary = "margin_cross_entropy op";
  let description = [{
    
    MarginCrossEntropy Operator
    .. math::
    
        L=-\frac{1}{N}\sum^N_{i=1}\log\frac{e^{s(cos(m_{1}\theta_{y_i}+m_{2})-m_{3})}}{e^{s(cos(m_{1}\theta_{y_i}+m_{2})-m_{3})}+\sum^n_{j=1,j\neq y_i} e^{scos\theta_{y_i}}}
    
    where the :math: `\theta_{y_i}` is the angle between the feature :math: `x` and
    the representation of class :math: `i`. The details of ArcFace loss
    could be referred to https://arxiv.org/abs/1801.07698.
    
    Note that the Op supports model parallel and single GPU. And Logits.shape[-1] can be different each rank.
    
    
  }];
  let arguments = (ins  PD_Tensor:$Logits, PD_Tensor:$Label, DefaultValuedAttr<BoolAttr, "false">:$return_softmax, DefaultValuedAttr<SI32Attr, "0">:$ring_id, DefaultValuedAttr<SI32Attr, "0">:$rank, DefaultValuedAttr<SI32Attr, "1">:$nranks, DefaultValuedAttr<F32Attr, "1.0">:$margin1, DefaultValuedAttr<F32Attr, "0.5">:$margin2, DefaultValuedAttr<F32Attr, "0.0">:$margin3, DefaultValuedAttr<F32Attr, "64.0">:$scale);

  let results = (outs PD_Tensor:$Softmax,PD_Tensor:$Loss);
}
def PD_PowOp : PD_Op<"pow", [NoSideEffect]> {
  let summary = "pow op";
  let description = [{
    
    Pow Activation Operator.
    
    $$out = x^{factor}$$
    
    
  }];
  let arguments = (ins  PD_Tensor:$X, PD_Tensor:$FactorTensor, DefaultValuedAttr<F32Attr, "1.0">:$factor);

  let results = (outs PD_Tensor:$Out);
}
def PD_StanhOp : PD_Op<"stanh", [NoSideEffect]> {
  let summary = "stanh op";
  let description = [{
    
    STanh Activation Operator.
    
    $$out = b * \\frac{e^{a * x} - e^{-a * x}}{e^{a * x} + e^{-a * x}}$$
    
    
  }];
  let arguments = (ins  PD_Tensor:$X, DefaultValuedAttr<F32Attr, "0.6700000166893005">:$scale_a, DefaultValuedAttr<F32Attr, "1.71589994430542">:$scale_b);

  let results = (outs PD_Tensor:$Out);
}
def PD_Label_smoothOp : PD_Op<"label_smooth", [NoSideEffect]> {
  let summary = "label_smooth op";
  let description = [{
    
    LabelSmooth Operator.
    
    Label smoothing is a mechanism to regularize the classifier layer. In machine 
    learning, optimizing the log-likelihood of the correct label directly may 
    cause two problems. First, it may result in overfitting: if the model learns 
    to assign full probability to the ground-truth label for each training example,
    it is not guaranteed to generalize. Second, it encourages the differences 
    between the largest logit and all others to become large, reducing the ability 
    of the model to adapt. Label smoothing is proposed to encourage the model to 
    be less confident, which replaces the ground-truth label $y$ with the weighted 
    sum of itself and some fixed distribution $\mu$, i.e.
    
    $$
        \tilde{y} = (1 - \epsilon) * y + \epsilon * \mu,
    $$
    
    where $(1 - \epsilon)$ and $\epsilon$ are the weights respectively, and 
    $\tilde{y}$ is the smoothed label. Usually uniform distribution is used for 
    $\mu$. This change in the ground-truth label is called label-smoothing 
    regularization or LSR.
    
    See more details about label smoothing in https://arxiv.org/abs/1512.00567.
    
    
  }];
  let arguments = (ins  PD_Tensor:$X, PD_Tensor:$PriorDist, DefaultValuedAttr<F32Attr, "0.0">:$epsilon);

  let results = (outs PD_Tensor:$Out);
}
def PD_Roi_perspective_transformOp : PD_Op<"roi_perspective_transform", [NoSideEffect]> {
  let summary = "roi_perspective_transform op";
  let description = [{
    
    **ROIPerspectiveTransform Operator**
    
        
  }];
  let arguments = (ins  PD_Tensor:$X, PD_Tensor:$ROIs, DefaultValuedAttr<F32Attr, "1.0">:$spatial_scale, DefaultValuedAttr<SI32Attr, "1">:$transformed_height, DefaultValuedAttr<SI32Attr, "1">:$transformed_width);

  let results = (outs PD_Tensor:$Out,PD_Tensor:$Mask,PD_Tensor:$TransformMatrix);
}
def PD_ExpandOp : PD_Op<"expand", [NoSideEffect]> {
  let summary = "expand op";
  let description = [{
    
    Expand operator tiles the input by given times number. You should set times
    number for each dimension by providing attribute 'expand_times'. The rank of X
    should be in [1, 6]. Please note that size of 'expand_times' must be the same
    with X's rank. Following is a using case:
    
    Input(X) is a 3-D tensor with shape [2, 3, 1]:
    
            [
               [[1], [2], [3]],
               [[4], [5], [6]]
            ]
    
    Attr(expand_times):  [1, 2, 2]
    
    Output(Out) is a 3-D tensor with shape [2, 6, 2]:
    
            [
                [[1, 1], [2, 2], [3, 3], [1, 1], [2, 2], [3, 3]],
                [[4, 4], [5, 5], [6, 6], [4, 4], [5, 5], [6, 6]]
            ]
    
    
  }];
  let arguments = (ins  PD_Tensor:$X, PD_Tensor:$ExpandTimes, PD_Tensor:$expand_times_tensor, DefaultValuedAttr<I32ArrayAttr, "{}">:$expand_times);

  let results = (outs PD_Tensor:$Out);
}
def PD_Rnn_memory_helperOp : PD_Op<"rnn_memory_helper", [NoSideEffect]> {
  let summary = "rnn_memory_helper op";
  let description = [{
    
  }];
  let arguments = (ins  PD_Tensor:$X, DefaultValuedAttr<SI32Attr, "5">:$dtype);

  let results = (outs PD_Tensor:$Out);
}
def PD_Prroi_poolOp : PD_Op<"prroi_pool", [NoSideEffect]> {
  let summary = "prroi_pool op";
  let description = [{
    
    **PRROIPool Operator**
    
    Precise region of interest pooling (also known as PRROIPooling) is to perform
     bilinear interpolation average pooling method for RoI Pooling.
    
    Please refer to https://arxiv.org/abs/1807.11590 for more details.
    
        
  }];
  let arguments = (ins  PD_Tensor:$X, PD_Tensor:$ROIs, PD_Tensor:$BatchRoINums, DefaultValuedAttr<F32Attr, "1.0">:$spatial_scale, DefaultValuedAttr<SI32Attr, "1">:$pooled_height, DefaultValuedAttr<SI32Attr, "1">:$pooled_width);

  let results = (outs PD_Tensor:$Out);
}
def PD_Pool3dOp : PD_Op<"pool3d", [NoSideEffect]> {
  let summary = "pool3d op";
  let description = [{
    
    This operation calculates the output based on
    the input, pooling_type, pool_size, pool_stride, and pool_padding parameters.
    Input(X) and output(Out) are in NCDHW or NDHWC format, where N is batch
    size, C is the number of channels, and D, H and W are the depth, height and
    width of the feature, respectively. Parameters(pool_size, pool_stride, pool_padding)
    hold three integer elements. These three elements represent depth, height and
    width, respectively. The input(X) size and output(Out) size may be different.
    
    Example:
      Input:
           X shape: $(N, C, D_{in}, H_{in}, W_{in})$
      Output:
           Out shape: $(N, C, D_{out}, H_{out}, W_{out})$
    
      For pool_padding = "SAME":
           $$
           D_{out} = \\frac{(D_{in} + strides[0] - 1)}{strides[0]}
           $$
           $$
           H_{out} = \\frac{(H_{in} + strides[1] - 1)}{strides[1]}
           $$
           $$
           W_{out} = \\frac{(W_{in} + strides[2] - 1)}{strides[2]}
           $$
    
      For pool_padding = "VALID":
           $$
           D_{out} = \\frac{(D_{in} - ksize[0] + strides[0])}{strides[0]}
           $$
           $$
           H_{out} = \\frac{(H_{in} - ksize[1] + strides[1])}{strides[1]}
           $$
           $$
           W_{out} = \\frac{(W_{in} - ksize[2] + strides[2])}{strides[2]}
           $$
    
      For ceil_mode = false:
           $$
           D_{out} = \\frac{(D_{in} - ksize[0] + pad_depth_front + pad_depth_back)}{strides[0]} + 1
           $$
           $$
           H_{out} = \\frac{(H_{in} - ksize[1] + pad_height_top + pad_height_bottom)}{strides[1]} + 1
           $$
           $$
           W_{out} = \\frac{(W_{in} - ksize[2] + pad_width_left + pad_width_right)}{strides[2]} + 1
           $$
      For ceil_mode = true:
           $$
           D_{out} = \\frac{(D_{in} - ksize[0] + pad_depth_front + pad_depth_back + strides[0] -1)}{strides[0]} + 1
           $$
           $$
           H_{out} = \\frac{(H_{in} - ksize[1] + pad_height_top + pad_height_bottom + strides[1] -1)}{strides[1]} + 1
           $$
           $$
           W_{out} = \\frac{(W_{in} - ksize[2] + pad_width_left + pad_width_right + strides[2] -1)}{strides[2]} + 1
           $$
    
      For exclusive = false:
           $$
           dstart = i * strides[0] - pad_depth_front
           $$
           $$
           dend = dstart + ksize[0]
           $$
           $$
           hstart = j * strides[1] - pad_height_top
           $$
           $$
           hend = hstart + ksize[1]
           $$
           $$
           wstart = k * strides[2] -  pad_width_left
           $$
           $$
           wend = wstart + ksize[2]
           $$
           $$
           Output(i ,j, k) = \\frac{sum(Input[dstart:dend, hstart:hend, wstart:wend])}{ksize[0] * ksize[1] * ksize[2]}
           $$
    
      For exclusive = true:
           $$
           dstart = max(0, i * strides[0] - pad_depth_front)
           $$
           $$
           dend = min(D, dstart + ksize[0])
           $$
           $$
           hstart = max(0, j * strides[1] - pad_height_top)
           $$
           $$
           hend = min(H, hstart + ksize[1])
           $$
           $$
           wstart = max(0, k * strides[2] - pad_width_left)
           $$
           $$
           wend = min(W, wstart + ksize[2])
           $$
           $$
           Output(i ,j, k) = \\frac{sum(Input[dstart:dend, hstart:hend, wstart:wend])}{(dend - dstart) * (hend - hstart) * (wend - wstart)}
           $$
    
    
  }];
  let arguments = (ins  PD_Tensor:$X,I32ArrayAttr:$ksize, DefaultValuedAttr<BoolAttr, "false">:$global_pooling, DefaultValuedAttr<I32ArrayAttr, "{1, 1, 1}">:$strides, DefaultValuedAttr<I32ArrayAttr, "{0, 0, 0}">:$paddings, DefaultValuedAttr<BoolAttr, "true">:$exclusive, DefaultValuedAttr<BoolAttr, "false">:$adaptive, DefaultValuedAttr<BoolAttr, "false">:$ceil_mode, DefaultValuedAttr<StrAttr, "NCDHW">:$data_format, DefaultValuedAttr<StrAttr, "EXPLICIT">:$padding_algorithm);

  let results = (outs PD_Tensor:$Out);
}
def PD_FrameOp : PD_Op<"frame", [NoSideEffect]> {
  let summary = "frame op";
  let description = [{
    
          Slice the N-dimensional (where N >= 1) input into (overlapping) frames.
        
  }];
  let arguments = (ins  PD_Tensor:$X, DefaultValuedAttr<SI32Attr, "-1">:$axis);

  let results = (outs PD_Tensor:$Out);
}
def PD_Group_normOp : PD_Op<"group_norm", [NoSideEffect]> {
  let summary = "group_norm op";
  let description = [{
    
    Group Normalization
    
    Refer to `Group Normalization <https://arxiv.org/abs/1803.08494>`_
    
  }];
  let arguments = (ins  PD_Tensor:$X, PD_Tensor:$Scale, PD_Tensor:$Bias, DefaultValuedAttr<F32Attr, "9.999999747378752e-06">:$epsilon, DefaultValuedAttr<StrAttr, "NCHW">:$data_layout);

  let results = (outs PD_Tensor:$Y);
}
def PD_C_softmax_with_cross_entropyOp : PD_Op<"c_softmax_with_cross_entropy", [NoSideEffect]> {
  let summary = "c_softmax_with_cross_entropy op";
  let description = [{
    
    CSoftmaxWithCrossEntropy Operator
    
    
  }];
  let arguments = (ins  PD_Tensor:$Logits, PD_Tensor:$Label, DefaultValuedAttr<SI32Attr, "0">:$ring_id, DefaultValuedAttr<SI32Attr, "0">:$rank, DefaultValuedAttr<SI32Attr, "0">:$nranks);

  let results = (outs PD_Tensor:$Softmax,PD_Tensor:$Loss);
}
def PD_Sequence_expand_asOp : PD_Op<"sequence_expand_as", [NoSideEffect]> {
  let summary = "sequence_expand_as op";
  let description = [{
    
    Sequence Expand As Operator.
    
    This operator expands `X` according to the zeroth level lod of `Y`. Current
    implementation requires the level number of Input(Y)'s lod should be 1, and
    the first dimension of Input(X) should be equal to the size of Input(Y)'s zeroth
    level lod, and lod of Input(X) is not considered.
    
    Following are cases to better explain how this works:
    
    Case 1:
    
    Given a 1-level LoDTensor input(X)
        X.data = [[a], [b], [c], [d]]
        X.dims = [4, 1]
    and input(Y)
        Y.lod = [[0, 3, 6, 7, 8]]
    ref_level: 0
    then we get 1-level LoDTensor
        Out.lod =  [[0,            3,              6,  7,  8]]
        Out.data = [[a], [a], [a], [b], [b], [b], [c], [d]]
        Out.dims = [8, 1]
    
    Case 2:
    
    Given a common Tensor input(X)
        X.data = [[a, b], [c, d], [e, f]]
        X.dims = [3, 2]
    and input(Y)
        Y.lod = [[0, 2, 3, 6]]
    ref_level: 0
    then we get a common LoDTensor
        Out.lod =  [[0,             2,     3,                    6]]
        Out.data = [[a, b], [a, b] [c, d], [e, f], [e, f], [e, f]]
        Out.dims = [6, 2]
    
    
  }];
  let arguments = (ins  PD_Tensor:$X, PD_Tensor:$Y);

  let results = (outs PD_Tensor:$Out);
}
def PD_QrOp : PD_Op<"qr", [NoSideEffect]> {
  let summary = "qr op";
  let description = [{
    
    Qr Operator.
    
    This operator is used to perform QR operation for batched matrics $X$.
    $$Q, R = qr(X)$$
    
    
  }];
  let arguments = (ins  PD_Tensor:$X, DefaultValuedAttr<StrAttr, "reduced">:$mode);

  let results = (outs PD_Tensor:$Q,PD_Tensor:$R);
}
def PD_Layer_normOp : PD_Op<"layer_norm", [NoSideEffect]> {
  let summary = "layer_norm op";
  let description = [{
    
    Assume feature vectors exist on dimensions
    :attr:`begin_norm_axis ... rank(input)` and calculate the moment statistics
    along these dimensions for each feature vector :math:`a` with size
    :math:`H`, then normalize each feature vector using the corresponding
    statistics. After that, apply learnable gain and bias on the normalized
    tensor to scale and shift if :attr:`scale` and :attr:`shift` are set.
    
    Refer to `Layer Normalization <https://arxiv.org/pdf/1607.06450v1.pdf>`_
    
  }];
  let arguments = (ins  PD_Tensor:$X, PD_Tensor:$Scale, PD_Tensor:$Bias, DefaultValuedAttr<F32Attr, "9.999999747378752e-06">:$epsilon, DefaultValuedAttr<SI32Attr, "1">:$begin_norm_axis);

  let results = (outs PD_Tensor:$Y);
}
def PD_RnnOp : PD_Op<"rnn", [NoSideEffect]> {
  let summary = "rnn op";
  let description = [{
    
    
  }];
  let arguments = (ins  PD_Tensor:$Input, PD_Tensor:$PreState, PD_Tensor:$WeightList, PD_Tensor:$SequenceLength, DefaultValuedAttr<F32Attr, "0.0">:$dropout_prob, DefaultValuedAttr<BoolAttr, "false">:$is_bidirec, DefaultValuedAttr<SI32Attr, "10">:$input_size, DefaultValuedAttr<SI32Attr, "100">:$hidden_size, DefaultValuedAttr<SI32Attr, "1">:$num_layers, DefaultValuedAttr<SI32Attr, "0">:$seed);

  let results = (outs PD_Tensor:$DropoutState,PD_Tensor:$Out,PD_Tensor:$State);
}
def PD_Hard_sigmoidOp : PD_Op<"hard_sigmoid", [NoSideEffect]> {
  let summary = "hard_sigmoid op";
  let description = [{
    
    HardSigmoid Activation Operator.
    
    A 3-part piecewise linear approximation of sigmoid(https://arxiv.org/abs/1603.00391),
    which is much faster than sigmoid.
    
    $$out = \max(0, \min(1, slope * x + offset))$$
    
    
  }];
  let arguments = (ins  PD_Tensor:$X, DefaultValuedAttr<F32Attr, "0.20000000298023224">:$slope, DefaultValuedAttr<F32Attr, "0.5">:$offset);

  let results = (outs PD_Tensor:$Out);
}
def PD_CorrelationOp : PD_Op<"correlation", [NoSideEffect]> {
  let summary = "correlation op";
  let description = [{
    Correlation of two feature map. Only support NCHW data format.
  }];
  let arguments = (ins  PD_Tensor:$Input1, PD_Tensor:$Input2, DefaultValuedAttr<SI32Attr, "1">:$corr_type_multiply);

  let results = (outs PD_Tensor:$Output);
}
def PD_Segment_poolOp : PD_Op<"segment_pool", [NoSideEffect]> {
  let summary = "segment_pool op";
  let description = [{
    
    Segment Pool Operator.
    
    This operator will pool the elements of input `X` which with the same index
    in `SegmentIds`.
    
    For SUM operation, it computes a tensor such that $Out_i = \sum_{j} X_{j}$
    where sum is over j such that `SegmentIds[j] == i`.
    
    For MEAN operation, it computes a tensor such that
    $Out_i = \frac{1}{n_i}  \sum_{j} X_{j}$ where sum is over j such that
    `SegmentIds[j] == i` and $n_i$ is the number of all index `SegmentIds[j] == i`.
    
    For MIN operation, it computes a tensor such that $Out_i = \min_{j} X_{j}$
    where min is over j such that `SegmentIds[j] == i`.
    
    For MAX operation, it computes a tensor such that $Out_i = \max_{j} X_{j}$
    where max is over j such that `SegmentIds[j] == i`.
        
  }];
  let arguments = (ins  PD_Tensor:$X, PD_Tensor:$SegmentIds, DefaultValuedAttr<StrAttr, "SUM">:$pooltype);

  let results = (outs PD_Tensor:$Out);
}
def PD_Filter_by_instagOp : PD_Op<"filter_by_instag", [NoSideEffect]> {
  let summary = "filter_by_instag op";
  let description = [{
    
    Filter By Instag Op 
    
    This operator is used to filter embeded ins.
    
    There are 3 inputs. First is embeded ins, Second is tags for ins, 
    Third is tags to filter.
    
    There are 3 outputs. First is filtered embeded ins, Second is Loss Weight,
    Third is the IndexMap from Out line number to X1 line number. 
    
  }];
  let arguments = (ins  PD_Tensor:$Ins, PD_Tensor:$Ins_tag, PD_Tensor:$Filter_tag, DefaultValuedAttr<SI64Attr, "0">:$out_val_if_empty);

  let results = (outs PD_Tensor:$Out,PD_Tensor:$LossWeight,PD_Tensor:$IndexMap);
}
def PD_Expand_as_v2Op : PD_Op<"expand_as_v2", [NoSideEffect]> {
  let summary = "expand_as_v2 op";
  let description = [{
    
    Expand the input to the given shape.
    
  }];
  let arguments = (ins  PD_Tensor:$X, DefaultValuedAttr<I32ArrayAttr, "{}">:$target_shape);

  let results = (outs PD_Tensor:$Out);
}
def PD_Nll_lossOp : PD_Op<"nll_loss", [NoSideEffect]> {
  let summary = "nll_loss op";
  let description = [{
    
    NLL(Negative Log Likelihood) Loss Operator.
    
    This operator computes the NLL loss according to the inputs.
    The loss can be described as:
    
    $Out[i] = -X[Label[i]]*Weight[Label[i]]$
    
    It can also be used for higher dimension inputs, such as 2D images, by 
    providing an input of shape (batch_size, C, d1, d2, ..., dK), with 
    K >= 1, where K is the number of dimensions, and a Label of 
    appropriate shape. In the case of images, it computes NLL loss 
    per-pixel.
    
    
  }];
  let arguments = (ins  PD_Tensor:$X, PD_Tensor:$Label, PD_Tensor:$Weight, DefaultValuedAttr<SI64Attr, "-100">:$ignore_index, DefaultValuedAttr<StrAttr, "mean">:$reduction);

  let results = (outs PD_Tensor:$Out,PD_Tensor:$Total_weight);
}
def PD_DotOp : PD_Op<"dot", [NoSideEffect]> {
  let summary = "dot op";
  let description = [{
    
  }];
  let arguments = (ins  PD_Tensor:$X, PD_Tensor:$Y);

  let results = (outs PD_Tensor:$Out);
}
def PD_Shuffle_batchOp : PD_Op<"shuffle_batch", [NoSideEffect]> {
  let summary = "shuffle_batch op";
  let description = [{
    
    Shuffle Batch Operator.
    
    This operator is used to shuffle input $X$'s elements.
    
    There is 2 input. The product of input dims (except last dim) numbers of elements will be shuffled. $Seed$ is tensor of seed.
    
    There are 3 outputs. $Out$ is shuffled tensor of input. $ShuffleIdx$ is the tensor used to record shuffle order. $SeedOut$ is same tensor of $Seed$.
    
  }];
  let arguments = (ins  PD_Tensor:$X, PD_Tensor:$Seed, DefaultValuedAttr<SI32Attr, "0">:$startup_seed);

  let results = (outs PD_Tensor:$Out,PD_Tensor:$ShuffleIdx,PD_Tensor:$SeedOut);
}
def PD_MultiplexOp : PD_Op<"multiplex", [NoSideEffect]> {
  let summary = "multiplex op";
  let description = [{
    
    Referring to the given index variable, this layer selects rows from the
    input variables to construct a multiplex variable. Assuming that there are
    :math:`m` input variables and :math:`I_i` represents the i-th input
    variable and :math:`i` is in [0, :math:`m`). All input variables are
    tensors with same shape [:math:`d_0`, :math:`d_1`, ..., :math:`d_R`].
    Please note that rank of the input tensor should be at least 2. Each input
    variable will be treated as a 2-D matrix with shape [:math:`M`, :math:`N`]
    where :math:`M` for :math:`d_0` and :math:`N` for :math:`d_1` * :math:`d_2`
    * ... * :math:`d_R`. Let :math:`I_i[j]` be the j-th row of the i-th input
    variable. The given index variable should be a 2-D tensor with shape
    [:math:`M`, 1]. Let `ID[i]` be the i-th index value of the index variable.
    Then the output variable will be a tensor with shape [:math:`d_0`,
    :math:`d_1`, ..., :math:`d_R`]. If we treat the output tensor as a 2-D
    matrix with shape [:math:`M`, :math:`N`] and let :math:`O[i]` be the i-th
    row of the matrix, then `O[i]` is equal to :math:`I_{ID[i]}[i]`.
    
    * Ids: the index tensor.
    
    * X[0 : N - 1]: the candidate tensors for output (N >= 2).
    
    * For each index i from 0 to batchSize - 1, the output is the i-th row of the
    the (Ids[i])-th tensor.
    
    For i-th row of the output tensor:
    
    $$
    y[i] = x_{k}[i]
    $$
    
    where $y$ is the output tensor, $x_{k}$ is the k-th input tensor,
    and $k = Ids[i]$.
    
    
  }];
  let arguments = (ins  PD_Tensor:$Ids, PD_Tensor:$X);

  let results = (outs PD_Tensor:$Out);
}
def PD_Leaky_reluOp : PD_Op<"leaky_relu", [NoSideEffect]> {
  let summary = "leaky_relu op";
  let description = [{
    
    LeakyRelu Activation Operator.
    
    $$out = \max(x, \alpha * x)$$
    
    
  }];
  let arguments = (ins  PD_Tensor:$X, DefaultValuedAttr<F32Attr, "0.019999999552965164">:$alpha);

  let results = (outs PD_Tensor:$Out);
}
def PD_Elementwise_powOp : PD_Op<"elementwise_pow", [NoSideEffect]> {
  let summary = "elementwise_pow op";
  let description = [{
    
    Elementwise Pow Operator.
    
    First tensor elements raised to powers from the second tensor, element-wise.
    
    The equation is:
    
    $$Out = X ^ Y$$
    
    - $X$: a tensor of any dimension.
    - $Y$: a tensor whose dimensions must be less than or equal to the dimensions of $X$.
    
    There are two cases for this operator:
    
    1. The shape of $Y$ is the same with $X$.
    2. The shape of $Y$ is a continuous subsequence of $X$.
    
    For case 2:
    
    1. Broadcast $Y$ to match the shape of $X$, where $axis$ is the start dimension index
       for broadcasting $Y$ onto $X$.
    2. If $axis$ is -1 (default), $axis = rank(X) - rank(Y)$.
    3. The trailing dimensions of size 1 for $Y$ will be ignored for the consideration of
       subsequence, such as shape(Y) = (2, 1) => (2).
    
    For example:
    
      .. code-block:: text
    
        shape(X) = (2, 3, 4, 5), shape(Y) = (,)
        shape(X) = (2, 3, 4, 5), shape(Y) = (5,)
        shape(X) = (2, 3, 4, 5), shape(Y) = (4, 5), with axis=-1(default) or axis=2
        shape(X) = (2, 3, 4, 5), shape(Y) = (3, 4), with axis=1
        shape(X) = (2, 3, 4, 5), shape(Y) = (2), with axis=0
        shape(X) = (2, 3, 4, 5), shape(Y) = (2, 1), with axis=0
    
    
  }];
  let arguments = (ins  PD_Tensor:$X, PD_Tensor:$Y, DefaultValuedAttr<SI32Attr, "-1">:$axis);

  let results = (outs PD_Tensor:$Out);
}
def PD_Shrink_rnn_memoryOp : PD_Op<"shrink_rnn_memory", [NoSideEffect]> {
  let summary = "shrink_rnn_memory op";
  let description = [{
    
    This operator is used to shrink output batch of memory defined in dynamic RNN.
    
    Dynamic RNN is able to handle variable-length sequences, in which, sequences in
    a mini-batch are sorted by their lengths first. After that, the longest sequence
    becomes the first one in the sorted batch, followed by the second longest, the
    third longest, and so on. Dynamic RNN then slices a batch input timestep by
    timestep from the sorted input. Once any sequence in the input batch reaches its
    end, memory defined in dynamicRNN has to shrink its outputs to adapt to the input
    batch size for the next time step.
    
  }];
  let arguments = (ins  PD_Tensor:$X, PD_Tensor:$RankTable, PD_Tensor:$I);

  let results = (outs PD_Tensor:$Out);
}
def PD_P_normOp : PD_Op<"p_norm", [NoSideEffect]> {
  let summary = "p_norm op";
  let description = [{
    
    Pnorm Operator.
    Given a tensor X, compute Lp-norm of X.
    
    When p = 0, defining $0^0 = 0$, the zero-norm of X is simply the number of non-zero elements of X.
    $$
    ||X||_{0} = \lim_{p \rightarrow 0} \sum_i |x_i|^p
    $$
    
    When p = inf, the inf-norm of X is the maximum element of X.
    $$
    ||X||_\infty = \max_i |x_i|
    $$
    
    When p = -inf, the negative-inf-norm of X is the minimum element of X.
    $$
    ||X||_{-\infty} = \min_i |x_i|
    $$
    
    Otherwise, the p-norm of X follows the formula,
    $$
    ||X||_{p} = (\sum_i |x_i|^p)^{1/p}
    $$
    where, $\sum_i $ is calculated along the `axis` dimension.
    
    
  }];
  let arguments = (ins  PD_Tensor:$X, DefaultValuedAttr<F32Attr, "2.0">:$porder, DefaultValuedAttr<SI32Attr, "-1">:$axis, DefaultValuedAttr<F32Attr, "9.999999960041972e-13">:$epsilon, DefaultValuedAttr<BoolAttr, "false">:$keepdim, DefaultValuedAttr<BoolAttr, "false">:$asvector);

  let results = (outs PD_Tensor:$Out);
}
def PD_Lod_resetOp : PD_Op<"lod_reset", [NoSideEffect]> {
  let summary = "lod_reset op";
  let description = [{
    LoDReset operator
    
    Set LoD of `X` to a new one specified by `Y` or attribute `target_lod`. When `Y`
    provided and `Y` is a LoDTensor, `Y.lod` would be considered as target LoD
    first, otherwise `Y.data` would be considered as target LoD. If `Y` is not
    provided, target LoD should be specified by attribute `target_lod`.
    If target LoD is specified by `Y.data` or `target_lod`, only one level LoD
    is supported.
    
    Example 1:
    
    Given a 1-level LoDTensor input(X):
        X.lod =  [[ 0,     2,                   5      6 ]]
        X.data = [[1.0], [2.0], [3.0], [4.0], [5.0], [6.0]]
        X.dims = [6, 1]
    
    attr(target_lod): [0, 4, 6]
    
    then we get a 1-level LoDTensor:
        Out.lod =  [[ 0,                   4,            6 ]]
        Out.data = [[1.0], [2.0], [3.0], [4.0], [5.0], [6.0]]
        Out.dims = [6, 1]
    
    Example 2:
    
    Given a 1-level LoDTensor input(X):
        X.lod =  [[ 0,     2,                   5      6 ]]
        X.data = [[1.0], [2.0], [3.0], [4.0], [5.0], [6.0]]
        X.dims = [6, 1]
    
    input(Y) is a Tensor:
        Y.data = [[0, 2, 6]]
        Y.dims = [1, 3]
    
    then we get a 1-level LoDTensor:
        Out.lod =  [[ 0,     2,                          6 ]]
        Out.data = [[1.0], [2.0], [3.0], [4.0], [5.0], [6.0]]
        Out.dims = [6, 1]
    
    Example 3:
    
    Given a 1-level LoDTensor input(X):
        X.lod =  [[ 0,      2,                   5     6 ]]
        X.data = [[1.0], [2.0], [3.0], [4.0], [5.0], [6.0]]
        X.dims = [6, 1]
    
    input(Y) is a 2-level LoDTensor:
        Y.lod =  [[0, 2, 4], [0, 2, 5, 6]]
        Y.data = [[1.1], [2.1], [3.1], [4.1], [5.1], [6.1]]
        Y.dims = [6, 1]
    
    then we get a 2-level LoDTensor:
        Out.lod =  [[0, 2, 4], [0, 2, 5, 6]]
        Out.data = [[1.0], [2.0], [3.0], [4.0], [5.0], [6.0]]
        Out.dims = [6, 1]
    
    
  }];
  let arguments = (ins  PD_Tensor:$X, PD_Tensor:$Y, DefaultValuedAttr<I32ArrayAttr, "{}">:$target_lod, DefaultValuedAttr<BoolAttr, "false">:$append);

  let results = (outs PD_Tensor:$Out);
}
def PD_PadOp : PD_Op<"pad", [NoSideEffect]> {
  let summary = "pad op";
  let description = [{
    
    Pad Operator.
    
    Pad input into output, as specified by paddings and pad_value. 
    The input should be a k-D tensor(k > 0 and k < 7). As an example:
    
    Given:
    
    X = [[1, 2],
         [3, 4]],
    
    paddings = [0, 1, 1, 2],
    
    and
    
    pad_value = 0,
    
    we have:
    
    Out = [[0, 1, 2, 0, 0]
           [0, 3, 4, 0, 0]
           [0, 0, 0, 0, 0]]
    
    
  }];
  let arguments = (ins  PD_Tensor:$X,I32ArrayAttr:$paddings, DefaultValuedAttr<F32Attr, "0.0">:$pad_value);

  let results = (outs PD_Tensor:$Out);
}
def PD_Sequence_convOp : PD_Op<"sequence_conv", [NoSideEffect]> {
  let summary = "sequence_conv op";
  let description = [{
    
    Sequence Conv Operator.
    
    SequenceConvOp performs convolution operation on features of contextLength
    time-steps of each instance. The convolution operation calculates the output
    based on the input, filter, strides and paddings parameters.
    The size of each dimension of the parameters is checked during infer-shape.
    In order to ensure the equal length of sequence before and after convolution,
    it is necessary to fill the top and bottom of each sequence based on
    context_length, context_stride and context_start.
    
        
  }];
  let arguments = (ins  PD_Tensor:$X, PD_Tensor:$PaddingData, PD_Tensor:$Filter, DefaultValuedAttr<BoolAttr, "false">:$paddingTrainable, DefaultValuedAttr<SI32Attr, "0">:$contextStart, DefaultValuedAttr<SI32Attr, "1">:$contextStride);

  let results = (outs PD_Tensor:$Out);
}
def PD_Log10Op : PD_Op<"log10", [NoSideEffect]> {
  let summary = "log10 op";
  let description = [{
    
    Log10 Activation Operator.
    
    $$out = \log_10_x$$
    
    logarithm of x base to 10.
    
    
  }];
  let arguments = (ins  PD_Tensor:$X);

  let results = (outs PD_Tensor:$Out);
}
def PD_Center_lossOp : PD_Op<"center_loss", [NoSideEffect]> {
  let summary = "center_loss op";
  let description = [{
    
    **CenterLoss operator**
    implemention of the center loss function in the papper<<A Discriminative 
    Feature Learning Approach for Deep Face Recognition>>, equations in this  implement
    is:loss = 1/2 * (x-y)^2 ,where x(X) means the deep feature(output of last hidden layer )
    and y(Label) the target label 
    
  }];
  let arguments = (ins  PD_Tensor:$X, PD_Tensor:$Label, PD_Tensor:$Centers, PD_Tensor:$CenterUpdateRate);

  let results = (outs PD_Tensor:$CentersOut,PD_Tensor:$SampleCenterDiff,PD_Tensor:$Loss);
}
def PD_SliceOp : PD_Op<"slice", [NoSideEffect]> {
  let summary = "slice op";
  let description = [{
    
    Slice Operator.
    
    Produces a slice of the input tensor along multiple axes. Similar to numpy:
    https://docs.scipy.org/doc/numpy/reference/arrays.indexing.html
    Slice uses `axes`, `starts` and `ends` attributes to specify the start and
    end dimension for each axis in the list of axes, it uses this information
    to slice the input data tensor. If a negative value is passed for any of
    the start or end indices, it represents number of elements before the end
    of that dimension. If the value passed to start or end is larger than
    the n (the number of elements in this dimension), it represents n.
    For slicing to the end of a dimension with unknown size, it is recommended
    to pass in INT_MAX. The size of axes must be equal to starts\' and ends\'.
    Following examples will explain how slice works:
    
    .. code-block:: text
    
        Case1:
            Given:
                data = [ [1, 2, 3, 4], [5, 6, 7, 8], ]
                axes = [0, 1]
                starts = [1, 0]
                ends = [2, 3]
            Then:
                result = [ [5, 6, 7], ]
    
        Case2:
            Given:
                data = [ [1, 2, 3, 4], [5, 6, 7, 8], ]
                starts = [0, 1]
                ends = [-1, 1000]
            Then:
                result = [ [2, 3, 4], ]
    
  }];
  let arguments = (ins  PD_Tensor:$Input, PD_Tensor:$StartsTensor, PD_Tensor:$EndsTensor, PD_Tensor:$StartsTensorList, PD_Tensor:$EndsTensorList,I32ArrayAttr:$axes, DefaultValuedAttr<I32ArrayAttr, "{}">:$starts, DefaultValuedAttr<I32ArrayAttr, "{}">:$ends, DefaultValuedAttr<I32ArrayAttr, "{}">:$infer_flags, DefaultValuedAttr<I32ArrayAttr, "{}">:$decrease_axis);

  let results = (outs PD_Tensor:$Out);
}
def PD_MeshgridOp : PD_Op<"meshgrid", [NoSideEffect]> {
  let summary = "meshgrid op";
  let description = [{
    
    Meshgrid Operator.
    Take: N tensors, each of which can be either scalr or 1-dimensional vector, and create
    N-dimensional grids.
    
    Args:
      tensors (list of tensor): if the input k tensors has (N1,), (N2,),..., (Nk,), then 
      the output tensors are all of size (N1, N2, ...., Nk).
    
    Example::
    >>> x = fluid.data(name='x', shape=[10], dtype='float64')
    >>> y = fluid.data(name='y', shape=[20], dtype='float64')
    >>> grid_x, grid_y = fluid.layers.meshgrid([x, y])
    >>> grid_x.shape
    (10,20)
    >>> grid_y.shape
    (10,20)
    
  }];
  let arguments = (ins  PD_Tensor:$X);

  let results = (outs PD_Tensor:$Out);
}
def PD_Hard_swishOp : PD_Op<"hard_swish", [NoSideEffect]> {
  let summary = "hard_swish op";
  let description = [{
    
    HardSwish Activation Operator.
    
    The hard version of swish(https://arxiv.org/pdf/1905.02244.pdf).
    
    $$out = \frac{x * (min(max(0, x+offset), threshold))}{scale}$$
    
    The threshold and scale should be positive. The offset can be either positive or negative.
    The default parameters are set according to the above reference.
    It is recommended to use the defaults for this activation.
    
    
  }];
  let arguments = (ins  PD_Tensor:$X, DefaultValuedAttr<F32Attr, "6.0">:$threshold, DefaultValuedAttr<F32Attr, "6.0">:$scale, DefaultValuedAttr<F32Attr, "3.0">:$offset);

  let results = (outs PD_Tensor:$Out);
}
def PD_SinOp : PD_Op<"sin", [NoSideEffect]> {
  let summary = "sin op";
  let description = [{
    
    Sine Activation Operator.
    
    $$out = sin(x)$$
    
    
  }];
  let arguments = (ins  PD_Tensor:$X);

  let results = (outs PD_Tensor:$Out);
}
def PD_Pad2dOp : PD_Op<"pad2d", [NoSideEffect]> {
  let summary = "pad2d op";
  let description = [{
    
    Pad2d Operator.
    Pad 2-d images according to 'paddings' and 'mode'. 
    If mode is 'reflect', paddings[0] and paddings[1] must be no greater
    than height-1. And the width dimension has the same condition.
    
    Given that X is a channel of image from input:
    
    X = [[1, 2, 3],
         [4, 5, 6]]
    
    Case 0:
    
    paddings = [0, 1, 2, 3],
    mode = 'constant'
    pad_value = 0
    
    Out = [[0, 0, 1, 2, 3, 0, 0, 0]
           [0, 0, 4, 5, 6, 0, 0, 0]
           [0, 0, 0, 0, 0, 0, 0, 0]]
    
    Case 1:
    
    paddings = [0, 1, 2, 1],
    mode = 'reflect'
    
    Out = [[3, 2, 1, 2, 3, 2]
           [6, 5, 4, 5, 6, 5]
           [3, 2, 1, 2, 3, 2]]
    
    Case 2:
    
    paddings = [0, 1, 2, 1],
    mode = 'edge'
    
    Out = [[1, 1, 1, 2, 3, 3]
           [4, 4, 4, 5, 6, 6]
           [4, 4, 4, 5, 6, 6]]
    
  }];
  let arguments = (ins  PD_Tensor:$X, PD_Tensor:$Paddings,I32ArrayAttr:$paddings, DefaultValuedAttr<F32Attr, "0.0">:$pad_value, DefaultValuedAttr<StrAttr, "constant">:$mode, DefaultValuedAttr<StrAttr, "NCHW">:$data_format);

  let results = (outs PD_Tensor:$Out);
}
def PD_InverseOp : PD_Op<"inverse", [NoSideEffect]> {
  let summary = "inverse op";
  let description = [{
    
    Inverse Operator
    
    Takes the inverse of the square matrix.
    
  }];
  let arguments = (ins  PD_Tensor:$Input);

  let results = (outs PD_Tensor:$Output);
}
def PD_Spectral_normOp : PD_Op<"spectral_norm", [NoSideEffect]> {
  let summary = "spectral_norm op";
  let description = [{
    
              This layer calculates the spectral normalization value of weight of
              fc, conv1d, conv2d, conv3d layers which should be 2-D, 3-D, 4-D, 5-D
              tensor.
    
              Spectral normalization stabilizes the training of critic in GANs
              (Generative Adversarial Networks). This layer rescaling weight tensor
              with spectral normalize value.
    
              For spectral normalization calculations, we rescaling weight
              tensor with :math:`\sigma`, while :math:`\sigma{\mathbf{W}}` is
    
                $$\sigma(\mathbf{W}) = \max_{\mathbf{h}: \mathbf{h} \ne 0} \\frac{\|\mathbf{W} \mathbf{h}\|_2}{\|\mathbf{h}\|_2}$$
    
              We calculate :math:`\sigma{\mathbf{W}}` through power iterations as
    
                $$
                \mathbf{v} = \mathbf{W}^{T} \mathbf{u}
                $$
                $$
                \mathbf{v} = \\frac{\mathbf{v}}{\|\mathbf{v}\|_2}
                $$
                $$
                \mathbf{u} = \mathbf{W}^{T} \mathbf{v}
                $$
                $$
                \mathbf{u} = \\frac{\mathbf{u}}{\|\mathbf{u}\|_2}
                $$
    
              And :math:`\sigma` should be
    
                $$\sigma{\mathbf{W}} = \mathbf{u}^{T} \mathbf{W} \mathbf{v}$$
    
              For details of spectral normalization, please refer to paper: 
              `Spectral Normalization <https://arxiv.org/abs/1802.05957>`_ .
             
  }];
  let arguments = (ins  PD_Tensor:$Weight, PD_Tensor:$U, PD_Tensor:$V, DefaultValuedAttr<SI32Attr, "0">:$dim, DefaultValuedAttr<SI32Attr, "1">:$power_iters, DefaultValuedAttr<F32Attr, "9.999999960041972e-13">:$eps);

  let results = (outs PD_Tensor:$Out);
}
def PD_Shuffle_channelOp : PD_Op<"shuffle_channel", [NoSideEffect]> {
  let summary = "shuffle_channel op";
  let description = [{
    
    		Shuffle Channel operator
    		This opearator shuffles the channels of input x.
    		It  divide the input channels in each group into several subgroups,
    		and obtain a new order by selecting element from every subgroup one by one.
    
    		Shuffle channel operation makes it possible to build more powerful structures
    		with multiple group convolutional layers.
    		please get more information from the following paper:
    		https://arxiv.org/pdf/1707.01083.pdf
            
  }];
  let arguments = (ins  PD_Tensor:$X, DefaultValuedAttr<SI32Attr, "1">:$group);

  let results = (outs PD_Tensor:$Out);
}
def PD_Psroi_poolOp : PD_Op<"psroi_pool", [NoSideEffect]> {
  let summary = "psroi_pool op";
  let description = [{
    
    Position sensitive region of interest pooling (also known as PSROIPooling) is to perform
    position-sensitive average pooling on regions of interest specified by input, takes as 
    input N position-sensitive score maps and a list of num_rois regions of interest. 
    
    PSROIPooling for R-FCN. Please refer to https://arxiv.org/abs/1605.06409 for more details.
        
  }];
  let arguments = (ins  PD_Tensor:$X, PD_Tensor:$ROIs, PD_Tensor:$RoisNum, DefaultValuedAttr<F32Attr, "1.0">:$spatial_scale, DefaultValuedAttr<SI32Attr, "1">:$pooled_height, DefaultValuedAttr<SI32Attr, "1">:$pooled_width);

  let results = (outs PD_Tensor:$Out);
}
def PD_CeilOp : PD_Op<"ceil", [NoSideEffect]> {
  let summary = "ceil op";
  let description = [{
    
    Ceil Operator. Computes ceil of x element-wise.
    
    $$out = \\lceil x \\rceil$$
    
    
  }];
  let arguments = (ins  PD_Tensor:$X);

  let results = (outs PD_Tensor:$Out);
}
def PD_EigOp : PD_Op<"eig", [NoSideEffect]> {
  let summary = "eig op";
  let description = [{
    
            Eig Operator.
    
    This API processes eigen decomposition for general square matrices.
    
    
  }];
  let arguments = (ins  PD_Tensor:$X);

  let results = (outs PD_Tensor:$Eigenvalues,PD_Tensor:$Eigenvectors);
}
def PD_Reduce_minOp : PD_Op<"reduce_min", [NoSideEffect]> {
  let summary = "reduce_min op";
  let description = [{
    
    Reduce reduce_min Operator.
    
    This operator computes the reduce_min of input tensor along the given dimension.
    The result tensor has 1 fewer dimension than the input unless keep_dim is true.
    If reduce_all is true, just reduce along all dimensions and output a scalar.
    
    
  }];
  let arguments = (ins  PD_Tensor:$X, DefaultValuedAttr<I32ArrayAttr, "{0}">:$dim, DefaultValuedAttr<BoolAttr, "false">:$keep_dim, DefaultValuedAttr<BoolAttr, "false">:$reduce_all, DefaultValuedAttr<SI32Attr, "-1">:$in_dtype, DefaultValuedAttr<SI32Attr, "-1">:$out_dtype);

  let results = (outs PD_Tensor:$Out);
}
def PD_CosOp : PD_Op<"cos", [NoSideEffect]> {
  let summary = "cos op";
  let description = [{
    
    Cosine Operator. Computes cosine of x element-wise.
    
    Input range is `(-inf, inf)` and output range is `[-1,1]`.
    
    $$out = cos(x)$$
    
    
  }];
  let arguments = (ins  PD_Tensor:$X);

  let results = (outs PD_Tensor:$Out);
}
def PD_Cudnn_lstmOp : PD_Op<"cudnn_lstm", [NoSideEffect]> {
  let summary = "cudnn_lstm op";
  let description = [{
    
    CUDNN LSTM implementation
    
    A four-gate Long Short-Term Memory network with no peephole connections.
    In the forward pass the output ht and cell output ct for a given iteration can be computed from the recurrent input ht-1, 
    the cell input ct-1 and the previous layer input xt given matrices W, R and biases bW, bR from the following equations:
    
    $$ i_t = sigmoid(W_{ix}x_{t} + W_{ih}h_{t-1} + bx_i + bh_i) $$
    
    $$ f_t = sigmoid(W_{fx}x_{t} + W_{fh}h_{t-1} + bx_f + bh_f) $$
    
    $$ o_t = sigmoid(W_{ox}x_{t} + W_{oh}h_{t-1} + bx_o + bh_o) $$
    
    $$ \\tilde{c_t} = tanh(W_{cx}x_t + W_{ch}h_{t-1} + bx_c + bh_c) $$
    
    $$ c_t = f_t \\odot c_{t-1} + i_t \\odot \\tilde{c_t} $$
    
    $$ h_t = o_t \\odot tanh(c_t) $$
    
    - W terms denote weight matrices (e.g. $W_{ix}$ is the matrix
      of weights from the input gate to the input)
    - The b terms denote bias vectors ($bx_i$ and $bh_i$ are the input gate bias vector).
    - sigmoid is the logistic sigmoid function.
    - $i, f, o$ and $c$ are the input gate, forget gate, output gate,
      and cell activation vectors, respectively, all of which have the same size as
      the cell output activation vector $h$.
    - The $\odot$ is the element-wise product of the vectors.
    - `tanh` is the activation functions.
    - $\tilde{c_t}$ is also called candidate hidden state,
      which is computed based on the current input and the previous hidden state.
    
    Where sigmoid is the sigmoid operator: sigmoid(x) = 1 / (1 + e^-x), * represents a point-wise multiplication, 
    X represensts a matrix multiplication
    
    
    
  }];
  let arguments = (ins  PD_Tensor:$Input, PD_Tensor:$InitH, PD_Tensor:$InitC, PD_Tensor:$W, PD_Tensor:$WeightList, PD_Tensor:$SequenceLength, DefaultValuedAttr<F32Attr, "0.0">:$dropout_prob, DefaultValuedAttr<BoolAttr, "false">:$is_bidirec, DefaultValuedAttr<SI32Attr, "10">:$input_size, DefaultValuedAttr<SI32Attr, "100">:$hidden_size, DefaultValuedAttr<SI32Attr, "1">:$num_layers, DefaultValuedAttr<SI32Attr, "0">:$seed);

  let results = (outs PD_Tensor:$StateOut,PD_Tensor:$Out,PD_Tensor:$LastH,PD_Tensor:$LastC);
}
def PD_Reduce_sumOp : PD_Op<"reduce_sum", [NoSideEffect]> {
  let summary = "reduce_sum op";
  let description = [{
    
    Reduce reduce_sum Operator.
    
    This operator computes the reduce_sum of input tensor along the given dimension.
    The result tensor has 1 fewer dimension than the input unless keep_dim is true.
    If reduce_all is true, just reduce along all dimensions and output a scalar.
    
    
  }];
  let arguments = (ins  PD_Tensor:$X, DefaultValuedAttr<I32ArrayAttr, "{0}">:$dim, DefaultValuedAttr<BoolAttr, "false">:$keep_dim, DefaultValuedAttr<BoolAttr, "false">:$reduce_all, DefaultValuedAttr<SI32Attr, "-1">:$in_dtype, DefaultValuedAttr<SI32Attr, "-1">:$out_dtype);

  let results = (outs PD_Tensor:$Out);
}
def PD_DigammaOp : PD_Op<"digamma", [NoSideEffect]> {
  let summary = "digamma op";
  let description = [{
    
    Digamma Operator.
    
    This operator is used to perform elementwise digamma for input $X$.
    $$out = \Psi(x) = \frac{ \Gamma^{'}(x) }{ \Gamma(x) }$$
    
    
  }];
  let arguments = (ins  PD_Tensor:$X);

  let results = (outs PD_Tensor:$Out);
}
def PD_Fused_softmax_maskOp : PD_Op<"fused_softmax_mask", [NoSideEffect]> {
  let summary = "fused_softmax_mask op";
  let description = [{
    
    Softmax Mask Fuse Operator.
    In general, the compute pass is:
    product = matmul(QK)/sqrt(dk)
    pre_softmax = product + attn_mask
    output = softmax(pre_softmax)
    To reduce the launch op time and reduce the number of forward and backward,
    and to reduce the memory cost for the pre_softmax var during the compute
    this op fuse last two operations into one, so users can simply call
    product = matmul(QK)/sqrt(dk)
    output = softmax_mask_fuse(product, attn_mask)
    to get the final output.
    By doing this fusion, we can optimize the training by
    1. saving one launch cost, one forward and one backward cost
    2. saving the memory cost used to save the tmp var
    
  }];
  let arguments = (ins  PD_Tensor:$X, PD_Tensor:$Mask);

  let results = (outs PD_Tensor:$Out);
}
def PD_EigvalshOp : PD_Op<"eigvalsh", [NoSideEffect]> {
  let summary = "eigvalsh op";
  let description = [{
    
    Eigvalsh Operator.
    
    Computes the eigenvalues of a complex Hermitian
     (conjugate symmetric) or a real symmetric matrix.
    
    
  }];
  let arguments = (ins  PD_Tensor:$X, DefaultValuedAttr<StrAttr, "L">:$UPLO);

  let results = (outs PD_Tensor:$Eigenvalues,PD_Tensor:$Eigenvectors);
}
def PD_DiagonalOp : PD_Op<"diagonal", [NoSideEffect]> {
  let summary = "diagonal op";
  let description = [{
    
    Diagonal Operator.
    Return a partial view of input with the its diagonal elements of the input tensor.
    The behavior of this operator is similar to how `numpy.diagonal` works.
    
    
  }];
  let arguments = (ins  PD_Tensor:$Input, DefaultValuedAttr<SI32Attr, "0">:$offset, DefaultValuedAttr<SI32Attr, "0">:$axis1, DefaultValuedAttr<SI32Attr, "1">:$axis2);

  let results = (outs PD_Tensor:$Out);
}
def PD_TruncOp : PD_Op<"trunc", [NoSideEffect]> {
  let summary = "trunc op";
  let description = [{
    
    Trunc Operator.
    Returns a new tensor with the truncated integer values  of input.
    $$out = trunc(x)$$
    
  }];
  let arguments = (ins  PD_Tensor:$X);

  let results = (outs PD_Tensor:$Out);
}
def PD_Log2Op : PD_Op<"log2", [NoSideEffect]> {
  let summary = "log2 op";
  let description = [{
    
    Log2 Activation Operator.
    
    $$out = \log_2x$$
    
    logarithm of x base to 2.
    
    
  }];
  let arguments = (ins  PD_Tensor:$X);

  let results = (outs PD_Tensor:$Out);
}
def PD_TanhOp : PD_Op<"tanh", [NoSideEffect]> {
  let summary = "tanh op";
  let description = [{
    
    Tanh Activation Operator.
    
    $$out = \\frac{e^{x} - e^{-x}}{e^{x} + e^{-x}}$$
    
    
  }];
  let arguments = (ins  PD_Tensor:$X);

  let results = (outs PD_Tensor:$Out);
}
def PD_Yolov3_lossOp : PD_Op<"yolov3_loss", [NoSideEffect]> {
  let summary = "yolov3_loss op";
  let description = [{
    
             This operator generates yolov3 loss based on given predict result and ground
             truth boxes.
             
             The output of previous network is in shape [N, C, H, W], while H and W
             should be the same, H and W specify the grid size, each grid point predict 
             given number bounding boxes, this given number, which following will be represented as S,
             is specified by the number of anchor clusters in each scale. In the second dimension(the channel
             dimension), C should be equal to S * (class_num + 5), class_num is the object 
             category number of source dataset(such as 80 in coco dataset), so in the 
             second(channel) dimension, apart from 4 box location coordinates x, y, w, h, 
             also includes confidence score of the box and class one-hot key of each anchor box.
    
             Assume the 4 location coordinates are :math:`t_x, t_y, t_w, t_h`, the box predictions
             should be as follows:
    
             $$
             b_x = \\sigma(t_x) + c_x
             $$
             $$
             b_y = \\sigma(t_y) + c_y
             $$
             $$
             b_w = p_w e^{t_w}
             $$
             $$
             b_h = p_h e^{t_h}
             $$
    
             In the equation above, :math:`c_x, c_y` is the left top corner of current grid
             and :math:`p_w, p_h` is specified by anchors.
    
             As for confidence score, it is the logistic regression value of IoU between
             anchor boxes and ground truth boxes, the score of the anchor box which has 
             the max IoU should be 1, and if the anchor box has IoU bigger than ignore 
             thresh, the confidence score loss of this anchor box will be ignored.
    
             Therefore, the yolov3 loss consists of three major parts: box location loss,
             objectness loss and classification loss. The L1 loss is used for 
             box coordinates (w, h), sigmoid cross entropy loss is used for box 
             coordinates (x, y), objectness loss and classification loss.
    
             Each groud truth box finds a best matching anchor box in all anchors. 
             Prediction of this anchor box will incur all three parts of losses, and
             prediction of anchor boxes with no GT box matched will only incur objectness
             loss.
    
             In order to trade off box coordinate losses between big boxes and small 
             boxes, box coordinate losses will be mutiplied by scale weight, which is
             calculated as follows.
    
             $$
             weight_{box} = 2.0 - t_w * t_h
             $$
    
             Final loss will be represented as follows.
    
             $$
             loss = (loss_{xy} + loss_{wh}) * weight_{box}
                  + loss_{conf} + loss_{class}
             $$
    
             While :attr:`use_label_smooth` is set to be :attr:`True`, the classification
             target will be smoothed when calculating classification loss, target of 
             positive samples will be smoothed to :math:`1.0 - 1.0 / class\_num` and target of
             negetive samples will be smoothed to :math:`1.0 / class\_num`.
    
             While :attr:`GTScore` is given, which means the mixup score of ground truth 
             boxes, all losses incured by a ground truth box will be multiplied by its 
             mixup score.
             
  }];
  let arguments = (ins  PD_Tensor:$X, PD_Tensor:$GTBox, PD_Tensor:$GTLabel, PD_Tensor:$GTScore, DefaultValuedAttr<I32ArrayAttr, "{}">:$anchors, DefaultValuedAttr<I32ArrayAttr, "{}">:$anchor_mask, DefaultValuedAttr<SI32Attr, "32">:$downsample_ratio, DefaultValuedAttr<F32Attr, "0.699999988079071">:$ignore_thresh, DefaultValuedAttr<BoolAttr, "true">:$use_label_smooth, DefaultValuedAttr<F32Attr, "1.0">:$scale_x_y);

  let results = (outs PD_Tensor:$Loss);
}
def PD_Graph_send_recvOp : PD_Op<"graph_send_recv", [NoSideEffect]> {
  let summary = "graph_send_recv op";
  let description = [{
    
    Graph Learning Send_Recv combine operator.
    
    $Out = Recv(Send(X, Src_index), Dst_index, pool_type)$
    
    This operator is mainly used in Graph Learning domain, and the main purpose is to reduce 
    intermediate memory consumption in the process of message passing. 
    Take `x` as the input tensor, we first use `src_index` to gather corresponding data, 
    and then use `dst_index` to update the corresponding position of output tensor in different 
    pooling types, like sum, mean, max, or min.
    
    
  }];
  let arguments = (ins  PD_Tensor:$X, PD_Tensor:$Src_index, PD_Tensor:$Dst_index, DefaultValuedAttr<StrAttr, "SUM">:$pool_type);

  let results = (outs PD_Tensor:$Out);
}
def PD_AtanOp : PD_Op<"atan", [NoSideEffect]> {
  let summary = "atan op";
  let description = [{
    
    Arctangent Operator.
    
    $$out = \tan^{-1}(x)$$
    
    
  }];
  let arguments = (ins  PD_Tensor:$X);

  let results = (outs PD_Tensor:$Out);
}
def PD_UnsqueezeOp : PD_Op<"unsqueeze", [NoSideEffect]> {
  let summary = "unsqueeze op";
  let description = [{
    
        Unsqueeze Operator.
    
        Insert single-dimensional entries to the shape of a tensor.
        Takes one required argument axes, a list of dimensions that will be inserted.
        Dimension indices in axes are as seen in the output tensor.
    
        For example:
          Given a tensor such that tensor with shape [3, 4, 5],
          then Unsqueeze(tensor, axes=[0, 4]) has shape [1, 3, 4, 5, 1]
        
  }];
  let arguments = (ins  PD_Tensor:$X, PD_Tensor:$AxesTensor, PD_Tensor:$AxesTensorList, DefaultValuedAttr<I32ArrayAttr, "{}">:$axes);

  let results = (outs PD_Tensor:$Out);
}
def PD_Log_softmaxOp : PD_Op<"log_softmax", [NoSideEffect]> {
  let summary = "log_softmax op";
  let description = [{
    
    LogSoftmax Operator.
    
    
  }];
  let arguments = (ins  PD_Tensor:$X, DefaultValuedAttr<SI32Attr, "-1">:$axis);

  let results = (outs PD_Tensor:$Out);
}
def PD_Top_k_v2Op : PD_Op<"top_k_v2", [NoSideEffect]> {
  let summary = "top_k_v2 op";
  let description = [{
    
    Top K operator
    
    If the input is a vector (1d tensor), this operator finds the k largest 
    entries in the vector and outputs their values and indices as vectors. 
    Thus values[j] is the j-th largest entry in input, and its index is indices[j].
    
    For matrices, this operator computes the top k entries in each row. 
  }];
  let arguments = (ins  PD_Tensor:$X, PD_Tensor:$K, DefaultValuedAttr<SI32Attr, "1">:$k, DefaultValuedAttr<SI32Attr, "-1">:$axis, DefaultValuedAttr<BoolAttr, "true">:$largest, DefaultValuedAttr<BoolAttr, "true">:$sorted);

  let results = (outs PD_Tensor:$Out,PD_Tensor:$Indices);
}
def PD_Tanh_shrinkOp : PD_Op<"tanh_shrink", [NoSideEffect]> {
  let summary = "tanh_shrink op";
  let description = [{
    
    TanhShrink Activation Operator.
    
    $$out = x - \\frac{e^{x} - e^{-x}}{e^{x} + e^{-x}}$$
    
    
  }];
  let arguments = (ins  PD_Tensor:$X);

  let results = (outs PD_Tensor:$Out);
}
def PD_Hard_shrinkOp : PD_Op<"hard_shrink", [NoSideEffect]> {
  let summary = "hard_shrink op";
  let description = [{
    
    :strong:`HardShrink activation operator`
    
    ..  math::
        out = \begin{cases}
                x, \text{if } x > \lambda \\
                x, \text{if } x < -\lambda \\
                0,  \text{otherwise}
              \end{cases}
    
    
  }];
  let arguments = (ins  PD_Tensor:$X, DefaultValuedAttr<F32Attr, "0.5">:$threshold);

  let results = (outs PD_Tensor:$Out);
}
def PD_Sequence_unpadOp : PD_Op<"sequence_unpad", [NoSideEffect]> {
  let summary = "sequence_unpad op";
  let description = [{
    
          Sequence Unpad Operator
    
          This operator removes the padding data in the input sequences and convert 
          them into sequences with actual length as output, identitied by lod 
          information.
    
          Example:
    
          Given input tensor Input(X):
              X.data = [[ 1.0,  2.0,  3.0,  4.0,  5.0],
                        [ 6.0,  7.0,  8.0,  9.0, 10.0],
                        [11.0, 12.0, 13.0, 14.0, 15.0]], 
    `     
          in which there are 3 sequences padded to length 5, and the actual length 
          specified by Input(Length):
    
              Length.data = [2, 3, 4],
    
          after unpadding, Output(Out) will be:
    
              Out.data = [[1.0, 2.0, 6.0, 7.0, 8.0, 11.0, 12.0, 13.0, 14.0]]
              Out.lod = [[0, 2, 5, 9]]      
    
        
  }];
  let arguments = (ins  PD_Tensor:$X, PD_Tensor:$Length);

  let results = (outs PD_Tensor:$Out);
}
def PD_Fused_elemwise_add_activationOp : PD_Op<"fused_elemwise_add_activation", [NoSideEffect]> {
  let summary = "fused_elemwise_add_activation op";
  let description = [{
    
    FusedElemwiseActivation Operator.
    
    At present, FusedElemwiseActivation only supports Two kinds of compound
    operators (elementwise_op and activation_op):
    
        Z = Binary(X, Unary(Y))
        Z = Unary(Binary(X, Y))
    
    There are two cases for this operator:
    
    1. The shape of $Y$ and $X$ is the same.
    2. The shape of $Y$ is a continuous subsequence of $X$ or the shape of $X$ is a continuous subsequence of $Y$.
    
    For case 2 (assume that the shape of $Y$ is a continuous subsequence of $X$ ):
    
    1. Broadcast $Y$ to match the shape of $X$, where $axis$ is the start dimension index
       for broadcasting $Y$ onto $X$.
    2. If $axis$ is -1 (default), $axis = rank(X) - rank(Y)$.
    3. The trailing dimensions of size 1 for $Y$ will be ignored for the consideration of
       subsequence, such as shape(Y) = (2, 1) => (2).
    
    For example:
    
      .. code-block:: python
    
        shape(X) = (2, 3, 4, 5), shape(Y) = (,)
        shape(X) = (2, 3, 4, 5), shape(Y) = (5,)
        shape(X) = (2, 3, 4, 5), shape(Y) = (4, 5), with axis=-1(default) or axis=2
        shape(X) = (2, 3, 4, 5), shape(Y) = (3, 4), with axis=1
        shape(X) = (2, 3, 4, 5), shape(Y) = (2), with axis=0
        shape(X) = (2, 3, 4, 5), shape(Y) = (2, 1), with axis=0
    
    
    The inputs $X$ and $Y$ can carry the different LoD information.
    But the output only shares the LoD information with the one whose shape is the same with Out.
    The attributions of activation_op can be get from fused_elemwise_activation_op's.
    The functor_list records the functions to be fused, for example
    ["scale", "elementwise_add"].
    
    
  }];
  let arguments = (ins  PD_Tensor:$X, PD_Tensor:$Y, DefaultValuedAttr<SI32Attr, "-1">:$axis, DefaultValuedAttr<F32Attr, "0.0">:$scale, DefaultValuedAttr<BoolAttr, "false">:$save_intermediate_out,StrArrayAttr:$functor_list);

  let results = (outs PD_Tensor:$Out);
}
def PD_Frobenius_normOp : PD_Op<"frobenius_norm", [NoSideEffect]> {
  let summary = "frobenius_norm op";
  let description = [{
    
    Reduce frobenius_norm Operator.
    
    This operator computes the frobenius_norm of input tensor along the given dimension.
    The result tensor has 1 fewer dimension than the input unless keep_dim is true.
    If reduce_all is true, just reduce along all dimensions and output a scalar.
    
    
  }];
  let arguments = (ins  PD_Tensor:$X, DefaultValuedAttr<I32ArrayAttr, "{0}">:$dim, DefaultValuedAttr<BoolAttr, "false">:$keep_dim, DefaultValuedAttr<BoolAttr, "false">:$reduce_all, DefaultValuedAttr<SI32Attr, "-1">:$in_dtype, DefaultValuedAttr<SI32Attr, "-1">:$out_dtype);

  let results = (outs PD_Tensor:$Out);
}
def PD_CropOp : PD_Op<"crop", [NoSideEffect]> {
  let summary = "crop op";
  let description = [{
    
    Crop Operator.
    
    Crop input into output, as specified by offsets and shape.
    
    There are two ways to set the offsets:
    1. In runtime: Using the input 'Offsets', which is a Vairbale and can be 
                   output of other operators. This way is suitable for 
                   dynamic offsets.
    2. In network configuration: Using the attribute 'offsets', which will be 
                                 set in Python configure script. This way is 
                                 suitable for fixed offsets.
    You CANNOT use these two ways at the same time. An exception will be raised 
    if input 'Offset' is configured and meanwhile the attribute 'offsets' is 
    not empty.
    
    There are two ways to set shape:
    1. reference input: crop input X into the same shape as reference input.
                        The dimension of reference input should
                        be the same as the dimension of input X.
    2. shape list: crop input X into the shape described by a list<int>.
                   The size of shape list should be the same as
                   the dimension size of input X.
    
    The input should be a k-D tensor(k > 0 and k < 7). As an example:
    
    Case 1:
    Given
    
        X = [[0, 1, 2, 0, 0]
             [0, 3, 4, 0, 0]
             [0, 0, 0, 0, 0]],
    
    and
    
        offsets = [0, 1],
    
    and
    
        shape = [2, 2],
    
    we get:
    
        Out = [[1, 2],
               [3, 4]].
    
    
    Case 2:
    Given
    
        X = [[0, 1, 2, 5, 0]
             [0, 3, 4, 6, 0]
             [0, 0, 0, 0, 0]],
    
    and
    
        offsets = [0, 1],
    
    and
    
        Y = [[0, 0, 0]
             [0, 0, 0]],
    
    we get:
    
        Out = [[1, 2, 5],
               [3, 4, 6]].
    
  }];
  let arguments = (ins  PD_Tensor:$X, PD_Tensor:$Y, PD_Tensor:$Offsets, DefaultValuedAttr<I32ArrayAttr, "{}">:$offsets, DefaultValuedAttr<I32ArrayAttr, "{}">:$shape);

  let results = (outs PD_Tensor:$Out);
}
def PD_Tensor_array_to_tensorOp : PD_Op<"tensor_array_to_tensor", [NoSideEffect]> {
  let summary = "tensor_array_to_tensor op";
  let description = [{
    
    tensor_array_to_tensor Operator.
    
    If use concat mode, concatenate all tensors in the input LoDTensorArray along
    axis into the output Tensor.
    
    Examples:
      Input = {[1,2], [3,4], [5,6]}
      axis = 0
      Output = [1,2,3,4,5,6]
      OutputIndex = [2,2,2]
    
    If use stack mode, stack all tensors in the input LoDTensorArray along axis into
    the output Tensor.
    
    Examples:
      Input = {[1,2], [3,4], [5,6]}
      axis = 0
      Output = [[1,2],
                [3,4],
                [5,6]]
      OutputIndex = [2,2,2]
    
    
  }];
  let arguments = (ins  PD_Tensor:$X, DefaultValuedAttr<SI32Attr, "0">:$axis, DefaultValuedAttr<BoolAttr, "false">:$use_stack);

  let results = (outs PD_Tensor:$Out,PD_Tensor:$OutIndex);
}
def PD_ErfOp : PD_Op<"erf", [NoSideEffect]> {
  let summary = "erf op";
  let description = [{
    
    Erf Operator.
    
    The equation is:
    $$
    f(x) = \frac{2}{\sqrt{\pi}} \int_{0}^{x}e^{- \eta^{2}}d\eta
    $$
    
    The input `X` can carry the LoD (Level of Details) information,
    or not. And the output shares the LoD information with input `X`.
    
  }];
  let arguments = (ins  PD_Tensor:$X);

  let results = (outs PD_Tensor:$Out);
}
def PD_Trilinear_interpOp : PD_Op<"trilinear_interp", [NoSideEffect]> {
  let summary = "trilinear_interp op";
  let description = [{
    
              This operator samples input X to given output shape by using specified
              interpolation method, the interpolation methods can be \"nearest\"
              for nearest neighbor interpolation and \"bilinear\" for bilinear 
              interpolation and \"linear\" for linear interpolation..
    
              Nearest neighbor interpolation is to perform nearest neighbor interpolation
              in both the 3rd dimension(in height direction) and the 4th dimension(in width 
              direction) on input tensor.
               
              Linear interpolation is the method of using a line connecting two known quantities 
              to determine the value of an unknown quantity between the two known quantities. 
              
              Bilinear interpolation is an extension of linear interpolation for 
              interpolating functions of two variables (e.g. H-direction and 
              W-direction in this op) on a rectilinear 2D grid. The key idea is 
              to perform linear interpolation first in one direction, and then 
              again in the other direction.
    
              Trilinear interpolation is an extension of linear interpolation for 
              interpolating functions of three variables (e.g. D-direction, 
              H-direction and W-direction in this op) on a rectilinear 3D grid. 
              The linear interpolation is performed on three directions.
    
              Bicubic interpolation is an extension of cubic interpolation for interpolating
              data points on a two-dimensional regular grid. The interpolated surface is
              smoother than corresponding surfaces obtained by bilinear interpolation or
              nearest-neighbor interpolation.
    
              Align_corners and align_mode are optional parameters,the calculation method 
              of interpolation can be selected by them.
              
              Example:
    
              For scale:
              
                if align_corners = True and out_{size}>1 :
    
                  scale_{factor} = (in_{size}-1.0)/(out_{size}-1.0)
                
                else:
                  
                  scale_{factor} = float(in_{size}/out_{size})
                
              
              Nearest neighbor interpolation:
              
              if:
                  align_corners = False
    
                  input : (N,C,H_in,W_in)
                  output: (N,C,H_out,W_out) where:
    
                  H_out = \left \lfloor {H_{in} * scale_{}factor}} \right \rfloor
                  W_out = \left \lfloor {W_{in} * scale_{}factor}} \right \rfloor
    
              else:
                  align_corners = True
    
                  input : (N,C,H_in,W_in)
                  output: (N,C,H_out,W_out) where:
    
                  H_out = round(H_{in} * scale_{factor})
                  W_out = round(W_{in} * scale_{factor})
    
              Bilinear interpolation:
    
              if:
                  align_corners = False , align_mode = 0
                  
                  input : (N,C,H_in,W_in)
                  output: (N,C,H_out,W_out) where:
                  
                  H_out = (H_{in}+0.5) * scale_{factor} - 0.5
                  W_out = (W_{in}+0.5) * scale_{factor} - 0.5
    
    
              else:
               
                  input : (N,C,H_in,W_in)
                  output: (N,C,H_out,W_out) where:
    
                  H_out = H_{in} * scale_{factor}
                  W_out = W_{in} * scale_{factor}
    
              Trilinear interpolation:
    
              if:
                  align_corners = False , align_mode = 0
                  
                  input : (N,C,D_in,H_in,W_in)
                  output: (N,C,D_out,H_out,W_out) where:
                  
                  D_out = (D_{in}+0.5) * scale_{factor} - 0.5
                  H_out = (H_{in}+0.5) * scale_{factor} - 0.5
                  W_out = (W_{in}+0.5) * scale_{factor} - 0.5
    
    
              else:
               
                  input : (N,C,D_in,H_in,W_in)
                  output: (N,C,D_out,H_out,W_out) where:
    
                  D_out = D_{in} * scale_{factor}
                  H_out = H_{in} * scale_{factor}
                  W_out = W_{in} * scale_{factor}
    
              Bicubic interpolation:
    
              if:
                  align_corners = False
                  input : (N,C,H_in,W_in)
                  output: (N,C,H_out,W_out) where:
                  H_out = (H_{in}+0.5) * scale_{factor} - 0.5
                  W_out = (W_{in}+0.5) * scale_{factor} - 0.5
              else:
                  input : (N,C,H_in,W_in)
                  output: (N,C,H_out,W_out) where:
                  H_out = H_{in} * scale_{factor}
                  W_out = W_{in} * scale_{factor}
    
              For details of nearest neighbor interpolation, please refer to Wikipedia: 
              https://en.wikipedia.org/wiki/Nearest-neighbor_interpolation
    
              For details of bilinear interpolation, please refer to Wikipedia: 
              https://en.wikipedia.org/wiki/Bilinear_interpolation
    
              For details of trilinear interpolation, please refer to Wikipedia: 
              https://en.wikipedia.org/wiki/Trilinear_interpolation
    
              For details of bicubic interpolation, please refer to Wikipedia:
              https://en.wikipedia.org/wiki/Bicubic_interpolation
             
  }];
  let arguments = (ins  PD_Tensor:$X, PD_Tensor:$OutSize, PD_Tensor:$SizeTensor, PD_Tensor:$Scale, DefaultValuedAttr<StrAttr, "NCHW">:$data_layout, DefaultValuedAttr<SI32Attr, "0">:$out_d, DefaultValuedAttr<SI32Attr, "0">:$out_h, DefaultValuedAttr<SI32Attr, "0">:$out_w, DefaultValuedAttr<F32Attr, "0.0">:$scale, DefaultValuedAttr<StrAttr, "bilinear">:$interp_method, DefaultValuedAttr<BoolAttr, "true">:$align_corners, DefaultValuedAttr<SI32Attr, "1">:$align_mode);

  let results = (outs PD_Tensor:$Out);
}
def PD_LogsumexpOp : PD_Op<"logsumexp", [NoSideEffect]> {
  let summary = "logsumexp op";
  let description = [{
    
    logsumexp Operator.
    
    This operator computes the logsumexp of input tensor along the given axis.
    The result tensor has 1 fewer dimension than the input unless keep_dim is true.
    If reduce_all is true, just reduce along all dimensions and output a scalar.
    
    
  }];
  let arguments = (ins  PD_Tensor:$X, DefaultValuedAttr<I32ArrayAttr, "{0}">:$axis, DefaultValuedAttr<BoolAttr, "false">:$keepdim, DefaultValuedAttr<BoolAttr, "false">:$reduce_all);

  let results = (outs PD_Tensor:$Out);
}
def PD_Sequence_concatOp : PD_Op<"sequence_concat", [NoSideEffect]> {
  let summary = "sequence_concat op";
  let description = [{
    Sequence Concat Op
    It will concat LoD tensors by its sequence information.
    For example:
      LoD of X1 = [0, 3, 7]
      LoD of X2 = [0, 7, 9]
      Result LoD is [0, (3+7), (7+9)]
                i.e.[0, 10, 16]
    
  }];
  let arguments = (ins  PD_Tensor:$X);

  let results = (outs PD_Tensor:$Out);
}
def PD_ArgsortOp : PD_Op<"argsort", [NoSideEffect]> {
  let summary = "argsort op";
  let description = [{
    
    Argsort operator
    
    Performs sorting on the input tensor along the given axis and outputs two 
    tensors, Output(Out) and Output(Indices). They reserve the same shape 
    with Input(X), and Output(Out) represents the sorted tensor while 
    Output(Indices) gives the sorted order along the given axis Attr(axis).
    
     
  }];
  let arguments = (ins  PD_Tensor:$X, DefaultValuedAttr<SI32Attr, "-1">:$axis, DefaultValuedAttr<BoolAttr, "false">:$descending);

  let results = (outs PD_Tensor:$Out,PD_Tensor:$Indices);
}
def PD_Sequence_expandOp : PD_Op<"sequence_expand", [NoSideEffect]> {
  let summary = "sequence_expand op";
  let description = [{
    
    Sequence Expand Operator.
    
    This operator expands `X` according to specified level lod of `Y`. Current
    implementation constaints that lod level of `X` should be at most 1. Attribute
    `ref_level` is used to specify which level lod of `Y` is referred to expand `X`.
    If set `ref_level` to -1, then last level lod of `Y` would be referred.
    Please note, rank of `X` should be at least 2, when the rank exceeds 2, `X`
    would be viewed as a 2-D tensor.
    
    Following are cases to better explain how this works:
    
    Case 1:
    
    Given a 1-level LoDTensor input(X)
        X.lod =  [[0,   2,        4]]
        X.data = [[a], [b], [c], [d]]
        X.dims = [4, 1]
    and input(Y)
        Y.lod = [[0,    2,    4],
                 [0, 3, 6, 7, 8]]
    ref_level: 0
    then we get 1-level LoDTensor
        Out.lod =  [[0,   2,        4,        6,        8]]
        Out.data = [[a], [b], [a], [b], [c], [d], [c], [d]]
        Out.dims = [8, 1]
    
    Case 2:
    
    Given 1-level LoDTensor input(X)
        X.lod =  [[0,   1,        4]]
        X.data = [[a], [b], [c], [d]]
        X.dims = [4, 1]
    and input(Y)
        Y.lod = [[0,    2,    4],
                 [0, 3, 6, 6, 8]]
    ref_level: 0
    then we get 1-level LoDTensor
        Out.lod =  [[0,   1,   2,        5,             8]]
        Out.data = [[a], [a], [b], [c], [d], [b], [c], [d]]
        Out.dims = [8, 1]
    
    Case 3:
    
    Given a common Tensor input(X)
        X.data = [[a], [b], [c]]
        X.dims = [3, 1]
    and input(Y)
        Y.lod = [[0, 2, 3, 6]]
    ref_level: -1
    then we get a common Tensor
        Out.data = [[a], [a], [b], [c], [c], [c]]
        Out.dims = [6, 1]
    
    Case 4:
    
    Given a common Tensor input(X)
        X.data = [[a, b], [c, d], [e, f]]
        X.dims = [3, 2]
    and input(Y)
        Y.lod = [[0, 2, 3, 6]]
    ref_level: 0
    then we get a common LoDTensor
        Out.data = [[a, b], [a, b] [c, d], [e, f], [e, f], [e, f]]
        Out.dims = [6, 2]
    
    
  }];
  let arguments = (ins  PD_Tensor:$X, PD_Tensor:$Y, DefaultValuedAttr<SI32Attr, "-1">:$ref_level);

  let results = (outs PD_Tensor:$Out);
}
def PD_Bilinear_interp_v2Op : PD_Op<"bilinear_interp_v2", [NoSideEffect]> {
  let summary = "bilinear_interp_v2 op";
  let description = [{
    
              This operator samples input X to given output shape by using specified
              interpolation method, the interpolation methods can be \"nearest\"
              for nearest neighbor interpolation and \"bilinear\" for bilinear 
              interpolation and \"linear\" for linear interpolation..
    
              Nearest neighbor interpolation is to perform nearest neighbor interpolation
              in both the 3rd dimension(in height direction) and the 4th dimension(in width 
              direction) on input tensor.
               
              Linear interpolation is the method of using a line connecting two known quantities 
              to determine the value of an unknown quantity between the two known quantities. 
              
              Bilinear interpolation is an extension of linear interpolation for 
              interpolating functions of two variables (e.g. H-direction and 
              W-direction in this op) on a rectilinear 2D grid. The key idea is 
              to perform linear interpolation first in one direction, and then 
              again in the other direction.
    
              Trilinear interpolation is an extension of linear interpolation for 
              interpolating functions of three variables (e.g. D-direction, 
              H-direction and W-direction in this op) on a rectilinear 3D grid. 
              The linear interpolation is performed on three directions.
    
              Bicubic interpolation is an extension of cubic interpolation for interpolating
              data points on a two-dimensional regular grid. The interpolated surface is
              smoother than corresponding surfaces obtained by bilinear interpolation or
              nearest-neighbor interpolation.
    
              Align_corners and align_mode are optional parameters,the calculation method 
              of interpolation can be selected by them.
              
              Example:
    
              For scale:
              
                if align_corners = True and out_{size}>1 :
    
                  scale_{factor} = (in_{size}-1.0)/(out_{size}-1.0)
                
                else:
                  
                  scale_{factor} = float(in_{size}/out_{size})
                
              
              Nearest neighbor interpolation:
              
              if:
                  align_corners = False
    
                  input : (N,C,H_in,W_in)
                  output: (N,C,H_out,W_out) where:
    
                  H_out = \left \lfloor {H_{in} * scale_{}factor}} \right \rfloor
                  W_out = \left \lfloor {W_{in} * scale_{}factor}} \right \rfloor
    
              else:
                  align_corners = True
    
                  input : (N,C,H_in,W_in)
                  output: (N,C,H_out,W_out) where:
    
                  H_out = round(H_{in} * scale_{factor})
                  W_out = round(W_{in} * scale_{factor})
    
              Bilinear interpolation:
    
              if:
                  align_corners = False , align_mode = 0
                  
                  input : (N,C,H_in,W_in)
                  output: (N,C,H_out,W_out) where:
                  
                  H_out = (H_{in}+0.5) * scale_{factor} - 0.5
                  W_out = (W_{in}+0.5) * scale_{factor} - 0.5
    
    
              else:
               
                  input : (N,C,H_in,W_in)
                  output: (N,C,H_out,W_out) where:
    
                  H_out = H_{in} * scale_{factor}
                  W_out = W_{in} * scale_{factor}
    
              Trilinear interpolation:
    
              if:
                  align_corners = False , align_mode = 0
                  
                  input : (N,C,D_in,H_in,W_in)
                  output: (N,C,D_out,H_out,W_out) where:
                  
                  D_out = (D_{in}+0.5) * scale_{factor} - 0.5
                  H_out = (H_{in}+0.5) * scale_{factor} - 0.5
                  W_out = (W_{in}+0.5) * scale_{factor} - 0.5
    
    
              else:
               
                  input : (N,C,D_in,H_in,W_in)
                  output: (N,C,D_out,H_out,W_out) where:
    
                  D_out = D_{in} * scale_{factor}
                  H_out = H_{in} * scale_{factor}
                  W_out = W_{in} * scale_{factor}
    
              Bicubic interpolation:
    
              if:
                  align_corners = False
                  input : (N,C,H_in,W_in)
                  output: (N,C,H_out,W_out) where:
                  H_out = (H_{in}+0.5) * scale_{factor} - 0.5
                  W_out = (W_{in}+0.5) * scale_{factor} - 0.5
              else:
                  input : (N,C,H_in,W_in)
                  output: (N,C,H_out,W_out) where:
                  H_out = H_{in} * scale_{factor}
                  W_out = W_{in} * scale_{factor}
    
              For details of nearest neighbor interpolation, please refer to Wikipedia: 
              https://en.wikipedia.org/wiki/Nearest-neighbor_interpolation
    
              For details of bilinear interpolation, please refer to Wikipedia: 
              https://en.wikipedia.org/wiki/Bilinear_interp_v2olation
    
              For details of trilinear interpolation, please refer to Wikipedia: 
              https://en.wikipedia.org/wiki/Trilinear_interp_v2olation
    
              For details of bicubic interpolation, please refer to Wikipedia:
              https://en.wikipedia.org/wiki/Bicubic_interpolation
             
  }];
  let arguments = (ins  PD_Tensor:$X, PD_Tensor:$OutSize, PD_Tensor:$SizeTensor, PD_Tensor:$Scale, DefaultValuedAttr<StrAttr, "NCHW">:$data_layout, DefaultValuedAttr<SI32Attr, "0">:$out_d, DefaultValuedAttr<SI32Attr, "0">:$out_h, DefaultValuedAttr<SI32Attr, "0">:$out_w, DefaultValuedAttr<F32ArrayAttr, "{}">:$scale, DefaultValuedAttr<StrAttr, "bilinear">:$interp_method, DefaultValuedAttr<BoolAttr, "true">:$align_corners, DefaultValuedAttr<SI32Attr, "1">:$align_mode);

  let results = (outs PD_Tensor:$Out);
}
def PD_ClipOp : PD_Op<"clip", [NoSideEffect]> {
  let summary = "clip op";
  let description = [{
    
    Clip Operator.
    
    The clip operator limits the value of given input within an interval [min, max],
    just as the following equation,
    
    $$
    Out = \MIN(\MAX(x, min), max)
    $$
    
    
  }];
  let arguments = (ins  PD_Tensor:$X, PD_Tensor:$Min, PD_Tensor:$Max, DefaultValuedAttr<StrAttr, "float32">:$mkldnn_data_type);

  let results = (outs PD_Tensor:$Out);
}
def PD_Deformable_conv_v1Op : PD_Op<"deformable_conv_v1", [NoSideEffect]> {
  let summary = "deformable_conv_v1 op";
  let description = [{
    
    **Deformable Convolution v1 Operator**
    
    Deformable Convolution is a new method based Convolution which feature has offset 
    in spatial location.
    
    1. Get offset of each pixel in feature map with convolution layers which number 
       of channels should be double of weight size.
    
    2. Add offset to pixel to get new location and the new value which are computed 
       directly through bilinear interpolation with four nearest pixel.
    
    3. Get the product of pixel and weight as result
    
    Compute 2-D deformable convolution on 4-D input.
    
    Given input image x, output feature map y, the deformable convolution operation can be expressed as follow:
    
    $$
    y(p) = \\sum_{k=1}^{K}{w_k * x(p + p_k + \\Delta p_k)}
    $$
    
    Where $$\\Delta p_k$$ is the learnable offset for the k-th location, respectively.
    
    Refer to 'https://arxiv.org/abs/1703.06211 '<https://arxiv.org/abs/1703.06211>
    
    Example:
      Input:
           Input shape: $(N, C_{in}, H_{in}, W_{in})$
           Filter shape: $(C_{out}, C_{in}, H_f, W_f)$
           Offset shape: $(N, 2 * deformable_groups, * H_f * W_f, H_{out}, W_{out})$
      Output:
           Output shape: $(N, C_{out}, H_{out}, W_{out})$
                         where $H_{out}, W_{out}$ must be equal to $H_{in}, W_{in}$ respectively.
      Where
    $$
           H_{out}= \frac{(H_{in} + 2 * paddings[0] - (dilations[0] * (H_f - 1) + 1))}{strides[0]}+ 1 \\
           W_{out}= \frac{(W_{in} + 2 * paddings[1] - (dilations[1] * (W_f - 1) + 1))}{strides[1]}+ 1
    $$
    
  }];
  let arguments = (ins  PD_Tensor:$Input, PD_Tensor:$Offset, PD_Tensor:$Filter, DefaultValuedAttr<I32ArrayAttr, "{1, 1}">:$strides, DefaultValuedAttr<I32ArrayAttr, "{0, 0}">:$paddings, DefaultValuedAttr<I32ArrayAttr, "{1, 1}">:$dilations, DefaultValuedAttr<SI32Attr, "1">:$groups, DefaultValuedAttr<SI32Attr, "1">:$deformable_groups, DefaultValuedAttr<SI32Attr, "64">:$im2col_step);

  let results = (outs PD_Tensor:$Output);
}
def PD_Hinge_lossOp : PD_Op<"hinge_loss", [NoSideEffect]> {
  let summary = "hinge_loss op";
  let description = [{
    
    HingeLoss Operator.
    
    Let x be a logit (prediction) and y be the actual label. The logit can
    take any values from (-inf, inf), but the labels should be either -1 or 1.
    Then, the hinge loss is computed as follows:
    
    $$
    L_(x, y) = max(1 - y.x, 0) 
    $$
    
    Note that the labels passed as input will have values as either 0 or 1.
    
    
  }];
  let arguments = (ins  PD_Tensor:$Logits, PD_Tensor:$Labels);

  let results = (outs PD_Tensor:$Loss);
}
def PD_DeterminantOp : PD_Op<"determinant", [NoSideEffect]> {
  let summary = "determinant op";
  let description = [{
    
    Determinant Operator.
  }];
  let arguments = (ins  PD_Tensor:$Input);

  let results = (outs PD_Tensor:$Out);
}
def PD_Conv2d_transposeOp : PD_Op<"conv2d_transpose", [NoSideEffect]> {
  let summary = "conv2d_transpose op";
  let description = [{
    
    Convolution2D Transpose Operator.
    
    The convolution transpose operation calculates the output based on the input, filter
    and dilations, strides, paddings, groups parameters. The size of each dimension of the
    parameters is checked in the infer-shape.
    Input(Input) and output(Output) are in NCHW or NHWC format. Where N is batchsize, C is the
    number of channels, H is the height of the feature, and W is the width of the feature.
    Filter(Input) is in MCHW format. Where M is the number of input feature channels,
    C is the number of output feature channels, H is the height of the filter,
    and W is the width of the filter.
    Parameters(strides, paddings) are two elements. These two elements represent height
    and width, respectively.
    The input(X) size and output(Out) size may be different.
    
    For an example:
      Input:
           Input shape: $(N, C_{in}, H_{in}, W_{in})$
           Filter shape: $(C_{in}, C_{out}, H_f, W_f)$
      Output:
           Output shape: $(N, C_{out}, H_{out}, W_{out})$
      Where
      $$
           H_{out} = (H_{in} - 1) * strides[0] - pad_height_top - pad_height_bottom  + dilations[0] * (H_f - 1) + 1 \\
           W_{out} = (W_{in} - 1) * strides[1] - pad_width_left  - pad_width_right + dilations[1] * (W_f - 1) + 1
      $$
    
  }];
  let arguments = (ins  PD_Tensor:$Input, PD_Tensor:$Filter, DefaultValuedAttr<I32ArrayAttr, "{}">:$output_padding, DefaultValuedAttr<I32ArrayAttr, "{}">:$output_size, DefaultValuedAttr<SI32Attr, "1">:$groups, DefaultValuedAttr<I32ArrayAttr, "{1, 1}">:$dilations, DefaultValuedAttr<I32ArrayAttr, "{1, 1}">:$strides, DefaultValuedAttr<I32ArrayAttr, "{0, 0}">:$paddings, DefaultValuedAttr<StrAttr, "NCHW">:$data_format, DefaultValuedAttr<StrAttr, "EXPLICIT">:$padding_algorithm);

  let results = (outs PD_Tensor:$Output);
}
def PD_SoftsignOp : PD_Op<"softsign", [NoSideEffect]> {
  let summary = "softsign op";
  let description = [{
    
    Softsign Activation Operator.
    
    $$out = \\frac{x}{1 + \|x\|}$$
    
    
  }];
  let arguments = (ins  PD_Tensor:$X);

  let results = (outs PD_Tensor:$Out);
}
def PD_Broadcast_tensorsOp : PD_Op<"broadcast_tensors", [NoSideEffect]> {
  let summary = "broadcast_tensors op";
  let description = [{
    This OP is used to broadcast a vector of inputs 
                         with Tensor or LoDTensor type, following broadcast semantics.
  }];
  let arguments = (ins  PD_Tensor:$X);

  let results = (outs PD_Tensor:$Out);
}
def PD_Grid_samplerOp : PD_Op<"grid_sampler", [NoSideEffect]> {
  let summary = "grid_sampler op";
  let description = [{
    
          This operation samples input X by using bilinear or nearest interpolation based on 
          flow field grid, which is usually generated by affine_grid. The grid of
          shape [N, H, W, 2] is the concatenation of (grid_x, grid_y) coordinates 
          with shape [N, H, W] each, where grid_x is indexing the 4th dimension 
          (in width dimension) of input data x and grid_y is indexing the 3rd 
          dimension (in height dimension), finally results is the bilinear 
          interpolation value or nearest value of 4 nearest corner points.
    
          For bilinear interpolation mode:
          Step 1:
            Get (x, y) grid coordinates and scale to [0, H-1/W-1].
    
            grid_x = 0.5 * (grid[:, :, :, 0] + 1) * (W - 1)
            grid_y = 0.5 * (grid[:, :, :, 1] + 1) * (H - 1)
    
          Step 2:
            Indices input data X with grid (x, y) in each [H, W] area, and bilinear 
            interpolate point value by 4 nearest points.
    
              wn ------- y_n ------- en
              |           |           |
              |          d_n          |
              |           |           |
             x_w --d_w-- grid--d_e-- x_e
              |           |           |
              |          d_s          |
              |           |           |
              ws ------- y_s ------- wn
    
            x_w = floor(x)              // west side x coord
            x_e = x_w + 1               // east side x coord
            y_n = floor(y)              // north side y coord
            y_s = y_s + 1               // south side y coord
    
            d_w = grid_x - x_w          // distance to west side
            d_e = x_e - grid_x          // distance to east side
            d_n = grid_y - y_n          // distance to north side
            d_s = y_s - grid_y          // distance to south side
    
            wn = X[:, :, y_n, x_w]      // north-west point value
            en = X[:, :, y_n, x_e]      // north-east point value
            ws = X[:, :, y_s, x_w]      // south-east point value
            es = X[:, :, y_s, x_w]      // north-east point value
    
            output = wn * d_e * d_s + en * d_w * d_s
                   + ws * d_e * d_n + es * d_w * d_n
            
  }];
  let arguments = (ins  PD_Tensor:$X, PD_Tensor:$Grid, DefaultValuedAttr<BoolAttr, "true">:$align_corners, DefaultValuedAttr<StrAttr, "bilinear">:$mode, DefaultValuedAttr<StrAttr, "zeros">:$padding_mode);

  let results = (outs PD_Tensor:$Output);
}
def PD_Fft_c2rOp : PD_Op<"fft_c2r", [NoSideEffect]> {
  let summary = "fft_c2r op";
  let description = [{
    
          Compute complex to complex FFT.
        
  }];
  let arguments = (ins  PD_Tensor:$X,I64ArrayAttr:$axes, DefaultValuedAttr<SI64Attr, "0">:$last_dim_size);

  let results = (outs PD_Tensor:$Out);
}
def PD_Pyramid_hashOp : PD_Op<"pyramid_hash", [NoSideEffect]> {
  let summary = "pyramid_hash op";
  let description = [{
    
          PyramidHash
    
          NOTE: only support 'float32' data type now.
    
        
  }];
  let arguments = (ins  PD_Tensor:$X, PD_Tensor:$W, PD_Tensor:$WhiteList, PD_Tensor:$BlackList, DefaultValuedAttr<SI32Attr, "0">:$num_emb, DefaultValuedAttr<SI32Attr, "0">:$space_len, DefaultValuedAttr<SI32Attr, "2">:$pyramid_layer, DefaultValuedAttr<SI32Attr, "0">:$rand_len, DefaultValuedAttr<F32Attr, "0.0">:$drop_out_percent, DefaultValuedAttr<SI32Attr, "0">:$is_training, DefaultValuedAttr<BoolAttr, "true">:$use_filter, DefaultValuedAttr<SI32Attr, "0">:$white_list_len, DefaultValuedAttr<SI32Attr, "0">:$black_list_len, DefaultValuedAttr<SI32Attr, "0">:$seed, DefaultValuedAttr<F32Attr, "0.0">:$lr, DefaultValuedAttr<StrAttr, "">:$distribute_update_vars);

  let results = (outs PD_Tensor:$Out,PD_Tensor:$DropPos);
}
def PD_Multi_dotOp : PD_Op<"multi_dot", [NoSideEffect]> {
  let summary = "multi_dot op";
  let description = [{
    
    Compute the dot product of two or more arrays in a single function call, while automatically selecting the fastest evaluation order.
    
    multi_dot chains MatMul and uses optimal parenthesization of the matrices [1] [2]. Depending on the shapes of the matrices, this can speed up the multiplication a lot.
    
    If the first argument is 1-D it is treated as a row vector. If the last argument is 1-D it is treated as a column vector. The other arguments must be 2-D.
          
  }];
  let arguments = (ins  PD_Tensor:$X);

  let results = (outs PD_Tensor:$Out);
}
def PD_Sequence_poolOp : PD_Op<"sequence_pool", [NoSideEffect]> {
  let summary = "sequence_pool op";
  let description = [{
    
    Sequence Pool Operator.
    
    The SequencePoolOp pools features of all time-steps of each instance.
    It supports six pooling types:
    1. AVERAGE: $$Out[i] = \frac{\sum_i X_i}{N}$$
    2. SUM:     $$Out[i] = \sum_jX_{ij}$$
    3. SQRT:    $$Out[i] = \frac{\sum_jX_{ij}}{\sqrt{len(X_i)}}$$
    4. LAST:    Out[i] = last instance in i-th sequence X[i]
    5. FIRST:   Out[i] = first instance in i-th sequence X[i]
    6. MAX:     $$Out[i] = max(X_i)$$
    
    and for the empty sequence Out[i] = attr(pad_value).
    
    The following example explains how this works:
    For a mini-batch of 3 variable-length sentences,
    containing 2, 3, and 2 time-steps:
    
    Assume X is a [7,M,N] LoDTensor, and X->lod()[0] = [0, 2, 5, 7], 7=2+3+2.
    Besides, for the sake of simplicity, we assume M=1 and N=1,
    and the value of X = [[1, 3], [2, 4, 6], [5, 1]].
    
    Thus, Out is a [3,1,1] Tensor without LoD information.
    And for different pooltype, the value of Out is as follows:
    
    - AVERAGE: [2, 4, 3], where 2=(1+3)/2, 4=(2+4+6)/3, 3=(5+1)/2
    - SUM: [4, 12, 6], where 4=1+3, 12=2+4+6, 6=5+1
    - SQRT: [2.82, 6.93, 4.24], where 2.82=(1+3)/sqrt(2),
               6.93=(2+4+6)/sqrt(3), 4.24=(5+1)/sqrt(2)
    - MAX: [3, 6, 5], where 3=max(1,3), 6=max(2,4,6), 5=max(5,1)
    - LAST: [3, 6, 1], where 3=last(1,3), 6=last(2,4,6), 1=last(5,1)
    - FIRST: [1, 2, 5], where 1=first(1,3), 2=first(2,4,6), 5=first(5,1)
    
        
  }];
  let arguments = (ins  PD_Tensor:$X, DefaultValuedAttr<StrAttr, "AVERAGE">:$pooltype, DefaultValuedAttr<F32Attr, "0.0">:$pad_value);

  let results = (outs PD_Tensor:$Out);
}
def PD_TransposeOp : PD_Op<"transpose", [NoSideEffect]> {
  let summary = "transpose op";
  let description = [{
    
    Transpose Operator.
    
    The input tensor will be permuted according to the axes given.
    The behavior of this operator is similar to how `numpy.transpose` works.
    
    - suppose the input `X` is a 2-D tensor:
        $$
        X = \begin{pmatrix}
        0 &1 &2 \\
        3 &4 &5
        \end{pmatrix}$$
    
        the given `axes` is: $[1, 0]$, and $Y$ = transpose($X$, axis)
    
        then the output $Y$ is:
    
        $$
        Y = \begin{pmatrix}
             0 &3 \\
             1 &4  \\
             2 &5
        \end{pmatrix}$$
    
    - Given a input tensor with shape $(N, C, H, W)$ and the `axes` is
    $[0, 2, 3, 1]$, then shape of the output tensor will be: $(N, H, W, C)$.
    
    
  }];
  let arguments = (ins  PD_Tensor:$X,I32ArrayAttr:$axis);

  let results = (outs PD_Tensor:$Out);
}
def PD_Top_kOp : PD_Op<"top_k", [NoSideEffect]> {
  let summary = "top_k op";
  let description = [{
    
    Top K operator
    
    If the input is a vector (1d tensor), this operator finds the k largest 
    entries in the vector and outputs their values and indices as vectors. 
    Thus values[j] is the j-th largest entry in input, and its index is indices[j].
    
    For matrices, this operator computes the top k entries in each row. 
  }];
  let arguments = (ins  PD_Tensor:$X, PD_Tensor:$K, DefaultValuedAttr<SI32Attr, "1">:$k);

  let results = (outs PD_Tensor:$Out,PD_Tensor:$Indices);
}
def PD_DistOp : PD_Op<"dist", [NoSideEffect]> {
  let summary = "dist op";
  let description = [{
    
    Dist Operator.
    Given two tensors X and Y, compute Lp-norm of (X-Y). It is not a norm in a strict sense,
    only as a measure of distance. The shapes of X and Y must be broadcastable. Where, Z = X - Y,
    
    When p = 0, defining $0^0 = 0$, the zero-norm of Z is simply the number of non-zero elements of z.
    $$
    ||Z||_{0} = \lim_{p \rightarrow 0} \sum_{i=1}^{m} |z_i|^p
    $$
    
    When p = inf, the inf-norm of Z is the maximum element of Z.
    $$
    ||Z||_\infty=\max_i |z_i|
    $$
    
    When p = -inf, the negative-inf-norm of Z is the minimum element of Z.
    $$
    ||Z||_{-\infty}=\min_i |z_i|
    $$
    
    Otherwise, the p-norm of Z follows the formula,
    $$
    ||Z||_{p} = (\sum_{i=i}^{m} |z_i|^p)^{1/p}
    $$
        
  }];
  let arguments = (ins  PD_Tensor:$X, PD_Tensor:$Y, DefaultValuedAttr<F32Attr, "2.0">:$p);

  let results = (outs PD_Tensor:$Out);
}
def PD_Affine_gridOp : PD_Op<"affine_grid", [NoSideEffect]> {
  let summary = "affine_grid op";
  let description = [{
    
        It generates a grid of (x,y) coordinates using the parameters of the
        affine transformation that correspond to a set of points where the input
        feature map should be sampled to produce the transformed output feature map.
    
        Given:
            Theta = [[[x_11, x_12, x_13]
                      [x_14, x_15, x_16]]
                     [[x_21, x_22, x_23]
                      [x_24, x_25, x_26]]]
        
            OutputShape = [2, 3, 5, 5]
    
        Step 1:
    
            Generate relative coordinates according to OutputShape.
            The values of relative coordinates are in the interval between -1 and 1.
            The shape of the relative coordinates is [2, H, W] as below:
        
            C = [[[-1.  -1.  -1.  -1.  -1. ]
                  [-0.5 -0.5 -0.5 -0.5 -0.5]
                  [ 0.   0.   0.   0.   0. ]
                  [ 0.5  0.5  0.5  0.5  0.5]
                  [ 1.   1.   1.   1.   1. ]] 
                 [[-1.  -0.5  0.   0.5  1. ]
                  [-1.  -0.5  0.   0.5  1. ]
                  [-1.  -0.5  0.   0.5  1. ]
                  [-1.  -0.5  0.   0.5  1. ]
                  [-1.  -0.5  0.   0.5  1. ]]]
            C[0] is the coordinates in height axis and  C[1] is the coordinates in
            width axis.
        
        Step2:
            Tanspose and reshape C to shape [H * W, 2] and append ones to last
            dimension. The we get:
            C_ = [[-1.  -1.   1. ]
                  [-0.5 -1.   1. ]
                  [ 0.  -1.   1. ]
                  [ 0.5 -1.   1. ]
                  [ 1.  -1.   1. ]
                  [-1.  -0.5  1. ]
                  [-0.5 -0.5  1. ]
                  [ 0.  -0.5  1. ]
                  [ 0.5 -0.5  1. ]
                  [ 1.  -0.5  1. ]
                  [-1.   0.   1. ]
                  [-0.5  0.   1. ]
                  [ 0.   0.   1. ]
                  [ 0.5  0.   1. ]
                  [ 1.   0.   1. ]
                  [-1.   0.5  1. ]
                  [-0.5  0.5  1. ]
                  [ 0.   0.5  1. ]
                  [ 0.5  0.5  1. ]
                  [ 1.   0.5  1. ]
                  [-1.   1.   1. ]
                  [-0.5  1.   1. ]
                  [ 0.   1.   1. ]
                  [ 0.5  1.   1. ]
                  [ 1.   1.   1. ]]
        Step3:
            Compute output by equation $$Output[i] = C_ * Theta[i]^T$$
        
  }];
  let arguments = (ins  PD_Tensor:$Theta, PD_Tensor:$OutputShape, DefaultValuedAttr<BoolAttr, "true">:$align_corners, DefaultValuedAttr<I32ArrayAttr, "{}">:$output_shape);

  let results = (outs PD_Tensor:$Output);
}
def PD_ReciprocalOp : PD_Op<"reciprocal", [NoSideEffect]> {
  let summary = "reciprocal op";
  let description = [{
    
    Reciprocal Activation Operator.
    
    $$out = \\frac{1}{x}$$
    
    
  }];
  let arguments = (ins  PD_Tensor:$X);

  let results = (outs PD_Tensor:$Out);
}
def PD_Fill_diagonal_tensorOp : PD_Op<"fill_diagonal_tensor", [NoSideEffect]> {
  let summary = "fill_diagonal_tensor op";
  let description = [{
    Fill replace operator
                    Fill the diagonal of an tensor with `Y` Tensor.
                    
  }];
  let arguments = (ins  PD_Tensor:$X, PD_Tensor:$Y, DefaultValuedAttr<SI32Attr, "0">:$dim1, DefaultValuedAttr<SI32Attr, "1">:$dim2, DefaultValuedAttr<SI64Attr, "0">:$offset);

  let results = (outs PD_Tensor:$Out);
}
def PD_AbsOp : PD_Op<"abs", [NoSideEffect]> {
  let summary = "abs op";
  let description = [{
    
    Abs Operator.
    
    This operator is used to perform elementwise abs for input $X$.
    $$out = |x|$$
    
    
  }];
  let arguments = (ins  PD_Tensor:$X);

  let results = (outs PD_Tensor:$Out);
}
def PD_Partial_concatOp : PD_Op<"partial_concat", [NoSideEffect]> {
  let summary = "partial_concat op";
  let description = [{
    
    Partial Concat Operator.
    Partial Concatenate the input tensors along the 2nd dimension.
    Only 2-D Tensor or LodTensor input is supported.
    Slice and concat can only be performed along the second dimension.
    Examples:
      Input[0] = [[1,2],[3,4]]
      Input[1] = [[5,6],[7,8]]
      start_index = 1
      length = 1
      Output = [[2,6],
                [4,8]]
    
  }];
  let arguments = (ins  PD_Tensor:$X, DefaultValuedAttr<SI32Attr, "0">:$start_index, DefaultValuedAttr<SI32Attr, "-1">:$length);

  let results = (outs PD_Tensor:$Out);
}
def PD_EluOp : PD_Op<"elu", [NoSideEffect]> {
  let summary = "elu op";
  let description = [{
    
    ELU Activation Operator.
    
    Applies the following element-wise computation on the input according to
    https://arxiv.org/abs/1511.07289.
    
    $$out = \max(0, x) + \min(0, \alpha * (e^x - 1))$$
    
    
  }];
  let arguments = (ins  PD_Tensor:$X, DefaultValuedAttr<F32Attr, "1.0">:$alpha);

  let results = (outs PD_Tensor:$Out);
}
def PD_Index_selectOp : PD_Op<"index_select", [NoSideEffect]> {
  let summary = "index_select op";
  let description = [{
    
        Returns a new tensor which indexes the input tensor
        along dimension dim using the entries in index which
        is a Tensor.
    
        The returned tensor has the same number of dimensions
        as the original tensor (input). The dim-th dimension
        has the same size as the length of index; other dimensions
        have the same size as in the original tensor.
        
  }];
  let arguments = (ins  PD_Tensor:$X, PD_Tensor:$Index, DefaultValuedAttr<SI32Attr, "0">:$dim);

  let results = (outs PD_Tensor:$Out);
}
def PD_Row_convOp : PD_Op<"row_conv", [NoSideEffect]> {
  let summary = "row_conv op";
  let description = [{
    
    :strong:`Row-convolution operator`
    
    The row convolution is called lookahead convolution.  This operator was 
    introduced in the following paper for DeepSpeech2:
    http://www.cs.cmu.edu/~dyogatam/papers/wang+etal.iclrworkshop2016.pdf 
    
    The main motivation is that a bidirectional RNN, useful in DeepSpeech 
    like speech models, learns representation for a sequence by performing a 
    forward and a backward pass through the entire sequence. However, unlike 
    unidirectional RNNs, bidirectional RNNs are challenging to deploy in an online
    and low-latency setting. The lookahead convolution incorporates information 
    from future subsequences in a computationally efficient manner to improve 
    unidirectional recurrent neural networks. The row convolution operator is 
    different from the 1D sequence convolution, and is computed as follows:
    
    Given an input sequence $X$ of length $t$ and input dimension $D$, 
    and a filter ($W$) of size $context \times D$,
    the output sequence is convolved as:
    
    $$
    out_{i} = \\sum_{j=i}^{i + context - 1} X_{j} \\cdot W_{j-i}
    $$
    
    In the above equation:
    
    * $Out_{i}$: The i-th row of output variable with shape [1, D].
    
    * $context$: Future context size.
    
    * $X_{j}$: The j-th row of input variable with shape [1, D].
    
    * $W_{j-i}$: The (j-i)-th row of parameters with shape [1, D].
    
    More details about row_conv please refer to
    the design document
    https://github.com/PaddlePaddle/Paddle/issues/2228#issuecomment-303903645 .
    
    
  }];
  let arguments = (ins  PD_Tensor:$X, PD_Tensor:$Filter);

  let results = (outs PD_Tensor:$Out);
}
def PD_CrossOp : PD_Op<"cross", [NoSideEffect]> {
  let summary = "cross op";
  let description = [{
    
        Returns the cross product of vectors in dimension dim of
        input and other. Input and other must have the same size,
        and the size of their dim dimension should be 3.
        If dim is not given, it defaults to the first dimension
        found with the size 3.
        
  }];
  let arguments = (ins  PD_Tensor:$X, PD_Tensor:$Y, DefaultValuedAttr<SI32Attr, "9">:$dim);

  let results = (outs PD_Tensor:$Out);
}
def PD_Elementwise_mulOp : PD_Op<"elementwise_mul", [NoSideEffect]> {
  let summary = "elementwise_mul op";
  let description = [{
    
    Elementwise Mul Operator.
    
    Multiply two tensors element-wise
    
    The equation is:
    
    $$Out = X \\odot Y$$
    
    - $X$: a tensor of any dimension.
    - $Y$: a tensor whose dimensions must be less than or equal to the dimensions of $X$.
    
    There are two cases for this operator:
    
    1. The shape of $Y$ is the same with $X$.
    2. The shape of $Y$ is a continuous subsequence of $X$.
    
    For case 2:
    
    1. Broadcast $Y$ to match the shape of $X$, where $axis$ is the start dimension index
       for broadcasting $Y$ onto $X$.
    2. If $axis$ is -1 (default), $axis = rank(X) - rank(Y)$.
    3. The trailing dimensions of size 1 for $Y$ will be ignored for the consideration of
       subsequence, such as shape(Y) = (2, 1) => (2).
    
    For example:
    
      .. code-block:: text
    
        shape(X) = (2, 3, 4, 5), shape(Y) = (,)
        shape(X) = (2, 3, 4, 5), shape(Y) = (5,)
        shape(X) = (2, 3, 4, 5), shape(Y) = (4, 5), with axis=-1(default) or axis=2
        shape(X) = (2, 3, 4, 5), shape(Y) = (3, 4), with axis=1
        shape(X) = (2, 3, 4, 5), shape(Y) = (2), with axis=0
        shape(X) = (2, 3, 4, 5), shape(Y) = (2, 1), with axis=0
    
    
  }];
  let arguments = (ins  PD_Tensor:$X, PD_Tensor:$Y, DefaultValuedAttr<SI32Attr, "-1">:$axis);

  let results = (outs PD_Tensor:$Out);
}
def PD_LstmOp : PD_Op<"lstm", [NoSideEffect]> {
  let summary = "lstm op";
  let description = [{
    
    Long-Short Term Memory (LSTM) Operator.
    
    The default implementation is diagonal/peephole connection
    (https://arxiv.org/pdf/1402.1128.pdf), the formula is as follows:
    
    $$ i_t = \\sigma(W_{ix}x_{t} + W_{ih}h_{t-1} + W_{ic}c_{t-1} + b_i) $$
    
    $$ f_t = \\sigma(W_{fx}x_{t} + W_{fh}h_{t-1} + W_{fc}c_{t-1} + b_f) $$
    
    $$ \\tilde{c_t} = act_g(W_{cx}x_t + W_{ch}h_{t-1} + b_c) $$
    
    $$ o_t = \\sigma(W_{ox}x_{t} + W_{oh}h_{t-1} + W_{oc}c_t + b_o) $$
    
    $$ c_t = f_t \\odot c_{t-1} + i_t \\odot \\tilde{c_t} $$
    
    $$ h_t = o_t \\odot act_h(c_t) $$
    
    - W terms denote weight matrices (e.g. $W_{xi}$ is the matrix
      of weights from the input gate to the input), $W_{ic}, W_{fc}, W_{oc}$
      are diagonal weight matrices for peephole connections. In our implementation,
      we use vectors to represent these diagonal weight matrices.
    - The b terms denote bias vectors ($b_i$ is the input gate bias vector).
    - $\sigma$ is the non-line activations, such as logistic sigmoid function.
    - $i, f, o$ and $c$ are the input gate, forget gate, output gate,
      and cell activation vectors, respectively, all of which have the same size as
      the cell output activation vector $h$.
    - The $\odot$ is the element-wise product of the vectors.
    - $act_g$ and $act_h$ are the cell input and cell output activation functions
      and `tanh` is usually used for them.
    - $\tilde{c_t}$ is also called candidate hidden state,
      which is computed based on the current input and the previous hidden state.
    
    Set `use_peepholes` False to disable peephole connection. The formula
    is omitted here, please refer to the paper
    http://www.bioinf.jku.at/publications/older/2604.pdf for details.
    
    Note that these $W_{xi}x_{t}, W_{xf}x_{t}, W_{xc}x_{t}, W_{xo}x_{t}$
    operations on the input $x_{t}$ are NOT included in this operator.
    Users can choose to use fully-connect operator before LSTM operator.
    
    
  }];
  let arguments = (ins  PD_Tensor:$Input, PD_Tensor:$H0, PD_Tensor:$C0, PD_Tensor:$Weight, PD_Tensor:$Bias, DefaultValuedAttr<BoolAttr, "true">:$use_peepholes, DefaultValuedAttr<BoolAttr, "false">:$is_reverse, DefaultValuedAttr<StrAttr, "sigmoid">:$gate_activation, DefaultValuedAttr<StrAttr, "tanh">:$cell_activation, DefaultValuedAttr<StrAttr, "tanh">:$candidate_activation);

  let results = (outs PD_Tensor:$Hidden,PD_Tensor:$Cell);
}
def PD_Match_matrix_tensorOp : PD_Op<"match_matrix_tensor", [NoSideEffect]> {
  let summary = "match_matrix_tensor op";
  let description = [{
    
          Match Matrix Tensor Operator
    
          This operator calculate X * W * Y, only support 2-D for X and Y.
          the output is a level-1 LodTensor: 
            level_0: dim_t
          
          NOTE: only support 'float32' data type now.
    
        
  }];
  let arguments = (ins  PD_Tensor:$X, PD_Tensor:$Y, PD_Tensor:$W, DefaultValuedAttr<SI32Attr, "1">:$dim_t);

  let results = (outs PD_Tensor:$Out,PD_Tensor:$Tmp);
}
def PD_Elementwise_divOp : PD_Op<"elementwise_div", [NoSideEffect]> {
  let summary = "elementwise_div op";
  let description = [{
    
    Elementwise Div Operator.
    
    Divide two tensors element-wise
    
    The equation is:
    
    $$Out = X / Y$$
    
    - $X$: a tensor of any dimension.
    - $Y$: a tensor whose dimensions must be less than or equal to the dimensions of $X$.
    
    There are two cases for this operator:
    
    1. The shape of $Y$ is the same with $X$.
    2. The shape of $Y$ is a continuous subsequence of $X$.
    
    For case 2:
    
    1. Broadcast $Y$ to match the shape of $X$, where $axis$ is the start dimension index
       for broadcasting $Y$ onto $X$.
    2. If $axis$ is -1 (default), $axis = rank(X) - rank(Y)$.
    3. The trailing dimensions of size 1 for $Y$ will be ignored for the consideration of
       subsequence, such as shape(Y) = (2, 1) => (2).
    
    For example:
    
      .. code-block:: text
    
        shape(X) = (2, 3, 4, 5), shape(Y) = (,)
        shape(X) = (2, 3, 4, 5), shape(Y) = (5,)
        shape(X) = (2, 3, 4, 5), shape(Y) = (4, 5), with axis=-1(default) or axis=2
        shape(X) = (2, 3, 4, 5), shape(Y) = (3, 4), with axis=1
        shape(X) = (2, 3, 4, 5), shape(Y) = (2), with axis=0
        shape(X) = (2, 3, 4, 5), shape(Y) = (2, 1), with axis=0
    
    
  }];
  let arguments = (ins  PD_Tensor:$X, PD_Tensor:$Y, DefaultValuedAttr<SI32Attr, "-1">:$axis);

  let results = (outs PD_Tensor:$Out);
}
def PD_Kldiv_lossOp : PD_Op<"kldiv_loss", [NoSideEffect]> {
  let summary = "kldiv_loss op";
  let description = [{
    
             This operator calculates the Kullback-Leibler divergence loss
             between Input(X) and Input(Target). Notes that Input(X) is the
             log-probability and Input(Target) is the probability.
    
             KL divergence loss is calculated as follows:
    
             $$l(x, y) = y * (\log(y) - x)$$
    
             While :math:`x` is Input(X) and :math:`y` is Input(Target).
    
             While :attr:`reduction` is :attr:`none`, output loss is in
             the same shape as Input(X), loss in each point is calculated 
             seperately and no reduction is applied.
             
             While :attr:`reduction` is :attr:`mean`, output loss is in
             shape of [1] and loss value is the mean value of all losses.
             
             While :attr:`reduction` is :attr:`sum`, output loss is in
             shape of [1] and loss value is the sum value of all losses.
             
             While :attr:`reduction` is :attr:`batchmean`, output loss is 
             in shape of [1] and loss value is the sum value of all losses
             divided by batch size.
             
             
  }];
  let arguments = (ins  PD_Tensor:$X, PD_Tensor:$Target, DefaultValuedAttr<StrAttr, "mean">:$reduction);

  let results = (outs PD_Tensor:$Loss);
}
def PD_SeluOp : PD_Op<"selu", [NoSideEffect]> {
  let summary = "selu op";
  let description = [{
    
    Selu Operator.
    
    The equation is:
    $$
    f(x) =\lambda*
    \begin{cases}
     \quad \quad   x,  \quad \quad \quad \text{if} \ x > 0 \\
     \alpha * e^x - \alpha,  \qquad  \text{if} \ x <= 0
    \end{cases}
    $$
    
    The input `X` can carry the LoD (Level of Details) information,
    or not. And the output shares the LoD information with input `X`.
    
  }];
  let arguments = (ins  PD_Tensor:$X, DefaultValuedAttr<F32Attr, "1.0507010221481323">:$scale, DefaultValuedAttr<F32Attr, "1.6732631921768188">:$alpha);

  let results = (outs PD_Tensor:$Out);
}
def PD_MeanOp : PD_Op<"mean", [NoSideEffect]> {
  let summary = "mean op";
  let description = [{
    
    Mean Operator calculates the mean of all elements in X.
    
    
  }];
  let arguments = (ins  PD_Tensor:$X);

  let results = (outs PD_Tensor:$Out);
}
def PD_Gumbel_softmaxOp : PD_Op<"gumbel_softmax", [NoSideEffect]> {
  let summary = "gumbel_softmax op";
  let description = [{
    
    GumbelSoftmax Operator.
    
    Samples from the Gumbel-Softmax distribution and optionally discretizes.
    
    
  }];
  let arguments = (ins  PD_Tensor:$X, DefaultValuedAttr<F32Attr, "1.0">:$temperature, DefaultValuedAttr<BoolAttr, "false">:$hard, DefaultValuedAttr<SI32Attr, "-1">:$axis);

  let results = (outs PD_Tensor:$Out);
}
def PD_Sequence_padOp : PD_Op<"sequence_pad", [NoSideEffect]> {
  let summary = "sequence_pad op";
  let description = [{
    
          Sequence Pad Operator
    
          This operator pads sequences in a same batch to a consistent length. 
          The length is specified by attribute 'padded_length'. New elements, 
          whose values are specified by input 'PadValue', will be appended to 
          the end of each sequence, to make their final lengths consistent.
    
          Following are cases to better explain how this works:
    
          Case 1:
    
          Given a 1-level LoDTensor input(X):
              X.lod = [[0, 2,       5]]
              X.data = [a, b, c, d, e]
          and Input(PadValue):
              PadValue.data = [0]
          and attribite 'padded_length' = 4,
          then we get LoDTensor:
              Out.data = [[a, b, 0, 0], 
                          [c, d, e, 0]]
              Length.data = [2, 3]
          
          Case 2:
    
          Given a 1-level LoDTensor input(X):
              X.lod = [[0,               2,                           5]]
              X.data = [[a1, a2], [b1, b2], [c1, c2], [d1, d2], [e1, e2]]
          and Input(PadValue):
              PadValue.data = [0]
          and attribite 'padded_length' = -1, which mean using the length 
          of longest input sequence(3 in this case),
          then we get LoDTensor:
              Out.data = [[[a1, a2], [b1, b2], [0, 0]], 
                          [[c1, c2], [d1, d2], [e1, e2]]]
              Length.data = [2, 3]
     
          Case 3:
    
          Given a 1-level LoDTensor input(X):
              X.lod = [[0,               2,                           5]]
              X.data = [[a1, a2], [b1, b2], [c1, c2], [d1, d2], [e1, e2]]
          and Input(PadValue):
              PadValue.data = [p1, p2]
          and attribite 'padded_length' = -1, which mean using the length 
          of longest input sequence(3 in this case),
          then we get LoDTensor:
              Out.data = [[[a1, a2], [b1, b2], [p1, p2]], 
                          [[c1, c2], [d1, d2], [e1, e2]]]
              Length.data = [2, 3]
    
        
  }];
  let arguments = (ins  PD_Tensor:$X, PD_Tensor:$PadValue, DefaultValuedAttr<SI32Attr, "-1">:$padded_length);

  let results = (outs PD_Tensor:$Out,PD_Tensor:$Length);
}
def PD_Tree_convOp : PD_Op<"tree_conv", [NoSideEffect]> {
  let summary = "tree_conv op";
  let description = [{
    
    **Tree-Based Convolution Operator**
    
    Tree-Based Convolution is a kind of convolution based on tree structure.
    Tree-Based Convolution is a part of Tree-Based Convolution Neural Network(TBCNN),
    which is used to classify tree structures, such as Abstract Syntax Tree.
    Tree-Based Convolution proposed a kind of data structure called continuous binary tree,
    which regards multiway tree as binary tree.
    The paper of Tree-Based Convolution Operator is here:
    https://arxiv.org/abs/1409.5718v1
    
  }];
  let arguments = (ins  PD_Tensor:$NodesVector, PD_Tensor:$EdgeSet, PD_Tensor:$Filter, DefaultValuedAttr<SI32Attr, "2">:$max_depth);

  let results = (outs PD_Tensor:$Out);
}
def PD_Flatten_contiguous_rangeOp : PD_Op<"flatten_contiguous_range", [NoSideEffect]> {
  let summary = "flatten_contiguous_range op";
  let description = [{
    
    Flatten Operator
    
    Flattens the input tensor into a new matrix according to start_axis and stop_axis.
    
    Examples:
    Case 1:
      Given
        X.shape = (3, 100, 100, 4)
      and
        start_axis = 2, stop_axis = -1
      We get:
        Out.shape = (3, 100, 400)
    
    Case 2:
      Given
        X.shape = (3, 100, 100, 4)
      and
        start_axis = 0, stop_axis = -1
      We get:
        Out.shape = (3 * 100 * 100 * 4)
    
  }];
  let arguments = (ins  PD_Tensor:$X, DefaultValuedAttr<SI32Attr, "1">:$start_axis, DefaultValuedAttr<SI32Attr, "1">:$stop_axis);

  let results = (outs PD_Tensor:$Out);
}
def PD_Tril_triuOp : PD_Op<"tril_triu", [NoSideEffect]> {
  let summary = "tril_triu op";
  let description = [{
    
    TrilTriu Operator.
    
    The tril operator returns the lower triangular part of the matrix (2-D tensor)
    or batch of matrices $input$. The lower triangular part of the matrix is defined 
    as the elements on and below the diagonal.
    The triu operator returns the upper triangular part of a matrix (2-D tensor) 
    or batch of matrices $input$. The upper triangular part of the matrix is defined
    as the elements on and above the diagonal.
    The other elements of the result tensor out are set to 0.
    
    The argument diagonal controls which diagonal to consider, default value is 0.
    
    
  }];
  let arguments = (ins  PD_Tensor:$X, DefaultValuedAttr<SI32Attr, "0">:$diagonal);

  let results = (outs PD_Tensor:$Out);
}
def PD_BreluOp : PD_Op<"brelu", [NoSideEffect]> {
  let summary = "brelu op";
  let description = [{
    
    BRelu Activation Operator.
    
    $$out = \min(\max(x, t_{min}), t_{max})$$
    
    
  }];
  let arguments = (ins  PD_Tensor:$X, DefaultValuedAttr<F32Attr, "0.0">:$t_min, DefaultValuedAttr<F32Attr, "24.0">:$t_max);

  let results = (outs PD_Tensor:$Out);
}
def PD_Reduce_meanOp : PD_Op<"reduce_mean", [NoSideEffect]> {
  let summary = "reduce_mean op";
  let description = [{
    
    Reduce reduce_mean Operator.
    
    This operator computes the reduce_mean of input tensor along the given dimension.
    The result tensor has 1 fewer dimension than the input unless keep_dim is true.
    If reduce_all is true, just reduce along all dimensions and output a scalar.
    
    
  }];
  let arguments = (ins  PD_Tensor:$X, DefaultValuedAttr<I32ArrayAttr, "{0}">:$dim, DefaultValuedAttr<BoolAttr, "false">:$keep_dim, DefaultValuedAttr<BoolAttr, "false">:$reduce_all, DefaultValuedAttr<SI32Attr, "-1">:$in_dtype, DefaultValuedAttr<SI32Attr, "-1">:$out_dtype);

  let results = (outs PD_Tensor:$Out);
}
def PD_SinhOp : PD_Op<"sinh", [NoSideEffect]> {
  let summary = "sinh op";
  let description = [{
    
    Sinh Activation Operator.
    
    $$out = sinh(x)$$
    
    
  }];
  let arguments = (ins  PD_Tensor:$X);

  let results = (outs PD_Tensor:$Out);
}
def PD_Rank_lossOp : PD_Op<"rank_loss", [NoSideEffect]> {
  let summary = "rank_loss op";
  let description = [{
    
    RankLoss Operator.
    
    RankLoss operator for RankNet
    (http://icml.cc/2015/wp-content/uploads/2015/06/icml_ranking.pdf). 
    RankNet is a pairwise ranking model with
    one training sample consisting of a pair of doc A and B, and the label P
    indicating that A is ranked higher than B or not:
    
    P = {0, 1} or {0, 0.5, 1}, where 0.5 means no information about the rank of
    the input pair.
    
    The RankLoss operator takes three inputs: Left (o_i), Right (o_j) and Label
    (P_{i,j}), which represent the output score of RankNet for the two docs and 
    the label respectively, and yields the rank loss C_{i,j} using the following 
    equation:
    
    $$
      C_{i,j} = -\tilde{P_{ij}} * o_{i,j} + \log(1 + e^{o_{i,j}}) \\
      o_{i,j} =  o_i - o_j  \\
      \tilde{P_{i,j}} = \left \{0, 0.5, 1 \right \} \ or \ \left \{0, 1 \right \}
    $$
    
    The operator can take batch inputs with size batch_size (batch_size >= 1).
    
    
  }];
  let arguments = (ins  PD_Tensor:$Label, PD_Tensor:$Left, PD_Tensor:$Right);

  let results = (outs PD_Tensor:$Out);
}
def PD_Reduce_maxOp : PD_Op<"reduce_max", [NoSideEffect]> {
  let summary = "reduce_max op";
  let description = [{
    
    Reduce reduce_max Operator.
    
    This operator computes the reduce_max of input tensor along the given dimension.
    The result tensor has 1 fewer dimension than the input unless keep_dim is true.
    If reduce_all is true, just reduce along all dimensions and output a scalar.
    
    
  }];
  let arguments = (ins  PD_Tensor:$X, DefaultValuedAttr<I32ArrayAttr, "{0}">:$dim, DefaultValuedAttr<BoolAttr, "false">:$keep_dim, DefaultValuedAttr<BoolAttr, "false">:$reduce_all, DefaultValuedAttr<SI32Attr, "-1">:$in_dtype, DefaultValuedAttr<SI32Attr, "-1">:$out_dtype);

  let results = (outs PD_Tensor:$Out);
}
def PD_Expm1Op : PD_Op<"expm1", [NoSideEffect]> {
  let summary = "expm1 op";
  let description = [{
    
    Expm1 Operator. Computes expm1 of x element-wise with a natural number :math:`e` as the base.
    
    $$out = e^x - 1$$
    
    
  }];
  let arguments = (ins  PD_Tensor:$X);

  let results = (outs PD_Tensor:$Out);
}
def PD_Squared_l2_normOp : PD_Op<"squared_l2_norm", [NoSideEffect]> {
  let summary = "squared_l2_norm op";
  let description = [{
    
    SquaredL2Norm Operator.
    
    Computes the squared L2 norm of a tensor.
    
    $$Out = \sum_{i} X_{i}^2$$
    
    
  }];
  let arguments = (ins  PD_Tensor:$X);

  let results = (outs PD_Tensor:$Out);
}
def PD_Elementwise_subOp : PD_Op<"elementwise_sub", [NoSideEffect]> {
  let summary = "elementwise_sub op";
  let description = [{
    
    Elementwise Sub Operator.
    
    Substract two tensors element-wise
    
    The equation is:
    
    $$Out = X - Y$$
    
    - $X$: a tensor of any dimension.
    - $Y$: a tensor whose dimensions must be less than or equal to the dimensions of $X$.
    
    There are two cases for this operator:
    
    1. The shape of $Y$ is the same with $X$.
    2. The shape of $Y$ is a continuous subsequence of $X$.
    
    For case 2:
    
    1. Broadcast $Y$ to match the shape of $X$, where $axis$ is the start dimension index
       for broadcasting $Y$ onto $X$.
    2. If $axis$ is -1 (default), $axis = rank(X) - rank(Y)$.
    3. The trailing dimensions of size 1 for $Y$ will be ignored for the consideration of
       subsequence, such as shape(Y) = (2, 1) => (2).
    
    For example:
    
      .. code-block:: text
    
        shape(X) = (2, 3, 4, 5), shape(Y) = (,)
        shape(X) = (2, 3, 4, 5), shape(Y) = (5,)
        shape(X) = (2, 3, 4, 5), shape(Y) = (4, 5), with axis=-1(default) or axis=2
        shape(X) = (2, 3, 4, 5), shape(Y) = (3, 4), with axis=1
        shape(X) = (2, 3, 4, 5), shape(Y) = (2), with axis=0
        shape(X) = (2, 3, 4, 5), shape(Y) = (2, 1), with axis=0
    
    
  }];
  let arguments = (ins  PD_Tensor:$X, PD_Tensor:$Y, DefaultValuedAttr<SI32Attr, "-1">:$axis);

  let results = (outs PD_Tensor:$Out);
}
def PD_Margin_rank_lossOp : PD_Op<"margin_rank_loss", [NoSideEffect]> {
  let summary = "margin_rank_loss op";
  let description = [{
    
    MarginRankLoss Operator.
    
    This operator measures the loss given a pair of training sample
    {`X1`, `X2`} and the `Label` with attribute `margin`, where `Label = +1` 
    indicating X1 is ranked higher than `X2` and `Label = -1` otherwise. The loss 
    is calculated as:
    
    $loss(X1, X2, Label) = \max(0, -Label * (X1 - X2) + margin)$
    
    The attribute `margin` here helps make the predictions more robust.
    Denote the item ranked higher as the positive sample, otherwise the negative 
    sample. If the score of the two samples satisfies 
    
    $positive sample - negative sample < margin$
    
    the pair of samples will contribute to the final loss, which will backpropagate 
    and train the ranking model to enlarge the difference between the two scores.
    
    For batch input with size `batch_size`, `X1`, `X2` and `Label`
    all have the same shape [batch_size x 1].
    
    
  }];
  let arguments = (ins  PD_Tensor:$X1, PD_Tensor:$X2, PD_Tensor:$Label, DefaultValuedAttr<F32Attr, "0.0">:$margin);

  let results = (outs PD_Tensor:$Out);
}
def PD_ReluOp : PD_Op<"relu", [NoSideEffect]> {
  let summary = "relu op";
  let description = [{
    
    Relu Activation Operator.
    
    $$out = \max(x, 0)$$
    
    
  }];
  let arguments = (ins  PD_Tensor:$X);

  let results = (outs PD_Tensor:$Out);
}
def PD_BmmOp : PD_Op<"bmm", [NoSideEffect]> {
  let summary = "bmm op";
  let description = [{
    
    The Bmm operator is used to perform batched matrix multiplication
    over the last two dimensions of the input tensors `X` and `Y` 
    which are both 3-dimentionsal. 
    
    Examples:
    - X: [B, M, K], Y: [B, K, N] => Out: [B, M, N]
    
          
  }];
  let arguments = (ins  PD_Tensor:$X, PD_Tensor:$Y);

  let results = (outs PD_Tensor:$Out);
}
def PD_Soft_reluOp : PD_Op<"soft_relu", [NoSideEffect]> {
  let summary = "soft_relu op";
  let description = [{
    
    SoftRelu Activation Operator.
    
    $$out = \ln(1 + \exp(\max(\min(x, threshold), -threshold)))$$
    
    
  }];
  let arguments = (ins  PD_Tensor:$X, DefaultValuedAttr<F32Attr, "40.0">:$threshold);

  let results = (outs PD_Tensor:$Out);
}
def PD_SwishOp : PD_Op<"swish", [NoSideEffect]> {
  let summary = "swish op";
  let description = [{
    
    Swish Activation Operator.
    
    $$out = \\frac{x}{1 + e^{- \beta \ x}}$$
    
    
  }];
  let arguments = (ins  PD_Tensor:$X, DefaultValuedAttr<F32Attr, "1.0">:$beta);

  let results = (outs PD_Tensor:$Out);
}
def PD_Cross_entropyOp : PD_Op<"cross_entropy", [NoSideEffect]> {
  let summary = "cross_entropy op";
  let description = [{
    
    CrossEntropy Operator.
    
    The input 'X' and 'Label' will first be logically flattened to 2-D matrixs. 
    The matrix's second dimension(row length) is as same as the original last 
    dimension, and the first dimension(column length) is the product of all other 
    original dimensions. Then the softmax computation will take palce on each raw 
    of flattened matrixs.
    
    It supports both standard cross-entropy and soft-label cross-entropy loss
    computation.
    1) One-hot cross-entropy:
        soft_label = false, Label[i, 0] indicates the class index for sample i:
    
                    $Y[i] = -\log(X[i, Label[i]])$
    
    2) Soft-label cross-entropy:
        soft_label = true, Label[i, j] indicates the soft label of class j
        for sample i:
    
                    $Y[i] = \sum_j{-Label[i, j] * log(X[i, j])}$
    
       Please make sure that in this case the summuation of each row of Label
       equals one.
    
    3) One-hot cross-entropy with vecterized Input(Label):
         As a special case of 2), when each row of Input(Label) has only one
         non-zero element (equals 1), soft-label cross-entropy degenerates to a
         one-hot cross-entropy with one-hot label representation.
    
    Both the input X and Label can carry the LoD (Level of Details) information,
    or not. But the output only shares the LoD information with input X.
    
    
  }];
  let arguments = (ins  PD_Tensor:$X, PD_Tensor:$Label, DefaultValuedAttr<BoolAttr, "false">:$soft_label, DefaultValuedAttr<SI32Attr, "-100">:$ignore_index);

  let results = (outs PD_Tensor:$Y);
}
def PD_CholeskyOp : PD_Op<"cholesky", [NoSideEffect]> {
  let summary = "cholesky op";
  let description = [{
    
    Cholesky Operator.
    
    Computes the Cholesky decomposition of one symmetric positive-definite matrix
    or batches of symmetric positive-definite matrices.
    
    
  }];
  let arguments = (ins  PD_Tensor:$X, DefaultValuedAttr<BoolAttr, "false">:$upper);

  let results = (outs PD_Tensor:$Out);
}
def PD_Batch_fcOp : PD_Op<"batch_fc", [NoSideEffect]> {
  let summary = "batch_fc op";
  let description = [{
    
    BatchFC Operator.
    Notice: It currently supports GPU device.
    This Op exists in contrib, which means that it is not shown to the public.
    
  }];
  let arguments = (ins  PD_Tensor:$Input, PD_Tensor:$W, PD_Tensor:$Bias);

  let results = (outs PD_Tensor:$Out);
}
def PD_Nearest_interpOp : PD_Op<"nearest_interp", [NoSideEffect]> {
  let summary = "nearest_interp op";
  let description = [{
    
              This operator samples input X to given output shape by using specified
              interpolation method, the interpolation methods can be \"nearest\"
              for nearest neighbor interpolation and \"bilinear\" for bilinear 
              interpolation and \"linear\" for linear interpolation..
    
              Nearest neighbor interpolation is to perform nearest neighbor interpolation
              in both the 3rd dimension(in height direction) and the 4th dimension(in width 
              direction) on input tensor.
               
              Linear interpolation is the method of using a line connecting two known quantities 
              to determine the value of an unknown quantity between the two known quantities. 
              
              Bilinear interpolation is an extension of linear interpolation for 
              interpolating functions of two variables (e.g. H-direction and 
              W-direction in this op) on a rectilinear 2D grid. The key idea is 
              to perform linear interpolation first in one direction, and then 
              again in the other direction.
    
              Trilinear interpolation is an extension of linear interpolation for 
              interpolating functions of three variables (e.g. D-direction, 
              H-direction and W-direction in this op) on a rectilinear 3D grid. 
              The linear interpolation is performed on three directions.
    
              Bicubic interpolation is an extension of cubic interpolation for interpolating
              data points on a two-dimensional regular grid. The interpolated surface is
              smoother than corresponding surfaces obtained by bilinear interpolation or
              nearest-neighbor interpolation.
    
              Align_corners and align_mode are optional parameters,the calculation method 
              of interpolation can be selected by them.
              
              Example:
    
              For scale:
              
                if align_corners = True and out_{size}>1 :
    
                  scale_{factor} = (in_{size}-1.0)/(out_{size}-1.0)
                
                else:
                  
                  scale_{factor} = float(in_{size}/out_{size})
                
              
              Nearest neighbor interpolation:
              
              if:
                  align_corners = False
    
                  input : (N,C,H_in,W_in)
                  output: (N,C,H_out,W_out) where:
    
                  H_out = \left \lfloor {H_{in} * scale_{}factor}} \right \rfloor
                  W_out = \left \lfloor {W_{in} * scale_{}factor}} \right \rfloor
    
              else:
                  align_corners = True
    
                  input : (N,C,H_in,W_in)
                  output: (N,C,H_out,W_out) where:
    
                  H_out = round(H_{in} * scale_{factor})
                  W_out = round(W_{in} * scale_{factor})
    
              Bilinear interpolation:
    
              if:
                  align_corners = False , align_mode = 0
                  
                  input : (N,C,H_in,W_in)
                  output: (N,C,H_out,W_out) where:
                  
                  H_out = (H_{in}+0.5) * scale_{factor} - 0.5
                  W_out = (W_{in}+0.5) * scale_{factor} - 0.5
    
    
              else:
               
                  input : (N,C,H_in,W_in)
                  output: (N,C,H_out,W_out) where:
    
                  H_out = H_{in} * scale_{factor}
                  W_out = W_{in} * scale_{factor}
    
              Trilinear interpolation:
    
              if:
                  align_corners = False , align_mode = 0
                  
                  input : (N,C,D_in,H_in,W_in)
                  output: (N,C,D_out,H_out,W_out) where:
                  
                  D_out = (D_{in}+0.5) * scale_{factor} - 0.5
                  H_out = (H_{in}+0.5) * scale_{factor} - 0.5
                  W_out = (W_{in}+0.5) * scale_{factor} - 0.5
    
    
              else:
               
                  input : (N,C,D_in,H_in,W_in)
                  output: (N,C,D_out,H_out,W_out) where:
    
                  D_out = D_{in} * scale_{factor}
                  H_out = H_{in} * scale_{factor}
                  W_out = W_{in} * scale_{factor}
    
              Bicubic interpolation:
    
              if:
                  align_corners = False
                  input : (N,C,H_in,W_in)
                  output: (N,C,H_out,W_out) where:
                  H_out = (H_{in}+0.5) * scale_{factor} - 0.5
                  W_out = (W_{in}+0.5) * scale_{factor} - 0.5
              else:
                  input : (N,C,H_in,W_in)
                  output: (N,C,H_out,W_out) where:
                  H_out = H_{in} * scale_{factor}
                  W_out = W_{in} * scale_{factor}
    
              For details of nearest neighbor interpolation, please refer to Wikipedia: 
              https://en.wikipedia.org/wiki/Nearest-neighbor_interpolation
    
              For details of bilinear interpolation, please refer to Wikipedia: 
              https://en.wikipedia.org/wiki/Bilinear_interpolation
    
              For details of trilinear interpolation, please refer to Wikipedia: 
              https://en.wikipedia.org/wiki/Trilinear_interpolation
    
              For details of bicubic interpolation, please refer to Wikipedia:
              https://en.wikipedia.org/wiki/Bicubic_interpolation
             
  }];
  let arguments = (ins  PD_Tensor:$X, PD_Tensor:$OutSize, PD_Tensor:$SizeTensor, PD_Tensor:$Scale, DefaultValuedAttr<StrAttr, "NCHW">:$data_layout, DefaultValuedAttr<SI32Attr, "0">:$out_d, DefaultValuedAttr<SI32Attr, "0">:$out_h, DefaultValuedAttr<SI32Attr, "0">:$out_w, DefaultValuedAttr<F32Attr, "0.0">:$scale, DefaultValuedAttr<StrAttr, "bilinear">:$interp_method, DefaultValuedAttr<BoolAttr, "true">:$align_corners, DefaultValuedAttr<SI32Attr, "1">:$align_mode);

  let results = (outs PD_Tensor:$Out);
}
def PD_GatherOp : PD_Op<"gather", [NoSideEffect]> {
  let summary = "gather op";
  let description = [{
    
    Gather Operator.
    
    $Out = X[Index]$
    
    Out is obtained by gathering entries of the outer-most dimension
    of X indexed by Index and concatenate them together.
    
    Example:
    
    X = [[1, 2],
         [3, 4],
         [5, 6]]
    
    Index = [[1, 2]]
    
    Then:
    
    Out = [[3, 4],
           [5, 6]]
    
    
  }];
  let arguments = (ins  PD_Tensor:$X, PD_Tensor:$Index, PD_Tensor:$Axis, DefaultValuedAttr<SI32Attr, "0">:$axis);

  let results = (outs PD_Tensor:$Out);
}
def PD_Trilinear_interp_v2Op : PD_Op<"trilinear_interp_v2", [NoSideEffect]> {
  let summary = "trilinear_interp_v2 op";
  let description = [{
    
              This operator samples input X to given output shape by using specified
              interpolation method, the interpolation methods can be \"nearest\"
              for nearest neighbor interpolation and \"bilinear\" for bilinear 
              interpolation and \"linear\" for linear interpolation..
    
              Nearest neighbor interpolation is to perform nearest neighbor interpolation
              in both the 3rd dimension(in height direction) and the 4th dimension(in width 
              direction) on input tensor.
               
              Linear interpolation is the method of using a line connecting two known quantities 
              to determine the value of an unknown quantity between the two known quantities. 
              
              Bilinear interpolation is an extension of linear interpolation for 
              interpolating functions of two variables (e.g. H-direction and 
              W-direction in this op) on a rectilinear 2D grid. The key idea is 
              to perform linear interpolation first in one direction, and then 
              again in the other direction.
    
              Trilinear interpolation is an extension of linear interpolation for 
              interpolating functions of three variables (e.g. D-direction, 
              H-direction and W-direction in this op) on a rectilinear 3D grid. 
              The linear interpolation is performed on three directions.
    
              Bicubic interpolation is an extension of cubic interpolation for interpolating
              data points on a two-dimensional regular grid. The interpolated surface is
              smoother than corresponding surfaces obtained by bilinear interpolation or
              nearest-neighbor interpolation.
    
              Align_corners and align_mode are optional parameters,the calculation method 
              of interpolation can be selected by them.
              
              Example:
    
              For scale:
              
                if align_corners = True and out_{size}>1 :
    
                  scale_{factor} = (in_{size}-1.0)/(out_{size}-1.0)
                
                else:
                  
                  scale_{factor} = float(in_{size}/out_{size})
                
              
              Nearest neighbor interpolation:
              
              if:
                  align_corners = False
    
                  input : (N,C,H_in,W_in)
                  output: (N,C,H_out,W_out) where:
    
                  H_out = \left \lfloor {H_{in} * scale_{}factor}} \right \rfloor
                  W_out = \left \lfloor {W_{in} * scale_{}factor}} \right \rfloor
    
              else:
                  align_corners = True
    
                  input : (N,C,H_in,W_in)
                  output: (N,C,H_out,W_out) where:
    
                  H_out = round(H_{in} * scale_{factor})
                  W_out = round(W_{in} * scale_{factor})
    
              Bilinear interpolation:
    
              if:
                  align_corners = False , align_mode = 0
                  
                  input : (N,C,H_in,W_in)
                  output: (N,C,H_out,W_out) where:
                  
                  H_out = (H_{in}+0.5) * scale_{factor} - 0.5
                  W_out = (W_{in}+0.5) * scale_{factor} - 0.5
    
    
              else:
               
                  input : (N,C,H_in,W_in)
                  output: (N,C,H_out,W_out) where:
    
                  H_out = H_{in} * scale_{factor}
                  W_out = W_{in} * scale_{factor}
    
              Trilinear interpolation:
    
              if:
                  align_corners = False , align_mode = 0
                  
                  input : (N,C,D_in,H_in,W_in)
                  output: (N,C,D_out,H_out,W_out) where:
                  
                  D_out = (D_{in}+0.5) * scale_{factor} - 0.5
                  H_out = (H_{in}+0.5) * scale_{factor} - 0.5
                  W_out = (W_{in}+0.5) * scale_{factor} - 0.5
    
    
              else:
               
                  input : (N,C,D_in,H_in,W_in)
                  output: (N,C,D_out,H_out,W_out) where:
    
                  D_out = D_{in} * scale_{factor}
                  H_out = H_{in} * scale_{factor}
                  W_out = W_{in} * scale_{factor}
    
              Bicubic interpolation:
    
              if:
                  align_corners = False
                  input : (N,C,H_in,W_in)
                  output: (N,C,H_out,W_out) where:
                  H_out = (H_{in}+0.5) * scale_{factor} - 0.5
                  W_out = (W_{in}+0.5) * scale_{factor} - 0.5
              else:
                  input : (N,C,H_in,W_in)
                  output: (N,C,H_out,W_out) where:
                  H_out = H_{in} * scale_{factor}
                  W_out = W_{in} * scale_{factor}
    
              For details of nearest neighbor interpolation, please refer to Wikipedia: 
              https://en.wikipedia.org/wiki/Nearest-neighbor_interpolation
    
              For details of bilinear interpolation, please refer to Wikipedia: 
              https://en.wikipedia.org/wiki/Bilinear_interp_v2olation
    
              For details of trilinear interpolation, please refer to Wikipedia: 
              https://en.wikipedia.org/wiki/Trilinear_interp_v2olation
    
              For details of bicubic interpolation, please refer to Wikipedia:
              https://en.wikipedia.org/wiki/Bicubic_interpolation
             
  }];
  let arguments = (ins  PD_Tensor:$X, PD_Tensor:$OutSize, PD_Tensor:$SizeTensor, PD_Tensor:$Scale, DefaultValuedAttr<StrAttr, "NCHW">:$data_layout, DefaultValuedAttr<SI32Attr, "0">:$out_d, DefaultValuedAttr<SI32Attr, "0">:$out_h, DefaultValuedAttr<SI32Attr, "0">:$out_w, DefaultValuedAttr<F32ArrayAttr, "{}">:$scale, DefaultValuedAttr<StrAttr, "bilinear">:$interp_method, DefaultValuedAttr<BoolAttr, "true">:$align_corners, DefaultValuedAttr<SI32Attr, "1">:$align_mode);

  let results = (outs PD_Tensor:$Out);
}
def PD_SoftmaxOp : PD_Op<"softmax", [NoSideEffect]> {
  let summary = "softmax op";
  let description = [{
    
    Softmax Operator.
    
    The input of the softmax operator is a tensor of any rank. The output tensor
    has the same shape as the input.
    
    The dimension :attr:`axis` of the input tensor will be permuted to the last.
    Then the input tensor will be logically flattened to a 2-D matrix. The matrix's
    second dimension(row length) is as same as the dimension :attr:`axis` of the input
    tensor, and the first dimension(column length) is the product of all other
    dimensions of the input tensor. For each row of the matrix, the softmax operator
    squashes the K-dimensional(K is the width of the matrix, which is also the size
    of the input tensor's dimension :attr:`axis`) vector of arbitrary real values to a
    K-dimensional vector of real values in the range [0, 1] that add up to 1.
    It computes the exponential of the given dimension and the sum of exponential
    values of all the other dimensions in the K-dimensional vector input.
    Then the ratio of the exponential of the given dimension and the sum of
    exponential values of all the other dimensions is the output of the softmax
    operator.
    
    For each row $i$ and each column $j$ in the matrix, we have:
        $$Out[i, j] = \frac{\exp(X[i, j])}{\sum_j(exp(X[i, j])}$$
    
    
  }];
  let arguments = (ins  PD_Tensor:$X, DefaultValuedAttr<SI32Attr, "-1">:$axis, DefaultValuedAttr<StrAttr, "AnyLayout">:$data_format);

  let results = (outs PD_Tensor:$Out);
}
def PD_Index_sampleOp : PD_Op<"index_sample", [NoSideEffect]> {
  let summary = "index_sample op";
  let description = [{
    
        IndexSample OP returns the element of the specified location of X, 
        and the location is specified by Index. 
    
        X tensor and Index tensor's shape must be 2-D, 
        dimension at 0 which usually is batch size must be equal.
    
        The returned tensor has the same shape and dimensions as the Index tensor.
        
  }];
  let arguments = (ins  PD_Tensor:$X, PD_Tensor:$Index);

  let results = (outs PD_Tensor:$Out);
}
def PD_Elementwise_minOp : PD_Op<"elementwise_min", [NoSideEffect]> {
  let summary = "elementwise_min op";
  let description = [{
    
    Elementwise Min Operator.
    
    Compare two tensors and returns a new tensor containing the element-wise minima.
    
    The equation is:
    
    $$Out = min(X, Y)$$
    
    - $X$: a tensor of any dimension.
    - $Y$: a tensor whose dimensions must be less than or equal to the dimensions of $X$.
    
    There are two cases for this operator:
    
    1. The shape of $Y$ is the same with $X$.
    2. The shape of $Y$ is a continuous subsequence of $X$.
    
    For case 2:
    
    1. Broadcast $Y$ to match the shape of $X$, where $axis$ is the start dimension index
       for broadcasting $Y$ onto $X$.
    2. If $axis$ is -1 (default), $axis = rank(X) - rank(Y)$.
    3. The trailing dimensions of size 1 for $Y$ will be ignored for the consideration of
       subsequence, such as shape(Y) = (2, 1) => (2).
    
    For example:
    
      .. code-block:: text
    
        shape(X) = (2, 3, 4, 5), shape(Y) = (,)
        shape(X) = (2, 3, 4, 5), shape(Y) = (5,)
        shape(X) = (2, 3, 4, 5), shape(Y) = (4, 5), with axis=-1(default) or axis=2
        shape(X) = (2, 3, 4, 5), shape(Y) = (3, 4), with axis=1
        shape(X) = (2, 3, 4, 5), shape(Y) = (2), with axis=0
        shape(X) = (2, 3, 4, 5), shape(Y) = (2, 1), with axis=0
    
    
  }];
  let arguments = (ins  PD_Tensor:$X, PD_Tensor:$Y, DefaultValuedAttr<SI32Attr, "-1">:$axis);

  let results = (outs PD_Tensor:$Out);
}
def PD_Pixel_shuffleOp : PD_Op<"pixel_shuffle", [NoSideEffect]> {
  let summary = "pixel_shuffle op";
  let description = [{
    
    		Pixel Shuffle operator
    		This operator rearranges elements in a tensor of shape :math:`(*, C \times r^2, H, W)`
        		to a tensor of shape :math:`(C, H \times r, W \times r)`.
    
    		This is useful for implementing efficient sub-pixel convolution
        		with a stride of :math:`1/r`.
    
    		Please refer to the paper:
    		 `Real-Time Single Image and Video Super-Resolution Using an Efficient 
    		 Sub-Pixel Convolutional Neural Network <https://arxiv.org/abs/1609.05158v2>`_
        		by Shi et. al (2016) for more details. 
    
            
  }];
  let arguments = (ins  PD_Tensor:$X, DefaultValuedAttr<SI32Attr, "1">:$upscale_factor, DefaultValuedAttr<StrAttr, "NCHW">:$data_format);

  let results = (outs PD_Tensor:$Out);
}
def PD_Thresholded_reluOp : PD_Op<"thresholded_relu", [NoSideEffect]> {
  let summary = "thresholded_relu op";
  let description = [{
    
    :strong:`ThresholdedRelu activation operator`
    
    ..  math::
    
        out = \begin{cases}
                 x,  \text{if } x > threshold \\
                 0,  \text{otherwise}
              \end{cases}
    
  }];
  let arguments = (ins  PD_Tensor:$X, DefaultValuedAttr<F32Attr, "1.0">:$threshold);

  let results = (outs PD_Tensor:$Out);
}
def PD_WarpctcOp : PD_Op<"warpctc", [NoSideEffect]> {
  let summary = "warpctc op";
  let description = [{
    
    An operator integrating the open-source
    [warp-ctc](https://github.com/baidu-research/warp-ctc) library, which is used in
    [Deep Speech 2: End-toEnd Speech Recognition in English and Mandarin](
    https://arxiv.org/pdf/1512.02595v1.pdf),
    to compute Connectionist Temporal Classification (CTC) loss.
    It can be aliased as softmax with ctc, since a native softmax activation is
    interated to the warp-ctc library, to to normalize values for each row of the
    input tensor.
    
    More detail of CTC loss can be found by referring to
    [Connectionist Temporal Classification: Labelling Unsegmented Sequence Data with
    Recurrent Neural Networks](
    http://machinelearning.wustl.edu/mlpapers/paper_files/icml2006_GravesFGS06.pdf).
    
  }];
  let arguments = (ins  PD_Tensor:$Logits, PD_Tensor:$Label, PD_Tensor:$LogitsLength, PD_Tensor:$LabelLength, DefaultValuedAttr<SI32Attr, "0">:$blank, DefaultValuedAttr<BoolAttr, "false">:$norm_by_times);

  let results = (outs PD_Tensor:$Loss);
}
def PD_SvdOp : PD_Op<"svd", [NoSideEffect]> {
  let summary = "svd op";
  let description = [{
    
    Svd Operator.
    
    This operator is used to perform SVD operation for batched matrics $X$.
    $$U, S, VH = svd(X)$$
    
    
  }];
  let arguments = (ins  PD_Tensor:$X, DefaultValuedAttr<BoolAttr, "false">:$full_matrices);

  let results = (outs PD_Tensor:$U,PD_Tensor:$S,PD_Tensor:$VH);
}
def PD_FeedOp : PD_Op<"feed", [NoSideEffect]> {
  let summary = "Feed Op";

  let description = [{
    Feed a tensor into the model.
  }];

  let arguments = (ins);
  let results = (outs PD_Tensor:$out);

  let assemblyFormat = [{
      `(` `)` attr-dict `:` type($out)
  }];
}

def PD_FetchOp : PD_Op<"fetch", [Terminator]> {
  let summary = "fetch Op";

  let description = [{
    Fetch tensor from the graph.
  }];

  let arguments = (ins Variadic<PD_Tensor>:$inputs);
}

def PD_GraphOp : PD_Op<"graph", [SingleBlockImplicitTerminator<"FetchOp">]> {
  let summary = "paddle graph Op";
  let description = [{
    Describe a paddle graph or subgraph.
  }];
  let regions = (region SizedRegion<1>:$body);
  let arguments = (ins Variadic<PD_Tensor>:$inputs);
  let results = (outs Variadic<PD_Tensor>:$outputs);
}

def PD_ConstantOp : PD_Op<"Constant", [NoSideEffect, ConstantLike, DeclareOpInterfaceMethods<InferTypeOpInterface>, AllTypesMatch<["value", "output"]>]> {
  let summary = "constant Op";
  let description = [{}];

  let arguments = (ins ElementsAttr:$value);
  let results = (outs PD_Tensor:$output);
  let hasFolder = 1;

  let builders = [
    OpBuilder<"OpBuilder &builder, OperationState &state, Attribute value">,
  ];
}

#endif  // PD_OPS
