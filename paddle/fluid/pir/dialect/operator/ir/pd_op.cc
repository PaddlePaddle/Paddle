// This file is generated by "paddle/fluid/pir/dialect/op_generator/op_gen.py"
#include "/home/aistudio/fix_op/Paddle/paddle/fluid/pir/dialect/operator/ir/pd_op.h"
#include "paddle/fluid/pir/dialect/operator/ir/op_type.h"
#include "paddle/fluid/pir/dialect/operator/ir/op_attribute.h"
#include "paddle/fluid/pir/dialect/operator/ir/ir_tensor.h"
#include "paddle/fluid/pir/dialect/operator/ir/ir_selected_rows.h"
#include "paddle/fluid/pir/dialect/operator/ir/ir_meta_tensor.h"
#include "paddle/pir/core/builtin_attribute.h"
#include "paddle/pir/core/builtin_type.h"
#include "paddle/pir/core/builtin_op.h"
#include "paddle/pir/core/ir_context.h"
#include "paddle/phi/core/enforce.h"
#include "paddle/phi/core/dense_tensor.h"
#include "paddle/phi/infermeta/binary.h"
#include "paddle/phi/infermeta/multiary.h"
#include "paddle/phi/infermeta/nullary.h"
#include "paddle/phi/infermeta/unary.h"
#include "paddle/phi/infermeta/ternary.h"
#include "paddle/phi/infermeta/backward.h"
#include "paddle/phi/infermeta/fusion.h"
#include "paddle/phi/api/lib/utils/allocator.h"
#include "paddle/fluid/primitive/rule/vjp/vjp.h"
#include "paddle/pir/core/op_base.h"

namespace paddle {
namespace dialect {

OpInfoTuple AbsOp::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("x", "paddle::dialect::DenseTensorType", false, false, false, true) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = {  };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("out", "paddle::dialect::DenseTensorType", false, false) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("RealAndImagInferMeta", {"x"}, "abs", {"x"}, {"x"}, {}, {}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "abs");
}

void AbsOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_) {
  VLOG(4) << "Start build AbsOp";



  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);
  paddle::dialect::IrTensor dense_out;
  paddle::dialect::IrMetaTensor meta_out(&dense_out);

  phi::RealAndImagInferMeta(meta_x, &meta_out);

  std::vector<pir::Type> argument_outputs;
  pir::Type out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_out.dtype()), dense_out.dims(), dense_out.layout(), dense_out.lod(), dense_out.offset());
  argument_outputs.push_back(out_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void AbsOp::VerifySig() {
  VLOG(4) << "Start Verifying inputs, outputs and attributes for: AbsOp.";
  VLOG(4) << "Verifying inputs:";
  {
  auto input_size = num_operands();
  IR_ENFORCE(input_size == 1u,
                    "The size %d of inputs must be equal to 1.", input_size);
  IR_ENFORCE((*this)->operand_source(0).type().isa<paddle::dialect::DenseTensorType>(),
                  "Type validation failed for the 0th input, got %s.", (*this)->operand_source(0).type());
  }
  VLOG(4) << "Verifying attributes:";
  {
  // Attributes num is 0, not need to check attributes type.
  }
  VLOG(4) << "Verifying outputs:";
  {
  auto output_size = num_results();
  IR_ENFORCE(output_size == 1u,
                    "The size %d of outputs must be equal to 1.", output_size);
  IR_ENFORCE((*this)->result(0).type().isa<paddle::dialect::DenseTensorType>(),
                 "Type validation failed for the 0th output.");
  }
  VLOG(4) << "End Verifying for: AbsOp.";
}

void AbsOp::InferMeta( phi::InferMetaContext *infer_meta ) {
  auto fn = PD_INFER_META(phi::RealAndImagInferMeta);
  fn(infer_meta);
}

phi::DataType AbsOp::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: AbsOp";
  


  return expected_kernel_dtype;
}

bool AbsOp::InferSymbolicShape(pir::ShapeConstraintIRAnalysis* shape_analysis) {
  VLOG(4) << "Infer symbolic shape for op: AbsOp";
  return AbsOpInferSymbolicShape(this->operation(), shape_analysis);
}

OpInfoTuple Abs_Op::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("x", "paddle::dialect::DenseTensorType", false, false, false, true) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = {  };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("out", "paddle::dialect::DenseTensorType", false, false) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("RealAndImagInferMeta", {"x"}, "abs", {"x"}, {"x"}, {}, {{"out", "x"}}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "abs");
}

void Abs_Op::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_) {
  VLOG(4) << "Start build Abs_Op";



  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);
  paddle::dialect::IrTensor dense_out;
  paddle::dialect::IrMetaTensor meta_out(&dense_out);

  phi::RealAndImagInferMeta(meta_x, &meta_out);

  std::vector<pir::Type> argument_outputs;
  pir::Type out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_out.dtype()), dense_out.dims(), dense_out.layout(), dense_out.lod(), dense_out.offset());
  argument_outputs.push_back(out_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void Abs_Op::VerifySig() {
  VLOG(4) << "Start Verifying inputs, outputs and attributes for: Abs_Op.";
  VLOG(4) << "Verifying inputs:";
  {
  auto input_size = num_operands();
  IR_ENFORCE(input_size == 1u,
                    "The size %d of inputs must be equal to 1.", input_size);
  IR_ENFORCE((*this)->operand_source(0).type().isa<paddle::dialect::DenseTensorType>(),
                  "Type validation failed for the 0th input, got %s.", (*this)->operand_source(0).type());
  }
  VLOG(4) << "Verifying attributes:";
  {
  // Attributes num is 0, not need to check attributes type.
  }
  VLOG(4) << "Verifying outputs:";
  {
  auto output_size = num_results();
  IR_ENFORCE(output_size == 1u,
                    "The size %d of outputs must be equal to 1.", output_size);
  IR_ENFORCE((*this)->result(0).type().isa<paddle::dialect::DenseTensorType>(),
                 "Type validation failed for the 0th output.");
  }
  VLOG(4) << "End Verifying for: Abs_Op.";
}

void Abs_Op::InferMeta( phi::InferMetaContext *infer_meta ) {
  auto fn = PD_INFER_META(phi::RealAndImagInferMeta);
  fn(infer_meta);
}

phi::DataType Abs_Op::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: Abs_Op";
  


  return expected_kernel_dtype;
}

bool Abs_Op::InferSymbolicShape(pir::ShapeConstraintIRAnalysis* shape_analysis) {
  VLOG(4) << "Infer symbolic shape for op: Abs_Op";
  return Abs_OpInferSymbolicShape(this->operation(), shape_analysis);
}

OpInfoTuple AccuracyOp::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("x", "paddle::dialect::DenseTensorType", false, false, false, false), paddle::dialect::OpInputInfo("indices", "paddle::dialect::DenseTensorType", false, false, false, false), paddle::dialect::OpInputInfo("label", "paddle::dialect::DenseTensorType", false, false, false, false) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = {  };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("accuracy", "paddle::dialect::DenseTensorType", false, false), paddle::dialect::OpOutputInfo("correct", "paddle::dialect::DenseTensorType", false, false), paddle::dialect::OpOutputInfo("total", "paddle::dialect::DenseTensorType", false, false) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("AccuracyInferMeta", {"x", "indices", "label"}, "accuracy", {"x", "indices", "label"}, {"x"}, {}, {}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "accuracy");
}

void AccuracyOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, pir::Value indices_, pir::Value label_) {
  VLOG(4) << "Start build AccuracyOp";



  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_, indices_, label_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;
  paddle::dialect::DenseTensorType indices = indices_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)indices;
  paddle::dialect::DenseTensorType label = label_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)label;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);

  VLOG(4) << "Builder construction  dense_indices";
  paddle::dialect::IrTensor ir_tensor_indices(paddle::dialect::TransToPhiDataType(indices.dtype()),
                                                      indices.dims(),
                                                      indices.data_layout(),
                                                      indices.lod(),
                                                      indices.offset());
  VLOG(4) << "Builder construction  meta_indices";
  paddle::dialect::IrMetaTensor meta_indices(&ir_tensor_indices);

  VLOG(4) << "Builder construction  dense_label";
  paddle::dialect::IrTensor ir_tensor_label(paddle::dialect::TransToPhiDataType(label.dtype()),
                                                      label.dims(),
                                                      label.data_layout(),
                                                      label.lod(),
                                                      label.offset());
  VLOG(4) << "Builder construction  meta_label";
  paddle::dialect::IrMetaTensor meta_label(&ir_tensor_label);
  paddle::dialect::IrTensor dense_accuracy;
  paddle::dialect::IrMetaTensor meta_accuracy(&dense_accuracy);
  paddle::dialect::IrTensor dense_correct;
  paddle::dialect::IrMetaTensor meta_correct(&dense_correct);
  paddle::dialect::IrTensor dense_total;
  paddle::dialect::IrMetaTensor meta_total(&dense_total);

  phi::AccuracyInferMeta(meta_x, meta_indices, meta_label, &meta_accuracy, &meta_correct, &meta_total);

  std::vector<pir::Type> argument_outputs;
  pir::Type accuracy_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_accuracy.dtype()), dense_accuracy.dims(), dense_accuracy.layout(), dense_accuracy.lod(), dense_accuracy.offset());
  argument_outputs.push_back(accuracy_dense_tensor_type);

  pir::Type correct_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_correct.dtype()), dense_correct.dims(), dense_correct.layout(), dense_correct.lod(), dense_correct.offset());
  argument_outputs.push_back(correct_dense_tensor_type);

  pir::Type total_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_total.dtype()), dense_total.dims(), dense_total.layout(), dense_total.lod(), dense_total.offset());
  argument_outputs.push_back(total_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void AccuracyOp::VerifySig() {
  VLOG(4) << "Start Verifying inputs, outputs and attributes for: AccuracyOp.";
  VLOG(4) << "Verifying inputs:";
  {
  auto input_size = num_operands();
  IR_ENFORCE(input_size == 3u,
                    "The size %d of inputs must be equal to 3.", input_size);
  IR_ENFORCE((*this)->operand_source(0).type().isa<paddle::dialect::DenseTensorType>(),
                  "Type validation failed for the 0th input, got %s.", (*this)->operand_source(0).type());
  IR_ENFORCE((*this)->operand_source(1).type().isa<paddle::dialect::DenseTensorType>(),
                  "Type validation failed for the 1th input, got %s.", (*this)->operand_source(1).type());
  IR_ENFORCE((*this)->operand_source(2).type().isa<paddle::dialect::DenseTensorType>(),
                  "Type validation failed for the 2th input, got %s.", (*this)->operand_source(2).type());
  }
  VLOG(4) << "Verifying attributes:";
  {
  // Attributes num is 0, not need to check attributes type.
  }
  VLOG(4) << "Verifying outputs:";
  {
  auto output_size = num_results();
  IR_ENFORCE(output_size == 3u,
                    "The size %d of outputs must be equal to 3.", output_size);
  IR_ENFORCE((*this)->result(0).type().isa<paddle::dialect::DenseTensorType>(),
                 "Type validation failed for the 0th output.");
  IR_ENFORCE((*this)->result(1).type().isa<paddle::dialect::DenseTensorType>(),
                 "Type validation failed for the 1th output.");
  IR_ENFORCE((*this)->result(2).type().isa<paddle::dialect::DenseTensorType>(),
                 "Type validation failed for the 2th output.");
  }
  VLOG(4) << "End Verifying for: AccuracyOp.";
}

void AccuracyOp::InferMeta( phi::InferMetaContext *infer_meta ) {
  auto fn = PD_INFER_META(phi::AccuracyInferMeta);
  fn(infer_meta);
}

phi::DataType AccuracyOp::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: AccuracyOp";
  


  return expected_kernel_dtype;
}

OpInfoTuple AcosOp::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("x", "paddle::dialect::DenseTensorType", false, false, false, true) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = {  };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("out", "paddle::dialect::DenseTensorType", false, false) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("UnchangedInferMeta", {"x"}, "acos", {"x"}, {}, {}, {}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "acos");
}

void AcosOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_) {
  VLOG(4) << "Start build AcosOp";



  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);
  paddle::dialect::IrTensor dense_out;
  paddle::dialect::IrMetaTensor meta_out(&dense_out);

  phi::UnchangedInferMeta(meta_x, &meta_out);

  std::vector<pir::Type> argument_outputs;
  pir::Type out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_out.dtype()), dense_out.dims(), dense_out.layout(), dense_out.lod(), dense_out.offset());
  argument_outputs.push_back(out_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void AcosOp::VerifySig() {
  VLOG(4) << "Start Verifying inputs, outputs and attributes for: AcosOp.";
  VLOG(4) << "Verifying inputs:";
  {
  auto input_size = num_operands();
  IR_ENFORCE(input_size == 1u,
                    "The size %d of inputs must be equal to 1.", input_size);
  IR_ENFORCE((*this)->operand_source(0).type().isa<paddle::dialect::DenseTensorType>(),
                  "Type validation failed for the 0th input, got %s.", (*this)->operand_source(0).type());
  }
  VLOG(4) << "Verifying attributes:";
  {
  // Attributes num is 0, not need to check attributes type.
  }
  VLOG(4) << "Verifying outputs:";
  {
  auto output_size = num_results();
  IR_ENFORCE(output_size == 1u,
                    "The size %d of outputs must be equal to 1.", output_size);
  IR_ENFORCE((*this)->result(0).type().isa<paddle::dialect::DenseTensorType>(),
                 "Type validation failed for the 0th output.");
  }
  VLOG(4) << "End Verifying for: AcosOp.";
}

void AcosOp::InferMeta( phi::InferMetaContext *infer_meta ) {
  auto fn = PD_INFER_META(phi::UnchangedInferMeta);
  fn(infer_meta);
}

phi::DataType AcosOp::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: AcosOp";
  


  return expected_kernel_dtype;
}

OpInfoTuple Acos_Op::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("x", "paddle::dialect::DenseTensorType", false, false, false, true) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = {  };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("out", "paddle::dialect::DenseTensorType", false, false) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("UnchangedInferMeta", {"x"}, "acos", {"x"}, {}, {}, {{"out", "x"}}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "acos");
}

void Acos_Op::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_) {
  VLOG(4) << "Start build Acos_Op";



  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);
  paddle::dialect::IrTensor dense_out;
  paddle::dialect::IrMetaTensor meta_out(&dense_out);

  phi::UnchangedInferMeta(meta_x, &meta_out);

  std::vector<pir::Type> argument_outputs;
  pir::Type out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_out.dtype()), dense_out.dims(), dense_out.layout(), dense_out.lod(), dense_out.offset());
  argument_outputs.push_back(out_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void Acos_Op::VerifySig() {
  VLOG(4) << "Start Verifying inputs, outputs and attributes for: Acos_Op.";
  VLOG(4) << "Verifying inputs:";
  {
  auto input_size = num_operands();
  IR_ENFORCE(input_size == 1u,
                    "The size %d of inputs must be equal to 1.", input_size);
  IR_ENFORCE((*this)->operand_source(0).type().isa<paddle::dialect::DenseTensorType>(),
                  "Type validation failed for the 0th input, got %s.", (*this)->operand_source(0).type());
  }
  VLOG(4) << "Verifying attributes:";
  {
  // Attributes num is 0, not need to check attributes type.
  }
  VLOG(4) << "Verifying outputs:";
  {
  auto output_size = num_results();
  IR_ENFORCE(output_size == 1u,
                    "The size %d of outputs must be equal to 1.", output_size);
  IR_ENFORCE((*this)->result(0).type().isa<paddle::dialect::DenseTensorType>(),
                 "Type validation failed for the 0th output.");
  }
  VLOG(4) << "End Verifying for: Acos_Op.";
}

void Acos_Op::InferMeta( phi::InferMetaContext *infer_meta ) {
  auto fn = PD_INFER_META(phi::UnchangedInferMeta);
  fn(infer_meta);
}

phi::DataType Acos_Op::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: Acos_Op";
  


  return expected_kernel_dtype;
}

OpInfoTuple AcoshOp::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("x", "paddle::dialect::DenseTensorType", false, false, false, true) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = {  };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("out", "paddle::dialect::DenseTensorType", false, false) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("UnchangedInferMeta", {"x"}, "acosh", {"x"}, {}, {}, {}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "acosh");
}

void AcoshOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_) {
  VLOG(4) << "Start build AcoshOp";



  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);
  paddle::dialect::IrTensor dense_out;
  paddle::dialect::IrMetaTensor meta_out(&dense_out);

  phi::UnchangedInferMeta(meta_x, &meta_out);

  std::vector<pir::Type> argument_outputs;
  pir::Type out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_out.dtype()), dense_out.dims(), dense_out.layout(), dense_out.lod(), dense_out.offset());
  argument_outputs.push_back(out_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void AcoshOp::VerifySig() {
  VLOG(4) << "Start Verifying inputs, outputs and attributes for: AcoshOp.";
  VLOG(4) << "Verifying inputs:";
  {
  auto input_size = num_operands();
  IR_ENFORCE(input_size == 1u,
                    "The size %d of inputs must be equal to 1.", input_size);
  IR_ENFORCE((*this)->operand_source(0).type().isa<paddle::dialect::DenseTensorType>(),
                  "Type validation failed for the 0th input, got %s.", (*this)->operand_source(0).type());
  }
  VLOG(4) << "Verifying attributes:";
  {
  // Attributes num is 0, not need to check attributes type.
  }
  VLOG(4) << "Verifying outputs:";
  {
  auto output_size = num_results();
  IR_ENFORCE(output_size == 1u,
                    "The size %d of outputs must be equal to 1.", output_size);
  IR_ENFORCE((*this)->result(0).type().isa<paddle::dialect::DenseTensorType>(),
                 "Type validation failed for the 0th output.");
  }
  VLOG(4) << "End Verifying for: AcoshOp.";
}

void AcoshOp::InferMeta( phi::InferMetaContext *infer_meta ) {
  auto fn = PD_INFER_META(phi::UnchangedInferMeta);
  fn(infer_meta);
}

phi::DataType AcoshOp::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: AcoshOp";
  


  return expected_kernel_dtype;
}

OpInfoTuple Acosh_Op::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("x", "paddle::dialect::DenseTensorType", false, false, false, true) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = {  };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("out", "paddle::dialect::DenseTensorType", false, false) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("UnchangedInferMeta", {"x"}, "acosh", {"x"}, {}, {}, {{"out", "x"}}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "acosh");
}

void Acosh_Op::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_) {
  VLOG(4) << "Start build Acosh_Op";



  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);
  paddle::dialect::IrTensor dense_out;
  paddle::dialect::IrMetaTensor meta_out(&dense_out);

  phi::UnchangedInferMeta(meta_x, &meta_out);

  std::vector<pir::Type> argument_outputs;
  pir::Type out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_out.dtype()), dense_out.dims(), dense_out.layout(), dense_out.lod(), dense_out.offset());
  argument_outputs.push_back(out_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void Acosh_Op::VerifySig() {
  VLOG(4) << "Start Verifying inputs, outputs and attributes for: Acosh_Op.";
  VLOG(4) << "Verifying inputs:";
  {
  auto input_size = num_operands();
  IR_ENFORCE(input_size == 1u,
                    "The size %d of inputs must be equal to 1.", input_size);
  IR_ENFORCE((*this)->operand_source(0).type().isa<paddle::dialect::DenseTensorType>(),
                  "Type validation failed for the 0th input, got %s.", (*this)->operand_source(0).type());
  }
  VLOG(4) << "Verifying attributes:";
  {
  // Attributes num is 0, not need to check attributes type.
  }
  VLOG(4) << "Verifying outputs:";
  {
  auto output_size = num_results();
  IR_ENFORCE(output_size == 1u,
                    "The size %d of outputs must be equal to 1.", output_size);
  IR_ENFORCE((*this)->result(0).type().isa<paddle::dialect::DenseTensorType>(),
                 "Type validation failed for the 0th output.");
  }
  VLOG(4) << "End Verifying for: Acosh_Op.";
}

void Acosh_Op::InferMeta( phi::InferMetaContext *infer_meta ) {
  auto fn = PD_INFER_META(phi::UnchangedInferMeta);
  fn(infer_meta);
}

phi::DataType Acosh_Op::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: Acosh_Op";
  


  return expected_kernel_dtype;
}

const char *Adagrad_Op::attributes_name[2] = { "epsilon", "multi_precision" };

OpInfoTuple Adagrad_Op::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("param", "paddle::dialect::DenseTensorType", false, false, false, false), paddle::dialect::OpInputInfo("grad", "paddle::dialect::DenseTensorType", false, false, false, false), paddle::dialect::OpInputInfo("moment", "paddle::dialect::DenseTensorType", false, false, false, false), paddle::dialect::OpInputInfo("learning_rate", "paddle::dialect::DenseTensorType", false, false, false, false), paddle::dialect::OpInputInfo("master_param", "paddle::dialect::DenseTensorType", true, false, false, false) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = { paddle::dialect::OpAttributeInfo("epsilon", "pir::FloatAttribute", ""), paddle::dialect::OpAttributeInfo("multi_precision", "pir::BoolAttribute", "") };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("param_out", "paddle::dialect::DenseTensorType", false, false), paddle::dialect::OpOutputInfo("moment_out", "paddle::dialect::DenseTensorType", false, false), paddle::dialect::OpOutputInfo("master_param_out", "paddle::dialect::DenseTensorType", true, false) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("AdagradInferMeta", {"param", "grad", "moment", "learning_rate", "master_param", "epsilon", "multi_precision"}, "adagrad", {"param", "grad", "moment", "learning_rate", "master_param", "epsilon", "multi_precision"}, {"param"}, {}, {{"param_out", "param"},{"moment_out", "moment"},{"master_param_out", "master_param"}}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "adagrad_");
}

void Adagrad_Op::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value param_, pir::Value grad_, pir::Value moment_, pir::Value learning_rate_, pir::Value master_param_, float epsilon, bool multi_precision) {
  VLOG(4) << "Start build Adagrad_Op";



  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {param_, grad_, moment_, learning_rate_, master_param_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_epsilon = pir::FloatAttribute::get(pir::IrContext::Instance(), epsilon);
  argument.AddAttribute("epsilon", attr_epsilon);
  pir::Attribute attr_multi_precision = pir::BoolAttribute::get(pir::IrContext::Instance(), multi_precision);
  argument.AddAttribute("multi_precision", attr_multi_precision);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType param = param_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)param;
  paddle::dialect::DenseTensorType grad = grad_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)grad;
  paddle::dialect::DenseTensorType moment = moment_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)moment;
  paddle::dialect::DenseTensorType learning_rate = learning_rate_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)learning_rate;

  VLOG(4) << "Builder construction  dense_param";
  paddle::dialect::IrTensor ir_tensor_param(paddle::dialect::TransToPhiDataType(param.dtype()),
                                                      param.dims(),
                                                      param.data_layout(),
                                                      param.lod(),
                                                      param.offset());
  VLOG(4) << "Builder construction  meta_param";
  paddle::dialect::IrMetaTensor meta_param(&ir_tensor_param);

  VLOG(4) << "Builder construction  dense_grad";
  paddle::dialect::IrTensor ir_tensor_grad(paddle::dialect::TransToPhiDataType(grad.dtype()),
                                                      grad.dims(),
                                                      grad.data_layout(),
                                                      grad.lod(),
                                                      grad.offset());
  VLOG(4) << "Builder construction  meta_grad";
  paddle::dialect::IrMetaTensor meta_grad(&ir_tensor_grad);

  VLOG(4) << "Builder construction  dense_moment";
  paddle::dialect::IrTensor ir_tensor_moment(paddle::dialect::TransToPhiDataType(moment.dtype()),
                                                      moment.dims(),
                                                      moment.data_layout(),
                                                      moment.lod(),
                                                      moment.offset());
  VLOG(4) << "Builder construction  meta_moment";
  paddle::dialect::IrMetaTensor meta_moment(&ir_tensor_moment);

  VLOG(4) << "Builder construction  dense_learning_rate";
  paddle::dialect::IrTensor ir_tensor_learning_rate(paddle::dialect::TransToPhiDataType(learning_rate.dtype()),
                                                      learning_rate.dims(),
                                                      learning_rate.data_layout(),
                                                      learning_rate.lod(),
                                                      learning_rate.offset());
  VLOG(4) << "Builder construction  meta_learning_rate";
  paddle::dialect::IrMetaTensor meta_learning_rate(&ir_tensor_learning_rate);

  paddle::dialect::IrMetaTensor meta_master_param;
  paddle::dialect::IrTensor ir_tensor_master_param;
  if (master_param_.impl() != nullptr) {
    paddle::dialect::DenseTensorType master_param = master_param_.type().dyn_cast<paddle::dialect::DenseTensorType>();
    VLOG(4) << "Builder construction  dense_master_param";
    ir_tensor_master_param = paddle::dialect::IrTensor(paddle::dialect::TransToPhiDataType(master_param.dtype()),
                                                        master_param.dims(),
                                                        master_param.data_layout(),
                                                        master_param.lod(),
                                                        master_param.offset());
    VLOG(4) << "Builder construction  meta_master_param";
    meta_master_param = paddle::dialect::IrMetaTensor(&ir_tensor_master_param);
  }

  paddle::dialect::IrTensor dense_param_out;
  paddle::dialect::IrMetaTensor meta_param_out(&dense_param_out);
  paddle::dialect::IrTensor dense_moment_out;
  paddle::dialect::IrMetaTensor meta_moment_out(&dense_moment_out);
  paddle::dialect::IrTensor dense_master_param_out;
  paddle::dialect::IrMetaTensor meta_master_param_out(&dense_master_param_out);

  phi::AdagradInferMeta(meta_param, meta_grad, meta_moment, meta_learning_rate, meta_master_param, epsilon, multi_precision, &meta_param_out, &meta_moment_out, &meta_master_param_out);

  std::vector<pir::Type> argument_outputs;
  pir::Type param_out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_param_out.dtype()), dense_param_out.dims(), dense_param_out.layout(), dense_param_out.lod(), dense_param_out.offset());
  argument_outputs.push_back(param_out_dense_tensor_type);

  pir::Type moment_out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_moment_out.dtype()), dense_moment_out.dims(), dense_moment_out.layout(), dense_moment_out.lod(), dense_moment_out.offset());
  argument_outputs.push_back(moment_out_dense_tensor_type);

  if (master_param_.impl() != nullptr) {
    pir::Type master_param_out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_master_param_out.dtype()), dense_master_param_out.dims(), dense_master_param_out.layout(), dense_master_param_out.lod(), dense_master_param_out.offset());
    argument_outputs.push_back(master_param_out_dense_tensor_type);
  } else {
    pir::Type master_param_out_type;
    argument_outputs.push_back(master_param_out_type);
  }

  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void Adagrad_Op::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value param_, pir::Value grad_, pir::Value moment_, pir::Value learning_rate_, pir::Value master_param_, pir::AttributeMap attributes) {
  VLOG(4) << "Start build Adagrad_Op";


  IR_ENFORCE(
      attributes.find("epsilon") != attributes.end(),
          "'epsilon' Attribute is expected for Adagrad_Op. ");
  float epsilon = attributes.at("epsilon").dyn_cast<pir::FloatAttribute>().data();

  IR_ENFORCE(
      attributes.find("multi_precision") != attributes.end(),
          "'multi_precision' Attribute is expected for Adagrad_Op. ");
  bool multi_precision = attributes.at("multi_precision").dyn_cast<pir::BoolAttribute>().data();


  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {param_, grad_, moment_, learning_rate_, master_param_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_epsilon = pir::FloatAttribute::get(pir::IrContext::Instance(), epsilon);
  argument.AddAttribute("epsilon", attr_epsilon);
  pir::Attribute attr_multi_precision = pir::BoolAttribute::get(pir::IrContext::Instance(), multi_precision);
  argument.AddAttribute("multi_precision", attr_multi_precision);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType param = param_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)param;
  paddle::dialect::DenseTensorType grad = grad_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)grad;
  paddle::dialect::DenseTensorType moment = moment_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)moment;
  paddle::dialect::DenseTensorType learning_rate = learning_rate_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)learning_rate;

  VLOG(4) << "Builder construction  dense_param";
  paddle::dialect::IrTensor ir_tensor_param(paddle::dialect::TransToPhiDataType(param.dtype()),
                                                      param.dims(),
                                                      param.data_layout(),
                                                      param.lod(),
                                                      param.offset());
  VLOG(4) << "Builder construction  meta_param";
  paddle::dialect::IrMetaTensor meta_param(&ir_tensor_param);

  VLOG(4) << "Builder construction  dense_grad";
  paddle::dialect::IrTensor ir_tensor_grad(paddle::dialect::TransToPhiDataType(grad.dtype()),
                                                      grad.dims(),
                                                      grad.data_layout(),
                                                      grad.lod(),
                                                      grad.offset());
  VLOG(4) << "Builder construction  meta_grad";
  paddle::dialect::IrMetaTensor meta_grad(&ir_tensor_grad);

  VLOG(4) << "Builder construction  dense_moment";
  paddle::dialect::IrTensor ir_tensor_moment(paddle::dialect::TransToPhiDataType(moment.dtype()),
                                                      moment.dims(),
                                                      moment.data_layout(),
                                                      moment.lod(),
                                                      moment.offset());
  VLOG(4) << "Builder construction  meta_moment";
  paddle::dialect::IrMetaTensor meta_moment(&ir_tensor_moment);

  VLOG(4) << "Builder construction  dense_learning_rate";
  paddle::dialect::IrTensor ir_tensor_learning_rate(paddle::dialect::TransToPhiDataType(learning_rate.dtype()),
                                                      learning_rate.dims(),
                                                      learning_rate.data_layout(),
                                                      learning_rate.lod(),
                                                      learning_rate.offset());
  VLOG(4) << "Builder construction  meta_learning_rate";
  paddle::dialect::IrMetaTensor meta_learning_rate(&ir_tensor_learning_rate);

  paddle::dialect::IrMetaTensor meta_master_param;
  paddle::dialect::IrTensor ir_tensor_master_param;
  if (master_param_.impl() != nullptr) {
    paddle::dialect::DenseTensorType master_param = master_param_.type().dyn_cast<paddle::dialect::DenseTensorType>();
    VLOG(4) << "Builder construction  dense_master_param";
    ir_tensor_master_param = paddle::dialect::IrTensor(paddle::dialect::TransToPhiDataType(master_param.dtype()),
                                                        master_param.dims(),
                                                        master_param.data_layout(),
                                                        master_param.lod(),
                                                        master_param.offset());
    VLOG(4) << "Builder construction  meta_master_param";
    meta_master_param = paddle::dialect::IrMetaTensor(&ir_tensor_master_param);
  }

  paddle::dialect::IrTensor dense_param_out;
  paddle::dialect::IrMetaTensor meta_param_out(&dense_param_out);
  paddle::dialect::IrTensor dense_moment_out;
  paddle::dialect::IrMetaTensor meta_moment_out(&dense_moment_out);
  paddle::dialect::IrTensor dense_master_param_out;
  paddle::dialect::IrMetaTensor meta_master_param_out(&dense_master_param_out);

  phi::AdagradInferMeta(meta_param, meta_grad, meta_moment, meta_learning_rate, meta_master_param, epsilon, multi_precision, &meta_param_out, &meta_moment_out, &meta_master_param_out);

  std::vector<pir::Type> argument_outputs;
  pir::Type param_out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_param_out.dtype()), dense_param_out.dims(), dense_param_out.layout(), dense_param_out.lod(), dense_param_out.offset());
  argument_outputs.push_back(param_out_dense_tensor_type);

  pir::Type moment_out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_moment_out.dtype()), dense_moment_out.dims(), dense_moment_out.layout(), dense_moment_out.lod(), dense_moment_out.offset());
  argument_outputs.push_back(moment_out_dense_tensor_type);

  if (master_param_.impl() != nullptr) {
    pir::Type master_param_out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_master_param_out.dtype()), dense_master_param_out.dims(), dense_master_param_out.layout(), dense_master_param_out.lod(), dense_master_param_out.offset());
    argument_outputs.push_back(master_param_out_dense_tensor_type);
  } else {
    pir::Type master_param_out_type;
    argument_outputs.push_back(master_param_out_type);
  }

  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void Adagrad_Op::VerifySig() {
  VLOG(4) << "Start Verifying inputs, outputs and attributes for: Adagrad_Op.";
  VLOG(4) << "Verifying inputs:";
  {
  auto input_size = num_operands();
  IR_ENFORCE(input_size == 5u,
                    "The size %d of inputs must be equal to 5.", input_size);
  IR_ENFORCE((*this)->operand_source(0).type().isa<paddle::dialect::DenseTensorType>(),
                  "Type validation failed for the 0th input, got %s.", (*this)->operand_source(0).type());
  IR_ENFORCE((*this)->operand_source(1).type().isa<paddle::dialect::DenseTensorType>(),
                  "Type validation failed for the 1th input, got %s.", (*this)->operand_source(1).type());
  IR_ENFORCE((*this)->operand_source(2).type().isa<paddle::dialect::DenseTensorType>(),
                  "Type validation failed for the 2th input, got %s.", (*this)->operand_source(2).type());
  IR_ENFORCE((*this)->operand_source(3).type().isa<paddle::dialect::DenseTensorType>(),
                  "Type validation failed for the 3th input, got %s.", (*this)->operand_source(3).type());
  if (auto val = (*this)->operand(4)) {
    IR_ENFORCE(val.type().isa<paddle::dialect::DenseTensorType>(),
                   "Type validation failed for the 4th input, got %s.", (*this)->operand_source(4).type());
  }
  }
  VLOG(4) << "Verifying attributes:";
  {
  auto& attributes = this->attributes();
  IR_ENFORCE(attributes.count("epsilon")>0,
                 "epsilon does not exist.");
  IR_ENFORCE(attributes.at("epsilon").isa<pir::FloatAttribute>(),
                 "Type of attribute: epsilon is not pir::FloatAttribute.");

  IR_ENFORCE(attributes.count("multi_precision")>0,
                 "multi_precision does not exist.");
  IR_ENFORCE(attributes.at("multi_precision").isa<pir::BoolAttribute>(),
                 "Type of attribute: multi_precision is not pir::BoolAttribute.");

  }
  VLOG(4) << "Verifying outputs:";
  {
  auto output_size = num_results();
  IR_ENFORCE(output_size == 3u,
                    "The size %d of outputs must be equal to 3.", output_size);
  IR_ENFORCE((*this)->result(0).type().isa<paddle::dialect::DenseTensorType>(),
                 "Type validation failed for the 0th output.");
  IR_ENFORCE((*this)->result(1).type().isa<paddle::dialect::DenseTensorType>(),
                 "Type validation failed for the 1th output.");
  if (auto output_2_type = (*this)->result(2).type()) {
    IR_ENFORCE(output_2_type.isa<paddle::dialect::DenseTensorType>(),
                   "Type validation failed for the 2th output.");
  }
  }
  VLOG(4) << "End Verifying for: Adagrad_Op.";
}

void Adagrad_Op::InferMeta( phi::InferMetaContext *infer_meta ) {
  auto fn = PD_INFER_META(phi::AdagradInferMeta);
  fn(infer_meta);
}

phi::DataType Adagrad_Op::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: Adagrad_Op";
  


  return expected_kernel_dtype;
}

const char *AdagradDenseParamSparseGrad_Op::attributes_name[2] = { "epsilon", "multi_precision" };

OpInfoTuple AdagradDenseParamSparseGrad_Op::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("param", "paddle::dialect::DenseTensorType", false, false, false, false), paddle::dialect::OpInputInfo("grad", "paddle::dialect::SelectedRowsType", false, false, false, false), paddle::dialect::OpInputInfo("moment", "paddle::dialect::DenseTensorType", false, false, false, false), paddle::dialect::OpInputInfo("learning_rate", "paddle::dialect::DenseTensorType", false, false, false, false), paddle::dialect::OpInputInfo("master_param", "paddle::dialect::DenseTensorType", true, false, false, false) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = { paddle::dialect::OpAttributeInfo("epsilon", "pir::FloatAttribute", ""), paddle::dialect::OpAttributeInfo("multi_precision", "pir::BoolAttribute", "") };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("param_out", "paddle::dialect::DenseTensorType", false, false), paddle::dialect::OpOutputInfo("moment_out", "paddle::dialect::DenseTensorType", false, false), paddle::dialect::OpOutputInfo("master_param_out", "paddle::dialect::DenseTensorType", true, false) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("AdagradInferMeta", {"param", "grad", "moment", "learning_rate", "master_param", "epsilon", "multi_precision"}, "adagrad_dense_param_sparse_grad", {"param", "grad", "moment", "learning_rate", "master_param", "epsilon", "multi_precision"}, {"param"}, {}, {{"param_out", "param"},{"moment_out", "moment"},{"master_param_out", "master_param"}}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "adagrad_");
}

void AdagradDenseParamSparseGrad_Op::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value param_, pir::Value grad_, pir::Value moment_, pir::Value learning_rate_, pir::Value master_param_, float epsilon, bool multi_precision) {
  VLOG(4) << "Start build AdagradDenseParamSparseGrad_Op";



  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {param_, grad_, moment_, learning_rate_, master_param_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_epsilon = pir::FloatAttribute::get(pir::IrContext::Instance(), epsilon);
  argument.AddAttribute("epsilon", attr_epsilon);
  pir::Attribute attr_multi_precision = pir::BoolAttribute::get(pir::IrContext::Instance(), multi_precision);
  argument.AddAttribute("multi_precision", attr_multi_precision);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType param = param_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)param;
  paddle::dialect::SelectedRowsType grad = grad_.type().dyn_cast<paddle::dialect::SelectedRowsType>(); (void)grad;
  paddle::dialect::DenseTensorType moment = moment_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)moment;
  paddle::dialect::DenseTensorType learning_rate = learning_rate_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)learning_rate;

  VLOG(4) << "Builder construction  dense_param";
  paddle::dialect::IrTensor ir_tensor_param(paddle::dialect::TransToPhiDataType(param.dtype()),
                                                      param.dims(),
                                                      param.data_layout(),
                                                      param.lod(),
                                                      param.offset());
  VLOG(4) << "Builder construction  meta_param";
  paddle::dialect::IrMetaTensor meta_param(&ir_tensor_param);

  VLOG(4) << "Builder construction  dense_grad";
  paddle::dialect::IrTensor ir_tensor_grad(paddle::dialect::TransToPhiDataType(grad.dtype()),
                                                      grad.dims(),
                                                      grad.data_layout(),
                                                      grad.lod(),
                                                      grad.offset());
  VLOG(4) << "Builder construction  meta_grad";
  paddle::dialect::IrMetaTensor meta_grad(&ir_tensor_grad);

  VLOG(4) << "Builder construction  dense_moment";
  paddle::dialect::IrTensor ir_tensor_moment(paddle::dialect::TransToPhiDataType(moment.dtype()),
                                                      moment.dims(),
                                                      moment.data_layout(),
                                                      moment.lod(),
                                                      moment.offset());
  VLOG(4) << "Builder construction  meta_moment";
  paddle::dialect::IrMetaTensor meta_moment(&ir_tensor_moment);

  VLOG(4) << "Builder construction  dense_learning_rate";
  paddle::dialect::IrTensor ir_tensor_learning_rate(paddle::dialect::TransToPhiDataType(learning_rate.dtype()),
                                                      learning_rate.dims(),
                                                      learning_rate.data_layout(),
                                                      learning_rate.lod(),
                                                      learning_rate.offset());
  VLOG(4) << "Builder construction  meta_learning_rate";
  paddle::dialect::IrMetaTensor meta_learning_rate(&ir_tensor_learning_rate);

  paddle::dialect::IrMetaTensor meta_master_param;
  paddle::dialect::IrTensor ir_tensor_master_param;
  if (master_param_.impl() != nullptr) {
    paddle::dialect::DenseTensorType master_param = master_param_.type().dyn_cast<paddle::dialect::DenseTensorType>();
    VLOG(4) << "Builder construction  dense_master_param";
    ir_tensor_master_param = paddle::dialect::IrTensor(paddle::dialect::TransToPhiDataType(master_param.dtype()),
                                                        master_param.dims(),
                                                        master_param.data_layout(),
                                                        master_param.lod(),
                                                        master_param.offset());
    VLOG(4) << "Builder construction  meta_master_param";
    meta_master_param = paddle::dialect::IrMetaTensor(&ir_tensor_master_param);
  }

  paddle::dialect::IrTensor dense_param_out;
  paddle::dialect::IrMetaTensor meta_param_out(&dense_param_out);
  paddle::dialect::IrTensor dense_moment_out;
  paddle::dialect::IrMetaTensor meta_moment_out(&dense_moment_out);
  paddle::dialect::IrTensor dense_master_param_out;
  paddle::dialect::IrMetaTensor meta_master_param_out(&dense_master_param_out);

  phi::AdagradInferMeta(meta_param, meta_grad, meta_moment, meta_learning_rate, meta_master_param, epsilon, multi_precision, &meta_param_out, &meta_moment_out, &meta_master_param_out);

  std::vector<pir::Type> argument_outputs;
  pir::Type param_out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_param_out.dtype()), dense_param_out.dims(), dense_param_out.layout(), dense_param_out.lod(), dense_param_out.offset());
  argument_outputs.push_back(param_out_dense_tensor_type);

  pir::Type moment_out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_moment_out.dtype()), dense_moment_out.dims(), dense_moment_out.layout(), dense_moment_out.lod(), dense_moment_out.offset());
  argument_outputs.push_back(moment_out_dense_tensor_type);

  if (master_param_.impl() != nullptr) {
    pir::Type master_param_out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_master_param_out.dtype()), dense_master_param_out.dims(), dense_master_param_out.layout(), dense_master_param_out.lod(), dense_master_param_out.offset());
    argument_outputs.push_back(master_param_out_dense_tensor_type);
  } else {
    pir::Type master_param_out_type;
    argument_outputs.push_back(master_param_out_type);
  }

  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void AdagradDenseParamSparseGrad_Op::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value param_, pir::Value grad_, pir::Value moment_, pir::Value learning_rate_, pir::Value master_param_, pir::AttributeMap attributes) {
  VLOG(4) << "Start build AdagradDenseParamSparseGrad_Op";


  IR_ENFORCE(
      attributes.find("epsilon") != attributes.end(),
          "'epsilon' Attribute is expected for AdagradDenseParamSparseGrad_Op. ");
  float epsilon = attributes.at("epsilon").dyn_cast<pir::FloatAttribute>().data();

  IR_ENFORCE(
      attributes.find("multi_precision") != attributes.end(),
          "'multi_precision' Attribute is expected for AdagradDenseParamSparseGrad_Op. ");
  bool multi_precision = attributes.at("multi_precision").dyn_cast<pir::BoolAttribute>().data();


  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {param_, grad_, moment_, learning_rate_, master_param_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_epsilon = pir::FloatAttribute::get(pir::IrContext::Instance(), epsilon);
  argument.AddAttribute("epsilon", attr_epsilon);
  pir::Attribute attr_multi_precision = pir::BoolAttribute::get(pir::IrContext::Instance(), multi_precision);
  argument.AddAttribute("multi_precision", attr_multi_precision);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType param = param_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)param;
  paddle::dialect::SelectedRowsType grad = grad_.type().dyn_cast<paddle::dialect::SelectedRowsType>(); (void)grad;
  paddle::dialect::DenseTensorType moment = moment_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)moment;
  paddle::dialect::DenseTensorType learning_rate = learning_rate_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)learning_rate;

  VLOG(4) << "Builder construction  dense_param";
  paddle::dialect::IrTensor ir_tensor_param(paddle::dialect::TransToPhiDataType(param.dtype()),
                                                      param.dims(),
                                                      param.data_layout(),
                                                      param.lod(),
                                                      param.offset());
  VLOG(4) << "Builder construction  meta_param";
  paddle::dialect::IrMetaTensor meta_param(&ir_tensor_param);

  VLOG(4) << "Builder construction  dense_grad";
  paddle::dialect::IrTensor ir_tensor_grad(paddle::dialect::TransToPhiDataType(grad.dtype()),
                                                      grad.dims(),
                                                      grad.data_layout(),
                                                      grad.lod(),
                                                      grad.offset());
  VLOG(4) << "Builder construction  meta_grad";
  paddle::dialect::IrMetaTensor meta_grad(&ir_tensor_grad);

  VLOG(4) << "Builder construction  dense_moment";
  paddle::dialect::IrTensor ir_tensor_moment(paddle::dialect::TransToPhiDataType(moment.dtype()),
                                                      moment.dims(),
                                                      moment.data_layout(),
                                                      moment.lod(),
                                                      moment.offset());
  VLOG(4) << "Builder construction  meta_moment";
  paddle::dialect::IrMetaTensor meta_moment(&ir_tensor_moment);

  VLOG(4) << "Builder construction  dense_learning_rate";
  paddle::dialect::IrTensor ir_tensor_learning_rate(paddle::dialect::TransToPhiDataType(learning_rate.dtype()),
                                                      learning_rate.dims(),
                                                      learning_rate.data_layout(),
                                                      learning_rate.lod(),
                                                      learning_rate.offset());
  VLOG(4) << "Builder construction  meta_learning_rate";
  paddle::dialect::IrMetaTensor meta_learning_rate(&ir_tensor_learning_rate);

  paddle::dialect::IrMetaTensor meta_master_param;
  paddle::dialect::IrTensor ir_tensor_master_param;
  if (master_param_.impl() != nullptr) {
    paddle::dialect::DenseTensorType master_param = master_param_.type().dyn_cast<paddle::dialect::DenseTensorType>();
    VLOG(4) << "Builder construction  dense_master_param";
    ir_tensor_master_param = paddle::dialect::IrTensor(paddle::dialect::TransToPhiDataType(master_param.dtype()),
                                                        master_param.dims(),
                                                        master_param.data_layout(),
                                                        master_param.lod(),
                                                        master_param.offset());
    VLOG(4) << "Builder construction  meta_master_param";
    meta_master_param = paddle::dialect::IrMetaTensor(&ir_tensor_master_param);
  }

  paddle::dialect::IrTensor dense_param_out;
  paddle::dialect::IrMetaTensor meta_param_out(&dense_param_out);
  paddle::dialect::IrTensor dense_moment_out;
  paddle::dialect::IrMetaTensor meta_moment_out(&dense_moment_out);
  paddle::dialect::IrTensor dense_master_param_out;
  paddle::dialect::IrMetaTensor meta_master_param_out(&dense_master_param_out);

  phi::AdagradInferMeta(meta_param, meta_grad, meta_moment, meta_learning_rate, meta_master_param, epsilon, multi_precision, &meta_param_out, &meta_moment_out, &meta_master_param_out);

  std::vector<pir::Type> argument_outputs;
  pir::Type param_out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_param_out.dtype()), dense_param_out.dims(), dense_param_out.layout(), dense_param_out.lod(), dense_param_out.offset());
  argument_outputs.push_back(param_out_dense_tensor_type);

  pir::Type moment_out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_moment_out.dtype()), dense_moment_out.dims(), dense_moment_out.layout(), dense_moment_out.lod(), dense_moment_out.offset());
  argument_outputs.push_back(moment_out_dense_tensor_type);

  if (master_param_.impl() != nullptr) {
    pir::Type master_param_out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_master_param_out.dtype()), dense_master_param_out.dims(), dense_master_param_out.layout(), dense_master_param_out.lod(), dense_master_param_out.offset());
    argument_outputs.push_back(master_param_out_dense_tensor_type);
  } else {
    pir::Type master_param_out_type;
    argument_outputs.push_back(master_param_out_type);
  }

  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void AdagradDenseParamSparseGrad_Op::VerifySig() {}

void AdagradDenseParamSparseGrad_Op::InferMeta( phi::InferMetaContext *infer_meta ) {
  auto fn = PD_INFER_META(phi::AdagradInferMeta);
  fn(infer_meta);
}

phi::DataType AdagradDenseParamSparseGrad_Op::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: AdagradDenseParamSparseGrad_Op";
  


  return expected_kernel_dtype;
}

const char *Adam_Op::attributes_name[4] = { "lazy_mode", "min_row_size_to_use_multithread", "multi_precision", "use_global_beta_pow" };

OpInfoTuple Adam_Op::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("param", "paddle::dialect::DenseTensorType", false, false, false, false), paddle::dialect::OpInputInfo("grad", "paddle::dialect::DenseTensorType", false, false, false, false), paddle::dialect::OpInputInfo("learning_rate", "paddle::dialect::DenseTensorType", false, false, false, false), paddle::dialect::OpInputInfo("moment1", "paddle::dialect::DenseTensorType", false, false, false, false), paddle::dialect::OpInputInfo("moment2", "paddle::dialect::DenseTensorType", false, false, false, false), paddle::dialect::OpInputInfo("beta1_pow", "paddle::dialect::DenseTensorType", false, false, false, false), paddle::dialect::OpInputInfo("beta2_pow", "paddle::dialect::DenseTensorType", false, false, false, false), paddle::dialect::OpInputInfo("master_param", "paddle::dialect::DenseTensorType", true, false, false, false), paddle::dialect::OpInputInfo("skip_update", "paddle::dialect::DenseTensorType", true, false, false, false), paddle::dialect::OpInputInfo("beta1", "paddle::dialect::ScalarAttribute", false, false, true, false), paddle::dialect::OpInputInfo("beta2", "paddle::dialect::ScalarAttribute", false, false, true, false), paddle::dialect::OpInputInfo("epsilon", "paddle::dialect::ScalarAttribute", false, false, true, false) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = { paddle::dialect::OpAttributeInfo("lazy_mode", "pir::BoolAttribute", ""), paddle::dialect::OpAttributeInfo("min_row_size_to_use_multithread", "pir::Int64Attribute", ""), paddle::dialect::OpAttributeInfo("multi_precision", "pir::BoolAttribute", ""), paddle::dialect::OpAttributeInfo("use_global_beta_pow", "pir::BoolAttribute", "") };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("param_out", "paddle::dialect::DenseTensorType", false, false), paddle::dialect::OpOutputInfo("moment1_out", "paddle::dialect::DenseTensorType", false, false), paddle::dialect::OpOutputInfo("moment2_out", "paddle::dialect::DenseTensorType", false, false), paddle::dialect::OpOutputInfo("beta1_pow_out", "paddle::dialect::DenseTensorType", false, false), paddle::dialect::OpOutputInfo("beta2_pow_out", "paddle::dialect::DenseTensorType", false, false), paddle::dialect::OpOutputInfo("master_param_out", "paddle::dialect::DenseTensorType", true, false) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("AdamInferMeta", {"param", "grad", "learning_rate", "moment1", "moment2", "beta1_pow", "beta2_pow", "master_param", "skip_update", "beta1", "beta2", "epsilon", "lazy_mode", "min_row_size_to_use_multithread", "multi_precision", "use_global_beta_pow"}, "adam", {"param", "grad", "learning_rate", "moment1", "moment2", "beta1_pow", "beta2_pow", "master_param", "skip_update", "beta1", "beta2", "epsilon", "lazy_mode", "min_row_size_to_use_multithread", "multi_precision", "use_global_beta_pow"}, {"param"}, {}, {{"param_out", "param"},{"moment1_out", "moment1"},{"moment2_out", "moment2"},{"beta1_pow_out", "beta1_pow"},{"beta2_pow_out", "beta2_pow"},{"master_param_out", "master_param"}}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "adam_");
}

void Adam_Op::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value param_, pir::Value grad_, pir::Value learning_rate_, pir::Value moment1_, pir::Value moment2_, pir::Value beta1_pow_, pir::Value beta2_pow_, pir::Value master_param_, pir::Value skip_update_, float beta1, float beta2, float epsilon, bool lazy_mode, int64_t min_row_size_to_use_multithread, bool multi_precision, bool use_global_beta_pow) {
  VLOG(4) << "Start build Adam_Op";


  // Generate scalar mutable attribute: beta1
  paddle::dialect::FullOp full_beta1_op = builder.Build<paddle::dialect::FullOp>(std::vector<int64_t>{1}, beta1, phi::DataType::FLOAT32, phi::CPUPlace());
  pir::OpResult beta1_ = full_beta1_op->result(0);
      // Generate scalar mutable attribute: beta2
  paddle::dialect::FullOp full_beta2_op = builder.Build<paddle::dialect::FullOp>(std::vector<int64_t>{1}, beta2, phi::DataType::FLOAT32, phi::CPUPlace());
  pir::OpResult beta2_ = full_beta2_op->result(0);
      // Generate scalar mutable attribute: epsilon
  paddle::dialect::FullOp full_epsilon_op = builder.Build<paddle::dialect::FullOp>(std::vector<int64_t>{1}, epsilon, phi::DataType::FLOAT32, phi::CPUPlace());
  pir::OpResult epsilon_ = full_epsilon_op->result(0);
    
  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {param_, grad_, learning_rate_, moment1_, moment2_, beta1_pow_, beta2_pow_, master_param_, skip_update_, beta1_, beta2_, epsilon_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_lazy_mode = pir::BoolAttribute::get(pir::IrContext::Instance(), lazy_mode);
  argument.AddAttribute("lazy_mode", attr_lazy_mode);
  pir::Attribute attr_min_row_size_to_use_multithread = pir::Int64Attribute::get(pir::IrContext::Instance(), min_row_size_to_use_multithread);
  argument.AddAttribute("min_row_size_to_use_multithread", attr_min_row_size_to_use_multithread);
  pir::Attribute attr_multi_precision = pir::BoolAttribute::get(pir::IrContext::Instance(), multi_precision);
  argument.AddAttribute("multi_precision", attr_multi_precision);
  pir::Attribute attr_use_global_beta_pow = pir::BoolAttribute::get(pir::IrContext::Instance(), use_global_beta_pow);
  argument.AddAttribute("use_global_beta_pow", attr_use_global_beta_pow);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType param = param_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)param;
  paddle::dialect::DenseTensorType grad = grad_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)grad;
  paddle::dialect::DenseTensorType learning_rate = learning_rate_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)learning_rate;
  paddle::dialect::DenseTensorType moment1 = moment1_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)moment1;
  paddle::dialect::DenseTensorType moment2 = moment2_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)moment2;
  paddle::dialect::DenseTensorType beta1_pow = beta1_pow_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)beta1_pow;
  paddle::dialect::DenseTensorType beta2_pow = beta2_pow_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)beta2_pow;

  VLOG(4) << "Builder construction  dense_param";
  paddle::dialect::IrTensor ir_tensor_param(paddle::dialect::TransToPhiDataType(param.dtype()),
                                                      param.dims(),
                                                      param.data_layout(),
                                                      param.lod(),
                                                      param.offset());
  VLOG(4) << "Builder construction  meta_param";
  paddle::dialect::IrMetaTensor meta_param(&ir_tensor_param);

  VLOG(4) << "Builder construction  dense_grad";
  paddle::dialect::IrTensor ir_tensor_grad(paddle::dialect::TransToPhiDataType(grad.dtype()),
                                                      grad.dims(),
                                                      grad.data_layout(),
                                                      grad.lod(),
                                                      grad.offset());
  VLOG(4) << "Builder construction  meta_grad";
  paddle::dialect::IrMetaTensor meta_grad(&ir_tensor_grad);

  VLOG(4) << "Builder construction  dense_learning_rate";
  paddle::dialect::IrTensor ir_tensor_learning_rate(paddle::dialect::TransToPhiDataType(learning_rate.dtype()),
                                                      learning_rate.dims(),
                                                      learning_rate.data_layout(),
                                                      learning_rate.lod(),
                                                      learning_rate.offset());
  VLOG(4) << "Builder construction  meta_learning_rate";
  paddle::dialect::IrMetaTensor meta_learning_rate(&ir_tensor_learning_rate);

  VLOG(4) << "Builder construction  dense_moment1";
  paddle::dialect::IrTensor ir_tensor_moment1(paddle::dialect::TransToPhiDataType(moment1.dtype()),
                                                      moment1.dims(),
                                                      moment1.data_layout(),
                                                      moment1.lod(),
                                                      moment1.offset());
  VLOG(4) << "Builder construction  meta_moment1";
  paddle::dialect::IrMetaTensor meta_moment1(&ir_tensor_moment1);

  VLOG(4) << "Builder construction  dense_moment2";
  paddle::dialect::IrTensor ir_tensor_moment2(paddle::dialect::TransToPhiDataType(moment2.dtype()),
                                                      moment2.dims(),
                                                      moment2.data_layout(),
                                                      moment2.lod(),
                                                      moment2.offset());
  VLOG(4) << "Builder construction  meta_moment2";
  paddle::dialect::IrMetaTensor meta_moment2(&ir_tensor_moment2);

  VLOG(4) << "Builder construction  dense_beta1_pow";
  paddle::dialect::IrTensor ir_tensor_beta1_pow(paddle::dialect::TransToPhiDataType(beta1_pow.dtype()),
                                                      beta1_pow.dims(),
                                                      beta1_pow.data_layout(),
                                                      beta1_pow.lod(),
                                                      beta1_pow.offset());
  VLOG(4) << "Builder construction  meta_beta1_pow";
  paddle::dialect::IrMetaTensor meta_beta1_pow(&ir_tensor_beta1_pow);

  VLOG(4) << "Builder construction  dense_beta2_pow";
  paddle::dialect::IrTensor ir_tensor_beta2_pow(paddle::dialect::TransToPhiDataType(beta2_pow.dtype()),
                                                      beta2_pow.dims(),
                                                      beta2_pow.data_layout(),
                                                      beta2_pow.lod(),
                                                      beta2_pow.offset());
  VLOG(4) << "Builder construction  meta_beta2_pow";
  paddle::dialect::IrMetaTensor meta_beta2_pow(&ir_tensor_beta2_pow);

  paddle::dialect::IrMetaTensor meta_master_param;
  paddle::dialect::IrTensor ir_tensor_master_param;
  if (master_param_.impl() != nullptr) {
    paddle::dialect::DenseTensorType master_param = master_param_.type().dyn_cast<paddle::dialect::DenseTensorType>();
    VLOG(4) << "Builder construction  dense_master_param";
    ir_tensor_master_param = paddle::dialect::IrTensor(paddle::dialect::TransToPhiDataType(master_param.dtype()),
                                                        master_param.dims(),
                                                        master_param.data_layout(),
                                                        master_param.lod(),
                                                        master_param.offset());
    VLOG(4) << "Builder construction  meta_master_param";
    meta_master_param = paddle::dialect::IrMetaTensor(&ir_tensor_master_param);
  }


  paddle::dialect::IrMetaTensor meta_skip_update;
  paddle::dialect::IrTensor ir_tensor_skip_update;
  if (skip_update_.impl() != nullptr) {
    paddle::dialect::DenseTensorType skip_update = skip_update_.type().dyn_cast<paddle::dialect::DenseTensorType>();
    VLOG(4) << "Builder construction  dense_skip_update";
    ir_tensor_skip_update = paddle::dialect::IrTensor(paddle::dialect::TransToPhiDataType(skip_update.dtype()),
                                                        skip_update.dims(),
                                                        skip_update.data_layout(),
                                                        skip_update.lod(),
                                                        skip_update.offset());
    VLOG(4) << "Builder construction  meta_skip_update";
    meta_skip_update = paddle::dialect::IrMetaTensor(&ir_tensor_skip_update);
  }

  paddle::dialect::IrTensor dense_param_out;
  paddle::dialect::IrMetaTensor meta_param_out(&dense_param_out);
  paddle::dialect::IrTensor dense_moment1_out;
  paddle::dialect::IrMetaTensor meta_moment1_out(&dense_moment1_out);
  paddle::dialect::IrTensor dense_moment2_out;
  paddle::dialect::IrMetaTensor meta_moment2_out(&dense_moment2_out);
  paddle::dialect::IrTensor dense_beta1_pow_out;
  paddle::dialect::IrMetaTensor meta_beta1_pow_out(&dense_beta1_pow_out);
  paddle::dialect::IrTensor dense_beta2_pow_out;
  paddle::dialect::IrMetaTensor meta_beta2_pow_out(&dense_beta2_pow_out);
  paddle::dialect::IrTensor dense_master_param_out;
  paddle::dialect::IrMetaTensor meta_master_param_out(&dense_master_param_out);

  phi::AdamInferMeta(meta_param, meta_grad, meta_learning_rate, meta_moment1, meta_moment2, meta_beta1_pow, meta_beta2_pow, meta_master_param, meta_skip_update, beta1, beta2, epsilon, lazy_mode, min_row_size_to_use_multithread, multi_precision, use_global_beta_pow, &meta_param_out, &meta_moment1_out, &meta_moment2_out, &meta_beta1_pow_out, &meta_beta2_pow_out, &meta_master_param_out);

  std::vector<pir::Type> argument_outputs;
  pir::Type param_out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_param_out.dtype()), dense_param_out.dims(), dense_param_out.layout(), dense_param_out.lod(), dense_param_out.offset());
  argument_outputs.push_back(param_out_dense_tensor_type);

  pir::Type moment1_out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_moment1_out.dtype()), dense_moment1_out.dims(), dense_moment1_out.layout(), dense_moment1_out.lod(), dense_moment1_out.offset());
  argument_outputs.push_back(moment1_out_dense_tensor_type);

  pir::Type moment2_out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_moment2_out.dtype()), dense_moment2_out.dims(), dense_moment2_out.layout(), dense_moment2_out.lod(), dense_moment2_out.offset());
  argument_outputs.push_back(moment2_out_dense_tensor_type);

  pir::Type beta1_pow_out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_beta1_pow_out.dtype()), dense_beta1_pow_out.dims(), dense_beta1_pow_out.layout(), dense_beta1_pow_out.lod(), dense_beta1_pow_out.offset());
  argument_outputs.push_back(beta1_pow_out_dense_tensor_type);

  pir::Type beta2_pow_out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_beta2_pow_out.dtype()), dense_beta2_pow_out.dims(), dense_beta2_pow_out.layout(), dense_beta2_pow_out.lod(), dense_beta2_pow_out.offset());
  argument_outputs.push_back(beta2_pow_out_dense_tensor_type);

  if (master_param_.impl() != nullptr) {
    pir::Type master_param_out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_master_param_out.dtype()), dense_master_param_out.dims(), dense_master_param_out.layout(), dense_master_param_out.lod(), dense_master_param_out.offset());
    argument_outputs.push_back(master_param_out_dense_tensor_type);
  } else {
    pir::Type master_param_out_type;
    argument_outputs.push_back(master_param_out_type);
  }

  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void Adam_Op::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value param_, pir::Value grad_, pir::Value learning_rate_, pir::Value moment1_, pir::Value moment2_, pir::Value beta1_pow_, pir::Value beta2_pow_, pir::Value master_param_, pir::Value skip_update_, pir::AttributeMap attributes) {
  VLOG(4) << "Start build Adam_Op";


  IR_ENFORCE(
      attributes.find("beta1") != attributes.end(),
          "'beta1' Attribute is expected for Adam_Op. ");
  float beta1 = attributes.at("beta1").dyn_cast<paddle::dialect::ScalarAttribute>().data().to<float>();

  IR_ENFORCE(
      attributes.find("beta2") != attributes.end(),
          "'beta2' Attribute is expected for Adam_Op. ");
  float beta2 = attributes.at("beta2").dyn_cast<paddle::dialect::ScalarAttribute>().data().to<float>();

  IR_ENFORCE(
      attributes.find("epsilon") != attributes.end(),
          "'epsilon' Attribute is expected for Adam_Op. ");
  float epsilon = attributes.at("epsilon").dyn_cast<paddle::dialect::ScalarAttribute>().data().to<float>();

  IR_ENFORCE(
      attributes.find("lazy_mode") != attributes.end(),
          "'lazy_mode' Attribute is expected for Adam_Op. ");
  bool lazy_mode = attributes.at("lazy_mode").dyn_cast<pir::BoolAttribute>().data();

  IR_ENFORCE(
      attributes.find("min_row_size_to_use_multithread") != attributes.end(),
          "'min_row_size_to_use_multithread' Attribute is expected for Adam_Op. ");
  int64_t min_row_size_to_use_multithread = attributes.at("min_row_size_to_use_multithread").dyn_cast<pir::Int64Attribute>().data();

  IR_ENFORCE(
      attributes.find("multi_precision") != attributes.end(),
          "'multi_precision' Attribute is expected for Adam_Op. ");
  bool multi_precision = attributes.at("multi_precision").dyn_cast<pir::BoolAttribute>().data();

  IR_ENFORCE(
      attributes.find("use_global_beta_pow") != attributes.end(),
          "'use_global_beta_pow' Attribute is expected for Adam_Op. ");
  bool use_global_beta_pow = attributes.at("use_global_beta_pow").dyn_cast<pir::BoolAttribute>().data();

  // Generate scalar mutable attribute: beta1
  paddle::dialect::FullOp full_beta1_op = builder.Build<paddle::dialect::FullOp>(std::vector<int64_t>{1}, beta1, phi::DataType::FLOAT32, phi::CPUPlace());
  pir::OpResult beta1_ = full_beta1_op->result(0);
      // Generate scalar mutable attribute: beta2
  paddle::dialect::FullOp full_beta2_op = builder.Build<paddle::dialect::FullOp>(std::vector<int64_t>{1}, beta2, phi::DataType::FLOAT32, phi::CPUPlace());
  pir::OpResult beta2_ = full_beta2_op->result(0);
      // Generate scalar mutable attribute: epsilon
  paddle::dialect::FullOp full_epsilon_op = builder.Build<paddle::dialect::FullOp>(std::vector<int64_t>{1}, epsilon, phi::DataType::FLOAT32, phi::CPUPlace());
  pir::OpResult epsilon_ = full_epsilon_op->result(0);
    
  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {param_, grad_, learning_rate_, moment1_, moment2_, beta1_pow_, beta2_pow_, master_param_, skip_update_, beta1_, beta2_, epsilon_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_lazy_mode = pir::BoolAttribute::get(pir::IrContext::Instance(), lazy_mode);
  argument.AddAttribute("lazy_mode", attr_lazy_mode);
  pir::Attribute attr_min_row_size_to_use_multithread = pir::Int64Attribute::get(pir::IrContext::Instance(), min_row_size_to_use_multithread);
  argument.AddAttribute("min_row_size_to_use_multithread", attr_min_row_size_to_use_multithread);
  pir::Attribute attr_multi_precision = pir::BoolAttribute::get(pir::IrContext::Instance(), multi_precision);
  argument.AddAttribute("multi_precision", attr_multi_precision);
  pir::Attribute attr_use_global_beta_pow = pir::BoolAttribute::get(pir::IrContext::Instance(), use_global_beta_pow);
  argument.AddAttribute("use_global_beta_pow", attr_use_global_beta_pow);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType param = param_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)param;
  paddle::dialect::DenseTensorType grad = grad_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)grad;
  paddle::dialect::DenseTensorType learning_rate = learning_rate_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)learning_rate;
  paddle::dialect::DenseTensorType moment1 = moment1_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)moment1;
  paddle::dialect::DenseTensorType moment2 = moment2_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)moment2;
  paddle::dialect::DenseTensorType beta1_pow = beta1_pow_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)beta1_pow;
  paddle::dialect::DenseTensorType beta2_pow = beta2_pow_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)beta2_pow;

  VLOG(4) << "Builder construction  dense_param";
  paddle::dialect::IrTensor ir_tensor_param(paddle::dialect::TransToPhiDataType(param.dtype()),
                                                      param.dims(),
                                                      param.data_layout(),
                                                      param.lod(),
                                                      param.offset());
  VLOG(4) << "Builder construction  meta_param";
  paddle::dialect::IrMetaTensor meta_param(&ir_tensor_param);

  VLOG(4) << "Builder construction  dense_grad";
  paddle::dialect::IrTensor ir_tensor_grad(paddle::dialect::TransToPhiDataType(grad.dtype()),
                                                      grad.dims(),
                                                      grad.data_layout(),
                                                      grad.lod(),
                                                      grad.offset());
  VLOG(4) << "Builder construction  meta_grad";
  paddle::dialect::IrMetaTensor meta_grad(&ir_tensor_grad);

  VLOG(4) << "Builder construction  dense_learning_rate";
  paddle::dialect::IrTensor ir_tensor_learning_rate(paddle::dialect::TransToPhiDataType(learning_rate.dtype()),
                                                      learning_rate.dims(),
                                                      learning_rate.data_layout(),
                                                      learning_rate.lod(),
                                                      learning_rate.offset());
  VLOG(4) << "Builder construction  meta_learning_rate";
  paddle::dialect::IrMetaTensor meta_learning_rate(&ir_tensor_learning_rate);

  VLOG(4) << "Builder construction  dense_moment1";
  paddle::dialect::IrTensor ir_tensor_moment1(paddle::dialect::TransToPhiDataType(moment1.dtype()),
                                                      moment1.dims(),
                                                      moment1.data_layout(),
                                                      moment1.lod(),
                                                      moment1.offset());
  VLOG(4) << "Builder construction  meta_moment1";
  paddle::dialect::IrMetaTensor meta_moment1(&ir_tensor_moment1);

  VLOG(4) << "Builder construction  dense_moment2";
  paddle::dialect::IrTensor ir_tensor_moment2(paddle::dialect::TransToPhiDataType(moment2.dtype()),
                                                      moment2.dims(),
                                                      moment2.data_layout(),
                                                      moment2.lod(),
                                                      moment2.offset());
  VLOG(4) << "Builder construction  meta_moment2";
  paddle::dialect::IrMetaTensor meta_moment2(&ir_tensor_moment2);

  VLOG(4) << "Builder construction  dense_beta1_pow";
  paddle::dialect::IrTensor ir_tensor_beta1_pow(paddle::dialect::TransToPhiDataType(beta1_pow.dtype()),
                                                      beta1_pow.dims(),
                                                      beta1_pow.data_layout(),
                                                      beta1_pow.lod(),
                                                      beta1_pow.offset());
  VLOG(4) << "Builder construction  meta_beta1_pow";
  paddle::dialect::IrMetaTensor meta_beta1_pow(&ir_tensor_beta1_pow);

  VLOG(4) << "Builder construction  dense_beta2_pow";
  paddle::dialect::IrTensor ir_tensor_beta2_pow(paddle::dialect::TransToPhiDataType(beta2_pow.dtype()),
                                                      beta2_pow.dims(),
                                                      beta2_pow.data_layout(),
                                                      beta2_pow.lod(),
                                                      beta2_pow.offset());
  VLOG(4) << "Builder construction  meta_beta2_pow";
  paddle::dialect::IrMetaTensor meta_beta2_pow(&ir_tensor_beta2_pow);

  paddle::dialect::IrMetaTensor meta_master_param;
  paddle::dialect::IrTensor ir_tensor_master_param;
  if (master_param_.impl() != nullptr) {
    paddle::dialect::DenseTensorType master_param = master_param_.type().dyn_cast<paddle::dialect::DenseTensorType>();
    VLOG(4) << "Builder construction  dense_master_param";
    ir_tensor_master_param = paddle::dialect::IrTensor(paddle::dialect::TransToPhiDataType(master_param.dtype()),
                                                        master_param.dims(),
                                                        master_param.data_layout(),
                                                        master_param.lod(),
                                                        master_param.offset());
    VLOG(4) << "Builder construction  meta_master_param";
    meta_master_param = paddle::dialect::IrMetaTensor(&ir_tensor_master_param);
  }


  paddle::dialect::IrMetaTensor meta_skip_update;
  paddle::dialect::IrTensor ir_tensor_skip_update;
  if (skip_update_.impl() != nullptr) {
    paddle::dialect::DenseTensorType skip_update = skip_update_.type().dyn_cast<paddle::dialect::DenseTensorType>();
    VLOG(4) << "Builder construction  dense_skip_update";
    ir_tensor_skip_update = paddle::dialect::IrTensor(paddle::dialect::TransToPhiDataType(skip_update.dtype()),
                                                        skip_update.dims(),
                                                        skip_update.data_layout(),
                                                        skip_update.lod(),
                                                        skip_update.offset());
    VLOG(4) << "Builder construction  meta_skip_update";
    meta_skip_update = paddle::dialect::IrMetaTensor(&ir_tensor_skip_update);
  }

  paddle::dialect::IrTensor dense_param_out;
  paddle::dialect::IrMetaTensor meta_param_out(&dense_param_out);
  paddle::dialect::IrTensor dense_moment1_out;
  paddle::dialect::IrMetaTensor meta_moment1_out(&dense_moment1_out);
  paddle::dialect::IrTensor dense_moment2_out;
  paddle::dialect::IrMetaTensor meta_moment2_out(&dense_moment2_out);
  paddle::dialect::IrTensor dense_beta1_pow_out;
  paddle::dialect::IrMetaTensor meta_beta1_pow_out(&dense_beta1_pow_out);
  paddle::dialect::IrTensor dense_beta2_pow_out;
  paddle::dialect::IrMetaTensor meta_beta2_pow_out(&dense_beta2_pow_out);
  paddle::dialect::IrTensor dense_master_param_out;
  paddle::dialect::IrMetaTensor meta_master_param_out(&dense_master_param_out);

  phi::AdamInferMeta(meta_param, meta_grad, meta_learning_rate, meta_moment1, meta_moment2, meta_beta1_pow, meta_beta2_pow, meta_master_param, meta_skip_update, beta1, beta2, epsilon, lazy_mode, min_row_size_to_use_multithread, multi_precision, use_global_beta_pow, &meta_param_out, &meta_moment1_out, &meta_moment2_out, &meta_beta1_pow_out, &meta_beta2_pow_out, &meta_master_param_out);

  std::vector<pir::Type> argument_outputs;
  pir::Type param_out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_param_out.dtype()), dense_param_out.dims(), dense_param_out.layout(), dense_param_out.lod(), dense_param_out.offset());
  argument_outputs.push_back(param_out_dense_tensor_type);

  pir::Type moment1_out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_moment1_out.dtype()), dense_moment1_out.dims(), dense_moment1_out.layout(), dense_moment1_out.lod(), dense_moment1_out.offset());
  argument_outputs.push_back(moment1_out_dense_tensor_type);

  pir::Type moment2_out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_moment2_out.dtype()), dense_moment2_out.dims(), dense_moment2_out.layout(), dense_moment2_out.lod(), dense_moment2_out.offset());
  argument_outputs.push_back(moment2_out_dense_tensor_type);

  pir::Type beta1_pow_out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_beta1_pow_out.dtype()), dense_beta1_pow_out.dims(), dense_beta1_pow_out.layout(), dense_beta1_pow_out.lod(), dense_beta1_pow_out.offset());
  argument_outputs.push_back(beta1_pow_out_dense_tensor_type);

  pir::Type beta2_pow_out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_beta2_pow_out.dtype()), dense_beta2_pow_out.dims(), dense_beta2_pow_out.layout(), dense_beta2_pow_out.lod(), dense_beta2_pow_out.offset());
  argument_outputs.push_back(beta2_pow_out_dense_tensor_type);

  if (master_param_.impl() != nullptr) {
    pir::Type master_param_out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_master_param_out.dtype()), dense_master_param_out.dims(), dense_master_param_out.layout(), dense_master_param_out.lod(), dense_master_param_out.offset());
    argument_outputs.push_back(master_param_out_dense_tensor_type);
  } else {
    pir::Type master_param_out_type;
    argument_outputs.push_back(master_param_out_type);
  }

  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void Adam_Op::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value param_, pir::Value grad_, pir::Value learning_rate_, pir::Value moment1_, pir::Value moment2_, pir::Value beta1_pow_, pir::Value beta2_pow_, pir::Value master_param_, pir::Value skip_update_, pir::Value beta1_, pir::Value beta2_, pir::Value epsilon_, bool lazy_mode, int64_t min_row_size_to_use_multithread, bool multi_precision, bool use_global_beta_pow) {
  VLOG(4) << "Start build Adam_Op";



  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {param_, grad_, learning_rate_, moment1_, moment2_, beta1_pow_, beta2_pow_, master_param_, skip_update_, beta1_, beta2_, epsilon_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_lazy_mode = pir::BoolAttribute::get(pir::IrContext::Instance(), lazy_mode);
  argument.AddAttribute("lazy_mode", attr_lazy_mode);
  pir::Attribute attr_min_row_size_to_use_multithread = pir::Int64Attribute::get(pir::IrContext::Instance(), min_row_size_to_use_multithread);
  argument.AddAttribute("min_row_size_to_use_multithread", attr_min_row_size_to_use_multithread);
  pir::Attribute attr_multi_precision = pir::BoolAttribute::get(pir::IrContext::Instance(), multi_precision);
  argument.AddAttribute("multi_precision", attr_multi_precision);
  pir::Attribute attr_use_global_beta_pow = pir::BoolAttribute::get(pir::IrContext::Instance(), use_global_beta_pow);
  argument.AddAttribute("use_global_beta_pow", attr_use_global_beta_pow);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType param = param_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)param;
  paddle::dialect::DenseTensorType grad = grad_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)grad;
  paddle::dialect::DenseTensorType learning_rate = learning_rate_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)learning_rate;
  paddle::dialect::DenseTensorType moment1 = moment1_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)moment1;
  paddle::dialect::DenseTensorType moment2 = moment2_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)moment2;
  paddle::dialect::DenseTensorType beta1_pow = beta1_pow_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)beta1_pow;
  paddle::dialect::DenseTensorType beta2_pow = beta2_pow_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)beta2_pow;
  phi::Scalar beta1;
  if (beta1_.dyn_cast<pir::OpResult>() && beta1_.dyn_cast<pir::OpResult>().owner()->isa<paddle::dialect::FullOp>()) {
    beta1 = std::move(phi::Scalar(beta1_.dyn_cast<pir::OpResult>().owner()
                                  ->dyn_cast<paddle::dialect::FullOp>()
                                  .attribute("value")
                                  .dyn_cast<paddle::dialect::ScalarAttribute>()
                                  .data()
                                  .to<int>()));
  }
  else {
    beta1 = std::move(phi::Scalar(-1));
    beta1.SetFromTensor(true);
  }
  phi::Scalar beta2;
  if (beta2_.dyn_cast<pir::OpResult>() && beta2_.dyn_cast<pir::OpResult>().owner()->isa<paddle::dialect::FullOp>()) {
    beta2 = std::move(phi::Scalar(beta2_.dyn_cast<pir::OpResult>().owner()
                                  ->dyn_cast<paddle::dialect::FullOp>()
                                  .attribute("value")
                                  .dyn_cast<paddle::dialect::ScalarAttribute>()
                                  .data()
                                  .to<int>()));
  }
  else {
    beta2 = std::move(phi::Scalar(-1));
    beta2.SetFromTensor(true);
  }
  phi::Scalar epsilon;
  if (epsilon_.dyn_cast<pir::OpResult>() && epsilon_.dyn_cast<pir::OpResult>().owner()->isa<paddle::dialect::FullOp>()) {
    epsilon = std::move(phi::Scalar(epsilon_.dyn_cast<pir::OpResult>().owner()
                                  ->dyn_cast<paddle::dialect::FullOp>()
                                  .attribute("value")
                                  .dyn_cast<paddle::dialect::ScalarAttribute>()
                                  .data()
                                  .to<int>()));
  }
  else {
    epsilon = std::move(phi::Scalar(-1));
    epsilon.SetFromTensor(true);
  }


  VLOG(4) << "Builder construction  dense_param";
  paddle::dialect::IrTensor ir_tensor_param(paddle::dialect::TransToPhiDataType(param.dtype()),
                                                      param.dims(),
                                                      param.data_layout(),
                                                      param.lod(),
                                                      param.offset());
  VLOG(4) << "Builder construction  meta_param";
  paddle::dialect::IrMetaTensor meta_param(&ir_tensor_param);

  VLOG(4) << "Builder construction  dense_grad";
  paddle::dialect::IrTensor ir_tensor_grad(paddle::dialect::TransToPhiDataType(grad.dtype()),
                                                      grad.dims(),
                                                      grad.data_layout(),
                                                      grad.lod(),
                                                      grad.offset());
  VLOG(4) << "Builder construction  meta_grad";
  paddle::dialect::IrMetaTensor meta_grad(&ir_tensor_grad);

  VLOG(4) << "Builder construction  dense_learning_rate";
  paddle::dialect::IrTensor ir_tensor_learning_rate(paddle::dialect::TransToPhiDataType(learning_rate.dtype()),
                                                      learning_rate.dims(),
                                                      learning_rate.data_layout(),
                                                      learning_rate.lod(),
                                                      learning_rate.offset());
  VLOG(4) << "Builder construction  meta_learning_rate";
  paddle::dialect::IrMetaTensor meta_learning_rate(&ir_tensor_learning_rate);

  VLOG(4) << "Builder construction  dense_moment1";
  paddle::dialect::IrTensor ir_tensor_moment1(paddle::dialect::TransToPhiDataType(moment1.dtype()),
                                                      moment1.dims(),
                                                      moment1.data_layout(),
                                                      moment1.lod(),
                                                      moment1.offset());
  VLOG(4) << "Builder construction  meta_moment1";
  paddle::dialect::IrMetaTensor meta_moment1(&ir_tensor_moment1);

  VLOG(4) << "Builder construction  dense_moment2";
  paddle::dialect::IrTensor ir_tensor_moment2(paddle::dialect::TransToPhiDataType(moment2.dtype()),
                                                      moment2.dims(),
                                                      moment2.data_layout(),
                                                      moment2.lod(),
                                                      moment2.offset());
  VLOG(4) << "Builder construction  meta_moment2";
  paddle::dialect::IrMetaTensor meta_moment2(&ir_tensor_moment2);

  VLOG(4) << "Builder construction  dense_beta1_pow";
  paddle::dialect::IrTensor ir_tensor_beta1_pow(paddle::dialect::TransToPhiDataType(beta1_pow.dtype()),
                                                      beta1_pow.dims(),
                                                      beta1_pow.data_layout(),
                                                      beta1_pow.lod(),
                                                      beta1_pow.offset());
  VLOG(4) << "Builder construction  meta_beta1_pow";
  paddle::dialect::IrMetaTensor meta_beta1_pow(&ir_tensor_beta1_pow);

  VLOG(4) << "Builder construction  dense_beta2_pow";
  paddle::dialect::IrTensor ir_tensor_beta2_pow(paddle::dialect::TransToPhiDataType(beta2_pow.dtype()),
                                                      beta2_pow.dims(),
                                                      beta2_pow.data_layout(),
                                                      beta2_pow.lod(),
                                                      beta2_pow.offset());
  VLOG(4) << "Builder construction  meta_beta2_pow";
  paddle::dialect::IrMetaTensor meta_beta2_pow(&ir_tensor_beta2_pow);

  paddle::dialect::IrMetaTensor meta_master_param;
  paddle::dialect::IrTensor ir_tensor_master_param;
  if (master_param_.impl() != nullptr) {
    paddle::dialect::DenseTensorType master_param = master_param_.type().dyn_cast<paddle::dialect::DenseTensorType>();
    VLOG(4) << "Builder construction  dense_master_param";
    ir_tensor_master_param = paddle::dialect::IrTensor(paddle::dialect::TransToPhiDataType(master_param.dtype()),
                                                        master_param.dims(),
                                                        master_param.data_layout(),
                                                        master_param.lod(),
                                                        master_param.offset());
    VLOG(4) << "Builder construction  meta_master_param";
    meta_master_param = paddle::dialect::IrMetaTensor(&ir_tensor_master_param);
  }


  paddle::dialect::IrMetaTensor meta_skip_update;
  paddle::dialect::IrTensor ir_tensor_skip_update;
  if (skip_update_.impl() != nullptr) {
    paddle::dialect::DenseTensorType skip_update = skip_update_.type().dyn_cast<paddle::dialect::DenseTensorType>();
    VLOG(4) << "Builder construction  dense_skip_update";
    ir_tensor_skip_update = paddle::dialect::IrTensor(paddle::dialect::TransToPhiDataType(skip_update.dtype()),
                                                        skip_update.dims(),
                                                        skip_update.data_layout(),
                                                        skip_update.lod(),
                                                        skip_update.offset());
    VLOG(4) << "Builder construction  meta_skip_update";
    meta_skip_update = paddle::dialect::IrMetaTensor(&ir_tensor_skip_update);
  }

  paddle::dialect::IrTensor dense_param_out;
  paddle::dialect::IrMetaTensor meta_param_out(&dense_param_out);
  paddle::dialect::IrTensor dense_moment1_out;
  paddle::dialect::IrMetaTensor meta_moment1_out(&dense_moment1_out);
  paddle::dialect::IrTensor dense_moment2_out;
  paddle::dialect::IrMetaTensor meta_moment2_out(&dense_moment2_out);
  paddle::dialect::IrTensor dense_beta1_pow_out;
  paddle::dialect::IrMetaTensor meta_beta1_pow_out(&dense_beta1_pow_out);
  paddle::dialect::IrTensor dense_beta2_pow_out;
  paddle::dialect::IrMetaTensor meta_beta2_pow_out(&dense_beta2_pow_out);
  paddle::dialect::IrTensor dense_master_param_out;
  paddle::dialect::IrMetaTensor meta_master_param_out(&dense_master_param_out);

  phi::AdamInferMeta(meta_param, meta_grad, meta_learning_rate, meta_moment1, meta_moment2, meta_beta1_pow, meta_beta2_pow, meta_master_param, meta_skip_update, beta1, beta2, epsilon, lazy_mode, min_row_size_to_use_multithread, multi_precision, use_global_beta_pow, &meta_param_out, &meta_moment1_out, &meta_moment2_out, &meta_beta1_pow_out, &meta_beta2_pow_out, &meta_master_param_out);

  std::vector<pir::Type> argument_outputs;
  pir::Type param_out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_param_out.dtype()), dense_param_out.dims(), dense_param_out.layout(), dense_param_out.lod(), dense_param_out.offset());
  argument_outputs.push_back(param_out_dense_tensor_type);

  pir::Type moment1_out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_moment1_out.dtype()), dense_moment1_out.dims(), dense_moment1_out.layout(), dense_moment1_out.lod(), dense_moment1_out.offset());
  argument_outputs.push_back(moment1_out_dense_tensor_type);

  pir::Type moment2_out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_moment2_out.dtype()), dense_moment2_out.dims(), dense_moment2_out.layout(), dense_moment2_out.lod(), dense_moment2_out.offset());
  argument_outputs.push_back(moment2_out_dense_tensor_type);

  pir::Type beta1_pow_out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_beta1_pow_out.dtype()), dense_beta1_pow_out.dims(), dense_beta1_pow_out.layout(), dense_beta1_pow_out.lod(), dense_beta1_pow_out.offset());
  argument_outputs.push_back(beta1_pow_out_dense_tensor_type);

  pir::Type beta2_pow_out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_beta2_pow_out.dtype()), dense_beta2_pow_out.dims(), dense_beta2_pow_out.layout(), dense_beta2_pow_out.lod(), dense_beta2_pow_out.offset());
  argument_outputs.push_back(beta2_pow_out_dense_tensor_type);

  if (master_param_.impl() != nullptr) {
    pir::Type master_param_out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_master_param_out.dtype()), dense_master_param_out.dims(), dense_master_param_out.layout(), dense_master_param_out.lod(), dense_master_param_out.offset());
    argument_outputs.push_back(master_param_out_dense_tensor_type);
  } else {
    pir::Type master_param_out_type;
    argument_outputs.push_back(master_param_out_type);
  }

  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void Adam_Op::VerifySig() {
  VLOG(4) << "Start Verifying inputs, outputs and attributes for: Adam_Op.";
  VLOG(4) << "Verifying inputs:";
  {
  auto input_size = num_operands();
  IR_ENFORCE(input_size == 12u,
                    "The size %d of inputs must be equal to 12.", input_size);
  IR_ENFORCE((*this)->operand_source(0).type().isa<paddle::dialect::DenseTensorType>(),
                  "Type validation failed for the 0th input, got %s.", (*this)->operand_source(0).type());
  IR_ENFORCE((*this)->operand_source(1).type().isa<paddle::dialect::DenseTensorType>(),
                  "Type validation failed for the 1th input, got %s.", (*this)->operand_source(1).type());
  IR_ENFORCE((*this)->operand_source(2).type().isa<paddle::dialect::DenseTensorType>(),
                  "Type validation failed for the 2th input, got %s.", (*this)->operand_source(2).type());
  IR_ENFORCE((*this)->operand_source(3).type().isa<paddle::dialect::DenseTensorType>(),
                  "Type validation failed for the 3th input, got %s.", (*this)->operand_source(3).type());
  IR_ENFORCE((*this)->operand_source(4).type().isa<paddle::dialect::DenseTensorType>(),
                  "Type validation failed for the 4th input, got %s.", (*this)->operand_source(4).type());
  IR_ENFORCE((*this)->operand_source(5).type().isa<paddle::dialect::DenseTensorType>(),
                  "Type validation failed for the 5th input, got %s.", (*this)->operand_source(5).type());
  IR_ENFORCE((*this)->operand_source(6).type().isa<paddle::dialect::DenseTensorType>(),
                  "Type validation failed for the 6th input, got %s.", (*this)->operand_source(6).type());
  if (auto val = (*this)->operand(7)) {
    IR_ENFORCE(val.type().isa<paddle::dialect::DenseTensorType>(),
                   "Type validation failed for the 7th input, got %s.", (*this)->operand_source(7).type());
  }
  if (auto val = (*this)->operand(8)) {
    IR_ENFORCE(val.type().isa<paddle::dialect::DenseTensorType>(),
                   "Type validation failed for the 8th input, got %s.", (*this)->operand_source(8).type());
  }
  IR_ENFORCE((*this)->operand_source(9).type().isa<paddle::dialect::DenseTensorType>(),
                  "Type validation failed for the 9th input, got %s.", (*this)->operand_source(9).type());
  IR_ENFORCE((*this)->operand_source(10).type().isa<paddle::dialect::DenseTensorType>(),
                  "Type validation failed for the 10th input, got %s.", (*this)->operand_source(10).type());
  IR_ENFORCE((*this)->operand_source(11).type().isa<paddle::dialect::DenseTensorType>(),
                  "Type validation failed for the 11th input, got %s.", (*this)->operand_source(11).type());
  }
  VLOG(4) << "Verifying attributes:";
  {
  auto& attributes = this->attributes();
  IR_ENFORCE(attributes.count("lazy_mode")>0,
                 "lazy_mode does not exist.");
  IR_ENFORCE(attributes.at("lazy_mode").isa<pir::BoolAttribute>(),
                 "Type of attribute: lazy_mode is not pir::BoolAttribute.");

  IR_ENFORCE(attributes.count("min_row_size_to_use_multithread")>0,
                 "min_row_size_to_use_multithread does not exist.");
  IR_ENFORCE(attributes.at("min_row_size_to_use_multithread").isa<pir::Int64Attribute>(),
                 "Type of attribute: min_row_size_to_use_multithread is not pir::Int64Attribute.");

  IR_ENFORCE(attributes.count("multi_precision")>0,
                 "multi_precision does not exist.");
  IR_ENFORCE(attributes.at("multi_precision").isa<pir::BoolAttribute>(),
                 "Type of attribute: multi_precision is not pir::BoolAttribute.");

  IR_ENFORCE(attributes.count("use_global_beta_pow")>0,
                 "use_global_beta_pow does not exist.");
  IR_ENFORCE(attributes.at("use_global_beta_pow").isa<pir::BoolAttribute>(),
                 "Type of attribute: use_global_beta_pow is not pir::BoolAttribute.");

  }
  VLOG(4) << "Verifying outputs:";
  {
  auto output_size = num_results();
  IR_ENFORCE(output_size == 6u,
                    "The size %d of outputs must be equal to 6.", output_size);
  IR_ENFORCE((*this)->result(0).type().isa<paddle::dialect::DenseTensorType>(),
                 "Type validation failed for the 0th output.");
  IR_ENFORCE((*this)->result(1).type().isa<paddle::dialect::DenseTensorType>(),
                 "Type validation failed for the 1th output.");
  IR_ENFORCE((*this)->result(2).type().isa<paddle::dialect::DenseTensorType>(),
                 "Type validation failed for the 2th output.");
  IR_ENFORCE((*this)->result(3).type().isa<paddle::dialect::DenseTensorType>(),
                 "Type validation failed for the 3th output.");
  IR_ENFORCE((*this)->result(4).type().isa<paddle::dialect::DenseTensorType>(),
                 "Type validation failed for the 4th output.");
  if (auto output_5_type = (*this)->result(5).type()) {
    IR_ENFORCE(output_5_type.isa<paddle::dialect::DenseTensorType>(),
                   "Type validation failed for the 5th output.");
  }
  }
  VLOG(4) << "End Verifying for: Adam_Op.";
}

void Adam_Op::InferMeta( phi::InferMetaContext *infer_meta ) {
  auto fn = PD_INFER_META(phi::AdamInferMeta);
  fn(infer_meta);
}

phi::DataType Adam_Op::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: Adam_Op";
  


  return expected_kernel_dtype;
}

const char *AdamDenseParamSparseGrad_Op::attributes_name[4] = { "lazy_mode", "min_row_size_to_use_multithread", "multi_precision", "use_global_beta_pow" };

OpInfoTuple AdamDenseParamSparseGrad_Op::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("param", "paddle::dialect::DenseTensorType", false, false, false, false), paddle::dialect::OpInputInfo("grad", "paddle::dialect::SelectedRowsType", false, false, false, false), paddle::dialect::OpInputInfo("learning_rate", "paddle::dialect::DenseTensorType", false, false, false, false), paddle::dialect::OpInputInfo("moment1", "paddle::dialect::DenseTensorType", false, false, false, false), paddle::dialect::OpInputInfo("moment2", "paddle::dialect::DenseTensorType", false, false, false, false), paddle::dialect::OpInputInfo("beta1_pow", "paddle::dialect::DenseTensorType", false, false, false, false), paddle::dialect::OpInputInfo("beta2_pow", "paddle::dialect::DenseTensorType", false, false, false, false), paddle::dialect::OpInputInfo("master_param", "paddle::dialect::DenseTensorType", true, false, false, false), paddle::dialect::OpInputInfo("skip_update", "paddle::dialect::DenseTensorType", true, false, false, false), paddle::dialect::OpInputInfo("beta1", "paddle::dialect::ScalarAttribute", false, false, true, false), paddle::dialect::OpInputInfo("beta2", "paddle::dialect::ScalarAttribute", false, false, true, false), paddle::dialect::OpInputInfo("epsilon", "paddle::dialect::ScalarAttribute", false, false, true, false) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = { paddle::dialect::OpAttributeInfo("lazy_mode", "pir::BoolAttribute", ""), paddle::dialect::OpAttributeInfo("min_row_size_to_use_multithread", "pir::Int64Attribute", ""), paddle::dialect::OpAttributeInfo("multi_precision", "pir::BoolAttribute", ""), paddle::dialect::OpAttributeInfo("use_global_beta_pow", "pir::BoolAttribute", "") };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("param_out", "paddle::dialect::DenseTensorType", false, false), paddle::dialect::OpOutputInfo("moment1_out", "paddle::dialect::DenseTensorType", false, false), paddle::dialect::OpOutputInfo("moment2_out", "paddle::dialect::DenseTensorType", false, false), paddle::dialect::OpOutputInfo("beta1_pow_out", "paddle::dialect::DenseTensorType", false, false), paddle::dialect::OpOutputInfo("beta2_pow_out", "paddle::dialect::DenseTensorType", false, false), paddle::dialect::OpOutputInfo("master_param_out", "paddle::dialect::DenseTensorType", true, false) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("AdamInferMeta", {"param", "grad", "learning_rate", "moment1", "moment2", "beta1_pow", "beta2_pow", "master_param", "skip_update", "beta1", "beta2", "epsilon", "lazy_mode", "min_row_size_to_use_multithread", "multi_precision", "use_global_beta_pow"}, "adam_dense_param_sparse_grad", {"param", "grad", "learning_rate", "moment1", "moment2", "beta1_pow", "beta2_pow", "master_param", "skip_update", "beta1", "beta2", "epsilon", "lazy_mode", "min_row_size_to_use_multithread", "multi_precision", "use_global_beta_pow"}, {"param"}, {}, {{"param_out", "param"},{"moment1_out", "moment1"},{"moment2_out", "moment2"},{"beta1_pow_out", "beta1_pow"},{"beta2_pow_out", "beta2_pow"},{"master_param_out", "master_param"}}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "adam_");
}

void AdamDenseParamSparseGrad_Op::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value param_, pir::Value grad_, pir::Value learning_rate_, pir::Value moment1_, pir::Value moment2_, pir::Value beta1_pow_, pir::Value beta2_pow_, pir::Value master_param_, pir::Value skip_update_, float beta1, float beta2, float epsilon, bool lazy_mode, int64_t min_row_size_to_use_multithread, bool multi_precision, bool use_global_beta_pow) {
  VLOG(4) << "Start build AdamDenseParamSparseGrad_Op";


  // Generate scalar mutable attribute: beta1
  paddle::dialect::FullOp full_beta1_op = builder.Build<paddle::dialect::FullOp>(std::vector<int64_t>{1}, beta1, phi::DataType::FLOAT32, phi::CPUPlace());
  pir::OpResult beta1_ = full_beta1_op->result(0);
      // Generate scalar mutable attribute: beta2
  paddle::dialect::FullOp full_beta2_op = builder.Build<paddle::dialect::FullOp>(std::vector<int64_t>{1}, beta2, phi::DataType::FLOAT32, phi::CPUPlace());
  pir::OpResult beta2_ = full_beta2_op->result(0);
      // Generate scalar mutable attribute: epsilon
  paddle::dialect::FullOp full_epsilon_op = builder.Build<paddle::dialect::FullOp>(std::vector<int64_t>{1}, epsilon, phi::DataType::FLOAT32, phi::CPUPlace());
  pir::OpResult epsilon_ = full_epsilon_op->result(0);
    
  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {param_, grad_, learning_rate_, moment1_, moment2_, beta1_pow_, beta2_pow_, master_param_, skip_update_, beta1_, beta2_, epsilon_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_lazy_mode = pir::BoolAttribute::get(pir::IrContext::Instance(), lazy_mode);
  argument.AddAttribute("lazy_mode", attr_lazy_mode);
  pir::Attribute attr_min_row_size_to_use_multithread = pir::Int64Attribute::get(pir::IrContext::Instance(), min_row_size_to_use_multithread);
  argument.AddAttribute("min_row_size_to_use_multithread", attr_min_row_size_to_use_multithread);
  pir::Attribute attr_multi_precision = pir::BoolAttribute::get(pir::IrContext::Instance(), multi_precision);
  argument.AddAttribute("multi_precision", attr_multi_precision);
  pir::Attribute attr_use_global_beta_pow = pir::BoolAttribute::get(pir::IrContext::Instance(), use_global_beta_pow);
  argument.AddAttribute("use_global_beta_pow", attr_use_global_beta_pow);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType param = param_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)param;
  paddle::dialect::SelectedRowsType grad = grad_.type().dyn_cast<paddle::dialect::SelectedRowsType>(); (void)grad;
  paddle::dialect::DenseTensorType learning_rate = learning_rate_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)learning_rate;
  paddle::dialect::DenseTensorType moment1 = moment1_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)moment1;
  paddle::dialect::DenseTensorType moment2 = moment2_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)moment2;
  paddle::dialect::DenseTensorType beta1_pow = beta1_pow_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)beta1_pow;
  paddle::dialect::DenseTensorType beta2_pow = beta2_pow_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)beta2_pow;

  VLOG(4) << "Builder construction  dense_param";
  paddle::dialect::IrTensor ir_tensor_param(paddle::dialect::TransToPhiDataType(param.dtype()),
                                                      param.dims(),
                                                      param.data_layout(),
                                                      param.lod(),
                                                      param.offset());
  VLOG(4) << "Builder construction  meta_param";
  paddle::dialect::IrMetaTensor meta_param(&ir_tensor_param);

  VLOG(4) << "Builder construction  dense_grad";
  paddle::dialect::IrTensor ir_tensor_grad(paddle::dialect::TransToPhiDataType(grad.dtype()),
                                                      grad.dims(),
                                                      grad.data_layout(),
                                                      grad.lod(),
                                                      grad.offset());
  VLOG(4) << "Builder construction  meta_grad";
  paddle::dialect::IrMetaTensor meta_grad(&ir_tensor_grad);

  VLOG(4) << "Builder construction  dense_learning_rate";
  paddle::dialect::IrTensor ir_tensor_learning_rate(paddle::dialect::TransToPhiDataType(learning_rate.dtype()),
                                                      learning_rate.dims(),
                                                      learning_rate.data_layout(),
                                                      learning_rate.lod(),
                                                      learning_rate.offset());
  VLOG(4) << "Builder construction  meta_learning_rate";
  paddle::dialect::IrMetaTensor meta_learning_rate(&ir_tensor_learning_rate);

  VLOG(4) << "Builder construction  dense_moment1";
  paddle::dialect::IrTensor ir_tensor_moment1(paddle::dialect::TransToPhiDataType(moment1.dtype()),
                                                      moment1.dims(),
                                                      moment1.data_layout(),
                                                      moment1.lod(),
                                                      moment1.offset());
  VLOG(4) << "Builder construction  meta_moment1";
  paddle::dialect::IrMetaTensor meta_moment1(&ir_tensor_moment1);

  VLOG(4) << "Builder construction  dense_moment2";
  paddle::dialect::IrTensor ir_tensor_moment2(paddle::dialect::TransToPhiDataType(moment2.dtype()),
                                                      moment2.dims(),
                                                      moment2.data_layout(),
                                                      moment2.lod(),
                                                      moment2.offset());
  VLOG(4) << "Builder construction  meta_moment2";
  paddle::dialect::IrMetaTensor meta_moment2(&ir_tensor_moment2);

  VLOG(4) << "Builder construction  dense_beta1_pow";
  paddle::dialect::IrTensor ir_tensor_beta1_pow(paddle::dialect::TransToPhiDataType(beta1_pow.dtype()),
                                                      beta1_pow.dims(),
                                                      beta1_pow.data_layout(),
                                                      beta1_pow.lod(),
                                                      beta1_pow.offset());
  VLOG(4) << "Builder construction  meta_beta1_pow";
  paddle::dialect::IrMetaTensor meta_beta1_pow(&ir_tensor_beta1_pow);

  VLOG(4) << "Builder construction  dense_beta2_pow";
  paddle::dialect::IrTensor ir_tensor_beta2_pow(paddle::dialect::TransToPhiDataType(beta2_pow.dtype()),
                                                      beta2_pow.dims(),
                                                      beta2_pow.data_layout(),
                                                      beta2_pow.lod(),
                                                      beta2_pow.offset());
  VLOG(4) << "Builder construction  meta_beta2_pow";
  paddle::dialect::IrMetaTensor meta_beta2_pow(&ir_tensor_beta2_pow);

  paddle::dialect::IrMetaTensor meta_master_param;
  paddle::dialect::IrTensor ir_tensor_master_param;
  if (master_param_.impl() != nullptr) {
    paddle::dialect::DenseTensorType master_param = master_param_.type().dyn_cast<paddle::dialect::DenseTensorType>();
    VLOG(4) << "Builder construction  dense_master_param";
    ir_tensor_master_param = paddle::dialect::IrTensor(paddle::dialect::TransToPhiDataType(master_param.dtype()),
                                                        master_param.dims(),
                                                        master_param.data_layout(),
                                                        master_param.lod(),
                                                        master_param.offset());
    VLOG(4) << "Builder construction  meta_master_param";
    meta_master_param = paddle::dialect::IrMetaTensor(&ir_tensor_master_param);
  }


  paddle::dialect::IrMetaTensor meta_skip_update;
  paddle::dialect::IrTensor ir_tensor_skip_update;
  if (skip_update_.impl() != nullptr) {
    paddle::dialect::DenseTensorType skip_update = skip_update_.type().dyn_cast<paddle::dialect::DenseTensorType>();
    VLOG(4) << "Builder construction  dense_skip_update";
    ir_tensor_skip_update = paddle::dialect::IrTensor(paddle::dialect::TransToPhiDataType(skip_update.dtype()),
                                                        skip_update.dims(),
                                                        skip_update.data_layout(),
                                                        skip_update.lod(),
                                                        skip_update.offset());
    VLOG(4) << "Builder construction  meta_skip_update";
    meta_skip_update = paddle::dialect::IrMetaTensor(&ir_tensor_skip_update);
  }

  paddle::dialect::IrTensor dense_param_out;
  paddle::dialect::IrMetaTensor meta_param_out(&dense_param_out);
  paddle::dialect::IrTensor dense_moment1_out;
  paddle::dialect::IrMetaTensor meta_moment1_out(&dense_moment1_out);
  paddle::dialect::IrTensor dense_moment2_out;
  paddle::dialect::IrMetaTensor meta_moment2_out(&dense_moment2_out);
  paddle::dialect::IrTensor dense_beta1_pow_out;
  paddle::dialect::IrMetaTensor meta_beta1_pow_out(&dense_beta1_pow_out);
  paddle::dialect::IrTensor dense_beta2_pow_out;
  paddle::dialect::IrMetaTensor meta_beta2_pow_out(&dense_beta2_pow_out);
  paddle::dialect::IrTensor dense_master_param_out;
  paddle::dialect::IrMetaTensor meta_master_param_out(&dense_master_param_out);

  phi::AdamInferMeta(meta_param, meta_grad, meta_learning_rate, meta_moment1, meta_moment2, meta_beta1_pow, meta_beta2_pow, meta_master_param, meta_skip_update, beta1, beta2, epsilon, lazy_mode, min_row_size_to_use_multithread, multi_precision, use_global_beta_pow, &meta_param_out, &meta_moment1_out, &meta_moment2_out, &meta_beta1_pow_out, &meta_beta2_pow_out, &meta_master_param_out);

  std::vector<pir::Type> argument_outputs;
  pir::Type param_out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_param_out.dtype()), dense_param_out.dims(), dense_param_out.layout(), dense_param_out.lod(), dense_param_out.offset());
  argument_outputs.push_back(param_out_dense_tensor_type);

  pir::Type moment1_out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_moment1_out.dtype()), dense_moment1_out.dims(), dense_moment1_out.layout(), dense_moment1_out.lod(), dense_moment1_out.offset());
  argument_outputs.push_back(moment1_out_dense_tensor_type);

  pir::Type moment2_out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_moment2_out.dtype()), dense_moment2_out.dims(), dense_moment2_out.layout(), dense_moment2_out.lod(), dense_moment2_out.offset());
  argument_outputs.push_back(moment2_out_dense_tensor_type);

  pir::Type beta1_pow_out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_beta1_pow_out.dtype()), dense_beta1_pow_out.dims(), dense_beta1_pow_out.layout(), dense_beta1_pow_out.lod(), dense_beta1_pow_out.offset());
  argument_outputs.push_back(beta1_pow_out_dense_tensor_type);

  pir::Type beta2_pow_out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_beta2_pow_out.dtype()), dense_beta2_pow_out.dims(), dense_beta2_pow_out.layout(), dense_beta2_pow_out.lod(), dense_beta2_pow_out.offset());
  argument_outputs.push_back(beta2_pow_out_dense_tensor_type);

  if (master_param_.impl() != nullptr) {
    pir::Type master_param_out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_master_param_out.dtype()), dense_master_param_out.dims(), dense_master_param_out.layout(), dense_master_param_out.lod(), dense_master_param_out.offset());
    argument_outputs.push_back(master_param_out_dense_tensor_type);
  } else {
    pir::Type master_param_out_type;
    argument_outputs.push_back(master_param_out_type);
  }

  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void AdamDenseParamSparseGrad_Op::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value param_, pir::Value grad_, pir::Value learning_rate_, pir::Value moment1_, pir::Value moment2_, pir::Value beta1_pow_, pir::Value beta2_pow_, pir::Value master_param_, pir::Value skip_update_, pir::AttributeMap attributes) {
  VLOG(4) << "Start build AdamDenseParamSparseGrad_Op";


  IR_ENFORCE(
      attributes.find("beta1") != attributes.end(),
          "'beta1' Attribute is expected for AdamDenseParamSparseGrad_Op. ");
  float beta1 = attributes.at("beta1").dyn_cast<paddle::dialect::ScalarAttribute>().data().to<float>();

  IR_ENFORCE(
      attributes.find("beta2") != attributes.end(),
          "'beta2' Attribute is expected for AdamDenseParamSparseGrad_Op. ");
  float beta2 = attributes.at("beta2").dyn_cast<paddle::dialect::ScalarAttribute>().data().to<float>();

  IR_ENFORCE(
      attributes.find("epsilon") != attributes.end(),
          "'epsilon' Attribute is expected for AdamDenseParamSparseGrad_Op. ");
  float epsilon = attributes.at("epsilon").dyn_cast<paddle::dialect::ScalarAttribute>().data().to<float>();

  IR_ENFORCE(
      attributes.find("lazy_mode") != attributes.end(),
          "'lazy_mode' Attribute is expected for AdamDenseParamSparseGrad_Op. ");
  bool lazy_mode = attributes.at("lazy_mode").dyn_cast<pir::BoolAttribute>().data();

  IR_ENFORCE(
      attributes.find("min_row_size_to_use_multithread") != attributes.end(),
          "'min_row_size_to_use_multithread' Attribute is expected for AdamDenseParamSparseGrad_Op. ");
  int64_t min_row_size_to_use_multithread = attributes.at("min_row_size_to_use_multithread").dyn_cast<pir::Int64Attribute>().data();

  IR_ENFORCE(
      attributes.find("multi_precision") != attributes.end(),
          "'multi_precision' Attribute is expected for AdamDenseParamSparseGrad_Op. ");
  bool multi_precision = attributes.at("multi_precision").dyn_cast<pir::BoolAttribute>().data();

  IR_ENFORCE(
      attributes.find("use_global_beta_pow") != attributes.end(),
          "'use_global_beta_pow' Attribute is expected for AdamDenseParamSparseGrad_Op. ");
  bool use_global_beta_pow = attributes.at("use_global_beta_pow").dyn_cast<pir::BoolAttribute>().data();

  // Generate scalar mutable attribute: beta1
  paddle::dialect::FullOp full_beta1_op = builder.Build<paddle::dialect::FullOp>(std::vector<int64_t>{1}, beta1, phi::DataType::FLOAT32, phi::CPUPlace());
  pir::OpResult beta1_ = full_beta1_op->result(0);
      // Generate scalar mutable attribute: beta2
  paddle::dialect::FullOp full_beta2_op = builder.Build<paddle::dialect::FullOp>(std::vector<int64_t>{1}, beta2, phi::DataType::FLOAT32, phi::CPUPlace());
  pir::OpResult beta2_ = full_beta2_op->result(0);
      // Generate scalar mutable attribute: epsilon
  paddle::dialect::FullOp full_epsilon_op = builder.Build<paddle::dialect::FullOp>(std::vector<int64_t>{1}, epsilon, phi::DataType::FLOAT32, phi::CPUPlace());
  pir::OpResult epsilon_ = full_epsilon_op->result(0);
    
  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {param_, grad_, learning_rate_, moment1_, moment2_, beta1_pow_, beta2_pow_, master_param_, skip_update_, beta1_, beta2_, epsilon_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_lazy_mode = pir::BoolAttribute::get(pir::IrContext::Instance(), lazy_mode);
  argument.AddAttribute("lazy_mode", attr_lazy_mode);
  pir::Attribute attr_min_row_size_to_use_multithread = pir::Int64Attribute::get(pir::IrContext::Instance(), min_row_size_to_use_multithread);
  argument.AddAttribute("min_row_size_to_use_multithread", attr_min_row_size_to_use_multithread);
  pir::Attribute attr_multi_precision = pir::BoolAttribute::get(pir::IrContext::Instance(), multi_precision);
  argument.AddAttribute("multi_precision", attr_multi_precision);
  pir::Attribute attr_use_global_beta_pow = pir::BoolAttribute::get(pir::IrContext::Instance(), use_global_beta_pow);
  argument.AddAttribute("use_global_beta_pow", attr_use_global_beta_pow);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType param = param_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)param;
  paddle::dialect::SelectedRowsType grad = grad_.type().dyn_cast<paddle::dialect::SelectedRowsType>(); (void)grad;
  paddle::dialect::DenseTensorType learning_rate = learning_rate_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)learning_rate;
  paddle::dialect::DenseTensorType moment1 = moment1_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)moment1;
  paddle::dialect::DenseTensorType moment2 = moment2_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)moment2;
  paddle::dialect::DenseTensorType beta1_pow = beta1_pow_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)beta1_pow;
  paddle::dialect::DenseTensorType beta2_pow = beta2_pow_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)beta2_pow;

  VLOG(4) << "Builder construction  dense_param";
  paddle::dialect::IrTensor ir_tensor_param(paddle::dialect::TransToPhiDataType(param.dtype()),
                                                      param.dims(),
                                                      param.data_layout(),
                                                      param.lod(),
                                                      param.offset());
  VLOG(4) << "Builder construction  meta_param";
  paddle::dialect::IrMetaTensor meta_param(&ir_tensor_param);

  VLOG(4) << "Builder construction  dense_grad";
  paddle::dialect::IrTensor ir_tensor_grad(paddle::dialect::TransToPhiDataType(grad.dtype()),
                                                      grad.dims(),
                                                      grad.data_layout(),
                                                      grad.lod(),
                                                      grad.offset());
  VLOG(4) << "Builder construction  meta_grad";
  paddle::dialect::IrMetaTensor meta_grad(&ir_tensor_grad);

  VLOG(4) << "Builder construction  dense_learning_rate";
  paddle::dialect::IrTensor ir_tensor_learning_rate(paddle::dialect::TransToPhiDataType(learning_rate.dtype()),
                                                      learning_rate.dims(),
                                                      learning_rate.data_layout(),
                                                      learning_rate.lod(),
                                                      learning_rate.offset());
  VLOG(4) << "Builder construction  meta_learning_rate";
  paddle::dialect::IrMetaTensor meta_learning_rate(&ir_tensor_learning_rate);

  VLOG(4) << "Builder construction  dense_moment1";
  paddle::dialect::IrTensor ir_tensor_moment1(paddle::dialect::TransToPhiDataType(moment1.dtype()),
                                                      moment1.dims(),
                                                      moment1.data_layout(),
                                                      moment1.lod(),
                                                      moment1.offset());
  VLOG(4) << "Builder construction  meta_moment1";
  paddle::dialect::IrMetaTensor meta_moment1(&ir_tensor_moment1);

  VLOG(4) << "Builder construction  dense_moment2";
  paddle::dialect::IrTensor ir_tensor_moment2(paddle::dialect::TransToPhiDataType(moment2.dtype()),
                                                      moment2.dims(),
                                                      moment2.data_layout(),
                                                      moment2.lod(),
                                                      moment2.offset());
  VLOG(4) << "Builder construction  meta_moment2";
  paddle::dialect::IrMetaTensor meta_moment2(&ir_tensor_moment2);

  VLOG(4) << "Builder construction  dense_beta1_pow";
  paddle::dialect::IrTensor ir_tensor_beta1_pow(paddle::dialect::TransToPhiDataType(beta1_pow.dtype()),
                                                      beta1_pow.dims(),
                                                      beta1_pow.data_layout(),
                                                      beta1_pow.lod(),
                                                      beta1_pow.offset());
  VLOG(4) << "Builder construction  meta_beta1_pow";
  paddle::dialect::IrMetaTensor meta_beta1_pow(&ir_tensor_beta1_pow);

  VLOG(4) << "Builder construction  dense_beta2_pow";
  paddle::dialect::IrTensor ir_tensor_beta2_pow(paddle::dialect::TransToPhiDataType(beta2_pow.dtype()),
                                                      beta2_pow.dims(),
                                                      beta2_pow.data_layout(),
                                                      beta2_pow.lod(),
                                                      beta2_pow.offset());
  VLOG(4) << "Builder construction  meta_beta2_pow";
  paddle::dialect::IrMetaTensor meta_beta2_pow(&ir_tensor_beta2_pow);

  paddle::dialect::IrMetaTensor meta_master_param;
  paddle::dialect::IrTensor ir_tensor_master_param;
  if (master_param_.impl() != nullptr) {
    paddle::dialect::DenseTensorType master_param = master_param_.type().dyn_cast<paddle::dialect::DenseTensorType>();
    VLOG(4) << "Builder construction  dense_master_param";
    ir_tensor_master_param = paddle::dialect::IrTensor(paddle::dialect::TransToPhiDataType(master_param.dtype()),
                                                        master_param.dims(),
                                                        master_param.data_layout(),
                                                        master_param.lod(),
                                                        master_param.offset());
    VLOG(4) << "Builder construction  meta_master_param";
    meta_master_param = paddle::dialect::IrMetaTensor(&ir_tensor_master_param);
  }


  paddle::dialect::IrMetaTensor meta_skip_update;
  paddle::dialect::IrTensor ir_tensor_skip_update;
  if (skip_update_.impl() != nullptr) {
    paddle::dialect::DenseTensorType skip_update = skip_update_.type().dyn_cast<paddle::dialect::DenseTensorType>();
    VLOG(4) << "Builder construction  dense_skip_update";
    ir_tensor_skip_update = paddle::dialect::IrTensor(paddle::dialect::TransToPhiDataType(skip_update.dtype()),
                                                        skip_update.dims(),
                                                        skip_update.data_layout(),
                                                        skip_update.lod(),
                                                        skip_update.offset());
    VLOG(4) << "Builder construction  meta_skip_update";
    meta_skip_update = paddle::dialect::IrMetaTensor(&ir_tensor_skip_update);
  }

  paddle::dialect::IrTensor dense_param_out;
  paddle::dialect::IrMetaTensor meta_param_out(&dense_param_out);
  paddle::dialect::IrTensor dense_moment1_out;
  paddle::dialect::IrMetaTensor meta_moment1_out(&dense_moment1_out);
  paddle::dialect::IrTensor dense_moment2_out;
  paddle::dialect::IrMetaTensor meta_moment2_out(&dense_moment2_out);
  paddle::dialect::IrTensor dense_beta1_pow_out;
  paddle::dialect::IrMetaTensor meta_beta1_pow_out(&dense_beta1_pow_out);
  paddle::dialect::IrTensor dense_beta2_pow_out;
  paddle::dialect::IrMetaTensor meta_beta2_pow_out(&dense_beta2_pow_out);
  paddle::dialect::IrTensor dense_master_param_out;
  paddle::dialect::IrMetaTensor meta_master_param_out(&dense_master_param_out);

  phi::AdamInferMeta(meta_param, meta_grad, meta_learning_rate, meta_moment1, meta_moment2, meta_beta1_pow, meta_beta2_pow, meta_master_param, meta_skip_update, beta1, beta2, epsilon, lazy_mode, min_row_size_to_use_multithread, multi_precision, use_global_beta_pow, &meta_param_out, &meta_moment1_out, &meta_moment2_out, &meta_beta1_pow_out, &meta_beta2_pow_out, &meta_master_param_out);

  std::vector<pir::Type> argument_outputs;
  pir::Type param_out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_param_out.dtype()), dense_param_out.dims(), dense_param_out.layout(), dense_param_out.lod(), dense_param_out.offset());
  argument_outputs.push_back(param_out_dense_tensor_type);

  pir::Type moment1_out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_moment1_out.dtype()), dense_moment1_out.dims(), dense_moment1_out.layout(), dense_moment1_out.lod(), dense_moment1_out.offset());
  argument_outputs.push_back(moment1_out_dense_tensor_type);

  pir::Type moment2_out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_moment2_out.dtype()), dense_moment2_out.dims(), dense_moment2_out.layout(), dense_moment2_out.lod(), dense_moment2_out.offset());
  argument_outputs.push_back(moment2_out_dense_tensor_type);

  pir::Type beta1_pow_out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_beta1_pow_out.dtype()), dense_beta1_pow_out.dims(), dense_beta1_pow_out.layout(), dense_beta1_pow_out.lod(), dense_beta1_pow_out.offset());
  argument_outputs.push_back(beta1_pow_out_dense_tensor_type);

  pir::Type beta2_pow_out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_beta2_pow_out.dtype()), dense_beta2_pow_out.dims(), dense_beta2_pow_out.layout(), dense_beta2_pow_out.lod(), dense_beta2_pow_out.offset());
  argument_outputs.push_back(beta2_pow_out_dense_tensor_type);

  if (master_param_.impl() != nullptr) {
    pir::Type master_param_out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_master_param_out.dtype()), dense_master_param_out.dims(), dense_master_param_out.layout(), dense_master_param_out.lod(), dense_master_param_out.offset());
    argument_outputs.push_back(master_param_out_dense_tensor_type);
  } else {
    pir::Type master_param_out_type;
    argument_outputs.push_back(master_param_out_type);
  }

  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void AdamDenseParamSparseGrad_Op::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value param_, pir::Value grad_, pir::Value learning_rate_, pir::Value moment1_, pir::Value moment2_, pir::Value beta1_pow_, pir::Value beta2_pow_, pir::Value master_param_, pir::Value skip_update_, pir::Value beta1_, pir::Value beta2_, pir::Value epsilon_, bool lazy_mode, int64_t min_row_size_to_use_multithread, bool multi_precision, bool use_global_beta_pow) {
  VLOG(4) << "Start build AdamDenseParamSparseGrad_Op";



  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {param_, grad_, learning_rate_, moment1_, moment2_, beta1_pow_, beta2_pow_, master_param_, skip_update_, beta1_, beta2_, epsilon_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_lazy_mode = pir::BoolAttribute::get(pir::IrContext::Instance(), lazy_mode);
  argument.AddAttribute("lazy_mode", attr_lazy_mode);
  pir::Attribute attr_min_row_size_to_use_multithread = pir::Int64Attribute::get(pir::IrContext::Instance(), min_row_size_to_use_multithread);
  argument.AddAttribute("min_row_size_to_use_multithread", attr_min_row_size_to_use_multithread);
  pir::Attribute attr_multi_precision = pir::BoolAttribute::get(pir::IrContext::Instance(), multi_precision);
  argument.AddAttribute("multi_precision", attr_multi_precision);
  pir::Attribute attr_use_global_beta_pow = pir::BoolAttribute::get(pir::IrContext::Instance(), use_global_beta_pow);
  argument.AddAttribute("use_global_beta_pow", attr_use_global_beta_pow);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType param = param_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)param;
  paddle::dialect::SelectedRowsType grad = grad_.type().dyn_cast<paddle::dialect::SelectedRowsType>(); (void)grad;
  paddle::dialect::DenseTensorType learning_rate = learning_rate_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)learning_rate;
  paddle::dialect::DenseTensorType moment1 = moment1_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)moment1;
  paddle::dialect::DenseTensorType moment2 = moment2_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)moment2;
  paddle::dialect::DenseTensorType beta1_pow = beta1_pow_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)beta1_pow;
  paddle::dialect::DenseTensorType beta2_pow = beta2_pow_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)beta2_pow;
  phi::Scalar beta1;
  if (beta1_.dyn_cast<pir::OpResult>() && beta1_.dyn_cast<pir::OpResult>().owner()->isa<paddle::dialect::FullOp>()) {
    beta1 = std::move(phi::Scalar(beta1_.dyn_cast<pir::OpResult>().owner()
                                  ->dyn_cast<paddle::dialect::FullOp>()
                                  .attribute("value")
                                  .dyn_cast<paddle::dialect::ScalarAttribute>()
                                  .data()
                                  .to<int>()));
  }
  else {
    beta1 = std::move(phi::Scalar(-1));
    beta1.SetFromTensor(true);
  }
  phi::Scalar beta2;
  if (beta2_.dyn_cast<pir::OpResult>() && beta2_.dyn_cast<pir::OpResult>().owner()->isa<paddle::dialect::FullOp>()) {
    beta2 = std::move(phi::Scalar(beta2_.dyn_cast<pir::OpResult>().owner()
                                  ->dyn_cast<paddle::dialect::FullOp>()
                                  .attribute("value")
                                  .dyn_cast<paddle::dialect::ScalarAttribute>()
                                  .data()
                                  .to<int>()));
  }
  else {
    beta2 = std::move(phi::Scalar(-1));
    beta2.SetFromTensor(true);
  }
  phi::Scalar epsilon;
  if (epsilon_.dyn_cast<pir::OpResult>() && epsilon_.dyn_cast<pir::OpResult>().owner()->isa<paddle::dialect::FullOp>()) {
    epsilon = std::move(phi::Scalar(epsilon_.dyn_cast<pir::OpResult>().owner()
                                  ->dyn_cast<paddle::dialect::FullOp>()
                                  .attribute("value")
                                  .dyn_cast<paddle::dialect::ScalarAttribute>()
                                  .data()
                                  .to<int>()));
  }
  else {
    epsilon = std::move(phi::Scalar(-1));
    epsilon.SetFromTensor(true);
  }


  VLOG(4) << "Builder construction  dense_param";
  paddle::dialect::IrTensor ir_tensor_param(paddle::dialect::TransToPhiDataType(param.dtype()),
                                                      param.dims(),
                                                      param.data_layout(),
                                                      param.lod(),
                                                      param.offset());
  VLOG(4) << "Builder construction  meta_param";
  paddle::dialect::IrMetaTensor meta_param(&ir_tensor_param);

  VLOG(4) << "Builder construction  dense_grad";
  paddle::dialect::IrTensor ir_tensor_grad(paddle::dialect::TransToPhiDataType(grad.dtype()),
                                                      grad.dims(),
                                                      grad.data_layout(),
                                                      grad.lod(),
                                                      grad.offset());
  VLOG(4) << "Builder construction  meta_grad";
  paddle::dialect::IrMetaTensor meta_grad(&ir_tensor_grad);

  VLOG(4) << "Builder construction  dense_learning_rate";
  paddle::dialect::IrTensor ir_tensor_learning_rate(paddle::dialect::TransToPhiDataType(learning_rate.dtype()),
                                                      learning_rate.dims(),
                                                      learning_rate.data_layout(),
                                                      learning_rate.lod(),
                                                      learning_rate.offset());
  VLOG(4) << "Builder construction  meta_learning_rate";
  paddle::dialect::IrMetaTensor meta_learning_rate(&ir_tensor_learning_rate);

  VLOG(4) << "Builder construction  dense_moment1";
  paddle::dialect::IrTensor ir_tensor_moment1(paddle::dialect::TransToPhiDataType(moment1.dtype()),
                                                      moment1.dims(),
                                                      moment1.data_layout(),
                                                      moment1.lod(),
                                                      moment1.offset());
  VLOG(4) << "Builder construction  meta_moment1";
  paddle::dialect::IrMetaTensor meta_moment1(&ir_tensor_moment1);

  VLOG(4) << "Builder construction  dense_moment2";
  paddle::dialect::IrTensor ir_tensor_moment2(paddle::dialect::TransToPhiDataType(moment2.dtype()),
                                                      moment2.dims(),
                                                      moment2.data_layout(),
                                                      moment2.lod(),
                                                      moment2.offset());
  VLOG(4) << "Builder construction  meta_moment2";
  paddle::dialect::IrMetaTensor meta_moment2(&ir_tensor_moment2);

  VLOG(4) << "Builder construction  dense_beta1_pow";
  paddle::dialect::IrTensor ir_tensor_beta1_pow(paddle::dialect::TransToPhiDataType(beta1_pow.dtype()),
                                                      beta1_pow.dims(),
                                                      beta1_pow.data_layout(),
                                                      beta1_pow.lod(),
                                                      beta1_pow.offset());
  VLOG(4) << "Builder construction  meta_beta1_pow";
  paddle::dialect::IrMetaTensor meta_beta1_pow(&ir_tensor_beta1_pow);

  VLOG(4) << "Builder construction  dense_beta2_pow";
  paddle::dialect::IrTensor ir_tensor_beta2_pow(paddle::dialect::TransToPhiDataType(beta2_pow.dtype()),
                                                      beta2_pow.dims(),
                                                      beta2_pow.data_layout(),
                                                      beta2_pow.lod(),
                                                      beta2_pow.offset());
  VLOG(4) << "Builder construction  meta_beta2_pow";
  paddle::dialect::IrMetaTensor meta_beta2_pow(&ir_tensor_beta2_pow);

  paddle::dialect::IrMetaTensor meta_master_param;
  paddle::dialect::IrTensor ir_tensor_master_param;
  if (master_param_.impl() != nullptr) {
    paddle::dialect::DenseTensorType master_param = master_param_.type().dyn_cast<paddle::dialect::DenseTensorType>();
    VLOG(4) << "Builder construction  dense_master_param";
    ir_tensor_master_param = paddle::dialect::IrTensor(paddle::dialect::TransToPhiDataType(master_param.dtype()),
                                                        master_param.dims(),
                                                        master_param.data_layout(),
                                                        master_param.lod(),
                                                        master_param.offset());
    VLOG(4) << "Builder construction  meta_master_param";
    meta_master_param = paddle::dialect::IrMetaTensor(&ir_tensor_master_param);
  }


  paddle::dialect::IrMetaTensor meta_skip_update;
  paddle::dialect::IrTensor ir_tensor_skip_update;
  if (skip_update_.impl() != nullptr) {
    paddle::dialect::DenseTensorType skip_update = skip_update_.type().dyn_cast<paddle::dialect::DenseTensorType>();
    VLOG(4) << "Builder construction  dense_skip_update";
    ir_tensor_skip_update = paddle::dialect::IrTensor(paddle::dialect::TransToPhiDataType(skip_update.dtype()),
                                                        skip_update.dims(),
                                                        skip_update.data_layout(),
                                                        skip_update.lod(),
                                                        skip_update.offset());
    VLOG(4) << "Builder construction  meta_skip_update";
    meta_skip_update = paddle::dialect::IrMetaTensor(&ir_tensor_skip_update);
  }

  paddle::dialect::IrTensor dense_param_out;
  paddle::dialect::IrMetaTensor meta_param_out(&dense_param_out);
  paddle::dialect::IrTensor dense_moment1_out;
  paddle::dialect::IrMetaTensor meta_moment1_out(&dense_moment1_out);
  paddle::dialect::IrTensor dense_moment2_out;
  paddle::dialect::IrMetaTensor meta_moment2_out(&dense_moment2_out);
  paddle::dialect::IrTensor dense_beta1_pow_out;
  paddle::dialect::IrMetaTensor meta_beta1_pow_out(&dense_beta1_pow_out);
  paddle::dialect::IrTensor dense_beta2_pow_out;
  paddle::dialect::IrMetaTensor meta_beta2_pow_out(&dense_beta2_pow_out);
  paddle::dialect::IrTensor dense_master_param_out;
  paddle::dialect::IrMetaTensor meta_master_param_out(&dense_master_param_out);

  phi::AdamInferMeta(meta_param, meta_grad, meta_learning_rate, meta_moment1, meta_moment2, meta_beta1_pow, meta_beta2_pow, meta_master_param, meta_skip_update, beta1, beta2, epsilon, lazy_mode, min_row_size_to_use_multithread, multi_precision, use_global_beta_pow, &meta_param_out, &meta_moment1_out, &meta_moment2_out, &meta_beta1_pow_out, &meta_beta2_pow_out, &meta_master_param_out);

  std::vector<pir::Type> argument_outputs;
  pir::Type param_out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_param_out.dtype()), dense_param_out.dims(), dense_param_out.layout(), dense_param_out.lod(), dense_param_out.offset());
  argument_outputs.push_back(param_out_dense_tensor_type);

  pir::Type moment1_out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_moment1_out.dtype()), dense_moment1_out.dims(), dense_moment1_out.layout(), dense_moment1_out.lod(), dense_moment1_out.offset());
  argument_outputs.push_back(moment1_out_dense_tensor_type);

  pir::Type moment2_out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_moment2_out.dtype()), dense_moment2_out.dims(), dense_moment2_out.layout(), dense_moment2_out.lod(), dense_moment2_out.offset());
  argument_outputs.push_back(moment2_out_dense_tensor_type);

  pir::Type beta1_pow_out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_beta1_pow_out.dtype()), dense_beta1_pow_out.dims(), dense_beta1_pow_out.layout(), dense_beta1_pow_out.lod(), dense_beta1_pow_out.offset());
  argument_outputs.push_back(beta1_pow_out_dense_tensor_type);

  pir::Type beta2_pow_out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_beta2_pow_out.dtype()), dense_beta2_pow_out.dims(), dense_beta2_pow_out.layout(), dense_beta2_pow_out.lod(), dense_beta2_pow_out.offset());
  argument_outputs.push_back(beta2_pow_out_dense_tensor_type);

  if (master_param_.impl() != nullptr) {
    pir::Type master_param_out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_master_param_out.dtype()), dense_master_param_out.dims(), dense_master_param_out.layout(), dense_master_param_out.lod(), dense_master_param_out.offset());
    argument_outputs.push_back(master_param_out_dense_tensor_type);
  } else {
    pir::Type master_param_out_type;
    argument_outputs.push_back(master_param_out_type);
  }

  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void AdamDenseParamSparseGrad_Op::VerifySig() {}

void AdamDenseParamSparseGrad_Op::InferMeta( phi::InferMetaContext *infer_meta ) {
  auto fn = PD_INFER_META(phi::AdamInferMeta);
  fn(infer_meta);
}

phi::DataType AdamDenseParamSparseGrad_Op::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: AdamDenseParamSparseGrad_Op";
  


  return expected_kernel_dtype;
}

const char *Adamax_Op::attributes_name[4] = { "beta1", "beta2", "epsilon", "multi_precision" };

OpInfoTuple Adamax_Op::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("param", "paddle::dialect::DenseTensorType", false, false, false, false), paddle::dialect::OpInputInfo("grad", "paddle::dialect::DenseTensorType", false, false, false, false), paddle::dialect::OpInputInfo("learning_rate", "paddle::dialect::DenseTensorType", false, false, false, false), paddle::dialect::OpInputInfo("moment", "paddle::dialect::DenseTensorType", false, false, false, false), paddle::dialect::OpInputInfo("inf_norm", "paddle::dialect::DenseTensorType", false, false, false, false), paddle::dialect::OpInputInfo("beta1_pow", "paddle::dialect::DenseTensorType", false, false, false, false), paddle::dialect::OpInputInfo("master_param", "paddle::dialect::DenseTensorType", true, false, false, false) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = { paddle::dialect::OpAttributeInfo("beta1", "pir::FloatAttribute", ""), paddle::dialect::OpAttributeInfo("beta2", "pir::FloatAttribute", ""), paddle::dialect::OpAttributeInfo("epsilon", "pir::FloatAttribute", ""), paddle::dialect::OpAttributeInfo("multi_precision", "pir::BoolAttribute", "") };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("param_out", "paddle::dialect::DenseTensorType", false, false), paddle::dialect::OpOutputInfo("moment_out", "paddle::dialect::DenseTensorType", false, false), paddle::dialect::OpOutputInfo("inf_norm_out", "paddle::dialect::DenseTensorType", false, false), paddle::dialect::OpOutputInfo("master_param_out", "paddle::dialect::DenseTensorType", true, false) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("AdamaxInferMeta", {"param", "grad", "learning_rate", "moment", "inf_norm", "beta1_pow", "master_param", "beta1", "beta2", "epsilon", "multi_precision"}, "adamax", {"param", "grad", "learning_rate", "moment", "inf_norm", "beta1_pow", "master_param", "beta1", "beta2", "epsilon", "multi_precision"}, {"param"}, {}, {{"param_out", "param"},{"moment_out", "moment"},{"inf_norm_out", "inf_norm"},{"master_param_out", "master_param"}}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "adamax_");
}

void Adamax_Op::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value param_, pir::Value grad_, pir::Value learning_rate_, pir::Value moment_, pir::Value inf_norm_, pir::Value beta1_pow_, pir::Value master_param_, float beta1, float beta2, float epsilon, bool multi_precision) {
  VLOG(4) << "Start build Adamax_Op";



  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {param_, grad_, learning_rate_, moment_, inf_norm_, beta1_pow_, master_param_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_beta1 = pir::FloatAttribute::get(pir::IrContext::Instance(), beta1);
  argument.AddAttribute("beta1", attr_beta1);
  pir::Attribute attr_beta2 = pir::FloatAttribute::get(pir::IrContext::Instance(), beta2);
  argument.AddAttribute("beta2", attr_beta2);
  pir::Attribute attr_epsilon = pir::FloatAttribute::get(pir::IrContext::Instance(), epsilon);
  argument.AddAttribute("epsilon", attr_epsilon);
  pir::Attribute attr_multi_precision = pir::BoolAttribute::get(pir::IrContext::Instance(), multi_precision);
  argument.AddAttribute("multi_precision", attr_multi_precision);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType param = param_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)param;
  paddle::dialect::DenseTensorType grad = grad_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)grad;
  paddle::dialect::DenseTensorType learning_rate = learning_rate_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)learning_rate;
  paddle::dialect::DenseTensorType moment = moment_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)moment;
  paddle::dialect::DenseTensorType inf_norm = inf_norm_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)inf_norm;
  paddle::dialect::DenseTensorType beta1_pow = beta1_pow_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)beta1_pow;

  VLOG(4) << "Builder construction  dense_param";
  paddle::dialect::IrTensor ir_tensor_param(paddle::dialect::TransToPhiDataType(param.dtype()),
                                                      param.dims(),
                                                      param.data_layout(),
                                                      param.lod(),
                                                      param.offset());
  VLOG(4) << "Builder construction  meta_param";
  paddle::dialect::IrMetaTensor meta_param(&ir_tensor_param);

  VLOG(4) << "Builder construction  dense_grad";
  paddle::dialect::IrTensor ir_tensor_grad(paddle::dialect::TransToPhiDataType(grad.dtype()),
                                                      grad.dims(),
                                                      grad.data_layout(),
                                                      grad.lod(),
                                                      grad.offset());
  VLOG(4) << "Builder construction  meta_grad";
  paddle::dialect::IrMetaTensor meta_grad(&ir_tensor_grad);

  VLOG(4) << "Builder construction  dense_learning_rate";
  paddle::dialect::IrTensor ir_tensor_learning_rate(paddle::dialect::TransToPhiDataType(learning_rate.dtype()),
                                                      learning_rate.dims(),
                                                      learning_rate.data_layout(),
                                                      learning_rate.lod(),
                                                      learning_rate.offset());
  VLOG(4) << "Builder construction  meta_learning_rate";
  paddle::dialect::IrMetaTensor meta_learning_rate(&ir_tensor_learning_rate);

  VLOG(4) << "Builder construction  dense_moment";
  paddle::dialect::IrTensor ir_tensor_moment(paddle::dialect::TransToPhiDataType(moment.dtype()),
                                                      moment.dims(),
                                                      moment.data_layout(),
                                                      moment.lod(),
                                                      moment.offset());
  VLOG(4) << "Builder construction  meta_moment";
  paddle::dialect::IrMetaTensor meta_moment(&ir_tensor_moment);

  VLOG(4) << "Builder construction  dense_inf_norm";
  paddle::dialect::IrTensor ir_tensor_inf_norm(paddle::dialect::TransToPhiDataType(inf_norm.dtype()),
                                                      inf_norm.dims(),
                                                      inf_norm.data_layout(),
                                                      inf_norm.lod(),
                                                      inf_norm.offset());
  VLOG(4) << "Builder construction  meta_inf_norm";
  paddle::dialect::IrMetaTensor meta_inf_norm(&ir_tensor_inf_norm);

  VLOG(4) << "Builder construction  dense_beta1_pow";
  paddle::dialect::IrTensor ir_tensor_beta1_pow(paddle::dialect::TransToPhiDataType(beta1_pow.dtype()),
                                                      beta1_pow.dims(),
                                                      beta1_pow.data_layout(),
                                                      beta1_pow.lod(),
                                                      beta1_pow.offset());
  VLOG(4) << "Builder construction  meta_beta1_pow";
  paddle::dialect::IrMetaTensor meta_beta1_pow(&ir_tensor_beta1_pow);

  paddle::dialect::IrMetaTensor meta_master_param;
  paddle::dialect::IrTensor ir_tensor_master_param;
  if (master_param_.impl() != nullptr) {
    paddle::dialect::DenseTensorType master_param = master_param_.type().dyn_cast<paddle::dialect::DenseTensorType>();
    VLOG(4) << "Builder construction  dense_master_param";
    ir_tensor_master_param = paddle::dialect::IrTensor(paddle::dialect::TransToPhiDataType(master_param.dtype()),
                                                        master_param.dims(),
                                                        master_param.data_layout(),
                                                        master_param.lod(),
                                                        master_param.offset());
    VLOG(4) << "Builder construction  meta_master_param";
    meta_master_param = paddle::dialect::IrMetaTensor(&ir_tensor_master_param);
  }

  paddle::dialect::IrTensor dense_param_out;
  paddle::dialect::IrMetaTensor meta_param_out(&dense_param_out);
  paddle::dialect::IrTensor dense_moment_out;
  paddle::dialect::IrMetaTensor meta_moment_out(&dense_moment_out);
  paddle::dialect::IrTensor dense_inf_norm_out;
  paddle::dialect::IrMetaTensor meta_inf_norm_out(&dense_inf_norm_out);
  paddle::dialect::IrTensor dense_master_param_out;
  paddle::dialect::IrMetaTensor meta_master_param_out(&dense_master_param_out);

  phi::AdamaxInferMeta(meta_param, meta_grad, meta_learning_rate, meta_moment, meta_inf_norm, meta_beta1_pow, meta_master_param, beta1, beta2, epsilon, multi_precision, &meta_param_out, &meta_moment_out, &meta_inf_norm_out, &meta_master_param_out);

  std::vector<pir::Type> argument_outputs;
  pir::Type param_out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_param_out.dtype()), dense_param_out.dims(), dense_param_out.layout(), dense_param_out.lod(), dense_param_out.offset());
  argument_outputs.push_back(param_out_dense_tensor_type);

  pir::Type moment_out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_moment_out.dtype()), dense_moment_out.dims(), dense_moment_out.layout(), dense_moment_out.lod(), dense_moment_out.offset());
  argument_outputs.push_back(moment_out_dense_tensor_type);

  pir::Type inf_norm_out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_inf_norm_out.dtype()), dense_inf_norm_out.dims(), dense_inf_norm_out.layout(), dense_inf_norm_out.lod(), dense_inf_norm_out.offset());
  argument_outputs.push_back(inf_norm_out_dense_tensor_type);

  if (master_param_.impl() != nullptr) {
    pir::Type master_param_out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_master_param_out.dtype()), dense_master_param_out.dims(), dense_master_param_out.layout(), dense_master_param_out.lod(), dense_master_param_out.offset());
    argument_outputs.push_back(master_param_out_dense_tensor_type);
  } else {
    pir::Type master_param_out_type;
    argument_outputs.push_back(master_param_out_type);
  }

  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void Adamax_Op::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value param_, pir::Value grad_, pir::Value learning_rate_, pir::Value moment_, pir::Value inf_norm_, pir::Value beta1_pow_, pir::Value master_param_, pir::AttributeMap attributes) {
  VLOG(4) << "Start build Adamax_Op";


  IR_ENFORCE(
      attributes.find("beta1") != attributes.end(),
          "'beta1' Attribute is expected for Adamax_Op. ");
  float beta1 = attributes.at("beta1").dyn_cast<pir::FloatAttribute>().data();

  IR_ENFORCE(
      attributes.find("beta2") != attributes.end(),
          "'beta2' Attribute is expected for Adamax_Op. ");
  float beta2 = attributes.at("beta2").dyn_cast<pir::FloatAttribute>().data();

  IR_ENFORCE(
      attributes.find("epsilon") != attributes.end(),
          "'epsilon' Attribute is expected for Adamax_Op. ");
  float epsilon = attributes.at("epsilon").dyn_cast<pir::FloatAttribute>().data();

  IR_ENFORCE(
      attributes.find("multi_precision") != attributes.end(),
          "'multi_precision' Attribute is expected for Adamax_Op. ");
  bool multi_precision = attributes.at("multi_precision").dyn_cast<pir::BoolAttribute>().data();


  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {param_, grad_, learning_rate_, moment_, inf_norm_, beta1_pow_, master_param_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_beta1 = pir::FloatAttribute::get(pir::IrContext::Instance(), beta1);
  argument.AddAttribute("beta1", attr_beta1);
  pir::Attribute attr_beta2 = pir::FloatAttribute::get(pir::IrContext::Instance(), beta2);
  argument.AddAttribute("beta2", attr_beta2);
  pir::Attribute attr_epsilon = pir::FloatAttribute::get(pir::IrContext::Instance(), epsilon);
  argument.AddAttribute("epsilon", attr_epsilon);
  pir::Attribute attr_multi_precision = pir::BoolAttribute::get(pir::IrContext::Instance(), multi_precision);
  argument.AddAttribute("multi_precision", attr_multi_precision);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType param = param_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)param;
  paddle::dialect::DenseTensorType grad = grad_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)grad;
  paddle::dialect::DenseTensorType learning_rate = learning_rate_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)learning_rate;
  paddle::dialect::DenseTensorType moment = moment_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)moment;
  paddle::dialect::DenseTensorType inf_norm = inf_norm_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)inf_norm;
  paddle::dialect::DenseTensorType beta1_pow = beta1_pow_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)beta1_pow;

  VLOG(4) << "Builder construction  dense_param";
  paddle::dialect::IrTensor ir_tensor_param(paddle::dialect::TransToPhiDataType(param.dtype()),
                                                      param.dims(),
                                                      param.data_layout(),
                                                      param.lod(),
                                                      param.offset());
  VLOG(4) << "Builder construction  meta_param";
  paddle::dialect::IrMetaTensor meta_param(&ir_tensor_param);

  VLOG(4) << "Builder construction  dense_grad";
  paddle::dialect::IrTensor ir_tensor_grad(paddle::dialect::TransToPhiDataType(grad.dtype()),
                                                      grad.dims(),
                                                      grad.data_layout(),
                                                      grad.lod(),
                                                      grad.offset());
  VLOG(4) << "Builder construction  meta_grad";
  paddle::dialect::IrMetaTensor meta_grad(&ir_tensor_grad);

  VLOG(4) << "Builder construction  dense_learning_rate";
  paddle::dialect::IrTensor ir_tensor_learning_rate(paddle::dialect::TransToPhiDataType(learning_rate.dtype()),
                                                      learning_rate.dims(),
                                                      learning_rate.data_layout(),
                                                      learning_rate.lod(),
                                                      learning_rate.offset());
  VLOG(4) << "Builder construction  meta_learning_rate";
  paddle::dialect::IrMetaTensor meta_learning_rate(&ir_tensor_learning_rate);

  VLOG(4) << "Builder construction  dense_moment";
  paddle::dialect::IrTensor ir_tensor_moment(paddle::dialect::TransToPhiDataType(moment.dtype()),
                                                      moment.dims(),
                                                      moment.data_layout(),
                                                      moment.lod(),
                                                      moment.offset());
  VLOG(4) << "Builder construction  meta_moment";
  paddle::dialect::IrMetaTensor meta_moment(&ir_tensor_moment);

  VLOG(4) << "Builder construction  dense_inf_norm";
  paddle::dialect::IrTensor ir_tensor_inf_norm(paddle::dialect::TransToPhiDataType(inf_norm.dtype()),
                                                      inf_norm.dims(),
                                                      inf_norm.data_layout(),
                                                      inf_norm.lod(),
                                                      inf_norm.offset());
  VLOG(4) << "Builder construction  meta_inf_norm";
  paddle::dialect::IrMetaTensor meta_inf_norm(&ir_tensor_inf_norm);

  VLOG(4) << "Builder construction  dense_beta1_pow";
  paddle::dialect::IrTensor ir_tensor_beta1_pow(paddle::dialect::TransToPhiDataType(beta1_pow.dtype()),
                                                      beta1_pow.dims(),
                                                      beta1_pow.data_layout(),
                                                      beta1_pow.lod(),
                                                      beta1_pow.offset());
  VLOG(4) << "Builder construction  meta_beta1_pow";
  paddle::dialect::IrMetaTensor meta_beta1_pow(&ir_tensor_beta1_pow);

  paddle::dialect::IrMetaTensor meta_master_param;
  paddle::dialect::IrTensor ir_tensor_master_param;
  if (master_param_.impl() != nullptr) {
    paddle::dialect::DenseTensorType master_param = master_param_.type().dyn_cast<paddle::dialect::DenseTensorType>();
    VLOG(4) << "Builder construction  dense_master_param";
    ir_tensor_master_param = paddle::dialect::IrTensor(paddle::dialect::TransToPhiDataType(master_param.dtype()),
                                                        master_param.dims(),
                                                        master_param.data_layout(),
                                                        master_param.lod(),
                                                        master_param.offset());
    VLOG(4) << "Builder construction  meta_master_param";
    meta_master_param = paddle::dialect::IrMetaTensor(&ir_tensor_master_param);
  }

  paddle::dialect::IrTensor dense_param_out;
  paddle::dialect::IrMetaTensor meta_param_out(&dense_param_out);
  paddle::dialect::IrTensor dense_moment_out;
  paddle::dialect::IrMetaTensor meta_moment_out(&dense_moment_out);
  paddle::dialect::IrTensor dense_inf_norm_out;
  paddle::dialect::IrMetaTensor meta_inf_norm_out(&dense_inf_norm_out);
  paddle::dialect::IrTensor dense_master_param_out;
  paddle::dialect::IrMetaTensor meta_master_param_out(&dense_master_param_out);

  phi::AdamaxInferMeta(meta_param, meta_grad, meta_learning_rate, meta_moment, meta_inf_norm, meta_beta1_pow, meta_master_param, beta1, beta2, epsilon, multi_precision, &meta_param_out, &meta_moment_out, &meta_inf_norm_out, &meta_master_param_out);

  std::vector<pir::Type> argument_outputs;
  pir::Type param_out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_param_out.dtype()), dense_param_out.dims(), dense_param_out.layout(), dense_param_out.lod(), dense_param_out.offset());
  argument_outputs.push_back(param_out_dense_tensor_type);

  pir::Type moment_out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_moment_out.dtype()), dense_moment_out.dims(), dense_moment_out.layout(), dense_moment_out.lod(), dense_moment_out.offset());
  argument_outputs.push_back(moment_out_dense_tensor_type);

  pir::Type inf_norm_out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_inf_norm_out.dtype()), dense_inf_norm_out.dims(), dense_inf_norm_out.layout(), dense_inf_norm_out.lod(), dense_inf_norm_out.offset());
  argument_outputs.push_back(inf_norm_out_dense_tensor_type);

  if (master_param_.impl() != nullptr) {
    pir::Type master_param_out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_master_param_out.dtype()), dense_master_param_out.dims(), dense_master_param_out.layout(), dense_master_param_out.lod(), dense_master_param_out.offset());
    argument_outputs.push_back(master_param_out_dense_tensor_type);
  } else {
    pir::Type master_param_out_type;
    argument_outputs.push_back(master_param_out_type);
  }

  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void Adamax_Op::VerifySig() {
  VLOG(4) << "Start Verifying inputs, outputs and attributes for: Adamax_Op.";
  VLOG(4) << "Verifying inputs:";
  {
  auto input_size = num_operands();
  IR_ENFORCE(input_size == 7u,
                    "The size %d of inputs must be equal to 7.", input_size);
  IR_ENFORCE((*this)->operand_source(0).type().isa<paddle::dialect::DenseTensorType>(),
                  "Type validation failed for the 0th input, got %s.", (*this)->operand_source(0).type());
  IR_ENFORCE((*this)->operand_source(1).type().isa<paddle::dialect::DenseTensorType>(),
                  "Type validation failed for the 1th input, got %s.", (*this)->operand_source(1).type());
  IR_ENFORCE((*this)->operand_source(2).type().isa<paddle::dialect::DenseTensorType>(),
                  "Type validation failed for the 2th input, got %s.", (*this)->operand_source(2).type());
  IR_ENFORCE((*this)->operand_source(3).type().isa<paddle::dialect::DenseTensorType>(),
                  "Type validation failed for the 3th input, got %s.", (*this)->operand_source(3).type());
  IR_ENFORCE((*this)->operand_source(4).type().isa<paddle::dialect::DenseTensorType>(),
                  "Type validation failed for the 4th input, got %s.", (*this)->operand_source(4).type());
  IR_ENFORCE((*this)->operand_source(5).type().isa<paddle::dialect::DenseTensorType>(),
                  "Type validation failed for the 5th input, got %s.", (*this)->operand_source(5).type());
  if (auto val = (*this)->operand(6)) {
    IR_ENFORCE(val.type().isa<paddle::dialect::DenseTensorType>(),
                   "Type validation failed for the 6th input, got %s.", (*this)->operand_source(6).type());
  }
  }
  VLOG(4) << "Verifying attributes:";
  {
  auto& attributes = this->attributes();
  IR_ENFORCE(attributes.count("beta1")>0,
                 "beta1 does not exist.");
  IR_ENFORCE(attributes.at("beta1").isa<pir::FloatAttribute>(),
                 "Type of attribute: beta1 is not pir::FloatAttribute.");

  IR_ENFORCE(attributes.count("beta2")>0,
                 "beta2 does not exist.");
  IR_ENFORCE(attributes.at("beta2").isa<pir::FloatAttribute>(),
                 "Type of attribute: beta2 is not pir::FloatAttribute.");

  IR_ENFORCE(attributes.count("epsilon")>0,
                 "epsilon does not exist.");
  IR_ENFORCE(attributes.at("epsilon").isa<pir::FloatAttribute>(),
                 "Type of attribute: epsilon is not pir::FloatAttribute.");

  IR_ENFORCE(attributes.count("multi_precision")>0,
                 "multi_precision does not exist.");
  IR_ENFORCE(attributes.at("multi_precision").isa<pir::BoolAttribute>(),
                 "Type of attribute: multi_precision is not pir::BoolAttribute.");

  }
  VLOG(4) << "Verifying outputs:";
  {
  auto output_size = num_results();
  IR_ENFORCE(output_size == 4u,
                    "The size %d of outputs must be equal to 4.", output_size);
  IR_ENFORCE((*this)->result(0).type().isa<paddle::dialect::DenseTensorType>(),
                 "Type validation failed for the 0th output.");
  IR_ENFORCE((*this)->result(1).type().isa<paddle::dialect::DenseTensorType>(),
                 "Type validation failed for the 1th output.");
  IR_ENFORCE((*this)->result(2).type().isa<paddle::dialect::DenseTensorType>(),
                 "Type validation failed for the 2th output.");
  if (auto output_3_type = (*this)->result(3).type()) {
    IR_ENFORCE(output_3_type.isa<paddle::dialect::DenseTensorType>(),
                   "Type validation failed for the 3th output.");
  }
  }
  VLOG(4) << "End Verifying for: Adamax_Op.";
}

void Adamax_Op::InferMeta( phi::InferMetaContext *infer_meta ) {
  auto fn = PD_INFER_META(phi::AdamaxInferMeta);
  fn(infer_meta);
}

phi::DataType Adamax_Op::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: Adamax_Op";
  


  return expected_kernel_dtype;
}

const char *Adamw_Op::attributes_name[7] = { "lr_ratio", "coeff", "with_decay", "lazy_mode", "min_row_size_to_use_multithread", "multi_precision", "use_global_beta_pow" };

OpInfoTuple Adamw_Op::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("param", "paddle::dialect::DenseTensorType", false, false, false, false), paddle::dialect::OpInputInfo("grad", "paddle::dialect::DenseTensorType", false, false, false, false), paddle::dialect::OpInputInfo("learning_rate", "paddle::dialect::DenseTensorType", false, false, false, false), paddle::dialect::OpInputInfo("moment1", "paddle::dialect::DenseTensorType", false, false, false, false), paddle::dialect::OpInputInfo("moment2", "paddle::dialect::DenseTensorType", false, false, false, false), paddle::dialect::OpInputInfo("beta1_pow", "paddle::dialect::DenseTensorType", false, false, false, false), paddle::dialect::OpInputInfo("beta2_pow", "paddle::dialect::DenseTensorType", false, false, false, false), paddle::dialect::OpInputInfo("master_param", "paddle::dialect::DenseTensorType", true, false, false, false), paddle::dialect::OpInputInfo("skip_update", "paddle::dialect::DenseTensorType", true, false, false, false), paddle::dialect::OpInputInfo("beta1", "paddle::dialect::ScalarAttribute", false, false, true, false), paddle::dialect::OpInputInfo("beta2", "paddle::dialect::ScalarAttribute", false, false, true, false), paddle::dialect::OpInputInfo("epsilon", "paddle::dialect::ScalarAttribute", false, false, true, false) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = { paddle::dialect::OpAttributeInfo("lr_ratio", "pir::FloatAttribute", ""), paddle::dialect::OpAttributeInfo("coeff", "pir::FloatAttribute", ""), paddle::dialect::OpAttributeInfo("with_decay", "pir::BoolAttribute", ""), paddle::dialect::OpAttributeInfo("lazy_mode", "pir::BoolAttribute", ""), paddle::dialect::OpAttributeInfo("min_row_size_to_use_multithread", "pir::Int64Attribute", ""), paddle::dialect::OpAttributeInfo("multi_precision", "pir::BoolAttribute", ""), paddle::dialect::OpAttributeInfo("use_global_beta_pow", "pir::BoolAttribute", "") };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("param_out", "paddle::dialect::DenseTensorType", false, false), paddle::dialect::OpOutputInfo("moment1_out", "paddle::dialect::DenseTensorType", false, false), paddle::dialect::OpOutputInfo("moment2_out", "paddle::dialect::DenseTensorType", false, false), paddle::dialect::OpOutputInfo("beta1_pow_out", "paddle::dialect::DenseTensorType", false, false), paddle::dialect::OpOutputInfo("beta2_pow_out", "paddle::dialect::DenseTensorType", false, false), paddle::dialect::OpOutputInfo("master_param_out", "paddle::dialect::DenseTensorType", true, false) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("AdamwInferMeta", {"param", "grad", "learning_rate", "moment1", "moment2", "beta1_pow", "beta2_pow", "master_param", "skip_update", "beta1", "beta2", "epsilon", "lr_ratio", "coeff", "with_decay", "lazy_mode", "min_row_size_to_use_multithread", "multi_precision", "use_global_beta_pow"}, "adamw", {"param", "grad", "learning_rate", "moment1", "moment2", "beta1_pow", "beta2_pow", "master_param", "skip_update", "beta1", "beta2", "epsilon", "lr_ratio", "coeff", "with_decay", "lazy_mode", "min_row_size_to_use_multithread", "multi_precision", "use_global_beta_pow"}, {"param"}, {}, {{"param_out", "param"},{"moment1_out", "moment1"},{"moment2_out", "moment2"},{"beta1_pow_out", "beta1_pow"},{"beta2_pow_out", "beta2_pow"},{"master_param_out", "master_param"}}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "adamw_");
}

void Adamw_Op::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value param_, pir::Value grad_, pir::Value learning_rate_, pir::Value moment1_, pir::Value moment2_, pir::Value beta1_pow_, pir::Value beta2_pow_, pir::Value master_param_, pir::Value skip_update_, float beta1, float beta2, float epsilon, float lr_ratio, float coeff, bool with_decay, bool lazy_mode, int64_t min_row_size_to_use_multithread, bool multi_precision, bool use_global_beta_pow) {
  VLOG(4) << "Start build Adamw_Op";


  // Generate scalar mutable attribute: beta1
  paddle::dialect::FullOp full_beta1_op = builder.Build<paddle::dialect::FullOp>(std::vector<int64_t>{1}, beta1, phi::DataType::FLOAT32, phi::CPUPlace());
  pir::OpResult beta1_ = full_beta1_op->result(0);
      // Generate scalar mutable attribute: beta2
  paddle::dialect::FullOp full_beta2_op = builder.Build<paddle::dialect::FullOp>(std::vector<int64_t>{1}, beta2, phi::DataType::FLOAT32, phi::CPUPlace());
  pir::OpResult beta2_ = full_beta2_op->result(0);
      // Generate scalar mutable attribute: epsilon
  paddle::dialect::FullOp full_epsilon_op = builder.Build<paddle::dialect::FullOp>(std::vector<int64_t>{1}, epsilon, phi::DataType::FLOAT32, phi::CPUPlace());
  pir::OpResult epsilon_ = full_epsilon_op->result(0);
    
  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {param_, grad_, learning_rate_, moment1_, moment2_, beta1_pow_, beta2_pow_, master_param_, skip_update_, beta1_, beta2_, epsilon_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_lr_ratio = pir::FloatAttribute::get(pir::IrContext::Instance(), lr_ratio);
  argument.AddAttribute("lr_ratio", attr_lr_ratio);
  pir::Attribute attr_coeff = pir::FloatAttribute::get(pir::IrContext::Instance(), coeff);
  argument.AddAttribute("coeff", attr_coeff);
  pir::Attribute attr_with_decay = pir::BoolAttribute::get(pir::IrContext::Instance(), with_decay);
  argument.AddAttribute("with_decay", attr_with_decay);
  pir::Attribute attr_lazy_mode = pir::BoolAttribute::get(pir::IrContext::Instance(), lazy_mode);
  argument.AddAttribute("lazy_mode", attr_lazy_mode);
  pir::Attribute attr_min_row_size_to_use_multithread = pir::Int64Attribute::get(pir::IrContext::Instance(), min_row_size_to_use_multithread);
  argument.AddAttribute("min_row_size_to_use_multithread", attr_min_row_size_to_use_multithread);
  pir::Attribute attr_multi_precision = pir::BoolAttribute::get(pir::IrContext::Instance(), multi_precision);
  argument.AddAttribute("multi_precision", attr_multi_precision);
  pir::Attribute attr_use_global_beta_pow = pir::BoolAttribute::get(pir::IrContext::Instance(), use_global_beta_pow);
  argument.AddAttribute("use_global_beta_pow", attr_use_global_beta_pow);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType param = param_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)param;
  paddle::dialect::DenseTensorType grad = grad_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)grad;
  paddle::dialect::DenseTensorType learning_rate = learning_rate_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)learning_rate;
  paddle::dialect::DenseTensorType moment1 = moment1_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)moment1;
  paddle::dialect::DenseTensorType moment2 = moment2_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)moment2;
  paddle::dialect::DenseTensorType beta1_pow = beta1_pow_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)beta1_pow;
  paddle::dialect::DenseTensorType beta2_pow = beta2_pow_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)beta2_pow;

  VLOG(4) << "Builder construction  dense_param";
  paddle::dialect::IrTensor ir_tensor_param(paddle::dialect::TransToPhiDataType(param.dtype()),
                                                      param.dims(),
                                                      param.data_layout(),
                                                      param.lod(),
                                                      param.offset());
  VLOG(4) << "Builder construction  meta_param";
  paddle::dialect::IrMetaTensor meta_param(&ir_tensor_param);

  VLOG(4) << "Builder construction  dense_grad";
  paddle::dialect::IrTensor ir_tensor_grad(paddle::dialect::TransToPhiDataType(grad.dtype()),
                                                      grad.dims(),
                                                      grad.data_layout(),
                                                      grad.lod(),
                                                      grad.offset());
  VLOG(4) << "Builder construction  meta_grad";
  paddle::dialect::IrMetaTensor meta_grad(&ir_tensor_grad);

  VLOG(4) << "Builder construction  dense_learning_rate";
  paddle::dialect::IrTensor ir_tensor_learning_rate(paddle::dialect::TransToPhiDataType(learning_rate.dtype()),
                                                      learning_rate.dims(),
                                                      learning_rate.data_layout(),
                                                      learning_rate.lod(),
                                                      learning_rate.offset());
  VLOG(4) << "Builder construction  meta_learning_rate";
  paddle::dialect::IrMetaTensor meta_learning_rate(&ir_tensor_learning_rate);

  VLOG(4) << "Builder construction  dense_moment1";
  paddle::dialect::IrTensor ir_tensor_moment1(paddle::dialect::TransToPhiDataType(moment1.dtype()),
                                                      moment1.dims(),
                                                      moment1.data_layout(),
                                                      moment1.lod(),
                                                      moment1.offset());
  VLOG(4) << "Builder construction  meta_moment1";
  paddle::dialect::IrMetaTensor meta_moment1(&ir_tensor_moment1);

  VLOG(4) << "Builder construction  dense_moment2";
  paddle::dialect::IrTensor ir_tensor_moment2(paddle::dialect::TransToPhiDataType(moment2.dtype()),
                                                      moment2.dims(),
                                                      moment2.data_layout(),
                                                      moment2.lod(),
                                                      moment2.offset());
  VLOG(4) << "Builder construction  meta_moment2";
  paddle::dialect::IrMetaTensor meta_moment2(&ir_tensor_moment2);

  VLOG(4) << "Builder construction  dense_beta1_pow";
  paddle::dialect::IrTensor ir_tensor_beta1_pow(paddle::dialect::TransToPhiDataType(beta1_pow.dtype()),
                                                      beta1_pow.dims(),
                                                      beta1_pow.data_layout(),
                                                      beta1_pow.lod(),
                                                      beta1_pow.offset());
  VLOG(4) << "Builder construction  meta_beta1_pow";
  paddle::dialect::IrMetaTensor meta_beta1_pow(&ir_tensor_beta1_pow);

  VLOG(4) << "Builder construction  dense_beta2_pow";
  paddle::dialect::IrTensor ir_tensor_beta2_pow(paddle::dialect::TransToPhiDataType(beta2_pow.dtype()),
                                                      beta2_pow.dims(),
                                                      beta2_pow.data_layout(),
                                                      beta2_pow.lod(),
                                                      beta2_pow.offset());
  VLOG(4) << "Builder construction  meta_beta2_pow";
  paddle::dialect::IrMetaTensor meta_beta2_pow(&ir_tensor_beta2_pow);

  paddle::dialect::IrMetaTensor meta_master_param;
  paddle::dialect::IrTensor ir_tensor_master_param;
  if (master_param_.impl() != nullptr) {
    paddle::dialect::DenseTensorType master_param = master_param_.type().dyn_cast<paddle::dialect::DenseTensorType>();
    VLOG(4) << "Builder construction  dense_master_param";
    ir_tensor_master_param = paddle::dialect::IrTensor(paddle::dialect::TransToPhiDataType(master_param.dtype()),
                                                        master_param.dims(),
                                                        master_param.data_layout(),
                                                        master_param.lod(),
                                                        master_param.offset());
    VLOG(4) << "Builder construction  meta_master_param";
    meta_master_param = paddle::dialect::IrMetaTensor(&ir_tensor_master_param);
  }


  paddle::dialect::IrMetaTensor meta_skip_update;
  paddle::dialect::IrTensor ir_tensor_skip_update;
  if (skip_update_.impl() != nullptr) {
    paddle::dialect::DenseTensorType skip_update = skip_update_.type().dyn_cast<paddle::dialect::DenseTensorType>();
    VLOG(4) << "Builder construction  dense_skip_update";
    ir_tensor_skip_update = paddle::dialect::IrTensor(paddle::dialect::TransToPhiDataType(skip_update.dtype()),
                                                        skip_update.dims(),
                                                        skip_update.data_layout(),
                                                        skip_update.lod(),
                                                        skip_update.offset());
    VLOG(4) << "Builder construction  meta_skip_update";
    meta_skip_update = paddle::dialect::IrMetaTensor(&ir_tensor_skip_update);
  }

  paddle::dialect::IrTensor dense_param_out;
  paddle::dialect::IrMetaTensor meta_param_out(&dense_param_out);
  paddle::dialect::IrTensor dense_moment1_out;
  paddle::dialect::IrMetaTensor meta_moment1_out(&dense_moment1_out);
  paddle::dialect::IrTensor dense_moment2_out;
  paddle::dialect::IrMetaTensor meta_moment2_out(&dense_moment2_out);
  paddle::dialect::IrTensor dense_beta1_pow_out;
  paddle::dialect::IrMetaTensor meta_beta1_pow_out(&dense_beta1_pow_out);
  paddle::dialect::IrTensor dense_beta2_pow_out;
  paddle::dialect::IrMetaTensor meta_beta2_pow_out(&dense_beta2_pow_out);
  paddle::dialect::IrTensor dense_master_param_out;
  paddle::dialect::IrMetaTensor meta_master_param_out(&dense_master_param_out);

  phi::AdamwInferMeta(meta_param, meta_grad, meta_learning_rate, meta_moment1, meta_moment2, meta_beta1_pow, meta_beta2_pow, meta_master_param, meta_skip_update, beta1, beta2, epsilon, lr_ratio, coeff, with_decay, lazy_mode, min_row_size_to_use_multithread, multi_precision, use_global_beta_pow, &meta_param_out, &meta_moment1_out, &meta_moment2_out, &meta_beta1_pow_out, &meta_beta2_pow_out, &meta_master_param_out);

  std::vector<pir::Type> argument_outputs;
  pir::Type param_out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_param_out.dtype()), dense_param_out.dims(), dense_param_out.layout(), dense_param_out.lod(), dense_param_out.offset());
  argument_outputs.push_back(param_out_dense_tensor_type);

  pir::Type moment1_out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_moment1_out.dtype()), dense_moment1_out.dims(), dense_moment1_out.layout(), dense_moment1_out.lod(), dense_moment1_out.offset());
  argument_outputs.push_back(moment1_out_dense_tensor_type);

  pir::Type moment2_out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_moment2_out.dtype()), dense_moment2_out.dims(), dense_moment2_out.layout(), dense_moment2_out.lod(), dense_moment2_out.offset());
  argument_outputs.push_back(moment2_out_dense_tensor_type);

  pir::Type beta1_pow_out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_beta1_pow_out.dtype()), dense_beta1_pow_out.dims(), dense_beta1_pow_out.layout(), dense_beta1_pow_out.lod(), dense_beta1_pow_out.offset());
  argument_outputs.push_back(beta1_pow_out_dense_tensor_type);

  pir::Type beta2_pow_out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_beta2_pow_out.dtype()), dense_beta2_pow_out.dims(), dense_beta2_pow_out.layout(), dense_beta2_pow_out.lod(), dense_beta2_pow_out.offset());
  argument_outputs.push_back(beta2_pow_out_dense_tensor_type);

  if (master_param_.impl() != nullptr) {
    pir::Type master_param_out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_master_param_out.dtype()), dense_master_param_out.dims(), dense_master_param_out.layout(), dense_master_param_out.lod(), dense_master_param_out.offset());
    argument_outputs.push_back(master_param_out_dense_tensor_type);
  } else {
    pir::Type master_param_out_type;
    argument_outputs.push_back(master_param_out_type);
  }

  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void Adamw_Op::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value param_, pir::Value grad_, pir::Value learning_rate_, pir::Value moment1_, pir::Value moment2_, pir::Value beta1_pow_, pir::Value beta2_pow_, pir::Value master_param_, pir::Value skip_update_, pir::AttributeMap attributes) {
  VLOG(4) << "Start build Adamw_Op";


  IR_ENFORCE(
      attributes.find("beta1") != attributes.end(),
          "'beta1' Attribute is expected for Adamw_Op. ");
  float beta1 = attributes.at("beta1").dyn_cast<paddle::dialect::ScalarAttribute>().data().to<float>();

  IR_ENFORCE(
      attributes.find("beta2") != attributes.end(),
          "'beta2' Attribute is expected for Adamw_Op. ");
  float beta2 = attributes.at("beta2").dyn_cast<paddle::dialect::ScalarAttribute>().data().to<float>();

  IR_ENFORCE(
      attributes.find("epsilon") != attributes.end(),
          "'epsilon' Attribute is expected for Adamw_Op. ");
  float epsilon = attributes.at("epsilon").dyn_cast<paddle::dialect::ScalarAttribute>().data().to<float>();

  IR_ENFORCE(
      attributes.find("lr_ratio") != attributes.end(),
          "'lr_ratio' Attribute is expected for Adamw_Op. ");
  float lr_ratio = attributes.at("lr_ratio").dyn_cast<pir::FloatAttribute>().data();

  IR_ENFORCE(
      attributes.find("coeff") != attributes.end(),
          "'coeff' Attribute is expected for Adamw_Op. ");
  float coeff = attributes.at("coeff").dyn_cast<pir::FloatAttribute>().data();

  IR_ENFORCE(
      attributes.find("with_decay") != attributes.end(),
          "'with_decay' Attribute is expected for Adamw_Op. ");
  bool with_decay = attributes.at("with_decay").dyn_cast<pir::BoolAttribute>().data();

  IR_ENFORCE(
      attributes.find("lazy_mode") != attributes.end(),
          "'lazy_mode' Attribute is expected for Adamw_Op. ");
  bool lazy_mode = attributes.at("lazy_mode").dyn_cast<pir::BoolAttribute>().data();

  IR_ENFORCE(
      attributes.find("min_row_size_to_use_multithread") != attributes.end(),
          "'min_row_size_to_use_multithread' Attribute is expected for Adamw_Op. ");
  int64_t min_row_size_to_use_multithread = attributes.at("min_row_size_to_use_multithread").dyn_cast<pir::Int64Attribute>().data();

  IR_ENFORCE(
      attributes.find("multi_precision") != attributes.end(),
          "'multi_precision' Attribute is expected for Adamw_Op. ");
  bool multi_precision = attributes.at("multi_precision").dyn_cast<pir::BoolAttribute>().data();

  IR_ENFORCE(
      attributes.find("use_global_beta_pow") != attributes.end(),
          "'use_global_beta_pow' Attribute is expected for Adamw_Op. ");
  bool use_global_beta_pow = attributes.at("use_global_beta_pow").dyn_cast<pir::BoolAttribute>().data();

  // Generate scalar mutable attribute: beta1
  paddle::dialect::FullOp full_beta1_op = builder.Build<paddle::dialect::FullOp>(std::vector<int64_t>{1}, beta1, phi::DataType::FLOAT32, phi::CPUPlace());
  pir::OpResult beta1_ = full_beta1_op->result(0);
      // Generate scalar mutable attribute: beta2
  paddle::dialect::FullOp full_beta2_op = builder.Build<paddle::dialect::FullOp>(std::vector<int64_t>{1}, beta2, phi::DataType::FLOAT32, phi::CPUPlace());
  pir::OpResult beta2_ = full_beta2_op->result(0);
      // Generate scalar mutable attribute: epsilon
  paddle::dialect::FullOp full_epsilon_op = builder.Build<paddle::dialect::FullOp>(std::vector<int64_t>{1}, epsilon, phi::DataType::FLOAT32, phi::CPUPlace());
  pir::OpResult epsilon_ = full_epsilon_op->result(0);
    
  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {param_, grad_, learning_rate_, moment1_, moment2_, beta1_pow_, beta2_pow_, master_param_, skip_update_, beta1_, beta2_, epsilon_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_lr_ratio = pir::FloatAttribute::get(pir::IrContext::Instance(), lr_ratio);
  argument.AddAttribute("lr_ratio", attr_lr_ratio);
  pir::Attribute attr_coeff = pir::FloatAttribute::get(pir::IrContext::Instance(), coeff);
  argument.AddAttribute("coeff", attr_coeff);
  pir::Attribute attr_with_decay = pir::BoolAttribute::get(pir::IrContext::Instance(), with_decay);
  argument.AddAttribute("with_decay", attr_with_decay);
  pir::Attribute attr_lazy_mode = pir::BoolAttribute::get(pir::IrContext::Instance(), lazy_mode);
  argument.AddAttribute("lazy_mode", attr_lazy_mode);
  pir::Attribute attr_min_row_size_to_use_multithread = pir::Int64Attribute::get(pir::IrContext::Instance(), min_row_size_to_use_multithread);
  argument.AddAttribute("min_row_size_to_use_multithread", attr_min_row_size_to_use_multithread);
  pir::Attribute attr_multi_precision = pir::BoolAttribute::get(pir::IrContext::Instance(), multi_precision);
  argument.AddAttribute("multi_precision", attr_multi_precision);
  pir::Attribute attr_use_global_beta_pow = pir::BoolAttribute::get(pir::IrContext::Instance(), use_global_beta_pow);
  argument.AddAttribute("use_global_beta_pow", attr_use_global_beta_pow);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType param = param_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)param;
  paddle::dialect::DenseTensorType grad = grad_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)grad;
  paddle::dialect::DenseTensorType learning_rate = learning_rate_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)learning_rate;
  paddle::dialect::DenseTensorType moment1 = moment1_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)moment1;
  paddle::dialect::DenseTensorType moment2 = moment2_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)moment2;
  paddle::dialect::DenseTensorType beta1_pow = beta1_pow_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)beta1_pow;
  paddle::dialect::DenseTensorType beta2_pow = beta2_pow_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)beta2_pow;

  VLOG(4) << "Builder construction  dense_param";
  paddle::dialect::IrTensor ir_tensor_param(paddle::dialect::TransToPhiDataType(param.dtype()),
                                                      param.dims(),
                                                      param.data_layout(),
                                                      param.lod(),
                                                      param.offset());
  VLOG(4) << "Builder construction  meta_param";
  paddle::dialect::IrMetaTensor meta_param(&ir_tensor_param);

  VLOG(4) << "Builder construction  dense_grad";
  paddle::dialect::IrTensor ir_tensor_grad(paddle::dialect::TransToPhiDataType(grad.dtype()),
                                                      grad.dims(),
                                                      grad.data_layout(),
                                                      grad.lod(),
                                                      grad.offset());
  VLOG(4) << "Builder construction  meta_grad";
  paddle::dialect::IrMetaTensor meta_grad(&ir_tensor_grad);

  VLOG(4) << "Builder construction  dense_learning_rate";
  paddle::dialect::IrTensor ir_tensor_learning_rate(paddle::dialect::TransToPhiDataType(learning_rate.dtype()),
                                                      learning_rate.dims(),
                                                      learning_rate.data_layout(),
                                                      learning_rate.lod(),
                                                      learning_rate.offset());
  VLOG(4) << "Builder construction  meta_learning_rate";
  paddle::dialect::IrMetaTensor meta_learning_rate(&ir_tensor_learning_rate);

  VLOG(4) << "Builder construction  dense_moment1";
  paddle::dialect::IrTensor ir_tensor_moment1(paddle::dialect::TransToPhiDataType(moment1.dtype()),
                                                      moment1.dims(),
                                                      moment1.data_layout(),
                                                      moment1.lod(),
                                                      moment1.offset());
  VLOG(4) << "Builder construction  meta_moment1";
  paddle::dialect::IrMetaTensor meta_moment1(&ir_tensor_moment1);

  VLOG(4) << "Builder construction  dense_moment2";
  paddle::dialect::IrTensor ir_tensor_moment2(paddle::dialect::TransToPhiDataType(moment2.dtype()),
                                                      moment2.dims(),
                                                      moment2.data_layout(),
                                                      moment2.lod(),
                                                      moment2.offset());
  VLOG(4) << "Builder construction  meta_moment2";
  paddle::dialect::IrMetaTensor meta_moment2(&ir_tensor_moment2);

  VLOG(4) << "Builder construction  dense_beta1_pow";
  paddle::dialect::IrTensor ir_tensor_beta1_pow(paddle::dialect::TransToPhiDataType(beta1_pow.dtype()),
                                                      beta1_pow.dims(),
                                                      beta1_pow.data_layout(),
                                                      beta1_pow.lod(),
                                                      beta1_pow.offset());
  VLOG(4) << "Builder construction  meta_beta1_pow";
  paddle::dialect::IrMetaTensor meta_beta1_pow(&ir_tensor_beta1_pow);

  VLOG(4) << "Builder construction  dense_beta2_pow";
  paddle::dialect::IrTensor ir_tensor_beta2_pow(paddle::dialect::TransToPhiDataType(beta2_pow.dtype()),
                                                      beta2_pow.dims(),
                                                      beta2_pow.data_layout(),
                                                      beta2_pow.lod(),
                                                      beta2_pow.offset());
  VLOG(4) << "Builder construction  meta_beta2_pow";
  paddle::dialect::IrMetaTensor meta_beta2_pow(&ir_tensor_beta2_pow);

  paddle::dialect::IrMetaTensor meta_master_param;
  paddle::dialect::IrTensor ir_tensor_master_param;
  if (master_param_.impl() != nullptr) {
    paddle::dialect::DenseTensorType master_param = master_param_.type().dyn_cast<paddle::dialect::DenseTensorType>();
    VLOG(4) << "Builder construction  dense_master_param";
    ir_tensor_master_param = paddle::dialect::IrTensor(paddle::dialect::TransToPhiDataType(master_param.dtype()),
                                                        master_param.dims(),
                                                        master_param.data_layout(),
                                                        master_param.lod(),
                                                        master_param.offset());
    VLOG(4) << "Builder construction  meta_master_param";
    meta_master_param = paddle::dialect::IrMetaTensor(&ir_tensor_master_param);
  }


  paddle::dialect::IrMetaTensor meta_skip_update;
  paddle::dialect::IrTensor ir_tensor_skip_update;
  if (skip_update_.impl() != nullptr) {
    paddle::dialect::DenseTensorType skip_update = skip_update_.type().dyn_cast<paddle::dialect::DenseTensorType>();
    VLOG(4) << "Builder construction  dense_skip_update";
    ir_tensor_skip_update = paddle::dialect::IrTensor(paddle::dialect::TransToPhiDataType(skip_update.dtype()),
                                                        skip_update.dims(),
                                                        skip_update.data_layout(),
                                                        skip_update.lod(),
                                                        skip_update.offset());
    VLOG(4) << "Builder construction  meta_skip_update";
    meta_skip_update = paddle::dialect::IrMetaTensor(&ir_tensor_skip_update);
  }

  paddle::dialect::IrTensor dense_param_out;
  paddle::dialect::IrMetaTensor meta_param_out(&dense_param_out);
  paddle::dialect::IrTensor dense_moment1_out;
  paddle::dialect::IrMetaTensor meta_moment1_out(&dense_moment1_out);
  paddle::dialect::IrTensor dense_moment2_out;
  paddle::dialect::IrMetaTensor meta_moment2_out(&dense_moment2_out);
  paddle::dialect::IrTensor dense_beta1_pow_out;
  paddle::dialect::IrMetaTensor meta_beta1_pow_out(&dense_beta1_pow_out);
  paddle::dialect::IrTensor dense_beta2_pow_out;
  paddle::dialect::IrMetaTensor meta_beta2_pow_out(&dense_beta2_pow_out);
  paddle::dialect::IrTensor dense_master_param_out;
  paddle::dialect::IrMetaTensor meta_master_param_out(&dense_master_param_out);

  phi::AdamwInferMeta(meta_param, meta_grad, meta_learning_rate, meta_moment1, meta_moment2, meta_beta1_pow, meta_beta2_pow, meta_master_param, meta_skip_update, beta1, beta2, epsilon, lr_ratio, coeff, with_decay, lazy_mode, min_row_size_to_use_multithread, multi_precision, use_global_beta_pow, &meta_param_out, &meta_moment1_out, &meta_moment2_out, &meta_beta1_pow_out, &meta_beta2_pow_out, &meta_master_param_out);

  std::vector<pir::Type> argument_outputs;
  pir::Type param_out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_param_out.dtype()), dense_param_out.dims(), dense_param_out.layout(), dense_param_out.lod(), dense_param_out.offset());
  argument_outputs.push_back(param_out_dense_tensor_type);

  pir::Type moment1_out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_moment1_out.dtype()), dense_moment1_out.dims(), dense_moment1_out.layout(), dense_moment1_out.lod(), dense_moment1_out.offset());
  argument_outputs.push_back(moment1_out_dense_tensor_type);

  pir::Type moment2_out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_moment2_out.dtype()), dense_moment2_out.dims(), dense_moment2_out.layout(), dense_moment2_out.lod(), dense_moment2_out.offset());
  argument_outputs.push_back(moment2_out_dense_tensor_type);

  pir::Type beta1_pow_out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_beta1_pow_out.dtype()), dense_beta1_pow_out.dims(), dense_beta1_pow_out.layout(), dense_beta1_pow_out.lod(), dense_beta1_pow_out.offset());
  argument_outputs.push_back(beta1_pow_out_dense_tensor_type);

  pir::Type beta2_pow_out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_beta2_pow_out.dtype()), dense_beta2_pow_out.dims(), dense_beta2_pow_out.layout(), dense_beta2_pow_out.lod(), dense_beta2_pow_out.offset());
  argument_outputs.push_back(beta2_pow_out_dense_tensor_type);

  if (master_param_.impl() != nullptr) {
    pir::Type master_param_out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_master_param_out.dtype()), dense_master_param_out.dims(), dense_master_param_out.layout(), dense_master_param_out.lod(), dense_master_param_out.offset());
    argument_outputs.push_back(master_param_out_dense_tensor_type);
  } else {
    pir::Type master_param_out_type;
    argument_outputs.push_back(master_param_out_type);
  }

  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void Adamw_Op::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value param_, pir::Value grad_, pir::Value learning_rate_, pir::Value moment1_, pir::Value moment2_, pir::Value beta1_pow_, pir::Value beta2_pow_, pir::Value master_param_, pir::Value skip_update_, pir::Value beta1_, pir::Value beta2_, pir::Value epsilon_, float lr_ratio, float coeff, bool with_decay, bool lazy_mode, int64_t min_row_size_to_use_multithread, bool multi_precision, bool use_global_beta_pow) {
  VLOG(4) << "Start build Adamw_Op";



  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {param_, grad_, learning_rate_, moment1_, moment2_, beta1_pow_, beta2_pow_, master_param_, skip_update_, beta1_, beta2_, epsilon_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_lr_ratio = pir::FloatAttribute::get(pir::IrContext::Instance(), lr_ratio);
  argument.AddAttribute("lr_ratio", attr_lr_ratio);
  pir::Attribute attr_coeff = pir::FloatAttribute::get(pir::IrContext::Instance(), coeff);
  argument.AddAttribute("coeff", attr_coeff);
  pir::Attribute attr_with_decay = pir::BoolAttribute::get(pir::IrContext::Instance(), with_decay);
  argument.AddAttribute("with_decay", attr_with_decay);
  pir::Attribute attr_lazy_mode = pir::BoolAttribute::get(pir::IrContext::Instance(), lazy_mode);
  argument.AddAttribute("lazy_mode", attr_lazy_mode);
  pir::Attribute attr_min_row_size_to_use_multithread = pir::Int64Attribute::get(pir::IrContext::Instance(), min_row_size_to_use_multithread);
  argument.AddAttribute("min_row_size_to_use_multithread", attr_min_row_size_to_use_multithread);
  pir::Attribute attr_multi_precision = pir::BoolAttribute::get(pir::IrContext::Instance(), multi_precision);
  argument.AddAttribute("multi_precision", attr_multi_precision);
  pir::Attribute attr_use_global_beta_pow = pir::BoolAttribute::get(pir::IrContext::Instance(), use_global_beta_pow);
  argument.AddAttribute("use_global_beta_pow", attr_use_global_beta_pow);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType param = param_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)param;
  paddle::dialect::DenseTensorType grad = grad_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)grad;
  paddle::dialect::DenseTensorType learning_rate = learning_rate_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)learning_rate;
  paddle::dialect::DenseTensorType moment1 = moment1_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)moment1;
  paddle::dialect::DenseTensorType moment2 = moment2_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)moment2;
  paddle::dialect::DenseTensorType beta1_pow = beta1_pow_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)beta1_pow;
  paddle::dialect::DenseTensorType beta2_pow = beta2_pow_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)beta2_pow;
  phi::Scalar beta1;
  if (beta1_.dyn_cast<pir::OpResult>() && beta1_.dyn_cast<pir::OpResult>().owner()->isa<paddle::dialect::FullOp>()) {
    beta1 = std::move(phi::Scalar(beta1_.dyn_cast<pir::OpResult>().owner()
                                  ->dyn_cast<paddle::dialect::FullOp>()
                                  .attribute("value")
                                  .dyn_cast<paddle::dialect::ScalarAttribute>()
                                  .data()
                                  .to<int>()));
  }
  else {
    beta1 = std::move(phi::Scalar(-1));
    beta1.SetFromTensor(true);
  }
  phi::Scalar beta2;
  if (beta2_.dyn_cast<pir::OpResult>() && beta2_.dyn_cast<pir::OpResult>().owner()->isa<paddle::dialect::FullOp>()) {
    beta2 = std::move(phi::Scalar(beta2_.dyn_cast<pir::OpResult>().owner()
                                  ->dyn_cast<paddle::dialect::FullOp>()
                                  .attribute("value")
                                  .dyn_cast<paddle::dialect::ScalarAttribute>()
                                  .data()
                                  .to<int>()));
  }
  else {
    beta2 = std::move(phi::Scalar(-1));
    beta2.SetFromTensor(true);
  }
  phi::Scalar epsilon;
  if (epsilon_.dyn_cast<pir::OpResult>() && epsilon_.dyn_cast<pir::OpResult>().owner()->isa<paddle::dialect::FullOp>()) {
    epsilon = std::move(phi::Scalar(epsilon_.dyn_cast<pir::OpResult>().owner()
                                  ->dyn_cast<paddle::dialect::FullOp>()
                                  .attribute("value")
                                  .dyn_cast<paddle::dialect::ScalarAttribute>()
                                  .data()
                                  .to<int>()));
  }
  else {
    epsilon = std::move(phi::Scalar(-1));
    epsilon.SetFromTensor(true);
  }


  VLOG(4) << "Builder construction  dense_param";
  paddle::dialect::IrTensor ir_tensor_param(paddle::dialect::TransToPhiDataType(param.dtype()),
                                                      param.dims(),
                                                      param.data_layout(),
                                                      param.lod(),
                                                      param.offset());
  VLOG(4) << "Builder construction  meta_param";
  paddle::dialect::IrMetaTensor meta_param(&ir_tensor_param);

  VLOG(4) << "Builder construction  dense_grad";
  paddle::dialect::IrTensor ir_tensor_grad(paddle::dialect::TransToPhiDataType(grad.dtype()),
                                                      grad.dims(),
                                                      grad.data_layout(),
                                                      grad.lod(),
                                                      grad.offset());
  VLOG(4) << "Builder construction  meta_grad";
  paddle::dialect::IrMetaTensor meta_grad(&ir_tensor_grad);

  VLOG(4) << "Builder construction  dense_learning_rate";
  paddle::dialect::IrTensor ir_tensor_learning_rate(paddle::dialect::TransToPhiDataType(learning_rate.dtype()),
                                                      learning_rate.dims(),
                                                      learning_rate.data_layout(),
                                                      learning_rate.lod(),
                                                      learning_rate.offset());
  VLOG(4) << "Builder construction  meta_learning_rate";
  paddle::dialect::IrMetaTensor meta_learning_rate(&ir_tensor_learning_rate);

  VLOG(4) << "Builder construction  dense_moment1";
  paddle::dialect::IrTensor ir_tensor_moment1(paddle::dialect::TransToPhiDataType(moment1.dtype()),
                                                      moment1.dims(),
                                                      moment1.data_layout(),
                                                      moment1.lod(),
                                                      moment1.offset());
  VLOG(4) << "Builder construction  meta_moment1";
  paddle::dialect::IrMetaTensor meta_moment1(&ir_tensor_moment1);

  VLOG(4) << "Builder construction  dense_moment2";
  paddle::dialect::IrTensor ir_tensor_moment2(paddle::dialect::TransToPhiDataType(moment2.dtype()),
                                                      moment2.dims(),
                                                      moment2.data_layout(),
                                                      moment2.lod(),
                                                      moment2.offset());
  VLOG(4) << "Builder construction  meta_moment2";
  paddle::dialect::IrMetaTensor meta_moment2(&ir_tensor_moment2);

  VLOG(4) << "Builder construction  dense_beta1_pow";
  paddle::dialect::IrTensor ir_tensor_beta1_pow(paddle::dialect::TransToPhiDataType(beta1_pow.dtype()),
                                                      beta1_pow.dims(),
                                                      beta1_pow.data_layout(),
                                                      beta1_pow.lod(),
                                                      beta1_pow.offset());
  VLOG(4) << "Builder construction  meta_beta1_pow";
  paddle::dialect::IrMetaTensor meta_beta1_pow(&ir_tensor_beta1_pow);

  VLOG(4) << "Builder construction  dense_beta2_pow";
  paddle::dialect::IrTensor ir_tensor_beta2_pow(paddle::dialect::TransToPhiDataType(beta2_pow.dtype()),
                                                      beta2_pow.dims(),
                                                      beta2_pow.data_layout(),
                                                      beta2_pow.lod(),
                                                      beta2_pow.offset());
  VLOG(4) << "Builder construction  meta_beta2_pow";
  paddle::dialect::IrMetaTensor meta_beta2_pow(&ir_tensor_beta2_pow);

  paddle::dialect::IrMetaTensor meta_master_param;
  paddle::dialect::IrTensor ir_tensor_master_param;
  if (master_param_.impl() != nullptr) {
    paddle::dialect::DenseTensorType master_param = master_param_.type().dyn_cast<paddle::dialect::DenseTensorType>();
    VLOG(4) << "Builder construction  dense_master_param";
    ir_tensor_master_param = paddle::dialect::IrTensor(paddle::dialect::TransToPhiDataType(master_param.dtype()),
                                                        master_param.dims(),
                                                        master_param.data_layout(),
                                                        master_param.lod(),
                                                        master_param.offset());
    VLOG(4) << "Builder construction  meta_master_param";
    meta_master_param = paddle::dialect::IrMetaTensor(&ir_tensor_master_param);
  }


  paddle::dialect::IrMetaTensor meta_skip_update;
  paddle::dialect::IrTensor ir_tensor_skip_update;
  if (skip_update_.impl() != nullptr) {
    paddle::dialect::DenseTensorType skip_update = skip_update_.type().dyn_cast<paddle::dialect::DenseTensorType>();
    VLOG(4) << "Builder construction  dense_skip_update";
    ir_tensor_skip_update = paddle::dialect::IrTensor(paddle::dialect::TransToPhiDataType(skip_update.dtype()),
                                                        skip_update.dims(),
                                                        skip_update.data_layout(),
                                                        skip_update.lod(),
                                                        skip_update.offset());
    VLOG(4) << "Builder construction  meta_skip_update";
    meta_skip_update = paddle::dialect::IrMetaTensor(&ir_tensor_skip_update);
  }

  paddle::dialect::IrTensor dense_param_out;
  paddle::dialect::IrMetaTensor meta_param_out(&dense_param_out);
  paddle::dialect::IrTensor dense_moment1_out;
  paddle::dialect::IrMetaTensor meta_moment1_out(&dense_moment1_out);
  paddle::dialect::IrTensor dense_moment2_out;
  paddle::dialect::IrMetaTensor meta_moment2_out(&dense_moment2_out);
  paddle::dialect::IrTensor dense_beta1_pow_out;
  paddle::dialect::IrMetaTensor meta_beta1_pow_out(&dense_beta1_pow_out);
  paddle::dialect::IrTensor dense_beta2_pow_out;
  paddle::dialect::IrMetaTensor meta_beta2_pow_out(&dense_beta2_pow_out);
  paddle::dialect::IrTensor dense_master_param_out;
  paddle::dialect::IrMetaTensor meta_master_param_out(&dense_master_param_out);

  phi::AdamwInferMeta(meta_param, meta_grad, meta_learning_rate, meta_moment1, meta_moment2, meta_beta1_pow, meta_beta2_pow, meta_master_param, meta_skip_update, beta1, beta2, epsilon, lr_ratio, coeff, with_decay, lazy_mode, min_row_size_to_use_multithread, multi_precision, use_global_beta_pow, &meta_param_out, &meta_moment1_out, &meta_moment2_out, &meta_beta1_pow_out, &meta_beta2_pow_out, &meta_master_param_out);

  std::vector<pir::Type> argument_outputs;
  pir::Type param_out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_param_out.dtype()), dense_param_out.dims(), dense_param_out.layout(), dense_param_out.lod(), dense_param_out.offset());
  argument_outputs.push_back(param_out_dense_tensor_type);

  pir::Type moment1_out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_moment1_out.dtype()), dense_moment1_out.dims(), dense_moment1_out.layout(), dense_moment1_out.lod(), dense_moment1_out.offset());
  argument_outputs.push_back(moment1_out_dense_tensor_type);

  pir::Type moment2_out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_moment2_out.dtype()), dense_moment2_out.dims(), dense_moment2_out.layout(), dense_moment2_out.lod(), dense_moment2_out.offset());
  argument_outputs.push_back(moment2_out_dense_tensor_type);

  pir::Type beta1_pow_out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_beta1_pow_out.dtype()), dense_beta1_pow_out.dims(), dense_beta1_pow_out.layout(), dense_beta1_pow_out.lod(), dense_beta1_pow_out.offset());
  argument_outputs.push_back(beta1_pow_out_dense_tensor_type);

  pir::Type beta2_pow_out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_beta2_pow_out.dtype()), dense_beta2_pow_out.dims(), dense_beta2_pow_out.layout(), dense_beta2_pow_out.lod(), dense_beta2_pow_out.offset());
  argument_outputs.push_back(beta2_pow_out_dense_tensor_type);

  if (master_param_.impl() != nullptr) {
    pir::Type master_param_out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_master_param_out.dtype()), dense_master_param_out.dims(), dense_master_param_out.layout(), dense_master_param_out.lod(), dense_master_param_out.offset());
    argument_outputs.push_back(master_param_out_dense_tensor_type);
  } else {
    pir::Type master_param_out_type;
    argument_outputs.push_back(master_param_out_type);
  }

  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void Adamw_Op::VerifySig() {
  VLOG(4) << "Start Verifying inputs, outputs and attributes for: Adamw_Op.";
  VLOG(4) << "Verifying inputs:";
  {
  auto input_size = num_operands();
  IR_ENFORCE(input_size == 12u,
                    "The size %d of inputs must be equal to 12.", input_size);
  IR_ENFORCE((*this)->operand_source(0).type().isa<paddle::dialect::DenseTensorType>(),
                  "Type validation failed for the 0th input, got %s.", (*this)->operand_source(0).type());
  IR_ENFORCE((*this)->operand_source(1).type().isa<paddle::dialect::DenseTensorType>(),
                  "Type validation failed for the 1th input, got %s.", (*this)->operand_source(1).type());
  IR_ENFORCE((*this)->operand_source(2).type().isa<paddle::dialect::DenseTensorType>(),
                  "Type validation failed for the 2th input, got %s.", (*this)->operand_source(2).type());
  IR_ENFORCE((*this)->operand_source(3).type().isa<paddle::dialect::DenseTensorType>(),
                  "Type validation failed for the 3th input, got %s.", (*this)->operand_source(3).type());
  IR_ENFORCE((*this)->operand_source(4).type().isa<paddle::dialect::DenseTensorType>(),
                  "Type validation failed for the 4th input, got %s.", (*this)->operand_source(4).type());
  IR_ENFORCE((*this)->operand_source(5).type().isa<paddle::dialect::DenseTensorType>(),
                  "Type validation failed for the 5th input, got %s.", (*this)->operand_source(5).type());
  IR_ENFORCE((*this)->operand_source(6).type().isa<paddle::dialect::DenseTensorType>(),
                  "Type validation failed for the 6th input, got %s.", (*this)->operand_source(6).type());
  if (auto val = (*this)->operand(7)) {
    IR_ENFORCE(val.type().isa<paddle::dialect::DenseTensorType>(),
                   "Type validation failed for the 7th input, got %s.", (*this)->operand_source(7).type());
  }
  if (auto val = (*this)->operand(8)) {
    IR_ENFORCE(val.type().isa<paddle::dialect::DenseTensorType>(),
                   "Type validation failed for the 8th input, got %s.", (*this)->operand_source(8).type());
  }
  IR_ENFORCE((*this)->operand_source(9).type().isa<paddle::dialect::DenseTensorType>(),
                  "Type validation failed for the 9th input, got %s.", (*this)->operand_source(9).type());
  IR_ENFORCE((*this)->operand_source(10).type().isa<paddle::dialect::DenseTensorType>(),
                  "Type validation failed for the 10th input, got %s.", (*this)->operand_source(10).type());
  IR_ENFORCE((*this)->operand_source(11).type().isa<paddle::dialect::DenseTensorType>(),
                  "Type validation failed for the 11th input, got %s.", (*this)->operand_source(11).type());
  }
  VLOG(4) << "Verifying attributes:";
  {
  auto& attributes = this->attributes();
  IR_ENFORCE(attributes.count("lr_ratio")>0,
                 "lr_ratio does not exist.");
  IR_ENFORCE(attributes.at("lr_ratio").isa<pir::FloatAttribute>(),
                 "Type of attribute: lr_ratio is not pir::FloatAttribute.");

  IR_ENFORCE(attributes.count("coeff")>0,
                 "coeff does not exist.");
  IR_ENFORCE(attributes.at("coeff").isa<pir::FloatAttribute>(),
                 "Type of attribute: coeff is not pir::FloatAttribute.");

  IR_ENFORCE(attributes.count("with_decay")>0,
                 "with_decay does not exist.");
  IR_ENFORCE(attributes.at("with_decay").isa<pir::BoolAttribute>(),
                 "Type of attribute: with_decay is not pir::BoolAttribute.");

  IR_ENFORCE(attributes.count("lazy_mode")>0,
                 "lazy_mode does not exist.");
  IR_ENFORCE(attributes.at("lazy_mode").isa<pir::BoolAttribute>(),
                 "Type of attribute: lazy_mode is not pir::BoolAttribute.");

  IR_ENFORCE(attributes.count("min_row_size_to_use_multithread")>0,
                 "min_row_size_to_use_multithread does not exist.");
  IR_ENFORCE(attributes.at("min_row_size_to_use_multithread").isa<pir::Int64Attribute>(),
                 "Type of attribute: min_row_size_to_use_multithread is not pir::Int64Attribute.");

  IR_ENFORCE(attributes.count("multi_precision")>0,
                 "multi_precision does not exist.");
  IR_ENFORCE(attributes.at("multi_precision").isa<pir::BoolAttribute>(),
                 "Type of attribute: multi_precision is not pir::BoolAttribute.");

  IR_ENFORCE(attributes.count("use_global_beta_pow")>0,
                 "use_global_beta_pow does not exist.");
  IR_ENFORCE(attributes.at("use_global_beta_pow").isa<pir::BoolAttribute>(),
                 "Type of attribute: use_global_beta_pow is not pir::BoolAttribute.");

  }
  VLOG(4) << "Verifying outputs:";
  {
  auto output_size = num_results();
  IR_ENFORCE(output_size == 6u,
                    "The size %d of outputs must be equal to 6.", output_size);
  IR_ENFORCE((*this)->result(0).type().isa<paddle::dialect::DenseTensorType>(),
                 "Type validation failed for the 0th output.");
  IR_ENFORCE((*this)->result(1).type().isa<paddle::dialect::DenseTensorType>(),
                 "Type validation failed for the 1th output.");
  IR_ENFORCE((*this)->result(2).type().isa<paddle::dialect::DenseTensorType>(),
                 "Type validation failed for the 2th output.");
  IR_ENFORCE((*this)->result(3).type().isa<paddle::dialect::DenseTensorType>(),
                 "Type validation failed for the 3th output.");
  IR_ENFORCE((*this)->result(4).type().isa<paddle::dialect::DenseTensorType>(),
                 "Type validation failed for the 4th output.");
  if (auto output_5_type = (*this)->result(5).type()) {
    IR_ENFORCE(output_5_type.isa<paddle::dialect::DenseTensorType>(),
                   "Type validation failed for the 5th output.");
  }
  }
  VLOG(4) << "End Verifying for: Adamw_Op.";
}

void Adamw_Op::InferMeta( phi::InferMetaContext *infer_meta ) {
  auto fn = PD_INFER_META(phi::AdamwInferMeta);
  fn(infer_meta);
}

phi::DataType Adamw_Op::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: Adamw_Op";
  


  return expected_kernel_dtype;
}

const char *AddmmOp::attributes_name[2] = { "beta", "alpha" };

OpInfoTuple AddmmOp::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("input", "paddle::dialect::DenseTensorType", false, false, false, true), paddle::dialect::OpInputInfo("x", "paddle::dialect::DenseTensorType", false, false, false, true), paddle::dialect::OpInputInfo("y", "paddle::dialect::DenseTensorType", false, false, false, true) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = { paddle::dialect::OpAttributeInfo("beta", "pir::FloatAttribute", ""), paddle::dialect::OpAttributeInfo("alpha", "pir::FloatAttribute", "") };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("out", "paddle::dialect::DenseTensorType", false, false) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("AddmmInferMeta", {"input", "x", "y", "beta", "alpha"}, "addmm", {"input", "x", "y", "beta", "alpha"}, {"x"}, {}, {}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "addmm");
}

void AddmmOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value input_, pir::Value x_, pir::Value y_, float beta, float alpha) {
  VLOG(4) << "Start build AddmmOp";



  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {input_, x_, y_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_beta = pir::FloatAttribute::get(pir::IrContext::Instance(), beta);
  argument.AddAttribute("beta", attr_beta);
  pir::Attribute attr_alpha = pir::FloatAttribute::get(pir::IrContext::Instance(), alpha);
  argument.AddAttribute("alpha", attr_alpha);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType input = input_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)input;
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;
  paddle::dialect::DenseTensorType y = y_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)y;

  VLOG(4) << "Builder construction  dense_input";
  paddle::dialect::IrTensor ir_tensor_input(paddle::dialect::TransToPhiDataType(input.dtype()),
                                                      input.dims(),
                                                      input.data_layout(),
                                                      input.lod(),
                                                      input.offset());
  VLOG(4) << "Builder construction  meta_input";
  paddle::dialect::IrMetaTensor meta_input(&ir_tensor_input);

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);

  VLOG(4) << "Builder construction  dense_y";
  paddle::dialect::IrTensor ir_tensor_y(paddle::dialect::TransToPhiDataType(y.dtype()),
                                                      y.dims(),
                                                      y.data_layout(),
                                                      y.lod(),
                                                      y.offset());
  VLOG(4) << "Builder construction  meta_y";
  paddle::dialect::IrMetaTensor meta_y(&ir_tensor_y);
  paddle::dialect::IrTensor dense_out;
  paddle::dialect::IrMetaTensor meta_out(&dense_out);

  phi::AddmmInferMeta(meta_input, meta_x, meta_y, beta, alpha, &meta_out);

  std::vector<pir::Type> argument_outputs;
  pir::Type out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_out.dtype()), dense_out.dims(), dense_out.layout(), dense_out.lod(), dense_out.offset());
  argument_outputs.push_back(out_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void AddmmOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value input_, pir::Value x_, pir::Value y_, pir::AttributeMap attributes) {
  VLOG(4) << "Start build AddmmOp";


  IR_ENFORCE(
      attributes.find("beta") != attributes.end(),
          "'beta' Attribute is expected for AddmmOp. ");
  float beta = attributes.at("beta").dyn_cast<pir::FloatAttribute>().data();

  IR_ENFORCE(
      attributes.find("alpha") != attributes.end(),
          "'alpha' Attribute is expected for AddmmOp. ");
  float alpha = attributes.at("alpha").dyn_cast<pir::FloatAttribute>().data();


  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {input_, x_, y_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_beta = pir::FloatAttribute::get(pir::IrContext::Instance(), beta);
  argument.AddAttribute("beta", attr_beta);
  pir::Attribute attr_alpha = pir::FloatAttribute::get(pir::IrContext::Instance(), alpha);
  argument.AddAttribute("alpha", attr_alpha);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType input = input_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)input;
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;
  paddle::dialect::DenseTensorType y = y_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)y;

  VLOG(4) << "Builder construction  dense_input";
  paddle::dialect::IrTensor ir_tensor_input(paddle::dialect::TransToPhiDataType(input.dtype()),
                                                      input.dims(),
                                                      input.data_layout(),
                                                      input.lod(),
                                                      input.offset());
  VLOG(4) << "Builder construction  meta_input";
  paddle::dialect::IrMetaTensor meta_input(&ir_tensor_input);

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);

  VLOG(4) << "Builder construction  dense_y";
  paddle::dialect::IrTensor ir_tensor_y(paddle::dialect::TransToPhiDataType(y.dtype()),
                                                      y.dims(),
                                                      y.data_layout(),
                                                      y.lod(),
                                                      y.offset());
  VLOG(4) << "Builder construction  meta_y";
  paddle::dialect::IrMetaTensor meta_y(&ir_tensor_y);
  paddle::dialect::IrTensor dense_out;
  paddle::dialect::IrMetaTensor meta_out(&dense_out);

  phi::AddmmInferMeta(meta_input, meta_x, meta_y, beta, alpha, &meta_out);

  std::vector<pir::Type> argument_outputs;
  pir::Type out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_out.dtype()), dense_out.dims(), dense_out.layout(), dense_out.lod(), dense_out.offset());
  argument_outputs.push_back(out_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void AddmmOp::VerifySig() {
  VLOG(4) << "Start Verifying inputs, outputs and attributes for: AddmmOp.";
  VLOG(4) << "Verifying inputs:";
  {
  auto input_size = num_operands();
  IR_ENFORCE(input_size == 3u,
                    "The size %d of inputs must be equal to 3.", input_size);
  IR_ENFORCE((*this)->operand_source(0).type().isa<paddle::dialect::DenseTensorType>(),
                  "Type validation failed for the 0th input, got %s.", (*this)->operand_source(0).type());
  IR_ENFORCE((*this)->operand_source(1).type().isa<paddle::dialect::DenseTensorType>(),
                  "Type validation failed for the 1th input, got %s.", (*this)->operand_source(1).type());
  IR_ENFORCE((*this)->operand_source(2).type().isa<paddle::dialect::DenseTensorType>(),
                  "Type validation failed for the 2th input, got %s.", (*this)->operand_source(2).type());
  }
  VLOG(4) << "Verifying attributes:";
  {
  auto& attributes = this->attributes();
  IR_ENFORCE(attributes.count("beta")>0,
                 "beta does not exist.");
  IR_ENFORCE(attributes.at("beta").isa<pir::FloatAttribute>(),
                 "Type of attribute: beta is not pir::FloatAttribute.");

  IR_ENFORCE(attributes.count("alpha")>0,
                 "alpha does not exist.");
  IR_ENFORCE(attributes.at("alpha").isa<pir::FloatAttribute>(),
                 "Type of attribute: alpha is not pir::FloatAttribute.");

  }
  VLOG(4) << "Verifying outputs:";
  {
  auto output_size = num_results();
  IR_ENFORCE(output_size == 1u,
                    "The size %d of outputs must be equal to 1.", output_size);
  IR_ENFORCE((*this)->result(0).type().isa<paddle::dialect::DenseTensorType>(),
                 "Type validation failed for the 0th output.");
  }
  VLOG(4) << "End Verifying for: AddmmOp.";
}

void AddmmOp::InferMeta( phi::InferMetaContext *infer_meta ) {
  auto fn = PD_INFER_META(phi::AddmmInferMeta);
  fn(infer_meta);
}

phi::DataType AddmmOp::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: AddmmOp";
  


  return expected_kernel_dtype;
}

const char *Addmm_Op::attributes_name[2] = { "beta", "alpha" };

OpInfoTuple Addmm_Op::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("input", "paddle::dialect::DenseTensorType", false, false, false, true), paddle::dialect::OpInputInfo("x", "paddle::dialect::DenseTensorType", false, false, false, true), paddle::dialect::OpInputInfo("y", "paddle::dialect::DenseTensorType", false, false, false, true) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = { paddle::dialect::OpAttributeInfo("beta", "pir::FloatAttribute", ""), paddle::dialect::OpAttributeInfo("alpha", "pir::FloatAttribute", "") };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("out", "paddle::dialect::DenseTensorType", false, false) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("AddmmInferMeta", {"input", "x", "y", "beta", "alpha"}, "addmm", {"input", "x", "y", "beta", "alpha"}, {"x"}, {}, {{"out", "input"}}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "addmm");
}

void Addmm_Op::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value input_, pir::Value x_, pir::Value y_, float beta, float alpha) {
  VLOG(4) << "Start build Addmm_Op";



  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {input_, x_, y_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_beta = pir::FloatAttribute::get(pir::IrContext::Instance(), beta);
  argument.AddAttribute("beta", attr_beta);
  pir::Attribute attr_alpha = pir::FloatAttribute::get(pir::IrContext::Instance(), alpha);
  argument.AddAttribute("alpha", attr_alpha);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType input = input_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)input;
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;
  paddle::dialect::DenseTensorType y = y_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)y;

  VLOG(4) << "Builder construction  dense_input";
  paddle::dialect::IrTensor ir_tensor_input(paddle::dialect::TransToPhiDataType(input.dtype()),
                                                      input.dims(),
                                                      input.data_layout(),
                                                      input.lod(),
                                                      input.offset());
  VLOG(4) << "Builder construction  meta_input";
  paddle::dialect::IrMetaTensor meta_input(&ir_tensor_input);

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);

  VLOG(4) << "Builder construction  dense_y";
  paddle::dialect::IrTensor ir_tensor_y(paddle::dialect::TransToPhiDataType(y.dtype()),
                                                      y.dims(),
                                                      y.data_layout(),
                                                      y.lod(),
                                                      y.offset());
  VLOG(4) << "Builder construction  meta_y";
  paddle::dialect::IrMetaTensor meta_y(&ir_tensor_y);
  paddle::dialect::IrTensor dense_out;
  paddle::dialect::IrMetaTensor meta_out(&dense_out);

  phi::AddmmInferMeta(meta_input, meta_x, meta_y, beta, alpha, &meta_out);

  std::vector<pir::Type> argument_outputs;
  pir::Type out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_out.dtype()), dense_out.dims(), dense_out.layout(), dense_out.lod(), dense_out.offset());
  argument_outputs.push_back(out_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void Addmm_Op::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value input_, pir::Value x_, pir::Value y_, pir::AttributeMap attributes) {
  VLOG(4) << "Start build Addmm_Op";


  IR_ENFORCE(
      attributes.find("beta") != attributes.end(),
          "'beta' Attribute is expected for Addmm_Op. ");
  float beta = attributes.at("beta").dyn_cast<pir::FloatAttribute>().data();

  IR_ENFORCE(
      attributes.find("alpha") != attributes.end(),
          "'alpha' Attribute is expected for Addmm_Op. ");
  float alpha = attributes.at("alpha").dyn_cast<pir::FloatAttribute>().data();


  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {input_, x_, y_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_beta = pir::FloatAttribute::get(pir::IrContext::Instance(), beta);
  argument.AddAttribute("beta", attr_beta);
  pir::Attribute attr_alpha = pir::FloatAttribute::get(pir::IrContext::Instance(), alpha);
  argument.AddAttribute("alpha", attr_alpha);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType input = input_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)input;
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;
  paddle::dialect::DenseTensorType y = y_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)y;

  VLOG(4) << "Builder construction  dense_input";
  paddle::dialect::IrTensor ir_tensor_input(paddle::dialect::TransToPhiDataType(input.dtype()),
                                                      input.dims(),
                                                      input.data_layout(),
                                                      input.lod(),
                                                      input.offset());
  VLOG(4) << "Builder construction  meta_input";
  paddle::dialect::IrMetaTensor meta_input(&ir_tensor_input);

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);

  VLOG(4) << "Builder construction  dense_y";
  paddle::dialect::IrTensor ir_tensor_y(paddle::dialect::TransToPhiDataType(y.dtype()),
                                                      y.dims(),
                                                      y.data_layout(),
                                                      y.lod(),
                                                      y.offset());
  VLOG(4) << "Builder construction  meta_y";
  paddle::dialect::IrMetaTensor meta_y(&ir_tensor_y);
  paddle::dialect::IrTensor dense_out;
  paddle::dialect::IrMetaTensor meta_out(&dense_out);

  phi::AddmmInferMeta(meta_input, meta_x, meta_y, beta, alpha, &meta_out);

  std::vector<pir::Type> argument_outputs;
  pir::Type out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_out.dtype()), dense_out.dims(), dense_out.layout(), dense_out.lod(), dense_out.offset());
  argument_outputs.push_back(out_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void Addmm_Op::VerifySig() {
  VLOG(4) << "Start Verifying inputs, outputs and attributes for: Addmm_Op.";
  VLOG(4) << "Verifying inputs:";
  {
  auto input_size = num_operands();
  IR_ENFORCE(input_size == 3u,
                    "The size %d of inputs must be equal to 3.", input_size);
  IR_ENFORCE((*this)->operand_source(0).type().isa<paddle::dialect::DenseTensorType>(),
                  "Type validation failed for the 0th input, got %s.", (*this)->operand_source(0).type());
  IR_ENFORCE((*this)->operand_source(1).type().isa<paddle::dialect::DenseTensorType>(),
                  "Type validation failed for the 1th input, got %s.", (*this)->operand_source(1).type());
  IR_ENFORCE((*this)->operand_source(2).type().isa<paddle::dialect::DenseTensorType>(),
                  "Type validation failed for the 2th input, got %s.", (*this)->operand_source(2).type());
  }
  VLOG(4) << "Verifying attributes:";
  {
  auto& attributes = this->attributes();
  IR_ENFORCE(attributes.count("beta")>0,
                 "beta does not exist.");
  IR_ENFORCE(attributes.at("beta").isa<pir::FloatAttribute>(),
                 "Type of attribute: beta is not pir::FloatAttribute.");

  IR_ENFORCE(attributes.count("alpha")>0,
                 "alpha does not exist.");
  IR_ENFORCE(attributes.at("alpha").isa<pir::FloatAttribute>(),
                 "Type of attribute: alpha is not pir::FloatAttribute.");

  }
  VLOG(4) << "Verifying outputs:";
  {
  auto output_size = num_results();
  IR_ENFORCE(output_size == 1u,
                    "The size %d of outputs must be equal to 1.", output_size);
  IR_ENFORCE((*this)->result(0).type().isa<paddle::dialect::DenseTensorType>(),
                 "Type validation failed for the 0th output.");
  }
  VLOG(4) << "End Verifying for: Addmm_Op.";
}

void Addmm_Op::InferMeta( phi::InferMetaContext *infer_meta ) {
  auto fn = PD_INFER_META(phi::AddmmInferMeta);
  fn(infer_meta);
}

phi::DataType Addmm_Op::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: Addmm_Op";
  


  return expected_kernel_dtype;
}

const char *AffineGridOp::attributes_name[1] = { "align_corners" };

OpInfoTuple AffineGridOp::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("input", "paddle::dialect::DenseTensorType", false, false, false, true), paddle::dialect::OpInputInfo("output_shape", "paddle::dialect::IntArrayAttribute", false, false, true, false) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = { paddle::dialect::OpAttributeInfo("align_corners", "pir::BoolAttribute", "") };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("output", "paddle::dialect::DenseTensorType", false, false) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("AffineGridInferMeta", {"input", "output_shape", "align_corners"}, "affine_grid", {"input", "output_shape", "align_corners"}, {"input"}, {}, {}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "affine_grid");
}

void AffineGridOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value input_, const std::vector<int64_t>& output_shape, bool align_corners) {
  VLOG(4) << "Start build AffineGridOp";


  // Generate int_array mutable attribute: output_shape
  paddle::dialect::FullIntArrayOp full_output_shape_op = builder.Build<paddle::dialect::FullIntArrayOp>(output_shape, phi::DataType::INT64, phi::CPUPlace());
  pir::OpResult output_shape_ = full_output_shape_op->result(0);
    
  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {input_, output_shape_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_align_corners = pir::BoolAttribute::get(pir::IrContext::Instance(), align_corners);
  argument.AddAttribute("align_corners", attr_align_corners);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType input = input_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)input;

  VLOG(4) << "Builder construction  dense_input";
  paddle::dialect::IrTensor ir_tensor_input(paddle::dialect::TransToPhiDataType(input.dtype()),
                                                      input.dims(),
                                                      input.data_layout(),
                                                      input.lod(),
                                                      input.offset());
  VLOG(4) << "Builder construction  meta_input";
  paddle::dialect::IrMetaTensor meta_input(&ir_tensor_input);
  paddle::dialect::IrTensor dense_output;
  paddle::dialect::IrMetaTensor meta_output(&dense_output);

  phi::AffineGridInferMeta(meta_input, output_shape, align_corners, &meta_output);

  std::vector<pir::Type> argument_outputs;
  pir::Type output_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_output.dtype()), dense_output.dims(), dense_output.layout(), dense_output.lod(), dense_output.offset());
  argument_outputs.push_back(output_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void AffineGridOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value input_, pir::AttributeMap attributes) {
  VLOG(4) << "Start build AffineGridOp";


  IR_ENFORCE(
      attributes.find("output_shape") != attributes.end(),
          "'output_shape' Attribute is expected for AffineGridOp. ");
  std::vector<int64_t> output_shape = attributes.at("output_shape").dyn_cast<paddle::dialect::IntArrayAttribute>().data().GetData();

  IR_ENFORCE(
      attributes.find("align_corners") != attributes.end(),
          "'align_corners' Attribute is expected for AffineGridOp. ");
  bool align_corners = attributes.at("align_corners").dyn_cast<pir::BoolAttribute>().data();

  // Generate int_array mutable attribute: output_shape
  paddle::dialect::FullIntArrayOp full_output_shape_op = builder.Build<paddle::dialect::FullIntArrayOp>(output_shape, phi::DataType::INT64, phi::CPUPlace());
  pir::OpResult output_shape_ = full_output_shape_op->result(0);
    
  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {input_, output_shape_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_align_corners = pir::BoolAttribute::get(pir::IrContext::Instance(), align_corners);
  argument.AddAttribute("align_corners", attr_align_corners);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType input = input_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)input;

  VLOG(4) << "Builder construction  dense_input";
  paddle::dialect::IrTensor ir_tensor_input(paddle::dialect::TransToPhiDataType(input.dtype()),
                                                      input.dims(),
                                                      input.data_layout(),
                                                      input.lod(),
                                                      input.offset());
  VLOG(4) << "Builder construction  meta_input";
  paddle::dialect::IrMetaTensor meta_input(&ir_tensor_input);
  paddle::dialect::IrTensor dense_output;
  paddle::dialect::IrMetaTensor meta_output(&dense_output);

  phi::AffineGridInferMeta(meta_input, output_shape, align_corners, &meta_output);

  std::vector<pir::Type> argument_outputs;
  pir::Type output_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_output.dtype()), dense_output.dims(), dense_output.layout(), dense_output.lod(), dense_output.offset());
  argument_outputs.push_back(output_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void AffineGridOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value input_, pir::Value output_shape_, bool align_corners) {
  VLOG(4) << "Start build AffineGridOp";



  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {input_, output_shape_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_align_corners = pir::BoolAttribute::get(pir::IrContext::Instance(), align_corners);
  argument.AddAttribute("align_corners", attr_align_corners);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType input = input_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)input;
  phi::IntArray output_shape;
  if (output_shape_.dyn_cast<pir::OpResult>() && output_shape_.dyn_cast<pir::OpResult>().owner()->isa<paddle::dialect::FullIntArrayOp>()) {
    output_shape = std::move(phi::IntArray(paddle::dialect::GetInt64Vector(
                          output_shape_.dyn_cast<pir::OpResult>().owner()
                          ->dyn_cast<paddle::dialect::FullIntArrayOp>()
                          .attribute("value"))));
  } else if (output_shape_.type().isa<pir::VectorType>()) {
    size_t output_shape_size = output_shape_.type().dyn_cast<pir::VectorType>().size();
    output_shape = std::move(phi::IntArray(std::vector<int64_t>(output_shape_size, -1)));
    output_shape.SetFromTensor(true);
  } else if (output_shape_.type().isa<paddle::dialect::DenseTensorType>()) {
    common::DDim output_shape_dim = output_shape_.type().dyn_cast<paddle::dialect::DenseTensorType>().dims();
    size_t output_shape_size = common::product(output_shape_dim);
    if (common::contain_unknown_dim(output_shape_dim)) {
      output_shape_size = 1;
    }
    output_shape = std::move(phi::IntArray(std::vector<int64_t>(output_shape_size, -1)));
    output_shape.SetFromTensor(true);
  } else {
    PADDLE_THROW(phi::errors::Unimplemented("Only support VectorType or DenseTensorType"));
  }


  VLOG(4) << "Builder construction  dense_input";
  paddle::dialect::IrTensor ir_tensor_input(paddle::dialect::TransToPhiDataType(input.dtype()),
                                                      input.dims(),
                                                      input.data_layout(),
                                                      input.lod(),
                                                      input.offset());
  VLOG(4) << "Builder construction  meta_input";
  paddle::dialect::IrMetaTensor meta_input(&ir_tensor_input);
  paddle::dialect::IrTensor dense_output;
  paddle::dialect::IrMetaTensor meta_output(&dense_output);

  phi::AffineGridInferMeta(meta_input, output_shape, align_corners, &meta_output);

  std::vector<pir::Type> argument_outputs;
  pir::Type output_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_output.dtype()), dense_output.dims(), dense_output.layout(), dense_output.lod(), dense_output.offset());
  argument_outputs.push_back(output_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void AffineGridOp::VerifySig() {
  VLOG(4) << "Start Verifying inputs, outputs and attributes for: AffineGridOp.";
  VLOG(4) << "Verifying inputs:";
  {
  auto input_size = num_operands();
  IR_ENFORCE(input_size == 2u,
                    "The size %d of inputs must be equal to 2.", input_size);
  IR_ENFORCE((*this)->operand_source(0).type().isa<paddle::dialect::DenseTensorType>(),
                  "Type validation failed for the 0th input, got %s.", (*this)->operand_source(0).type());
  if (auto vec_type = (*this)->operand_source(1).type().dyn_cast<pir::VectorType>()) {
      for (size_t i = 0; i < vec_type.size(); ++i) {
        IR_ENFORCE(vec_type[i].isa<paddle::dialect::DenseTensorType>(),
                       "Type validation failed for the 1th input, got %s.", (*this)->operand_source(1).type());
      }
  }
  else {
    IR_ENFORCE((*this)->operand_source(1).type().isa<paddle::dialect::DenseTensorType>(),
                   "Type validation failed for the 1th input, got %s.", (*this)->operand_source(1).type());
  }
  }
  VLOG(4) << "Verifying attributes:";
  {
  auto& attributes = this->attributes();
  IR_ENFORCE(attributes.count("align_corners")>0,
                 "align_corners does not exist.");
  IR_ENFORCE(attributes.at("align_corners").isa<pir::BoolAttribute>(),
                 "Type of attribute: align_corners is not pir::BoolAttribute.");

  }
  VLOG(4) << "Verifying outputs:";
  {
  auto output_size = num_results();
  IR_ENFORCE(output_size == 1u,
                    "The size %d of outputs must be equal to 1.", output_size);
  IR_ENFORCE((*this)->result(0).type().isa<paddle::dialect::DenseTensorType>(),
                 "Type validation failed for the 0th output.");
  }
  VLOG(4) << "End Verifying for: AffineGridOp.";
}

void AffineGridOp::InferMeta( phi::InferMetaContext *infer_meta ) {
  auto fn = PD_INFER_META(phi::AffineGridInferMeta);
  fn(infer_meta);
}

phi::DataType AffineGridOp::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: AffineGridOp";
  


  return expected_kernel_dtype;
}

const char *AllcloseOp::attributes_name[1] = { "equal_nan" };

OpInfoTuple AllcloseOp::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("x", "paddle::dialect::DenseTensorType", false, false, false, false), paddle::dialect::OpInputInfo("y", "paddle::dialect::DenseTensorType", false, false, false, false), paddle::dialect::OpInputInfo("rtol", "paddle::dialect::ScalarAttribute", false, false, true, false), paddle::dialect::OpInputInfo("atol", "paddle::dialect::ScalarAttribute", false, false, true, false) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = { paddle::dialect::OpAttributeInfo("equal_nan", "pir::BoolAttribute", "") };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("out", "paddle::dialect::DenseTensorType", false, false) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("AllValueCompareInferMeta", {"x", "y"}, "allclose", {"x", "y", "rtol", "atol", "equal_nan"}, {"x"}, {}, {}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "allclose");
}

void AllcloseOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, pir::Value y_, float rtol, float atol, bool equal_nan) {
  VLOG(4) << "Start build AllcloseOp";


  // Generate scalar mutable attribute: rtol
  paddle::dialect::FullOp full_rtol_op = builder.Build<paddle::dialect::FullOp>(std::vector<int64_t>{1}, rtol, phi::DataType::FLOAT32, phi::CPUPlace());
  pir::OpResult rtol_ = full_rtol_op->result(0);
      // Generate scalar mutable attribute: atol
  paddle::dialect::FullOp full_atol_op = builder.Build<paddle::dialect::FullOp>(std::vector<int64_t>{1}, atol, phi::DataType::FLOAT32, phi::CPUPlace());
  pir::OpResult atol_ = full_atol_op->result(0);
    
  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_, y_, rtol_, atol_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_equal_nan = pir::BoolAttribute::get(pir::IrContext::Instance(), equal_nan);
  argument.AddAttribute("equal_nan", attr_equal_nan);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;
  paddle::dialect::DenseTensorType y = y_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)y;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);

  VLOG(4) << "Builder construction  dense_y";
  paddle::dialect::IrTensor ir_tensor_y(paddle::dialect::TransToPhiDataType(y.dtype()),
                                                      y.dims(),
                                                      y.data_layout(),
                                                      y.lod(),
                                                      y.offset());
  VLOG(4) << "Builder construction  meta_y";
  paddle::dialect::IrMetaTensor meta_y(&ir_tensor_y);
  paddle::dialect::IrTensor dense_out;
  paddle::dialect::IrMetaTensor meta_out(&dense_out);

  phi::AllValueCompareInferMeta(meta_x, meta_y, &meta_out);

  std::vector<pir::Type> argument_outputs;
  pir::Type out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_out.dtype()), dense_out.dims(), dense_out.layout(), dense_out.lod(), dense_out.offset());
  argument_outputs.push_back(out_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void AllcloseOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, pir::Value y_, pir::AttributeMap attributes) {
  VLOG(4) << "Start build AllcloseOp";


  IR_ENFORCE(
      attributes.find("rtol") != attributes.end(),
          "'rtol' Attribute is expected for AllcloseOp. ");
  float rtol = attributes.at("rtol").dyn_cast<paddle::dialect::ScalarAttribute>().data().to<float>();

  IR_ENFORCE(
      attributes.find("atol") != attributes.end(),
          "'atol' Attribute is expected for AllcloseOp. ");
  float atol = attributes.at("atol").dyn_cast<paddle::dialect::ScalarAttribute>().data().to<float>();

  IR_ENFORCE(
      attributes.find("equal_nan") != attributes.end(),
          "'equal_nan' Attribute is expected for AllcloseOp. ");
  bool equal_nan = attributes.at("equal_nan").dyn_cast<pir::BoolAttribute>().data();

  // Generate scalar mutable attribute: rtol
  paddle::dialect::FullOp full_rtol_op = builder.Build<paddle::dialect::FullOp>(std::vector<int64_t>{1}, rtol, phi::DataType::FLOAT32, phi::CPUPlace());
  pir::OpResult rtol_ = full_rtol_op->result(0);
      // Generate scalar mutable attribute: atol
  paddle::dialect::FullOp full_atol_op = builder.Build<paddle::dialect::FullOp>(std::vector<int64_t>{1}, atol, phi::DataType::FLOAT32, phi::CPUPlace());
  pir::OpResult atol_ = full_atol_op->result(0);
    
  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_, y_, rtol_, atol_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_equal_nan = pir::BoolAttribute::get(pir::IrContext::Instance(), equal_nan);
  argument.AddAttribute("equal_nan", attr_equal_nan);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;
  paddle::dialect::DenseTensorType y = y_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)y;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);

  VLOG(4) << "Builder construction  dense_y";
  paddle::dialect::IrTensor ir_tensor_y(paddle::dialect::TransToPhiDataType(y.dtype()),
                                                      y.dims(),
                                                      y.data_layout(),
                                                      y.lod(),
                                                      y.offset());
  VLOG(4) << "Builder construction  meta_y";
  paddle::dialect::IrMetaTensor meta_y(&ir_tensor_y);
  paddle::dialect::IrTensor dense_out;
  paddle::dialect::IrMetaTensor meta_out(&dense_out);

  phi::AllValueCompareInferMeta(meta_x, meta_y, &meta_out);

  std::vector<pir::Type> argument_outputs;
  pir::Type out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_out.dtype()), dense_out.dims(), dense_out.layout(), dense_out.lod(), dense_out.offset());
  argument_outputs.push_back(out_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void AllcloseOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, pir::Value y_, pir::Value rtol_, pir::Value atol_, bool equal_nan) {
  VLOG(4) << "Start build AllcloseOp";



  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_, y_, rtol_, atol_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_equal_nan = pir::BoolAttribute::get(pir::IrContext::Instance(), equal_nan);
  argument.AddAttribute("equal_nan", attr_equal_nan);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;
  paddle::dialect::DenseTensorType y = y_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)y;
  phi::Scalar rtol;
  if (rtol_.dyn_cast<pir::OpResult>() && rtol_.dyn_cast<pir::OpResult>().owner()->isa<paddle::dialect::FullOp>()) {
    rtol = std::move(phi::Scalar(rtol_.dyn_cast<pir::OpResult>().owner()
                                  ->dyn_cast<paddle::dialect::FullOp>()
                                  .attribute("value")
                                  .dyn_cast<paddle::dialect::ScalarAttribute>()
                                  .data()
                                  .to<int>()));
  }
  else {
    rtol = std::move(phi::Scalar(-1));
    rtol.SetFromTensor(true);
  }
  phi::Scalar atol;
  if (atol_.dyn_cast<pir::OpResult>() && atol_.dyn_cast<pir::OpResult>().owner()->isa<paddle::dialect::FullOp>()) {
    atol = std::move(phi::Scalar(atol_.dyn_cast<pir::OpResult>().owner()
                                  ->dyn_cast<paddle::dialect::FullOp>()
                                  .attribute("value")
                                  .dyn_cast<paddle::dialect::ScalarAttribute>()
                                  .data()
                                  .to<int>()));
  }
  else {
    atol = std::move(phi::Scalar(-1));
    atol.SetFromTensor(true);
  }


  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);

  VLOG(4) << "Builder construction  dense_y";
  paddle::dialect::IrTensor ir_tensor_y(paddle::dialect::TransToPhiDataType(y.dtype()),
                                                      y.dims(),
                                                      y.data_layout(),
                                                      y.lod(),
                                                      y.offset());
  VLOG(4) << "Builder construction  meta_y";
  paddle::dialect::IrMetaTensor meta_y(&ir_tensor_y);
  paddle::dialect::IrTensor dense_out;
  paddle::dialect::IrMetaTensor meta_out(&dense_out);

  phi::AllValueCompareInferMeta(meta_x, meta_y, &meta_out);

  std::vector<pir::Type> argument_outputs;
  pir::Type out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_out.dtype()), dense_out.dims(), dense_out.layout(), dense_out.lod(), dense_out.offset());
  argument_outputs.push_back(out_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void AllcloseOp::VerifySig() {
  VLOG(4) << "Start Verifying inputs, outputs and attributes for: AllcloseOp.";
  VLOG(4) << "Verifying inputs:";
  {
  auto input_size = num_operands();
  IR_ENFORCE(input_size == 4u,
                    "The size %d of inputs must be equal to 4.", input_size);
  IR_ENFORCE((*this)->operand_source(0).type().isa<paddle::dialect::DenseTensorType>(),
                  "Type validation failed for the 0th input, got %s.", (*this)->operand_source(0).type());
  IR_ENFORCE((*this)->operand_source(1).type().isa<paddle::dialect::DenseTensorType>(),
                  "Type validation failed for the 1th input, got %s.", (*this)->operand_source(1).type());
  IR_ENFORCE((*this)->operand_source(2).type().isa<paddle::dialect::DenseTensorType>(),
                  "Type validation failed for the 2th input, got %s.", (*this)->operand_source(2).type());
  IR_ENFORCE((*this)->operand_source(3).type().isa<paddle::dialect::DenseTensorType>(),
                  "Type validation failed for the 3th input, got %s.", (*this)->operand_source(3).type());
  }
  VLOG(4) << "Verifying attributes:";
  {
  auto& attributes = this->attributes();
  IR_ENFORCE(attributes.count("equal_nan")>0,
                 "equal_nan does not exist.");
  IR_ENFORCE(attributes.at("equal_nan").isa<pir::BoolAttribute>(),
                 "Type of attribute: equal_nan is not pir::BoolAttribute.");

  }
  VLOG(4) << "Verifying outputs:";
  {
  auto output_size = num_results();
  IR_ENFORCE(output_size == 1u,
                    "The size %d of outputs must be equal to 1.", output_size);
  IR_ENFORCE((*this)->result(0).type().isa<paddle::dialect::DenseTensorType>(),
                 "Type validation failed for the 0th output.");
  }
  VLOG(4) << "End Verifying for: AllcloseOp.";
}

void AllcloseOp::InferMeta( phi::InferMetaContext *infer_meta ) {
  auto fn = PD_INFER_META(phi::AllValueCompareInferMeta);
  fn(infer_meta);
}

phi::DataType AllcloseOp::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: AllcloseOp";
  


  return expected_kernel_dtype;
}

OpInfoTuple AngleOp::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("x", "paddle::dialect::DenseTensorType", false, false, false, true) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = {  };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("out", "paddle::dialect::DenseTensorType", false, false) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("RealAndImagInferMeta", {"x"}, "angle", {"x"}, {}, {}, {}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "angle");
}

void AngleOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_) {
  VLOG(4) << "Start build AngleOp";



  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);
  paddle::dialect::IrTensor dense_out;
  paddle::dialect::IrMetaTensor meta_out(&dense_out);

  phi::RealAndImagInferMeta(meta_x, &meta_out);

  std::vector<pir::Type> argument_outputs;
  pir::Type out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_out.dtype()), dense_out.dims(), dense_out.layout(), dense_out.lod(), dense_out.offset());
  argument_outputs.push_back(out_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void AngleOp::VerifySig() {
  VLOG(4) << "Start Verifying inputs, outputs and attributes for: AngleOp.";
  VLOG(4) << "Verifying inputs:";
  {
  auto input_size = num_operands();
  IR_ENFORCE(input_size == 1u,
                    "The size %d of inputs must be equal to 1.", input_size);
  IR_ENFORCE((*this)->operand_source(0).type().isa<paddle::dialect::DenseTensorType>(),
                  "Type validation failed for the 0th input, got %s.", (*this)->operand_source(0).type());
  }
  VLOG(4) << "Verifying attributes:";
  {
  // Attributes num is 0, not need to check attributes type.
  }
  VLOG(4) << "Verifying outputs:";
  {
  auto output_size = num_results();
  IR_ENFORCE(output_size == 1u,
                    "The size %d of outputs must be equal to 1.", output_size);
  IR_ENFORCE((*this)->result(0).type().isa<paddle::dialect::DenseTensorType>(),
                 "Type validation failed for the 0th output.");
  }
  VLOG(4) << "End Verifying for: AngleOp.";
}

void AngleOp::InferMeta( phi::InferMetaContext *infer_meta ) {
  auto fn = PD_INFER_META(phi::RealAndImagInferMeta);
  fn(infer_meta);
}

phi::DataType AngleOp::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: AngleOp";
  


  return expected_kernel_dtype;
}

const char *ArgmaxOp::attributes_name[3] = { "keepdims", "flatten", "dtype" };

OpInfoTuple ArgmaxOp::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("x", "paddle::dialect::DenseTensorType", false, false, false, false), paddle::dialect::OpInputInfo("axis", "paddle::dialect::ScalarAttribute", false, false, true, false) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = { paddle::dialect::OpAttributeInfo("keepdims", "pir::BoolAttribute", ""), paddle::dialect::OpAttributeInfo("flatten", "pir::BoolAttribute", ""), paddle::dialect::OpAttributeInfo("dtype", "paddle::dialect::DataTypeAttribute", "") };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("out", "paddle::dialect::DenseTensorType", false, false) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("ArgMinMaxInferMeta", {"x", "axis", "keepdims", "flatten", "dtype"}, "argmax", {"x", "axis", "keepdims", "flatten", "dtype"}, {"x"}, {}, {}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "argmax");
}

void ArgmaxOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, int64_t axis, bool keepdims, bool flatten, phi::DataType dtype) {
  VLOG(4) << "Start build ArgmaxOp";


  // Generate scalar mutable attribute: axis
  paddle::dialect::FullOp full_axis_op = builder.Build<paddle::dialect::FullOp>(std::vector<int64_t>{1}, axis, phi::DataType::INT64, phi::CPUPlace());
  pir::OpResult axis_ = full_axis_op->result(0);
    
  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_, axis_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_keepdims = pir::BoolAttribute::get(pir::IrContext::Instance(), keepdims);
  argument.AddAttribute("keepdims", attr_keepdims);
  pir::Attribute attr_flatten = pir::BoolAttribute::get(pir::IrContext::Instance(), flatten);
  argument.AddAttribute("flatten", attr_flatten);
  pir::Attribute attr_dtype = paddle::dialect::DataTypeAttribute::get(pir::IrContext::Instance(), dtype);
  argument.AddAttribute("dtype", attr_dtype);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);
  paddle::dialect::IrTensor dense_out;
  paddle::dialect::IrMetaTensor meta_out(&dense_out);

  phi::ArgMinMaxInferMeta(meta_x, axis, keepdims, flatten, dtype, &meta_out);

  std::vector<pir::Type> argument_outputs;
  pir::Type out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_out.dtype()), dense_out.dims(), dense_out.layout(), dense_out.lod(), dense_out.offset());
  argument_outputs.push_back(out_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void ArgmaxOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, pir::AttributeMap attributes) {
  VLOG(4) << "Start build ArgmaxOp";


  IR_ENFORCE(
      attributes.find("axis") != attributes.end(),
          "'axis' Attribute is expected for ArgmaxOp. ");
  int64_t axis = attributes.at("axis").dyn_cast<pir::Int64Attribute>().data();

  IR_ENFORCE(
      attributes.find("keepdims") != attributes.end(),
          "'keepdims' Attribute is expected for ArgmaxOp. ");
  bool keepdims = attributes.at("keepdims").dyn_cast<pir::BoolAttribute>().data();

  IR_ENFORCE(
      attributes.find("flatten") != attributes.end(),
          "'flatten' Attribute is expected for ArgmaxOp. ");
  bool flatten = attributes.at("flatten").dyn_cast<pir::BoolAttribute>().data();

  IR_ENFORCE(
      attributes.find("dtype") != attributes.end(),
          "'dtype' Attribute is expected for ArgmaxOp. ");
  phi::DataType dtype = attributes.at("dtype").dyn_cast<paddle::dialect::DataTypeAttribute>().data();

  // Generate scalar mutable attribute: axis
  paddle::dialect::FullOp full_axis_op = builder.Build<paddle::dialect::FullOp>(std::vector<int64_t>{1}, axis, phi::DataType::INT64, phi::CPUPlace());
  pir::OpResult axis_ = full_axis_op->result(0);
    
  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_, axis_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_keepdims = pir::BoolAttribute::get(pir::IrContext::Instance(), keepdims);
  argument.AddAttribute("keepdims", attr_keepdims);
  pir::Attribute attr_flatten = pir::BoolAttribute::get(pir::IrContext::Instance(), flatten);
  argument.AddAttribute("flatten", attr_flatten);
  pir::Attribute attr_dtype = paddle::dialect::DataTypeAttribute::get(pir::IrContext::Instance(), dtype);
  argument.AddAttribute("dtype", attr_dtype);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);
  paddle::dialect::IrTensor dense_out;
  paddle::dialect::IrMetaTensor meta_out(&dense_out);

  phi::ArgMinMaxInferMeta(meta_x, axis, keepdims, flatten, dtype, &meta_out);

  std::vector<pir::Type> argument_outputs;
  pir::Type out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_out.dtype()), dense_out.dims(), dense_out.layout(), dense_out.lod(), dense_out.offset());
  argument_outputs.push_back(out_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void ArgmaxOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, pir::Value axis_, bool keepdims, bool flatten, phi::DataType dtype) {
  VLOG(4) << "Start build ArgmaxOp";



  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_, axis_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_keepdims = pir::BoolAttribute::get(pir::IrContext::Instance(), keepdims);
  argument.AddAttribute("keepdims", attr_keepdims);
  pir::Attribute attr_flatten = pir::BoolAttribute::get(pir::IrContext::Instance(), flatten);
  argument.AddAttribute("flatten", attr_flatten);
  pir::Attribute attr_dtype = paddle::dialect::DataTypeAttribute::get(pir::IrContext::Instance(), dtype);
  argument.AddAttribute("dtype", attr_dtype);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;
  phi::Scalar axis;
  if (axis_.dyn_cast<pir::OpResult>() && axis_.dyn_cast<pir::OpResult>().owner()->isa<paddle::dialect::FullOp>()) {
    axis = std::move(phi::Scalar(axis_.dyn_cast<pir::OpResult>().owner()
                                  ->dyn_cast<paddle::dialect::FullOp>()
                                  .attribute("value")
                                  .dyn_cast<paddle::dialect::ScalarAttribute>()
                                  .data()
                                  .to<int>()));
  }
  else {
    axis = std::move(phi::Scalar(-1));
    axis.SetFromTensor(true);
  }


  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);
  paddle::dialect::IrTensor dense_out;
  paddle::dialect::IrMetaTensor meta_out(&dense_out);

  phi::ArgMinMaxInferMeta(meta_x, axis, keepdims, flatten, dtype, &meta_out);

  std::vector<pir::Type> argument_outputs;
  pir::Type out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_out.dtype()), dense_out.dims(), dense_out.layout(), dense_out.lod(), dense_out.offset());
  argument_outputs.push_back(out_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void ArgmaxOp::VerifySig() {
  VLOG(4) << "Start Verifying inputs, outputs and attributes for: ArgmaxOp.";
  VLOG(4) << "Verifying inputs:";
  {
  auto input_size = num_operands();
  IR_ENFORCE(input_size == 2u,
                    "The size %d of inputs must be equal to 2.", input_size);
  IR_ENFORCE((*this)->operand_source(0).type().isa<paddle::dialect::DenseTensorType>(),
                  "Type validation failed for the 0th input, got %s.", (*this)->operand_source(0).type());
  IR_ENFORCE((*this)->operand_source(1).type().isa<paddle::dialect::DenseTensorType>(),
                  "Type validation failed for the 1th input, got %s.", (*this)->operand_source(1).type());
  }
  VLOG(4) << "Verifying attributes:";
  {
  auto& attributes = this->attributes();
  IR_ENFORCE(attributes.count("keepdims")>0,
                 "keepdims does not exist.");
  IR_ENFORCE(attributes.at("keepdims").isa<pir::BoolAttribute>(),
                 "Type of attribute: keepdims is not pir::BoolAttribute.");

  IR_ENFORCE(attributes.count("flatten")>0,
                 "flatten does not exist.");
  IR_ENFORCE(attributes.at("flatten").isa<pir::BoolAttribute>(),
                 "Type of attribute: flatten is not pir::BoolAttribute.");

  IR_ENFORCE(attributes.count("dtype")>0,
                 "dtype does not exist.");
  IR_ENFORCE(attributes.at("dtype").isa<paddle::dialect::DataTypeAttribute>(),
                 "Type of attribute: dtype is not paddle::dialect::DataTypeAttribute.");

  }
  VLOG(4) << "Verifying outputs:";
  {
  auto output_size = num_results();
  IR_ENFORCE(output_size == 1u,
                    "The size %d of outputs must be equal to 1.", output_size);
  IR_ENFORCE((*this)->result(0).type().isa<paddle::dialect::DenseTensorType>(),
                 "Type validation failed for the 0th output.");
  }
  VLOG(4) << "End Verifying for: ArgmaxOp.";
}

void ArgmaxOp::InferMeta( phi::InferMetaContext *infer_meta ) {
  auto fn = PD_INFER_META(phi::ArgMinMaxInferMeta);
  fn(infer_meta);
}

phi::DataType ArgmaxOp::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: ArgmaxOp";
  


  return expected_kernel_dtype;
}

const char *ArgminOp::attributes_name[3] = { "keepdims", "flatten", "dtype" };

OpInfoTuple ArgminOp::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("x", "paddle::dialect::DenseTensorType", false, false, false, false), paddle::dialect::OpInputInfo("axis", "paddle::dialect::ScalarAttribute", false, false, true, false) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = { paddle::dialect::OpAttributeInfo("keepdims", "pir::BoolAttribute", ""), paddle::dialect::OpAttributeInfo("flatten", "pir::BoolAttribute", ""), paddle::dialect::OpAttributeInfo("dtype", "paddle::dialect::DataTypeAttribute", "") };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("out", "paddle::dialect::DenseTensorType", false, false) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("ArgMinMaxInferMeta", {"x", "axis", "keepdims", "flatten", "dtype"}, "argmin", {"x", "axis", "keepdims", "flatten", "dtype"}, {"x"}, {}, {}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "argmin");
}

void ArgminOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, int64_t axis, bool keepdims, bool flatten, phi::DataType dtype) {
  VLOG(4) << "Start build ArgminOp";


  // Generate scalar mutable attribute: axis
  paddle::dialect::FullOp full_axis_op = builder.Build<paddle::dialect::FullOp>(std::vector<int64_t>{1}, axis, phi::DataType::INT64, phi::CPUPlace());
  pir::OpResult axis_ = full_axis_op->result(0);
    
  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_, axis_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_keepdims = pir::BoolAttribute::get(pir::IrContext::Instance(), keepdims);
  argument.AddAttribute("keepdims", attr_keepdims);
  pir::Attribute attr_flatten = pir::BoolAttribute::get(pir::IrContext::Instance(), flatten);
  argument.AddAttribute("flatten", attr_flatten);
  pir::Attribute attr_dtype = paddle::dialect::DataTypeAttribute::get(pir::IrContext::Instance(), dtype);
  argument.AddAttribute("dtype", attr_dtype);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);
  paddle::dialect::IrTensor dense_out;
  paddle::dialect::IrMetaTensor meta_out(&dense_out);

  phi::ArgMinMaxInferMeta(meta_x, axis, keepdims, flatten, dtype, &meta_out);

  std::vector<pir::Type> argument_outputs;
  pir::Type out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_out.dtype()), dense_out.dims(), dense_out.layout(), dense_out.lod(), dense_out.offset());
  argument_outputs.push_back(out_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void ArgminOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, pir::AttributeMap attributes) {
  VLOG(4) << "Start build ArgminOp";


  IR_ENFORCE(
      attributes.find("axis") != attributes.end(),
          "'axis' Attribute is expected for ArgminOp. ");
  int64_t axis = attributes.at("axis").dyn_cast<pir::Int64Attribute>().data();

  IR_ENFORCE(
      attributes.find("keepdims") != attributes.end(),
          "'keepdims' Attribute is expected for ArgminOp. ");
  bool keepdims = attributes.at("keepdims").dyn_cast<pir::BoolAttribute>().data();

  IR_ENFORCE(
      attributes.find("flatten") != attributes.end(),
          "'flatten' Attribute is expected for ArgminOp. ");
  bool flatten = attributes.at("flatten").dyn_cast<pir::BoolAttribute>().data();

  IR_ENFORCE(
      attributes.find("dtype") != attributes.end(),
          "'dtype' Attribute is expected for ArgminOp. ");
  phi::DataType dtype = attributes.at("dtype").dyn_cast<paddle::dialect::DataTypeAttribute>().data();

  // Generate scalar mutable attribute: axis
  paddle::dialect::FullOp full_axis_op = builder.Build<paddle::dialect::FullOp>(std::vector<int64_t>{1}, axis, phi::DataType::INT64, phi::CPUPlace());
  pir::OpResult axis_ = full_axis_op->result(0);
    
  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_, axis_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_keepdims = pir::BoolAttribute::get(pir::IrContext::Instance(), keepdims);
  argument.AddAttribute("keepdims", attr_keepdims);
  pir::Attribute attr_flatten = pir::BoolAttribute::get(pir::IrContext::Instance(), flatten);
  argument.AddAttribute("flatten", attr_flatten);
  pir::Attribute attr_dtype = paddle::dialect::DataTypeAttribute::get(pir::IrContext::Instance(), dtype);
  argument.AddAttribute("dtype", attr_dtype);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);
  paddle::dialect::IrTensor dense_out;
  paddle::dialect::IrMetaTensor meta_out(&dense_out);

  phi::ArgMinMaxInferMeta(meta_x, axis, keepdims, flatten, dtype, &meta_out);

  std::vector<pir::Type> argument_outputs;
  pir::Type out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_out.dtype()), dense_out.dims(), dense_out.layout(), dense_out.lod(), dense_out.offset());
  argument_outputs.push_back(out_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void ArgminOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, pir::Value axis_, bool keepdims, bool flatten, phi::DataType dtype) {
  VLOG(4) << "Start build ArgminOp";



  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_, axis_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_keepdims = pir::BoolAttribute::get(pir::IrContext::Instance(), keepdims);
  argument.AddAttribute("keepdims", attr_keepdims);
  pir::Attribute attr_flatten = pir::BoolAttribute::get(pir::IrContext::Instance(), flatten);
  argument.AddAttribute("flatten", attr_flatten);
  pir::Attribute attr_dtype = paddle::dialect::DataTypeAttribute::get(pir::IrContext::Instance(), dtype);
  argument.AddAttribute("dtype", attr_dtype);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;
  phi::Scalar axis;
  if (axis_.dyn_cast<pir::OpResult>() && axis_.dyn_cast<pir::OpResult>().owner()->isa<paddle::dialect::FullOp>()) {
    axis = std::move(phi::Scalar(axis_.dyn_cast<pir::OpResult>().owner()
                                  ->dyn_cast<paddle::dialect::FullOp>()
                                  .attribute("value")
                                  .dyn_cast<paddle::dialect::ScalarAttribute>()
                                  .data()
                                  .to<int>()));
  }
  else {
    axis = std::move(phi::Scalar(-1));
    axis.SetFromTensor(true);
  }


  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);
  paddle::dialect::IrTensor dense_out;
  paddle::dialect::IrMetaTensor meta_out(&dense_out);

  phi::ArgMinMaxInferMeta(meta_x, axis, keepdims, flatten, dtype, &meta_out);

  std::vector<pir::Type> argument_outputs;
  pir::Type out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_out.dtype()), dense_out.dims(), dense_out.layout(), dense_out.lod(), dense_out.offset());
  argument_outputs.push_back(out_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void ArgminOp::VerifySig() {
  VLOG(4) << "Start Verifying inputs, outputs and attributes for: ArgminOp.";
  VLOG(4) << "Verifying inputs:";
  {
  auto input_size = num_operands();
  IR_ENFORCE(input_size == 2u,
                    "The size %d of inputs must be equal to 2.", input_size);
  IR_ENFORCE((*this)->operand_source(0).type().isa<paddle::dialect::DenseTensorType>(),
                  "Type validation failed for the 0th input, got %s.", (*this)->operand_source(0).type());
  IR_ENFORCE((*this)->operand_source(1).type().isa<paddle::dialect::DenseTensorType>(),
                  "Type validation failed for the 1th input, got %s.", (*this)->operand_source(1).type());
  }
  VLOG(4) << "Verifying attributes:";
  {
  auto& attributes = this->attributes();
  IR_ENFORCE(attributes.count("keepdims")>0,
                 "keepdims does not exist.");
  IR_ENFORCE(attributes.at("keepdims").isa<pir::BoolAttribute>(),
                 "Type of attribute: keepdims is not pir::BoolAttribute.");

  IR_ENFORCE(attributes.count("flatten")>0,
                 "flatten does not exist.");
  IR_ENFORCE(attributes.at("flatten").isa<pir::BoolAttribute>(),
                 "Type of attribute: flatten is not pir::BoolAttribute.");

  IR_ENFORCE(attributes.count("dtype")>0,
                 "dtype does not exist.");
  IR_ENFORCE(attributes.at("dtype").isa<paddle::dialect::DataTypeAttribute>(),
                 "Type of attribute: dtype is not paddle::dialect::DataTypeAttribute.");

  }
  VLOG(4) << "Verifying outputs:";
  {
  auto output_size = num_results();
  IR_ENFORCE(output_size == 1u,
                    "The size %d of outputs must be equal to 1.", output_size);
  IR_ENFORCE((*this)->result(0).type().isa<paddle::dialect::DenseTensorType>(),
                 "Type validation failed for the 0th output.");
  }
  VLOG(4) << "End Verifying for: ArgminOp.";
}

void ArgminOp::InferMeta( phi::InferMetaContext *infer_meta ) {
  auto fn = PD_INFER_META(phi::ArgMinMaxInferMeta);
  fn(infer_meta);
}

phi::DataType ArgminOp::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: ArgminOp";
  


  return expected_kernel_dtype;
}

const char *ArgsortOp::attributes_name[2] = { "axis", "descending" };

OpInfoTuple ArgsortOp::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("x", "paddle::dialect::DenseTensorType", false, false, false, true) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = { paddle::dialect::OpAttributeInfo("axis", "pir::Int32Attribute", ""), paddle::dialect::OpAttributeInfo("descending", "pir::BoolAttribute", "") };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("out", "paddle::dialect::DenseTensorType", false, false), paddle::dialect::OpOutputInfo("indices", "paddle::dialect::DenseTensorType", false, false) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("ArgsortInferMeta", {"x", "axis", "descending"}, "argsort", {"x", "axis", "descending"}, {}, {}, {}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "argsort");
}

void ArgsortOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, int axis, bool descending) {
  VLOG(4) << "Start build ArgsortOp";



  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_axis = pir::Int32Attribute::get(pir::IrContext::Instance(), axis);
  argument.AddAttribute("axis", attr_axis);
  pir::Attribute attr_descending = pir::BoolAttribute::get(pir::IrContext::Instance(), descending);
  argument.AddAttribute("descending", attr_descending);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);
  paddle::dialect::IrTensor dense_out;
  paddle::dialect::IrMetaTensor meta_out(&dense_out);
  paddle::dialect::IrTensor dense_indices;
  paddle::dialect::IrMetaTensor meta_indices(&dense_indices);

  phi::ArgsortInferMeta(meta_x, axis, descending, &meta_out, &meta_indices);

  std::vector<pir::Type> argument_outputs;
  pir::Type out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_out.dtype()), dense_out.dims(), dense_out.layout(), dense_out.lod(), dense_out.offset());
  argument_outputs.push_back(out_dense_tensor_type);

  pir::Type indices_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_indices.dtype()), dense_indices.dims(), dense_indices.layout(), dense_indices.lod(), dense_indices.offset());
  argument_outputs.push_back(indices_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void ArgsortOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, pir::AttributeMap attributes) {
  VLOG(4) << "Start build ArgsortOp";


  IR_ENFORCE(
      attributes.find("axis") != attributes.end(),
          "'axis' Attribute is expected for ArgsortOp. ");
  int axis = attributes.at("axis").dyn_cast<pir::Int32Attribute>().data();

  IR_ENFORCE(
      attributes.find("descending") != attributes.end(),
          "'descending' Attribute is expected for ArgsortOp. ");
  bool descending = attributes.at("descending").dyn_cast<pir::BoolAttribute>().data();


  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_axis = pir::Int32Attribute::get(pir::IrContext::Instance(), axis);
  argument.AddAttribute("axis", attr_axis);
  pir::Attribute attr_descending = pir::BoolAttribute::get(pir::IrContext::Instance(), descending);
  argument.AddAttribute("descending", attr_descending);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);
  paddle::dialect::IrTensor dense_out;
  paddle::dialect::IrMetaTensor meta_out(&dense_out);
  paddle::dialect::IrTensor dense_indices;
  paddle::dialect::IrMetaTensor meta_indices(&dense_indices);

  phi::ArgsortInferMeta(meta_x, axis, descending, &meta_out, &meta_indices);

  std::vector<pir::Type> argument_outputs;
  pir::Type out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_out.dtype()), dense_out.dims(), dense_out.layout(), dense_out.lod(), dense_out.offset());
  argument_outputs.push_back(out_dense_tensor_type);

  pir::Type indices_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_indices.dtype()), dense_indices.dims(), dense_indices.layout(), dense_indices.lod(), dense_indices.offset());
  argument_outputs.push_back(indices_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void ArgsortOp::VerifySig() {
  VLOG(4) << "Start Verifying inputs, outputs and attributes for: ArgsortOp.";
  VLOG(4) << "Verifying inputs:";
  {
  auto input_size = num_operands();
  IR_ENFORCE(input_size == 1u,
                    "The size %d of inputs must be equal to 1.", input_size);
  IR_ENFORCE((*this)->operand_source(0).type().isa<paddle::dialect::DenseTensorType>(),
                  "Type validation failed for the 0th input, got %s.", (*this)->operand_source(0).type());
  }
  VLOG(4) << "Verifying attributes:";
  {
  auto& attributes = this->attributes();
  IR_ENFORCE(attributes.count("axis")>0,
                 "axis does not exist.");
  IR_ENFORCE(attributes.at("axis").isa<pir::Int32Attribute>(),
                 "Type of attribute: axis is not pir::Int32Attribute.");

  IR_ENFORCE(attributes.count("descending")>0,
                 "descending does not exist.");
  IR_ENFORCE(attributes.at("descending").isa<pir::BoolAttribute>(),
                 "Type of attribute: descending is not pir::BoolAttribute.");

  }
  VLOG(4) << "Verifying outputs:";
  {
  auto output_size = num_results();
  IR_ENFORCE(output_size == 2u,
                    "The size %d of outputs must be equal to 2.", output_size);
  IR_ENFORCE((*this)->result(0).type().isa<paddle::dialect::DenseTensorType>(),
                 "Type validation failed for the 0th output.");
  IR_ENFORCE((*this)->result(1).type().isa<paddle::dialect::DenseTensorType>(),
                 "Type validation failed for the 1th output.");
  }
  VLOG(4) << "End Verifying for: ArgsortOp.";
}

void ArgsortOp::InferMeta( phi::InferMetaContext *infer_meta ) {
  auto fn = PD_INFER_META(phi::ArgsortInferMeta);
  fn(infer_meta);
}

phi::DataType ArgsortOp::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: ArgsortOp";
  


  return expected_kernel_dtype;
}

OpInfoTuple AsComplexOp::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("x", "paddle::dialect::DenseTensorType", false, false, false, true) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = {  };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("out", "paddle::dialect::DenseTensorType", false, false) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("AsComplexInferMeta", {"x"}, "as_complex", {"x"}, {}, {}, {}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "as_complex");
}

void AsComplexOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_) {
  VLOG(4) << "Start build AsComplexOp";



  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);
  paddle::dialect::IrTensor dense_out;
  paddle::dialect::IrMetaTensor meta_out(&dense_out);

  phi::AsComplexInferMeta(meta_x, &meta_out);

  std::vector<pir::Type> argument_outputs;
  pir::Type out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_out.dtype()), dense_out.dims(), dense_out.layout(), dense_out.lod(), dense_out.offset());
  argument_outputs.push_back(out_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void AsComplexOp::VerifySig() {
  VLOG(4) << "Start Verifying inputs, outputs and attributes for: AsComplexOp.";
  VLOG(4) << "Verifying inputs:";
  {
  auto input_size = num_operands();
  IR_ENFORCE(input_size == 1u,
                    "The size %d of inputs must be equal to 1.", input_size);
  IR_ENFORCE((*this)->operand_source(0).type().isa<paddle::dialect::DenseTensorType>(),
                  "Type validation failed for the 0th input, got %s.", (*this)->operand_source(0).type());
  }
  VLOG(4) << "Verifying attributes:";
  {
  // Attributes num is 0, not need to check attributes type.
  }
  VLOG(4) << "Verifying outputs:";
  {
  auto output_size = num_results();
  IR_ENFORCE(output_size == 1u,
                    "The size %d of outputs must be equal to 1.", output_size);
  IR_ENFORCE((*this)->result(0).type().isa<paddle::dialect::DenseTensorType>(),
                 "Type validation failed for the 0th output.");
  }
  VLOG(4) << "End Verifying for: AsComplexOp.";
}

void AsComplexOp::InferMeta( phi::InferMetaContext *infer_meta ) {
  auto fn = PD_INFER_META(phi::AsComplexInferMeta);
  fn(infer_meta);
}

phi::DataType AsComplexOp::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: AsComplexOp";
  


  return expected_kernel_dtype;
}

OpInfoTuple AsRealOp::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("x", "paddle::dialect::DenseTensorType", false, false, false, true) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = {  };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("out", "paddle::dialect::DenseTensorType", false, false) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("AsRealInferMeta", {"x"}, "as_real", {"x"}, {}, {}, {}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "as_real");
}

void AsRealOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_) {
  VLOG(4) << "Start build AsRealOp";



  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);
  paddle::dialect::IrTensor dense_out;
  paddle::dialect::IrMetaTensor meta_out(&dense_out);

  phi::AsRealInferMeta(meta_x, &meta_out);

  std::vector<pir::Type> argument_outputs;
  pir::Type out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_out.dtype()), dense_out.dims(), dense_out.layout(), dense_out.lod(), dense_out.offset());
  argument_outputs.push_back(out_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void AsRealOp::VerifySig() {
  VLOG(4) << "Start Verifying inputs, outputs and attributes for: AsRealOp.";
  VLOG(4) << "Verifying inputs:";
  {
  auto input_size = num_operands();
  IR_ENFORCE(input_size == 1u,
                    "The size %d of inputs must be equal to 1.", input_size);
  IR_ENFORCE((*this)->operand_source(0).type().isa<paddle::dialect::DenseTensorType>(),
                  "Type validation failed for the 0th input, got %s.", (*this)->operand_source(0).type());
  }
  VLOG(4) << "Verifying attributes:";
  {
  // Attributes num is 0, not need to check attributes type.
  }
  VLOG(4) << "Verifying outputs:";
  {
  auto output_size = num_results();
  IR_ENFORCE(output_size == 1u,
                    "The size %d of outputs must be equal to 1.", output_size);
  IR_ENFORCE((*this)->result(0).type().isa<paddle::dialect::DenseTensorType>(),
                 "Type validation failed for the 0th output.");
  }
  VLOG(4) << "End Verifying for: AsRealOp.";
}

void AsRealOp::InferMeta( phi::InferMetaContext *infer_meta ) {
  auto fn = PD_INFER_META(phi::AsRealInferMeta);
  fn(infer_meta);
}

phi::DataType AsRealOp::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: AsRealOp";
  


  return expected_kernel_dtype;
}

const char *AsStridedOp::attributes_name[3] = { "dims", "stride", "offset" };

OpInfoTuple AsStridedOp::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("input", "paddle::dialect::DenseTensorType", false, true, false, true) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = { paddle::dialect::OpAttributeInfo("dims", "pir::ArrayAttribute<pir::Int64Attribute>", ""), paddle::dialect::OpAttributeInfo("stride", "pir::ArrayAttribute<pir::Int64Attribute>", ""), paddle::dialect::OpAttributeInfo("offset", "pir::Int64Attribute", "") };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("out", "paddle::dialect::DenseTensorType", false, false) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("StridedUnChangedInferMeta", {"input"}, "as_strided", {"input", "dims", "stride", "offset"}, {}, {}, {}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "as_strided");
}

void AsStridedOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value input_, const std::vector<int64_t>& dims, const std::vector<int64_t>& stride, int64_t offset) {
  VLOG(4) << "Start build AsStridedOp";



  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {input_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  std::vector<pir::Attribute> vec_dims;
  for (size_t i = 0; i < static_cast<size_t>(dims.size()); i++) {
      pir::Attribute attr_dims = pir::Int64Attribute::get(pir::IrContext::Instance(), dims[i]);

    vec_dims.push_back(attr_dims);
  }
  pir::Attribute attr_dims = pir::ArrayAttribute::get(pir::IrContext::Instance(), vec_dims);
  argument.AddAttribute("dims", attr_dims);
  std::vector<pir::Attribute> vec_stride;
  for (size_t i = 0; i < static_cast<size_t>(stride.size()); i++) {
      pir::Attribute attr_stride = pir::Int64Attribute::get(pir::IrContext::Instance(), stride[i]);

    vec_stride.push_back(attr_stride);
  }
  pir::Attribute attr_stride = pir::ArrayAttribute::get(pir::IrContext::Instance(), vec_stride);
  argument.AddAttribute("stride", attr_stride);
  pir::Attribute attr_offset = pir::Int64Attribute::get(pir::IrContext::Instance(), offset);
  argument.AddAttribute("offset", attr_offset);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType input = input_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)input;

  VLOG(4) << "Builder construction  dense_input";
  paddle::dialect::IrTensor ir_tensor_input(paddle::dialect::TransToPhiDataType(input.dtype()),
                                                      input.dims(),
                                                      input.data_layout(),
                                                      input.lod(),
                                                      input.offset());
  VLOG(4) << "Builder construction  meta_input";
  paddle::dialect::IrMetaTensor meta_input(&ir_tensor_input);
  paddle::dialect::IrTensor dense_out;
  paddle::dialect::IrMetaTensor meta_out(&dense_out);

  phi::StridedUnChangedInferMeta(meta_input, &meta_out);

  std::vector<pir::Type> argument_outputs;
  pir::Type out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_out.dtype()), dense_out.dims(), dense_out.layout(), dense_out.lod(), dense_out.offset());
  argument_outputs.push_back(out_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void AsStridedOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value input_, pir::AttributeMap attributes) {
  VLOG(4) << "Start build AsStridedOp";


  IR_ENFORCE(
      attributes.find("dims") != attributes.end(),
          "'dims' Attribute is expected for AsStridedOp. ");
  std::vector<int64_t> dims;
  for (size_t i = 0; i < attributes.at("dims").dyn_cast<pir::ArrayAttribute>().size(); i++) {
    dims.push_back(attributes.at("dims").dyn_cast<pir::ArrayAttribute>().at(i).dyn_cast<pir::Int64Attribute>().data());
  }

  IR_ENFORCE(
      attributes.find("stride") != attributes.end(),
          "'stride' Attribute is expected for AsStridedOp. ");
  std::vector<int64_t> stride;
  for (size_t i = 0; i < attributes.at("stride").dyn_cast<pir::ArrayAttribute>().size(); i++) {
    stride.push_back(attributes.at("stride").dyn_cast<pir::ArrayAttribute>().at(i).dyn_cast<pir::Int64Attribute>().data());
  }

  IR_ENFORCE(
      attributes.find("offset") != attributes.end(),
          "'offset' Attribute is expected for AsStridedOp. ");
  int64_t offset = attributes.at("offset").dyn_cast<pir::Int64Attribute>().data();


  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {input_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  std::vector<pir::Attribute> vec_dims;
  for (size_t i = 0; i < static_cast<size_t>(dims.size()); i++) {
      pir::Attribute attr_dims = pir::Int64Attribute::get(pir::IrContext::Instance(), dims[i]);

    vec_dims.push_back(attr_dims);
  }
  pir::Attribute attr_dims = pir::ArrayAttribute::get(pir::IrContext::Instance(), vec_dims);
  argument.AddAttribute("dims", attr_dims);
  std::vector<pir::Attribute> vec_stride;
  for (size_t i = 0; i < static_cast<size_t>(stride.size()); i++) {
      pir::Attribute attr_stride = pir::Int64Attribute::get(pir::IrContext::Instance(), stride[i]);

    vec_stride.push_back(attr_stride);
  }
  pir::Attribute attr_stride = pir::ArrayAttribute::get(pir::IrContext::Instance(), vec_stride);
  argument.AddAttribute("stride", attr_stride);
  pir::Attribute attr_offset = pir::Int64Attribute::get(pir::IrContext::Instance(), offset);
  argument.AddAttribute("offset", attr_offset);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType input = input_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)input;

  VLOG(4) << "Builder construction  dense_input";
  paddle::dialect::IrTensor ir_tensor_input(paddle::dialect::TransToPhiDataType(input.dtype()),
                                                      input.dims(),
                                                      input.data_layout(),
                                                      input.lod(),
                                                      input.offset());
  VLOG(4) << "Builder construction  meta_input";
  paddle::dialect::IrMetaTensor meta_input(&ir_tensor_input);
  paddle::dialect::IrTensor dense_out;
  paddle::dialect::IrMetaTensor meta_out(&dense_out);

  phi::StridedUnChangedInferMeta(meta_input, &meta_out);

  std::vector<pir::Type> argument_outputs;
  pir::Type out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_out.dtype()), dense_out.dims(), dense_out.layout(), dense_out.lod(), dense_out.offset());
  argument_outputs.push_back(out_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void AsStridedOp::VerifySig() {
  VLOG(4) << "Start Verifying inputs, outputs and attributes for: AsStridedOp.";
  VLOG(4) << "Verifying inputs:";
  {
  auto input_size = num_operands();
  IR_ENFORCE(input_size == 1u,
                    "The size %d of inputs must be equal to 1.", input_size);
  IR_ENFORCE((*this)->operand_source(0).type().isa<paddle::dialect::DenseTensorType>(),
                  "Type validation failed for the 0th input, got %s.", (*this)->operand_source(0).type());
  }
  VLOG(4) << "Verifying attributes:";
  {
  auto& attributes = this->attributes();
  IR_ENFORCE(attributes.count("dims")>0,
                 "dims does not exist.");
  IR_ENFORCE(attributes.at("dims").isa<pir::ArrayAttribute>(),
                 "Type of attribute: dims is not pir::ArrayAttribute.");
  for (size_t i = 0; i < attributes.at("dims").dyn_cast<pir::ArrayAttribute>().size(); i++) {
    IR_ENFORCE(attributes.at("dims").dyn_cast<pir::ArrayAttribute>().at(i).isa<pir::Int64Attribute>(),
                   "Type of attribute: dims is not right.");
  }
  IR_ENFORCE(attributes.count("stride")>0,
                 "stride does not exist.");
  IR_ENFORCE(attributes.at("stride").isa<pir::ArrayAttribute>(),
                 "Type of attribute: stride is not pir::ArrayAttribute.");
  for (size_t i = 0; i < attributes.at("stride").dyn_cast<pir::ArrayAttribute>().size(); i++) {
    IR_ENFORCE(attributes.at("stride").dyn_cast<pir::ArrayAttribute>().at(i).isa<pir::Int64Attribute>(),
                   "Type of attribute: stride is not right.");
  }
  IR_ENFORCE(attributes.count("offset")>0,
                 "offset does not exist.");
  IR_ENFORCE(attributes.at("offset").isa<pir::Int64Attribute>(),
                 "Type of attribute: offset is not pir::Int64Attribute.");

  }
  VLOG(4) << "Verifying outputs:";
  {
  auto output_size = num_results();
  IR_ENFORCE(output_size == 1u,
                    "The size %d of outputs must be equal to 1.", output_size);
  IR_ENFORCE((*this)->result(0).type().isa<paddle::dialect::DenseTensorType>(),
                 "Type validation failed for the 0th output.");
  }
  VLOG(4) << "End Verifying for: AsStridedOp.";
}

void AsStridedOp::InferMeta( phi::InferMetaContext *infer_meta ) {
  auto fn = PD_INFER_META(phi::StridedUnChangedInferMeta);
  fn(infer_meta);
}

phi::DataType AsStridedOp::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: AsStridedOp";
  


  return expected_kernel_dtype;
}

OpInfoTuple AsinOp::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("x", "paddle::dialect::DenseTensorType", false, false, false, true) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = {  };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("out", "paddle::dialect::DenseTensorType", false, false) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("UnchangedInferMeta", {"x"}, "asin", {"x"}, {}, {}, {}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "asin");
}

void AsinOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_) {
  VLOG(4) << "Start build AsinOp";



  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);
  paddle::dialect::IrTensor dense_out;
  paddle::dialect::IrMetaTensor meta_out(&dense_out);

  phi::UnchangedInferMeta(meta_x, &meta_out);

  std::vector<pir::Type> argument_outputs;
  pir::Type out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_out.dtype()), dense_out.dims(), dense_out.layout(), dense_out.lod(), dense_out.offset());
  argument_outputs.push_back(out_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void AsinOp::VerifySig() {
  VLOG(4) << "Start Verifying inputs, outputs and attributes for: AsinOp.";
  VLOG(4) << "Verifying inputs:";
  {
  auto input_size = num_operands();
  IR_ENFORCE(input_size == 1u,
                    "The size %d of inputs must be equal to 1.", input_size);
  IR_ENFORCE((*this)->operand_source(0).type().isa<paddle::dialect::DenseTensorType>(),
                  "Type validation failed for the 0th input, got %s.", (*this)->operand_source(0).type());
  }
  VLOG(4) << "Verifying attributes:";
  {
  // Attributes num is 0, not need to check attributes type.
  }
  VLOG(4) << "Verifying outputs:";
  {
  auto output_size = num_results();
  IR_ENFORCE(output_size == 1u,
                    "The size %d of outputs must be equal to 1.", output_size);
  IR_ENFORCE((*this)->result(0).type().isa<paddle::dialect::DenseTensorType>(),
                 "Type validation failed for the 0th output.");
  }
  VLOG(4) << "End Verifying for: AsinOp.";
}

void AsinOp::InferMeta( phi::InferMetaContext *infer_meta ) {
  auto fn = PD_INFER_META(phi::UnchangedInferMeta);
  fn(infer_meta);
}

phi::DataType AsinOp::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: AsinOp";
  


  return expected_kernel_dtype;
}

OpInfoTuple Asin_Op::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("x", "paddle::dialect::DenseTensorType", false, false, false, true) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = {  };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("out", "paddle::dialect::DenseTensorType", false, false) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("UnchangedInferMeta", {"x"}, "asin", {"x"}, {}, {}, {{"out", "x"}}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "asin");
}

void Asin_Op::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_) {
  VLOG(4) << "Start build Asin_Op";



  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);
  paddle::dialect::IrTensor dense_out;
  paddle::dialect::IrMetaTensor meta_out(&dense_out);

  phi::UnchangedInferMeta(meta_x, &meta_out);

  std::vector<pir::Type> argument_outputs;
  pir::Type out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_out.dtype()), dense_out.dims(), dense_out.layout(), dense_out.lod(), dense_out.offset());
  argument_outputs.push_back(out_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void Asin_Op::VerifySig() {
  VLOG(4) << "Start Verifying inputs, outputs and attributes for: Asin_Op.";
  VLOG(4) << "Verifying inputs:";
  {
  auto input_size = num_operands();
  IR_ENFORCE(input_size == 1u,
                    "The size %d of inputs must be equal to 1.", input_size);
  IR_ENFORCE((*this)->operand_source(0).type().isa<paddle::dialect::DenseTensorType>(),
                  "Type validation failed for the 0th input, got %s.", (*this)->operand_source(0).type());
  }
  VLOG(4) << "Verifying attributes:";
  {
  // Attributes num is 0, not need to check attributes type.
  }
  VLOG(4) << "Verifying outputs:";
  {
  auto output_size = num_results();
  IR_ENFORCE(output_size == 1u,
                    "The size %d of outputs must be equal to 1.", output_size);
  IR_ENFORCE((*this)->result(0).type().isa<paddle::dialect::DenseTensorType>(),
                 "Type validation failed for the 0th output.");
  }
  VLOG(4) << "End Verifying for: Asin_Op.";
}

void Asin_Op::InferMeta( phi::InferMetaContext *infer_meta ) {
  auto fn = PD_INFER_META(phi::UnchangedInferMeta);
  fn(infer_meta);
}

phi::DataType Asin_Op::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: Asin_Op";
  


  return expected_kernel_dtype;
}

OpInfoTuple AsinhOp::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("x", "paddle::dialect::DenseTensorType", false, false, false, true) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = {  };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("out", "paddle::dialect::DenseTensorType", false, false) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("UnchangedInferMeta", {"x"}, "asinh", {"x"}, {}, {}, {}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "asinh");
}

void AsinhOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_) {
  VLOG(4) << "Start build AsinhOp";



  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);
  paddle::dialect::IrTensor dense_out;
  paddle::dialect::IrMetaTensor meta_out(&dense_out);

  phi::UnchangedInferMeta(meta_x, &meta_out);

  std::vector<pir::Type> argument_outputs;
  pir::Type out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_out.dtype()), dense_out.dims(), dense_out.layout(), dense_out.lod(), dense_out.offset());
  argument_outputs.push_back(out_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void AsinhOp::VerifySig() {
  VLOG(4) << "Start Verifying inputs, outputs and attributes for: AsinhOp.";
  VLOG(4) << "Verifying inputs:";
  {
  auto input_size = num_operands();
  IR_ENFORCE(input_size == 1u,
                    "The size %d of inputs must be equal to 1.", input_size);
  IR_ENFORCE((*this)->operand_source(0).type().isa<paddle::dialect::DenseTensorType>(),
                  "Type validation failed for the 0th input, got %s.", (*this)->operand_source(0).type());
  }
  VLOG(4) << "Verifying attributes:";
  {
  // Attributes num is 0, not need to check attributes type.
  }
  VLOG(4) << "Verifying outputs:";
  {
  auto output_size = num_results();
  IR_ENFORCE(output_size == 1u,
                    "The size %d of outputs must be equal to 1.", output_size);
  IR_ENFORCE((*this)->result(0).type().isa<paddle::dialect::DenseTensorType>(),
                 "Type validation failed for the 0th output.");
  }
  VLOG(4) << "End Verifying for: AsinhOp.";
}

void AsinhOp::InferMeta( phi::InferMetaContext *infer_meta ) {
  auto fn = PD_INFER_META(phi::UnchangedInferMeta);
  fn(infer_meta);
}

phi::DataType AsinhOp::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: AsinhOp";
  


  return expected_kernel_dtype;
}

OpInfoTuple Asinh_Op::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("x", "paddle::dialect::DenseTensorType", false, false, false, true) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = {  };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("out", "paddle::dialect::DenseTensorType", false, false) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("UnchangedInferMeta", {"x"}, "asinh", {"x"}, {}, {}, {{"out", "x"}}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "asinh");
}

void Asinh_Op::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_) {
  VLOG(4) << "Start build Asinh_Op";



  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);
  paddle::dialect::IrTensor dense_out;
  paddle::dialect::IrMetaTensor meta_out(&dense_out);

  phi::UnchangedInferMeta(meta_x, &meta_out);

  std::vector<pir::Type> argument_outputs;
  pir::Type out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_out.dtype()), dense_out.dims(), dense_out.layout(), dense_out.lod(), dense_out.offset());
  argument_outputs.push_back(out_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void Asinh_Op::VerifySig() {
  VLOG(4) << "Start Verifying inputs, outputs and attributes for: Asinh_Op.";
  VLOG(4) << "Verifying inputs:";
  {
  auto input_size = num_operands();
  IR_ENFORCE(input_size == 1u,
                    "The size %d of inputs must be equal to 1.", input_size);
  IR_ENFORCE((*this)->operand_source(0).type().isa<paddle::dialect::DenseTensorType>(),
                  "Type validation failed for the 0th input, got %s.", (*this)->operand_source(0).type());
  }
  VLOG(4) << "Verifying attributes:";
  {
  // Attributes num is 0, not need to check attributes type.
  }
  VLOG(4) << "Verifying outputs:";
  {
  auto output_size = num_results();
  IR_ENFORCE(output_size == 1u,
                    "The size %d of outputs must be equal to 1.", output_size);
  IR_ENFORCE((*this)->result(0).type().isa<paddle::dialect::DenseTensorType>(),
                 "Type validation failed for the 0th output.");
  }
  VLOG(4) << "End Verifying for: Asinh_Op.";
}

void Asinh_Op::InferMeta( phi::InferMetaContext *infer_meta ) {
  auto fn = PD_INFER_META(phi::UnchangedInferMeta);
  fn(infer_meta);
}

phi::DataType Asinh_Op::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: Asinh_Op";
  


  return expected_kernel_dtype;
}

OpInfoTuple AtanOp::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("x", "paddle::dialect::DenseTensorType", false, false, false, true) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = {  };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("out", "paddle::dialect::DenseTensorType", false, false) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("UnchangedInferMeta", {"x"}, "atan", {"x"}, {}, {}, {}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "atan");
}

void AtanOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_) {
  VLOG(4) << "Start build AtanOp";



  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);
  paddle::dialect::IrTensor dense_out;
  paddle::dialect::IrMetaTensor meta_out(&dense_out);

  phi::UnchangedInferMeta(meta_x, &meta_out);

  std::vector<pir::Type> argument_outputs;
  pir::Type out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_out.dtype()), dense_out.dims(), dense_out.layout(), dense_out.lod(), dense_out.offset());
  argument_outputs.push_back(out_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void AtanOp::VerifySig() {
  VLOG(4) << "Start Verifying inputs, outputs and attributes for: AtanOp.";
  VLOG(4) << "Verifying inputs:";
  {
  auto input_size = num_operands();
  IR_ENFORCE(input_size == 1u,
                    "The size %d of inputs must be equal to 1.", input_size);
  IR_ENFORCE((*this)->operand_source(0).type().isa<paddle::dialect::DenseTensorType>(),
                  "Type validation failed for the 0th input, got %s.", (*this)->operand_source(0).type());
  }
  VLOG(4) << "Verifying attributes:";
  {
  // Attributes num is 0, not need to check attributes type.
  }
  VLOG(4) << "Verifying outputs:";
  {
  auto output_size = num_results();
  IR_ENFORCE(output_size == 1u,
                    "The size %d of outputs must be equal to 1.", output_size);
  IR_ENFORCE((*this)->result(0).type().isa<paddle::dialect::DenseTensorType>(),
                 "Type validation failed for the 0th output.");
  }
  VLOG(4) << "End Verifying for: AtanOp.";
}

void AtanOp::InferMeta( phi::InferMetaContext *infer_meta ) {
  auto fn = PD_INFER_META(phi::UnchangedInferMeta);
  fn(infer_meta);
}

phi::DataType AtanOp::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: AtanOp";
  


  return expected_kernel_dtype;
}

OpInfoTuple Atan_Op::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("x", "paddle::dialect::DenseTensorType", false, false, false, true) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = {  };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("out", "paddle::dialect::DenseTensorType", false, false) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("UnchangedInferMeta", {"x"}, "atan", {"x"}, {}, {}, {{"out", "x"}}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "atan");
}

void Atan_Op::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_) {
  VLOG(4) << "Start build Atan_Op";



  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);
  paddle::dialect::IrTensor dense_out;
  paddle::dialect::IrMetaTensor meta_out(&dense_out);

  phi::UnchangedInferMeta(meta_x, &meta_out);

  std::vector<pir::Type> argument_outputs;
  pir::Type out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_out.dtype()), dense_out.dims(), dense_out.layout(), dense_out.lod(), dense_out.offset());
  argument_outputs.push_back(out_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void Atan_Op::VerifySig() {
  VLOG(4) << "Start Verifying inputs, outputs and attributes for: Atan_Op.";
  VLOG(4) << "Verifying inputs:";
  {
  auto input_size = num_operands();
  IR_ENFORCE(input_size == 1u,
                    "The size %d of inputs must be equal to 1.", input_size);
  IR_ENFORCE((*this)->operand_source(0).type().isa<paddle::dialect::DenseTensorType>(),
                  "Type validation failed for the 0th input, got %s.", (*this)->operand_source(0).type());
  }
  VLOG(4) << "Verifying attributes:";
  {
  // Attributes num is 0, not need to check attributes type.
  }
  VLOG(4) << "Verifying outputs:";
  {
  auto output_size = num_results();
  IR_ENFORCE(output_size == 1u,
                    "The size %d of outputs must be equal to 1.", output_size);
  IR_ENFORCE((*this)->result(0).type().isa<paddle::dialect::DenseTensorType>(),
                 "Type validation failed for the 0th output.");
  }
  VLOG(4) << "End Verifying for: Atan_Op.";
}

void Atan_Op::InferMeta( phi::InferMetaContext *infer_meta ) {
  auto fn = PD_INFER_META(phi::UnchangedInferMeta);
  fn(infer_meta);
}

phi::DataType Atan_Op::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: Atan_Op";
  


  return expected_kernel_dtype;
}

OpInfoTuple Atan2Op::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("x", "paddle::dialect::DenseTensorType", false, false, false, true), paddle::dialect::OpInputInfo("y", "paddle::dialect::DenseTensorType", false, false, false, true) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = {  };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("out", "paddle::dialect::DenseTensorType", false, false) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("Atan2InferMeta", {"x", "y"}, "atan2", {"x", "y"}, {}, {}, {}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "atan2");
}

void Atan2Op::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, pir::Value y_) {
  VLOG(4) << "Start build Atan2Op";



  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_, y_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;
  paddle::dialect::DenseTensorType y = y_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)y;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);

  VLOG(4) << "Builder construction  dense_y";
  paddle::dialect::IrTensor ir_tensor_y(paddle::dialect::TransToPhiDataType(y.dtype()),
                                                      y.dims(),
                                                      y.data_layout(),
                                                      y.lod(),
                                                      y.offset());
  VLOG(4) << "Builder construction  meta_y";
  paddle::dialect::IrMetaTensor meta_y(&ir_tensor_y);
  paddle::dialect::IrTensor dense_out;
  paddle::dialect::IrMetaTensor meta_out(&dense_out);

  phi::Atan2InferMeta(meta_x, meta_y, &meta_out);

  std::vector<pir::Type> argument_outputs;
  pir::Type out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_out.dtype()), dense_out.dims(), dense_out.layout(), dense_out.lod(), dense_out.offset());
  argument_outputs.push_back(out_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void Atan2Op::VerifySig() {
  VLOG(4) << "Start Verifying inputs, outputs and attributes for: Atan2Op.";
  VLOG(4) << "Verifying inputs:";
  {
  auto input_size = num_operands();
  IR_ENFORCE(input_size == 2u,
                    "The size %d of inputs must be equal to 2.", input_size);
  IR_ENFORCE((*this)->operand_source(0).type().isa<paddle::dialect::DenseTensorType>(),
                  "Type validation failed for the 0th input, got %s.", (*this)->operand_source(0).type());
  IR_ENFORCE((*this)->operand_source(1).type().isa<paddle::dialect::DenseTensorType>(),
                  "Type validation failed for the 1th input, got %s.", (*this)->operand_source(1).type());
  }
  VLOG(4) << "Verifying attributes:";
  {
  // Attributes num is 0, not need to check attributes type.
  }
  VLOG(4) << "Verifying outputs:";
  {
  auto output_size = num_results();
  IR_ENFORCE(output_size == 1u,
                    "The size %d of outputs must be equal to 1.", output_size);
  IR_ENFORCE((*this)->result(0).type().isa<paddle::dialect::DenseTensorType>(),
                 "Type validation failed for the 0th output.");
  }
  VLOG(4) << "End Verifying for: Atan2Op.";
}

void Atan2Op::InferMeta( phi::InferMetaContext *infer_meta ) {
  auto fn = PD_INFER_META(phi::Atan2InferMeta);
  fn(infer_meta);
}

phi::DataType Atan2Op::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: Atan2Op";
  


  return expected_kernel_dtype;
}

OpInfoTuple AtanhOp::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("x", "paddle::dialect::DenseTensorType", false, false, false, true) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = {  };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("out", "paddle::dialect::DenseTensorType", false, false) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("UnchangedInferMeta", {"x"}, "atanh", {"x"}, {}, {}, {}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "atanh");
}

void AtanhOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_) {
  VLOG(4) << "Start build AtanhOp";



  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);
  paddle::dialect::IrTensor dense_out;
  paddle::dialect::IrMetaTensor meta_out(&dense_out);

  phi::UnchangedInferMeta(meta_x, &meta_out);

  std::vector<pir::Type> argument_outputs;
  pir::Type out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_out.dtype()), dense_out.dims(), dense_out.layout(), dense_out.lod(), dense_out.offset());
  argument_outputs.push_back(out_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void AtanhOp::VerifySig() {
  VLOG(4) << "Start Verifying inputs, outputs and attributes for: AtanhOp.";
  VLOG(4) << "Verifying inputs:";
  {
  auto input_size = num_operands();
  IR_ENFORCE(input_size == 1u,
                    "The size %d of inputs must be equal to 1.", input_size);
  IR_ENFORCE((*this)->operand_source(0).type().isa<paddle::dialect::DenseTensorType>(),
                  "Type validation failed for the 0th input, got %s.", (*this)->operand_source(0).type());
  }
  VLOG(4) << "Verifying attributes:";
  {
  // Attributes num is 0, not need to check attributes type.
  }
  VLOG(4) << "Verifying outputs:";
  {
  auto output_size = num_results();
  IR_ENFORCE(output_size == 1u,
                    "The size %d of outputs must be equal to 1.", output_size);
  IR_ENFORCE((*this)->result(0).type().isa<paddle::dialect::DenseTensorType>(),
                 "Type validation failed for the 0th output.");
  }
  VLOG(4) << "End Verifying for: AtanhOp.";
}

void AtanhOp::InferMeta( phi::InferMetaContext *infer_meta ) {
  auto fn = PD_INFER_META(phi::UnchangedInferMeta);
  fn(infer_meta);
}

phi::DataType AtanhOp::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: AtanhOp";
  


  return expected_kernel_dtype;
}

OpInfoTuple Atanh_Op::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("x", "paddle::dialect::DenseTensorType", false, false, false, true) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = {  };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("out", "paddle::dialect::DenseTensorType", false, false) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("UnchangedInferMeta", {"x"}, "atanh", {"x"}, {}, {}, {{"out", "x"}}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "atanh");
}

void Atanh_Op::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_) {
  VLOG(4) << "Start build Atanh_Op";



  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);
  paddle::dialect::IrTensor dense_out;
  paddle::dialect::IrMetaTensor meta_out(&dense_out);

  phi::UnchangedInferMeta(meta_x, &meta_out);

  std::vector<pir::Type> argument_outputs;
  pir::Type out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_out.dtype()), dense_out.dims(), dense_out.layout(), dense_out.lod(), dense_out.offset());
  argument_outputs.push_back(out_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void Atanh_Op::VerifySig() {
  VLOG(4) << "Start Verifying inputs, outputs and attributes for: Atanh_Op.";
  VLOG(4) << "Verifying inputs:";
  {
  auto input_size = num_operands();
  IR_ENFORCE(input_size == 1u,
                    "The size %d of inputs must be equal to 1.", input_size);
  IR_ENFORCE((*this)->operand_source(0).type().isa<paddle::dialect::DenseTensorType>(),
                  "Type validation failed for the 0th input, got %s.", (*this)->operand_source(0).type());
  }
  VLOG(4) << "Verifying attributes:";
  {
  // Attributes num is 0, not need to check attributes type.
  }
  VLOG(4) << "Verifying outputs:";
  {
  auto output_size = num_results();
  IR_ENFORCE(output_size == 1u,
                    "The size %d of outputs must be equal to 1.", output_size);
  IR_ENFORCE((*this)->result(0).type().isa<paddle::dialect::DenseTensorType>(),
                 "Type validation failed for the 0th output.");
  }
  VLOG(4) << "End Verifying for: Atanh_Op.";
}

void Atanh_Op::InferMeta( phi::InferMetaContext *infer_meta ) {
  auto fn = PD_INFER_META(phi::UnchangedInferMeta);
  fn(infer_meta);
}

phi::DataType Atanh_Op::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: Atanh_Op";
  


  return expected_kernel_dtype;
}

const char *AucOp::attributes_name[3] = { "curve", "num_thresholds", "slide_steps" };

OpInfoTuple AucOp::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("x", "paddle::dialect::DenseTensorType", false, false, false, false), paddle::dialect::OpInputInfo("label", "paddle::dialect::DenseTensorType", false, false, false, false), paddle::dialect::OpInputInfo("stat_pos", "paddle::dialect::DenseTensorType", false, false, false, false), paddle::dialect::OpInputInfo("stat_neg", "paddle::dialect::DenseTensorType", false, false, false, false), paddle::dialect::OpInputInfo("ins_tag_weight", "paddle::dialect::DenseTensorType", true, false, false, false) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = { paddle::dialect::OpAttributeInfo("curve", "pir::StrAttribute", ""), paddle::dialect::OpAttributeInfo("num_thresholds", "pir::Int32Attribute", ""), paddle::dialect::OpAttributeInfo("slide_steps", "pir::Int32Attribute", "") };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("auc", "paddle::dialect::DenseTensorType", false, false), paddle::dialect::OpOutputInfo("stat_pos_out", "paddle::dialect::DenseTensorType", false, false), paddle::dialect::OpOutputInfo("stat_neg_out", "paddle::dialect::DenseTensorType", false, false) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("AucInferMeta", {"x", "label", "stat_pos", "stat_neg", "ins_tag_weight", "curve", "num_thresholds", "slide_steps"}, "auc", {"x", "label", "stat_pos", "stat_neg", "ins_tag_weight", "curve", "num_thresholds", "slide_steps"}, {"x"}, {}, {}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "auc");
}

void AucOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, pir::Value label_, pir::Value stat_pos_, pir::Value stat_neg_, pir::Value ins_tag_weight_, const std::string& curve, int num_thresholds, int slide_steps) {
  VLOG(4) << "Start build AucOp";



  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_, label_, stat_pos_, stat_neg_, ins_tag_weight_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_curve = pir::StrAttribute::get(pir::IrContext::Instance(), curve);
  argument.AddAttribute("curve", attr_curve);
  pir::Attribute attr_num_thresholds = pir::Int32Attribute::get(pir::IrContext::Instance(), num_thresholds);
  argument.AddAttribute("num_thresholds", attr_num_thresholds);
  pir::Attribute attr_slide_steps = pir::Int32Attribute::get(pir::IrContext::Instance(), slide_steps);
  argument.AddAttribute("slide_steps", attr_slide_steps);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;
  paddle::dialect::DenseTensorType label = label_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)label;
  paddle::dialect::DenseTensorType stat_pos = stat_pos_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)stat_pos;
  paddle::dialect::DenseTensorType stat_neg = stat_neg_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)stat_neg;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);

  VLOG(4) << "Builder construction  dense_label";
  paddle::dialect::IrTensor ir_tensor_label(paddle::dialect::TransToPhiDataType(label.dtype()),
                                                      label.dims(),
                                                      label.data_layout(),
                                                      label.lod(),
                                                      label.offset());
  VLOG(4) << "Builder construction  meta_label";
  paddle::dialect::IrMetaTensor meta_label(&ir_tensor_label);

  VLOG(4) << "Builder construction  dense_stat_pos";
  paddle::dialect::IrTensor ir_tensor_stat_pos(paddle::dialect::TransToPhiDataType(stat_pos.dtype()),
                                                      stat_pos.dims(),
                                                      stat_pos.data_layout(),
                                                      stat_pos.lod(),
                                                      stat_pos.offset());
  VLOG(4) << "Builder construction  meta_stat_pos";
  paddle::dialect::IrMetaTensor meta_stat_pos(&ir_tensor_stat_pos);

  VLOG(4) << "Builder construction  dense_stat_neg";
  paddle::dialect::IrTensor ir_tensor_stat_neg(paddle::dialect::TransToPhiDataType(stat_neg.dtype()),
                                                      stat_neg.dims(),
                                                      stat_neg.data_layout(),
                                                      stat_neg.lod(),
                                                      stat_neg.offset());
  VLOG(4) << "Builder construction  meta_stat_neg";
  paddle::dialect::IrMetaTensor meta_stat_neg(&ir_tensor_stat_neg);

  paddle::dialect::IrMetaTensor meta_ins_tag_weight;
  paddle::dialect::IrTensor ir_tensor_ins_tag_weight;
  if (ins_tag_weight_.impl() != nullptr) {
    paddle::dialect::DenseTensorType ins_tag_weight = ins_tag_weight_.type().dyn_cast<paddle::dialect::DenseTensorType>();
    VLOG(4) << "Builder construction  dense_ins_tag_weight";
    ir_tensor_ins_tag_weight = paddle::dialect::IrTensor(paddle::dialect::TransToPhiDataType(ins_tag_weight.dtype()),
                                                        ins_tag_weight.dims(),
                                                        ins_tag_weight.data_layout(),
                                                        ins_tag_weight.lod(),
                                                        ins_tag_weight.offset());
    VLOG(4) << "Builder construction  meta_ins_tag_weight";
    meta_ins_tag_weight = paddle::dialect::IrMetaTensor(&ir_tensor_ins_tag_weight);
  }

  paddle::dialect::IrTensor dense_auc;
  paddle::dialect::IrMetaTensor meta_auc(&dense_auc);
  paddle::dialect::IrTensor dense_stat_pos_out;
  paddle::dialect::IrMetaTensor meta_stat_pos_out(&dense_stat_pos_out);
  paddle::dialect::IrTensor dense_stat_neg_out;
  paddle::dialect::IrMetaTensor meta_stat_neg_out(&dense_stat_neg_out);

  phi::AucInferMeta(meta_x, meta_label, meta_stat_pos, meta_stat_neg, meta_ins_tag_weight, curve, num_thresholds, slide_steps, &meta_auc, &meta_stat_pos_out, &meta_stat_neg_out);

  std::vector<pir::Type> argument_outputs;
  pir::Type auc_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_auc.dtype()), dense_auc.dims(), dense_auc.layout(), dense_auc.lod(), dense_auc.offset());
  argument_outputs.push_back(auc_dense_tensor_type);

  pir::Type stat_pos_out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_stat_pos_out.dtype()), dense_stat_pos_out.dims(), dense_stat_pos_out.layout(), dense_stat_pos_out.lod(), dense_stat_pos_out.offset());
  argument_outputs.push_back(stat_pos_out_dense_tensor_type);

  pir::Type stat_neg_out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_stat_neg_out.dtype()), dense_stat_neg_out.dims(), dense_stat_neg_out.layout(), dense_stat_neg_out.lod(), dense_stat_neg_out.offset());
  argument_outputs.push_back(stat_neg_out_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void AucOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, pir::Value label_, pir::Value stat_pos_, pir::Value stat_neg_, pir::Value ins_tag_weight_, pir::AttributeMap attributes) {
  VLOG(4) << "Start build AucOp";


  IR_ENFORCE(
      attributes.find("curve") != attributes.end(),
          "'curve' Attribute is expected for AucOp. ");
  std::string curve = attributes.at("curve").dyn_cast<pir::StrAttribute>().AsString();

  IR_ENFORCE(
      attributes.find("num_thresholds") != attributes.end(),
          "'num_thresholds' Attribute is expected for AucOp. ");
  int num_thresholds = attributes.at("num_thresholds").dyn_cast<pir::Int32Attribute>().data();

  IR_ENFORCE(
      attributes.find("slide_steps") != attributes.end(),
          "'slide_steps' Attribute is expected for AucOp. ");
  int slide_steps = attributes.at("slide_steps").dyn_cast<pir::Int32Attribute>().data();


  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_, label_, stat_pos_, stat_neg_, ins_tag_weight_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_curve = pir::StrAttribute::get(pir::IrContext::Instance(), curve);
  argument.AddAttribute("curve", attr_curve);
  pir::Attribute attr_num_thresholds = pir::Int32Attribute::get(pir::IrContext::Instance(), num_thresholds);
  argument.AddAttribute("num_thresholds", attr_num_thresholds);
  pir::Attribute attr_slide_steps = pir::Int32Attribute::get(pir::IrContext::Instance(), slide_steps);
  argument.AddAttribute("slide_steps", attr_slide_steps);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;
  paddle::dialect::DenseTensorType label = label_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)label;
  paddle::dialect::DenseTensorType stat_pos = stat_pos_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)stat_pos;
  paddle::dialect::DenseTensorType stat_neg = stat_neg_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)stat_neg;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);

  VLOG(4) << "Builder construction  dense_label";
  paddle::dialect::IrTensor ir_tensor_label(paddle::dialect::TransToPhiDataType(label.dtype()),
                                                      label.dims(),
                                                      label.data_layout(),
                                                      label.lod(),
                                                      label.offset());
  VLOG(4) << "Builder construction  meta_label";
  paddle::dialect::IrMetaTensor meta_label(&ir_tensor_label);

  VLOG(4) << "Builder construction  dense_stat_pos";
  paddle::dialect::IrTensor ir_tensor_stat_pos(paddle::dialect::TransToPhiDataType(stat_pos.dtype()),
                                                      stat_pos.dims(),
                                                      stat_pos.data_layout(),
                                                      stat_pos.lod(),
                                                      stat_pos.offset());
  VLOG(4) << "Builder construction  meta_stat_pos";
  paddle::dialect::IrMetaTensor meta_stat_pos(&ir_tensor_stat_pos);

  VLOG(4) << "Builder construction  dense_stat_neg";
  paddle::dialect::IrTensor ir_tensor_stat_neg(paddle::dialect::TransToPhiDataType(stat_neg.dtype()),
                                                      stat_neg.dims(),
                                                      stat_neg.data_layout(),
                                                      stat_neg.lod(),
                                                      stat_neg.offset());
  VLOG(4) << "Builder construction  meta_stat_neg";
  paddle::dialect::IrMetaTensor meta_stat_neg(&ir_tensor_stat_neg);

  paddle::dialect::IrMetaTensor meta_ins_tag_weight;
  paddle::dialect::IrTensor ir_tensor_ins_tag_weight;
  if (ins_tag_weight_.impl() != nullptr) {
    paddle::dialect::DenseTensorType ins_tag_weight = ins_tag_weight_.type().dyn_cast<paddle::dialect::DenseTensorType>();
    VLOG(4) << "Builder construction  dense_ins_tag_weight";
    ir_tensor_ins_tag_weight = paddle::dialect::IrTensor(paddle::dialect::TransToPhiDataType(ins_tag_weight.dtype()),
                                                        ins_tag_weight.dims(),
                                                        ins_tag_weight.data_layout(),
                                                        ins_tag_weight.lod(),
                                                        ins_tag_weight.offset());
    VLOG(4) << "Builder construction  meta_ins_tag_weight";
    meta_ins_tag_weight = paddle::dialect::IrMetaTensor(&ir_tensor_ins_tag_weight);
  }

  paddle::dialect::IrTensor dense_auc;
  paddle::dialect::IrMetaTensor meta_auc(&dense_auc);
  paddle::dialect::IrTensor dense_stat_pos_out;
  paddle::dialect::IrMetaTensor meta_stat_pos_out(&dense_stat_pos_out);
  paddle::dialect::IrTensor dense_stat_neg_out;
  paddle::dialect::IrMetaTensor meta_stat_neg_out(&dense_stat_neg_out);

  phi::AucInferMeta(meta_x, meta_label, meta_stat_pos, meta_stat_neg, meta_ins_tag_weight, curve, num_thresholds, slide_steps, &meta_auc, &meta_stat_pos_out, &meta_stat_neg_out);

  std::vector<pir::Type> argument_outputs;
  pir::Type auc_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_auc.dtype()), dense_auc.dims(), dense_auc.layout(), dense_auc.lod(), dense_auc.offset());
  argument_outputs.push_back(auc_dense_tensor_type);

  pir::Type stat_pos_out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_stat_pos_out.dtype()), dense_stat_pos_out.dims(), dense_stat_pos_out.layout(), dense_stat_pos_out.lod(), dense_stat_pos_out.offset());
  argument_outputs.push_back(stat_pos_out_dense_tensor_type);

  pir::Type stat_neg_out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_stat_neg_out.dtype()), dense_stat_neg_out.dims(), dense_stat_neg_out.layout(), dense_stat_neg_out.lod(), dense_stat_neg_out.offset());
  argument_outputs.push_back(stat_neg_out_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void AucOp::VerifySig() {
  VLOG(4) << "Start Verifying inputs, outputs and attributes for: AucOp.";
  VLOG(4) << "Verifying inputs:";
  {
  auto input_size = num_operands();
  IR_ENFORCE(input_size == 5u,
                    "The size %d of inputs must be equal to 5.", input_size);
  IR_ENFORCE((*this)->operand_source(0).type().isa<paddle::dialect::DenseTensorType>(),
                  "Type validation failed for the 0th input, got %s.", (*this)->operand_source(0).type());
  IR_ENFORCE((*this)->operand_source(1).type().isa<paddle::dialect::DenseTensorType>(),
                  "Type validation failed for the 1th input, got %s.", (*this)->operand_source(1).type());
  IR_ENFORCE((*this)->operand_source(2).type().isa<paddle::dialect::DenseTensorType>(),
                  "Type validation failed for the 2th input, got %s.", (*this)->operand_source(2).type());
  IR_ENFORCE((*this)->operand_source(3).type().isa<paddle::dialect::DenseTensorType>(),
                  "Type validation failed for the 3th input, got %s.", (*this)->operand_source(3).type());
  if (auto val = (*this)->operand(4)) {
    IR_ENFORCE(val.type().isa<paddle::dialect::DenseTensorType>(),
                   "Type validation failed for the 4th input, got %s.", (*this)->operand_source(4).type());
  }
  }
  VLOG(4) << "Verifying attributes:";
  {
  auto& attributes = this->attributes();
  IR_ENFORCE(attributes.count("curve")>0,
                 "curve does not exist.");
  IR_ENFORCE(attributes.at("curve").isa<pir::StrAttribute>(),
                 "Type of attribute: curve is not pir::StrAttribute.");

  IR_ENFORCE(attributes.count("num_thresholds")>0,
                 "num_thresholds does not exist.");
  IR_ENFORCE(attributes.at("num_thresholds").isa<pir::Int32Attribute>(),
                 "Type of attribute: num_thresholds is not pir::Int32Attribute.");

  IR_ENFORCE(attributes.count("slide_steps")>0,
                 "slide_steps does not exist.");
  IR_ENFORCE(attributes.at("slide_steps").isa<pir::Int32Attribute>(),
                 "Type of attribute: slide_steps is not pir::Int32Attribute.");

  }
  VLOG(4) << "Verifying outputs:";
  {
  auto output_size = num_results();
  IR_ENFORCE(output_size == 3u,
                    "The size %d of outputs must be equal to 3.", output_size);
  IR_ENFORCE((*this)->result(0).type().isa<paddle::dialect::DenseTensorType>(),
                 "Type validation failed for the 0th output.");
  IR_ENFORCE((*this)->result(1).type().isa<paddle::dialect::DenseTensorType>(),
                 "Type validation failed for the 1th output.");
  IR_ENFORCE((*this)->result(2).type().isa<paddle::dialect::DenseTensorType>(),
                 "Type validation failed for the 2th output.");
  }
  VLOG(4) << "End Verifying for: AucOp.";
}

void AucOp::InferMeta( phi::InferMetaContext *infer_meta ) {
  auto fn = PD_INFER_META(phi::AucInferMeta);
  fn(infer_meta);
}

phi::DataType AucOp::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: AucOp";
  


  return expected_kernel_dtype;
}

const char *AverageAccumulates_Op::attributes_name[3] = { "average_window", "max_average_window", "min_average_window" };

OpInfoTuple AverageAccumulates_Op::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("param", "paddle::dialect::DenseTensorType", false, false, false, false), paddle::dialect::OpInputInfo("in_sum_1", "paddle::dialect::DenseTensorType", false, false, false, false), paddle::dialect::OpInputInfo("in_sum_2", "paddle::dialect::DenseTensorType", false, false, false, false), paddle::dialect::OpInputInfo("in_sum_3", "paddle::dialect::DenseTensorType", false, false, false, false), paddle::dialect::OpInputInfo("in_num_accumulates", "paddle::dialect::DenseTensorType", false, false, false, false), paddle::dialect::OpInputInfo("in_old_num_accumulates", "paddle::dialect::DenseTensorType", false, false, false, false), paddle::dialect::OpInputInfo("in_num_updates", "paddle::dialect::DenseTensorType", false, false, false, false) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = { paddle::dialect::OpAttributeInfo("average_window", "pir::FloatAttribute", ""), paddle::dialect::OpAttributeInfo("max_average_window", "pir::Int64Attribute", ""), paddle::dialect::OpAttributeInfo("min_average_window", "pir::Int64Attribute", "") };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("out_sum_1", "paddle::dialect::DenseTensorType", false, false), paddle::dialect::OpOutputInfo("out_sum_2", "paddle::dialect::DenseTensorType", false, false), paddle::dialect::OpOutputInfo("out_sum_3", "paddle::dialect::DenseTensorType", false, false), paddle::dialect::OpOutputInfo("out_num_accumulates", "paddle::dialect::DenseTensorType", false, false), paddle::dialect::OpOutputInfo("out_old_num_accumulates", "paddle::dialect::DenseTensorType", false, false), paddle::dialect::OpOutputInfo("out_num_updates", "paddle::dialect::DenseTensorType", false, false) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("AverageAccumulatesInferMeta", {"param", "in_sum_1", "in_sum_2", "in_sum_3", "in_num_accumulates", "in_old_num_accumulates", "in_num_updates", "average_window", "max_average_window", "min_average_window"}, "average_accumulates", {"param", "in_sum_1", "in_sum_2", "in_sum_3", "in_num_accumulates", "in_old_num_accumulates", "in_num_updates", "average_window", "max_average_window", "min_average_window"}, {"param"}, {}, {{"out_sum_1", "in_sum_1"},{"out_sum_2", "in_sum_2"},{"out_sum_3", "in_sum_3"},{"out_num_accumulates", "in_num_accumulates"},{"out_old_num_accumulates", "in_old_num_accumulates"},{"out_num_updates", "in_num_updates"}}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "average_accumulates_");
}

void AverageAccumulates_Op::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value param_, pir::Value in_sum_1_, pir::Value in_sum_2_, pir::Value in_sum_3_, pir::Value in_num_accumulates_, pir::Value in_old_num_accumulates_, pir::Value in_num_updates_, float average_window, int64_t max_average_window, int64_t min_average_window) {
  VLOG(4) << "Start build AverageAccumulates_Op";



  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {param_, in_sum_1_, in_sum_2_, in_sum_3_, in_num_accumulates_, in_old_num_accumulates_, in_num_updates_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_average_window = pir::FloatAttribute::get(pir::IrContext::Instance(), average_window);
  argument.AddAttribute("average_window", attr_average_window);
  pir::Attribute attr_max_average_window = pir::Int64Attribute::get(pir::IrContext::Instance(), max_average_window);
  argument.AddAttribute("max_average_window", attr_max_average_window);
  pir::Attribute attr_min_average_window = pir::Int64Attribute::get(pir::IrContext::Instance(), min_average_window);
  argument.AddAttribute("min_average_window", attr_min_average_window);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType param = param_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)param;
  paddle::dialect::DenseTensorType in_sum_1 = in_sum_1_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)in_sum_1;
  paddle::dialect::DenseTensorType in_sum_2 = in_sum_2_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)in_sum_2;
  paddle::dialect::DenseTensorType in_sum_3 = in_sum_3_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)in_sum_3;
  paddle::dialect::DenseTensorType in_num_accumulates = in_num_accumulates_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)in_num_accumulates;
  paddle::dialect::DenseTensorType in_old_num_accumulates = in_old_num_accumulates_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)in_old_num_accumulates;
  paddle::dialect::DenseTensorType in_num_updates = in_num_updates_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)in_num_updates;

  VLOG(4) << "Builder construction  dense_param";
  paddle::dialect::IrTensor ir_tensor_param(paddle::dialect::TransToPhiDataType(param.dtype()),
                                                      param.dims(),
                                                      param.data_layout(),
                                                      param.lod(),
                                                      param.offset());
  VLOG(4) << "Builder construction  meta_param";
  paddle::dialect::IrMetaTensor meta_param(&ir_tensor_param);

  VLOG(4) << "Builder construction  dense_in_sum_1";
  paddle::dialect::IrTensor ir_tensor_in_sum_1(paddle::dialect::TransToPhiDataType(in_sum_1.dtype()),
                                                      in_sum_1.dims(),
                                                      in_sum_1.data_layout(),
                                                      in_sum_1.lod(),
                                                      in_sum_1.offset());
  VLOG(4) << "Builder construction  meta_in_sum_1";
  paddle::dialect::IrMetaTensor meta_in_sum_1(&ir_tensor_in_sum_1);

  VLOG(4) << "Builder construction  dense_in_sum_2";
  paddle::dialect::IrTensor ir_tensor_in_sum_2(paddle::dialect::TransToPhiDataType(in_sum_2.dtype()),
                                                      in_sum_2.dims(),
                                                      in_sum_2.data_layout(),
                                                      in_sum_2.lod(),
                                                      in_sum_2.offset());
  VLOG(4) << "Builder construction  meta_in_sum_2";
  paddle::dialect::IrMetaTensor meta_in_sum_2(&ir_tensor_in_sum_2);

  VLOG(4) << "Builder construction  dense_in_sum_3";
  paddle::dialect::IrTensor ir_tensor_in_sum_3(paddle::dialect::TransToPhiDataType(in_sum_3.dtype()),
                                                      in_sum_3.dims(),
                                                      in_sum_3.data_layout(),
                                                      in_sum_3.lod(),
                                                      in_sum_3.offset());
  VLOG(4) << "Builder construction  meta_in_sum_3";
  paddle::dialect::IrMetaTensor meta_in_sum_3(&ir_tensor_in_sum_3);

  VLOG(4) << "Builder construction  dense_in_num_accumulates";
  paddle::dialect::IrTensor ir_tensor_in_num_accumulates(paddle::dialect::TransToPhiDataType(in_num_accumulates.dtype()),
                                                      in_num_accumulates.dims(),
                                                      in_num_accumulates.data_layout(),
                                                      in_num_accumulates.lod(),
                                                      in_num_accumulates.offset());
  VLOG(4) << "Builder construction  meta_in_num_accumulates";
  paddle::dialect::IrMetaTensor meta_in_num_accumulates(&ir_tensor_in_num_accumulates);

  VLOG(4) << "Builder construction  dense_in_old_num_accumulates";
  paddle::dialect::IrTensor ir_tensor_in_old_num_accumulates(paddle::dialect::TransToPhiDataType(in_old_num_accumulates.dtype()),
                                                      in_old_num_accumulates.dims(),
                                                      in_old_num_accumulates.data_layout(),
                                                      in_old_num_accumulates.lod(),
                                                      in_old_num_accumulates.offset());
  VLOG(4) << "Builder construction  meta_in_old_num_accumulates";
  paddle::dialect::IrMetaTensor meta_in_old_num_accumulates(&ir_tensor_in_old_num_accumulates);

  VLOG(4) << "Builder construction  dense_in_num_updates";
  paddle::dialect::IrTensor ir_tensor_in_num_updates(paddle::dialect::TransToPhiDataType(in_num_updates.dtype()),
                                                      in_num_updates.dims(),
                                                      in_num_updates.data_layout(),
                                                      in_num_updates.lod(),
                                                      in_num_updates.offset());
  VLOG(4) << "Builder construction  meta_in_num_updates";
  paddle::dialect::IrMetaTensor meta_in_num_updates(&ir_tensor_in_num_updates);
  paddle::dialect::IrTensor dense_out_sum_1;
  paddle::dialect::IrMetaTensor meta_out_sum_1(&dense_out_sum_1);
  paddle::dialect::IrTensor dense_out_sum_2;
  paddle::dialect::IrMetaTensor meta_out_sum_2(&dense_out_sum_2);
  paddle::dialect::IrTensor dense_out_sum_3;
  paddle::dialect::IrMetaTensor meta_out_sum_3(&dense_out_sum_3);
  paddle::dialect::IrTensor dense_out_num_accumulates;
  paddle::dialect::IrMetaTensor meta_out_num_accumulates(&dense_out_num_accumulates);
  paddle::dialect::IrTensor dense_out_old_num_accumulates;
  paddle::dialect::IrMetaTensor meta_out_old_num_accumulates(&dense_out_old_num_accumulates);
  paddle::dialect::IrTensor dense_out_num_updates;
  paddle::dialect::IrMetaTensor meta_out_num_updates(&dense_out_num_updates);

  phi::AverageAccumulatesInferMeta(meta_param, meta_in_sum_1, meta_in_sum_2, meta_in_sum_3, meta_in_num_accumulates, meta_in_old_num_accumulates, meta_in_num_updates, average_window, max_average_window, min_average_window, &meta_out_sum_1, &meta_out_sum_2, &meta_out_sum_3, &meta_out_num_accumulates, &meta_out_old_num_accumulates, &meta_out_num_updates);

  std::vector<pir::Type> argument_outputs;
  pir::Type out_sum_1_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_out_sum_1.dtype()), dense_out_sum_1.dims(), dense_out_sum_1.layout(), dense_out_sum_1.lod(), dense_out_sum_1.offset());
  argument_outputs.push_back(out_sum_1_dense_tensor_type);

  pir::Type out_sum_2_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_out_sum_2.dtype()), dense_out_sum_2.dims(), dense_out_sum_2.layout(), dense_out_sum_2.lod(), dense_out_sum_2.offset());
  argument_outputs.push_back(out_sum_2_dense_tensor_type);

  pir::Type out_sum_3_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_out_sum_3.dtype()), dense_out_sum_3.dims(), dense_out_sum_3.layout(), dense_out_sum_3.lod(), dense_out_sum_3.offset());
  argument_outputs.push_back(out_sum_3_dense_tensor_type);

  pir::Type out_num_accumulates_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_out_num_accumulates.dtype()), dense_out_num_accumulates.dims(), dense_out_num_accumulates.layout(), dense_out_num_accumulates.lod(), dense_out_num_accumulates.offset());
  argument_outputs.push_back(out_num_accumulates_dense_tensor_type);

  pir::Type out_old_num_accumulates_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_out_old_num_accumulates.dtype()), dense_out_old_num_accumulates.dims(), dense_out_old_num_accumulates.layout(), dense_out_old_num_accumulates.lod(), dense_out_old_num_accumulates.offset());
  argument_outputs.push_back(out_old_num_accumulates_dense_tensor_type);

  pir::Type out_num_updates_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_out_num_updates.dtype()), dense_out_num_updates.dims(), dense_out_num_updates.layout(), dense_out_num_updates.lod(), dense_out_num_updates.offset());
  argument_outputs.push_back(out_num_updates_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void AverageAccumulates_Op::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value param_, pir::Value in_sum_1_, pir::Value in_sum_2_, pir::Value in_sum_3_, pir::Value in_num_accumulates_, pir::Value in_old_num_accumulates_, pir::Value in_num_updates_, pir::AttributeMap attributes) {
  VLOG(4) << "Start build AverageAccumulates_Op";


  IR_ENFORCE(
      attributes.find("average_window") != attributes.end(),
          "'average_window' Attribute is expected for AverageAccumulates_Op. ");
  float average_window = attributes.at("average_window").dyn_cast<pir::FloatAttribute>().data();

  IR_ENFORCE(
      attributes.find("max_average_window") != attributes.end(),
          "'max_average_window' Attribute is expected for AverageAccumulates_Op. ");
  int64_t max_average_window = attributes.at("max_average_window").dyn_cast<pir::Int64Attribute>().data();

  IR_ENFORCE(
      attributes.find("min_average_window") != attributes.end(),
          "'min_average_window' Attribute is expected for AverageAccumulates_Op. ");
  int64_t min_average_window = attributes.at("min_average_window").dyn_cast<pir::Int64Attribute>().data();


  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {param_, in_sum_1_, in_sum_2_, in_sum_3_, in_num_accumulates_, in_old_num_accumulates_, in_num_updates_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_average_window = pir::FloatAttribute::get(pir::IrContext::Instance(), average_window);
  argument.AddAttribute("average_window", attr_average_window);
  pir::Attribute attr_max_average_window = pir::Int64Attribute::get(pir::IrContext::Instance(), max_average_window);
  argument.AddAttribute("max_average_window", attr_max_average_window);
  pir::Attribute attr_min_average_window = pir::Int64Attribute::get(pir::IrContext::Instance(), min_average_window);
  argument.AddAttribute("min_average_window", attr_min_average_window);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType param = param_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)param;
  paddle::dialect::DenseTensorType in_sum_1 = in_sum_1_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)in_sum_1;
  paddle::dialect::DenseTensorType in_sum_2 = in_sum_2_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)in_sum_2;
  paddle::dialect::DenseTensorType in_sum_3 = in_sum_3_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)in_sum_3;
  paddle::dialect::DenseTensorType in_num_accumulates = in_num_accumulates_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)in_num_accumulates;
  paddle::dialect::DenseTensorType in_old_num_accumulates = in_old_num_accumulates_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)in_old_num_accumulates;
  paddle::dialect::DenseTensorType in_num_updates = in_num_updates_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)in_num_updates;

  VLOG(4) << "Builder construction  dense_param";
  paddle::dialect::IrTensor ir_tensor_param(paddle::dialect::TransToPhiDataType(param.dtype()),
                                                      param.dims(),
                                                      param.data_layout(),
                                                      param.lod(),
                                                      param.offset());
  VLOG(4) << "Builder construction  meta_param";
  paddle::dialect::IrMetaTensor meta_param(&ir_tensor_param);

  VLOG(4) << "Builder construction  dense_in_sum_1";
  paddle::dialect::IrTensor ir_tensor_in_sum_1(paddle::dialect::TransToPhiDataType(in_sum_1.dtype()),
                                                      in_sum_1.dims(),
                                                      in_sum_1.data_layout(),
                                                      in_sum_1.lod(),
                                                      in_sum_1.offset());
  VLOG(4) << "Builder construction  meta_in_sum_1";
  paddle::dialect::IrMetaTensor meta_in_sum_1(&ir_tensor_in_sum_1);

  VLOG(4) << "Builder construction  dense_in_sum_2";
  paddle::dialect::IrTensor ir_tensor_in_sum_2(paddle::dialect::TransToPhiDataType(in_sum_2.dtype()),
                                                      in_sum_2.dims(),
                                                      in_sum_2.data_layout(),
                                                      in_sum_2.lod(),
                                                      in_sum_2.offset());
  VLOG(4) << "Builder construction  meta_in_sum_2";
  paddle::dialect::IrMetaTensor meta_in_sum_2(&ir_tensor_in_sum_2);

  VLOG(4) << "Builder construction  dense_in_sum_3";
  paddle::dialect::IrTensor ir_tensor_in_sum_3(paddle::dialect::TransToPhiDataType(in_sum_3.dtype()),
                                                      in_sum_3.dims(),
                                                      in_sum_3.data_layout(),
                                                      in_sum_3.lod(),
                                                      in_sum_3.offset());
  VLOG(4) << "Builder construction  meta_in_sum_3";
  paddle::dialect::IrMetaTensor meta_in_sum_3(&ir_tensor_in_sum_3);

  VLOG(4) << "Builder construction  dense_in_num_accumulates";
  paddle::dialect::IrTensor ir_tensor_in_num_accumulates(paddle::dialect::TransToPhiDataType(in_num_accumulates.dtype()),
                                                      in_num_accumulates.dims(),
                                                      in_num_accumulates.data_layout(),
                                                      in_num_accumulates.lod(),
                                                      in_num_accumulates.offset());
  VLOG(4) << "Builder construction  meta_in_num_accumulates";
  paddle::dialect::IrMetaTensor meta_in_num_accumulates(&ir_tensor_in_num_accumulates);

  VLOG(4) << "Builder construction  dense_in_old_num_accumulates";
  paddle::dialect::IrTensor ir_tensor_in_old_num_accumulates(paddle::dialect::TransToPhiDataType(in_old_num_accumulates.dtype()),
                                                      in_old_num_accumulates.dims(),
                                                      in_old_num_accumulates.data_layout(),
                                                      in_old_num_accumulates.lod(),
                                                      in_old_num_accumulates.offset());
  VLOG(4) << "Builder construction  meta_in_old_num_accumulates";
  paddle::dialect::IrMetaTensor meta_in_old_num_accumulates(&ir_tensor_in_old_num_accumulates);

  VLOG(4) << "Builder construction  dense_in_num_updates";
  paddle::dialect::IrTensor ir_tensor_in_num_updates(paddle::dialect::TransToPhiDataType(in_num_updates.dtype()),
                                                      in_num_updates.dims(),
                                                      in_num_updates.data_layout(),
                                                      in_num_updates.lod(),
                                                      in_num_updates.offset());
  VLOG(4) << "Builder construction  meta_in_num_updates";
  paddle::dialect::IrMetaTensor meta_in_num_updates(&ir_tensor_in_num_updates);
  paddle::dialect::IrTensor dense_out_sum_1;
  paddle::dialect::IrMetaTensor meta_out_sum_1(&dense_out_sum_1);
  paddle::dialect::IrTensor dense_out_sum_2;
  paddle::dialect::IrMetaTensor meta_out_sum_2(&dense_out_sum_2);
  paddle::dialect::IrTensor dense_out_sum_3;
  paddle::dialect::IrMetaTensor meta_out_sum_3(&dense_out_sum_3);
  paddle::dialect::IrTensor dense_out_num_accumulates;
  paddle::dialect::IrMetaTensor meta_out_num_accumulates(&dense_out_num_accumulates);
  paddle::dialect::IrTensor dense_out_old_num_accumulates;
  paddle::dialect::IrMetaTensor meta_out_old_num_accumulates(&dense_out_old_num_accumulates);
  paddle::dialect::IrTensor dense_out_num_updates;
  paddle::dialect::IrMetaTensor meta_out_num_updates(&dense_out_num_updates);

  phi::AverageAccumulatesInferMeta(meta_param, meta_in_sum_1, meta_in_sum_2, meta_in_sum_3, meta_in_num_accumulates, meta_in_old_num_accumulates, meta_in_num_updates, average_window, max_average_window, min_average_window, &meta_out_sum_1, &meta_out_sum_2, &meta_out_sum_3, &meta_out_num_accumulates, &meta_out_old_num_accumulates, &meta_out_num_updates);

  std::vector<pir::Type> argument_outputs;
  pir::Type out_sum_1_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_out_sum_1.dtype()), dense_out_sum_1.dims(), dense_out_sum_1.layout(), dense_out_sum_1.lod(), dense_out_sum_1.offset());
  argument_outputs.push_back(out_sum_1_dense_tensor_type);

  pir::Type out_sum_2_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_out_sum_2.dtype()), dense_out_sum_2.dims(), dense_out_sum_2.layout(), dense_out_sum_2.lod(), dense_out_sum_2.offset());
  argument_outputs.push_back(out_sum_2_dense_tensor_type);

  pir::Type out_sum_3_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_out_sum_3.dtype()), dense_out_sum_3.dims(), dense_out_sum_3.layout(), dense_out_sum_3.lod(), dense_out_sum_3.offset());
  argument_outputs.push_back(out_sum_3_dense_tensor_type);

  pir::Type out_num_accumulates_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_out_num_accumulates.dtype()), dense_out_num_accumulates.dims(), dense_out_num_accumulates.layout(), dense_out_num_accumulates.lod(), dense_out_num_accumulates.offset());
  argument_outputs.push_back(out_num_accumulates_dense_tensor_type);

  pir::Type out_old_num_accumulates_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_out_old_num_accumulates.dtype()), dense_out_old_num_accumulates.dims(), dense_out_old_num_accumulates.layout(), dense_out_old_num_accumulates.lod(), dense_out_old_num_accumulates.offset());
  argument_outputs.push_back(out_old_num_accumulates_dense_tensor_type);

  pir::Type out_num_updates_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_out_num_updates.dtype()), dense_out_num_updates.dims(), dense_out_num_updates.layout(), dense_out_num_updates.lod(), dense_out_num_updates.offset());
  argument_outputs.push_back(out_num_updates_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void AverageAccumulates_Op::VerifySig() {
  VLOG(4) << "Start Verifying inputs, outputs and attributes for: AverageAccumulates_Op.";
  VLOG(4) << "Verifying inputs:";
  {
  auto input_size = num_operands();
  IR_ENFORCE(input_size == 7u,
                    "The size %d of inputs must be equal to 7.", input_size);
  IR_ENFORCE((*this)->operand_source(0).type().isa<paddle::dialect::DenseTensorType>(),
                  "Type validation failed for the 0th input, got %s.", (*this)->operand_source(0).type());
  IR_ENFORCE((*this)->operand_source(1).type().isa<paddle::dialect::DenseTensorType>(),
                  "Type validation failed for the 1th input, got %s.", (*this)->operand_source(1).type());
  IR_ENFORCE((*this)->operand_source(2).type().isa<paddle::dialect::DenseTensorType>(),
                  "Type validation failed for the 2th input, got %s.", (*this)->operand_source(2).type());
  IR_ENFORCE((*this)->operand_source(3).type().isa<paddle::dialect::DenseTensorType>(),
                  "Type validation failed for the 3th input, got %s.", (*this)->operand_source(3).type());
  IR_ENFORCE((*this)->operand_source(4).type().isa<paddle::dialect::DenseTensorType>(),
                  "Type validation failed for the 4th input, got %s.", (*this)->operand_source(4).type());
  IR_ENFORCE((*this)->operand_source(5).type().isa<paddle::dialect::DenseTensorType>(),
                  "Type validation failed for the 5th input, got %s.", (*this)->operand_source(5).type());
  IR_ENFORCE((*this)->operand_source(6).type().isa<paddle::dialect::DenseTensorType>(),
                  "Type validation failed for the 6th input, got %s.", (*this)->operand_source(6).type());
  }
  VLOG(4) << "Verifying attributes:";
  {
  auto& attributes = this->attributes();
  IR_ENFORCE(attributes.count("average_window")>0,
                 "average_window does not exist.");
  IR_ENFORCE(attributes.at("average_window").isa<pir::FloatAttribute>(),
                 "Type of attribute: average_window is not pir::FloatAttribute.");

  IR_ENFORCE(attributes.count("max_average_window")>0,
                 "max_average_window does not exist.");
  IR_ENFORCE(attributes.at("max_average_window").isa<pir::Int64Attribute>(),
                 "Type of attribute: max_average_window is not pir::Int64Attribute.");

  IR_ENFORCE(attributes.count("min_average_window")>0,
                 "min_average_window does not exist.");
  IR_ENFORCE(attributes.at("min_average_window").isa<pir::Int64Attribute>(),
                 "Type of attribute: min_average_window is not pir::Int64Attribute.");

  }
  VLOG(4) << "Verifying outputs:";
  {
  auto output_size = num_results();
  IR_ENFORCE(output_size == 6u,
                    "The size %d of outputs must be equal to 6.", output_size);
  IR_ENFORCE((*this)->result(0).type().isa<paddle::dialect::DenseTensorType>(),
                 "Type validation failed for the 0th output.");
  IR_ENFORCE((*this)->result(1).type().isa<paddle::dialect::DenseTensorType>(),
                 "Type validation failed for the 1th output.");
  IR_ENFORCE((*this)->result(2).type().isa<paddle::dialect::DenseTensorType>(),
                 "Type validation failed for the 2th output.");
  IR_ENFORCE((*this)->result(3).type().isa<paddle::dialect::DenseTensorType>(),
                 "Type validation failed for the 3th output.");
  IR_ENFORCE((*this)->result(4).type().isa<paddle::dialect::DenseTensorType>(),
                 "Type validation failed for the 4th output.");
  IR_ENFORCE((*this)->result(5).type().isa<paddle::dialect::DenseTensorType>(),
                 "Type validation failed for the 5th output.");
  }
  VLOG(4) << "End Verifying for: AverageAccumulates_Op.";
}

void AverageAccumulates_Op::InferMeta( phi::InferMetaContext *infer_meta ) {
  auto fn = PD_INFER_META(phi::AverageAccumulatesInferMeta);
  fn(infer_meta);
}

phi::DataType AverageAccumulates_Op::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: AverageAccumulates_Op";
  


  return expected_kernel_dtype;
}

OpInfoTuple BceLossOp::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("input", "paddle::dialect::DenseTensorType", false, false, false, true), paddle::dialect::OpInputInfo("label", "paddle::dialect::DenseTensorType", false, false, false, false) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = {  };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("out", "paddle::dialect::DenseTensorType", false, false) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("BCELossInferMeta", {"input", "label"}, "bce_loss", {"input", "label"}, {"input"}, {}, {}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "bce_loss");
}

void BceLossOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value input_, pir::Value label_) {
  VLOG(4) << "Start build BceLossOp";



  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {input_, label_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType input = input_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)input;
  paddle::dialect::DenseTensorType label = label_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)label;

  VLOG(4) << "Builder construction  dense_input";
  paddle::dialect::IrTensor ir_tensor_input(paddle::dialect::TransToPhiDataType(input.dtype()),
                                                      input.dims(),
                                                      input.data_layout(),
                                                      input.lod(),
                                                      input.offset());
  VLOG(4) << "Builder construction  meta_input";
  paddle::dialect::IrMetaTensor meta_input(&ir_tensor_input);

  VLOG(4) << "Builder construction  dense_label";
  paddle::dialect::IrTensor ir_tensor_label(paddle::dialect::TransToPhiDataType(label.dtype()),
                                                      label.dims(),
                                                      label.data_layout(),
                                                      label.lod(),
                                                      label.offset());
  VLOG(4) << "Builder construction  meta_label";
  paddle::dialect::IrMetaTensor meta_label(&ir_tensor_label);
  paddle::dialect::IrTensor dense_out;
  paddle::dialect::IrMetaTensor meta_out(&dense_out);

  phi::BCELossInferMeta(meta_input, meta_label, &meta_out);

  std::vector<pir::Type> argument_outputs;
  pir::Type out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_out.dtype()), dense_out.dims(), dense_out.layout(), dense_out.lod(), dense_out.offset());
  argument_outputs.push_back(out_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void BceLossOp::VerifySig() {
  VLOG(4) << "Start Verifying inputs, outputs and attributes for: BceLossOp.";
  VLOG(4) << "Verifying inputs:";
  {
  auto input_size = num_operands();
  IR_ENFORCE(input_size == 2u,
                    "The size %d of inputs must be equal to 2.", input_size);
  IR_ENFORCE((*this)->operand_source(0).type().isa<paddle::dialect::DenseTensorType>(),
                  "Type validation failed for the 0th input, got %s.", (*this)->operand_source(0).type());
  IR_ENFORCE((*this)->operand_source(1).type().isa<paddle::dialect::DenseTensorType>(),
                  "Type validation failed for the 1th input, got %s.", (*this)->operand_source(1).type());
  }
  VLOG(4) << "Verifying attributes:";
  {
  // Attributes num is 0, not need to check attributes type.
  }
  VLOG(4) << "Verifying outputs:";
  {
  auto output_size = num_results();
  IR_ENFORCE(output_size == 1u,
                    "The size %d of outputs must be equal to 1.", output_size);
  IR_ENFORCE((*this)->result(0).type().isa<paddle::dialect::DenseTensorType>(),
                 "Type validation failed for the 0th output.");
  }
  VLOG(4) << "End Verifying for: BceLossOp.";
}

void BceLossOp::InferMeta( phi::InferMetaContext *infer_meta ) {
  auto fn = PD_INFER_META(phi::BCELossInferMeta);
  fn(infer_meta);
}

phi::DataType BceLossOp::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: BceLossOp";
  


  return expected_kernel_dtype;
}

OpInfoTuple BceLoss_Op::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("input", "paddle::dialect::DenseTensorType", false, false, false, true), paddle::dialect::OpInputInfo("label", "paddle::dialect::DenseTensorType", false, false, false, false) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = {  };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("out", "paddle::dialect::DenseTensorType", false, false) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("BCELossInferMeta", {"input", "label"}, "bce_loss", {"input", "label"}, {"input"}, {}, {{"out", "input"}}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "bce_loss");
}

void BceLoss_Op::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value input_, pir::Value label_) {
  VLOG(4) << "Start build BceLoss_Op";



  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {input_, label_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType input = input_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)input;
  paddle::dialect::DenseTensorType label = label_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)label;

  VLOG(4) << "Builder construction  dense_input";
  paddle::dialect::IrTensor ir_tensor_input(paddle::dialect::TransToPhiDataType(input.dtype()),
                                                      input.dims(),
                                                      input.data_layout(),
                                                      input.lod(),
                                                      input.offset());
  VLOG(4) << "Builder construction  meta_input";
  paddle::dialect::IrMetaTensor meta_input(&ir_tensor_input);

  VLOG(4) << "Builder construction  dense_label";
  paddle::dialect::IrTensor ir_tensor_label(paddle::dialect::TransToPhiDataType(label.dtype()),
                                                      label.dims(),
                                                      label.data_layout(),
                                                      label.lod(),
                                                      label.offset());
  VLOG(4) << "Builder construction  meta_label";
  paddle::dialect::IrMetaTensor meta_label(&ir_tensor_label);
  paddle::dialect::IrTensor dense_out;
  paddle::dialect::IrMetaTensor meta_out(&dense_out);

  phi::BCELossInferMeta(meta_input, meta_label, &meta_out);

  std::vector<pir::Type> argument_outputs;
  pir::Type out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_out.dtype()), dense_out.dims(), dense_out.layout(), dense_out.lod(), dense_out.offset());
  argument_outputs.push_back(out_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void BceLoss_Op::VerifySig() {
  VLOG(4) << "Start Verifying inputs, outputs and attributes for: BceLoss_Op.";
  VLOG(4) << "Verifying inputs:";
  {
  auto input_size = num_operands();
  IR_ENFORCE(input_size == 2u,
                    "The size %d of inputs must be equal to 2.", input_size);
  IR_ENFORCE((*this)->operand_source(0).type().isa<paddle::dialect::DenseTensorType>(),
                  "Type validation failed for the 0th input, got %s.", (*this)->operand_source(0).type());
  IR_ENFORCE((*this)->operand_source(1).type().isa<paddle::dialect::DenseTensorType>(),
                  "Type validation failed for the 1th input, got %s.", (*this)->operand_source(1).type());
  }
  VLOG(4) << "Verifying attributes:";
  {
  // Attributes num is 0, not need to check attributes type.
  }
  VLOG(4) << "Verifying outputs:";
  {
  auto output_size = num_results();
  IR_ENFORCE(output_size == 1u,
                    "The size %d of outputs must be equal to 1.", output_size);
  IR_ENFORCE((*this)->result(0).type().isa<paddle::dialect::DenseTensorType>(),
                 "Type validation failed for the 0th output.");
  }
  VLOG(4) << "End Verifying for: BceLoss_Op.";
}

void BceLoss_Op::InferMeta( phi::InferMetaContext *infer_meta ) {
  auto fn = PD_INFER_META(phi::BCELossInferMeta);
  fn(infer_meta);
}

phi::DataType BceLoss_Op::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: BceLoss_Op";
  


  return expected_kernel_dtype;
}

OpInfoTuple BernoulliOp::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("x", "paddle::dialect::DenseTensorType", false, false, false, false) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = {  };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("out", "paddle::dialect::DenseTensorType", false, false) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("UnchangedInferMeta", {"x"}, "bernoulli", {"x"}, {}, {}, {}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "bernoulli");
}

void BernoulliOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_) {
  VLOG(4) << "Start build BernoulliOp";



  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);
  paddle::dialect::IrTensor dense_out;
  paddle::dialect::IrMetaTensor meta_out(&dense_out);

  phi::UnchangedInferMeta(meta_x, &meta_out);

  std::vector<pir::Type> argument_outputs;
  pir::Type out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_out.dtype()), dense_out.dims(), dense_out.layout(), dense_out.lod(), dense_out.offset());
  argument_outputs.push_back(out_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void BernoulliOp::VerifySig() {
  VLOG(4) << "Start Verifying inputs, outputs and attributes for: BernoulliOp.";
  VLOG(4) << "Verifying inputs:";
  {
  auto input_size = num_operands();
  IR_ENFORCE(input_size == 1u,
                    "The size %d of inputs must be equal to 1.", input_size);
  IR_ENFORCE((*this)->operand_source(0).type().isa<paddle::dialect::DenseTensorType>(),
                  "Type validation failed for the 0th input, got %s.", (*this)->operand_source(0).type());
  }
  VLOG(4) << "Verifying attributes:";
  {
  // Attributes num is 0, not need to check attributes type.
  }
  VLOG(4) << "Verifying outputs:";
  {
  auto output_size = num_results();
  IR_ENFORCE(output_size == 1u,
                    "The size %d of outputs must be equal to 1.", output_size);
  IR_ENFORCE((*this)->result(0).type().isa<paddle::dialect::DenseTensorType>(),
                 "Type validation failed for the 0th output.");
  }
  VLOG(4) << "End Verifying for: BernoulliOp.";
}

void BernoulliOp::InferMeta( phi::InferMetaContext *infer_meta ) {
  auto fn = PD_INFER_META(phi::UnchangedInferMeta);
  fn(infer_meta);
}

phi::DataType BernoulliOp::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: BernoulliOp";
  


  return expected_kernel_dtype;
}

const char *BicubicInterpOp::attributes_name[8] = { "data_layout", "out_d", "out_h", "out_w", "scale", "interp_method", "align_corners", "align_mode" };

OpInfoTuple BicubicInterpOp::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("x", "paddle::dialect::DenseTensorType", false, false, false, true), paddle::dialect::OpInputInfo("out_size", "paddle::dialect::DenseTensorType", true, false, false, false), paddle::dialect::OpInputInfo("size_tensor", "pir::VectorType<paddle::dialect::DenseTensorType>", true, false, false, false), paddle::dialect::OpInputInfo("scale_tensor", "paddle::dialect::DenseTensorType", true, false, false, false) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = { paddle::dialect::OpAttributeInfo("data_layout", "pir::StrAttribute", ""), paddle::dialect::OpAttributeInfo("out_d", "pir::Int32Attribute", ""), paddle::dialect::OpAttributeInfo("out_h", "pir::Int32Attribute", ""), paddle::dialect::OpAttributeInfo("out_w", "pir::Int32Attribute", ""), paddle::dialect::OpAttributeInfo("scale", "pir::ArrayAttribute<pir::FloatAttribute>", ""), paddle::dialect::OpAttributeInfo("interp_method", "pir::StrAttribute", ""), paddle::dialect::OpAttributeInfo("align_corners", "pir::BoolAttribute", ""), paddle::dialect::OpAttributeInfo("align_mode", "pir::Int32Attribute", "") };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("output", "paddle::dialect::DenseTensorType", false, false) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("InterpolateInferMeta", {"x", "out_size", "size_tensor", "scale_tensor", "data_layout", "out_d", "out_h", "out_w", "scale", "interp_method", "align_corners", "align_mode"}, "bicubic_interp", {"x", "out_size", "size_tensor", "scale_tensor", "data_layout", "out_d", "out_h", "out_w", "scale", "interp_method", "align_corners", "align_mode"}, {"x"}, {}, {}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "bicubic_interp");
}

void BicubicInterpOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, pir::Value out_size_, pir::Value size_tensor_, pir::Value scale_tensor_, const std::string& data_layout, int out_d, int out_h, int out_w, const std::vector<float>& scale, const std::string& interp_method, bool align_corners, int align_mode) {
  VLOG(4) << "Start build BicubicInterpOp";



  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_, out_size_, size_tensor_, scale_tensor_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_data_layout = pir::StrAttribute::get(pir::IrContext::Instance(), data_layout);
  argument.AddAttribute("data_layout", attr_data_layout);
  pir::Attribute attr_out_d = pir::Int32Attribute::get(pir::IrContext::Instance(), out_d);
  argument.AddAttribute("out_d", attr_out_d);
  pir::Attribute attr_out_h = pir::Int32Attribute::get(pir::IrContext::Instance(), out_h);
  argument.AddAttribute("out_h", attr_out_h);
  pir::Attribute attr_out_w = pir::Int32Attribute::get(pir::IrContext::Instance(), out_w);
  argument.AddAttribute("out_w", attr_out_w);
  std::vector<pir::Attribute> vec_scale;
  for (size_t i = 0; i < static_cast<size_t>(scale.size()); i++) {
      pir::Attribute attr_scale = pir::FloatAttribute::get(pir::IrContext::Instance(), scale[i]);

    vec_scale.push_back(attr_scale);
  }
  pir::Attribute attr_scale = pir::ArrayAttribute::get(pir::IrContext::Instance(), vec_scale);
  argument.AddAttribute("scale", attr_scale);
  pir::Attribute attr_interp_method = pir::StrAttribute::get(pir::IrContext::Instance(), interp_method);
  argument.AddAttribute("interp_method", attr_interp_method);
  pir::Attribute attr_align_corners = pir::BoolAttribute::get(pir::IrContext::Instance(), align_corners);
  argument.AddAttribute("align_corners", attr_align_corners);
  pir::Attribute attr_align_mode = pir::Int32Attribute::get(pir::IrContext::Instance(), align_mode);
  argument.AddAttribute("align_mode", attr_align_mode);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);

  paddle::dialect::IrMetaTensor meta_out_size;
  paddle::dialect::IrTensor ir_tensor_out_size;
  if (out_size_.impl() != nullptr) {
    paddle::dialect::DenseTensorType out_size = out_size_.type().dyn_cast<paddle::dialect::DenseTensorType>();
    VLOG(4) << "Builder construction  dense_out_size";
    ir_tensor_out_size = paddle::dialect::IrTensor(paddle::dialect::TransToPhiDataType(out_size.dtype()),
                                                        out_size.dims(),
                                                        out_size.data_layout(),
                                                        out_size.lod(),
                                                        out_size.offset());
    VLOG(4) << "Builder construction  meta_out_size";
    meta_out_size = paddle::dialect::IrMetaTensor(&ir_tensor_out_size);
  }

  std::vector<paddle::dialect::IrTensor> vec_ir_tensor_size_tensor;
  if (size_tensor_.impl() != nullptr) {
    pir::VectorType size_tensor = size_tensor_.type().dyn_cast<pir::VectorType>();
    for (size_t i=0; i < static_cast<size_t>(size_tensor.size()); i++) {
        vec_ir_tensor_size_tensor.push_back(paddle::dialect::IrTensor(paddle::dialect::TransToPhiDataType(size_tensor[i].dyn_cast<paddle::dialect::DenseTensorType>().dtype()),
                                                                        size_tensor[i].dyn_cast<paddle::dialect::DenseTensorType>().dims(),
                                                                        size_tensor[i].dyn_cast<paddle::dialect::DenseTensorType>().data_layout(),
                                                                        size_tensor[i].dyn_cast<paddle::dialect::DenseTensorType>().lod(),
                                                                        size_tensor[i].dyn_cast<paddle::dialect::DenseTensorType>().offset()));
    }
  }

  std::vector<paddle::dialect::IrMetaTensor> vec_meta_size_tensor;
  for (size_t i=0; i < vec_ir_tensor_size_tensor.size(); i++) {
    vec_meta_size_tensor.push_back(paddle::dialect::IrMetaTensor(&vec_ir_tensor_size_tensor[i]));
  }

  std::vector<const phi::MetaTensor*> meta_size_tensor;
  for (size_t i=0; i < static_cast<size_t>(vec_meta_size_tensor.size()); i++) {
    meta_size_tensor.push_back(&vec_meta_size_tensor[i]);
  }


  paddle::dialect::IrMetaTensor meta_scale_tensor;
  paddle::dialect::IrTensor ir_tensor_scale_tensor;
  if (scale_tensor_.impl() != nullptr) {
    paddle::dialect::DenseTensorType scale_tensor = scale_tensor_.type().dyn_cast<paddle::dialect::DenseTensorType>();
    VLOG(4) << "Builder construction  dense_scale_tensor";
    ir_tensor_scale_tensor = paddle::dialect::IrTensor(paddle::dialect::TransToPhiDataType(scale_tensor.dtype()),
                                                        scale_tensor.dims(),
                                                        scale_tensor.data_layout(),
                                                        scale_tensor.lod(),
                                                        scale_tensor.offset());
    VLOG(4) << "Builder construction  meta_scale_tensor";
    meta_scale_tensor = paddle::dialect::IrMetaTensor(&ir_tensor_scale_tensor);
  }

  paddle::dialect::IrTensor dense_output;
  paddle::dialect::IrMetaTensor meta_output(&dense_output);

  phi::InterpolateInferMeta(meta_x, meta_out_size, meta_size_tensor, meta_scale_tensor, data_layout, out_d, out_h, out_w, scale, interp_method, align_corners, align_mode, &meta_output, phi::MetaConfig(false, false));

  std::vector<pir::Type> argument_outputs;
  pir::Type output_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_output.dtype()), dense_output.dims(), dense_output.layout(), dense_output.lod(), dense_output.offset());
  argument_outputs.push_back(output_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void BicubicInterpOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, pir::Value out_size_, pir::Value size_tensor_, pir::Value scale_tensor_, pir::AttributeMap attributes) {
  VLOG(4) << "Start build BicubicInterpOp";


  IR_ENFORCE(
      attributes.find("data_layout") != attributes.end(),
          "'data_layout' Attribute is expected for BicubicInterpOp. ");
  std::string data_layout = attributes.at("data_layout").dyn_cast<pir::StrAttribute>().AsString();

  IR_ENFORCE(
      attributes.find("out_d") != attributes.end(),
          "'out_d' Attribute is expected for BicubicInterpOp. ");
  int out_d = attributes.at("out_d").dyn_cast<pir::Int32Attribute>().data();

  IR_ENFORCE(
      attributes.find("out_h") != attributes.end(),
          "'out_h' Attribute is expected for BicubicInterpOp. ");
  int out_h = attributes.at("out_h").dyn_cast<pir::Int32Attribute>().data();

  IR_ENFORCE(
      attributes.find("out_w") != attributes.end(),
          "'out_w' Attribute is expected for BicubicInterpOp. ");
  int out_w = attributes.at("out_w").dyn_cast<pir::Int32Attribute>().data();

  IR_ENFORCE(
      attributes.find("scale") != attributes.end(),
          "'scale' Attribute is expected for BicubicInterpOp. ");
  std::vector<float> scale;
  for (size_t i = 0; i < attributes.at("scale").dyn_cast<pir::ArrayAttribute>().size(); i++) {
    scale.push_back(attributes.at("scale").dyn_cast<pir::ArrayAttribute>().at(i).dyn_cast<pir::FloatAttribute>().data());
  }

  IR_ENFORCE(
      attributes.find("interp_method") != attributes.end(),
          "'interp_method' Attribute is expected for BicubicInterpOp. ");
  std::string interp_method = attributes.at("interp_method").dyn_cast<pir::StrAttribute>().AsString();

  IR_ENFORCE(
      attributes.find("align_corners") != attributes.end(),
          "'align_corners' Attribute is expected for BicubicInterpOp. ");
  bool align_corners = attributes.at("align_corners").dyn_cast<pir::BoolAttribute>().data();

  IR_ENFORCE(
      attributes.find("align_mode") != attributes.end(),
          "'align_mode' Attribute is expected for BicubicInterpOp. ");
  int align_mode = attributes.at("align_mode").dyn_cast<pir::Int32Attribute>().data();


  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_, out_size_, size_tensor_, scale_tensor_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_data_layout = pir::StrAttribute::get(pir::IrContext::Instance(), data_layout);
  argument.AddAttribute("data_layout", attr_data_layout);
  pir::Attribute attr_out_d = pir::Int32Attribute::get(pir::IrContext::Instance(), out_d);
  argument.AddAttribute("out_d", attr_out_d);
  pir::Attribute attr_out_h = pir::Int32Attribute::get(pir::IrContext::Instance(), out_h);
  argument.AddAttribute("out_h", attr_out_h);
  pir::Attribute attr_out_w = pir::Int32Attribute::get(pir::IrContext::Instance(), out_w);
  argument.AddAttribute("out_w", attr_out_w);
  std::vector<pir::Attribute> vec_scale;
  for (size_t i = 0; i < static_cast<size_t>(scale.size()); i++) {
      pir::Attribute attr_scale = pir::FloatAttribute::get(pir::IrContext::Instance(), scale[i]);

    vec_scale.push_back(attr_scale);
  }
  pir::Attribute attr_scale = pir::ArrayAttribute::get(pir::IrContext::Instance(), vec_scale);
  argument.AddAttribute("scale", attr_scale);
  pir::Attribute attr_interp_method = pir::StrAttribute::get(pir::IrContext::Instance(), interp_method);
  argument.AddAttribute("interp_method", attr_interp_method);
  pir::Attribute attr_align_corners = pir::BoolAttribute::get(pir::IrContext::Instance(), align_corners);
  argument.AddAttribute("align_corners", attr_align_corners);
  pir::Attribute attr_align_mode = pir::Int32Attribute::get(pir::IrContext::Instance(), align_mode);
  argument.AddAttribute("align_mode", attr_align_mode);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);

  paddle::dialect::IrMetaTensor meta_out_size;
  paddle::dialect::IrTensor ir_tensor_out_size;
  if (out_size_.impl() != nullptr) {
    paddle::dialect::DenseTensorType out_size = out_size_.type().dyn_cast<paddle::dialect::DenseTensorType>();
    VLOG(4) << "Builder construction  dense_out_size";
    ir_tensor_out_size = paddle::dialect::IrTensor(paddle::dialect::TransToPhiDataType(out_size.dtype()),
                                                        out_size.dims(),
                                                        out_size.data_layout(),
                                                        out_size.lod(),
                                                        out_size.offset());
    VLOG(4) << "Builder construction  meta_out_size";
    meta_out_size = paddle::dialect::IrMetaTensor(&ir_tensor_out_size);
  }

  std::vector<paddle::dialect::IrTensor> vec_ir_tensor_size_tensor;
  if (size_tensor_.impl() != nullptr) {
    pir::VectorType size_tensor = size_tensor_.type().dyn_cast<pir::VectorType>();
    for (size_t i=0; i < static_cast<size_t>(size_tensor.size()); i++) {
        vec_ir_tensor_size_tensor.push_back(paddle::dialect::IrTensor(paddle::dialect::TransToPhiDataType(size_tensor[i].dyn_cast<paddle::dialect::DenseTensorType>().dtype()),
                                                                        size_tensor[i].dyn_cast<paddle::dialect::DenseTensorType>().dims(),
                                                                        size_tensor[i].dyn_cast<paddle::dialect::DenseTensorType>().data_layout(),
                                                                        size_tensor[i].dyn_cast<paddle::dialect::DenseTensorType>().lod(),
                                                                        size_tensor[i].dyn_cast<paddle::dialect::DenseTensorType>().offset()));
    }
  }

  std::vector<paddle::dialect::IrMetaTensor> vec_meta_size_tensor;
  for (size_t i=0; i < vec_ir_tensor_size_tensor.size(); i++) {
    vec_meta_size_tensor.push_back(paddle::dialect::IrMetaTensor(&vec_ir_tensor_size_tensor[i]));
  }

  std::vector<const phi::MetaTensor*> meta_size_tensor;
  for (size_t i=0; i < static_cast<size_t>(vec_meta_size_tensor.size()); i++) {
    meta_size_tensor.push_back(&vec_meta_size_tensor[i]);
  }


  paddle::dialect::IrMetaTensor meta_scale_tensor;
  paddle::dialect::IrTensor ir_tensor_scale_tensor;
  if (scale_tensor_.impl() != nullptr) {
    paddle::dialect::DenseTensorType scale_tensor = scale_tensor_.type().dyn_cast<paddle::dialect::DenseTensorType>();
    VLOG(4) << "Builder construction  dense_scale_tensor";
    ir_tensor_scale_tensor = paddle::dialect::IrTensor(paddle::dialect::TransToPhiDataType(scale_tensor.dtype()),
                                                        scale_tensor.dims(),
                                                        scale_tensor.data_layout(),
                                                        scale_tensor.lod(),
                                                        scale_tensor.offset());
    VLOG(4) << "Builder construction  meta_scale_tensor";
    meta_scale_tensor = paddle::dialect::IrMetaTensor(&ir_tensor_scale_tensor);
  }

  paddle::dialect::IrTensor dense_output;
  paddle::dialect::IrMetaTensor meta_output(&dense_output);

  phi::InterpolateInferMeta(meta_x, meta_out_size, meta_size_tensor, meta_scale_tensor, data_layout, out_d, out_h, out_w, scale, interp_method, align_corners, align_mode, &meta_output, phi::MetaConfig(false, false));

  std::vector<pir::Type> argument_outputs;
  pir::Type output_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_output.dtype()), dense_output.dims(), dense_output.layout(), dense_output.lod(), dense_output.offset());
  argument_outputs.push_back(output_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void BicubicInterpOp::VerifySig() {
  VLOG(4) << "Start Verifying inputs, outputs and attributes for: BicubicInterpOp.";
  VLOG(4) << "Verifying inputs:";
  {
  auto input_size = num_operands();
  IR_ENFORCE(input_size == 4u,
                    "The size %d of inputs must be equal to 4.", input_size);
  IR_ENFORCE((*this)->operand_source(0).type().isa<paddle::dialect::DenseTensorType>(),
                  "Type validation failed for the 0th input, got %s.", (*this)->operand_source(0).type());
  if (auto val = (*this)->operand(1)) {
    IR_ENFORCE(val.type().isa<paddle::dialect::DenseTensorType>(),
                   "Type validation failed for the 1th input, got %s.", (*this)->operand_source(1).type());
  }
  if (auto val =  (*this)->operand(2)) {
    if (auto vec_type = val.type().dyn_cast<pir::VectorType>()) {
      for (size_t i = 0; i < vec_type.size(); i++) {
        IR_ENFORCE(vec_type[i].isa<paddle::dialect::DenseTensorType>(),
                          "Type validation failed for the 2th input, got %s.", (*this)->operand_source(2).type());
      }
    }
    else {
      IR_ENFORCE(val.type().isa<paddle::dialect::DenseTensorType>(),
                        "Type validation failed for the 2th input, got %s.", (*this)->operand_source(2).type());
    }
  }
  if (auto val = (*this)->operand(3)) {
    IR_ENFORCE(val.type().isa<paddle::dialect::DenseTensorType>(),
                   "Type validation failed for the 3th input, got %s.", (*this)->operand_source(3).type());
  }
  }
  VLOG(4) << "Verifying attributes:";
  {
  auto& attributes = this->attributes();
  IR_ENFORCE(attributes.count("data_layout")>0,
                 "data_layout does not exist.");
  IR_ENFORCE(attributes.at("data_layout").isa<pir::StrAttribute>(),
                 "Type of attribute: data_layout is not pir::StrAttribute.");

  IR_ENFORCE(attributes.count("out_d")>0,
                 "out_d does not exist.");
  IR_ENFORCE(attributes.at("out_d").isa<pir::Int32Attribute>(),
                 "Type of attribute: out_d is not pir::Int32Attribute.");

  IR_ENFORCE(attributes.count("out_h")>0,
                 "out_h does not exist.");
  IR_ENFORCE(attributes.at("out_h").isa<pir::Int32Attribute>(),
                 "Type of attribute: out_h is not pir::Int32Attribute.");

  IR_ENFORCE(attributes.count("out_w")>0,
                 "out_w does not exist.");
  IR_ENFORCE(attributes.at("out_w").isa<pir::Int32Attribute>(),
                 "Type of attribute: out_w is not pir::Int32Attribute.");

  IR_ENFORCE(attributes.count("scale")>0,
                 "scale does not exist.");
  IR_ENFORCE(attributes.at("scale").isa<pir::ArrayAttribute>(),
                 "Type of attribute: scale is not pir::ArrayAttribute.");
  for (size_t i = 0; i < attributes.at("scale").dyn_cast<pir::ArrayAttribute>().size(); i++) {
    IR_ENFORCE(attributes.at("scale").dyn_cast<pir::ArrayAttribute>().at(i).isa<pir::FloatAttribute>(),
                   "Type of attribute: scale is not right.");
  }
  IR_ENFORCE(attributes.count("interp_method")>0,
                 "interp_method does not exist.");
  IR_ENFORCE(attributes.at("interp_method").isa<pir::StrAttribute>(),
                 "Type of attribute: interp_method is not pir::StrAttribute.");

  IR_ENFORCE(attributes.count("align_corners")>0,
                 "align_corners does not exist.");
  IR_ENFORCE(attributes.at("align_corners").isa<pir::BoolAttribute>(),
                 "Type of attribute: align_corners is not pir::BoolAttribute.");

  IR_ENFORCE(attributes.count("align_mode")>0,
                 "align_mode does not exist.");
  IR_ENFORCE(attributes.at("align_mode").isa<pir::Int32Attribute>(),
                 "Type of attribute: align_mode is not pir::Int32Attribute.");

  }
  VLOG(4) << "Verifying outputs:";
  {
  auto output_size = num_results();
  IR_ENFORCE(output_size == 1u,
                    "The size %d of outputs must be equal to 1.", output_size);
  IR_ENFORCE((*this)->result(0).type().isa<paddle::dialect::DenseTensorType>(),
                 "Type validation failed for the 0th output.");
  }
  VLOG(4) << "End Verifying for: BicubicInterpOp.";
}

void BicubicInterpOp::InferMeta( phi::InferMetaContext *infer_meta ) {
  auto fn = PD_INFER_META(phi::InterpolateInferMeta);
  fn(infer_meta);
}

phi::DataType BicubicInterpOp::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: BicubicInterpOp";
  

  // deal skip data transform
  if (var_name == "out_size" || var_name == "size_tensor" || var_name == "scale_tensor"){
    return expected_kernel_dtype;
  }


  return expected_kernel_dtype;
}

OpInfoTuple BilinearOp::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("x", "paddle::dialect::DenseTensorType", false, false, false, true), paddle::dialect::OpInputInfo("y", "paddle::dialect::DenseTensorType", false, false, false, true), paddle::dialect::OpInputInfo("weight", "paddle::dialect::DenseTensorType", false, false, false, true), paddle::dialect::OpInputInfo("bias", "paddle::dialect::DenseTensorType", true, false, false, true) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = {  };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("out", "paddle::dialect::DenseTensorType", false, false) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("BilinearInferMeta", {"x", "y", "weight", "bias"}, "bilinear", {"x", "y", "weight", "bias"}, {}, {}, {}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "bilinear");
}

void BilinearOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, pir::Value y_, pir::Value weight_, pir::Value bias_) {
  VLOG(4) << "Start build BilinearOp";



  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_, y_, weight_, bias_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;
  paddle::dialect::DenseTensorType y = y_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)y;
  paddle::dialect::DenseTensorType weight = weight_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)weight;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);

  VLOG(4) << "Builder construction  dense_y";
  paddle::dialect::IrTensor ir_tensor_y(paddle::dialect::TransToPhiDataType(y.dtype()),
                                                      y.dims(),
                                                      y.data_layout(),
                                                      y.lod(),
                                                      y.offset());
  VLOG(4) << "Builder construction  meta_y";
  paddle::dialect::IrMetaTensor meta_y(&ir_tensor_y);

  VLOG(4) << "Builder construction  dense_weight";
  paddle::dialect::IrTensor ir_tensor_weight(paddle::dialect::TransToPhiDataType(weight.dtype()),
                                                      weight.dims(),
                                                      weight.data_layout(),
                                                      weight.lod(),
                                                      weight.offset());
  VLOG(4) << "Builder construction  meta_weight";
  paddle::dialect::IrMetaTensor meta_weight(&ir_tensor_weight);

  paddle::dialect::IrMetaTensor meta_bias;
  paddle::dialect::IrTensor ir_tensor_bias;
  if (bias_.impl() != nullptr) {
    paddle::dialect::DenseTensorType bias = bias_.type().dyn_cast<paddle::dialect::DenseTensorType>();
    VLOG(4) << "Builder construction  dense_bias";
    ir_tensor_bias = paddle::dialect::IrTensor(paddle::dialect::TransToPhiDataType(bias.dtype()),
                                                        bias.dims(),
                                                        bias.data_layout(),
                                                        bias.lod(),
                                                        bias.offset());
    VLOG(4) << "Builder construction  meta_bias";
    meta_bias = paddle::dialect::IrMetaTensor(&ir_tensor_bias);
  }

  paddle::dialect::IrTensor dense_out;
  paddle::dialect::IrMetaTensor meta_out(&dense_out);

  phi::BilinearInferMeta(meta_x, meta_y, meta_weight, meta_bias, &meta_out);

  std::vector<pir::Type> argument_outputs;
  pir::Type out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_out.dtype()), dense_out.dims(), dense_out.layout(), dense_out.lod(), dense_out.offset());
  argument_outputs.push_back(out_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void BilinearOp::VerifySig() {
  VLOG(4) << "Start Verifying inputs, outputs and attributes for: BilinearOp.";
  VLOG(4) << "Verifying inputs:";
  {
  auto input_size = num_operands();
  IR_ENFORCE(input_size == 4u,
                    "The size %d of inputs must be equal to 4.", input_size);
  IR_ENFORCE((*this)->operand_source(0).type().isa<paddle::dialect::DenseTensorType>(),
                  "Type validation failed for the 0th input, got %s.", (*this)->operand_source(0).type());
  IR_ENFORCE((*this)->operand_source(1).type().isa<paddle::dialect::DenseTensorType>(),
                  "Type validation failed for the 1th input, got %s.", (*this)->operand_source(1).type());
  IR_ENFORCE((*this)->operand_source(2).type().isa<paddle::dialect::DenseTensorType>(),
                  "Type validation failed for the 2th input, got %s.", (*this)->operand_source(2).type());
  if (auto val = (*this)->operand(3)) {
    IR_ENFORCE(val.type().isa<paddle::dialect::DenseTensorType>(),
                   "Type validation failed for the 3th input, got %s.", (*this)->operand_source(3).type());
  }
  }
  VLOG(4) << "Verifying attributes:";
  {
  // Attributes num is 0, not need to check attributes type.
  }
  VLOG(4) << "Verifying outputs:";
  {
  auto output_size = num_results();
  IR_ENFORCE(output_size == 1u,
                    "The size %d of outputs must be equal to 1.", output_size);
  IR_ENFORCE((*this)->result(0).type().isa<paddle::dialect::DenseTensorType>(),
                 "Type validation failed for the 0th output.");
  }
  VLOG(4) << "End Verifying for: BilinearOp.";
}

void BilinearOp::InferMeta( phi::InferMetaContext *infer_meta ) {
  auto fn = PD_INFER_META(phi::BilinearInferMeta);
  fn(infer_meta);
}

phi::DataType BilinearOp::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: BilinearOp";
  


  return expected_kernel_dtype;
}

const char *BilinearInterpOp::attributes_name[8] = { "data_layout", "out_d", "out_h", "out_w", "scale", "interp_method", "align_corners", "align_mode" };

OpInfoTuple BilinearInterpOp::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("x", "paddle::dialect::DenseTensorType", false, false, false, true), paddle::dialect::OpInputInfo("out_size", "paddle::dialect::DenseTensorType", true, false, false, false), paddle::dialect::OpInputInfo("size_tensor", "pir::VectorType<paddle::dialect::DenseTensorType>", true, false, false, false), paddle::dialect::OpInputInfo("scale_tensor", "paddle::dialect::DenseTensorType", true, false, false, false) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = { paddle::dialect::OpAttributeInfo("data_layout", "pir::StrAttribute", ""), paddle::dialect::OpAttributeInfo("out_d", "pir::Int32Attribute", ""), paddle::dialect::OpAttributeInfo("out_h", "pir::Int32Attribute", ""), paddle::dialect::OpAttributeInfo("out_w", "pir::Int32Attribute", ""), paddle::dialect::OpAttributeInfo("scale", "pir::ArrayAttribute<pir::FloatAttribute>", ""), paddle::dialect::OpAttributeInfo("interp_method", "pir::StrAttribute", ""), paddle::dialect::OpAttributeInfo("align_corners", "pir::BoolAttribute", ""), paddle::dialect::OpAttributeInfo("align_mode", "pir::Int32Attribute", "") };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("output", "paddle::dialect::DenseTensorType", false, false) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("InterpolateInferMeta", {"x", "out_size", "size_tensor", "scale_tensor", "data_layout", "out_d", "out_h", "out_w", "scale", "interp_method", "align_corners", "align_mode"}, "bilinear_interp", {"x", "out_size", "size_tensor", "scale_tensor", "data_layout", "out_d", "out_h", "out_w", "scale", "interp_method", "align_corners", "align_mode"}, {"x"}, {}, {}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "bilinear_interp");
}

void BilinearInterpOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, pir::Value out_size_, pir::Value size_tensor_, pir::Value scale_tensor_, const std::string& data_layout, int out_d, int out_h, int out_w, const std::vector<float>& scale, const std::string& interp_method, bool align_corners, int align_mode) {
  VLOG(4) << "Start build BilinearInterpOp";



  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_, out_size_, size_tensor_, scale_tensor_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_data_layout = pir::StrAttribute::get(pir::IrContext::Instance(), data_layout);
  argument.AddAttribute("data_layout", attr_data_layout);
  pir::Attribute attr_out_d = pir::Int32Attribute::get(pir::IrContext::Instance(), out_d);
  argument.AddAttribute("out_d", attr_out_d);
  pir::Attribute attr_out_h = pir::Int32Attribute::get(pir::IrContext::Instance(), out_h);
  argument.AddAttribute("out_h", attr_out_h);
  pir::Attribute attr_out_w = pir::Int32Attribute::get(pir::IrContext::Instance(), out_w);
  argument.AddAttribute("out_w", attr_out_w);
  std::vector<pir::Attribute> vec_scale;
  for (size_t i = 0; i < static_cast<size_t>(scale.size()); i++) {
      pir::Attribute attr_scale = pir::FloatAttribute::get(pir::IrContext::Instance(), scale[i]);

    vec_scale.push_back(attr_scale);
  }
  pir::Attribute attr_scale = pir::ArrayAttribute::get(pir::IrContext::Instance(), vec_scale);
  argument.AddAttribute("scale", attr_scale);
  pir::Attribute attr_interp_method = pir::StrAttribute::get(pir::IrContext::Instance(), interp_method);
  argument.AddAttribute("interp_method", attr_interp_method);
  pir::Attribute attr_align_corners = pir::BoolAttribute::get(pir::IrContext::Instance(), align_corners);
  argument.AddAttribute("align_corners", attr_align_corners);
  pir::Attribute attr_align_mode = pir::Int32Attribute::get(pir::IrContext::Instance(), align_mode);
  argument.AddAttribute("align_mode", attr_align_mode);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);

  paddle::dialect::IrMetaTensor meta_out_size;
  paddle::dialect::IrTensor ir_tensor_out_size;
  if (out_size_.impl() != nullptr) {
    paddle::dialect::DenseTensorType out_size = out_size_.type().dyn_cast<paddle::dialect::DenseTensorType>();
    VLOG(4) << "Builder construction  dense_out_size";
    ir_tensor_out_size = paddle::dialect::IrTensor(paddle::dialect::TransToPhiDataType(out_size.dtype()),
                                                        out_size.dims(),
                                                        out_size.data_layout(),
                                                        out_size.lod(),
                                                        out_size.offset());
    VLOG(4) << "Builder construction  meta_out_size";
    meta_out_size = paddle::dialect::IrMetaTensor(&ir_tensor_out_size);
  }

  std::vector<paddle::dialect::IrTensor> vec_ir_tensor_size_tensor;
  if (size_tensor_.impl() != nullptr) {
    pir::VectorType size_tensor = size_tensor_.type().dyn_cast<pir::VectorType>();
    for (size_t i=0; i < static_cast<size_t>(size_tensor.size()); i++) {
        vec_ir_tensor_size_tensor.push_back(paddle::dialect::IrTensor(paddle::dialect::TransToPhiDataType(size_tensor[i].dyn_cast<paddle::dialect::DenseTensorType>().dtype()),
                                                                        size_tensor[i].dyn_cast<paddle::dialect::DenseTensorType>().dims(),
                                                                        size_tensor[i].dyn_cast<paddle::dialect::DenseTensorType>().data_layout(),
                                                                        size_tensor[i].dyn_cast<paddle::dialect::DenseTensorType>().lod(),
                                                                        size_tensor[i].dyn_cast<paddle::dialect::DenseTensorType>().offset()));
    }
  }

  std::vector<paddle::dialect::IrMetaTensor> vec_meta_size_tensor;
  for (size_t i=0; i < vec_ir_tensor_size_tensor.size(); i++) {
    vec_meta_size_tensor.push_back(paddle::dialect::IrMetaTensor(&vec_ir_tensor_size_tensor[i]));
  }

  std::vector<const phi::MetaTensor*> meta_size_tensor;
  for (size_t i=0; i < static_cast<size_t>(vec_meta_size_tensor.size()); i++) {
    meta_size_tensor.push_back(&vec_meta_size_tensor[i]);
  }


  paddle::dialect::IrMetaTensor meta_scale_tensor;
  paddle::dialect::IrTensor ir_tensor_scale_tensor;
  if (scale_tensor_.impl() != nullptr) {
    paddle::dialect::DenseTensorType scale_tensor = scale_tensor_.type().dyn_cast<paddle::dialect::DenseTensorType>();
    VLOG(4) << "Builder construction  dense_scale_tensor";
    ir_tensor_scale_tensor = paddle::dialect::IrTensor(paddle::dialect::TransToPhiDataType(scale_tensor.dtype()),
                                                        scale_tensor.dims(),
                                                        scale_tensor.data_layout(),
                                                        scale_tensor.lod(),
                                                        scale_tensor.offset());
    VLOG(4) << "Builder construction  meta_scale_tensor";
    meta_scale_tensor = paddle::dialect::IrMetaTensor(&ir_tensor_scale_tensor);
  }

  paddle::dialect::IrTensor dense_output;
  paddle::dialect::IrMetaTensor meta_output(&dense_output);

  phi::InterpolateInferMeta(meta_x, meta_out_size, meta_size_tensor, meta_scale_tensor, data_layout, out_d, out_h, out_w, scale, interp_method, align_corners, align_mode, &meta_output, phi::MetaConfig(false, false));

  std::vector<pir::Type> argument_outputs;
  pir::Type output_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_output.dtype()), dense_output.dims(), dense_output.layout(), dense_output.lod(), dense_output.offset());
  argument_outputs.push_back(output_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void BilinearInterpOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, pir::Value out_size_, pir::Value size_tensor_, pir::Value scale_tensor_, pir::AttributeMap attributes) {
  VLOG(4) << "Start build BilinearInterpOp";


  IR_ENFORCE(
      attributes.find("data_layout") != attributes.end(),
          "'data_layout' Attribute is expected for BilinearInterpOp. ");
  std::string data_layout = attributes.at("data_layout").dyn_cast<pir::StrAttribute>().AsString();

  IR_ENFORCE(
      attributes.find("out_d") != attributes.end(),
          "'out_d' Attribute is expected for BilinearInterpOp. ");
  int out_d = attributes.at("out_d").dyn_cast<pir::Int32Attribute>().data();

  IR_ENFORCE(
      attributes.find("out_h") != attributes.end(),
          "'out_h' Attribute is expected for BilinearInterpOp. ");
  int out_h = attributes.at("out_h").dyn_cast<pir::Int32Attribute>().data();

  IR_ENFORCE(
      attributes.find("out_w") != attributes.end(),
          "'out_w' Attribute is expected for BilinearInterpOp. ");
  int out_w = attributes.at("out_w").dyn_cast<pir::Int32Attribute>().data();

  IR_ENFORCE(
      attributes.find("scale") != attributes.end(),
          "'scale' Attribute is expected for BilinearInterpOp. ");
  std::vector<float> scale;
  for (size_t i = 0; i < attributes.at("scale").dyn_cast<pir::ArrayAttribute>().size(); i++) {
    scale.push_back(attributes.at("scale").dyn_cast<pir::ArrayAttribute>().at(i).dyn_cast<pir::FloatAttribute>().data());
  }

  IR_ENFORCE(
      attributes.find("interp_method") != attributes.end(),
          "'interp_method' Attribute is expected for BilinearInterpOp. ");
  std::string interp_method = attributes.at("interp_method").dyn_cast<pir::StrAttribute>().AsString();

  IR_ENFORCE(
      attributes.find("align_corners") != attributes.end(),
          "'align_corners' Attribute is expected for BilinearInterpOp. ");
  bool align_corners = attributes.at("align_corners").dyn_cast<pir::BoolAttribute>().data();

  IR_ENFORCE(
      attributes.find("align_mode") != attributes.end(),
          "'align_mode' Attribute is expected for BilinearInterpOp. ");
  int align_mode = attributes.at("align_mode").dyn_cast<pir::Int32Attribute>().data();


  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_, out_size_, size_tensor_, scale_tensor_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_data_layout = pir::StrAttribute::get(pir::IrContext::Instance(), data_layout);
  argument.AddAttribute("data_layout", attr_data_layout);
  pir::Attribute attr_out_d = pir::Int32Attribute::get(pir::IrContext::Instance(), out_d);
  argument.AddAttribute("out_d", attr_out_d);
  pir::Attribute attr_out_h = pir::Int32Attribute::get(pir::IrContext::Instance(), out_h);
  argument.AddAttribute("out_h", attr_out_h);
  pir::Attribute attr_out_w = pir::Int32Attribute::get(pir::IrContext::Instance(), out_w);
  argument.AddAttribute("out_w", attr_out_w);
  std::vector<pir::Attribute> vec_scale;
  for (size_t i = 0; i < static_cast<size_t>(scale.size()); i++) {
      pir::Attribute attr_scale = pir::FloatAttribute::get(pir::IrContext::Instance(), scale[i]);

    vec_scale.push_back(attr_scale);
  }
  pir::Attribute attr_scale = pir::ArrayAttribute::get(pir::IrContext::Instance(), vec_scale);
  argument.AddAttribute("scale", attr_scale);
  pir::Attribute attr_interp_method = pir::StrAttribute::get(pir::IrContext::Instance(), interp_method);
  argument.AddAttribute("interp_method", attr_interp_method);
  pir::Attribute attr_align_corners = pir::BoolAttribute::get(pir::IrContext::Instance(), align_corners);
  argument.AddAttribute("align_corners", attr_align_corners);
  pir::Attribute attr_align_mode = pir::Int32Attribute::get(pir::IrContext::Instance(), align_mode);
  argument.AddAttribute("align_mode", attr_align_mode);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);

  paddle::dialect::IrMetaTensor meta_out_size;
  paddle::dialect::IrTensor ir_tensor_out_size;
  if (out_size_.impl() != nullptr) {
    paddle::dialect::DenseTensorType out_size = out_size_.type().dyn_cast<paddle::dialect::DenseTensorType>();
    VLOG(4) << "Builder construction  dense_out_size";
    ir_tensor_out_size = paddle::dialect::IrTensor(paddle::dialect::TransToPhiDataType(out_size.dtype()),
                                                        out_size.dims(),
                                                        out_size.data_layout(),
                                                        out_size.lod(),
                                                        out_size.offset());
    VLOG(4) << "Builder construction  meta_out_size";
    meta_out_size = paddle::dialect::IrMetaTensor(&ir_tensor_out_size);
  }

  std::vector<paddle::dialect::IrTensor> vec_ir_tensor_size_tensor;
  if (size_tensor_.impl() != nullptr) {
    pir::VectorType size_tensor = size_tensor_.type().dyn_cast<pir::VectorType>();
    for (size_t i=0; i < static_cast<size_t>(size_tensor.size()); i++) {
        vec_ir_tensor_size_tensor.push_back(paddle::dialect::IrTensor(paddle::dialect::TransToPhiDataType(size_tensor[i].dyn_cast<paddle::dialect::DenseTensorType>().dtype()),
                                                                        size_tensor[i].dyn_cast<paddle::dialect::DenseTensorType>().dims(),
                                                                        size_tensor[i].dyn_cast<paddle::dialect::DenseTensorType>().data_layout(),
                                                                        size_tensor[i].dyn_cast<paddle::dialect::DenseTensorType>().lod(),
                                                                        size_tensor[i].dyn_cast<paddle::dialect::DenseTensorType>().offset()));
    }
  }

  std::vector<paddle::dialect::IrMetaTensor> vec_meta_size_tensor;
  for (size_t i=0; i < vec_ir_tensor_size_tensor.size(); i++) {
    vec_meta_size_tensor.push_back(paddle::dialect::IrMetaTensor(&vec_ir_tensor_size_tensor[i]));
  }

  std::vector<const phi::MetaTensor*> meta_size_tensor;
  for (size_t i=0; i < static_cast<size_t>(vec_meta_size_tensor.size()); i++) {
    meta_size_tensor.push_back(&vec_meta_size_tensor[i]);
  }


  paddle::dialect::IrMetaTensor meta_scale_tensor;
  paddle::dialect::IrTensor ir_tensor_scale_tensor;
  if (scale_tensor_.impl() != nullptr) {
    paddle::dialect::DenseTensorType scale_tensor = scale_tensor_.type().dyn_cast<paddle::dialect::DenseTensorType>();
    VLOG(4) << "Builder construction  dense_scale_tensor";
    ir_tensor_scale_tensor = paddle::dialect::IrTensor(paddle::dialect::TransToPhiDataType(scale_tensor.dtype()),
                                                        scale_tensor.dims(),
                                                        scale_tensor.data_layout(),
                                                        scale_tensor.lod(),
                                                        scale_tensor.offset());
    VLOG(4) << "Builder construction  meta_scale_tensor";
    meta_scale_tensor = paddle::dialect::IrMetaTensor(&ir_tensor_scale_tensor);
  }

  paddle::dialect::IrTensor dense_output;
  paddle::dialect::IrMetaTensor meta_output(&dense_output);

  phi::InterpolateInferMeta(meta_x, meta_out_size, meta_size_tensor, meta_scale_tensor, data_layout, out_d, out_h, out_w, scale, interp_method, align_corners, align_mode, &meta_output, phi::MetaConfig(false, false));

  std::vector<pir::Type> argument_outputs;
  pir::Type output_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_output.dtype()), dense_output.dims(), dense_output.layout(), dense_output.lod(), dense_output.offset());
  argument_outputs.push_back(output_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void BilinearInterpOp::VerifySig() {
  VLOG(4) << "Start Verifying inputs, outputs and attributes for: BilinearInterpOp.";
  VLOG(4) << "Verifying inputs:";
  {
  auto input_size = num_operands();
  IR_ENFORCE(input_size == 4u,
                    "The size %d of inputs must be equal to 4.", input_size);
  IR_ENFORCE((*this)->operand_source(0).type().isa<paddle::dialect::DenseTensorType>(),
                  "Type validation failed for the 0th input, got %s.", (*this)->operand_source(0).type());
  if (auto val = (*this)->operand(1)) {
    IR_ENFORCE(val.type().isa<paddle::dialect::DenseTensorType>(),
                   "Type validation failed for the 1th input, got %s.", (*this)->operand_source(1).type());
  }
  if (auto val =  (*this)->operand(2)) {
    if (auto vec_type = val.type().dyn_cast<pir::VectorType>()) {
      for (size_t i = 0; i < vec_type.size(); i++) {
        IR_ENFORCE(vec_type[i].isa<paddle::dialect::DenseTensorType>(),
                          "Type validation failed for the 2th input, got %s.", (*this)->operand_source(2).type());
      }
    }
    else {
      IR_ENFORCE(val.type().isa<paddle::dialect::DenseTensorType>(),
                        "Type validation failed for the 2th input, got %s.", (*this)->operand_source(2).type());
    }
  }
  if (auto val = (*this)->operand(3)) {
    IR_ENFORCE(val.type().isa<paddle::dialect::DenseTensorType>(),
                   "Type validation failed for the 3th input, got %s.", (*this)->operand_source(3).type());
  }
  }
  VLOG(4) << "Verifying attributes:";
  {
  auto& attributes = this->attributes();
  IR_ENFORCE(attributes.count("data_layout")>0,
                 "data_layout does not exist.");
  IR_ENFORCE(attributes.at("data_layout").isa<pir::StrAttribute>(),
                 "Type of attribute: data_layout is not pir::StrAttribute.");

  IR_ENFORCE(attributes.count("out_d")>0,
                 "out_d does not exist.");
  IR_ENFORCE(attributes.at("out_d").isa<pir::Int32Attribute>(),
                 "Type of attribute: out_d is not pir::Int32Attribute.");

  IR_ENFORCE(attributes.count("out_h")>0,
                 "out_h does not exist.");
  IR_ENFORCE(attributes.at("out_h").isa<pir::Int32Attribute>(),
                 "Type of attribute: out_h is not pir::Int32Attribute.");

  IR_ENFORCE(attributes.count("out_w")>0,
                 "out_w does not exist.");
  IR_ENFORCE(attributes.at("out_w").isa<pir::Int32Attribute>(),
                 "Type of attribute: out_w is not pir::Int32Attribute.");

  IR_ENFORCE(attributes.count("scale")>0,
                 "scale does not exist.");
  IR_ENFORCE(attributes.at("scale").isa<pir::ArrayAttribute>(),
                 "Type of attribute: scale is not pir::ArrayAttribute.");
  for (size_t i = 0; i < attributes.at("scale").dyn_cast<pir::ArrayAttribute>().size(); i++) {
    IR_ENFORCE(attributes.at("scale").dyn_cast<pir::ArrayAttribute>().at(i).isa<pir::FloatAttribute>(),
                   "Type of attribute: scale is not right.");
  }
  IR_ENFORCE(attributes.count("interp_method")>0,
                 "interp_method does not exist.");
  IR_ENFORCE(attributes.at("interp_method").isa<pir::StrAttribute>(),
                 "Type of attribute: interp_method is not pir::StrAttribute.");

  IR_ENFORCE(attributes.count("align_corners")>0,
                 "align_corners does not exist.");
  IR_ENFORCE(attributes.at("align_corners").isa<pir::BoolAttribute>(),
                 "Type of attribute: align_corners is not pir::BoolAttribute.");

  IR_ENFORCE(attributes.count("align_mode")>0,
                 "align_mode does not exist.");
  IR_ENFORCE(attributes.at("align_mode").isa<pir::Int32Attribute>(),
                 "Type of attribute: align_mode is not pir::Int32Attribute.");

  }
  VLOG(4) << "Verifying outputs:";
  {
  auto output_size = num_results();
  IR_ENFORCE(output_size == 1u,
                    "The size %d of outputs must be equal to 1.", output_size);
  IR_ENFORCE((*this)->result(0).type().isa<paddle::dialect::DenseTensorType>(),
                 "Type validation failed for the 0th output.");
  }
  VLOG(4) << "End Verifying for: BilinearInterpOp.";
}

void BilinearInterpOp::InferMeta( phi::InferMetaContext *infer_meta ) {
  auto fn = PD_INFER_META(phi::InterpolateInferMeta);
  fn(infer_meta);
}

phi::DataType BilinearInterpOp::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: BilinearInterpOp";
  

  // deal skip data transform
  if (var_name == "out_size" || var_name == "size_tensor" || var_name == "scale_tensor"){
    return expected_kernel_dtype;
  }


  return expected_kernel_dtype;
}

OpInfoTuple BincountOp::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("x", "paddle::dialect::DenseTensorType", false, false, false, false), paddle::dialect::OpInputInfo("weights", "paddle::dialect::DenseTensorType", true, false, false, false), paddle::dialect::OpInputInfo("minlength", "paddle::dialect::ScalarAttribute", false, false, true, false) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = {  };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("out", "paddle::dialect::DenseTensorType", false, false) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("BincountInferMeta", {"x", "weights", "minlength"}, "bincount", {"x", "weights", "minlength"}, {}, {}, {}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "bincount");
}

void BincountOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, pir::Value weights_, int minlength) {
  VLOG(4) << "Start build BincountOp";


  // Generate scalar mutable attribute: minlength
  paddle::dialect::FullOp full_minlength_op = builder.Build<paddle::dialect::FullOp>(std::vector<int64_t>{1}, minlength, phi::DataType::INT32, phi::CPUPlace());
  pir::OpResult minlength_ = full_minlength_op->result(0);
    
  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_, weights_, minlength_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);

  paddle::dialect::IrMetaTensor meta_weights;
  paddle::dialect::IrTensor ir_tensor_weights;
  if (weights_.impl() != nullptr) {
    paddle::dialect::DenseTensorType weights = weights_.type().dyn_cast<paddle::dialect::DenseTensorType>();
    VLOG(4) << "Builder construction  dense_weights";
    ir_tensor_weights = paddle::dialect::IrTensor(paddle::dialect::TransToPhiDataType(weights.dtype()),
                                                        weights.dims(),
                                                        weights.data_layout(),
                                                        weights.lod(),
                                                        weights.offset());
    VLOG(4) << "Builder construction  meta_weights";
    meta_weights = paddle::dialect::IrMetaTensor(&ir_tensor_weights);
  }

  paddle::dialect::IrTensor dense_out;
  paddle::dialect::IrMetaTensor meta_out(&dense_out);

  phi::BincountInferMeta(meta_x, meta_weights, minlength, &meta_out);

  std::vector<pir::Type> argument_outputs;
  pir::Type out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_out.dtype()), dense_out.dims(), dense_out.layout(), dense_out.lod(), dense_out.offset());
  argument_outputs.push_back(out_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void BincountOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, pir::Value weights_, pir::AttributeMap attributes) {
  VLOG(4) << "Start build BincountOp";


  IR_ENFORCE(
      attributes.find("minlength") != attributes.end(),
          "'minlength' Attribute is expected for BincountOp. ");
  int minlength = attributes.at("minlength").dyn_cast<pir::Int32Attribute>().data();

  // Generate scalar mutable attribute: minlength
  paddle::dialect::FullOp full_minlength_op = builder.Build<paddle::dialect::FullOp>(std::vector<int64_t>{1}, minlength, phi::DataType::INT32, phi::CPUPlace());
  pir::OpResult minlength_ = full_minlength_op->result(0);
    
  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_, weights_, minlength_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);

  paddle::dialect::IrMetaTensor meta_weights;
  paddle::dialect::IrTensor ir_tensor_weights;
  if (weights_.impl() != nullptr) {
    paddle::dialect::DenseTensorType weights = weights_.type().dyn_cast<paddle::dialect::DenseTensorType>();
    VLOG(4) << "Builder construction  dense_weights";
    ir_tensor_weights = paddle::dialect::IrTensor(paddle::dialect::TransToPhiDataType(weights.dtype()),
                                                        weights.dims(),
                                                        weights.data_layout(),
                                                        weights.lod(),
                                                        weights.offset());
    VLOG(4) << "Builder construction  meta_weights";
    meta_weights = paddle::dialect::IrMetaTensor(&ir_tensor_weights);
  }

  paddle::dialect::IrTensor dense_out;
  paddle::dialect::IrMetaTensor meta_out(&dense_out);

  phi::BincountInferMeta(meta_x, meta_weights, minlength, &meta_out);

  std::vector<pir::Type> argument_outputs;
  pir::Type out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_out.dtype()), dense_out.dims(), dense_out.layout(), dense_out.lod(), dense_out.offset());
  argument_outputs.push_back(out_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void BincountOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, pir::Value weights_, pir::Value minlength_) {
  VLOG(4) << "Start build BincountOp";



  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_, weights_, minlength_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;
  phi::Scalar minlength;
  if (minlength_.dyn_cast<pir::OpResult>() && minlength_.dyn_cast<pir::OpResult>().owner()->isa<paddle::dialect::FullOp>()) {
    minlength = std::move(phi::Scalar(minlength_.dyn_cast<pir::OpResult>().owner()
                                  ->dyn_cast<paddle::dialect::FullOp>()
                                  .attribute("value")
                                  .dyn_cast<paddle::dialect::ScalarAttribute>()
                                  .data()
                                  .to<int>()));
  }
  else {
    minlength = std::move(phi::Scalar(-1));
    minlength.SetFromTensor(true);
  }


  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);

  paddle::dialect::IrMetaTensor meta_weights;
  paddle::dialect::IrTensor ir_tensor_weights;
  if (weights_.impl() != nullptr) {
    paddle::dialect::DenseTensorType weights = weights_.type().dyn_cast<paddle::dialect::DenseTensorType>();
    VLOG(4) << "Builder construction  dense_weights";
    ir_tensor_weights = paddle::dialect::IrTensor(paddle::dialect::TransToPhiDataType(weights.dtype()),
                                                        weights.dims(),
                                                        weights.data_layout(),
                                                        weights.lod(),
                                                        weights.offset());
    VLOG(4) << "Builder construction  meta_weights";
    meta_weights = paddle::dialect::IrMetaTensor(&ir_tensor_weights);
  }

  paddle::dialect::IrTensor dense_out;
  paddle::dialect::IrMetaTensor meta_out(&dense_out);

  phi::BincountInferMeta(meta_x, meta_weights, minlength, &meta_out);

  std::vector<pir::Type> argument_outputs;
  pir::Type out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_out.dtype()), dense_out.dims(), dense_out.layout(), dense_out.lod(), dense_out.offset());
  argument_outputs.push_back(out_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void BincountOp::VerifySig() {
  VLOG(4) << "Start Verifying inputs, outputs and attributes for: BincountOp.";
  VLOG(4) << "Verifying inputs:";
  {
  auto input_size = num_operands();
  IR_ENFORCE(input_size == 3u,
                    "The size %d of inputs must be equal to 3.", input_size);
  IR_ENFORCE((*this)->operand_source(0).type().isa<paddle::dialect::DenseTensorType>(),
                  "Type validation failed for the 0th input, got %s.", (*this)->operand_source(0).type());
  if (auto val = (*this)->operand(1)) {
    IR_ENFORCE(val.type().isa<paddle::dialect::DenseTensorType>(),
                   "Type validation failed for the 1th input, got %s.", (*this)->operand_source(1).type());
  }
  IR_ENFORCE((*this)->operand_source(2).type().isa<paddle::dialect::DenseTensorType>(),
                  "Type validation failed for the 2th input, got %s.", (*this)->operand_source(2).type());
  }
  VLOG(4) << "Verifying attributes:";
  {
  // Attributes num is 0, not need to check attributes type.
  }
  VLOG(4) << "Verifying outputs:";
  {
  auto output_size = num_results();
  IR_ENFORCE(output_size == 1u,
                    "The size %d of outputs must be equal to 1.", output_size);
  IR_ENFORCE((*this)->result(0).type().isa<paddle::dialect::DenseTensorType>(),
                 "Type validation failed for the 0th output.");
  }
  VLOG(4) << "End Verifying for: BincountOp.";
}

void BincountOp::InferMeta( phi::InferMetaContext *infer_meta ) {
  auto fn = PD_INFER_META(phi::BincountInferMeta);
  fn(infer_meta);
}

phi::DataType BincountOp::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: BincountOp";
  


  return expected_kernel_dtype;
}

OpInfoTuple BinomialOp::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("count", "paddle::dialect::DenseTensorType", false, false, false, false), paddle::dialect::OpInputInfo("prob", "paddle::dialect::DenseTensorType", false, false, false, false) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = {  };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("out", "paddle::dialect::DenseTensorType", false, false) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("BinomialInferMeta", {"count", "prob"}, "binomial", {"count", "prob"}, {}, {}, {}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "binomial");
}

void BinomialOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value count_, pir::Value prob_) {
  VLOG(4) << "Start build BinomialOp";



  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {count_, prob_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType count = count_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)count;
  paddle::dialect::DenseTensorType prob = prob_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)prob;

  VLOG(4) << "Builder construction  dense_count";
  paddle::dialect::IrTensor ir_tensor_count(paddle::dialect::TransToPhiDataType(count.dtype()),
                                                      count.dims(),
                                                      count.data_layout(),
                                                      count.lod(),
                                                      count.offset());
  VLOG(4) << "Builder construction  meta_count";
  paddle::dialect::IrMetaTensor meta_count(&ir_tensor_count);

  VLOG(4) << "Builder construction  dense_prob";
  paddle::dialect::IrTensor ir_tensor_prob(paddle::dialect::TransToPhiDataType(prob.dtype()),
                                                      prob.dims(),
                                                      prob.data_layout(),
                                                      prob.lod(),
                                                      prob.offset());
  VLOG(4) << "Builder construction  meta_prob";
  paddle::dialect::IrMetaTensor meta_prob(&ir_tensor_prob);
  paddle::dialect::IrTensor dense_out;
  paddle::dialect::IrMetaTensor meta_out(&dense_out);

  phi::BinomialInferMeta(meta_count, meta_prob, &meta_out);

  std::vector<pir::Type> argument_outputs;
  pir::Type out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_out.dtype()), dense_out.dims(), dense_out.layout(), dense_out.lod(), dense_out.offset());
  argument_outputs.push_back(out_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void BinomialOp::VerifySig() {
  VLOG(4) << "Start Verifying inputs, outputs and attributes for: BinomialOp.";
  VLOG(4) << "Verifying inputs:";
  {
  auto input_size = num_operands();
  IR_ENFORCE(input_size == 2u,
                    "The size %d of inputs must be equal to 2.", input_size);
  IR_ENFORCE((*this)->operand_source(0).type().isa<paddle::dialect::DenseTensorType>(),
                  "Type validation failed for the 0th input, got %s.", (*this)->operand_source(0).type());
  IR_ENFORCE((*this)->operand_source(1).type().isa<paddle::dialect::DenseTensorType>(),
                  "Type validation failed for the 1th input, got %s.", (*this)->operand_source(1).type());
  }
  VLOG(4) << "Verifying attributes:";
  {
  // Attributes num is 0, not need to check attributes type.
  }
  VLOG(4) << "Verifying outputs:";
  {
  auto output_size = num_results();
  IR_ENFORCE(output_size == 1u,
                    "The size %d of outputs must be equal to 1.", output_size);
  IR_ENFORCE((*this)->result(0).type().isa<paddle::dialect::DenseTensorType>(),
                 "Type validation failed for the 0th output.");
  }
  VLOG(4) << "End Verifying for: BinomialOp.";
}

void BinomialOp::InferMeta( phi::InferMetaContext *infer_meta ) {
  auto fn = PD_INFER_META(phi::BinomialInferMeta);
  fn(infer_meta);
}

phi::DataType BinomialOp::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: BinomialOp";
  


  return expected_kernel_dtype;
}

OpInfoTuple BitwiseAndOp::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("x", "paddle::dialect::DenseTensorType", false, false, false, false), paddle::dialect::OpInputInfo("y", "paddle::dialect::DenseTensorType", false, false, false, false) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = {  };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("out", "paddle::dialect::DenseTensorType", false, false) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("ElementwiseInferMeta", {"x", "y"}, "bitwise_and", {"x", "y"}, {}, {"x"}, {}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "bitwise_and");
}

void BitwiseAndOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, pir::Value y_) {
  VLOG(4) << "Start build BitwiseAndOp";



  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_, y_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;
  paddle::dialect::DenseTensorType y = y_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)y;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);

  VLOG(4) << "Builder construction  dense_y";
  paddle::dialect::IrTensor ir_tensor_y(paddle::dialect::TransToPhiDataType(y.dtype()),
                                                      y.dims(),
                                                      y.data_layout(),
                                                      y.lod(),
                                                      y.offset());
  VLOG(4) << "Builder construction  meta_y";
  paddle::dialect::IrMetaTensor meta_y(&ir_tensor_y);
  paddle::dialect::IrTensor dense_out;
  paddle::dialect::IrMetaTensor meta_out(&dense_out);

  phi::ElementwiseInferMeta(meta_x, meta_y, &meta_out);

  std::vector<pir::Type> argument_outputs;
  pir::Type out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_out.dtype()), dense_out.dims(), dense_out.layout(), dense_out.lod(), dense_out.offset());
  argument_outputs.push_back(out_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void BitwiseAndOp::VerifySig() {
  VLOG(4) << "Start Verifying inputs, outputs and attributes for: BitwiseAndOp.";
  VLOG(4) << "Verifying inputs:";
  {
  auto input_size = num_operands();
  IR_ENFORCE(input_size == 2u,
                    "The size %d of inputs must be equal to 2.", input_size);
  IR_ENFORCE((*this)->operand_source(0).type().isa<paddle::dialect::DenseTensorType>(),
                  "Type validation failed for the 0th input, got %s.", (*this)->operand_source(0).type());
  IR_ENFORCE((*this)->operand_source(1).type().isa<paddle::dialect::DenseTensorType>(),
                  "Type validation failed for the 1th input, got %s.", (*this)->operand_source(1).type());
  }
  VLOG(4) << "Verifying attributes:";
  {
  // Attributes num is 0, not need to check attributes type.
  }
  VLOG(4) << "Verifying outputs:";
  {
  auto output_size = num_results();
  IR_ENFORCE(output_size == 1u,
                    "The size %d of outputs must be equal to 1.", output_size);
  IR_ENFORCE((*this)->result(0).type().isa<paddle::dialect::DenseTensorType>(),
                 "Type validation failed for the 0th output.");
  }
  VLOG(4) << "End Verifying for: BitwiseAndOp.";
}

void BitwiseAndOp::InferMeta( phi::InferMetaContext *infer_meta ) {
  auto fn = PD_INFER_META(phi::ElementwiseInferMeta);
  fn(infer_meta);
}

phi::DataType BitwiseAndOp::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: BitwiseAndOp";
  


  return expected_kernel_dtype;
}

OpInfoTuple BitwiseAnd_Op::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("x", "paddle::dialect::DenseTensorType", false, false, false, false), paddle::dialect::OpInputInfo("y", "paddle::dialect::DenseTensorType", false, false, false, false) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = {  };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("out", "paddle::dialect::DenseTensorType", false, false) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("ElementwiseInferMeta", {"x", "y"}, "bitwise_and", {"x", "y"}, {}, {"x"}, {{"out", "x"}}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "bitwise_and");
}

void BitwiseAnd_Op::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, pir::Value y_) {
  VLOG(4) << "Start build BitwiseAnd_Op";



  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_, y_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;
  paddle::dialect::DenseTensorType y = y_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)y;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);

  VLOG(4) << "Builder construction  dense_y";
  paddle::dialect::IrTensor ir_tensor_y(paddle::dialect::TransToPhiDataType(y.dtype()),
                                                      y.dims(),
                                                      y.data_layout(),
                                                      y.lod(),
                                                      y.offset());
  VLOG(4) << "Builder construction  meta_y";
  paddle::dialect::IrMetaTensor meta_y(&ir_tensor_y);
  paddle::dialect::IrTensor dense_out;
  paddle::dialect::IrMetaTensor meta_out(&dense_out);

  phi::ElementwiseInferMeta(meta_x, meta_y, &meta_out);

  std::vector<pir::Type> argument_outputs;
  pir::Type out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_out.dtype()), dense_out.dims(), dense_out.layout(), dense_out.lod(), dense_out.offset());
  argument_outputs.push_back(out_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void BitwiseAnd_Op::VerifySig() {
  VLOG(4) << "Start Verifying inputs, outputs and attributes for: BitwiseAnd_Op.";
  VLOG(4) << "Verifying inputs:";
  {
  auto input_size = num_operands();
  IR_ENFORCE(input_size == 2u,
                    "The size %d of inputs must be equal to 2.", input_size);
  IR_ENFORCE((*this)->operand_source(0).type().isa<paddle::dialect::DenseTensorType>(),
                  "Type validation failed for the 0th input, got %s.", (*this)->operand_source(0).type());
  IR_ENFORCE((*this)->operand_source(1).type().isa<paddle::dialect::DenseTensorType>(),
                  "Type validation failed for the 1th input, got %s.", (*this)->operand_source(1).type());
  }
  VLOG(4) << "Verifying attributes:";
  {
  // Attributes num is 0, not need to check attributes type.
  }
  VLOG(4) << "Verifying outputs:";
  {
  auto output_size = num_results();
  IR_ENFORCE(output_size == 1u,
                    "The size %d of outputs must be equal to 1.", output_size);
  IR_ENFORCE((*this)->result(0).type().isa<paddle::dialect::DenseTensorType>(),
                 "Type validation failed for the 0th output.");
  }
  VLOG(4) << "End Verifying for: BitwiseAnd_Op.";
}

void BitwiseAnd_Op::InferMeta( phi::InferMetaContext *infer_meta ) {
  auto fn = PD_INFER_META(phi::ElementwiseInferMeta);
  fn(infer_meta);
}

phi::DataType BitwiseAnd_Op::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: BitwiseAnd_Op";
  


  return expected_kernel_dtype;
}

OpInfoTuple BitwiseNotOp::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("x", "paddle::dialect::DenseTensorType", false, false, false, false) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = {  };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("out", "paddle::dialect::DenseTensorType", false, false) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("UnchangedInferMeta", {"x"}, "bitwise_not", {"x"}, {}, {"x"}, {}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "bitwise_not");
}

void BitwiseNotOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_) {
  VLOG(4) << "Start build BitwiseNotOp";



  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);
  paddle::dialect::IrTensor dense_out;
  paddle::dialect::IrMetaTensor meta_out(&dense_out);

  phi::UnchangedInferMeta(meta_x, &meta_out);

  std::vector<pir::Type> argument_outputs;
  pir::Type out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_out.dtype()), dense_out.dims(), dense_out.layout(), dense_out.lod(), dense_out.offset());
  argument_outputs.push_back(out_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void BitwiseNotOp::VerifySig() {
  VLOG(4) << "Start Verifying inputs, outputs and attributes for: BitwiseNotOp.";
  VLOG(4) << "Verifying inputs:";
  {
  auto input_size = num_operands();
  IR_ENFORCE(input_size == 1u,
                    "The size %d of inputs must be equal to 1.", input_size);
  IR_ENFORCE((*this)->operand_source(0).type().isa<paddle::dialect::DenseTensorType>(),
                  "Type validation failed for the 0th input, got %s.", (*this)->operand_source(0).type());
  }
  VLOG(4) << "Verifying attributes:";
  {
  // Attributes num is 0, not need to check attributes type.
  }
  VLOG(4) << "Verifying outputs:";
  {
  auto output_size = num_results();
  IR_ENFORCE(output_size == 1u,
                    "The size %d of outputs must be equal to 1.", output_size);
  IR_ENFORCE((*this)->result(0).type().isa<paddle::dialect::DenseTensorType>(),
                 "Type validation failed for the 0th output.");
  }
  VLOG(4) << "End Verifying for: BitwiseNotOp.";
}

void BitwiseNotOp::InferMeta( phi::InferMetaContext *infer_meta ) {
  auto fn = PD_INFER_META(phi::UnchangedInferMeta);
  fn(infer_meta);
}

phi::DataType BitwiseNotOp::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: BitwiseNotOp";
  


  return expected_kernel_dtype;
}

OpInfoTuple BitwiseNot_Op::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("x", "paddle::dialect::DenseTensorType", false, false, false, false) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = {  };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("out", "paddle::dialect::DenseTensorType", false, false) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("UnchangedInferMeta", {"x"}, "bitwise_not", {"x"}, {}, {"x"}, {{"out", "x"}}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "bitwise_not");
}

void BitwiseNot_Op::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_) {
  VLOG(4) << "Start build BitwiseNot_Op";



  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);
  paddle::dialect::IrTensor dense_out;
  paddle::dialect::IrMetaTensor meta_out(&dense_out);

  phi::UnchangedInferMeta(meta_x, &meta_out);

  std::vector<pir::Type> argument_outputs;
  pir::Type out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_out.dtype()), dense_out.dims(), dense_out.layout(), dense_out.lod(), dense_out.offset());
  argument_outputs.push_back(out_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void BitwiseNot_Op::VerifySig() {
  VLOG(4) << "Start Verifying inputs, outputs and attributes for: BitwiseNot_Op.";
  VLOG(4) << "Verifying inputs:";
  {
  auto input_size = num_operands();
  IR_ENFORCE(input_size == 1u,
                    "The size %d of inputs must be equal to 1.", input_size);
  IR_ENFORCE((*this)->operand_source(0).type().isa<paddle::dialect::DenseTensorType>(),
                  "Type validation failed for the 0th input, got %s.", (*this)->operand_source(0).type());
  }
  VLOG(4) << "Verifying attributes:";
  {
  // Attributes num is 0, not need to check attributes type.
  }
  VLOG(4) << "Verifying outputs:";
  {
  auto output_size = num_results();
  IR_ENFORCE(output_size == 1u,
                    "The size %d of outputs must be equal to 1.", output_size);
  IR_ENFORCE((*this)->result(0).type().isa<paddle::dialect::DenseTensorType>(),
                 "Type validation failed for the 0th output.");
  }
  VLOG(4) << "End Verifying for: BitwiseNot_Op.";
}

void BitwiseNot_Op::InferMeta( phi::InferMetaContext *infer_meta ) {
  auto fn = PD_INFER_META(phi::UnchangedInferMeta);
  fn(infer_meta);
}

phi::DataType BitwiseNot_Op::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: BitwiseNot_Op";
  


  return expected_kernel_dtype;
}

OpInfoTuple BitwiseOrOp::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("x", "paddle::dialect::DenseTensorType", false, false, false, false), paddle::dialect::OpInputInfo("y", "paddle::dialect::DenseTensorType", false, false, false, false) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = {  };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("out", "paddle::dialect::DenseTensorType", false, false) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("ElementwiseInferMeta", {"x", "y"}, "bitwise_or", {"x", "y"}, {}, {"x"}, {}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "bitwise_or");
}

void BitwiseOrOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, pir::Value y_) {
  VLOG(4) << "Start build BitwiseOrOp";



  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_, y_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;
  paddle::dialect::DenseTensorType y = y_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)y;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);

  VLOG(4) << "Builder construction  dense_y";
  paddle::dialect::IrTensor ir_tensor_y(paddle::dialect::TransToPhiDataType(y.dtype()),
                                                      y.dims(),
                                                      y.data_layout(),
                                                      y.lod(),
                                                      y.offset());
  VLOG(4) << "Builder construction  meta_y";
  paddle::dialect::IrMetaTensor meta_y(&ir_tensor_y);
  paddle::dialect::IrTensor dense_out;
  paddle::dialect::IrMetaTensor meta_out(&dense_out);

  phi::ElementwiseInferMeta(meta_x, meta_y, &meta_out);

  std::vector<pir::Type> argument_outputs;
  pir::Type out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_out.dtype()), dense_out.dims(), dense_out.layout(), dense_out.lod(), dense_out.offset());
  argument_outputs.push_back(out_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void BitwiseOrOp::VerifySig() {
  VLOG(4) << "Start Verifying inputs, outputs and attributes for: BitwiseOrOp.";
  VLOG(4) << "Verifying inputs:";
  {
  auto input_size = num_operands();
  IR_ENFORCE(input_size == 2u,
                    "The size %d of inputs must be equal to 2.", input_size);
  IR_ENFORCE((*this)->operand_source(0).type().isa<paddle::dialect::DenseTensorType>(),
                  "Type validation failed for the 0th input, got %s.", (*this)->operand_source(0).type());
  IR_ENFORCE((*this)->operand_source(1).type().isa<paddle::dialect::DenseTensorType>(),
                  "Type validation failed for the 1th input, got %s.", (*this)->operand_source(1).type());
  }
  VLOG(4) << "Verifying attributes:";
  {
  // Attributes num is 0, not need to check attributes type.
  }
  VLOG(4) << "Verifying outputs:";
  {
  auto output_size = num_results();
  IR_ENFORCE(output_size == 1u,
                    "The size %d of outputs must be equal to 1.", output_size);
  IR_ENFORCE((*this)->result(0).type().isa<paddle::dialect::DenseTensorType>(),
                 "Type validation failed for the 0th output.");
  }
  VLOG(4) << "End Verifying for: BitwiseOrOp.";
}

void BitwiseOrOp::InferMeta( phi::InferMetaContext *infer_meta ) {
  auto fn = PD_INFER_META(phi::ElementwiseInferMeta);
  fn(infer_meta);
}

phi::DataType BitwiseOrOp::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: BitwiseOrOp";
  


  return expected_kernel_dtype;
}

OpInfoTuple BitwiseOr_Op::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("x", "paddle::dialect::DenseTensorType", false, false, false, false), paddle::dialect::OpInputInfo("y", "paddle::dialect::DenseTensorType", false, false, false, false) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = {  };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("out", "paddle::dialect::DenseTensorType", false, false) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("ElementwiseInferMeta", {"x", "y"}, "bitwise_or", {"x", "y"}, {}, {"x"}, {{"out", "x"}}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "bitwise_or");
}

void BitwiseOr_Op::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, pir::Value y_) {
  VLOG(4) << "Start build BitwiseOr_Op";



  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_, y_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;
  paddle::dialect::DenseTensorType y = y_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)y;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);

  VLOG(4) << "Builder construction  dense_y";
  paddle::dialect::IrTensor ir_tensor_y(paddle::dialect::TransToPhiDataType(y.dtype()),
                                                      y.dims(),
                                                      y.data_layout(),
                                                      y.lod(),
                                                      y.offset());
  VLOG(4) << "Builder construction  meta_y";
  paddle::dialect::IrMetaTensor meta_y(&ir_tensor_y);
  paddle::dialect::IrTensor dense_out;
  paddle::dialect::IrMetaTensor meta_out(&dense_out);

  phi::ElementwiseInferMeta(meta_x, meta_y, &meta_out);

  std::vector<pir::Type> argument_outputs;
  pir::Type out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_out.dtype()), dense_out.dims(), dense_out.layout(), dense_out.lod(), dense_out.offset());
  argument_outputs.push_back(out_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void BitwiseOr_Op::VerifySig() {
  VLOG(4) << "Start Verifying inputs, outputs and attributes for: BitwiseOr_Op.";
  VLOG(4) << "Verifying inputs:";
  {
  auto input_size = num_operands();
  IR_ENFORCE(input_size == 2u,
                    "The size %d of inputs must be equal to 2.", input_size);
  IR_ENFORCE((*this)->operand_source(0).type().isa<paddle::dialect::DenseTensorType>(),
                  "Type validation failed for the 0th input, got %s.", (*this)->operand_source(0).type());
  IR_ENFORCE((*this)->operand_source(1).type().isa<paddle::dialect::DenseTensorType>(),
                  "Type validation failed for the 1th input, got %s.", (*this)->operand_source(1).type());
  }
  VLOG(4) << "Verifying attributes:";
  {
  // Attributes num is 0, not need to check attributes type.
  }
  VLOG(4) << "Verifying outputs:";
  {
  auto output_size = num_results();
  IR_ENFORCE(output_size == 1u,
                    "The size %d of outputs must be equal to 1.", output_size);
  IR_ENFORCE((*this)->result(0).type().isa<paddle::dialect::DenseTensorType>(),
                 "Type validation failed for the 0th output.");
  }
  VLOG(4) << "End Verifying for: BitwiseOr_Op.";
}

void BitwiseOr_Op::InferMeta( phi::InferMetaContext *infer_meta ) {
  auto fn = PD_INFER_META(phi::ElementwiseInferMeta);
  fn(infer_meta);
}

phi::DataType BitwiseOr_Op::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: BitwiseOr_Op";
  


  return expected_kernel_dtype;
}

OpInfoTuple BitwiseXorOp::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("x", "paddle::dialect::DenseTensorType", false, false, false, false), paddle::dialect::OpInputInfo("y", "paddle::dialect::DenseTensorType", false, false, false, false) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = {  };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("out", "paddle::dialect::DenseTensorType", false, false) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("ElementwiseInferMeta", {"x", "y"}, "bitwise_xor", {"x", "y"}, {}, {"x"}, {}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "bitwise_xor");
}

void BitwiseXorOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, pir::Value y_) {
  VLOG(4) << "Start build BitwiseXorOp";



  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_, y_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;
  paddle::dialect::DenseTensorType y = y_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)y;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);

  VLOG(4) << "Builder construction  dense_y";
  paddle::dialect::IrTensor ir_tensor_y(paddle::dialect::TransToPhiDataType(y.dtype()),
                                                      y.dims(),
                                                      y.data_layout(),
                                                      y.lod(),
                                                      y.offset());
  VLOG(4) << "Builder construction  meta_y";
  paddle::dialect::IrMetaTensor meta_y(&ir_tensor_y);
  paddle::dialect::IrTensor dense_out;
  paddle::dialect::IrMetaTensor meta_out(&dense_out);

  phi::ElementwiseInferMeta(meta_x, meta_y, &meta_out);

  std::vector<pir::Type> argument_outputs;
  pir::Type out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_out.dtype()), dense_out.dims(), dense_out.layout(), dense_out.lod(), dense_out.offset());
  argument_outputs.push_back(out_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void BitwiseXorOp::VerifySig() {
  VLOG(4) << "Start Verifying inputs, outputs and attributes for: BitwiseXorOp.";
  VLOG(4) << "Verifying inputs:";
  {
  auto input_size = num_operands();
  IR_ENFORCE(input_size == 2u,
                    "The size %d of inputs must be equal to 2.", input_size);
  IR_ENFORCE((*this)->operand_source(0).type().isa<paddle::dialect::DenseTensorType>(),
                  "Type validation failed for the 0th input, got %s.", (*this)->operand_source(0).type());
  IR_ENFORCE((*this)->operand_source(1).type().isa<paddle::dialect::DenseTensorType>(),
                  "Type validation failed for the 1th input, got %s.", (*this)->operand_source(1).type());
  }
  VLOG(4) << "Verifying attributes:";
  {
  // Attributes num is 0, not need to check attributes type.
  }
  VLOG(4) << "Verifying outputs:";
  {
  auto output_size = num_results();
  IR_ENFORCE(output_size == 1u,
                    "The size %d of outputs must be equal to 1.", output_size);
  IR_ENFORCE((*this)->result(0).type().isa<paddle::dialect::DenseTensorType>(),
                 "Type validation failed for the 0th output.");
  }
  VLOG(4) << "End Verifying for: BitwiseXorOp.";
}

void BitwiseXorOp::InferMeta( phi::InferMetaContext *infer_meta ) {
  auto fn = PD_INFER_META(phi::ElementwiseInferMeta);
  fn(infer_meta);
}

phi::DataType BitwiseXorOp::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: BitwiseXorOp";
  


  return expected_kernel_dtype;
}

OpInfoTuple BitwiseXor_Op::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("x", "paddle::dialect::DenseTensorType", false, false, false, false), paddle::dialect::OpInputInfo("y", "paddle::dialect::DenseTensorType", false, false, false, false) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = {  };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("out", "paddle::dialect::DenseTensorType", false, false) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("ElementwiseInferMeta", {"x", "y"}, "bitwise_xor", {"x", "y"}, {}, {"x"}, {{"out", "x"}}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "bitwise_xor");
}

void BitwiseXor_Op::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, pir::Value y_) {
  VLOG(4) << "Start build BitwiseXor_Op";



  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_, y_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;
  paddle::dialect::DenseTensorType y = y_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)y;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);

  VLOG(4) << "Builder construction  dense_y";
  paddle::dialect::IrTensor ir_tensor_y(paddle::dialect::TransToPhiDataType(y.dtype()),
                                                      y.dims(),
                                                      y.data_layout(),
                                                      y.lod(),
                                                      y.offset());
  VLOG(4) << "Builder construction  meta_y";
  paddle::dialect::IrMetaTensor meta_y(&ir_tensor_y);
  paddle::dialect::IrTensor dense_out;
  paddle::dialect::IrMetaTensor meta_out(&dense_out);

  phi::ElementwiseInferMeta(meta_x, meta_y, &meta_out);

  std::vector<pir::Type> argument_outputs;
  pir::Type out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_out.dtype()), dense_out.dims(), dense_out.layout(), dense_out.lod(), dense_out.offset());
  argument_outputs.push_back(out_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void BitwiseXor_Op::VerifySig() {
  VLOG(4) << "Start Verifying inputs, outputs and attributes for: BitwiseXor_Op.";
  VLOG(4) << "Verifying inputs:";
  {
  auto input_size = num_operands();
  IR_ENFORCE(input_size == 2u,
                    "The size %d of inputs must be equal to 2.", input_size);
  IR_ENFORCE((*this)->operand_source(0).type().isa<paddle::dialect::DenseTensorType>(),
                  "Type validation failed for the 0th input, got %s.", (*this)->operand_source(0).type());
  IR_ENFORCE((*this)->operand_source(1).type().isa<paddle::dialect::DenseTensorType>(),
                  "Type validation failed for the 1th input, got %s.", (*this)->operand_source(1).type());
  }
  VLOG(4) << "Verifying attributes:";
  {
  // Attributes num is 0, not need to check attributes type.
  }
  VLOG(4) << "Verifying outputs:";
  {
  auto output_size = num_results();
  IR_ENFORCE(output_size == 1u,
                    "The size %d of outputs must be equal to 1.", output_size);
  IR_ENFORCE((*this)->result(0).type().isa<paddle::dialect::DenseTensorType>(),
                 "Type validation failed for the 0th output.");
  }
  VLOG(4) << "End Verifying for: BitwiseXor_Op.";
}

void BitwiseXor_Op::InferMeta( phi::InferMetaContext *infer_meta ) {
  auto fn = PD_INFER_META(phi::ElementwiseInferMeta);
  fn(infer_meta);
}

phi::DataType BitwiseXor_Op::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: BitwiseXor_Op";
  


  return expected_kernel_dtype;
}

OpInfoTuple BmmOp::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("x", "paddle::dialect::DenseTensorType", false, false, false, true), paddle::dialect::OpInputInfo("y", "paddle::dialect::DenseTensorType", false, false, false, true) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = {  };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("out", "paddle::dialect::DenseTensorType", false, false) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("BmmInferMeta", {"x", "y"}, "bmm", {"x", "y"}, {}, {}, {}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "bmm");
}

void BmmOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, pir::Value y_) {
  VLOG(4) << "Start build BmmOp";



  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_, y_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;
  paddle::dialect::DenseTensorType y = y_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)y;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);

  VLOG(4) << "Builder construction  dense_y";
  paddle::dialect::IrTensor ir_tensor_y(paddle::dialect::TransToPhiDataType(y.dtype()),
                                                      y.dims(),
                                                      y.data_layout(),
                                                      y.lod(),
                                                      y.offset());
  VLOG(4) << "Builder construction  meta_y";
  paddle::dialect::IrMetaTensor meta_y(&ir_tensor_y);
  paddle::dialect::IrTensor dense_out;
  paddle::dialect::IrMetaTensor meta_out(&dense_out);

  phi::BmmInferMeta(meta_x, meta_y, &meta_out);

  std::vector<pir::Type> argument_outputs;
  pir::Type out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_out.dtype()), dense_out.dims(), dense_out.layout(), dense_out.lod(), dense_out.offset());
  argument_outputs.push_back(out_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void BmmOp::VerifySig() {
  VLOG(4) << "Start Verifying inputs, outputs and attributes for: BmmOp.";
  VLOG(4) << "Verifying inputs:";
  {
  auto input_size = num_operands();
  IR_ENFORCE(input_size == 2u,
                    "The size %d of inputs must be equal to 2.", input_size);
  IR_ENFORCE((*this)->operand_source(0).type().isa<paddle::dialect::DenseTensorType>(),
                  "Type validation failed for the 0th input, got %s.", (*this)->operand_source(0).type());
  IR_ENFORCE((*this)->operand_source(1).type().isa<paddle::dialect::DenseTensorType>(),
                  "Type validation failed for the 1th input, got %s.", (*this)->operand_source(1).type());
  }
  VLOG(4) << "Verifying attributes:";
  {
  // Attributes num is 0, not need to check attributes type.
  }
  VLOG(4) << "Verifying outputs:";
  {
  auto output_size = num_results();
  IR_ENFORCE(output_size == 1u,
                    "The size %d of outputs must be equal to 1.", output_size);
  IR_ENFORCE((*this)->result(0).type().isa<paddle::dialect::DenseTensorType>(),
                 "Type validation failed for the 0th output.");
  }
  VLOG(4) << "End Verifying for: BmmOp.";
}

void BmmOp::InferMeta( phi::InferMetaContext *infer_meta ) {
  auto fn = PD_INFER_META(phi::BmmInferMeta);
  fn(infer_meta);
}

phi::DataType BmmOp::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: BmmOp";
  


  return expected_kernel_dtype;
}

const char *BoxCoderOp::attributes_name[4] = { "code_type", "box_normalized", "axis", "variance" };

OpInfoTuple BoxCoderOp::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("prior_box", "paddle::dialect::DenseTensorType", false, false, false, false), paddle::dialect::OpInputInfo("prior_box_var", "paddle::dialect::DenseTensorType", true, false, false, false), paddle::dialect::OpInputInfo("target_box", "paddle::dialect::DenseTensorType", false, false, false, false) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = { paddle::dialect::OpAttributeInfo("code_type", "pir::StrAttribute", ""), paddle::dialect::OpAttributeInfo("box_normalized", "pir::BoolAttribute", ""), paddle::dialect::OpAttributeInfo("axis", "pir::Int32Attribute", ""), paddle::dialect::OpAttributeInfo("variance", "pir::ArrayAttribute<pir::FloatAttribute>", "") };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("output_box", "paddle::dialect::DenseTensorType", false, false) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("BoxCoderInferMeta", {"prior_box", "prior_box_var", "target_box", "code_type", "box_normalized", "axis", "variance"}, "box_coder", {"prior_box", "prior_box_var", "target_box", "code_type", "box_normalized", "axis", "variance"}, {}, {}, {}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "box_coder");
}

void BoxCoderOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value prior_box_, pir::Value prior_box_var_, pir::Value target_box_, const std::string& code_type, bool box_normalized, int axis, const std::vector<float>& variance) {
  VLOG(4) << "Start build BoxCoderOp";



  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {prior_box_, prior_box_var_, target_box_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_code_type = pir::StrAttribute::get(pir::IrContext::Instance(), code_type);
  argument.AddAttribute("code_type", attr_code_type);
  pir::Attribute attr_box_normalized = pir::BoolAttribute::get(pir::IrContext::Instance(), box_normalized);
  argument.AddAttribute("box_normalized", attr_box_normalized);
  pir::Attribute attr_axis = pir::Int32Attribute::get(pir::IrContext::Instance(), axis);
  argument.AddAttribute("axis", attr_axis);
  std::vector<pir::Attribute> vec_variance;
  for (size_t i = 0; i < static_cast<size_t>(variance.size()); i++) {
      pir::Attribute attr_variance = pir::FloatAttribute::get(pir::IrContext::Instance(), variance[i]);

    vec_variance.push_back(attr_variance);
  }
  pir::Attribute attr_variance = pir::ArrayAttribute::get(pir::IrContext::Instance(), vec_variance);
  argument.AddAttribute("variance", attr_variance);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType prior_box = prior_box_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)prior_box;
  paddle::dialect::DenseTensorType target_box = target_box_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)target_box;

  VLOG(4) << "Builder construction  dense_prior_box";
  paddle::dialect::IrTensor ir_tensor_prior_box(paddle::dialect::TransToPhiDataType(prior_box.dtype()),
                                                      prior_box.dims(),
                                                      prior_box.data_layout(),
                                                      prior_box.lod(),
                                                      prior_box.offset());
  VLOG(4) << "Builder construction  meta_prior_box";
  paddle::dialect::IrMetaTensor meta_prior_box(&ir_tensor_prior_box);

  paddle::dialect::IrMetaTensor meta_prior_box_var;
  paddle::dialect::IrTensor ir_tensor_prior_box_var;
  if (prior_box_var_.impl() != nullptr) {
    paddle::dialect::DenseTensorType prior_box_var = prior_box_var_.type().dyn_cast<paddle::dialect::DenseTensorType>();
    VLOG(4) << "Builder construction  dense_prior_box_var";
    ir_tensor_prior_box_var = paddle::dialect::IrTensor(paddle::dialect::TransToPhiDataType(prior_box_var.dtype()),
                                                        prior_box_var.dims(),
                                                        prior_box_var.data_layout(),
                                                        prior_box_var.lod(),
                                                        prior_box_var.offset());
    VLOG(4) << "Builder construction  meta_prior_box_var";
    meta_prior_box_var = paddle::dialect::IrMetaTensor(&ir_tensor_prior_box_var);
  }


  VLOG(4) << "Builder construction  dense_target_box";
  paddle::dialect::IrTensor ir_tensor_target_box(paddle::dialect::TransToPhiDataType(target_box.dtype()),
                                                      target_box.dims(),
                                                      target_box.data_layout(),
                                                      target_box.lod(),
                                                      target_box.offset());
  VLOG(4) << "Builder construction  meta_target_box";
  paddle::dialect::IrMetaTensor meta_target_box(&ir_tensor_target_box);
  paddle::dialect::IrTensor dense_output_box;
  paddle::dialect::IrMetaTensor meta_output_box(&dense_output_box);

  phi::BoxCoderInferMeta(meta_prior_box, meta_prior_box_var, meta_target_box, code_type, box_normalized, axis, variance, &meta_output_box);

  std::vector<pir::Type> argument_outputs;
  pir::Type output_box_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_output_box.dtype()), dense_output_box.dims(), dense_output_box.layout(), dense_output_box.lod(), dense_output_box.offset());
  argument_outputs.push_back(output_box_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void BoxCoderOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value prior_box_, pir::Value prior_box_var_, pir::Value target_box_, pir::AttributeMap attributes) {
  VLOG(4) << "Start build BoxCoderOp";


  IR_ENFORCE(
      attributes.find("code_type") != attributes.end(),
          "'code_type' Attribute is expected for BoxCoderOp. ");
  std::string code_type = attributes.at("code_type").dyn_cast<pir::StrAttribute>().AsString();

  IR_ENFORCE(
      attributes.find("box_normalized") != attributes.end(),
          "'box_normalized' Attribute is expected for BoxCoderOp. ");
  bool box_normalized = attributes.at("box_normalized").dyn_cast<pir::BoolAttribute>().data();

  IR_ENFORCE(
      attributes.find("axis") != attributes.end(),
          "'axis' Attribute is expected for BoxCoderOp. ");
  int axis = attributes.at("axis").dyn_cast<pir::Int32Attribute>().data();

  IR_ENFORCE(
      attributes.find("variance") != attributes.end(),
          "'variance' Attribute is expected for BoxCoderOp. ");
  std::vector<float> variance;
  for (size_t i = 0; i < attributes.at("variance").dyn_cast<pir::ArrayAttribute>().size(); i++) {
    variance.push_back(attributes.at("variance").dyn_cast<pir::ArrayAttribute>().at(i).dyn_cast<pir::FloatAttribute>().data());
  }


  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {prior_box_, prior_box_var_, target_box_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_code_type = pir::StrAttribute::get(pir::IrContext::Instance(), code_type);
  argument.AddAttribute("code_type", attr_code_type);
  pir::Attribute attr_box_normalized = pir::BoolAttribute::get(pir::IrContext::Instance(), box_normalized);
  argument.AddAttribute("box_normalized", attr_box_normalized);
  pir::Attribute attr_axis = pir::Int32Attribute::get(pir::IrContext::Instance(), axis);
  argument.AddAttribute("axis", attr_axis);
  std::vector<pir::Attribute> vec_variance;
  for (size_t i = 0; i < static_cast<size_t>(variance.size()); i++) {
      pir::Attribute attr_variance = pir::FloatAttribute::get(pir::IrContext::Instance(), variance[i]);

    vec_variance.push_back(attr_variance);
  }
  pir::Attribute attr_variance = pir::ArrayAttribute::get(pir::IrContext::Instance(), vec_variance);
  argument.AddAttribute("variance", attr_variance);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType prior_box = prior_box_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)prior_box;
  paddle::dialect::DenseTensorType target_box = target_box_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)target_box;

  VLOG(4) << "Builder construction  dense_prior_box";
  paddle::dialect::IrTensor ir_tensor_prior_box(paddle::dialect::TransToPhiDataType(prior_box.dtype()),
                                                      prior_box.dims(),
                                                      prior_box.data_layout(),
                                                      prior_box.lod(),
                                                      prior_box.offset());
  VLOG(4) << "Builder construction  meta_prior_box";
  paddle::dialect::IrMetaTensor meta_prior_box(&ir_tensor_prior_box);

  paddle::dialect::IrMetaTensor meta_prior_box_var;
  paddle::dialect::IrTensor ir_tensor_prior_box_var;
  if (prior_box_var_.impl() != nullptr) {
    paddle::dialect::DenseTensorType prior_box_var = prior_box_var_.type().dyn_cast<paddle::dialect::DenseTensorType>();
    VLOG(4) << "Builder construction  dense_prior_box_var";
    ir_tensor_prior_box_var = paddle::dialect::IrTensor(paddle::dialect::TransToPhiDataType(prior_box_var.dtype()),
                                                        prior_box_var.dims(),
                                                        prior_box_var.data_layout(),
                                                        prior_box_var.lod(),
                                                        prior_box_var.offset());
    VLOG(4) << "Builder construction  meta_prior_box_var";
    meta_prior_box_var = paddle::dialect::IrMetaTensor(&ir_tensor_prior_box_var);
  }


  VLOG(4) << "Builder construction  dense_target_box";
  paddle::dialect::IrTensor ir_tensor_target_box(paddle::dialect::TransToPhiDataType(target_box.dtype()),
                                                      target_box.dims(),
                                                      target_box.data_layout(),
                                                      target_box.lod(),
                                                      target_box.offset());
  VLOG(4) << "Builder construction  meta_target_box";
  paddle::dialect::IrMetaTensor meta_target_box(&ir_tensor_target_box);
  paddle::dialect::IrTensor dense_output_box;
  paddle::dialect::IrMetaTensor meta_output_box(&dense_output_box);

  phi::BoxCoderInferMeta(meta_prior_box, meta_prior_box_var, meta_target_box, code_type, box_normalized, axis, variance, &meta_output_box);

  std::vector<pir::Type> argument_outputs;
  pir::Type output_box_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_output_box.dtype()), dense_output_box.dims(), dense_output_box.layout(), dense_output_box.lod(), dense_output_box.offset());
  argument_outputs.push_back(output_box_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void BoxCoderOp::VerifySig() {
  VLOG(4) << "Start Verifying inputs, outputs and attributes for: BoxCoderOp.";
  VLOG(4) << "Verifying inputs:";
  {
  auto input_size = num_operands();
  IR_ENFORCE(input_size == 3u,
                    "The size %d of inputs must be equal to 3.", input_size);
  IR_ENFORCE((*this)->operand_source(0).type().isa<paddle::dialect::DenseTensorType>(),
                  "Type validation failed for the 0th input, got %s.", (*this)->operand_source(0).type());
  if (auto val = (*this)->operand(1)) {
    IR_ENFORCE(val.type().isa<paddle::dialect::DenseTensorType>(),
                   "Type validation failed for the 1th input, got %s.", (*this)->operand_source(1).type());
  }
  IR_ENFORCE((*this)->operand_source(2).type().isa<paddle::dialect::DenseTensorType>(),
                  "Type validation failed for the 2th input, got %s.", (*this)->operand_source(2).type());
  }
  VLOG(4) << "Verifying attributes:";
  {
  auto& attributes = this->attributes();
  IR_ENFORCE(attributes.count("code_type")>0,
                 "code_type does not exist.");
  IR_ENFORCE(attributes.at("code_type").isa<pir::StrAttribute>(),
                 "Type of attribute: code_type is not pir::StrAttribute.");

  IR_ENFORCE(attributes.count("box_normalized")>0,
                 "box_normalized does not exist.");
  IR_ENFORCE(attributes.at("box_normalized").isa<pir::BoolAttribute>(),
                 "Type of attribute: box_normalized is not pir::BoolAttribute.");

  IR_ENFORCE(attributes.count("axis")>0,
                 "axis does not exist.");
  IR_ENFORCE(attributes.at("axis").isa<pir::Int32Attribute>(),
                 "Type of attribute: axis is not pir::Int32Attribute.");

  IR_ENFORCE(attributes.count("variance")>0,
                 "variance does not exist.");
  IR_ENFORCE(attributes.at("variance").isa<pir::ArrayAttribute>(),
                 "Type of attribute: variance is not pir::ArrayAttribute.");
  for (size_t i = 0; i < attributes.at("variance").dyn_cast<pir::ArrayAttribute>().size(); i++) {
    IR_ENFORCE(attributes.at("variance").dyn_cast<pir::ArrayAttribute>().at(i).isa<pir::FloatAttribute>(),
                   "Type of attribute: variance is not right.");
  }
  }
  VLOG(4) << "Verifying outputs:";
  {
  auto output_size = num_results();
  IR_ENFORCE(output_size == 1u,
                    "The size %d of outputs must be equal to 1.", output_size);
  IR_ENFORCE((*this)->result(0).type().isa<paddle::dialect::DenseTensorType>(),
                 "Type validation failed for the 0th output.");
  }
  VLOG(4) << "End Verifying for: BoxCoderOp.";
}

void BoxCoderOp::InferMeta( phi::InferMetaContext *infer_meta ) {
  auto fn = PD_INFER_META(phi::BoxCoderInferMeta);
  fn(infer_meta);
}

phi::DataType BoxCoderOp::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: BoxCoderOp";
  


  return expected_kernel_dtype;
}

OpInfoTuple BroadcastTensorsOp::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("input", "pir::VectorType<paddle::dialect::DenseTensorType>", false, false, false, true) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = {  };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("out", "pir::VectorType<paddle::dialect::DenseTensorType>", false, false) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("BroadcastTensorsInferMeta", {"input"}, "broadcast_tensors", {"input"}, {"input"}, {}, {}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "broadcast_tensors");
}

void BroadcastTensorsOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value input_) {
  VLOG(4) << "Start build BroadcastTensorsOp";



  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {input_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";

  VLOG(4) << "Builder construction outputs";
  pir::VectorType input = input_.type().dyn_cast<pir::VectorType>(); (void)input;
  std::vector<paddle::dialect::IrTensor> vec_ir_tensor_input;
  for (size_t i=0; i < static_cast<size_t>(input.size()); i++) {
    vec_ir_tensor_input.push_back(paddle::dialect::IrTensor(paddle::dialect::TransToPhiDataType(input[i].dyn_cast<paddle::dialect::DenseTensorType>().dtype()),
                                                                     input[i].dyn_cast<paddle::dialect::DenseTensorType>().dims(),
                                                                     input[i].dyn_cast<paddle::dialect::DenseTensorType>().data_layout(),
                                                                     input[i].dyn_cast<paddle::dialect::DenseTensorType>().lod(),
                                                                     input[i].dyn_cast<paddle::dialect::DenseTensorType>().offset()));
  }
  std::vector<paddle::dialect::IrMetaTensor> vec_meta_input;
  for (size_t i=0; i < vec_ir_tensor_input.size(); i++) {
    vec_meta_input.push_back(paddle::dialect::IrMetaTensor(&vec_ir_tensor_input[i]));
  }

  std::vector<const phi::MetaTensor*> meta_input;
  for (size_t i=0; i < static_cast<size_t>(vec_meta_input.size()); i++) {
    meta_input.push_back(&vec_meta_input[i]);
  }
   std::vector<paddle::dialect::IrTensor> vec_dense_out((input.size()), paddle::dialect::IrTensor());
  std::vector<paddle::dialect::IrMetaTensor> vec_meta_out;
  for (size_t i=0; i < static_cast<size_t>(input.size()); i++) {
    vec_meta_out.push_back(paddle::dialect::IrMetaTensor(&vec_dense_out[i]));
  }
  std::vector<phi::MetaTensor*> meta_out;
  for (size_t i=0; i < static_cast<size_t>(vec_meta_out.size()); i++) {
    meta_out.push_back(&vec_meta_out[i]);
  }

  phi::BroadcastTensorsInferMeta(meta_input, meta_out);

  std::vector<pir::Type> argument_outputs;
  std::vector<pir::Type> out_types;
  for (size_t i=0; i < static_cast<size_t>(input.size()); i++) {
    out_types.push_back(paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(vec_dense_out[i].dtype()), vec_dense_out[i].dims(), vec_dense_out[i].layout(), vec_dense_out[i].lod(), vec_dense_out[i].offset()));
  }
  pir::Type out_vector_type = pir::VectorType::get(pir::IrContext::Instance(), out_types);
  argument_outputs.push_back(out_vector_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void BroadcastTensorsOp::VerifySig() {
  VLOG(4) << "Start Verifying inputs, outputs and attributes for: BroadcastTensorsOp.";
  VLOG(4) << "Verifying inputs:";
  {
  auto input_size = num_operands();
  IR_ENFORCE(input_size == 1u,
                    "The size %d of inputs must be equal to 1.", input_size);
  if (auto vec_type = (*this)->operand_source(0).type().dyn_cast<pir::VectorType>()) {
      for (size_t i = 0; i < vec_type.size(); ++i) {
        IR_ENFORCE(vec_type[i].isa<paddle::dialect::DenseTensorType>(),
                       "Type validation failed for the 0th input, got %s.", (*this)->operand_source(0).type());
      }
  }
  else {
    IR_ENFORCE((*this)->operand_source(0).type().isa<paddle::dialect::DenseTensorType>(),
                   "Type validation failed for the 0th input, got %s.", (*this)->operand_source(0).type());
  }
  }
  VLOG(4) << "Verifying attributes:";
  {
  // Attributes num is 0, not need to check attributes type.
  }
  VLOG(4) << "Verifying outputs:";
  {
  auto output_size = num_results();
  IR_ENFORCE(output_size == 1u,
                    "The size %d of outputs must be equal to 1.", output_size);
  auto output_0_type = (*this)->result(0).type();
  if (auto vec_type = output_0_type.dyn_cast<pir::VectorType>()) {
    for (size_t i = 0; i < vec_type.size(); i++) {
      IR_ENFORCE(vec_type[i].isa<paddle::dialect::DenseTensorType>(),
                     "Type validation failed for the 0th output.");
    }
  }
  else {
    IR_ENFORCE(output_0_type.isa<paddle::dialect::DenseTensorType>(),
                   "Type validation failed for the 0th output.");
  }
  }
  VLOG(4) << "End Verifying for: BroadcastTensorsOp.";
}

void BroadcastTensorsOp::InferMeta( phi::InferMetaContext *infer_meta ) {
  auto fn = PD_INFER_META(phi::BroadcastTensorsInferMeta);
  fn(infer_meta);
}

phi::DataType BroadcastTensorsOp::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: BroadcastTensorsOp";
  


  return expected_kernel_dtype;
}

OpInfoTuple CeilOp::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("x", "paddle::dialect::DenseTensorType", false, false, false, true) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = {  };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("out", "paddle::dialect::DenseTensorType", false, false) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("UnchangedInferMeta", {"x"}, "ceil", {"x"}, {}, {}, {}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "ceil");
}

void CeilOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_) {
  VLOG(4) << "Start build CeilOp";



  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);
  paddle::dialect::IrTensor dense_out;
  paddle::dialect::IrMetaTensor meta_out(&dense_out);

  phi::UnchangedInferMeta(meta_x, &meta_out);

  std::vector<pir::Type> argument_outputs;
  pir::Type out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_out.dtype()), dense_out.dims(), dense_out.layout(), dense_out.lod(), dense_out.offset());
  argument_outputs.push_back(out_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void CeilOp::VerifySig() {
  VLOG(4) << "Start Verifying inputs, outputs and attributes for: CeilOp.";
  VLOG(4) << "Verifying inputs:";
  {
  auto input_size = num_operands();
  IR_ENFORCE(input_size == 1u,
                    "The size %d of inputs must be equal to 1.", input_size);
  IR_ENFORCE((*this)->operand_source(0).type().isa<paddle::dialect::DenseTensorType>(),
                  "Type validation failed for the 0th input, got %s.", (*this)->operand_source(0).type());
  }
  VLOG(4) << "Verifying attributes:";
  {
  // Attributes num is 0, not need to check attributes type.
  }
  VLOG(4) << "Verifying outputs:";
  {
  auto output_size = num_results();
  IR_ENFORCE(output_size == 1u,
                    "The size %d of outputs must be equal to 1.", output_size);
  IR_ENFORCE((*this)->result(0).type().isa<paddle::dialect::DenseTensorType>(),
                 "Type validation failed for the 0th output.");
  }
  VLOG(4) << "End Verifying for: CeilOp.";
}

void CeilOp::InferMeta( phi::InferMetaContext *infer_meta ) {
  auto fn = PD_INFER_META(phi::UnchangedInferMeta);
  fn(infer_meta);
}

phi::DataType CeilOp::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: CeilOp";
  


  return expected_kernel_dtype;
}

OpInfoTuple Ceil_Op::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("x", "paddle::dialect::DenseTensorType", false, false, false, true) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = {  };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("out", "paddle::dialect::DenseTensorType", false, false) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("UnchangedInferMeta", {"x"}, "ceil", {"x"}, {}, {}, {{"out", "x"}}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "ceil");
}

void Ceil_Op::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_) {
  VLOG(4) << "Start build Ceil_Op";



  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);
  paddle::dialect::IrTensor dense_out;
  paddle::dialect::IrMetaTensor meta_out(&dense_out);

  phi::UnchangedInferMeta(meta_x, &meta_out);

  std::vector<pir::Type> argument_outputs;
  pir::Type out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_out.dtype()), dense_out.dims(), dense_out.layout(), dense_out.lod(), dense_out.offset());
  argument_outputs.push_back(out_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void Ceil_Op::VerifySig() {
  VLOG(4) << "Start Verifying inputs, outputs and attributes for: Ceil_Op.";
  VLOG(4) << "Verifying inputs:";
  {
  auto input_size = num_operands();
  IR_ENFORCE(input_size == 1u,
                    "The size %d of inputs must be equal to 1.", input_size);
  IR_ENFORCE((*this)->operand_source(0).type().isa<paddle::dialect::DenseTensorType>(),
                  "Type validation failed for the 0th input, got %s.", (*this)->operand_source(0).type());
  }
  VLOG(4) << "Verifying attributes:";
  {
  // Attributes num is 0, not need to check attributes type.
  }
  VLOG(4) << "Verifying outputs:";
  {
  auto output_size = num_results();
  IR_ENFORCE(output_size == 1u,
                    "The size %d of outputs must be equal to 1.", output_size);
  IR_ENFORCE((*this)->result(0).type().isa<paddle::dialect::DenseTensorType>(),
                 "Type validation failed for the 0th output.");
  }
  VLOG(4) << "End Verifying for: Ceil_Op.";
}

void Ceil_Op::InferMeta( phi::InferMetaContext *infer_meta ) {
  auto fn = PD_INFER_META(phi::UnchangedInferMeta);
  fn(infer_meta);
}

phi::DataType Ceil_Op::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: Ceil_Op";
  


  return expected_kernel_dtype;
}

const char *CeluOp::attributes_name[1] = { "alpha" };

OpInfoTuple CeluOp::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("x", "paddle::dialect::DenseTensorType", false, false, false, true) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = { paddle::dialect::OpAttributeInfo("alpha", "pir::FloatAttribute", "") };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("out", "paddle::dialect::DenseTensorType", false, false) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("UnchangedInferMeta", {"x"}, "celu", {"x", "alpha"}, {}, {}, {}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "celu");
}

void CeluOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, float alpha) {
  VLOG(4) << "Start build CeluOp";



  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_alpha = pir::FloatAttribute::get(pir::IrContext::Instance(), alpha);
  argument.AddAttribute("alpha", attr_alpha);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);
  paddle::dialect::IrTensor dense_out;
  paddle::dialect::IrMetaTensor meta_out(&dense_out);

  phi::UnchangedInferMeta(meta_x, &meta_out);

  std::vector<pir::Type> argument_outputs;
  pir::Type out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_out.dtype()), dense_out.dims(), dense_out.layout(), dense_out.lod(), dense_out.offset());
  argument_outputs.push_back(out_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void CeluOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, pir::AttributeMap attributes) {
  VLOG(4) << "Start build CeluOp";


  IR_ENFORCE(
      attributes.find("alpha") != attributes.end(),
          "'alpha' Attribute is expected for CeluOp. ");
  float alpha = attributes.at("alpha").dyn_cast<pir::FloatAttribute>().data();


  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_alpha = pir::FloatAttribute::get(pir::IrContext::Instance(), alpha);
  argument.AddAttribute("alpha", attr_alpha);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);
  paddle::dialect::IrTensor dense_out;
  paddle::dialect::IrMetaTensor meta_out(&dense_out);

  phi::UnchangedInferMeta(meta_x, &meta_out);

  std::vector<pir::Type> argument_outputs;
  pir::Type out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_out.dtype()), dense_out.dims(), dense_out.layout(), dense_out.lod(), dense_out.offset());
  argument_outputs.push_back(out_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void CeluOp::VerifySig() {
  VLOG(4) << "Start Verifying inputs, outputs and attributes for: CeluOp.";
  VLOG(4) << "Verifying inputs:";
  {
  auto input_size = num_operands();
  IR_ENFORCE(input_size == 1u,
                    "The size %d of inputs must be equal to 1.", input_size);
  IR_ENFORCE((*this)->operand_source(0).type().isa<paddle::dialect::DenseTensorType>(),
                  "Type validation failed for the 0th input, got %s.", (*this)->operand_source(0).type());
  }
  VLOG(4) << "Verifying attributes:";
  {
  auto& attributes = this->attributes();
  IR_ENFORCE(attributes.count("alpha")>0,
                 "alpha does not exist.");
  IR_ENFORCE(attributes.at("alpha").isa<pir::FloatAttribute>(),
                 "Type of attribute: alpha is not pir::FloatAttribute.");

  }
  VLOG(4) << "Verifying outputs:";
  {
  auto output_size = num_results();
  IR_ENFORCE(output_size == 1u,
                    "The size %d of outputs must be equal to 1.", output_size);
  IR_ENFORCE((*this)->result(0).type().isa<paddle::dialect::DenseTensorType>(),
                 "Type validation failed for the 0th output.");
  }
  VLOG(4) << "End Verifying for: CeluOp.";
}

void CeluOp::InferMeta( phi::InferMetaContext *infer_meta ) {
  auto fn = PD_INFER_META(phi::UnchangedInferMeta);
  fn(infer_meta);
}

phi::DataType CeluOp::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: CeluOp";
  


  return expected_kernel_dtype;
}

OpInfoTuple CheckFiniteAndUnscale_Op::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("x", "pir::VectorType<paddle::dialect::DenseTensorType>", false, false, false, false), paddle::dialect::OpInputInfo("scale", "paddle::dialect::DenseTensorType", false, false, false, false) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = {  };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("out", "pir::VectorType<paddle::dialect::DenseTensorType>", false, false), paddle::dialect::OpOutputInfo("found_infinite", "paddle::dialect::DenseTensorType", false, false) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("CheckFiniteAndUnscaleInferMeta", {"x", "scale"}, "check_finite_and_unscale", {"x", "scale"}, {"x"}, {}, {{"out", "x"}}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "check_finite_and_unscale_");
}

void CheckFiniteAndUnscale_Op::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, pir::Value scale_) {
  VLOG(4) << "Start build CheckFiniteAndUnscale_Op";



  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_, scale_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";

  VLOG(4) << "Builder construction outputs";
  pir::VectorType x = x_.type().dyn_cast<pir::VectorType>(); (void)x;
  paddle::dialect::DenseTensorType scale = scale_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)scale;
  std::vector<paddle::dialect::IrTensor> vec_ir_tensor_x;
  for (size_t i=0; i < static_cast<size_t>(x.size()); i++) {
    vec_ir_tensor_x.push_back(paddle::dialect::IrTensor(paddle::dialect::TransToPhiDataType(x[i].dyn_cast<paddle::dialect::DenseTensorType>().dtype()),
                                                                     x[i].dyn_cast<paddle::dialect::DenseTensorType>().dims(),
                                                                     x[i].dyn_cast<paddle::dialect::DenseTensorType>().data_layout(),
                                                                     x[i].dyn_cast<paddle::dialect::DenseTensorType>().lod(),
                                                                     x[i].dyn_cast<paddle::dialect::DenseTensorType>().offset()));
  }
  std::vector<paddle::dialect::IrMetaTensor> vec_meta_x;
  for (size_t i=0; i < vec_ir_tensor_x.size(); i++) {
    vec_meta_x.push_back(paddle::dialect::IrMetaTensor(&vec_ir_tensor_x[i]));
  }

  std::vector<const phi::MetaTensor*> meta_x;
  for (size_t i=0; i < static_cast<size_t>(vec_meta_x.size()); i++) {
    meta_x.push_back(&vec_meta_x[i]);
  }
 
  VLOG(4) << "Builder construction  dense_scale";
  paddle::dialect::IrTensor ir_tensor_scale(paddle::dialect::TransToPhiDataType(scale.dtype()),
                                                      scale.dims(),
                                                      scale.data_layout(),
                                                      scale.lod(),
                                                      scale.offset());
  VLOG(4) << "Builder construction  meta_scale";
  paddle::dialect::IrMetaTensor meta_scale(&ir_tensor_scale);
  std::vector<paddle::dialect::IrTensor> vec_dense_out((x.size()), paddle::dialect::IrTensor());
  std::vector<paddle::dialect::IrMetaTensor> vec_meta_out;
  for (size_t i=0; i < static_cast<size_t>(x.size()); i++) {
    vec_meta_out.push_back(paddle::dialect::IrMetaTensor(&vec_dense_out[i]));
  }
  std::vector<phi::MetaTensor*> meta_out;
  for (size_t i=0; i < static_cast<size_t>(vec_meta_out.size()); i++) {
    meta_out.push_back(&vec_meta_out[i]);
  }
  paddle::dialect::IrTensor dense_found_infinite;
  paddle::dialect::IrMetaTensor meta_found_infinite(&dense_found_infinite);

  phi::CheckFiniteAndUnscaleInferMeta(meta_x, meta_scale, meta_out, &meta_found_infinite);

  std::vector<pir::Type> argument_outputs;
  std::vector<pir::Type> out_types;
  for (size_t i=0; i < static_cast<size_t>(x.size()); i++) {
    out_types.push_back(paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(vec_dense_out[i].dtype()), vec_dense_out[i].dims(), vec_dense_out[i].layout(), vec_dense_out[i].lod(), vec_dense_out[i].offset()));
  }
  pir::Type out_vector_type = pir::VectorType::get(pir::IrContext::Instance(), out_types);
  argument_outputs.push_back(out_vector_type);

  pir::Type found_infinite_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_found_infinite.dtype()), dense_found_infinite.dims(), dense_found_infinite.layout(), dense_found_infinite.lod(), dense_found_infinite.offset());
  argument_outputs.push_back(found_infinite_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void CheckFiniteAndUnscale_Op::VerifySig() {
  VLOG(4) << "Start Verifying inputs, outputs and attributes for: CheckFiniteAndUnscale_Op.";
  VLOG(4) << "Verifying inputs:";
  {
  auto input_size = num_operands();
  IR_ENFORCE(input_size == 2u,
                    "The size %d of inputs must be equal to 2.", input_size);
  if (auto vec_type = (*this)->operand_source(0).type().dyn_cast<pir::VectorType>()) {
      for (size_t i = 0; i < vec_type.size(); ++i) {
        IR_ENFORCE(vec_type[i].isa<paddle::dialect::DenseTensorType>(),
                       "Type validation failed for the 0th input, got %s.", (*this)->operand_source(0).type());
      }
  }
  else {
    IR_ENFORCE((*this)->operand_source(0).type().isa<paddle::dialect::DenseTensorType>(),
                   "Type validation failed for the 0th input, got %s.", (*this)->operand_source(0).type());
  }
  IR_ENFORCE((*this)->operand_source(1).type().isa<paddle::dialect::DenseTensorType>(),
                  "Type validation failed for the 1th input, got %s.", (*this)->operand_source(1).type());
  }
  VLOG(4) << "Verifying attributes:";
  {
  // Attributes num is 0, not need to check attributes type.
  }
  VLOG(4) << "Verifying outputs:";
  {
  auto output_size = num_results();
  IR_ENFORCE(output_size == 2u,
                    "The size %d of outputs must be equal to 2.", output_size);
  auto output_0_type = (*this)->result(0).type();
  if (auto vec_type = output_0_type.dyn_cast<pir::VectorType>()) {
    for (size_t i = 0; i < vec_type.size(); i++) {
      IR_ENFORCE(vec_type[i].isa<paddle::dialect::DenseTensorType>(),
                     "Type validation failed for the 0th output.");
    }
  }
  else {
    IR_ENFORCE(output_0_type.isa<paddle::dialect::DenseTensorType>(),
                   "Type validation failed for the 0th output.");
  }
  IR_ENFORCE((*this)->result(1).type().isa<paddle::dialect::DenseTensorType>(),
                 "Type validation failed for the 1th output.");
  }
  VLOG(4) << "End Verifying for: CheckFiniteAndUnscale_Op.";
}

void CheckFiniteAndUnscale_Op::InferMeta( phi::InferMetaContext *infer_meta ) {
  auto fn = PD_INFER_META(phi::CheckFiniteAndUnscaleInferMeta);
  fn(infer_meta);
}

phi::DataType CheckFiniteAndUnscale_Op::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: CheckFiniteAndUnscale_Op";
  


  return expected_kernel_dtype;
}

const char *CheckNumericsOp::attributes_name[5] = { "op_type", "var_name", "check_nan_inf_level", "stack_height_limit", "output_dir" };

OpInfoTuple CheckNumericsOp::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("tensor", "paddle::dialect::DenseTensorType", false, false, false, false) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = { paddle::dialect::OpAttributeInfo("op_type", "pir::StrAttribute", ""), paddle::dialect::OpAttributeInfo("var_name", "pir::StrAttribute", ""), paddle::dialect::OpAttributeInfo("check_nan_inf_level", "pir::Int32Attribute", ""), paddle::dialect::OpAttributeInfo("stack_height_limit", "pir::Int32Attribute", ""), paddle::dialect::OpAttributeInfo("output_dir", "pir::StrAttribute", "") };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("stats", "paddle::dialect::DenseTensorType", false, false), paddle::dialect::OpOutputInfo("values", "paddle::dialect::DenseTensorType", false, false) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("CheckNumericsInferMeta", {"tensor", "op_type", "var_name", "check_nan_inf_level", "stack_height_limit", "output_dir"}, "check_numerics", {"tensor", "op_type", "var_name", "check_nan_inf_level", "stack_height_limit", "output_dir"}, {}, {}, {}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "check_numerics");
}

void CheckNumericsOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value tensor_, const std::string& op_type, const std::string& var_name, int check_nan_inf_level, int stack_height_limit, const std::string& output_dir) {
  VLOG(4) << "Start build CheckNumericsOp";



  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {tensor_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_op_type = pir::StrAttribute::get(pir::IrContext::Instance(), op_type);
  argument.AddAttribute("op_type", attr_op_type);
  pir::Attribute attr_var_name = pir::StrAttribute::get(pir::IrContext::Instance(), var_name);
  argument.AddAttribute("var_name", attr_var_name);
  pir::Attribute attr_check_nan_inf_level = pir::Int32Attribute::get(pir::IrContext::Instance(), check_nan_inf_level);
  argument.AddAttribute("check_nan_inf_level", attr_check_nan_inf_level);
  pir::Attribute attr_stack_height_limit = pir::Int32Attribute::get(pir::IrContext::Instance(), stack_height_limit);
  argument.AddAttribute("stack_height_limit", attr_stack_height_limit);
  pir::Attribute attr_output_dir = pir::StrAttribute::get(pir::IrContext::Instance(), output_dir);
  argument.AddAttribute("output_dir", attr_output_dir);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType tensor = tensor_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)tensor;

  VLOG(4) << "Builder construction  dense_tensor";
  paddle::dialect::IrTensor ir_tensor_tensor(paddle::dialect::TransToPhiDataType(tensor.dtype()),
                                                      tensor.dims(),
                                                      tensor.data_layout(),
                                                      tensor.lod(),
                                                      tensor.offset());
  VLOG(4) << "Builder construction  meta_tensor";
  paddle::dialect::IrMetaTensor meta_tensor(&ir_tensor_tensor);
  paddle::dialect::IrTensor dense_stats;
  paddle::dialect::IrMetaTensor meta_stats(&dense_stats);
  paddle::dialect::IrTensor dense_values;
  paddle::dialect::IrMetaTensor meta_values(&dense_values);

  phi::CheckNumericsInferMeta(meta_tensor, op_type, var_name, check_nan_inf_level, stack_height_limit, output_dir, &meta_stats, &meta_values);

  std::vector<pir::Type> argument_outputs;
  pir::Type stats_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_stats.dtype()), dense_stats.dims(), dense_stats.layout(), dense_stats.lod(), dense_stats.offset());
  argument_outputs.push_back(stats_dense_tensor_type);

  pir::Type values_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_values.dtype()), dense_values.dims(), dense_values.layout(), dense_values.lod(), dense_values.offset());
  argument_outputs.push_back(values_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void CheckNumericsOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value tensor_, pir::AttributeMap attributes) {
  VLOG(4) << "Start build CheckNumericsOp";


  IR_ENFORCE(
      attributes.find("op_type") != attributes.end(),
          "'op_type' Attribute is expected for CheckNumericsOp. ");
  std::string op_type = attributes.at("op_type").dyn_cast<pir::StrAttribute>().AsString();

  IR_ENFORCE(
      attributes.find("var_name") != attributes.end(),
          "'var_name' Attribute is expected for CheckNumericsOp. ");
  std::string var_name = attributes.at("var_name").dyn_cast<pir::StrAttribute>().AsString();

  IR_ENFORCE(
      attributes.find("check_nan_inf_level") != attributes.end(),
          "'check_nan_inf_level' Attribute is expected for CheckNumericsOp. ");
  int check_nan_inf_level = attributes.at("check_nan_inf_level").dyn_cast<pir::Int32Attribute>().data();

  IR_ENFORCE(
      attributes.find("stack_height_limit") != attributes.end(),
          "'stack_height_limit' Attribute is expected for CheckNumericsOp. ");
  int stack_height_limit = attributes.at("stack_height_limit").dyn_cast<pir::Int32Attribute>().data();

  IR_ENFORCE(
      attributes.find("output_dir") != attributes.end(),
          "'output_dir' Attribute is expected for CheckNumericsOp. ");
  std::string output_dir = attributes.at("output_dir").dyn_cast<pir::StrAttribute>().AsString();


  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {tensor_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_op_type = pir::StrAttribute::get(pir::IrContext::Instance(), op_type);
  argument.AddAttribute("op_type", attr_op_type);
  pir::Attribute attr_var_name = pir::StrAttribute::get(pir::IrContext::Instance(), var_name);
  argument.AddAttribute("var_name", attr_var_name);
  pir::Attribute attr_check_nan_inf_level = pir::Int32Attribute::get(pir::IrContext::Instance(), check_nan_inf_level);
  argument.AddAttribute("check_nan_inf_level", attr_check_nan_inf_level);
  pir::Attribute attr_stack_height_limit = pir::Int32Attribute::get(pir::IrContext::Instance(), stack_height_limit);
  argument.AddAttribute("stack_height_limit", attr_stack_height_limit);
  pir::Attribute attr_output_dir = pir::StrAttribute::get(pir::IrContext::Instance(), output_dir);
  argument.AddAttribute("output_dir", attr_output_dir);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType tensor = tensor_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)tensor;

  VLOG(4) << "Builder construction  dense_tensor";
  paddle::dialect::IrTensor ir_tensor_tensor(paddle::dialect::TransToPhiDataType(tensor.dtype()),
                                                      tensor.dims(),
                                                      tensor.data_layout(),
                                                      tensor.lod(),
                                                      tensor.offset());
  VLOG(4) << "Builder construction  meta_tensor";
  paddle::dialect::IrMetaTensor meta_tensor(&ir_tensor_tensor);
  paddle::dialect::IrTensor dense_stats;
  paddle::dialect::IrMetaTensor meta_stats(&dense_stats);
  paddle::dialect::IrTensor dense_values;
  paddle::dialect::IrMetaTensor meta_values(&dense_values);

  phi::CheckNumericsInferMeta(meta_tensor, op_type, var_name, check_nan_inf_level, stack_height_limit, output_dir, &meta_stats, &meta_values);

  std::vector<pir::Type> argument_outputs;
  pir::Type stats_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_stats.dtype()), dense_stats.dims(), dense_stats.layout(), dense_stats.lod(), dense_stats.offset());
  argument_outputs.push_back(stats_dense_tensor_type);

  pir::Type values_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_values.dtype()), dense_values.dims(), dense_values.layout(), dense_values.lod(), dense_values.offset());
  argument_outputs.push_back(values_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void CheckNumericsOp::VerifySig() {
  VLOG(4) << "Start Verifying inputs, outputs and attributes for: CheckNumericsOp.";
  VLOG(4) << "Verifying inputs:";
  {
  auto input_size = num_operands();
  IR_ENFORCE(input_size == 1u,
                    "The size %d of inputs must be equal to 1.", input_size);
  IR_ENFORCE((*this)->operand_source(0).type().isa<paddle::dialect::DenseTensorType>(),
                  "Type validation failed for the 0th input, got %s.", (*this)->operand_source(0).type());
  }
  VLOG(4) << "Verifying attributes:";
  {
  auto& attributes = this->attributes();
  IR_ENFORCE(attributes.count("op_type")>0,
                 "op_type does not exist.");
  IR_ENFORCE(attributes.at("op_type").isa<pir::StrAttribute>(),
                 "Type of attribute: op_type is not pir::StrAttribute.");

  IR_ENFORCE(attributes.count("var_name")>0,
                 "var_name does not exist.");
  IR_ENFORCE(attributes.at("var_name").isa<pir::StrAttribute>(),
                 "Type of attribute: var_name is not pir::StrAttribute.");

  IR_ENFORCE(attributes.count("check_nan_inf_level")>0,
                 "check_nan_inf_level does not exist.");
  IR_ENFORCE(attributes.at("check_nan_inf_level").isa<pir::Int32Attribute>(),
                 "Type of attribute: check_nan_inf_level is not pir::Int32Attribute.");

  IR_ENFORCE(attributes.count("stack_height_limit")>0,
                 "stack_height_limit does not exist.");
  IR_ENFORCE(attributes.at("stack_height_limit").isa<pir::Int32Attribute>(),
                 "Type of attribute: stack_height_limit is not pir::Int32Attribute.");

  IR_ENFORCE(attributes.count("output_dir")>0,
                 "output_dir does not exist.");
  IR_ENFORCE(attributes.at("output_dir").isa<pir::StrAttribute>(),
                 "Type of attribute: output_dir is not pir::StrAttribute.");

  }
  VLOG(4) << "Verifying outputs:";
  {
  auto output_size = num_results();
  IR_ENFORCE(output_size == 2u,
                    "The size %d of outputs must be equal to 2.", output_size);
  IR_ENFORCE((*this)->result(0).type().isa<paddle::dialect::DenseTensorType>(),
                 "Type validation failed for the 0th output.");
  IR_ENFORCE((*this)->result(1).type().isa<paddle::dialect::DenseTensorType>(),
                 "Type validation failed for the 1th output.");
  }
  VLOG(4) << "End Verifying for: CheckNumericsOp.";
}

void CheckNumericsOp::InferMeta( phi::InferMetaContext *infer_meta ) {
  auto fn = PD_INFER_META(phi::CheckNumericsInferMeta);
  fn(infer_meta);
}

phi::DataType CheckNumericsOp::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: CheckNumericsOp";
  


  return expected_kernel_dtype;
}

const char *CholeskyOp::attributes_name[1] = { "upper" };

OpInfoTuple CholeskyOp::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("x", "paddle::dialect::DenseTensorType", false, false, false, true) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = { paddle::dialect::OpAttributeInfo("upper", "pir::BoolAttribute", "") };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("out", "paddle::dialect::DenseTensorType", false, false) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("CholeskyInferMeta", {"x", "upper"}, "cholesky", {"x", "upper"}, {}, {}, {}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "cholesky");
}

void CholeskyOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, bool upper) {
  VLOG(4) << "Start build CholeskyOp";



  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_upper = pir::BoolAttribute::get(pir::IrContext::Instance(), upper);
  argument.AddAttribute("upper", attr_upper);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);
  paddle::dialect::IrTensor dense_out;
  paddle::dialect::IrMetaTensor meta_out(&dense_out);

  phi::CholeskyInferMeta(meta_x, upper, &meta_out);

  std::vector<pir::Type> argument_outputs;
  pir::Type out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_out.dtype()), dense_out.dims(), dense_out.layout(), dense_out.lod(), dense_out.offset());
  argument_outputs.push_back(out_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void CholeskyOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, pir::AttributeMap attributes) {
  VLOG(4) << "Start build CholeskyOp";


  IR_ENFORCE(
      attributes.find("upper") != attributes.end(),
          "'upper' Attribute is expected for CholeskyOp. ");
  bool upper = attributes.at("upper").dyn_cast<pir::BoolAttribute>().data();


  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_upper = pir::BoolAttribute::get(pir::IrContext::Instance(), upper);
  argument.AddAttribute("upper", attr_upper);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);
  paddle::dialect::IrTensor dense_out;
  paddle::dialect::IrMetaTensor meta_out(&dense_out);

  phi::CholeskyInferMeta(meta_x, upper, &meta_out);

  std::vector<pir::Type> argument_outputs;
  pir::Type out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_out.dtype()), dense_out.dims(), dense_out.layout(), dense_out.lod(), dense_out.offset());
  argument_outputs.push_back(out_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void CholeskyOp::VerifySig() {
  VLOG(4) << "Start Verifying inputs, outputs and attributes for: CholeskyOp.";
  VLOG(4) << "Verifying inputs:";
  {
  auto input_size = num_operands();
  IR_ENFORCE(input_size == 1u,
                    "The size %d of inputs must be equal to 1.", input_size);
  IR_ENFORCE((*this)->operand_source(0).type().isa<paddle::dialect::DenseTensorType>(),
                  "Type validation failed for the 0th input, got %s.", (*this)->operand_source(0).type());
  }
  VLOG(4) << "Verifying attributes:";
  {
  auto& attributes = this->attributes();
  IR_ENFORCE(attributes.count("upper")>0,
                 "upper does not exist.");
  IR_ENFORCE(attributes.at("upper").isa<pir::BoolAttribute>(),
                 "Type of attribute: upper is not pir::BoolAttribute.");

  }
  VLOG(4) << "Verifying outputs:";
  {
  auto output_size = num_results();
  IR_ENFORCE(output_size == 1u,
                    "The size %d of outputs must be equal to 1.", output_size);
  IR_ENFORCE((*this)->result(0).type().isa<paddle::dialect::DenseTensorType>(),
                 "Type validation failed for the 0th output.");
  }
  VLOG(4) << "End Verifying for: CholeskyOp.";
}

void CholeskyOp::InferMeta( phi::InferMetaContext *infer_meta ) {
  auto fn = PD_INFER_META(phi::CholeskyInferMeta);
  fn(infer_meta);
}

phi::DataType CholeskyOp::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: CholeskyOp";
  


  return expected_kernel_dtype;
}

const char *CholeskySolveOp::attributes_name[1] = { "upper" };

OpInfoTuple CholeskySolveOp::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("x", "paddle::dialect::DenseTensorType", false, false, false, true), paddle::dialect::OpInputInfo("y", "paddle::dialect::DenseTensorType", false, false, false, true) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = { paddle::dialect::OpAttributeInfo("upper", "pir::BoolAttribute", "") };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("out", "paddle::dialect::DenseTensorType", false, false) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("CholeskySolveInferMeta", {"x", "y", "upper"}, "cholesky_solve", {"x", "y", "upper"}, {}, {}, {}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "cholesky_solve");
}

void CholeskySolveOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, pir::Value y_, bool upper) {
  VLOG(4) << "Start build CholeskySolveOp";



  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_, y_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_upper = pir::BoolAttribute::get(pir::IrContext::Instance(), upper);
  argument.AddAttribute("upper", attr_upper);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;
  paddle::dialect::DenseTensorType y = y_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)y;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);

  VLOG(4) << "Builder construction  dense_y";
  paddle::dialect::IrTensor ir_tensor_y(paddle::dialect::TransToPhiDataType(y.dtype()),
                                                      y.dims(),
                                                      y.data_layout(),
                                                      y.lod(),
                                                      y.offset());
  VLOG(4) << "Builder construction  meta_y";
  paddle::dialect::IrMetaTensor meta_y(&ir_tensor_y);
  paddle::dialect::IrTensor dense_out;
  paddle::dialect::IrMetaTensor meta_out(&dense_out);

  phi::CholeskySolveInferMeta(meta_x, meta_y, upper, &meta_out);

  std::vector<pir::Type> argument_outputs;
  pir::Type out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_out.dtype()), dense_out.dims(), dense_out.layout(), dense_out.lod(), dense_out.offset());
  argument_outputs.push_back(out_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void CholeskySolveOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, pir::Value y_, pir::AttributeMap attributes) {
  VLOG(4) << "Start build CholeskySolveOp";


  IR_ENFORCE(
      attributes.find("upper") != attributes.end(),
          "'upper' Attribute is expected for CholeskySolveOp. ");
  bool upper = attributes.at("upper").dyn_cast<pir::BoolAttribute>().data();


  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_, y_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_upper = pir::BoolAttribute::get(pir::IrContext::Instance(), upper);
  argument.AddAttribute("upper", attr_upper);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;
  paddle::dialect::DenseTensorType y = y_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)y;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);

  VLOG(4) << "Builder construction  dense_y";
  paddle::dialect::IrTensor ir_tensor_y(paddle::dialect::TransToPhiDataType(y.dtype()),
                                                      y.dims(),
                                                      y.data_layout(),
                                                      y.lod(),
                                                      y.offset());
  VLOG(4) << "Builder construction  meta_y";
  paddle::dialect::IrMetaTensor meta_y(&ir_tensor_y);
  paddle::dialect::IrTensor dense_out;
  paddle::dialect::IrMetaTensor meta_out(&dense_out);

  phi::CholeskySolveInferMeta(meta_x, meta_y, upper, &meta_out);

  std::vector<pir::Type> argument_outputs;
  pir::Type out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_out.dtype()), dense_out.dims(), dense_out.layout(), dense_out.lod(), dense_out.offset());
  argument_outputs.push_back(out_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void CholeskySolveOp::VerifySig() {
  VLOG(4) << "Start Verifying inputs, outputs and attributes for: CholeskySolveOp.";
  VLOG(4) << "Verifying inputs:";
  {
  auto input_size = num_operands();
  IR_ENFORCE(input_size == 2u,
                    "The size %d of inputs must be equal to 2.", input_size);
  IR_ENFORCE((*this)->operand_source(0).type().isa<paddle::dialect::DenseTensorType>(),
                  "Type validation failed for the 0th input, got %s.", (*this)->operand_source(0).type());
  IR_ENFORCE((*this)->operand_source(1).type().isa<paddle::dialect::DenseTensorType>(),
                  "Type validation failed for the 1th input, got %s.", (*this)->operand_source(1).type());
  }
  VLOG(4) << "Verifying attributes:";
  {
  auto& attributes = this->attributes();
  IR_ENFORCE(attributes.count("upper")>0,
                 "upper does not exist.");
  IR_ENFORCE(attributes.at("upper").isa<pir::BoolAttribute>(),
                 "Type of attribute: upper is not pir::BoolAttribute.");

  }
  VLOG(4) << "Verifying outputs:";
  {
  auto output_size = num_results();
  IR_ENFORCE(output_size == 1u,
                    "The size %d of outputs must be equal to 1.", output_size);
  IR_ENFORCE((*this)->result(0).type().isa<paddle::dialect::DenseTensorType>(),
                 "Type validation failed for the 0th output.");
  }
  VLOG(4) << "End Verifying for: CholeskySolveOp.";
}

void CholeskySolveOp::InferMeta( phi::InferMetaContext *infer_meta ) {
  auto fn = PD_INFER_META(phi::CholeskySolveInferMeta);
  fn(infer_meta);
}

phi::DataType CholeskySolveOp::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: CholeskySolveOp";
  


  return expected_kernel_dtype;
}

const char *ClassCenterSampleOp::attributes_name[7] = { "num_classes", "num_samples", "ring_id", "rank", "nranks", "fix_seed", "seed" };

OpInfoTuple ClassCenterSampleOp::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("label", "paddle::dialect::DenseTensorType", false, false, false, false) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = { paddle::dialect::OpAttributeInfo("num_classes", "pir::Int32Attribute", ""), paddle::dialect::OpAttributeInfo("num_samples", "pir::Int32Attribute", ""), paddle::dialect::OpAttributeInfo("ring_id", "pir::Int32Attribute", ""), paddle::dialect::OpAttributeInfo("rank", "pir::Int32Attribute", ""), paddle::dialect::OpAttributeInfo("nranks", "pir::Int32Attribute", ""), paddle::dialect::OpAttributeInfo("fix_seed", "pir::BoolAttribute", ""), paddle::dialect::OpAttributeInfo("seed", "pir::Int32Attribute", "") };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("remapped_label", "paddle::dialect::DenseTensorType", false, false), paddle::dialect::OpOutputInfo("sampled_local_class_center", "paddle::dialect::DenseTensorType", false, false) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("ClassCenterSampleInferMeta", {"label", "num_classes", "num_samples", "ring_id", "rank", "nranks", "fix_seed", "seed"}, "class_center_sample", {"label", "num_classes", "num_samples", "ring_id", "rank", "nranks", "fix_seed", "seed"}, {"label"}, {}, {}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "class_center_sample");
}

void ClassCenterSampleOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value label_, int num_classes, int num_samples, int ring_id, int rank, int nranks, bool fix_seed, int seed) {
  VLOG(4) << "Start build ClassCenterSampleOp";



  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {label_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_num_classes = pir::Int32Attribute::get(pir::IrContext::Instance(), num_classes);
  argument.AddAttribute("num_classes", attr_num_classes);
  pir::Attribute attr_num_samples = pir::Int32Attribute::get(pir::IrContext::Instance(), num_samples);
  argument.AddAttribute("num_samples", attr_num_samples);
  pir::Attribute attr_ring_id = pir::Int32Attribute::get(pir::IrContext::Instance(), ring_id);
  argument.AddAttribute("ring_id", attr_ring_id);
  pir::Attribute attr_rank = pir::Int32Attribute::get(pir::IrContext::Instance(), rank);
  argument.AddAttribute("rank", attr_rank);
  pir::Attribute attr_nranks = pir::Int32Attribute::get(pir::IrContext::Instance(), nranks);
  argument.AddAttribute("nranks", attr_nranks);
  pir::Attribute attr_fix_seed = pir::BoolAttribute::get(pir::IrContext::Instance(), fix_seed);
  argument.AddAttribute("fix_seed", attr_fix_seed);
  pir::Attribute attr_seed = pir::Int32Attribute::get(pir::IrContext::Instance(), seed);
  argument.AddAttribute("seed", attr_seed);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType label = label_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)label;

  VLOG(4) << "Builder construction  dense_label";
  paddle::dialect::IrTensor ir_tensor_label(paddle::dialect::TransToPhiDataType(label.dtype()),
                                                      label.dims(),
                                                      label.data_layout(),
                                                      label.lod(),
                                                      label.offset());
  VLOG(4) << "Builder construction  meta_label";
  paddle::dialect::IrMetaTensor meta_label(&ir_tensor_label);
  paddle::dialect::IrTensor dense_remapped_label;
  paddle::dialect::IrMetaTensor meta_remapped_label(&dense_remapped_label);
  paddle::dialect::IrTensor dense_sampled_local_class_center;
  paddle::dialect::IrMetaTensor meta_sampled_local_class_center(&dense_sampled_local_class_center);

  phi::ClassCenterSampleInferMeta(meta_label, num_classes, num_samples, ring_id, rank, nranks, fix_seed, seed, &meta_remapped_label, &meta_sampled_local_class_center);

  std::vector<pir::Type> argument_outputs;
  pir::Type remapped_label_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_remapped_label.dtype()), dense_remapped_label.dims(), dense_remapped_label.layout(), dense_remapped_label.lod(), dense_remapped_label.offset());
  argument_outputs.push_back(remapped_label_dense_tensor_type);

  pir::Type sampled_local_class_center_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_sampled_local_class_center.dtype()), dense_sampled_local_class_center.dims(), dense_sampled_local_class_center.layout(), dense_sampled_local_class_center.lod(), dense_sampled_local_class_center.offset());
  argument_outputs.push_back(sampled_local_class_center_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void ClassCenterSampleOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value label_, pir::AttributeMap attributes) {
  VLOG(4) << "Start build ClassCenterSampleOp";


  IR_ENFORCE(
      attributes.find("num_classes") != attributes.end(),
          "'num_classes' Attribute is expected for ClassCenterSampleOp. ");
  int num_classes = attributes.at("num_classes").dyn_cast<pir::Int32Attribute>().data();

  IR_ENFORCE(
      attributes.find("num_samples") != attributes.end(),
          "'num_samples' Attribute is expected for ClassCenterSampleOp. ");
  int num_samples = attributes.at("num_samples").dyn_cast<pir::Int32Attribute>().data();

  IR_ENFORCE(
      attributes.find("ring_id") != attributes.end(),
          "'ring_id' Attribute is expected for ClassCenterSampleOp. ");
  int ring_id = attributes.at("ring_id").dyn_cast<pir::Int32Attribute>().data();

  IR_ENFORCE(
      attributes.find("rank") != attributes.end(),
          "'rank' Attribute is expected for ClassCenterSampleOp. ");
  int rank = attributes.at("rank").dyn_cast<pir::Int32Attribute>().data();

  IR_ENFORCE(
      attributes.find("nranks") != attributes.end(),
          "'nranks' Attribute is expected for ClassCenterSampleOp. ");
  int nranks = attributes.at("nranks").dyn_cast<pir::Int32Attribute>().data();

  IR_ENFORCE(
      attributes.find("fix_seed") != attributes.end(),
          "'fix_seed' Attribute is expected for ClassCenterSampleOp. ");
  bool fix_seed = attributes.at("fix_seed").dyn_cast<pir::BoolAttribute>().data();

  IR_ENFORCE(
      attributes.find("seed") != attributes.end(),
          "'seed' Attribute is expected for ClassCenterSampleOp. ");
  int seed = attributes.at("seed").dyn_cast<pir::Int32Attribute>().data();


  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {label_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_num_classes = pir::Int32Attribute::get(pir::IrContext::Instance(), num_classes);
  argument.AddAttribute("num_classes", attr_num_classes);
  pir::Attribute attr_num_samples = pir::Int32Attribute::get(pir::IrContext::Instance(), num_samples);
  argument.AddAttribute("num_samples", attr_num_samples);
  pir::Attribute attr_ring_id = pir::Int32Attribute::get(pir::IrContext::Instance(), ring_id);
  argument.AddAttribute("ring_id", attr_ring_id);
  pir::Attribute attr_rank = pir::Int32Attribute::get(pir::IrContext::Instance(), rank);
  argument.AddAttribute("rank", attr_rank);
  pir::Attribute attr_nranks = pir::Int32Attribute::get(pir::IrContext::Instance(), nranks);
  argument.AddAttribute("nranks", attr_nranks);
  pir::Attribute attr_fix_seed = pir::BoolAttribute::get(pir::IrContext::Instance(), fix_seed);
  argument.AddAttribute("fix_seed", attr_fix_seed);
  pir::Attribute attr_seed = pir::Int32Attribute::get(pir::IrContext::Instance(), seed);
  argument.AddAttribute("seed", attr_seed);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType label = label_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)label;

  VLOG(4) << "Builder construction  dense_label";
  paddle::dialect::IrTensor ir_tensor_label(paddle::dialect::TransToPhiDataType(label.dtype()),
                                                      label.dims(),
                                                      label.data_layout(),
                                                      label.lod(),
                                                      label.offset());
  VLOG(4) << "Builder construction  meta_label";
  paddle::dialect::IrMetaTensor meta_label(&ir_tensor_label);
  paddle::dialect::IrTensor dense_remapped_label;
  paddle::dialect::IrMetaTensor meta_remapped_label(&dense_remapped_label);
  paddle::dialect::IrTensor dense_sampled_local_class_center;
  paddle::dialect::IrMetaTensor meta_sampled_local_class_center(&dense_sampled_local_class_center);

  phi::ClassCenterSampleInferMeta(meta_label, num_classes, num_samples, ring_id, rank, nranks, fix_seed, seed, &meta_remapped_label, &meta_sampled_local_class_center);

  std::vector<pir::Type> argument_outputs;
  pir::Type remapped_label_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_remapped_label.dtype()), dense_remapped_label.dims(), dense_remapped_label.layout(), dense_remapped_label.lod(), dense_remapped_label.offset());
  argument_outputs.push_back(remapped_label_dense_tensor_type);

  pir::Type sampled_local_class_center_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_sampled_local_class_center.dtype()), dense_sampled_local_class_center.dims(), dense_sampled_local_class_center.layout(), dense_sampled_local_class_center.lod(), dense_sampled_local_class_center.offset());
  argument_outputs.push_back(sampled_local_class_center_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void ClassCenterSampleOp::VerifySig() {
  VLOG(4) << "Start Verifying inputs, outputs and attributes for: ClassCenterSampleOp.";
  VLOG(4) << "Verifying inputs:";
  {
  auto input_size = num_operands();
  IR_ENFORCE(input_size == 1u,
                    "The size %d of inputs must be equal to 1.", input_size);
  IR_ENFORCE((*this)->operand_source(0).type().isa<paddle::dialect::DenseTensorType>(),
                  "Type validation failed for the 0th input, got %s.", (*this)->operand_source(0).type());
  }
  VLOG(4) << "Verifying attributes:";
  {
  auto& attributes = this->attributes();
  IR_ENFORCE(attributes.count("num_classes")>0,
                 "num_classes does not exist.");
  IR_ENFORCE(attributes.at("num_classes").isa<pir::Int32Attribute>(),
                 "Type of attribute: num_classes is not pir::Int32Attribute.");

  IR_ENFORCE(attributes.count("num_samples")>0,
                 "num_samples does not exist.");
  IR_ENFORCE(attributes.at("num_samples").isa<pir::Int32Attribute>(),
                 "Type of attribute: num_samples is not pir::Int32Attribute.");

  IR_ENFORCE(attributes.count("ring_id")>0,
                 "ring_id does not exist.");
  IR_ENFORCE(attributes.at("ring_id").isa<pir::Int32Attribute>(),
                 "Type of attribute: ring_id is not pir::Int32Attribute.");

  IR_ENFORCE(attributes.count("rank")>0,
                 "rank does not exist.");
  IR_ENFORCE(attributes.at("rank").isa<pir::Int32Attribute>(),
                 "Type of attribute: rank is not pir::Int32Attribute.");

  IR_ENFORCE(attributes.count("nranks")>0,
                 "nranks does not exist.");
  IR_ENFORCE(attributes.at("nranks").isa<pir::Int32Attribute>(),
                 "Type of attribute: nranks is not pir::Int32Attribute.");

  IR_ENFORCE(attributes.count("fix_seed")>0,
                 "fix_seed does not exist.");
  IR_ENFORCE(attributes.at("fix_seed").isa<pir::BoolAttribute>(),
                 "Type of attribute: fix_seed is not pir::BoolAttribute.");

  IR_ENFORCE(attributes.count("seed")>0,
                 "seed does not exist.");
  IR_ENFORCE(attributes.at("seed").isa<pir::Int32Attribute>(),
                 "Type of attribute: seed is not pir::Int32Attribute.");

  }
  VLOG(4) << "Verifying outputs:";
  {
  auto output_size = num_results();
  IR_ENFORCE(output_size == 2u,
                    "The size %d of outputs must be equal to 2.", output_size);
  IR_ENFORCE((*this)->result(0).type().isa<paddle::dialect::DenseTensorType>(),
                 "Type validation failed for the 0th output.");
  IR_ENFORCE((*this)->result(1).type().isa<paddle::dialect::DenseTensorType>(),
                 "Type validation failed for the 1th output.");
  }
  VLOG(4) << "End Verifying for: ClassCenterSampleOp.";
}

void ClassCenterSampleOp::InferMeta( phi::InferMetaContext *infer_meta ) {
  auto fn = PD_INFER_META(phi::ClassCenterSampleInferMeta);
  fn(infer_meta);
}

phi::DataType ClassCenterSampleOp::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: ClassCenterSampleOp";
  


  return expected_kernel_dtype;
}

OpInfoTuple ClipOp::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("x", "paddle::dialect::DenseTensorType", false, false, false, true), paddle::dialect::OpInputInfo("min", "paddle::dialect::ScalarAttribute", false, false, true, false), paddle::dialect::OpInputInfo("max", "paddle::dialect::ScalarAttribute", false, false, true, false) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = {  };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("out", "paddle::dialect::DenseTensorType", false, false) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("UnchangedInferMeta", {"x"}, "clip", {"x", "min", "max"}, {"x"}, {}, {}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "clip");
}

void ClipOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, float min, float max) {
  VLOG(4) << "Start build ClipOp";


  // Generate scalar mutable attribute: min
  paddle::dialect::FullOp full_min_op = builder.Build<paddle::dialect::FullOp>(std::vector<int64_t>{1}, min, phi::DataType::FLOAT32, phi::CPUPlace());
  pir::OpResult min_ = full_min_op->result(0);
      // Generate scalar mutable attribute: max
  paddle::dialect::FullOp full_max_op = builder.Build<paddle::dialect::FullOp>(std::vector<int64_t>{1}, max, phi::DataType::FLOAT32, phi::CPUPlace());
  pir::OpResult max_ = full_max_op->result(0);
    
  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_, min_, max_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);
  paddle::dialect::IrTensor dense_out;
  paddle::dialect::IrMetaTensor meta_out(&dense_out);

  phi::UnchangedInferMeta(meta_x, &meta_out);

  std::vector<pir::Type> argument_outputs;
  pir::Type out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_out.dtype()), dense_out.dims(), dense_out.layout(), dense_out.lod(), dense_out.offset());
  argument_outputs.push_back(out_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void ClipOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, pir::AttributeMap attributes) {
  VLOG(4) << "Start build ClipOp";


  IR_ENFORCE(
      attributes.find("min") != attributes.end(),
          "'min' Attribute is expected for ClipOp. ");
  float min = attributes.at("min").dyn_cast<pir::FloatAttribute>().data();

  IR_ENFORCE(
      attributes.find("max") != attributes.end(),
          "'max' Attribute is expected for ClipOp. ");
  float max = attributes.at("max").dyn_cast<pir::FloatAttribute>().data();

  // Generate scalar mutable attribute: min
  paddle::dialect::FullOp full_min_op = builder.Build<paddle::dialect::FullOp>(std::vector<int64_t>{1}, min, phi::DataType::FLOAT32, phi::CPUPlace());
  pir::OpResult min_ = full_min_op->result(0);
      // Generate scalar mutable attribute: max
  paddle::dialect::FullOp full_max_op = builder.Build<paddle::dialect::FullOp>(std::vector<int64_t>{1}, max, phi::DataType::FLOAT32, phi::CPUPlace());
  pir::OpResult max_ = full_max_op->result(0);
    
  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_, min_, max_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);
  paddle::dialect::IrTensor dense_out;
  paddle::dialect::IrMetaTensor meta_out(&dense_out);

  phi::UnchangedInferMeta(meta_x, &meta_out);

  std::vector<pir::Type> argument_outputs;
  pir::Type out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_out.dtype()), dense_out.dims(), dense_out.layout(), dense_out.lod(), dense_out.offset());
  argument_outputs.push_back(out_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void ClipOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, pir::Value min_, pir::Value max_) {
  VLOG(4) << "Start build ClipOp";



  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_, min_, max_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;
  phi::Scalar min;
  if (min_.dyn_cast<pir::OpResult>() && min_.dyn_cast<pir::OpResult>().owner()->isa<paddle::dialect::FullOp>()) {
    min = std::move(phi::Scalar(min_.dyn_cast<pir::OpResult>().owner()
                                  ->dyn_cast<paddle::dialect::FullOp>()
                                  .attribute("value")
                                  .dyn_cast<paddle::dialect::ScalarAttribute>()
                                  .data()
                                  .to<int>()));
  }
  else {
    min = std::move(phi::Scalar(-1));
    min.SetFromTensor(true);
  }
  phi::Scalar max;
  if (max_.dyn_cast<pir::OpResult>() && max_.dyn_cast<pir::OpResult>().owner()->isa<paddle::dialect::FullOp>()) {
    max = std::move(phi::Scalar(max_.dyn_cast<pir::OpResult>().owner()
                                  ->dyn_cast<paddle::dialect::FullOp>()
                                  .attribute("value")
                                  .dyn_cast<paddle::dialect::ScalarAttribute>()
                                  .data()
                                  .to<int>()));
  }
  else {
    max = std::move(phi::Scalar(-1));
    max.SetFromTensor(true);
  }


  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);
  paddle::dialect::IrTensor dense_out;
  paddle::dialect::IrMetaTensor meta_out(&dense_out);

  phi::UnchangedInferMeta(meta_x, &meta_out);

  std::vector<pir::Type> argument_outputs;
  pir::Type out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_out.dtype()), dense_out.dims(), dense_out.layout(), dense_out.lod(), dense_out.offset());
  argument_outputs.push_back(out_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void ClipOp::VerifySig() {
  VLOG(4) << "Start Verifying inputs, outputs and attributes for: ClipOp.";
  VLOG(4) << "Verifying inputs:";
  {
  auto input_size = num_operands();
  IR_ENFORCE(input_size == 3u,
                    "The size %d of inputs must be equal to 3.", input_size);
  IR_ENFORCE((*this)->operand_source(0).type().isa<paddle::dialect::DenseTensorType>(),
                  "Type validation failed for the 0th input, got %s.", (*this)->operand_source(0).type());
  IR_ENFORCE((*this)->operand_source(1).type().isa<paddle::dialect::DenseTensorType>(),
                  "Type validation failed for the 1th input, got %s.", (*this)->operand_source(1).type());
  IR_ENFORCE((*this)->operand_source(2).type().isa<paddle::dialect::DenseTensorType>(),
                  "Type validation failed for the 2th input, got %s.", (*this)->operand_source(2).type());
  }
  VLOG(4) << "Verifying attributes:";
  {
  // Attributes num is 0, not need to check attributes type.
  }
  VLOG(4) << "Verifying outputs:";
  {
  auto output_size = num_results();
  IR_ENFORCE(output_size == 1u,
                    "The size %d of outputs must be equal to 1.", output_size);
  IR_ENFORCE((*this)->result(0).type().isa<paddle::dialect::DenseTensorType>(),
                 "Type validation failed for the 0th output.");
  }
  VLOG(4) << "End Verifying for: ClipOp.";
}

void ClipOp::InferMeta( phi::InferMetaContext *infer_meta ) {
  auto fn = PD_INFER_META(phi::UnchangedInferMeta);
  fn(infer_meta);
}

phi::DataType ClipOp::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: ClipOp";
  


  return expected_kernel_dtype;
}

OpInfoTuple Clip_Op::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("x", "paddle::dialect::DenseTensorType", false, false, false, true), paddle::dialect::OpInputInfo("min", "paddle::dialect::ScalarAttribute", false, false, true, false), paddle::dialect::OpInputInfo("max", "paddle::dialect::ScalarAttribute", false, false, true, false) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = {  };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("out", "paddle::dialect::DenseTensorType", false, false) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("UnchangedInferMeta", {"x"}, "clip", {"x", "min", "max"}, {"x"}, {}, {{"out", "x"}}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "clip");
}

void Clip_Op::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, float min, float max) {
  VLOG(4) << "Start build Clip_Op";


  // Generate scalar mutable attribute: min
  paddle::dialect::FullOp full_min_op = builder.Build<paddle::dialect::FullOp>(std::vector<int64_t>{1}, min, phi::DataType::FLOAT32, phi::CPUPlace());
  pir::OpResult min_ = full_min_op->result(0);
      // Generate scalar mutable attribute: max
  paddle::dialect::FullOp full_max_op = builder.Build<paddle::dialect::FullOp>(std::vector<int64_t>{1}, max, phi::DataType::FLOAT32, phi::CPUPlace());
  pir::OpResult max_ = full_max_op->result(0);
    
  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_, min_, max_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);
  paddle::dialect::IrTensor dense_out;
  paddle::dialect::IrMetaTensor meta_out(&dense_out);

  phi::UnchangedInferMeta(meta_x, &meta_out);

  std::vector<pir::Type> argument_outputs;
  pir::Type out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_out.dtype()), dense_out.dims(), dense_out.layout(), dense_out.lod(), dense_out.offset());
  argument_outputs.push_back(out_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void Clip_Op::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, pir::AttributeMap attributes) {
  VLOG(4) << "Start build Clip_Op";


  IR_ENFORCE(
      attributes.find("min") != attributes.end(),
          "'min' Attribute is expected for Clip_Op. ");
  float min = attributes.at("min").dyn_cast<pir::FloatAttribute>().data();

  IR_ENFORCE(
      attributes.find("max") != attributes.end(),
          "'max' Attribute is expected for Clip_Op. ");
  float max = attributes.at("max").dyn_cast<pir::FloatAttribute>().data();

  // Generate scalar mutable attribute: min
  paddle::dialect::FullOp full_min_op = builder.Build<paddle::dialect::FullOp>(std::vector<int64_t>{1}, min, phi::DataType::FLOAT32, phi::CPUPlace());
  pir::OpResult min_ = full_min_op->result(0);
      // Generate scalar mutable attribute: max
  paddle::dialect::FullOp full_max_op = builder.Build<paddle::dialect::FullOp>(std::vector<int64_t>{1}, max, phi::DataType::FLOAT32, phi::CPUPlace());
  pir::OpResult max_ = full_max_op->result(0);
    
  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_, min_, max_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);
  paddle::dialect::IrTensor dense_out;
  paddle::dialect::IrMetaTensor meta_out(&dense_out);

  phi::UnchangedInferMeta(meta_x, &meta_out);

  std::vector<pir::Type> argument_outputs;
  pir::Type out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_out.dtype()), dense_out.dims(), dense_out.layout(), dense_out.lod(), dense_out.offset());
  argument_outputs.push_back(out_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void Clip_Op::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, pir::Value min_, pir::Value max_) {
  VLOG(4) << "Start build Clip_Op";



  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_, min_, max_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;
  phi::Scalar min;
  if (min_.dyn_cast<pir::OpResult>() && min_.dyn_cast<pir::OpResult>().owner()->isa<paddle::dialect::FullOp>()) {
    min = std::move(phi::Scalar(min_.dyn_cast<pir::OpResult>().owner()
                                  ->dyn_cast<paddle::dialect::FullOp>()
                                  .attribute("value")
                                  .dyn_cast<paddle::dialect::ScalarAttribute>()
                                  .data()
                                  .to<int>()));
  }
  else {
    min = std::move(phi::Scalar(-1));
    min.SetFromTensor(true);
  }
  phi::Scalar max;
  if (max_.dyn_cast<pir::OpResult>() && max_.dyn_cast<pir::OpResult>().owner()->isa<paddle::dialect::FullOp>()) {
    max = std::move(phi::Scalar(max_.dyn_cast<pir::OpResult>().owner()
                                  ->dyn_cast<paddle::dialect::FullOp>()
                                  .attribute("value")
                                  .dyn_cast<paddle::dialect::ScalarAttribute>()
                                  .data()
                                  .to<int>()));
  }
  else {
    max = std::move(phi::Scalar(-1));
    max.SetFromTensor(true);
  }


  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);
  paddle::dialect::IrTensor dense_out;
  paddle::dialect::IrMetaTensor meta_out(&dense_out);

  phi::UnchangedInferMeta(meta_x, &meta_out);

  std::vector<pir::Type> argument_outputs;
  pir::Type out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_out.dtype()), dense_out.dims(), dense_out.layout(), dense_out.lod(), dense_out.offset());
  argument_outputs.push_back(out_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void Clip_Op::VerifySig() {
  VLOG(4) << "Start Verifying inputs, outputs and attributes for: Clip_Op.";
  VLOG(4) << "Verifying inputs:";
  {
  auto input_size = num_operands();
  IR_ENFORCE(input_size == 3u,
                    "The size %d of inputs must be equal to 3.", input_size);
  IR_ENFORCE((*this)->operand_source(0).type().isa<paddle::dialect::DenseTensorType>(),
                  "Type validation failed for the 0th input, got %s.", (*this)->operand_source(0).type());
  IR_ENFORCE((*this)->operand_source(1).type().isa<paddle::dialect::DenseTensorType>(),
                  "Type validation failed for the 1th input, got %s.", (*this)->operand_source(1).type());
  IR_ENFORCE((*this)->operand_source(2).type().isa<paddle::dialect::DenseTensorType>(),
                  "Type validation failed for the 2th input, got %s.", (*this)->operand_source(2).type());
  }
  VLOG(4) << "Verifying attributes:";
  {
  // Attributes num is 0, not need to check attributes type.
  }
  VLOG(4) << "Verifying outputs:";
  {
  auto output_size = num_results();
  IR_ENFORCE(output_size == 1u,
                    "The size %d of outputs must be equal to 1.", output_size);
  IR_ENFORCE((*this)->result(0).type().isa<paddle::dialect::DenseTensorType>(),
                 "Type validation failed for the 0th output.");
  }
  VLOG(4) << "End Verifying for: Clip_Op.";
}

void Clip_Op::InferMeta( phi::InferMetaContext *infer_meta ) {
  auto fn = PD_INFER_META(phi::UnchangedInferMeta);
  fn(infer_meta);
}

phi::DataType Clip_Op::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: Clip_Op";
  


  return expected_kernel_dtype;
}

const char *ClipByNormOp::attributes_name[1] = { "max_norm" };

OpInfoTuple ClipByNormOp::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("x", "paddle::dialect::DenseTensorType", false, false, false, false) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = { paddle::dialect::OpAttributeInfo("max_norm", "pir::FloatAttribute", "") };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("out", "paddle::dialect::DenseTensorType", false, false) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("ClipByNormInferMeta", {"x", "max_norm"}, "clip_by_norm", {"x", "max_norm"}, {}, {}, {}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "clip_by_norm");
}

void ClipByNormOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, float max_norm) {
  VLOG(4) << "Start build ClipByNormOp";



  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_max_norm = pir::FloatAttribute::get(pir::IrContext::Instance(), max_norm);
  argument.AddAttribute("max_norm", attr_max_norm);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);
  paddle::dialect::IrTensor dense_out;
  paddle::dialect::IrMetaTensor meta_out(&dense_out);

  phi::ClipByNormInferMeta(meta_x, max_norm, &meta_out);

  std::vector<pir::Type> argument_outputs;
  pir::Type out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_out.dtype()), dense_out.dims(), dense_out.layout(), dense_out.lod(), dense_out.offset());
  argument_outputs.push_back(out_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void ClipByNormOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, pir::AttributeMap attributes) {
  VLOG(4) << "Start build ClipByNormOp";


  IR_ENFORCE(
      attributes.find("max_norm") != attributes.end(),
          "'max_norm' Attribute is expected for ClipByNormOp. ");
  float max_norm = attributes.at("max_norm").dyn_cast<pir::FloatAttribute>().data();


  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_max_norm = pir::FloatAttribute::get(pir::IrContext::Instance(), max_norm);
  argument.AddAttribute("max_norm", attr_max_norm);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);
  paddle::dialect::IrTensor dense_out;
  paddle::dialect::IrMetaTensor meta_out(&dense_out);

  phi::ClipByNormInferMeta(meta_x, max_norm, &meta_out);

  std::vector<pir::Type> argument_outputs;
  pir::Type out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_out.dtype()), dense_out.dims(), dense_out.layout(), dense_out.lod(), dense_out.offset());
  argument_outputs.push_back(out_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void ClipByNormOp::VerifySig() {
  VLOG(4) << "Start Verifying inputs, outputs and attributes for: ClipByNormOp.";
  VLOG(4) << "Verifying inputs:";
  {
  auto input_size = num_operands();
  IR_ENFORCE(input_size == 1u,
                    "The size %d of inputs must be equal to 1.", input_size);
  IR_ENFORCE((*this)->operand_source(0).type().isa<paddle::dialect::DenseTensorType>(),
                  "Type validation failed for the 0th input, got %s.", (*this)->operand_source(0).type());
  }
  VLOG(4) << "Verifying attributes:";
  {
  auto& attributes = this->attributes();
  IR_ENFORCE(attributes.count("max_norm")>0,
                 "max_norm does not exist.");
  IR_ENFORCE(attributes.at("max_norm").isa<pir::FloatAttribute>(),
                 "Type of attribute: max_norm is not pir::FloatAttribute.");

  }
  VLOG(4) << "Verifying outputs:";
  {
  auto output_size = num_results();
  IR_ENFORCE(output_size == 1u,
                    "The size %d of outputs must be equal to 1.", output_size);
  IR_ENFORCE((*this)->result(0).type().isa<paddle::dialect::DenseTensorType>(),
                 "Type validation failed for the 0th output.");
  }
  VLOG(4) << "End Verifying for: ClipByNormOp.";
}

void ClipByNormOp::InferMeta( phi::InferMetaContext *infer_meta ) {
  auto fn = PD_INFER_META(phi::ClipByNormInferMeta);
  fn(infer_meta);
}

phi::DataType ClipByNormOp::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: ClipByNormOp";
  


  return expected_kernel_dtype;
}

const char *ClipByNormSrOp::attributes_name[1] = { "max_norm" };

OpInfoTuple ClipByNormSrOp::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("x", "paddle::dialect::SelectedRowsType", false, false, false, false) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = { paddle::dialect::OpAttributeInfo("max_norm", "pir::FloatAttribute", "") };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("out", "paddle::dialect::SelectedRowsType", false, false) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("ClipByNormInferMeta", {"x", "max_norm"}, "clip_by_norm_sr", {"x", "max_norm"}, {}, {}, {}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "clip_by_norm");
}

void ClipByNormSrOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, float max_norm) {
  VLOG(4) << "Start build ClipByNormSrOp";



  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_max_norm = pir::FloatAttribute::get(pir::IrContext::Instance(), max_norm);
  argument.AddAttribute("max_norm", attr_max_norm);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::SelectedRowsType x = x_.type().dyn_cast<paddle::dialect::SelectedRowsType>(); (void)x;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);
  paddle::dialect::IrSelectedRows dense_out;
  paddle::dialect::IrMetaTensor meta_out(&dense_out);

  phi::ClipByNormInferMeta(meta_x, max_norm, &meta_out);

  std::vector<pir::Type> argument_outputs;
  pir::Type out_dense_tensor_type = paddle::dialect::SelectedRowsType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_out.dtype()), dense_out.dims(), dense_out.layout(), dense_out.lod(), dense_out.offset());
  argument_outputs.push_back(out_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void ClipByNormSrOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, pir::AttributeMap attributes) {
  VLOG(4) << "Start build ClipByNormSrOp";


  IR_ENFORCE(
      attributes.find("max_norm") != attributes.end(),
          "'max_norm' Attribute is expected for ClipByNormSrOp. ");
  float max_norm = attributes.at("max_norm").dyn_cast<pir::FloatAttribute>().data();


  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_max_norm = pir::FloatAttribute::get(pir::IrContext::Instance(), max_norm);
  argument.AddAttribute("max_norm", attr_max_norm);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::SelectedRowsType x = x_.type().dyn_cast<paddle::dialect::SelectedRowsType>(); (void)x;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);
  paddle::dialect::IrSelectedRows dense_out;
  paddle::dialect::IrMetaTensor meta_out(&dense_out);

  phi::ClipByNormInferMeta(meta_x, max_norm, &meta_out);

  std::vector<pir::Type> argument_outputs;
  pir::Type out_dense_tensor_type = paddle::dialect::SelectedRowsType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_out.dtype()), dense_out.dims(), dense_out.layout(), dense_out.lod(), dense_out.offset());
  argument_outputs.push_back(out_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void ClipByNormSrOp::VerifySig() {
  VLOG(4) << "Start Verifying inputs, outputs and attributes for: ClipByNormSrOp.";
  VLOG(4) << "Verifying inputs:";
  {
  auto input_size = num_operands();
  IR_ENFORCE(input_size == 1u,
                    "The size %d of inputs must be equal to 1.", input_size);
  IR_ENFORCE((*this)->operand_source(0).type().isa<paddle::dialect::SelectedRowsType>(),
                  "Type validation failed for the 0th input, got %s.", (*this)->operand_source(0).type());
  }
  VLOG(4) << "Verifying attributes:";
  {
  auto& attributes = this->attributes();
  IR_ENFORCE(attributes.count("max_norm")>0,
                 "max_norm does not exist.");
  IR_ENFORCE(attributes.at("max_norm").isa<pir::FloatAttribute>(),
                 "Type of attribute: max_norm is not pir::FloatAttribute.");

  }
  VLOG(4) << "Verifying outputs:";
  {
  auto output_size = num_results();
  IR_ENFORCE(output_size == 1u,
                    "The size %d of outputs must be equal to 1.", output_size);
  IR_ENFORCE((*this)->result(0).type().isa<paddle::dialect::SelectedRowsType>(),
                 "Type validation failed for the 0th output.");
  }
  VLOG(4) << "End Verifying for: ClipByNormSrOp.";
}

void ClipByNormSrOp::InferMeta( phi::InferMetaContext *infer_meta ) {
  auto fn = PD_INFER_META(phi::ClipByNormInferMeta);
  fn(infer_meta);
}

phi::DataType ClipByNormSrOp::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: ClipByNormSrOp";
  


  return expected_kernel_dtype;
}

const char *CoalesceTensorOp::attributes_name[10] = { "dtype", "copy_data", "set_constant", "persist_output", "constant", "use_align", "align_size", "size_of_dtype", "concated_shapes", "concated_ranks" };

OpInfoTuple CoalesceTensorOp::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("input", "pir::VectorType<paddle::dialect::DenseTensorType>", false, false, false, false) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = { paddle::dialect::OpAttributeInfo("dtype", "paddle::dialect::DataTypeAttribute", ""), paddle::dialect::OpAttributeInfo("copy_data", "pir::BoolAttribute", ""), paddle::dialect::OpAttributeInfo("set_constant", "pir::BoolAttribute", ""), paddle::dialect::OpAttributeInfo("persist_output", "pir::BoolAttribute", ""), paddle::dialect::OpAttributeInfo("constant", "pir::FloatAttribute", ""), paddle::dialect::OpAttributeInfo("use_align", "pir::BoolAttribute", ""), paddle::dialect::OpAttributeInfo("align_size", "pir::Int32Attribute", ""), paddle::dialect::OpAttributeInfo("size_of_dtype", "pir::Int32Attribute", ""), paddle::dialect::OpAttributeInfo("concated_shapes", "pir::ArrayAttribute<pir::Int64Attribute>", ""), paddle::dialect::OpAttributeInfo("concated_ranks", "pir::ArrayAttribute<pir::Int64Attribute>", "") };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("output", "pir::VectorType<paddle::dialect::DenseTensorType>", false, false), paddle::dialect::OpOutputInfo("fused_output", "paddle::dialect::DenseTensorType", false, false) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("CoalesceTensorInferMeta", {"input", "dtype", "copy_data", "set_constant", "persist_output", "constant", "use_align", "align_size", "size_of_dtype", "concated_shapes", "concated_ranks"}, "coalesce_tensor", {"input", "dtype", "copy_data", "set_constant", "persist_output", "constant", "use_align", "align_size", "size_of_dtype", "concated_shapes", "concated_ranks"}, {"dtype"}, {}, {}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "coalesce_tensor");
}

void CoalesceTensorOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value input_, phi::DataType dtype, bool copy_data, bool set_constant, bool persist_output, float constant, bool use_align, int align_size, int size_of_dtype, const std::vector<int64_t>& concated_shapes, const std::vector<int64_t>& concated_ranks) {
  VLOG(4) << "Start build CoalesceTensorOp";



  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {input_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_dtype = paddle::dialect::DataTypeAttribute::get(pir::IrContext::Instance(), dtype);
  argument.AddAttribute("dtype", attr_dtype);
  pir::Attribute attr_copy_data = pir::BoolAttribute::get(pir::IrContext::Instance(), copy_data);
  argument.AddAttribute("copy_data", attr_copy_data);
  pir::Attribute attr_set_constant = pir::BoolAttribute::get(pir::IrContext::Instance(), set_constant);
  argument.AddAttribute("set_constant", attr_set_constant);
  pir::Attribute attr_persist_output = pir::BoolAttribute::get(pir::IrContext::Instance(), persist_output);
  argument.AddAttribute("persist_output", attr_persist_output);
  pir::Attribute attr_constant = pir::FloatAttribute::get(pir::IrContext::Instance(), constant);
  argument.AddAttribute("constant", attr_constant);
  pir::Attribute attr_use_align = pir::BoolAttribute::get(pir::IrContext::Instance(), use_align);
  argument.AddAttribute("use_align", attr_use_align);
  pir::Attribute attr_align_size = pir::Int32Attribute::get(pir::IrContext::Instance(), align_size);
  argument.AddAttribute("align_size", attr_align_size);
  pir::Attribute attr_size_of_dtype = pir::Int32Attribute::get(pir::IrContext::Instance(), size_of_dtype);
  argument.AddAttribute("size_of_dtype", attr_size_of_dtype);
  std::vector<pir::Attribute> vec_concated_shapes;
  for (size_t i = 0; i < static_cast<size_t>(concated_shapes.size()); i++) {
      pir::Attribute attr_concated_shapes = pir::Int64Attribute::get(pir::IrContext::Instance(), concated_shapes[i]);

    vec_concated_shapes.push_back(attr_concated_shapes);
  }
  pir::Attribute attr_concated_shapes = pir::ArrayAttribute::get(pir::IrContext::Instance(), vec_concated_shapes);
  argument.AddAttribute("concated_shapes", attr_concated_shapes);
  std::vector<pir::Attribute> vec_concated_ranks;
  for (size_t i = 0; i < static_cast<size_t>(concated_ranks.size()); i++) {
      pir::Attribute attr_concated_ranks = pir::Int64Attribute::get(pir::IrContext::Instance(), concated_ranks[i]);

    vec_concated_ranks.push_back(attr_concated_ranks);
  }
  pir::Attribute attr_concated_ranks = pir::ArrayAttribute::get(pir::IrContext::Instance(), vec_concated_ranks);
  argument.AddAttribute("concated_ranks", attr_concated_ranks);

  VLOG(4) << "Builder construction outputs";
  pir::VectorType input = input_.type().dyn_cast<pir::VectorType>(); (void)input;
  std::vector<paddle::dialect::IrTensor> vec_ir_tensor_input;
  for (size_t i=0; i < static_cast<size_t>(input.size()); i++) {
    vec_ir_tensor_input.push_back(paddle::dialect::IrTensor(paddle::dialect::TransToPhiDataType(input[i].dyn_cast<paddle::dialect::DenseTensorType>().dtype()),
                                                                     input[i].dyn_cast<paddle::dialect::DenseTensorType>().dims(),
                                                                     input[i].dyn_cast<paddle::dialect::DenseTensorType>().data_layout(),
                                                                     input[i].dyn_cast<paddle::dialect::DenseTensorType>().lod(),
                                                                     input[i].dyn_cast<paddle::dialect::DenseTensorType>().offset()));
  }
  std::vector<paddle::dialect::IrMetaTensor> vec_meta_input;
  for (size_t i=0; i < vec_ir_tensor_input.size(); i++) {
    vec_meta_input.push_back(paddle::dialect::IrMetaTensor(&vec_ir_tensor_input[i]));
  }

  std::vector<const phi::MetaTensor*> meta_input;
  for (size_t i=0; i < static_cast<size_t>(vec_meta_input.size()); i++) {
    meta_input.push_back(&vec_meta_input[i]);
  }
   std::vector<paddle::dialect::IrTensor> vec_dense_output((input.size()), paddle::dialect::IrTensor());
  std::vector<paddle::dialect::IrMetaTensor> vec_meta_output;
  for (size_t i=0; i < static_cast<size_t>(input.size()); i++) {
    vec_meta_output.push_back(paddle::dialect::IrMetaTensor(&vec_dense_output[i]));
  }
  std::vector<phi::MetaTensor*> meta_output;
  for (size_t i=0; i < static_cast<size_t>(vec_meta_output.size()); i++) {
    meta_output.push_back(&vec_meta_output[i]);
  }
  paddle::dialect::IrTensor dense_fused_output;
  paddle::dialect::IrMetaTensor meta_fused_output(&dense_fused_output);

  phi::CoalesceTensorInferMeta(meta_input, dtype, copy_data, set_constant, persist_output, constant, use_align, align_size, size_of_dtype, concated_shapes, concated_ranks, meta_output, &meta_fused_output);

  std::vector<pir::Type> argument_outputs;
  std::vector<pir::Type> output_types;
  for (size_t i=0; i < static_cast<size_t>(input.size()); i++) {
    output_types.push_back(paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(vec_dense_output[i].dtype()), vec_dense_output[i].dims(), vec_dense_output[i].layout(), vec_dense_output[i].lod(), vec_dense_output[i].offset()));
  }
  pir::Type output_vector_type = pir::VectorType::get(pir::IrContext::Instance(), output_types);
  argument_outputs.push_back(output_vector_type);

  pir::Type fused_output_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_fused_output.dtype()), dense_fused_output.dims(), dense_fused_output.layout(), dense_fused_output.lod(), dense_fused_output.offset());
  argument_outputs.push_back(fused_output_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void CoalesceTensorOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value input_, pir::AttributeMap attributes) {
  VLOG(4) << "Start build CoalesceTensorOp";


  IR_ENFORCE(
      attributes.find("dtype") != attributes.end(),
          "'dtype' Attribute is expected for CoalesceTensorOp. ");
  phi::DataType dtype = attributes.at("dtype").dyn_cast<paddle::dialect::DataTypeAttribute>().data();

  IR_ENFORCE(
      attributes.find("copy_data") != attributes.end(),
          "'copy_data' Attribute is expected for CoalesceTensorOp. ");
  bool copy_data = attributes.at("copy_data").dyn_cast<pir::BoolAttribute>().data();

  IR_ENFORCE(
      attributes.find("set_constant") != attributes.end(),
          "'set_constant' Attribute is expected for CoalesceTensorOp. ");
  bool set_constant = attributes.at("set_constant").dyn_cast<pir::BoolAttribute>().data();

  IR_ENFORCE(
      attributes.find("persist_output") != attributes.end(),
          "'persist_output' Attribute is expected for CoalesceTensorOp. ");
  bool persist_output = attributes.at("persist_output").dyn_cast<pir::BoolAttribute>().data();

  IR_ENFORCE(
      attributes.find("constant") != attributes.end(),
          "'constant' Attribute is expected for CoalesceTensorOp. ");
  float constant = attributes.at("constant").dyn_cast<pir::FloatAttribute>().data();

  IR_ENFORCE(
      attributes.find("use_align") != attributes.end(),
          "'use_align' Attribute is expected for CoalesceTensorOp. ");
  bool use_align = attributes.at("use_align").dyn_cast<pir::BoolAttribute>().data();

  IR_ENFORCE(
      attributes.find("align_size") != attributes.end(),
          "'align_size' Attribute is expected for CoalesceTensorOp. ");
  int align_size = attributes.at("align_size").dyn_cast<pir::Int32Attribute>().data();

  IR_ENFORCE(
      attributes.find("size_of_dtype") != attributes.end(),
          "'size_of_dtype' Attribute is expected for CoalesceTensorOp. ");
  int size_of_dtype = attributes.at("size_of_dtype").dyn_cast<pir::Int32Attribute>().data();

  IR_ENFORCE(
      attributes.find("concated_shapes") != attributes.end(),
          "'concated_shapes' Attribute is expected for CoalesceTensorOp. ");
  std::vector<int64_t> concated_shapes;
  for (size_t i = 0; i < attributes.at("concated_shapes").dyn_cast<pir::ArrayAttribute>().size(); i++) {
    concated_shapes.push_back(attributes.at("concated_shapes").dyn_cast<pir::ArrayAttribute>().at(i).dyn_cast<pir::Int64Attribute>().data());
  }

  IR_ENFORCE(
      attributes.find("concated_ranks") != attributes.end(),
          "'concated_ranks' Attribute is expected for CoalesceTensorOp. ");
  std::vector<int64_t> concated_ranks;
  for (size_t i = 0; i < attributes.at("concated_ranks").dyn_cast<pir::ArrayAttribute>().size(); i++) {
    concated_ranks.push_back(attributes.at("concated_ranks").dyn_cast<pir::ArrayAttribute>().at(i).dyn_cast<pir::Int64Attribute>().data());
  }


  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {input_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_dtype = paddle::dialect::DataTypeAttribute::get(pir::IrContext::Instance(), dtype);
  argument.AddAttribute("dtype", attr_dtype);
  pir::Attribute attr_copy_data = pir::BoolAttribute::get(pir::IrContext::Instance(), copy_data);
  argument.AddAttribute("copy_data", attr_copy_data);
  pir::Attribute attr_set_constant = pir::BoolAttribute::get(pir::IrContext::Instance(), set_constant);
  argument.AddAttribute("set_constant", attr_set_constant);
  pir::Attribute attr_persist_output = pir::BoolAttribute::get(pir::IrContext::Instance(), persist_output);
  argument.AddAttribute("persist_output", attr_persist_output);
  pir::Attribute attr_constant = pir::FloatAttribute::get(pir::IrContext::Instance(), constant);
  argument.AddAttribute("constant", attr_constant);
  pir::Attribute attr_use_align = pir::BoolAttribute::get(pir::IrContext::Instance(), use_align);
  argument.AddAttribute("use_align", attr_use_align);
  pir::Attribute attr_align_size = pir::Int32Attribute::get(pir::IrContext::Instance(), align_size);
  argument.AddAttribute("align_size", attr_align_size);
  pir::Attribute attr_size_of_dtype = pir::Int32Attribute::get(pir::IrContext::Instance(), size_of_dtype);
  argument.AddAttribute("size_of_dtype", attr_size_of_dtype);
  std::vector<pir::Attribute> vec_concated_shapes;
  for (size_t i = 0; i < static_cast<size_t>(concated_shapes.size()); i++) {
      pir::Attribute attr_concated_shapes = pir::Int64Attribute::get(pir::IrContext::Instance(), concated_shapes[i]);

    vec_concated_shapes.push_back(attr_concated_shapes);
  }
  pir::Attribute attr_concated_shapes = pir::ArrayAttribute::get(pir::IrContext::Instance(), vec_concated_shapes);
  argument.AddAttribute("concated_shapes", attr_concated_shapes);
  std::vector<pir::Attribute> vec_concated_ranks;
  for (size_t i = 0; i < static_cast<size_t>(concated_ranks.size()); i++) {
      pir::Attribute attr_concated_ranks = pir::Int64Attribute::get(pir::IrContext::Instance(), concated_ranks[i]);

    vec_concated_ranks.push_back(attr_concated_ranks);
  }
  pir::Attribute attr_concated_ranks = pir::ArrayAttribute::get(pir::IrContext::Instance(), vec_concated_ranks);
  argument.AddAttribute("concated_ranks", attr_concated_ranks);

  VLOG(4) << "Builder construction outputs";
  pir::VectorType input = input_.type().dyn_cast<pir::VectorType>(); (void)input;
  std::vector<paddle::dialect::IrTensor> vec_ir_tensor_input;
  for (size_t i=0; i < static_cast<size_t>(input.size()); i++) {
    vec_ir_tensor_input.push_back(paddle::dialect::IrTensor(paddle::dialect::TransToPhiDataType(input[i].dyn_cast<paddle::dialect::DenseTensorType>().dtype()),
                                                                     input[i].dyn_cast<paddle::dialect::DenseTensorType>().dims(),
                                                                     input[i].dyn_cast<paddle::dialect::DenseTensorType>().data_layout(),
                                                                     input[i].dyn_cast<paddle::dialect::DenseTensorType>().lod(),
                                                                     input[i].dyn_cast<paddle::dialect::DenseTensorType>().offset()));
  }
  std::vector<paddle::dialect::IrMetaTensor> vec_meta_input;
  for (size_t i=0; i < vec_ir_tensor_input.size(); i++) {
    vec_meta_input.push_back(paddle::dialect::IrMetaTensor(&vec_ir_tensor_input[i]));
  }

  std::vector<const phi::MetaTensor*> meta_input;
  for (size_t i=0; i < static_cast<size_t>(vec_meta_input.size()); i++) {
    meta_input.push_back(&vec_meta_input[i]);
  }
   std::vector<paddle::dialect::IrTensor> vec_dense_output((input.size()), paddle::dialect::IrTensor());
  std::vector<paddle::dialect::IrMetaTensor> vec_meta_output;
  for (size_t i=0; i < static_cast<size_t>(input.size()); i++) {
    vec_meta_output.push_back(paddle::dialect::IrMetaTensor(&vec_dense_output[i]));
  }
  std::vector<phi::MetaTensor*> meta_output;
  for (size_t i=0; i < static_cast<size_t>(vec_meta_output.size()); i++) {
    meta_output.push_back(&vec_meta_output[i]);
  }
  paddle::dialect::IrTensor dense_fused_output;
  paddle::dialect::IrMetaTensor meta_fused_output(&dense_fused_output);

  phi::CoalesceTensorInferMeta(meta_input, dtype, copy_data, set_constant, persist_output, constant, use_align, align_size, size_of_dtype, concated_shapes, concated_ranks, meta_output, &meta_fused_output);

  std::vector<pir::Type> argument_outputs;
  std::vector<pir::Type> output_types;
  for (size_t i=0; i < static_cast<size_t>(input.size()); i++) {
    output_types.push_back(paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(vec_dense_output[i].dtype()), vec_dense_output[i].dims(), vec_dense_output[i].layout(), vec_dense_output[i].lod(), vec_dense_output[i].offset()));
  }
  pir::Type output_vector_type = pir::VectorType::get(pir::IrContext::Instance(), output_types);
  argument_outputs.push_back(output_vector_type);

  pir::Type fused_output_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_fused_output.dtype()), dense_fused_output.dims(), dense_fused_output.layout(), dense_fused_output.lod(), dense_fused_output.offset());
  argument_outputs.push_back(fused_output_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void CoalesceTensorOp::VerifySig() {
  VLOG(4) << "Start Verifying inputs, outputs and attributes for: CoalesceTensorOp.";
  VLOG(4) << "Verifying inputs:";
  {
  auto input_size = num_operands();
  IR_ENFORCE(input_size == 1u,
                    "The size %d of inputs must be equal to 1.", input_size);
  if (auto vec_type = (*this)->operand_source(0).type().dyn_cast<pir::VectorType>()) {
      for (size_t i = 0; i < vec_type.size(); ++i) {
        IR_ENFORCE(vec_type[i].isa<paddle::dialect::DenseTensorType>(),
                       "Type validation failed for the 0th input, got %s.", (*this)->operand_source(0).type());
      }
  }
  else {
    IR_ENFORCE((*this)->operand_source(0).type().isa<paddle::dialect::DenseTensorType>(),
                   "Type validation failed for the 0th input, got %s.", (*this)->operand_source(0).type());
  }
  }
  VLOG(4) << "Verifying attributes:";
  {
  auto& attributes = this->attributes();
  IR_ENFORCE(attributes.count("dtype")>0,
                 "dtype does not exist.");
  IR_ENFORCE(attributes.at("dtype").isa<paddle::dialect::DataTypeAttribute>(),
                 "Type of attribute: dtype is not paddle::dialect::DataTypeAttribute.");

  IR_ENFORCE(attributes.count("copy_data")>0,
                 "copy_data does not exist.");
  IR_ENFORCE(attributes.at("copy_data").isa<pir::BoolAttribute>(),
                 "Type of attribute: copy_data is not pir::BoolAttribute.");

  IR_ENFORCE(attributes.count("set_constant")>0,
                 "set_constant does not exist.");
  IR_ENFORCE(attributes.at("set_constant").isa<pir::BoolAttribute>(),
                 "Type of attribute: set_constant is not pir::BoolAttribute.");

  IR_ENFORCE(attributes.count("persist_output")>0,
                 "persist_output does not exist.");
  IR_ENFORCE(attributes.at("persist_output").isa<pir::BoolAttribute>(),
                 "Type of attribute: persist_output is not pir::BoolAttribute.");

  IR_ENFORCE(attributes.count("constant")>0,
                 "constant does not exist.");
  IR_ENFORCE(attributes.at("constant").isa<pir::FloatAttribute>(),
                 "Type of attribute: constant is not pir::FloatAttribute.");

  IR_ENFORCE(attributes.count("use_align")>0,
                 "use_align does not exist.");
  IR_ENFORCE(attributes.at("use_align").isa<pir::BoolAttribute>(),
                 "Type of attribute: use_align is not pir::BoolAttribute.");

  IR_ENFORCE(attributes.count("align_size")>0,
                 "align_size does not exist.");
  IR_ENFORCE(attributes.at("align_size").isa<pir::Int32Attribute>(),
                 "Type of attribute: align_size is not pir::Int32Attribute.");

  IR_ENFORCE(attributes.count("size_of_dtype")>0,
                 "size_of_dtype does not exist.");
  IR_ENFORCE(attributes.at("size_of_dtype").isa<pir::Int32Attribute>(),
                 "Type of attribute: size_of_dtype is not pir::Int32Attribute.");

  IR_ENFORCE(attributes.count("concated_shapes")>0,
                 "concated_shapes does not exist.");
  IR_ENFORCE(attributes.at("concated_shapes").isa<pir::ArrayAttribute>(),
                 "Type of attribute: concated_shapes is not pir::ArrayAttribute.");
  for (size_t i = 0; i < attributes.at("concated_shapes").dyn_cast<pir::ArrayAttribute>().size(); i++) {
    IR_ENFORCE(attributes.at("concated_shapes").dyn_cast<pir::ArrayAttribute>().at(i).isa<pir::Int64Attribute>(),
                   "Type of attribute: concated_shapes is not right.");
  }
  IR_ENFORCE(attributes.count("concated_ranks")>0,
                 "concated_ranks does not exist.");
  IR_ENFORCE(attributes.at("concated_ranks").isa<pir::ArrayAttribute>(),
                 "Type of attribute: concated_ranks is not pir::ArrayAttribute.");
  for (size_t i = 0; i < attributes.at("concated_ranks").dyn_cast<pir::ArrayAttribute>().size(); i++) {
    IR_ENFORCE(attributes.at("concated_ranks").dyn_cast<pir::ArrayAttribute>().at(i).isa<pir::Int64Attribute>(),
                   "Type of attribute: concated_ranks is not right.");
  }
  }
  VLOG(4) << "Verifying outputs:";
  {
  auto output_size = num_results();
  IR_ENFORCE(output_size == 2u,
                    "The size %d of outputs must be equal to 2.", output_size);
  auto output_0_type = (*this)->result(0).type();
  if (auto vec_type = output_0_type.dyn_cast<pir::VectorType>()) {
    for (size_t i = 0; i < vec_type.size(); i++) {
      IR_ENFORCE(vec_type[i].isa<paddle::dialect::DenseTensorType>(),
                     "Type validation failed for the 0th output.");
    }
  }
  else {
    IR_ENFORCE(output_0_type.isa<paddle::dialect::DenseTensorType>(),
                   "Type validation failed for the 0th output.");
  }
  IR_ENFORCE((*this)->result(1).type().isa<paddle::dialect::DenseTensorType>(),
                 "Type validation failed for the 1th output.");
  }
  VLOG(4) << "End Verifying for: CoalesceTensorOp.";
}

void CoalesceTensorOp::InferMeta( phi::InferMetaContext *infer_meta ) {
  auto fn = PD_INFER_META(phi::CoalesceTensorInferMeta);
  fn(infer_meta);
}

phi::DataType CoalesceTensorOp::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: CoalesceTensorOp";
  


  return expected_kernel_dtype;
}

OpInfoTuple ComplexOp::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("real", "paddle::dialect::DenseTensorType", false, false, false, true), paddle::dialect::OpInputInfo("imag", "paddle::dialect::DenseTensorType", false, false, false, true) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = {  };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("out", "paddle::dialect::DenseTensorType", false, false) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("ComplexInferMeta", {"real", "imag"}, "complex", {"real", "imag"}, {"real"}, {}, {}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "complex");
}

void ComplexOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value real_, pir::Value imag_) {
  VLOG(4) << "Start build ComplexOp";



  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {real_, imag_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType real = real_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)real;
  paddle::dialect::DenseTensorType imag = imag_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)imag;

  VLOG(4) << "Builder construction  dense_real";
  paddle::dialect::IrTensor ir_tensor_real(paddle::dialect::TransToPhiDataType(real.dtype()),
                                                      real.dims(),
                                                      real.data_layout(),
                                                      real.lod(),
                                                      real.offset());
  VLOG(4) << "Builder construction  meta_real";
  paddle::dialect::IrMetaTensor meta_real(&ir_tensor_real);

  VLOG(4) << "Builder construction  dense_imag";
  paddle::dialect::IrTensor ir_tensor_imag(paddle::dialect::TransToPhiDataType(imag.dtype()),
                                                      imag.dims(),
                                                      imag.data_layout(),
                                                      imag.lod(),
                                                      imag.offset());
  VLOG(4) << "Builder construction  meta_imag";
  paddle::dialect::IrMetaTensor meta_imag(&ir_tensor_imag);
  paddle::dialect::IrTensor dense_out;
  paddle::dialect::IrMetaTensor meta_out(&dense_out);

  phi::ComplexInferMeta(meta_real, meta_imag, &meta_out);

  std::vector<pir::Type> argument_outputs;
  pir::Type out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_out.dtype()), dense_out.dims(), dense_out.layout(), dense_out.lod(), dense_out.offset());
  argument_outputs.push_back(out_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void ComplexOp::VerifySig() {
  VLOG(4) << "Start Verifying inputs, outputs and attributes for: ComplexOp.";
  VLOG(4) << "Verifying inputs:";
  {
  auto input_size = num_operands();
  IR_ENFORCE(input_size == 2u,
                    "The size %d of inputs must be equal to 2.", input_size);
  IR_ENFORCE((*this)->operand_source(0).type().isa<paddle::dialect::DenseTensorType>(),
                  "Type validation failed for the 0th input, got %s.", (*this)->operand_source(0).type());
  IR_ENFORCE((*this)->operand_source(1).type().isa<paddle::dialect::DenseTensorType>(),
                  "Type validation failed for the 1th input, got %s.", (*this)->operand_source(1).type());
  }
  VLOG(4) << "Verifying attributes:";
  {
  // Attributes num is 0, not need to check attributes type.
  }
  VLOG(4) << "Verifying outputs:";
  {
  auto output_size = num_results();
  IR_ENFORCE(output_size == 1u,
                    "The size %d of outputs must be equal to 1.", output_size);
  IR_ENFORCE((*this)->result(0).type().isa<paddle::dialect::DenseTensorType>(),
                 "Type validation failed for the 0th output.");
  }
  VLOG(4) << "End Verifying for: ComplexOp.";
}

void ComplexOp::InferMeta( phi::InferMetaContext *infer_meta ) {
  auto fn = PD_INFER_META(phi::ComplexInferMeta);
  fn(infer_meta);
}

phi::DataType ComplexOp::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: ComplexOp";
  


  return expected_kernel_dtype;
}

OpInfoTuple ConcatOp::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("x", "pir::VectorType<paddle::dialect::DenseTensorType>", false, false, false, true), paddle::dialect::OpInputInfo("axis", "paddle::dialect::ScalarAttribute", false, false, true, false) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = {  };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("out", "paddle::dialect::DenseTensorType", false, false) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("ConcatInferMeta", {"x", "axis"}, "concat", {"x", "axis"}, {"x"}, {}, {}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "concat");
}

void ConcatOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, int axis) {
  VLOG(4) << "Start build ConcatOp";


  // Generate scalar mutable attribute: axis
  paddle::dialect::FullOp full_axis_op = builder.Build<paddle::dialect::FullOp>(std::vector<int64_t>{1}, axis, phi::DataType::INT32, phi::CPUPlace());
  pir::OpResult axis_ = full_axis_op->result(0);
    
  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_, axis_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";

  VLOG(4) << "Builder construction outputs";
  pir::VectorType x = x_.type().dyn_cast<pir::VectorType>(); (void)x;
  std::vector<paddle::dialect::IrTensor> vec_ir_tensor_x;
  for (size_t i=0; i < static_cast<size_t>(x.size()); i++) {
    vec_ir_tensor_x.push_back(paddle::dialect::IrTensor(paddle::dialect::TransToPhiDataType(x[i].dyn_cast<paddle::dialect::DenseTensorType>().dtype()),
                                                                     x[i].dyn_cast<paddle::dialect::DenseTensorType>().dims(),
                                                                     x[i].dyn_cast<paddle::dialect::DenseTensorType>().data_layout(),
                                                                     x[i].dyn_cast<paddle::dialect::DenseTensorType>().lod(),
                                                                     x[i].dyn_cast<paddle::dialect::DenseTensorType>().offset()));
  }
  std::vector<paddle::dialect::IrMetaTensor> vec_meta_x;
  for (size_t i=0; i < vec_ir_tensor_x.size(); i++) {
    vec_meta_x.push_back(paddle::dialect::IrMetaTensor(&vec_ir_tensor_x[i]));
  }

  std::vector<const phi::MetaTensor*> meta_x;
  for (size_t i=0; i < static_cast<size_t>(vec_meta_x.size()); i++) {
    meta_x.push_back(&vec_meta_x[i]);
  }
   paddle::dialect::IrTensor dense_out;
  paddle::dialect::IrMetaTensor meta_out(&dense_out);

  phi::ConcatInferMeta(meta_x, axis, &meta_out, phi::MetaConfig(false, false));

  std::vector<pir::Type> argument_outputs;
  pir::Type out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_out.dtype()), dense_out.dims(), dense_out.layout(), dense_out.lod(), dense_out.offset());
  argument_outputs.push_back(out_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void ConcatOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, pir::AttributeMap attributes) {
  VLOG(4) << "Start build ConcatOp";


  IR_ENFORCE(
      attributes.find("axis") != attributes.end(),
          "'axis' Attribute is expected for ConcatOp. ");
  int axis = attributes.at("axis").dyn_cast<paddle::dialect::ScalarAttribute>().data().to<int>();

  // Generate scalar mutable attribute: axis
  paddle::dialect::FullOp full_axis_op = builder.Build<paddle::dialect::FullOp>(std::vector<int64_t>{1}, axis, phi::DataType::INT32, phi::CPUPlace());
  pir::OpResult axis_ = full_axis_op->result(0);
    
  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_, axis_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";

  VLOG(4) << "Builder construction outputs";
  pir::VectorType x = x_.type().dyn_cast<pir::VectorType>(); (void)x;
  std::vector<paddle::dialect::IrTensor> vec_ir_tensor_x;
  for (size_t i=0; i < static_cast<size_t>(x.size()); i++) {
    vec_ir_tensor_x.push_back(paddle::dialect::IrTensor(paddle::dialect::TransToPhiDataType(x[i].dyn_cast<paddle::dialect::DenseTensorType>().dtype()),
                                                                     x[i].dyn_cast<paddle::dialect::DenseTensorType>().dims(),
                                                                     x[i].dyn_cast<paddle::dialect::DenseTensorType>().data_layout(),
                                                                     x[i].dyn_cast<paddle::dialect::DenseTensorType>().lod(),
                                                                     x[i].dyn_cast<paddle::dialect::DenseTensorType>().offset()));
  }
  std::vector<paddle::dialect::IrMetaTensor> vec_meta_x;
  for (size_t i=0; i < vec_ir_tensor_x.size(); i++) {
    vec_meta_x.push_back(paddle::dialect::IrMetaTensor(&vec_ir_tensor_x[i]));
  }

  std::vector<const phi::MetaTensor*> meta_x;
  for (size_t i=0; i < static_cast<size_t>(vec_meta_x.size()); i++) {
    meta_x.push_back(&vec_meta_x[i]);
  }
   paddle::dialect::IrTensor dense_out;
  paddle::dialect::IrMetaTensor meta_out(&dense_out);

  phi::ConcatInferMeta(meta_x, axis, &meta_out, phi::MetaConfig(false, false));

  std::vector<pir::Type> argument_outputs;
  pir::Type out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_out.dtype()), dense_out.dims(), dense_out.layout(), dense_out.lod(), dense_out.offset());
  argument_outputs.push_back(out_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void ConcatOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, pir::Value axis_) {
  VLOG(4) << "Start build ConcatOp";



  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_, axis_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";

  VLOG(4) << "Builder construction outputs";
  pir::VectorType x = x_.type().dyn_cast<pir::VectorType>(); (void)x;
  phi::Scalar axis;
  if (axis_.dyn_cast<pir::OpResult>() && axis_.dyn_cast<pir::OpResult>().owner()->isa<paddle::dialect::FullOp>()) {
    axis = std::move(phi::Scalar(axis_.dyn_cast<pir::OpResult>().owner()
                                  ->dyn_cast<paddle::dialect::FullOp>()
                                  .attribute("value")
                                  .dyn_cast<paddle::dialect::ScalarAttribute>()
                                  .data()
                                  .to<int>()));
  }
  else {
    axis = std::move(phi::Scalar(-1));
    axis.SetFromTensor(true);
  }

  std::vector<paddle::dialect::IrTensor> vec_ir_tensor_x;
  for (size_t i=0; i < static_cast<size_t>(x.size()); i++) {
    vec_ir_tensor_x.push_back(paddle::dialect::IrTensor(paddle::dialect::TransToPhiDataType(x[i].dyn_cast<paddle::dialect::DenseTensorType>().dtype()),
                                                                     x[i].dyn_cast<paddle::dialect::DenseTensorType>().dims(),
                                                                     x[i].dyn_cast<paddle::dialect::DenseTensorType>().data_layout(),
                                                                     x[i].dyn_cast<paddle::dialect::DenseTensorType>().lod(),
                                                                     x[i].dyn_cast<paddle::dialect::DenseTensorType>().offset()));
  }
  std::vector<paddle::dialect::IrMetaTensor> vec_meta_x;
  for (size_t i=0; i < vec_ir_tensor_x.size(); i++) {
    vec_meta_x.push_back(paddle::dialect::IrMetaTensor(&vec_ir_tensor_x[i]));
  }

  std::vector<const phi::MetaTensor*> meta_x;
  for (size_t i=0; i < static_cast<size_t>(vec_meta_x.size()); i++) {
    meta_x.push_back(&vec_meta_x[i]);
  }
   paddle::dialect::IrTensor dense_out;
  paddle::dialect::IrMetaTensor meta_out(&dense_out);

  phi::ConcatInferMeta(meta_x, axis, &meta_out, phi::MetaConfig(false, false));

  std::vector<pir::Type> argument_outputs;
  pir::Type out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_out.dtype()), dense_out.dims(), dense_out.layout(), dense_out.lod(), dense_out.offset());
  argument_outputs.push_back(out_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void ConcatOp::VerifySig() {
  VLOG(4) << "Start Verifying inputs, outputs and attributes for: ConcatOp.";
  VLOG(4) << "Verifying inputs:";
  {
  auto input_size = num_operands();
  IR_ENFORCE(input_size == 2u,
                    "The size %d of inputs must be equal to 2.", input_size);
  if (auto vec_type = (*this)->operand_source(0).type().dyn_cast<pir::VectorType>()) {
      for (size_t i = 0; i < vec_type.size(); ++i) {
        IR_ENFORCE(vec_type[i].isa<paddle::dialect::DenseTensorType>(),
                       "Type validation failed for the 0th input, got %s.", (*this)->operand_source(0).type());
      }
  }
  else {
    IR_ENFORCE((*this)->operand_source(0).type().isa<paddle::dialect::DenseTensorType>(),
                   "Type validation failed for the 0th input, got %s.", (*this)->operand_source(0).type());
  }
  IR_ENFORCE((*this)->operand_source(1).type().isa<paddle::dialect::DenseTensorType>(),
                  "Type validation failed for the 1th input, got %s.", (*this)->operand_source(1).type());
  }
  VLOG(4) << "Verifying attributes:";
  {
  // Attributes num is 0, not need to check attributes type.
  }
  VLOG(4) << "Verifying outputs:";
  {
  auto output_size = num_results();
  IR_ENFORCE(output_size == 1u,
                    "The size %d of outputs must be equal to 1.", output_size);
  IR_ENFORCE((*this)->result(0).type().isa<paddle::dialect::DenseTensorType>(),
                 "Type validation failed for the 0th output.");
  }
  VLOG(4) << "End Verifying for: ConcatOp.";
}

void ConcatOp::InferMeta( phi::InferMetaContext *infer_meta ) {
  auto fn = PD_INFER_META(phi::ConcatInferMeta);
  fn(infer_meta);
}

phi::DataType ConcatOp::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: ConcatOp";
  


  return expected_kernel_dtype;
}

OpInfoTuple ConjOp::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("x", "paddle::dialect::DenseTensorType", false, false, false, true) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = {  };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("out", "paddle::dialect::DenseTensorType", false, false) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("UnchangedInferMeta", {"x"}, "conj", {"x"}, {}, {}, {}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "conj");
}

void ConjOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_) {
  VLOG(4) << "Start build ConjOp";



  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);
  paddle::dialect::IrTensor dense_out;
  paddle::dialect::IrMetaTensor meta_out(&dense_out);

  phi::UnchangedInferMeta(meta_x, &meta_out);

  std::vector<pir::Type> argument_outputs;
  pir::Type out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_out.dtype()), dense_out.dims(), dense_out.layout(), dense_out.lod(), dense_out.offset());
  argument_outputs.push_back(out_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void ConjOp::VerifySig() {
  VLOG(4) << "Start Verifying inputs, outputs and attributes for: ConjOp.";
  VLOG(4) << "Verifying inputs:";
  {
  auto input_size = num_operands();
  IR_ENFORCE(input_size == 1u,
                    "The size %d of inputs must be equal to 1.", input_size);
  IR_ENFORCE((*this)->operand_source(0).type().isa<paddle::dialect::DenseTensorType>(),
                  "Type validation failed for the 0th input, got %s.", (*this)->operand_source(0).type());
  }
  VLOG(4) << "Verifying attributes:";
  {
  // Attributes num is 0, not need to check attributes type.
  }
  VLOG(4) << "Verifying outputs:";
  {
  auto output_size = num_results();
  IR_ENFORCE(output_size == 1u,
                    "The size %d of outputs must be equal to 1.", output_size);
  IR_ENFORCE((*this)->result(0).type().isa<paddle::dialect::DenseTensorType>(),
                 "Type validation failed for the 0th output.");
  }
  VLOG(4) << "End Verifying for: ConjOp.";
}

void ConjOp::InferMeta( phi::InferMetaContext *infer_meta ) {
  auto fn = PD_INFER_META(phi::UnchangedInferMeta);
  fn(infer_meta);
}

phi::DataType ConjOp::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: ConjOp";
  


  return expected_kernel_dtype;
}

const char *Conv2dOp::attributes_name[6] = { "strides", "paddings", "padding_algorithm", "dilations", "groups", "data_format" };

OpInfoTuple Conv2dOp::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("input", "paddle::dialect::DenseTensorType", false, false, false, true), paddle::dialect::OpInputInfo("filter", "paddle::dialect::DenseTensorType", false, false, false, true) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = { paddle::dialect::OpAttributeInfo("strides", "pir::ArrayAttribute<pir::Int32Attribute>", ""), paddle::dialect::OpAttributeInfo("paddings", "pir::ArrayAttribute<pir::Int32Attribute>", ""), paddle::dialect::OpAttributeInfo("padding_algorithm", "pir::StrAttribute", ""), paddle::dialect::OpAttributeInfo("dilations", "pir::ArrayAttribute<pir::Int32Attribute>", ""), paddle::dialect::OpAttributeInfo("groups", "pir::Int32Attribute", ""), paddle::dialect::OpAttributeInfo("data_format", "pir::StrAttribute", "") };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("out", "paddle::dialect::DenseTensorType", false, false) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("ConvInferMeta", {"input", "filter", "strides", "paddings", "padding_algorithm", "dilations", "groups", "data_format"}, "conv2d", {"input", "filter", "strides", "paddings", "padding_algorithm", "dilations", "groups", "data_format"}, {}, {}, {}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "conv2d");
}

void Conv2dOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value input_, pir::Value filter_, const std::vector<int>& strides, const std::vector<int>& paddings, const std::string& padding_algorithm, const std::vector<int>& dilations, int groups, const std::string& data_format) {
  VLOG(4) << "Start build Conv2dOp";



  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {input_, filter_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  std::vector<pir::Attribute> vec_strides;
  for (size_t i = 0; i < static_cast<size_t>(strides.size()); i++) {
      pir::Attribute attr_strides = pir::Int32Attribute::get(pir::IrContext::Instance(), strides[i]);

    vec_strides.push_back(attr_strides);
  }
  pir::Attribute attr_strides = pir::ArrayAttribute::get(pir::IrContext::Instance(), vec_strides);
  argument.AddAttribute("strides", attr_strides);
  std::vector<pir::Attribute> vec_paddings;
  for (size_t i = 0; i < static_cast<size_t>(paddings.size()); i++) {
      pir::Attribute attr_paddings = pir::Int32Attribute::get(pir::IrContext::Instance(), paddings[i]);

    vec_paddings.push_back(attr_paddings);
  }
  pir::Attribute attr_paddings = pir::ArrayAttribute::get(pir::IrContext::Instance(), vec_paddings);
  argument.AddAttribute("paddings", attr_paddings);
  pir::Attribute attr_padding_algorithm = pir::StrAttribute::get(pir::IrContext::Instance(), padding_algorithm);
  argument.AddAttribute("padding_algorithm", attr_padding_algorithm);
  std::vector<pir::Attribute> vec_dilations;
  for (size_t i = 0; i < static_cast<size_t>(dilations.size()); i++) {
      pir::Attribute attr_dilations = pir::Int32Attribute::get(pir::IrContext::Instance(), dilations[i]);

    vec_dilations.push_back(attr_dilations);
  }
  pir::Attribute attr_dilations = pir::ArrayAttribute::get(pir::IrContext::Instance(), vec_dilations);
  argument.AddAttribute("dilations", attr_dilations);
  pir::Attribute attr_groups = pir::Int32Attribute::get(pir::IrContext::Instance(), groups);
  argument.AddAttribute("groups", attr_groups);
  pir::Attribute attr_data_format = pir::StrAttribute::get(pir::IrContext::Instance(), data_format);
  argument.AddAttribute("data_format", attr_data_format);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType input = input_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)input;
  paddle::dialect::DenseTensorType filter = filter_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)filter;

  VLOG(4) << "Builder construction  dense_input";
  paddle::dialect::IrTensor ir_tensor_input(paddle::dialect::TransToPhiDataType(input.dtype()),
                                                      input.dims(),
                                                      input.data_layout(),
                                                      input.lod(),
                                                      input.offset());
  VLOG(4) << "Builder construction  meta_input";
  paddle::dialect::IrMetaTensor meta_input(&ir_tensor_input);

  VLOG(4) << "Builder construction  dense_filter";
  paddle::dialect::IrTensor ir_tensor_filter(paddle::dialect::TransToPhiDataType(filter.dtype()),
                                                      filter.dims(),
                                                      filter.data_layout(),
                                                      filter.lod(),
                                                      filter.offset());
  VLOG(4) << "Builder construction  meta_filter";
  paddle::dialect::IrMetaTensor meta_filter(&ir_tensor_filter);
  paddle::dialect::IrTensor dense_out;
  paddle::dialect::IrMetaTensor meta_out(&dense_out);

  phi::ConvInferMeta(meta_input, meta_filter, strides, paddings, padding_algorithm, dilations, groups, data_format, &meta_out);

  std::vector<pir::Type> argument_outputs;
  pir::Type out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_out.dtype()), dense_out.dims(), dense_out.layout(), dense_out.lod(), dense_out.offset());
  argument_outputs.push_back(out_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void Conv2dOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value input_, pir::Value filter_, pir::AttributeMap attributes) {
  VLOG(4) << "Start build Conv2dOp";


  IR_ENFORCE(
      attributes.find("strides") != attributes.end(),
          "'strides' Attribute is expected for Conv2dOp. ");
  std::vector<int> strides;
  for (size_t i = 0; i < attributes.at("strides").dyn_cast<pir::ArrayAttribute>().size(); i++) {
    strides.push_back(attributes.at("strides").dyn_cast<pir::ArrayAttribute>().at(i).dyn_cast<pir::Int32Attribute>().data());
  }

  IR_ENFORCE(
      attributes.find("paddings") != attributes.end(),
          "'paddings' Attribute is expected for Conv2dOp. ");
  std::vector<int> paddings;
  for (size_t i = 0; i < attributes.at("paddings").dyn_cast<pir::ArrayAttribute>().size(); i++) {
    paddings.push_back(attributes.at("paddings").dyn_cast<pir::ArrayAttribute>().at(i).dyn_cast<pir::Int32Attribute>().data());
  }

  IR_ENFORCE(
      attributes.find("padding_algorithm") != attributes.end(),
          "'padding_algorithm' Attribute is expected for Conv2dOp. ");
  std::string padding_algorithm = attributes.at("padding_algorithm").dyn_cast<pir::StrAttribute>().AsString();

  IR_ENFORCE(
      attributes.find("dilations") != attributes.end(),
          "'dilations' Attribute is expected for Conv2dOp. ");
  std::vector<int> dilations;
  for (size_t i = 0; i < attributes.at("dilations").dyn_cast<pir::ArrayAttribute>().size(); i++) {
    dilations.push_back(attributes.at("dilations").dyn_cast<pir::ArrayAttribute>().at(i).dyn_cast<pir::Int32Attribute>().data());
  }

  IR_ENFORCE(
      attributes.find("groups") != attributes.end(),
          "'groups' Attribute is expected for Conv2dOp. ");
  int groups = attributes.at("groups").dyn_cast<pir::Int32Attribute>().data();

  IR_ENFORCE(
      attributes.find("data_format") != attributes.end(),
          "'data_format' Attribute is expected for Conv2dOp. ");
  std::string data_format = attributes.at("data_format").dyn_cast<pir::StrAttribute>().AsString();


  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {input_, filter_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  std::vector<pir::Attribute> vec_strides;
  for (size_t i = 0; i < static_cast<size_t>(strides.size()); i++) {
      pir::Attribute attr_strides = pir::Int32Attribute::get(pir::IrContext::Instance(), strides[i]);

    vec_strides.push_back(attr_strides);
  }
  pir::Attribute attr_strides = pir::ArrayAttribute::get(pir::IrContext::Instance(), vec_strides);
  argument.AddAttribute("strides", attr_strides);
  std::vector<pir::Attribute> vec_paddings;
  for (size_t i = 0; i < static_cast<size_t>(paddings.size()); i++) {
      pir::Attribute attr_paddings = pir::Int32Attribute::get(pir::IrContext::Instance(), paddings[i]);

    vec_paddings.push_back(attr_paddings);
  }
  pir::Attribute attr_paddings = pir::ArrayAttribute::get(pir::IrContext::Instance(), vec_paddings);
  argument.AddAttribute("paddings", attr_paddings);
  pir::Attribute attr_padding_algorithm = pir::StrAttribute::get(pir::IrContext::Instance(), padding_algorithm);
  argument.AddAttribute("padding_algorithm", attr_padding_algorithm);
  std::vector<pir::Attribute> vec_dilations;
  for (size_t i = 0; i < static_cast<size_t>(dilations.size()); i++) {
      pir::Attribute attr_dilations = pir::Int32Attribute::get(pir::IrContext::Instance(), dilations[i]);

    vec_dilations.push_back(attr_dilations);
  }
  pir::Attribute attr_dilations = pir::ArrayAttribute::get(pir::IrContext::Instance(), vec_dilations);
  argument.AddAttribute("dilations", attr_dilations);
  pir::Attribute attr_groups = pir::Int32Attribute::get(pir::IrContext::Instance(), groups);
  argument.AddAttribute("groups", attr_groups);
  pir::Attribute attr_data_format = pir::StrAttribute::get(pir::IrContext::Instance(), data_format);
  argument.AddAttribute("data_format", attr_data_format);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType input = input_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)input;
  paddle::dialect::DenseTensorType filter = filter_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)filter;

  VLOG(4) << "Builder construction  dense_input";
  paddle::dialect::IrTensor ir_tensor_input(paddle::dialect::TransToPhiDataType(input.dtype()),
                                                      input.dims(),
                                                      input.data_layout(),
                                                      input.lod(),
                                                      input.offset());
  VLOG(4) << "Builder construction  meta_input";
  paddle::dialect::IrMetaTensor meta_input(&ir_tensor_input);

  VLOG(4) << "Builder construction  dense_filter";
  paddle::dialect::IrTensor ir_tensor_filter(paddle::dialect::TransToPhiDataType(filter.dtype()),
                                                      filter.dims(),
                                                      filter.data_layout(),
                                                      filter.lod(),
                                                      filter.offset());
  VLOG(4) << "Builder construction  meta_filter";
  paddle::dialect::IrMetaTensor meta_filter(&ir_tensor_filter);
  paddle::dialect::IrTensor dense_out;
  paddle::dialect::IrMetaTensor meta_out(&dense_out);

  phi::ConvInferMeta(meta_input, meta_filter, strides, paddings, padding_algorithm, dilations, groups, data_format, &meta_out);

  std::vector<pir::Type> argument_outputs;
  pir::Type out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_out.dtype()), dense_out.dims(), dense_out.layout(), dense_out.lod(), dense_out.offset());
  argument_outputs.push_back(out_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void Conv2dOp::VerifySig() {
  VLOG(4) << "Start Verifying inputs, outputs and attributes for: Conv2dOp.";
  VLOG(4) << "Verifying inputs:";
  {
  auto input_size = num_operands();
  IR_ENFORCE(input_size == 2u,
                    "The size %d of inputs must be equal to 2.", input_size);
  IR_ENFORCE((*this)->operand_source(0).type().isa<paddle::dialect::DenseTensorType>(),
                  "Type validation failed for the 0th input, got %s.", (*this)->operand_source(0).type());
  IR_ENFORCE((*this)->operand_source(1).type().isa<paddle::dialect::DenseTensorType>(),
                  "Type validation failed for the 1th input, got %s.", (*this)->operand_source(1).type());
  }
  VLOG(4) << "Verifying attributes:";
  {
  auto& attributes = this->attributes();
  IR_ENFORCE(attributes.count("strides")>0,
                 "strides does not exist.");
  IR_ENFORCE(attributes.at("strides").isa<pir::ArrayAttribute>(),
                 "Type of attribute: strides is not pir::ArrayAttribute.");
  for (size_t i = 0; i < attributes.at("strides").dyn_cast<pir::ArrayAttribute>().size(); i++) {
    IR_ENFORCE(attributes.at("strides").dyn_cast<pir::ArrayAttribute>().at(i).isa<pir::Int32Attribute>(),
                   "Type of attribute: strides is not right.");
  }
  IR_ENFORCE(attributes.count("paddings")>0,
                 "paddings does not exist.");
  IR_ENFORCE(attributes.at("paddings").isa<pir::ArrayAttribute>(),
                 "Type of attribute: paddings is not pir::ArrayAttribute.");
  for (size_t i = 0; i < attributes.at("paddings").dyn_cast<pir::ArrayAttribute>().size(); i++) {
    IR_ENFORCE(attributes.at("paddings").dyn_cast<pir::ArrayAttribute>().at(i).isa<pir::Int32Attribute>(),
                   "Type of attribute: paddings is not right.");
  }
  IR_ENFORCE(attributes.count("padding_algorithm")>0,
                 "padding_algorithm does not exist.");
  IR_ENFORCE(attributes.at("padding_algorithm").isa<pir::StrAttribute>(),
                 "Type of attribute: padding_algorithm is not pir::StrAttribute.");

  IR_ENFORCE(attributes.count("dilations")>0,
                 "dilations does not exist.");
  IR_ENFORCE(attributes.at("dilations").isa<pir::ArrayAttribute>(),
                 "Type of attribute: dilations is not pir::ArrayAttribute.");
  for (size_t i = 0; i < attributes.at("dilations").dyn_cast<pir::ArrayAttribute>().size(); i++) {
    IR_ENFORCE(attributes.at("dilations").dyn_cast<pir::ArrayAttribute>().at(i).isa<pir::Int32Attribute>(),
                   "Type of attribute: dilations is not right.");
  }
  IR_ENFORCE(attributes.count("groups")>0,
                 "groups does not exist.");
  IR_ENFORCE(attributes.at("groups").isa<pir::Int32Attribute>(),
                 "Type of attribute: groups is not pir::Int32Attribute.");

  IR_ENFORCE(attributes.count("data_format")>0,
                 "data_format does not exist.");
  IR_ENFORCE(attributes.at("data_format").isa<pir::StrAttribute>(),
                 "Type of attribute: data_format is not pir::StrAttribute.");

  }
  VLOG(4) << "Verifying outputs:";
  {
  auto output_size = num_results();
  IR_ENFORCE(output_size == 1u,
                    "The size %d of outputs must be equal to 1.", output_size);
  IR_ENFORCE((*this)->result(0).type().isa<paddle::dialect::DenseTensorType>(),
                 "Type validation failed for the 0th output.");
  }
  VLOG(4) << "End Verifying for: Conv2dOp.";
}

void Conv2dOp::InferMeta( phi::InferMetaContext *infer_meta ) {
  auto fn = PD_INFER_META(phi::ConvInferMeta);
  fn(infer_meta);
}

phi::DataType Conv2dOp::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: Conv2dOp";
  


  return expected_kernel_dtype;
}

const char *Conv3dOp::attributes_name[6] = { "strides", "paddings", "padding_algorithm", "groups", "dilations", "data_format" };

OpInfoTuple Conv3dOp::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("input", "paddle::dialect::DenseTensorType", false, false, false, true), paddle::dialect::OpInputInfo("filter", "paddle::dialect::DenseTensorType", false, false, false, true) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = { paddle::dialect::OpAttributeInfo("strides", "pir::ArrayAttribute<pir::Int32Attribute>", ""), paddle::dialect::OpAttributeInfo("paddings", "pir::ArrayAttribute<pir::Int32Attribute>", ""), paddle::dialect::OpAttributeInfo("padding_algorithm", "pir::StrAttribute", ""), paddle::dialect::OpAttributeInfo("groups", "pir::Int32Attribute", ""), paddle::dialect::OpAttributeInfo("dilations", "pir::ArrayAttribute<pir::Int32Attribute>", ""), paddle::dialect::OpAttributeInfo("data_format", "pir::StrAttribute", "") };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("out", "paddle::dialect::DenseTensorType", false, false) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("Conv3DInferMeta", {"input", "filter", "strides", "paddings", "padding_algorithm", "groups", "dilations", "data_format"}, "conv3d", {"input", "filter", "strides", "paddings", "padding_algorithm", "groups", "dilations", "data_format"}, {}, {}, {}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "conv3d");
}

void Conv3dOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value input_, pir::Value filter_, const std::vector<int>& strides, const std::vector<int>& paddings, const std::string& padding_algorithm, int groups, const std::vector<int>& dilations, const std::string& data_format) {
  VLOG(4) << "Start build Conv3dOp";



  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {input_, filter_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  std::vector<pir::Attribute> vec_strides;
  for (size_t i = 0; i < static_cast<size_t>(strides.size()); i++) {
      pir::Attribute attr_strides = pir::Int32Attribute::get(pir::IrContext::Instance(), strides[i]);

    vec_strides.push_back(attr_strides);
  }
  pir::Attribute attr_strides = pir::ArrayAttribute::get(pir::IrContext::Instance(), vec_strides);
  argument.AddAttribute("strides", attr_strides);
  std::vector<pir::Attribute> vec_paddings;
  for (size_t i = 0; i < static_cast<size_t>(paddings.size()); i++) {
      pir::Attribute attr_paddings = pir::Int32Attribute::get(pir::IrContext::Instance(), paddings[i]);

    vec_paddings.push_back(attr_paddings);
  }
  pir::Attribute attr_paddings = pir::ArrayAttribute::get(pir::IrContext::Instance(), vec_paddings);
  argument.AddAttribute("paddings", attr_paddings);
  pir::Attribute attr_padding_algorithm = pir::StrAttribute::get(pir::IrContext::Instance(), padding_algorithm);
  argument.AddAttribute("padding_algorithm", attr_padding_algorithm);
  pir::Attribute attr_groups = pir::Int32Attribute::get(pir::IrContext::Instance(), groups);
  argument.AddAttribute("groups", attr_groups);
  std::vector<pir::Attribute> vec_dilations;
  for (size_t i = 0; i < static_cast<size_t>(dilations.size()); i++) {
      pir::Attribute attr_dilations = pir::Int32Attribute::get(pir::IrContext::Instance(), dilations[i]);

    vec_dilations.push_back(attr_dilations);
  }
  pir::Attribute attr_dilations = pir::ArrayAttribute::get(pir::IrContext::Instance(), vec_dilations);
  argument.AddAttribute("dilations", attr_dilations);
  pir::Attribute attr_data_format = pir::StrAttribute::get(pir::IrContext::Instance(), data_format);
  argument.AddAttribute("data_format", attr_data_format);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType input = input_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)input;
  paddle::dialect::DenseTensorType filter = filter_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)filter;

  VLOG(4) << "Builder construction  dense_input";
  paddle::dialect::IrTensor ir_tensor_input(paddle::dialect::TransToPhiDataType(input.dtype()),
                                                      input.dims(),
                                                      input.data_layout(),
                                                      input.lod(),
                                                      input.offset());
  VLOG(4) << "Builder construction  meta_input";
  paddle::dialect::IrMetaTensor meta_input(&ir_tensor_input);

  VLOG(4) << "Builder construction  dense_filter";
  paddle::dialect::IrTensor ir_tensor_filter(paddle::dialect::TransToPhiDataType(filter.dtype()),
                                                      filter.dims(),
                                                      filter.data_layout(),
                                                      filter.lod(),
                                                      filter.offset());
  VLOG(4) << "Builder construction  meta_filter";
  paddle::dialect::IrMetaTensor meta_filter(&ir_tensor_filter);
  paddle::dialect::IrTensor dense_out;
  paddle::dialect::IrMetaTensor meta_out(&dense_out);

  phi::Conv3DInferMeta(meta_input, meta_filter, strides, paddings, padding_algorithm, groups, dilations, data_format, &meta_out);

  std::vector<pir::Type> argument_outputs;
  pir::Type out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_out.dtype()), dense_out.dims(), dense_out.layout(), dense_out.lod(), dense_out.offset());
  argument_outputs.push_back(out_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void Conv3dOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value input_, pir::Value filter_, pir::AttributeMap attributes) {
  VLOG(4) << "Start build Conv3dOp";


  IR_ENFORCE(
      attributes.find("strides") != attributes.end(),
          "'strides' Attribute is expected for Conv3dOp. ");
  std::vector<int> strides;
  for (size_t i = 0; i < attributes.at("strides").dyn_cast<pir::ArrayAttribute>().size(); i++) {
    strides.push_back(attributes.at("strides").dyn_cast<pir::ArrayAttribute>().at(i).dyn_cast<pir::Int32Attribute>().data());
  }

  IR_ENFORCE(
      attributes.find("paddings") != attributes.end(),
          "'paddings' Attribute is expected for Conv3dOp. ");
  std::vector<int> paddings;
  for (size_t i = 0; i < attributes.at("paddings").dyn_cast<pir::ArrayAttribute>().size(); i++) {
    paddings.push_back(attributes.at("paddings").dyn_cast<pir::ArrayAttribute>().at(i).dyn_cast<pir::Int32Attribute>().data());
  }

  IR_ENFORCE(
      attributes.find("padding_algorithm") != attributes.end(),
          "'padding_algorithm' Attribute is expected for Conv3dOp. ");
  std::string padding_algorithm = attributes.at("padding_algorithm").dyn_cast<pir::StrAttribute>().AsString();

  IR_ENFORCE(
      attributes.find("groups") != attributes.end(),
          "'groups' Attribute is expected for Conv3dOp. ");
  int groups = attributes.at("groups").dyn_cast<pir::Int32Attribute>().data();

  IR_ENFORCE(
      attributes.find("dilations") != attributes.end(),
          "'dilations' Attribute is expected for Conv3dOp. ");
  std::vector<int> dilations;
  for (size_t i = 0; i < attributes.at("dilations").dyn_cast<pir::ArrayAttribute>().size(); i++) {
    dilations.push_back(attributes.at("dilations").dyn_cast<pir::ArrayAttribute>().at(i).dyn_cast<pir::Int32Attribute>().data());
  }

  IR_ENFORCE(
      attributes.find("data_format") != attributes.end(),
          "'data_format' Attribute is expected for Conv3dOp. ");
  std::string data_format = attributes.at("data_format").dyn_cast<pir::StrAttribute>().AsString();


  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {input_, filter_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  std::vector<pir::Attribute> vec_strides;
  for (size_t i = 0; i < static_cast<size_t>(strides.size()); i++) {
      pir::Attribute attr_strides = pir::Int32Attribute::get(pir::IrContext::Instance(), strides[i]);

    vec_strides.push_back(attr_strides);
  }
  pir::Attribute attr_strides = pir::ArrayAttribute::get(pir::IrContext::Instance(), vec_strides);
  argument.AddAttribute("strides", attr_strides);
  std::vector<pir::Attribute> vec_paddings;
  for (size_t i = 0; i < static_cast<size_t>(paddings.size()); i++) {
      pir::Attribute attr_paddings = pir::Int32Attribute::get(pir::IrContext::Instance(), paddings[i]);

    vec_paddings.push_back(attr_paddings);
  }
  pir::Attribute attr_paddings = pir::ArrayAttribute::get(pir::IrContext::Instance(), vec_paddings);
  argument.AddAttribute("paddings", attr_paddings);
  pir::Attribute attr_padding_algorithm = pir::StrAttribute::get(pir::IrContext::Instance(), padding_algorithm);
  argument.AddAttribute("padding_algorithm", attr_padding_algorithm);
  pir::Attribute attr_groups = pir::Int32Attribute::get(pir::IrContext::Instance(), groups);
  argument.AddAttribute("groups", attr_groups);
  std::vector<pir::Attribute> vec_dilations;
  for (size_t i = 0; i < static_cast<size_t>(dilations.size()); i++) {
      pir::Attribute attr_dilations = pir::Int32Attribute::get(pir::IrContext::Instance(), dilations[i]);

    vec_dilations.push_back(attr_dilations);
  }
  pir::Attribute attr_dilations = pir::ArrayAttribute::get(pir::IrContext::Instance(), vec_dilations);
  argument.AddAttribute("dilations", attr_dilations);
  pir::Attribute attr_data_format = pir::StrAttribute::get(pir::IrContext::Instance(), data_format);
  argument.AddAttribute("data_format", attr_data_format);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType input = input_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)input;
  paddle::dialect::DenseTensorType filter = filter_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)filter;

  VLOG(4) << "Builder construction  dense_input";
  paddle::dialect::IrTensor ir_tensor_input(paddle::dialect::TransToPhiDataType(input.dtype()),
                                                      input.dims(),
                                                      input.data_layout(),
                                                      input.lod(),
                                                      input.offset());
  VLOG(4) << "Builder construction  meta_input";
  paddle::dialect::IrMetaTensor meta_input(&ir_tensor_input);

  VLOG(4) << "Builder construction  dense_filter";
  paddle::dialect::IrTensor ir_tensor_filter(paddle::dialect::TransToPhiDataType(filter.dtype()),
                                                      filter.dims(),
                                                      filter.data_layout(),
                                                      filter.lod(),
                                                      filter.offset());
  VLOG(4) << "Builder construction  meta_filter";
  paddle::dialect::IrMetaTensor meta_filter(&ir_tensor_filter);
  paddle::dialect::IrTensor dense_out;
  paddle::dialect::IrMetaTensor meta_out(&dense_out);

  phi::Conv3DInferMeta(meta_input, meta_filter, strides, paddings, padding_algorithm, groups, dilations, data_format, &meta_out);

  std::vector<pir::Type> argument_outputs;
  pir::Type out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_out.dtype()), dense_out.dims(), dense_out.layout(), dense_out.lod(), dense_out.offset());
  argument_outputs.push_back(out_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void Conv3dOp::VerifySig() {
  VLOG(4) << "Start Verifying inputs, outputs and attributes for: Conv3dOp.";
  VLOG(4) << "Verifying inputs:";
  {
  auto input_size = num_operands();
  IR_ENFORCE(input_size == 2u,
                    "The size %d of inputs must be equal to 2.", input_size);
  IR_ENFORCE((*this)->operand_source(0).type().isa<paddle::dialect::DenseTensorType>(),
                  "Type validation failed for the 0th input, got %s.", (*this)->operand_source(0).type());
  IR_ENFORCE((*this)->operand_source(1).type().isa<paddle::dialect::DenseTensorType>(),
                  "Type validation failed for the 1th input, got %s.", (*this)->operand_source(1).type());
  }
  VLOG(4) << "Verifying attributes:";
  {
  auto& attributes = this->attributes();
  IR_ENFORCE(attributes.count("strides")>0,
                 "strides does not exist.");
  IR_ENFORCE(attributes.at("strides").isa<pir::ArrayAttribute>(),
                 "Type of attribute: strides is not pir::ArrayAttribute.");
  for (size_t i = 0; i < attributes.at("strides").dyn_cast<pir::ArrayAttribute>().size(); i++) {
    IR_ENFORCE(attributes.at("strides").dyn_cast<pir::ArrayAttribute>().at(i).isa<pir::Int32Attribute>(),
                   "Type of attribute: strides is not right.");
  }
  IR_ENFORCE(attributes.count("paddings")>0,
                 "paddings does not exist.");
  IR_ENFORCE(attributes.at("paddings").isa<pir::ArrayAttribute>(),
                 "Type of attribute: paddings is not pir::ArrayAttribute.");
  for (size_t i = 0; i < attributes.at("paddings").dyn_cast<pir::ArrayAttribute>().size(); i++) {
    IR_ENFORCE(attributes.at("paddings").dyn_cast<pir::ArrayAttribute>().at(i).isa<pir::Int32Attribute>(),
                   "Type of attribute: paddings is not right.");
  }
  IR_ENFORCE(attributes.count("padding_algorithm")>0,
                 "padding_algorithm does not exist.");
  IR_ENFORCE(attributes.at("padding_algorithm").isa<pir::StrAttribute>(),
                 "Type of attribute: padding_algorithm is not pir::StrAttribute.");

  IR_ENFORCE(attributes.count("groups")>0,
                 "groups does not exist.");
  IR_ENFORCE(attributes.at("groups").isa<pir::Int32Attribute>(),
                 "Type of attribute: groups is not pir::Int32Attribute.");

  IR_ENFORCE(attributes.count("dilations")>0,
                 "dilations does not exist.");
  IR_ENFORCE(attributes.at("dilations").isa<pir::ArrayAttribute>(),
                 "Type of attribute: dilations is not pir::ArrayAttribute.");
  for (size_t i = 0; i < attributes.at("dilations").dyn_cast<pir::ArrayAttribute>().size(); i++) {
    IR_ENFORCE(attributes.at("dilations").dyn_cast<pir::ArrayAttribute>().at(i).isa<pir::Int32Attribute>(),
                   "Type of attribute: dilations is not right.");
  }
  IR_ENFORCE(attributes.count("data_format")>0,
                 "data_format does not exist.");
  IR_ENFORCE(attributes.at("data_format").isa<pir::StrAttribute>(),
                 "Type of attribute: data_format is not pir::StrAttribute.");

  }
  VLOG(4) << "Verifying outputs:";
  {
  auto output_size = num_results();
  IR_ENFORCE(output_size == 1u,
                    "The size %d of outputs must be equal to 1.", output_size);
  IR_ENFORCE((*this)->result(0).type().isa<paddle::dialect::DenseTensorType>(),
                 "Type validation failed for the 0th output.");
  }
  VLOG(4) << "End Verifying for: Conv3dOp.";
}

void Conv3dOp::InferMeta( phi::InferMetaContext *infer_meta ) {
  auto fn = PD_INFER_META(phi::Conv3DInferMeta);
  fn(infer_meta);
}

phi::DataType Conv3dOp::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: Conv3dOp";
  


  return expected_kernel_dtype;
}

const char *Conv3dTransposeOp::attributes_name[8] = { "strides", "paddings", "output_padding", "output_size", "padding_algorithm", "groups", "dilations", "data_format" };

OpInfoTuple Conv3dTransposeOp::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("x", "paddle::dialect::DenseTensorType", false, false, false, true), paddle::dialect::OpInputInfo("filter", "paddle::dialect::DenseTensorType", false, false, false, true) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = { paddle::dialect::OpAttributeInfo("strides", "pir::ArrayAttribute<pir::Int32Attribute>", ""), paddle::dialect::OpAttributeInfo("paddings", "pir::ArrayAttribute<pir::Int32Attribute>", ""), paddle::dialect::OpAttributeInfo("output_padding", "pir::ArrayAttribute<pir::Int32Attribute>", ""), paddle::dialect::OpAttributeInfo("output_size", "pir::ArrayAttribute<pir::Int32Attribute>", ""), paddle::dialect::OpAttributeInfo("padding_algorithm", "pir::StrAttribute", ""), paddle::dialect::OpAttributeInfo("groups", "pir::Int32Attribute", ""), paddle::dialect::OpAttributeInfo("dilations", "pir::ArrayAttribute<pir::Int32Attribute>", ""), paddle::dialect::OpAttributeInfo("data_format", "pir::StrAttribute", "") };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("out", "paddle::dialect::DenseTensorType", false, false) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("ConvTransposeInferMeta", {"x", "filter", "strides", "paddings", "output_padding", "output_size", "padding_algorithm", "groups", "dilations", "data_format"}, "conv3d_transpose", {"x", "filter", "strides", "paddings", "output_padding", "output_size", "padding_algorithm", "groups", "dilations", "data_format"}, {"x"}, {}, {}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "conv3d_transpose");
}

void Conv3dTransposeOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, pir::Value filter_, const std::vector<int>& strides, const std::vector<int>& paddings, const std::vector<int>& output_padding, const std::vector<int>& output_size, const std::string& padding_algorithm, int groups, const std::vector<int>& dilations, const std::string& data_format) {
  VLOG(4) << "Start build Conv3dTransposeOp";



  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_, filter_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  std::vector<pir::Attribute> vec_strides;
  for (size_t i = 0; i < static_cast<size_t>(strides.size()); i++) {
      pir::Attribute attr_strides = pir::Int32Attribute::get(pir::IrContext::Instance(), strides[i]);

    vec_strides.push_back(attr_strides);
  }
  pir::Attribute attr_strides = pir::ArrayAttribute::get(pir::IrContext::Instance(), vec_strides);
  argument.AddAttribute("strides", attr_strides);
  std::vector<pir::Attribute> vec_paddings;
  for (size_t i = 0; i < static_cast<size_t>(paddings.size()); i++) {
      pir::Attribute attr_paddings = pir::Int32Attribute::get(pir::IrContext::Instance(), paddings[i]);

    vec_paddings.push_back(attr_paddings);
  }
  pir::Attribute attr_paddings = pir::ArrayAttribute::get(pir::IrContext::Instance(), vec_paddings);
  argument.AddAttribute("paddings", attr_paddings);
  std::vector<pir::Attribute> vec_output_padding;
  for (size_t i = 0; i < static_cast<size_t>(output_padding.size()); i++) {
      pir::Attribute attr_output_padding = pir::Int32Attribute::get(pir::IrContext::Instance(), output_padding[i]);

    vec_output_padding.push_back(attr_output_padding);
  }
  pir::Attribute attr_output_padding = pir::ArrayAttribute::get(pir::IrContext::Instance(), vec_output_padding);
  argument.AddAttribute("output_padding", attr_output_padding);
  std::vector<pir::Attribute> vec_output_size;
  for (size_t i = 0; i < static_cast<size_t>(output_size.size()); i++) {
      pir::Attribute attr_output_size = pir::Int32Attribute::get(pir::IrContext::Instance(), output_size[i]);

    vec_output_size.push_back(attr_output_size);
  }
  pir::Attribute attr_output_size = pir::ArrayAttribute::get(pir::IrContext::Instance(), vec_output_size);
  argument.AddAttribute("output_size", attr_output_size);
  pir::Attribute attr_padding_algorithm = pir::StrAttribute::get(pir::IrContext::Instance(), padding_algorithm);
  argument.AddAttribute("padding_algorithm", attr_padding_algorithm);
  pir::Attribute attr_groups = pir::Int32Attribute::get(pir::IrContext::Instance(), groups);
  argument.AddAttribute("groups", attr_groups);
  std::vector<pir::Attribute> vec_dilations;
  for (size_t i = 0; i < static_cast<size_t>(dilations.size()); i++) {
      pir::Attribute attr_dilations = pir::Int32Attribute::get(pir::IrContext::Instance(), dilations[i]);

    vec_dilations.push_back(attr_dilations);
  }
  pir::Attribute attr_dilations = pir::ArrayAttribute::get(pir::IrContext::Instance(), vec_dilations);
  argument.AddAttribute("dilations", attr_dilations);
  pir::Attribute attr_data_format = pir::StrAttribute::get(pir::IrContext::Instance(), data_format);
  argument.AddAttribute("data_format", attr_data_format);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;
  paddle::dialect::DenseTensorType filter = filter_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)filter;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);

  VLOG(4) << "Builder construction  dense_filter";
  paddle::dialect::IrTensor ir_tensor_filter(paddle::dialect::TransToPhiDataType(filter.dtype()),
                                                      filter.dims(),
                                                      filter.data_layout(),
                                                      filter.lod(),
                                                      filter.offset());
  VLOG(4) << "Builder construction  meta_filter";
  paddle::dialect::IrMetaTensor meta_filter(&ir_tensor_filter);
  paddle::dialect::IrTensor dense_out;
  paddle::dialect::IrMetaTensor meta_out(&dense_out);

  phi::ConvTransposeInferMeta(meta_x, meta_filter, strides, paddings, output_padding, output_size, padding_algorithm, groups, dilations, data_format, &meta_out);

  std::vector<pir::Type> argument_outputs;
  pir::Type out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_out.dtype()), dense_out.dims(), dense_out.layout(), dense_out.lod(), dense_out.offset());
  argument_outputs.push_back(out_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void Conv3dTransposeOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, pir::Value filter_, pir::AttributeMap attributes) {
  VLOG(4) << "Start build Conv3dTransposeOp";


  IR_ENFORCE(
      attributes.find("strides") != attributes.end(),
          "'strides' Attribute is expected for Conv3dTransposeOp. ");
  std::vector<int> strides;
  for (size_t i = 0; i < attributes.at("strides").dyn_cast<pir::ArrayAttribute>().size(); i++) {
    strides.push_back(attributes.at("strides").dyn_cast<pir::ArrayAttribute>().at(i).dyn_cast<pir::Int32Attribute>().data());
  }

  IR_ENFORCE(
      attributes.find("paddings") != attributes.end(),
          "'paddings' Attribute is expected for Conv3dTransposeOp. ");
  std::vector<int> paddings;
  for (size_t i = 0; i < attributes.at("paddings").dyn_cast<pir::ArrayAttribute>().size(); i++) {
    paddings.push_back(attributes.at("paddings").dyn_cast<pir::ArrayAttribute>().at(i).dyn_cast<pir::Int32Attribute>().data());
  }

  IR_ENFORCE(
      attributes.find("output_padding") != attributes.end(),
          "'output_padding' Attribute is expected for Conv3dTransposeOp. ");
  std::vector<int> output_padding;
  for (size_t i = 0; i < attributes.at("output_padding").dyn_cast<pir::ArrayAttribute>().size(); i++) {
    output_padding.push_back(attributes.at("output_padding").dyn_cast<pir::ArrayAttribute>().at(i).dyn_cast<pir::Int32Attribute>().data());
  }

  IR_ENFORCE(
      attributes.find("output_size") != attributes.end(),
          "'output_size' Attribute is expected for Conv3dTransposeOp. ");
  std::vector<int> output_size;
  for (size_t i = 0; i < attributes.at("output_size").dyn_cast<pir::ArrayAttribute>().size(); i++) {
    output_size.push_back(attributes.at("output_size").dyn_cast<pir::ArrayAttribute>().at(i).dyn_cast<pir::Int32Attribute>().data());
  }

  IR_ENFORCE(
      attributes.find("padding_algorithm") != attributes.end(),
          "'padding_algorithm' Attribute is expected for Conv3dTransposeOp. ");
  std::string padding_algorithm = attributes.at("padding_algorithm").dyn_cast<pir::StrAttribute>().AsString();

  IR_ENFORCE(
      attributes.find("groups") != attributes.end(),
          "'groups' Attribute is expected for Conv3dTransposeOp. ");
  int groups = attributes.at("groups").dyn_cast<pir::Int32Attribute>().data();

  IR_ENFORCE(
      attributes.find("dilations") != attributes.end(),
          "'dilations' Attribute is expected for Conv3dTransposeOp. ");
  std::vector<int> dilations;
  for (size_t i = 0; i < attributes.at("dilations").dyn_cast<pir::ArrayAttribute>().size(); i++) {
    dilations.push_back(attributes.at("dilations").dyn_cast<pir::ArrayAttribute>().at(i).dyn_cast<pir::Int32Attribute>().data());
  }

  IR_ENFORCE(
      attributes.find("data_format") != attributes.end(),
          "'data_format' Attribute is expected for Conv3dTransposeOp. ");
  std::string data_format = attributes.at("data_format").dyn_cast<pir::StrAttribute>().AsString();


  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_, filter_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  std::vector<pir::Attribute> vec_strides;
  for (size_t i = 0; i < static_cast<size_t>(strides.size()); i++) {
      pir::Attribute attr_strides = pir::Int32Attribute::get(pir::IrContext::Instance(), strides[i]);

    vec_strides.push_back(attr_strides);
  }
  pir::Attribute attr_strides = pir::ArrayAttribute::get(pir::IrContext::Instance(), vec_strides);
  argument.AddAttribute("strides", attr_strides);
  std::vector<pir::Attribute> vec_paddings;
  for (size_t i = 0; i < static_cast<size_t>(paddings.size()); i++) {
      pir::Attribute attr_paddings = pir::Int32Attribute::get(pir::IrContext::Instance(), paddings[i]);

    vec_paddings.push_back(attr_paddings);
  }
  pir::Attribute attr_paddings = pir::ArrayAttribute::get(pir::IrContext::Instance(), vec_paddings);
  argument.AddAttribute("paddings", attr_paddings);
  std::vector<pir::Attribute> vec_output_padding;
  for (size_t i = 0; i < static_cast<size_t>(output_padding.size()); i++) {
      pir::Attribute attr_output_padding = pir::Int32Attribute::get(pir::IrContext::Instance(), output_padding[i]);

    vec_output_padding.push_back(attr_output_padding);
  }
  pir::Attribute attr_output_padding = pir::ArrayAttribute::get(pir::IrContext::Instance(), vec_output_padding);
  argument.AddAttribute("output_padding", attr_output_padding);
  std::vector<pir::Attribute> vec_output_size;
  for (size_t i = 0; i < static_cast<size_t>(output_size.size()); i++) {
      pir::Attribute attr_output_size = pir::Int32Attribute::get(pir::IrContext::Instance(), output_size[i]);

    vec_output_size.push_back(attr_output_size);
  }
  pir::Attribute attr_output_size = pir::ArrayAttribute::get(pir::IrContext::Instance(), vec_output_size);
  argument.AddAttribute("output_size", attr_output_size);
  pir::Attribute attr_padding_algorithm = pir::StrAttribute::get(pir::IrContext::Instance(), padding_algorithm);
  argument.AddAttribute("padding_algorithm", attr_padding_algorithm);
  pir::Attribute attr_groups = pir::Int32Attribute::get(pir::IrContext::Instance(), groups);
  argument.AddAttribute("groups", attr_groups);
  std::vector<pir::Attribute> vec_dilations;
  for (size_t i = 0; i < static_cast<size_t>(dilations.size()); i++) {
      pir::Attribute attr_dilations = pir::Int32Attribute::get(pir::IrContext::Instance(), dilations[i]);

    vec_dilations.push_back(attr_dilations);
  }
  pir::Attribute attr_dilations = pir::ArrayAttribute::get(pir::IrContext::Instance(), vec_dilations);
  argument.AddAttribute("dilations", attr_dilations);
  pir::Attribute attr_data_format = pir::StrAttribute::get(pir::IrContext::Instance(), data_format);
  argument.AddAttribute("data_format", attr_data_format);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;
  paddle::dialect::DenseTensorType filter = filter_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)filter;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);

  VLOG(4) << "Builder construction  dense_filter";
  paddle::dialect::IrTensor ir_tensor_filter(paddle::dialect::TransToPhiDataType(filter.dtype()),
                                                      filter.dims(),
                                                      filter.data_layout(),
                                                      filter.lod(),
                                                      filter.offset());
  VLOG(4) << "Builder construction  meta_filter";
  paddle::dialect::IrMetaTensor meta_filter(&ir_tensor_filter);
  paddle::dialect::IrTensor dense_out;
  paddle::dialect::IrMetaTensor meta_out(&dense_out);

  phi::ConvTransposeInferMeta(meta_x, meta_filter, strides, paddings, output_padding, output_size, padding_algorithm, groups, dilations, data_format, &meta_out);

  std::vector<pir::Type> argument_outputs;
  pir::Type out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_out.dtype()), dense_out.dims(), dense_out.layout(), dense_out.lod(), dense_out.offset());
  argument_outputs.push_back(out_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void Conv3dTransposeOp::VerifySig() {
  VLOG(4) << "Start Verifying inputs, outputs and attributes for: Conv3dTransposeOp.";
  VLOG(4) << "Verifying inputs:";
  {
  auto input_size = num_operands();
  IR_ENFORCE(input_size == 2u,
                    "The size %d of inputs must be equal to 2.", input_size);
  IR_ENFORCE((*this)->operand_source(0).type().isa<paddle::dialect::DenseTensorType>(),
                  "Type validation failed for the 0th input, got %s.", (*this)->operand_source(0).type());
  IR_ENFORCE((*this)->operand_source(1).type().isa<paddle::dialect::DenseTensorType>(),
                  "Type validation failed for the 1th input, got %s.", (*this)->operand_source(1).type());
  }
  VLOG(4) << "Verifying attributes:";
  {
  auto& attributes = this->attributes();
  IR_ENFORCE(attributes.count("strides")>0,
                 "strides does not exist.");
  IR_ENFORCE(attributes.at("strides").isa<pir::ArrayAttribute>(),
                 "Type of attribute: strides is not pir::ArrayAttribute.");
  for (size_t i = 0; i < attributes.at("strides").dyn_cast<pir::ArrayAttribute>().size(); i++) {
    IR_ENFORCE(attributes.at("strides").dyn_cast<pir::ArrayAttribute>().at(i).isa<pir::Int32Attribute>(),
                   "Type of attribute: strides is not right.");
  }
  IR_ENFORCE(attributes.count("paddings")>0,
                 "paddings does not exist.");
  IR_ENFORCE(attributes.at("paddings").isa<pir::ArrayAttribute>(),
                 "Type of attribute: paddings is not pir::ArrayAttribute.");
  for (size_t i = 0; i < attributes.at("paddings").dyn_cast<pir::ArrayAttribute>().size(); i++) {
    IR_ENFORCE(attributes.at("paddings").dyn_cast<pir::ArrayAttribute>().at(i).isa<pir::Int32Attribute>(),
                   "Type of attribute: paddings is not right.");
  }
  IR_ENFORCE(attributes.count("output_padding")>0,
                 "output_padding does not exist.");
  IR_ENFORCE(attributes.at("output_padding").isa<pir::ArrayAttribute>(),
                 "Type of attribute: output_padding is not pir::ArrayAttribute.");
  for (size_t i = 0; i < attributes.at("output_padding").dyn_cast<pir::ArrayAttribute>().size(); i++) {
    IR_ENFORCE(attributes.at("output_padding").dyn_cast<pir::ArrayAttribute>().at(i).isa<pir::Int32Attribute>(),
                   "Type of attribute: output_padding is not right.");
  }
  IR_ENFORCE(attributes.count("output_size")>0,
                 "output_size does not exist.");
  IR_ENFORCE(attributes.at("output_size").isa<pir::ArrayAttribute>(),
                 "Type of attribute: output_size is not pir::ArrayAttribute.");
  for (size_t i = 0; i < attributes.at("output_size").dyn_cast<pir::ArrayAttribute>().size(); i++) {
    IR_ENFORCE(attributes.at("output_size").dyn_cast<pir::ArrayAttribute>().at(i).isa<pir::Int32Attribute>(),
                   "Type of attribute: output_size is not right.");
  }
  IR_ENFORCE(attributes.count("padding_algorithm")>0,
                 "padding_algorithm does not exist.");
  IR_ENFORCE(attributes.at("padding_algorithm").isa<pir::StrAttribute>(),
                 "Type of attribute: padding_algorithm is not pir::StrAttribute.");

  IR_ENFORCE(attributes.count("groups")>0,
                 "groups does not exist.");
  IR_ENFORCE(attributes.at("groups").isa<pir::Int32Attribute>(),
                 "Type of attribute: groups is not pir::Int32Attribute.");

  IR_ENFORCE(attributes.count("dilations")>0,
                 "dilations does not exist.");
  IR_ENFORCE(attributes.at("dilations").isa<pir::ArrayAttribute>(),
                 "Type of attribute: dilations is not pir::ArrayAttribute.");
  for (size_t i = 0; i < attributes.at("dilations").dyn_cast<pir::ArrayAttribute>().size(); i++) {
    IR_ENFORCE(attributes.at("dilations").dyn_cast<pir::ArrayAttribute>().at(i).isa<pir::Int32Attribute>(),
                   "Type of attribute: dilations is not right.");
  }
  IR_ENFORCE(attributes.count("data_format")>0,
                 "data_format does not exist.");
  IR_ENFORCE(attributes.at("data_format").isa<pir::StrAttribute>(),
                 "Type of attribute: data_format is not pir::StrAttribute.");

  }
  VLOG(4) << "Verifying outputs:";
  {
  auto output_size = num_results();
  IR_ENFORCE(output_size == 1u,
                    "The size %d of outputs must be equal to 1.", output_size);
  IR_ENFORCE((*this)->result(0).type().isa<paddle::dialect::DenseTensorType>(),
                 "Type validation failed for the 0th output.");
  }
  VLOG(4) << "End Verifying for: Conv3dTransposeOp.";
}

void Conv3dTransposeOp::InferMeta( phi::InferMetaContext *infer_meta ) {
  auto fn = PD_INFER_META(phi::ConvTransposeInferMeta);
  fn(infer_meta);
}

phi::DataType Conv3dTransposeOp::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: Conv3dTransposeOp";
  


  return expected_kernel_dtype;
}

OpInfoTuple CosOp::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("x", "paddle::dialect::DenseTensorType", false, false, false, true) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = {  };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("out", "paddle::dialect::DenseTensorType", false, false) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("UnchangedInferMeta", {"x"}, "cos", {"x"}, {}, {}, {}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "cos");
}

void CosOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_) {
  VLOG(4) << "Start build CosOp";



  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);
  paddle::dialect::IrTensor dense_out;
  paddle::dialect::IrMetaTensor meta_out(&dense_out);

  phi::UnchangedInferMeta(meta_x, &meta_out);

  std::vector<pir::Type> argument_outputs;
  pir::Type out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_out.dtype()), dense_out.dims(), dense_out.layout(), dense_out.lod(), dense_out.offset());
  argument_outputs.push_back(out_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void CosOp::VerifySig() {
  VLOG(4) << "Start Verifying inputs, outputs and attributes for: CosOp.";
  VLOG(4) << "Verifying inputs:";
  {
  auto input_size = num_operands();
  IR_ENFORCE(input_size == 1u,
                    "The size %d of inputs must be equal to 1.", input_size);
  IR_ENFORCE((*this)->operand_source(0).type().isa<paddle::dialect::DenseTensorType>(),
                  "Type validation failed for the 0th input, got %s.", (*this)->operand_source(0).type());
  }
  VLOG(4) << "Verifying attributes:";
  {
  // Attributes num is 0, not need to check attributes type.
  }
  VLOG(4) << "Verifying outputs:";
  {
  auto output_size = num_results();
  IR_ENFORCE(output_size == 1u,
                    "The size %d of outputs must be equal to 1.", output_size);
  IR_ENFORCE((*this)->result(0).type().isa<paddle::dialect::DenseTensorType>(),
                 "Type validation failed for the 0th output.");
  }
  VLOG(4) << "End Verifying for: CosOp.";
}

void CosOp::InferMeta( phi::InferMetaContext *infer_meta ) {
  auto fn = PD_INFER_META(phi::UnchangedInferMeta);
  fn(infer_meta);
}

phi::DataType CosOp::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: CosOp";
  


  return expected_kernel_dtype;
}

OpInfoTuple Cos_Op::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("x", "paddle::dialect::DenseTensorType", false, false, false, true) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = {  };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("out", "paddle::dialect::DenseTensorType", false, false) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("UnchangedInferMeta", {"x"}, "cos", {"x"}, {}, {}, {{"out", "x"}}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "cos");
}

void Cos_Op::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_) {
  VLOG(4) << "Start build Cos_Op";



  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);
  paddle::dialect::IrTensor dense_out;
  paddle::dialect::IrMetaTensor meta_out(&dense_out);

  phi::UnchangedInferMeta(meta_x, &meta_out);

  std::vector<pir::Type> argument_outputs;
  pir::Type out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_out.dtype()), dense_out.dims(), dense_out.layout(), dense_out.lod(), dense_out.offset());
  argument_outputs.push_back(out_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void Cos_Op::VerifySig() {
  VLOG(4) << "Start Verifying inputs, outputs and attributes for: Cos_Op.";
  VLOG(4) << "Verifying inputs:";
  {
  auto input_size = num_operands();
  IR_ENFORCE(input_size == 1u,
                    "The size %d of inputs must be equal to 1.", input_size);
  IR_ENFORCE((*this)->operand_source(0).type().isa<paddle::dialect::DenseTensorType>(),
                  "Type validation failed for the 0th input, got %s.", (*this)->operand_source(0).type());
  }
  VLOG(4) << "Verifying attributes:";
  {
  // Attributes num is 0, not need to check attributes type.
  }
  VLOG(4) << "Verifying outputs:";
  {
  auto output_size = num_results();
  IR_ENFORCE(output_size == 1u,
                    "The size %d of outputs must be equal to 1.", output_size);
  IR_ENFORCE((*this)->result(0).type().isa<paddle::dialect::DenseTensorType>(),
                 "Type validation failed for the 0th output.");
  }
  VLOG(4) << "End Verifying for: Cos_Op.";
}

void Cos_Op::InferMeta( phi::InferMetaContext *infer_meta ) {
  auto fn = PD_INFER_META(phi::UnchangedInferMeta);
  fn(infer_meta);
}

phi::DataType Cos_Op::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: Cos_Op";
  


  return expected_kernel_dtype;
}

OpInfoTuple CoshOp::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("x", "paddle::dialect::DenseTensorType", false, false, false, true) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = {  };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("out", "paddle::dialect::DenseTensorType", false, false) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("UnchangedInferMeta", {"x"}, "cosh", {"x"}, {}, {}, {}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "cosh");
}

void CoshOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_) {
  VLOG(4) << "Start build CoshOp";



  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);
  paddle::dialect::IrTensor dense_out;
  paddle::dialect::IrMetaTensor meta_out(&dense_out);

  phi::UnchangedInferMeta(meta_x, &meta_out);

  std::vector<pir::Type> argument_outputs;
  pir::Type out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_out.dtype()), dense_out.dims(), dense_out.layout(), dense_out.lod(), dense_out.offset());
  argument_outputs.push_back(out_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void CoshOp::VerifySig() {
  VLOG(4) << "Start Verifying inputs, outputs and attributes for: CoshOp.";
  VLOG(4) << "Verifying inputs:";
  {
  auto input_size = num_operands();
  IR_ENFORCE(input_size == 1u,
                    "The size %d of inputs must be equal to 1.", input_size);
  IR_ENFORCE((*this)->operand_source(0).type().isa<paddle::dialect::DenseTensorType>(),
                  "Type validation failed for the 0th input, got %s.", (*this)->operand_source(0).type());
  }
  VLOG(4) << "Verifying attributes:";
  {
  // Attributes num is 0, not need to check attributes type.
  }
  VLOG(4) << "Verifying outputs:";
  {
  auto output_size = num_results();
  IR_ENFORCE(output_size == 1u,
                    "The size %d of outputs must be equal to 1.", output_size);
  IR_ENFORCE((*this)->result(0).type().isa<paddle::dialect::DenseTensorType>(),
                 "Type validation failed for the 0th output.");
  }
  VLOG(4) << "End Verifying for: CoshOp.";
}

void CoshOp::InferMeta( phi::InferMetaContext *infer_meta ) {
  auto fn = PD_INFER_META(phi::UnchangedInferMeta);
  fn(infer_meta);
}

phi::DataType CoshOp::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: CoshOp";
  


  return expected_kernel_dtype;
}

OpInfoTuple Cosh_Op::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("x", "paddle::dialect::DenseTensorType", false, false, false, true) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = {  };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("out", "paddle::dialect::DenseTensorType", false, false) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("UnchangedInferMeta", {"x"}, "cosh", {"x"}, {}, {}, {{"out", "x"}}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "cosh");
}

void Cosh_Op::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_) {
  VLOG(4) << "Start build Cosh_Op";



  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);
  paddle::dialect::IrTensor dense_out;
  paddle::dialect::IrMetaTensor meta_out(&dense_out);

  phi::UnchangedInferMeta(meta_x, &meta_out);

  std::vector<pir::Type> argument_outputs;
  pir::Type out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_out.dtype()), dense_out.dims(), dense_out.layout(), dense_out.lod(), dense_out.offset());
  argument_outputs.push_back(out_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void Cosh_Op::VerifySig() {
  VLOG(4) << "Start Verifying inputs, outputs and attributes for: Cosh_Op.";
  VLOG(4) << "Verifying inputs:";
  {
  auto input_size = num_operands();
  IR_ENFORCE(input_size == 1u,
                    "The size %d of inputs must be equal to 1.", input_size);
  IR_ENFORCE((*this)->operand_source(0).type().isa<paddle::dialect::DenseTensorType>(),
                  "Type validation failed for the 0th input, got %s.", (*this)->operand_source(0).type());
  }
  VLOG(4) << "Verifying attributes:";
  {
  // Attributes num is 0, not need to check attributes type.
  }
  VLOG(4) << "Verifying outputs:";
  {
  auto output_size = num_results();
  IR_ENFORCE(output_size == 1u,
                    "The size %d of outputs must be equal to 1.", output_size);
  IR_ENFORCE((*this)->result(0).type().isa<paddle::dialect::DenseTensorType>(),
                 "Type validation failed for the 0th output.");
  }
  VLOG(4) << "End Verifying for: Cosh_Op.";
}

void Cosh_Op::InferMeta( phi::InferMetaContext *infer_meta ) {
  auto fn = PD_INFER_META(phi::UnchangedInferMeta);
  fn(infer_meta);
}

phi::DataType Cosh_Op::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: Cosh_Op";
  


  return expected_kernel_dtype;
}

OpInfoTuple CropOp::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("x", "paddle::dialect::DenseTensorType", false, false, false, true), paddle::dialect::OpInputInfo("shape", "paddle::dialect::IntArrayAttribute", false, false, true, false), paddle::dialect::OpInputInfo("offsets", "paddle::dialect::IntArrayAttribute", false, false, true, false) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = {  };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("out", "paddle::dialect::DenseTensorType", false, false) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("CropInferMeta", {"x", "shape", "offsets"}, "crop", {"x", "shape", "offsets"}, {"x"}, {}, {}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "crop");
}

void CropOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, const std::vector<int64_t>& shape, const std::vector<int64_t>& offsets) {
  VLOG(4) << "Start build CropOp";


  // Generate int_array mutable attribute: shape
  paddle::dialect::FullIntArrayOp full_shape_op = builder.Build<paddle::dialect::FullIntArrayOp>(shape, phi::DataType::INT64, phi::CPUPlace());
  pir::OpResult shape_ = full_shape_op->result(0);
      // Generate int_array mutable attribute: offsets
  paddle::dialect::FullIntArrayOp full_offsets_op = builder.Build<paddle::dialect::FullIntArrayOp>(offsets, phi::DataType::INT64, phi::CPUPlace());
  pir::OpResult offsets_ = full_offsets_op->result(0);
    
  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_, shape_, offsets_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);
  paddle::dialect::IrTensor dense_out;
  paddle::dialect::IrMetaTensor meta_out(&dense_out);

  phi::CropInferMeta(meta_x, shape, offsets, &meta_out);

  std::vector<pir::Type> argument_outputs;
  pir::Type out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_out.dtype()), dense_out.dims(), dense_out.layout(), dense_out.lod(), dense_out.offset());
  argument_outputs.push_back(out_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void CropOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, pir::AttributeMap attributes) {
  VLOG(4) << "Start build CropOp";


  IR_ENFORCE(
      attributes.find("shape") != attributes.end(),
          "'shape' Attribute is expected for CropOp. ");
  std::vector<int64_t> shape = attributes.at("shape").dyn_cast<paddle::dialect::IntArrayAttribute>().data().GetData();

  IR_ENFORCE(
      attributes.find("offsets") != attributes.end(),
          "'offsets' Attribute is expected for CropOp. ");
  std::vector<int64_t> offsets = attributes.at("offsets").dyn_cast<paddle::dialect::IntArrayAttribute>().data().GetData();

  // Generate int_array mutable attribute: shape
  paddle::dialect::FullIntArrayOp full_shape_op = builder.Build<paddle::dialect::FullIntArrayOp>(shape, phi::DataType::INT64, phi::CPUPlace());
  pir::OpResult shape_ = full_shape_op->result(0);
      // Generate int_array mutable attribute: offsets
  paddle::dialect::FullIntArrayOp full_offsets_op = builder.Build<paddle::dialect::FullIntArrayOp>(offsets, phi::DataType::INT64, phi::CPUPlace());
  pir::OpResult offsets_ = full_offsets_op->result(0);
    
  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_, shape_, offsets_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);
  paddle::dialect::IrTensor dense_out;
  paddle::dialect::IrMetaTensor meta_out(&dense_out);

  phi::CropInferMeta(meta_x, shape, offsets, &meta_out);

  std::vector<pir::Type> argument_outputs;
  pir::Type out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_out.dtype()), dense_out.dims(), dense_out.layout(), dense_out.lod(), dense_out.offset());
  argument_outputs.push_back(out_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void CropOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, pir::Value shape_, pir::Value offsets_) {
  VLOG(4) << "Start build CropOp";



  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_, shape_, offsets_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;
  phi::IntArray shape;
  if (shape_.dyn_cast<pir::OpResult>() && shape_.dyn_cast<pir::OpResult>().owner()->isa<paddle::dialect::FullIntArrayOp>()) {
    shape = std::move(phi::IntArray(paddle::dialect::GetInt64Vector(
                          shape_.dyn_cast<pir::OpResult>().owner()
                          ->dyn_cast<paddle::dialect::FullIntArrayOp>()
                          .attribute("value"))));
  } else if (shape_.type().isa<pir::VectorType>()) {
    size_t shape_size = shape_.type().dyn_cast<pir::VectorType>().size();
    shape = std::move(phi::IntArray(std::vector<int64_t>(shape_size, -1)));
    shape.SetFromTensor(true);
  } else if (shape_.type().isa<paddle::dialect::DenseTensorType>()) {
    common::DDim shape_dim = shape_.type().dyn_cast<paddle::dialect::DenseTensorType>().dims();
    size_t shape_size = common::product(shape_dim);
    if (common::contain_unknown_dim(shape_dim)) {
      shape_size = 1;
    }
    shape = std::move(phi::IntArray(std::vector<int64_t>(shape_size, -1)));
    shape.SetFromTensor(true);
  } else {
    PADDLE_THROW(phi::errors::Unimplemented("Only support VectorType or DenseTensorType"));
  }
  phi::IntArray offsets;
  if (offsets_.dyn_cast<pir::OpResult>() && offsets_.dyn_cast<pir::OpResult>().owner()->isa<paddle::dialect::FullIntArrayOp>()) {
    offsets = std::move(phi::IntArray(paddle::dialect::GetInt64Vector(
                          offsets_.dyn_cast<pir::OpResult>().owner()
                          ->dyn_cast<paddle::dialect::FullIntArrayOp>()
                          .attribute("value"))));
  } else if (offsets_.type().isa<pir::VectorType>()) {
    size_t offsets_size = offsets_.type().dyn_cast<pir::VectorType>().size();
    offsets = std::move(phi::IntArray(std::vector<int64_t>(offsets_size, -1)));
    offsets.SetFromTensor(true);
  } else if (offsets_.type().isa<paddle::dialect::DenseTensorType>()) {
    common::DDim offsets_dim = offsets_.type().dyn_cast<paddle::dialect::DenseTensorType>().dims();
    size_t offsets_size = common::product(offsets_dim);
    if (common::contain_unknown_dim(offsets_dim)) {
      offsets_size = 1;
    }
    offsets = std::move(phi::IntArray(std::vector<int64_t>(offsets_size, -1)));
    offsets.SetFromTensor(true);
  } else {
    PADDLE_THROW(phi::errors::Unimplemented("Only support VectorType or DenseTensorType"));
  }


  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);
  paddle::dialect::IrTensor dense_out;
  paddle::dialect::IrMetaTensor meta_out(&dense_out);

  phi::CropInferMeta(meta_x, shape, offsets, &meta_out);

  std::vector<pir::Type> argument_outputs;
  pir::Type out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_out.dtype()), dense_out.dims(), dense_out.layout(), dense_out.lod(), dense_out.offset());
  argument_outputs.push_back(out_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void CropOp::VerifySig() {
  VLOG(4) << "Start Verifying inputs, outputs and attributes for: CropOp.";
  VLOG(4) << "Verifying inputs:";
  {
  auto input_size = num_operands();
  IR_ENFORCE(input_size == 3u,
                    "The size %d of inputs must be equal to 3.", input_size);
  IR_ENFORCE((*this)->operand_source(0).type().isa<paddle::dialect::DenseTensorType>(),
                  "Type validation failed for the 0th input, got %s.", (*this)->operand_source(0).type());
  if (auto vec_type = (*this)->operand_source(1).type().dyn_cast<pir::VectorType>()) {
      for (size_t i = 0; i < vec_type.size(); ++i) {
        IR_ENFORCE(vec_type[i].isa<paddle::dialect::DenseTensorType>(),
                       "Type validation failed for the 1th input, got %s.", (*this)->operand_source(1).type());
      }
  }
  else {
    IR_ENFORCE((*this)->operand_source(1).type().isa<paddle::dialect::DenseTensorType>(),
                   "Type validation failed for the 1th input, got %s.", (*this)->operand_source(1).type());
  }
  if (auto vec_type = (*this)->operand_source(2).type().dyn_cast<pir::VectorType>()) {
      for (size_t i = 0; i < vec_type.size(); ++i) {
        IR_ENFORCE(vec_type[i].isa<paddle::dialect::DenseTensorType>(),
                       "Type validation failed for the 2th input, got %s.", (*this)->operand_source(2).type());
      }
  }
  else {
    IR_ENFORCE((*this)->operand_source(2).type().isa<paddle::dialect::DenseTensorType>(),
                   "Type validation failed for the 2th input, got %s.", (*this)->operand_source(2).type());
  }
  }
  VLOG(4) << "Verifying attributes:";
  {
  // Attributes num is 0, not need to check attributes type.
  }
  VLOG(4) << "Verifying outputs:";
  {
  auto output_size = num_results();
  IR_ENFORCE(output_size == 1u,
                    "The size %d of outputs must be equal to 1.", output_size);
  IR_ENFORCE((*this)->result(0).type().isa<paddle::dialect::DenseTensorType>(),
                 "Type validation failed for the 0th output.");
  }
  VLOG(4) << "End Verifying for: CropOp.";
}

void CropOp::InferMeta( phi::InferMetaContext *infer_meta ) {
  auto fn = PD_INFER_META(phi::CropInferMeta);
  fn(infer_meta);
}

phi::DataType CropOp::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: CropOp";
  


  return expected_kernel_dtype;
}

const char *CrossOp::attributes_name[1] = { "axis" };

OpInfoTuple CrossOp::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("x", "paddle::dialect::DenseTensorType", false, false, false, true), paddle::dialect::OpInputInfo("y", "paddle::dialect::DenseTensorType", false, false, false, true) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = { paddle::dialect::OpAttributeInfo("axis", "pir::Int32Attribute", "") };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("out", "paddle::dialect::DenseTensorType", false, false) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("CrossInferMeta", {"x", "y", "axis"}, "cross", {"x", "y", "axis"}, {"x"}, {}, {}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "cross");
}

void CrossOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, pir::Value y_, int axis) {
  VLOG(4) << "Start build CrossOp";



  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_, y_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_axis = pir::Int32Attribute::get(pir::IrContext::Instance(), axis);
  argument.AddAttribute("axis", attr_axis);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;
  paddle::dialect::DenseTensorType y = y_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)y;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);

  VLOG(4) << "Builder construction  dense_y";
  paddle::dialect::IrTensor ir_tensor_y(paddle::dialect::TransToPhiDataType(y.dtype()),
                                                      y.dims(),
                                                      y.data_layout(),
                                                      y.lod(),
                                                      y.offset());
  VLOG(4) << "Builder construction  meta_y";
  paddle::dialect::IrMetaTensor meta_y(&ir_tensor_y);
  paddle::dialect::IrTensor dense_out;
  paddle::dialect::IrMetaTensor meta_out(&dense_out);

  phi::CrossInferMeta(meta_x, meta_y, axis, &meta_out);

  std::vector<pir::Type> argument_outputs;
  pir::Type out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_out.dtype()), dense_out.dims(), dense_out.layout(), dense_out.lod(), dense_out.offset());
  argument_outputs.push_back(out_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void CrossOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, pir::Value y_, pir::AttributeMap attributes) {
  VLOG(4) << "Start build CrossOp";


  IR_ENFORCE(
      attributes.find("axis") != attributes.end(),
          "'axis' Attribute is expected for CrossOp. ");
  int axis = attributes.at("axis").dyn_cast<pir::Int32Attribute>().data();


  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_, y_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_axis = pir::Int32Attribute::get(pir::IrContext::Instance(), axis);
  argument.AddAttribute("axis", attr_axis);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;
  paddle::dialect::DenseTensorType y = y_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)y;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);

  VLOG(4) << "Builder construction  dense_y";
  paddle::dialect::IrTensor ir_tensor_y(paddle::dialect::TransToPhiDataType(y.dtype()),
                                                      y.dims(),
                                                      y.data_layout(),
                                                      y.lod(),
                                                      y.offset());
  VLOG(4) << "Builder construction  meta_y";
  paddle::dialect::IrMetaTensor meta_y(&ir_tensor_y);
  paddle::dialect::IrTensor dense_out;
  paddle::dialect::IrMetaTensor meta_out(&dense_out);

  phi::CrossInferMeta(meta_x, meta_y, axis, &meta_out);

  std::vector<pir::Type> argument_outputs;
  pir::Type out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_out.dtype()), dense_out.dims(), dense_out.layout(), dense_out.lod(), dense_out.offset());
  argument_outputs.push_back(out_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void CrossOp::VerifySig() {
  VLOG(4) << "Start Verifying inputs, outputs and attributes for: CrossOp.";
  VLOG(4) << "Verifying inputs:";
  {
  auto input_size = num_operands();
  IR_ENFORCE(input_size == 2u,
                    "The size %d of inputs must be equal to 2.", input_size);
  IR_ENFORCE((*this)->operand_source(0).type().isa<paddle::dialect::DenseTensorType>(),
                  "Type validation failed for the 0th input, got %s.", (*this)->operand_source(0).type());
  IR_ENFORCE((*this)->operand_source(1).type().isa<paddle::dialect::DenseTensorType>(),
                  "Type validation failed for the 1th input, got %s.", (*this)->operand_source(1).type());
  }
  VLOG(4) << "Verifying attributes:";
  {
  auto& attributes = this->attributes();
  IR_ENFORCE(attributes.count("axis")>0,
                 "axis does not exist.");
  IR_ENFORCE(attributes.at("axis").isa<pir::Int32Attribute>(),
                 "Type of attribute: axis is not pir::Int32Attribute.");

  }
  VLOG(4) << "Verifying outputs:";
  {
  auto output_size = num_results();
  IR_ENFORCE(output_size == 1u,
                    "The size %d of outputs must be equal to 1.", output_size);
  IR_ENFORCE((*this)->result(0).type().isa<paddle::dialect::DenseTensorType>(),
                 "Type validation failed for the 0th output.");
  }
  VLOG(4) << "End Verifying for: CrossOp.";
}

void CrossOp::InferMeta( phi::InferMetaContext *infer_meta ) {
  auto fn = PD_INFER_META(phi::CrossInferMeta);
  fn(infer_meta);
}

phi::DataType CrossOp::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: CrossOp";
  


  return expected_kernel_dtype;
}

const char *CrossEntropyWithSoftmaxOp::attributes_name[5] = { "soft_label", "use_softmax", "numeric_stable_mode", "ignore_index", "axis" };

OpInfoTuple CrossEntropyWithSoftmaxOp::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("input", "paddle::dialect::DenseTensorType", false, false, false, true), paddle::dialect::OpInputInfo("label", "paddle::dialect::DenseTensorType", false, false, false, false) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = { paddle::dialect::OpAttributeInfo("soft_label", "pir::BoolAttribute", ""), paddle::dialect::OpAttributeInfo("use_softmax", "pir::BoolAttribute", ""), paddle::dialect::OpAttributeInfo("numeric_stable_mode", "pir::BoolAttribute", ""), paddle::dialect::OpAttributeInfo("ignore_index", "pir::Int32Attribute", ""), paddle::dialect::OpAttributeInfo("axis", "pir::Int32Attribute", "") };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("softmax", "paddle::dialect::DenseTensorType", false, false), paddle::dialect::OpOutputInfo("loss", "paddle::dialect::DenseTensorType", false, false) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("CrossEntropyWithSoftmaxInferMeta", {"input", "label", "soft_label", "use_softmax", "numeric_stable_mode", "ignore_index", "axis"}, "cross_entropy_with_softmax", {"input", "label", "soft_label", "use_softmax", "numeric_stable_mode", "ignore_index", "axis"}, {"input"}, {}, {}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "cross_entropy_with_softmax");
}

void CrossEntropyWithSoftmaxOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value input_, pir::Value label_, bool soft_label, bool use_softmax, bool numeric_stable_mode, int ignore_index, int axis) {
  VLOG(4) << "Start build CrossEntropyWithSoftmaxOp";



  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {input_, label_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_soft_label = pir::BoolAttribute::get(pir::IrContext::Instance(), soft_label);
  argument.AddAttribute("soft_label", attr_soft_label);
  pir::Attribute attr_use_softmax = pir::BoolAttribute::get(pir::IrContext::Instance(), use_softmax);
  argument.AddAttribute("use_softmax", attr_use_softmax);
  pir::Attribute attr_numeric_stable_mode = pir::BoolAttribute::get(pir::IrContext::Instance(), numeric_stable_mode);
  argument.AddAttribute("numeric_stable_mode", attr_numeric_stable_mode);
  pir::Attribute attr_ignore_index = pir::Int32Attribute::get(pir::IrContext::Instance(), ignore_index);
  argument.AddAttribute("ignore_index", attr_ignore_index);
  pir::Attribute attr_axis = pir::Int32Attribute::get(pir::IrContext::Instance(), axis);
  argument.AddAttribute("axis", attr_axis);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType input = input_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)input;
  paddle::dialect::DenseTensorType label = label_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)label;

  VLOG(4) << "Builder construction  dense_input";
  paddle::dialect::IrTensor ir_tensor_input(paddle::dialect::TransToPhiDataType(input.dtype()),
                                                      input.dims(),
                                                      input.data_layout(),
                                                      input.lod(),
                                                      input.offset());
  VLOG(4) << "Builder construction  meta_input";
  paddle::dialect::IrMetaTensor meta_input(&ir_tensor_input);

  VLOG(4) << "Builder construction  dense_label";
  paddle::dialect::IrTensor ir_tensor_label(paddle::dialect::TransToPhiDataType(label.dtype()),
                                                      label.dims(),
                                                      label.data_layout(),
                                                      label.lod(),
                                                      label.offset());
  VLOG(4) << "Builder construction  meta_label";
  paddle::dialect::IrMetaTensor meta_label(&ir_tensor_label);
  paddle::dialect::IrTensor dense_softmax;
  paddle::dialect::IrMetaTensor meta_softmax(&dense_softmax);
  paddle::dialect::IrTensor dense_loss;
  paddle::dialect::IrMetaTensor meta_loss(&dense_loss);

  phi::CrossEntropyWithSoftmaxInferMeta(meta_input, meta_label, soft_label, use_softmax, numeric_stable_mode, ignore_index, axis, &meta_softmax, &meta_loss);

  std::vector<pir::Type> argument_outputs;
  pir::Type softmax_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_softmax.dtype()), dense_softmax.dims(), dense_softmax.layout(), dense_softmax.lod(), dense_softmax.offset());
  argument_outputs.push_back(softmax_dense_tensor_type);

  pir::Type loss_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_loss.dtype()), dense_loss.dims(), dense_loss.layout(), dense_loss.lod(), dense_loss.offset());
  argument_outputs.push_back(loss_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void CrossEntropyWithSoftmaxOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value input_, pir::Value label_, pir::AttributeMap attributes) {
  VLOG(4) << "Start build CrossEntropyWithSoftmaxOp";


  IR_ENFORCE(
      attributes.find("soft_label") != attributes.end(),
          "'soft_label' Attribute is expected for CrossEntropyWithSoftmaxOp. ");
  bool soft_label = attributes.at("soft_label").dyn_cast<pir::BoolAttribute>().data();

  IR_ENFORCE(
      attributes.find("use_softmax") != attributes.end(),
          "'use_softmax' Attribute is expected for CrossEntropyWithSoftmaxOp. ");
  bool use_softmax = attributes.at("use_softmax").dyn_cast<pir::BoolAttribute>().data();

  IR_ENFORCE(
      attributes.find("numeric_stable_mode") != attributes.end(),
          "'numeric_stable_mode' Attribute is expected for CrossEntropyWithSoftmaxOp. ");
  bool numeric_stable_mode = attributes.at("numeric_stable_mode").dyn_cast<pir::BoolAttribute>().data();

  IR_ENFORCE(
      attributes.find("ignore_index") != attributes.end(),
          "'ignore_index' Attribute is expected for CrossEntropyWithSoftmaxOp. ");
  int ignore_index = attributes.at("ignore_index").dyn_cast<pir::Int32Attribute>().data();

  IR_ENFORCE(
      attributes.find("axis") != attributes.end(),
          "'axis' Attribute is expected for CrossEntropyWithSoftmaxOp. ");
  int axis = attributes.at("axis").dyn_cast<pir::Int32Attribute>().data();


  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {input_, label_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_soft_label = pir::BoolAttribute::get(pir::IrContext::Instance(), soft_label);
  argument.AddAttribute("soft_label", attr_soft_label);
  pir::Attribute attr_use_softmax = pir::BoolAttribute::get(pir::IrContext::Instance(), use_softmax);
  argument.AddAttribute("use_softmax", attr_use_softmax);
  pir::Attribute attr_numeric_stable_mode = pir::BoolAttribute::get(pir::IrContext::Instance(), numeric_stable_mode);
  argument.AddAttribute("numeric_stable_mode", attr_numeric_stable_mode);
  pir::Attribute attr_ignore_index = pir::Int32Attribute::get(pir::IrContext::Instance(), ignore_index);
  argument.AddAttribute("ignore_index", attr_ignore_index);
  pir::Attribute attr_axis = pir::Int32Attribute::get(pir::IrContext::Instance(), axis);
  argument.AddAttribute("axis", attr_axis);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType input = input_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)input;
  paddle::dialect::DenseTensorType label = label_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)label;

  VLOG(4) << "Builder construction  dense_input";
  paddle::dialect::IrTensor ir_tensor_input(paddle::dialect::TransToPhiDataType(input.dtype()),
                                                      input.dims(),
                                                      input.data_layout(),
                                                      input.lod(),
                                                      input.offset());
  VLOG(4) << "Builder construction  meta_input";
  paddle::dialect::IrMetaTensor meta_input(&ir_tensor_input);

  VLOG(4) << "Builder construction  dense_label";
  paddle::dialect::IrTensor ir_tensor_label(paddle::dialect::TransToPhiDataType(label.dtype()),
                                                      label.dims(),
                                                      label.data_layout(),
                                                      label.lod(),
                                                      label.offset());
  VLOG(4) << "Builder construction  meta_label";
  paddle::dialect::IrMetaTensor meta_label(&ir_tensor_label);
  paddle::dialect::IrTensor dense_softmax;
  paddle::dialect::IrMetaTensor meta_softmax(&dense_softmax);
  paddle::dialect::IrTensor dense_loss;
  paddle::dialect::IrMetaTensor meta_loss(&dense_loss);

  phi::CrossEntropyWithSoftmaxInferMeta(meta_input, meta_label, soft_label, use_softmax, numeric_stable_mode, ignore_index, axis, &meta_softmax, &meta_loss);

  std::vector<pir::Type> argument_outputs;
  pir::Type softmax_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_softmax.dtype()), dense_softmax.dims(), dense_softmax.layout(), dense_softmax.lod(), dense_softmax.offset());
  argument_outputs.push_back(softmax_dense_tensor_type);

  pir::Type loss_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_loss.dtype()), dense_loss.dims(), dense_loss.layout(), dense_loss.lod(), dense_loss.offset());
  argument_outputs.push_back(loss_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void CrossEntropyWithSoftmaxOp::VerifySig() {
  VLOG(4) << "Start Verifying inputs, outputs and attributes for: CrossEntropyWithSoftmaxOp.";
  VLOG(4) << "Verifying inputs:";
  {
  auto input_size = num_operands();
  IR_ENFORCE(input_size == 2u,
                    "The size %d of inputs must be equal to 2.", input_size);
  IR_ENFORCE((*this)->operand_source(0).type().isa<paddle::dialect::DenseTensorType>(),
                  "Type validation failed for the 0th input, got %s.", (*this)->operand_source(0).type());
  IR_ENFORCE((*this)->operand_source(1).type().isa<paddle::dialect::DenseTensorType>(),
                  "Type validation failed for the 1th input, got %s.", (*this)->operand_source(1).type());
  }
  VLOG(4) << "Verifying attributes:";
  {
  auto& attributes = this->attributes();
  IR_ENFORCE(attributes.count("soft_label")>0,
                 "soft_label does not exist.");
  IR_ENFORCE(attributes.at("soft_label").isa<pir::BoolAttribute>(),
                 "Type of attribute: soft_label is not pir::BoolAttribute.");

  IR_ENFORCE(attributes.count("use_softmax")>0,
                 "use_softmax does not exist.");
  IR_ENFORCE(attributes.at("use_softmax").isa<pir::BoolAttribute>(),
                 "Type of attribute: use_softmax is not pir::BoolAttribute.");

  IR_ENFORCE(attributes.count("numeric_stable_mode")>0,
                 "numeric_stable_mode does not exist.");
  IR_ENFORCE(attributes.at("numeric_stable_mode").isa<pir::BoolAttribute>(),
                 "Type of attribute: numeric_stable_mode is not pir::BoolAttribute.");

  IR_ENFORCE(attributes.count("ignore_index")>0,
                 "ignore_index does not exist.");
  IR_ENFORCE(attributes.at("ignore_index").isa<pir::Int32Attribute>(),
                 "Type of attribute: ignore_index is not pir::Int32Attribute.");

  IR_ENFORCE(attributes.count("axis")>0,
                 "axis does not exist.");
  IR_ENFORCE(attributes.at("axis").isa<pir::Int32Attribute>(),
                 "Type of attribute: axis is not pir::Int32Attribute.");

  }
  VLOG(4) << "Verifying outputs:";
  {
  auto output_size = num_results();
  IR_ENFORCE(output_size == 2u,
                    "The size %d of outputs must be equal to 2.", output_size);
  IR_ENFORCE((*this)->result(0).type().isa<paddle::dialect::DenseTensorType>(),
                 "Type validation failed for the 0th output.");
  IR_ENFORCE((*this)->result(1).type().isa<paddle::dialect::DenseTensorType>(),
                 "Type validation failed for the 1th output.");
  }
  VLOG(4) << "End Verifying for: CrossEntropyWithSoftmaxOp.";
}

void CrossEntropyWithSoftmaxOp::InferMeta( phi::InferMetaContext *infer_meta ) {
  auto fn = PD_INFER_META(phi::CrossEntropyWithSoftmaxInferMeta);
  fn(infer_meta);
}

phi::DataType CrossEntropyWithSoftmaxOp::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: CrossEntropyWithSoftmaxOp";
  


  return expected_kernel_dtype;
}

const char *CrossEntropyWithSoftmax_Op::attributes_name[5] = { "soft_label", "use_softmax", "numeric_stable_mode", "ignore_index", "axis" };

OpInfoTuple CrossEntropyWithSoftmax_Op::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("input", "paddle::dialect::DenseTensorType", false, false, false, true), paddle::dialect::OpInputInfo("label", "paddle::dialect::DenseTensorType", false, false, false, false) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = { paddle::dialect::OpAttributeInfo("soft_label", "pir::BoolAttribute", ""), paddle::dialect::OpAttributeInfo("use_softmax", "pir::BoolAttribute", ""), paddle::dialect::OpAttributeInfo("numeric_stable_mode", "pir::BoolAttribute", ""), paddle::dialect::OpAttributeInfo("ignore_index", "pir::Int32Attribute", ""), paddle::dialect::OpAttributeInfo("axis", "pir::Int32Attribute", "") };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("softmax", "paddle::dialect::DenseTensorType", false, false), paddle::dialect::OpOutputInfo("loss", "paddle::dialect::DenseTensorType", false, false) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("CrossEntropyWithSoftmaxInferMeta", {"input", "label", "soft_label", "use_softmax", "numeric_stable_mode", "ignore_index", "axis"}, "cross_entropy_with_softmax", {"input", "label", "soft_label", "use_softmax", "numeric_stable_mode", "ignore_index", "axis"}, {"input"}, {}, {{"softmax", "input"}}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "cross_entropy_with_softmax");
}

void CrossEntropyWithSoftmax_Op::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value input_, pir::Value label_, bool soft_label, bool use_softmax, bool numeric_stable_mode, int ignore_index, int axis) {
  VLOG(4) << "Start build CrossEntropyWithSoftmax_Op";



  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {input_, label_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_soft_label = pir::BoolAttribute::get(pir::IrContext::Instance(), soft_label);
  argument.AddAttribute("soft_label", attr_soft_label);
  pir::Attribute attr_use_softmax = pir::BoolAttribute::get(pir::IrContext::Instance(), use_softmax);
  argument.AddAttribute("use_softmax", attr_use_softmax);
  pir::Attribute attr_numeric_stable_mode = pir::BoolAttribute::get(pir::IrContext::Instance(), numeric_stable_mode);
  argument.AddAttribute("numeric_stable_mode", attr_numeric_stable_mode);
  pir::Attribute attr_ignore_index = pir::Int32Attribute::get(pir::IrContext::Instance(), ignore_index);
  argument.AddAttribute("ignore_index", attr_ignore_index);
  pir::Attribute attr_axis = pir::Int32Attribute::get(pir::IrContext::Instance(), axis);
  argument.AddAttribute("axis", attr_axis);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType input = input_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)input;
  paddle::dialect::DenseTensorType label = label_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)label;

  VLOG(4) << "Builder construction  dense_input";
  paddle::dialect::IrTensor ir_tensor_input(paddle::dialect::TransToPhiDataType(input.dtype()),
                                                      input.dims(),
                                                      input.data_layout(),
                                                      input.lod(),
                                                      input.offset());
  VLOG(4) << "Builder construction  meta_input";
  paddle::dialect::IrMetaTensor meta_input(&ir_tensor_input);

  VLOG(4) << "Builder construction  dense_label";
  paddle::dialect::IrTensor ir_tensor_label(paddle::dialect::TransToPhiDataType(label.dtype()),
                                                      label.dims(),
                                                      label.data_layout(),
                                                      label.lod(),
                                                      label.offset());
  VLOG(4) << "Builder construction  meta_label";
  paddle::dialect::IrMetaTensor meta_label(&ir_tensor_label);
  paddle::dialect::IrTensor dense_softmax;
  paddle::dialect::IrMetaTensor meta_softmax(&dense_softmax);
  paddle::dialect::IrTensor dense_loss;
  paddle::dialect::IrMetaTensor meta_loss(&dense_loss);

  phi::CrossEntropyWithSoftmaxInferMeta(meta_input, meta_label, soft_label, use_softmax, numeric_stable_mode, ignore_index, axis, &meta_softmax, &meta_loss);

  std::vector<pir::Type> argument_outputs;
  pir::Type softmax_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_softmax.dtype()), dense_softmax.dims(), dense_softmax.layout(), dense_softmax.lod(), dense_softmax.offset());
  argument_outputs.push_back(softmax_dense_tensor_type);

  pir::Type loss_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_loss.dtype()), dense_loss.dims(), dense_loss.layout(), dense_loss.lod(), dense_loss.offset());
  argument_outputs.push_back(loss_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void CrossEntropyWithSoftmax_Op::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value input_, pir::Value label_, pir::AttributeMap attributes) {
  VLOG(4) << "Start build CrossEntropyWithSoftmax_Op";


  IR_ENFORCE(
      attributes.find("soft_label") != attributes.end(),
          "'soft_label' Attribute is expected for CrossEntropyWithSoftmax_Op. ");
  bool soft_label = attributes.at("soft_label").dyn_cast<pir::BoolAttribute>().data();

  IR_ENFORCE(
      attributes.find("use_softmax") != attributes.end(),
          "'use_softmax' Attribute is expected for CrossEntropyWithSoftmax_Op. ");
  bool use_softmax = attributes.at("use_softmax").dyn_cast<pir::BoolAttribute>().data();

  IR_ENFORCE(
      attributes.find("numeric_stable_mode") != attributes.end(),
          "'numeric_stable_mode' Attribute is expected for CrossEntropyWithSoftmax_Op. ");
  bool numeric_stable_mode = attributes.at("numeric_stable_mode").dyn_cast<pir::BoolAttribute>().data();

  IR_ENFORCE(
      attributes.find("ignore_index") != attributes.end(),
          "'ignore_index' Attribute is expected for CrossEntropyWithSoftmax_Op. ");
  int ignore_index = attributes.at("ignore_index").dyn_cast<pir::Int32Attribute>().data();

  IR_ENFORCE(
      attributes.find("axis") != attributes.end(),
          "'axis' Attribute is expected for CrossEntropyWithSoftmax_Op. ");
  int axis = attributes.at("axis").dyn_cast<pir::Int32Attribute>().data();


  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {input_, label_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_soft_label = pir::BoolAttribute::get(pir::IrContext::Instance(), soft_label);
  argument.AddAttribute("soft_label", attr_soft_label);
  pir::Attribute attr_use_softmax = pir::BoolAttribute::get(pir::IrContext::Instance(), use_softmax);
  argument.AddAttribute("use_softmax", attr_use_softmax);
  pir::Attribute attr_numeric_stable_mode = pir::BoolAttribute::get(pir::IrContext::Instance(), numeric_stable_mode);
  argument.AddAttribute("numeric_stable_mode", attr_numeric_stable_mode);
  pir::Attribute attr_ignore_index = pir::Int32Attribute::get(pir::IrContext::Instance(), ignore_index);
  argument.AddAttribute("ignore_index", attr_ignore_index);
  pir::Attribute attr_axis = pir::Int32Attribute::get(pir::IrContext::Instance(), axis);
  argument.AddAttribute("axis", attr_axis);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType input = input_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)input;
  paddle::dialect::DenseTensorType label = label_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)label;

  VLOG(4) << "Builder construction  dense_input";
  paddle::dialect::IrTensor ir_tensor_input(paddle::dialect::TransToPhiDataType(input.dtype()),
                                                      input.dims(),
                                                      input.data_layout(),
                                                      input.lod(),
                                                      input.offset());
  VLOG(4) << "Builder construction  meta_input";
  paddle::dialect::IrMetaTensor meta_input(&ir_tensor_input);

  VLOG(4) << "Builder construction  dense_label";
  paddle::dialect::IrTensor ir_tensor_label(paddle::dialect::TransToPhiDataType(label.dtype()),
                                                      label.dims(),
                                                      label.data_layout(),
                                                      label.lod(),
                                                      label.offset());
  VLOG(4) << "Builder construction  meta_label";
  paddle::dialect::IrMetaTensor meta_label(&ir_tensor_label);
  paddle::dialect::IrTensor dense_softmax;
  paddle::dialect::IrMetaTensor meta_softmax(&dense_softmax);
  paddle::dialect::IrTensor dense_loss;
  paddle::dialect::IrMetaTensor meta_loss(&dense_loss);

  phi::CrossEntropyWithSoftmaxInferMeta(meta_input, meta_label, soft_label, use_softmax, numeric_stable_mode, ignore_index, axis, &meta_softmax, &meta_loss);

  std::vector<pir::Type> argument_outputs;
  pir::Type softmax_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_softmax.dtype()), dense_softmax.dims(), dense_softmax.layout(), dense_softmax.lod(), dense_softmax.offset());
  argument_outputs.push_back(softmax_dense_tensor_type);

  pir::Type loss_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_loss.dtype()), dense_loss.dims(), dense_loss.layout(), dense_loss.lod(), dense_loss.offset());
  argument_outputs.push_back(loss_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void CrossEntropyWithSoftmax_Op::VerifySig() {
  VLOG(4) << "Start Verifying inputs, outputs and attributes for: CrossEntropyWithSoftmax_Op.";
  VLOG(4) << "Verifying inputs:";
  {
  auto input_size = num_operands();
  IR_ENFORCE(input_size == 2u,
                    "The size %d of inputs must be equal to 2.", input_size);
  IR_ENFORCE((*this)->operand_source(0).type().isa<paddle::dialect::DenseTensorType>(),
                  "Type validation failed for the 0th input, got %s.", (*this)->operand_source(0).type());
  IR_ENFORCE((*this)->operand_source(1).type().isa<paddle::dialect::DenseTensorType>(),
                  "Type validation failed for the 1th input, got %s.", (*this)->operand_source(1).type());
  }
  VLOG(4) << "Verifying attributes:";
  {
  auto& attributes = this->attributes();
  IR_ENFORCE(attributes.count("soft_label")>0,
                 "soft_label does not exist.");
  IR_ENFORCE(attributes.at("soft_label").isa<pir::BoolAttribute>(),
                 "Type of attribute: soft_label is not pir::BoolAttribute.");

  IR_ENFORCE(attributes.count("use_softmax")>0,
                 "use_softmax does not exist.");
  IR_ENFORCE(attributes.at("use_softmax").isa<pir::BoolAttribute>(),
                 "Type of attribute: use_softmax is not pir::BoolAttribute.");

  IR_ENFORCE(attributes.count("numeric_stable_mode")>0,
                 "numeric_stable_mode does not exist.");
  IR_ENFORCE(attributes.at("numeric_stable_mode").isa<pir::BoolAttribute>(),
                 "Type of attribute: numeric_stable_mode is not pir::BoolAttribute.");

  IR_ENFORCE(attributes.count("ignore_index")>0,
                 "ignore_index does not exist.");
  IR_ENFORCE(attributes.at("ignore_index").isa<pir::Int32Attribute>(),
                 "Type of attribute: ignore_index is not pir::Int32Attribute.");

  IR_ENFORCE(attributes.count("axis")>0,
                 "axis does not exist.");
  IR_ENFORCE(attributes.at("axis").isa<pir::Int32Attribute>(),
                 "Type of attribute: axis is not pir::Int32Attribute.");

  }
  VLOG(4) << "Verifying outputs:";
  {
  auto output_size = num_results();
  IR_ENFORCE(output_size == 2u,
                    "The size %d of outputs must be equal to 2.", output_size);
  IR_ENFORCE((*this)->result(0).type().isa<paddle::dialect::DenseTensorType>(),
                 "Type validation failed for the 0th output.");
  IR_ENFORCE((*this)->result(1).type().isa<paddle::dialect::DenseTensorType>(),
                 "Type validation failed for the 1th output.");
  }
  VLOG(4) << "End Verifying for: CrossEntropyWithSoftmax_Op.";
}

void CrossEntropyWithSoftmax_Op::InferMeta( phi::InferMetaContext *infer_meta ) {
  auto fn = PD_INFER_META(phi::CrossEntropyWithSoftmaxInferMeta);
  fn(infer_meta);
}

phi::DataType CrossEntropyWithSoftmax_Op::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: CrossEntropyWithSoftmax_Op";
  


  return expected_kernel_dtype;
}

const char *CummaxOp::attributes_name[2] = { "axis", "dtype" };

OpInfoTuple CummaxOp::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("x", "paddle::dialect::DenseTensorType", false, false, false, true) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = { paddle::dialect::OpAttributeInfo("axis", "pir::Int32Attribute", ""), paddle::dialect::OpAttributeInfo("dtype", "paddle::dialect::DataTypeAttribute", "") };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("out", "paddle::dialect::DenseTensorType", false, false), paddle::dialect::OpOutputInfo("indices", "paddle::dialect::DenseTensorType", false, false) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("CumWithIndicesInferMeta", {"x", "axis", "dtype"}, "cummax", {"x", "axis", "dtype"}, {"x"}, {}, {}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "cummax");
}

void CummaxOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, int axis, phi::DataType dtype) {
  VLOG(4) << "Start build CummaxOp";



  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_axis = pir::Int32Attribute::get(pir::IrContext::Instance(), axis);
  argument.AddAttribute("axis", attr_axis);
  pir::Attribute attr_dtype = paddle::dialect::DataTypeAttribute::get(pir::IrContext::Instance(), dtype);
  argument.AddAttribute("dtype", attr_dtype);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);
  paddle::dialect::IrTensor dense_out;
  paddle::dialect::IrMetaTensor meta_out(&dense_out);
  paddle::dialect::IrTensor dense_indices;
  paddle::dialect::IrMetaTensor meta_indices(&dense_indices);

  phi::CumWithIndicesInferMeta(meta_x, axis, dtype, &meta_out, &meta_indices);

  std::vector<pir::Type> argument_outputs;
  pir::Type out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_out.dtype()), dense_out.dims(), dense_out.layout(), dense_out.lod(), dense_out.offset());
  argument_outputs.push_back(out_dense_tensor_type);

  pir::Type indices_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_indices.dtype()), dense_indices.dims(), dense_indices.layout(), dense_indices.lod(), dense_indices.offset());
  argument_outputs.push_back(indices_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void CummaxOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, pir::AttributeMap attributes) {
  VLOG(4) << "Start build CummaxOp";


  IR_ENFORCE(
      attributes.find("axis") != attributes.end(),
          "'axis' Attribute is expected for CummaxOp. ");
  int axis = attributes.at("axis").dyn_cast<pir::Int32Attribute>().data();

  IR_ENFORCE(
      attributes.find("dtype") != attributes.end(),
          "'dtype' Attribute is expected for CummaxOp. ");
  phi::DataType dtype = attributes.at("dtype").dyn_cast<paddle::dialect::DataTypeAttribute>().data();


  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_axis = pir::Int32Attribute::get(pir::IrContext::Instance(), axis);
  argument.AddAttribute("axis", attr_axis);
  pir::Attribute attr_dtype = paddle::dialect::DataTypeAttribute::get(pir::IrContext::Instance(), dtype);
  argument.AddAttribute("dtype", attr_dtype);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);
  paddle::dialect::IrTensor dense_out;
  paddle::dialect::IrMetaTensor meta_out(&dense_out);
  paddle::dialect::IrTensor dense_indices;
  paddle::dialect::IrMetaTensor meta_indices(&dense_indices);

  phi::CumWithIndicesInferMeta(meta_x, axis, dtype, &meta_out, &meta_indices);

  std::vector<pir::Type> argument_outputs;
  pir::Type out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_out.dtype()), dense_out.dims(), dense_out.layout(), dense_out.lod(), dense_out.offset());
  argument_outputs.push_back(out_dense_tensor_type);

  pir::Type indices_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_indices.dtype()), dense_indices.dims(), dense_indices.layout(), dense_indices.lod(), dense_indices.offset());
  argument_outputs.push_back(indices_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void CummaxOp::VerifySig() {
  VLOG(4) << "Start Verifying inputs, outputs and attributes for: CummaxOp.";
  VLOG(4) << "Verifying inputs:";
  {
  auto input_size = num_operands();
  IR_ENFORCE(input_size == 1u,
                    "The size %d of inputs must be equal to 1.", input_size);
  IR_ENFORCE((*this)->operand_source(0).type().isa<paddle::dialect::DenseTensorType>(),
                  "Type validation failed for the 0th input, got %s.", (*this)->operand_source(0).type());
  }
  VLOG(4) << "Verifying attributes:";
  {
  auto& attributes = this->attributes();
  IR_ENFORCE(attributes.count("axis")>0,
                 "axis does not exist.");
  IR_ENFORCE(attributes.at("axis").isa<pir::Int32Attribute>(),
                 "Type of attribute: axis is not pir::Int32Attribute.");

  IR_ENFORCE(attributes.count("dtype")>0,
                 "dtype does not exist.");
  IR_ENFORCE(attributes.at("dtype").isa<paddle::dialect::DataTypeAttribute>(),
                 "Type of attribute: dtype is not paddle::dialect::DataTypeAttribute.");

  }
  VLOG(4) << "Verifying outputs:";
  {
  auto output_size = num_results();
  IR_ENFORCE(output_size == 2u,
                    "The size %d of outputs must be equal to 2.", output_size);
  IR_ENFORCE((*this)->result(0).type().isa<paddle::dialect::DenseTensorType>(),
                 "Type validation failed for the 0th output.");
  IR_ENFORCE((*this)->result(1).type().isa<paddle::dialect::DenseTensorType>(),
                 "Type validation failed for the 1th output.");
  }
  VLOG(4) << "End Verifying for: CummaxOp.";
}

void CummaxOp::InferMeta( phi::InferMetaContext *infer_meta ) {
  auto fn = PD_INFER_META(phi::CumWithIndicesInferMeta);
  fn(infer_meta);
}

phi::DataType CummaxOp::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: CummaxOp";
  


  return expected_kernel_dtype;
}

const char *CumminOp::attributes_name[2] = { "axis", "dtype" };

OpInfoTuple CumminOp::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("x", "paddle::dialect::DenseTensorType", false, false, false, true) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = { paddle::dialect::OpAttributeInfo("axis", "pir::Int32Attribute", ""), paddle::dialect::OpAttributeInfo("dtype", "paddle::dialect::DataTypeAttribute", "") };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("out", "paddle::dialect::DenseTensorType", false, false), paddle::dialect::OpOutputInfo("indices", "paddle::dialect::DenseTensorType", false, false) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("CumWithIndicesInferMeta", {"x", "axis", "dtype"}, "cummin", {"x", "axis", "dtype"}, {"x"}, {}, {}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "cummin");
}

void CumminOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, int axis, phi::DataType dtype) {
  VLOG(4) << "Start build CumminOp";



  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_axis = pir::Int32Attribute::get(pir::IrContext::Instance(), axis);
  argument.AddAttribute("axis", attr_axis);
  pir::Attribute attr_dtype = paddle::dialect::DataTypeAttribute::get(pir::IrContext::Instance(), dtype);
  argument.AddAttribute("dtype", attr_dtype);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);
  paddle::dialect::IrTensor dense_out;
  paddle::dialect::IrMetaTensor meta_out(&dense_out);
  paddle::dialect::IrTensor dense_indices;
  paddle::dialect::IrMetaTensor meta_indices(&dense_indices);

  phi::CumWithIndicesInferMeta(meta_x, axis, dtype, &meta_out, &meta_indices);

  std::vector<pir::Type> argument_outputs;
  pir::Type out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_out.dtype()), dense_out.dims(), dense_out.layout(), dense_out.lod(), dense_out.offset());
  argument_outputs.push_back(out_dense_tensor_type);

  pir::Type indices_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_indices.dtype()), dense_indices.dims(), dense_indices.layout(), dense_indices.lod(), dense_indices.offset());
  argument_outputs.push_back(indices_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void CumminOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, pir::AttributeMap attributes) {
  VLOG(4) << "Start build CumminOp";


  IR_ENFORCE(
      attributes.find("axis") != attributes.end(),
          "'axis' Attribute is expected for CumminOp. ");
  int axis = attributes.at("axis").dyn_cast<pir::Int32Attribute>().data();

  IR_ENFORCE(
      attributes.find("dtype") != attributes.end(),
          "'dtype' Attribute is expected for CumminOp. ");
  phi::DataType dtype = attributes.at("dtype").dyn_cast<paddle::dialect::DataTypeAttribute>().data();


  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_axis = pir::Int32Attribute::get(pir::IrContext::Instance(), axis);
  argument.AddAttribute("axis", attr_axis);
  pir::Attribute attr_dtype = paddle::dialect::DataTypeAttribute::get(pir::IrContext::Instance(), dtype);
  argument.AddAttribute("dtype", attr_dtype);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);
  paddle::dialect::IrTensor dense_out;
  paddle::dialect::IrMetaTensor meta_out(&dense_out);
  paddle::dialect::IrTensor dense_indices;
  paddle::dialect::IrMetaTensor meta_indices(&dense_indices);

  phi::CumWithIndicesInferMeta(meta_x, axis, dtype, &meta_out, &meta_indices);

  std::vector<pir::Type> argument_outputs;
  pir::Type out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_out.dtype()), dense_out.dims(), dense_out.layout(), dense_out.lod(), dense_out.offset());
  argument_outputs.push_back(out_dense_tensor_type);

  pir::Type indices_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_indices.dtype()), dense_indices.dims(), dense_indices.layout(), dense_indices.lod(), dense_indices.offset());
  argument_outputs.push_back(indices_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void CumminOp::VerifySig() {
  VLOG(4) << "Start Verifying inputs, outputs and attributes for: CumminOp.";
  VLOG(4) << "Verifying inputs:";
  {
  auto input_size = num_operands();
  IR_ENFORCE(input_size == 1u,
                    "The size %d of inputs must be equal to 1.", input_size);
  IR_ENFORCE((*this)->operand_source(0).type().isa<paddle::dialect::DenseTensorType>(),
                  "Type validation failed for the 0th input, got %s.", (*this)->operand_source(0).type());
  }
  VLOG(4) << "Verifying attributes:";
  {
  auto& attributes = this->attributes();
  IR_ENFORCE(attributes.count("axis")>0,
                 "axis does not exist.");
  IR_ENFORCE(attributes.at("axis").isa<pir::Int32Attribute>(),
                 "Type of attribute: axis is not pir::Int32Attribute.");

  IR_ENFORCE(attributes.count("dtype")>0,
                 "dtype does not exist.");
  IR_ENFORCE(attributes.at("dtype").isa<paddle::dialect::DataTypeAttribute>(),
                 "Type of attribute: dtype is not paddle::dialect::DataTypeAttribute.");

  }
  VLOG(4) << "Verifying outputs:";
  {
  auto output_size = num_results();
  IR_ENFORCE(output_size == 2u,
                    "The size %d of outputs must be equal to 2.", output_size);
  IR_ENFORCE((*this)->result(0).type().isa<paddle::dialect::DenseTensorType>(),
                 "Type validation failed for the 0th output.");
  IR_ENFORCE((*this)->result(1).type().isa<paddle::dialect::DenseTensorType>(),
                 "Type validation failed for the 1th output.");
  }
  VLOG(4) << "End Verifying for: CumminOp.";
}

void CumminOp::InferMeta( phi::InferMetaContext *infer_meta ) {
  auto fn = PD_INFER_META(phi::CumWithIndicesInferMeta);
  fn(infer_meta);
}

phi::DataType CumminOp::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: CumminOp";
  


  return expected_kernel_dtype;
}

const char *CumprodOp::attributes_name[1] = { "dim" };

OpInfoTuple CumprodOp::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("x", "paddle::dialect::DenseTensorType", false, false, false, true) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = { paddle::dialect::OpAttributeInfo("dim", "pir::Int32Attribute", "") };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("out", "paddle::dialect::DenseTensorType", false, false) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("UnchangedInferMetaCheckAxis", {"x", "dim"}, "cumprod", {"x", "dim"}, {}, {}, {}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "cumprod");
}

void CumprodOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, int dim) {
  VLOG(4) << "Start build CumprodOp";



  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_dim = pir::Int32Attribute::get(pir::IrContext::Instance(), dim);
  argument.AddAttribute("dim", attr_dim);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);
  paddle::dialect::IrTensor dense_out;
  paddle::dialect::IrMetaTensor meta_out(&dense_out);

  phi::UnchangedInferMetaCheckAxis(meta_x, dim, &meta_out);

  std::vector<pir::Type> argument_outputs;
  pir::Type out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_out.dtype()), dense_out.dims(), dense_out.layout(), dense_out.lod(), dense_out.offset());
  argument_outputs.push_back(out_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void CumprodOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, pir::AttributeMap attributes) {
  VLOG(4) << "Start build CumprodOp";


  IR_ENFORCE(
      attributes.find("dim") != attributes.end(),
          "'dim' Attribute is expected for CumprodOp. ");
  int dim = attributes.at("dim").dyn_cast<pir::Int32Attribute>().data();


  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_dim = pir::Int32Attribute::get(pir::IrContext::Instance(), dim);
  argument.AddAttribute("dim", attr_dim);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);
  paddle::dialect::IrTensor dense_out;
  paddle::dialect::IrMetaTensor meta_out(&dense_out);

  phi::UnchangedInferMetaCheckAxis(meta_x, dim, &meta_out);

  std::vector<pir::Type> argument_outputs;
  pir::Type out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_out.dtype()), dense_out.dims(), dense_out.layout(), dense_out.lod(), dense_out.offset());
  argument_outputs.push_back(out_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void CumprodOp::VerifySig() {
  VLOG(4) << "Start Verifying inputs, outputs and attributes for: CumprodOp.";
  VLOG(4) << "Verifying inputs:";
  {
  auto input_size = num_operands();
  IR_ENFORCE(input_size == 1u,
                    "The size %d of inputs must be equal to 1.", input_size);
  IR_ENFORCE((*this)->operand_source(0).type().isa<paddle::dialect::DenseTensorType>(),
                  "Type validation failed for the 0th input, got %s.", (*this)->operand_source(0).type());
  }
  VLOG(4) << "Verifying attributes:";
  {
  auto& attributes = this->attributes();
  IR_ENFORCE(attributes.count("dim")>0,
                 "dim does not exist.");
  IR_ENFORCE(attributes.at("dim").isa<pir::Int32Attribute>(),
                 "Type of attribute: dim is not pir::Int32Attribute.");

  }
  VLOG(4) << "Verifying outputs:";
  {
  auto output_size = num_results();
  IR_ENFORCE(output_size == 1u,
                    "The size %d of outputs must be equal to 1.", output_size);
  IR_ENFORCE((*this)->result(0).type().isa<paddle::dialect::DenseTensorType>(),
                 "Type validation failed for the 0th output.");
  }
  VLOG(4) << "End Verifying for: CumprodOp.";
}

void CumprodOp::InferMeta( phi::InferMetaContext *infer_meta ) {
  auto fn = PD_INFER_META(phi::UnchangedInferMetaCheckAxis);
  fn(infer_meta);
}

phi::DataType CumprodOp::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: CumprodOp";
  


  return expected_kernel_dtype;
}

const char *Cumprod_Op::attributes_name[1] = { "dim" };

OpInfoTuple Cumprod_Op::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("x", "paddle::dialect::DenseTensorType", false, false, false, true) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = { paddle::dialect::OpAttributeInfo("dim", "pir::Int32Attribute", "") };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("out", "paddle::dialect::DenseTensorType", false, false) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("UnchangedInferMetaCheckAxis", {"x", "dim"}, "cumprod", {"x", "dim"}, {}, {}, {{"out", "x"}}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "cumprod");
}

void Cumprod_Op::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, int dim) {
  VLOG(4) << "Start build Cumprod_Op";



  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_dim = pir::Int32Attribute::get(pir::IrContext::Instance(), dim);
  argument.AddAttribute("dim", attr_dim);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);
  paddle::dialect::IrTensor dense_out;
  paddle::dialect::IrMetaTensor meta_out(&dense_out);

  phi::UnchangedInferMetaCheckAxis(meta_x, dim, &meta_out);

  std::vector<pir::Type> argument_outputs;
  pir::Type out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_out.dtype()), dense_out.dims(), dense_out.layout(), dense_out.lod(), dense_out.offset());
  argument_outputs.push_back(out_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void Cumprod_Op::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, pir::AttributeMap attributes) {
  VLOG(4) << "Start build Cumprod_Op";


  IR_ENFORCE(
      attributes.find("dim") != attributes.end(),
          "'dim' Attribute is expected for Cumprod_Op. ");
  int dim = attributes.at("dim").dyn_cast<pir::Int32Attribute>().data();


  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_dim = pir::Int32Attribute::get(pir::IrContext::Instance(), dim);
  argument.AddAttribute("dim", attr_dim);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);
  paddle::dialect::IrTensor dense_out;
  paddle::dialect::IrMetaTensor meta_out(&dense_out);

  phi::UnchangedInferMetaCheckAxis(meta_x, dim, &meta_out);

  std::vector<pir::Type> argument_outputs;
  pir::Type out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_out.dtype()), dense_out.dims(), dense_out.layout(), dense_out.lod(), dense_out.offset());
  argument_outputs.push_back(out_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void Cumprod_Op::VerifySig() {
  VLOG(4) << "Start Verifying inputs, outputs and attributes for: Cumprod_Op.";
  VLOG(4) << "Verifying inputs:";
  {
  auto input_size = num_operands();
  IR_ENFORCE(input_size == 1u,
                    "The size %d of inputs must be equal to 1.", input_size);
  IR_ENFORCE((*this)->operand_source(0).type().isa<paddle::dialect::DenseTensorType>(),
                  "Type validation failed for the 0th input, got %s.", (*this)->operand_source(0).type());
  }
  VLOG(4) << "Verifying attributes:";
  {
  auto& attributes = this->attributes();
  IR_ENFORCE(attributes.count("dim")>0,
                 "dim does not exist.");
  IR_ENFORCE(attributes.at("dim").isa<pir::Int32Attribute>(),
                 "Type of attribute: dim is not pir::Int32Attribute.");

  }
  VLOG(4) << "Verifying outputs:";
  {
  auto output_size = num_results();
  IR_ENFORCE(output_size == 1u,
                    "The size %d of outputs must be equal to 1.", output_size);
  IR_ENFORCE((*this)->result(0).type().isa<paddle::dialect::DenseTensorType>(),
                 "Type validation failed for the 0th output.");
  }
  VLOG(4) << "End Verifying for: Cumprod_Op.";
}

void Cumprod_Op::InferMeta( phi::InferMetaContext *infer_meta ) {
  auto fn = PD_INFER_META(phi::UnchangedInferMetaCheckAxis);
  fn(infer_meta);
}

phi::DataType Cumprod_Op::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: Cumprod_Op";
  


  return expected_kernel_dtype;
}

const char *CumsumOp::attributes_name[3] = { "flatten", "exclusive", "reverse" };

OpInfoTuple CumsumOp::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("x", "paddle::dialect::DenseTensorType", false, false, false, true), paddle::dialect::OpInputInfo("axis", "paddle::dialect::ScalarAttribute", false, false, true, false) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = { paddle::dialect::OpAttributeInfo("flatten", "pir::BoolAttribute", ""), paddle::dialect::OpAttributeInfo("exclusive", "pir::BoolAttribute", ""), paddle::dialect::OpAttributeInfo("reverse", "pir::BoolAttribute", "") };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("out", "paddle::dialect::DenseTensorType", false, false) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("CumScalarAxisInferMeta", {"x", "axis", "flatten", "exclusive", "reverse"}, "cumsum", {"x", "axis", "flatten", "exclusive", "reverse"}, {"x"}, {}, {}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "cumsum");
}

void CumsumOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, int axis, bool flatten, bool exclusive, bool reverse) {
  VLOG(4) << "Start build CumsumOp";


  // Generate scalar mutable attribute: axis
  paddle::dialect::FullOp full_axis_op = builder.Build<paddle::dialect::FullOp>(std::vector<int64_t>{1}, axis, phi::DataType::INT32, phi::CPUPlace());
  pir::OpResult axis_ = full_axis_op->result(0);
    
  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_, axis_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_flatten = pir::BoolAttribute::get(pir::IrContext::Instance(), flatten);
  argument.AddAttribute("flatten", attr_flatten);
  pir::Attribute attr_exclusive = pir::BoolAttribute::get(pir::IrContext::Instance(), exclusive);
  argument.AddAttribute("exclusive", attr_exclusive);
  pir::Attribute attr_reverse = pir::BoolAttribute::get(pir::IrContext::Instance(), reverse);
  argument.AddAttribute("reverse", attr_reverse);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);
  paddle::dialect::IrTensor dense_out;
  paddle::dialect::IrMetaTensor meta_out(&dense_out);

  phi::CumScalarAxisInferMeta(meta_x, axis, flatten, exclusive, reverse, &meta_out);

  std::vector<pir::Type> argument_outputs;
  pir::Type out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_out.dtype()), dense_out.dims(), dense_out.layout(), dense_out.lod(), dense_out.offset());
  argument_outputs.push_back(out_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void CumsumOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, pir::AttributeMap attributes) {
  VLOG(4) << "Start build CumsumOp";


  IR_ENFORCE(
      attributes.find("axis") != attributes.end(),
          "'axis' Attribute is expected for CumsumOp. ");
  int axis = attributes.at("axis").dyn_cast<paddle::dialect::ScalarAttribute>().data().to<int>();

  IR_ENFORCE(
      attributes.find("flatten") != attributes.end(),
          "'flatten' Attribute is expected for CumsumOp. ");
  bool flatten = attributes.at("flatten").dyn_cast<pir::BoolAttribute>().data();

  IR_ENFORCE(
      attributes.find("exclusive") != attributes.end(),
          "'exclusive' Attribute is expected for CumsumOp. ");
  bool exclusive = attributes.at("exclusive").dyn_cast<pir::BoolAttribute>().data();

  IR_ENFORCE(
      attributes.find("reverse") != attributes.end(),
          "'reverse' Attribute is expected for CumsumOp. ");
  bool reverse = attributes.at("reverse").dyn_cast<pir::BoolAttribute>().data();

  // Generate scalar mutable attribute: axis
  paddle::dialect::FullOp full_axis_op = builder.Build<paddle::dialect::FullOp>(std::vector<int64_t>{1}, axis, phi::DataType::INT32, phi::CPUPlace());
  pir::OpResult axis_ = full_axis_op->result(0);
    
  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_, axis_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_flatten = pir::BoolAttribute::get(pir::IrContext::Instance(), flatten);
  argument.AddAttribute("flatten", attr_flatten);
  pir::Attribute attr_exclusive = pir::BoolAttribute::get(pir::IrContext::Instance(), exclusive);
  argument.AddAttribute("exclusive", attr_exclusive);
  pir::Attribute attr_reverse = pir::BoolAttribute::get(pir::IrContext::Instance(), reverse);
  argument.AddAttribute("reverse", attr_reverse);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);
  paddle::dialect::IrTensor dense_out;
  paddle::dialect::IrMetaTensor meta_out(&dense_out);

  phi::CumScalarAxisInferMeta(meta_x, axis, flatten, exclusive, reverse, &meta_out);

  std::vector<pir::Type> argument_outputs;
  pir::Type out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_out.dtype()), dense_out.dims(), dense_out.layout(), dense_out.lod(), dense_out.offset());
  argument_outputs.push_back(out_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void CumsumOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, pir::Value axis_, bool flatten, bool exclusive, bool reverse) {
  VLOG(4) << "Start build CumsumOp";



  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_, axis_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_flatten = pir::BoolAttribute::get(pir::IrContext::Instance(), flatten);
  argument.AddAttribute("flatten", attr_flatten);
  pir::Attribute attr_exclusive = pir::BoolAttribute::get(pir::IrContext::Instance(), exclusive);
  argument.AddAttribute("exclusive", attr_exclusive);
  pir::Attribute attr_reverse = pir::BoolAttribute::get(pir::IrContext::Instance(), reverse);
  argument.AddAttribute("reverse", attr_reverse);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;
  phi::Scalar axis;
  if (axis_.dyn_cast<pir::OpResult>() && axis_.dyn_cast<pir::OpResult>().owner()->isa<paddle::dialect::FullOp>()) {
    axis = std::move(phi::Scalar(axis_.dyn_cast<pir::OpResult>().owner()
                                  ->dyn_cast<paddle::dialect::FullOp>()
                                  .attribute("value")
                                  .dyn_cast<paddle::dialect::ScalarAttribute>()
                                  .data()
                                  .to<int>()));
  }
  else {
    axis = std::move(phi::Scalar(-1));
    axis.SetFromTensor(true);
  }


  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);
  paddle::dialect::IrTensor dense_out;
  paddle::dialect::IrMetaTensor meta_out(&dense_out);

  phi::CumScalarAxisInferMeta(meta_x, axis, flatten, exclusive, reverse, &meta_out);

  std::vector<pir::Type> argument_outputs;
  pir::Type out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_out.dtype()), dense_out.dims(), dense_out.layout(), dense_out.lod(), dense_out.offset());
  argument_outputs.push_back(out_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void CumsumOp::VerifySig() {
  VLOG(4) << "Start Verifying inputs, outputs and attributes for: CumsumOp.";
  VLOG(4) << "Verifying inputs:";
  {
  auto input_size = num_operands();
  IR_ENFORCE(input_size == 2u,
                    "The size %d of inputs must be equal to 2.", input_size);
  IR_ENFORCE((*this)->operand_source(0).type().isa<paddle::dialect::DenseTensorType>(),
                  "Type validation failed for the 0th input, got %s.", (*this)->operand_source(0).type());
  IR_ENFORCE((*this)->operand_source(1).type().isa<paddle::dialect::DenseTensorType>(),
                  "Type validation failed for the 1th input, got %s.", (*this)->operand_source(1).type());
  }
  VLOG(4) << "Verifying attributes:";
  {
  auto& attributes = this->attributes();
  IR_ENFORCE(attributes.count("flatten")>0,
                 "flatten does not exist.");
  IR_ENFORCE(attributes.at("flatten").isa<pir::BoolAttribute>(),
                 "Type of attribute: flatten is not pir::BoolAttribute.");

  IR_ENFORCE(attributes.count("exclusive")>0,
                 "exclusive does not exist.");
  IR_ENFORCE(attributes.at("exclusive").isa<pir::BoolAttribute>(),
                 "Type of attribute: exclusive is not pir::BoolAttribute.");

  IR_ENFORCE(attributes.count("reverse")>0,
                 "reverse does not exist.");
  IR_ENFORCE(attributes.at("reverse").isa<pir::BoolAttribute>(),
                 "Type of attribute: reverse is not pir::BoolAttribute.");

  }
  VLOG(4) << "Verifying outputs:";
  {
  auto output_size = num_results();
  IR_ENFORCE(output_size == 1u,
                    "The size %d of outputs must be equal to 1.", output_size);
  IR_ENFORCE((*this)->result(0).type().isa<paddle::dialect::DenseTensorType>(),
                 "Type validation failed for the 0th output.");
  }
  VLOG(4) << "End Verifying for: CumsumOp.";
}

void CumsumOp::InferMeta( phi::InferMetaContext *infer_meta ) {
  auto fn = PD_INFER_META(phi::CumScalarAxisInferMeta);
  fn(infer_meta);
}

phi::DataType CumsumOp::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: CumsumOp";
  


  return expected_kernel_dtype;
}

const char *Cumsum_Op::attributes_name[3] = { "flatten", "exclusive", "reverse" };

OpInfoTuple Cumsum_Op::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("x", "paddle::dialect::DenseTensorType", false, false, false, true), paddle::dialect::OpInputInfo("axis", "paddle::dialect::ScalarAttribute", false, false, true, false) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = { paddle::dialect::OpAttributeInfo("flatten", "pir::BoolAttribute", ""), paddle::dialect::OpAttributeInfo("exclusive", "pir::BoolAttribute", ""), paddle::dialect::OpAttributeInfo("reverse", "pir::BoolAttribute", "") };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("out", "paddle::dialect::DenseTensorType", false, false) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("CumScalarAxisInferMeta", {"x", "axis", "flatten", "exclusive", "reverse"}, "cumsum", {"x", "axis", "flatten", "exclusive", "reverse"}, {"x"}, {}, {{"out", "x"}}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "cumsum");
}

void Cumsum_Op::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, int axis, bool flatten, bool exclusive, bool reverse) {
  VLOG(4) << "Start build Cumsum_Op";


  // Generate scalar mutable attribute: axis
  paddle::dialect::FullOp full_axis_op = builder.Build<paddle::dialect::FullOp>(std::vector<int64_t>{1}, axis, phi::DataType::INT32, phi::CPUPlace());
  pir::OpResult axis_ = full_axis_op->result(0);
    
  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_, axis_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_flatten = pir::BoolAttribute::get(pir::IrContext::Instance(), flatten);
  argument.AddAttribute("flatten", attr_flatten);
  pir::Attribute attr_exclusive = pir::BoolAttribute::get(pir::IrContext::Instance(), exclusive);
  argument.AddAttribute("exclusive", attr_exclusive);
  pir::Attribute attr_reverse = pir::BoolAttribute::get(pir::IrContext::Instance(), reverse);
  argument.AddAttribute("reverse", attr_reverse);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);
  paddle::dialect::IrTensor dense_out;
  paddle::dialect::IrMetaTensor meta_out(&dense_out);

  phi::CumScalarAxisInferMeta(meta_x, axis, flatten, exclusive, reverse, &meta_out);

  std::vector<pir::Type> argument_outputs;
  pir::Type out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_out.dtype()), dense_out.dims(), dense_out.layout(), dense_out.lod(), dense_out.offset());
  argument_outputs.push_back(out_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void Cumsum_Op::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, pir::AttributeMap attributes) {
  VLOG(4) << "Start build Cumsum_Op";


  IR_ENFORCE(
      attributes.find("axis") != attributes.end(),
          "'axis' Attribute is expected for Cumsum_Op. ");
  int axis = attributes.at("axis").dyn_cast<paddle::dialect::ScalarAttribute>().data().to<int>();

  IR_ENFORCE(
      attributes.find("flatten") != attributes.end(),
          "'flatten' Attribute is expected for Cumsum_Op. ");
  bool flatten = attributes.at("flatten").dyn_cast<pir::BoolAttribute>().data();

  IR_ENFORCE(
      attributes.find("exclusive") != attributes.end(),
          "'exclusive' Attribute is expected for Cumsum_Op. ");
  bool exclusive = attributes.at("exclusive").dyn_cast<pir::BoolAttribute>().data();

  IR_ENFORCE(
      attributes.find("reverse") != attributes.end(),
          "'reverse' Attribute is expected for Cumsum_Op. ");
  bool reverse = attributes.at("reverse").dyn_cast<pir::BoolAttribute>().data();

  // Generate scalar mutable attribute: axis
  paddle::dialect::FullOp full_axis_op = builder.Build<paddle::dialect::FullOp>(std::vector<int64_t>{1}, axis, phi::DataType::INT32, phi::CPUPlace());
  pir::OpResult axis_ = full_axis_op->result(0);
    
  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_, axis_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_flatten = pir::BoolAttribute::get(pir::IrContext::Instance(), flatten);
  argument.AddAttribute("flatten", attr_flatten);
  pir::Attribute attr_exclusive = pir::BoolAttribute::get(pir::IrContext::Instance(), exclusive);
  argument.AddAttribute("exclusive", attr_exclusive);
  pir::Attribute attr_reverse = pir::BoolAttribute::get(pir::IrContext::Instance(), reverse);
  argument.AddAttribute("reverse", attr_reverse);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);
  paddle::dialect::IrTensor dense_out;
  paddle::dialect::IrMetaTensor meta_out(&dense_out);

  phi::CumScalarAxisInferMeta(meta_x, axis, flatten, exclusive, reverse, &meta_out);

  std::vector<pir::Type> argument_outputs;
  pir::Type out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_out.dtype()), dense_out.dims(), dense_out.layout(), dense_out.lod(), dense_out.offset());
  argument_outputs.push_back(out_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void Cumsum_Op::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, pir::Value axis_, bool flatten, bool exclusive, bool reverse) {
  VLOG(4) << "Start build Cumsum_Op";



  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_, axis_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_flatten = pir::BoolAttribute::get(pir::IrContext::Instance(), flatten);
  argument.AddAttribute("flatten", attr_flatten);
  pir::Attribute attr_exclusive = pir::BoolAttribute::get(pir::IrContext::Instance(), exclusive);
  argument.AddAttribute("exclusive", attr_exclusive);
  pir::Attribute attr_reverse = pir::BoolAttribute::get(pir::IrContext::Instance(), reverse);
  argument.AddAttribute("reverse", attr_reverse);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;
  phi::Scalar axis;
  if (axis_.dyn_cast<pir::OpResult>() && axis_.dyn_cast<pir::OpResult>().owner()->isa<paddle::dialect::FullOp>()) {
    axis = std::move(phi::Scalar(axis_.dyn_cast<pir::OpResult>().owner()
                                  ->dyn_cast<paddle::dialect::FullOp>()
                                  .attribute("value")
                                  .dyn_cast<paddle::dialect::ScalarAttribute>()
                                  .data()
                                  .to<int>()));
  }
  else {
    axis = std::move(phi::Scalar(-1));
    axis.SetFromTensor(true);
  }


  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);
  paddle::dialect::IrTensor dense_out;
  paddle::dialect::IrMetaTensor meta_out(&dense_out);

  phi::CumScalarAxisInferMeta(meta_x, axis, flatten, exclusive, reverse, &meta_out);

  std::vector<pir::Type> argument_outputs;
  pir::Type out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_out.dtype()), dense_out.dims(), dense_out.layout(), dense_out.lod(), dense_out.offset());
  argument_outputs.push_back(out_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void Cumsum_Op::VerifySig() {
  VLOG(4) << "Start Verifying inputs, outputs and attributes for: Cumsum_Op.";
  VLOG(4) << "Verifying inputs:";
  {
  auto input_size = num_operands();
  IR_ENFORCE(input_size == 2u,
                    "The size %d of inputs must be equal to 2.", input_size);
  IR_ENFORCE((*this)->operand_source(0).type().isa<paddle::dialect::DenseTensorType>(),
                  "Type validation failed for the 0th input, got %s.", (*this)->operand_source(0).type());
  IR_ENFORCE((*this)->operand_source(1).type().isa<paddle::dialect::DenseTensorType>(),
                  "Type validation failed for the 1th input, got %s.", (*this)->operand_source(1).type());
  }
  VLOG(4) << "Verifying attributes:";
  {
  auto& attributes = this->attributes();
  IR_ENFORCE(attributes.count("flatten")>0,
                 "flatten does not exist.");
  IR_ENFORCE(attributes.at("flatten").isa<pir::BoolAttribute>(),
                 "Type of attribute: flatten is not pir::BoolAttribute.");

  IR_ENFORCE(attributes.count("exclusive")>0,
                 "exclusive does not exist.");
  IR_ENFORCE(attributes.at("exclusive").isa<pir::BoolAttribute>(),
                 "Type of attribute: exclusive is not pir::BoolAttribute.");

  IR_ENFORCE(attributes.count("reverse")>0,
                 "reverse does not exist.");
  IR_ENFORCE(attributes.at("reverse").isa<pir::BoolAttribute>(),
                 "Type of attribute: reverse is not pir::BoolAttribute.");

  }
  VLOG(4) << "Verifying outputs:";
  {
  auto output_size = num_results();
  IR_ENFORCE(output_size == 1u,
                    "The size %d of outputs must be equal to 1.", output_size);
  IR_ENFORCE((*this)->result(0).type().isa<paddle::dialect::DenseTensorType>(),
                 "Type validation failed for the 0th output.");
  }
  VLOG(4) << "End Verifying for: Cumsum_Op.";
}

void Cumsum_Op::InferMeta( phi::InferMetaContext *infer_meta ) {
  auto fn = PD_INFER_META(phi::CumScalarAxisInferMeta);
  fn(infer_meta);
}

phi::DataType Cumsum_Op::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: Cumsum_Op";
  


  return expected_kernel_dtype;
}

const char *DataOp::attributes_name[4] = { "name", "shape", "dtype", "place" };

OpInfoTuple DataOp::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = {  };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = { paddle::dialect::OpAttributeInfo("name", "pir::StrAttribute", ""), paddle::dialect::OpAttributeInfo("shape", "paddle::dialect::IntArrayAttribute", "std::vector<int64_t>"), paddle::dialect::OpAttributeInfo("dtype", "paddle::dialect::DataTypeAttribute", ""), paddle::dialect::OpAttributeInfo("place", "paddle::dialect::PlaceAttribute", "") };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("out", "paddle::dialect::DenseTensorType", false, false) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("DataInferMeta", {"name", "shape", "dtype"}, "data", {"name", "shape", "dtype"}, {"dtype"}, {"place"}, {}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "data");
}

void DataOp::Build(pir::Builder &builder, pir::OperationArgument &argument, const std::string& name, const std::vector<int64_t>& shape, phi::DataType dtype, const Place& place) {
  VLOG(4) << "Start build DataOp";



  VLOG(4) << "Builder construction inputs";

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_name = pir::StrAttribute::get(pir::IrContext::Instance(), name);
  argument.AddAttribute("name", attr_name);
  pir::Attribute attr_shape = paddle::dialect::IntArrayAttribute::get(pir::IrContext::Instance(), phi::IntArray(shape));
  argument.AddAttribute("shape", attr_shape);
  pir::Attribute attr_dtype = paddle::dialect::DataTypeAttribute::get(pir::IrContext::Instance(), dtype);
  argument.AddAttribute("dtype", attr_dtype);
  pir::Attribute attr_place = paddle::dialect::PlaceAttribute::get(pir::IrContext::Instance(), place);
  argument.AddAttribute("place", attr_place);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::IrTensor dense_out;
  paddle::dialect::IrMetaTensor meta_out(&dense_out);

  phi::DataInferMeta(name, shape, dtype, &meta_out);

  std::vector<pir::Type> argument_outputs;
  pir::Type out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_out.dtype()), dense_out.dims(), dense_out.layout(), dense_out.lod(), dense_out.offset());
  argument_outputs.push_back(out_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void DataOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::AttributeMap attributes) {
  VLOG(4) << "Start build DataOp";


  IR_ENFORCE(
      attributes.find("name") != attributes.end(),
          "'name' Attribute is expected for DataOp. ");
  std::string name = attributes.at("name").dyn_cast<pir::StrAttribute>().AsString();

  IR_ENFORCE(
      attributes.find("shape") != attributes.end(),
          "'shape' Attribute is expected for DataOp. ");
  std::vector<int64_t> shape = attributes.at("shape").dyn_cast<paddle::dialect::IntArrayAttribute>().data().GetData();

  IR_ENFORCE(
      attributes.find("dtype") != attributes.end(),
          "'dtype' Attribute is expected for DataOp. ");
  phi::DataType dtype = attributes.at("dtype").dyn_cast<paddle::dialect::DataTypeAttribute>().data();

  IR_ENFORCE(
      attributes.find("place") != attributes.end(),
          "'place' Attribute is expected for DataOp. ");
  Place place = attributes.at("place").dyn_cast<paddle::dialect::PlaceAttribute>().data();


  VLOG(4) << "Builder construction inputs";

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_name = pir::StrAttribute::get(pir::IrContext::Instance(), name);
  argument.AddAttribute("name", attr_name);
  pir::Attribute attr_shape = paddle::dialect::IntArrayAttribute::get(pir::IrContext::Instance(), phi::IntArray(shape));
  argument.AddAttribute("shape", attr_shape);
  pir::Attribute attr_dtype = paddle::dialect::DataTypeAttribute::get(pir::IrContext::Instance(), dtype);
  argument.AddAttribute("dtype", attr_dtype);
  pir::Attribute attr_place = paddle::dialect::PlaceAttribute::get(pir::IrContext::Instance(), place);
  argument.AddAttribute("place", attr_place);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::IrTensor dense_out;
  paddle::dialect::IrMetaTensor meta_out(&dense_out);

  phi::DataInferMeta(name, shape, dtype, &meta_out);

  std::vector<pir::Type> argument_outputs;
  pir::Type out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_out.dtype()), dense_out.dims(), dense_out.layout(), dense_out.lod(), dense_out.offset());
  argument_outputs.push_back(out_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void DataOp::VerifySig() {
  VLOG(4) << "Start Verifying inputs, outputs and attributes for: DataOp.";
  VLOG(4) << "Verifying inputs:";
  {
  auto input_size = num_operands();
  IR_ENFORCE(input_size == 0u,
                    "The size %d of inputs must be equal to 0.", input_size);
  // Inputs num is 0, not need to check inputs type.
  }
  VLOG(4) << "Verifying attributes:";
  {
  auto& attributes = this->attributes();
  IR_ENFORCE(attributes.count("name")>0,
                 "name does not exist.");
  IR_ENFORCE(attributes.at("name").isa<pir::StrAttribute>(),
                 "Type of attribute: name is not pir::StrAttribute.");

  IR_ENFORCE(attributes.count("shape")>0,
                 "shape does not exist.");
  IR_ENFORCE(attributes.at("shape").isa<paddle::dialect::IntArrayAttribute>(),
                 "Type of attribute: shape is not paddle::dialect::IntArrayAttribute.");

  IR_ENFORCE(attributes.count("dtype")>0,
                 "dtype does not exist.");
  IR_ENFORCE(attributes.at("dtype").isa<paddle::dialect::DataTypeAttribute>(),
                 "Type of attribute: dtype is not paddle::dialect::DataTypeAttribute.");

  IR_ENFORCE(attributes.count("place")>0,
                 "place does not exist.");
  IR_ENFORCE(attributes.at("place").isa<paddle::dialect::PlaceAttribute>(),
                 "Type of attribute: place is not paddle::dialect::PlaceAttribute.");

  }
  VLOG(4) << "Verifying outputs:";
  {
  auto output_size = num_results();
  IR_ENFORCE(output_size == 1u,
                    "The size %d of outputs must be equal to 1.", output_size);
  IR_ENFORCE((*this)->result(0).type().isa<paddle::dialect::DenseTensorType>(),
                 "Type validation failed for the 0th output.");
  }
  VLOG(4) << "End Verifying for: DataOp.";
}

void DataOp::InferMeta( phi::InferMetaContext *infer_meta ) {
  auto fn = PD_INFER_META(phi::DataInferMeta);
  fn(infer_meta);
}

phi::DataType DataOp::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: DataOp";
  


  return expected_kernel_dtype;
}

bool DataOp::InferSymbolicShape(pir::ShapeConstraintIRAnalysis* shape_analysis) {
  VLOG(4) << "Infer symbolic shape for op: DataOp";
  return DataOpInferSymbolicShape(this->operation(), shape_analysis);
}

const char *DepthwiseConv2dOp::attributes_name[6] = { "strides", "paddings", "padding_algorithm", "groups", "dilations", "data_format" };

OpInfoTuple DepthwiseConv2dOp::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("input", "paddle::dialect::DenseTensorType", false, false, false, true), paddle::dialect::OpInputInfo("filter", "paddle::dialect::DenseTensorType", false, false, false, true) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = { paddle::dialect::OpAttributeInfo("strides", "pir::ArrayAttribute<pir::Int32Attribute>", ""), paddle::dialect::OpAttributeInfo("paddings", "pir::ArrayAttribute<pir::Int32Attribute>", ""), paddle::dialect::OpAttributeInfo("padding_algorithm", "pir::StrAttribute", ""), paddle::dialect::OpAttributeInfo("groups", "pir::Int32Attribute", ""), paddle::dialect::OpAttributeInfo("dilations", "pir::ArrayAttribute<pir::Int32Attribute>", ""), paddle::dialect::OpAttributeInfo("data_format", "pir::StrAttribute", "") };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("out", "paddle::dialect::DenseTensorType", false, false) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("DepthwiseConvInferMeta", {"input", "filter", "strides", "paddings", "padding_algorithm", "groups", "dilations", "data_format"}, "depthwise_conv2d", {"input", "filter", "strides", "paddings", "padding_algorithm", "groups", "dilations", "data_format"}, {}, {}, {}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "depthwise_conv2d");
}

void DepthwiseConv2dOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value input_, pir::Value filter_, const std::vector<int>& strides, const std::vector<int>& paddings, const std::string& padding_algorithm, int groups, const std::vector<int>& dilations, const std::string& data_format) {
  VLOG(4) << "Start build DepthwiseConv2dOp";



  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {input_, filter_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  std::vector<pir::Attribute> vec_strides;
  for (size_t i = 0; i < static_cast<size_t>(strides.size()); i++) {
      pir::Attribute attr_strides = pir::Int32Attribute::get(pir::IrContext::Instance(), strides[i]);

    vec_strides.push_back(attr_strides);
  }
  pir::Attribute attr_strides = pir::ArrayAttribute::get(pir::IrContext::Instance(), vec_strides);
  argument.AddAttribute("strides", attr_strides);
  std::vector<pir::Attribute> vec_paddings;
  for (size_t i = 0; i < static_cast<size_t>(paddings.size()); i++) {
      pir::Attribute attr_paddings = pir::Int32Attribute::get(pir::IrContext::Instance(), paddings[i]);

    vec_paddings.push_back(attr_paddings);
  }
  pir::Attribute attr_paddings = pir::ArrayAttribute::get(pir::IrContext::Instance(), vec_paddings);
  argument.AddAttribute("paddings", attr_paddings);
  pir::Attribute attr_padding_algorithm = pir::StrAttribute::get(pir::IrContext::Instance(), padding_algorithm);
  argument.AddAttribute("padding_algorithm", attr_padding_algorithm);
  pir::Attribute attr_groups = pir::Int32Attribute::get(pir::IrContext::Instance(), groups);
  argument.AddAttribute("groups", attr_groups);
  std::vector<pir::Attribute> vec_dilations;
  for (size_t i = 0; i < static_cast<size_t>(dilations.size()); i++) {
      pir::Attribute attr_dilations = pir::Int32Attribute::get(pir::IrContext::Instance(), dilations[i]);

    vec_dilations.push_back(attr_dilations);
  }
  pir::Attribute attr_dilations = pir::ArrayAttribute::get(pir::IrContext::Instance(), vec_dilations);
  argument.AddAttribute("dilations", attr_dilations);
  pir::Attribute attr_data_format = pir::StrAttribute::get(pir::IrContext::Instance(), data_format);
  argument.AddAttribute("data_format", attr_data_format);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType input = input_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)input;
  paddle::dialect::DenseTensorType filter = filter_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)filter;

  VLOG(4) << "Builder construction  dense_input";
  paddle::dialect::IrTensor ir_tensor_input(paddle::dialect::TransToPhiDataType(input.dtype()),
                                                      input.dims(),
                                                      input.data_layout(),
                                                      input.lod(),
                                                      input.offset());
  VLOG(4) << "Builder construction  meta_input";
  paddle::dialect::IrMetaTensor meta_input(&ir_tensor_input);

  VLOG(4) << "Builder construction  dense_filter";
  paddle::dialect::IrTensor ir_tensor_filter(paddle::dialect::TransToPhiDataType(filter.dtype()),
                                                      filter.dims(),
                                                      filter.data_layout(),
                                                      filter.lod(),
                                                      filter.offset());
  VLOG(4) << "Builder construction  meta_filter";
  paddle::dialect::IrMetaTensor meta_filter(&ir_tensor_filter);
  paddle::dialect::IrTensor dense_out;
  paddle::dialect::IrMetaTensor meta_out(&dense_out);

  phi::DepthwiseConvInferMeta(meta_input, meta_filter, strides, paddings, padding_algorithm, groups, dilations, data_format, &meta_out);

  std::vector<pir::Type> argument_outputs;
  pir::Type out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_out.dtype()), dense_out.dims(), dense_out.layout(), dense_out.lod(), dense_out.offset());
  argument_outputs.push_back(out_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void DepthwiseConv2dOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value input_, pir::Value filter_, pir::AttributeMap attributes) {
  VLOG(4) << "Start build DepthwiseConv2dOp";


  IR_ENFORCE(
      attributes.find("strides") != attributes.end(),
          "'strides' Attribute is expected for DepthwiseConv2dOp. ");
  std::vector<int> strides;
  for (size_t i = 0; i < attributes.at("strides").dyn_cast<pir::ArrayAttribute>().size(); i++) {
    strides.push_back(attributes.at("strides").dyn_cast<pir::ArrayAttribute>().at(i).dyn_cast<pir::Int32Attribute>().data());
  }

  IR_ENFORCE(
      attributes.find("paddings") != attributes.end(),
          "'paddings' Attribute is expected for DepthwiseConv2dOp. ");
  std::vector<int> paddings;
  for (size_t i = 0; i < attributes.at("paddings").dyn_cast<pir::ArrayAttribute>().size(); i++) {
    paddings.push_back(attributes.at("paddings").dyn_cast<pir::ArrayAttribute>().at(i).dyn_cast<pir::Int32Attribute>().data());
  }

  IR_ENFORCE(
      attributes.find("padding_algorithm") != attributes.end(),
          "'padding_algorithm' Attribute is expected for DepthwiseConv2dOp. ");
  std::string padding_algorithm = attributes.at("padding_algorithm").dyn_cast<pir::StrAttribute>().AsString();

  IR_ENFORCE(
      attributes.find("groups") != attributes.end(),
          "'groups' Attribute is expected for DepthwiseConv2dOp. ");
  int groups = attributes.at("groups").dyn_cast<pir::Int32Attribute>().data();

  IR_ENFORCE(
      attributes.find("dilations") != attributes.end(),
          "'dilations' Attribute is expected for DepthwiseConv2dOp. ");
  std::vector<int> dilations;
  for (size_t i = 0; i < attributes.at("dilations").dyn_cast<pir::ArrayAttribute>().size(); i++) {
    dilations.push_back(attributes.at("dilations").dyn_cast<pir::ArrayAttribute>().at(i).dyn_cast<pir::Int32Attribute>().data());
  }

  IR_ENFORCE(
      attributes.find("data_format") != attributes.end(),
          "'data_format' Attribute is expected for DepthwiseConv2dOp. ");
  std::string data_format = attributes.at("data_format").dyn_cast<pir::StrAttribute>().AsString();


  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {input_, filter_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  std::vector<pir::Attribute> vec_strides;
  for (size_t i = 0; i < static_cast<size_t>(strides.size()); i++) {
      pir::Attribute attr_strides = pir::Int32Attribute::get(pir::IrContext::Instance(), strides[i]);

    vec_strides.push_back(attr_strides);
  }
  pir::Attribute attr_strides = pir::ArrayAttribute::get(pir::IrContext::Instance(), vec_strides);
  argument.AddAttribute("strides", attr_strides);
  std::vector<pir::Attribute> vec_paddings;
  for (size_t i = 0; i < static_cast<size_t>(paddings.size()); i++) {
      pir::Attribute attr_paddings = pir::Int32Attribute::get(pir::IrContext::Instance(), paddings[i]);

    vec_paddings.push_back(attr_paddings);
  }
  pir::Attribute attr_paddings = pir::ArrayAttribute::get(pir::IrContext::Instance(), vec_paddings);
  argument.AddAttribute("paddings", attr_paddings);
  pir::Attribute attr_padding_algorithm = pir::StrAttribute::get(pir::IrContext::Instance(), padding_algorithm);
  argument.AddAttribute("padding_algorithm", attr_padding_algorithm);
  pir::Attribute attr_groups = pir::Int32Attribute::get(pir::IrContext::Instance(), groups);
  argument.AddAttribute("groups", attr_groups);
  std::vector<pir::Attribute> vec_dilations;
  for (size_t i = 0; i < static_cast<size_t>(dilations.size()); i++) {
      pir::Attribute attr_dilations = pir::Int32Attribute::get(pir::IrContext::Instance(), dilations[i]);

    vec_dilations.push_back(attr_dilations);
  }
  pir::Attribute attr_dilations = pir::ArrayAttribute::get(pir::IrContext::Instance(), vec_dilations);
  argument.AddAttribute("dilations", attr_dilations);
  pir::Attribute attr_data_format = pir::StrAttribute::get(pir::IrContext::Instance(), data_format);
  argument.AddAttribute("data_format", attr_data_format);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType input = input_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)input;
  paddle::dialect::DenseTensorType filter = filter_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)filter;

  VLOG(4) << "Builder construction  dense_input";
  paddle::dialect::IrTensor ir_tensor_input(paddle::dialect::TransToPhiDataType(input.dtype()),
                                                      input.dims(),
                                                      input.data_layout(),
                                                      input.lod(),
                                                      input.offset());
  VLOG(4) << "Builder construction  meta_input";
  paddle::dialect::IrMetaTensor meta_input(&ir_tensor_input);

  VLOG(4) << "Builder construction  dense_filter";
  paddle::dialect::IrTensor ir_tensor_filter(paddle::dialect::TransToPhiDataType(filter.dtype()),
                                                      filter.dims(),
                                                      filter.data_layout(),
                                                      filter.lod(),
                                                      filter.offset());
  VLOG(4) << "Builder construction  meta_filter";
  paddle::dialect::IrMetaTensor meta_filter(&ir_tensor_filter);
  paddle::dialect::IrTensor dense_out;
  paddle::dialect::IrMetaTensor meta_out(&dense_out);

  phi::DepthwiseConvInferMeta(meta_input, meta_filter, strides, paddings, padding_algorithm, groups, dilations, data_format, &meta_out);

  std::vector<pir::Type> argument_outputs;
  pir::Type out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_out.dtype()), dense_out.dims(), dense_out.layout(), dense_out.lod(), dense_out.offset());
  argument_outputs.push_back(out_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void DepthwiseConv2dOp::VerifySig() {
  VLOG(4) << "Start Verifying inputs, outputs and attributes for: DepthwiseConv2dOp.";
  VLOG(4) << "Verifying inputs:";
  {
  auto input_size = num_operands();
  IR_ENFORCE(input_size == 2u,
                    "The size %d of inputs must be equal to 2.", input_size);
  IR_ENFORCE((*this)->operand_source(0).type().isa<paddle::dialect::DenseTensorType>(),
                  "Type validation failed for the 0th input, got %s.", (*this)->operand_source(0).type());
  IR_ENFORCE((*this)->operand_source(1).type().isa<paddle::dialect::DenseTensorType>(),
                  "Type validation failed for the 1th input, got %s.", (*this)->operand_source(1).type());
  }
  VLOG(4) << "Verifying attributes:";
  {
  auto& attributes = this->attributes();
  IR_ENFORCE(attributes.count("strides")>0,
                 "strides does not exist.");
  IR_ENFORCE(attributes.at("strides").isa<pir::ArrayAttribute>(),
                 "Type of attribute: strides is not pir::ArrayAttribute.");
  for (size_t i = 0; i < attributes.at("strides").dyn_cast<pir::ArrayAttribute>().size(); i++) {
    IR_ENFORCE(attributes.at("strides").dyn_cast<pir::ArrayAttribute>().at(i).isa<pir::Int32Attribute>(),
                   "Type of attribute: strides is not right.");
  }
  IR_ENFORCE(attributes.count("paddings")>0,
                 "paddings does not exist.");
  IR_ENFORCE(attributes.at("paddings").isa<pir::ArrayAttribute>(),
                 "Type of attribute: paddings is not pir::ArrayAttribute.");
  for (size_t i = 0; i < attributes.at("paddings").dyn_cast<pir::ArrayAttribute>().size(); i++) {
    IR_ENFORCE(attributes.at("paddings").dyn_cast<pir::ArrayAttribute>().at(i).isa<pir::Int32Attribute>(),
                   "Type of attribute: paddings is not right.");
  }
  IR_ENFORCE(attributes.count("padding_algorithm")>0,
                 "padding_algorithm does not exist.");
  IR_ENFORCE(attributes.at("padding_algorithm").isa<pir::StrAttribute>(),
                 "Type of attribute: padding_algorithm is not pir::StrAttribute.");

  IR_ENFORCE(attributes.count("groups")>0,
                 "groups does not exist.");
  IR_ENFORCE(attributes.at("groups").isa<pir::Int32Attribute>(),
                 "Type of attribute: groups is not pir::Int32Attribute.");

  IR_ENFORCE(attributes.count("dilations")>0,
                 "dilations does not exist.");
  IR_ENFORCE(attributes.at("dilations").isa<pir::ArrayAttribute>(),
                 "Type of attribute: dilations is not pir::ArrayAttribute.");
  for (size_t i = 0; i < attributes.at("dilations").dyn_cast<pir::ArrayAttribute>().size(); i++) {
    IR_ENFORCE(attributes.at("dilations").dyn_cast<pir::ArrayAttribute>().at(i).isa<pir::Int32Attribute>(),
                   "Type of attribute: dilations is not right.");
  }
  IR_ENFORCE(attributes.count("data_format")>0,
                 "data_format does not exist.");
  IR_ENFORCE(attributes.at("data_format").isa<pir::StrAttribute>(),
                 "Type of attribute: data_format is not pir::StrAttribute.");

  }
  VLOG(4) << "Verifying outputs:";
  {
  auto output_size = num_results();
  IR_ENFORCE(output_size == 1u,
                    "The size %d of outputs must be equal to 1.", output_size);
  IR_ENFORCE((*this)->result(0).type().isa<paddle::dialect::DenseTensorType>(),
                 "Type validation failed for the 0th output.");
  }
  VLOG(4) << "End Verifying for: DepthwiseConv2dOp.";
}

void DepthwiseConv2dOp::InferMeta( phi::InferMetaContext *infer_meta ) {
  auto fn = PD_INFER_META(phi::DepthwiseConvInferMeta);
  fn(infer_meta);
}

phi::DataType DepthwiseConv2dOp::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: DepthwiseConv2dOp";
  


  return expected_kernel_dtype;
}

OpInfoTuple DetOp::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("x", "paddle::dialect::DenseTensorType", false, false, false, true) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = {  };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("out", "paddle::dialect::DenseTensorType", false, false) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("UnchangedInferMeta", {"x"}, "determinant", {"x"}, {}, {}, {}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "det");
}

void DetOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_) {
  VLOG(4) << "Start build DetOp";



  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);
  paddle::dialect::IrTensor dense_out;
  paddle::dialect::IrMetaTensor meta_out(&dense_out);

  phi::UnchangedInferMeta(meta_x, &meta_out);

  std::vector<pir::Type> argument_outputs;
  pir::Type out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_out.dtype()), dense_out.dims(), dense_out.layout(), dense_out.lod(), dense_out.offset());
  argument_outputs.push_back(out_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void DetOp::VerifySig() {
  VLOG(4) << "Start Verifying inputs, outputs and attributes for: DetOp.";
  VLOG(4) << "Verifying inputs:";
  {
  auto input_size = num_operands();
  IR_ENFORCE(input_size == 1u,
                    "The size %d of inputs must be equal to 1.", input_size);
  IR_ENFORCE((*this)->operand_source(0).type().isa<paddle::dialect::DenseTensorType>(),
                  "Type validation failed for the 0th input, got %s.", (*this)->operand_source(0).type());
  }
  VLOG(4) << "Verifying attributes:";
  {
  // Attributes num is 0, not need to check attributes type.
  }
  VLOG(4) << "Verifying outputs:";
  {
  auto output_size = num_results();
  IR_ENFORCE(output_size == 1u,
                    "The size %d of outputs must be equal to 1.", output_size);
  IR_ENFORCE((*this)->result(0).type().isa<paddle::dialect::DenseTensorType>(),
                 "Type validation failed for the 0th output.");
  }
  VLOG(4) << "End Verifying for: DetOp.";
}

void DetOp::InferMeta( phi::InferMetaContext *infer_meta ) {
  auto fn = PD_INFER_META(phi::UnchangedInferMeta);
  fn(infer_meta);
}

phi::DataType DetOp::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: DetOp";
  


  return expected_kernel_dtype;
}

const char *DiagOp::attributes_name[2] = { "offset", "padding_value" };

OpInfoTuple DiagOp::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("x", "paddle::dialect::DenseTensorType", false, false, false, true) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = { paddle::dialect::OpAttributeInfo("offset", "pir::Int32Attribute", ""), paddle::dialect::OpAttributeInfo("padding_value", "pir::FloatAttribute", "") };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("out", "paddle::dialect::DenseTensorType", false, false) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("DiagInferMeta", {"x", "offset", "padding_value"}, "diag", {"x", "offset", "padding_value"}, {}, {}, {}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "diag");
}

void DiagOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, int offset, float padding_value) {
  VLOG(4) << "Start build DiagOp";



  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_offset = pir::Int32Attribute::get(pir::IrContext::Instance(), offset);
  argument.AddAttribute("offset", attr_offset);
  pir::Attribute attr_padding_value = pir::FloatAttribute::get(pir::IrContext::Instance(), padding_value);
  argument.AddAttribute("padding_value", attr_padding_value);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);
  paddle::dialect::IrTensor dense_out;
  paddle::dialect::IrMetaTensor meta_out(&dense_out);

  phi::DiagInferMeta(meta_x, offset, padding_value, &meta_out);

  std::vector<pir::Type> argument_outputs;
  pir::Type out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_out.dtype()), dense_out.dims(), dense_out.layout(), dense_out.lod(), dense_out.offset());
  argument_outputs.push_back(out_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void DiagOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, pir::AttributeMap attributes) {
  VLOG(4) << "Start build DiagOp";


  IR_ENFORCE(
      attributes.find("offset") != attributes.end(),
          "'offset' Attribute is expected for DiagOp. ");
  int offset = attributes.at("offset").dyn_cast<pir::Int32Attribute>().data();

  IR_ENFORCE(
      attributes.find("padding_value") != attributes.end(),
          "'padding_value' Attribute is expected for DiagOp. ");
  float padding_value = attributes.at("padding_value").dyn_cast<pir::FloatAttribute>().data();


  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_offset = pir::Int32Attribute::get(pir::IrContext::Instance(), offset);
  argument.AddAttribute("offset", attr_offset);
  pir::Attribute attr_padding_value = pir::FloatAttribute::get(pir::IrContext::Instance(), padding_value);
  argument.AddAttribute("padding_value", attr_padding_value);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);
  paddle::dialect::IrTensor dense_out;
  paddle::dialect::IrMetaTensor meta_out(&dense_out);

  phi::DiagInferMeta(meta_x, offset, padding_value, &meta_out);

  std::vector<pir::Type> argument_outputs;
  pir::Type out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_out.dtype()), dense_out.dims(), dense_out.layout(), dense_out.lod(), dense_out.offset());
  argument_outputs.push_back(out_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void DiagOp::VerifySig() {
  VLOG(4) << "Start Verifying inputs, outputs and attributes for: DiagOp.";
  VLOG(4) << "Verifying inputs:";
  {
  auto input_size = num_operands();
  IR_ENFORCE(input_size == 1u,
                    "The size %d of inputs must be equal to 1.", input_size);
  IR_ENFORCE((*this)->operand_source(0).type().isa<paddle::dialect::DenseTensorType>(),
                  "Type validation failed for the 0th input, got %s.", (*this)->operand_source(0).type());
  }
  VLOG(4) << "Verifying attributes:";
  {
  auto& attributes = this->attributes();
  IR_ENFORCE(attributes.count("offset")>0,
                 "offset does not exist.");
  IR_ENFORCE(attributes.at("offset").isa<pir::Int32Attribute>(),
                 "Type of attribute: offset is not pir::Int32Attribute.");

  IR_ENFORCE(attributes.count("padding_value")>0,
                 "padding_value does not exist.");
  IR_ENFORCE(attributes.at("padding_value").isa<pir::FloatAttribute>(),
                 "Type of attribute: padding_value is not pir::FloatAttribute.");

  }
  VLOG(4) << "Verifying outputs:";
  {
  auto output_size = num_results();
  IR_ENFORCE(output_size == 1u,
                    "The size %d of outputs must be equal to 1.", output_size);
  IR_ENFORCE((*this)->result(0).type().isa<paddle::dialect::DenseTensorType>(),
                 "Type validation failed for the 0th output.");
  }
  VLOG(4) << "End Verifying for: DiagOp.";
}

void DiagOp::InferMeta( phi::InferMetaContext *infer_meta ) {
  auto fn = PD_INFER_META(phi::DiagInferMeta);
  fn(infer_meta);
}

phi::DataType DiagOp::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: DiagOp";
  


  return expected_kernel_dtype;
}

const char *DiagEmbedOp::attributes_name[3] = { "offset", "dim1", "dim2" };

OpInfoTuple DiagEmbedOp::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("input", "paddle::dialect::DenseTensorType", false, false, false, false) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = { paddle::dialect::OpAttributeInfo("offset", "pir::Int32Attribute", ""), paddle::dialect::OpAttributeInfo("dim1", "pir::Int32Attribute", ""), paddle::dialect::OpAttributeInfo("dim2", "pir::Int32Attribute", "") };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("out", "paddle::dialect::DenseTensorType", false, false) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("DiagEmbedInferMeta", {"input", "offset", "dim1", "dim2"}, "diag_embed", {"input", "offset", "dim1", "dim2"}, {}, {}, {}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "diag_embed");
}

void DiagEmbedOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value input_, int offset, int dim1, int dim2) {
  VLOG(4) << "Start build DiagEmbedOp";



  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {input_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_offset = pir::Int32Attribute::get(pir::IrContext::Instance(), offset);
  argument.AddAttribute("offset", attr_offset);
  pir::Attribute attr_dim1 = pir::Int32Attribute::get(pir::IrContext::Instance(), dim1);
  argument.AddAttribute("dim1", attr_dim1);
  pir::Attribute attr_dim2 = pir::Int32Attribute::get(pir::IrContext::Instance(), dim2);
  argument.AddAttribute("dim2", attr_dim2);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType input = input_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)input;

  VLOG(4) << "Builder construction  dense_input";
  paddle::dialect::IrTensor ir_tensor_input(paddle::dialect::TransToPhiDataType(input.dtype()),
                                                      input.dims(),
                                                      input.data_layout(),
                                                      input.lod(),
                                                      input.offset());
  VLOG(4) << "Builder construction  meta_input";
  paddle::dialect::IrMetaTensor meta_input(&ir_tensor_input);
  paddle::dialect::IrTensor dense_out;
  paddle::dialect::IrMetaTensor meta_out(&dense_out);

  phi::DiagEmbedInferMeta(meta_input, offset, dim1, dim2, &meta_out);

  std::vector<pir::Type> argument_outputs;
  pir::Type out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_out.dtype()), dense_out.dims(), dense_out.layout(), dense_out.lod(), dense_out.offset());
  argument_outputs.push_back(out_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void DiagEmbedOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value input_, pir::AttributeMap attributes) {
  VLOG(4) << "Start build DiagEmbedOp";


  IR_ENFORCE(
      attributes.find("offset") != attributes.end(),
          "'offset' Attribute is expected for DiagEmbedOp. ");
  int offset = attributes.at("offset").dyn_cast<pir::Int32Attribute>().data();

  IR_ENFORCE(
      attributes.find("dim1") != attributes.end(),
          "'dim1' Attribute is expected for DiagEmbedOp. ");
  int dim1 = attributes.at("dim1").dyn_cast<pir::Int32Attribute>().data();

  IR_ENFORCE(
      attributes.find("dim2") != attributes.end(),
          "'dim2' Attribute is expected for DiagEmbedOp. ");
  int dim2 = attributes.at("dim2").dyn_cast<pir::Int32Attribute>().data();


  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {input_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_offset = pir::Int32Attribute::get(pir::IrContext::Instance(), offset);
  argument.AddAttribute("offset", attr_offset);
  pir::Attribute attr_dim1 = pir::Int32Attribute::get(pir::IrContext::Instance(), dim1);
  argument.AddAttribute("dim1", attr_dim1);
  pir::Attribute attr_dim2 = pir::Int32Attribute::get(pir::IrContext::Instance(), dim2);
  argument.AddAttribute("dim2", attr_dim2);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType input = input_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)input;

  VLOG(4) << "Builder construction  dense_input";
  paddle::dialect::IrTensor ir_tensor_input(paddle::dialect::TransToPhiDataType(input.dtype()),
                                                      input.dims(),
                                                      input.data_layout(),
                                                      input.lod(),
                                                      input.offset());
  VLOG(4) << "Builder construction  meta_input";
  paddle::dialect::IrMetaTensor meta_input(&ir_tensor_input);
  paddle::dialect::IrTensor dense_out;
  paddle::dialect::IrMetaTensor meta_out(&dense_out);

  phi::DiagEmbedInferMeta(meta_input, offset, dim1, dim2, &meta_out);

  std::vector<pir::Type> argument_outputs;
  pir::Type out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_out.dtype()), dense_out.dims(), dense_out.layout(), dense_out.lod(), dense_out.offset());
  argument_outputs.push_back(out_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void DiagEmbedOp::VerifySig() {
  VLOG(4) << "Start Verifying inputs, outputs and attributes for: DiagEmbedOp.";
  VLOG(4) << "Verifying inputs:";
  {
  auto input_size = num_operands();
  IR_ENFORCE(input_size == 1u,
                    "The size %d of inputs must be equal to 1.", input_size);
  IR_ENFORCE((*this)->operand_source(0).type().isa<paddle::dialect::DenseTensorType>(),
                  "Type validation failed for the 0th input, got %s.", (*this)->operand_source(0).type());
  }
  VLOG(4) << "Verifying attributes:";
  {
  auto& attributes = this->attributes();
  IR_ENFORCE(attributes.count("offset")>0,
                 "offset does not exist.");
  IR_ENFORCE(attributes.at("offset").isa<pir::Int32Attribute>(),
                 "Type of attribute: offset is not pir::Int32Attribute.");

  IR_ENFORCE(attributes.count("dim1")>0,
                 "dim1 does not exist.");
  IR_ENFORCE(attributes.at("dim1").isa<pir::Int32Attribute>(),
                 "Type of attribute: dim1 is not pir::Int32Attribute.");

  IR_ENFORCE(attributes.count("dim2")>0,
                 "dim2 does not exist.");
  IR_ENFORCE(attributes.at("dim2").isa<pir::Int32Attribute>(),
                 "Type of attribute: dim2 is not pir::Int32Attribute.");

  }
  VLOG(4) << "Verifying outputs:";
  {
  auto output_size = num_results();
  IR_ENFORCE(output_size == 1u,
                    "The size %d of outputs must be equal to 1.", output_size);
  IR_ENFORCE((*this)->result(0).type().isa<paddle::dialect::DenseTensorType>(),
                 "Type validation failed for the 0th output.");
  }
  VLOG(4) << "End Verifying for: DiagEmbedOp.";
}

void DiagEmbedOp::InferMeta( phi::InferMetaContext *infer_meta ) {
  auto fn = PD_INFER_META(phi::DiagEmbedInferMeta);
  fn(infer_meta);
}

phi::DataType DiagEmbedOp::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: DiagEmbedOp";
  


  return expected_kernel_dtype;
}

const char *DiagonalOp::attributes_name[3] = { "offset", "axis1", "axis2" };

OpInfoTuple DiagonalOp::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("x", "paddle::dialect::DenseTensorType", false, false, false, true) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = { paddle::dialect::OpAttributeInfo("offset", "pir::Int32Attribute", ""), paddle::dialect::OpAttributeInfo("axis1", "pir::Int32Attribute", ""), paddle::dialect::OpAttributeInfo("axis2", "pir::Int32Attribute", "") };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("out", "paddle::dialect::DenseTensorType", false, false) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("DiagonalInferMeta", {"x", "offset", "axis1", "axis2"}, "diagonal", {"x", "offset", "axis1", "axis2"}, {}, {}, {}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "diagonal");
}

void DiagonalOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, int offset, int axis1, int axis2) {
  VLOG(4) << "Start build DiagonalOp";



  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_offset = pir::Int32Attribute::get(pir::IrContext::Instance(), offset);
  argument.AddAttribute("offset", attr_offset);
  pir::Attribute attr_axis1 = pir::Int32Attribute::get(pir::IrContext::Instance(), axis1);
  argument.AddAttribute("axis1", attr_axis1);
  pir::Attribute attr_axis2 = pir::Int32Attribute::get(pir::IrContext::Instance(), axis2);
  argument.AddAttribute("axis2", attr_axis2);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);
  paddle::dialect::IrTensor dense_out;
  paddle::dialect::IrMetaTensor meta_out(&dense_out);

  phi::DiagonalInferMeta(meta_x, offset, axis1, axis2, &meta_out);

  std::vector<pir::Type> argument_outputs;
  pir::Type out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_out.dtype()), dense_out.dims(), dense_out.layout(), dense_out.lod(), dense_out.offset());
  argument_outputs.push_back(out_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void DiagonalOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, pir::AttributeMap attributes) {
  VLOG(4) << "Start build DiagonalOp";


  IR_ENFORCE(
      attributes.find("offset") != attributes.end(),
          "'offset' Attribute is expected for DiagonalOp. ");
  int offset = attributes.at("offset").dyn_cast<pir::Int32Attribute>().data();

  IR_ENFORCE(
      attributes.find("axis1") != attributes.end(),
          "'axis1' Attribute is expected for DiagonalOp. ");
  int axis1 = attributes.at("axis1").dyn_cast<pir::Int32Attribute>().data();

  IR_ENFORCE(
      attributes.find("axis2") != attributes.end(),
          "'axis2' Attribute is expected for DiagonalOp. ");
  int axis2 = attributes.at("axis2").dyn_cast<pir::Int32Attribute>().data();


  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_offset = pir::Int32Attribute::get(pir::IrContext::Instance(), offset);
  argument.AddAttribute("offset", attr_offset);
  pir::Attribute attr_axis1 = pir::Int32Attribute::get(pir::IrContext::Instance(), axis1);
  argument.AddAttribute("axis1", attr_axis1);
  pir::Attribute attr_axis2 = pir::Int32Attribute::get(pir::IrContext::Instance(), axis2);
  argument.AddAttribute("axis2", attr_axis2);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);
  paddle::dialect::IrTensor dense_out;
  paddle::dialect::IrMetaTensor meta_out(&dense_out);

  phi::DiagonalInferMeta(meta_x, offset, axis1, axis2, &meta_out);

  std::vector<pir::Type> argument_outputs;
  pir::Type out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_out.dtype()), dense_out.dims(), dense_out.layout(), dense_out.lod(), dense_out.offset());
  argument_outputs.push_back(out_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void DiagonalOp::VerifySig() {
  VLOG(4) << "Start Verifying inputs, outputs and attributes for: DiagonalOp.";
  VLOG(4) << "Verifying inputs:";
  {
  auto input_size = num_operands();
  IR_ENFORCE(input_size == 1u,
                    "The size %d of inputs must be equal to 1.", input_size);
  IR_ENFORCE((*this)->operand_source(0).type().isa<paddle::dialect::DenseTensorType>(),
                  "Type validation failed for the 0th input, got %s.", (*this)->operand_source(0).type());
  }
  VLOG(4) << "Verifying attributes:";
  {
  auto& attributes = this->attributes();
  IR_ENFORCE(attributes.count("offset")>0,
                 "offset does not exist.");
  IR_ENFORCE(attributes.at("offset").isa<pir::Int32Attribute>(),
                 "Type of attribute: offset is not pir::Int32Attribute.");

  IR_ENFORCE(attributes.count("axis1")>0,
                 "axis1 does not exist.");
  IR_ENFORCE(attributes.at("axis1").isa<pir::Int32Attribute>(),
                 "Type of attribute: axis1 is not pir::Int32Attribute.");

  IR_ENFORCE(attributes.count("axis2")>0,
                 "axis2 does not exist.");
  IR_ENFORCE(attributes.at("axis2").isa<pir::Int32Attribute>(),
                 "Type of attribute: axis2 is not pir::Int32Attribute.");

  }
  VLOG(4) << "Verifying outputs:";
  {
  auto output_size = num_results();
  IR_ENFORCE(output_size == 1u,
                    "The size %d of outputs must be equal to 1.", output_size);
  IR_ENFORCE((*this)->result(0).type().isa<paddle::dialect::DenseTensorType>(),
                 "Type validation failed for the 0th output.");
  }
  VLOG(4) << "End Verifying for: DiagonalOp.";
}

void DiagonalOp::InferMeta( phi::InferMetaContext *infer_meta ) {
  auto fn = PD_INFER_META(phi::DiagonalInferMeta);
  fn(infer_meta);
}

phi::DataType DiagonalOp::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: DiagonalOp";
  


  return expected_kernel_dtype;
}

OpInfoTuple DigammaOp::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("x", "paddle::dialect::DenseTensorType", false, false, false, true) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = {  };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("out", "paddle::dialect::DenseTensorType", false, false) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("UnchangedInferMeta", {"x"}, "digamma", {"x"}, {}, {}, {}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "digamma");
}

void DigammaOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_) {
  VLOG(4) << "Start build DigammaOp";



  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);
  paddle::dialect::IrTensor dense_out;
  paddle::dialect::IrMetaTensor meta_out(&dense_out);

  phi::UnchangedInferMeta(meta_x, &meta_out);

  std::vector<pir::Type> argument_outputs;
  pir::Type out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_out.dtype()), dense_out.dims(), dense_out.layout(), dense_out.lod(), dense_out.offset());
  argument_outputs.push_back(out_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void DigammaOp::VerifySig() {
  VLOG(4) << "Start Verifying inputs, outputs and attributes for: DigammaOp.";
  VLOG(4) << "Verifying inputs:";
  {
  auto input_size = num_operands();
  IR_ENFORCE(input_size == 1u,
                    "The size %d of inputs must be equal to 1.", input_size);
  IR_ENFORCE((*this)->operand_source(0).type().isa<paddle::dialect::DenseTensorType>(),
                  "Type validation failed for the 0th input, got %s.", (*this)->operand_source(0).type());
  }
  VLOG(4) << "Verifying attributes:";
  {
  // Attributes num is 0, not need to check attributes type.
  }
  VLOG(4) << "Verifying outputs:";
  {
  auto output_size = num_results();
  IR_ENFORCE(output_size == 1u,
                    "The size %d of outputs must be equal to 1.", output_size);
  IR_ENFORCE((*this)->result(0).type().isa<paddle::dialect::DenseTensorType>(),
                 "Type validation failed for the 0th output.");
  }
  VLOG(4) << "End Verifying for: DigammaOp.";
}

void DigammaOp::InferMeta( phi::InferMetaContext *infer_meta ) {
  auto fn = PD_INFER_META(phi::UnchangedInferMeta);
  fn(infer_meta);
}

phi::DataType DigammaOp::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: DigammaOp";
  


  return expected_kernel_dtype;
}

OpInfoTuple Digamma_Op::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("x", "paddle::dialect::DenseTensorType", false, false, false, true) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = {  };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("out", "paddle::dialect::DenseTensorType", false, false) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("UnchangedInferMeta", {"x"}, "digamma", {"x"}, {}, {}, {{"out", "x"}}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "digamma");
}

void Digamma_Op::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_) {
  VLOG(4) << "Start build Digamma_Op";



  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);
  paddle::dialect::IrTensor dense_out;
  paddle::dialect::IrMetaTensor meta_out(&dense_out);

  phi::UnchangedInferMeta(meta_x, &meta_out);

  std::vector<pir::Type> argument_outputs;
  pir::Type out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_out.dtype()), dense_out.dims(), dense_out.layout(), dense_out.lod(), dense_out.offset());
  argument_outputs.push_back(out_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void Digamma_Op::VerifySig() {
  VLOG(4) << "Start Verifying inputs, outputs and attributes for: Digamma_Op.";
  VLOG(4) << "Verifying inputs:";
  {
  auto input_size = num_operands();
  IR_ENFORCE(input_size == 1u,
                    "The size %d of inputs must be equal to 1.", input_size);
  IR_ENFORCE((*this)->operand_source(0).type().isa<paddle::dialect::DenseTensorType>(),
                  "Type validation failed for the 0th input, got %s.", (*this)->operand_source(0).type());
  }
  VLOG(4) << "Verifying attributes:";
  {
  // Attributes num is 0, not need to check attributes type.
  }
  VLOG(4) << "Verifying outputs:";
  {
  auto output_size = num_results();
  IR_ENFORCE(output_size == 1u,
                    "The size %d of outputs must be equal to 1.", output_size);
  IR_ENFORCE((*this)->result(0).type().isa<paddle::dialect::DenseTensorType>(),
                 "Type validation failed for the 0th output.");
  }
  VLOG(4) << "End Verifying for: Digamma_Op.";
}

void Digamma_Op::InferMeta( phi::InferMetaContext *infer_meta ) {
  auto fn = PD_INFER_META(phi::UnchangedInferMeta);
  fn(infer_meta);
}

phi::DataType Digamma_Op::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: Digamma_Op";
  


  return expected_kernel_dtype;
}

OpInfoTuple DirichletOp::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("alpha", "paddle::dialect::DenseTensorType", false, false, false, false) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = {  };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("out", "paddle::dialect::DenseTensorType", false, false) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("DirichletInferMeta", {"alpha"}, "dirichlet", {"alpha"}, {}, {}, {}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "dirichlet");
}

void DirichletOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value alpha_) {
  VLOG(4) << "Start build DirichletOp";



  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {alpha_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType alpha = alpha_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)alpha;

  VLOG(4) << "Builder construction  dense_alpha";
  paddle::dialect::IrTensor ir_tensor_alpha(paddle::dialect::TransToPhiDataType(alpha.dtype()),
                                                      alpha.dims(),
                                                      alpha.data_layout(),
                                                      alpha.lod(),
                                                      alpha.offset());
  VLOG(4) << "Builder construction  meta_alpha";
  paddle::dialect::IrMetaTensor meta_alpha(&ir_tensor_alpha);
  paddle::dialect::IrTensor dense_out;
  paddle::dialect::IrMetaTensor meta_out(&dense_out);

  phi::DirichletInferMeta(meta_alpha, &meta_out);

  std::vector<pir::Type> argument_outputs;
  pir::Type out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_out.dtype()), dense_out.dims(), dense_out.layout(), dense_out.lod(), dense_out.offset());
  argument_outputs.push_back(out_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void DirichletOp::VerifySig() {
  VLOG(4) << "Start Verifying inputs, outputs and attributes for: DirichletOp.";
  VLOG(4) << "Verifying inputs:";
  {
  auto input_size = num_operands();
  IR_ENFORCE(input_size == 1u,
                    "The size %d of inputs must be equal to 1.", input_size);
  IR_ENFORCE((*this)->operand_source(0).type().isa<paddle::dialect::DenseTensorType>(),
                  "Type validation failed for the 0th input, got %s.", (*this)->operand_source(0).type());
  }
  VLOG(4) << "Verifying attributes:";
  {
  // Attributes num is 0, not need to check attributes type.
  }
  VLOG(4) << "Verifying outputs:";
  {
  auto output_size = num_results();
  IR_ENFORCE(output_size == 1u,
                    "The size %d of outputs must be equal to 1.", output_size);
  IR_ENFORCE((*this)->result(0).type().isa<paddle::dialect::DenseTensorType>(),
                 "Type validation failed for the 0th output.");
  }
  VLOG(4) << "End Verifying for: DirichletOp.";
}

void DirichletOp::InferMeta( phi::InferMetaContext *infer_meta ) {
  auto fn = PD_INFER_META(phi::DirichletInferMeta);
  fn(infer_meta);
}

phi::DataType DirichletOp::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: DirichletOp";
  


  return expected_kernel_dtype;
}

const char *DistOp::attributes_name[1] = { "p" };

OpInfoTuple DistOp::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("x", "paddle::dialect::DenseTensorType", false, false, false, true), paddle::dialect::OpInputInfo("y", "paddle::dialect::DenseTensorType", false, false, false, true) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = { paddle::dialect::OpAttributeInfo("p", "pir::FloatAttribute", "") };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("out", "paddle::dialect::DenseTensorType", false, false) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("DistInferMeta", {"x", "y", "p"}, "dist", {"x", "y", "p"}, {}, {}, {}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "dist");
}

void DistOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, pir::Value y_, float p) {
  VLOG(4) << "Start build DistOp";



  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_, y_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_p = pir::FloatAttribute::get(pir::IrContext::Instance(), p);
  argument.AddAttribute("p", attr_p);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;
  paddle::dialect::DenseTensorType y = y_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)y;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);

  VLOG(4) << "Builder construction  dense_y";
  paddle::dialect::IrTensor ir_tensor_y(paddle::dialect::TransToPhiDataType(y.dtype()),
                                                      y.dims(),
                                                      y.data_layout(),
                                                      y.lod(),
                                                      y.offset());
  VLOG(4) << "Builder construction  meta_y";
  paddle::dialect::IrMetaTensor meta_y(&ir_tensor_y);
  paddle::dialect::IrTensor dense_out;
  paddle::dialect::IrMetaTensor meta_out(&dense_out);

  phi::DistInferMeta(meta_x, meta_y, p, &meta_out);

  std::vector<pir::Type> argument_outputs;
  pir::Type out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_out.dtype()), dense_out.dims(), dense_out.layout(), dense_out.lod(), dense_out.offset());
  argument_outputs.push_back(out_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void DistOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, pir::Value y_, pir::AttributeMap attributes) {
  VLOG(4) << "Start build DistOp";


  IR_ENFORCE(
      attributes.find("p") != attributes.end(),
          "'p' Attribute is expected for DistOp. ");
  float p = attributes.at("p").dyn_cast<pir::FloatAttribute>().data();


  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_, y_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_p = pir::FloatAttribute::get(pir::IrContext::Instance(), p);
  argument.AddAttribute("p", attr_p);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;
  paddle::dialect::DenseTensorType y = y_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)y;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);

  VLOG(4) << "Builder construction  dense_y";
  paddle::dialect::IrTensor ir_tensor_y(paddle::dialect::TransToPhiDataType(y.dtype()),
                                                      y.dims(),
                                                      y.data_layout(),
                                                      y.lod(),
                                                      y.offset());
  VLOG(4) << "Builder construction  meta_y";
  paddle::dialect::IrMetaTensor meta_y(&ir_tensor_y);
  paddle::dialect::IrTensor dense_out;
  paddle::dialect::IrMetaTensor meta_out(&dense_out);

  phi::DistInferMeta(meta_x, meta_y, p, &meta_out);

  std::vector<pir::Type> argument_outputs;
  pir::Type out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_out.dtype()), dense_out.dims(), dense_out.layout(), dense_out.lod(), dense_out.offset());
  argument_outputs.push_back(out_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void DistOp::VerifySig() {
  VLOG(4) << "Start Verifying inputs, outputs and attributes for: DistOp.";
  VLOG(4) << "Verifying inputs:";
  {
  auto input_size = num_operands();
  IR_ENFORCE(input_size == 2u,
                    "The size %d of inputs must be equal to 2.", input_size);
  IR_ENFORCE((*this)->operand_source(0).type().isa<paddle::dialect::DenseTensorType>(),
                  "Type validation failed for the 0th input, got %s.", (*this)->operand_source(0).type());
  IR_ENFORCE((*this)->operand_source(1).type().isa<paddle::dialect::DenseTensorType>(),
                  "Type validation failed for the 1th input, got %s.", (*this)->operand_source(1).type());
  }
  VLOG(4) << "Verifying attributes:";
  {
  auto& attributes = this->attributes();
  IR_ENFORCE(attributes.count("p")>0,
                 "p does not exist.");
  IR_ENFORCE(attributes.at("p").isa<pir::FloatAttribute>(),
                 "Type of attribute: p is not pir::FloatAttribute.");

  }
  VLOG(4) << "Verifying outputs:";
  {
  auto output_size = num_results();
  IR_ENFORCE(output_size == 1u,
                    "The size %d of outputs must be equal to 1.", output_size);
  IR_ENFORCE((*this)->result(0).type().isa<paddle::dialect::DenseTensorType>(),
                 "Type validation failed for the 0th output.");
  }
  VLOG(4) << "End Verifying for: DistOp.";
}

void DistOp::InferMeta( phi::InferMetaContext *infer_meta ) {
  auto fn = PD_INFER_META(phi::DistInferMeta);
  fn(infer_meta);
}

phi::DataType DistOp::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: DistOp";
  


  return expected_kernel_dtype;
}

OpInfoTuple DotOp::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("x", "paddle::dialect::DenseTensorType", false, false, false, true), paddle::dialect::OpInputInfo("y", "paddle::dialect::DenseTensorType", false, false, false, true) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = {  };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("out", "paddle::dialect::DenseTensorType", false, false) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("DotInferMeta", {"x", "y"}, "dot", {"x", "y"}, {"x"}, {}, {}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "dot");
}

void DotOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, pir::Value y_) {
  VLOG(4) << "Start build DotOp";



  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_, y_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;
  paddle::dialect::DenseTensorType y = y_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)y;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);

  VLOG(4) << "Builder construction  dense_y";
  paddle::dialect::IrTensor ir_tensor_y(paddle::dialect::TransToPhiDataType(y.dtype()),
                                                      y.dims(),
                                                      y.data_layout(),
                                                      y.lod(),
                                                      y.offset());
  VLOG(4) << "Builder construction  meta_y";
  paddle::dialect::IrMetaTensor meta_y(&ir_tensor_y);
  paddle::dialect::IrTensor dense_out;
  paddle::dialect::IrMetaTensor meta_out(&dense_out);

  phi::DotInferMeta(meta_x, meta_y, &meta_out);

  std::vector<pir::Type> argument_outputs;
  pir::Type out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_out.dtype()), dense_out.dims(), dense_out.layout(), dense_out.lod(), dense_out.offset());
  argument_outputs.push_back(out_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void DotOp::VerifySig() {
  VLOG(4) << "Start Verifying inputs, outputs and attributes for: DotOp.";
  VLOG(4) << "Verifying inputs:";
  {
  auto input_size = num_operands();
  IR_ENFORCE(input_size == 2u,
                    "The size %d of inputs must be equal to 2.", input_size);
  IR_ENFORCE((*this)->operand_source(0).type().isa<paddle::dialect::DenseTensorType>(),
                  "Type validation failed for the 0th input, got %s.", (*this)->operand_source(0).type());
  IR_ENFORCE((*this)->operand_source(1).type().isa<paddle::dialect::DenseTensorType>(),
                  "Type validation failed for the 1th input, got %s.", (*this)->operand_source(1).type());
  }
  VLOG(4) << "Verifying attributes:";
  {
  // Attributes num is 0, not need to check attributes type.
  }
  VLOG(4) << "Verifying outputs:";
  {
  auto output_size = num_results();
  IR_ENFORCE(output_size == 1u,
                    "The size %d of outputs must be equal to 1.", output_size);
  IR_ENFORCE((*this)->result(0).type().isa<paddle::dialect::DenseTensorType>(),
                 "Type validation failed for the 0th output.");
  }
  VLOG(4) << "End Verifying for: DotOp.";
}

void DotOp::InferMeta( phi::InferMetaContext *infer_meta ) {
  auto fn = PD_INFER_META(phi::DotInferMeta);
  fn(infer_meta);
}

phi::DataType DotOp::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: DotOp";
  


  return expected_kernel_dtype;
}

const char *EditDistanceOp::attributes_name[1] = { "normalized" };

OpInfoTuple EditDistanceOp::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("hyps", "paddle::dialect::DenseTensorType", false, false, false, false), paddle::dialect::OpInputInfo("refs", "paddle::dialect::DenseTensorType", false, false, false, false), paddle::dialect::OpInputInfo("hypslength", "paddle::dialect::DenseTensorType", true, false, false, false), paddle::dialect::OpInputInfo("refslength", "paddle::dialect::DenseTensorType", true, false, false, false) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = { paddle::dialect::OpAttributeInfo("normalized", "pir::BoolAttribute", "") };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("sequencenum", "paddle::dialect::DenseTensorType", false, false), paddle::dialect::OpOutputInfo("out", "paddle::dialect::DenseTensorType", false, false) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("EditDistanceInferMeta", {"hyps", "refs", "hypslength", "refslength", "normalized"}, "edit_distance", {"hyps", "refs", "hypslength", "refslength", "normalized"}, {"DataType::FLOAT32"}, {}, {}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "edit_distance");
}

void EditDistanceOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value hyps_, pir::Value refs_, pir::Value hypslength_, pir::Value refslength_, bool normalized) {
  VLOG(4) << "Start build EditDistanceOp";



  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {hyps_, refs_, hypslength_, refslength_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_normalized = pir::BoolAttribute::get(pir::IrContext::Instance(), normalized);
  argument.AddAttribute("normalized", attr_normalized);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType hyps = hyps_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)hyps;
  paddle::dialect::DenseTensorType refs = refs_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)refs;

  VLOG(4) << "Builder construction  dense_hyps";
  paddle::dialect::IrTensor ir_tensor_hyps(paddle::dialect::TransToPhiDataType(hyps.dtype()),
                                                      hyps.dims(),
                                                      hyps.data_layout(),
                                                      hyps.lod(),
                                                      hyps.offset());
  VLOG(4) << "Builder construction  meta_hyps";
  paddle::dialect::IrMetaTensor meta_hyps(&ir_tensor_hyps);

  VLOG(4) << "Builder construction  dense_refs";
  paddle::dialect::IrTensor ir_tensor_refs(paddle::dialect::TransToPhiDataType(refs.dtype()),
                                                      refs.dims(),
                                                      refs.data_layout(),
                                                      refs.lod(),
                                                      refs.offset());
  VLOG(4) << "Builder construction  meta_refs";
  paddle::dialect::IrMetaTensor meta_refs(&ir_tensor_refs);

  paddle::dialect::IrMetaTensor meta_hypslength;
  paddle::dialect::IrTensor ir_tensor_hypslength;
  if (hypslength_.impl() != nullptr) {
    paddle::dialect::DenseTensorType hypslength = hypslength_.type().dyn_cast<paddle::dialect::DenseTensorType>();
    VLOG(4) << "Builder construction  dense_hypslength";
    ir_tensor_hypslength = paddle::dialect::IrTensor(paddle::dialect::TransToPhiDataType(hypslength.dtype()),
                                                        hypslength.dims(),
                                                        hypslength.data_layout(),
                                                        hypslength.lod(),
                                                        hypslength.offset());
    VLOG(4) << "Builder construction  meta_hypslength";
    meta_hypslength = paddle::dialect::IrMetaTensor(&ir_tensor_hypslength);
  }


  paddle::dialect::IrMetaTensor meta_refslength;
  paddle::dialect::IrTensor ir_tensor_refslength;
  if (refslength_.impl() != nullptr) {
    paddle::dialect::DenseTensorType refslength = refslength_.type().dyn_cast<paddle::dialect::DenseTensorType>();
    VLOG(4) << "Builder construction  dense_refslength";
    ir_tensor_refslength = paddle::dialect::IrTensor(paddle::dialect::TransToPhiDataType(refslength.dtype()),
                                                        refslength.dims(),
                                                        refslength.data_layout(),
                                                        refslength.lod(),
                                                        refslength.offset());
    VLOG(4) << "Builder construction  meta_refslength";
    meta_refslength = paddle::dialect::IrMetaTensor(&ir_tensor_refslength);
  }

  paddle::dialect::IrTensor dense_sequencenum;
  paddle::dialect::IrMetaTensor meta_sequencenum(&dense_sequencenum);
  paddle::dialect::IrTensor dense_out;
  paddle::dialect::IrMetaTensor meta_out(&dense_out);

  phi::EditDistanceInferMeta(meta_hyps, meta_refs, meta_hypslength, meta_refslength, normalized, &meta_sequencenum, &meta_out);

  std::vector<pir::Type> argument_outputs;
  pir::Type sequencenum_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_sequencenum.dtype()), dense_sequencenum.dims(), dense_sequencenum.layout(), dense_sequencenum.lod(), dense_sequencenum.offset());
  argument_outputs.push_back(sequencenum_dense_tensor_type);

  pir::Type out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_out.dtype()), dense_out.dims(), dense_out.layout(), dense_out.lod(), dense_out.offset());
  argument_outputs.push_back(out_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void EditDistanceOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value hyps_, pir::Value refs_, pir::Value hypslength_, pir::Value refslength_, pir::AttributeMap attributes) {
  VLOG(4) << "Start build EditDistanceOp";


  IR_ENFORCE(
      attributes.find("normalized") != attributes.end(),
          "'normalized' Attribute is expected for EditDistanceOp. ");
  bool normalized = attributes.at("normalized").dyn_cast<pir::BoolAttribute>().data();


  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {hyps_, refs_, hypslength_, refslength_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_normalized = pir::BoolAttribute::get(pir::IrContext::Instance(), normalized);
  argument.AddAttribute("normalized", attr_normalized);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType hyps = hyps_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)hyps;
  paddle::dialect::DenseTensorType refs = refs_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)refs;

  VLOG(4) << "Builder construction  dense_hyps";
  paddle::dialect::IrTensor ir_tensor_hyps(paddle::dialect::TransToPhiDataType(hyps.dtype()),
                                                      hyps.dims(),
                                                      hyps.data_layout(),
                                                      hyps.lod(),
                                                      hyps.offset());
  VLOG(4) << "Builder construction  meta_hyps";
  paddle::dialect::IrMetaTensor meta_hyps(&ir_tensor_hyps);

  VLOG(4) << "Builder construction  dense_refs";
  paddle::dialect::IrTensor ir_tensor_refs(paddle::dialect::TransToPhiDataType(refs.dtype()),
                                                      refs.dims(),
                                                      refs.data_layout(),
                                                      refs.lod(),
                                                      refs.offset());
  VLOG(4) << "Builder construction  meta_refs";
  paddle::dialect::IrMetaTensor meta_refs(&ir_tensor_refs);

  paddle::dialect::IrMetaTensor meta_hypslength;
  paddle::dialect::IrTensor ir_tensor_hypslength;
  if (hypslength_.impl() != nullptr) {
    paddle::dialect::DenseTensorType hypslength = hypslength_.type().dyn_cast<paddle::dialect::DenseTensorType>();
    VLOG(4) << "Builder construction  dense_hypslength";
    ir_tensor_hypslength = paddle::dialect::IrTensor(paddle::dialect::TransToPhiDataType(hypslength.dtype()),
                                                        hypslength.dims(),
                                                        hypslength.data_layout(),
                                                        hypslength.lod(),
                                                        hypslength.offset());
    VLOG(4) << "Builder construction  meta_hypslength";
    meta_hypslength = paddle::dialect::IrMetaTensor(&ir_tensor_hypslength);
  }


  paddle::dialect::IrMetaTensor meta_refslength;
  paddle::dialect::IrTensor ir_tensor_refslength;
  if (refslength_.impl() != nullptr) {
    paddle::dialect::DenseTensorType refslength = refslength_.type().dyn_cast<paddle::dialect::DenseTensorType>();
    VLOG(4) << "Builder construction  dense_refslength";
    ir_tensor_refslength = paddle::dialect::IrTensor(paddle::dialect::TransToPhiDataType(refslength.dtype()),
                                                        refslength.dims(),
                                                        refslength.data_layout(),
                                                        refslength.lod(),
                                                        refslength.offset());
    VLOG(4) << "Builder construction  meta_refslength";
    meta_refslength = paddle::dialect::IrMetaTensor(&ir_tensor_refslength);
  }

  paddle::dialect::IrTensor dense_sequencenum;
  paddle::dialect::IrMetaTensor meta_sequencenum(&dense_sequencenum);
  paddle::dialect::IrTensor dense_out;
  paddle::dialect::IrMetaTensor meta_out(&dense_out);

  phi::EditDistanceInferMeta(meta_hyps, meta_refs, meta_hypslength, meta_refslength, normalized, &meta_sequencenum, &meta_out);

  std::vector<pir::Type> argument_outputs;
  pir::Type sequencenum_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_sequencenum.dtype()), dense_sequencenum.dims(), dense_sequencenum.layout(), dense_sequencenum.lod(), dense_sequencenum.offset());
  argument_outputs.push_back(sequencenum_dense_tensor_type);

  pir::Type out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_out.dtype()), dense_out.dims(), dense_out.layout(), dense_out.lod(), dense_out.offset());
  argument_outputs.push_back(out_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void EditDistanceOp::VerifySig() {
  VLOG(4) << "Start Verifying inputs, outputs and attributes for: EditDistanceOp.";
  VLOG(4) << "Verifying inputs:";
  {
  auto input_size = num_operands();
  IR_ENFORCE(input_size == 4u,
                    "The size %d of inputs must be equal to 4.", input_size);
  IR_ENFORCE((*this)->operand_source(0).type().isa<paddle::dialect::DenseTensorType>(),
                  "Type validation failed for the 0th input, got %s.", (*this)->operand_source(0).type());
  IR_ENFORCE((*this)->operand_source(1).type().isa<paddle::dialect::DenseTensorType>(),
                  "Type validation failed for the 1th input, got %s.", (*this)->operand_source(1).type());
  if (auto val = (*this)->operand(2)) {
    IR_ENFORCE(val.type().isa<paddle::dialect::DenseTensorType>(),
                   "Type validation failed for the 2th input, got %s.", (*this)->operand_source(2).type());
  }
  if (auto val = (*this)->operand(3)) {
    IR_ENFORCE(val.type().isa<paddle::dialect::DenseTensorType>(),
                   "Type validation failed for the 3th input, got %s.", (*this)->operand_source(3).type());
  }
  }
  VLOG(4) << "Verifying attributes:";
  {
  auto& attributes = this->attributes();
  IR_ENFORCE(attributes.count("normalized")>0,
                 "normalized does not exist.");
  IR_ENFORCE(attributes.at("normalized").isa<pir::BoolAttribute>(),
                 "Type of attribute: normalized is not pir::BoolAttribute.");

  }
  VLOG(4) << "Verifying outputs:";
  {
  auto output_size = num_results();
  IR_ENFORCE(output_size == 2u,
                    "The size %d of outputs must be equal to 2.", output_size);
  IR_ENFORCE((*this)->result(0).type().isa<paddle::dialect::DenseTensorType>(),
                 "Type validation failed for the 0th output.");
  IR_ENFORCE((*this)->result(1).type().isa<paddle::dialect::DenseTensorType>(),
                 "Type validation failed for the 1th output.");
  }
  VLOG(4) << "End Verifying for: EditDistanceOp.";
}

void EditDistanceOp::InferMeta( phi::InferMetaContext *infer_meta ) {
  auto fn = PD_INFER_META(phi::EditDistanceInferMeta);
  fn(infer_meta);
}

phi::DataType EditDistanceOp::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: EditDistanceOp";
  


  return expected_kernel_dtype;
}

OpInfoTuple EigOp::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("x", "paddle::dialect::DenseTensorType", false, false, false, true) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = {  };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("out_w", "paddle::dialect::DenseTensorType", false, false), paddle::dialect::OpOutputInfo("out_v", "paddle::dialect::DenseTensorType", false, false) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("EigInferMeta", {"x"}, "eig", {"x"}, {}, {}, {}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "eig");
}

void EigOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_) {
  VLOG(4) << "Start build EigOp";



  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);
  paddle::dialect::IrTensor dense_out_w;
  paddle::dialect::IrMetaTensor meta_out_w(&dense_out_w);
  paddle::dialect::IrTensor dense_out_v;
  paddle::dialect::IrMetaTensor meta_out_v(&dense_out_v);

  phi::EigInferMeta(meta_x, &meta_out_w, &meta_out_v);

  std::vector<pir::Type> argument_outputs;
  pir::Type out_w_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_out_w.dtype()), dense_out_w.dims(), dense_out_w.layout(), dense_out_w.lod(), dense_out_w.offset());
  argument_outputs.push_back(out_w_dense_tensor_type);

  pir::Type out_v_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_out_v.dtype()), dense_out_v.dims(), dense_out_v.layout(), dense_out_v.lod(), dense_out_v.offset());
  argument_outputs.push_back(out_v_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void EigOp::VerifySig() {
  VLOG(4) << "Start Verifying inputs, outputs and attributes for: EigOp.";
  VLOG(4) << "Verifying inputs:";
  {
  auto input_size = num_operands();
  IR_ENFORCE(input_size == 1u,
                    "The size %d of inputs must be equal to 1.", input_size);
  IR_ENFORCE((*this)->operand_source(0).type().isa<paddle::dialect::DenseTensorType>(),
                  "Type validation failed for the 0th input, got %s.", (*this)->operand_source(0).type());
  }
  VLOG(4) << "Verifying attributes:";
  {
  // Attributes num is 0, not need to check attributes type.
  }
  VLOG(4) << "Verifying outputs:";
  {
  auto output_size = num_results();
  IR_ENFORCE(output_size == 2u,
                    "The size %d of outputs must be equal to 2.", output_size);
  IR_ENFORCE((*this)->result(0).type().isa<paddle::dialect::DenseTensorType>(),
                 "Type validation failed for the 0th output.");
  IR_ENFORCE((*this)->result(1).type().isa<paddle::dialect::DenseTensorType>(),
                 "Type validation failed for the 1th output.");
  }
  VLOG(4) << "End Verifying for: EigOp.";
}

void EigOp::InferMeta( phi::InferMetaContext *infer_meta ) {
  auto fn = PD_INFER_META(phi::EigInferMeta);
  fn(infer_meta);
}

phi::DataType EigOp::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: EigOp";
  


  return expected_kernel_dtype;
}

const char *EighOp::attributes_name[1] = { "UPLO" };

OpInfoTuple EighOp::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("x", "paddle::dialect::DenseTensorType", false, false, false, true) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = { paddle::dialect::OpAttributeInfo("UPLO", "pir::StrAttribute", "") };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("out_w", "paddle::dialect::DenseTensorType", false, false), paddle::dialect::OpOutputInfo("out_v", "paddle::dialect::DenseTensorType", false, false) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("EighInferMeta", {"x", "UPLO"}, "eigh", {"x", "UPLO"}, {}, {}, {}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "eigh");
}

void EighOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, const std::string& UPLO) {
  VLOG(4) << "Start build EighOp";



  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_UPLO = pir::StrAttribute::get(pir::IrContext::Instance(), UPLO);
  argument.AddAttribute("UPLO", attr_UPLO);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);
  paddle::dialect::IrTensor dense_out_w;
  paddle::dialect::IrMetaTensor meta_out_w(&dense_out_w);
  paddle::dialect::IrTensor dense_out_v;
  paddle::dialect::IrMetaTensor meta_out_v(&dense_out_v);

  phi::EighInferMeta(meta_x, UPLO, &meta_out_w, &meta_out_v);

  std::vector<pir::Type> argument_outputs;
  pir::Type out_w_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_out_w.dtype()), dense_out_w.dims(), dense_out_w.layout(), dense_out_w.lod(), dense_out_w.offset());
  argument_outputs.push_back(out_w_dense_tensor_type);

  pir::Type out_v_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_out_v.dtype()), dense_out_v.dims(), dense_out_v.layout(), dense_out_v.lod(), dense_out_v.offset());
  argument_outputs.push_back(out_v_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void EighOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, pir::AttributeMap attributes) {
  VLOG(4) << "Start build EighOp";


  IR_ENFORCE(
      attributes.find("UPLO") != attributes.end(),
          "'UPLO' Attribute is expected for EighOp. ");
  std::string UPLO = attributes.at("UPLO").dyn_cast<pir::StrAttribute>().AsString();


  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_UPLO = pir::StrAttribute::get(pir::IrContext::Instance(), UPLO);
  argument.AddAttribute("UPLO", attr_UPLO);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);
  paddle::dialect::IrTensor dense_out_w;
  paddle::dialect::IrMetaTensor meta_out_w(&dense_out_w);
  paddle::dialect::IrTensor dense_out_v;
  paddle::dialect::IrMetaTensor meta_out_v(&dense_out_v);

  phi::EighInferMeta(meta_x, UPLO, &meta_out_w, &meta_out_v);

  std::vector<pir::Type> argument_outputs;
  pir::Type out_w_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_out_w.dtype()), dense_out_w.dims(), dense_out_w.layout(), dense_out_w.lod(), dense_out_w.offset());
  argument_outputs.push_back(out_w_dense_tensor_type);

  pir::Type out_v_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_out_v.dtype()), dense_out_v.dims(), dense_out_v.layout(), dense_out_v.lod(), dense_out_v.offset());
  argument_outputs.push_back(out_v_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void EighOp::VerifySig() {
  VLOG(4) << "Start Verifying inputs, outputs and attributes for: EighOp.";
  VLOG(4) << "Verifying inputs:";
  {
  auto input_size = num_operands();
  IR_ENFORCE(input_size == 1u,
                    "The size %d of inputs must be equal to 1.", input_size);
  IR_ENFORCE((*this)->operand_source(0).type().isa<paddle::dialect::DenseTensorType>(),
                  "Type validation failed for the 0th input, got %s.", (*this)->operand_source(0).type());
  }
  VLOG(4) << "Verifying attributes:";
  {
  auto& attributes = this->attributes();
  IR_ENFORCE(attributes.count("UPLO")>0,
                 "UPLO does not exist.");
  IR_ENFORCE(attributes.at("UPLO").isa<pir::StrAttribute>(),
                 "Type of attribute: UPLO is not pir::StrAttribute.");

  }
  VLOG(4) << "Verifying outputs:";
  {
  auto output_size = num_results();
  IR_ENFORCE(output_size == 2u,
                    "The size %d of outputs must be equal to 2.", output_size);
  IR_ENFORCE((*this)->result(0).type().isa<paddle::dialect::DenseTensorType>(),
                 "Type validation failed for the 0th output.");
  IR_ENFORCE((*this)->result(1).type().isa<paddle::dialect::DenseTensorType>(),
                 "Type validation failed for the 1th output.");
  }
  VLOG(4) << "End Verifying for: EighOp.";
}

void EighOp::InferMeta( phi::InferMetaContext *infer_meta ) {
  auto fn = PD_INFER_META(phi::EighInferMeta);
  fn(infer_meta);
}

phi::DataType EighOp::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: EighOp";
  


  return expected_kernel_dtype;
}

OpInfoTuple EigvalsOp::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("x", "paddle::dialect::DenseTensorType", false, false, false, false) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = {  };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("out", "paddle::dialect::DenseTensorType", false, false) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("EigvalsInferMeta", {"x"}, "eigvals", {"x"}, {}, {}, {}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "eigvals");
}

void EigvalsOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_) {
  VLOG(4) << "Start build EigvalsOp";



  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);
  paddle::dialect::IrTensor dense_out;
  paddle::dialect::IrMetaTensor meta_out(&dense_out);

  phi::EigvalsInferMeta(meta_x, &meta_out);

  std::vector<pir::Type> argument_outputs;
  pir::Type out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_out.dtype()), dense_out.dims(), dense_out.layout(), dense_out.lod(), dense_out.offset());
  argument_outputs.push_back(out_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void EigvalsOp::VerifySig() {
  VLOG(4) << "Start Verifying inputs, outputs and attributes for: EigvalsOp.";
  VLOG(4) << "Verifying inputs:";
  {
  auto input_size = num_operands();
  IR_ENFORCE(input_size == 1u,
                    "The size %d of inputs must be equal to 1.", input_size);
  IR_ENFORCE((*this)->operand_source(0).type().isa<paddle::dialect::DenseTensorType>(),
                  "Type validation failed for the 0th input, got %s.", (*this)->operand_source(0).type());
  }
  VLOG(4) << "Verifying attributes:";
  {
  // Attributes num is 0, not need to check attributes type.
  }
  VLOG(4) << "Verifying outputs:";
  {
  auto output_size = num_results();
  IR_ENFORCE(output_size == 1u,
                    "The size %d of outputs must be equal to 1.", output_size);
  IR_ENFORCE((*this)->result(0).type().isa<paddle::dialect::DenseTensorType>(),
                 "Type validation failed for the 0th output.");
  }
  VLOG(4) << "End Verifying for: EigvalsOp.";
}

void EigvalsOp::InferMeta( phi::InferMetaContext *infer_meta ) {
  auto fn = PD_INFER_META(phi::EigvalsInferMeta);
  fn(infer_meta);
}

phi::DataType EigvalsOp::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: EigvalsOp";
  


  return expected_kernel_dtype;
}

const char *EigvalshOp::attributes_name[2] = { "uplo", "is_test" };

OpInfoTuple EigvalshOp::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("x", "paddle::dialect::DenseTensorType", false, false, false, true) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = { paddle::dialect::OpAttributeInfo("uplo", "pir::StrAttribute", ""), paddle::dialect::OpAttributeInfo("is_test", "pir::BoolAttribute", "") };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("eigenvalues", "paddle::dialect::DenseTensorType", false, false), paddle::dialect::OpOutputInfo("eigenvectors", "paddle::dialect::DenseTensorType", false, false) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("EigvalshInferMeta", {"x", "uplo", "is_test"}, "eigvalsh", {"x", "uplo", "is_test"}, {"x"}, {}, {}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "eigvalsh");
}

void EigvalshOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, const std::string& uplo, bool is_test) {
  VLOG(4) << "Start build EigvalshOp";



  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_uplo = pir::StrAttribute::get(pir::IrContext::Instance(), uplo);
  argument.AddAttribute("uplo", attr_uplo);
  pir::Attribute attr_is_test = pir::BoolAttribute::get(pir::IrContext::Instance(), is_test);
  argument.AddAttribute("is_test", attr_is_test);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);
  paddle::dialect::IrTensor dense_eigenvalues;
  paddle::dialect::IrMetaTensor meta_eigenvalues(&dense_eigenvalues);
  paddle::dialect::IrTensor dense_eigenvectors;
  paddle::dialect::IrMetaTensor meta_eigenvectors(&dense_eigenvectors);

  phi::EigvalshInferMeta(meta_x, uplo, is_test, &meta_eigenvalues, &meta_eigenvectors);

  std::vector<pir::Type> argument_outputs;
  pir::Type eigenvalues_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_eigenvalues.dtype()), dense_eigenvalues.dims(), dense_eigenvalues.layout(), dense_eigenvalues.lod(), dense_eigenvalues.offset());
  argument_outputs.push_back(eigenvalues_dense_tensor_type);

  pir::Type eigenvectors_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_eigenvectors.dtype()), dense_eigenvectors.dims(), dense_eigenvectors.layout(), dense_eigenvectors.lod(), dense_eigenvectors.offset());
  argument_outputs.push_back(eigenvectors_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void EigvalshOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, pir::AttributeMap attributes) {
  VLOG(4) << "Start build EigvalshOp";


  IR_ENFORCE(
      attributes.find("uplo") != attributes.end(),
          "'uplo' Attribute is expected for EigvalshOp. ");
  std::string uplo = attributes.at("uplo").dyn_cast<pir::StrAttribute>().AsString();

  IR_ENFORCE(
      attributes.find("is_test") != attributes.end(),
          "'is_test' Attribute is expected for EigvalshOp. ");
  bool is_test = attributes.at("is_test").dyn_cast<pir::BoolAttribute>().data();


  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_uplo = pir::StrAttribute::get(pir::IrContext::Instance(), uplo);
  argument.AddAttribute("uplo", attr_uplo);
  pir::Attribute attr_is_test = pir::BoolAttribute::get(pir::IrContext::Instance(), is_test);
  argument.AddAttribute("is_test", attr_is_test);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);
  paddle::dialect::IrTensor dense_eigenvalues;
  paddle::dialect::IrMetaTensor meta_eigenvalues(&dense_eigenvalues);
  paddle::dialect::IrTensor dense_eigenvectors;
  paddle::dialect::IrMetaTensor meta_eigenvectors(&dense_eigenvectors);

  phi::EigvalshInferMeta(meta_x, uplo, is_test, &meta_eigenvalues, &meta_eigenvectors);

  std::vector<pir::Type> argument_outputs;
  pir::Type eigenvalues_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_eigenvalues.dtype()), dense_eigenvalues.dims(), dense_eigenvalues.layout(), dense_eigenvalues.lod(), dense_eigenvalues.offset());
  argument_outputs.push_back(eigenvalues_dense_tensor_type);

  pir::Type eigenvectors_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_eigenvectors.dtype()), dense_eigenvectors.dims(), dense_eigenvectors.layout(), dense_eigenvectors.lod(), dense_eigenvectors.offset());
  argument_outputs.push_back(eigenvectors_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void EigvalshOp::VerifySig() {
  VLOG(4) << "Start Verifying inputs, outputs and attributes for: EigvalshOp.";
  VLOG(4) << "Verifying inputs:";
  {
  auto input_size = num_operands();
  IR_ENFORCE(input_size == 1u,
                    "The size %d of inputs must be equal to 1.", input_size);
  IR_ENFORCE((*this)->operand_source(0).type().isa<paddle::dialect::DenseTensorType>(),
                  "Type validation failed for the 0th input, got %s.", (*this)->operand_source(0).type());
  }
  VLOG(4) << "Verifying attributes:";
  {
  auto& attributes = this->attributes();
  IR_ENFORCE(attributes.count("uplo")>0,
                 "uplo does not exist.");
  IR_ENFORCE(attributes.at("uplo").isa<pir::StrAttribute>(),
                 "Type of attribute: uplo is not pir::StrAttribute.");

  IR_ENFORCE(attributes.count("is_test")>0,
                 "is_test does not exist.");
  IR_ENFORCE(attributes.at("is_test").isa<pir::BoolAttribute>(),
                 "Type of attribute: is_test is not pir::BoolAttribute.");

  }
  VLOG(4) << "Verifying outputs:";
  {
  auto output_size = num_results();
  IR_ENFORCE(output_size == 2u,
                    "The size %d of outputs must be equal to 2.", output_size);
  IR_ENFORCE((*this)->result(0).type().isa<paddle::dialect::DenseTensorType>(),
                 "Type validation failed for the 0th output.");
  IR_ENFORCE((*this)->result(1).type().isa<paddle::dialect::DenseTensorType>(),
                 "Type validation failed for the 1th output.");
  }
  VLOG(4) << "End Verifying for: EigvalshOp.";
}

void EigvalshOp::InferMeta( phi::InferMetaContext *infer_meta ) {
  auto fn = PD_INFER_META(phi::EigvalshInferMeta);
  fn(infer_meta);
}

phi::DataType EigvalshOp::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: EigvalshOp";
  


  return expected_kernel_dtype;
}

const char *EluOp::attributes_name[1] = { "alpha" };

OpInfoTuple EluOp::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("x", "paddle::dialect::DenseTensorType", false, false, false, true) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = { paddle::dialect::OpAttributeInfo("alpha", "pir::FloatAttribute", "") };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("out", "paddle::dialect::DenseTensorType", false, false) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("UnchangedInferMeta", {"x"}, "elu", {"x", "alpha"}, {}, {}, {}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "elu");
}

void EluOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, float alpha) {
  VLOG(4) << "Start build EluOp";



  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_alpha = pir::FloatAttribute::get(pir::IrContext::Instance(), alpha);
  argument.AddAttribute("alpha", attr_alpha);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);
  paddle::dialect::IrTensor dense_out;
  paddle::dialect::IrMetaTensor meta_out(&dense_out);

  phi::UnchangedInferMeta(meta_x, &meta_out);

  std::vector<pir::Type> argument_outputs;
  pir::Type out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_out.dtype()), dense_out.dims(), dense_out.layout(), dense_out.lod(), dense_out.offset());
  argument_outputs.push_back(out_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void EluOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, pir::AttributeMap attributes) {
  VLOG(4) << "Start build EluOp";


  IR_ENFORCE(
      attributes.find("alpha") != attributes.end(),
          "'alpha' Attribute is expected for EluOp. ");
  float alpha = attributes.at("alpha").dyn_cast<pir::FloatAttribute>().data();


  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_alpha = pir::FloatAttribute::get(pir::IrContext::Instance(), alpha);
  argument.AddAttribute("alpha", attr_alpha);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);
  paddle::dialect::IrTensor dense_out;
  paddle::dialect::IrMetaTensor meta_out(&dense_out);

  phi::UnchangedInferMeta(meta_x, &meta_out);

  std::vector<pir::Type> argument_outputs;
  pir::Type out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_out.dtype()), dense_out.dims(), dense_out.layout(), dense_out.lod(), dense_out.offset());
  argument_outputs.push_back(out_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void EluOp::VerifySig() {
  VLOG(4) << "Start Verifying inputs, outputs and attributes for: EluOp.";
  VLOG(4) << "Verifying inputs:";
  {
  auto input_size = num_operands();
  IR_ENFORCE(input_size == 1u,
                    "The size %d of inputs must be equal to 1.", input_size);
  IR_ENFORCE((*this)->operand_source(0).type().isa<paddle::dialect::DenseTensorType>(),
                  "Type validation failed for the 0th input, got %s.", (*this)->operand_source(0).type());
  }
  VLOG(4) << "Verifying attributes:";
  {
  auto& attributes = this->attributes();
  IR_ENFORCE(attributes.count("alpha")>0,
                 "alpha does not exist.");
  IR_ENFORCE(attributes.at("alpha").isa<pir::FloatAttribute>(),
                 "Type of attribute: alpha is not pir::FloatAttribute.");

  }
  VLOG(4) << "Verifying outputs:";
  {
  auto output_size = num_results();
  IR_ENFORCE(output_size == 1u,
                    "The size %d of outputs must be equal to 1.", output_size);
  IR_ENFORCE((*this)->result(0).type().isa<paddle::dialect::DenseTensorType>(),
                 "Type validation failed for the 0th output.");
  }
  VLOG(4) << "End Verifying for: EluOp.";
}

void EluOp::InferMeta( phi::InferMetaContext *infer_meta ) {
  auto fn = PD_INFER_META(phi::UnchangedInferMeta);
  fn(infer_meta);
}

phi::DataType EluOp::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: EluOp";
  


  return expected_kernel_dtype;
}

const char *Elu_Op::attributes_name[1] = { "alpha" };

OpInfoTuple Elu_Op::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("x", "paddle::dialect::DenseTensorType", false, false, false, true) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = { paddle::dialect::OpAttributeInfo("alpha", "pir::FloatAttribute", "") };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("out", "paddle::dialect::DenseTensorType", false, false) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("UnchangedInferMeta", {"x"}, "elu", {"x", "alpha"}, {}, {}, {{"out", "x"}}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "elu");
}

void Elu_Op::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, float alpha) {
  VLOG(4) << "Start build Elu_Op";



  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_alpha = pir::FloatAttribute::get(pir::IrContext::Instance(), alpha);
  argument.AddAttribute("alpha", attr_alpha);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);
  paddle::dialect::IrTensor dense_out;
  paddle::dialect::IrMetaTensor meta_out(&dense_out);

  phi::UnchangedInferMeta(meta_x, &meta_out);

  std::vector<pir::Type> argument_outputs;
  pir::Type out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_out.dtype()), dense_out.dims(), dense_out.layout(), dense_out.lod(), dense_out.offset());
  argument_outputs.push_back(out_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void Elu_Op::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, pir::AttributeMap attributes) {
  VLOG(4) << "Start build Elu_Op";


  IR_ENFORCE(
      attributes.find("alpha") != attributes.end(),
          "'alpha' Attribute is expected for Elu_Op. ");
  float alpha = attributes.at("alpha").dyn_cast<pir::FloatAttribute>().data();


  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_alpha = pir::FloatAttribute::get(pir::IrContext::Instance(), alpha);
  argument.AddAttribute("alpha", attr_alpha);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);
  paddle::dialect::IrTensor dense_out;
  paddle::dialect::IrMetaTensor meta_out(&dense_out);

  phi::UnchangedInferMeta(meta_x, &meta_out);

  std::vector<pir::Type> argument_outputs;
  pir::Type out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_out.dtype()), dense_out.dims(), dense_out.layout(), dense_out.lod(), dense_out.offset());
  argument_outputs.push_back(out_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void Elu_Op::VerifySig() {
  VLOG(4) << "Start Verifying inputs, outputs and attributes for: Elu_Op.";
  VLOG(4) << "Verifying inputs:";
  {
  auto input_size = num_operands();
  IR_ENFORCE(input_size == 1u,
                    "The size %d of inputs must be equal to 1.", input_size);
  IR_ENFORCE((*this)->operand_source(0).type().isa<paddle::dialect::DenseTensorType>(),
                  "Type validation failed for the 0th input, got %s.", (*this)->operand_source(0).type());
  }
  VLOG(4) << "Verifying attributes:";
  {
  auto& attributes = this->attributes();
  IR_ENFORCE(attributes.count("alpha")>0,
                 "alpha does not exist.");
  IR_ENFORCE(attributes.at("alpha").isa<pir::FloatAttribute>(),
                 "Type of attribute: alpha is not pir::FloatAttribute.");

  }
  VLOG(4) << "Verifying outputs:";
  {
  auto output_size = num_results();
  IR_ENFORCE(output_size == 1u,
                    "The size %d of outputs must be equal to 1.", output_size);
  IR_ENFORCE((*this)->result(0).type().isa<paddle::dialect::DenseTensorType>(),
                 "Type validation failed for the 0th output.");
  }
  VLOG(4) << "End Verifying for: Elu_Op.";
}

void Elu_Op::InferMeta( phi::InferMetaContext *infer_meta ) {
  auto fn = PD_INFER_META(phi::UnchangedInferMeta);
  fn(infer_meta);
}

phi::DataType Elu_Op::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: Elu_Op";
  


  return expected_kernel_dtype;
}

OpInfoTuple EqualAllOp::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("x", "paddle::dialect::DenseTensorType", false, false, false, false), paddle::dialect::OpInputInfo("y", "paddle::dialect::DenseTensorType", false, false, false, false) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = {  };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("out", "paddle::dialect::DenseTensorType", false, false) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("CompareAllInferMeta", {"x", "y"}, "equal_all", {"x", "y"}, {}, {}, {}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "equal_all");
}

void EqualAllOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, pir::Value y_) {
  VLOG(4) << "Start build EqualAllOp";



  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_, y_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;
  paddle::dialect::DenseTensorType y = y_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)y;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);

  VLOG(4) << "Builder construction  dense_y";
  paddle::dialect::IrTensor ir_tensor_y(paddle::dialect::TransToPhiDataType(y.dtype()),
                                                      y.dims(),
                                                      y.data_layout(),
                                                      y.lod(),
                                                      y.offset());
  VLOG(4) << "Builder construction  meta_y";
  paddle::dialect::IrMetaTensor meta_y(&ir_tensor_y);
  paddle::dialect::IrTensor dense_out;
  paddle::dialect::IrMetaTensor meta_out(&dense_out);

  phi::CompareAllInferMeta(meta_x, meta_y, &meta_out);

  std::vector<pir::Type> argument_outputs;
  pir::Type out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_out.dtype()), dense_out.dims(), dense_out.layout(), dense_out.lod(), dense_out.offset());
  argument_outputs.push_back(out_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void EqualAllOp::VerifySig() {
  VLOG(4) << "Start Verifying inputs, outputs and attributes for: EqualAllOp.";
  VLOG(4) << "Verifying inputs:";
  {
  auto input_size = num_operands();
  IR_ENFORCE(input_size == 2u,
                    "The size %d of inputs must be equal to 2.", input_size);
  IR_ENFORCE((*this)->operand_source(0).type().isa<paddle::dialect::DenseTensorType>(),
                  "Type validation failed for the 0th input, got %s.", (*this)->operand_source(0).type());
  IR_ENFORCE((*this)->operand_source(1).type().isa<paddle::dialect::DenseTensorType>(),
                  "Type validation failed for the 1th input, got %s.", (*this)->operand_source(1).type());
  }
  VLOG(4) << "Verifying attributes:";
  {
  // Attributes num is 0, not need to check attributes type.
  }
  VLOG(4) << "Verifying outputs:";
  {
  auto output_size = num_results();
  IR_ENFORCE(output_size == 1u,
                    "The size %d of outputs must be equal to 1.", output_size);
  IR_ENFORCE((*this)->result(0).type().isa<paddle::dialect::DenseTensorType>(),
                 "Type validation failed for the 0th output.");
  }
  VLOG(4) << "End Verifying for: EqualAllOp.";
}

void EqualAllOp::InferMeta( phi::InferMetaContext *infer_meta ) {
  auto fn = PD_INFER_META(phi::CompareAllInferMeta);
  fn(infer_meta);
}

phi::DataType EqualAllOp::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: EqualAllOp";
  


  return expected_kernel_dtype;
}

OpInfoTuple ErfOp::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("x", "paddle::dialect::DenseTensorType", false, false, false, true) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = {  };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("out", "paddle::dialect::DenseTensorType", false, false) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("UnchangedInferMeta", {"x"}, "erf", {"x"}, {}, {}, {}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "erf");
}

void ErfOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_) {
  VLOG(4) << "Start build ErfOp";



  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);
  paddle::dialect::IrTensor dense_out;
  paddle::dialect::IrMetaTensor meta_out(&dense_out);

  phi::UnchangedInferMeta(meta_x, &meta_out);

  std::vector<pir::Type> argument_outputs;
  pir::Type out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_out.dtype()), dense_out.dims(), dense_out.layout(), dense_out.lod(), dense_out.offset());
  argument_outputs.push_back(out_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void ErfOp::VerifySig() {
  VLOG(4) << "Start Verifying inputs, outputs and attributes for: ErfOp.";
  VLOG(4) << "Verifying inputs:";
  {
  auto input_size = num_operands();
  IR_ENFORCE(input_size == 1u,
                    "The size %d of inputs must be equal to 1.", input_size);
  IR_ENFORCE((*this)->operand_source(0).type().isa<paddle::dialect::DenseTensorType>(),
                  "Type validation failed for the 0th input, got %s.", (*this)->operand_source(0).type());
  }
  VLOG(4) << "Verifying attributes:";
  {
  // Attributes num is 0, not need to check attributes type.
  }
  VLOG(4) << "Verifying outputs:";
  {
  auto output_size = num_results();
  IR_ENFORCE(output_size == 1u,
                    "The size %d of outputs must be equal to 1.", output_size);
  IR_ENFORCE((*this)->result(0).type().isa<paddle::dialect::DenseTensorType>(),
                 "Type validation failed for the 0th output.");
  }
  VLOG(4) << "End Verifying for: ErfOp.";
}

void ErfOp::InferMeta( phi::InferMetaContext *infer_meta ) {
  auto fn = PD_INFER_META(phi::UnchangedInferMeta);
  fn(infer_meta);
}

phi::DataType ErfOp::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: ErfOp";
  


  return expected_kernel_dtype;
}

OpInfoTuple Erf_Op::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("x", "paddle::dialect::DenseTensorType", false, false, false, true) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = {  };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("out", "paddle::dialect::DenseTensorType", false, false) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("UnchangedInferMeta", {"x"}, "erf", {"x"}, {}, {}, {{"out", "x"}}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "erf");
}

void Erf_Op::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_) {
  VLOG(4) << "Start build Erf_Op";



  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);
  paddle::dialect::IrTensor dense_out;
  paddle::dialect::IrMetaTensor meta_out(&dense_out);

  phi::UnchangedInferMeta(meta_x, &meta_out);

  std::vector<pir::Type> argument_outputs;
  pir::Type out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_out.dtype()), dense_out.dims(), dense_out.layout(), dense_out.lod(), dense_out.offset());
  argument_outputs.push_back(out_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void Erf_Op::VerifySig() {
  VLOG(4) << "Start Verifying inputs, outputs and attributes for: Erf_Op.";
  VLOG(4) << "Verifying inputs:";
  {
  auto input_size = num_operands();
  IR_ENFORCE(input_size == 1u,
                    "The size %d of inputs must be equal to 1.", input_size);
  IR_ENFORCE((*this)->operand_source(0).type().isa<paddle::dialect::DenseTensorType>(),
                  "Type validation failed for the 0th input, got %s.", (*this)->operand_source(0).type());
  }
  VLOG(4) << "Verifying attributes:";
  {
  // Attributes num is 0, not need to check attributes type.
  }
  VLOG(4) << "Verifying outputs:";
  {
  auto output_size = num_results();
  IR_ENFORCE(output_size == 1u,
                    "The size %d of outputs must be equal to 1.", output_size);
  IR_ENFORCE((*this)->result(0).type().isa<paddle::dialect::DenseTensorType>(),
                 "Type validation failed for the 0th output.");
  }
  VLOG(4) << "End Verifying for: Erf_Op.";
}

void Erf_Op::InferMeta( phi::InferMetaContext *infer_meta ) {
  auto fn = PD_INFER_META(phi::UnchangedInferMeta);
  fn(infer_meta);
}

phi::DataType Erf_Op::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: Erf_Op";
  


  return expected_kernel_dtype;
}

OpInfoTuple ErfinvOp::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("x", "paddle::dialect::DenseTensorType", false, false, false, true) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = {  };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("out", "paddle::dialect::DenseTensorType", false, false) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("UnchangedInferMeta", {"x"}, "erfinv", {"x"}, {}, {}, {}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "erfinv");
}

void ErfinvOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_) {
  VLOG(4) << "Start build ErfinvOp";



  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);
  paddle::dialect::IrTensor dense_out;
  paddle::dialect::IrMetaTensor meta_out(&dense_out);

  phi::UnchangedInferMeta(meta_x, &meta_out);

  std::vector<pir::Type> argument_outputs;
  pir::Type out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_out.dtype()), dense_out.dims(), dense_out.layout(), dense_out.lod(), dense_out.offset());
  argument_outputs.push_back(out_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void ErfinvOp::VerifySig() {
  VLOG(4) << "Start Verifying inputs, outputs and attributes for: ErfinvOp.";
  VLOG(4) << "Verifying inputs:";
  {
  auto input_size = num_operands();
  IR_ENFORCE(input_size == 1u,
                    "The size %d of inputs must be equal to 1.", input_size);
  IR_ENFORCE((*this)->operand_source(0).type().isa<paddle::dialect::DenseTensorType>(),
                  "Type validation failed for the 0th input, got %s.", (*this)->operand_source(0).type());
  }
  VLOG(4) << "Verifying attributes:";
  {
  // Attributes num is 0, not need to check attributes type.
  }
  VLOG(4) << "Verifying outputs:";
  {
  auto output_size = num_results();
  IR_ENFORCE(output_size == 1u,
                    "The size %d of outputs must be equal to 1.", output_size);
  IR_ENFORCE((*this)->result(0).type().isa<paddle::dialect::DenseTensorType>(),
                 "Type validation failed for the 0th output.");
  }
  VLOG(4) << "End Verifying for: ErfinvOp.";
}

void ErfinvOp::InferMeta( phi::InferMetaContext *infer_meta ) {
  auto fn = PD_INFER_META(phi::UnchangedInferMeta);
  fn(infer_meta);
}

phi::DataType ErfinvOp::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: ErfinvOp";
  


  return expected_kernel_dtype;
}

OpInfoTuple Erfinv_Op::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("x", "paddle::dialect::DenseTensorType", false, false, false, true) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = {  };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("out", "paddle::dialect::DenseTensorType", false, false) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("UnchangedInferMeta", {"x"}, "erfinv", {"x"}, {}, {}, {{"out", "x"}}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "erfinv");
}

void Erfinv_Op::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_) {
  VLOG(4) << "Start build Erfinv_Op";



  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);
  paddle::dialect::IrTensor dense_out;
  paddle::dialect::IrMetaTensor meta_out(&dense_out);

  phi::UnchangedInferMeta(meta_x, &meta_out);

  std::vector<pir::Type> argument_outputs;
  pir::Type out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_out.dtype()), dense_out.dims(), dense_out.layout(), dense_out.lod(), dense_out.offset());
  argument_outputs.push_back(out_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void Erfinv_Op::VerifySig() {
  VLOG(4) << "Start Verifying inputs, outputs and attributes for: Erfinv_Op.";
  VLOG(4) << "Verifying inputs:";
  {
  auto input_size = num_operands();
  IR_ENFORCE(input_size == 1u,
                    "The size %d of inputs must be equal to 1.", input_size);
  IR_ENFORCE((*this)->operand_source(0).type().isa<paddle::dialect::DenseTensorType>(),
                  "Type validation failed for the 0th input, got %s.", (*this)->operand_source(0).type());
  }
  VLOG(4) << "Verifying attributes:";
  {
  // Attributes num is 0, not need to check attributes type.
  }
  VLOG(4) << "Verifying outputs:";
  {
  auto output_size = num_results();
  IR_ENFORCE(output_size == 1u,
                    "The size %d of outputs must be equal to 1.", output_size);
  IR_ENFORCE((*this)->result(0).type().isa<paddle::dialect::DenseTensorType>(),
                 "Type validation failed for the 0th output.");
  }
  VLOG(4) << "End Verifying for: Erfinv_Op.";
}

void Erfinv_Op::InferMeta( phi::InferMetaContext *infer_meta ) {
  auto fn = PD_INFER_META(phi::UnchangedInferMeta);
  fn(infer_meta);
}

phi::DataType Erfinv_Op::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: Erfinv_Op";
  


  return expected_kernel_dtype;
}

OpInfoTuple ExpOp::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("x", "paddle::dialect::DenseTensorType", false, false, false, true) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = {  };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("out", "paddle::dialect::DenseTensorType", false, false) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("UnchangedInferMeta", {"x"}, "exp", {"x"}, {}, {}, {}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "exp");
}

void ExpOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_) {
  VLOG(4) << "Start build ExpOp";



  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);
  paddle::dialect::IrTensor dense_out;
  paddle::dialect::IrMetaTensor meta_out(&dense_out);

  phi::UnchangedInferMeta(meta_x, &meta_out);

  std::vector<pir::Type> argument_outputs;
  pir::Type out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_out.dtype()), dense_out.dims(), dense_out.layout(), dense_out.lod(), dense_out.offset());
  argument_outputs.push_back(out_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void ExpOp::VerifySig() {
  VLOG(4) << "Start Verifying inputs, outputs and attributes for: ExpOp.";
  VLOG(4) << "Verifying inputs:";
  {
  auto input_size = num_operands();
  IR_ENFORCE(input_size == 1u,
                    "The size %d of inputs must be equal to 1.", input_size);
  IR_ENFORCE((*this)->operand_source(0).type().isa<paddle::dialect::DenseTensorType>(),
                  "Type validation failed for the 0th input, got %s.", (*this)->operand_source(0).type());
  }
  VLOG(4) << "Verifying attributes:";
  {
  // Attributes num is 0, not need to check attributes type.
  }
  VLOG(4) << "Verifying outputs:";
  {
  auto output_size = num_results();
  IR_ENFORCE(output_size == 1u,
                    "The size %d of outputs must be equal to 1.", output_size);
  IR_ENFORCE((*this)->result(0).type().isa<paddle::dialect::DenseTensorType>(),
                 "Type validation failed for the 0th output.");
  }
  VLOG(4) << "End Verifying for: ExpOp.";
}

void ExpOp::InferMeta( phi::InferMetaContext *infer_meta ) {
  auto fn = PD_INFER_META(phi::UnchangedInferMeta);
  fn(infer_meta);
}

phi::DataType ExpOp::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: ExpOp";
  


  return expected_kernel_dtype;
}

bool ExpOp::InferSymbolicShape(pir::ShapeConstraintIRAnalysis* shape_analysis) {
  VLOG(4) << "Infer symbolic shape for op: ExpOp";
  return ExpOpInferSymbolicShape(this->operation(), shape_analysis);
}

OpInfoTuple Exp_Op::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("x", "paddle::dialect::DenseTensorType", false, false, false, true) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = {  };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("out", "paddle::dialect::DenseTensorType", false, false) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("UnchangedInferMeta", {"x"}, "exp", {"x"}, {}, {}, {{"out", "x"}}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "exp");
}

void Exp_Op::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_) {
  VLOG(4) << "Start build Exp_Op";



  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);
  paddle::dialect::IrTensor dense_out;
  paddle::dialect::IrMetaTensor meta_out(&dense_out);

  phi::UnchangedInferMeta(meta_x, &meta_out);

  std::vector<pir::Type> argument_outputs;
  pir::Type out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_out.dtype()), dense_out.dims(), dense_out.layout(), dense_out.lod(), dense_out.offset());
  argument_outputs.push_back(out_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void Exp_Op::VerifySig() {
  VLOG(4) << "Start Verifying inputs, outputs and attributes for: Exp_Op.";
  VLOG(4) << "Verifying inputs:";
  {
  auto input_size = num_operands();
  IR_ENFORCE(input_size == 1u,
                    "The size %d of inputs must be equal to 1.", input_size);
  IR_ENFORCE((*this)->operand_source(0).type().isa<paddle::dialect::DenseTensorType>(),
                  "Type validation failed for the 0th input, got %s.", (*this)->operand_source(0).type());
  }
  VLOG(4) << "Verifying attributes:";
  {
  // Attributes num is 0, not need to check attributes type.
  }
  VLOG(4) << "Verifying outputs:";
  {
  auto output_size = num_results();
  IR_ENFORCE(output_size == 1u,
                    "The size %d of outputs must be equal to 1.", output_size);
  IR_ENFORCE((*this)->result(0).type().isa<paddle::dialect::DenseTensorType>(),
                 "Type validation failed for the 0th output.");
  }
  VLOG(4) << "End Verifying for: Exp_Op.";
}

void Exp_Op::InferMeta( phi::InferMetaContext *infer_meta ) {
  auto fn = PD_INFER_META(phi::UnchangedInferMeta);
  fn(infer_meta);
}

phi::DataType Exp_Op::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: Exp_Op";
  


  return expected_kernel_dtype;
}

bool Exp_Op::InferSymbolicShape(pir::ShapeConstraintIRAnalysis* shape_analysis) {
  VLOG(4) << "Infer symbolic shape for op: Exp_Op";
  return Exp_OpInferSymbolicShape(this->operation(), shape_analysis);
}

const char *ExpandAsOp::attributes_name[1] = { "target_shape" };

OpInfoTuple ExpandAsOp::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("x", "paddle::dialect::DenseTensorType", false, false, false, true), paddle::dialect::OpInputInfo("y", "paddle::dialect::DenseTensorType", true, false, false, false) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = { paddle::dialect::OpAttributeInfo("target_shape", "pir::ArrayAttribute<pir::Int32Attribute>", "") };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("out", "paddle::dialect::DenseTensorType", false, false) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("ExpandAsInferMeta", {"x", "y", "target_shape"}, "expand_as", {"x", "y", "target_shape"}, {"x"}, {}, {}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "expand_as");
}

void ExpandAsOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, pir::Value y_, const std::vector<int>& target_shape) {
  VLOG(4) << "Start build ExpandAsOp";



  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_, y_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  std::vector<pir::Attribute> vec_target_shape;
  for (size_t i = 0; i < static_cast<size_t>(target_shape.size()); i++) {
      pir::Attribute attr_target_shape = pir::Int32Attribute::get(pir::IrContext::Instance(), target_shape[i]);

    vec_target_shape.push_back(attr_target_shape);
  }
  pir::Attribute attr_target_shape = pir::ArrayAttribute::get(pir::IrContext::Instance(), vec_target_shape);
  argument.AddAttribute("target_shape", attr_target_shape);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);

  paddle::dialect::IrMetaTensor meta_y;
  paddle::dialect::IrTensor ir_tensor_y;
  if (y_.impl() != nullptr) {
    paddle::dialect::DenseTensorType y = y_.type().dyn_cast<paddle::dialect::DenseTensorType>();
    VLOG(4) << "Builder construction  dense_y";
    ir_tensor_y = paddle::dialect::IrTensor(paddle::dialect::TransToPhiDataType(y.dtype()),
                                                        y.dims(),
                                                        y.data_layout(),
                                                        y.lod(),
                                                        y.offset());
    VLOG(4) << "Builder construction  meta_y";
    meta_y = paddle::dialect::IrMetaTensor(&ir_tensor_y);
  }

  paddle::dialect::IrTensor dense_out;
  paddle::dialect::IrMetaTensor meta_out(&dense_out);

  phi::ExpandAsInferMeta(meta_x, meta_y, target_shape, &meta_out);

  std::vector<pir::Type> argument_outputs;
  pir::Type out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_out.dtype()), dense_out.dims(), dense_out.layout(), dense_out.lod(), dense_out.offset());
  argument_outputs.push_back(out_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void ExpandAsOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, pir::Value y_, pir::AttributeMap attributes) {
  VLOG(4) << "Start build ExpandAsOp";


  IR_ENFORCE(
      attributes.find("target_shape") != attributes.end(),
          "'target_shape' Attribute is expected for ExpandAsOp. ");
  std::vector<int> target_shape;
  for (size_t i = 0; i < attributes.at("target_shape").dyn_cast<pir::ArrayAttribute>().size(); i++) {
    target_shape.push_back(attributes.at("target_shape").dyn_cast<pir::ArrayAttribute>().at(i).dyn_cast<pir::Int32Attribute>().data());
  }


  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_, y_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  std::vector<pir::Attribute> vec_target_shape;
  for (size_t i = 0; i < static_cast<size_t>(target_shape.size()); i++) {
      pir::Attribute attr_target_shape = pir::Int32Attribute::get(pir::IrContext::Instance(), target_shape[i]);

    vec_target_shape.push_back(attr_target_shape);
  }
  pir::Attribute attr_target_shape = pir::ArrayAttribute::get(pir::IrContext::Instance(), vec_target_shape);
  argument.AddAttribute("target_shape", attr_target_shape);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);

  paddle::dialect::IrMetaTensor meta_y;
  paddle::dialect::IrTensor ir_tensor_y;
  if (y_.impl() != nullptr) {
    paddle::dialect::DenseTensorType y = y_.type().dyn_cast<paddle::dialect::DenseTensorType>();
    VLOG(4) << "Builder construction  dense_y";
    ir_tensor_y = paddle::dialect::IrTensor(paddle::dialect::TransToPhiDataType(y.dtype()),
                                                        y.dims(),
                                                        y.data_layout(),
                                                        y.lod(),
                                                        y.offset());
    VLOG(4) << "Builder construction  meta_y";
    meta_y = paddle::dialect::IrMetaTensor(&ir_tensor_y);
  }

  paddle::dialect::IrTensor dense_out;
  paddle::dialect::IrMetaTensor meta_out(&dense_out);

  phi::ExpandAsInferMeta(meta_x, meta_y, target_shape, &meta_out);

  std::vector<pir::Type> argument_outputs;
  pir::Type out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_out.dtype()), dense_out.dims(), dense_out.layout(), dense_out.lod(), dense_out.offset());
  argument_outputs.push_back(out_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void ExpandAsOp::VerifySig() {
  VLOG(4) << "Start Verifying inputs, outputs and attributes for: ExpandAsOp.";
  VLOG(4) << "Verifying inputs:";
  {
  auto input_size = num_operands();
  IR_ENFORCE(input_size == 2u,
                    "The size %d of inputs must be equal to 2.", input_size);
  IR_ENFORCE((*this)->operand_source(0).type().isa<paddle::dialect::DenseTensorType>(),
                  "Type validation failed for the 0th input, got %s.", (*this)->operand_source(0).type());
  if (auto val = (*this)->operand(1)) {
    IR_ENFORCE(val.type().isa<paddle::dialect::DenseTensorType>(),
                   "Type validation failed for the 1th input, got %s.", (*this)->operand_source(1).type());
  }
  }
  VLOG(4) << "Verifying attributes:";
  {
  auto& attributes = this->attributes();
  IR_ENFORCE(attributes.count("target_shape")>0,
                 "target_shape does not exist.");
  IR_ENFORCE(attributes.at("target_shape").isa<pir::ArrayAttribute>(),
                 "Type of attribute: target_shape is not pir::ArrayAttribute.");
  for (size_t i = 0; i < attributes.at("target_shape").dyn_cast<pir::ArrayAttribute>().size(); i++) {
    IR_ENFORCE(attributes.at("target_shape").dyn_cast<pir::ArrayAttribute>().at(i).isa<pir::Int32Attribute>(),
                   "Type of attribute: target_shape is not right.");
  }
  }
  VLOG(4) << "Verifying outputs:";
  {
  auto output_size = num_results();
  IR_ENFORCE(output_size == 1u,
                    "The size %d of outputs must be equal to 1.", output_size);
  IR_ENFORCE((*this)->result(0).type().isa<paddle::dialect::DenseTensorType>(),
                 "Type validation failed for the 0th output.");
  }
  VLOG(4) << "End Verifying for: ExpandAsOp.";
}

void ExpandAsOp::InferMeta( phi::InferMetaContext *infer_meta ) {
  auto fn = PD_INFER_META(phi::ExpandAsInferMeta);
  fn(infer_meta);
}

phi::DataType ExpandAsOp::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: ExpandAsOp";
  


  return expected_kernel_dtype;
}

OpInfoTuple Expm1Op::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("x", "paddle::dialect::DenseTensorType", false, false, false, true) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = {  };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("out", "paddle::dialect::DenseTensorType", false, false) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("UnchangedInferMeta", {"x"}, "expm1", {"x"}, {}, {}, {}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "expm1");
}

void Expm1Op::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_) {
  VLOG(4) << "Start build Expm1Op";



  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);
  paddle::dialect::IrTensor dense_out;
  paddle::dialect::IrMetaTensor meta_out(&dense_out);

  phi::UnchangedInferMeta(meta_x, &meta_out);

  std::vector<pir::Type> argument_outputs;
  pir::Type out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_out.dtype()), dense_out.dims(), dense_out.layout(), dense_out.lod(), dense_out.offset());
  argument_outputs.push_back(out_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void Expm1Op::VerifySig() {
  VLOG(4) << "Start Verifying inputs, outputs and attributes for: Expm1Op.";
  VLOG(4) << "Verifying inputs:";
  {
  auto input_size = num_operands();
  IR_ENFORCE(input_size == 1u,
                    "The size %d of inputs must be equal to 1.", input_size);
  IR_ENFORCE((*this)->operand_source(0).type().isa<paddle::dialect::DenseTensorType>(),
                  "Type validation failed for the 0th input, got %s.", (*this)->operand_source(0).type());
  }
  VLOG(4) << "Verifying attributes:";
  {
  // Attributes num is 0, not need to check attributes type.
  }
  VLOG(4) << "Verifying outputs:";
  {
  auto output_size = num_results();
  IR_ENFORCE(output_size == 1u,
                    "The size %d of outputs must be equal to 1.", output_size);
  IR_ENFORCE((*this)->result(0).type().isa<paddle::dialect::DenseTensorType>(),
                 "Type validation failed for the 0th output.");
  }
  VLOG(4) << "End Verifying for: Expm1Op.";
}

void Expm1Op::InferMeta( phi::InferMetaContext *infer_meta ) {
  auto fn = PD_INFER_META(phi::UnchangedInferMeta);
  fn(infer_meta);
}

phi::DataType Expm1Op::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: Expm1Op";
  


  return expected_kernel_dtype;
}

OpInfoTuple Expm1_Op::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("x", "paddle::dialect::DenseTensorType", false, false, false, true) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = {  };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("out", "paddle::dialect::DenseTensorType", false, false) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("UnchangedInferMeta", {"x"}, "expm1", {"x"}, {}, {}, {{"out", "x"}}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "expm1");
}

void Expm1_Op::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_) {
  VLOG(4) << "Start build Expm1_Op";



  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);
  paddle::dialect::IrTensor dense_out;
  paddle::dialect::IrMetaTensor meta_out(&dense_out);

  phi::UnchangedInferMeta(meta_x, &meta_out);

  std::vector<pir::Type> argument_outputs;
  pir::Type out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_out.dtype()), dense_out.dims(), dense_out.layout(), dense_out.lod(), dense_out.offset());
  argument_outputs.push_back(out_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void Expm1_Op::VerifySig() {
  VLOG(4) << "Start Verifying inputs, outputs and attributes for: Expm1_Op.";
  VLOG(4) << "Verifying inputs:";
  {
  auto input_size = num_operands();
  IR_ENFORCE(input_size == 1u,
                    "The size %d of inputs must be equal to 1.", input_size);
  IR_ENFORCE((*this)->operand_source(0).type().isa<paddle::dialect::DenseTensorType>(),
                  "Type validation failed for the 0th input, got %s.", (*this)->operand_source(0).type());
  }
  VLOG(4) << "Verifying attributes:";
  {
  // Attributes num is 0, not need to check attributes type.
  }
  VLOG(4) << "Verifying outputs:";
  {
  auto output_size = num_results();
  IR_ENFORCE(output_size == 1u,
                    "The size %d of outputs must be equal to 1.", output_size);
  IR_ENFORCE((*this)->result(0).type().isa<paddle::dialect::DenseTensorType>(),
                 "Type validation failed for the 0th output.");
  }
  VLOG(4) << "End Verifying for: Expm1_Op.";
}

void Expm1_Op::InferMeta( phi::InferMetaContext *infer_meta ) {
  auto fn = PD_INFER_META(phi::UnchangedInferMeta);
  fn(infer_meta);
}

phi::DataType Expm1_Op::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: Expm1_Op";
  


  return expected_kernel_dtype;
}

const char *FftC2cOp::attributes_name[3] = { "axes", "normalization", "forward" };

OpInfoTuple FftC2cOp::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("x", "paddle::dialect::DenseTensorType", false, false, false, true) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = { paddle::dialect::OpAttributeInfo("axes", "pir::ArrayAttribute<pir::Int64Attribute>", ""), paddle::dialect::OpAttributeInfo("normalization", "pir::StrAttribute", ""), paddle::dialect::OpAttributeInfo("forward", "pir::BoolAttribute", "") };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("out", "paddle::dialect::DenseTensorType", false, false) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("FFTC2CInferMeta", {"x", "axes", "normalization", "forward"}, "fft_c2c", {"x", "axes", "normalization", "forward"}, {}, {}, {}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "fft_c2c");
}

void FftC2cOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, const std::vector<int64_t>& axes, const std::string& normalization, bool forward) {
  VLOG(4) << "Start build FftC2cOp";



  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  std::vector<pir::Attribute> vec_axes;
  for (size_t i = 0; i < static_cast<size_t>(axes.size()); i++) {
      pir::Attribute attr_axes = pir::Int64Attribute::get(pir::IrContext::Instance(), axes[i]);

    vec_axes.push_back(attr_axes);
  }
  pir::Attribute attr_axes = pir::ArrayAttribute::get(pir::IrContext::Instance(), vec_axes);
  argument.AddAttribute("axes", attr_axes);
  pir::Attribute attr_normalization = pir::StrAttribute::get(pir::IrContext::Instance(), normalization);
  argument.AddAttribute("normalization", attr_normalization);
  pir::Attribute attr_forward = pir::BoolAttribute::get(pir::IrContext::Instance(), forward);
  argument.AddAttribute("forward", attr_forward);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);
  paddle::dialect::IrTensor dense_out;
  paddle::dialect::IrMetaTensor meta_out(&dense_out);

  phi::FFTC2CInferMeta(meta_x, axes, normalization, forward, &meta_out);

  std::vector<pir::Type> argument_outputs;
  pir::Type out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_out.dtype()), dense_out.dims(), dense_out.layout(), dense_out.lod(), dense_out.offset());
  argument_outputs.push_back(out_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void FftC2cOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, pir::AttributeMap attributes) {
  VLOG(4) << "Start build FftC2cOp";


  IR_ENFORCE(
      attributes.find("axes") != attributes.end(),
          "'axes' Attribute is expected for FftC2cOp. ");
  std::vector<int64_t> axes;
  for (size_t i = 0; i < attributes.at("axes").dyn_cast<pir::ArrayAttribute>().size(); i++) {
    axes.push_back(attributes.at("axes").dyn_cast<pir::ArrayAttribute>().at(i).dyn_cast<pir::Int64Attribute>().data());
  }

  IR_ENFORCE(
      attributes.find("normalization") != attributes.end(),
          "'normalization' Attribute is expected for FftC2cOp. ");
  std::string normalization = attributes.at("normalization").dyn_cast<pir::StrAttribute>().AsString();

  IR_ENFORCE(
      attributes.find("forward") != attributes.end(),
          "'forward' Attribute is expected for FftC2cOp. ");
  bool forward = attributes.at("forward").dyn_cast<pir::BoolAttribute>().data();


  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  std::vector<pir::Attribute> vec_axes;
  for (size_t i = 0; i < static_cast<size_t>(axes.size()); i++) {
      pir::Attribute attr_axes = pir::Int64Attribute::get(pir::IrContext::Instance(), axes[i]);

    vec_axes.push_back(attr_axes);
  }
  pir::Attribute attr_axes = pir::ArrayAttribute::get(pir::IrContext::Instance(), vec_axes);
  argument.AddAttribute("axes", attr_axes);
  pir::Attribute attr_normalization = pir::StrAttribute::get(pir::IrContext::Instance(), normalization);
  argument.AddAttribute("normalization", attr_normalization);
  pir::Attribute attr_forward = pir::BoolAttribute::get(pir::IrContext::Instance(), forward);
  argument.AddAttribute("forward", attr_forward);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);
  paddle::dialect::IrTensor dense_out;
  paddle::dialect::IrMetaTensor meta_out(&dense_out);

  phi::FFTC2CInferMeta(meta_x, axes, normalization, forward, &meta_out);

  std::vector<pir::Type> argument_outputs;
  pir::Type out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_out.dtype()), dense_out.dims(), dense_out.layout(), dense_out.lod(), dense_out.offset());
  argument_outputs.push_back(out_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void FftC2cOp::VerifySig() {
  VLOG(4) << "Start Verifying inputs, outputs and attributes for: FftC2cOp.";
  VLOG(4) << "Verifying inputs:";
  {
  auto input_size = num_operands();
  IR_ENFORCE(input_size == 1u,
                    "The size %d of inputs must be equal to 1.", input_size);
  IR_ENFORCE((*this)->operand_source(0).type().isa<paddle::dialect::DenseTensorType>(),
                  "Type validation failed for the 0th input, got %s.", (*this)->operand_source(0).type());
  }
  VLOG(4) << "Verifying attributes:";
  {
  auto& attributes = this->attributes();
  IR_ENFORCE(attributes.count("axes")>0,
                 "axes does not exist.");
  IR_ENFORCE(attributes.at("axes").isa<pir::ArrayAttribute>(),
                 "Type of attribute: axes is not pir::ArrayAttribute.");
  for (size_t i = 0; i < attributes.at("axes").dyn_cast<pir::ArrayAttribute>().size(); i++) {
    IR_ENFORCE(attributes.at("axes").dyn_cast<pir::ArrayAttribute>().at(i).isa<pir::Int64Attribute>(),
                   "Type of attribute: axes is not right.");
  }
  IR_ENFORCE(attributes.count("normalization")>0,
                 "normalization does not exist.");
  IR_ENFORCE(attributes.at("normalization").isa<pir::StrAttribute>(),
                 "Type of attribute: normalization is not pir::StrAttribute.");

  IR_ENFORCE(attributes.count("forward")>0,
                 "forward does not exist.");
  IR_ENFORCE(attributes.at("forward").isa<pir::BoolAttribute>(),
                 "Type of attribute: forward is not pir::BoolAttribute.");

  }
  VLOG(4) << "Verifying outputs:";
  {
  auto output_size = num_results();
  IR_ENFORCE(output_size == 1u,
                    "The size %d of outputs must be equal to 1.", output_size);
  IR_ENFORCE((*this)->result(0).type().isa<paddle::dialect::DenseTensorType>(),
                 "Type validation failed for the 0th output.");
  }
  VLOG(4) << "End Verifying for: FftC2cOp.";
}

void FftC2cOp::InferMeta( phi::InferMetaContext *infer_meta ) {
  auto fn = PD_INFER_META(phi::FFTC2CInferMeta);
  fn(infer_meta);
}

phi::DataType FftC2cOp::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: FftC2cOp";
  


  return expected_kernel_dtype;
}

const char *FftC2rOp::attributes_name[4] = { "axes", "normalization", "forward", "last_dim_size" };

OpInfoTuple FftC2rOp::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("x", "paddle::dialect::DenseTensorType", false, false, false, true) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = { paddle::dialect::OpAttributeInfo("axes", "pir::ArrayAttribute<pir::Int64Attribute>", ""), paddle::dialect::OpAttributeInfo("normalization", "pir::StrAttribute", ""), paddle::dialect::OpAttributeInfo("forward", "pir::BoolAttribute", ""), paddle::dialect::OpAttributeInfo("last_dim_size", "pir::Int64Attribute", "") };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("out", "paddle::dialect::DenseTensorType", false, false) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("FFTC2RInferMeta", {"x", "axes", "normalization", "forward", "last_dim_size"}, "fft_c2r", {"x", "axes", "normalization", "forward", "last_dim_size"}, {}, {}, {}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "fft_c2r");
}

void FftC2rOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, const std::vector<int64_t>& axes, const std::string& normalization, bool forward, int64_t last_dim_size) {
  VLOG(4) << "Start build FftC2rOp";



  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  std::vector<pir::Attribute> vec_axes;
  for (size_t i = 0; i < static_cast<size_t>(axes.size()); i++) {
      pir::Attribute attr_axes = pir::Int64Attribute::get(pir::IrContext::Instance(), axes[i]);

    vec_axes.push_back(attr_axes);
  }
  pir::Attribute attr_axes = pir::ArrayAttribute::get(pir::IrContext::Instance(), vec_axes);
  argument.AddAttribute("axes", attr_axes);
  pir::Attribute attr_normalization = pir::StrAttribute::get(pir::IrContext::Instance(), normalization);
  argument.AddAttribute("normalization", attr_normalization);
  pir::Attribute attr_forward = pir::BoolAttribute::get(pir::IrContext::Instance(), forward);
  argument.AddAttribute("forward", attr_forward);
  pir::Attribute attr_last_dim_size = pir::Int64Attribute::get(pir::IrContext::Instance(), last_dim_size);
  argument.AddAttribute("last_dim_size", attr_last_dim_size);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);
  paddle::dialect::IrTensor dense_out;
  paddle::dialect::IrMetaTensor meta_out(&dense_out);

  phi::FFTC2RInferMeta(meta_x, axes, normalization, forward, last_dim_size, &meta_out);

  std::vector<pir::Type> argument_outputs;
  pir::Type out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_out.dtype()), dense_out.dims(), dense_out.layout(), dense_out.lod(), dense_out.offset());
  argument_outputs.push_back(out_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void FftC2rOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, pir::AttributeMap attributes) {
  VLOG(4) << "Start build FftC2rOp";


  IR_ENFORCE(
      attributes.find("axes") != attributes.end(),
          "'axes' Attribute is expected for FftC2rOp. ");
  std::vector<int64_t> axes;
  for (size_t i = 0; i < attributes.at("axes").dyn_cast<pir::ArrayAttribute>().size(); i++) {
    axes.push_back(attributes.at("axes").dyn_cast<pir::ArrayAttribute>().at(i).dyn_cast<pir::Int64Attribute>().data());
  }

  IR_ENFORCE(
      attributes.find("normalization") != attributes.end(),
          "'normalization' Attribute is expected for FftC2rOp. ");
  std::string normalization = attributes.at("normalization").dyn_cast<pir::StrAttribute>().AsString();

  IR_ENFORCE(
      attributes.find("forward") != attributes.end(),
          "'forward' Attribute is expected for FftC2rOp. ");
  bool forward = attributes.at("forward").dyn_cast<pir::BoolAttribute>().data();

  IR_ENFORCE(
      attributes.find("last_dim_size") != attributes.end(),
          "'last_dim_size' Attribute is expected for FftC2rOp. ");
  int64_t last_dim_size = attributes.at("last_dim_size").dyn_cast<pir::Int64Attribute>().data();


  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  std::vector<pir::Attribute> vec_axes;
  for (size_t i = 0; i < static_cast<size_t>(axes.size()); i++) {
      pir::Attribute attr_axes = pir::Int64Attribute::get(pir::IrContext::Instance(), axes[i]);

    vec_axes.push_back(attr_axes);
  }
  pir::Attribute attr_axes = pir::ArrayAttribute::get(pir::IrContext::Instance(), vec_axes);
  argument.AddAttribute("axes", attr_axes);
  pir::Attribute attr_normalization = pir::StrAttribute::get(pir::IrContext::Instance(), normalization);
  argument.AddAttribute("normalization", attr_normalization);
  pir::Attribute attr_forward = pir::BoolAttribute::get(pir::IrContext::Instance(), forward);
  argument.AddAttribute("forward", attr_forward);
  pir::Attribute attr_last_dim_size = pir::Int64Attribute::get(pir::IrContext::Instance(), last_dim_size);
  argument.AddAttribute("last_dim_size", attr_last_dim_size);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);
  paddle::dialect::IrTensor dense_out;
  paddle::dialect::IrMetaTensor meta_out(&dense_out);

  phi::FFTC2RInferMeta(meta_x, axes, normalization, forward, last_dim_size, &meta_out);

  std::vector<pir::Type> argument_outputs;
  pir::Type out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_out.dtype()), dense_out.dims(), dense_out.layout(), dense_out.lod(), dense_out.offset());
  argument_outputs.push_back(out_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void FftC2rOp::VerifySig() {
  VLOG(4) << "Start Verifying inputs, outputs and attributes for: FftC2rOp.";
  VLOG(4) << "Verifying inputs:";
  {
  auto input_size = num_operands();
  IR_ENFORCE(input_size == 1u,
                    "The size %d of inputs must be equal to 1.", input_size);
  IR_ENFORCE((*this)->operand_source(0).type().isa<paddle::dialect::DenseTensorType>(),
                  "Type validation failed for the 0th input, got %s.", (*this)->operand_source(0).type());
  }
  VLOG(4) << "Verifying attributes:";
  {
  auto& attributes = this->attributes();
  IR_ENFORCE(attributes.count("axes")>0,
                 "axes does not exist.");
  IR_ENFORCE(attributes.at("axes").isa<pir::ArrayAttribute>(),
                 "Type of attribute: axes is not pir::ArrayAttribute.");
  for (size_t i = 0; i < attributes.at("axes").dyn_cast<pir::ArrayAttribute>().size(); i++) {
    IR_ENFORCE(attributes.at("axes").dyn_cast<pir::ArrayAttribute>().at(i).isa<pir::Int64Attribute>(),
                   "Type of attribute: axes is not right.");
  }
  IR_ENFORCE(attributes.count("normalization")>0,
                 "normalization does not exist.");
  IR_ENFORCE(attributes.at("normalization").isa<pir::StrAttribute>(),
                 "Type of attribute: normalization is not pir::StrAttribute.");

  IR_ENFORCE(attributes.count("forward")>0,
                 "forward does not exist.");
  IR_ENFORCE(attributes.at("forward").isa<pir::BoolAttribute>(),
                 "Type of attribute: forward is not pir::BoolAttribute.");

  IR_ENFORCE(attributes.count("last_dim_size")>0,
                 "last_dim_size does not exist.");
  IR_ENFORCE(attributes.at("last_dim_size").isa<pir::Int64Attribute>(),
                 "Type of attribute: last_dim_size is not pir::Int64Attribute.");

  }
  VLOG(4) << "Verifying outputs:";
  {
  auto output_size = num_results();
  IR_ENFORCE(output_size == 1u,
                    "The size %d of outputs must be equal to 1.", output_size);
  IR_ENFORCE((*this)->result(0).type().isa<paddle::dialect::DenseTensorType>(),
                 "Type validation failed for the 0th output.");
  }
  VLOG(4) << "End Verifying for: FftC2rOp.";
}

void FftC2rOp::InferMeta( phi::InferMetaContext *infer_meta ) {
  auto fn = PD_INFER_META(phi::FFTC2RInferMeta);
  fn(infer_meta);
}

phi::DataType FftC2rOp::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: FftC2rOp";
  


  return expected_kernel_dtype;
}

const char *FftR2cOp::attributes_name[4] = { "axes", "normalization", "forward", "onesided" };

OpInfoTuple FftR2cOp::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("x", "paddle::dialect::DenseTensorType", false, false, false, true) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = { paddle::dialect::OpAttributeInfo("axes", "pir::ArrayAttribute<pir::Int64Attribute>", ""), paddle::dialect::OpAttributeInfo("normalization", "pir::StrAttribute", ""), paddle::dialect::OpAttributeInfo("forward", "pir::BoolAttribute", ""), paddle::dialect::OpAttributeInfo("onesided", "pir::BoolAttribute", "") };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("out", "paddle::dialect::DenseTensorType", false, false) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("FFTR2CInferMeta", {"x", "axes", "normalization", "forward", "onesided"}, "fft_r2c", {"x", "axes", "normalization", "forward", "onesided"}, {}, {}, {}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "fft_r2c");
}

void FftR2cOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, const std::vector<int64_t>& axes, const std::string& normalization, bool forward, bool onesided) {
  VLOG(4) << "Start build FftR2cOp";



  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  std::vector<pir::Attribute> vec_axes;
  for (size_t i = 0; i < static_cast<size_t>(axes.size()); i++) {
      pir::Attribute attr_axes = pir::Int64Attribute::get(pir::IrContext::Instance(), axes[i]);

    vec_axes.push_back(attr_axes);
  }
  pir::Attribute attr_axes = pir::ArrayAttribute::get(pir::IrContext::Instance(), vec_axes);
  argument.AddAttribute("axes", attr_axes);
  pir::Attribute attr_normalization = pir::StrAttribute::get(pir::IrContext::Instance(), normalization);
  argument.AddAttribute("normalization", attr_normalization);
  pir::Attribute attr_forward = pir::BoolAttribute::get(pir::IrContext::Instance(), forward);
  argument.AddAttribute("forward", attr_forward);
  pir::Attribute attr_onesided = pir::BoolAttribute::get(pir::IrContext::Instance(), onesided);
  argument.AddAttribute("onesided", attr_onesided);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);
  paddle::dialect::IrTensor dense_out;
  paddle::dialect::IrMetaTensor meta_out(&dense_out);

  phi::FFTR2CInferMeta(meta_x, axes, normalization, forward, onesided, &meta_out);

  std::vector<pir::Type> argument_outputs;
  pir::Type out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_out.dtype()), dense_out.dims(), dense_out.layout(), dense_out.lod(), dense_out.offset());
  argument_outputs.push_back(out_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void FftR2cOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, pir::AttributeMap attributes) {
  VLOG(4) << "Start build FftR2cOp";


  IR_ENFORCE(
      attributes.find("axes") != attributes.end(),
          "'axes' Attribute is expected for FftR2cOp. ");
  std::vector<int64_t> axes;
  for (size_t i = 0; i < attributes.at("axes").dyn_cast<pir::ArrayAttribute>().size(); i++) {
    axes.push_back(attributes.at("axes").dyn_cast<pir::ArrayAttribute>().at(i).dyn_cast<pir::Int64Attribute>().data());
  }

  IR_ENFORCE(
      attributes.find("normalization") != attributes.end(),
          "'normalization' Attribute is expected for FftR2cOp. ");
  std::string normalization = attributes.at("normalization").dyn_cast<pir::StrAttribute>().AsString();

  IR_ENFORCE(
      attributes.find("forward") != attributes.end(),
          "'forward' Attribute is expected for FftR2cOp. ");
  bool forward = attributes.at("forward").dyn_cast<pir::BoolAttribute>().data();

  IR_ENFORCE(
      attributes.find("onesided") != attributes.end(),
          "'onesided' Attribute is expected for FftR2cOp. ");
  bool onesided = attributes.at("onesided").dyn_cast<pir::BoolAttribute>().data();


  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  std::vector<pir::Attribute> vec_axes;
  for (size_t i = 0; i < static_cast<size_t>(axes.size()); i++) {
      pir::Attribute attr_axes = pir::Int64Attribute::get(pir::IrContext::Instance(), axes[i]);

    vec_axes.push_back(attr_axes);
  }
  pir::Attribute attr_axes = pir::ArrayAttribute::get(pir::IrContext::Instance(), vec_axes);
  argument.AddAttribute("axes", attr_axes);
  pir::Attribute attr_normalization = pir::StrAttribute::get(pir::IrContext::Instance(), normalization);
  argument.AddAttribute("normalization", attr_normalization);
  pir::Attribute attr_forward = pir::BoolAttribute::get(pir::IrContext::Instance(), forward);
  argument.AddAttribute("forward", attr_forward);
  pir::Attribute attr_onesided = pir::BoolAttribute::get(pir::IrContext::Instance(), onesided);
  argument.AddAttribute("onesided", attr_onesided);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);
  paddle::dialect::IrTensor dense_out;
  paddle::dialect::IrMetaTensor meta_out(&dense_out);

  phi::FFTR2CInferMeta(meta_x, axes, normalization, forward, onesided, &meta_out);

  std::vector<pir::Type> argument_outputs;
  pir::Type out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_out.dtype()), dense_out.dims(), dense_out.layout(), dense_out.lod(), dense_out.offset());
  argument_outputs.push_back(out_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void FftR2cOp::VerifySig() {
  VLOG(4) << "Start Verifying inputs, outputs and attributes for: FftR2cOp.";
  VLOG(4) << "Verifying inputs:";
  {
  auto input_size = num_operands();
  IR_ENFORCE(input_size == 1u,
                    "The size %d of inputs must be equal to 1.", input_size);
  IR_ENFORCE((*this)->operand_source(0).type().isa<paddle::dialect::DenseTensorType>(),
                  "Type validation failed for the 0th input, got %s.", (*this)->operand_source(0).type());
  }
  VLOG(4) << "Verifying attributes:";
  {
  auto& attributes = this->attributes();
  IR_ENFORCE(attributes.count("axes")>0,
                 "axes does not exist.");
  IR_ENFORCE(attributes.at("axes").isa<pir::ArrayAttribute>(),
                 "Type of attribute: axes is not pir::ArrayAttribute.");
  for (size_t i = 0; i < attributes.at("axes").dyn_cast<pir::ArrayAttribute>().size(); i++) {
    IR_ENFORCE(attributes.at("axes").dyn_cast<pir::ArrayAttribute>().at(i).isa<pir::Int64Attribute>(),
                   "Type of attribute: axes is not right.");
  }
  IR_ENFORCE(attributes.count("normalization")>0,
                 "normalization does not exist.");
  IR_ENFORCE(attributes.at("normalization").isa<pir::StrAttribute>(),
                 "Type of attribute: normalization is not pir::StrAttribute.");

  IR_ENFORCE(attributes.count("forward")>0,
                 "forward does not exist.");
  IR_ENFORCE(attributes.at("forward").isa<pir::BoolAttribute>(),
                 "Type of attribute: forward is not pir::BoolAttribute.");

  IR_ENFORCE(attributes.count("onesided")>0,
                 "onesided does not exist.");
  IR_ENFORCE(attributes.at("onesided").isa<pir::BoolAttribute>(),
                 "Type of attribute: onesided is not pir::BoolAttribute.");

  }
  VLOG(4) << "Verifying outputs:";
  {
  auto output_size = num_results();
  IR_ENFORCE(output_size == 1u,
                    "The size %d of outputs must be equal to 1.", output_size);
  IR_ENFORCE((*this)->result(0).type().isa<paddle::dialect::DenseTensorType>(),
                 "Type validation failed for the 0th output.");
  }
  VLOG(4) << "End Verifying for: FftR2cOp.";
}

void FftR2cOp::InferMeta( phi::InferMetaContext *infer_meta ) {
  auto fn = PD_INFER_META(phi::FFTR2CInferMeta);
  fn(infer_meta);
}

phi::DataType FftR2cOp::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: FftR2cOp";
  


  return expected_kernel_dtype;
}

OpInfoTuple FillOp::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("x", "paddle::dialect::DenseTensorType", false, false, false, true), paddle::dialect::OpInputInfo("value", "paddle::dialect::ScalarAttribute", false, false, true, false) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = {  };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("out", "paddle::dialect::DenseTensorType", false, false) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("UnchangedInferMeta", {"x"}, "fill", {"x", "value"}, {}, {}, {}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "fill");
}

void FillOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, float value) {
  VLOG(4) << "Start build FillOp";


  // Generate scalar mutable attribute: value
  paddle::dialect::FullOp full_value_op = builder.Build<paddle::dialect::FullOp>(std::vector<int64_t>{1}, value, phi::DataType::FLOAT32, phi::CPUPlace());
  pir::OpResult value_ = full_value_op->result(0);
    
  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_, value_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);
  paddle::dialect::IrTensor dense_out;
  paddle::dialect::IrMetaTensor meta_out(&dense_out);

  phi::UnchangedInferMeta(meta_x, &meta_out);

  std::vector<pir::Type> argument_outputs;
  pir::Type out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_out.dtype()), dense_out.dims(), dense_out.layout(), dense_out.lod(), dense_out.offset());
  argument_outputs.push_back(out_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void FillOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, pir::AttributeMap attributes) {
  VLOG(4) << "Start build FillOp";


  IR_ENFORCE(
      attributes.find("value") != attributes.end(),
          "'value' Attribute is expected for FillOp. ");
  float value = attributes.at("value").dyn_cast<paddle::dialect::ScalarAttribute>().data().to<float>();

  // Generate scalar mutable attribute: value
  paddle::dialect::FullOp full_value_op = builder.Build<paddle::dialect::FullOp>(std::vector<int64_t>{1}, value, phi::DataType::FLOAT32, phi::CPUPlace());
  pir::OpResult value_ = full_value_op->result(0);
    
  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_, value_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);
  paddle::dialect::IrTensor dense_out;
  paddle::dialect::IrMetaTensor meta_out(&dense_out);

  phi::UnchangedInferMeta(meta_x, &meta_out);

  std::vector<pir::Type> argument_outputs;
  pir::Type out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_out.dtype()), dense_out.dims(), dense_out.layout(), dense_out.lod(), dense_out.offset());
  argument_outputs.push_back(out_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void FillOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, pir::Value value_) {
  VLOG(4) << "Start build FillOp";



  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_, value_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;
  phi::Scalar value;
  if (value_.dyn_cast<pir::OpResult>() && value_.dyn_cast<pir::OpResult>().owner()->isa<paddle::dialect::FullOp>()) {
    value = std::move(phi::Scalar(value_.dyn_cast<pir::OpResult>().owner()
                                  ->dyn_cast<paddle::dialect::FullOp>()
                                  .attribute("value")
                                  .dyn_cast<paddle::dialect::ScalarAttribute>()
                                  .data()
                                  .to<int>()));
  }
  else {
    value = std::move(phi::Scalar(-1));
    value.SetFromTensor(true);
  }


  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);
  paddle::dialect::IrTensor dense_out;
  paddle::dialect::IrMetaTensor meta_out(&dense_out);

  phi::UnchangedInferMeta(meta_x, &meta_out);

  std::vector<pir::Type> argument_outputs;
  pir::Type out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_out.dtype()), dense_out.dims(), dense_out.layout(), dense_out.lod(), dense_out.offset());
  argument_outputs.push_back(out_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void FillOp::VerifySig() {
  VLOG(4) << "Start Verifying inputs, outputs and attributes for: FillOp.";
  VLOG(4) << "Verifying inputs:";
  {
  auto input_size = num_operands();
  IR_ENFORCE(input_size == 2u,
                    "The size %d of inputs must be equal to 2.", input_size);
  IR_ENFORCE((*this)->operand_source(0).type().isa<paddle::dialect::DenseTensorType>(),
                  "Type validation failed for the 0th input, got %s.", (*this)->operand_source(0).type());
  IR_ENFORCE((*this)->operand_source(1).type().isa<paddle::dialect::DenseTensorType>(),
                  "Type validation failed for the 1th input, got %s.", (*this)->operand_source(1).type());
  }
  VLOG(4) << "Verifying attributes:";
  {
  // Attributes num is 0, not need to check attributes type.
  }
  VLOG(4) << "Verifying outputs:";
  {
  auto output_size = num_results();
  IR_ENFORCE(output_size == 1u,
                    "The size %d of outputs must be equal to 1.", output_size);
  IR_ENFORCE((*this)->result(0).type().isa<paddle::dialect::DenseTensorType>(),
                 "Type validation failed for the 0th output.");
  }
  VLOG(4) << "End Verifying for: FillOp.";
}

void FillOp::InferMeta( phi::InferMetaContext *infer_meta ) {
  auto fn = PD_INFER_META(phi::UnchangedInferMeta);
  fn(infer_meta);
}

phi::DataType FillOp::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: FillOp";
  


  return expected_kernel_dtype;
}

OpInfoTuple Fill_Op::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("x", "paddle::dialect::DenseTensorType", false, false, false, true), paddle::dialect::OpInputInfo("value", "paddle::dialect::ScalarAttribute", false, false, true, false) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = {  };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("out", "paddle::dialect::DenseTensorType", false, false) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("UnchangedInferMeta", {"x"}, "fill", {"x", "value"}, {}, {}, {{"out", "x"}}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "fill");
}

void Fill_Op::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, float value) {
  VLOG(4) << "Start build Fill_Op";


  // Generate scalar mutable attribute: value
  paddle::dialect::FullOp full_value_op = builder.Build<paddle::dialect::FullOp>(std::vector<int64_t>{1}, value, phi::DataType::FLOAT32, phi::CPUPlace());
  pir::OpResult value_ = full_value_op->result(0);
    
  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_, value_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);
  paddle::dialect::IrTensor dense_out;
  paddle::dialect::IrMetaTensor meta_out(&dense_out);

  phi::UnchangedInferMeta(meta_x, &meta_out);

  std::vector<pir::Type> argument_outputs;
  pir::Type out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_out.dtype()), dense_out.dims(), dense_out.layout(), dense_out.lod(), dense_out.offset());
  argument_outputs.push_back(out_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void Fill_Op::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, pir::AttributeMap attributes) {
  VLOG(4) << "Start build Fill_Op";


  IR_ENFORCE(
      attributes.find("value") != attributes.end(),
          "'value' Attribute is expected for Fill_Op. ");
  float value = attributes.at("value").dyn_cast<paddle::dialect::ScalarAttribute>().data().to<float>();

  // Generate scalar mutable attribute: value
  paddle::dialect::FullOp full_value_op = builder.Build<paddle::dialect::FullOp>(std::vector<int64_t>{1}, value, phi::DataType::FLOAT32, phi::CPUPlace());
  pir::OpResult value_ = full_value_op->result(0);
    
  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_, value_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);
  paddle::dialect::IrTensor dense_out;
  paddle::dialect::IrMetaTensor meta_out(&dense_out);

  phi::UnchangedInferMeta(meta_x, &meta_out);

  std::vector<pir::Type> argument_outputs;
  pir::Type out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_out.dtype()), dense_out.dims(), dense_out.layout(), dense_out.lod(), dense_out.offset());
  argument_outputs.push_back(out_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void Fill_Op::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, pir::Value value_) {
  VLOG(4) << "Start build Fill_Op";



  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_, value_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;
  phi::Scalar value;
  if (value_.dyn_cast<pir::OpResult>() && value_.dyn_cast<pir::OpResult>().owner()->isa<paddle::dialect::FullOp>()) {
    value = std::move(phi::Scalar(value_.dyn_cast<pir::OpResult>().owner()
                                  ->dyn_cast<paddle::dialect::FullOp>()
                                  .attribute("value")
                                  .dyn_cast<paddle::dialect::ScalarAttribute>()
                                  .data()
                                  .to<int>()));
  }
  else {
    value = std::move(phi::Scalar(-1));
    value.SetFromTensor(true);
  }


  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);
  paddle::dialect::IrTensor dense_out;
  paddle::dialect::IrMetaTensor meta_out(&dense_out);

  phi::UnchangedInferMeta(meta_x, &meta_out);

  std::vector<pir::Type> argument_outputs;
  pir::Type out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_out.dtype()), dense_out.dims(), dense_out.layout(), dense_out.lod(), dense_out.offset());
  argument_outputs.push_back(out_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void Fill_Op::VerifySig() {
  VLOG(4) << "Start Verifying inputs, outputs and attributes for: Fill_Op.";
  VLOG(4) << "Verifying inputs:";
  {
  auto input_size = num_operands();
  IR_ENFORCE(input_size == 2u,
                    "The size %d of inputs must be equal to 2.", input_size);
  IR_ENFORCE((*this)->operand_source(0).type().isa<paddle::dialect::DenseTensorType>(),
                  "Type validation failed for the 0th input, got %s.", (*this)->operand_source(0).type());
  IR_ENFORCE((*this)->operand_source(1).type().isa<paddle::dialect::DenseTensorType>(),
                  "Type validation failed for the 1th input, got %s.", (*this)->operand_source(1).type());
  }
  VLOG(4) << "Verifying attributes:";
  {
  // Attributes num is 0, not need to check attributes type.
  }
  VLOG(4) << "Verifying outputs:";
  {
  auto output_size = num_results();
  IR_ENFORCE(output_size == 1u,
                    "The size %d of outputs must be equal to 1.", output_size);
  IR_ENFORCE((*this)->result(0).type().isa<paddle::dialect::DenseTensorType>(),
                 "Type validation failed for the 0th output.");
  }
  VLOG(4) << "End Verifying for: Fill_Op.";
}

void Fill_Op::InferMeta( phi::InferMetaContext *infer_meta ) {
  auto fn = PD_INFER_META(phi::UnchangedInferMeta);
  fn(infer_meta);
}

phi::DataType Fill_Op::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: Fill_Op";
  


  return expected_kernel_dtype;
}

const char *FillDiagonalOp::attributes_name[3] = { "value", "offset", "wrap" };

OpInfoTuple FillDiagonalOp::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("x", "paddle::dialect::DenseTensorType", false, false, false, true) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = { paddle::dialect::OpAttributeInfo("value", "pir::FloatAttribute", ""), paddle::dialect::OpAttributeInfo("offset", "pir::Int32Attribute", ""), paddle::dialect::OpAttributeInfo("wrap", "pir::BoolAttribute", "") };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("out", "paddle::dialect::DenseTensorType", false, false) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("FillDiagonalInferMeta", {"x", "value", "offset", "wrap"}, "fill_diagonal", {"x", "value", "offset", "wrap"}, {"x"}, {}, {}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "fill_diagonal");
}

void FillDiagonalOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, float value, int offset, bool wrap) {
  VLOG(4) << "Start build FillDiagonalOp";



  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_value = pir::FloatAttribute::get(pir::IrContext::Instance(), value);
  argument.AddAttribute("value", attr_value);
  pir::Attribute attr_offset = pir::Int32Attribute::get(pir::IrContext::Instance(), offset);
  argument.AddAttribute("offset", attr_offset);
  pir::Attribute attr_wrap = pir::BoolAttribute::get(pir::IrContext::Instance(), wrap);
  argument.AddAttribute("wrap", attr_wrap);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);
  paddle::dialect::IrTensor dense_out;
  paddle::dialect::IrMetaTensor meta_out(&dense_out);

  phi::FillDiagonalInferMeta(meta_x, value, offset, wrap, &meta_out);

  std::vector<pir::Type> argument_outputs;
  pir::Type out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_out.dtype()), dense_out.dims(), dense_out.layout(), dense_out.lod(), dense_out.offset());
  argument_outputs.push_back(out_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void FillDiagonalOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, pir::AttributeMap attributes) {
  VLOG(4) << "Start build FillDiagonalOp";


  IR_ENFORCE(
      attributes.find("value") != attributes.end(),
          "'value' Attribute is expected for FillDiagonalOp. ");
  float value = attributes.at("value").dyn_cast<pir::FloatAttribute>().data();

  IR_ENFORCE(
      attributes.find("offset") != attributes.end(),
          "'offset' Attribute is expected for FillDiagonalOp. ");
  int offset = attributes.at("offset").dyn_cast<pir::Int32Attribute>().data();

  IR_ENFORCE(
      attributes.find("wrap") != attributes.end(),
          "'wrap' Attribute is expected for FillDiagonalOp. ");
  bool wrap = attributes.at("wrap").dyn_cast<pir::BoolAttribute>().data();


  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_value = pir::FloatAttribute::get(pir::IrContext::Instance(), value);
  argument.AddAttribute("value", attr_value);
  pir::Attribute attr_offset = pir::Int32Attribute::get(pir::IrContext::Instance(), offset);
  argument.AddAttribute("offset", attr_offset);
  pir::Attribute attr_wrap = pir::BoolAttribute::get(pir::IrContext::Instance(), wrap);
  argument.AddAttribute("wrap", attr_wrap);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);
  paddle::dialect::IrTensor dense_out;
  paddle::dialect::IrMetaTensor meta_out(&dense_out);

  phi::FillDiagonalInferMeta(meta_x, value, offset, wrap, &meta_out);

  std::vector<pir::Type> argument_outputs;
  pir::Type out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_out.dtype()), dense_out.dims(), dense_out.layout(), dense_out.lod(), dense_out.offset());
  argument_outputs.push_back(out_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void FillDiagonalOp::VerifySig() {
  VLOG(4) << "Start Verifying inputs, outputs and attributes for: FillDiagonalOp.";
  VLOG(4) << "Verifying inputs:";
  {
  auto input_size = num_operands();
  IR_ENFORCE(input_size == 1u,
                    "The size %d of inputs must be equal to 1.", input_size);
  IR_ENFORCE((*this)->operand_source(0).type().isa<paddle::dialect::DenseTensorType>(),
                  "Type validation failed for the 0th input, got %s.", (*this)->operand_source(0).type());
  }
  VLOG(4) << "Verifying attributes:";
  {
  auto& attributes = this->attributes();
  IR_ENFORCE(attributes.count("value")>0,
                 "value does not exist.");
  IR_ENFORCE(attributes.at("value").isa<pir::FloatAttribute>(),
                 "Type of attribute: value is not pir::FloatAttribute.");

  IR_ENFORCE(attributes.count("offset")>0,
                 "offset does not exist.");
  IR_ENFORCE(attributes.at("offset").isa<pir::Int32Attribute>(),
                 "Type of attribute: offset is not pir::Int32Attribute.");

  IR_ENFORCE(attributes.count("wrap")>0,
                 "wrap does not exist.");
  IR_ENFORCE(attributes.at("wrap").isa<pir::BoolAttribute>(),
                 "Type of attribute: wrap is not pir::BoolAttribute.");

  }
  VLOG(4) << "Verifying outputs:";
  {
  auto output_size = num_results();
  IR_ENFORCE(output_size == 1u,
                    "The size %d of outputs must be equal to 1.", output_size);
  IR_ENFORCE((*this)->result(0).type().isa<paddle::dialect::DenseTensorType>(),
                 "Type validation failed for the 0th output.");
  }
  VLOG(4) << "End Verifying for: FillDiagonalOp.";
}

void FillDiagonalOp::InferMeta( phi::InferMetaContext *infer_meta ) {
  auto fn = PD_INFER_META(phi::FillDiagonalInferMeta);
  fn(infer_meta);
}

phi::DataType FillDiagonalOp::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: FillDiagonalOp";
  


  return expected_kernel_dtype;
}

const char *FillDiagonal_Op::attributes_name[3] = { "value", "offset", "wrap" };

OpInfoTuple FillDiagonal_Op::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("x", "paddle::dialect::DenseTensorType", false, false, false, true) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = { paddle::dialect::OpAttributeInfo("value", "pir::FloatAttribute", ""), paddle::dialect::OpAttributeInfo("offset", "pir::Int32Attribute", ""), paddle::dialect::OpAttributeInfo("wrap", "pir::BoolAttribute", "") };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("out", "paddle::dialect::DenseTensorType", false, false) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("FillDiagonalInferMeta", {"x", "value", "offset", "wrap"}, "fill_diagonal", {"x", "value", "offset", "wrap"}, {"x"}, {}, {{"out", "x"}}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "fill_diagonal");
}

void FillDiagonal_Op::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, float value, int offset, bool wrap) {
  VLOG(4) << "Start build FillDiagonal_Op";



  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_value = pir::FloatAttribute::get(pir::IrContext::Instance(), value);
  argument.AddAttribute("value", attr_value);
  pir::Attribute attr_offset = pir::Int32Attribute::get(pir::IrContext::Instance(), offset);
  argument.AddAttribute("offset", attr_offset);
  pir::Attribute attr_wrap = pir::BoolAttribute::get(pir::IrContext::Instance(), wrap);
  argument.AddAttribute("wrap", attr_wrap);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);
  paddle::dialect::IrTensor dense_out;
  paddle::dialect::IrMetaTensor meta_out(&dense_out);

  phi::FillDiagonalInferMeta(meta_x, value, offset, wrap, &meta_out);

  std::vector<pir::Type> argument_outputs;
  pir::Type out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_out.dtype()), dense_out.dims(), dense_out.layout(), dense_out.lod(), dense_out.offset());
  argument_outputs.push_back(out_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void FillDiagonal_Op::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, pir::AttributeMap attributes) {
  VLOG(4) << "Start build FillDiagonal_Op";


  IR_ENFORCE(
      attributes.find("value") != attributes.end(),
          "'value' Attribute is expected for FillDiagonal_Op. ");
  float value = attributes.at("value").dyn_cast<pir::FloatAttribute>().data();

  IR_ENFORCE(
      attributes.find("offset") != attributes.end(),
          "'offset' Attribute is expected for FillDiagonal_Op. ");
  int offset = attributes.at("offset").dyn_cast<pir::Int32Attribute>().data();

  IR_ENFORCE(
      attributes.find("wrap") != attributes.end(),
          "'wrap' Attribute is expected for FillDiagonal_Op. ");
  bool wrap = attributes.at("wrap").dyn_cast<pir::BoolAttribute>().data();


  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_value = pir::FloatAttribute::get(pir::IrContext::Instance(), value);
  argument.AddAttribute("value", attr_value);
  pir::Attribute attr_offset = pir::Int32Attribute::get(pir::IrContext::Instance(), offset);
  argument.AddAttribute("offset", attr_offset);
  pir::Attribute attr_wrap = pir::BoolAttribute::get(pir::IrContext::Instance(), wrap);
  argument.AddAttribute("wrap", attr_wrap);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);
  paddle::dialect::IrTensor dense_out;
  paddle::dialect::IrMetaTensor meta_out(&dense_out);

  phi::FillDiagonalInferMeta(meta_x, value, offset, wrap, &meta_out);

  std::vector<pir::Type> argument_outputs;
  pir::Type out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_out.dtype()), dense_out.dims(), dense_out.layout(), dense_out.lod(), dense_out.offset());
  argument_outputs.push_back(out_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void FillDiagonal_Op::VerifySig() {
  VLOG(4) << "Start Verifying inputs, outputs and attributes for: FillDiagonal_Op.";
  VLOG(4) << "Verifying inputs:";
  {
  auto input_size = num_operands();
  IR_ENFORCE(input_size == 1u,
                    "The size %d of inputs must be equal to 1.", input_size);
  IR_ENFORCE((*this)->operand_source(0).type().isa<paddle::dialect::DenseTensorType>(),
                  "Type validation failed for the 0th input, got %s.", (*this)->operand_source(0).type());
  }
  VLOG(4) << "Verifying attributes:";
  {
  auto& attributes = this->attributes();
  IR_ENFORCE(attributes.count("value")>0,
                 "value does not exist.");
  IR_ENFORCE(attributes.at("value").isa<pir::FloatAttribute>(),
                 "Type of attribute: value is not pir::FloatAttribute.");

  IR_ENFORCE(attributes.count("offset")>0,
                 "offset does not exist.");
  IR_ENFORCE(attributes.at("offset").isa<pir::Int32Attribute>(),
                 "Type of attribute: offset is not pir::Int32Attribute.");

  IR_ENFORCE(attributes.count("wrap")>0,
                 "wrap does not exist.");
  IR_ENFORCE(attributes.at("wrap").isa<pir::BoolAttribute>(),
                 "Type of attribute: wrap is not pir::BoolAttribute.");

  }
  VLOG(4) << "Verifying outputs:";
  {
  auto output_size = num_results();
  IR_ENFORCE(output_size == 1u,
                    "The size %d of outputs must be equal to 1.", output_size);
  IR_ENFORCE((*this)->result(0).type().isa<paddle::dialect::DenseTensorType>(),
                 "Type validation failed for the 0th output.");
  }
  VLOG(4) << "End Verifying for: FillDiagonal_Op.";
}

void FillDiagonal_Op::InferMeta( phi::InferMetaContext *infer_meta ) {
  auto fn = PD_INFER_META(phi::FillDiagonalInferMeta);
  fn(infer_meta);
}

phi::DataType FillDiagonal_Op::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: FillDiagonal_Op";
  


  return expected_kernel_dtype;
}

const char *FillDiagonalTensorOp::attributes_name[3] = { "offset", "dim1", "dim2" };

OpInfoTuple FillDiagonalTensorOp::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("x", "paddle::dialect::DenseTensorType", false, false, false, true), paddle::dialect::OpInputInfo("y", "paddle::dialect::DenseTensorType", false, false, false, false) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = { paddle::dialect::OpAttributeInfo("offset", "pir::Int64Attribute", ""), paddle::dialect::OpAttributeInfo("dim1", "pir::Int32Attribute", ""), paddle::dialect::OpAttributeInfo("dim2", "pir::Int32Attribute", "") };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("out", "paddle::dialect::DenseTensorType", false, false) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("FillDiagonalTensorInferMeta", {"x", "y", "offset", "dim1", "dim2"}, "fill_diagonal_tensor", {"x", "y", "offset", "dim1", "dim2"}, {}, {}, {}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "fill_diagonal_tensor");
}

void FillDiagonalTensorOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, pir::Value y_, int64_t offset, int dim1, int dim2) {
  VLOG(4) << "Start build FillDiagonalTensorOp";



  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_, y_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_offset = pir::Int64Attribute::get(pir::IrContext::Instance(), offset);
  argument.AddAttribute("offset", attr_offset);
  pir::Attribute attr_dim1 = pir::Int32Attribute::get(pir::IrContext::Instance(), dim1);
  argument.AddAttribute("dim1", attr_dim1);
  pir::Attribute attr_dim2 = pir::Int32Attribute::get(pir::IrContext::Instance(), dim2);
  argument.AddAttribute("dim2", attr_dim2);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;
  paddle::dialect::DenseTensorType y = y_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)y;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);

  VLOG(4) << "Builder construction  dense_y";
  paddle::dialect::IrTensor ir_tensor_y(paddle::dialect::TransToPhiDataType(y.dtype()),
                                                      y.dims(),
                                                      y.data_layout(),
                                                      y.lod(),
                                                      y.offset());
  VLOG(4) << "Builder construction  meta_y";
  paddle::dialect::IrMetaTensor meta_y(&ir_tensor_y);
  paddle::dialect::IrTensor dense_out;
  paddle::dialect::IrMetaTensor meta_out(&dense_out);

  phi::FillDiagonalTensorInferMeta(meta_x, meta_y, offset, dim1, dim2, &meta_out);

  std::vector<pir::Type> argument_outputs;
  pir::Type out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_out.dtype()), dense_out.dims(), dense_out.layout(), dense_out.lod(), dense_out.offset());
  argument_outputs.push_back(out_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void FillDiagonalTensorOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, pir::Value y_, pir::AttributeMap attributes) {
  VLOG(4) << "Start build FillDiagonalTensorOp";


  IR_ENFORCE(
      attributes.find("offset") != attributes.end(),
          "'offset' Attribute is expected for FillDiagonalTensorOp. ");
  int64_t offset = attributes.at("offset").dyn_cast<pir::Int64Attribute>().data();

  IR_ENFORCE(
      attributes.find("dim1") != attributes.end(),
          "'dim1' Attribute is expected for FillDiagonalTensorOp. ");
  int dim1 = attributes.at("dim1").dyn_cast<pir::Int32Attribute>().data();

  IR_ENFORCE(
      attributes.find("dim2") != attributes.end(),
          "'dim2' Attribute is expected for FillDiagonalTensorOp. ");
  int dim2 = attributes.at("dim2").dyn_cast<pir::Int32Attribute>().data();


  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_, y_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_offset = pir::Int64Attribute::get(pir::IrContext::Instance(), offset);
  argument.AddAttribute("offset", attr_offset);
  pir::Attribute attr_dim1 = pir::Int32Attribute::get(pir::IrContext::Instance(), dim1);
  argument.AddAttribute("dim1", attr_dim1);
  pir::Attribute attr_dim2 = pir::Int32Attribute::get(pir::IrContext::Instance(), dim2);
  argument.AddAttribute("dim2", attr_dim2);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;
  paddle::dialect::DenseTensorType y = y_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)y;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);

  VLOG(4) << "Builder construction  dense_y";
  paddle::dialect::IrTensor ir_tensor_y(paddle::dialect::TransToPhiDataType(y.dtype()),
                                                      y.dims(),
                                                      y.data_layout(),
                                                      y.lod(),
                                                      y.offset());
  VLOG(4) << "Builder construction  meta_y";
  paddle::dialect::IrMetaTensor meta_y(&ir_tensor_y);
  paddle::dialect::IrTensor dense_out;
  paddle::dialect::IrMetaTensor meta_out(&dense_out);

  phi::FillDiagonalTensorInferMeta(meta_x, meta_y, offset, dim1, dim2, &meta_out);

  std::vector<pir::Type> argument_outputs;
  pir::Type out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_out.dtype()), dense_out.dims(), dense_out.layout(), dense_out.lod(), dense_out.offset());
  argument_outputs.push_back(out_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void FillDiagonalTensorOp::VerifySig() {
  VLOG(4) << "Start Verifying inputs, outputs and attributes for: FillDiagonalTensorOp.";
  VLOG(4) << "Verifying inputs:";
  {
  auto input_size = num_operands();
  IR_ENFORCE(input_size == 2u,
                    "The size %d of inputs must be equal to 2.", input_size);
  IR_ENFORCE((*this)->operand_source(0).type().isa<paddle::dialect::DenseTensorType>(),
                  "Type validation failed for the 0th input, got %s.", (*this)->operand_source(0).type());
  IR_ENFORCE((*this)->operand_source(1).type().isa<paddle::dialect::DenseTensorType>(),
                  "Type validation failed for the 1th input, got %s.", (*this)->operand_source(1).type());
  }
  VLOG(4) << "Verifying attributes:";
  {
  auto& attributes = this->attributes();
  IR_ENFORCE(attributes.count("offset")>0,
                 "offset does not exist.");
  IR_ENFORCE(attributes.at("offset").isa<pir::Int64Attribute>(),
                 "Type of attribute: offset is not pir::Int64Attribute.");

  IR_ENFORCE(attributes.count("dim1")>0,
                 "dim1 does not exist.");
  IR_ENFORCE(attributes.at("dim1").isa<pir::Int32Attribute>(),
                 "Type of attribute: dim1 is not pir::Int32Attribute.");

  IR_ENFORCE(attributes.count("dim2")>0,
                 "dim2 does not exist.");
  IR_ENFORCE(attributes.at("dim2").isa<pir::Int32Attribute>(),
                 "Type of attribute: dim2 is not pir::Int32Attribute.");

  }
  VLOG(4) << "Verifying outputs:";
  {
  auto output_size = num_results();
  IR_ENFORCE(output_size == 1u,
                    "The size %d of outputs must be equal to 1.", output_size);
  IR_ENFORCE((*this)->result(0).type().isa<paddle::dialect::DenseTensorType>(),
                 "Type validation failed for the 0th output.");
  }
  VLOG(4) << "End Verifying for: FillDiagonalTensorOp.";
}

void FillDiagonalTensorOp::InferMeta( phi::InferMetaContext *infer_meta ) {
  auto fn = PD_INFER_META(phi::FillDiagonalTensorInferMeta);
  fn(infer_meta);
}

phi::DataType FillDiagonalTensorOp::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: FillDiagonalTensorOp";
  


  return expected_kernel_dtype;
}

const char *FillDiagonalTensor_Op::attributes_name[3] = { "offset", "dim1", "dim2" };

OpInfoTuple FillDiagonalTensor_Op::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("x", "paddle::dialect::DenseTensorType", false, false, false, true), paddle::dialect::OpInputInfo("y", "paddle::dialect::DenseTensorType", false, false, false, false) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = { paddle::dialect::OpAttributeInfo("offset", "pir::Int64Attribute", ""), paddle::dialect::OpAttributeInfo("dim1", "pir::Int32Attribute", ""), paddle::dialect::OpAttributeInfo("dim2", "pir::Int32Attribute", "") };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("out", "paddle::dialect::DenseTensorType", false, false) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("FillDiagonalTensorInferMeta", {"x", "y", "offset", "dim1", "dim2"}, "fill_diagonal_tensor", {"x", "y", "offset", "dim1", "dim2"}, {}, {}, {{"out", "x"}}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "fill_diagonal_tensor");
}

void FillDiagonalTensor_Op::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, pir::Value y_, int64_t offset, int dim1, int dim2) {
  VLOG(4) << "Start build FillDiagonalTensor_Op";



  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_, y_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_offset = pir::Int64Attribute::get(pir::IrContext::Instance(), offset);
  argument.AddAttribute("offset", attr_offset);
  pir::Attribute attr_dim1 = pir::Int32Attribute::get(pir::IrContext::Instance(), dim1);
  argument.AddAttribute("dim1", attr_dim1);
  pir::Attribute attr_dim2 = pir::Int32Attribute::get(pir::IrContext::Instance(), dim2);
  argument.AddAttribute("dim2", attr_dim2);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;
  paddle::dialect::DenseTensorType y = y_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)y;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);

  VLOG(4) << "Builder construction  dense_y";
  paddle::dialect::IrTensor ir_tensor_y(paddle::dialect::TransToPhiDataType(y.dtype()),
                                                      y.dims(),
                                                      y.data_layout(),
                                                      y.lod(),
                                                      y.offset());
  VLOG(4) << "Builder construction  meta_y";
  paddle::dialect::IrMetaTensor meta_y(&ir_tensor_y);
  paddle::dialect::IrTensor dense_out;
  paddle::dialect::IrMetaTensor meta_out(&dense_out);

  phi::FillDiagonalTensorInferMeta(meta_x, meta_y, offset, dim1, dim2, &meta_out);

  std::vector<pir::Type> argument_outputs;
  pir::Type out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_out.dtype()), dense_out.dims(), dense_out.layout(), dense_out.lod(), dense_out.offset());
  argument_outputs.push_back(out_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void FillDiagonalTensor_Op::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, pir::Value y_, pir::AttributeMap attributes) {
  VLOG(4) << "Start build FillDiagonalTensor_Op";


  IR_ENFORCE(
      attributes.find("offset") != attributes.end(),
          "'offset' Attribute is expected for FillDiagonalTensor_Op. ");
  int64_t offset = attributes.at("offset").dyn_cast<pir::Int64Attribute>().data();

  IR_ENFORCE(
      attributes.find("dim1") != attributes.end(),
          "'dim1' Attribute is expected for FillDiagonalTensor_Op. ");
  int dim1 = attributes.at("dim1").dyn_cast<pir::Int32Attribute>().data();

  IR_ENFORCE(
      attributes.find("dim2") != attributes.end(),
          "'dim2' Attribute is expected for FillDiagonalTensor_Op. ");
  int dim2 = attributes.at("dim2").dyn_cast<pir::Int32Attribute>().data();


  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_, y_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_offset = pir::Int64Attribute::get(pir::IrContext::Instance(), offset);
  argument.AddAttribute("offset", attr_offset);
  pir::Attribute attr_dim1 = pir::Int32Attribute::get(pir::IrContext::Instance(), dim1);
  argument.AddAttribute("dim1", attr_dim1);
  pir::Attribute attr_dim2 = pir::Int32Attribute::get(pir::IrContext::Instance(), dim2);
  argument.AddAttribute("dim2", attr_dim2);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;
  paddle::dialect::DenseTensorType y = y_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)y;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);

  VLOG(4) << "Builder construction  dense_y";
  paddle::dialect::IrTensor ir_tensor_y(paddle::dialect::TransToPhiDataType(y.dtype()),
                                                      y.dims(),
                                                      y.data_layout(),
                                                      y.lod(),
                                                      y.offset());
  VLOG(4) << "Builder construction  meta_y";
  paddle::dialect::IrMetaTensor meta_y(&ir_tensor_y);
  paddle::dialect::IrTensor dense_out;
  paddle::dialect::IrMetaTensor meta_out(&dense_out);

  phi::FillDiagonalTensorInferMeta(meta_x, meta_y, offset, dim1, dim2, &meta_out);

  std::vector<pir::Type> argument_outputs;
  pir::Type out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_out.dtype()), dense_out.dims(), dense_out.layout(), dense_out.lod(), dense_out.offset());
  argument_outputs.push_back(out_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void FillDiagonalTensor_Op::VerifySig() {
  VLOG(4) << "Start Verifying inputs, outputs and attributes for: FillDiagonalTensor_Op.";
  VLOG(4) << "Verifying inputs:";
  {
  auto input_size = num_operands();
  IR_ENFORCE(input_size == 2u,
                    "The size %d of inputs must be equal to 2.", input_size);
  IR_ENFORCE((*this)->operand_source(0).type().isa<paddle::dialect::DenseTensorType>(),
                  "Type validation failed for the 0th input, got %s.", (*this)->operand_source(0).type());
  IR_ENFORCE((*this)->operand_source(1).type().isa<paddle::dialect::DenseTensorType>(),
                  "Type validation failed for the 1th input, got %s.", (*this)->operand_source(1).type());
  }
  VLOG(4) << "Verifying attributes:";
  {
  auto& attributes = this->attributes();
  IR_ENFORCE(attributes.count("offset")>0,
                 "offset does not exist.");
  IR_ENFORCE(attributes.at("offset").isa<pir::Int64Attribute>(),
                 "Type of attribute: offset is not pir::Int64Attribute.");

  IR_ENFORCE(attributes.count("dim1")>0,
                 "dim1 does not exist.");
  IR_ENFORCE(attributes.at("dim1").isa<pir::Int32Attribute>(),
                 "Type of attribute: dim1 is not pir::Int32Attribute.");

  IR_ENFORCE(attributes.count("dim2")>0,
                 "dim2 does not exist.");
  IR_ENFORCE(attributes.at("dim2").isa<pir::Int32Attribute>(),
                 "Type of attribute: dim2 is not pir::Int32Attribute.");

  }
  VLOG(4) << "Verifying outputs:";
  {
  auto output_size = num_results();
  IR_ENFORCE(output_size == 1u,
                    "The size %d of outputs must be equal to 1.", output_size);
  IR_ENFORCE((*this)->result(0).type().isa<paddle::dialect::DenseTensorType>(),
                 "Type validation failed for the 0th output.");
  }
  VLOG(4) << "End Verifying for: FillDiagonalTensor_Op.";
}

void FillDiagonalTensor_Op::InferMeta( phi::InferMetaContext *infer_meta ) {
  auto fn = PD_INFER_META(phi::FillDiagonalTensorInferMeta);
  fn(infer_meta);
}

phi::DataType FillDiagonalTensor_Op::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: FillDiagonalTensor_Op";
  


  return expected_kernel_dtype;
}

const char *FlashAttnOp::attributes_name[5] = { "dropout", "causal", "return_softmax", "is_test", "rng_name" };

OpInfoTuple FlashAttnOp::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("q", "paddle::dialect::DenseTensorType", false, false, false, true), paddle::dialect::OpInputInfo("k", "paddle::dialect::DenseTensorType", false, false, false, true), paddle::dialect::OpInputInfo("v", "paddle::dialect::DenseTensorType", false, false, false, true), paddle::dialect::OpInputInfo("fixed_seed_offset", "paddle::dialect::DenseTensorType", true, false, false, false), paddle::dialect::OpInputInfo("attn_mask", "paddle::dialect::DenseTensorType", true, false, false, false) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = { paddle::dialect::OpAttributeInfo("dropout", "pir::FloatAttribute", ""), paddle::dialect::OpAttributeInfo("causal", "pir::BoolAttribute", ""), paddle::dialect::OpAttributeInfo("return_softmax", "pir::BoolAttribute", ""), paddle::dialect::OpAttributeInfo("is_test", "pir::BoolAttribute", ""), paddle::dialect::OpAttributeInfo("rng_name", "pir::StrAttribute", "") };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("out", "paddle::dialect::DenseTensorType", false, false), paddle::dialect::OpOutputInfo("softmax", "paddle::dialect::DenseTensorType", false, false), paddle::dialect::OpOutputInfo("softmax_lse", "paddle::dialect::DenseTensorType", false, false), paddle::dialect::OpOutputInfo("seed_offset", "paddle::dialect::DenseTensorType", false, false) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("FlashAttnInferMeta", {"q", "k", "v"}, "flash_attn", {"q", "k", "v", "fixed_seed_offset", "attn_mask", "dropout", "causal", "return_softmax", "is_test", "rng_name"}, {"q"}, {}, {}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "flash_attn");
}

void FlashAttnOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value q_, pir::Value k_, pir::Value v_, pir::Value fixed_seed_offset_, pir::Value attn_mask_, float dropout, bool causal, bool return_softmax, bool is_test, const std::string& rng_name) {
  VLOG(4) << "Start build FlashAttnOp";



  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {q_, k_, v_, fixed_seed_offset_, attn_mask_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_dropout = pir::FloatAttribute::get(pir::IrContext::Instance(), dropout);
  argument.AddAttribute("dropout", attr_dropout);
  pir::Attribute attr_causal = pir::BoolAttribute::get(pir::IrContext::Instance(), causal);
  argument.AddAttribute("causal", attr_causal);
  pir::Attribute attr_return_softmax = pir::BoolAttribute::get(pir::IrContext::Instance(), return_softmax);
  argument.AddAttribute("return_softmax", attr_return_softmax);
  pir::Attribute attr_is_test = pir::BoolAttribute::get(pir::IrContext::Instance(), is_test);
  argument.AddAttribute("is_test", attr_is_test);
  pir::Attribute attr_rng_name = pir::StrAttribute::get(pir::IrContext::Instance(), rng_name);
  argument.AddAttribute("rng_name", attr_rng_name);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType q = q_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)q;
  paddle::dialect::DenseTensorType k = k_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)k;
  paddle::dialect::DenseTensorType v = v_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)v;

  VLOG(4) << "Builder construction  dense_q";
  paddle::dialect::IrTensor ir_tensor_q(paddle::dialect::TransToPhiDataType(q.dtype()),
                                                      q.dims(),
                                                      q.data_layout(),
                                                      q.lod(),
                                                      q.offset());
  VLOG(4) << "Builder construction  meta_q";
  paddle::dialect::IrMetaTensor meta_q(&ir_tensor_q);

  VLOG(4) << "Builder construction  dense_k";
  paddle::dialect::IrTensor ir_tensor_k(paddle::dialect::TransToPhiDataType(k.dtype()),
                                                      k.dims(),
                                                      k.data_layout(),
                                                      k.lod(),
                                                      k.offset());
  VLOG(4) << "Builder construction  meta_k";
  paddle::dialect::IrMetaTensor meta_k(&ir_tensor_k);

  VLOG(4) << "Builder construction  dense_v";
  paddle::dialect::IrTensor ir_tensor_v(paddle::dialect::TransToPhiDataType(v.dtype()),
                                                      v.dims(),
                                                      v.data_layout(),
                                                      v.lod(),
                                                      v.offset());
  VLOG(4) << "Builder construction  meta_v";
  paddle::dialect::IrMetaTensor meta_v(&ir_tensor_v);
  paddle::dialect::IrTensor dense_out;
  paddle::dialect::IrMetaTensor meta_out(&dense_out);
  paddle::dialect::IrTensor dense_softmax;
  paddle::dialect::IrMetaTensor meta_softmax(&dense_softmax);
  paddle::dialect::IrTensor dense_softmax_lse;
  paddle::dialect::IrMetaTensor meta_softmax_lse(&dense_softmax_lse);
  paddle::dialect::IrTensor dense_seed_offset;
  paddle::dialect::IrMetaTensor meta_seed_offset(&dense_seed_offset);

  phi::FlashAttnInferMeta(meta_q, meta_k, meta_v, &meta_out, &meta_softmax, &meta_softmax_lse, &meta_seed_offset);

  std::vector<pir::Type> argument_outputs;
  pir::Type out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_out.dtype()), dense_out.dims(), dense_out.layout(), dense_out.lod(), dense_out.offset());
  argument_outputs.push_back(out_dense_tensor_type);

  pir::Type softmax_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_softmax.dtype()), dense_softmax.dims(), dense_softmax.layout(), dense_softmax.lod(), dense_softmax.offset());
  argument_outputs.push_back(softmax_dense_tensor_type);

  pir::Type softmax_lse_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_softmax_lse.dtype()), dense_softmax_lse.dims(), dense_softmax_lse.layout(), dense_softmax_lse.lod(), dense_softmax_lse.offset());
  argument_outputs.push_back(softmax_lse_dense_tensor_type);

  pir::Type seed_offset_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_seed_offset.dtype()), dense_seed_offset.dims(), dense_seed_offset.layout(), dense_seed_offset.lod(), dense_seed_offset.offset());
  argument_outputs.push_back(seed_offset_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void FlashAttnOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value q_, pir::Value k_, pir::Value v_, pir::Value fixed_seed_offset_, pir::Value attn_mask_, pir::AttributeMap attributes) {
  VLOG(4) << "Start build FlashAttnOp";


  IR_ENFORCE(
      attributes.find("dropout") != attributes.end(),
          "'dropout' Attribute is expected for FlashAttnOp. ");
  float dropout = attributes.at("dropout").dyn_cast<pir::FloatAttribute>().data();

  IR_ENFORCE(
      attributes.find("causal") != attributes.end(),
          "'causal' Attribute is expected for FlashAttnOp. ");
  bool causal = attributes.at("causal").dyn_cast<pir::BoolAttribute>().data();

  IR_ENFORCE(
      attributes.find("return_softmax") != attributes.end(),
          "'return_softmax' Attribute is expected for FlashAttnOp. ");
  bool return_softmax = attributes.at("return_softmax").dyn_cast<pir::BoolAttribute>().data();

  IR_ENFORCE(
      attributes.find("is_test") != attributes.end(),
          "'is_test' Attribute is expected for FlashAttnOp. ");
  bool is_test = attributes.at("is_test").dyn_cast<pir::BoolAttribute>().data();

  IR_ENFORCE(
      attributes.find("rng_name") != attributes.end(),
          "'rng_name' Attribute is expected for FlashAttnOp. ");
  std::string rng_name = attributes.at("rng_name").dyn_cast<pir::StrAttribute>().AsString();


  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {q_, k_, v_, fixed_seed_offset_, attn_mask_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_dropout = pir::FloatAttribute::get(pir::IrContext::Instance(), dropout);
  argument.AddAttribute("dropout", attr_dropout);
  pir::Attribute attr_causal = pir::BoolAttribute::get(pir::IrContext::Instance(), causal);
  argument.AddAttribute("causal", attr_causal);
  pir::Attribute attr_return_softmax = pir::BoolAttribute::get(pir::IrContext::Instance(), return_softmax);
  argument.AddAttribute("return_softmax", attr_return_softmax);
  pir::Attribute attr_is_test = pir::BoolAttribute::get(pir::IrContext::Instance(), is_test);
  argument.AddAttribute("is_test", attr_is_test);
  pir::Attribute attr_rng_name = pir::StrAttribute::get(pir::IrContext::Instance(), rng_name);
  argument.AddAttribute("rng_name", attr_rng_name);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType q = q_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)q;
  paddle::dialect::DenseTensorType k = k_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)k;
  paddle::dialect::DenseTensorType v = v_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)v;

  VLOG(4) << "Builder construction  dense_q";
  paddle::dialect::IrTensor ir_tensor_q(paddle::dialect::TransToPhiDataType(q.dtype()),
                                                      q.dims(),
                                                      q.data_layout(),
                                                      q.lod(),
                                                      q.offset());
  VLOG(4) << "Builder construction  meta_q";
  paddle::dialect::IrMetaTensor meta_q(&ir_tensor_q);

  VLOG(4) << "Builder construction  dense_k";
  paddle::dialect::IrTensor ir_tensor_k(paddle::dialect::TransToPhiDataType(k.dtype()),
                                                      k.dims(),
                                                      k.data_layout(),
                                                      k.lod(),
                                                      k.offset());
  VLOG(4) << "Builder construction  meta_k";
  paddle::dialect::IrMetaTensor meta_k(&ir_tensor_k);

  VLOG(4) << "Builder construction  dense_v";
  paddle::dialect::IrTensor ir_tensor_v(paddle::dialect::TransToPhiDataType(v.dtype()),
                                                      v.dims(),
                                                      v.data_layout(),
                                                      v.lod(),
                                                      v.offset());
  VLOG(4) << "Builder construction  meta_v";
  paddle::dialect::IrMetaTensor meta_v(&ir_tensor_v);
  paddle::dialect::IrTensor dense_out;
  paddle::dialect::IrMetaTensor meta_out(&dense_out);
  paddle::dialect::IrTensor dense_softmax;
  paddle::dialect::IrMetaTensor meta_softmax(&dense_softmax);
  paddle::dialect::IrTensor dense_softmax_lse;
  paddle::dialect::IrMetaTensor meta_softmax_lse(&dense_softmax_lse);
  paddle::dialect::IrTensor dense_seed_offset;
  paddle::dialect::IrMetaTensor meta_seed_offset(&dense_seed_offset);

  phi::FlashAttnInferMeta(meta_q, meta_k, meta_v, &meta_out, &meta_softmax, &meta_softmax_lse, &meta_seed_offset);

  std::vector<pir::Type> argument_outputs;
  pir::Type out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_out.dtype()), dense_out.dims(), dense_out.layout(), dense_out.lod(), dense_out.offset());
  argument_outputs.push_back(out_dense_tensor_type);

  pir::Type softmax_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_softmax.dtype()), dense_softmax.dims(), dense_softmax.layout(), dense_softmax.lod(), dense_softmax.offset());
  argument_outputs.push_back(softmax_dense_tensor_type);

  pir::Type softmax_lse_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_softmax_lse.dtype()), dense_softmax_lse.dims(), dense_softmax_lse.layout(), dense_softmax_lse.lod(), dense_softmax_lse.offset());
  argument_outputs.push_back(softmax_lse_dense_tensor_type);

  pir::Type seed_offset_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_seed_offset.dtype()), dense_seed_offset.dims(), dense_seed_offset.layout(), dense_seed_offset.lod(), dense_seed_offset.offset());
  argument_outputs.push_back(seed_offset_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void FlashAttnOp::VerifySig() {
  VLOG(4) << "Start Verifying inputs, outputs and attributes for: FlashAttnOp.";
  VLOG(4) << "Verifying inputs:";
  {
  auto input_size = num_operands();
  IR_ENFORCE(input_size == 5u,
                    "The size %d of inputs must be equal to 5.", input_size);
  IR_ENFORCE((*this)->operand_source(0).type().isa<paddle::dialect::DenseTensorType>(),
                  "Type validation failed for the 0th input, got %s.", (*this)->operand_source(0).type());
  IR_ENFORCE((*this)->operand_source(1).type().isa<paddle::dialect::DenseTensorType>(),
                  "Type validation failed for the 1th input, got %s.", (*this)->operand_source(1).type());
  IR_ENFORCE((*this)->operand_source(2).type().isa<paddle::dialect::DenseTensorType>(),
                  "Type validation failed for the 2th input, got %s.", (*this)->operand_source(2).type());
  if (auto val = (*this)->operand(3)) {
    IR_ENFORCE(val.type().isa<paddle::dialect::DenseTensorType>(),
                   "Type validation failed for the 3th input, got %s.", (*this)->operand_source(3).type());
  }
  if (auto val = (*this)->operand(4)) {
    IR_ENFORCE(val.type().isa<paddle::dialect::DenseTensorType>(),
                   "Type validation failed for the 4th input, got %s.", (*this)->operand_source(4).type());
  }
  }
  VLOG(4) << "Verifying attributes:";
  {
  auto& attributes = this->attributes();
  IR_ENFORCE(attributes.count("dropout")>0,
                 "dropout does not exist.");
  IR_ENFORCE(attributes.at("dropout").isa<pir::FloatAttribute>(),
                 "Type of attribute: dropout is not pir::FloatAttribute.");

  IR_ENFORCE(attributes.count("causal")>0,
                 "causal does not exist.");
  IR_ENFORCE(attributes.at("causal").isa<pir::BoolAttribute>(),
                 "Type of attribute: causal is not pir::BoolAttribute.");

  IR_ENFORCE(attributes.count("return_softmax")>0,
                 "return_softmax does not exist.");
  IR_ENFORCE(attributes.at("return_softmax").isa<pir::BoolAttribute>(),
                 "Type of attribute: return_softmax is not pir::BoolAttribute.");

  IR_ENFORCE(attributes.count("is_test")>0,
                 "is_test does not exist.");
  IR_ENFORCE(attributes.at("is_test").isa<pir::BoolAttribute>(),
                 "Type of attribute: is_test is not pir::BoolAttribute.");

  IR_ENFORCE(attributes.count("rng_name")>0,
                 "rng_name does not exist.");
  IR_ENFORCE(attributes.at("rng_name").isa<pir::StrAttribute>(),
                 "Type of attribute: rng_name is not pir::StrAttribute.");

  }
  VLOG(4) << "Verifying outputs:";
  {
  auto output_size = num_results();
  IR_ENFORCE(output_size == 4u,
                    "The size %d of outputs must be equal to 4.", output_size);
  IR_ENFORCE((*this)->result(0).type().isa<paddle::dialect::DenseTensorType>(),
                 "Type validation failed for the 0th output.");
  IR_ENFORCE((*this)->result(1).type().isa<paddle::dialect::DenseTensorType>(),
                 "Type validation failed for the 1th output.");
  IR_ENFORCE((*this)->result(2).type().isa<paddle::dialect::DenseTensorType>(),
                 "Type validation failed for the 2th output.");
  IR_ENFORCE((*this)->result(3).type().isa<paddle::dialect::DenseTensorType>(),
                 "Type validation failed for the 3th output.");
  }
  VLOG(4) << "End Verifying for: FlashAttnOp.";
}

void FlashAttnOp::InferMeta( phi::InferMetaContext *infer_meta ) {
  auto fn = PD_INFER_META(phi::FlashAttnInferMeta);
  fn(infer_meta);
}

phi::DataType FlashAttnOp::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: FlashAttnOp";
  


  return expected_kernel_dtype;
}

const char *FlashAttnUnpaddedOp::attributes_name[8] = { "max_seqlen_q", "max_seqlen_k", "scale", "dropout", "causal", "return_softmax", "is_test", "rng_name" };

OpInfoTuple FlashAttnUnpaddedOp::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("q", "paddle::dialect::DenseTensorType", false, false, false, true), paddle::dialect::OpInputInfo("k", "paddle::dialect::DenseTensorType", false, false, false, true), paddle::dialect::OpInputInfo("v", "paddle::dialect::DenseTensorType", false, false, false, true), paddle::dialect::OpInputInfo("cu_seqlens_q", "paddle::dialect::DenseTensorType", false, false, false, false), paddle::dialect::OpInputInfo("cu_seqlens_k", "paddle::dialect::DenseTensorType", false, false, false, false), paddle::dialect::OpInputInfo("fixed_seed_offset", "paddle::dialect::DenseTensorType", true, false, false, false), paddle::dialect::OpInputInfo("attn_mask", "paddle::dialect::DenseTensorType", true, false, false, false) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = { paddle::dialect::OpAttributeInfo("max_seqlen_q", "pir::Int64Attribute", ""), paddle::dialect::OpAttributeInfo("max_seqlen_k", "pir::Int64Attribute", ""), paddle::dialect::OpAttributeInfo("scale", "pir::FloatAttribute", ""), paddle::dialect::OpAttributeInfo("dropout", "pir::FloatAttribute", ""), paddle::dialect::OpAttributeInfo("causal", "pir::BoolAttribute", ""), paddle::dialect::OpAttributeInfo("return_softmax", "pir::BoolAttribute", ""), paddle::dialect::OpAttributeInfo("is_test", "pir::BoolAttribute", ""), paddle::dialect::OpAttributeInfo("rng_name", "pir::StrAttribute", "") };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("out", "paddle::dialect::DenseTensorType", false, false), paddle::dialect::OpOutputInfo("softmax", "paddle::dialect::DenseTensorType", false, false), paddle::dialect::OpOutputInfo("softmax_lse", "paddle::dialect::DenseTensorType", true, true), paddle::dialect::OpOutputInfo("seed_offset", "paddle::dialect::DenseTensorType", true, true) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("FlashAttnInferMeta", {"q", "k", "v"}, "flash_attn_unpadded", {"q", "k", "v", "cu_seqlens_q", "cu_seqlens_k", "fixed_seed_offset", "attn_mask", "max_seqlen_q", "max_seqlen_k", "scale", "dropout", "causal", "return_softmax", "is_test", "rng_name"}, {"q"}, {}, {}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "flash_attn_unpadded");
}

void FlashAttnUnpaddedOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value q_, pir::Value k_, pir::Value v_, pir::Value cu_seqlens_q_, pir::Value cu_seqlens_k_, pir::Value fixed_seed_offset_, pir::Value attn_mask_, int64_t max_seqlen_q, int64_t max_seqlen_k, float scale, float dropout, bool causal, bool return_softmax, bool is_test, const std::string& rng_name) {
  VLOG(4) << "Start build FlashAttnUnpaddedOp";



  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {q_, k_, v_, cu_seqlens_q_, cu_seqlens_k_, fixed_seed_offset_, attn_mask_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_max_seqlen_q = pir::Int64Attribute::get(pir::IrContext::Instance(), max_seqlen_q);
  argument.AddAttribute("max_seqlen_q", attr_max_seqlen_q);
  pir::Attribute attr_max_seqlen_k = pir::Int64Attribute::get(pir::IrContext::Instance(), max_seqlen_k);
  argument.AddAttribute("max_seqlen_k", attr_max_seqlen_k);
  pir::Attribute attr_scale = pir::FloatAttribute::get(pir::IrContext::Instance(), scale);
  argument.AddAttribute("scale", attr_scale);
  pir::Attribute attr_dropout = pir::FloatAttribute::get(pir::IrContext::Instance(), dropout);
  argument.AddAttribute("dropout", attr_dropout);
  pir::Attribute attr_causal = pir::BoolAttribute::get(pir::IrContext::Instance(), causal);
  argument.AddAttribute("causal", attr_causal);
  pir::Attribute attr_return_softmax = pir::BoolAttribute::get(pir::IrContext::Instance(), return_softmax);
  argument.AddAttribute("return_softmax", attr_return_softmax);
  pir::Attribute attr_is_test = pir::BoolAttribute::get(pir::IrContext::Instance(), is_test);
  argument.AddAttribute("is_test", attr_is_test);
  pir::Attribute attr_rng_name = pir::StrAttribute::get(pir::IrContext::Instance(), rng_name);
  argument.AddAttribute("rng_name", attr_rng_name);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType q = q_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)q;
  paddle::dialect::DenseTensorType k = k_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)k;
  paddle::dialect::DenseTensorType v = v_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)v;
  paddle::dialect::DenseTensorType cu_seqlens_q = cu_seqlens_q_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)cu_seqlens_q;
  paddle::dialect::DenseTensorType cu_seqlens_k = cu_seqlens_k_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)cu_seqlens_k;

  VLOG(4) << "Builder construction  dense_q";
  paddle::dialect::IrTensor ir_tensor_q(paddle::dialect::TransToPhiDataType(q.dtype()),
                                                      q.dims(),
                                                      q.data_layout(),
                                                      q.lod(),
                                                      q.offset());
  VLOG(4) << "Builder construction  meta_q";
  paddle::dialect::IrMetaTensor meta_q(&ir_tensor_q);

  VLOG(4) << "Builder construction  dense_k";
  paddle::dialect::IrTensor ir_tensor_k(paddle::dialect::TransToPhiDataType(k.dtype()),
                                                      k.dims(),
                                                      k.data_layout(),
                                                      k.lod(),
                                                      k.offset());
  VLOG(4) << "Builder construction  meta_k";
  paddle::dialect::IrMetaTensor meta_k(&ir_tensor_k);

  VLOG(4) << "Builder construction  dense_v";
  paddle::dialect::IrTensor ir_tensor_v(paddle::dialect::TransToPhiDataType(v.dtype()),
                                                      v.dims(),
                                                      v.data_layout(),
                                                      v.lod(),
                                                      v.offset());
  VLOG(4) << "Builder construction  meta_v";
  paddle::dialect::IrMetaTensor meta_v(&ir_tensor_v);
  paddle::dialect::IrTensor dense_out;
  paddle::dialect::IrMetaTensor meta_out(&dense_out);
  paddle::dialect::IrTensor dense_softmax;
  paddle::dialect::IrMetaTensor meta_softmax(&dense_softmax);
  paddle::dialect::IrTensor dense_softmax_lse;
  paddle::dialect::IrMetaTensor meta_softmax_lse(&dense_softmax_lse);
  paddle::dialect::IrTensor dense_seed_offset;
  paddle::dialect::IrMetaTensor meta_seed_offset(&dense_seed_offset);

  phi::FlashAttnInferMeta(meta_q, meta_k, meta_v, &meta_out, &meta_softmax, &meta_softmax_lse, &meta_seed_offset);

  std::vector<pir::Type> argument_outputs;
  pir::Type out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_out.dtype()), dense_out.dims(), dense_out.layout(), dense_out.lod(), dense_out.offset());
  argument_outputs.push_back(out_dense_tensor_type);

  pir::Type softmax_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_softmax.dtype()), dense_softmax.dims(), dense_softmax.layout(), dense_softmax.lod(), dense_softmax.offset());
  argument_outputs.push_back(softmax_dense_tensor_type);

  pir::Type softmax_lse_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_softmax_lse.dtype()), dense_softmax_lse.dims(), dense_softmax_lse.layout(), dense_softmax_lse.lod(), dense_softmax_lse.offset());
  argument_outputs.push_back(softmax_lse_dense_tensor_type);

  pir::Type seed_offset_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_seed_offset.dtype()), dense_seed_offset.dims(), dense_seed_offset.layout(), dense_seed_offset.lod(), dense_seed_offset.offset());
  argument_outputs.push_back(seed_offset_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void FlashAttnUnpaddedOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value q_, pir::Value k_, pir::Value v_, pir::Value cu_seqlens_q_, pir::Value cu_seqlens_k_, pir::Value fixed_seed_offset_, pir::Value attn_mask_, pir::AttributeMap attributes) {
  VLOG(4) << "Start build FlashAttnUnpaddedOp";


  IR_ENFORCE(
      attributes.find("max_seqlen_q") != attributes.end(),
          "'max_seqlen_q' Attribute is expected for FlashAttnUnpaddedOp. ");
  int64_t max_seqlen_q = attributes.at("max_seqlen_q").dyn_cast<pir::Int64Attribute>().data();

  IR_ENFORCE(
      attributes.find("max_seqlen_k") != attributes.end(),
          "'max_seqlen_k' Attribute is expected for FlashAttnUnpaddedOp. ");
  int64_t max_seqlen_k = attributes.at("max_seqlen_k").dyn_cast<pir::Int64Attribute>().data();

  IR_ENFORCE(
      attributes.find("scale") != attributes.end(),
          "'scale' Attribute is expected for FlashAttnUnpaddedOp. ");
  float scale = attributes.at("scale").dyn_cast<pir::FloatAttribute>().data();

  IR_ENFORCE(
      attributes.find("dropout") != attributes.end(),
          "'dropout' Attribute is expected for FlashAttnUnpaddedOp. ");
  float dropout = attributes.at("dropout").dyn_cast<pir::FloatAttribute>().data();

  IR_ENFORCE(
      attributes.find("causal") != attributes.end(),
          "'causal' Attribute is expected for FlashAttnUnpaddedOp. ");
  bool causal = attributes.at("causal").dyn_cast<pir::BoolAttribute>().data();

  IR_ENFORCE(
      attributes.find("return_softmax") != attributes.end(),
          "'return_softmax' Attribute is expected for FlashAttnUnpaddedOp. ");
  bool return_softmax = attributes.at("return_softmax").dyn_cast<pir::BoolAttribute>().data();

  IR_ENFORCE(
      attributes.find("is_test") != attributes.end(),
          "'is_test' Attribute is expected for FlashAttnUnpaddedOp. ");
  bool is_test = attributes.at("is_test").dyn_cast<pir::BoolAttribute>().data();

  IR_ENFORCE(
      attributes.find("rng_name") != attributes.end(),
          "'rng_name' Attribute is expected for FlashAttnUnpaddedOp. ");
  std::string rng_name = attributes.at("rng_name").dyn_cast<pir::StrAttribute>().AsString();


  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {q_, k_, v_, cu_seqlens_q_, cu_seqlens_k_, fixed_seed_offset_, attn_mask_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_max_seqlen_q = pir::Int64Attribute::get(pir::IrContext::Instance(), max_seqlen_q);
  argument.AddAttribute("max_seqlen_q", attr_max_seqlen_q);
  pir::Attribute attr_max_seqlen_k = pir::Int64Attribute::get(pir::IrContext::Instance(), max_seqlen_k);
  argument.AddAttribute("max_seqlen_k", attr_max_seqlen_k);
  pir::Attribute attr_scale = pir::FloatAttribute::get(pir::IrContext::Instance(), scale);
  argument.AddAttribute("scale", attr_scale);
  pir::Attribute attr_dropout = pir::FloatAttribute::get(pir::IrContext::Instance(), dropout);
  argument.AddAttribute("dropout", attr_dropout);
  pir::Attribute attr_causal = pir::BoolAttribute::get(pir::IrContext::Instance(), causal);
  argument.AddAttribute("causal", attr_causal);
  pir::Attribute attr_return_softmax = pir::BoolAttribute::get(pir::IrContext::Instance(), return_softmax);
  argument.AddAttribute("return_softmax", attr_return_softmax);
  pir::Attribute attr_is_test = pir::BoolAttribute::get(pir::IrContext::Instance(), is_test);
  argument.AddAttribute("is_test", attr_is_test);
  pir::Attribute attr_rng_name = pir::StrAttribute::get(pir::IrContext::Instance(), rng_name);
  argument.AddAttribute("rng_name", attr_rng_name);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType q = q_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)q;
  paddle::dialect::DenseTensorType k = k_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)k;
  paddle::dialect::DenseTensorType v = v_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)v;
  paddle::dialect::DenseTensorType cu_seqlens_q = cu_seqlens_q_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)cu_seqlens_q;
  paddle::dialect::DenseTensorType cu_seqlens_k = cu_seqlens_k_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)cu_seqlens_k;

  VLOG(4) << "Builder construction  dense_q";
  paddle::dialect::IrTensor ir_tensor_q(paddle::dialect::TransToPhiDataType(q.dtype()),
                                                      q.dims(),
                                                      q.data_layout(),
                                                      q.lod(),
                                                      q.offset());
  VLOG(4) << "Builder construction  meta_q";
  paddle::dialect::IrMetaTensor meta_q(&ir_tensor_q);

  VLOG(4) << "Builder construction  dense_k";
  paddle::dialect::IrTensor ir_tensor_k(paddle::dialect::TransToPhiDataType(k.dtype()),
                                                      k.dims(),
                                                      k.data_layout(),
                                                      k.lod(),
                                                      k.offset());
  VLOG(4) << "Builder construction  meta_k";
  paddle::dialect::IrMetaTensor meta_k(&ir_tensor_k);

  VLOG(4) << "Builder construction  dense_v";
  paddle::dialect::IrTensor ir_tensor_v(paddle::dialect::TransToPhiDataType(v.dtype()),
                                                      v.dims(),
                                                      v.data_layout(),
                                                      v.lod(),
                                                      v.offset());
  VLOG(4) << "Builder construction  meta_v";
  paddle::dialect::IrMetaTensor meta_v(&ir_tensor_v);
  paddle::dialect::IrTensor dense_out;
  paddle::dialect::IrMetaTensor meta_out(&dense_out);
  paddle::dialect::IrTensor dense_softmax;
  paddle::dialect::IrMetaTensor meta_softmax(&dense_softmax);
  paddle::dialect::IrTensor dense_softmax_lse;
  paddle::dialect::IrMetaTensor meta_softmax_lse(&dense_softmax_lse);
  paddle::dialect::IrTensor dense_seed_offset;
  paddle::dialect::IrMetaTensor meta_seed_offset(&dense_seed_offset);

  phi::FlashAttnInferMeta(meta_q, meta_k, meta_v, &meta_out, &meta_softmax, &meta_softmax_lse, &meta_seed_offset);

  std::vector<pir::Type> argument_outputs;
  pir::Type out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_out.dtype()), dense_out.dims(), dense_out.layout(), dense_out.lod(), dense_out.offset());
  argument_outputs.push_back(out_dense_tensor_type);

  pir::Type softmax_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_softmax.dtype()), dense_softmax.dims(), dense_softmax.layout(), dense_softmax.lod(), dense_softmax.offset());
  argument_outputs.push_back(softmax_dense_tensor_type);

  pir::Type softmax_lse_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_softmax_lse.dtype()), dense_softmax_lse.dims(), dense_softmax_lse.layout(), dense_softmax_lse.lod(), dense_softmax_lse.offset());
  argument_outputs.push_back(softmax_lse_dense_tensor_type);

  pir::Type seed_offset_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_seed_offset.dtype()), dense_seed_offset.dims(), dense_seed_offset.layout(), dense_seed_offset.lod(), dense_seed_offset.offset());
  argument_outputs.push_back(seed_offset_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void FlashAttnUnpaddedOp::VerifySig() {
  VLOG(4) << "Start Verifying inputs, outputs and attributes for: FlashAttnUnpaddedOp.";
  VLOG(4) << "Verifying inputs:";
  {
  auto input_size = num_operands();
  IR_ENFORCE(input_size == 7u,
                    "The size %d of inputs must be equal to 7.", input_size);
  IR_ENFORCE((*this)->operand_source(0).type().isa<paddle::dialect::DenseTensorType>(),
                  "Type validation failed for the 0th input, got %s.", (*this)->operand_source(0).type());
  IR_ENFORCE((*this)->operand_source(1).type().isa<paddle::dialect::DenseTensorType>(),
                  "Type validation failed for the 1th input, got %s.", (*this)->operand_source(1).type());
  IR_ENFORCE((*this)->operand_source(2).type().isa<paddle::dialect::DenseTensorType>(),
                  "Type validation failed for the 2th input, got %s.", (*this)->operand_source(2).type());
  IR_ENFORCE((*this)->operand_source(3).type().isa<paddle::dialect::DenseTensorType>(),
                  "Type validation failed for the 3th input, got %s.", (*this)->operand_source(3).type());
  IR_ENFORCE((*this)->operand_source(4).type().isa<paddle::dialect::DenseTensorType>(),
                  "Type validation failed for the 4th input, got %s.", (*this)->operand_source(4).type());
  if (auto val = (*this)->operand(5)) {
    IR_ENFORCE(val.type().isa<paddle::dialect::DenseTensorType>(),
                   "Type validation failed for the 5th input, got %s.", (*this)->operand_source(5).type());
  }
  if (auto val = (*this)->operand(6)) {
    IR_ENFORCE(val.type().isa<paddle::dialect::DenseTensorType>(),
                   "Type validation failed for the 6th input, got %s.", (*this)->operand_source(6).type());
  }
  }
  VLOG(4) << "Verifying attributes:";
  {
  auto& attributes = this->attributes();
  IR_ENFORCE(attributes.count("max_seqlen_q")>0,
                 "max_seqlen_q does not exist.");
  IR_ENFORCE(attributes.at("max_seqlen_q").isa<pir::Int64Attribute>(),
                 "Type of attribute: max_seqlen_q is not pir::Int64Attribute.");

  IR_ENFORCE(attributes.count("max_seqlen_k")>0,
                 "max_seqlen_k does not exist.");
  IR_ENFORCE(attributes.at("max_seqlen_k").isa<pir::Int64Attribute>(),
                 "Type of attribute: max_seqlen_k is not pir::Int64Attribute.");

  IR_ENFORCE(attributes.count("scale")>0,
                 "scale does not exist.");
  IR_ENFORCE(attributes.at("scale").isa<pir::FloatAttribute>(),
                 "Type of attribute: scale is not pir::FloatAttribute.");

  IR_ENFORCE(attributes.count("dropout")>0,
                 "dropout does not exist.");
  IR_ENFORCE(attributes.at("dropout").isa<pir::FloatAttribute>(),
                 "Type of attribute: dropout is not pir::FloatAttribute.");

  IR_ENFORCE(attributes.count("causal")>0,
                 "causal does not exist.");
  IR_ENFORCE(attributes.at("causal").isa<pir::BoolAttribute>(),
                 "Type of attribute: causal is not pir::BoolAttribute.");

  IR_ENFORCE(attributes.count("return_softmax")>0,
                 "return_softmax does not exist.");
  IR_ENFORCE(attributes.at("return_softmax").isa<pir::BoolAttribute>(),
                 "Type of attribute: return_softmax is not pir::BoolAttribute.");

  IR_ENFORCE(attributes.count("is_test")>0,
                 "is_test does not exist.");
  IR_ENFORCE(attributes.at("is_test").isa<pir::BoolAttribute>(),
                 "Type of attribute: is_test is not pir::BoolAttribute.");

  IR_ENFORCE(attributes.count("rng_name")>0,
                 "rng_name does not exist.");
  IR_ENFORCE(attributes.at("rng_name").isa<pir::StrAttribute>(),
                 "Type of attribute: rng_name is not pir::StrAttribute.");

  }
  VLOG(4) << "Verifying outputs:";
  {
  auto output_size = num_results();
  IR_ENFORCE(output_size == 4u,
                    "The size %d of outputs must be equal to 4.", output_size);
  IR_ENFORCE((*this)->result(0).type().isa<paddle::dialect::DenseTensorType>(),
                 "Type validation failed for the 0th output.");
  IR_ENFORCE((*this)->result(1).type().isa<paddle::dialect::DenseTensorType>(),
                 "Type validation failed for the 1th output.");
  if (auto output_2_type = (*this)->result(2).type()) {
    IR_ENFORCE(output_2_type.isa<paddle::dialect::DenseTensorType>(),
                   "Type validation failed for the 2th output.");
  }
  if (auto output_3_type = (*this)->result(3).type()) {
    IR_ENFORCE(output_3_type.isa<paddle::dialect::DenseTensorType>(),
                   "Type validation failed for the 3th output.");
  }
  }
  VLOG(4) << "End Verifying for: FlashAttnUnpaddedOp.";
}

void FlashAttnUnpaddedOp::InferMeta( phi::InferMetaContext *infer_meta ) {
  auto fn = PD_INFER_META(phi::FlashAttnInferMeta);
  fn(infer_meta);
}

phi::DataType FlashAttnUnpaddedOp::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: FlashAttnUnpaddedOp";
  


  return expected_kernel_dtype;
}

const char *FlattenOp::attributes_name[2] = { "start_axis", "stop_axis" };

OpInfoTuple FlattenOp::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("x", "paddle::dialect::DenseTensorType", false, false, false, true) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = { paddle::dialect::OpAttributeInfo("start_axis", "pir::Int32Attribute", ""), paddle::dialect::OpAttributeInfo("stop_axis", "pir::Int32Attribute", "") };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("out", "paddle::dialect::DenseTensorType", false, false), paddle::dialect::OpOutputInfo("xshape", "paddle::dialect::DenseTensorType", true, true) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("FlattenWithXShapeInferMeta", {"x", "start_axis", "stop_axis"}, "flatten", {"x", "start_axis", "stop_axis"}, {"x"}, {}, {}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "flatten");
}

void FlattenOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, int start_axis, int stop_axis) {
  VLOG(4) << "Start build FlattenOp";



  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_start_axis = pir::Int32Attribute::get(pir::IrContext::Instance(), start_axis);
  argument.AddAttribute("start_axis", attr_start_axis);
  pir::Attribute attr_stop_axis = pir::Int32Attribute::get(pir::IrContext::Instance(), stop_axis);
  argument.AddAttribute("stop_axis", attr_stop_axis);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);
  paddle::dialect::IrTensor dense_out;
  paddle::dialect::IrMetaTensor meta_out(&dense_out);
  paddle::dialect::IrTensor dense_xshape;
  paddle::dialect::IrMetaTensor meta_xshape(&dense_xshape);

  phi::FlattenWithXShapeInferMeta(meta_x, start_axis, stop_axis, &meta_out, &meta_xshape);

  std::vector<pir::Type> argument_outputs;
  pir::Type out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_out.dtype()), dense_out.dims(), dense_out.layout(), dense_out.lod(), dense_out.offset());
  argument_outputs.push_back(out_dense_tensor_type);

  pir::Type xshape_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_xshape.dtype()), dense_xshape.dims(), dense_xshape.layout(), dense_xshape.lod(), dense_xshape.offset());
  argument_outputs.push_back(xshape_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void FlattenOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, pir::AttributeMap attributes) {
  VLOG(4) << "Start build FlattenOp";


  IR_ENFORCE(
      attributes.find("start_axis") != attributes.end(),
          "'start_axis' Attribute is expected for FlattenOp. ");
  int start_axis = attributes.at("start_axis").dyn_cast<pir::Int32Attribute>().data();

  IR_ENFORCE(
      attributes.find("stop_axis") != attributes.end(),
          "'stop_axis' Attribute is expected for FlattenOp. ");
  int stop_axis = attributes.at("stop_axis").dyn_cast<pir::Int32Attribute>().data();


  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_start_axis = pir::Int32Attribute::get(pir::IrContext::Instance(), start_axis);
  argument.AddAttribute("start_axis", attr_start_axis);
  pir::Attribute attr_stop_axis = pir::Int32Attribute::get(pir::IrContext::Instance(), stop_axis);
  argument.AddAttribute("stop_axis", attr_stop_axis);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);
  paddle::dialect::IrTensor dense_out;
  paddle::dialect::IrMetaTensor meta_out(&dense_out);
  paddle::dialect::IrTensor dense_xshape;
  paddle::dialect::IrMetaTensor meta_xshape(&dense_xshape);

  phi::FlattenWithXShapeInferMeta(meta_x, start_axis, stop_axis, &meta_out, &meta_xshape);

  std::vector<pir::Type> argument_outputs;
  pir::Type out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_out.dtype()), dense_out.dims(), dense_out.layout(), dense_out.lod(), dense_out.offset());
  argument_outputs.push_back(out_dense_tensor_type);

  pir::Type xshape_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_xshape.dtype()), dense_xshape.dims(), dense_xshape.layout(), dense_xshape.lod(), dense_xshape.offset());
  argument_outputs.push_back(xshape_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void FlattenOp::VerifySig() {
  VLOG(4) << "Start Verifying inputs, outputs and attributes for: FlattenOp.";
  VLOG(4) << "Verifying inputs:";
  {
  auto input_size = num_operands();
  IR_ENFORCE(input_size == 1u,
                    "The size %d of inputs must be equal to 1.", input_size);
  IR_ENFORCE((*this)->operand_source(0).type().isa<paddle::dialect::DenseTensorType>(),
                  "Type validation failed for the 0th input, got %s.", (*this)->operand_source(0).type());
  }
  VLOG(4) << "Verifying attributes:";
  {
  auto& attributes = this->attributes();
  IR_ENFORCE(attributes.count("start_axis")>0,
                 "start_axis does not exist.");
  IR_ENFORCE(attributes.at("start_axis").isa<pir::Int32Attribute>(),
                 "Type of attribute: start_axis is not pir::Int32Attribute.");

  IR_ENFORCE(attributes.count("stop_axis")>0,
                 "stop_axis does not exist.");
  IR_ENFORCE(attributes.at("stop_axis").isa<pir::Int32Attribute>(),
                 "Type of attribute: stop_axis is not pir::Int32Attribute.");

  }
  VLOG(4) << "Verifying outputs:";
  {
  auto output_size = num_results();
  IR_ENFORCE(output_size == 2u,
                    "The size %d of outputs must be equal to 2.", output_size);
  IR_ENFORCE((*this)->result(0).type().isa<paddle::dialect::DenseTensorType>(),
                 "Type validation failed for the 0th output.");
  if (auto output_1_type = (*this)->result(1).type()) {
    IR_ENFORCE(output_1_type.isa<paddle::dialect::DenseTensorType>(),
                   "Type validation failed for the 1th output.");
  }
  }
  VLOG(4) << "End Verifying for: FlattenOp.";
}

void FlattenOp::InferMeta( phi::InferMetaContext *infer_meta ) {
  auto fn = PD_INFER_META(phi::FlattenWithXShapeInferMeta);
  fn(infer_meta);
}

phi::DataType FlattenOp::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: FlattenOp";
  


  return expected_kernel_dtype;
}

const char *Flatten_Op::attributes_name[2] = { "start_axis", "stop_axis" };

OpInfoTuple Flatten_Op::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("x", "paddle::dialect::DenseTensorType", false, false, false, true) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = { paddle::dialect::OpAttributeInfo("start_axis", "pir::Int32Attribute", ""), paddle::dialect::OpAttributeInfo("stop_axis", "pir::Int32Attribute", "") };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("out", "paddle::dialect::DenseTensorType", false, false), paddle::dialect::OpOutputInfo("xshape", "paddle::dialect::DenseTensorType", true, true) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("FlattenWithXShapeInferMeta", {"x", "start_axis", "stop_axis"}, "flatten", {"x", "start_axis", "stop_axis"}, {"x"}, {}, {{"out", "x"}}, {{"out", "x"}});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "flatten");
}

void Flatten_Op::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, int start_axis, int stop_axis) {
  VLOG(4) << "Start build Flatten_Op";



  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_start_axis = pir::Int32Attribute::get(pir::IrContext::Instance(), start_axis);
  argument.AddAttribute("start_axis", attr_start_axis);
  pir::Attribute attr_stop_axis = pir::Int32Attribute::get(pir::IrContext::Instance(), stop_axis);
  argument.AddAttribute("stop_axis", attr_stop_axis);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);
  paddle::dialect::IrTensor dense_out;
  paddle::dialect::IrMetaTensor meta_out(&dense_out);
  paddle::dialect::IrTensor dense_xshape;
  paddle::dialect::IrMetaTensor meta_xshape(&dense_xshape);

  phi::FlattenWithXShapeInferMeta(meta_x, start_axis, stop_axis, &meta_out, &meta_xshape);

  std::vector<pir::Type> argument_outputs;
  pir::Type out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_out.dtype()), dense_out.dims(), dense_out.layout(), dense_out.lod(), dense_out.offset());
  argument_outputs.push_back(out_dense_tensor_type);

  pir::Type xshape_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_xshape.dtype()), dense_xshape.dims(), dense_xshape.layout(), dense_xshape.lod(), dense_xshape.offset());
  argument_outputs.push_back(xshape_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void Flatten_Op::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, pir::AttributeMap attributes) {
  VLOG(4) << "Start build Flatten_Op";


  IR_ENFORCE(
      attributes.find("start_axis") != attributes.end(),
          "'start_axis' Attribute is expected for Flatten_Op. ");
  int start_axis = attributes.at("start_axis").dyn_cast<pir::Int32Attribute>().data();

  IR_ENFORCE(
      attributes.find("stop_axis") != attributes.end(),
          "'stop_axis' Attribute is expected for Flatten_Op. ");
  int stop_axis = attributes.at("stop_axis").dyn_cast<pir::Int32Attribute>().data();


  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_start_axis = pir::Int32Attribute::get(pir::IrContext::Instance(), start_axis);
  argument.AddAttribute("start_axis", attr_start_axis);
  pir::Attribute attr_stop_axis = pir::Int32Attribute::get(pir::IrContext::Instance(), stop_axis);
  argument.AddAttribute("stop_axis", attr_stop_axis);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);
  paddle::dialect::IrTensor dense_out;
  paddle::dialect::IrMetaTensor meta_out(&dense_out);
  paddle::dialect::IrTensor dense_xshape;
  paddle::dialect::IrMetaTensor meta_xshape(&dense_xshape);

  phi::FlattenWithXShapeInferMeta(meta_x, start_axis, stop_axis, &meta_out, &meta_xshape);

  std::vector<pir::Type> argument_outputs;
  pir::Type out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_out.dtype()), dense_out.dims(), dense_out.layout(), dense_out.lod(), dense_out.offset());
  argument_outputs.push_back(out_dense_tensor_type);

  pir::Type xshape_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_xshape.dtype()), dense_xshape.dims(), dense_xshape.layout(), dense_xshape.lod(), dense_xshape.offset());
  argument_outputs.push_back(xshape_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void Flatten_Op::VerifySig() {
  VLOG(4) << "Start Verifying inputs, outputs and attributes for: Flatten_Op.";
  VLOG(4) << "Verifying inputs:";
  {
  auto input_size = num_operands();
  IR_ENFORCE(input_size == 1u,
                    "The size %d of inputs must be equal to 1.", input_size);
  IR_ENFORCE((*this)->operand_source(0).type().isa<paddle::dialect::DenseTensorType>(),
                  "Type validation failed for the 0th input, got %s.", (*this)->operand_source(0).type());
  }
  VLOG(4) << "Verifying attributes:";
  {
  auto& attributes = this->attributes();
  IR_ENFORCE(attributes.count("start_axis")>0,
                 "start_axis does not exist.");
  IR_ENFORCE(attributes.at("start_axis").isa<pir::Int32Attribute>(),
                 "Type of attribute: start_axis is not pir::Int32Attribute.");

  IR_ENFORCE(attributes.count("stop_axis")>0,
                 "stop_axis does not exist.");
  IR_ENFORCE(attributes.at("stop_axis").isa<pir::Int32Attribute>(),
                 "Type of attribute: stop_axis is not pir::Int32Attribute.");

  }
  VLOG(4) << "Verifying outputs:";
  {
  auto output_size = num_results();
  IR_ENFORCE(output_size == 2u,
                    "The size %d of outputs must be equal to 2.", output_size);
  IR_ENFORCE((*this)->result(0).type().isa<paddle::dialect::DenseTensorType>(),
                 "Type validation failed for the 0th output.");
  if (auto output_1_type = (*this)->result(1).type()) {
    IR_ENFORCE(output_1_type.isa<paddle::dialect::DenseTensorType>(),
                   "Type validation failed for the 1th output.");
  }
  }
  VLOG(4) << "End Verifying for: Flatten_Op.";
}

void Flatten_Op::InferMeta( phi::InferMetaContext *infer_meta ) {
  auto fn = PD_INFER_META(phi::FlattenWithXShapeInferMeta);
  fn(infer_meta);
}

phi::DataType Flatten_Op::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: Flatten_Op";
  


  return expected_kernel_dtype;
}

const char *FlipOp::attributes_name[1] = { "axis" };

OpInfoTuple FlipOp::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("x", "paddle::dialect::DenseTensorType", false, false, false, true) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = { paddle::dialect::OpAttributeInfo("axis", "pir::ArrayAttribute<pir::Int32Attribute>", "") };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("out", "paddle::dialect::DenseTensorType", false, false) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("FlipInferMeta", {"x", "axis"}, "flip", {"x", "axis"}, {}, {}, {}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "flip");
}

void FlipOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, const std::vector<int>& axis) {
  VLOG(4) << "Start build FlipOp";



  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  std::vector<pir::Attribute> vec_axis;
  for (size_t i = 0; i < static_cast<size_t>(axis.size()); i++) {
      pir::Attribute attr_axis = pir::Int32Attribute::get(pir::IrContext::Instance(), axis[i]);

    vec_axis.push_back(attr_axis);
  }
  pir::Attribute attr_axis = pir::ArrayAttribute::get(pir::IrContext::Instance(), vec_axis);
  argument.AddAttribute("axis", attr_axis);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);
  paddle::dialect::IrTensor dense_out;
  paddle::dialect::IrMetaTensor meta_out(&dense_out);

  phi::FlipInferMeta(meta_x, axis, &meta_out);

  std::vector<pir::Type> argument_outputs;
  pir::Type out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_out.dtype()), dense_out.dims(), dense_out.layout(), dense_out.lod(), dense_out.offset());
  argument_outputs.push_back(out_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void FlipOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, pir::AttributeMap attributes) {
  VLOG(4) << "Start build FlipOp";


  IR_ENFORCE(
      attributes.find("axis") != attributes.end(),
          "'axis' Attribute is expected for FlipOp. ");
  std::vector<int> axis;
  for (size_t i = 0; i < attributes.at("axis").dyn_cast<pir::ArrayAttribute>().size(); i++) {
    axis.push_back(attributes.at("axis").dyn_cast<pir::ArrayAttribute>().at(i).dyn_cast<pir::Int32Attribute>().data());
  }


  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  std::vector<pir::Attribute> vec_axis;
  for (size_t i = 0; i < static_cast<size_t>(axis.size()); i++) {
      pir::Attribute attr_axis = pir::Int32Attribute::get(pir::IrContext::Instance(), axis[i]);

    vec_axis.push_back(attr_axis);
  }
  pir::Attribute attr_axis = pir::ArrayAttribute::get(pir::IrContext::Instance(), vec_axis);
  argument.AddAttribute("axis", attr_axis);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);
  paddle::dialect::IrTensor dense_out;
  paddle::dialect::IrMetaTensor meta_out(&dense_out);

  phi::FlipInferMeta(meta_x, axis, &meta_out);

  std::vector<pir::Type> argument_outputs;
  pir::Type out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_out.dtype()), dense_out.dims(), dense_out.layout(), dense_out.lod(), dense_out.offset());
  argument_outputs.push_back(out_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void FlipOp::VerifySig() {
  VLOG(4) << "Start Verifying inputs, outputs and attributes for: FlipOp.";
  VLOG(4) << "Verifying inputs:";
  {
  auto input_size = num_operands();
  IR_ENFORCE(input_size == 1u,
                    "The size %d of inputs must be equal to 1.", input_size);
  IR_ENFORCE((*this)->operand_source(0).type().isa<paddle::dialect::DenseTensorType>(),
                  "Type validation failed for the 0th input, got %s.", (*this)->operand_source(0).type());
  }
  VLOG(4) << "Verifying attributes:";
  {
  auto& attributes = this->attributes();
  IR_ENFORCE(attributes.count("axis")>0,
                 "axis does not exist.");
  IR_ENFORCE(attributes.at("axis").isa<pir::ArrayAttribute>(),
                 "Type of attribute: axis is not pir::ArrayAttribute.");
  for (size_t i = 0; i < attributes.at("axis").dyn_cast<pir::ArrayAttribute>().size(); i++) {
    IR_ENFORCE(attributes.at("axis").dyn_cast<pir::ArrayAttribute>().at(i).isa<pir::Int32Attribute>(),
                   "Type of attribute: axis is not right.");
  }
  }
  VLOG(4) << "Verifying outputs:";
  {
  auto output_size = num_results();
  IR_ENFORCE(output_size == 1u,
                    "The size %d of outputs must be equal to 1.", output_size);
  IR_ENFORCE((*this)->result(0).type().isa<paddle::dialect::DenseTensorType>(),
                 "Type validation failed for the 0th output.");
  }
  VLOG(4) << "End Verifying for: FlipOp.";
}

void FlipOp::InferMeta( phi::InferMetaContext *infer_meta ) {
  auto fn = PD_INFER_META(phi::FlipInferMeta);
  fn(infer_meta);
}

phi::DataType FlipOp::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: FlipOp";
  


  return expected_kernel_dtype;
}

OpInfoTuple FloorOp::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("x", "paddle::dialect::DenseTensorType", false, false, false, true) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = {  };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("out", "paddle::dialect::DenseTensorType", false, false) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("UnchangedInferMeta", {"x"}, "floor", {"x"}, {}, {}, {}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "floor");
}

void FloorOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_) {
  VLOG(4) << "Start build FloorOp";



  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);
  paddle::dialect::IrTensor dense_out;
  paddle::dialect::IrMetaTensor meta_out(&dense_out);

  phi::UnchangedInferMeta(meta_x, &meta_out);

  std::vector<pir::Type> argument_outputs;
  pir::Type out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_out.dtype()), dense_out.dims(), dense_out.layout(), dense_out.lod(), dense_out.offset());
  argument_outputs.push_back(out_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void FloorOp::VerifySig() {
  VLOG(4) << "Start Verifying inputs, outputs and attributes for: FloorOp.";
  VLOG(4) << "Verifying inputs:";
  {
  auto input_size = num_operands();
  IR_ENFORCE(input_size == 1u,
                    "The size %d of inputs must be equal to 1.", input_size);
  IR_ENFORCE((*this)->operand_source(0).type().isa<paddle::dialect::DenseTensorType>(),
                  "Type validation failed for the 0th input, got %s.", (*this)->operand_source(0).type());
  }
  VLOG(4) << "Verifying attributes:";
  {
  // Attributes num is 0, not need to check attributes type.
  }
  VLOG(4) << "Verifying outputs:";
  {
  auto output_size = num_results();
  IR_ENFORCE(output_size == 1u,
                    "The size %d of outputs must be equal to 1.", output_size);
  IR_ENFORCE((*this)->result(0).type().isa<paddle::dialect::DenseTensorType>(),
                 "Type validation failed for the 0th output.");
  }
  VLOG(4) << "End Verifying for: FloorOp.";
}

void FloorOp::InferMeta( phi::InferMetaContext *infer_meta ) {
  auto fn = PD_INFER_META(phi::UnchangedInferMeta);
  fn(infer_meta);
}

phi::DataType FloorOp::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: FloorOp";
  


  return expected_kernel_dtype;
}

OpInfoTuple Floor_Op::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("x", "paddle::dialect::DenseTensorType", false, false, false, true) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = {  };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("out", "paddle::dialect::DenseTensorType", false, false) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("UnchangedInferMeta", {"x"}, "floor", {"x"}, {}, {}, {{"out", "x"}}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "floor");
}

void Floor_Op::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_) {
  VLOG(4) << "Start build Floor_Op";



  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);
  paddle::dialect::IrTensor dense_out;
  paddle::dialect::IrMetaTensor meta_out(&dense_out);

  phi::UnchangedInferMeta(meta_x, &meta_out);

  std::vector<pir::Type> argument_outputs;
  pir::Type out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_out.dtype()), dense_out.dims(), dense_out.layout(), dense_out.lod(), dense_out.offset());
  argument_outputs.push_back(out_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void Floor_Op::VerifySig() {
  VLOG(4) << "Start Verifying inputs, outputs and attributes for: Floor_Op.";
  VLOG(4) << "Verifying inputs:";
  {
  auto input_size = num_operands();
  IR_ENFORCE(input_size == 1u,
                    "The size %d of inputs must be equal to 1.", input_size);
  IR_ENFORCE((*this)->operand_source(0).type().isa<paddle::dialect::DenseTensorType>(),
                  "Type validation failed for the 0th input, got %s.", (*this)->operand_source(0).type());
  }
  VLOG(4) << "Verifying attributes:";
  {
  // Attributes num is 0, not need to check attributes type.
  }
  VLOG(4) << "Verifying outputs:";
  {
  auto output_size = num_results();
  IR_ENFORCE(output_size == 1u,
                    "The size %d of outputs must be equal to 1.", output_size);
  IR_ENFORCE((*this)->result(0).type().isa<paddle::dialect::DenseTensorType>(),
                 "Type validation failed for the 0th output.");
  }
  VLOG(4) << "End Verifying for: Floor_Op.";
}

void Floor_Op::InferMeta( phi::InferMetaContext *infer_meta ) {
  auto fn = PD_INFER_META(phi::UnchangedInferMeta);
  fn(infer_meta);
}

phi::DataType Floor_Op::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: Floor_Op";
  


  return expected_kernel_dtype;
}

OpInfoTuple FmaxOp::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("x", "paddle::dialect::DenseTensorType", false, false, false, true), paddle::dialect::OpInputInfo("y", "paddle::dialect::DenseTensorType", false, false, false, true) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = {  };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("out", "paddle::dialect::DenseTensorType", false, false) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("ElementwiseInferMeta", {"x", "y"}, "fmax", {"x", "y"}, {}, {}, {}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "fmax");
}

void FmaxOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, pir::Value y_) {
  VLOG(4) << "Start build FmaxOp";



  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_, y_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;
  paddle::dialect::DenseTensorType y = y_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)y;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);

  VLOG(4) << "Builder construction  dense_y";
  paddle::dialect::IrTensor ir_tensor_y(paddle::dialect::TransToPhiDataType(y.dtype()),
                                                      y.dims(),
                                                      y.data_layout(),
                                                      y.lod(),
                                                      y.offset());
  VLOG(4) << "Builder construction  meta_y";
  paddle::dialect::IrMetaTensor meta_y(&ir_tensor_y);
  paddle::dialect::IrTensor dense_out;
  paddle::dialect::IrMetaTensor meta_out(&dense_out);

  phi::ElementwiseInferMeta(meta_x, meta_y, &meta_out);

  std::vector<pir::Type> argument_outputs;
  pir::Type out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_out.dtype()), dense_out.dims(), dense_out.layout(), dense_out.lod(), dense_out.offset());
  argument_outputs.push_back(out_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void FmaxOp::VerifySig() {
  VLOG(4) << "Start Verifying inputs, outputs and attributes for: FmaxOp.";
  VLOG(4) << "Verifying inputs:";
  {
  auto input_size = num_operands();
  IR_ENFORCE(input_size == 2u,
                    "The size %d of inputs must be equal to 2.", input_size);
  IR_ENFORCE((*this)->operand_source(0).type().isa<paddle::dialect::DenseTensorType>(),
                  "Type validation failed for the 0th input, got %s.", (*this)->operand_source(0).type());
  IR_ENFORCE((*this)->operand_source(1).type().isa<paddle::dialect::DenseTensorType>(),
                  "Type validation failed for the 1th input, got %s.", (*this)->operand_source(1).type());
  }
  VLOG(4) << "Verifying attributes:";
  {
  // Attributes num is 0, not need to check attributes type.
  }
  VLOG(4) << "Verifying outputs:";
  {
  auto output_size = num_results();
  IR_ENFORCE(output_size == 1u,
                    "The size %d of outputs must be equal to 1.", output_size);
  IR_ENFORCE((*this)->result(0).type().isa<paddle::dialect::DenseTensorType>(),
                 "Type validation failed for the 0th output.");
  }
  VLOG(4) << "End Verifying for: FmaxOp.";
}

void FmaxOp::InferMeta( phi::InferMetaContext *infer_meta ) {
  auto fn = PD_INFER_META(phi::ElementwiseInferMeta);
  fn(infer_meta);
}

phi::DataType FmaxOp::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: FmaxOp";
  


  // deal complex_promote
  if (framework::IsComplexType(expected_kernel_dtype)) {{
    // only promote inputs’s types when contains complex input
    return tensor_dtype;
  }}

  return expected_kernel_dtype;
}

OpInfoTuple FminOp::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("x", "paddle::dialect::DenseTensorType", false, false, false, true), paddle::dialect::OpInputInfo("y", "paddle::dialect::DenseTensorType", false, false, false, true) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = {  };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("out", "paddle::dialect::DenseTensorType", false, false) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("ElementwiseInferMeta", {"x", "y"}, "fmin", {"x", "y"}, {}, {}, {}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "fmin");
}

void FminOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, pir::Value y_) {
  VLOG(4) << "Start build FminOp";



  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_, y_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;
  paddle::dialect::DenseTensorType y = y_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)y;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);

  VLOG(4) << "Builder construction  dense_y";
  paddle::dialect::IrTensor ir_tensor_y(paddle::dialect::TransToPhiDataType(y.dtype()),
                                                      y.dims(),
                                                      y.data_layout(),
                                                      y.lod(),
                                                      y.offset());
  VLOG(4) << "Builder construction  meta_y";
  paddle::dialect::IrMetaTensor meta_y(&ir_tensor_y);
  paddle::dialect::IrTensor dense_out;
  paddle::dialect::IrMetaTensor meta_out(&dense_out);

  phi::ElementwiseInferMeta(meta_x, meta_y, &meta_out);

  std::vector<pir::Type> argument_outputs;
  pir::Type out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_out.dtype()), dense_out.dims(), dense_out.layout(), dense_out.lod(), dense_out.offset());
  argument_outputs.push_back(out_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void FminOp::VerifySig() {
  VLOG(4) << "Start Verifying inputs, outputs and attributes for: FminOp.";
  VLOG(4) << "Verifying inputs:";
  {
  auto input_size = num_operands();
  IR_ENFORCE(input_size == 2u,
                    "The size %d of inputs must be equal to 2.", input_size);
  IR_ENFORCE((*this)->operand_source(0).type().isa<paddle::dialect::DenseTensorType>(),
                  "Type validation failed for the 0th input, got %s.", (*this)->operand_source(0).type());
  IR_ENFORCE((*this)->operand_source(1).type().isa<paddle::dialect::DenseTensorType>(),
                  "Type validation failed for the 1th input, got %s.", (*this)->operand_source(1).type());
  }
  VLOG(4) << "Verifying attributes:";
  {
  // Attributes num is 0, not need to check attributes type.
  }
  VLOG(4) << "Verifying outputs:";
  {
  auto output_size = num_results();
  IR_ENFORCE(output_size == 1u,
                    "The size %d of outputs must be equal to 1.", output_size);
  IR_ENFORCE((*this)->result(0).type().isa<paddle::dialect::DenseTensorType>(),
                 "Type validation failed for the 0th output.");
  }
  VLOG(4) << "End Verifying for: FminOp.";
}

void FminOp::InferMeta( phi::InferMetaContext *infer_meta ) {
  auto fn = PD_INFER_META(phi::ElementwiseInferMeta);
  fn(infer_meta);
}

phi::DataType FminOp::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: FminOp";
  


  // deal complex_promote
  if (framework::IsComplexType(expected_kernel_dtype)) {{
    // only promote inputs’s types when contains complex input
    return tensor_dtype;
  }}

  return expected_kernel_dtype;
}

const char *FoldOp::attributes_name[5] = { "output_sizes", "kernel_sizes", "strides", "paddings", "dilations" };

OpInfoTuple FoldOp::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("x", "paddle::dialect::DenseTensorType", false, false, false, true) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = { paddle::dialect::OpAttributeInfo("output_sizes", "pir::ArrayAttribute<pir::Int32Attribute>", ""), paddle::dialect::OpAttributeInfo("kernel_sizes", "pir::ArrayAttribute<pir::Int32Attribute>", ""), paddle::dialect::OpAttributeInfo("strides", "pir::ArrayAttribute<pir::Int32Attribute>", ""), paddle::dialect::OpAttributeInfo("paddings", "pir::ArrayAttribute<pir::Int32Attribute>", ""), paddle::dialect::OpAttributeInfo("dilations", "pir::ArrayAttribute<pir::Int32Attribute>", "") };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("out", "paddle::dialect::DenseTensorType", false, false) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("FoldInferMeta", {"x", "output_sizes", "kernel_sizes", "strides", "paddings", "dilations"}, "fold", {"x", "output_sizes", "kernel_sizes", "strides", "paddings", "dilations"}, {}, {}, {}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "fold");
}

void FoldOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, const std::vector<int>& output_sizes, const std::vector<int>& kernel_sizes, const std::vector<int>& strides, const std::vector<int>& paddings, const std::vector<int>& dilations) {
  VLOG(4) << "Start build FoldOp";



  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  std::vector<pir::Attribute> vec_output_sizes;
  for (size_t i = 0; i < static_cast<size_t>(output_sizes.size()); i++) {
      pir::Attribute attr_output_sizes = pir::Int32Attribute::get(pir::IrContext::Instance(), output_sizes[i]);

    vec_output_sizes.push_back(attr_output_sizes);
  }
  pir::Attribute attr_output_sizes = pir::ArrayAttribute::get(pir::IrContext::Instance(), vec_output_sizes);
  argument.AddAttribute("output_sizes", attr_output_sizes);
  std::vector<pir::Attribute> vec_kernel_sizes;
  for (size_t i = 0; i < static_cast<size_t>(kernel_sizes.size()); i++) {
      pir::Attribute attr_kernel_sizes = pir::Int32Attribute::get(pir::IrContext::Instance(), kernel_sizes[i]);

    vec_kernel_sizes.push_back(attr_kernel_sizes);
  }
  pir::Attribute attr_kernel_sizes = pir::ArrayAttribute::get(pir::IrContext::Instance(), vec_kernel_sizes);
  argument.AddAttribute("kernel_sizes", attr_kernel_sizes);
  std::vector<pir::Attribute> vec_strides;
  for (size_t i = 0; i < static_cast<size_t>(strides.size()); i++) {
      pir::Attribute attr_strides = pir::Int32Attribute::get(pir::IrContext::Instance(), strides[i]);

    vec_strides.push_back(attr_strides);
  }
  pir::Attribute attr_strides = pir::ArrayAttribute::get(pir::IrContext::Instance(), vec_strides);
  argument.AddAttribute("strides", attr_strides);
  std::vector<pir::Attribute> vec_paddings;
  for (size_t i = 0; i < static_cast<size_t>(paddings.size()); i++) {
      pir::Attribute attr_paddings = pir::Int32Attribute::get(pir::IrContext::Instance(), paddings[i]);

    vec_paddings.push_back(attr_paddings);
  }
  pir::Attribute attr_paddings = pir::ArrayAttribute::get(pir::IrContext::Instance(), vec_paddings);
  argument.AddAttribute("paddings", attr_paddings);
  std::vector<pir::Attribute> vec_dilations;
  for (size_t i = 0; i < static_cast<size_t>(dilations.size()); i++) {
      pir::Attribute attr_dilations = pir::Int32Attribute::get(pir::IrContext::Instance(), dilations[i]);

    vec_dilations.push_back(attr_dilations);
  }
  pir::Attribute attr_dilations = pir::ArrayAttribute::get(pir::IrContext::Instance(), vec_dilations);
  argument.AddAttribute("dilations", attr_dilations);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);
  paddle::dialect::IrTensor dense_out;
  paddle::dialect::IrMetaTensor meta_out(&dense_out);

  phi::FoldInferMeta(meta_x, output_sizes, kernel_sizes, strides, paddings, dilations, &meta_out);

  std::vector<pir::Type> argument_outputs;
  pir::Type out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_out.dtype()), dense_out.dims(), dense_out.layout(), dense_out.lod(), dense_out.offset());
  argument_outputs.push_back(out_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void FoldOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, pir::AttributeMap attributes) {
  VLOG(4) << "Start build FoldOp";


  IR_ENFORCE(
      attributes.find("output_sizes") != attributes.end(),
          "'output_sizes' Attribute is expected for FoldOp. ");
  std::vector<int> output_sizes;
  for (size_t i = 0; i < attributes.at("output_sizes").dyn_cast<pir::ArrayAttribute>().size(); i++) {
    output_sizes.push_back(attributes.at("output_sizes").dyn_cast<pir::ArrayAttribute>().at(i).dyn_cast<pir::Int32Attribute>().data());
  }

  IR_ENFORCE(
      attributes.find("kernel_sizes") != attributes.end(),
          "'kernel_sizes' Attribute is expected for FoldOp. ");
  std::vector<int> kernel_sizes;
  for (size_t i = 0; i < attributes.at("kernel_sizes").dyn_cast<pir::ArrayAttribute>().size(); i++) {
    kernel_sizes.push_back(attributes.at("kernel_sizes").dyn_cast<pir::ArrayAttribute>().at(i).dyn_cast<pir::Int32Attribute>().data());
  }

  IR_ENFORCE(
      attributes.find("strides") != attributes.end(),
          "'strides' Attribute is expected for FoldOp. ");
  std::vector<int> strides;
  for (size_t i = 0; i < attributes.at("strides").dyn_cast<pir::ArrayAttribute>().size(); i++) {
    strides.push_back(attributes.at("strides").dyn_cast<pir::ArrayAttribute>().at(i).dyn_cast<pir::Int32Attribute>().data());
  }

  IR_ENFORCE(
      attributes.find("paddings") != attributes.end(),
          "'paddings' Attribute is expected for FoldOp. ");
  std::vector<int> paddings;
  for (size_t i = 0; i < attributes.at("paddings").dyn_cast<pir::ArrayAttribute>().size(); i++) {
    paddings.push_back(attributes.at("paddings").dyn_cast<pir::ArrayAttribute>().at(i).dyn_cast<pir::Int32Attribute>().data());
  }

  IR_ENFORCE(
      attributes.find("dilations") != attributes.end(),
          "'dilations' Attribute is expected for FoldOp. ");
  std::vector<int> dilations;
  for (size_t i = 0; i < attributes.at("dilations").dyn_cast<pir::ArrayAttribute>().size(); i++) {
    dilations.push_back(attributes.at("dilations").dyn_cast<pir::ArrayAttribute>().at(i).dyn_cast<pir::Int32Attribute>().data());
  }


  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  std::vector<pir::Attribute> vec_output_sizes;
  for (size_t i = 0; i < static_cast<size_t>(output_sizes.size()); i++) {
      pir::Attribute attr_output_sizes = pir::Int32Attribute::get(pir::IrContext::Instance(), output_sizes[i]);

    vec_output_sizes.push_back(attr_output_sizes);
  }
  pir::Attribute attr_output_sizes = pir::ArrayAttribute::get(pir::IrContext::Instance(), vec_output_sizes);
  argument.AddAttribute("output_sizes", attr_output_sizes);
  std::vector<pir::Attribute> vec_kernel_sizes;
  for (size_t i = 0; i < static_cast<size_t>(kernel_sizes.size()); i++) {
      pir::Attribute attr_kernel_sizes = pir::Int32Attribute::get(pir::IrContext::Instance(), kernel_sizes[i]);

    vec_kernel_sizes.push_back(attr_kernel_sizes);
  }
  pir::Attribute attr_kernel_sizes = pir::ArrayAttribute::get(pir::IrContext::Instance(), vec_kernel_sizes);
  argument.AddAttribute("kernel_sizes", attr_kernel_sizes);
  std::vector<pir::Attribute> vec_strides;
  for (size_t i = 0; i < static_cast<size_t>(strides.size()); i++) {
      pir::Attribute attr_strides = pir::Int32Attribute::get(pir::IrContext::Instance(), strides[i]);

    vec_strides.push_back(attr_strides);
  }
  pir::Attribute attr_strides = pir::ArrayAttribute::get(pir::IrContext::Instance(), vec_strides);
  argument.AddAttribute("strides", attr_strides);
  std::vector<pir::Attribute> vec_paddings;
  for (size_t i = 0; i < static_cast<size_t>(paddings.size()); i++) {
      pir::Attribute attr_paddings = pir::Int32Attribute::get(pir::IrContext::Instance(), paddings[i]);

    vec_paddings.push_back(attr_paddings);
  }
  pir::Attribute attr_paddings = pir::ArrayAttribute::get(pir::IrContext::Instance(), vec_paddings);
  argument.AddAttribute("paddings", attr_paddings);
  std::vector<pir::Attribute> vec_dilations;
  for (size_t i = 0; i < static_cast<size_t>(dilations.size()); i++) {
      pir::Attribute attr_dilations = pir::Int32Attribute::get(pir::IrContext::Instance(), dilations[i]);

    vec_dilations.push_back(attr_dilations);
  }
  pir::Attribute attr_dilations = pir::ArrayAttribute::get(pir::IrContext::Instance(), vec_dilations);
  argument.AddAttribute("dilations", attr_dilations);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);
  paddle::dialect::IrTensor dense_out;
  paddle::dialect::IrMetaTensor meta_out(&dense_out);

  phi::FoldInferMeta(meta_x, output_sizes, kernel_sizes, strides, paddings, dilations, &meta_out);

  std::vector<pir::Type> argument_outputs;
  pir::Type out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_out.dtype()), dense_out.dims(), dense_out.layout(), dense_out.lod(), dense_out.offset());
  argument_outputs.push_back(out_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void FoldOp::VerifySig() {
  VLOG(4) << "Start Verifying inputs, outputs and attributes for: FoldOp.";
  VLOG(4) << "Verifying inputs:";
  {
  auto input_size = num_operands();
  IR_ENFORCE(input_size == 1u,
                    "The size %d of inputs must be equal to 1.", input_size);
  IR_ENFORCE((*this)->operand_source(0).type().isa<paddle::dialect::DenseTensorType>(),
                  "Type validation failed for the 0th input, got %s.", (*this)->operand_source(0).type());
  }
  VLOG(4) << "Verifying attributes:";
  {
  auto& attributes = this->attributes();
  IR_ENFORCE(attributes.count("output_sizes")>0,
                 "output_sizes does not exist.");
  IR_ENFORCE(attributes.at("output_sizes").isa<pir::ArrayAttribute>(),
                 "Type of attribute: output_sizes is not pir::ArrayAttribute.");
  for (size_t i = 0; i < attributes.at("output_sizes").dyn_cast<pir::ArrayAttribute>().size(); i++) {
    IR_ENFORCE(attributes.at("output_sizes").dyn_cast<pir::ArrayAttribute>().at(i).isa<pir::Int32Attribute>(),
                   "Type of attribute: output_sizes is not right.");
  }
  IR_ENFORCE(attributes.count("kernel_sizes")>0,
                 "kernel_sizes does not exist.");
  IR_ENFORCE(attributes.at("kernel_sizes").isa<pir::ArrayAttribute>(),
                 "Type of attribute: kernel_sizes is not pir::ArrayAttribute.");
  for (size_t i = 0; i < attributes.at("kernel_sizes").dyn_cast<pir::ArrayAttribute>().size(); i++) {
    IR_ENFORCE(attributes.at("kernel_sizes").dyn_cast<pir::ArrayAttribute>().at(i).isa<pir::Int32Attribute>(),
                   "Type of attribute: kernel_sizes is not right.");
  }
  IR_ENFORCE(attributes.count("strides")>0,
                 "strides does not exist.");
  IR_ENFORCE(attributes.at("strides").isa<pir::ArrayAttribute>(),
                 "Type of attribute: strides is not pir::ArrayAttribute.");
  for (size_t i = 0; i < attributes.at("strides").dyn_cast<pir::ArrayAttribute>().size(); i++) {
    IR_ENFORCE(attributes.at("strides").dyn_cast<pir::ArrayAttribute>().at(i).isa<pir::Int32Attribute>(),
                   "Type of attribute: strides is not right.");
  }
  IR_ENFORCE(attributes.count("paddings")>0,
                 "paddings does not exist.");
  IR_ENFORCE(attributes.at("paddings").isa<pir::ArrayAttribute>(),
                 "Type of attribute: paddings is not pir::ArrayAttribute.");
  for (size_t i = 0; i < attributes.at("paddings").dyn_cast<pir::ArrayAttribute>().size(); i++) {
    IR_ENFORCE(attributes.at("paddings").dyn_cast<pir::ArrayAttribute>().at(i).isa<pir::Int32Attribute>(),
                   "Type of attribute: paddings is not right.");
  }
  IR_ENFORCE(attributes.count("dilations")>0,
                 "dilations does not exist.");
  IR_ENFORCE(attributes.at("dilations").isa<pir::ArrayAttribute>(),
                 "Type of attribute: dilations is not pir::ArrayAttribute.");
  for (size_t i = 0; i < attributes.at("dilations").dyn_cast<pir::ArrayAttribute>().size(); i++) {
    IR_ENFORCE(attributes.at("dilations").dyn_cast<pir::ArrayAttribute>().at(i).isa<pir::Int32Attribute>(),
                   "Type of attribute: dilations is not right.");
  }
  }
  VLOG(4) << "Verifying outputs:";
  {
  auto output_size = num_results();
  IR_ENFORCE(output_size == 1u,
                    "The size %d of outputs must be equal to 1.", output_size);
  IR_ENFORCE((*this)->result(0).type().isa<paddle::dialect::DenseTensorType>(),
                 "Type validation failed for the 0th output.");
  }
  VLOG(4) << "End Verifying for: FoldOp.";
}

void FoldOp::InferMeta( phi::InferMetaContext *infer_meta ) {
  auto fn = PD_INFER_META(phi::FoldInferMeta);
  fn(infer_meta);
}

phi::DataType FoldOp::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: FoldOp";
  


  return expected_kernel_dtype;
}

const char *FrameOp::attributes_name[3] = { "frame_length", "hop_length", "axis" };

OpInfoTuple FrameOp::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("x", "paddle::dialect::DenseTensorType", false, false, false, true) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = { paddle::dialect::OpAttributeInfo("frame_length", "pir::Int32Attribute", ""), paddle::dialect::OpAttributeInfo("hop_length", "pir::Int32Attribute", ""), paddle::dialect::OpAttributeInfo("axis", "pir::Int32Attribute", "") };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("out", "paddle::dialect::DenseTensorType", false, false) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("FrameInferMeta", {"x", "frame_length", "hop_length", "axis"}, "frame", {"x", "frame_length", "hop_length", "axis"}, {}, {}, {}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "frame");
}

void FrameOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, int frame_length, int hop_length, int axis) {
  VLOG(4) << "Start build FrameOp";



  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_frame_length = pir::Int32Attribute::get(pir::IrContext::Instance(), frame_length);
  argument.AddAttribute("frame_length", attr_frame_length);
  pir::Attribute attr_hop_length = pir::Int32Attribute::get(pir::IrContext::Instance(), hop_length);
  argument.AddAttribute("hop_length", attr_hop_length);
  pir::Attribute attr_axis = pir::Int32Attribute::get(pir::IrContext::Instance(), axis);
  argument.AddAttribute("axis", attr_axis);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);
  paddle::dialect::IrTensor dense_out;
  paddle::dialect::IrMetaTensor meta_out(&dense_out);

  phi::FrameInferMeta(meta_x, frame_length, hop_length, axis, &meta_out);

  std::vector<pir::Type> argument_outputs;
  pir::Type out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_out.dtype()), dense_out.dims(), dense_out.layout(), dense_out.lod(), dense_out.offset());
  argument_outputs.push_back(out_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void FrameOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, pir::AttributeMap attributes) {
  VLOG(4) << "Start build FrameOp";


  IR_ENFORCE(
      attributes.find("frame_length") != attributes.end(),
          "'frame_length' Attribute is expected for FrameOp. ");
  int frame_length = attributes.at("frame_length").dyn_cast<pir::Int32Attribute>().data();

  IR_ENFORCE(
      attributes.find("hop_length") != attributes.end(),
          "'hop_length' Attribute is expected for FrameOp. ");
  int hop_length = attributes.at("hop_length").dyn_cast<pir::Int32Attribute>().data();

  IR_ENFORCE(
      attributes.find("axis") != attributes.end(),
          "'axis' Attribute is expected for FrameOp. ");
  int axis = attributes.at("axis").dyn_cast<pir::Int32Attribute>().data();


  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_frame_length = pir::Int32Attribute::get(pir::IrContext::Instance(), frame_length);
  argument.AddAttribute("frame_length", attr_frame_length);
  pir::Attribute attr_hop_length = pir::Int32Attribute::get(pir::IrContext::Instance(), hop_length);
  argument.AddAttribute("hop_length", attr_hop_length);
  pir::Attribute attr_axis = pir::Int32Attribute::get(pir::IrContext::Instance(), axis);
  argument.AddAttribute("axis", attr_axis);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);
  paddle::dialect::IrTensor dense_out;
  paddle::dialect::IrMetaTensor meta_out(&dense_out);

  phi::FrameInferMeta(meta_x, frame_length, hop_length, axis, &meta_out);

  std::vector<pir::Type> argument_outputs;
  pir::Type out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_out.dtype()), dense_out.dims(), dense_out.layout(), dense_out.lod(), dense_out.offset());
  argument_outputs.push_back(out_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void FrameOp::VerifySig() {
  VLOG(4) << "Start Verifying inputs, outputs and attributes for: FrameOp.";
  VLOG(4) << "Verifying inputs:";
  {
  auto input_size = num_operands();
  IR_ENFORCE(input_size == 1u,
                    "The size %d of inputs must be equal to 1.", input_size);
  IR_ENFORCE((*this)->operand_source(0).type().isa<paddle::dialect::DenseTensorType>(),
                  "Type validation failed for the 0th input, got %s.", (*this)->operand_source(0).type());
  }
  VLOG(4) << "Verifying attributes:";
  {
  auto& attributes = this->attributes();
  IR_ENFORCE(attributes.count("frame_length")>0,
                 "frame_length does not exist.");
  IR_ENFORCE(attributes.at("frame_length").isa<pir::Int32Attribute>(),
                 "Type of attribute: frame_length is not pir::Int32Attribute.");

  IR_ENFORCE(attributes.count("hop_length")>0,
                 "hop_length does not exist.");
  IR_ENFORCE(attributes.at("hop_length").isa<pir::Int32Attribute>(),
                 "Type of attribute: hop_length is not pir::Int32Attribute.");

  IR_ENFORCE(attributes.count("axis")>0,
                 "axis does not exist.");
  IR_ENFORCE(attributes.at("axis").isa<pir::Int32Attribute>(),
                 "Type of attribute: axis is not pir::Int32Attribute.");

  }
  VLOG(4) << "Verifying outputs:";
  {
  auto output_size = num_results();
  IR_ENFORCE(output_size == 1u,
                    "The size %d of outputs must be equal to 1.", output_size);
  IR_ENFORCE((*this)->result(0).type().isa<paddle::dialect::DenseTensorType>(),
                 "Type validation failed for the 0th output.");
  }
  VLOG(4) << "End Verifying for: FrameOp.";
}

void FrameOp::InferMeta( phi::InferMetaContext *infer_meta ) {
  auto fn = PD_INFER_META(phi::FrameInferMeta);
  fn(infer_meta);
}

phi::DataType FrameOp::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: FrameOp";
  


  return expected_kernel_dtype;
}

const char *FullIntArrayOp::attributes_name[3] = { "value", "dtype", "place" };

OpInfoTuple FullIntArrayOp::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = {  };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = { paddle::dialect::OpAttributeInfo("value", "pir::ArrayAttribute<pir::Int64Attribute>", ""), paddle::dialect::OpAttributeInfo("dtype", "paddle::dialect::DataTypeAttribute", ""), paddle::dialect::OpAttributeInfo("place", "paddle::dialect::PlaceAttribute", "") };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("out", "paddle::dialect::DenseTensorType", false, false) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("CreateVecShapeInferMeta", {"value", "dtype"}, "full_int_array", {"value", "dtype"}, {"dtype"}, {"place"}, {}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "full_int_array");
}

void FullIntArrayOp::Build(pir::Builder &builder, pir::OperationArgument &argument, const std::vector<int64_t>& value, phi::DataType dtype, const Place& place) {
  VLOG(4) << "Start build FullIntArrayOp";



  VLOG(4) << "Builder construction inputs";

  VLOG(4) << "Builder construction attributes";
  std::vector<pir::Attribute> vec_value;
  for (size_t i = 0; i < static_cast<size_t>(value.size()); i++) {
      pir::Attribute attr_value = pir::Int64Attribute::get(pir::IrContext::Instance(), value[i]);

    vec_value.push_back(attr_value);
  }
  pir::Attribute attr_value = pir::ArrayAttribute::get(pir::IrContext::Instance(), vec_value);
  argument.AddAttribute("value", attr_value);
  pir::Attribute attr_dtype = paddle::dialect::DataTypeAttribute::get(pir::IrContext::Instance(), dtype);
  argument.AddAttribute("dtype", attr_dtype);
  pir::Attribute attr_place = paddle::dialect::PlaceAttribute::get(pir::IrContext::Instance(), place);
  argument.AddAttribute("place", attr_place);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::IrTensor dense_out;
  paddle::dialect::IrMetaTensor meta_out(&dense_out);

  phi::CreateVecShapeInferMeta(value, dtype, &meta_out);

  std::vector<pir::Type> argument_outputs;
  pir::Type out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_out.dtype()), dense_out.dims(), dense_out.layout(), dense_out.lod(), dense_out.offset());
  argument_outputs.push_back(out_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void FullIntArrayOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::AttributeMap attributes) {
  VLOG(4) << "Start build FullIntArrayOp";


  IR_ENFORCE(
      attributes.find("value") != attributes.end(),
          "'value' Attribute is expected for FullIntArrayOp. ");
  std::vector<int64_t> value;
  for (size_t i = 0; i < attributes.at("value").dyn_cast<pir::ArrayAttribute>().size(); i++) {
    value.push_back(attributes.at("value").dyn_cast<pir::ArrayAttribute>().at(i).dyn_cast<pir::Int64Attribute>().data());
  }

  IR_ENFORCE(
      attributes.find("dtype") != attributes.end(),
          "'dtype' Attribute is expected for FullIntArrayOp. ");
  phi::DataType dtype = attributes.at("dtype").dyn_cast<paddle::dialect::DataTypeAttribute>().data();

  IR_ENFORCE(
      attributes.find("place") != attributes.end(),
          "'place' Attribute is expected for FullIntArrayOp. ");
  Place place = attributes.at("place").dyn_cast<paddle::dialect::PlaceAttribute>().data();


  VLOG(4) << "Builder construction inputs";

  VLOG(4) << "Builder construction attributes";
  std::vector<pir::Attribute> vec_value;
  for (size_t i = 0; i < static_cast<size_t>(value.size()); i++) {
      pir::Attribute attr_value = pir::Int64Attribute::get(pir::IrContext::Instance(), value[i]);

    vec_value.push_back(attr_value);
  }
  pir::Attribute attr_value = pir::ArrayAttribute::get(pir::IrContext::Instance(), vec_value);
  argument.AddAttribute("value", attr_value);
  pir::Attribute attr_dtype = paddle::dialect::DataTypeAttribute::get(pir::IrContext::Instance(), dtype);
  argument.AddAttribute("dtype", attr_dtype);
  pir::Attribute attr_place = paddle::dialect::PlaceAttribute::get(pir::IrContext::Instance(), place);
  argument.AddAttribute("place", attr_place);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::IrTensor dense_out;
  paddle::dialect::IrMetaTensor meta_out(&dense_out);

  phi::CreateVecShapeInferMeta(value, dtype, &meta_out);

  std::vector<pir::Type> argument_outputs;
  pir::Type out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_out.dtype()), dense_out.dims(), dense_out.layout(), dense_out.lod(), dense_out.offset());
  argument_outputs.push_back(out_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void FullIntArrayOp::VerifySig() {
  VLOG(4) << "Start Verifying inputs, outputs and attributes for: FullIntArrayOp.";
  VLOG(4) << "Verifying inputs:";
  {
  auto input_size = num_operands();
  IR_ENFORCE(input_size == 0u,
                    "The size %d of inputs must be equal to 0.", input_size);
  // Inputs num is 0, not need to check inputs type.
  }
  VLOG(4) << "Verifying attributes:";
  {
  auto& attributes = this->attributes();
  IR_ENFORCE(attributes.count("value")>0,
                 "value does not exist.");
  IR_ENFORCE(attributes.at("value").isa<pir::ArrayAttribute>(),
                 "Type of attribute: value is not pir::ArrayAttribute.");
  for (size_t i = 0; i < attributes.at("value").dyn_cast<pir::ArrayAttribute>().size(); i++) {
    IR_ENFORCE(attributes.at("value").dyn_cast<pir::ArrayAttribute>().at(i).isa<pir::Int64Attribute>(),
                   "Type of attribute: value is not right.");
  }
  IR_ENFORCE(attributes.count("dtype")>0,
                 "dtype does not exist.");
  IR_ENFORCE(attributes.at("dtype").isa<paddle::dialect::DataTypeAttribute>(),
                 "Type of attribute: dtype is not paddle::dialect::DataTypeAttribute.");

  IR_ENFORCE(attributes.count("place")>0,
                 "place does not exist.");
  IR_ENFORCE(attributes.at("place").isa<paddle::dialect::PlaceAttribute>(),
                 "Type of attribute: place is not paddle::dialect::PlaceAttribute.");

  }
  VLOG(4) << "Verifying outputs:";
  {
  auto output_size = num_results();
  IR_ENFORCE(output_size == 1u,
                    "The size %d of outputs must be equal to 1.", output_size);
  IR_ENFORCE((*this)->result(0).type().isa<paddle::dialect::DenseTensorType>(),
                 "Type validation failed for the 0th output.");
  }
  VLOG(4) << "End Verifying for: FullIntArrayOp.";
}

void FullIntArrayOp::InferMeta( phi::InferMetaContext *infer_meta ) {
  auto fn = PD_INFER_META(phi::CreateVecShapeInferMeta);
  fn(infer_meta);
}

phi::DataType FullIntArrayOp::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: FullIntArrayOp";
  


  return expected_kernel_dtype;
}

bool FullIntArrayOp::InferSymbolicShape(pir::ShapeConstraintIRAnalysis* shape_analysis) {
  VLOG(4) << "Infer symbolic shape for op: FullIntArrayOp";
  return FullIntArrayOpInferSymbolicShape(this->operation(), shape_analysis);
}

OpInfoTuple GammalnOp::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("x", "paddle::dialect::DenseTensorType", false, false, false, true) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = {  };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("out", "paddle::dialect::DenseTensorType", false, false) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("UnchangedInferMeta", {"x"}, "gammaln", {"x"}, {}, {}, {}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "gammaln");
}

void GammalnOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_) {
  VLOG(4) << "Start build GammalnOp";



  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);
  paddle::dialect::IrTensor dense_out;
  paddle::dialect::IrMetaTensor meta_out(&dense_out);

  phi::UnchangedInferMeta(meta_x, &meta_out);

  std::vector<pir::Type> argument_outputs;
  pir::Type out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_out.dtype()), dense_out.dims(), dense_out.layout(), dense_out.lod(), dense_out.offset());
  argument_outputs.push_back(out_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void GammalnOp::VerifySig() {
  VLOG(4) << "Start Verifying inputs, outputs and attributes for: GammalnOp.";
  VLOG(4) << "Verifying inputs:";
  {
  auto input_size = num_operands();
  IR_ENFORCE(input_size == 1u,
                    "The size %d of inputs must be equal to 1.", input_size);
  IR_ENFORCE((*this)->operand_source(0).type().isa<paddle::dialect::DenseTensorType>(),
                  "Type validation failed for the 0th input, got %s.", (*this)->operand_source(0).type());
  }
  VLOG(4) << "Verifying attributes:";
  {
  // Attributes num is 0, not need to check attributes type.
  }
  VLOG(4) << "Verifying outputs:";
  {
  auto output_size = num_results();
  IR_ENFORCE(output_size == 1u,
                    "The size %d of outputs must be equal to 1.", output_size);
  IR_ENFORCE((*this)->result(0).type().isa<paddle::dialect::DenseTensorType>(),
                 "Type validation failed for the 0th output.");
  }
  VLOG(4) << "End Verifying for: GammalnOp.";
}

void GammalnOp::InferMeta( phi::InferMetaContext *infer_meta ) {
  auto fn = PD_INFER_META(phi::UnchangedInferMeta);
  fn(infer_meta);
}

phi::DataType GammalnOp::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: GammalnOp";
  


  return expected_kernel_dtype;
}

OpInfoTuple Gammaln_Op::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("x", "paddle::dialect::DenseTensorType", false, false, false, true) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = {  };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("out", "paddle::dialect::DenseTensorType", false, false) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("UnchangedInferMeta", {"x"}, "gammaln", {"x"}, {}, {}, {{"out", "x"}}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "gammaln");
}

void Gammaln_Op::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_) {
  VLOG(4) << "Start build Gammaln_Op";



  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);
  paddle::dialect::IrTensor dense_out;
  paddle::dialect::IrMetaTensor meta_out(&dense_out);

  phi::UnchangedInferMeta(meta_x, &meta_out);

  std::vector<pir::Type> argument_outputs;
  pir::Type out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_out.dtype()), dense_out.dims(), dense_out.layout(), dense_out.lod(), dense_out.offset());
  argument_outputs.push_back(out_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void Gammaln_Op::VerifySig() {
  VLOG(4) << "Start Verifying inputs, outputs and attributes for: Gammaln_Op.";
  VLOG(4) << "Verifying inputs:";
  {
  auto input_size = num_operands();
  IR_ENFORCE(input_size == 1u,
                    "The size %d of inputs must be equal to 1.", input_size);
  IR_ENFORCE((*this)->operand_source(0).type().isa<paddle::dialect::DenseTensorType>(),
                  "Type validation failed for the 0th input, got %s.", (*this)->operand_source(0).type());
  }
  VLOG(4) << "Verifying attributes:";
  {
  // Attributes num is 0, not need to check attributes type.
  }
  VLOG(4) << "Verifying outputs:";
  {
  auto output_size = num_results();
  IR_ENFORCE(output_size == 1u,
                    "The size %d of outputs must be equal to 1.", output_size);
  IR_ENFORCE((*this)->result(0).type().isa<paddle::dialect::DenseTensorType>(),
                 "Type validation failed for the 0th output.");
  }
  VLOG(4) << "End Verifying for: Gammaln_Op.";
}

void Gammaln_Op::InferMeta( phi::InferMetaContext *infer_meta ) {
  auto fn = PD_INFER_META(phi::UnchangedInferMeta);
  fn(infer_meta);
}

phi::DataType Gammaln_Op::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: Gammaln_Op";
  


  return expected_kernel_dtype;
}

OpInfoTuple GatherOp::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("x", "paddle::dialect::DenseTensorType", false, false, false, true), paddle::dialect::OpInputInfo("index", "paddle::dialect::DenseTensorType", false, false, false, false), paddle::dialect::OpInputInfo("axis", "paddle::dialect::ScalarAttribute", false, false, true, false) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = {  };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("out", "paddle::dialect::DenseTensorType", false, false) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("GatherInferMeta", {"x", "index", "axis"}, "gather", {"x", "index", "axis"}, {"x"}, {}, {}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "gather");
}

void GatherOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, pir::Value index_, int axis) {
  VLOG(4) << "Start build GatherOp";


  // Generate scalar mutable attribute: axis
  paddle::dialect::FullOp full_axis_op = builder.Build<paddle::dialect::FullOp>(std::vector<int64_t>{1}, axis, phi::DataType::INT32, phi::CPUPlace());
  pir::OpResult axis_ = full_axis_op->result(0);
    
  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_, index_, axis_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;
  paddle::dialect::DenseTensorType index = index_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)index;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);

  VLOG(4) << "Builder construction  dense_index";
  paddle::dialect::IrTensor ir_tensor_index(paddle::dialect::TransToPhiDataType(index.dtype()),
                                                      index.dims(),
                                                      index.data_layout(),
                                                      index.lod(),
                                                      index.offset());
  VLOG(4) << "Builder construction  meta_index";
  paddle::dialect::IrMetaTensor meta_index(&ir_tensor_index);
  paddle::dialect::IrTensor dense_out;
  paddle::dialect::IrMetaTensor meta_out(&dense_out);

  phi::GatherInferMeta(meta_x, meta_index, axis, &meta_out);

  std::vector<pir::Type> argument_outputs;
  pir::Type out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_out.dtype()), dense_out.dims(), dense_out.layout(), dense_out.lod(), dense_out.offset());
  argument_outputs.push_back(out_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void GatherOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, pir::Value index_, pir::AttributeMap attributes) {
  VLOG(4) << "Start build GatherOp";


  IR_ENFORCE(
      attributes.find("axis") != attributes.end(),
          "'axis' Attribute is expected for GatherOp. ");
  int axis = attributes.at("axis").dyn_cast<paddle::dialect::ScalarAttribute>().data().to<int>();

  // Generate scalar mutable attribute: axis
  paddle::dialect::FullOp full_axis_op = builder.Build<paddle::dialect::FullOp>(std::vector<int64_t>{1}, axis, phi::DataType::INT32, phi::CPUPlace());
  pir::OpResult axis_ = full_axis_op->result(0);
    
  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_, index_, axis_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;
  paddle::dialect::DenseTensorType index = index_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)index;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);

  VLOG(4) << "Builder construction  dense_index";
  paddle::dialect::IrTensor ir_tensor_index(paddle::dialect::TransToPhiDataType(index.dtype()),
                                                      index.dims(),
                                                      index.data_layout(),
                                                      index.lod(),
                                                      index.offset());
  VLOG(4) << "Builder construction  meta_index";
  paddle::dialect::IrMetaTensor meta_index(&ir_tensor_index);
  paddle::dialect::IrTensor dense_out;
  paddle::dialect::IrMetaTensor meta_out(&dense_out);

  phi::GatherInferMeta(meta_x, meta_index, axis, &meta_out);

  std::vector<pir::Type> argument_outputs;
  pir::Type out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_out.dtype()), dense_out.dims(), dense_out.layout(), dense_out.lod(), dense_out.offset());
  argument_outputs.push_back(out_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void GatherOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, pir::Value index_, pir::Value axis_) {
  VLOG(4) << "Start build GatherOp";



  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_, index_, axis_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;
  paddle::dialect::DenseTensorType index = index_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)index;
  phi::Scalar axis;
  if (axis_.dyn_cast<pir::OpResult>() && axis_.dyn_cast<pir::OpResult>().owner()->isa<paddle::dialect::FullOp>()) {
    axis = std::move(phi::Scalar(axis_.dyn_cast<pir::OpResult>().owner()
                                  ->dyn_cast<paddle::dialect::FullOp>()
                                  .attribute("value")
                                  .dyn_cast<paddle::dialect::ScalarAttribute>()
                                  .data()
                                  .to<int>()));
  }
  else {
    axis = std::move(phi::Scalar(-1));
    axis.SetFromTensor(true);
  }


  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);

  VLOG(4) << "Builder construction  dense_index";
  paddle::dialect::IrTensor ir_tensor_index(paddle::dialect::TransToPhiDataType(index.dtype()),
                                                      index.dims(),
                                                      index.data_layout(),
                                                      index.lod(),
                                                      index.offset());
  VLOG(4) << "Builder construction  meta_index";
  paddle::dialect::IrMetaTensor meta_index(&ir_tensor_index);
  paddle::dialect::IrTensor dense_out;
  paddle::dialect::IrMetaTensor meta_out(&dense_out);

  phi::GatherInferMeta(meta_x, meta_index, axis, &meta_out);

  std::vector<pir::Type> argument_outputs;
  pir::Type out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_out.dtype()), dense_out.dims(), dense_out.layout(), dense_out.lod(), dense_out.offset());
  argument_outputs.push_back(out_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void GatherOp::VerifySig() {
  VLOG(4) << "Start Verifying inputs, outputs and attributes for: GatherOp.";
  VLOG(4) << "Verifying inputs:";
  {
  auto input_size = num_operands();
  IR_ENFORCE(input_size == 3u,
                    "The size %d of inputs must be equal to 3.", input_size);
  IR_ENFORCE((*this)->operand_source(0).type().isa<paddle::dialect::DenseTensorType>(),
                  "Type validation failed for the 0th input, got %s.", (*this)->operand_source(0).type());
  IR_ENFORCE((*this)->operand_source(1).type().isa<paddle::dialect::DenseTensorType>(),
                  "Type validation failed for the 1th input, got %s.", (*this)->operand_source(1).type());
  IR_ENFORCE((*this)->operand_source(2).type().isa<paddle::dialect::DenseTensorType>(),
                  "Type validation failed for the 2th input, got %s.", (*this)->operand_source(2).type());
  }
  VLOG(4) << "Verifying attributes:";
  {
  // Attributes num is 0, not need to check attributes type.
  }
  VLOG(4) << "Verifying outputs:";
  {
  auto output_size = num_results();
  IR_ENFORCE(output_size == 1u,
                    "The size %d of outputs must be equal to 1.", output_size);
  IR_ENFORCE((*this)->result(0).type().isa<paddle::dialect::DenseTensorType>(),
                 "Type validation failed for the 0th output.");
  }
  VLOG(4) << "End Verifying for: GatherOp.";
}

void GatherOp::InferMeta( phi::InferMetaContext *infer_meta ) {
  auto fn = PD_INFER_META(phi::GatherInferMeta);
  fn(infer_meta);
}

phi::DataType GatherOp::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: GatherOp";
  


  return expected_kernel_dtype;
}

OpInfoTuple GatherNdOp::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("x", "paddle::dialect::DenseTensorType", false, false, false, true), paddle::dialect::OpInputInfo("index", "paddle::dialect::DenseTensorType", false, false, false, false) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = {  };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("out", "paddle::dialect::DenseTensorType", false, false) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("GatherNdInferMeta", {"x", "index"}, "gather_nd", {"x", "index"}, {"x"}, {}, {}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "gather_nd");
}

void GatherNdOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, pir::Value index_) {
  VLOG(4) << "Start build GatherNdOp";



  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_, index_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;
  paddle::dialect::DenseTensorType index = index_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)index;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);

  VLOG(4) << "Builder construction  dense_index";
  paddle::dialect::IrTensor ir_tensor_index(paddle::dialect::TransToPhiDataType(index.dtype()),
                                                      index.dims(),
                                                      index.data_layout(),
                                                      index.lod(),
                                                      index.offset());
  VLOG(4) << "Builder construction  meta_index";
  paddle::dialect::IrMetaTensor meta_index(&ir_tensor_index);
  paddle::dialect::IrTensor dense_out;
  paddle::dialect::IrMetaTensor meta_out(&dense_out);

  phi::GatherNdInferMeta(meta_x, meta_index, &meta_out);

  std::vector<pir::Type> argument_outputs;
  pir::Type out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_out.dtype()), dense_out.dims(), dense_out.layout(), dense_out.lod(), dense_out.offset());
  argument_outputs.push_back(out_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void GatherNdOp::VerifySig() {
  VLOG(4) << "Start Verifying inputs, outputs and attributes for: GatherNdOp.";
  VLOG(4) << "Verifying inputs:";
  {
  auto input_size = num_operands();
  IR_ENFORCE(input_size == 2u,
                    "The size %d of inputs must be equal to 2.", input_size);
  IR_ENFORCE((*this)->operand_source(0).type().isa<paddle::dialect::DenseTensorType>(),
                  "Type validation failed for the 0th input, got %s.", (*this)->operand_source(0).type());
  IR_ENFORCE((*this)->operand_source(1).type().isa<paddle::dialect::DenseTensorType>(),
                  "Type validation failed for the 1th input, got %s.", (*this)->operand_source(1).type());
  }
  VLOG(4) << "Verifying attributes:";
  {
  // Attributes num is 0, not need to check attributes type.
  }
  VLOG(4) << "Verifying outputs:";
  {
  auto output_size = num_results();
  IR_ENFORCE(output_size == 1u,
                    "The size %d of outputs must be equal to 1.", output_size);
  IR_ENFORCE((*this)->result(0).type().isa<paddle::dialect::DenseTensorType>(),
                 "Type validation failed for the 0th output.");
  }
  VLOG(4) << "End Verifying for: GatherNdOp.";
}

void GatherNdOp::InferMeta( phi::InferMetaContext *infer_meta ) {
  auto fn = PD_INFER_META(phi::GatherNdInferMeta);
  fn(infer_meta);
}

phi::DataType GatherNdOp::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: GatherNdOp";
  


  return expected_kernel_dtype;
}

OpInfoTuple GatherTreeOp::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("ids", "paddle::dialect::DenseTensorType", false, false, false, false), paddle::dialect::OpInputInfo("parents", "paddle::dialect::DenseTensorType", false, false, false, false) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = {  };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("out", "paddle::dialect::DenseTensorType", false, false) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("GatherTreeMeta", {"ids", "parents"}, "gather_tree", {"ids", "parents"}, {"ids"}, {}, {}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "gather_tree");
}

void GatherTreeOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value ids_, pir::Value parents_) {
  VLOG(4) << "Start build GatherTreeOp";



  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {ids_, parents_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType ids = ids_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)ids;
  paddle::dialect::DenseTensorType parents = parents_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)parents;

  VLOG(4) << "Builder construction  dense_ids";
  paddle::dialect::IrTensor ir_tensor_ids(paddle::dialect::TransToPhiDataType(ids.dtype()),
                                                      ids.dims(),
                                                      ids.data_layout(),
                                                      ids.lod(),
                                                      ids.offset());
  VLOG(4) << "Builder construction  meta_ids";
  paddle::dialect::IrMetaTensor meta_ids(&ir_tensor_ids);

  VLOG(4) << "Builder construction  dense_parents";
  paddle::dialect::IrTensor ir_tensor_parents(paddle::dialect::TransToPhiDataType(parents.dtype()),
                                                      parents.dims(),
                                                      parents.data_layout(),
                                                      parents.lod(),
                                                      parents.offset());
  VLOG(4) << "Builder construction  meta_parents";
  paddle::dialect::IrMetaTensor meta_parents(&ir_tensor_parents);
  paddle::dialect::IrTensor dense_out;
  paddle::dialect::IrMetaTensor meta_out(&dense_out);

  phi::GatherTreeMeta(meta_ids, meta_parents, &meta_out);

  std::vector<pir::Type> argument_outputs;
  pir::Type out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_out.dtype()), dense_out.dims(), dense_out.layout(), dense_out.lod(), dense_out.offset());
  argument_outputs.push_back(out_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void GatherTreeOp::VerifySig() {
  VLOG(4) << "Start Verifying inputs, outputs and attributes for: GatherTreeOp.";
  VLOG(4) << "Verifying inputs:";
  {
  auto input_size = num_operands();
  IR_ENFORCE(input_size == 2u,
                    "The size %d of inputs must be equal to 2.", input_size);
  IR_ENFORCE((*this)->operand_source(0).type().isa<paddle::dialect::DenseTensorType>(),
                  "Type validation failed for the 0th input, got %s.", (*this)->operand_source(0).type());
  IR_ENFORCE((*this)->operand_source(1).type().isa<paddle::dialect::DenseTensorType>(),
                  "Type validation failed for the 1th input, got %s.", (*this)->operand_source(1).type());
  }
  VLOG(4) << "Verifying attributes:";
  {
  // Attributes num is 0, not need to check attributes type.
  }
  VLOG(4) << "Verifying outputs:";
  {
  auto output_size = num_results();
  IR_ENFORCE(output_size == 1u,
                    "The size %d of outputs must be equal to 1.", output_size);
  IR_ENFORCE((*this)->result(0).type().isa<paddle::dialect::DenseTensorType>(),
                 "Type validation failed for the 0th output.");
  }
  VLOG(4) << "End Verifying for: GatherTreeOp.";
}

void GatherTreeOp::InferMeta( phi::InferMetaContext *infer_meta ) {
  auto fn = PD_INFER_META(phi::GatherTreeMeta);
  fn(infer_meta);
}

phi::DataType GatherTreeOp::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: GatherTreeOp";
  


  return expected_kernel_dtype;
}

const char *GaussianInplaceOp::attributes_name[3] = { "mean", "std", "seed" };

OpInfoTuple GaussianInplaceOp::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("x", "paddle::dialect::DenseTensorType", false, false, false, true) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = { paddle::dialect::OpAttributeInfo("mean", "pir::FloatAttribute", ""), paddle::dialect::OpAttributeInfo("std", "pir::FloatAttribute", ""), paddle::dialect::OpAttributeInfo("seed", "pir::Int32Attribute", "") };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("out", "paddle::dialect::DenseTensorType", false, false) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("UnchangedInferMeta", {"x"}, "gaussian_inplace", {"x", "mean", "std", "seed"}, {"x"}, {"x"}, {}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "gaussian_inplace");
}

void GaussianInplaceOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, float mean, float std, int seed) {
  VLOG(4) << "Start build GaussianInplaceOp";



  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_mean = pir::FloatAttribute::get(pir::IrContext::Instance(), mean);
  argument.AddAttribute("mean", attr_mean);
  pir::Attribute attr_std = pir::FloatAttribute::get(pir::IrContext::Instance(), std);
  argument.AddAttribute("std", attr_std);
  pir::Attribute attr_seed = pir::Int32Attribute::get(pir::IrContext::Instance(), seed);
  argument.AddAttribute("seed", attr_seed);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);
  paddle::dialect::IrTensor dense_out;
  paddle::dialect::IrMetaTensor meta_out(&dense_out);

  phi::UnchangedInferMeta(meta_x, &meta_out);

  std::vector<pir::Type> argument_outputs;
  pir::Type out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_out.dtype()), dense_out.dims(), dense_out.layout(), dense_out.lod(), dense_out.offset());
  argument_outputs.push_back(out_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void GaussianInplaceOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, pir::AttributeMap attributes) {
  VLOG(4) << "Start build GaussianInplaceOp";


  IR_ENFORCE(
      attributes.find("mean") != attributes.end(),
          "'mean' Attribute is expected for GaussianInplaceOp. ");
  float mean = attributes.at("mean").dyn_cast<pir::FloatAttribute>().data();

  IR_ENFORCE(
      attributes.find("std") != attributes.end(),
          "'std' Attribute is expected for GaussianInplaceOp. ");
  float std = attributes.at("std").dyn_cast<pir::FloatAttribute>().data();

  IR_ENFORCE(
      attributes.find("seed") != attributes.end(),
          "'seed' Attribute is expected for GaussianInplaceOp. ");
  int seed = attributes.at("seed").dyn_cast<pir::Int32Attribute>().data();


  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_mean = pir::FloatAttribute::get(pir::IrContext::Instance(), mean);
  argument.AddAttribute("mean", attr_mean);
  pir::Attribute attr_std = pir::FloatAttribute::get(pir::IrContext::Instance(), std);
  argument.AddAttribute("std", attr_std);
  pir::Attribute attr_seed = pir::Int32Attribute::get(pir::IrContext::Instance(), seed);
  argument.AddAttribute("seed", attr_seed);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);
  paddle::dialect::IrTensor dense_out;
  paddle::dialect::IrMetaTensor meta_out(&dense_out);

  phi::UnchangedInferMeta(meta_x, &meta_out);

  std::vector<pir::Type> argument_outputs;
  pir::Type out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_out.dtype()), dense_out.dims(), dense_out.layout(), dense_out.lod(), dense_out.offset());
  argument_outputs.push_back(out_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void GaussianInplaceOp::VerifySig() {
  VLOG(4) << "Start Verifying inputs, outputs and attributes for: GaussianInplaceOp.";
  VLOG(4) << "Verifying inputs:";
  {
  auto input_size = num_operands();
  IR_ENFORCE(input_size == 1u,
                    "The size %d of inputs must be equal to 1.", input_size);
  IR_ENFORCE((*this)->operand_source(0).type().isa<paddle::dialect::DenseTensorType>(),
                  "Type validation failed for the 0th input, got %s.", (*this)->operand_source(0).type());
  }
  VLOG(4) << "Verifying attributes:";
  {
  auto& attributes = this->attributes();
  IR_ENFORCE(attributes.count("mean")>0,
                 "mean does not exist.");
  IR_ENFORCE(attributes.at("mean").isa<pir::FloatAttribute>(),
                 "Type of attribute: mean is not pir::FloatAttribute.");

  IR_ENFORCE(attributes.count("std")>0,
                 "std does not exist.");
  IR_ENFORCE(attributes.at("std").isa<pir::FloatAttribute>(),
                 "Type of attribute: std is not pir::FloatAttribute.");

  IR_ENFORCE(attributes.count("seed")>0,
                 "seed does not exist.");
  IR_ENFORCE(attributes.at("seed").isa<pir::Int32Attribute>(),
                 "Type of attribute: seed is not pir::Int32Attribute.");

  }
  VLOG(4) << "Verifying outputs:";
  {
  auto output_size = num_results();
  IR_ENFORCE(output_size == 1u,
                    "The size %d of outputs must be equal to 1.", output_size);
  IR_ENFORCE((*this)->result(0).type().isa<paddle::dialect::DenseTensorType>(),
                 "Type validation failed for the 0th output.");
  }
  VLOG(4) << "End Verifying for: GaussianInplaceOp.";
}

void GaussianInplaceOp::InferMeta( phi::InferMetaContext *infer_meta ) {
  auto fn = PD_INFER_META(phi::UnchangedInferMeta);
  fn(infer_meta);
}

phi::DataType GaussianInplaceOp::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: GaussianInplaceOp";
  


  return expected_kernel_dtype;
}

const char *GaussianInplace_Op::attributes_name[3] = { "mean", "std", "seed" };

OpInfoTuple GaussianInplace_Op::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("x", "paddle::dialect::DenseTensorType", false, false, false, true) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = { paddle::dialect::OpAttributeInfo("mean", "pir::FloatAttribute", ""), paddle::dialect::OpAttributeInfo("std", "pir::FloatAttribute", ""), paddle::dialect::OpAttributeInfo("seed", "pir::Int32Attribute", "") };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("out", "paddle::dialect::DenseTensorType", false, false) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("UnchangedInferMeta", {"x"}, "gaussian_inplace", {"x", "mean", "std", "seed"}, {"x"}, {"x"}, {{"out", "x"}}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "gaussian_inplace");
}

void GaussianInplace_Op::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, float mean, float std, int seed) {
  VLOG(4) << "Start build GaussianInplace_Op";



  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_mean = pir::FloatAttribute::get(pir::IrContext::Instance(), mean);
  argument.AddAttribute("mean", attr_mean);
  pir::Attribute attr_std = pir::FloatAttribute::get(pir::IrContext::Instance(), std);
  argument.AddAttribute("std", attr_std);
  pir::Attribute attr_seed = pir::Int32Attribute::get(pir::IrContext::Instance(), seed);
  argument.AddAttribute("seed", attr_seed);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);
  paddle::dialect::IrTensor dense_out;
  paddle::dialect::IrMetaTensor meta_out(&dense_out);

  phi::UnchangedInferMeta(meta_x, &meta_out);

  std::vector<pir::Type> argument_outputs;
  pir::Type out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_out.dtype()), dense_out.dims(), dense_out.layout(), dense_out.lod(), dense_out.offset());
  argument_outputs.push_back(out_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void GaussianInplace_Op::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, pir::AttributeMap attributes) {
  VLOG(4) << "Start build GaussianInplace_Op";


  IR_ENFORCE(
      attributes.find("mean") != attributes.end(),
          "'mean' Attribute is expected for GaussianInplace_Op. ");
  float mean = attributes.at("mean").dyn_cast<pir::FloatAttribute>().data();

  IR_ENFORCE(
      attributes.find("std") != attributes.end(),
          "'std' Attribute is expected for GaussianInplace_Op. ");
  float std = attributes.at("std").dyn_cast<pir::FloatAttribute>().data();

  IR_ENFORCE(
      attributes.find("seed") != attributes.end(),
          "'seed' Attribute is expected for GaussianInplace_Op. ");
  int seed = attributes.at("seed").dyn_cast<pir::Int32Attribute>().data();


  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_mean = pir::FloatAttribute::get(pir::IrContext::Instance(), mean);
  argument.AddAttribute("mean", attr_mean);
  pir::Attribute attr_std = pir::FloatAttribute::get(pir::IrContext::Instance(), std);
  argument.AddAttribute("std", attr_std);
  pir::Attribute attr_seed = pir::Int32Attribute::get(pir::IrContext::Instance(), seed);
  argument.AddAttribute("seed", attr_seed);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);
  paddle::dialect::IrTensor dense_out;
  paddle::dialect::IrMetaTensor meta_out(&dense_out);

  phi::UnchangedInferMeta(meta_x, &meta_out);

  std::vector<pir::Type> argument_outputs;
  pir::Type out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_out.dtype()), dense_out.dims(), dense_out.layout(), dense_out.lod(), dense_out.offset());
  argument_outputs.push_back(out_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void GaussianInplace_Op::VerifySig() {
  VLOG(4) << "Start Verifying inputs, outputs and attributes for: GaussianInplace_Op.";
  VLOG(4) << "Verifying inputs:";
  {
  auto input_size = num_operands();
  IR_ENFORCE(input_size == 1u,
                    "The size %d of inputs must be equal to 1.", input_size);
  IR_ENFORCE((*this)->operand_source(0).type().isa<paddle::dialect::DenseTensorType>(),
                  "Type validation failed for the 0th input, got %s.", (*this)->operand_source(0).type());
  }
  VLOG(4) << "Verifying attributes:";
  {
  auto& attributes = this->attributes();
  IR_ENFORCE(attributes.count("mean")>0,
                 "mean does not exist.");
  IR_ENFORCE(attributes.at("mean").isa<pir::FloatAttribute>(),
                 "Type of attribute: mean is not pir::FloatAttribute.");

  IR_ENFORCE(attributes.count("std")>0,
                 "std does not exist.");
  IR_ENFORCE(attributes.at("std").isa<pir::FloatAttribute>(),
                 "Type of attribute: std is not pir::FloatAttribute.");

  IR_ENFORCE(attributes.count("seed")>0,
                 "seed does not exist.");
  IR_ENFORCE(attributes.at("seed").isa<pir::Int32Attribute>(),
                 "Type of attribute: seed is not pir::Int32Attribute.");

  }
  VLOG(4) << "Verifying outputs:";
  {
  auto output_size = num_results();
  IR_ENFORCE(output_size == 1u,
                    "The size %d of outputs must be equal to 1.", output_size);
  IR_ENFORCE((*this)->result(0).type().isa<paddle::dialect::DenseTensorType>(),
                 "Type validation failed for the 0th output.");
  }
  VLOG(4) << "End Verifying for: GaussianInplace_Op.";
}

void GaussianInplace_Op::InferMeta( phi::InferMetaContext *infer_meta ) {
  auto fn = PD_INFER_META(phi::UnchangedInferMeta);
  fn(infer_meta);
}

phi::DataType GaussianInplace_Op::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: GaussianInplace_Op";
  


  return expected_kernel_dtype;
}

const char *GeluOp::attributes_name[1] = { "approximate" };

OpInfoTuple GeluOp::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("x", "paddle::dialect::DenseTensorType", false, false, false, true) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = { paddle::dialect::OpAttributeInfo("approximate", "pir::BoolAttribute", "") };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("out", "paddle::dialect::DenseTensorType", false, false) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("UnchangedInferMeta", {"x"}, "gelu", {"x", "approximate"}, {}, {}, {}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "gelu");
}

void GeluOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, bool approximate) {
  VLOG(4) << "Start build GeluOp";



  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_approximate = pir::BoolAttribute::get(pir::IrContext::Instance(), approximate);
  argument.AddAttribute("approximate", attr_approximate);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);
  paddle::dialect::IrTensor dense_out;
  paddle::dialect::IrMetaTensor meta_out(&dense_out);

  phi::UnchangedInferMeta(meta_x, &meta_out);

  std::vector<pir::Type> argument_outputs;
  pir::Type out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_out.dtype()), dense_out.dims(), dense_out.layout(), dense_out.lod(), dense_out.offset());
  argument_outputs.push_back(out_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void GeluOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, pir::AttributeMap attributes) {
  VLOG(4) << "Start build GeluOp";


  IR_ENFORCE(
      attributes.find("approximate") != attributes.end(),
          "'approximate' Attribute is expected for GeluOp. ");
  bool approximate = attributes.at("approximate").dyn_cast<pir::BoolAttribute>().data();


  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_approximate = pir::BoolAttribute::get(pir::IrContext::Instance(), approximate);
  argument.AddAttribute("approximate", attr_approximate);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);
  paddle::dialect::IrTensor dense_out;
  paddle::dialect::IrMetaTensor meta_out(&dense_out);

  phi::UnchangedInferMeta(meta_x, &meta_out);

  std::vector<pir::Type> argument_outputs;
  pir::Type out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_out.dtype()), dense_out.dims(), dense_out.layout(), dense_out.lod(), dense_out.offset());
  argument_outputs.push_back(out_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void GeluOp::VerifySig() {
  VLOG(4) << "Start Verifying inputs, outputs and attributes for: GeluOp.";
  VLOG(4) << "Verifying inputs:";
  {
  auto input_size = num_operands();
  IR_ENFORCE(input_size == 1u,
                    "The size %d of inputs must be equal to 1.", input_size);
  IR_ENFORCE((*this)->operand_source(0).type().isa<paddle::dialect::DenseTensorType>(),
                  "Type validation failed for the 0th input, got %s.", (*this)->operand_source(0).type());
  }
  VLOG(4) << "Verifying attributes:";
  {
  auto& attributes = this->attributes();
  IR_ENFORCE(attributes.count("approximate")>0,
                 "approximate does not exist.");
  IR_ENFORCE(attributes.at("approximate").isa<pir::BoolAttribute>(),
                 "Type of attribute: approximate is not pir::BoolAttribute.");

  }
  VLOG(4) << "Verifying outputs:";
  {
  auto output_size = num_results();
  IR_ENFORCE(output_size == 1u,
                    "The size %d of outputs must be equal to 1.", output_size);
  IR_ENFORCE((*this)->result(0).type().isa<paddle::dialect::DenseTensorType>(),
                 "Type validation failed for the 0th output.");
  }
  VLOG(4) << "End Verifying for: GeluOp.";
}

void GeluOp::InferMeta( phi::InferMetaContext *infer_meta ) {
  auto fn = PD_INFER_META(phi::UnchangedInferMeta);
  fn(infer_meta);
}

phi::DataType GeluOp::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: GeluOp";
  


  return expected_kernel_dtype;
}

const char *GenerateProposalsOp::attributes_name[6] = { "pre_nms_top_n", "post_nms_top_n", "nms_thresh", "min_size", "eta", "pixel_offset" };

OpInfoTuple GenerateProposalsOp::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("scores", "paddle::dialect::DenseTensorType", false, false, false, false), paddle::dialect::OpInputInfo("bbox_deltas", "paddle::dialect::DenseTensorType", false, false, false, false), paddle::dialect::OpInputInfo("im_shape", "paddle::dialect::DenseTensorType", false, false, false, false), paddle::dialect::OpInputInfo("anchors", "paddle::dialect::DenseTensorType", false, false, false, false), paddle::dialect::OpInputInfo("variances", "paddle::dialect::DenseTensorType", false, false, false, false) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = { paddle::dialect::OpAttributeInfo("pre_nms_top_n", "pir::Int32Attribute", ""), paddle::dialect::OpAttributeInfo("post_nms_top_n", "pir::Int32Attribute", ""), paddle::dialect::OpAttributeInfo("nms_thresh", "pir::FloatAttribute", ""), paddle::dialect::OpAttributeInfo("min_size", "pir::FloatAttribute", ""), paddle::dialect::OpAttributeInfo("eta", "pir::FloatAttribute", ""), paddle::dialect::OpAttributeInfo("pixel_offset", "pir::BoolAttribute", "") };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("rpn_rois", "paddle::dialect::DenseTensorType", false, false), paddle::dialect::OpOutputInfo("rpn_roi_probs", "paddle::dialect::DenseTensorType", false, false), paddle::dialect::OpOutputInfo("rpn_rois_num", "paddle::dialect::DenseTensorType", true, false) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("GenerateProposalsV2InferMeta", {"scores", "bbox_deltas", "im_shape", "anchors", "variances", "pre_nms_top_n", "post_nms_top_n", "nms_thresh", "min_size", "eta", "pixel_offset"}, "generate_proposals", {"scores", "bbox_deltas", "im_shape", "anchors", "variances", "pre_nms_top_n", "post_nms_top_n", "nms_thresh", "min_size", "eta", "pixel_offset"}, {"anchors"}, {}, {}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "generate_proposals");
}

void GenerateProposalsOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value scores_, pir::Value bbox_deltas_, pir::Value im_shape_, pir::Value anchors_, pir::Value variances_, int pre_nms_top_n, int post_nms_top_n, float nms_thresh, float min_size, float eta, bool pixel_offset) {
  VLOG(4) << "Start build GenerateProposalsOp";



  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {scores_, bbox_deltas_, im_shape_, anchors_, variances_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_pre_nms_top_n = pir::Int32Attribute::get(pir::IrContext::Instance(), pre_nms_top_n);
  argument.AddAttribute("pre_nms_top_n", attr_pre_nms_top_n);
  pir::Attribute attr_post_nms_top_n = pir::Int32Attribute::get(pir::IrContext::Instance(), post_nms_top_n);
  argument.AddAttribute("post_nms_top_n", attr_post_nms_top_n);
  pir::Attribute attr_nms_thresh = pir::FloatAttribute::get(pir::IrContext::Instance(), nms_thresh);
  argument.AddAttribute("nms_thresh", attr_nms_thresh);
  pir::Attribute attr_min_size = pir::FloatAttribute::get(pir::IrContext::Instance(), min_size);
  argument.AddAttribute("min_size", attr_min_size);
  pir::Attribute attr_eta = pir::FloatAttribute::get(pir::IrContext::Instance(), eta);
  argument.AddAttribute("eta", attr_eta);
  pir::Attribute attr_pixel_offset = pir::BoolAttribute::get(pir::IrContext::Instance(), pixel_offset);
  argument.AddAttribute("pixel_offset", attr_pixel_offset);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType scores = scores_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)scores;
  paddle::dialect::DenseTensorType bbox_deltas = bbox_deltas_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)bbox_deltas;
  paddle::dialect::DenseTensorType im_shape = im_shape_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)im_shape;
  paddle::dialect::DenseTensorType anchors = anchors_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)anchors;
  paddle::dialect::DenseTensorType variances = variances_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)variances;

  VLOG(4) << "Builder construction  dense_scores";
  paddle::dialect::IrTensor ir_tensor_scores(paddle::dialect::TransToPhiDataType(scores.dtype()),
                                                      scores.dims(),
                                                      scores.data_layout(),
                                                      scores.lod(),
                                                      scores.offset());
  VLOG(4) << "Builder construction  meta_scores";
  paddle::dialect::IrMetaTensor meta_scores(&ir_tensor_scores);

  VLOG(4) << "Builder construction  dense_bbox_deltas";
  paddle::dialect::IrTensor ir_tensor_bbox_deltas(paddle::dialect::TransToPhiDataType(bbox_deltas.dtype()),
                                                      bbox_deltas.dims(),
                                                      bbox_deltas.data_layout(),
                                                      bbox_deltas.lod(),
                                                      bbox_deltas.offset());
  VLOG(4) << "Builder construction  meta_bbox_deltas";
  paddle::dialect::IrMetaTensor meta_bbox_deltas(&ir_tensor_bbox_deltas);

  VLOG(4) << "Builder construction  dense_im_shape";
  paddle::dialect::IrTensor ir_tensor_im_shape(paddle::dialect::TransToPhiDataType(im_shape.dtype()),
                                                      im_shape.dims(),
                                                      im_shape.data_layout(),
                                                      im_shape.lod(),
                                                      im_shape.offset());
  VLOG(4) << "Builder construction  meta_im_shape";
  paddle::dialect::IrMetaTensor meta_im_shape(&ir_tensor_im_shape);

  VLOG(4) << "Builder construction  dense_anchors";
  paddle::dialect::IrTensor ir_tensor_anchors(paddle::dialect::TransToPhiDataType(anchors.dtype()),
                                                      anchors.dims(),
                                                      anchors.data_layout(),
                                                      anchors.lod(),
                                                      anchors.offset());
  VLOG(4) << "Builder construction  meta_anchors";
  paddle::dialect::IrMetaTensor meta_anchors(&ir_tensor_anchors);

  VLOG(4) << "Builder construction  dense_variances";
  paddle::dialect::IrTensor ir_tensor_variances(paddle::dialect::TransToPhiDataType(variances.dtype()),
                                                      variances.dims(),
                                                      variances.data_layout(),
                                                      variances.lod(),
                                                      variances.offset());
  VLOG(4) << "Builder construction  meta_variances";
  paddle::dialect::IrMetaTensor meta_variances(&ir_tensor_variances);
  paddle::dialect::IrTensor dense_rpn_rois;
  paddle::dialect::IrMetaTensor meta_rpn_rois(&dense_rpn_rois);
  paddle::dialect::IrTensor dense_rpn_roi_probs;
  paddle::dialect::IrMetaTensor meta_rpn_roi_probs(&dense_rpn_roi_probs);
  paddle::dialect::IrTensor dense_rpn_rois_num;
  paddle::dialect::IrMetaTensor meta_rpn_rois_num(&dense_rpn_rois_num);

  phi::GenerateProposalsV2InferMeta(meta_scores, meta_bbox_deltas, meta_im_shape, meta_anchors, meta_variances, pre_nms_top_n, post_nms_top_n, nms_thresh, min_size, eta, pixel_offset, &meta_rpn_rois, &meta_rpn_roi_probs, &meta_rpn_rois_num);

  std::vector<pir::Type> argument_outputs;
  pir::Type rpn_rois_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_rpn_rois.dtype()), dense_rpn_rois.dims(), dense_rpn_rois.layout(), dense_rpn_rois.lod(), dense_rpn_rois.offset());
  argument_outputs.push_back(rpn_rois_dense_tensor_type);

  pir::Type rpn_roi_probs_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_rpn_roi_probs.dtype()), dense_rpn_roi_probs.dims(), dense_rpn_roi_probs.layout(), dense_rpn_roi_probs.lod(), dense_rpn_roi_probs.offset());
  argument_outputs.push_back(rpn_roi_probs_dense_tensor_type);

  pir::Type rpn_rois_num_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_rpn_rois_num.dtype()), dense_rpn_rois_num.dims(), dense_rpn_rois_num.layout(), dense_rpn_rois_num.lod(), dense_rpn_rois_num.offset());
  argument_outputs.push_back(rpn_rois_num_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void GenerateProposalsOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value scores_, pir::Value bbox_deltas_, pir::Value im_shape_, pir::Value anchors_, pir::Value variances_, pir::AttributeMap attributes) {
  VLOG(4) << "Start build GenerateProposalsOp";


  IR_ENFORCE(
      attributes.find("pre_nms_top_n") != attributes.end(),
          "'pre_nms_top_n' Attribute is expected for GenerateProposalsOp. ");
  int pre_nms_top_n = attributes.at("pre_nms_top_n").dyn_cast<pir::Int32Attribute>().data();

  IR_ENFORCE(
      attributes.find("post_nms_top_n") != attributes.end(),
          "'post_nms_top_n' Attribute is expected for GenerateProposalsOp. ");
  int post_nms_top_n = attributes.at("post_nms_top_n").dyn_cast<pir::Int32Attribute>().data();

  IR_ENFORCE(
      attributes.find("nms_thresh") != attributes.end(),
          "'nms_thresh' Attribute is expected for GenerateProposalsOp. ");
  float nms_thresh = attributes.at("nms_thresh").dyn_cast<pir::FloatAttribute>().data();

  IR_ENFORCE(
      attributes.find("min_size") != attributes.end(),
          "'min_size' Attribute is expected for GenerateProposalsOp. ");
  float min_size = attributes.at("min_size").dyn_cast<pir::FloatAttribute>().data();

  IR_ENFORCE(
      attributes.find("eta") != attributes.end(),
          "'eta' Attribute is expected for GenerateProposalsOp. ");
  float eta = attributes.at("eta").dyn_cast<pir::FloatAttribute>().data();

  IR_ENFORCE(
      attributes.find("pixel_offset") != attributes.end(),
          "'pixel_offset' Attribute is expected for GenerateProposalsOp. ");
  bool pixel_offset = attributes.at("pixel_offset").dyn_cast<pir::BoolAttribute>().data();


  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {scores_, bbox_deltas_, im_shape_, anchors_, variances_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_pre_nms_top_n = pir::Int32Attribute::get(pir::IrContext::Instance(), pre_nms_top_n);
  argument.AddAttribute("pre_nms_top_n", attr_pre_nms_top_n);
  pir::Attribute attr_post_nms_top_n = pir::Int32Attribute::get(pir::IrContext::Instance(), post_nms_top_n);
  argument.AddAttribute("post_nms_top_n", attr_post_nms_top_n);
  pir::Attribute attr_nms_thresh = pir::FloatAttribute::get(pir::IrContext::Instance(), nms_thresh);
  argument.AddAttribute("nms_thresh", attr_nms_thresh);
  pir::Attribute attr_min_size = pir::FloatAttribute::get(pir::IrContext::Instance(), min_size);
  argument.AddAttribute("min_size", attr_min_size);
  pir::Attribute attr_eta = pir::FloatAttribute::get(pir::IrContext::Instance(), eta);
  argument.AddAttribute("eta", attr_eta);
  pir::Attribute attr_pixel_offset = pir::BoolAttribute::get(pir::IrContext::Instance(), pixel_offset);
  argument.AddAttribute("pixel_offset", attr_pixel_offset);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType scores = scores_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)scores;
  paddle::dialect::DenseTensorType bbox_deltas = bbox_deltas_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)bbox_deltas;
  paddle::dialect::DenseTensorType im_shape = im_shape_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)im_shape;
  paddle::dialect::DenseTensorType anchors = anchors_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)anchors;
  paddle::dialect::DenseTensorType variances = variances_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)variances;

  VLOG(4) << "Builder construction  dense_scores";
  paddle::dialect::IrTensor ir_tensor_scores(paddle::dialect::TransToPhiDataType(scores.dtype()),
                                                      scores.dims(),
                                                      scores.data_layout(),
                                                      scores.lod(),
                                                      scores.offset());
  VLOG(4) << "Builder construction  meta_scores";
  paddle::dialect::IrMetaTensor meta_scores(&ir_tensor_scores);

  VLOG(4) << "Builder construction  dense_bbox_deltas";
  paddle::dialect::IrTensor ir_tensor_bbox_deltas(paddle::dialect::TransToPhiDataType(bbox_deltas.dtype()),
                                                      bbox_deltas.dims(),
                                                      bbox_deltas.data_layout(),
                                                      bbox_deltas.lod(),
                                                      bbox_deltas.offset());
  VLOG(4) << "Builder construction  meta_bbox_deltas";
  paddle::dialect::IrMetaTensor meta_bbox_deltas(&ir_tensor_bbox_deltas);

  VLOG(4) << "Builder construction  dense_im_shape";
  paddle::dialect::IrTensor ir_tensor_im_shape(paddle::dialect::TransToPhiDataType(im_shape.dtype()),
                                                      im_shape.dims(),
                                                      im_shape.data_layout(),
                                                      im_shape.lod(),
                                                      im_shape.offset());
  VLOG(4) << "Builder construction  meta_im_shape";
  paddle::dialect::IrMetaTensor meta_im_shape(&ir_tensor_im_shape);

  VLOG(4) << "Builder construction  dense_anchors";
  paddle::dialect::IrTensor ir_tensor_anchors(paddle::dialect::TransToPhiDataType(anchors.dtype()),
                                                      anchors.dims(),
                                                      anchors.data_layout(),
                                                      anchors.lod(),
                                                      anchors.offset());
  VLOG(4) << "Builder construction  meta_anchors";
  paddle::dialect::IrMetaTensor meta_anchors(&ir_tensor_anchors);

  VLOG(4) << "Builder construction  dense_variances";
  paddle::dialect::IrTensor ir_tensor_variances(paddle::dialect::TransToPhiDataType(variances.dtype()),
                                                      variances.dims(),
                                                      variances.data_layout(),
                                                      variances.lod(),
                                                      variances.offset());
  VLOG(4) << "Builder construction  meta_variances";
  paddle::dialect::IrMetaTensor meta_variances(&ir_tensor_variances);
  paddle::dialect::IrTensor dense_rpn_rois;
  paddle::dialect::IrMetaTensor meta_rpn_rois(&dense_rpn_rois);
  paddle::dialect::IrTensor dense_rpn_roi_probs;
  paddle::dialect::IrMetaTensor meta_rpn_roi_probs(&dense_rpn_roi_probs);
  paddle::dialect::IrTensor dense_rpn_rois_num;
  paddle::dialect::IrMetaTensor meta_rpn_rois_num(&dense_rpn_rois_num);

  phi::GenerateProposalsV2InferMeta(meta_scores, meta_bbox_deltas, meta_im_shape, meta_anchors, meta_variances, pre_nms_top_n, post_nms_top_n, nms_thresh, min_size, eta, pixel_offset, &meta_rpn_rois, &meta_rpn_roi_probs, &meta_rpn_rois_num);

  std::vector<pir::Type> argument_outputs;
  pir::Type rpn_rois_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_rpn_rois.dtype()), dense_rpn_rois.dims(), dense_rpn_rois.layout(), dense_rpn_rois.lod(), dense_rpn_rois.offset());
  argument_outputs.push_back(rpn_rois_dense_tensor_type);

  pir::Type rpn_roi_probs_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_rpn_roi_probs.dtype()), dense_rpn_roi_probs.dims(), dense_rpn_roi_probs.layout(), dense_rpn_roi_probs.lod(), dense_rpn_roi_probs.offset());
  argument_outputs.push_back(rpn_roi_probs_dense_tensor_type);

  pir::Type rpn_rois_num_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_rpn_rois_num.dtype()), dense_rpn_rois_num.dims(), dense_rpn_rois_num.layout(), dense_rpn_rois_num.lod(), dense_rpn_rois_num.offset());
  argument_outputs.push_back(rpn_rois_num_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void GenerateProposalsOp::VerifySig() {
  VLOG(4) << "Start Verifying inputs, outputs and attributes for: GenerateProposalsOp.";
  VLOG(4) << "Verifying inputs:";
  {
  auto input_size = num_operands();
  IR_ENFORCE(input_size == 5u,
                    "The size %d of inputs must be equal to 5.", input_size);
  IR_ENFORCE((*this)->operand_source(0).type().isa<paddle::dialect::DenseTensorType>(),
                  "Type validation failed for the 0th input, got %s.", (*this)->operand_source(0).type());
  IR_ENFORCE((*this)->operand_source(1).type().isa<paddle::dialect::DenseTensorType>(),
                  "Type validation failed for the 1th input, got %s.", (*this)->operand_source(1).type());
  IR_ENFORCE((*this)->operand_source(2).type().isa<paddle::dialect::DenseTensorType>(),
                  "Type validation failed for the 2th input, got %s.", (*this)->operand_source(2).type());
  IR_ENFORCE((*this)->operand_source(3).type().isa<paddle::dialect::DenseTensorType>(),
                  "Type validation failed for the 3th input, got %s.", (*this)->operand_source(3).type());
  IR_ENFORCE((*this)->operand_source(4).type().isa<paddle::dialect::DenseTensorType>(),
                  "Type validation failed for the 4th input, got %s.", (*this)->operand_source(4).type());
  }
  VLOG(4) << "Verifying attributes:";
  {
  auto& attributes = this->attributes();
  IR_ENFORCE(attributes.count("pre_nms_top_n")>0,
                 "pre_nms_top_n does not exist.");
  IR_ENFORCE(attributes.at("pre_nms_top_n").isa<pir::Int32Attribute>(),
                 "Type of attribute: pre_nms_top_n is not pir::Int32Attribute.");

  IR_ENFORCE(attributes.count("post_nms_top_n")>0,
                 "post_nms_top_n does not exist.");
  IR_ENFORCE(attributes.at("post_nms_top_n").isa<pir::Int32Attribute>(),
                 "Type of attribute: post_nms_top_n is not pir::Int32Attribute.");

  IR_ENFORCE(attributes.count("nms_thresh")>0,
                 "nms_thresh does not exist.");
  IR_ENFORCE(attributes.at("nms_thresh").isa<pir::FloatAttribute>(),
                 "Type of attribute: nms_thresh is not pir::FloatAttribute.");

  IR_ENFORCE(attributes.count("min_size")>0,
                 "min_size does not exist.");
  IR_ENFORCE(attributes.at("min_size").isa<pir::FloatAttribute>(),
                 "Type of attribute: min_size is not pir::FloatAttribute.");

  IR_ENFORCE(attributes.count("eta")>0,
                 "eta does not exist.");
  IR_ENFORCE(attributes.at("eta").isa<pir::FloatAttribute>(),
                 "Type of attribute: eta is not pir::FloatAttribute.");

  IR_ENFORCE(attributes.count("pixel_offset")>0,
                 "pixel_offset does not exist.");
  IR_ENFORCE(attributes.at("pixel_offset").isa<pir::BoolAttribute>(),
                 "Type of attribute: pixel_offset is not pir::BoolAttribute.");

  }
  VLOG(4) << "Verifying outputs:";
  {
  auto output_size = num_results();
  IR_ENFORCE(output_size == 3u,
                    "The size %d of outputs must be equal to 3.", output_size);
  IR_ENFORCE((*this)->result(0).type().isa<paddle::dialect::DenseTensorType>(),
                 "Type validation failed for the 0th output.");
  IR_ENFORCE((*this)->result(1).type().isa<paddle::dialect::DenseTensorType>(),
                 "Type validation failed for the 1th output.");
  if (auto output_2_type = (*this)->result(2).type()) {
    IR_ENFORCE(output_2_type.isa<paddle::dialect::DenseTensorType>(),
                   "Type validation failed for the 2th output.");
  }
  }
  VLOG(4) << "End Verifying for: GenerateProposalsOp.";
}

void GenerateProposalsOp::InferMeta( phi::InferMetaContext *infer_meta ) {
  auto fn = PD_INFER_META(phi::GenerateProposalsV2InferMeta);
  fn(infer_meta);
}

phi::DataType GenerateProposalsOp::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: GenerateProposalsOp";
  


  return expected_kernel_dtype;
}

const char *GridSampleOp::attributes_name[3] = { "mode", "padding_mode", "align_corners" };

OpInfoTuple GridSampleOp::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("x", "paddle::dialect::DenseTensorType", false, false, false, true), paddle::dialect::OpInputInfo("grid", "paddle::dialect::DenseTensorType", false, false, false, true) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = { paddle::dialect::OpAttributeInfo("mode", "pir::StrAttribute", ""), paddle::dialect::OpAttributeInfo("padding_mode", "pir::StrAttribute", ""), paddle::dialect::OpAttributeInfo("align_corners", "pir::BoolAttribute", "") };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("out", "paddle::dialect::DenseTensorType", false, false) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("GridSampleBaseInferMeta", {"x", "grid"}, "grid_sample", {"x", "grid", "mode", "padding_mode", "align_corners"}, {"x"}, {}, {}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "grid_sample");
}

void GridSampleOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, pir::Value grid_, const std::string& mode, const std::string& padding_mode, bool align_corners) {
  VLOG(4) << "Start build GridSampleOp";



  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_, grid_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_mode = pir::StrAttribute::get(pir::IrContext::Instance(), mode);
  argument.AddAttribute("mode", attr_mode);
  pir::Attribute attr_padding_mode = pir::StrAttribute::get(pir::IrContext::Instance(), padding_mode);
  argument.AddAttribute("padding_mode", attr_padding_mode);
  pir::Attribute attr_align_corners = pir::BoolAttribute::get(pir::IrContext::Instance(), align_corners);
  argument.AddAttribute("align_corners", attr_align_corners);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;
  paddle::dialect::DenseTensorType grid = grid_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)grid;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);

  VLOG(4) << "Builder construction  dense_grid";
  paddle::dialect::IrTensor ir_tensor_grid(paddle::dialect::TransToPhiDataType(grid.dtype()),
                                                      grid.dims(),
                                                      grid.data_layout(),
                                                      grid.lod(),
                                                      grid.offset());
  VLOG(4) << "Builder construction  meta_grid";
  paddle::dialect::IrMetaTensor meta_grid(&ir_tensor_grid);
  paddle::dialect::IrTensor dense_out;
  paddle::dialect::IrMetaTensor meta_out(&dense_out);

  phi::GridSampleBaseInferMeta(meta_x, meta_grid, &meta_out);

  std::vector<pir::Type> argument_outputs;
  pir::Type out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_out.dtype()), dense_out.dims(), dense_out.layout(), dense_out.lod(), dense_out.offset());
  argument_outputs.push_back(out_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void GridSampleOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, pir::Value grid_, pir::AttributeMap attributes) {
  VLOG(4) << "Start build GridSampleOp";


  IR_ENFORCE(
      attributes.find("mode") != attributes.end(),
          "'mode' Attribute is expected for GridSampleOp. ");
  std::string mode = attributes.at("mode").dyn_cast<pir::StrAttribute>().AsString();

  IR_ENFORCE(
      attributes.find("padding_mode") != attributes.end(),
          "'padding_mode' Attribute is expected for GridSampleOp. ");
  std::string padding_mode = attributes.at("padding_mode").dyn_cast<pir::StrAttribute>().AsString();

  IR_ENFORCE(
      attributes.find("align_corners") != attributes.end(),
          "'align_corners' Attribute is expected for GridSampleOp. ");
  bool align_corners = attributes.at("align_corners").dyn_cast<pir::BoolAttribute>().data();


  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_, grid_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_mode = pir::StrAttribute::get(pir::IrContext::Instance(), mode);
  argument.AddAttribute("mode", attr_mode);
  pir::Attribute attr_padding_mode = pir::StrAttribute::get(pir::IrContext::Instance(), padding_mode);
  argument.AddAttribute("padding_mode", attr_padding_mode);
  pir::Attribute attr_align_corners = pir::BoolAttribute::get(pir::IrContext::Instance(), align_corners);
  argument.AddAttribute("align_corners", attr_align_corners);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;
  paddle::dialect::DenseTensorType grid = grid_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)grid;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);

  VLOG(4) << "Builder construction  dense_grid";
  paddle::dialect::IrTensor ir_tensor_grid(paddle::dialect::TransToPhiDataType(grid.dtype()),
                                                      grid.dims(),
                                                      grid.data_layout(),
                                                      grid.lod(),
                                                      grid.offset());
  VLOG(4) << "Builder construction  meta_grid";
  paddle::dialect::IrMetaTensor meta_grid(&ir_tensor_grid);
  paddle::dialect::IrTensor dense_out;
  paddle::dialect::IrMetaTensor meta_out(&dense_out);

  phi::GridSampleBaseInferMeta(meta_x, meta_grid, &meta_out);

  std::vector<pir::Type> argument_outputs;
  pir::Type out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_out.dtype()), dense_out.dims(), dense_out.layout(), dense_out.lod(), dense_out.offset());
  argument_outputs.push_back(out_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void GridSampleOp::VerifySig() {
  VLOG(4) << "Start Verifying inputs, outputs and attributes for: GridSampleOp.";
  VLOG(4) << "Verifying inputs:";
  {
  auto input_size = num_operands();
  IR_ENFORCE(input_size == 2u,
                    "The size %d of inputs must be equal to 2.", input_size);
  IR_ENFORCE((*this)->operand_source(0).type().isa<paddle::dialect::DenseTensorType>(),
                  "Type validation failed for the 0th input, got %s.", (*this)->operand_source(0).type());
  IR_ENFORCE((*this)->operand_source(1).type().isa<paddle::dialect::DenseTensorType>(),
                  "Type validation failed for the 1th input, got %s.", (*this)->operand_source(1).type());
  }
  VLOG(4) << "Verifying attributes:";
  {
  auto& attributes = this->attributes();
  IR_ENFORCE(attributes.count("mode")>0,
                 "mode does not exist.");
  IR_ENFORCE(attributes.at("mode").isa<pir::StrAttribute>(),
                 "Type of attribute: mode is not pir::StrAttribute.");

  IR_ENFORCE(attributes.count("padding_mode")>0,
                 "padding_mode does not exist.");
  IR_ENFORCE(attributes.at("padding_mode").isa<pir::StrAttribute>(),
                 "Type of attribute: padding_mode is not pir::StrAttribute.");

  IR_ENFORCE(attributes.count("align_corners")>0,
                 "align_corners does not exist.");
  IR_ENFORCE(attributes.at("align_corners").isa<pir::BoolAttribute>(),
                 "Type of attribute: align_corners is not pir::BoolAttribute.");

  }
  VLOG(4) << "Verifying outputs:";
  {
  auto output_size = num_results();
  IR_ENFORCE(output_size == 1u,
                    "The size %d of outputs must be equal to 1.", output_size);
  IR_ENFORCE((*this)->result(0).type().isa<paddle::dialect::DenseTensorType>(),
                 "Type validation failed for the 0th output.");
  }
  VLOG(4) << "End Verifying for: GridSampleOp.";
}

void GridSampleOp::InferMeta( phi::InferMetaContext *infer_meta ) {
  auto fn = PD_INFER_META(phi::GridSampleBaseInferMeta);
  fn(infer_meta);
}

phi::DataType GridSampleOp::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: GridSampleOp";
  


  return expected_kernel_dtype;
}

const char *GroupNormOp::attributes_name[3] = { "epsilon", "groups", "data_layout" };

OpInfoTuple GroupNormOp::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("x", "paddle::dialect::DenseTensorType", false, false, false, true), paddle::dialect::OpInputInfo("scale", "paddle::dialect::DenseTensorType", true, false, false, true), paddle::dialect::OpInputInfo("bias", "paddle::dialect::DenseTensorType", true, false, false, true) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = { paddle::dialect::OpAttributeInfo("epsilon", "pir::FloatAttribute", ""), paddle::dialect::OpAttributeInfo("groups", "pir::Int32Attribute", ""), paddle::dialect::OpAttributeInfo("data_layout", "pir::StrAttribute", "") };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("y", "paddle::dialect::DenseTensorType", false, false), paddle::dialect::OpOutputInfo("mean", "paddle::dialect::DenseTensorType", true, true), paddle::dialect::OpOutputInfo("variance", "paddle::dialect::DenseTensorType", true, true) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("GroupNormInferMeta", {"x", "scale", "bias", "epsilon", "groups", "data_layout"}, "group_norm", {"x", "scale", "bias", "epsilon", "groups", "data_layout"}, {}, {}, {}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "group_norm");
}

void GroupNormOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, pir::Value scale_, pir::Value bias_, float epsilon, int groups, const std::string& data_layout) {
  VLOG(4) << "Start build GroupNormOp";



  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_, scale_, bias_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_epsilon = pir::FloatAttribute::get(pir::IrContext::Instance(), epsilon);
  argument.AddAttribute("epsilon", attr_epsilon);
  pir::Attribute attr_groups = pir::Int32Attribute::get(pir::IrContext::Instance(), groups);
  argument.AddAttribute("groups", attr_groups);
  pir::Attribute attr_data_layout = pir::StrAttribute::get(pir::IrContext::Instance(), data_layout);
  argument.AddAttribute("data_layout", attr_data_layout);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);

  paddle::dialect::IrMetaTensor meta_scale;
  paddle::dialect::IrTensor ir_tensor_scale;
  if (scale_.impl() != nullptr) {
    paddle::dialect::DenseTensorType scale = scale_.type().dyn_cast<paddle::dialect::DenseTensorType>();
    VLOG(4) << "Builder construction  dense_scale";
    ir_tensor_scale = paddle::dialect::IrTensor(paddle::dialect::TransToPhiDataType(scale.dtype()),
                                                        scale.dims(),
                                                        scale.data_layout(),
                                                        scale.lod(),
                                                        scale.offset());
    VLOG(4) << "Builder construction  meta_scale";
    meta_scale = paddle::dialect::IrMetaTensor(&ir_tensor_scale);
  }


  paddle::dialect::IrMetaTensor meta_bias;
  paddle::dialect::IrTensor ir_tensor_bias;
  if (bias_.impl() != nullptr) {
    paddle::dialect::DenseTensorType bias = bias_.type().dyn_cast<paddle::dialect::DenseTensorType>();
    VLOG(4) << "Builder construction  dense_bias";
    ir_tensor_bias = paddle::dialect::IrTensor(paddle::dialect::TransToPhiDataType(bias.dtype()),
                                                        bias.dims(),
                                                        bias.data_layout(),
                                                        bias.lod(),
                                                        bias.offset());
    VLOG(4) << "Builder construction  meta_bias";
    meta_bias = paddle::dialect::IrMetaTensor(&ir_tensor_bias);
  }

  paddle::dialect::IrTensor dense_y;
  paddle::dialect::IrMetaTensor meta_y(&dense_y);
  paddle::dialect::IrTensor dense_mean;
  paddle::dialect::IrMetaTensor meta_mean(&dense_mean);
  paddle::dialect::IrTensor dense_variance;
  paddle::dialect::IrMetaTensor meta_variance(&dense_variance);

  phi::GroupNormInferMeta(meta_x, meta_scale, meta_bias, epsilon, groups, data_layout, &meta_y, &meta_mean, &meta_variance);

  std::vector<pir::Type> argument_outputs;
  pir::Type y_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_y.dtype()), dense_y.dims(), dense_y.layout(), dense_y.lod(), dense_y.offset());
  argument_outputs.push_back(y_dense_tensor_type);

  pir::Type mean_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_mean.dtype()), dense_mean.dims(), dense_mean.layout(), dense_mean.lod(), dense_mean.offset());
  argument_outputs.push_back(mean_dense_tensor_type);

  pir::Type variance_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_variance.dtype()), dense_variance.dims(), dense_variance.layout(), dense_variance.lod(), dense_variance.offset());
  argument_outputs.push_back(variance_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void GroupNormOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, pir::Value scale_, pir::Value bias_, pir::AttributeMap attributes) {
  VLOG(4) << "Start build GroupNormOp";


  IR_ENFORCE(
      attributes.find("epsilon") != attributes.end(),
          "'epsilon' Attribute is expected for GroupNormOp. ");
  float epsilon = attributes.at("epsilon").dyn_cast<pir::FloatAttribute>().data();

  IR_ENFORCE(
      attributes.find("groups") != attributes.end(),
          "'groups' Attribute is expected for GroupNormOp. ");
  int groups = attributes.at("groups").dyn_cast<pir::Int32Attribute>().data();

  IR_ENFORCE(
      attributes.find("data_layout") != attributes.end(),
          "'data_layout' Attribute is expected for GroupNormOp. ");
  std::string data_layout = attributes.at("data_layout").dyn_cast<pir::StrAttribute>().AsString();


  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_, scale_, bias_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_epsilon = pir::FloatAttribute::get(pir::IrContext::Instance(), epsilon);
  argument.AddAttribute("epsilon", attr_epsilon);
  pir::Attribute attr_groups = pir::Int32Attribute::get(pir::IrContext::Instance(), groups);
  argument.AddAttribute("groups", attr_groups);
  pir::Attribute attr_data_layout = pir::StrAttribute::get(pir::IrContext::Instance(), data_layout);
  argument.AddAttribute("data_layout", attr_data_layout);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);

  paddle::dialect::IrMetaTensor meta_scale;
  paddle::dialect::IrTensor ir_tensor_scale;
  if (scale_.impl() != nullptr) {
    paddle::dialect::DenseTensorType scale = scale_.type().dyn_cast<paddle::dialect::DenseTensorType>();
    VLOG(4) << "Builder construction  dense_scale";
    ir_tensor_scale = paddle::dialect::IrTensor(paddle::dialect::TransToPhiDataType(scale.dtype()),
                                                        scale.dims(),
                                                        scale.data_layout(),
                                                        scale.lod(),
                                                        scale.offset());
    VLOG(4) << "Builder construction  meta_scale";
    meta_scale = paddle::dialect::IrMetaTensor(&ir_tensor_scale);
  }


  paddle::dialect::IrMetaTensor meta_bias;
  paddle::dialect::IrTensor ir_tensor_bias;
  if (bias_.impl() != nullptr) {
    paddle::dialect::DenseTensorType bias = bias_.type().dyn_cast<paddle::dialect::DenseTensorType>();
    VLOG(4) << "Builder construction  dense_bias";
    ir_tensor_bias = paddle::dialect::IrTensor(paddle::dialect::TransToPhiDataType(bias.dtype()),
                                                        bias.dims(),
                                                        bias.data_layout(),
                                                        bias.lod(),
                                                        bias.offset());
    VLOG(4) << "Builder construction  meta_bias";
    meta_bias = paddle::dialect::IrMetaTensor(&ir_tensor_bias);
  }

  paddle::dialect::IrTensor dense_y;
  paddle::dialect::IrMetaTensor meta_y(&dense_y);
  paddle::dialect::IrTensor dense_mean;
  paddle::dialect::IrMetaTensor meta_mean(&dense_mean);
  paddle::dialect::IrTensor dense_variance;
  paddle::dialect::IrMetaTensor meta_variance(&dense_variance);

  phi::GroupNormInferMeta(meta_x, meta_scale, meta_bias, epsilon, groups, data_layout, &meta_y, &meta_mean, &meta_variance);

  std::vector<pir::Type> argument_outputs;
  pir::Type y_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_y.dtype()), dense_y.dims(), dense_y.layout(), dense_y.lod(), dense_y.offset());
  argument_outputs.push_back(y_dense_tensor_type);

  pir::Type mean_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_mean.dtype()), dense_mean.dims(), dense_mean.layout(), dense_mean.lod(), dense_mean.offset());
  argument_outputs.push_back(mean_dense_tensor_type);

  pir::Type variance_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_variance.dtype()), dense_variance.dims(), dense_variance.layout(), dense_variance.lod(), dense_variance.offset());
  argument_outputs.push_back(variance_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void GroupNormOp::VerifySig() {
  VLOG(4) << "Start Verifying inputs, outputs and attributes for: GroupNormOp.";
  VLOG(4) << "Verifying inputs:";
  {
  auto input_size = num_operands();
  IR_ENFORCE(input_size == 3u,
                    "The size %d of inputs must be equal to 3.", input_size);
  IR_ENFORCE((*this)->operand_source(0).type().isa<paddle::dialect::DenseTensorType>(),
                  "Type validation failed for the 0th input, got %s.", (*this)->operand_source(0).type());
  if (auto val = (*this)->operand(1)) {
    IR_ENFORCE(val.type().isa<paddle::dialect::DenseTensorType>(),
                   "Type validation failed for the 1th input, got %s.", (*this)->operand_source(1).type());
  }
  if (auto val = (*this)->operand(2)) {
    IR_ENFORCE(val.type().isa<paddle::dialect::DenseTensorType>(),
                   "Type validation failed for the 2th input, got %s.", (*this)->operand_source(2).type());
  }
  }
  VLOG(4) << "Verifying attributes:";
  {
  auto& attributes = this->attributes();
  IR_ENFORCE(attributes.count("epsilon")>0,
                 "epsilon does not exist.");
  IR_ENFORCE(attributes.at("epsilon").isa<pir::FloatAttribute>(),
                 "Type of attribute: epsilon is not pir::FloatAttribute.");

  IR_ENFORCE(attributes.count("groups")>0,
                 "groups does not exist.");
  IR_ENFORCE(attributes.at("groups").isa<pir::Int32Attribute>(),
                 "Type of attribute: groups is not pir::Int32Attribute.");

  IR_ENFORCE(attributes.count("data_layout")>0,
                 "data_layout does not exist.");
  IR_ENFORCE(attributes.at("data_layout").isa<pir::StrAttribute>(),
                 "Type of attribute: data_layout is not pir::StrAttribute.");

  }
  VLOG(4) << "Verifying outputs:";
  {
  auto output_size = num_results();
  IR_ENFORCE(output_size == 3u,
                    "The size %d of outputs must be equal to 3.", output_size);
  IR_ENFORCE((*this)->result(0).type().isa<paddle::dialect::DenseTensorType>(),
                 "Type validation failed for the 0th output.");
  if (auto output_1_type = (*this)->result(1).type()) {
    IR_ENFORCE(output_1_type.isa<paddle::dialect::DenseTensorType>(),
                   "Type validation failed for the 1th output.");
  }
  if (auto output_2_type = (*this)->result(2).type()) {
    IR_ENFORCE(output_2_type.isa<paddle::dialect::DenseTensorType>(),
                   "Type validation failed for the 2th output.");
  }
  }
  VLOG(4) << "End Verifying for: GroupNormOp.";
}

void GroupNormOp::InferMeta( phi::InferMetaContext *infer_meta ) {
  auto fn = PD_INFER_META(phi::GroupNormInferMeta);
  fn(infer_meta);
}

phi::DataType GroupNormOp::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: GroupNormOp";
  


  return expected_kernel_dtype;
}

const char *GumbelSoftmaxOp::attributes_name[3] = { "temperature", "hard", "axis" };

OpInfoTuple GumbelSoftmaxOp::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("x", "paddle::dialect::DenseTensorType", false, false, false, true) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = { paddle::dialect::OpAttributeInfo("temperature", "pir::FloatAttribute", ""), paddle::dialect::OpAttributeInfo("hard", "pir::BoolAttribute", ""), paddle::dialect::OpAttributeInfo("axis", "pir::Int32Attribute", "") };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("out", "paddle::dialect::DenseTensorType", false, false) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("GumbelSoftmaxInferMeta", {"x", "temperature", "hard", "axis"}, "gumbel_softmax", {"x", "temperature", "hard", "axis"}, {}, {}, {}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "gumbel_softmax");
}

void GumbelSoftmaxOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, float temperature, bool hard, int axis) {
  VLOG(4) << "Start build GumbelSoftmaxOp";



  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_temperature = pir::FloatAttribute::get(pir::IrContext::Instance(), temperature);
  argument.AddAttribute("temperature", attr_temperature);
  pir::Attribute attr_hard = pir::BoolAttribute::get(pir::IrContext::Instance(), hard);
  argument.AddAttribute("hard", attr_hard);
  pir::Attribute attr_axis = pir::Int32Attribute::get(pir::IrContext::Instance(), axis);
  argument.AddAttribute("axis", attr_axis);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);
  paddle::dialect::IrTensor dense_out;
  paddle::dialect::IrMetaTensor meta_out(&dense_out);

  phi::GumbelSoftmaxInferMeta(meta_x, temperature, hard, axis, &meta_out);

  std::vector<pir::Type> argument_outputs;
  pir::Type out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_out.dtype()), dense_out.dims(), dense_out.layout(), dense_out.lod(), dense_out.offset());
  argument_outputs.push_back(out_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void GumbelSoftmaxOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, pir::AttributeMap attributes) {
  VLOG(4) << "Start build GumbelSoftmaxOp";


  IR_ENFORCE(
      attributes.find("temperature") != attributes.end(),
          "'temperature' Attribute is expected for GumbelSoftmaxOp. ");
  float temperature = attributes.at("temperature").dyn_cast<pir::FloatAttribute>().data();

  IR_ENFORCE(
      attributes.find("hard") != attributes.end(),
          "'hard' Attribute is expected for GumbelSoftmaxOp. ");
  bool hard = attributes.at("hard").dyn_cast<pir::BoolAttribute>().data();

  IR_ENFORCE(
      attributes.find("axis") != attributes.end(),
          "'axis' Attribute is expected for GumbelSoftmaxOp. ");
  int axis = attributes.at("axis").dyn_cast<pir::Int32Attribute>().data();


  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_temperature = pir::FloatAttribute::get(pir::IrContext::Instance(), temperature);
  argument.AddAttribute("temperature", attr_temperature);
  pir::Attribute attr_hard = pir::BoolAttribute::get(pir::IrContext::Instance(), hard);
  argument.AddAttribute("hard", attr_hard);
  pir::Attribute attr_axis = pir::Int32Attribute::get(pir::IrContext::Instance(), axis);
  argument.AddAttribute("axis", attr_axis);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);
  paddle::dialect::IrTensor dense_out;
  paddle::dialect::IrMetaTensor meta_out(&dense_out);

  phi::GumbelSoftmaxInferMeta(meta_x, temperature, hard, axis, &meta_out);

  std::vector<pir::Type> argument_outputs;
  pir::Type out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_out.dtype()), dense_out.dims(), dense_out.layout(), dense_out.lod(), dense_out.offset());
  argument_outputs.push_back(out_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void GumbelSoftmaxOp::VerifySig() {
  VLOG(4) << "Start Verifying inputs, outputs and attributes for: GumbelSoftmaxOp.";
  VLOG(4) << "Verifying inputs:";
  {
  auto input_size = num_operands();
  IR_ENFORCE(input_size == 1u,
                    "The size %d of inputs must be equal to 1.", input_size);
  IR_ENFORCE((*this)->operand_source(0).type().isa<paddle::dialect::DenseTensorType>(),
                  "Type validation failed for the 0th input, got %s.", (*this)->operand_source(0).type());
  }
  VLOG(4) << "Verifying attributes:";
  {
  auto& attributes = this->attributes();
  IR_ENFORCE(attributes.count("temperature")>0,
                 "temperature does not exist.");
  IR_ENFORCE(attributes.at("temperature").isa<pir::FloatAttribute>(),
                 "Type of attribute: temperature is not pir::FloatAttribute.");

  IR_ENFORCE(attributes.count("hard")>0,
                 "hard does not exist.");
  IR_ENFORCE(attributes.at("hard").isa<pir::BoolAttribute>(),
                 "Type of attribute: hard is not pir::BoolAttribute.");

  IR_ENFORCE(attributes.count("axis")>0,
                 "axis does not exist.");
  IR_ENFORCE(attributes.at("axis").isa<pir::Int32Attribute>(),
                 "Type of attribute: axis is not pir::Int32Attribute.");

  }
  VLOG(4) << "Verifying outputs:";
  {
  auto output_size = num_results();
  IR_ENFORCE(output_size == 1u,
                    "The size %d of outputs must be equal to 1.", output_size);
  IR_ENFORCE((*this)->result(0).type().isa<paddle::dialect::DenseTensorType>(),
                 "Type validation failed for the 0th output.");
  }
  VLOG(4) << "End Verifying for: GumbelSoftmaxOp.";
}

void GumbelSoftmaxOp::InferMeta( phi::InferMetaContext *infer_meta ) {
  auto fn = PD_INFER_META(phi::GumbelSoftmaxInferMeta);
  fn(infer_meta);
}

phi::DataType GumbelSoftmaxOp::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: GumbelSoftmaxOp";
  


  return expected_kernel_dtype;
}

const char *HardshrinkOp::attributes_name[1] = { "threshold" };

OpInfoTuple HardshrinkOp::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("x", "paddle::dialect::DenseTensorType", false, false, false, true) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = { paddle::dialect::OpAttributeInfo("threshold", "pir::FloatAttribute", "") };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("out", "paddle::dialect::DenseTensorType", false, false) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("UnchangedInferMeta", {"x"}, "hard_shrink", {"x", "threshold"}, {}, {}, {}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "hardshrink");
}

void HardshrinkOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, float threshold) {
  VLOG(4) << "Start build HardshrinkOp";



  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_threshold = pir::FloatAttribute::get(pir::IrContext::Instance(), threshold);
  argument.AddAttribute("threshold", attr_threshold);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);
  paddle::dialect::IrTensor dense_out;
  paddle::dialect::IrMetaTensor meta_out(&dense_out);

  phi::UnchangedInferMeta(meta_x, &meta_out);

  std::vector<pir::Type> argument_outputs;
  pir::Type out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_out.dtype()), dense_out.dims(), dense_out.layout(), dense_out.lod(), dense_out.offset());
  argument_outputs.push_back(out_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void HardshrinkOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, pir::AttributeMap attributes) {
  VLOG(4) << "Start build HardshrinkOp";


  IR_ENFORCE(
      attributes.find("threshold") != attributes.end(),
          "'threshold' Attribute is expected for HardshrinkOp. ");
  float threshold = attributes.at("threshold").dyn_cast<pir::FloatAttribute>().data();


  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_threshold = pir::FloatAttribute::get(pir::IrContext::Instance(), threshold);
  argument.AddAttribute("threshold", attr_threshold);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);
  paddle::dialect::IrTensor dense_out;
  paddle::dialect::IrMetaTensor meta_out(&dense_out);

  phi::UnchangedInferMeta(meta_x, &meta_out);

  std::vector<pir::Type> argument_outputs;
  pir::Type out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_out.dtype()), dense_out.dims(), dense_out.layout(), dense_out.lod(), dense_out.offset());
  argument_outputs.push_back(out_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void HardshrinkOp::VerifySig() {
  VLOG(4) << "Start Verifying inputs, outputs and attributes for: HardshrinkOp.";
  VLOG(4) << "Verifying inputs:";
  {
  auto input_size = num_operands();
  IR_ENFORCE(input_size == 1u,
                    "The size %d of inputs must be equal to 1.", input_size);
  IR_ENFORCE((*this)->operand_source(0).type().isa<paddle::dialect::DenseTensorType>(),
                  "Type validation failed for the 0th input, got %s.", (*this)->operand_source(0).type());
  }
  VLOG(4) << "Verifying attributes:";
  {
  auto& attributes = this->attributes();
  IR_ENFORCE(attributes.count("threshold")>0,
                 "threshold does not exist.");
  IR_ENFORCE(attributes.at("threshold").isa<pir::FloatAttribute>(),
                 "Type of attribute: threshold is not pir::FloatAttribute.");

  }
  VLOG(4) << "Verifying outputs:";
  {
  auto output_size = num_results();
  IR_ENFORCE(output_size == 1u,
                    "The size %d of outputs must be equal to 1.", output_size);
  IR_ENFORCE((*this)->result(0).type().isa<paddle::dialect::DenseTensorType>(),
                 "Type validation failed for the 0th output.");
  }
  VLOG(4) << "End Verifying for: HardshrinkOp.";
}

void HardshrinkOp::InferMeta( phi::InferMetaContext *infer_meta ) {
  auto fn = PD_INFER_META(phi::UnchangedInferMeta);
  fn(infer_meta);
}

phi::DataType HardshrinkOp::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: HardshrinkOp";
  


  return expected_kernel_dtype;
}

const char *HardsigmoidOp::attributes_name[2] = { "slope", "offset" };

OpInfoTuple HardsigmoidOp::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("x", "paddle::dialect::DenseTensorType", false, false, false, true) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = { paddle::dialect::OpAttributeInfo("slope", "pir::FloatAttribute", ""), paddle::dialect::OpAttributeInfo("offset", "pir::FloatAttribute", "") };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("out", "paddle::dialect::DenseTensorType", false, false) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("UnchangedInferMeta", {"x"}, "hardsigmoid", {"x", "slope", "offset"}, {}, {}, {}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "hardsigmoid");
}

void HardsigmoidOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, float slope, float offset) {
  VLOG(4) << "Start build HardsigmoidOp";



  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_slope = pir::FloatAttribute::get(pir::IrContext::Instance(), slope);
  argument.AddAttribute("slope", attr_slope);
  pir::Attribute attr_offset = pir::FloatAttribute::get(pir::IrContext::Instance(), offset);
  argument.AddAttribute("offset", attr_offset);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);
  paddle::dialect::IrTensor dense_out;
  paddle::dialect::IrMetaTensor meta_out(&dense_out);

  phi::UnchangedInferMeta(meta_x, &meta_out);

  std::vector<pir::Type> argument_outputs;
  pir::Type out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_out.dtype()), dense_out.dims(), dense_out.layout(), dense_out.lod(), dense_out.offset());
  argument_outputs.push_back(out_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void HardsigmoidOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, pir::AttributeMap attributes) {
  VLOG(4) << "Start build HardsigmoidOp";


  IR_ENFORCE(
      attributes.find("slope") != attributes.end(),
          "'slope' Attribute is expected for HardsigmoidOp. ");
  float slope = attributes.at("slope").dyn_cast<pir::FloatAttribute>().data();

  IR_ENFORCE(
      attributes.find("offset") != attributes.end(),
          "'offset' Attribute is expected for HardsigmoidOp. ");
  float offset = attributes.at("offset").dyn_cast<pir::FloatAttribute>().data();


  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_slope = pir::FloatAttribute::get(pir::IrContext::Instance(), slope);
  argument.AddAttribute("slope", attr_slope);
  pir::Attribute attr_offset = pir::FloatAttribute::get(pir::IrContext::Instance(), offset);
  argument.AddAttribute("offset", attr_offset);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);
  paddle::dialect::IrTensor dense_out;
  paddle::dialect::IrMetaTensor meta_out(&dense_out);

  phi::UnchangedInferMeta(meta_x, &meta_out);

  std::vector<pir::Type> argument_outputs;
  pir::Type out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_out.dtype()), dense_out.dims(), dense_out.layout(), dense_out.lod(), dense_out.offset());
  argument_outputs.push_back(out_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void HardsigmoidOp::VerifySig() {
  VLOG(4) << "Start Verifying inputs, outputs and attributes for: HardsigmoidOp.";
  VLOG(4) << "Verifying inputs:";
  {
  auto input_size = num_operands();
  IR_ENFORCE(input_size == 1u,
                    "The size %d of inputs must be equal to 1.", input_size);
  IR_ENFORCE((*this)->operand_source(0).type().isa<paddle::dialect::DenseTensorType>(),
                  "Type validation failed for the 0th input, got %s.", (*this)->operand_source(0).type());
  }
  VLOG(4) << "Verifying attributes:";
  {
  auto& attributes = this->attributes();
  IR_ENFORCE(attributes.count("slope")>0,
                 "slope does not exist.");
  IR_ENFORCE(attributes.at("slope").isa<pir::FloatAttribute>(),
                 "Type of attribute: slope is not pir::FloatAttribute.");

  IR_ENFORCE(attributes.count("offset")>0,
                 "offset does not exist.");
  IR_ENFORCE(attributes.at("offset").isa<pir::FloatAttribute>(),
                 "Type of attribute: offset is not pir::FloatAttribute.");

  }
  VLOG(4) << "Verifying outputs:";
  {
  auto output_size = num_results();
  IR_ENFORCE(output_size == 1u,
                    "The size %d of outputs must be equal to 1.", output_size);
  IR_ENFORCE((*this)->result(0).type().isa<paddle::dialect::DenseTensorType>(),
                 "Type validation failed for the 0th output.");
  }
  VLOG(4) << "End Verifying for: HardsigmoidOp.";
}

void HardsigmoidOp::InferMeta( phi::InferMetaContext *infer_meta ) {
  auto fn = PD_INFER_META(phi::UnchangedInferMeta);
  fn(infer_meta);
}

phi::DataType HardsigmoidOp::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: HardsigmoidOp";
  


  return expected_kernel_dtype;
}

const char *HardtanhOp::attributes_name[2] = { "t_min", "t_max" };

OpInfoTuple HardtanhOp::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("x", "paddle::dialect::DenseTensorType", false, false, false, true) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = { paddle::dialect::OpAttributeInfo("t_min", "pir::FloatAttribute", ""), paddle::dialect::OpAttributeInfo("t_max", "pir::FloatAttribute", "") };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("out", "paddle::dialect::DenseTensorType", false, false) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("UnchangedInferMeta", {"x"}, "hardtanh", {"x", "t_min", "t_max"}, {}, {}, {}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "hardtanh");
}

void HardtanhOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, float t_min, float t_max) {
  VLOG(4) << "Start build HardtanhOp";



  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_t_min = pir::FloatAttribute::get(pir::IrContext::Instance(), t_min);
  argument.AddAttribute("t_min", attr_t_min);
  pir::Attribute attr_t_max = pir::FloatAttribute::get(pir::IrContext::Instance(), t_max);
  argument.AddAttribute("t_max", attr_t_max);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);
  paddle::dialect::IrTensor dense_out;
  paddle::dialect::IrMetaTensor meta_out(&dense_out);

  phi::UnchangedInferMeta(meta_x, &meta_out);

  std::vector<pir::Type> argument_outputs;
  pir::Type out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_out.dtype()), dense_out.dims(), dense_out.layout(), dense_out.lod(), dense_out.offset());
  argument_outputs.push_back(out_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void HardtanhOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, pir::AttributeMap attributes) {
  VLOG(4) << "Start build HardtanhOp";


  IR_ENFORCE(
      attributes.find("t_min") != attributes.end(),
          "'t_min' Attribute is expected for HardtanhOp. ");
  float t_min = attributes.at("t_min").dyn_cast<pir::FloatAttribute>().data();

  IR_ENFORCE(
      attributes.find("t_max") != attributes.end(),
          "'t_max' Attribute is expected for HardtanhOp. ");
  float t_max = attributes.at("t_max").dyn_cast<pir::FloatAttribute>().data();


  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_t_min = pir::FloatAttribute::get(pir::IrContext::Instance(), t_min);
  argument.AddAttribute("t_min", attr_t_min);
  pir::Attribute attr_t_max = pir::FloatAttribute::get(pir::IrContext::Instance(), t_max);
  argument.AddAttribute("t_max", attr_t_max);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);
  paddle::dialect::IrTensor dense_out;
  paddle::dialect::IrMetaTensor meta_out(&dense_out);

  phi::UnchangedInferMeta(meta_x, &meta_out);

  std::vector<pir::Type> argument_outputs;
  pir::Type out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_out.dtype()), dense_out.dims(), dense_out.layout(), dense_out.lod(), dense_out.offset());
  argument_outputs.push_back(out_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void HardtanhOp::VerifySig() {
  VLOG(4) << "Start Verifying inputs, outputs and attributes for: HardtanhOp.";
  VLOG(4) << "Verifying inputs:";
  {
  auto input_size = num_operands();
  IR_ENFORCE(input_size == 1u,
                    "The size %d of inputs must be equal to 1.", input_size);
  IR_ENFORCE((*this)->operand_source(0).type().isa<paddle::dialect::DenseTensorType>(),
                  "Type validation failed for the 0th input, got %s.", (*this)->operand_source(0).type());
  }
  VLOG(4) << "Verifying attributes:";
  {
  auto& attributes = this->attributes();
  IR_ENFORCE(attributes.count("t_min")>0,
                 "t_min does not exist.");
  IR_ENFORCE(attributes.at("t_min").isa<pir::FloatAttribute>(),
                 "Type of attribute: t_min is not pir::FloatAttribute.");

  IR_ENFORCE(attributes.count("t_max")>0,
                 "t_max does not exist.");
  IR_ENFORCE(attributes.at("t_max").isa<pir::FloatAttribute>(),
                 "Type of attribute: t_max is not pir::FloatAttribute.");

  }
  VLOG(4) << "Verifying outputs:";
  {
  auto output_size = num_results();
  IR_ENFORCE(output_size == 1u,
                    "The size %d of outputs must be equal to 1.", output_size);
  IR_ENFORCE((*this)->result(0).type().isa<paddle::dialect::DenseTensorType>(),
                 "Type validation failed for the 0th output.");
  }
  VLOG(4) << "End Verifying for: HardtanhOp.";
}

void HardtanhOp::InferMeta( phi::InferMetaContext *infer_meta ) {
  auto fn = PD_INFER_META(phi::UnchangedInferMeta);
  fn(infer_meta);
}

phi::DataType HardtanhOp::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: HardtanhOp";
  


  return expected_kernel_dtype;
}

const char *Hardtanh_Op::attributes_name[2] = { "t_min", "t_max" };

OpInfoTuple Hardtanh_Op::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("x", "paddle::dialect::DenseTensorType", false, false, false, true) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = { paddle::dialect::OpAttributeInfo("t_min", "pir::FloatAttribute", ""), paddle::dialect::OpAttributeInfo("t_max", "pir::FloatAttribute", "") };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("out", "paddle::dialect::DenseTensorType", false, false) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("UnchangedInferMeta", {"x"}, "hardtanh", {"x", "t_min", "t_max"}, {}, {}, {{"out", "x"}}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "hardtanh");
}

void Hardtanh_Op::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, float t_min, float t_max) {
  VLOG(4) << "Start build Hardtanh_Op";



  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_t_min = pir::FloatAttribute::get(pir::IrContext::Instance(), t_min);
  argument.AddAttribute("t_min", attr_t_min);
  pir::Attribute attr_t_max = pir::FloatAttribute::get(pir::IrContext::Instance(), t_max);
  argument.AddAttribute("t_max", attr_t_max);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);
  paddle::dialect::IrTensor dense_out;
  paddle::dialect::IrMetaTensor meta_out(&dense_out);

  phi::UnchangedInferMeta(meta_x, &meta_out);

  std::vector<pir::Type> argument_outputs;
  pir::Type out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_out.dtype()), dense_out.dims(), dense_out.layout(), dense_out.lod(), dense_out.offset());
  argument_outputs.push_back(out_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void Hardtanh_Op::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, pir::AttributeMap attributes) {
  VLOG(4) << "Start build Hardtanh_Op";


  IR_ENFORCE(
      attributes.find("t_min") != attributes.end(),
          "'t_min' Attribute is expected for Hardtanh_Op. ");
  float t_min = attributes.at("t_min").dyn_cast<pir::FloatAttribute>().data();

  IR_ENFORCE(
      attributes.find("t_max") != attributes.end(),
          "'t_max' Attribute is expected for Hardtanh_Op. ");
  float t_max = attributes.at("t_max").dyn_cast<pir::FloatAttribute>().data();


  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_t_min = pir::FloatAttribute::get(pir::IrContext::Instance(), t_min);
  argument.AddAttribute("t_min", attr_t_min);
  pir::Attribute attr_t_max = pir::FloatAttribute::get(pir::IrContext::Instance(), t_max);
  argument.AddAttribute("t_max", attr_t_max);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);
  paddle::dialect::IrTensor dense_out;
  paddle::dialect::IrMetaTensor meta_out(&dense_out);

  phi::UnchangedInferMeta(meta_x, &meta_out);

  std::vector<pir::Type> argument_outputs;
  pir::Type out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_out.dtype()), dense_out.dims(), dense_out.layout(), dense_out.lod(), dense_out.offset());
  argument_outputs.push_back(out_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void Hardtanh_Op::VerifySig() {
  VLOG(4) << "Start Verifying inputs, outputs and attributes for: Hardtanh_Op.";
  VLOG(4) << "Verifying inputs:";
  {
  auto input_size = num_operands();
  IR_ENFORCE(input_size == 1u,
                    "The size %d of inputs must be equal to 1.", input_size);
  IR_ENFORCE((*this)->operand_source(0).type().isa<paddle::dialect::DenseTensorType>(),
                  "Type validation failed for the 0th input, got %s.", (*this)->operand_source(0).type());
  }
  VLOG(4) << "Verifying attributes:";
  {
  auto& attributes = this->attributes();
  IR_ENFORCE(attributes.count("t_min")>0,
                 "t_min does not exist.");
  IR_ENFORCE(attributes.at("t_min").isa<pir::FloatAttribute>(),
                 "Type of attribute: t_min is not pir::FloatAttribute.");

  IR_ENFORCE(attributes.count("t_max")>0,
                 "t_max does not exist.");
  IR_ENFORCE(attributes.at("t_max").isa<pir::FloatAttribute>(),
                 "Type of attribute: t_max is not pir::FloatAttribute.");

  }
  VLOG(4) << "Verifying outputs:";
  {
  auto output_size = num_results();
  IR_ENFORCE(output_size == 1u,
                    "The size %d of outputs must be equal to 1.", output_size);
  IR_ENFORCE((*this)->result(0).type().isa<paddle::dialect::DenseTensorType>(),
                 "Type validation failed for the 0th output.");
  }
  VLOG(4) << "End Verifying for: Hardtanh_Op.";
}

void Hardtanh_Op::InferMeta( phi::InferMetaContext *infer_meta ) {
  auto fn = PD_INFER_META(phi::UnchangedInferMeta);
  fn(infer_meta);
}

phi::DataType Hardtanh_Op::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: Hardtanh_Op";
  


  return expected_kernel_dtype;
}

OpInfoTuple HeavisideOp::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("x", "paddle::dialect::DenseTensorType", false, false, false, true), paddle::dialect::OpInputInfo("y", "paddle::dialect::DenseTensorType", false, false, false, true) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = {  };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("out", "paddle::dialect::DenseTensorType", false, false) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("ElementwiseInferMeta", {"x", "y"}, "heaviside", {"x", "y"}, {}, {}, {}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "heaviside");
}

void HeavisideOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, pir::Value y_) {
  VLOG(4) << "Start build HeavisideOp";



  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_, y_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;
  paddle::dialect::DenseTensorType y = y_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)y;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);

  VLOG(4) << "Builder construction  dense_y";
  paddle::dialect::IrTensor ir_tensor_y(paddle::dialect::TransToPhiDataType(y.dtype()),
                                                      y.dims(),
                                                      y.data_layout(),
                                                      y.lod(),
                                                      y.offset());
  VLOG(4) << "Builder construction  meta_y";
  paddle::dialect::IrMetaTensor meta_y(&ir_tensor_y);
  paddle::dialect::IrTensor dense_out;
  paddle::dialect::IrMetaTensor meta_out(&dense_out);

  phi::ElementwiseInferMeta(meta_x, meta_y, &meta_out);

  std::vector<pir::Type> argument_outputs;
  pir::Type out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_out.dtype()), dense_out.dims(), dense_out.layout(), dense_out.lod(), dense_out.offset());
  argument_outputs.push_back(out_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void HeavisideOp::VerifySig() {
  VLOG(4) << "Start Verifying inputs, outputs and attributes for: HeavisideOp.";
  VLOG(4) << "Verifying inputs:";
  {
  auto input_size = num_operands();
  IR_ENFORCE(input_size == 2u,
                    "The size %d of inputs must be equal to 2.", input_size);
  IR_ENFORCE((*this)->operand_source(0).type().isa<paddle::dialect::DenseTensorType>(),
                  "Type validation failed for the 0th input, got %s.", (*this)->operand_source(0).type());
  IR_ENFORCE((*this)->operand_source(1).type().isa<paddle::dialect::DenseTensorType>(),
                  "Type validation failed for the 1th input, got %s.", (*this)->operand_source(1).type());
  }
  VLOG(4) << "Verifying attributes:";
  {
  // Attributes num is 0, not need to check attributes type.
  }
  VLOG(4) << "Verifying outputs:";
  {
  auto output_size = num_results();
  IR_ENFORCE(output_size == 1u,
                    "The size %d of outputs must be equal to 1.", output_size);
  IR_ENFORCE((*this)->result(0).type().isa<paddle::dialect::DenseTensorType>(),
                 "Type validation failed for the 0th output.");
  }
  VLOG(4) << "End Verifying for: HeavisideOp.";
}

void HeavisideOp::InferMeta( phi::InferMetaContext *infer_meta ) {
  auto fn = PD_INFER_META(phi::ElementwiseInferMeta);
  fn(infer_meta);
}

phi::DataType HeavisideOp::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: HeavisideOp";
  


  // deal complex_promote
  if (framework::IsComplexType(expected_kernel_dtype)) {{
    // only promote inputs’s types when contains complex input
    return tensor_dtype;
  }}

  return expected_kernel_dtype;
}

const char *HistogramOp::attributes_name[3] = { "bins", "min", "max" };

OpInfoTuple HistogramOp::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("input", "paddle::dialect::DenseTensorType", false, false, false, false) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = { paddle::dialect::OpAttributeInfo("bins", "pir::Int64Attribute", ""), paddle::dialect::OpAttributeInfo("min", "pir::Int32Attribute", ""), paddle::dialect::OpAttributeInfo("max", "pir::Int32Attribute", "") };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("out", "paddle::dialect::DenseTensorType", false, false) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("HistogramInferMeta", {"input", "bins", "min", "max"}, "histogram", {"input", "bins", "min", "max"}, {}, {}, {}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "histogram");
}

void HistogramOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value input_, int64_t bins, int min, int max) {
  VLOG(4) << "Start build HistogramOp";



  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {input_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_bins = pir::Int64Attribute::get(pir::IrContext::Instance(), bins);
  argument.AddAttribute("bins", attr_bins);
  pir::Attribute attr_min = pir::Int32Attribute::get(pir::IrContext::Instance(), min);
  argument.AddAttribute("min", attr_min);
  pir::Attribute attr_max = pir::Int32Attribute::get(pir::IrContext::Instance(), max);
  argument.AddAttribute("max", attr_max);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType input = input_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)input;

  VLOG(4) << "Builder construction  dense_input";
  paddle::dialect::IrTensor ir_tensor_input(paddle::dialect::TransToPhiDataType(input.dtype()),
                                                      input.dims(),
                                                      input.data_layout(),
                                                      input.lod(),
                                                      input.offset());
  VLOG(4) << "Builder construction  meta_input";
  paddle::dialect::IrMetaTensor meta_input(&ir_tensor_input);
  paddle::dialect::IrTensor dense_out;
  paddle::dialect::IrMetaTensor meta_out(&dense_out);

  phi::HistogramInferMeta(meta_input, bins, min, max, &meta_out);

  std::vector<pir::Type> argument_outputs;
  pir::Type out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_out.dtype()), dense_out.dims(), dense_out.layout(), dense_out.lod(), dense_out.offset());
  argument_outputs.push_back(out_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void HistogramOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value input_, pir::AttributeMap attributes) {
  VLOG(4) << "Start build HistogramOp";


  IR_ENFORCE(
      attributes.find("bins") != attributes.end(),
          "'bins' Attribute is expected for HistogramOp. ");
  int64_t bins = attributes.at("bins").dyn_cast<pir::Int64Attribute>().data();

  IR_ENFORCE(
      attributes.find("min") != attributes.end(),
          "'min' Attribute is expected for HistogramOp. ");
  int min = attributes.at("min").dyn_cast<pir::Int32Attribute>().data();

  IR_ENFORCE(
      attributes.find("max") != attributes.end(),
          "'max' Attribute is expected for HistogramOp. ");
  int max = attributes.at("max").dyn_cast<pir::Int32Attribute>().data();


  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {input_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_bins = pir::Int64Attribute::get(pir::IrContext::Instance(), bins);
  argument.AddAttribute("bins", attr_bins);
  pir::Attribute attr_min = pir::Int32Attribute::get(pir::IrContext::Instance(), min);
  argument.AddAttribute("min", attr_min);
  pir::Attribute attr_max = pir::Int32Attribute::get(pir::IrContext::Instance(), max);
  argument.AddAttribute("max", attr_max);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType input = input_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)input;

  VLOG(4) << "Builder construction  dense_input";
  paddle::dialect::IrTensor ir_tensor_input(paddle::dialect::TransToPhiDataType(input.dtype()),
                                                      input.dims(),
                                                      input.data_layout(),
                                                      input.lod(),
                                                      input.offset());
  VLOG(4) << "Builder construction  meta_input";
  paddle::dialect::IrMetaTensor meta_input(&ir_tensor_input);
  paddle::dialect::IrTensor dense_out;
  paddle::dialect::IrMetaTensor meta_out(&dense_out);

  phi::HistogramInferMeta(meta_input, bins, min, max, &meta_out);

  std::vector<pir::Type> argument_outputs;
  pir::Type out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_out.dtype()), dense_out.dims(), dense_out.layout(), dense_out.lod(), dense_out.offset());
  argument_outputs.push_back(out_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void HistogramOp::VerifySig() {
  VLOG(4) << "Start Verifying inputs, outputs and attributes for: HistogramOp.";
  VLOG(4) << "Verifying inputs:";
  {
  auto input_size = num_operands();
  IR_ENFORCE(input_size == 1u,
                    "The size %d of inputs must be equal to 1.", input_size);
  IR_ENFORCE((*this)->operand_source(0).type().isa<paddle::dialect::DenseTensorType>(),
                  "Type validation failed for the 0th input, got %s.", (*this)->operand_source(0).type());
  }
  VLOG(4) << "Verifying attributes:";
  {
  auto& attributes = this->attributes();
  IR_ENFORCE(attributes.count("bins")>0,
                 "bins does not exist.");
  IR_ENFORCE(attributes.at("bins").isa<pir::Int64Attribute>(),
                 "Type of attribute: bins is not pir::Int64Attribute.");

  IR_ENFORCE(attributes.count("min")>0,
                 "min does not exist.");
  IR_ENFORCE(attributes.at("min").isa<pir::Int32Attribute>(),
                 "Type of attribute: min is not pir::Int32Attribute.");

  IR_ENFORCE(attributes.count("max")>0,
                 "max does not exist.");
  IR_ENFORCE(attributes.at("max").isa<pir::Int32Attribute>(),
                 "Type of attribute: max is not pir::Int32Attribute.");

  }
  VLOG(4) << "Verifying outputs:";
  {
  auto output_size = num_results();
  IR_ENFORCE(output_size == 1u,
                    "The size %d of outputs must be equal to 1.", output_size);
  IR_ENFORCE((*this)->result(0).type().isa<paddle::dialect::DenseTensorType>(),
                 "Type validation failed for the 0th output.");
  }
  VLOG(4) << "End Verifying for: HistogramOp.";
}

void HistogramOp::InferMeta( phi::InferMetaContext *infer_meta ) {
  auto fn = PD_INFER_META(phi::HistogramInferMeta);
  fn(infer_meta);
}

phi::DataType HistogramOp::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: HistogramOp";
  


  return expected_kernel_dtype;
}

const char *HuberLossOp::attributes_name[1] = { "delta" };

OpInfoTuple HuberLossOp::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("input", "paddle::dialect::DenseTensorType", false, false, false, true), paddle::dialect::OpInputInfo("label", "paddle::dialect::DenseTensorType", false, false, false, true) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = { paddle::dialect::OpAttributeInfo("delta", "pir::FloatAttribute", "") };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("out", "paddle::dialect::DenseTensorType", false, false), paddle::dialect::OpOutputInfo("residual", "paddle::dialect::DenseTensorType", true, true) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("HuberLossInferMeta", {"input", "label", "delta"}, "huber_loss", {"input", "label", "delta"}, {}, {}, {}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "huber_loss");
}

void HuberLossOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value input_, pir::Value label_, float delta) {
  VLOG(4) << "Start build HuberLossOp";



  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {input_, label_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_delta = pir::FloatAttribute::get(pir::IrContext::Instance(), delta);
  argument.AddAttribute("delta", attr_delta);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType input = input_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)input;
  paddle::dialect::DenseTensorType label = label_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)label;

  VLOG(4) << "Builder construction  dense_input";
  paddle::dialect::IrTensor ir_tensor_input(paddle::dialect::TransToPhiDataType(input.dtype()),
                                                      input.dims(),
                                                      input.data_layout(),
                                                      input.lod(),
                                                      input.offset());
  VLOG(4) << "Builder construction  meta_input";
  paddle::dialect::IrMetaTensor meta_input(&ir_tensor_input);

  VLOG(4) << "Builder construction  dense_label";
  paddle::dialect::IrTensor ir_tensor_label(paddle::dialect::TransToPhiDataType(label.dtype()),
                                                      label.dims(),
                                                      label.data_layout(),
                                                      label.lod(),
                                                      label.offset());
  VLOG(4) << "Builder construction  meta_label";
  paddle::dialect::IrMetaTensor meta_label(&ir_tensor_label);
  paddle::dialect::IrTensor dense_out;
  paddle::dialect::IrMetaTensor meta_out(&dense_out);
  paddle::dialect::IrTensor dense_residual;
  paddle::dialect::IrMetaTensor meta_residual(&dense_residual);

  phi::HuberLossInferMeta(meta_input, meta_label, delta, &meta_out, &meta_residual);

  std::vector<pir::Type> argument_outputs;
  pir::Type out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_out.dtype()), dense_out.dims(), dense_out.layout(), dense_out.lod(), dense_out.offset());
  argument_outputs.push_back(out_dense_tensor_type);

  pir::Type residual_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_residual.dtype()), dense_residual.dims(), dense_residual.layout(), dense_residual.lod(), dense_residual.offset());
  argument_outputs.push_back(residual_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void HuberLossOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value input_, pir::Value label_, pir::AttributeMap attributes) {
  VLOG(4) << "Start build HuberLossOp";


  IR_ENFORCE(
      attributes.find("delta") != attributes.end(),
          "'delta' Attribute is expected for HuberLossOp. ");
  float delta = attributes.at("delta").dyn_cast<pir::FloatAttribute>().data();


  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {input_, label_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_delta = pir::FloatAttribute::get(pir::IrContext::Instance(), delta);
  argument.AddAttribute("delta", attr_delta);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType input = input_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)input;
  paddle::dialect::DenseTensorType label = label_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)label;

  VLOG(4) << "Builder construction  dense_input";
  paddle::dialect::IrTensor ir_tensor_input(paddle::dialect::TransToPhiDataType(input.dtype()),
                                                      input.dims(),
                                                      input.data_layout(),
                                                      input.lod(),
                                                      input.offset());
  VLOG(4) << "Builder construction  meta_input";
  paddle::dialect::IrMetaTensor meta_input(&ir_tensor_input);

  VLOG(4) << "Builder construction  dense_label";
  paddle::dialect::IrTensor ir_tensor_label(paddle::dialect::TransToPhiDataType(label.dtype()),
                                                      label.dims(),
                                                      label.data_layout(),
                                                      label.lod(),
                                                      label.offset());
  VLOG(4) << "Builder construction  meta_label";
  paddle::dialect::IrMetaTensor meta_label(&ir_tensor_label);
  paddle::dialect::IrTensor dense_out;
  paddle::dialect::IrMetaTensor meta_out(&dense_out);
  paddle::dialect::IrTensor dense_residual;
  paddle::dialect::IrMetaTensor meta_residual(&dense_residual);

  phi::HuberLossInferMeta(meta_input, meta_label, delta, &meta_out, &meta_residual);

  std::vector<pir::Type> argument_outputs;
  pir::Type out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_out.dtype()), dense_out.dims(), dense_out.layout(), dense_out.lod(), dense_out.offset());
  argument_outputs.push_back(out_dense_tensor_type);

  pir::Type residual_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_residual.dtype()), dense_residual.dims(), dense_residual.layout(), dense_residual.lod(), dense_residual.offset());
  argument_outputs.push_back(residual_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void HuberLossOp::VerifySig() {
  VLOG(4) << "Start Verifying inputs, outputs and attributes for: HuberLossOp.";
  VLOG(4) << "Verifying inputs:";
  {
  auto input_size = num_operands();
  IR_ENFORCE(input_size == 2u,
                    "The size %d of inputs must be equal to 2.", input_size);
  IR_ENFORCE((*this)->operand_source(0).type().isa<paddle::dialect::DenseTensorType>(),
                  "Type validation failed for the 0th input, got %s.", (*this)->operand_source(0).type());
  IR_ENFORCE((*this)->operand_source(1).type().isa<paddle::dialect::DenseTensorType>(),
                  "Type validation failed for the 1th input, got %s.", (*this)->operand_source(1).type());
  }
  VLOG(4) << "Verifying attributes:";
  {
  auto& attributes = this->attributes();
  IR_ENFORCE(attributes.count("delta")>0,
                 "delta does not exist.");
  IR_ENFORCE(attributes.at("delta").isa<pir::FloatAttribute>(),
                 "Type of attribute: delta is not pir::FloatAttribute.");

  }
  VLOG(4) << "Verifying outputs:";
  {
  auto output_size = num_results();
  IR_ENFORCE(output_size == 2u,
                    "The size %d of outputs must be equal to 2.", output_size);
  IR_ENFORCE((*this)->result(0).type().isa<paddle::dialect::DenseTensorType>(),
                 "Type validation failed for the 0th output.");
  if (auto output_1_type = (*this)->result(1).type()) {
    IR_ENFORCE(output_1_type.isa<paddle::dialect::DenseTensorType>(),
                   "Type validation failed for the 1th output.");
  }
  }
  VLOG(4) << "End Verifying for: HuberLossOp.";
}

void HuberLossOp::InferMeta( phi::InferMetaContext *infer_meta ) {
  auto fn = PD_INFER_META(phi::HuberLossInferMeta);
  fn(infer_meta);
}

phi::DataType HuberLossOp::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: HuberLossOp";
  


  return expected_kernel_dtype;
}

OpInfoTuple I0Op::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("x", "paddle::dialect::DenseTensorType", false, false, false, true) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = {  };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("out", "paddle::dialect::DenseTensorType", false, false) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("UnchangedInferMeta", {"x"}, "i0", {"x"}, {}, {}, {}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "i0");
}

void I0Op::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_) {
  VLOG(4) << "Start build I0Op";



  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);
  paddle::dialect::IrTensor dense_out;
  paddle::dialect::IrMetaTensor meta_out(&dense_out);

  phi::UnchangedInferMeta(meta_x, &meta_out);

  std::vector<pir::Type> argument_outputs;
  pir::Type out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_out.dtype()), dense_out.dims(), dense_out.layout(), dense_out.lod(), dense_out.offset());
  argument_outputs.push_back(out_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void I0Op::VerifySig() {
  VLOG(4) << "Start Verifying inputs, outputs and attributes for: I0Op.";
  VLOG(4) << "Verifying inputs:";
  {
  auto input_size = num_operands();
  IR_ENFORCE(input_size == 1u,
                    "The size %d of inputs must be equal to 1.", input_size);
  IR_ENFORCE((*this)->operand_source(0).type().isa<paddle::dialect::DenseTensorType>(),
                  "Type validation failed for the 0th input, got %s.", (*this)->operand_source(0).type());
  }
  VLOG(4) << "Verifying attributes:";
  {
  // Attributes num is 0, not need to check attributes type.
  }
  VLOG(4) << "Verifying outputs:";
  {
  auto output_size = num_results();
  IR_ENFORCE(output_size == 1u,
                    "The size %d of outputs must be equal to 1.", output_size);
  IR_ENFORCE((*this)->result(0).type().isa<paddle::dialect::DenseTensorType>(),
                 "Type validation failed for the 0th output.");
  }
  VLOG(4) << "End Verifying for: I0Op.";
}

void I0Op::InferMeta( phi::InferMetaContext *infer_meta ) {
  auto fn = PD_INFER_META(phi::UnchangedInferMeta);
  fn(infer_meta);
}

phi::DataType I0Op::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: I0Op";
  


  return expected_kernel_dtype;
}

OpInfoTuple I0_Op::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("x", "paddle::dialect::DenseTensorType", false, false, false, true) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = {  };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("out", "paddle::dialect::DenseTensorType", false, false) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("UnchangedInferMeta", {"x"}, "i0", {"x"}, {}, {}, {{"out", "x"}}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "i0");
}

void I0_Op::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_) {
  VLOG(4) << "Start build I0_Op";



  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);
  paddle::dialect::IrTensor dense_out;
  paddle::dialect::IrMetaTensor meta_out(&dense_out);

  phi::UnchangedInferMeta(meta_x, &meta_out);

  std::vector<pir::Type> argument_outputs;
  pir::Type out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_out.dtype()), dense_out.dims(), dense_out.layout(), dense_out.lod(), dense_out.offset());
  argument_outputs.push_back(out_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void I0_Op::VerifySig() {
  VLOG(4) << "Start Verifying inputs, outputs and attributes for: I0_Op.";
  VLOG(4) << "Verifying inputs:";
  {
  auto input_size = num_operands();
  IR_ENFORCE(input_size == 1u,
                    "The size %d of inputs must be equal to 1.", input_size);
  IR_ENFORCE((*this)->operand_source(0).type().isa<paddle::dialect::DenseTensorType>(),
                  "Type validation failed for the 0th input, got %s.", (*this)->operand_source(0).type());
  }
  VLOG(4) << "Verifying attributes:";
  {
  // Attributes num is 0, not need to check attributes type.
  }
  VLOG(4) << "Verifying outputs:";
  {
  auto output_size = num_results();
  IR_ENFORCE(output_size == 1u,
                    "The size %d of outputs must be equal to 1.", output_size);
  IR_ENFORCE((*this)->result(0).type().isa<paddle::dialect::DenseTensorType>(),
                 "Type validation failed for the 0th output.");
  }
  VLOG(4) << "End Verifying for: I0_Op.";
}

void I0_Op::InferMeta( phi::InferMetaContext *infer_meta ) {
  auto fn = PD_INFER_META(phi::UnchangedInferMeta);
  fn(infer_meta);
}

phi::DataType I0_Op::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: I0_Op";
  


  return expected_kernel_dtype;
}

OpInfoTuple I0eOp::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("x", "paddle::dialect::DenseTensorType", false, false, false, true) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = {  };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("out", "paddle::dialect::DenseTensorType", false, false) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("UnchangedInferMeta", {"x"}, "i0e", {"x"}, {}, {}, {}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "i0e");
}

void I0eOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_) {
  VLOG(4) << "Start build I0eOp";



  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);
  paddle::dialect::IrTensor dense_out;
  paddle::dialect::IrMetaTensor meta_out(&dense_out);

  phi::UnchangedInferMeta(meta_x, &meta_out);

  std::vector<pir::Type> argument_outputs;
  pir::Type out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_out.dtype()), dense_out.dims(), dense_out.layout(), dense_out.lod(), dense_out.offset());
  argument_outputs.push_back(out_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void I0eOp::VerifySig() {
  VLOG(4) << "Start Verifying inputs, outputs and attributes for: I0eOp.";
  VLOG(4) << "Verifying inputs:";
  {
  auto input_size = num_operands();
  IR_ENFORCE(input_size == 1u,
                    "The size %d of inputs must be equal to 1.", input_size);
  IR_ENFORCE((*this)->operand_source(0).type().isa<paddle::dialect::DenseTensorType>(),
                  "Type validation failed for the 0th input, got %s.", (*this)->operand_source(0).type());
  }
  VLOG(4) << "Verifying attributes:";
  {
  // Attributes num is 0, not need to check attributes type.
  }
  VLOG(4) << "Verifying outputs:";
  {
  auto output_size = num_results();
  IR_ENFORCE(output_size == 1u,
                    "The size %d of outputs must be equal to 1.", output_size);
  IR_ENFORCE((*this)->result(0).type().isa<paddle::dialect::DenseTensorType>(),
                 "Type validation failed for the 0th output.");
  }
  VLOG(4) << "End Verifying for: I0eOp.";
}

void I0eOp::InferMeta( phi::InferMetaContext *infer_meta ) {
  auto fn = PD_INFER_META(phi::UnchangedInferMeta);
  fn(infer_meta);
}

phi::DataType I0eOp::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: I0eOp";
  


  return expected_kernel_dtype;
}

OpInfoTuple I1Op::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("x", "paddle::dialect::DenseTensorType", false, false, false, true) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = {  };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("out", "paddle::dialect::DenseTensorType", false, false) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("UnchangedInferMeta", {"x"}, "i1", {"x"}, {}, {}, {}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "i1");
}

void I1Op::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_) {
  VLOG(4) << "Start build I1Op";



  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);
  paddle::dialect::IrTensor dense_out;
  paddle::dialect::IrMetaTensor meta_out(&dense_out);

  phi::UnchangedInferMeta(meta_x, &meta_out);

  std::vector<pir::Type> argument_outputs;
  pir::Type out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_out.dtype()), dense_out.dims(), dense_out.layout(), dense_out.lod(), dense_out.offset());
  argument_outputs.push_back(out_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void I1Op::VerifySig() {
  VLOG(4) << "Start Verifying inputs, outputs and attributes for: I1Op.";
  VLOG(4) << "Verifying inputs:";
  {
  auto input_size = num_operands();
  IR_ENFORCE(input_size == 1u,
                    "The size %d of inputs must be equal to 1.", input_size);
  IR_ENFORCE((*this)->operand_source(0).type().isa<paddle::dialect::DenseTensorType>(),
                  "Type validation failed for the 0th input, got %s.", (*this)->operand_source(0).type());
  }
  VLOG(4) << "Verifying attributes:";
  {
  // Attributes num is 0, not need to check attributes type.
  }
  VLOG(4) << "Verifying outputs:";
  {
  auto output_size = num_results();
  IR_ENFORCE(output_size == 1u,
                    "The size %d of outputs must be equal to 1.", output_size);
  IR_ENFORCE((*this)->result(0).type().isa<paddle::dialect::DenseTensorType>(),
                 "Type validation failed for the 0th output.");
  }
  VLOG(4) << "End Verifying for: I1Op.";
}

void I1Op::InferMeta( phi::InferMetaContext *infer_meta ) {
  auto fn = PD_INFER_META(phi::UnchangedInferMeta);
  fn(infer_meta);
}

phi::DataType I1Op::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: I1Op";
  


  return expected_kernel_dtype;
}

OpInfoTuple I1eOp::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("x", "paddle::dialect::DenseTensorType", false, false, false, true) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = {  };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("out", "paddle::dialect::DenseTensorType", false, false) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("UnchangedInferMeta", {"x"}, "i1e", {"x"}, {}, {}, {}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "i1e");
}

void I1eOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_) {
  VLOG(4) << "Start build I1eOp";



  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);
  paddle::dialect::IrTensor dense_out;
  paddle::dialect::IrMetaTensor meta_out(&dense_out);

  phi::UnchangedInferMeta(meta_x, &meta_out);

  std::vector<pir::Type> argument_outputs;
  pir::Type out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_out.dtype()), dense_out.dims(), dense_out.layout(), dense_out.lod(), dense_out.offset());
  argument_outputs.push_back(out_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void I1eOp::VerifySig() {
  VLOG(4) << "Start Verifying inputs, outputs and attributes for: I1eOp.";
  VLOG(4) << "Verifying inputs:";
  {
  auto input_size = num_operands();
  IR_ENFORCE(input_size == 1u,
                    "The size %d of inputs must be equal to 1.", input_size);
  IR_ENFORCE((*this)->operand_source(0).type().isa<paddle::dialect::DenseTensorType>(),
                  "Type validation failed for the 0th input, got %s.", (*this)->operand_source(0).type());
  }
  VLOG(4) << "Verifying attributes:";
  {
  // Attributes num is 0, not need to check attributes type.
  }
  VLOG(4) << "Verifying outputs:";
  {
  auto output_size = num_results();
  IR_ENFORCE(output_size == 1u,
                    "The size %d of outputs must be equal to 1.", output_size);
  IR_ENFORCE((*this)->result(0).type().isa<paddle::dialect::DenseTensorType>(),
                 "Type validation failed for the 0th output.");
  }
  VLOG(4) << "End Verifying for: I1eOp.";
}

void I1eOp::InferMeta( phi::InferMetaContext *infer_meta ) {
  auto fn = PD_INFER_META(phi::UnchangedInferMeta);
  fn(infer_meta);
}

phi::DataType I1eOp::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: I1eOp";
  


  return expected_kernel_dtype;
}

const char *IdentityLossOp::attributes_name[1] = { "reduction" };

OpInfoTuple IdentityLossOp::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("x", "paddle::dialect::DenseTensorType", false, false, false, true) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = { paddle::dialect::OpAttributeInfo("reduction", "pir::Int32Attribute", "") };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("out", "paddle::dialect::DenseTensorType", false, false) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("IdentityLossInferMeta", {"x", "reduction"}, "identity_loss", {"x", "reduction"}, {}, {}, {}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "identity_loss");
}

void IdentityLossOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, int reduction) {
  VLOG(4) << "Start build IdentityLossOp";



  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_reduction = pir::Int32Attribute::get(pir::IrContext::Instance(), reduction);
  argument.AddAttribute("reduction", attr_reduction);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);
  paddle::dialect::IrTensor dense_out;
  paddle::dialect::IrMetaTensor meta_out(&dense_out);

  phi::IdentityLossInferMeta(meta_x, reduction, &meta_out);

  std::vector<pir::Type> argument_outputs;
  pir::Type out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_out.dtype()), dense_out.dims(), dense_out.layout(), dense_out.lod(), dense_out.offset());
  argument_outputs.push_back(out_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void IdentityLossOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, pir::AttributeMap attributes) {
  VLOG(4) << "Start build IdentityLossOp";


  IR_ENFORCE(
      attributes.find("reduction") != attributes.end(),
          "'reduction' Attribute is expected for IdentityLossOp. ");
  int reduction = attributes.at("reduction").dyn_cast<pir::Int32Attribute>().data();


  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_reduction = pir::Int32Attribute::get(pir::IrContext::Instance(), reduction);
  argument.AddAttribute("reduction", attr_reduction);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);
  paddle::dialect::IrTensor dense_out;
  paddle::dialect::IrMetaTensor meta_out(&dense_out);

  phi::IdentityLossInferMeta(meta_x, reduction, &meta_out);

  std::vector<pir::Type> argument_outputs;
  pir::Type out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_out.dtype()), dense_out.dims(), dense_out.layout(), dense_out.lod(), dense_out.offset());
  argument_outputs.push_back(out_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void IdentityLossOp::VerifySig() {
  VLOG(4) << "Start Verifying inputs, outputs and attributes for: IdentityLossOp.";
  VLOG(4) << "Verifying inputs:";
  {
  auto input_size = num_operands();
  IR_ENFORCE(input_size == 1u,
                    "The size %d of inputs must be equal to 1.", input_size);
  IR_ENFORCE((*this)->operand_source(0).type().isa<paddle::dialect::DenseTensorType>(),
                  "Type validation failed for the 0th input, got %s.", (*this)->operand_source(0).type());
  }
  VLOG(4) << "Verifying attributes:";
  {
  auto& attributes = this->attributes();
  IR_ENFORCE(attributes.count("reduction")>0,
                 "reduction does not exist.");
  IR_ENFORCE(attributes.at("reduction").isa<pir::Int32Attribute>(),
                 "Type of attribute: reduction is not pir::Int32Attribute.");

  }
  VLOG(4) << "Verifying outputs:";
  {
  auto output_size = num_results();
  IR_ENFORCE(output_size == 1u,
                    "The size %d of outputs must be equal to 1.", output_size);
  IR_ENFORCE((*this)->result(0).type().isa<paddle::dialect::DenseTensorType>(),
                 "Type validation failed for the 0th output.");
  }
  VLOG(4) << "End Verifying for: IdentityLossOp.";
}

void IdentityLossOp::InferMeta( phi::InferMetaContext *infer_meta ) {
  auto fn = PD_INFER_META(phi::IdentityLossInferMeta);
  fn(infer_meta);
}

phi::DataType IdentityLossOp::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: IdentityLossOp";
  


  return expected_kernel_dtype;
}

const char *IdentityLoss_Op::attributes_name[1] = { "reduction" };

OpInfoTuple IdentityLoss_Op::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("x", "paddle::dialect::DenseTensorType", false, false, false, true) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = { paddle::dialect::OpAttributeInfo("reduction", "pir::Int32Attribute", "") };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("out", "paddle::dialect::DenseTensorType", false, false) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("IdentityLossInferMeta", {"x", "reduction"}, "identity_loss", {"x", "reduction"}, {}, {}, {{"out", "x"}}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "identity_loss");
}

void IdentityLoss_Op::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, int reduction) {
  VLOG(4) << "Start build IdentityLoss_Op";



  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_reduction = pir::Int32Attribute::get(pir::IrContext::Instance(), reduction);
  argument.AddAttribute("reduction", attr_reduction);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);
  paddle::dialect::IrTensor dense_out;
  paddle::dialect::IrMetaTensor meta_out(&dense_out);

  phi::IdentityLossInferMeta(meta_x, reduction, &meta_out);

  std::vector<pir::Type> argument_outputs;
  pir::Type out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_out.dtype()), dense_out.dims(), dense_out.layout(), dense_out.lod(), dense_out.offset());
  argument_outputs.push_back(out_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void IdentityLoss_Op::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, pir::AttributeMap attributes) {
  VLOG(4) << "Start build IdentityLoss_Op";


  IR_ENFORCE(
      attributes.find("reduction") != attributes.end(),
          "'reduction' Attribute is expected for IdentityLoss_Op. ");
  int reduction = attributes.at("reduction").dyn_cast<pir::Int32Attribute>().data();


  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_reduction = pir::Int32Attribute::get(pir::IrContext::Instance(), reduction);
  argument.AddAttribute("reduction", attr_reduction);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);
  paddle::dialect::IrTensor dense_out;
  paddle::dialect::IrMetaTensor meta_out(&dense_out);

  phi::IdentityLossInferMeta(meta_x, reduction, &meta_out);

  std::vector<pir::Type> argument_outputs;
  pir::Type out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_out.dtype()), dense_out.dims(), dense_out.layout(), dense_out.lod(), dense_out.offset());
  argument_outputs.push_back(out_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void IdentityLoss_Op::VerifySig() {
  VLOG(4) << "Start Verifying inputs, outputs and attributes for: IdentityLoss_Op.";
  VLOG(4) << "Verifying inputs:";
  {
  auto input_size = num_operands();
  IR_ENFORCE(input_size == 1u,
                    "The size %d of inputs must be equal to 1.", input_size);
  IR_ENFORCE((*this)->operand_source(0).type().isa<paddle::dialect::DenseTensorType>(),
                  "Type validation failed for the 0th input, got %s.", (*this)->operand_source(0).type());
  }
  VLOG(4) << "Verifying attributes:";
  {
  auto& attributes = this->attributes();
  IR_ENFORCE(attributes.count("reduction")>0,
                 "reduction does not exist.");
  IR_ENFORCE(attributes.at("reduction").isa<pir::Int32Attribute>(),
                 "Type of attribute: reduction is not pir::Int32Attribute.");

  }
  VLOG(4) << "Verifying outputs:";
  {
  auto output_size = num_results();
  IR_ENFORCE(output_size == 1u,
                    "The size %d of outputs must be equal to 1.", output_size);
  IR_ENFORCE((*this)->result(0).type().isa<paddle::dialect::DenseTensorType>(),
                 "Type validation failed for the 0th output.");
  }
  VLOG(4) << "End Verifying for: IdentityLoss_Op.";
}

void IdentityLoss_Op::InferMeta( phi::InferMetaContext *infer_meta ) {
  auto fn = PD_INFER_META(phi::IdentityLossInferMeta);
  fn(infer_meta);
}

phi::DataType IdentityLoss_Op::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: IdentityLoss_Op";
  


  return expected_kernel_dtype;
}

OpInfoTuple ImagOp::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("x", "paddle::dialect::DenseTensorType", false, false, false, true) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = {  };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("out", "paddle::dialect::DenseTensorType", false, false) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("RealAndImagInferMeta", {"x"}, "imag", {"x"}, {}, {}, {}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "imag");
}

void ImagOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_) {
  VLOG(4) << "Start build ImagOp";



  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);
  paddle::dialect::IrTensor dense_out;
  paddle::dialect::IrMetaTensor meta_out(&dense_out);

  phi::RealAndImagInferMeta(meta_x, &meta_out);

  std::vector<pir::Type> argument_outputs;
  pir::Type out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_out.dtype()), dense_out.dims(), dense_out.layout(), dense_out.lod(), dense_out.offset());
  argument_outputs.push_back(out_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void ImagOp::VerifySig() {
  VLOG(4) << "Start Verifying inputs, outputs and attributes for: ImagOp.";
  VLOG(4) << "Verifying inputs:";
  {
  auto input_size = num_operands();
  IR_ENFORCE(input_size == 1u,
                    "The size %d of inputs must be equal to 1.", input_size);
  IR_ENFORCE((*this)->operand_source(0).type().isa<paddle::dialect::DenseTensorType>(),
                  "Type validation failed for the 0th input, got %s.", (*this)->operand_source(0).type());
  }
  VLOG(4) << "Verifying attributes:";
  {
  // Attributes num is 0, not need to check attributes type.
  }
  VLOG(4) << "Verifying outputs:";
  {
  auto output_size = num_results();
  IR_ENFORCE(output_size == 1u,
                    "The size %d of outputs must be equal to 1.", output_size);
  IR_ENFORCE((*this)->result(0).type().isa<paddle::dialect::DenseTensorType>(),
                 "Type validation failed for the 0th output.");
  }
  VLOG(4) << "End Verifying for: ImagOp.";
}

void ImagOp::InferMeta( phi::InferMetaContext *infer_meta ) {
  auto fn = PD_INFER_META(phi::RealAndImagInferMeta);
  fn(infer_meta);
}

phi::DataType ImagOp::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: ImagOp";
  


  return expected_kernel_dtype;
}

const char *IndexAddOp::attributes_name[1] = { "axis" };

OpInfoTuple IndexAddOp::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("x", "paddle::dialect::DenseTensorType", false, false, false, true), paddle::dialect::OpInputInfo("index", "paddle::dialect::DenseTensorType", false, false, false, false), paddle::dialect::OpInputInfo("add_value", "paddle::dialect::DenseTensorType", false, false, false, true) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = { paddle::dialect::OpAttributeInfo("axis", "pir::Int32Attribute", "") };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("out", "paddle::dialect::DenseTensorType", false, false) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("IndexAddInferMeta", {"x", "index", "add_value", "axis"}, "index_add", {"x", "index", "add_value", "axis"}, {"x"}, {}, {}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "index_add");
}

void IndexAddOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, pir::Value index_, pir::Value add_value_, int axis) {
  VLOG(4) << "Start build IndexAddOp";



  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_, index_, add_value_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_axis = pir::Int32Attribute::get(pir::IrContext::Instance(), axis);
  argument.AddAttribute("axis", attr_axis);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;
  paddle::dialect::DenseTensorType index = index_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)index;
  paddle::dialect::DenseTensorType add_value = add_value_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)add_value;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);

  VLOG(4) << "Builder construction  dense_index";
  paddle::dialect::IrTensor ir_tensor_index(paddle::dialect::TransToPhiDataType(index.dtype()),
                                                      index.dims(),
                                                      index.data_layout(),
                                                      index.lod(),
                                                      index.offset());
  VLOG(4) << "Builder construction  meta_index";
  paddle::dialect::IrMetaTensor meta_index(&ir_tensor_index);

  VLOG(4) << "Builder construction  dense_add_value";
  paddle::dialect::IrTensor ir_tensor_add_value(paddle::dialect::TransToPhiDataType(add_value.dtype()),
                                                      add_value.dims(),
                                                      add_value.data_layout(),
                                                      add_value.lod(),
                                                      add_value.offset());
  VLOG(4) << "Builder construction  meta_add_value";
  paddle::dialect::IrMetaTensor meta_add_value(&ir_tensor_add_value);
  paddle::dialect::IrTensor dense_out;
  paddle::dialect::IrMetaTensor meta_out(&dense_out);

  phi::IndexAddInferMeta(meta_x, meta_index, meta_add_value, axis, &meta_out);

  std::vector<pir::Type> argument_outputs;
  pir::Type out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_out.dtype()), dense_out.dims(), dense_out.layout(), dense_out.lod(), dense_out.offset());
  argument_outputs.push_back(out_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void IndexAddOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, pir::Value index_, pir::Value add_value_, pir::AttributeMap attributes) {
  VLOG(4) << "Start build IndexAddOp";


  IR_ENFORCE(
      attributes.find("axis") != attributes.end(),
          "'axis' Attribute is expected for IndexAddOp. ");
  int axis = attributes.at("axis").dyn_cast<pir::Int32Attribute>().data();


  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_, index_, add_value_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_axis = pir::Int32Attribute::get(pir::IrContext::Instance(), axis);
  argument.AddAttribute("axis", attr_axis);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;
  paddle::dialect::DenseTensorType index = index_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)index;
  paddle::dialect::DenseTensorType add_value = add_value_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)add_value;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);

  VLOG(4) << "Builder construction  dense_index";
  paddle::dialect::IrTensor ir_tensor_index(paddle::dialect::TransToPhiDataType(index.dtype()),
                                                      index.dims(),
                                                      index.data_layout(),
                                                      index.lod(),
                                                      index.offset());
  VLOG(4) << "Builder construction  meta_index";
  paddle::dialect::IrMetaTensor meta_index(&ir_tensor_index);

  VLOG(4) << "Builder construction  dense_add_value";
  paddle::dialect::IrTensor ir_tensor_add_value(paddle::dialect::TransToPhiDataType(add_value.dtype()),
                                                      add_value.dims(),
                                                      add_value.data_layout(),
                                                      add_value.lod(),
                                                      add_value.offset());
  VLOG(4) << "Builder construction  meta_add_value";
  paddle::dialect::IrMetaTensor meta_add_value(&ir_tensor_add_value);
  paddle::dialect::IrTensor dense_out;
  paddle::dialect::IrMetaTensor meta_out(&dense_out);

  phi::IndexAddInferMeta(meta_x, meta_index, meta_add_value, axis, &meta_out);

  std::vector<pir::Type> argument_outputs;
  pir::Type out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_out.dtype()), dense_out.dims(), dense_out.layout(), dense_out.lod(), dense_out.offset());
  argument_outputs.push_back(out_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void IndexAddOp::VerifySig() {
  VLOG(4) << "Start Verifying inputs, outputs and attributes for: IndexAddOp.";
  VLOG(4) << "Verifying inputs:";
  {
  auto input_size = num_operands();
  IR_ENFORCE(input_size == 3u,
                    "The size %d of inputs must be equal to 3.", input_size);
  IR_ENFORCE((*this)->operand_source(0).type().isa<paddle::dialect::DenseTensorType>(),
                  "Type validation failed for the 0th input, got %s.", (*this)->operand_source(0).type());
  IR_ENFORCE((*this)->operand_source(1).type().isa<paddle::dialect::DenseTensorType>(),
                  "Type validation failed for the 1th input, got %s.", (*this)->operand_source(1).type());
  IR_ENFORCE((*this)->operand_source(2).type().isa<paddle::dialect::DenseTensorType>(),
                  "Type validation failed for the 2th input, got %s.", (*this)->operand_source(2).type());
  }
  VLOG(4) << "Verifying attributes:";
  {
  auto& attributes = this->attributes();
  IR_ENFORCE(attributes.count("axis")>0,
                 "axis does not exist.");
  IR_ENFORCE(attributes.at("axis").isa<pir::Int32Attribute>(),
                 "Type of attribute: axis is not pir::Int32Attribute.");

  }
  VLOG(4) << "Verifying outputs:";
  {
  auto output_size = num_results();
  IR_ENFORCE(output_size == 1u,
                    "The size %d of outputs must be equal to 1.", output_size);
  IR_ENFORCE((*this)->result(0).type().isa<paddle::dialect::DenseTensorType>(),
                 "Type validation failed for the 0th output.");
  }
  VLOG(4) << "End Verifying for: IndexAddOp.";
}

void IndexAddOp::InferMeta( phi::InferMetaContext *infer_meta ) {
  auto fn = PD_INFER_META(phi::IndexAddInferMeta);
  fn(infer_meta);
}

phi::DataType IndexAddOp::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: IndexAddOp";
  


  return expected_kernel_dtype;
}

const char *IndexAdd_Op::attributes_name[1] = { "axis" };

OpInfoTuple IndexAdd_Op::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("x", "paddle::dialect::DenseTensorType", false, false, false, true), paddle::dialect::OpInputInfo("index", "paddle::dialect::DenseTensorType", false, false, false, false), paddle::dialect::OpInputInfo("add_value", "paddle::dialect::DenseTensorType", false, false, false, true) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = { paddle::dialect::OpAttributeInfo("axis", "pir::Int32Attribute", "") };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("out", "paddle::dialect::DenseTensorType", false, false) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("IndexAddInferMeta", {"x", "index", "add_value", "axis"}, "index_add", {"x", "index", "add_value", "axis"}, {"x"}, {}, {{"out", "x"}}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "index_add");
}

void IndexAdd_Op::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, pir::Value index_, pir::Value add_value_, int axis) {
  VLOG(4) << "Start build IndexAdd_Op";



  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_, index_, add_value_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_axis = pir::Int32Attribute::get(pir::IrContext::Instance(), axis);
  argument.AddAttribute("axis", attr_axis);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;
  paddle::dialect::DenseTensorType index = index_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)index;
  paddle::dialect::DenseTensorType add_value = add_value_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)add_value;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);

  VLOG(4) << "Builder construction  dense_index";
  paddle::dialect::IrTensor ir_tensor_index(paddle::dialect::TransToPhiDataType(index.dtype()),
                                                      index.dims(),
                                                      index.data_layout(),
                                                      index.lod(),
                                                      index.offset());
  VLOG(4) << "Builder construction  meta_index";
  paddle::dialect::IrMetaTensor meta_index(&ir_tensor_index);

  VLOG(4) << "Builder construction  dense_add_value";
  paddle::dialect::IrTensor ir_tensor_add_value(paddle::dialect::TransToPhiDataType(add_value.dtype()),
                                                      add_value.dims(),
                                                      add_value.data_layout(),
                                                      add_value.lod(),
                                                      add_value.offset());
  VLOG(4) << "Builder construction  meta_add_value";
  paddle::dialect::IrMetaTensor meta_add_value(&ir_tensor_add_value);
  paddle::dialect::IrTensor dense_out;
  paddle::dialect::IrMetaTensor meta_out(&dense_out);

  phi::IndexAddInferMeta(meta_x, meta_index, meta_add_value, axis, &meta_out);

  std::vector<pir::Type> argument_outputs;
  pir::Type out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_out.dtype()), dense_out.dims(), dense_out.layout(), dense_out.lod(), dense_out.offset());
  argument_outputs.push_back(out_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void IndexAdd_Op::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, pir::Value index_, pir::Value add_value_, pir::AttributeMap attributes) {
  VLOG(4) << "Start build IndexAdd_Op";


  IR_ENFORCE(
      attributes.find("axis") != attributes.end(),
          "'axis' Attribute is expected for IndexAdd_Op. ");
  int axis = attributes.at("axis").dyn_cast<pir::Int32Attribute>().data();


  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_, index_, add_value_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_axis = pir::Int32Attribute::get(pir::IrContext::Instance(), axis);
  argument.AddAttribute("axis", attr_axis);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;
  paddle::dialect::DenseTensorType index = index_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)index;
  paddle::dialect::DenseTensorType add_value = add_value_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)add_value;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);

  VLOG(4) << "Builder construction  dense_index";
  paddle::dialect::IrTensor ir_tensor_index(paddle::dialect::TransToPhiDataType(index.dtype()),
                                                      index.dims(),
                                                      index.data_layout(),
                                                      index.lod(),
                                                      index.offset());
  VLOG(4) << "Builder construction  meta_index";
  paddle::dialect::IrMetaTensor meta_index(&ir_tensor_index);

  VLOG(4) << "Builder construction  dense_add_value";
  paddle::dialect::IrTensor ir_tensor_add_value(paddle::dialect::TransToPhiDataType(add_value.dtype()),
                                                      add_value.dims(),
                                                      add_value.data_layout(),
                                                      add_value.lod(),
                                                      add_value.offset());
  VLOG(4) << "Builder construction  meta_add_value";
  paddle::dialect::IrMetaTensor meta_add_value(&ir_tensor_add_value);
  paddle::dialect::IrTensor dense_out;
  paddle::dialect::IrMetaTensor meta_out(&dense_out);

  phi::IndexAddInferMeta(meta_x, meta_index, meta_add_value, axis, &meta_out);

  std::vector<pir::Type> argument_outputs;
  pir::Type out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_out.dtype()), dense_out.dims(), dense_out.layout(), dense_out.lod(), dense_out.offset());
  argument_outputs.push_back(out_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void IndexAdd_Op::VerifySig() {
  VLOG(4) << "Start Verifying inputs, outputs and attributes for: IndexAdd_Op.";
  VLOG(4) << "Verifying inputs:";
  {
  auto input_size = num_operands();
  IR_ENFORCE(input_size == 3u,
                    "The size %d of inputs must be equal to 3.", input_size);
  IR_ENFORCE((*this)->operand_source(0).type().isa<paddle::dialect::DenseTensorType>(),
                  "Type validation failed for the 0th input, got %s.", (*this)->operand_source(0).type());
  IR_ENFORCE((*this)->operand_source(1).type().isa<paddle::dialect::DenseTensorType>(),
                  "Type validation failed for the 1th input, got %s.", (*this)->operand_source(1).type());
  IR_ENFORCE((*this)->operand_source(2).type().isa<paddle::dialect::DenseTensorType>(),
                  "Type validation failed for the 2th input, got %s.", (*this)->operand_source(2).type());
  }
  VLOG(4) << "Verifying attributes:";
  {
  auto& attributes = this->attributes();
  IR_ENFORCE(attributes.count("axis")>0,
                 "axis does not exist.");
  IR_ENFORCE(attributes.at("axis").isa<pir::Int32Attribute>(),
                 "Type of attribute: axis is not pir::Int32Attribute.");

  }
  VLOG(4) << "Verifying outputs:";
  {
  auto output_size = num_results();
  IR_ENFORCE(output_size == 1u,
                    "The size %d of outputs must be equal to 1.", output_size);
  IR_ENFORCE((*this)->result(0).type().isa<paddle::dialect::DenseTensorType>(),
                 "Type validation failed for the 0th output.");
  }
  VLOG(4) << "End Verifying for: IndexAdd_Op.";
}

void IndexAdd_Op::InferMeta( phi::InferMetaContext *infer_meta ) {
  auto fn = PD_INFER_META(phi::IndexAddInferMeta);
  fn(infer_meta);
}

phi::DataType IndexAdd_Op::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: IndexAdd_Op";
  


  return expected_kernel_dtype;
}

const char *IndexPutOp::attributes_name[1] = { "accumulate" };

OpInfoTuple IndexPutOp::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("x", "paddle::dialect::DenseTensorType", false, false, false, true), paddle::dialect::OpInputInfo("indices", "pir::VectorType<paddle::dialect::DenseTensorType>", false, false, false, false), paddle::dialect::OpInputInfo("value", "paddle::dialect::DenseTensorType", false, false, false, true) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = { paddle::dialect::OpAttributeInfo("accumulate", "pir::BoolAttribute", "") };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("out", "paddle::dialect::DenseTensorType", false, false) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("IndexPutInferMeta", {"x", "indices", "value", "accumulate"}, "index_put", {"x", "indices", "value", "accumulate"}, {"x"}, {}, {}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "index_put");
}

void IndexPutOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, pir::Value indices_, pir::Value value_, bool accumulate) {
  VLOG(4) << "Start build IndexPutOp";



  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_, indices_, value_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_accumulate = pir::BoolAttribute::get(pir::IrContext::Instance(), accumulate);
  argument.AddAttribute("accumulate", attr_accumulate);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;
  pir::VectorType indices = indices_.type().dyn_cast<pir::VectorType>(); (void)indices;
  paddle::dialect::DenseTensorType value = value_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)value;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);
  std::vector<paddle::dialect::IrTensor> vec_ir_tensor_indices;
  for (size_t i=0; i < static_cast<size_t>(indices.size()); i++) {
    vec_ir_tensor_indices.push_back(paddle::dialect::IrTensor(paddle::dialect::TransToPhiDataType(indices[i].dyn_cast<paddle::dialect::DenseTensorType>().dtype()),
                                                                     indices[i].dyn_cast<paddle::dialect::DenseTensorType>().dims(),
                                                                     indices[i].dyn_cast<paddle::dialect::DenseTensorType>().data_layout(),
                                                                     indices[i].dyn_cast<paddle::dialect::DenseTensorType>().lod(),
                                                                     indices[i].dyn_cast<paddle::dialect::DenseTensorType>().offset()));
  }
  std::vector<paddle::dialect::IrMetaTensor> vec_meta_indices;
  for (size_t i=0; i < vec_ir_tensor_indices.size(); i++) {
    vec_meta_indices.push_back(paddle::dialect::IrMetaTensor(&vec_ir_tensor_indices[i]));
  }

  std::vector<const phi::MetaTensor*> meta_indices;
  for (size_t i=0; i < static_cast<size_t>(vec_meta_indices.size()); i++) {
    meta_indices.push_back(&vec_meta_indices[i]);
  }
 
  VLOG(4) << "Builder construction  dense_value";
  paddle::dialect::IrTensor ir_tensor_value(paddle::dialect::TransToPhiDataType(value.dtype()),
                                                      value.dims(),
                                                      value.data_layout(),
                                                      value.lod(),
                                                      value.offset());
  VLOG(4) << "Builder construction  meta_value";
  paddle::dialect::IrMetaTensor meta_value(&ir_tensor_value);
  paddle::dialect::IrTensor dense_out;
  paddle::dialect::IrMetaTensor meta_out(&dense_out);

  phi::IndexPutInferMeta(meta_x, meta_indices, meta_value, accumulate, &meta_out);

  std::vector<pir::Type> argument_outputs;
  pir::Type out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_out.dtype()), dense_out.dims(), dense_out.layout(), dense_out.lod(), dense_out.offset());
  argument_outputs.push_back(out_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void IndexPutOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, pir::Value indices_, pir::Value value_, pir::AttributeMap attributes) {
  VLOG(4) << "Start build IndexPutOp";


  IR_ENFORCE(
      attributes.find("accumulate") != attributes.end(),
          "'accumulate' Attribute is expected for IndexPutOp. ");
  bool accumulate = attributes.at("accumulate").dyn_cast<pir::BoolAttribute>().data();


  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_, indices_, value_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_accumulate = pir::BoolAttribute::get(pir::IrContext::Instance(), accumulate);
  argument.AddAttribute("accumulate", attr_accumulate);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;
  pir::VectorType indices = indices_.type().dyn_cast<pir::VectorType>(); (void)indices;
  paddle::dialect::DenseTensorType value = value_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)value;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);
  std::vector<paddle::dialect::IrTensor> vec_ir_tensor_indices;
  for (size_t i=0; i < static_cast<size_t>(indices.size()); i++) {
    vec_ir_tensor_indices.push_back(paddle::dialect::IrTensor(paddle::dialect::TransToPhiDataType(indices[i].dyn_cast<paddle::dialect::DenseTensorType>().dtype()),
                                                                     indices[i].dyn_cast<paddle::dialect::DenseTensorType>().dims(),
                                                                     indices[i].dyn_cast<paddle::dialect::DenseTensorType>().data_layout(),
                                                                     indices[i].dyn_cast<paddle::dialect::DenseTensorType>().lod(),
                                                                     indices[i].dyn_cast<paddle::dialect::DenseTensorType>().offset()));
  }
  std::vector<paddle::dialect::IrMetaTensor> vec_meta_indices;
  for (size_t i=0; i < vec_ir_tensor_indices.size(); i++) {
    vec_meta_indices.push_back(paddle::dialect::IrMetaTensor(&vec_ir_tensor_indices[i]));
  }

  std::vector<const phi::MetaTensor*> meta_indices;
  for (size_t i=0; i < static_cast<size_t>(vec_meta_indices.size()); i++) {
    meta_indices.push_back(&vec_meta_indices[i]);
  }
 
  VLOG(4) << "Builder construction  dense_value";
  paddle::dialect::IrTensor ir_tensor_value(paddle::dialect::TransToPhiDataType(value.dtype()),
                                                      value.dims(),
                                                      value.data_layout(),
                                                      value.lod(),
                                                      value.offset());
  VLOG(4) << "Builder construction  meta_value";
  paddle::dialect::IrMetaTensor meta_value(&ir_tensor_value);
  paddle::dialect::IrTensor dense_out;
  paddle::dialect::IrMetaTensor meta_out(&dense_out);

  phi::IndexPutInferMeta(meta_x, meta_indices, meta_value, accumulate, &meta_out);

  std::vector<pir::Type> argument_outputs;
  pir::Type out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_out.dtype()), dense_out.dims(), dense_out.layout(), dense_out.lod(), dense_out.offset());
  argument_outputs.push_back(out_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void IndexPutOp::VerifySig() {
  VLOG(4) << "Start Verifying inputs, outputs and attributes for: IndexPutOp.";
  VLOG(4) << "Verifying inputs:";
  {
  auto input_size = num_operands();
  IR_ENFORCE(input_size == 3u,
                    "The size %d of inputs must be equal to 3.", input_size);
  IR_ENFORCE((*this)->operand_source(0).type().isa<paddle::dialect::DenseTensorType>(),
                  "Type validation failed for the 0th input, got %s.", (*this)->operand_source(0).type());
  if (auto vec_type = (*this)->operand_source(1).type().dyn_cast<pir::VectorType>()) {
      for (size_t i = 0; i < vec_type.size(); ++i) {
        IR_ENFORCE(vec_type[i].isa<paddle::dialect::DenseTensorType>(),
                       "Type validation failed for the 1th input, got %s.", (*this)->operand_source(1).type());
      }
  }
  else {
    IR_ENFORCE((*this)->operand_source(1).type().isa<paddle::dialect::DenseTensorType>(),
                   "Type validation failed for the 1th input, got %s.", (*this)->operand_source(1).type());
  }
  IR_ENFORCE((*this)->operand_source(2).type().isa<paddle::dialect::DenseTensorType>(),
                  "Type validation failed for the 2th input, got %s.", (*this)->operand_source(2).type());
  }
  VLOG(4) << "Verifying attributes:";
  {
  auto& attributes = this->attributes();
  IR_ENFORCE(attributes.count("accumulate")>0,
                 "accumulate does not exist.");
  IR_ENFORCE(attributes.at("accumulate").isa<pir::BoolAttribute>(),
                 "Type of attribute: accumulate is not pir::BoolAttribute.");

  }
  VLOG(4) << "Verifying outputs:";
  {
  auto output_size = num_results();
  IR_ENFORCE(output_size == 1u,
                    "The size %d of outputs must be equal to 1.", output_size);
  IR_ENFORCE((*this)->result(0).type().isa<paddle::dialect::DenseTensorType>(),
                 "Type validation failed for the 0th output.");
  }
  VLOG(4) << "End Verifying for: IndexPutOp.";
}

void IndexPutOp::InferMeta( phi::InferMetaContext *infer_meta ) {
  auto fn = PD_INFER_META(phi::IndexPutInferMeta);
  fn(infer_meta);
}

phi::DataType IndexPutOp::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: IndexPutOp";
  


  return expected_kernel_dtype;
}

const char *IndexPut_Op::attributes_name[1] = { "accumulate" };

OpInfoTuple IndexPut_Op::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("x", "paddle::dialect::DenseTensorType", false, false, false, true), paddle::dialect::OpInputInfo("indices", "pir::VectorType<paddle::dialect::DenseTensorType>", false, false, false, false), paddle::dialect::OpInputInfo("value", "paddle::dialect::DenseTensorType", false, false, false, true) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = { paddle::dialect::OpAttributeInfo("accumulate", "pir::BoolAttribute", "") };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("out", "paddle::dialect::DenseTensorType", false, false) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("IndexPutInferMeta", {"x", "indices", "value", "accumulate"}, "index_put", {"x", "indices", "value", "accumulate"}, {"x"}, {}, {{"out", "x"}}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "index_put");
}

void IndexPut_Op::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, pir::Value indices_, pir::Value value_, bool accumulate) {
  VLOG(4) << "Start build IndexPut_Op";



  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_, indices_, value_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_accumulate = pir::BoolAttribute::get(pir::IrContext::Instance(), accumulate);
  argument.AddAttribute("accumulate", attr_accumulate);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;
  pir::VectorType indices = indices_.type().dyn_cast<pir::VectorType>(); (void)indices;
  paddle::dialect::DenseTensorType value = value_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)value;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);
  std::vector<paddle::dialect::IrTensor> vec_ir_tensor_indices;
  for (size_t i=0; i < static_cast<size_t>(indices.size()); i++) {
    vec_ir_tensor_indices.push_back(paddle::dialect::IrTensor(paddle::dialect::TransToPhiDataType(indices[i].dyn_cast<paddle::dialect::DenseTensorType>().dtype()),
                                                                     indices[i].dyn_cast<paddle::dialect::DenseTensorType>().dims(),
                                                                     indices[i].dyn_cast<paddle::dialect::DenseTensorType>().data_layout(),
                                                                     indices[i].dyn_cast<paddle::dialect::DenseTensorType>().lod(),
                                                                     indices[i].dyn_cast<paddle::dialect::DenseTensorType>().offset()));
  }
  std::vector<paddle::dialect::IrMetaTensor> vec_meta_indices;
  for (size_t i=0; i < vec_ir_tensor_indices.size(); i++) {
    vec_meta_indices.push_back(paddle::dialect::IrMetaTensor(&vec_ir_tensor_indices[i]));
  }

  std::vector<const phi::MetaTensor*> meta_indices;
  for (size_t i=0; i < static_cast<size_t>(vec_meta_indices.size()); i++) {
    meta_indices.push_back(&vec_meta_indices[i]);
  }
 
  VLOG(4) << "Builder construction  dense_value";
  paddle::dialect::IrTensor ir_tensor_value(paddle::dialect::TransToPhiDataType(value.dtype()),
                                                      value.dims(),
                                                      value.data_layout(),
                                                      value.lod(),
                                                      value.offset());
  VLOG(4) << "Builder construction  meta_value";
  paddle::dialect::IrMetaTensor meta_value(&ir_tensor_value);
  paddle::dialect::IrTensor dense_out;
  paddle::dialect::IrMetaTensor meta_out(&dense_out);

  phi::IndexPutInferMeta(meta_x, meta_indices, meta_value, accumulate, &meta_out);

  std::vector<pir::Type> argument_outputs;
  pir::Type out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_out.dtype()), dense_out.dims(), dense_out.layout(), dense_out.lod(), dense_out.offset());
  argument_outputs.push_back(out_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void IndexPut_Op::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, pir::Value indices_, pir::Value value_, pir::AttributeMap attributes) {
  VLOG(4) << "Start build IndexPut_Op";


  IR_ENFORCE(
      attributes.find("accumulate") != attributes.end(),
          "'accumulate' Attribute is expected for IndexPut_Op. ");
  bool accumulate = attributes.at("accumulate").dyn_cast<pir::BoolAttribute>().data();


  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_, indices_, value_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_accumulate = pir::BoolAttribute::get(pir::IrContext::Instance(), accumulate);
  argument.AddAttribute("accumulate", attr_accumulate);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;
  pir::VectorType indices = indices_.type().dyn_cast<pir::VectorType>(); (void)indices;
  paddle::dialect::DenseTensorType value = value_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)value;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);
  std::vector<paddle::dialect::IrTensor> vec_ir_tensor_indices;
  for (size_t i=0; i < static_cast<size_t>(indices.size()); i++) {
    vec_ir_tensor_indices.push_back(paddle::dialect::IrTensor(paddle::dialect::TransToPhiDataType(indices[i].dyn_cast<paddle::dialect::DenseTensorType>().dtype()),
                                                                     indices[i].dyn_cast<paddle::dialect::DenseTensorType>().dims(),
                                                                     indices[i].dyn_cast<paddle::dialect::DenseTensorType>().data_layout(),
                                                                     indices[i].dyn_cast<paddle::dialect::DenseTensorType>().lod(),
                                                                     indices[i].dyn_cast<paddle::dialect::DenseTensorType>().offset()));
  }
  std::vector<paddle::dialect::IrMetaTensor> vec_meta_indices;
  for (size_t i=0; i < vec_ir_tensor_indices.size(); i++) {
    vec_meta_indices.push_back(paddle::dialect::IrMetaTensor(&vec_ir_tensor_indices[i]));
  }

  std::vector<const phi::MetaTensor*> meta_indices;
  for (size_t i=0; i < static_cast<size_t>(vec_meta_indices.size()); i++) {
    meta_indices.push_back(&vec_meta_indices[i]);
  }
 
  VLOG(4) << "Builder construction  dense_value";
  paddle::dialect::IrTensor ir_tensor_value(paddle::dialect::TransToPhiDataType(value.dtype()),
                                                      value.dims(),
                                                      value.data_layout(),
                                                      value.lod(),
                                                      value.offset());
  VLOG(4) << "Builder construction  meta_value";
  paddle::dialect::IrMetaTensor meta_value(&ir_tensor_value);
  paddle::dialect::IrTensor dense_out;
  paddle::dialect::IrMetaTensor meta_out(&dense_out);

  phi::IndexPutInferMeta(meta_x, meta_indices, meta_value, accumulate, &meta_out);

  std::vector<pir::Type> argument_outputs;
  pir::Type out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_out.dtype()), dense_out.dims(), dense_out.layout(), dense_out.lod(), dense_out.offset());
  argument_outputs.push_back(out_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void IndexPut_Op::VerifySig() {
  VLOG(4) << "Start Verifying inputs, outputs and attributes for: IndexPut_Op.";
  VLOG(4) << "Verifying inputs:";
  {
  auto input_size = num_operands();
  IR_ENFORCE(input_size == 3u,
                    "The size %d of inputs must be equal to 3.", input_size);
  IR_ENFORCE((*this)->operand_source(0).type().isa<paddle::dialect::DenseTensorType>(),
                  "Type validation failed for the 0th input, got %s.", (*this)->operand_source(0).type());
  if (auto vec_type = (*this)->operand_source(1).type().dyn_cast<pir::VectorType>()) {
      for (size_t i = 0; i < vec_type.size(); ++i) {
        IR_ENFORCE(vec_type[i].isa<paddle::dialect::DenseTensorType>(),
                       "Type validation failed for the 1th input, got %s.", (*this)->operand_source(1).type());
      }
  }
  else {
    IR_ENFORCE((*this)->operand_source(1).type().isa<paddle::dialect::DenseTensorType>(),
                   "Type validation failed for the 1th input, got %s.", (*this)->operand_source(1).type());
  }
  IR_ENFORCE((*this)->operand_source(2).type().isa<paddle::dialect::DenseTensorType>(),
                  "Type validation failed for the 2th input, got %s.", (*this)->operand_source(2).type());
  }
  VLOG(4) << "Verifying attributes:";
  {
  auto& attributes = this->attributes();
  IR_ENFORCE(attributes.count("accumulate")>0,
                 "accumulate does not exist.");
  IR_ENFORCE(attributes.at("accumulate").isa<pir::BoolAttribute>(),
                 "Type of attribute: accumulate is not pir::BoolAttribute.");

  }
  VLOG(4) << "Verifying outputs:";
  {
  auto output_size = num_results();
  IR_ENFORCE(output_size == 1u,
                    "The size %d of outputs must be equal to 1.", output_size);
  IR_ENFORCE((*this)->result(0).type().isa<paddle::dialect::DenseTensorType>(),
                 "Type validation failed for the 0th output.");
  }
  VLOG(4) << "End Verifying for: IndexPut_Op.";
}

void IndexPut_Op::InferMeta( phi::InferMetaContext *infer_meta ) {
  auto fn = PD_INFER_META(phi::IndexPutInferMeta);
  fn(infer_meta);
}

phi::DataType IndexPut_Op::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: IndexPut_Op";
  


  return expected_kernel_dtype;
}

OpInfoTuple IndexSampleOp::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("x", "paddle::dialect::DenseTensorType", false, false, false, true), paddle::dialect::OpInputInfo("index", "paddle::dialect::DenseTensorType", false, false, false, false) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = {  };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("out", "paddle::dialect::DenseTensorType", false, false) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("IndexSampleInferMeta", {"x", "index"}, "index_sample", {"x", "index"}, {"x"}, {}, {}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "index_sample");
}

void IndexSampleOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, pir::Value index_) {
  VLOG(4) << "Start build IndexSampleOp";



  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_, index_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;
  paddle::dialect::DenseTensorType index = index_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)index;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);

  VLOG(4) << "Builder construction  dense_index";
  paddle::dialect::IrTensor ir_tensor_index(paddle::dialect::TransToPhiDataType(index.dtype()),
                                                      index.dims(),
                                                      index.data_layout(),
                                                      index.lod(),
                                                      index.offset());
  VLOG(4) << "Builder construction  meta_index";
  paddle::dialect::IrMetaTensor meta_index(&ir_tensor_index);
  paddle::dialect::IrTensor dense_out;
  paddle::dialect::IrMetaTensor meta_out(&dense_out);

  phi::IndexSampleInferMeta(meta_x, meta_index, &meta_out);

  std::vector<pir::Type> argument_outputs;
  pir::Type out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_out.dtype()), dense_out.dims(), dense_out.layout(), dense_out.lod(), dense_out.offset());
  argument_outputs.push_back(out_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void IndexSampleOp::VerifySig() {
  VLOG(4) << "Start Verifying inputs, outputs and attributes for: IndexSampleOp.";
  VLOG(4) << "Verifying inputs:";
  {
  auto input_size = num_operands();
  IR_ENFORCE(input_size == 2u,
                    "The size %d of inputs must be equal to 2.", input_size);
  IR_ENFORCE((*this)->operand_source(0).type().isa<paddle::dialect::DenseTensorType>(),
                  "Type validation failed for the 0th input, got %s.", (*this)->operand_source(0).type());
  IR_ENFORCE((*this)->operand_source(1).type().isa<paddle::dialect::DenseTensorType>(),
                  "Type validation failed for the 1th input, got %s.", (*this)->operand_source(1).type());
  }
  VLOG(4) << "Verifying attributes:";
  {
  // Attributes num is 0, not need to check attributes type.
  }
  VLOG(4) << "Verifying outputs:";
  {
  auto output_size = num_results();
  IR_ENFORCE(output_size == 1u,
                    "The size %d of outputs must be equal to 1.", output_size);
  IR_ENFORCE((*this)->result(0).type().isa<paddle::dialect::DenseTensorType>(),
                 "Type validation failed for the 0th output.");
  }
  VLOG(4) << "End Verifying for: IndexSampleOp.";
}

void IndexSampleOp::InferMeta( phi::InferMetaContext *infer_meta ) {
  auto fn = PD_INFER_META(phi::IndexSampleInferMeta);
  fn(infer_meta);
}

phi::DataType IndexSampleOp::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: IndexSampleOp";
  

  // deal skip data transform
  if (var_name == "index"){
    return expected_kernel_dtype;
  }


  return expected_kernel_dtype;
}

const char *IndexSelectOp::attributes_name[1] = { "axis" };

OpInfoTuple IndexSelectOp::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("x", "paddle::dialect::DenseTensorType", false, false, false, true), paddle::dialect::OpInputInfo("index", "paddle::dialect::DenseTensorType", false, false, false, false) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = { paddle::dialect::OpAttributeInfo("axis", "pir::Int32Attribute", "") };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("out", "paddle::dialect::DenseTensorType", false, false) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("IndexSelectInferMeta", {"x", "index", "axis"}, "index_select", {"x", "index", "axis"}, {"x"}, {}, {}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "index_select");
}

void IndexSelectOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, pir::Value index_, int axis) {
  VLOG(4) << "Start build IndexSelectOp";



  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_, index_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_axis = pir::Int32Attribute::get(pir::IrContext::Instance(), axis);
  argument.AddAttribute("axis", attr_axis);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;
  paddle::dialect::DenseTensorType index = index_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)index;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);

  VLOG(4) << "Builder construction  dense_index";
  paddle::dialect::IrTensor ir_tensor_index(paddle::dialect::TransToPhiDataType(index.dtype()),
                                                      index.dims(),
                                                      index.data_layout(),
                                                      index.lod(),
                                                      index.offset());
  VLOG(4) << "Builder construction  meta_index";
  paddle::dialect::IrMetaTensor meta_index(&ir_tensor_index);
  paddle::dialect::IrTensor dense_out;
  paddle::dialect::IrMetaTensor meta_out(&dense_out);

  phi::IndexSelectInferMeta(meta_x, meta_index, axis, &meta_out);

  std::vector<pir::Type> argument_outputs;
  pir::Type out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_out.dtype()), dense_out.dims(), dense_out.layout(), dense_out.lod(), dense_out.offset());
  argument_outputs.push_back(out_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void IndexSelectOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, pir::Value index_, pir::AttributeMap attributes) {
  VLOG(4) << "Start build IndexSelectOp";


  IR_ENFORCE(
      attributes.find("axis") != attributes.end(),
          "'axis' Attribute is expected for IndexSelectOp. ");
  int axis = attributes.at("axis").dyn_cast<pir::Int32Attribute>().data();


  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_, index_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_axis = pir::Int32Attribute::get(pir::IrContext::Instance(), axis);
  argument.AddAttribute("axis", attr_axis);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;
  paddle::dialect::DenseTensorType index = index_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)index;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);

  VLOG(4) << "Builder construction  dense_index";
  paddle::dialect::IrTensor ir_tensor_index(paddle::dialect::TransToPhiDataType(index.dtype()),
                                                      index.dims(),
                                                      index.data_layout(),
                                                      index.lod(),
                                                      index.offset());
  VLOG(4) << "Builder construction  meta_index";
  paddle::dialect::IrMetaTensor meta_index(&ir_tensor_index);
  paddle::dialect::IrTensor dense_out;
  paddle::dialect::IrMetaTensor meta_out(&dense_out);

  phi::IndexSelectInferMeta(meta_x, meta_index, axis, &meta_out);

  std::vector<pir::Type> argument_outputs;
  pir::Type out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_out.dtype()), dense_out.dims(), dense_out.layout(), dense_out.lod(), dense_out.offset());
  argument_outputs.push_back(out_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void IndexSelectOp::VerifySig() {
  VLOG(4) << "Start Verifying inputs, outputs and attributes for: IndexSelectOp.";
  VLOG(4) << "Verifying inputs:";
  {
  auto input_size = num_operands();
  IR_ENFORCE(input_size == 2u,
                    "The size %d of inputs must be equal to 2.", input_size);
  IR_ENFORCE((*this)->operand_source(0).type().isa<paddle::dialect::DenseTensorType>(),
                  "Type validation failed for the 0th input, got %s.", (*this)->operand_source(0).type());
  IR_ENFORCE((*this)->operand_source(1).type().isa<paddle::dialect::DenseTensorType>(),
                  "Type validation failed for the 1th input, got %s.", (*this)->operand_source(1).type());
  }
  VLOG(4) << "Verifying attributes:";
  {
  auto& attributes = this->attributes();
  IR_ENFORCE(attributes.count("axis")>0,
                 "axis does not exist.");
  IR_ENFORCE(attributes.at("axis").isa<pir::Int32Attribute>(),
                 "Type of attribute: axis is not pir::Int32Attribute.");

  }
  VLOG(4) << "Verifying outputs:";
  {
  auto output_size = num_results();
  IR_ENFORCE(output_size == 1u,
                    "The size %d of outputs must be equal to 1.", output_size);
  IR_ENFORCE((*this)->result(0).type().isa<paddle::dialect::DenseTensorType>(),
                 "Type validation failed for the 0th output.");
  }
  VLOG(4) << "End Verifying for: IndexSelectOp.";
}

void IndexSelectOp::InferMeta( phi::InferMetaContext *infer_meta ) {
  auto fn = PD_INFER_META(phi::IndexSelectInferMeta);
  fn(infer_meta);
}

phi::DataType IndexSelectOp::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: IndexSelectOp";
  

  // deal skip data transform
  if (var_name == "index"){
    return expected_kernel_dtype;
  }


  return expected_kernel_dtype;
}

const char *IndexSelectStridedOp::attributes_name[2] = { "index", "axis" };

OpInfoTuple IndexSelectStridedOp::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("x", "paddle::dialect::DenseTensorType", false, false, false, true) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = { paddle::dialect::OpAttributeInfo("index", "pir::Int64Attribute", ""), paddle::dialect::OpAttributeInfo("axis", "pir::Int32Attribute", "") };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("out", "paddle::dialect::DenseTensorType", false, false) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("IndexSelectStridedInferMeta", {"x", "index", "axis"}, "index_select_strided", {"x", "index", "axis"}, {"x"}, {}, {}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "index_select_strided");
}

void IndexSelectStridedOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, int64_t index, int axis) {
  VLOG(4) << "Start build IndexSelectStridedOp";



  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_index = pir::Int64Attribute::get(pir::IrContext::Instance(), index);
  argument.AddAttribute("index", attr_index);
  pir::Attribute attr_axis = pir::Int32Attribute::get(pir::IrContext::Instance(), axis);
  argument.AddAttribute("axis", attr_axis);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);
  paddle::dialect::IrTensor dense_out;
  paddle::dialect::IrMetaTensor meta_out(&dense_out);

  phi::IndexSelectStridedInferMeta(meta_x, index, axis, &meta_out);

  std::vector<pir::Type> argument_outputs;
  pir::Type out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_out.dtype()), dense_out.dims(), dense_out.layout(), dense_out.lod(), dense_out.offset());
  argument_outputs.push_back(out_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void IndexSelectStridedOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, pir::AttributeMap attributes) {
  VLOG(4) << "Start build IndexSelectStridedOp";


  IR_ENFORCE(
      attributes.find("index") != attributes.end(),
          "'index' Attribute is expected for IndexSelectStridedOp. ");
  int64_t index = attributes.at("index").dyn_cast<pir::Int64Attribute>().data();

  IR_ENFORCE(
      attributes.find("axis") != attributes.end(),
          "'axis' Attribute is expected for IndexSelectStridedOp. ");
  int axis = attributes.at("axis").dyn_cast<pir::Int32Attribute>().data();


  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_index = pir::Int64Attribute::get(pir::IrContext::Instance(), index);
  argument.AddAttribute("index", attr_index);
  pir::Attribute attr_axis = pir::Int32Attribute::get(pir::IrContext::Instance(), axis);
  argument.AddAttribute("axis", attr_axis);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);
  paddle::dialect::IrTensor dense_out;
  paddle::dialect::IrMetaTensor meta_out(&dense_out);

  phi::IndexSelectStridedInferMeta(meta_x, index, axis, &meta_out);

  std::vector<pir::Type> argument_outputs;
  pir::Type out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_out.dtype()), dense_out.dims(), dense_out.layout(), dense_out.lod(), dense_out.offset());
  argument_outputs.push_back(out_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void IndexSelectStridedOp::VerifySig() {
  VLOG(4) << "Start Verifying inputs, outputs and attributes for: IndexSelectStridedOp.";
  VLOG(4) << "Verifying inputs:";
  {
  auto input_size = num_operands();
  IR_ENFORCE(input_size == 1u,
                    "The size %d of inputs must be equal to 1.", input_size);
  IR_ENFORCE((*this)->operand_source(0).type().isa<paddle::dialect::DenseTensorType>(),
                  "Type validation failed for the 0th input, got %s.", (*this)->operand_source(0).type());
  }
  VLOG(4) << "Verifying attributes:";
  {
  auto& attributes = this->attributes();
  IR_ENFORCE(attributes.count("index")>0,
                 "index does not exist.");
  IR_ENFORCE(attributes.at("index").isa<pir::Int64Attribute>(),
                 "Type of attribute: index is not pir::Int64Attribute.");

  IR_ENFORCE(attributes.count("axis")>0,
                 "axis does not exist.");
  IR_ENFORCE(attributes.at("axis").isa<pir::Int32Attribute>(),
                 "Type of attribute: axis is not pir::Int32Attribute.");

  }
  VLOG(4) << "Verifying outputs:";
  {
  auto output_size = num_results();
  IR_ENFORCE(output_size == 1u,
                    "The size %d of outputs must be equal to 1.", output_size);
  IR_ENFORCE((*this)->result(0).type().isa<paddle::dialect::DenseTensorType>(),
                 "Type validation failed for the 0th output.");
  }
  VLOG(4) << "End Verifying for: IndexSelectStridedOp.";
}

void IndexSelectStridedOp::InferMeta( phi::InferMetaContext *infer_meta ) {
  auto fn = PD_INFER_META(phi::IndexSelectStridedInferMeta);
  fn(infer_meta);
}

phi::DataType IndexSelectStridedOp::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: IndexSelectStridedOp";
  


  return expected_kernel_dtype;
}

const char *InstanceNormOp::attributes_name[1] = { "epsilon" };

OpInfoTuple InstanceNormOp::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("x", "paddle::dialect::DenseTensorType", false, false, false, true), paddle::dialect::OpInputInfo("scale", "paddle::dialect::DenseTensorType", true, false, false, true), paddle::dialect::OpInputInfo("bias", "paddle::dialect::DenseTensorType", true, false, false, true) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = { paddle::dialect::OpAttributeInfo("epsilon", "pir::FloatAttribute", "") };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("y", "paddle::dialect::DenseTensorType", false, false), paddle::dialect::OpOutputInfo("saved_mean", "paddle::dialect::DenseTensorType", true, true), paddle::dialect::OpOutputInfo("saved_variance", "paddle::dialect::DenseTensorType", true, true) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("InstanceNormInferMeta", {"x", "scale", "bias", "epsilon"}, "instance_norm", {"x", "scale", "bias", "epsilon"}, {"x"}, {}, {}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "instance_norm");
}

void InstanceNormOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, pir::Value scale_, pir::Value bias_, float epsilon) {
  VLOG(4) << "Start build InstanceNormOp";



  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_, scale_, bias_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_epsilon = pir::FloatAttribute::get(pir::IrContext::Instance(), epsilon);
  argument.AddAttribute("epsilon", attr_epsilon);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);

  paddle::dialect::IrMetaTensor meta_scale;
  paddle::dialect::IrTensor ir_tensor_scale;
  if (scale_.impl() != nullptr) {
    paddle::dialect::DenseTensorType scale = scale_.type().dyn_cast<paddle::dialect::DenseTensorType>();
    VLOG(4) << "Builder construction  dense_scale";
    ir_tensor_scale = paddle::dialect::IrTensor(paddle::dialect::TransToPhiDataType(scale.dtype()),
                                                        scale.dims(),
                                                        scale.data_layout(),
                                                        scale.lod(),
                                                        scale.offset());
    VLOG(4) << "Builder construction  meta_scale";
    meta_scale = paddle::dialect::IrMetaTensor(&ir_tensor_scale);
  }


  paddle::dialect::IrMetaTensor meta_bias;
  paddle::dialect::IrTensor ir_tensor_bias;
  if (bias_.impl() != nullptr) {
    paddle::dialect::DenseTensorType bias = bias_.type().dyn_cast<paddle::dialect::DenseTensorType>();
    VLOG(4) << "Builder construction  dense_bias";
    ir_tensor_bias = paddle::dialect::IrTensor(paddle::dialect::TransToPhiDataType(bias.dtype()),
                                                        bias.dims(),
                                                        bias.data_layout(),
                                                        bias.lod(),
                                                        bias.offset());
    VLOG(4) << "Builder construction  meta_bias";
    meta_bias = paddle::dialect::IrMetaTensor(&ir_tensor_bias);
  }

  paddle::dialect::IrTensor dense_y;
  paddle::dialect::IrMetaTensor meta_y(&dense_y);
  paddle::dialect::IrTensor dense_saved_mean;
  paddle::dialect::IrMetaTensor meta_saved_mean(&dense_saved_mean);
  paddle::dialect::IrTensor dense_saved_variance;
  paddle::dialect::IrMetaTensor meta_saved_variance(&dense_saved_variance);

  phi::InstanceNormInferMeta(meta_x, meta_scale, meta_bias, epsilon, &meta_y, &meta_saved_mean, &meta_saved_variance);

  std::vector<pir::Type> argument_outputs;
  pir::Type y_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_y.dtype()), dense_y.dims(), dense_y.layout(), dense_y.lod(), dense_y.offset());
  argument_outputs.push_back(y_dense_tensor_type);

  pir::Type saved_mean_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_saved_mean.dtype()), dense_saved_mean.dims(), dense_saved_mean.layout(), dense_saved_mean.lod(), dense_saved_mean.offset());
  argument_outputs.push_back(saved_mean_dense_tensor_type);

  pir::Type saved_variance_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_saved_variance.dtype()), dense_saved_variance.dims(), dense_saved_variance.layout(), dense_saved_variance.lod(), dense_saved_variance.offset());
  argument_outputs.push_back(saved_variance_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void InstanceNormOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, pir::Value scale_, pir::Value bias_, pir::AttributeMap attributes) {
  VLOG(4) << "Start build InstanceNormOp";


  IR_ENFORCE(
      attributes.find("epsilon") != attributes.end(),
          "'epsilon' Attribute is expected for InstanceNormOp. ");
  float epsilon = attributes.at("epsilon").dyn_cast<pir::FloatAttribute>().data();


  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_, scale_, bias_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_epsilon = pir::FloatAttribute::get(pir::IrContext::Instance(), epsilon);
  argument.AddAttribute("epsilon", attr_epsilon);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);

  paddle::dialect::IrMetaTensor meta_scale;
  paddle::dialect::IrTensor ir_tensor_scale;
  if (scale_.impl() != nullptr) {
    paddle::dialect::DenseTensorType scale = scale_.type().dyn_cast<paddle::dialect::DenseTensorType>();
    VLOG(4) << "Builder construction  dense_scale";
    ir_tensor_scale = paddle::dialect::IrTensor(paddle::dialect::TransToPhiDataType(scale.dtype()),
                                                        scale.dims(),
                                                        scale.data_layout(),
                                                        scale.lod(),
                                                        scale.offset());
    VLOG(4) << "Builder construction  meta_scale";
    meta_scale = paddle::dialect::IrMetaTensor(&ir_tensor_scale);
  }


  paddle::dialect::IrMetaTensor meta_bias;
  paddle::dialect::IrTensor ir_tensor_bias;
  if (bias_.impl() != nullptr) {
    paddle::dialect::DenseTensorType bias = bias_.type().dyn_cast<paddle::dialect::DenseTensorType>();
    VLOG(4) << "Builder construction  dense_bias";
    ir_tensor_bias = paddle::dialect::IrTensor(paddle::dialect::TransToPhiDataType(bias.dtype()),
                                                        bias.dims(),
                                                        bias.data_layout(),
                                                        bias.lod(),
                                                        bias.offset());
    VLOG(4) << "Builder construction  meta_bias";
    meta_bias = paddle::dialect::IrMetaTensor(&ir_tensor_bias);
  }

  paddle::dialect::IrTensor dense_y;
  paddle::dialect::IrMetaTensor meta_y(&dense_y);
  paddle::dialect::IrTensor dense_saved_mean;
  paddle::dialect::IrMetaTensor meta_saved_mean(&dense_saved_mean);
  paddle::dialect::IrTensor dense_saved_variance;
  paddle::dialect::IrMetaTensor meta_saved_variance(&dense_saved_variance);

  phi::InstanceNormInferMeta(meta_x, meta_scale, meta_bias, epsilon, &meta_y, &meta_saved_mean, &meta_saved_variance);

  std::vector<pir::Type> argument_outputs;
  pir::Type y_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_y.dtype()), dense_y.dims(), dense_y.layout(), dense_y.lod(), dense_y.offset());
  argument_outputs.push_back(y_dense_tensor_type);

  pir::Type saved_mean_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_saved_mean.dtype()), dense_saved_mean.dims(), dense_saved_mean.layout(), dense_saved_mean.lod(), dense_saved_mean.offset());
  argument_outputs.push_back(saved_mean_dense_tensor_type);

  pir::Type saved_variance_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_saved_variance.dtype()), dense_saved_variance.dims(), dense_saved_variance.layout(), dense_saved_variance.lod(), dense_saved_variance.offset());
  argument_outputs.push_back(saved_variance_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void InstanceNormOp::VerifySig() {
  VLOG(4) << "Start Verifying inputs, outputs and attributes for: InstanceNormOp.";
  VLOG(4) << "Verifying inputs:";
  {
  auto input_size = num_operands();
  IR_ENFORCE(input_size == 3u,
                    "The size %d of inputs must be equal to 3.", input_size);
  IR_ENFORCE((*this)->operand_source(0).type().isa<paddle::dialect::DenseTensorType>(),
                  "Type validation failed for the 0th input, got %s.", (*this)->operand_source(0).type());
  if (auto val = (*this)->operand(1)) {
    IR_ENFORCE(val.type().isa<paddle::dialect::DenseTensorType>(),
                   "Type validation failed for the 1th input, got %s.", (*this)->operand_source(1).type());
  }
  if (auto val = (*this)->operand(2)) {
    IR_ENFORCE(val.type().isa<paddle::dialect::DenseTensorType>(),
                   "Type validation failed for the 2th input, got %s.", (*this)->operand_source(2).type());
  }
  }
  VLOG(4) << "Verifying attributes:";
  {
  auto& attributes = this->attributes();
  IR_ENFORCE(attributes.count("epsilon")>0,
                 "epsilon does not exist.");
  IR_ENFORCE(attributes.at("epsilon").isa<pir::FloatAttribute>(),
                 "Type of attribute: epsilon is not pir::FloatAttribute.");

  }
  VLOG(4) << "Verifying outputs:";
  {
  auto output_size = num_results();
  IR_ENFORCE(output_size == 3u,
                    "The size %d of outputs must be equal to 3.", output_size);
  IR_ENFORCE((*this)->result(0).type().isa<paddle::dialect::DenseTensorType>(),
                 "Type validation failed for the 0th output.");
  if (auto output_1_type = (*this)->result(1).type()) {
    IR_ENFORCE(output_1_type.isa<paddle::dialect::DenseTensorType>(),
                   "Type validation failed for the 1th output.");
  }
  if (auto output_2_type = (*this)->result(2).type()) {
    IR_ENFORCE(output_2_type.isa<paddle::dialect::DenseTensorType>(),
                   "Type validation failed for the 2th output.");
  }
  }
  VLOG(4) << "End Verifying for: InstanceNormOp.";
}

void InstanceNormOp::InferMeta( phi::InferMetaContext *infer_meta ) {
  auto fn = PD_INFER_META(phi::InstanceNormInferMeta);
  fn(infer_meta);
}

phi::DataType InstanceNormOp::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: InstanceNormOp";
  


  return expected_kernel_dtype;
}

OpInfoTuple InverseOp::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("x", "paddle::dialect::DenseTensorType", false, false, false, true) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = {  };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("out", "paddle::dialect::DenseTensorType", false, false) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("InverseInferMeta", {"x"}, "inverse", {"x"}, {}, {}, {}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "inverse");
}

void InverseOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_) {
  VLOG(4) << "Start build InverseOp";



  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);
  paddle::dialect::IrTensor dense_out;
  paddle::dialect::IrMetaTensor meta_out(&dense_out);

  phi::InverseInferMeta(meta_x, &meta_out);

  std::vector<pir::Type> argument_outputs;
  pir::Type out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_out.dtype()), dense_out.dims(), dense_out.layout(), dense_out.lod(), dense_out.offset());
  argument_outputs.push_back(out_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void InverseOp::VerifySig() {
  VLOG(4) << "Start Verifying inputs, outputs and attributes for: InverseOp.";
  VLOG(4) << "Verifying inputs:";
  {
  auto input_size = num_operands();
  IR_ENFORCE(input_size == 1u,
                    "The size %d of inputs must be equal to 1.", input_size);
  IR_ENFORCE((*this)->operand_source(0).type().isa<paddle::dialect::DenseTensorType>(),
                  "Type validation failed for the 0th input, got %s.", (*this)->operand_source(0).type());
  }
  VLOG(4) << "Verifying attributes:";
  {
  // Attributes num is 0, not need to check attributes type.
  }
  VLOG(4) << "Verifying outputs:";
  {
  auto output_size = num_results();
  IR_ENFORCE(output_size == 1u,
                    "The size %d of outputs must be equal to 1.", output_size);
  IR_ENFORCE((*this)->result(0).type().isa<paddle::dialect::DenseTensorType>(),
                 "Type validation failed for the 0th output.");
  }
  VLOG(4) << "End Verifying for: InverseOp.";
}

void InverseOp::InferMeta( phi::InferMetaContext *infer_meta ) {
  auto fn = PD_INFER_META(phi::InverseInferMeta);
  fn(infer_meta);
}

phi::DataType InverseOp::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: InverseOp";
  


  return expected_kernel_dtype;
}

OpInfoTuple IsEmptyOp::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("x", "paddle::dialect::DenseTensorType", false, false, false, false) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = {  };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("out", "paddle::dialect::DenseTensorType", false, false) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("IsEmptyInferMeta", {"x"}, "is_empty", {"x"}, {}, {}, {}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "is_empty");
}

void IsEmptyOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_) {
  VLOG(4) << "Start build IsEmptyOp";



  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);
  paddle::dialect::IrTensor dense_out;
  paddle::dialect::IrMetaTensor meta_out(&dense_out);

  phi::IsEmptyInferMeta(meta_x, &meta_out);

  std::vector<pir::Type> argument_outputs;
  pir::Type out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_out.dtype()), dense_out.dims(), dense_out.layout(), dense_out.lod(), dense_out.offset());
  argument_outputs.push_back(out_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void IsEmptyOp::VerifySig() {
  VLOG(4) << "Start Verifying inputs, outputs and attributes for: IsEmptyOp.";
  VLOG(4) << "Verifying inputs:";
  {
  auto input_size = num_operands();
  IR_ENFORCE(input_size == 1u,
                    "The size %d of inputs must be equal to 1.", input_size);
  IR_ENFORCE((*this)->operand_source(0).type().isa<paddle::dialect::DenseTensorType>(),
                  "Type validation failed for the 0th input, got %s.", (*this)->operand_source(0).type());
  }
  VLOG(4) << "Verifying attributes:";
  {
  // Attributes num is 0, not need to check attributes type.
  }
  VLOG(4) << "Verifying outputs:";
  {
  auto output_size = num_results();
  IR_ENFORCE(output_size == 1u,
                    "The size %d of outputs must be equal to 1.", output_size);
  IR_ENFORCE((*this)->result(0).type().isa<paddle::dialect::DenseTensorType>(),
                 "Type validation failed for the 0th output.");
  }
  VLOG(4) << "End Verifying for: IsEmptyOp.";
}

void IsEmptyOp::InferMeta( phi::InferMetaContext *infer_meta ) {
  auto fn = PD_INFER_META(phi::IsEmptyInferMeta);
  fn(infer_meta);
}

phi::DataType IsEmptyOp::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: IsEmptyOp";
  


  return expected_kernel_dtype;
}

const char *IscloseOp::attributes_name[1] = { "equal_nan" };

OpInfoTuple IscloseOp::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("x", "paddle::dialect::DenseTensorType", false, false, false, false), paddle::dialect::OpInputInfo("y", "paddle::dialect::DenseTensorType", false, false, false, false), paddle::dialect::OpInputInfo("rtol", "paddle::dialect::ScalarAttribute", false, false, true, false), paddle::dialect::OpInputInfo("atol", "paddle::dialect::ScalarAttribute", false, false, true, false) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = { paddle::dialect::OpAttributeInfo("equal_nan", "pir::BoolAttribute", "") };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("out", "paddle::dialect::DenseTensorType", false, false) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("ValueCompareInferMeta", {"x", "y"}, "isclose", {"x", "y", "rtol", "atol", "equal_nan"}, {"x"}, {}, {}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "isclose");
}

void IscloseOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, pir::Value y_, double rtol, double atol, bool equal_nan) {
  VLOG(4) << "Start build IscloseOp";


  // Generate scalar mutable attribute: rtol
  paddle::dialect::FullOp full_rtol_op = builder.Build<paddle::dialect::FullOp>(std::vector<int64_t>{1}, rtol, phi::DataType::FLOAT64, phi::CPUPlace());
  pir::OpResult rtol_ = full_rtol_op->result(0);
      // Generate scalar mutable attribute: atol
  paddle::dialect::FullOp full_atol_op = builder.Build<paddle::dialect::FullOp>(std::vector<int64_t>{1}, atol, phi::DataType::FLOAT64, phi::CPUPlace());
  pir::OpResult atol_ = full_atol_op->result(0);
    
  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_, y_, rtol_, atol_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_equal_nan = pir::BoolAttribute::get(pir::IrContext::Instance(), equal_nan);
  argument.AddAttribute("equal_nan", attr_equal_nan);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;
  paddle::dialect::DenseTensorType y = y_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)y;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);

  VLOG(4) << "Builder construction  dense_y";
  paddle::dialect::IrTensor ir_tensor_y(paddle::dialect::TransToPhiDataType(y.dtype()),
                                                      y.dims(),
                                                      y.data_layout(),
                                                      y.lod(),
                                                      y.offset());
  VLOG(4) << "Builder construction  meta_y";
  paddle::dialect::IrMetaTensor meta_y(&ir_tensor_y);
  paddle::dialect::IrTensor dense_out;
  paddle::dialect::IrMetaTensor meta_out(&dense_out);

  phi::ValueCompareInferMeta(meta_x, meta_y, &meta_out);

  std::vector<pir::Type> argument_outputs;
  pir::Type out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_out.dtype()), dense_out.dims(), dense_out.layout(), dense_out.lod(), dense_out.offset());
  argument_outputs.push_back(out_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void IscloseOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, pir::Value y_, pir::AttributeMap attributes) {
  VLOG(4) << "Start build IscloseOp";


  IR_ENFORCE(
      attributes.find("rtol") != attributes.end(),
          "'rtol' Attribute is expected for IscloseOp. ");
  double rtol = attributes.at("rtol").dyn_cast<pir::DoubleAttribute>().data();

  IR_ENFORCE(
      attributes.find("atol") != attributes.end(),
          "'atol' Attribute is expected for IscloseOp. ");
  double atol = attributes.at("atol").dyn_cast<pir::DoubleAttribute>().data();

  IR_ENFORCE(
      attributes.find("equal_nan") != attributes.end(),
          "'equal_nan' Attribute is expected for IscloseOp. ");
  bool equal_nan = attributes.at("equal_nan").dyn_cast<pir::BoolAttribute>().data();

  // Generate scalar mutable attribute: rtol
  paddle::dialect::FullOp full_rtol_op = builder.Build<paddle::dialect::FullOp>(std::vector<int64_t>{1}, rtol, phi::DataType::FLOAT64, phi::CPUPlace());
  pir::OpResult rtol_ = full_rtol_op->result(0);
      // Generate scalar mutable attribute: atol
  paddle::dialect::FullOp full_atol_op = builder.Build<paddle::dialect::FullOp>(std::vector<int64_t>{1}, atol, phi::DataType::FLOAT64, phi::CPUPlace());
  pir::OpResult atol_ = full_atol_op->result(0);
    
  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_, y_, rtol_, atol_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_equal_nan = pir::BoolAttribute::get(pir::IrContext::Instance(), equal_nan);
  argument.AddAttribute("equal_nan", attr_equal_nan);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;
  paddle::dialect::DenseTensorType y = y_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)y;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);

  VLOG(4) << "Builder construction  dense_y";
  paddle::dialect::IrTensor ir_tensor_y(paddle::dialect::TransToPhiDataType(y.dtype()),
                                                      y.dims(),
                                                      y.data_layout(),
                                                      y.lod(),
                                                      y.offset());
  VLOG(4) << "Builder construction  meta_y";
  paddle::dialect::IrMetaTensor meta_y(&ir_tensor_y);
  paddle::dialect::IrTensor dense_out;
  paddle::dialect::IrMetaTensor meta_out(&dense_out);

  phi::ValueCompareInferMeta(meta_x, meta_y, &meta_out);

  std::vector<pir::Type> argument_outputs;
  pir::Type out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_out.dtype()), dense_out.dims(), dense_out.layout(), dense_out.lod(), dense_out.offset());
  argument_outputs.push_back(out_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void IscloseOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, pir::Value y_, pir::Value rtol_, pir::Value atol_, bool equal_nan) {
  VLOG(4) << "Start build IscloseOp";



  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_, y_, rtol_, atol_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_equal_nan = pir::BoolAttribute::get(pir::IrContext::Instance(), equal_nan);
  argument.AddAttribute("equal_nan", attr_equal_nan);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;
  paddle::dialect::DenseTensorType y = y_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)y;
  phi::Scalar rtol;
  if (rtol_.dyn_cast<pir::OpResult>() && rtol_.dyn_cast<pir::OpResult>().owner()->isa<paddle::dialect::FullOp>()) {
    rtol = std::move(phi::Scalar(rtol_.dyn_cast<pir::OpResult>().owner()
                                  ->dyn_cast<paddle::dialect::FullOp>()
                                  .attribute("value")
                                  .dyn_cast<paddle::dialect::ScalarAttribute>()
                                  .data()
                                  .to<int>()));
  }
  else {
    rtol = std::move(phi::Scalar(-1));
    rtol.SetFromTensor(true);
  }
  phi::Scalar atol;
  if (atol_.dyn_cast<pir::OpResult>() && atol_.dyn_cast<pir::OpResult>().owner()->isa<paddle::dialect::FullOp>()) {
    atol = std::move(phi::Scalar(atol_.dyn_cast<pir::OpResult>().owner()
                                  ->dyn_cast<paddle::dialect::FullOp>()
                                  .attribute("value")
                                  .dyn_cast<paddle::dialect::ScalarAttribute>()
                                  .data()
                                  .to<int>()));
  }
  else {
    atol = std::move(phi::Scalar(-1));
    atol.SetFromTensor(true);
  }


  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);

  VLOG(4) << "Builder construction  dense_y";
  paddle::dialect::IrTensor ir_tensor_y(paddle::dialect::TransToPhiDataType(y.dtype()),
                                                      y.dims(),
                                                      y.data_layout(),
                                                      y.lod(),
                                                      y.offset());
  VLOG(4) << "Builder construction  meta_y";
  paddle::dialect::IrMetaTensor meta_y(&ir_tensor_y);
  paddle::dialect::IrTensor dense_out;
  paddle::dialect::IrMetaTensor meta_out(&dense_out);

  phi::ValueCompareInferMeta(meta_x, meta_y, &meta_out);

  std::vector<pir::Type> argument_outputs;
  pir::Type out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_out.dtype()), dense_out.dims(), dense_out.layout(), dense_out.lod(), dense_out.offset());
  argument_outputs.push_back(out_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void IscloseOp::VerifySig() {
  VLOG(4) << "Start Verifying inputs, outputs and attributes for: IscloseOp.";
  VLOG(4) << "Verifying inputs:";
  {
  auto input_size = num_operands();
  IR_ENFORCE(input_size == 4u,
                    "The size %d of inputs must be equal to 4.", input_size);
  IR_ENFORCE((*this)->operand_source(0).type().isa<paddle::dialect::DenseTensorType>(),
                  "Type validation failed for the 0th input, got %s.", (*this)->operand_source(0).type());
  IR_ENFORCE((*this)->operand_source(1).type().isa<paddle::dialect::DenseTensorType>(),
                  "Type validation failed for the 1th input, got %s.", (*this)->operand_source(1).type());
  IR_ENFORCE((*this)->operand_source(2).type().isa<paddle::dialect::DenseTensorType>(),
                  "Type validation failed for the 2th input, got %s.", (*this)->operand_source(2).type());
  IR_ENFORCE((*this)->operand_source(3).type().isa<paddle::dialect::DenseTensorType>(),
                  "Type validation failed for the 3th input, got %s.", (*this)->operand_source(3).type());
  }
  VLOG(4) << "Verifying attributes:";
  {
  auto& attributes = this->attributes();
  IR_ENFORCE(attributes.count("equal_nan")>0,
                 "equal_nan does not exist.");
  IR_ENFORCE(attributes.at("equal_nan").isa<pir::BoolAttribute>(),
                 "Type of attribute: equal_nan is not pir::BoolAttribute.");

  }
  VLOG(4) << "Verifying outputs:";
  {
  auto output_size = num_results();
  IR_ENFORCE(output_size == 1u,
                    "The size %d of outputs must be equal to 1.", output_size);
  IR_ENFORCE((*this)->result(0).type().isa<paddle::dialect::DenseTensorType>(),
                 "Type validation failed for the 0th output.");
  }
  VLOG(4) << "End Verifying for: IscloseOp.";
}

void IscloseOp::InferMeta( phi::InferMetaContext *infer_meta ) {
  auto fn = PD_INFER_META(phi::ValueCompareInferMeta);
  fn(infer_meta);
}

phi::DataType IscloseOp::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: IscloseOp";
  


  return expected_kernel_dtype;
}

OpInfoTuple IsfiniteOp::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("x", "paddle::dialect::DenseTensorType", false, false, false, false) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = {  };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("out", "paddle::dialect::DenseTensorType", false, false) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("IsfiniteInferMeta", {"x"}, "isfinite", {"x"}, {}, {}, {}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "isfinite");
}

void IsfiniteOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_) {
  VLOG(4) << "Start build IsfiniteOp";



  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);
  paddle::dialect::IrTensor dense_out;
  paddle::dialect::IrMetaTensor meta_out(&dense_out);

  phi::IsfiniteInferMeta(meta_x, &meta_out);

  std::vector<pir::Type> argument_outputs;
  pir::Type out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_out.dtype()), dense_out.dims(), dense_out.layout(), dense_out.lod(), dense_out.offset());
  argument_outputs.push_back(out_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void IsfiniteOp::VerifySig() {
  VLOG(4) << "Start Verifying inputs, outputs and attributes for: IsfiniteOp.";
  VLOG(4) << "Verifying inputs:";
  {
  auto input_size = num_operands();
  IR_ENFORCE(input_size == 1u,
                    "The size %d of inputs must be equal to 1.", input_size);
  IR_ENFORCE((*this)->operand_source(0).type().isa<paddle::dialect::DenseTensorType>(),
                  "Type validation failed for the 0th input, got %s.", (*this)->operand_source(0).type());
  }
  VLOG(4) << "Verifying attributes:";
  {
  // Attributes num is 0, not need to check attributes type.
  }
  VLOG(4) << "Verifying outputs:";
  {
  auto output_size = num_results();
  IR_ENFORCE(output_size == 1u,
                    "The size %d of outputs must be equal to 1.", output_size);
  IR_ENFORCE((*this)->result(0).type().isa<paddle::dialect::DenseTensorType>(),
                 "Type validation failed for the 0th output.");
  }
  VLOG(4) << "End Verifying for: IsfiniteOp.";
}

void IsfiniteOp::InferMeta( phi::InferMetaContext *infer_meta ) {
  auto fn = PD_INFER_META(phi::IsfiniteInferMeta);
  fn(infer_meta);
}

phi::DataType IsfiniteOp::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: IsfiniteOp";
  


  return expected_kernel_dtype;
}

OpInfoTuple IsfiniteSrOp::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("x", "paddle::dialect::SelectedRowsType", false, false, false, false) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = {  };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("out", "paddle::dialect::SelectedRowsType", false, false) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("IsfiniteInferMeta", {"x"}, "isfinite_sr", {"x"}, {}, {}, {}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "isfinite");
}

void IsfiniteSrOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_) {
  VLOG(4) << "Start build IsfiniteSrOp";



  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::SelectedRowsType x = x_.type().dyn_cast<paddle::dialect::SelectedRowsType>(); (void)x;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);
  paddle::dialect::IrSelectedRows dense_out;
  paddle::dialect::IrMetaTensor meta_out(&dense_out);

  phi::IsfiniteInferMeta(meta_x, &meta_out);

  std::vector<pir::Type> argument_outputs;
  pir::Type out_dense_tensor_type = paddle::dialect::SelectedRowsType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_out.dtype()), dense_out.dims(), dense_out.layout(), dense_out.lod(), dense_out.offset());
  argument_outputs.push_back(out_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void IsfiniteSrOp::VerifySig() {
  VLOG(4) << "Start Verifying inputs, outputs and attributes for: IsfiniteSrOp.";
  VLOG(4) << "Verifying inputs:";
  {
  auto input_size = num_operands();
  IR_ENFORCE(input_size == 1u,
                    "The size %d of inputs must be equal to 1.", input_size);
  IR_ENFORCE((*this)->operand_source(0).type().isa<paddle::dialect::SelectedRowsType>(),
                  "Type validation failed for the 0th input, got %s.", (*this)->operand_source(0).type());
  }
  VLOG(4) << "Verifying attributes:";
  {
  // Attributes num is 0, not need to check attributes type.
  }
  VLOG(4) << "Verifying outputs:";
  {
  auto output_size = num_results();
  IR_ENFORCE(output_size == 1u,
                    "The size %d of outputs must be equal to 1.", output_size);
  IR_ENFORCE((*this)->result(0).type().isa<paddle::dialect::SelectedRowsType>(),
                 "Type validation failed for the 0th output.");
  }
  VLOG(4) << "End Verifying for: IsfiniteSrOp.";
}

void IsfiniteSrOp::InferMeta( phi::InferMetaContext *infer_meta ) {
  auto fn = PD_INFER_META(phi::IsfiniteInferMeta);
  fn(infer_meta);
}

phi::DataType IsfiniteSrOp::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: IsfiniteSrOp";
  


  return expected_kernel_dtype;
}

OpInfoTuple IsinfOp::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("x", "paddle::dialect::DenseTensorType", false, false, false, false) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = {  };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("out", "paddle::dialect::DenseTensorType", false, false) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("IsfiniteInferMeta", {"x"}, "isinf", {"x"}, {}, {}, {}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "isinf");
}

void IsinfOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_) {
  VLOG(4) << "Start build IsinfOp";



  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);
  paddle::dialect::IrTensor dense_out;
  paddle::dialect::IrMetaTensor meta_out(&dense_out);

  phi::IsfiniteInferMeta(meta_x, &meta_out);

  std::vector<pir::Type> argument_outputs;
  pir::Type out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_out.dtype()), dense_out.dims(), dense_out.layout(), dense_out.lod(), dense_out.offset());
  argument_outputs.push_back(out_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void IsinfOp::VerifySig() {
  VLOG(4) << "Start Verifying inputs, outputs and attributes for: IsinfOp.";
  VLOG(4) << "Verifying inputs:";
  {
  auto input_size = num_operands();
  IR_ENFORCE(input_size == 1u,
                    "The size %d of inputs must be equal to 1.", input_size);
  IR_ENFORCE((*this)->operand_source(0).type().isa<paddle::dialect::DenseTensorType>(),
                  "Type validation failed for the 0th input, got %s.", (*this)->operand_source(0).type());
  }
  VLOG(4) << "Verifying attributes:";
  {
  // Attributes num is 0, not need to check attributes type.
  }
  VLOG(4) << "Verifying outputs:";
  {
  auto output_size = num_results();
  IR_ENFORCE(output_size == 1u,
                    "The size %d of outputs must be equal to 1.", output_size);
  IR_ENFORCE((*this)->result(0).type().isa<paddle::dialect::DenseTensorType>(),
                 "Type validation failed for the 0th output.");
  }
  VLOG(4) << "End Verifying for: IsinfOp.";
}

void IsinfOp::InferMeta( phi::InferMetaContext *infer_meta ) {
  auto fn = PD_INFER_META(phi::IsfiniteInferMeta);
  fn(infer_meta);
}

phi::DataType IsinfOp::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: IsinfOp";
  


  return expected_kernel_dtype;
}

OpInfoTuple IsinfSrOp::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("x", "paddle::dialect::SelectedRowsType", false, false, false, false) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = {  };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("out", "paddle::dialect::SelectedRowsType", false, false) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("IsfiniteInferMeta", {"x"}, "isinf_sr", {"x"}, {}, {}, {}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "isinf");
}

void IsinfSrOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_) {
  VLOG(4) << "Start build IsinfSrOp";



  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::SelectedRowsType x = x_.type().dyn_cast<paddle::dialect::SelectedRowsType>(); (void)x;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);
  paddle::dialect::IrSelectedRows dense_out;
  paddle::dialect::IrMetaTensor meta_out(&dense_out);

  phi::IsfiniteInferMeta(meta_x, &meta_out);

  std::vector<pir::Type> argument_outputs;
  pir::Type out_dense_tensor_type = paddle::dialect::SelectedRowsType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_out.dtype()), dense_out.dims(), dense_out.layout(), dense_out.lod(), dense_out.offset());
  argument_outputs.push_back(out_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void IsinfSrOp::VerifySig() {
  VLOG(4) << "Start Verifying inputs, outputs and attributes for: IsinfSrOp.";
  VLOG(4) << "Verifying inputs:";
  {
  auto input_size = num_operands();
  IR_ENFORCE(input_size == 1u,
                    "The size %d of inputs must be equal to 1.", input_size);
  IR_ENFORCE((*this)->operand_source(0).type().isa<paddle::dialect::SelectedRowsType>(),
                  "Type validation failed for the 0th input, got %s.", (*this)->operand_source(0).type());
  }
  VLOG(4) << "Verifying attributes:";
  {
  // Attributes num is 0, not need to check attributes type.
  }
  VLOG(4) << "Verifying outputs:";
  {
  auto output_size = num_results();
  IR_ENFORCE(output_size == 1u,
                    "The size %d of outputs must be equal to 1.", output_size);
  IR_ENFORCE((*this)->result(0).type().isa<paddle::dialect::SelectedRowsType>(),
                 "Type validation failed for the 0th output.");
  }
  VLOG(4) << "End Verifying for: IsinfSrOp.";
}

void IsinfSrOp::InferMeta( phi::InferMetaContext *infer_meta ) {
  auto fn = PD_INFER_META(phi::IsfiniteInferMeta);
  fn(infer_meta);
}

phi::DataType IsinfSrOp::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: IsinfSrOp";
  


  return expected_kernel_dtype;
}

OpInfoTuple IsnanOp::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("x", "paddle::dialect::DenseTensorType", false, false, false, false) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = {  };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("out", "paddle::dialect::DenseTensorType", false, false) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("IsfiniteInferMeta", {"x"}, "isnan", {"x"}, {}, {}, {}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "isnan");
}

void IsnanOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_) {
  VLOG(4) << "Start build IsnanOp";



  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);
  paddle::dialect::IrTensor dense_out;
  paddle::dialect::IrMetaTensor meta_out(&dense_out);

  phi::IsfiniteInferMeta(meta_x, &meta_out);

  std::vector<pir::Type> argument_outputs;
  pir::Type out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_out.dtype()), dense_out.dims(), dense_out.layout(), dense_out.lod(), dense_out.offset());
  argument_outputs.push_back(out_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void IsnanOp::VerifySig() {
  VLOG(4) << "Start Verifying inputs, outputs and attributes for: IsnanOp.";
  VLOG(4) << "Verifying inputs:";
  {
  auto input_size = num_operands();
  IR_ENFORCE(input_size == 1u,
                    "The size %d of inputs must be equal to 1.", input_size);
  IR_ENFORCE((*this)->operand_source(0).type().isa<paddle::dialect::DenseTensorType>(),
                  "Type validation failed for the 0th input, got %s.", (*this)->operand_source(0).type());
  }
  VLOG(4) << "Verifying attributes:";
  {
  // Attributes num is 0, not need to check attributes type.
  }
  VLOG(4) << "Verifying outputs:";
  {
  auto output_size = num_results();
  IR_ENFORCE(output_size == 1u,
                    "The size %d of outputs must be equal to 1.", output_size);
  IR_ENFORCE((*this)->result(0).type().isa<paddle::dialect::DenseTensorType>(),
                 "Type validation failed for the 0th output.");
  }
  VLOG(4) << "End Verifying for: IsnanOp.";
}

void IsnanOp::InferMeta( phi::InferMetaContext *infer_meta ) {
  auto fn = PD_INFER_META(phi::IsfiniteInferMeta);
  fn(infer_meta);
}

phi::DataType IsnanOp::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: IsnanOp";
  


  return expected_kernel_dtype;
}

OpInfoTuple IsnanSrOp::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("x", "paddle::dialect::SelectedRowsType", false, false, false, false) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = {  };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("out", "paddle::dialect::SelectedRowsType", false, false) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("IsfiniteInferMeta", {"x"}, "isnan_sr", {"x"}, {}, {}, {}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "isnan");
}

void IsnanSrOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_) {
  VLOG(4) << "Start build IsnanSrOp";



  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::SelectedRowsType x = x_.type().dyn_cast<paddle::dialect::SelectedRowsType>(); (void)x;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);
  paddle::dialect::IrSelectedRows dense_out;
  paddle::dialect::IrMetaTensor meta_out(&dense_out);

  phi::IsfiniteInferMeta(meta_x, &meta_out);

  std::vector<pir::Type> argument_outputs;
  pir::Type out_dense_tensor_type = paddle::dialect::SelectedRowsType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_out.dtype()), dense_out.dims(), dense_out.layout(), dense_out.lod(), dense_out.offset());
  argument_outputs.push_back(out_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void IsnanSrOp::VerifySig() {
  VLOG(4) << "Start Verifying inputs, outputs and attributes for: IsnanSrOp.";
  VLOG(4) << "Verifying inputs:";
  {
  auto input_size = num_operands();
  IR_ENFORCE(input_size == 1u,
                    "The size %d of inputs must be equal to 1.", input_size);
  IR_ENFORCE((*this)->operand_source(0).type().isa<paddle::dialect::SelectedRowsType>(),
                  "Type validation failed for the 0th input, got %s.", (*this)->operand_source(0).type());
  }
  VLOG(4) << "Verifying attributes:";
  {
  // Attributes num is 0, not need to check attributes type.
  }
  VLOG(4) << "Verifying outputs:";
  {
  auto output_size = num_results();
  IR_ENFORCE(output_size == 1u,
                    "The size %d of outputs must be equal to 1.", output_size);
  IR_ENFORCE((*this)->result(0).type().isa<paddle::dialect::SelectedRowsType>(),
                 "Type validation failed for the 0th output.");
  }
  VLOG(4) << "End Verifying for: IsnanSrOp.";
}

void IsnanSrOp::InferMeta( phi::InferMetaContext *infer_meta ) {
  auto fn = PD_INFER_META(phi::IsfiniteInferMeta);
  fn(infer_meta);
}

phi::DataType IsnanSrOp::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: IsnanSrOp";
  


  return expected_kernel_dtype;
}

const char *KldivLossOp::attributes_name[1] = { "reduction" };

OpInfoTuple KldivLossOp::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("x", "paddle::dialect::DenseTensorType", false, false, false, true), paddle::dialect::OpInputInfo("label", "paddle::dialect::DenseTensorType", false, false, false, false) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = { paddle::dialect::OpAttributeInfo("reduction", "pir::StrAttribute", "") };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("out", "paddle::dialect::DenseTensorType", false, false) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("KLDivInferMeta", {"x", "label", "reduction"}, "kldiv_loss", {"x", "label", "reduction"}, {"x"}, {}, {}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "kldiv_loss");
}

void KldivLossOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, pir::Value label_, const std::string& reduction) {
  VLOG(4) << "Start build KldivLossOp";



  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_, label_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_reduction = pir::StrAttribute::get(pir::IrContext::Instance(), reduction);
  argument.AddAttribute("reduction", attr_reduction);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;
  paddle::dialect::DenseTensorType label = label_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)label;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);

  VLOG(4) << "Builder construction  dense_label";
  paddle::dialect::IrTensor ir_tensor_label(paddle::dialect::TransToPhiDataType(label.dtype()),
                                                      label.dims(),
                                                      label.data_layout(),
                                                      label.lod(),
                                                      label.offset());
  VLOG(4) << "Builder construction  meta_label";
  paddle::dialect::IrMetaTensor meta_label(&ir_tensor_label);
  paddle::dialect::IrTensor dense_out;
  paddle::dialect::IrMetaTensor meta_out(&dense_out);

  phi::KLDivInferMeta(meta_x, meta_label, reduction, &meta_out);

  std::vector<pir::Type> argument_outputs;
  pir::Type out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_out.dtype()), dense_out.dims(), dense_out.layout(), dense_out.lod(), dense_out.offset());
  argument_outputs.push_back(out_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void KldivLossOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, pir::Value label_, pir::AttributeMap attributes) {
  VLOG(4) << "Start build KldivLossOp";


  IR_ENFORCE(
      attributes.find("reduction") != attributes.end(),
          "'reduction' Attribute is expected for KldivLossOp. ");
  std::string reduction = attributes.at("reduction").dyn_cast<pir::StrAttribute>().AsString();


  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_, label_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_reduction = pir::StrAttribute::get(pir::IrContext::Instance(), reduction);
  argument.AddAttribute("reduction", attr_reduction);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;
  paddle::dialect::DenseTensorType label = label_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)label;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);

  VLOG(4) << "Builder construction  dense_label";
  paddle::dialect::IrTensor ir_tensor_label(paddle::dialect::TransToPhiDataType(label.dtype()),
                                                      label.dims(),
                                                      label.data_layout(),
                                                      label.lod(),
                                                      label.offset());
  VLOG(4) << "Builder construction  meta_label";
  paddle::dialect::IrMetaTensor meta_label(&ir_tensor_label);
  paddle::dialect::IrTensor dense_out;
  paddle::dialect::IrMetaTensor meta_out(&dense_out);

  phi::KLDivInferMeta(meta_x, meta_label, reduction, &meta_out);

  std::vector<pir::Type> argument_outputs;
  pir::Type out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_out.dtype()), dense_out.dims(), dense_out.layout(), dense_out.lod(), dense_out.offset());
  argument_outputs.push_back(out_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void KldivLossOp::VerifySig() {
  VLOG(4) << "Start Verifying inputs, outputs and attributes for: KldivLossOp.";
  VLOG(4) << "Verifying inputs:";
  {
  auto input_size = num_operands();
  IR_ENFORCE(input_size == 2u,
                    "The size %d of inputs must be equal to 2.", input_size);
  IR_ENFORCE((*this)->operand_source(0).type().isa<paddle::dialect::DenseTensorType>(),
                  "Type validation failed for the 0th input, got %s.", (*this)->operand_source(0).type());
  IR_ENFORCE((*this)->operand_source(1).type().isa<paddle::dialect::DenseTensorType>(),
                  "Type validation failed for the 1th input, got %s.", (*this)->operand_source(1).type());
  }
  VLOG(4) << "Verifying attributes:";
  {
  auto& attributes = this->attributes();
  IR_ENFORCE(attributes.count("reduction")>0,
                 "reduction does not exist.");
  IR_ENFORCE(attributes.at("reduction").isa<pir::StrAttribute>(),
                 "Type of attribute: reduction is not pir::StrAttribute.");

  }
  VLOG(4) << "Verifying outputs:";
  {
  auto output_size = num_results();
  IR_ENFORCE(output_size == 1u,
                    "The size %d of outputs must be equal to 1.", output_size);
  IR_ENFORCE((*this)->result(0).type().isa<paddle::dialect::DenseTensorType>(),
                 "Type validation failed for the 0th output.");
  }
  VLOG(4) << "End Verifying for: KldivLossOp.";
}

void KldivLossOp::InferMeta( phi::InferMetaContext *infer_meta ) {
  auto fn = PD_INFER_META(phi::KLDivInferMeta);
  fn(infer_meta);
}

phi::DataType KldivLossOp::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: KldivLossOp";
  


  return expected_kernel_dtype;
}

OpInfoTuple KronOp::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("x", "paddle::dialect::DenseTensorType", false, false, false, true), paddle::dialect::OpInputInfo("y", "paddle::dialect::DenseTensorType", false, false, false, true) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = {  };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("out", "paddle::dialect::DenseTensorType", false, false) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("KronInferMeta", {"x", "y"}, "kron", {"x", "y"}, {}, {}, {}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "kron");
}

void KronOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, pir::Value y_) {
  VLOG(4) << "Start build KronOp";



  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_, y_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;
  paddle::dialect::DenseTensorType y = y_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)y;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);

  VLOG(4) << "Builder construction  dense_y";
  paddle::dialect::IrTensor ir_tensor_y(paddle::dialect::TransToPhiDataType(y.dtype()),
                                                      y.dims(),
                                                      y.data_layout(),
                                                      y.lod(),
                                                      y.offset());
  VLOG(4) << "Builder construction  meta_y";
  paddle::dialect::IrMetaTensor meta_y(&ir_tensor_y);
  paddle::dialect::IrTensor dense_out;
  paddle::dialect::IrMetaTensor meta_out(&dense_out);

  phi::KronInferMeta(meta_x, meta_y, &meta_out);

  std::vector<pir::Type> argument_outputs;
  pir::Type out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_out.dtype()), dense_out.dims(), dense_out.layout(), dense_out.lod(), dense_out.offset());
  argument_outputs.push_back(out_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void KronOp::VerifySig() {
  VLOG(4) << "Start Verifying inputs, outputs and attributes for: KronOp.";
  VLOG(4) << "Verifying inputs:";
  {
  auto input_size = num_operands();
  IR_ENFORCE(input_size == 2u,
                    "The size %d of inputs must be equal to 2.", input_size);
  IR_ENFORCE((*this)->operand_source(0).type().isa<paddle::dialect::DenseTensorType>(),
                  "Type validation failed for the 0th input, got %s.", (*this)->operand_source(0).type());
  IR_ENFORCE((*this)->operand_source(1).type().isa<paddle::dialect::DenseTensorType>(),
                  "Type validation failed for the 1th input, got %s.", (*this)->operand_source(1).type());
  }
  VLOG(4) << "Verifying attributes:";
  {
  // Attributes num is 0, not need to check attributes type.
  }
  VLOG(4) << "Verifying outputs:";
  {
  auto output_size = num_results();
  IR_ENFORCE(output_size == 1u,
                    "The size %d of outputs must be equal to 1.", output_size);
  IR_ENFORCE((*this)->result(0).type().isa<paddle::dialect::DenseTensorType>(),
                 "Type validation failed for the 0th output.");
  }
  VLOG(4) << "End Verifying for: KronOp.";
}

void KronOp::InferMeta( phi::InferMetaContext *infer_meta ) {
  auto fn = PD_INFER_META(phi::KronInferMeta);
  fn(infer_meta);
}

phi::DataType KronOp::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: KronOp";
  


  // deal complex_promote
  if (framework::IsComplexType(expected_kernel_dtype)) {{
    // only promote inputs’s types when contains complex input
    return tensor_dtype;
  }}

  return expected_kernel_dtype;
}

const char *KthvalueOp::attributes_name[3] = { "k", "axis", "keepdim" };

OpInfoTuple KthvalueOp::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("x", "paddle::dialect::DenseTensorType", false, false, false, true) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = { paddle::dialect::OpAttributeInfo("k", "pir::Int32Attribute", ""), paddle::dialect::OpAttributeInfo("axis", "pir::Int32Attribute", ""), paddle::dialect::OpAttributeInfo("keepdim", "pir::BoolAttribute", "") };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("out", "paddle::dialect::DenseTensorType", false, false), paddle::dialect::OpOutputInfo("indices", "paddle::dialect::DenseTensorType", false, false) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("KthvalueInferMeta", {"x", "k", "axis", "keepdim"}, "kthvalue", {"x", "k", "axis", "keepdim"}, {}, {}, {}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "kthvalue");
}

void KthvalueOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, int k, int axis, bool keepdim) {
  VLOG(4) << "Start build KthvalueOp";



  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_k = pir::Int32Attribute::get(pir::IrContext::Instance(), k);
  argument.AddAttribute("k", attr_k);
  pir::Attribute attr_axis = pir::Int32Attribute::get(pir::IrContext::Instance(), axis);
  argument.AddAttribute("axis", attr_axis);
  pir::Attribute attr_keepdim = pir::BoolAttribute::get(pir::IrContext::Instance(), keepdim);
  argument.AddAttribute("keepdim", attr_keepdim);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);
  paddle::dialect::IrTensor dense_out;
  paddle::dialect::IrMetaTensor meta_out(&dense_out);
  paddle::dialect::IrTensor dense_indices;
  paddle::dialect::IrMetaTensor meta_indices(&dense_indices);

  phi::KthvalueInferMeta(meta_x, k, axis, keepdim, &meta_out, &meta_indices);

  std::vector<pir::Type> argument_outputs;
  pir::Type out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_out.dtype()), dense_out.dims(), dense_out.layout(), dense_out.lod(), dense_out.offset());
  argument_outputs.push_back(out_dense_tensor_type);

  pir::Type indices_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_indices.dtype()), dense_indices.dims(), dense_indices.layout(), dense_indices.lod(), dense_indices.offset());
  argument_outputs.push_back(indices_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void KthvalueOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, pir::AttributeMap attributes) {
  VLOG(4) << "Start build KthvalueOp";


  IR_ENFORCE(
      attributes.find("k") != attributes.end(),
          "'k' Attribute is expected for KthvalueOp. ");
  int k = attributes.at("k").dyn_cast<pir::Int32Attribute>().data();

  IR_ENFORCE(
      attributes.find("axis") != attributes.end(),
          "'axis' Attribute is expected for KthvalueOp. ");
  int axis = attributes.at("axis").dyn_cast<pir::Int32Attribute>().data();

  IR_ENFORCE(
      attributes.find("keepdim") != attributes.end(),
          "'keepdim' Attribute is expected for KthvalueOp. ");
  bool keepdim = attributes.at("keepdim").dyn_cast<pir::BoolAttribute>().data();


  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_k = pir::Int32Attribute::get(pir::IrContext::Instance(), k);
  argument.AddAttribute("k", attr_k);
  pir::Attribute attr_axis = pir::Int32Attribute::get(pir::IrContext::Instance(), axis);
  argument.AddAttribute("axis", attr_axis);
  pir::Attribute attr_keepdim = pir::BoolAttribute::get(pir::IrContext::Instance(), keepdim);
  argument.AddAttribute("keepdim", attr_keepdim);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);
  paddle::dialect::IrTensor dense_out;
  paddle::dialect::IrMetaTensor meta_out(&dense_out);
  paddle::dialect::IrTensor dense_indices;
  paddle::dialect::IrMetaTensor meta_indices(&dense_indices);

  phi::KthvalueInferMeta(meta_x, k, axis, keepdim, &meta_out, &meta_indices);

  std::vector<pir::Type> argument_outputs;
  pir::Type out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_out.dtype()), dense_out.dims(), dense_out.layout(), dense_out.lod(), dense_out.offset());
  argument_outputs.push_back(out_dense_tensor_type);

  pir::Type indices_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_indices.dtype()), dense_indices.dims(), dense_indices.layout(), dense_indices.lod(), dense_indices.offset());
  argument_outputs.push_back(indices_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void KthvalueOp::VerifySig() {
  VLOG(4) << "Start Verifying inputs, outputs and attributes for: KthvalueOp.";
  VLOG(4) << "Verifying inputs:";
  {
  auto input_size = num_operands();
  IR_ENFORCE(input_size == 1u,
                    "The size %d of inputs must be equal to 1.", input_size);
  IR_ENFORCE((*this)->operand_source(0).type().isa<paddle::dialect::DenseTensorType>(),
                  "Type validation failed for the 0th input, got %s.", (*this)->operand_source(0).type());
  }
  VLOG(4) << "Verifying attributes:";
  {
  auto& attributes = this->attributes();
  IR_ENFORCE(attributes.count("k")>0,
                 "k does not exist.");
  IR_ENFORCE(attributes.at("k").isa<pir::Int32Attribute>(),
                 "Type of attribute: k is not pir::Int32Attribute.");

  IR_ENFORCE(attributes.count("axis")>0,
                 "axis does not exist.");
  IR_ENFORCE(attributes.at("axis").isa<pir::Int32Attribute>(),
                 "Type of attribute: axis is not pir::Int32Attribute.");

  IR_ENFORCE(attributes.count("keepdim")>0,
                 "keepdim does not exist.");
  IR_ENFORCE(attributes.at("keepdim").isa<pir::BoolAttribute>(),
                 "Type of attribute: keepdim is not pir::BoolAttribute.");

  }
  VLOG(4) << "Verifying outputs:";
  {
  auto output_size = num_results();
  IR_ENFORCE(output_size == 2u,
                    "The size %d of outputs must be equal to 2.", output_size);
  IR_ENFORCE((*this)->result(0).type().isa<paddle::dialect::DenseTensorType>(),
                 "Type validation failed for the 0th output.");
  IR_ENFORCE((*this)->result(1).type().isa<paddle::dialect::DenseTensorType>(),
                 "Type validation failed for the 1th output.");
  }
  VLOG(4) << "End Verifying for: KthvalueOp.";
}

void KthvalueOp::InferMeta( phi::InferMetaContext *infer_meta ) {
  auto fn = PD_INFER_META(phi::KthvalueInferMeta);
  fn(infer_meta);
}

phi::DataType KthvalueOp::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: KthvalueOp";
  


  return expected_kernel_dtype;
}

const char *LabelSmoothOp::attributes_name[1] = { "epsilon" };

OpInfoTuple LabelSmoothOp::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("label", "paddle::dialect::DenseTensorType", false, false, false, true), paddle::dialect::OpInputInfo("prior_dist", "paddle::dialect::DenseTensorType", true, false, false, false) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = { paddle::dialect::OpAttributeInfo("epsilon", "pir::FloatAttribute", "") };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("out", "paddle::dialect::DenseTensorType", false, false) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("UnchangedInferMeta", {"label"}, "label_smooth", {"label", "prior_dist", "epsilon"}, {"label"}, {}, {}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "label_smooth");
}

void LabelSmoothOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value label_, pir::Value prior_dist_, float epsilon) {
  VLOG(4) << "Start build LabelSmoothOp";



  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {label_, prior_dist_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_epsilon = pir::FloatAttribute::get(pir::IrContext::Instance(), epsilon);
  argument.AddAttribute("epsilon", attr_epsilon);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType label = label_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)label;

  VLOG(4) << "Builder construction  dense_label";
  paddle::dialect::IrTensor ir_tensor_label(paddle::dialect::TransToPhiDataType(label.dtype()),
                                                      label.dims(),
                                                      label.data_layout(),
                                                      label.lod(),
                                                      label.offset());
  VLOG(4) << "Builder construction  meta_label";
  paddle::dialect::IrMetaTensor meta_label(&ir_tensor_label);
  paddle::dialect::IrTensor dense_out;
  paddle::dialect::IrMetaTensor meta_out(&dense_out);

  phi::UnchangedInferMeta(meta_label, &meta_out);

  std::vector<pir::Type> argument_outputs;
  pir::Type out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_out.dtype()), dense_out.dims(), dense_out.layout(), dense_out.lod(), dense_out.offset());
  argument_outputs.push_back(out_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void LabelSmoothOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value label_, pir::Value prior_dist_, pir::AttributeMap attributes) {
  VLOG(4) << "Start build LabelSmoothOp";


  IR_ENFORCE(
      attributes.find("epsilon") != attributes.end(),
          "'epsilon' Attribute is expected for LabelSmoothOp. ");
  float epsilon = attributes.at("epsilon").dyn_cast<pir::FloatAttribute>().data();


  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {label_, prior_dist_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_epsilon = pir::FloatAttribute::get(pir::IrContext::Instance(), epsilon);
  argument.AddAttribute("epsilon", attr_epsilon);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType label = label_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)label;

  VLOG(4) << "Builder construction  dense_label";
  paddle::dialect::IrTensor ir_tensor_label(paddle::dialect::TransToPhiDataType(label.dtype()),
                                                      label.dims(),
                                                      label.data_layout(),
                                                      label.lod(),
                                                      label.offset());
  VLOG(4) << "Builder construction  meta_label";
  paddle::dialect::IrMetaTensor meta_label(&ir_tensor_label);
  paddle::dialect::IrTensor dense_out;
  paddle::dialect::IrMetaTensor meta_out(&dense_out);

  phi::UnchangedInferMeta(meta_label, &meta_out);

  std::vector<pir::Type> argument_outputs;
  pir::Type out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_out.dtype()), dense_out.dims(), dense_out.layout(), dense_out.lod(), dense_out.offset());
  argument_outputs.push_back(out_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void LabelSmoothOp::VerifySig() {
  VLOG(4) << "Start Verifying inputs, outputs and attributes for: LabelSmoothOp.";
  VLOG(4) << "Verifying inputs:";
  {
  auto input_size = num_operands();
  IR_ENFORCE(input_size == 2u,
                    "The size %d of inputs must be equal to 2.", input_size);
  IR_ENFORCE((*this)->operand_source(0).type().isa<paddle::dialect::DenseTensorType>(),
                  "Type validation failed for the 0th input, got %s.", (*this)->operand_source(0).type());
  if (auto val = (*this)->operand(1)) {
    IR_ENFORCE(val.type().isa<paddle::dialect::DenseTensorType>(),
                   "Type validation failed for the 1th input, got %s.", (*this)->operand_source(1).type());
  }
  }
  VLOG(4) << "Verifying attributes:";
  {
  auto& attributes = this->attributes();
  IR_ENFORCE(attributes.count("epsilon")>0,
                 "epsilon does not exist.");
  IR_ENFORCE(attributes.at("epsilon").isa<pir::FloatAttribute>(),
                 "Type of attribute: epsilon is not pir::FloatAttribute.");

  }
  VLOG(4) << "Verifying outputs:";
  {
  auto output_size = num_results();
  IR_ENFORCE(output_size == 1u,
                    "The size %d of outputs must be equal to 1.", output_size);
  IR_ENFORCE((*this)->result(0).type().isa<paddle::dialect::DenseTensorType>(),
                 "Type validation failed for the 0th output.");
  }
  VLOG(4) << "End Verifying for: LabelSmoothOp.";
}

void LabelSmoothOp::InferMeta( phi::InferMetaContext *infer_meta ) {
  auto fn = PD_INFER_META(phi::UnchangedInferMeta);
  fn(infer_meta);
}

phi::DataType LabelSmoothOp::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: LabelSmoothOp";
  


  return expected_kernel_dtype;
}

const char *Lamb_Op::attributes_name[6] = { "weight_decay", "beta1", "beta2", "epsilon", "always_adapt", "multi_precision" };

OpInfoTuple Lamb_Op::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("param", "paddle::dialect::DenseTensorType", false, false, false, false), paddle::dialect::OpInputInfo("grad", "paddle::dialect::DenseTensorType", false, false, false, false), paddle::dialect::OpInputInfo("learning_rate", "paddle::dialect::DenseTensorType", false, false, false, false), paddle::dialect::OpInputInfo("moment1", "paddle::dialect::DenseTensorType", false, false, false, false), paddle::dialect::OpInputInfo("moment2", "paddle::dialect::DenseTensorType", false, false, false, false), paddle::dialect::OpInputInfo("beta1_pow", "paddle::dialect::DenseTensorType", false, false, false, false), paddle::dialect::OpInputInfo("beta2_pow", "paddle::dialect::DenseTensorType", false, false, false, false), paddle::dialect::OpInputInfo("master_param", "paddle::dialect::DenseTensorType", true, false, false, false), paddle::dialect::OpInputInfo("skip_update", "paddle::dialect::DenseTensorType", true, false, false, false) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = { paddle::dialect::OpAttributeInfo("weight_decay", "pir::FloatAttribute", ""), paddle::dialect::OpAttributeInfo("beta1", "pir::FloatAttribute", ""), paddle::dialect::OpAttributeInfo("beta2", "pir::FloatAttribute", ""), paddle::dialect::OpAttributeInfo("epsilon", "pir::FloatAttribute", ""), paddle::dialect::OpAttributeInfo("always_adapt", "pir::BoolAttribute", ""), paddle::dialect::OpAttributeInfo("multi_precision", "pir::BoolAttribute", "") };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("param_out", "paddle::dialect::DenseTensorType", false, false), paddle::dialect::OpOutputInfo("moment1_out", "paddle::dialect::DenseTensorType", false, false), paddle::dialect::OpOutputInfo("moment2_out", "paddle::dialect::DenseTensorType", false, false), paddle::dialect::OpOutputInfo("beta1_pow_out", "paddle::dialect::DenseTensorType", true, false), paddle::dialect::OpOutputInfo("beta2_pow_out", "paddle::dialect::DenseTensorType", true, false), paddle::dialect::OpOutputInfo("master_param_outs", "paddle::dialect::DenseTensorType", true, false) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("LambInferMeta", {"param", "grad", "learning_rate", "moment1", "moment2", "beta1_pow", "beta2_pow", "master_param", "skip_update", "weight_decay", "beta1", "beta2", "epsilon", "always_adapt", "multi_precision"}, "lamb", {"param", "grad", "learning_rate", "moment1", "moment2", "beta1_pow", "beta2_pow", "master_param", "skip_update", "weight_decay", "beta1", "beta2", "epsilon", "always_adapt", "multi_precision"}, {"param"}, {}, {{"param_out", "param"},{"moment1_out", "moment1"},{"moment2_out", "moment2"},{"beta1_pow_out", "beta1_pow"},{"beta2_pow_out", "beta2_pow"},{"master_param_outs", "master_param"}}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "lamb_");
}

void Lamb_Op::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value param_, pir::Value grad_, pir::Value learning_rate_, pir::Value moment1_, pir::Value moment2_, pir::Value beta1_pow_, pir::Value beta2_pow_, pir::Value master_param_, pir::Value skip_update_, float weight_decay, float beta1, float beta2, float epsilon, bool always_adapt, bool multi_precision) {
  VLOG(4) << "Start build Lamb_Op";



  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {param_, grad_, learning_rate_, moment1_, moment2_, beta1_pow_, beta2_pow_, master_param_, skip_update_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_weight_decay = pir::FloatAttribute::get(pir::IrContext::Instance(), weight_decay);
  argument.AddAttribute("weight_decay", attr_weight_decay);
  pir::Attribute attr_beta1 = pir::FloatAttribute::get(pir::IrContext::Instance(), beta1);
  argument.AddAttribute("beta1", attr_beta1);
  pir::Attribute attr_beta2 = pir::FloatAttribute::get(pir::IrContext::Instance(), beta2);
  argument.AddAttribute("beta2", attr_beta2);
  pir::Attribute attr_epsilon = pir::FloatAttribute::get(pir::IrContext::Instance(), epsilon);
  argument.AddAttribute("epsilon", attr_epsilon);
  pir::Attribute attr_always_adapt = pir::BoolAttribute::get(pir::IrContext::Instance(), always_adapt);
  argument.AddAttribute("always_adapt", attr_always_adapt);
  pir::Attribute attr_multi_precision = pir::BoolAttribute::get(pir::IrContext::Instance(), multi_precision);
  argument.AddAttribute("multi_precision", attr_multi_precision);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType param = param_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)param;
  paddle::dialect::DenseTensorType grad = grad_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)grad;
  paddle::dialect::DenseTensorType learning_rate = learning_rate_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)learning_rate;
  paddle::dialect::DenseTensorType moment1 = moment1_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)moment1;
  paddle::dialect::DenseTensorType moment2 = moment2_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)moment2;
  paddle::dialect::DenseTensorType beta1_pow = beta1_pow_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)beta1_pow;
  paddle::dialect::DenseTensorType beta2_pow = beta2_pow_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)beta2_pow;

  VLOG(4) << "Builder construction  dense_param";
  paddle::dialect::IrTensor ir_tensor_param(paddle::dialect::TransToPhiDataType(param.dtype()),
                                                      param.dims(),
                                                      param.data_layout(),
                                                      param.lod(),
                                                      param.offset());
  VLOG(4) << "Builder construction  meta_param";
  paddle::dialect::IrMetaTensor meta_param(&ir_tensor_param);

  VLOG(4) << "Builder construction  dense_grad";
  paddle::dialect::IrTensor ir_tensor_grad(paddle::dialect::TransToPhiDataType(grad.dtype()),
                                                      grad.dims(),
                                                      grad.data_layout(),
                                                      grad.lod(),
                                                      grad.offset());
  VLOG(4) << "Builder construction  meta_grad";
  paddle::dialect::IrMetaTensor meta_grad(&ir_tensor_grad);

  VLOG(4) << "Builder construction  dense_learning_rate";
  paddle::dialect::IrTensor ir_tensor_learning_rate(paddle::dialect::TransToPhiDataType(learning_rate.dtype()),
                                                      learning_rate.dims(),
                                                      learning_rate.data_layout(),
                                                      learning_rate.lod(),
                                                      learning_rate.offset());
  VLOG(4) << "Builder construction  meta_learning_rate";
  paddle::dialect::IrMetaTensor meta_learning_rate(&ir_tensor_learning_rate);

  VLOG(4) << "Builder construction  dense_moment1";
  paddle::dialect::IrTensor ir_tensor_moment1(paddle::dialect::TransToPhiDataType(moment1.dtype()),
                                                      moment1.dims(),
                                                      moment1.data_layout(),
                                                      moment1.lod(),
                                                      moment1.offset());
  VLOG(4) << "Builder construction  meta_moment1";
  paddle::dialect::IrMetaTensor meta_moment1(&ir_tensor_moment1);

  VLOG(4) << "Builder construction  dense_moment2";
  paddle::dialect::IrTensor ir_tensor_moment2(paddle::dialect::TransToPhiDataType(moment2.dtype()),
                                                      moment2.dims(),
                                                      moment2.data_layout(),
                                                      moment2.lod(),
                                                      moment2.offset());
  VLOG(4) << "Builder construction  meta_moment2";
  paddle::dialect::IrMetaTensor meta_moment2(&ir_tensor_moment2);

  VLOG(4) << "Builder construction  dense_beta1_pow";
  paddle::dialect::IrTensor ir_tensor_beta1_pow(paddle::dialect::TransToPhiDataType(beta1_pow.dtype()),
                                                      beta1_pow.dims(),
                                                      beta1_pow.data_layout(),
                                                      beta1_pow.lod(),
                                                      beta1_pow.offset());
  VLOG(4) << "Builder construction  meta_beta1_pow";
  paddle::dialect::IrMetaTensor meta_beta1_pow(&ir_tensor_beta1_pow);

  VLOG(4) << "Builder construction  dense_beta2_pow";
  paddle::dialect::IrTensor ir_tensor_beta2_pow(paddle::dialect::TransToPhiDataType(beta2_pow.dtype()),
                                                      beta2_pow.dims(),
                                                      beta2_pow.data_layout(),
                                                      beta2_pow.lod(),
                                                      beta2_pow.offset());
  VLOG(4) << "Builder construction  meta_beta2_pow";
  paddle::dialect::IrMetaTensor meta_beta2_pow(&ir_tensor_beta2_pow);

  paddle::dialect::IrMetaTensor meta_master_param;
  paddle::dialect::IrTensor ir_tensor_master_param;
  if (master_param_.impl() != nullptr) {
    paddle::dialect::DenseTensorType master_param = master_param_.type().dyn_cast<paddle::dialect::DenseTensorType>();
    VLOG(4) << "Builder construction  dense_master_param";
    ir_tensor_master_param = paddle::dialect::IrTensor(paddle::dialect::TransToPhiDataType(master_param.dtype()),
                                                        master_param.dims(),
                                                        master_param.data_layout(),
                                                        master_param.lod(),
                                                        master_param.offset());
    VLOG(4) << "Builder construction  meta_master_param";
    meta_master_param = paddle::dialect::IrMetaTensor(&ir_tensor_master_param);
  }


  paddle::dialect::IrMetaTensor meta_skip_update;
  paddle::dialect::IrTensor ir_tensor_skip_update;
  if (skip_update_.impl() != nullptr) {
    paddle::dialect::DenseTensorType skip_update = skip_update_.type().dyn_cast<paddle::dialect::DenseTensorType>();
    VLOG(4) << "Builder construction  dense_skip_update";
    ir_tensor_skip_update = paddle::dialect::IrTensor(paddle::dialect::TransToPhiDataType(skip_update.dtype()),
                                                        skip_update.dims(),
                                                        skip_update.data_layout(),
                                                        skip_update.lod(),
                                                        skip_update.offset());
    VLOG(4) << "Builder construction  meta_skip_update";
    meta_skip_update = paddle::dialect::IrMetaTensor(&ir_tensor_skip_update);
  }

  paddle::dialect::IrTensor dense_param_out;
  paddle::dialect::IrMetaTensor meta_param_out(&dense_param_out);
  paddle::dialect::IrTensor dense_moment1_out;
  paddle::dialect::IrMetaTensor meta_moment1_out(&dense_moment1_out);
  paddle::dialect::IrTensor dense_moment2_out;
  paddle::dialect::IrMetaTensor meta_moment2_out(&dense_moment2_out);
  paddle::dialect::IrTensor dense_beta1_pow_out;
  paddle::dialect::IrMetaTensor meta_beta1_pow_out(&dense_beta1_pow_out);
  paddle::dialect::IrTensor dense_beta2_pow_out;
  paddle::dialect::IrMetaTensor meta_beta2_pow_out(&dense_beta2_pow_out);
  paddle::dialect::IrTensor dense_master_param_outs;
  paddle::dialect::IrMetaTensor meta_master_param_outs(&dense_master_param_outs);

  phi::LambInferMeta(meta_param, meta_grad, meta_learning_rate, meta_moment1, meta_moment2, meta_beta1_pow, meta_beta2_pow, meta_master_param, meta_skip_update, weight_decay, beta1, beta2, epsilon, always_adapt, multi_precision, &meta_param_out, &meta_moment1_out, &meta_moment2_out, &meta_beta1_pow_out, &meta_beta2_pow_out, &meta_master_param_outs);

  std::vector<pir::Type> argument_outputs;
  pir::Type param_out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_param_out.dtype()), dense_param_out.dims(), dense_param_out.layout(), dense_param_out.lod(), dense_param_out.offset());
  argument_outputs.push_back(param_out_dense_tensor_type);

  pir::Type moment1_out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_moment1_out.dtype()), dense_moment1_out.dims(), dense_moment1_out.layout(), dense_moment1_out.lod(), dense_moment1_out.offset());
  argument_outputs.push_back(moment1_out_dense_tensor_type);

  pir::Type moment2_out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_moment2_out.dtype()), dense_moment2_out.dims(), dense_moment2_out.layout(), dense_moment2_out.lod(), dense_moment2_out.offset());
  argument_outputs.push_back(moment2_out_dense_tensor_type);

  if (beta1_pow_.impl() != nullptr) {
    pir::Type beta1_pow_out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_beta1_pow_out.dtype()), dense_beta1_pow_out.dims(), dense_beta1_pow_out.layout(), dense_beta1_pow_out.lod(), dense_beta1_pow_out.offset());
    argument_outputs.push_back(beta1_pow_out_dense_tensor_type);
  } else {
    pir::Type beta1_pow_out_type;
    argument_outputs.push_back(beta1_pow_out_type);
  }


  if (beta2_pow_.impl() != nullptr) {
    pir::Type beta2_pow_out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_beta2_pow_out.dtype()), dense_beta2_pow_out.dims(), dense_beta2_pow_out.layout(), dense_beta2_pow_out.lod(), dense_beta2_pow_out.offset());
    argument_outputs.push_back(beta2_pow_out_dense_tensor_type);
  } else {
    pir::Type beta2_pow_out_type;
    argument_outputs.push_back(beta2_pow_out_type);
  }


  if (master_param_.impl() != nullptr) {
    pir::Type master_param_outs_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_master_param_outs.dtype()), dense_master_param_outs.dims(), dense_master_param_outs.layout(), dense_master_param_outs.lod(), dense_master_param_outs.offset());
    argument_outputs.push_back(master_param_outs_dense_tensor_type);
  } else {
    pir::Type master_param_outs_type;
    argument_outputs.push_back(master_param_outs_type);
  }

  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void Lamb_Op::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value param_, pir::Value grad_, pir::Value learning_rate_, pir::Value moment1_, pir::Value moment2_, pir::Value beta1_pow_, pir::Value beta2_pow_, pir::Value master_param_, pir::Value skip_update_, pir::AttributeMap attributes) {
  VLOG(4) << "Start build Lamb_Op";


  IR_ENFORCE(
      attributes.find("weight_decay") != attributes.end(),
          "'weight_decay' Attribute is expected for Lamb_Op. ");
  float weight_decay = attributes.at("weight_decay").dyn_cast<pir::FloatAttribute>().data();

  IR_ENFORCE(
      attributes.find("beta1") != attributes.end(),
          "'beta1' Attribute is expected for Lamb_Op. ");
  float beta1 = attributes.at("beta1").dyn_cast<pir::FloatAttribute>().data();

  IR_ENFORCE(
      attributes.find("beta2") != attributes.end(),
          "'beta2' Attribute is expected for Lamb_Op. ");
  float beta2 = attributes.at("beta2").dyn_cast<pir::FloatAttribute>().data();

  IR_ENFORCE(
      attributes.find("epsilon") != attributes.end(),
          "'epsilon' Attribute is expected for Lamb_Op. ");
  float epsilon = attributes.at("epsilon").dyn_cast<pir::FloatAttribute>().data();

  IR_ENFORCE(
      attributes.find("always_adapt") != attributes.end(),
          "'always_adapt' Attribute is expected for Lamb_Op. ");
  bool always_adapt = attributes.at("always_adapt").dyn_cast<pir::BoolAttribute>().data();

  IR_ENFORCE(
      attributes.find("multi_precision") != attributes.end(),
          "'multi_precision' Attribute is expected for Lamb_Op. ");
  bool multi_precision = attributes.at("multi_precision").dyn_cast<pir::BoolAttribute>().data();


  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {param_, grad_, learning_rate_, moment1_, moment2_, beta1_pow_, beta2_pow_, master_param_, skip_update_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_weight_decay = pir::FloatAttribute::get(pir::IrContext::Instance(), weight_decay);
  argument.AddAttribute("weight_decay", attr_weight_decay);
  pir::Attribute attr_beta1 = pir::FloatAttribute::get(pir::IrContext::Instance(), beta1);
  argument.AddAttribute("beta1", attr_beta1);
  pir::Attribute attr_beta2 = pir::FloatAttribute::get(pir::IrContext::Instance(), beta2);
  argument.AddAttribute("beta2", attr_beta2);
  pir::Attribute attr_epsilon = pir::FloatAttribute::get(pir::IrContext::Instance(), epsilon);
  argument.AddAttribute("epsilon", attr_epsilon);
  pir::Attribute attr_always_adapt = pir::BoolAttribute::get(pir::IrContext::Instance(), always_adapt);
  argument.AddAttribute("always_adapt", attr_always_adapt);
  pir::Attribute attr_multi_precision = pir::BoolAttribute::get(pir::IrContext::Instance(), multi_precision);
  argument.AddAttribute("multi_precision", attr_multi_precision);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType param = param_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)param;
  paddle::dialect::DenseTensorType grad = grad_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)grad;
  paddle::dialect::DenseTensorType learning_rate = learning_rate_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)learning_rate;
  paddle::dialect::DenseTensorType moment1 = moment1_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)moment1;
  paddle::dialect::DenseTensorType moment2 = moment2_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)moment2;
  paddle::dialect::DenseTensorType beta1_pow = beta1_pow_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)beta1_pow;
  paddle::dialect::DenseTensorType beta2_pow = beta2_pow_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)beta2_pow;

  VLOG(4) << "Builder construction  dense_param";
  paddle::dialect::IrTensor ir_tensor_param(paddle::dialect::TransToPhiDataType(param.dtype()),
                                                      param.dims(),
                                                      param.data_layout(),
                                                      param.lod(),
                                                      param.offset());
  VLOG(4) << "Builder construction  meta_param";
  paddle::dialect::IrMetaTensor meta_param(&ir_tensor_param);

  VLOG(4) << "Builder construction  dense_grad";
  paddle::dialect::IrTensor ir_tensor_grad(paddle::dialect::TransToPhiDataType(grad.dtype()),
                                                      grad.dims(),
                                                      grad.data_layout(),
                                                      grad.lod(),
                                                      grad.offset());
  VLOG(4) << "Builder construction  meta_grad";
  paddle::dialect::IrMetaTensor meta_grad(&ir_tensor_grad);

  VLOG(4) << "Builder construction  dense_learning_rate";
  paddle::dialect::IrTensor ir_tensor_learning_rate(paddle::dialect::TransToPhiDataType(learning_rate.dtype()),
                                                      learning_rate.dims(),
                                                      learning_rate.data_layout(),
                                                      learning_rate.lod(),
                                                      learning_rate.offset());
  VLOG(4) << "Builder construction  meta_learning_rate";
  paddle::dialect::IrMetaTensor meta_learning_rate(&ir_tensor_learning_rate);

  VLOG(4) << "Builder construction  dense_moment1";
  paddle::dialect::IrTensor ir_tensor_moment1(paddle::dialect::TransToPhiDataType(moment1.dtype()),
                                                      moment1.dims(),
                                                      moment1.data_layout(),
                                                      moment1.lod(),
                                                      moment1.offset());
  VLOG(4) << "Builder construction  meta_moment1";
  paddle::dialect::IrMetaTensor meta_moment1(&ir_tensor_moment1);

  VLOG(4) << "Builder construction  dense_moment2";
  paddle::dialect::IrTensor ir_tensor_moment2(paddle::dialect::TransToPhiDataType(moment2.dtype()),
                                                      moment2.dims(),
                                                      moment2.data_layout(),
                                                      moment2.lod(),
                                                      moment2.offset());
  VLOG(4) << "Builder construction  meta_moment2";
  paddle::dialect::IrMetaTensor meta_moment2(&ir_tensor_moment2);

  VLOG(4) << "Builder construction  dense_beta1_pow";
  paddle::dialect::IrTensor ir_tensor_beta1_pow(paddle::dialect::TransToPhiDataType(beta1_pow.dtype()),
                                                      beta1_pow.dims(),
                                                      beta1_pow.data_layout(),
                                                      beta1_pow.lod(),
                                                      beta1_pow.offset());
  VLOG(4) << "Builder construction  meta_beta1_pow";
  paddle::dialect::IrMetaTensor meta_beta1_pow(&ir_tensor_beta1_pow);

  VLOG(4) << "Builder construction  dense_beta2_pow";
  paddle::dialect::IrTensor ir_tensor_beta2_pow(paddle::dialect::TransToPhiDataType(beta2_pow.dtype()),
                                                      beta2_pow.dims(),
                                                      beta2_pow.data_layout(),
                                                      beta2_pow.lod(),
                                                      beta2_pow.offset());
  VLOG(4) << "Builder construction  meta_beta2_pow";
  paddle::dialect::IrMetaTensor meta_beta2_pow(&ir_tensor_beta2_pow);

  paddle::dialect::IrMetaTensor meta_master_param;
  paddle::dialect::IrTensor ir_tensor_master_param;
  if (master_param_.impl() != nullptr) {
    paddle::dialect::DenseTensorType master_param = master_param_.type().dyn_cast<paddle::dialect::DenseTensorType>();
    VLOG(4) << "Builder construction  dense_master_param";
    ir_tensor_master_param = paddle::dialect::IrTensor(paddle::dialect::TransToPhiDataType(master_param.dtype()),
                                                        master_param.dims(),
                                                        master_param.data_layout(),
                                                        master_param.lod(),
                                                        master_param.offset());
    VLOG(4) << "Builder construction  meta_master_param";
    meta_master_param = paddle::dialect::IrMetaTensor(&ir_tensor_master_param);
  }


  paddle::dialect::IrMetaTensor meta_skip_update;
  paddle::dialect::IrTensor ir_tensor_skip_update;
  if (skip_update_.impl() != nullptr) {
    paddle::dialect::DenseTensorType skip_update = skip_update_.type().dyn_cast<paddle::dialect::DenseTensorType>();
    VLOG(4) << "Builder construction  dense_skip_update";
    ir_tensor_skip_update = paddle::dialect::IrTensor(paddle::dialect::TransToPhiDataType(skip_update.dtype()),
                                                        skip_update.dims(),
                                                        skip_update.data_layout(),
                                                        skip_update.lod(),
                                                        skip_update.offset());
    VLOG(4) << "Builder construction  meta_skip_update";
    meta_skip_update = paddle::dialect::IrMetaTensor(&ir_tensor_skip_update);
  }

  paddle::dialect::IrTensor dense_param_out;
  paddle::dialect::IrMetaTensor meta_param_out(&dense_param_out);
  paddle::dialect::IrTensor dense_moment1_out;
  paddle::dialect::IrMetaTensor meta_moment1_out(&dense_moment1_out);
  paddle::dialect::IrTensor dense_moment2_out;
  paddle::dialect::IrMetaTensor meta_moment2_out(&dense_moment2_out);
  paddle::dialect::IrTensor dense_beta1_pow_out;
  paddle::dialect::IrMetaTensor meta_beta1_pow_out(&dense_beta1_pow_out);
  paddle::dialect::IrTensor dense_beta2_pow_out;
  paddle::dialect::IrMetaTensor meta_beta2_pow_out(&dense_beta2_pow_out);
  paddle::dialect::IrTensor dense_master_param_outs;
  paddle::dialect::IrMetaTensor meta_master_param_outs(&dense_master_param_outs);

  phi::LambInferMeta(meta_param, meta_grad, meta_learning_rate, meta_moment1, meta_moment2, meta_beta1_pow, meta_beta2_pow, meta_master_param, meta_skip_update, weight_decay, beta1, beta2, epsilon, always_adapt, multi_precision, &meta_param_out, &meta_moment1_out, &meta_moment2_out, &meta_beta1_pow_out, &meta_beta2_pow_out, &meta_master_param_outs);

  std::vector<pir::Type> argument_outputs;
  pir::Type param_out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_param_out.dtype()), dense_param_out.dims(), dense_param_out.layout(), dense_param_out.lod(), dense_param_out.offset());
  argument_outputs.push_back(param_out_dense_tensor_type);

  pir::Type moment1_out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_moment1_out.dtype()), dense_moment1_out.dims(), dense_moment1_out.layout(), dense_moment1_out.lod(), dense_moment1_out.offset());
  argument_outputs.push_back(moment1_out_dense_tensor_type);

  pir::Type moment2_out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_moment2_out.dtype()), dense_moment2_out.dims(), dense_moment2_out.layout(), dense_moment2_out.lod(), dense_moment2_out.offset());
  argument_outputs.push_back(moment2_out_dense_tensor_type);

  if (beta1_pow_.impl() != nullptr) {
    pir::Type beta1_pow_out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_beta1_pow_out.dtype()), dense_beta1_pow_out.dims(), dense_beta1_pow_out.layout(), dense_beta1_pow_out.lod(), dense_beta1_pow_out.offset());
    argument_outputs.push_back(beta1_pow_out_dense_tensor_type);
  } else {
    pir::Type beta1_pow_out_type;
    argument_outputs.push_back(beta1_pow_out_type);
  }


  if (beta2_pow_.impl() != nullptr) {
    pir::Type beta2_pow_out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_beta2_pow_out.dtype()), dense_beta2_pow_out.dims(), dense_beta2_pow_out.layout(), dense_beta2_pow_out.lod(), dense_beta2_pow_out.offset());
    argument_outputs.push_back(beta2_pow_out_dense_tensor_type);
  } else {
    pir::Type beta2_pow_out_type;
    argument_outputs.push_back(beta2_pow_out_type);
  }


  if (master_param_.impl() != nullptr) {
    pir::Type master_param_outs_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_master_param_outs.dtype()), dense_master_param_outs.dims(), dense_master_param_outs.layout(), dense_master_param_outs.lod(), dense_master_param_outs.offset());
    argument_outputs.push_back(master_param_outs_dense_tensor_type);
  } else {
    pir::Type master_param_outs_type;
    argument_outputs.push_back(master_param_outs_type);
  }

  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void Lamb_Op::VerifySig() {
  VLOG(4) << "Start Verifying inputs, outputs and attributes for: Lamb_Op.";
  VLOG(4) << "Verifying inputs:";
  {
  auto input_size = num_operands();
  IR_ENFORCE(input_size == 9u,
                    "The size %d of inputs must be equal to 9.", input_size);
  IR_ENFORCE((*this)->operand_source(0).type().isa<paddle::dialect::DenseTensorType>(),
                  "Type validation failed for the 0th input, got %s.", (*this)->operand_source(0).type());
  IR_ENFORCE((*this)->operand_source(1).type().isa<paddle::dialect::DenseTensorType>(),
                  "Type validation failed for the 1th input, got %s.", (*this)->operand_source(1).type());
  IR_ENFORCE((*this)->operand_source(2).type().isa<paddle::dialect::DenseTensorType>(),
                  "Type validation failed for the 2th input, got %s.", (*this)->operand_source(2).type());
  IR_ENFORCE((*this)->operand_source(3).type().isa<paddle::dialect::DenseTensorType>(),
                  "Type validation failed for the 3th input, got %s.", (*this)->operand_source(3).type());
  IR_ENFORCE((*this)->operand_source(4).type().isa<paddle::dialect::DenseTensorType>(),
                  "Type validation failed for the 4th input, got %s.", (*this)->operand_source(4).type());
  IR_ENFORCE((*this)->operand_source(5).type().isa<paddle::dialect::DenseTensorType>(),
                  "Type validation failed for the 5th input, got %s.", (*this)->operand_source(5).type());
  IR_ENFORCE((*this)->operand_source(6).type().isa<paddle::dialect::DenseTensorType>(),
                  "Type validation failed for the 6th input, got %s.", (*this)->operand_source(6).type());
  if (auto val = (*this)->operand(7)) {
    IR_ENFORCE(val.type().isa<paddle::dialect::DenseTensorType>(),
                   "Type validation failed for the 7th input, got %s.", (*this)->operand_source(7).type());
  }
  if (auto val = (*this)->operand(8)) {
    IR_ENFORCE(val.type().isa<paddle::dialect::DenseTensorType>(),
                   "Type validation failed for the 8th input, got %s.", (*this)->operand_source(8).type());
  }
  }
  VLOG(4) << "Verifying attributes:";
  {
  auto& attributes = this->attributes();
  IR_ENFORCE(attributes.count("weight_decay")>0,
                 "weight_decay does not exist.");
  IR_ENFORCE(attributes.at("weight_decay").isa<pir::FloatAttribute>(),
                 "Type of attribute: weight_decay is not pir::FloatAttribute.");

  IR_ENFORCE(attributes.count("beta1")>0,
                 "beta1 does not exist.");
  IR_ENFORCE(attributes.at("beta1").isa<pir::FloatAttribute>(),
                 "Type of attribute: beta1 is not pir::FloatAttribute.");

  IR_ENFORCE(attributes.count("beta2")>0,
                 "beta2 does not exist.");
  IR_ENFORCE(attributes.at("beta2").isa<pir::FloatAttribute>(),
                 "Type of attribute: beta2 is not pir::FloatAttribute.");

  IR_ENFORCE(attributes.count("epsilon")>0,
                 "epsilon does not exist.");
  IR_ENFORCE(attributes.at("epsilon").isa<pir::FloatAttribute>(),
                 "Type of attribute: epsilon is not pir::FloatAttribute.");

  IR_ENFORCE(attributes.count("always_adapt")>0,
                 "always_adapt does not exist.");
  IR_ENFORCE(attributes.at("always_adapt").isa<pir::BoolAttribute>(),
                 "Type of attribute: always_adapt is not pir::BoolAttribute.");

  IR_ENFORCE(attributes.count("multi_precision")>0,
                 "multi_precision does not exist.");
  IR_ENFORCE(attributes.at("multi_precision").isa<pir::BoolAttribute>(),
                 "Type of attribute: multi_precision is not pir::BoolAttribute.");

  }
  VLOG(4) << "Verifying outputs:";
  {
  auto output_size = num_results();
  IR_ENFORCE(output_size == 6u,
                    "The size %d of outputs must be equal to 6.", output_size);
  IR_ENFORCE((*this)->result(0).type().isa<paddle::dialect::DenseTensorType>(),
                 "Type validation failed for the 0th output.");
  IR_ENFORCE((*this)->result(1).type().isa<paddle::dialect::DenseTensorType>(),
                 "Type validation failed for the 1th output.");
  IR_ENFORCE((*this)->result(2).type().isa<paddle::dialect::DenseTensorType>(),
                 "Type validation failed for the 2th output.");
  if (auto output_3_type = (*this)->result(3).type()) {
    IR_ENFORCE(output_3_type.isa<paddle::dialect::DenseTensorType>(),
                   "Type validation failed for the 3th output.");
  }
  if (auto output_4_type = (*this)->result(4).type()) {
    IR_ENFORCE(output_4_type.isa<paddle::dialect::DenseTensorType>(),
                   "Type validation failed for the 4th output.");
  }
  if (auto output_5_type = (*this)->result(5).type()) {
    IR_ENFORCE(output_5_type.isa<paddle::dialect::DenseTensorType>(),
                   "Type validation failed for the 5th output.");
  }
  }
  VLOG(4) << "End Verifying for: Lamb_Op.";
}

void Lamb_Op::InferMeta( phi::InferMetaContext *infer_meta ) {
  auto fn = PD_INFER_META(phi::LambInferMeta);
  fn(infer_meta);
}

phi::DataType Lamb_Op::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: Lamb_Op";
  


  return expected_kernel_dtype;
}

const char *LambSr_Op::attributes_name[6] = { "weight_decay", "beta1", "beta2", "epsilon", "always_adapt", "multi_precision" };

OpInfoTuple LambSr_Op::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("param", "paddle::dialect::DenseTensorType", false, false, false, false), paddle::dialect::OpInputInfo("grad", "paddle::dialect::SelectedRowsType", false, false, false, false), paddle::dialect::OpInputInfo("learning_rate", "paddle::dialect::DenseTensorType", false, false, false, false), paddle::dialect::OpInputInfo("moment1", "paddle::dialect::DenseTensorType", false, false, false, false), paddle::dialect::OpInputInfo("moment2", "paddle::dialect::DenseTensorType", false, false, false, false), paddle::dialect::OpInputInfo("beta1_pow", "paddle::dialect::DenseTensorType", false, false, false, false), paddle::dialect::OpInputInfo("beta2_pow", "paddle::dialect::DenseTensorType", false, false, false, false), paddle::dialect::OpInputInfo("master_param", "paddle::dialect::DenseTensorType", true, false, false, false), paddle::dialect::OpInputInfo("skip_update", "paddle::dialect::DenseTensorType", true, false, false, false) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = { paddle::dialect::OpAttributeInfo("weight_decay", "pir::FloatAttribute", ""), paddle::dialect::OpAttributeInfo("beta1", "pir::FloatAttribute", ""), paddle::dialect::OpAttributeInfo("beta2", "pir::FloatAttribute", ""), paddle::dialect::OpAttributeInfo("epsilon", "pir::FloatAttribute", ""), paddle::dialect::OpAttributeInfo("always_adapt", "pir::BoolAttribute", ""), paddle::dialect::OpAttributeInfo("multi_precision", "pir::BoolAttribute", "") };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("param_out", "paddle::dialect::DenseTensorType", false, false), paddle::dialect::OpOutputInfo("moment1_out", "paddle::dialect::DenseTensorType", false, false), paddle::dialect::OpOutputInfo("moment2_out", "paddle::dialect::DenseTensorType", false, false), paddle::dialect::OpOutputInfo("beta1_pow_out", "paddle::dialect::DenseTensorType", true, false), paddle::dialect::OpOutputInfo("beta2_pow_out", "paddle::dialect::DenseTensorType", true, false), paddle::dialect::OpOutputInfo("master_param_outs", "paddle::dialect::DenseTensorType", true, false) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("LambInferMeta", {"param", "grad", "learning_rate", "moment1", "moment2", "beta1_pow", "beta2_pow", "master_param", "skip_update", "weight_decay", "beta1", "beta2", "epsilon", "always_adapt", "multi_precision"}, "lamb_sr", {"param", "grad", "learning_rate", "moment1", "moment2", "beta1_pow", "beta2_pow", "master_param", "skip_update", "weight_decay", "beta1", "beta2", "epsilon", "always_adapt", "multi_precision"}, {"param"}, {}, {{"param_out", "param"},{"moment1_out", "moment1"},{"moment2_out", "moment2"},{"beta1_pow_out", "beta1_pow"},{"beta2_pow_out", "beta2_pow"},{"master_param_outs", "master_param"}}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "lamb_");
}

void LambSr_Op::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value param_, pir::Value grad_, pir::Value learning_rate_, pir::Value moment1_, pir::Value moment2_, pir::Value beta1_pow_, pir::Value beta2_pow_, pir::Value master_param_, pir::Value skip_update_, float weight_decay, float beta1, float beta2, float epsilon, bool always_adapt, bool multi_precision) {
  VLOG(4) << "Start build LambSr_Op";



  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {param_, grad_, learning_rate_, moment1_, moment2_, beta1_pow_, beta2_pow_, master_param_, skip_update_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_weight_decay = pir::FloatAttribute::get(pir::IrContext::Instance(), weight_decay);
  argument.AddAttribute("weight_decay", attr_weight_decay);
  pir::Attribute attr_beta1 = pir::FloatAttribute::get(pir::IrContext::Instance(), beta1);
  argument.AddAttribute("beta1", attr_beta1);
  pir::Attribute attr_beta2 = pir::FloatAttribute::get(pir::IrContext::Instance(), beta2);
  argument.AddAttribute("beta2", attr_beta2);
  pir::Attribute attr_epsilon = pir::FloatAttribute::get(pir::IrContext::Instance(), epsilon);
  argument.AddAttribute("epsilon", attr_epsilon);
  pir::Attribute attr_always_adapt = pir::BoolAttribute::get(pir::IrContext::Instance(), always_adapt);
  argument.AddAttribute("always_adapt", attr_always_adapt);
  pir::Attribute attr_multi_precision = pir::BoolAttribute::get(pir::IrContext::Instance(), multi_precision);
  argument.AddAttribute("multi_precision", attr_multi_precision);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType param = param_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)param;
  paddle::dialect::SelectedRowsType grad = grad_.type().dyn_cast<paddle::dialect::SelectedRowsType>(); (void)grad;
  paddle::dialect::DenseTensorType learning_rate = learning_rate_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)learning_rate;
  paddle::dialect::DenseTensorType moment1 = moment1_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)moment1;
  paddle::dialect::DenseTensorType moment2 = moment2_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)moment2;
  paddle::dialect::DenseTensorType beta1_pow = beta1_pow_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)beta1_pow;
  paddle::dialect::DenseTensorType beta2_pow = beta2_pow_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)beta2_pow;

  VLOG(4) << "Builder construction  dense_param";
  paddle::dialect::IrTensor ir_tensor_param(paddle::dialect::TransToPhiDataType(param.dtype()),
                                                      param.dims(),
                                                      param.data_layout(),
                                                      param.lod(),
                                                      param.offset());
  VLOG(4) << "Builder construction  meta_param";
  paddle::dialect::IrMetaTensor meta_param(&ir_tensor_param);

  VLOG(4) << "Builder construction  dense_grad";
  paddle::dialect::IrTensor ir_tensor_grad(paddle::dialect::TransToPhiDataType(grad.dtype()),
                                                      grad.dims(),
                                                      grad.data_layout(),
                                                      grad.lod(),
                                                      grad.offset());
  VLOG(4) << "Builder construction  meta_grad";
  paddle::dialect::IrMetaTensor meta_grad(&ir_tensor_grad);

  VLOG(4) << "Builder construction  dense_learning_rate";
  paddle::dialect::IrTensor ir_tensor_learning_rate(paddle::dialect::TransToPhiDataType(learning_rate.dtype()),
                                                      learning_rate.dims(),
                                                      learning_rate.data_layout(),
                                                      learning_rate.lod(),
                                                      learning_rate.offset());
  VLOG(4) << "Builder construction  meta_learning_rate";
  paddle::dialect::IrMetaTensor meta_learning_rate(&ir_tensor_learning_rate);

  VLOG(4) << "Builder construction  dense_moment1";
  paddle::dialect::IrTensor ir_tensor_moment1(paddle::dialect::TransToPhiDataType(moment1.dtype()),
                                                      moment1.dims(),
                                                      moment1.data_layout(),
                                                      moment1.lod(),
                                                      moment1.offset());
  VLOG(4) << "Builder construction  meta_moment1";
  paddle::dialect::IrMetaTensor meta_moment1(&ir_tensor_moment1);

  VLOG(4) << "Builder construction  dense_moment2";
  paddle::dialect::IrTensor ir_tensor_moment2(paddle::dialect::TransToPhiDataType(moment2.dtype()),
                                                      moment2.dims(),
                                                      moment2.data_layout(),
                                                      moment2.lod(),
                                                      moment2.offset());
  VLOG(4) << "Builder construction  meta_moment2";
  paddle::dialect::IrMetaTensor meta_moment2(&ir_tensor_moment2);

  VLOG(4) << "Builder construction  dense_beta1_pow";
  paddle::dialect::IrTensor ir_tensor_beta1_pow(paddle::dialect::TransToPhiDataType(beta1_pow.dtype()),
                                                      beta1_pow.dims(),
                                                      beta1_pow.data_layout(),
                                                      beta1_pow.lod(),
                                                      beta1_pow.offset());
  VLOG(4) << "Builder construction  meta_beta1_pow";
  paddle::dialect::IrMetaTensor meta_beta1_pow(&ir_tensor_beta1_pow);

  VLOG(4) << "Builder construction  dense_beta2_pow";
  paddle::dialect::IrTensor ir_tensor_beta2_pow(paddle::dialect::TransToPhiDataType(beta2_pow.dtype()),
                                                      beta2_pow.dims(),
                                                      beta2_pow.data_layout(),
                                                      beta2_pow.lod(),
                                                      beta2_pow.offset());
  VLOG(4) << "Builder construction  meta_beta2_pow";
  paddle::dialect::IrMetaTensor meta_beta2_pow(&ir_tensor_beta2_pow);

  paddle::dialect::IrMetaTensor meta_master_param;
  paddle::dialect::IrTensor ir_tensor_master_param;
  if (master_param_.impl() != nullptr) {
    paddle::dialect::DenseTensorType master_param = master_param_.type().dyn_cast<paddle::dialect::DenseTensorType>();
    VLOG(4) << "Builder construction  dense_master_param";
    ir_tensor_master_param = paddle::dialect::IrTensor(paddle::dialect::TransToPhiDataType(master_param.dtype()),
                                                        master_param.dims(),
                                                        master_param.data_layout(),
                                                        master_param.lod(),
                                                        master_param.offset());
    VLOG(4) << "Builder construction  meta_master_param";
    meta_master_param = paddle::dialect::IrMetaTensor(&ir_tensor_master_param);
  }


  paddle::dialect::IrMetaTensor meta_skip_update;
  paddle::dialect::IrTensor ir_tensor_skip_update;
  if (skip_update_.impl() != nullptr) {
    paddle::dialect::DenseTensorType skip_update = skip_update_.type().dyn_cast<paddle::dialect::DenseTensorType>();
    VLOG(4) << "Builder construction  dense_skip_update";
    ir_tensor_skip_update = paddle::dialect::IrTensor(paddle::dialect::TransToPhiDataType(skip_update.dtype()),
                                                        skip_update.dims(),
                                                        skip_update.data_layout(),
                                                        skip_update.lod(),
                                                        skip_update.offset());
    VLOG(4) << "Builder construction  meta_skip_update";
    meta_skip_update = paddle::dialect::IrMetaTensor(&ir_tensor_skip_update);
  }

  paddle::dialect::IrTensor dense_param_out;
  paddle::dialect::IrMetaTensor meta_param_out(&dense_param_out);
  paddle::dialect::IrTensor dense_moment1_out;
  paddle::dialect::IrMetaTensor meta_moment1_out(&dense_moment1_out);
  paddle::dialect::IrTensor dense_moment2_out;
  paddle::dialect::IrMetaTensor meta_moment2_out(&dense_moment2_out);
  paddle::dialect::IrTensor dense_beta1_pow_out;
  paddle::dialect::IrMetaTensor meta_beta1_pow_out(&dense_beta1_pow_out);
  paddle::dialect::IrTensor dense_beta2_pow_out;
  paddle::dialect::IrMetaTensor meta_beta2_pow_out(&dense_beta2_pow_out);
  paddle::dialect::IrTensor dense_master_param_outs;
  paddle::dialect::IrMetaTensor meta_master_param_outs(&dense_master_param_outs);

  phi::LambInferMeta(meta_param, meta_grad, meta_learning_rate, meta_moment1, meta_moment2, meta_beta1_pow, meta_beta2_pow, meta_master_param, meta_skip_update, weight_decay, beta1, beta2, epsilon, always_adapt, multi_precision, &meta_param_out, &meta_moment1_out, &meta_moment2_out, &meta_beta1_pow_out, &meta_beta2_pow_out, &meta_master_param_outs);

  std::vector<pir::Type> argument_outputs;
  pir::Type param_out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_param_out.dtype()), dense_param_out.dims(), dense_param_out.layout(), dense_param_out.lod(), dense_param_out.offset());
  argument_outputs.push_back(param_out_dense_tensor_type);

  pir::Type moment1_out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_moment1_out.dtype()), dense_moment1_out.dims(), dense_moment1_out.layout(), dense_moment1_out.lod(), dense_moment1_out.offset());
  argument_outputs.push_back(moment1_out_dense_tensor_type);

  pir::Type moment2_out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_moment2_out.dtype()), dense_moment2_out.dims(), dense_moment2_out.layout(), dense_moment2_out.lod(), dense_moment2_out.offset());
  argument_outputs.push_back(moment2_out_dense_tensor_type);

  if (beta1_pow_.impl() != nullptr) {
    pir::Type beta1_pow_out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_beta1_pow_out.dtype()), dense_beta1_pow_out.dims(), dense_beta1_pow_out.layout(), dense_beta1_pow_out.lod(), dense_beta1_pow_out.offset());
    argument_outputs.push_back(beta1_pow_out_dense_tensor_type);
  } else {
    pir::Type beta1_pow_out_type;
    argument_outputs.push_back(beta1_pow_out_type);
  }


  if (beta2_pow_.impl() != nullptr) {
    pir::Type beta2_pow_out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_beta2_pow_out.dtype()), dense_beta2_pow_out.dims(), dense_beta2_pow_out.layout(), dense_beta2_pow_out.lod(), dense_beta2_pow_out.offset());
    argument_outputs.push_back(beta2_pow_out_dense_tensor_type);
  } else {
    pir::Type beta2_pow_out_type;
    argument_outputs.push_back(beta2_pow_out_type);
  }


  if (master_param_.impl() != nullptr) {
    pir::Type master_param_outs_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_master_param_outs.dtype()), dense_master_param_outs.dims(), dense_master_param_outs.layout(), dense_master_param_outs.lod(), dense_master_param_outs.offset());
    argument_outputs.push_back(master_param_outs_dense_tensor_type);
  } else {
    pir::Type master_param_outs_type;
    argument_outputs.push_back(master_param_outs_type);
  }

  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void LambSr_Op::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value param_, pir::Value grad_, pir::Value learning_rate_, pir::Value moment1_, pir::Value moment2_, pir::Value beta1_pow_, pir::Value beta2_pow_, pir::Value master_param_, pir::Value skip_update_, pir::AttributeMap attributes) {
  VLOG(4) << "Start build LambSr_Op";


  IR_ENFORCE(
      attributes.find("weight_decay") != attributes.end(),
          "'weight_decay' Attribute is expected for LambSr_Op. ");
  float weight_decay = attributes.at("weight_decay").dyn_cast<pir::FloatAttribute>().data();

  IR_ENFORCE(
      attributes.find("beta1") != attributes.end(),
          "'beta1' Attribute is expected for LambSr_Op. ");
  float beta1 = attributes.at("beta1").dyn_cast<pir::FloatAttribute>().data();

  IR_ENFORCE(
      attributes.find("beta2") != attributes.end(),
          "'beta2' Attribute is expected for LambSr_Op. ");
  float beta2 = attributes.at("beta2").dyn_cast<pir::FloatAttribute>().data();

  IR_ENFORCE(
      attributes.find("epsilon") != attributes.end(),
          "'epsilon' Attribute is expected for LambSr_Op. ");
  float epsilon = attributes.at("epsilon").dyn_cast<pir::FloatAttribute>().data();

  IR_ENFORCE(
      attributes.find("always_adapt") != attributes.end(),
          "'always_adapt' Attribute is expected for LambSr_Op. ");
  bool always_adapt = attributes.at("always_adapt").dyn_cast<pir::BoolAttribute>().data();

  IR_ENFORCE(
      attributes.find("multi_precision") != attributes.end(),
          "'multi_precision' Attribute is expected for LambSr_Op. ");
  bool multi_precision = attributes.at("multi_precision").dyn_cast<pir::BoolAttribute>().data();


  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {param_, grad_, learning_rate_, moment1_, moment2_, beta1_pow_, beta2_pow_, master_param_, skip_update_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_weight_decay = pir::FloatAttribute::get(pir::IrContext::Instance(), weight_decay);
  argument.AddAttribute("weight_decay", attr_weight_decay);
  pir::Attribute attr_beta1 = pir::FloatAttribute::get(pir::IrContext::Instance(), beta1);
  argument.AddAttribute("beta1", attr_beta1);
  pir::Attribute attr_beta2 = pir::FloatAttribute::get(pir::IrContext::Instance(), beta2);
  argument.AddAttribute("beta2", attr_beta2);
  pir::Attribute attr_epsilon = pir::FloatAttribute::get(pir::IrContext::Instance(), epsilon);
  argument.AddAttribute("epsilon", attr_epsilon);
  pir::Attribute attr_always_adapt = pir::BoolAttribute::get(pir::IrContext::Instance(), always_adapt);
  argument.AddAttribute("always_adapt", attr_always_adapt);
  pir::Attribute attr_multi_precision = pir::BoolAttribute::get(pir::IrContext::Instance(), multi_precision);
  argument.AddAttribute("multi_precision", attr_multi_precision);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType param = param_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)param;
  paddle::dialect::SelectedRowsType grad = grad_.type().dyn_cast<paddle::dialect::SelectedRowsType>(); (void)grad;
  paddle::dialect::DenseTensorType learning_rate = learning_rate_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)learning_rate;
  paddle::dialect::DenseTensorType moment1 = moment1_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)moment1;
  paddle::dialect::DenseTensorType moment2 = moment2_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)moment2;
  paddle::dialect::DenseTensorType beta1_pow = beta1_pow_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)beta1_pow;
  paddle::dialect::DenseTensorType beta2_pow = beta2_pow_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)beta2_pow;

  VLOG(4) << "Builder construction  dense_param";
  paddle::dialect::IrTensor ir_tensor_param(paddle::dialect::TransToPhiDataType(param.dtype()),
                                                      param.dims(),
                                                      param.data_layout(),
                                                      param.lod(),
                                                      param.offset());
  VLOG(4) << "Builder construction  meta_param";
  paddle::dialect::IrMetaTensor meta_param(&ir_tensor_param);

  VLOG(4) << "Builder construction  dense_grad";
  paddle::dialect::IrTensor ir_tensor_grad(paddle::dialect::TransToPhiDataType(grad.dtype()),
                                                      grad.dims(),
                                                      grad.data_layout(),
                                                      grad.lod(),
                                                      grad.offset());
  VLOG(4) << "Builder construction  meta_grad";
  paddle::dialect::IrMetaTensor meta_grad(&ir_tensor_grad);

  VLOG(4) << "Builder construction  dense_learning_rate";
  paddle::dialect::IrTensor ir_tensor_learning_rate(paddle::dialect::TransToPhiDataType(learning_rate.dtype()),
                                                      learning_rate.dims(),
                                                      learning_rate.data_layout(),
                                                      learning_rate.lod(),
                                                      learning_rate.offset());
  VLOG(4) << "Builder construction  meta_learning_rate";
  paddle::dialect::IrMetaTensor meta_learning_rate(&ir_tensor_learning_rate);

  VLOG(4) << "Builder construction  dense_moment1";
  paddle::dialect::IrTensor ir_tensor_moment1(paddle::dialect::TransToPhiDataType(moment1.dtype()),
                                                      moment1.dims(),
                                                      moment1.data_layout(),
                                                      moment1.lod(),
                                                      moment1.offset());
  VLOG(4) << "Builder construction  meta_moment1";
  paddle::dialect::IrMetaTensor meta_moment1(&ir_tensor_moment1);

  VLOG(4) << "Builder construction  dense_moment2";
  paddle::dialect::IrTensor ir_tensor_moment2(paddle::dialect::TransToPhiDataType(moment2.dtype()),
                                                      moment2.dims(),
                                                      moment2.data_layout(),
                                                      moment2.lod(),
                                                      moment2.offset());
  VLOG(4) << "Builder construction  meta_moment2";
  paddle::dialect::IrMetaTensor meta_moment2(&ir_tensor_moment2);

  VLOG(4) << "Builder construction  dense_beta1_pow";
  paddle::dialect::IrTensor ir_tensor_beta1_pow(paddle::dialect::TransToPhiDataType(beta1_pow.dtype()),
                                                      beta1_pow.dims(),
                                                      beta1_pow.data_layout(),
                                                      beta1_pow.lod(),
                                                      beta1_pow.offset());
  VLOG(4) << "Builder construction  meta_beta1_pow";
  paddle::dialect::IrMetaTensor meta_beta1_pow(&ir_tensor_beta1_pow);

  VLOG(4) << "Builder construction  dense_beta2_pow";
  paddle::dialect::IrTensor ir_tensor_beta2_pow(paddle::dialect::TransToPhiDataType(beta2_pow.dtype()),
                                                      beta2_pow.dims(),
                                                      beta2_pow.data_layout(),
                                                      beta2_pow.lod(),
                                                      beta2_pow.offset());
  VLOG(4) << "Builder construction  meta_beta2_pow";
  paddle::dialect::IrMetaTensor meta_beta2_pow(&ir_tensor_beta2_pow);

  paddle::dialect::IrMetaTensor meta_master_param;
  paddle::dialect::IrTensor ir_tensor_master_param;
  if (master_param_.impl() != nullptr) {
    paddle::dialect::DenseTensorType master_param = master_param_.type().dyn_cast<paddle::dialect::DenseTensorType>();
    VLOG(4) << "Builder construction  dense_master_param";
    ir_tensor_master_param = paddle::dialect::IrTensor(paddle::dialect::TransToPhiDataType(master_param.dtype()),
                                                        master_param.dims(),
                                                        master_param.data_layout(),
                                                        master_param.lod(),
                                                        master_param.offset());
    VLOG(4) << "Builder construction  meta_master_param";
    meta_master_param = paddle::dialect::IrMetaTensor(&ir_tensor_master_param);
  }


  paddle::dialect::IrMetaTensor meta_skip_update;
  paddle::dialect::IrTensor ir_tensor_skip_update;
  if (skip_update_.impl() != nullptr) {
    paddle::dialect::DenseTensorType skip_update = skip_update_.type().dyn_cast<paddle::dialect::DenseTensorType>();
    VLOG(4) << "Builder construction  dense_skip_update";
    ir_tensor_skip_update = paddle::dialect::IrTensor(paddle::dialect::TransToPhiDataType(skip_update.dtype()),
                                                        skip_update.dims(),
                                                        skip_update.data_layout(),
                                                        skip_update.lod(),
                                                        skip_update.offset());
    VLOG(4) << "Builder construction  meta_skip_update";
    meta_skip_update = paddle::dialect::IrMetaTensor(&ir_tensor_skip_update);
  }

  paddle::dialect::IrTensor dense_param_out;
  paddle::dialect::IrMetaTensor meta_param_out(&dense_param_out);
  paddle::dialect::IrTensor dense_moment1_out;
  paddle::dialect::IrMetaTensor meta_moment1_out(&dense_moment1_out);
  paddle::dialect::IrTensor dense_moment2_out;
  paddle::dialect::IrMetaTensor meta_moment2_out(&dense_moment2_out);
  paddle::dialect::IrTensor dense_beta1_pow_out;
  paddle::dialect::IrMetaTensor meta_beta1_pow_out(&dense_beta1_pow_out);
  paddle::dialect::IrTensor dense_beta2_pow_out;
  paddle::dialect::IrMetaTensor meta_beta2_pow_out(&dense_beta2_pow_out);
  paddle::dialect::IrTensor dense_master_param_outs;
  paddle::dialect::IrMetaTensor meta_master_param_outs(&dense_master_param_outs);

  phi::LambInferMeta(meta_param, meta_grad, meta_learning_rate, meta_moment1, meta_moment2, meta_beta1_pow, meta_beta2_pow, meta_master_param, meta_skip_update, weight_decay, beta1, beta2, epsilon, always_adapt, multi_precision, &meta_param_out, &meta_moment1_out, &meta_moment2_out, &meta_beta1_pow_out, &meta_beta2_pow_out, &meta_master_param_outs);

  std::vector<pir::Type> argument_outputs;
  pir::Type param_out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_param_out.dtype()), dense_param_out.dims(), dense_param_out.layout(), dense_param_out.lod(), dense_param_out.offset());
  argument_outputs.push_back(param_out_dense_tensor_type);

  pir::Type moment1_out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_moment1_out.dtype()), dense_moment1_out.dims(), dense_moment1_out.layout(), dense_moment1_out.lod(), dense_moment1_out.offset());
  argument_outputs.push_back(moment1_out_dense_tensor_type);

  pir::Type moment2_out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_moment2_out.dtype()), dense_moment2_out.dims(), dense_moment2_out.layout(), dense_moment2_out.lod(), dense_moment2_out.offset());
  argument_outputs.push_back(moment2_out_dense_tensor_type);

  if (beta1_pow_.impl() != nullptr) {
    pir::Type beta1_pow_out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_beta1_pow_out.dtype()), dense_beta1_pow_out.dims(), dense_beta1_pow_out.layout(), dense_beta1_pow_out.lod(), dense_beta1_pow_out.offset());
    argument_outputs.push_back(beta1_pow_out_dense_tensor_type);
  } else {
    pir::Type beta1_pow_out_type;
    argument_outputs.push_back(beta1_pow_out_type);
  }


  if (beta2_pow_.impl() != nullptr) {
    pir::Type beta2_pow_out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_beta2_pow_out.dtype()), dense_beta2_pow_out.dims(), dense_beta2_pow_out.layout(), dense_beta2_pow_out.lod(), dense_beta2_pow_out.offset());
    argument_outputs.push_back(beta2_pow_out_dense_tensor_type);
  } else {
    pir::Type beta2_pow_out_type;
    argument_outputs.push_back(beta2_pow_out_type);
  }


  if (master_param_.impl() != nullptr) {
    pir::Type master_param_outs_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_master_param_outs.dtype()), dense_master_param_outs.dims(), dense_master_param_outs.layout(), dense_master_param_outs.lod(), dense_master_param_outs.offset());
    argument_outputs.push_back(master_param_outs_dense_tensor_type);
  } else {
    pir::Type master_param_outs_type;
    argument_outputs.push_back(master_param_outs_type);
  }

  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void LambSr_Op::VerifySig() {
  VLOG(4) << "Start Verifying inputs, outputs and attributes for: LambSr_Op.";
  VLOG(4) << "Verifying inputs:";
  {
  auto input_size = num_operands();
  IR_ENFORCE(input_size == 9u,
                    "The size %d of inputs must be equal to 9.", input_size);
  IR_ENFORCE((*this)->operand_source(0).type().isa<paddle::dialect::DenseTensorType>(),
                  "Type validation failed for the 0th input, got %s.", (*this)->operand_source(0).type());
  IR_ENFORCE((*this)->operand_source(1).type().isa<paddle::dialect::SelectedRowsType>(),
                  "Type validation failed for the 1th input, got %s.", (*this)->operand_source(1).type());
  IR_ENFORCE((*this)->operand_source(2).type().isa<paddle::dialect::DenseTensorType>(),
                  "Type validation failed for the 2th input, got %s.", (*this)->operand_source(2).type());
  IR_ENFORCE((*this)->operand_source(3).type().isa<paddle::dialect::DenseTensorType>(),
                  "Type validation failed for the 3th input, got %s.", (*this)->operand_source(3).type());
  IR_ENFORCE((*this)->operand_source(4).type().isa<paddle::dialect::DenseTensorType>(),
                  "Type validation failed for the 4th input, got %s.", (*this)->operand_source(4).type());
  IR_ENFORCE((*this)->operand_source(5).type().isa<paddle::dialect::DenseTensorType>(),
                  "Type validation failed for the 5th input, got %s.", (*this)->operand_source(5).type());
  IR_ENFORCE((*this)->operand_source(6).type().isa<paddle::dialect::DenseTensorType>(),
                  "Type validation failed for the 6th input, got %s.", (*this)->operand_source(6).type());
  if (auto val = (*this)->operand(7)) {
    IR_ENFORCE(val.type().isa<paddle::dialect::DenseTensorType>(),
                   "Type validation failed for the 7th input, got %s.", (*this)->operand_source(7).type());
  }
  if (auto val = (*this)->operand(8)) {
    IR_ENFORCE(val.type().isa<paddle::dialect::DenseTensorType>(),
                   "Type validation failed for the 8th input, got %s.", (*this)->operand_source(8).type());
  }
  }
  VLOG(4) << "Verifying attributes:";
  {
  auto& attributes = this->attributes();
  IR_ENFORCE(attributes.count("weight_decay")>0,
                 "weight_decay does not exist.");
  IR_ENFORCE(attributes.at("weight_decay").isa<pir::FloatAttribute>(),
                 "Type of attribute: weight_decay is not pir::FloatAttribute.");

  IR_ENFORCE(attributes.count("beta1")>0,
                 "beta1 does not exist.");
  IR_ENFORCE(attributes.at("beta1").isa<pir::FloatAttribute>(),
                 "Type of attribute: beta1 is not pir::FloatAttribute.");

  IR_ENFORCE(attributes.count("beta2")>0,
                 "beta2 does not exist.");
  IR_ENFORCE(attributes.at("beta2").isa<pir::FloatAttribute>(),
                 "Type of attribute: beta2 is not pir::FloatAttribute.");

  IR_ENFORCE(attributes.count("epsilon")>0,
                 "epsilon does not exist.");
  IR_ENFORCE(attributes.at("epsilon").isa<pir::FloatAttribute>(),
                 "Type of attribute: epsilon is not pir::FloatAttribute.");

  IR_ENFORCE(attributes.count("always_adapt")>0,
                 "always_adapt does not exist.");
  IR_ENFORCE(attributes.at("always_adapt").isa<pir::BoolAttribute>(),
                 "Type of attribute: always_adapt is not pir::BoolAttribute.");

  IR_ENFORCE(attributes.count("multi_precision")>0,
                 "multi_precision does not exist.");
  IR_ENFORCE(attributes.at("multi_precision").isa<pir::BoolAttribute>(),
                 "Type of attribute: multi_precision is not pir::BoolAttribute.");

  }
  VLOG(4) << "Verifying outputs:";
  {
  auto output_size = num_results();
  IR_ENFORCE(output_size == 6u,
                    "The size %d of outputs must be equal to 6.", output_size);
  IR_ENFORCE((*this)->result(0).type().isa<paddle::dialect::DenseTensorType>(),
                 "Type validation failed for the 0th output.");
  IR_ENFORCE((*this)->result(1).type().isa<paddle::dialect::DenseTensorType>(),
                 "Type validation failed for the 1th output.");
  IR_ENFORCE((*this)->result(2).type().isa<paddle::dialect::DenseTensorType>(),
                 "Type validation failed for the 2th output.");
  if (auto output_3_type = (*this)->result(3).type()) {
    IR_ENFORCE(output_3_type.isa<paddle::dialect::DenseTensorType>(),
                   "Type validation failed for the 3th output.");
  }
  if (auto output_4_type = (*this)->result(4).type()) {
    IR_ENFORCE(output_4_type.isa<paddle::dialect::DenseTensorType>(),
                   "Type validation failed for the 4th output.");
  }
  if (auto output_5_type = (*this)->result(5).type()) {
    IR_ENFORCE(output_5_type.isa<paddle::dialect::DenseTensorType>(),
                   "Type validation failed for the 5th output.");
  }
  }
  VLOG(4) << "End Verifying for: LambSr_Op.";
}

void LambSr_Op::InferMeta( phi::InferMetaContext *infer_meta ) {
  auto fn = PD_INFER_META(phi::LambInferMeta);
  fn(infer_meta);
}

phi::DataType LambSr_Op::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: LambSr_Op";
  


  return expected_kernel_dtype;
}

const char *LayerNormOp::attributes_name[2] = { "epsilon", "begin_norm_axis" };

OpInfoTuple LayerNormOp::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("x", "paddle::dialect::DenseTensorType", false, false, false, true), paddle::dialect::OpInputInfo("scale", "paddle::dialect::DenseTensorType", true, false, false, true), paddle::dialect::OpInputInfo("bias", "paddle::dialect::DenseTensorType", true, false, false, true) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = { paddle::dialect::OpAttributeInfo("epsilon", "pir::FloatAttribute", ""), paddle::dialect::OpAttributeInfo("begin_norm_axis", "pir::Int32Attribute", "") };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("out", "paddle::dialect::DenseTensorType", false, false), paddle::dialect::OpOutputInfo("mean", "paddle::dialect::DenseTensorType", true, true), paddle::dialect::OpOutputInfo("variance", "paddle::dialect::DenseTensorType", true, true) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("LayerNormInferMeta", {"x", "scale", "bias", "epsilon", "begin_norm_axis"}, "layer_norm", {"x", "scale", "bias", "epsilon", "begin_norm_axis"}, {"x"}, {}, {}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "layer_norm");
}

void LayerNormOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, pir::Value scale_, pir::Value bias_, float epsilon, int begin_norm_axis) {
  VLOG(4) << "Start build LayerNormOp";



  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_, scale_, bias_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_epsilon = pir::FloatAttribute::get(pir::IrContext::Instance(), epsilon);
  argument.AddAttribute("epsilon", attr_epsilon);
  pir::Attribute attr_begin_norm_axis = pir::Int32Attribute::get(pir::IrContext::Instance(), begin_norm_axis);
  argument.AddAttribute("begin_norm_axis", attr_begin_norm_axis);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);

  paddle::dialect::IrMetaTensor meta_scale;
  paddle::dialect::IrTensor ir_tensor_scale;
  if (scale_.impl() != nullptr) {
    paddle::dialect::DenseTensorType scale = scale_.type().dyn_cast<paddle::dialect::DenseTensorType>();
    VLOG(4) << "Builder construction  dense_scale";
    ir_tensor_scale = paddle::dialect::IrTensor(paddle::dialect::TransToPhiDataType(scale.dtype()),
                                                        scale.dims(),
                                                        scale.data_layout(),
                                                        scale.lod(),
                                                        scale.offset());
    VLOG(4) << "Builder construction  meta_scale";
    meta_scale = paddle::dialect::IrMetaTensor(&ir_tensor_scale);
  }


  paddle::dialect::IrMetaTensor meta_bias;
  paddle::dialect::IrTensor ir_tensor_bias;
  if (bias_.impl() != nullptr) {
    paddle::dialect::DenseTensorType bias = bias_.type().dyn_cast<paddle::dialect::DenseTensorType>();
    VLOG(4) << "Builder construction  dense_bias";
    ir_tensor_bias = paddle::dialect::IrTensor(paddle::dialect::TransToPhiDataType(bias.dtype()),
                                                        bias.dims(),
                                                        bias.data_layout(),
                                                        bias.lod(),
                                                        bias.offset());
    VLOG(4) << "Builder construction  meta_bias";
    meta_bias = paddle::dialect::IrMetaTensor(&ir_tensor_bias);
  }

  paddle::dialect::IrTensor dense_out;
  paddle::dialect::IrMetaTensor meta_out(&dense_out);
  paddle::dialect::IrTensor dense_mean;
  paddle::dialect::IrMetaTensor meta_mean(&dense_mean);
  paddle::dialect::IrTensor dense_variance;
  paddle::dialect::IrMetaTensor meta_variance(&dense_variance);

  phi::LayerNormInferMeta(meta_x, meta_scale, meta_bias, epsilon, begin_norm_axis, &meta_out, &meta_mean, &meta_variance);

  std::vector<pir::Type> argument_outputs;
  pir::Type out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_out.dtype()), dense_out.dims(), dense_out.layout(), dense_out.lod(), dense_out.offset());
  argument_outputs.push_back(out_dense_tensor_type);

  pir::Type mean_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_mean.dtype()), dense_mean.dims(), dense_mean.layout(), dense_mean.lod(), dense_mean.offset());
  argument_outputs.push_back(mean_dense_tensor_type);

  pir::Type variance_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_variance.dtype()), dense_variance.dims(), dense_variance.layout(), dense_variance.lod(), dense_variance.offset());
  argument_outputs.push_back(variance_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void LayerNormOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, pir::Value scale_, pir::Value bias_, pir::AttributeMap attributes) {
  VLOG(4) << "Start build LayerNormOp";


  IR_ENFORCE(
      attributes.find("epsilon") != attributes.end(),
          "'epsilon' Attribute is expected for LayerNormOp. ");
  float epsilon = attributes.at("epsilon").dyn_cast<pir::FloatAttribute>().data();

  IR_ENFORCE(
      attributes.find("begin_norm_axis") != attributes.end(),
          "'begin_norm_axis' Attribute is expected for LayerNormOp. ");
  int begin_norm_axis = attributes.at("begin_norm_axis").dyn_cast<pir::Int32Attribute>().data();


  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_, scale_, bias_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_epsilon = pir::FloatAttribute::get(pir::IrContext::Instance(), epsilon);
  argument.AddAttribute("epsilon", attr_epsilon);
  pir::Attribute attr_begin_norm_axis = pir::Int32Attribute::get(pir::IrContext::Instance(), begin_norm_axis);
  argument.AddAttribute("begin_norm_axis", attr_begin_norm_axis);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);

  paddle::dialect::IrMetaTensor meta_scale;
  paddle::dialect::IrTensor ir_tensor_scale;
  if (scale_.impl() != nullptr) {
    paddle::dialect::DenseTensorType scale = scale_.type().dyn_cast<paddle::dialect::DenseTensorType>();
    VLOG(4) << "Builder construction  dense_scale";
    ir_tensor_scale = paddle::dialect::IrTensor(paddle::dialect::TransToPhiDataType(scale.dtype()),
                                                        scale.dims(),
                                                        scale.data_layout(),
                                                        scale.lod(),
                                                        scale.offset());
    VLOG(4) << "Builder construction  meta_scale";
    meta_scale = paddle::dialect::IrMetaTensor(&ir_tensor_scale);
  }


  paddle::dialect::IrMetaTensor meta_bias;
  paddle::dialect::IrTensor ir_tensor_bias;
  if (bias_.impl() != nullptr) {
    paddle::dialect::DenseTensorType bias = bias_.type().dyn_cast<paddle::dialect::DenseTensorType>();
    VLOG(4) << "Builder construction  dense_bias";
    ir_tensor_bias = paddle::dialect::IrTensor(paddle::dialect::TransToPhiDataType(bias.dtype()),
                                                        bias.dims(),
                                                        bias.data_layout(),
                                                        bias.lod(),
                                                        bias.offset());
    VLOG(4) << "Builder construction  meta_bias";
    meta_bias = paddle::dialect::IrMetaTensor(&ir_tensor_bias);
  }

  paddle::dialect::IrTensor dense_out;
  paddle::dialect::IrMetaTensor meta_out(&dense_out);
  paddle::dialect::IrTensor dense_mean;
  paddle::dialect::IrMetaTensor meta_mean(&dense_mean);
  paddle::dialect::IrTensor dense_variance;
  paddle::dialect::IrMetaTensor meta_variance(&dense_variance);

  phi::LayerNormInferMeta(meta_x, meta_scale, meta_bias, epsilon, begin_norm_axis, &meta_out, &meta_mean, &meta_variance);

  std::vector<pir::Type> argument_outputs;
  pir::Type out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_out.dtype()), dense_out.dims(), dense_out.layout(), dense_out.lod(), dense_out.offset());
  argument_outputs.push_back(out_dense_tensor_type);

  pir::Type mean_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_mean.dtype()), dense_mean.dims(), dense_mean.layout(), dense_mean.lod(), dense_mean.offset());
  argument_outputs.push_back(mean_dense_tensor_type);

  pir::Type variance_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_variance.dtype()), dense_variance.dims(), dense_variance.layout(), dense_variance.lod(), dense_variance.offset());
  argument_outputs.push_back(variance_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void LayerNormOp::VerifySig() {
  VLOG(4) << "Start Verifying inputs, outputs and attributes for: LayerNormOp.";
  VLOG(4) << "Verifying inputs:";
  {
  auto input_size = num_operands();
  IR_ENFORCE(input_size == 3u,
                    "The size %d of inputs must be equal to 3.", input_size);
  IR_ENFORCE((*this)->operand_source(0).type().isa<paddle::dialect::DenseTensorType>(),
                  "Type validation failed for the 0th input, got %s.", (*this)->operand_source(0).type());
  if (auto val = (*this)->operand(1)) {
    IR_ENFORCE(val.type().isa<paddle::dialect::DenseTensorType>(),
                   "Type validation failed for the 1th input, got %s.", (*this)->operand_source(1).type());
  }
  if (auto val = (*this)->operand(2)) {
    IR_ENFORCE(val.type().isa<paddle::dialect::DenseTensorType>(),
                   "Type validation failed for the 2th input, got %s.", (*this)->operand_source(2).type());
  }
  }
  VLOG(4) << "Verifying attributes:";
  {
  auto& attributes = this->attributes();
  IR_ENFORCE(attributes.count("epsilon")>0,
                 "epsilon does not exist.");
  IR_ENFORCE(attributes.at("epsilon").isa<pir::FloatAttribute>(),
                 "Type of attribute: epsilon is not pir::FloatAttribute.");

  IR_ENFORCE(attributes.count("begin_norm_axis")>0,
                 "begin_norm_axis does not exist.");
  IR_ENFORCE(attributes.at("begin_norm_axis").isa<pir::Int32Attribute>(),
                 "Type of attribute: begin_norm_axis is not pir::Int32Attribute.");

  }
  VLOG(4) << "Verifying outputs:";
  {
  auto output_size = num_results();
  IR_ENFORCE(output_size == 3u,
                    "The size %d of outputs must be equal to 3.", output_size);
  IR_ENFORCE((*this)->result(0).type().isa<paddle::dialect::DenseTensorType>(),
                 "Type validation failed for the 0th output.");
  if (auto output_1_type = (*this)->result(1).type()) {
    IR_ENFORCE(output_1_type.isa<paddle::dialect::DenseTensorType>(),
                   "Type validation failed for the 1th output.");
  }
  if (auto output_2_type = (*this)->result(2).type()) {
    IR_ENFORCE(output_2_type.isa<paddle::dialect::DenseTensorType>(),
                   "Type validation failed for the 2th output.");
  }
  }
  VLOG(4) << "End Verifying for: LayerNormOp.";
}

void LayerNormOp::InferMeta( phi::InferMetaContext *infer_meta ) {
  auto fn = PD_INFER_META(phi::LayerNormInferMeta);
  fn(infer_meta);
}

phi::DataType LayerNormOp::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: LayerNormOp";
  


  return expected_kernel_dtype;
}

const char *LeakyReluOp::attributes_name[1] = { "negative_slope" };

OpInfoTuple LeakyReluOp::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("x", "paddle::dialect::DenseTensorType", false, false, false, true) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = { paddle::dialect::OpAttributeInfo("negative_slope", "pir::FloatAttribute", "") };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("out", "paddle::dialect::DenseTensorType", false, false) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("UnchangedInferMeta", {"x"}, "leaky_relu", {"x", "negative_slope"}, {}, {}, {}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "leaky_relu");
}

void LeakyReluOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, float negative_slope) {
  VLOG(4) << "Start build LeakyReluOp";



  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_negative_slope = pir::FloatAttribute::get(pir::IrContext::Instance(), negative_slope);
  argument.AddAttribute("negative_slope", attr_negative_slope);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);
  paddle::dialect::IrTensor dense_out;
  paddle::dialect::IrMetaTensor meta_out(&dense_out);

  phi::UnchangedInferMeta(meta_x, &meta_out);

  std::vector<pir::Type> argument_outputs;
  pir::Type out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_out.dtype()), dense_out.dims(), dense_out.layout(), dense_out.lod(), dense_out.offset());
  argument_outputs.push_back(out_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void LeakyReluOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, pir::AttributeMap attributes) {
  VLOG(4) << "Start build LeakyReluOp";


  IR_ENFORCE(
      attributes.find("negative_slope") != attributes.end(),
          "'negative_slope' Attribute is expected for LeakyReluOp. ");
  float negative_slope = attributes.at("negative_slope").dyn_cast<pir::FloatAttribute>().data();


  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_negative_slope = pir::FloatAttribute::get(pir::IrContext::Instance(), negative_slope);
  argument.AddAttribute("negative_slope", attr_negative_slope);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);
  paddle::dialect::IrTensor dense_out;
  paddle::dialect::IrMetaTensor meta_out(&dense_out);

  phi::UnchangedInferMeta(meta_x, &meta_out);

  std::vector<pir::Type> argument_outputs;
  pir::Type out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_out.dtype()), dense_out.dims(), dense_out.layout(), dense_out.lod(), dense_out.offset());
  argument_outputs.push_back(out_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void LeakyReluOp::VerifySig() {
  VLOG(4) << "Start Verifying inputs, outputs and attributes for: LeakyReluOp.";
  VLOG(4) << "Verifying inputs:";
  {
  auto input_size = num_operands();
  IR_ENFORCE(input_size == 1u,
                    "The size %d of inputs must be equal to 1.", input_size);
  IR_ENFORCE((*this)->operand_source(0).type().isa<paddle::dialect::DenseTensorType>(),
                  "Type validation failed for the 0th input, got %s.", (*this)->operand_source(0).type());
  }
  VLOG(4) << "Verifying attributes:";
  {
  auto& attributes = this->attributes();
  IR_ENFORCE(attributes.count("negative_slope")>0,
                 "negative_slope does not exist.");
  IR_ENFORCE(attributes.at("negative_slope").isa<pir::FloatAttribute>(),
                 "Type of attribute: negative_slope is not pir::FloatAttribute.");

  }
  VLOG(4) << "Verifying outputs:";
  {
  auto output_size = num_results();
  IR_ENFORCE(output_size == 1u,
                    "The size %d of outputs must be equal to 1.", output_size);
  IR_ENFORCE((*this)->result(0).type().isa<paddle::dialect::DenseTensorType>(),
                 "Type validation failed for the 0th output.");
  }
  VLOG(4) << "End Verifying for: LeakyReluOp.";
}

void LeakyReluOp::InferMeta( phi::InferMetaContext *infer_meta ) {
  auto fn = PD_INFER_META(phi::UnchangedInferMeta);
  fn(infer_meta);
}

phi::DataType LeakyReluOp::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: LeakyReluOp";
  


  return expected_kernel_dtype;
}

const char *LeakyRelu_Op::attributes_name[1] = { "negative_slope" };

OpInfoTuple LeakyRelu_Op::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("x", "paddle::dialect::DenseTensorType", false, false, false, true) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = { paddle::dialect::OpAttributeInfo("negative_slope", "pir::FloatAttribute", "") };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("out", "paddle::dialect::DenseTensorType", false, false) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("UnchangedInferMeta", {"x"}, "leaky_relu", {"x", "negative_slope"}, {}, {}, {{"out", "x"}}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "leaky_relu");
}

void LeakyRelu_Op::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, float negative_slope) {
  VLOG(4) << "Start build LeakyRelu_Op";



  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_negative_slope = pir::FloatAttribute::get(pir::IrContext::Instance(), negative_slope);
  argument.AddAttribute("negative_slope", attr_negative_slope);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);
  paddle::dialect::IrTensor dense_out;
  paddle::dialect::IrMetaTensor meta_out(&dense_out);

  phi::UnchangedInferMeta(meta_x, &meta_out);

  std::vector<pir::Type> argument_outputs;
  pir::Type out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_out.dtype()), dense_out.dims(), dense_out.layout(), dense_out.lod(), dense_out.offset());
  argument_outputs.push_back(out_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void LeakyRelu_Op::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, pir::AttributeMap attributes) {
  VLOG(4) << "Start build LeakyRelu_Op";


  IR_ENFORCE(
      attributes.find("negative_slope") != attributes.end(),
          "'negative_slope' Attribute is expected for LeakyRelu_Op. ");
  float negative_slope = attributes.at("negative_slope").dyn_cast<pir::FloatAttribute>().data();


  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_negative_slope = pir::FloatAttribute::get(pir::IrContext::Instance(), negative_slope);
  argument.AddAttribute("negative_slope", attr_negative_slope);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);
  paddle::dialect::IrTensor dense_out;
  paddle::dialect::IrMetaTensor meta_out(&dense_out);

  phi::UnchangedInferMeta(meta_x, &meta_out);

  std::vector<pir::Type> argument_outputs;
  pir::Type out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_out.dtype()), dense_out.dims(), dense_out.layout(), dense_out.lod(), dense_out.offset());
  argument_outputs.push_back(out_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void LeakyRelu_Op::VerifySig() {
  VLOG(4) << "Start Verifying inputs, outputs and attributes for: LeakyRelu_Op.";
  VLOG(4) << "Verifying inputs:";
  {
  auto input_size = num_operands();
  IR_ENFORCE(input_size == 1u,
                    "The size %d of inputs must be equal to 1.", input_size);
  IR_ENFORCE((*this)->operand_source(0).type().isa<paddle::dialect::DenseTensorType>(),
                  "Type validation failed for the 0th input, got %s.", (*this)->operand_source(0).type());
  }
  VLOG(4) << "Verifying attributes:";
  {
  auto& attributes = this->attributes();
  IR_ENFORCE(attributes.count("negative_slope")>0,
                 "negative_slope does not exist.");
  IR_ENFORCE(attributes.at("negative_slope").isa<pir::FloatAttribute>(),
                 "Type of attribute: negative_slope is not pir::FloatAttribute.");

  }
  VLOG(4) << "Verifying outputs:";
  {
  auto output_size = num_results();
  IR_ENFORCE(output_size == 1u,
                    "The size %d of outputs must be equal to 1.", output_size);
  IR_ENFORCE((*this)->result(0).type().isa<paddle::dialect::DenseTensorType>(),
                 "Type validation failed for the 0th output.");
  }
  VLOG(4) << "End Verifying for: LeakyRelu_Op.";
}

void LeakyRelu_Op::InferMeta( phi::InferMetaContext *infer_meta ) {
  auto fn = PD_INFER_META(phi::UnchangedInferMeta);
  fn(infer_meta);
}

phi::DataType LeakyRelu_Op::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: LeakyRelu_Op";
  


  return expected_kernel_dtype;
}

OpInfoTuple LerpOp::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("x", "paddle::dialect::DenseTensorType", false, false, false, true), paddle::dialect::OpInputInfo("y", "paddle::dialect::DenseTensorType", false, false, false, true), paddle::dialect::OpInputInfo("weight", "paddle::dialect::DenseTensorType", false, false, false, false) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = {  };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("out", "paddle::dialect::DenseTensorType", false, false) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("LerpInferMeta", {"x", "y", "weight"}, "lerp", {"x", "y", "weight"}, {}, {}, {}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "lerp");
}

void LerpOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, pir::Value y_, pir::Value weight_) {
  VLOG(4) << "Start build LerpOp";



  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_, y_, weight_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;
  paddle::dialect::DenseTensorType y = y_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)y;
  paddle::dialect::DenseTensorType weight = weight_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)weight;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);

  VLOG(4) << "Builder construction  dense_y";
  paddle::dialect::IrTensor ir_tensor_y(paddle::dialect::TransToPhiDataType(y.dtype()),
                                                      y.dims(),
                                                      y.data_layout(),
                                                      y.lod(),
                                                      y.offset());
  VLOG(4) << "Builder construction  meta_y";
  paddle::dialect::IrMetaTensor meta_y(&ir_tensor_y);

  VLOG(4) << "Builder construction  dense_weight";
  paddle::dialect::IrTensor ir_tensor_weight(paddle::dialect::TransToPhiDataType(weight.dtype()),
                                                      weight.dims(),
                                                      weight.data_layout(),
                                                      weight.lod(),
                                                      weight.offset());
  VLOG(4) << "Builder construction  meta_weight";
  paddle::dialect::IrMetaTensor meta_weight(&ir_tensor_weight);
  paddle::dialect::IrTensor dense_out;
  paddle::dialect::IrMetaTensor meta_out(&dense_out);

  phi::LerpInferMeta(meta_x, meta_y, meta_weight, &meta_out);

  std::vector<pir::Type> argument_outputs;
  pir::Type out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_out.dtype()), dense_out.dims(), dense_out.layout(), dense_out.lod(), dense_out.offset());
  argument_outputs.push_back(out_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void LerpOp::VerifySig() {
  VLOG(4) << "Start Verifying inputs, outputs and attributes for: LerpOp.";
  VLOG(4) << "Verifying inputs:";
  {
  auto input_size = num_operands();
  IR_ENFORCE(input_size == 3u,
                    "The size %d of inputs must be equal to 3.", input_size);
  IR_ENFORCE((*this)->operand_source(0).type().isa<paddle::dialect::DenseTensorType>(),
                  "Type validation failed for the 0th input, got %s.", (*this)->operand_source(0).type());
  IR_ENFORCE((*this)->operand_source(1).type().isa<paddle::dialect::DenseTensorType>(),
                  "Type validation failed for the 1th input, got %s.", (*this)->operand_source(1).type());
  IR_ENFORCE((*this)->operand_source(2).type().isa<paddle::dialect::DenseTensorType>(),
                  "Type validation failed for the 2th input, got %s.", (*this)->operand_source(2).type());
  }
  VLOG(4) << "Verifying attributes:";
  {
  // Attributes num is 0, not need to check attributes type.
  }
  VLOG(4) << "Verifying outputs:";
  {
  auto output_size = num_results();
  IR_ENFORCE(output_size == 1u,
                    "The size %d of outputs must be equal to 1.", output_size);
  IR_ENFORCE((*this)->result(0).type().isa<paddle::dialect::DenseTensorType>(),
                 "Type validation failed for the 0th output.");
  }
  VLOG(4) << "End Verifying for: LerpOp.";
}

void LerpOp::InferMeta( phi::InferMetaContext *infer_meta ) {
  auto fn = PD_INFER_META(phi::LerpInferMeta);
  fn(infer_meta);
}

phi::DataType LerpOp::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: LerpOp";
  


  return expected_kernel_dtype;
}

OpInfoTuple Lerp_Op::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("x", "paddle::dialect::DenseTensorType", false, false, false, true), paddle::dialect::OpInputInfo("y", "paddle::dialect::DenseTensorType", false, false, false, true), paddle::dialect::OpInputInfo("weight", "paddle::dialect::DenseTensorType", false, false, false, false) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = {  };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("out", "paddle::dialect::DenseTensorType", false, false) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("LerpInferMeta", {"x", "y", "weight"}, "lerp", {"x", "y", "weight"}, {}, {}, {{"out", "x"}}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "lerp");
}

void Lerp_Op::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, pir::Value y_, pir::Value weight_) {
  VLOG(4) << "Start build Lerp_Op";



  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_, y_, weight_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;
  paddle::dialect::DenseTensorType y = y_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)y;
  paddle::dialect::DenseTensorType weight = weight_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)weight;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);

  VLOG(4) << "Builder construction  dense_y";
  paddle::dialect::IrTensor ir_tensor_y(paddle::dialect::TransToPhiDataType(y.dtype()),
                                                      y.dims(),
                                                      y.data_layout(),
                                                      y.lod(),
                                                      y.offset());
  VLOG(4) << "Builder construction  meta_y";
  paddle::dialect::IrMetaTensor meta_y(&ir_tensor_y);

  VLOG(4) << "Builder construction  dense_weight";
  paddle::dialect::IrTensor ir_tensor_weight(paddle::dialect::TransToPhiDataType(weight.dtype()),
                                                      weight.dims(),
                                                      weight.data_layout(),
                                                      weight.lod(),
                                                      weight.offset());
  VLOG(4) << "Builder construction  meta_weight";
  paddle::dialect::IrMetaTensor meta_weight(&ir_tensor_weight);
  paddle::dialect::IrTensor dense_out;
  paddle::dialect::IrMetaTensor meta_out(&dense_out);

  phi::LerpInferMeta(meta_x, meta_y, meta_weight, &meta_out);

  std::vector<pir::Type> argument_outputs;
  pir::Type out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_out.dtype()), dense_out.dims(), dense_out.layout(), dense_out.lod(), dense_out.offset());
  argument_outputs.push_back(out_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void Lerp_Op::VerifySig() {
  VLOG(4) << "Start Verifying inputs, outputs and attributes for: Lerp_Op.";
  VLOG(4) << "Verifying inputs:";
  {
  auto input_size = num_operands();
  IR_ENFORCE(input_size == 3u,
                    "The size %d of inputs must be equal to 3.", input_size);
  IR_ENFORCE((*this)->operand_source(0).type().isa<paddle::dialect::DenseTensorType>(),
                  "Type validation failed for the 0th input, got %s.", (*this)->operand_source(0).type());
  IR_ENFORCE((*this)->operand_source(1).type().isa<paddle::dialect::DenseTensorType>(),
                  "Type validation failed for the 1th input, got %s.", (*this)->operand_source(1).type());
  IR_ENFORCE((*this)->operand_source(2).type().isa<paddle::dialect::DenseTensorType>(),
                  "Type validation failed for the 2th input, got %s.", (*this)->operand_source(2).type());
  }
  VLOG(4) << "Verifying attributes:";
  {
  // Attributes num is 0, not need to check attributes type.
  }
  VLOG(4) << "Verifying outputs:";
  {
  auto output_size = num_results();
  IR_ENFORCE(output_size == 1u,
                    "The size %d of outputs must be equal to 1.", output_size);
  IR_ENFORCE((*this)->result(0).type().isa<paddle::dialect::DenseTensorType>(),
                 "Type validation failed for the 0th output.");
  }
  VLOG(4) << "End Verifying for: Lerp_Op.";
}

void Lerp_Op::InferMeta( phi::InferMetaContext *infer_meta ) {
  auto fn = PD_INFER_META(phi::LerpInferMeta);
  fn(infer_meta);
}

phi::DataType Lerp_Op::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: Lerp_Op";
  


  return expected_kernel_dtype;
}

OpInfoTuple LgammaOp::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("x", "paddle::dialect::DenseTensorType", false, false, false, true) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = {  };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("out", "paddle::dialect::DenseTensorType", false, false) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("UnchangedInferMeta", {"x"}, "lgamma", {"x"}, {}, {}, {}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "lgamma");
}

void LgammaOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_) {
  VLOG(4) << "Start build LgammaOp";



  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);
  paddle::dialect::IrTensor dense_out;
  paddle::dialect::IrMetaTensor meta_out(&dense_out);

  phi::UnchangedInferMeta(meta_x, &meta_out);

  std::vector<pir::Type> argument_outputs;
  pir::Type out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_out.dtype()), dense_out.dims(), dense_out.layout(), dense_out.lod(), dense_out.offset());
  argument_outputs.push_back(out_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void LgammaOp::VerifySig() {
  VLOG(4) << "Start Verifying inputs, outputs and attributes for: LgammaOp.";
  VLOG(4) << "Verifying inputs:";
  {
  auto input_size = num_operands();
  IR_ENFORCE(input_size == 1u,
                    "The size %d of inputs must be equal to 1.", input_size);
  IR_ENFORCE((*this)->operand_source(0).type().isa<paddle::dialect::DenseTensorType>(),
                  "Type validation failed for the 0th input, got %s.", (*this)->operand_source(0).type());
  }
  VLOG(4) << "Verifying attributes:";
  {
  // Attributes num is 0, not need to check attributes type.
  }
  VLOG(4) << "Verifying outputs:";
  {
  auto output_size = num_results();
  IR_ENFORCE(output_size == 1u,
                    "The size %d of outputs must be equal to 1.", output_size);
  IR_ENFORCE((*this)->result(0).type().isa<paddle::dialect::DenseTensorType>(),
                 "Type validation failed for the 0th output.");
  }
  VLOG(4) << "End Verifying for: LgammaOp.";
}

void LgammaOp::InferMeta( phi::InferMetaContext *infer_meta ) {
  auto fn = PD_INFER_META(phi::UnchangedInferMeta);
  fn(infer_meta);
}

phi::DataType LgammaOp::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: LgammaOp";
  


  return expected_kernel_dtype;
}

OpInfoTuple Lgamma_Op::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("x", "paddle::dialect::DenseTensorType", false, false, false, true) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = {  };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("out", "paddle::dialect::DenseTensorType", false, false) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("UnchangedInferMeta", {"x"}, "lgamma", {"x"}, {}, {}, {{"out", "x"}}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "lgamma");
}

void Lgamma_Op::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_) {
  VLOG(4) << "Start build Lgamma_Op";



  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);
  paddle::dialect::IrTensor dense_out;
  paddle::dialect::IrMetaTensor meta_out(&dense_out);

  phi::UnchangedInferMeta(meta_x, &meta_out);

  std::vector<pir::Type> argument_outputs;
  pir::Type out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_out.dtype()), dense_out.dims(), dense_out.layout(), dense_out.lod(), dense_out.offset());
  argument_outputs.push_back(out_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void Lgamma_Op::VerifySig() {
  VLOG(4) << "Start Verifying inputs, outputs and attributes for: Lgamma_Op.";
  VLOG(4) << "Verifying inputs:";
  {
  auto input_size = num_operands();
  IR_ENFORCE(input_size == 1u,
                    "The size %d of inputs must be equal to 1.", input_size);
  IR_ENFORCE((*this)->operand_source(0).type().isa<paddle::dialect::DenseTensorType>(),
                  "Type validation failed for the 0th input, got %s.", (*this)->operand_source(0).type());
  }
  VLOG(4) << "Verifying attributes:";
  {
  // Attributes num is 0, not need to check attributes type.
  }
  VLOG(4) << "Verifying outputs:";
  {
  auto output_size = num_results();
  IR_ENFORCE(output_size == 1u,
                    "The size %d of outputs must be equal to 1.", output_size);
  IR_ENFORCE((*this)->result(0).type().isa<paddle::dialect::DenseTensorType>(),
                 "Type validation failed for the 0th output.");
  }
  VLOG(4) << "End Verifying for: Lgamma_Op.";
}

void Lgamma_Op::InferMeta( phi::InferMetaContext *infer_meta ) {
  auto fn = PD_INFER_META(phi::UnchangedInferMeta);
  fn(infer_meta);
}

phi::DataType Lgamma_Op::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: Lgamma_Op";
  


  return expected_kernel_dtype;
}

const char *LinearInterpOp::attributes_name[8] = { "data_layout", "out_d", "out_h", "out_w", "scale", "interp_method", "align_corners", "align_mode" };

OpInfoTuple LinearInterpOp::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("x", "paddle::dialect::DenseTensorType", false, false, false, true), paddle::dialect::OpInputInfo("out_size", "paddle::dialect::DenseTensorType", true, false, false, false), paddle::dialect::OpInputInfo("size_tensor", "pir::VectorType<paddle::dialect::DenseTensorType>", true, false, false, false), paddle::dialect::OpInputInfo("scale_tensor", "paddle::dialect::DenseTensorType", true, false, false, false) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = { paddle::dialect::OpAttributeInfo("data_layout", "pir::StrAttribute", ""), paddle::dialect::OpAttributeInfo("out_d", "pir::Int32Attribute", ""), paddle::dialect::OpAttributeInfo("out_h", "pir::Int32Attribute", ""), paddle::dialect::OpAttributeInfo("out_w", "pir::Int32Attribute", ""), paddle::dialect::OpAttributeInfo("scale", "pir::ArrayAttribute<pir::FloatAttribute>", ""), paddle::dialect::OpAttributeInfo("interp_method", "pir::StrAttribute", ""), paddle::dialect::OpAttributeInfo("align_corners", "pir::BoolAttribute", ""), paddle::dialect::OpAttributeInfo("align_mode", "pir::Int32Attribute", "") };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("output", "paddle::dialect::DenseTensorType", false, false) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("InterpolateInferMeta", {"x", "out_size", "size_tensor", "scale_tensor", "data_layout", "out_d", "out_h", "out_w", "scale", "interp_method", "align_corners", "align_mode"}, "linear_interp", {"x", "out_size", "size_tensor", "scale_tensor", "data_layout", "out_d", "out_h", "out_w", "scale", "interp_method", "align_corners", "align_mode"}, {"x"}, {}, {}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "linear_interp");
}

void LinearInterpOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, pir::Value out_size_, pir::Value size_tensor_, pir::Value scale_tensor_, const std::string& data_layout, int out_d, int out_h, int out_w, const std::vector<float>& scale, const std::string& interp_method, bool align_corners, int align_mode) {
  VLOG(4) << "Start build LinearInterpOp";



  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_, out_size_, size_tensor_, scale_tensor_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_data_layout = pir::StrAttribute::get(pir::IrContext::Instance(), data_layout);
  argument.AddAttribute("data_layout", attr_data_layout);
  pir::Attribute attr_out_d = pir::Int32Attribute::get(pir::IrContext::Instance(), out_d);
  argument.AddAttribute("out_d", attr_out_d);
  pir::Attribute attr_out_h = pir::Int32Attribute::get(pir::IrContext::Instance(), out_h);
  argument.AddAttribute("out_h", attr_out_h);
  pir::Attribute attr_out_w = pir::Int32Attribute::get(pir::IrContext::Instance(), out_w);
  argument.AddAttribute("out_w", attr_out_w);
  std::vector<pir::Attribute> vec_scale;
  for (size_t i = 0; i < static_cast<size_t>(scale.size()); i++) {
      pir::Attribute attr_scale = pir::FloatAttribute::get(pir::IrContext::Instance(), scale[i]);

    vec_scale.push_back(attr_scale);
  }
  pir::Attribute attr_scale = pir::ArrayAttribute::get(pir::IrContext::Instance(), vec_scale);
  argument.AddAttribute("scale", attr_scale);
  pir::Attribute attr_interp_method = pir::StrAttribute::get(pir::IrContext::Instance(), interp_method);
  argument.AddAttribute("interp_method", attr_interp_method);
  pir::Attribute attr_align_corners = pir::BoolAttribute::get(pir::IrContext::Instance(), align_corners);
  argument.AddAttribute("align_corners", attr_align_corners);
  pir::Attribute attr_align_mode = pir::Int32Attribute::get(pir::IrContext::Instance(), align_mode);
  argument.AddAttribute("align_mode", attr_align_mode);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);

  paddle::dialect::IrMetaTensor meta_out_size;
  paddle::dialect::IrTensor ir_tensor_out_size;
  if (out_size_.impl() != nullptr) {
    paddle::dialect::DenseTensorType out_size = out_size_.type().dyn_cast<paddle::dialect::DenseTensorType>();
    VLOG(4) << "Builder construction  dense_out_size";
    ir_tensor_out_size = paddle::dialect::IrTensor(paddle::dialect::TransToPhiDataType(out_size.dtype()),
                                                        out_size.dims(),
                                                        out_size.data_layout(),
                                                        out_size.lod(),
                                                        out_size.offset());
    VLOG(4) << "Builder construction  meta_out_size";
    meta_out_size = paddle::dialect::IrMetaTensor(&ir_tensor_out_size);
  }

  std::vector<paddle::dialect::IrTensor> vec_ir_tensor_size_tensor;
  if (size_tensor_.impl() != nullptr) {
    pir::VectorType size_tensor = size_tensor_.type().dyn_cast<pir::VectorType>();
    for (size_t i=0; i < static_cast<size_t>(size_tensor.size()); i++) {
        vec_ir_tensor_size_tensor.push_back(paddle::dialect::IrTensor(paddle::dialect::TransToPhiDataType(size_tensor[i].dyn_cast<paddle::dialect::DenseTensorType>().dtype()),
                                                                        size_tensor[i].dyn_cast<paddle::dialect::DenseTensorType>().dims(),
                                                                        size_tensor[i].dyn_cast<paddle::dialect::DenseTensorType>().data_layout(),
                                                                        size_tensor[i].dyn_cast<paddle::dialect::DenseTensorType>().lod(),
                                                                        size_tensor[i].dyn_cast<paddle::dialect::DenseTensorType>().offset()));
    }
  }

  std::vector<paddle::dialect::IrMetaTensor> vec_meta_size_tensor;
  for (size_t i=0; i < vec_ir_tensor_size_tensor.size(); i++) {
    vec_meta_size_tensor.push_back(paddle::dialect::IrMetaTensor(&vec_ir_tensor_size_tensor[i]));
  }

  std::vector<const phi::MetaTensor*> meta_size_tensor;
  for (size_t i=0; i < static_cast<size_t>(vec_meta_size_tensor.size()); i++) {
    meta_size_tensor.push_back(&vec_meta_size_tensor[i]);
  }


  paddle::dialect::IrMetaTensor meta_scale_tensor;
  paddle::dialect::IrTensor ir_tensor_scale_tensor;
  if (scale_tensor_.impl() != nullptr) {
    paddle::dialect::DenseTensorType scale_tensor = scale_tensor_.type().dyn_cast<paddle::dialect::DenseTensorType>();
    VLOG(4) << "Builder construction  dense_scale_tensor";
    ir_tensor_scale_tensor = paddle::dialect::IrTensor(paddle::dialect::TransToPhiDataType(scale_tensor.dtype()),
                                                        scale_tensor.dims(),
                                                        scale_tensor.data_layout(),
                                                        scale_tensor.lod(),
                                                        scale_tensor.offset());
    VLOG(4) << "Builder construction  meta_scale_tensor";
    meta_scale_tensor = paddle::dialect::IrMetaTensor(&ir_tensor_scale_tensor);
  }

  paddle::dialect::IrTensor dense_output;
  paddle::dialect::IrMetaTensor meta_output(&dense_output);

  phi::InterpolateInferMeta(meta_x, meta_out_size, meta_size_tensor, meta_scale_tensor, data_layout, out_d, out_h, out_w, scale, interp_method, align_corners, align_mode, &meta_output, phi::MetaConfig(false, false));

  std::vector<pir::Type> argument_outputs;
  pir::Type output_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_output.dtype()), dense_output.dims(), dense_output.layout(), dense_output.lod(), dense_output.offset());
  argument_outputs.push_back(output_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void LinearInterpOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, pir::Value out_size_, pir::Value size_tensor_, pir::Value scale_tensor_, pir::AttributeMap attributes) {
  VLOG(4) << "Start build LinearInterpOp";


  IR_ENFORCE(
      attributes.find("data_layout") != attributes.end(),
          "'data_layout' Attribute is expected for LinearInterpOp. ");
  std::string data_layout = attributes.at("data_layout").dyn_cast<pir::StrAttribute>().AsString();

  IR_ENFORCE(
      attributes.find("out_d") != attributes.end(),
          "'out_d' Attribute is expected for LinearInterpOp. ");
  int out_d = attributes.at("out_d").dyn_cast<pir::Int32Attribute>().data();

  IR_ENFORCE(
      attributes.find("out_h") != attributes.end(),
          "'out_h' Attribute is expected for LinearInterpOp. ");
  int out_h = attributes.at("out_h").dyn_cast<pir::Int32Attribute>().data();

  IR_ENFORCE(
      attributes.find("out_w") != attributes.end(),
          "'out_w' Attribute is expected for LinearInterpOp. ");
  int out_w = attributes.at("out_w").dyn_cast<pir::Int32Attribute>().data();

  IR_ENFORCE(
      attributes.find("scale") != attributes.end(),
          "'scale' Attribute is expected for LinearInterpOp. ");
  std::vector<float> scale;
  for (size_t i = 0; i < attributes.at("scale").dyn_cast<pir::ArrayAttribute>().size(); i++) {
    scale.push_back(attributes.at("scale").dyn_cast<pir::ArrayAttribute>().at(i).dyn_cast<pir::FloatAttribute>().data());
  }

  IR_ENFORCE(
      attributes.find("interp_method") != attributes.end(),
          "'interp_method' Attribute is expected for LinearInterpOp. ");
  std::string interp_method = attributes.at("interp_method").dyn_cast<pir::StrAttribute>().AsString();

  IR_ENFORCE(
      attributes.find("align_corners") != attributes.end(),
          "'align_corners' Attribute is expected for LinearInterpOp. ");
  bool align_corners = attributes.at("align_corners").dyn_cast<pir::BoolAttribute>().data();

  IR_ENFORCE(
      attributes.find("align_mode") != attributes.end(),
          "'align_mode' Attribute is expected for LinearInterpOp. ");
  int align_mode = attributes.at("align_mode").dyn_cast<pir::Int32Attribute>().data();


  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_, out_size_, size_tensor_, scale_tensor_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_data_layout = pir::StrAttribute::get(pir::IrContext::Instance(), data_layout);
  argument.AddAttribute("data_layout", attr_data_layout);
  pir::Attribute attr_out_d = pir::Int32Attribute::get(pir::IrContext::Instance(), out_d);
  argument.AddAttribute("out_d", attr_out_d);
  pir::Attribute attr_out_h = pir::Int32Attribute::get(pir::IrContext::Instance(), out_h);
  argument.AddAttribute("out_h", attr_out_h);
  pir::Attribute attr_out_w = pir::Int32Attribute::get(pir::IrContext::Instance(), out_w);
  argument.AddAttribute("out_w", attr_out_w);
  std::vector<pir::Attribute> vec_scale;
  for (size_t i = 0; i < static_cast<size_t>(scale.size()); i++) {
      pir::Attribute attr_scale = pir::FloatAttribute::get(pir::IrContext::Instance(), scale[i]);

    vec_scale.push_back(attr_scale);
  }
  pir::Attribute attr_scale = pir::ArrayAttribute::get(pir::IrContext::Instance(), vec_scale);
  argument.AddAttribute("scale", attr_scale);
  pir::Attribute attr_interp_method = pir::StrAttribute::get(pir::IrContext::Instance(), interp_method);
  argument.AddAttribute("interp_method", attr_interp_method);
  pir::Attribute attr_align_corners = pir::BoolAttribute::get(pir::IrContext::Instance(), align_corners);
  argument.AddAttribute("align_corners", attr_align_corners);
  pir::Attribute attr_align_mode = pir::Int32Attribute::get(pir::IrContext::Instance(), align_mode);
  argument.AddAttribute("align_mode", attr_align_mode);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);

  paddle::dialect::IrMetaTensor meta_out_size;
  paddle::dialect::IrTensor ir_tensor_out_size;
  if (out_size_.impl() != nullptr) {
    paddle::dialect::DenseTensorType out_size = out_size_.type().dyn_cast<paddle::dialect::DenseTensorType>();
    VLOG(4) << "Builder construction  dense_out_size";
    ir_tensor_out_size = paddle::dialect::IrTensor(paddle::dialect::TransToPhiDataType(out_size.dtype()),
                                                        out_size.dims(),
                                                        out_size.data_layout(),
                                                        out_size.lod(),
                                                        out_size.offset());
    VLOG(4) << "Builder construction  meta_out_size";
    meta_out_size = paddle::dialect::IrMetaTensor(&ir_tensor_out_size);
  }

  std::vector<paddle::dialect::IrTensor> vec_ir_tensor_size_tensor;
  if (size_tensor_.impl() != nullptr) {
    pir::VectorType size_tensor = size_tensor_.type().dyn_cast<pir::VectorType>();
    for (size_t i=0; i < static_cast<size_t>(size_tensor.size()); i++) {
        vec_ir_tensor_size_tensor.push_back(paddle::dialect::IrTensor(paddle::dialect::TransToPhiDataType(size_tensor[i].dyn_cast<paddle::dialect::DenseTensorType>().dtype()),
                                                                        size_tensor[i].dyn_cast<paddle::dialect::DenseTensorType>().dims(),
                                                                        size_tensor[i].dyn_cast<paddle::dialect::DenseTensorType>().data_layout(),
                                                                        size_tensor[i].dyn_cast<paddle::dialect::DenseTensorType>().lod(),
                                                                        size_tensor[i].dyn_cast<paddle::dialect::DenseTensorType>().offset()));
    }
  }

  std::vector<paddle::dialect::IrMetaTensor> vec_meta_size_tensor;
  for (size_t i=0; i < vec_ir_tensor_size_tensor.size(); i++) {
    vec_meta_size_tensor.push_back(paddle::dialect::IrMetaTensor(&vec_ir_tensor_size_tensor[i]));
  }

  std::vector<const phi::MetaTensor*> meta_size_tensor;
  for (size_t i=0; i < static_cast<size_t>(vec_meta_size_tensor.size()); i++) {
    meta_size_tensor.push_back(&vec_meta_size_tensor[i]);
  }


  paddle::dialect::IrMetaTensor meta_scale_tensor;
  paddle::dialect::IrTensor ir_tensor_scale_tensor;
  if (scale_tensor_.impl() != nullptr) {
    paddle::dialect::DenseTensorType scale_tensor = scale_tensor_.type().dyn_cast<paddle::dialect::DenseTensorType>();
    VLOG(4) << "Builder construction  dense_scale_tensor";
    ir_tensor_scale_tensor = paddle::dialect::IrTensor(paddle::dialect::TransToPhiDataType(scale_tensor.dtype()),
                                                        scale_tensor.dims(),
                                                        scale_tensor.data_layout(),
                                                        scale_tensor.lod(),
                                                        scale_tensor.offset());
    VLOG(4) << "Builder construction  meta_scale_tensor";
    meta_scale_tensor = paddle::dialect::IrMetaTensor(&ir_tensor_scale_tensor);
  }

  paddle::dialect::IrTensor dense_output;
  paddle::dialect::IrMetaTensor meta_output(&dense_output);

  phi::InterpolateInferMeta(meta_x, meta_out_size, meta_size_tensor, meta_scale_tensor, data_layout, out_d, out_h, out_w, scale, interp_method, align_corners, align_mode, &meta_output, phi::MetaConfig(false, false));

  std::vector<pir::Type> argument_outputs;
  pir::Type output_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_output.dtype()), dense_output.dims(), dense_output.layout(), dense_output.lod(), dense_output.offset());
  argument_outputs.push_back(output_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void LinearInterpOp::VerifySig() {
  VLOG(4) << "Start Verifying inputs, outputs and attributes for: LinearInterpOp.";
  VLOG(4) << "Verifying inputs:";
  {
  auto input_size = num_operands();
  IR_ENFORCE(input_size == 4u,
                    "The size %d of inputs must be equal to 4.", input_size);
  IR_ENFORCE((*this)->operand_source(0).type().isa<paddle::dialect::DenseTensorType>(),
                  "Type validation failed for the 0th input, got %s.", (*this)->operand_source(0).type());
  if (auto val = (*this)->operand(1)) {
    IR_ENFORCE(val.type().isa<paddle::dialect::DenseTensorType>(),
                   "Type validation failed for the 1th input, got %s.", (*this)->operand_source(1).type());
  }
  if (auto val =  (*this)->operand(2)) {
    if (auto vec_type = val.type().dyn_cast<pir::VectorType>()) {
      for (size_t i = 0; i < vec_type.size(); i++) {
        IR_ENFORCE(vec_type[i].isa<paddle::dialect::DenseTensorType>(),
                          "Type validation failed for the 2th input, got %s.", (*this)->operand_source(2).type());
      }
    }
    else {
      IR_ENFORCE(val.type().isa<paddle::dialect::DenseTensorType>(),
                        "Type validation failed for the 2th input, got %s.", (*this)->operand_source(2).type());
    }
  }
  if (auto val = (*this)->operand(3)) {
    IR_ENFORCE(val.type().isa<paddle::dialect::DenseTensorType>(),
                   "Type validation failed for the 3th input, got %s.", (*this)->operand_source(3).type());
  }
  }
  VLOG(4) << "Verifying attributes:";
  {
  auto& attributes = this->attributes();
  IR_ENFORCE(attributes.count("data_layout")>0,
                 "data_layout does not exist.");
  IR_ENFORCE(attributes.at("data_layout").isa<pir::StrAttribute>(),
                 "Type of attribute: data_layout is not pir::StrAttribute.");

  IR_ENFORCE(attributes.count("out_d")>0,
                 "out_d does not exist.");
  IR_ENFORCE(attributes.at("out_d").isa<pir::Int32Attribute>(),
                 "Type of attribute: out_d is not pir::Int32Attribute.");

  IR_ENFORCE(attributes.count("out_h")>0,
                 "out_h does not exist.");
  IR_ENFORCE(attributes.at("out_h").isa<pir::Int32Attribute>(),
                 "Type of attribute: out_h is not pir::Int32Attribute.");

  IR_ENFORCE(attributes.count("out_w")>0,
                 "out_w does not exist.");
  IR_ENFORCE(attributes.at("out_w").isa<pir::Int32Attribute>(),
                 "Type of attribute: out_w is not pir::Int32Attribute.");

  IR_ENFORCE(attributes.count("scale")>0,
                 "scale does not exist.");
  IR_ENFORCE(attributes.at("scale").isa<pir::ArrayAttribute>(),
                 "Type of attribute: scale is not pir::ArrayAttribute.");
  for (size_t i = 0; i < attributes.at("scale").dyn_cast<pir::ArrayAttribute>().size(); i++) {
    IR_ENFORCE(attributes.at("scale").dyn_cast<pir::ArrayAttribute>().at(i).isa<pir::FloatAttribute>(),
                   "Type of attribute: scale is not right.");
  }
  IR_ENFORCE(attributes.count("interp_method")>0,
                 "interp_method does not exist.");
  IR_ENFORCE(attributes.at("interp_method").isa<pir::StrAttribute>(),
                 "Type of attribute: interp_method is not pir::StrAttribute.");

  IR_ENFORCE(attributes.count("align_corners")>0,
                 "align_corners does not exist.");
  IR_ENFORCE(attributes.at("align_corners").isa<pir::BoolAttribute>(),
                 "Type of attribute: align_corners is not pir::BoolAttribute.");

  IR_ENFORCE(attributes.count("align_mode")>0,
                 "align_mode does not exist.");
  IR_ENFORCE(attributes.at("align_mode").isa<pir::Int32Attribute>(),
                 "Type of attribute: align_mode is not pir::Int32Attribute.");

  }
  VLOG(4) << "Verifying outputs:";
  {
  auto output_size = num_results();
  IR_ENFORCE(output_size == 1u,
                    "The size %d of outputs must be equal to 1.", output_size);
  IR_ENFORCE((*this)->result(0).type().isa<paddle::dialect::DenseTensorType>(),
                 "Type validation failed for the 0th output.");
  }
  VLOG(4) << "End Verifying for: LinearInterpOp.";
}

void LinearInterpOp::InferMeta( phi::InferMetaContext *infer_meta ) {
  auto fn = PD_INFER_META(phi::InterpolateInferMeta);
  fn(infer_meta);
}

phi::DataType LinearInterpOp::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: LinearInterpOp";
  

  // deal skip data transform
  if (var_name == "out_size" || var_name == "size_tensor" || var_name == "scale_tensor"){
    return expected_kernel_dtype;
  }


  return expected_kernel_dtype;
}

const char *LlmInt8LinearOp::attributes_name[1] = { "threshold" };

OpInfoTuple LlmInt8LinearOp::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("x", "paddle::dialect::DenseTensorType", false, false, false, false), paddle::dialect::OpInputInfo("weight", "paddle::dialect::DenseTensorType", false, false, false, false), paddle::dialect::OpInputInfo("bias", "paddle::dialect::DenseTensorType", true, false, false, false), paddle::dialect::OpInputInfo("weight_scale", "paddle::dialect::DenseTensorType", false, false, false, false) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = { paddle::dialect::OpAttributeInfo("threshold", "pir::FloatAttribute", "") };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("out", "paddle::dialect::DenseTensorType", false, false) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("LLMInt8LinearInferMeta", {"x", "weight", "bias", "weight_scale", "threshold"}, "llm_int8_linear", {"x", "weight", "bias", "weight_scale", "threshold"}, {"x"}, {}, {}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "llm_int8_linear");
}

void LlmInt8LinearOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, pir::Value weight_, pir::Value bias_, pir::Value weight_scale_, float threshold) {
  VLOG(4) << "Start build LlmInt8LinearOp";



  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_, weight_, bias_, weight_scale_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_threshold = pir::FloatAttribute::get(pir::IrContext::Instance(), threshold);
  argument.AddAttribute("threshold", attr_threshold);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;
  paddle::dialect::DenseTensorType weight = weight_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)weight;
  paddle::dialect::DenseTensorType weight_scale = weight_scale_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)weight_scale;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);

  VLOG(4) << "Builder construction  dense_weight";
  paddle::dialect::IrTensor ir_tensor_weight(paddle::dialect::TransToPhiDataType(weight.dtype()),
                                                      weight.dims(),
                                                      weight.data_layout(),
                                                      weight.lod(),
                                                      weight.offset());
  VLOG(4) << "Builder construction  meta_weight";
  paddle::dialect::IrMetaTensor meta_weight(&ir_tensor_weight);

  paddle::dialect::IrMetaTensor meta_bias;
  paddle::dialect::IrTensor ir_tensor_bias;
  if (bias_.impl() != nullptr) {
    paddle::dialect::DenseTensorType bias = bias_.type().dyn_cast<paddle::dialect::DenseTensorType>();
    VLOG(4) << "Builder construction  dense_bias";
    ir_tensor_bias = paddle::dialect::IrTensor(paddle::dialect::TransToPhiDataType(bias.dtype()),
                                                        bias.dims(),
                                                        bias.data_layout(),
                                                        bias.lod(),
                                                        bias.offset());
    VLOG(4) << "Builder construction  meta_bias";
    meta_bias = paddle::dialect::IrMetaTensor(&ir_tensor_bias);
  }


  VLOG(4) << "Builder construction  dense_weight_scale";
  paddle::dialect::IrTensor ir_tensor_weight_scale(paddle::dialect::TransToPhiDataType(weight_scale.dtype()),
                                                      weight_scale.dims(),
                                                      weight_scale.data_layout(),
                                                      weight_scale.lod(),
                                                      weight_scale.offset());
  VLOG(4) << "Builder construction  meta_weight_scale";
  paddle::dialect::IrMetaTensor meta_weight_scale(&ir_tensor_weight_scale);
  paddle::dialect::IrTensor dense_out;
  paddle::dialect::IrMetaTensor meta_out(&dense_out);

  phi::LLMInt8LinearInferMeta(meta_x, meta_weight, meta_bias, meta_weight_scale, threshold, &meta_out);

  std::vector<pir::Type> argument_outputs;
  pir::Type out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_out.dtype()), dense_out.dims(), dense_out.layout(), dense_out.lod(), dense_out.offset());
  argument_outputs.push_back(out_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void LlmInt8LinearOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, pir::Value weight_, pir::Value bias_, pir::Value weight_scale_, pir::AttributeMap attributes) {
  VLOG(4) << "Start build LlmInt8LinearOp";


  IR_ENFORCE(
      attributes.find("threshold") != attributes.end(),
          "'threshold' Attribute is expected for LlmInt8LinearOp. ");
  float threshold = attributes.at("threshold").dyn_cast<pir::FloatAttribute>().data();


  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_, weight_, bias_, weight_scale_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_threshold = pir::FloatAttribute::get(pir::IrContext::Instance(), threshold);
  argument.AddAttribute("threshold", attr_threshold);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;
  paddle::dialect::DenseTensorType weight = weight_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)weight;
  paddle::dialect::DenseTensorType weight_scale = weight_scale_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)weight_scale;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);

  VLOG(4) << "Builder construction  dense_weight";
  paddle::dialect::IrTensor ir_tensor_weight(paddle::dialect::TransToPhiDataType(weight.dtype()),
                                                      weight.dims(),
                                                      weight.data_layout(),
                                                      weight.lod(),
                                                      weight.offset());
  VLOG(4) << "Builder construction  meta_weight";
  paddle::dialect::IrMetaTensor meta_weight(&ir_tensor_weight);

  paddle::dialect::IrMetaTensor meta_bias;
  paddle::dialect::IrTensor ir_tensor_bias;
  if (bias_.impl() != nullptr) {
    paddle::dialect::DenseTensorType bias = bias_.type().dyn_cast<paddle::dialect::DenseTensorType>();
    VLOG(4) << "Builder construction  dense_bias";
    ir_tensor_bias = paddle::dialect::IrTensor(paddle::dialect::TransToPhiDataType(bias.dtype()),
                                                        bias.dims(),
                                                        bias.data_layout(),
                                                        bias.lod(),
                                                        bias.offset());
    VLOG(4) << "Builder construction  meta_bias";
    meta_bias = paddle::dialect::IrMetaTensor(&ir_tensor_bias);
  }


  VLOG(4) << "Builder construction  dense_weight_scale";
  paddle::dialect::IrTensor ir_tensor_weight_scale(paddle::dialect::TransToPhiDataType(weight_scale.dtype()),
                                                      weight_scale.dims(),
                                                      weight_scale.data_layout(),
                                                      weight_scale.lod(),
                                                      weight_scale.offset());
  VLOG(4) << "Builder construction  meta_weight_scale";
  paddle::dialect::IrMetaTensor meta_weight_scale(&ir_tensor_weight_scale);
  paddle::dialect::IrTensor dense_out;
  paddle::dialect::IrMetaTensor meta_out(&dense_out);

  phi::LLMInt8LinearInferMeta(meta_x, meta_weight, meta_bias, meta_weight_scale, threshold, &meta_out);

  std::vector<pir::Type> argument_outputs;
  pir::Type out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_out.dtype()), dense_out.dims(), dense_out.layout(), dense_out.lod(), dense_out.offset());
  argument_outputs.push_back(out_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void LlmInt8LinearOp::VerifySig() {
  VLOG(4) << "Start Verifying inputs, outputs and attributes for: LlmInt8LinearOp.";
  VLOG(4) << "Verifying inputs:";
  {
  auto input_size = num_operands();
  IR_ENFORCE(input_size == 4u,
                    "The size %d of inputs must be equal to 4.", input_size);
  IR_ENFORCE((*this)->operand_source(0).type().isa<paddle::dialect::DenseTensorType>(),
                  "Type validation failed for the 0th input, got %s.", (*this)->operand_source(0).type());
  IR_ENFORCE((*this)->operand_source(1).type().isa<paddle::dialect::DenseTensorType>(),
                  "Type validation failed for the 1th input, got %s.", (*this)->operand_source(1).type());
  if (auto val = (*this)->operand(2)) {
    IR_ENFORCE(val.type().isa<paddle::dialect::DenseTensorType>(),
                   "Type validation failed for the 2th input, got %s.", (*this)->operand_source(2).type());
  }
  IR_ENFORCE((*this)->operand_source(3).type().isa<paddle::dialect::DenseTensorType>(),
                  "Type validation failed for the 3th input, got %s.", (*this)->operand_source(3).type());
  }
  VLOG(4) << "Verifying attributes:";
  {
  auto& attributes = this->attributes();
  IR_ENFORCE(attributes.count("threshold")>0,
                 "threshold does not exist.");
  IR_ENFORCE(attributes.at("threshold").isa<pir::FloatAttribute>(),
                 "Type of attribute: threshold is not pir::FloatAttribute.");

  }
  VLOG(4) << "Verifying outputs:";
  {
  auto output_size = num_results();
  IR_ENFORCE(output_size == 1u,
                    "The size %d of outputs must be equal to 1.", output_size);
  IR_ENFORCE((*this)->result(0).type().isa<paddle::dialect::DenseTensorType>(),
                 "Type validation failed for the 0th output.");
  }
  VLOG(4) << "End Verifying for: LlmInt8LinearOp.";
}

void LlmInt8LinearOp::InferMeta( phi::InferMetaContext *infer_meta ) {
  auto fn = PD_INFER_META(phi::LLMInt8LinearInferMeta);
  fn(infer_meta);
}

phi::DataType LlmInt8LinearOp::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: LlmInt8LinearOp";
  


  return expected_kernel_dtype;
}

OpInfoTuple LogOp::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("x", "paddle::dialect::DenseTensorType", false, false, false, true) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = {  };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("out", "paddle::dialect::DenseTensorType", false, false) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("UnchangedInferMeta", {"x"}, "log", {"x"}, {}, {}, {}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "log");
}

void LogOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_) {
  VLOG(4) << "Start build LogOp";



  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);
  paddle::dialect::IrTensor dense_out;
  paddle::dialect::IrMetaTensor meta_out(&dense_out);

  phi::UnchangedInferMeta(meta_x, &meta_out);

  std::vector<pir::Type> argument_outputs;
  pir::Type out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_out.dtype()), dense_out.dims(), dense_out.layout(), dense_out.lod(), dense_out.offset());
  argument_outputs.push_back(out_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void LogOp::VerifySig() {
  VLOG(4) << "Start Verifying inputs, outputs and attributes for: LogOp.";
  VLOG(4) << "Verifying inputs:";
  {
  auto input_size = num_operands();
  IR_ENFORCE(input_size == 1u,
                    "The size %d of inputs must be equal to 1.", input_size);
  IR_ENFORCE((*this)->operand_source(0).type().isa<paddle::dialect::DenseTensorType>(),
                  "Type validation failed for the 0th input, got %s.", (*this)->operand_source(0).type());
  }
  VLOG(4) << "Verifying attributes:";
  {
  // Attributes num is 0, not need to check attributes type.
  }
  VLOG(4) << "Verifying outputs:";
  {
  auto output_size = num_results();
  IR_ENFORCE(output_size == 1u,
                    "The size %d of outputs must be equal to 1.", output_size);
  IR_ENFORCE((*this)->result(0).type().isa<paddle::dialect::DenseTensorType>(),
                 "Type validation failed for the 0th output.");
  }
  VLOG(4) << "End Verifying for: LogOp.";
}

void LogOp::InferMeta( phi::InferMetaContext *infer_meta ) {
  auto fn = PD_INFER_META(phi::UnchangedInferMeta);
  fn(infer_meta);
}

phi::DataType LogOp::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: LogOp";
  


  return expected_kernel_dtype;
}

OpInfoTuple Log_Op::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("x", "paddle::dialect::DenseTensorType", false, false, false, true) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = {  };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("out", "paddle::dialect::DenseTensorType", false, false) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("UnchangedInferMeta", {"x"}, "log", {"x"}, {}, {}, {{"out", "x"}}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "log");
}

void Log_Op::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_) {
  VLOG(4) << "Start build Log_Op";



  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);
  paddle::dialect::IrTensor dense_out;
  paddle::dialect::IrMetaTensor meta_out(&dense_out);

  phi::UnchangedInferMeta(meta_x, &meta_out);

  std::vector<pir::Type> argument_outputs;
  pir::Type out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_out.dtype()), dense_out.dims(), dense_out.layout(), dense_out.lod(), dense_out.offset());
  argument_outputs.push_back(out_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void Log_Op::VerifySig() {
  VLOG(4) << "Start Verifying inputs, outputs and attributes for: Log_Op.";
  VLOG(4) << "Verifying inputs:";
  {
  auto input_size = num_operands();
  IR_ENFORCE(input_size == 1u,
                    "The size %d of inputs must be equal to 1.", input_size);
  IR_ENFORCE((*this)->operand_source(0).type().isa<paddle::dialect::DenseTensorType>(),
                  "Type validation failed for the 0th input, got %s.", (*this)->operand_source(0).type());
  }
  VLOG(4) << "Verifying attributes:";
  {
  // Attributes num is 0, not need to check attributes type.
  }
  VLOG(4) << "Verifying outputs:";
  {
  auto output_size = num_results();
  IR_ENFORCE(output_size == 1u,
                    "The size %d of outputs must be equal to 1.", output_size);
  IR_ENFORCE((*this)->result(0).type().isa<paddle::dialect::DenseTensorType>(),
                 "Type validation failed for the 0th output.");
  }
  VLOG(4) << "End Verifying for: Log_Op.";
}

void Log_Op::InferMeta( phi::InferMetaContext *infer_meta ) {
  auto fn = PD_INFER_META(phi::UnchangedInferMeta);
  fn(infer_meta);
}

phi::DataType Log_Op::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: Log_Op";
  


  return expected_kernel_dtype;
}

OpInfoTuple Log10Op::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("x", "paddle::dialect::DenseTensorType", false, false, false, true) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = {  };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("out", "paddle::dialect::DenseTensorType", false, false) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("UnchangedInferMeta", {"x"}, "log10", {"x"}, {}, {}, {}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "log10");
}

void Log10Op::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_) {
  VLOG(4) << "Start build Log10Op";



  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);
  paddle::dialect::IrTensor dense_out;
  paddle::dialect::IrMetaTensor meta_out(&dense_out);

  phi::UnchangedInferMeta(meta_x, &meta_out);

  std::vector<pir::Type> argument_outputs;
  pir::Type out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_out.dtype()), dense_out.dims(), dense_out.layout(), dense_out.lod(), dense_out.offset());
  argument_outputs.push_back(out_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void Log10Op::VerifySig() {
  VLOG(4) << "Start Verifying inputs, outputs and attributes for: Log10Op.";
  VLOG(4) << "Verifying inputs:";
  {
  auto input_size = num_operands();
  IR_ENFORCE(input_size == 1u,
                    "The size %d of inputs must be equal to 1.", input_size);
  IR_ENFORCE((*this)->operand_source(0).type().isa<paddle::dialect::DenseTensorType>(),
                  "Type validation failed for the 0th input, got %s.", (*this)->operand_source(0).type());
  }
  VLOG(4) << "Verifying attributes:";
  {
  // Attributes num is 0, not need to check attributes type.
  }
  VLOG(4) << "Verifying outputs:";
  {
  auto output_size = num_results();
  IR_ENFORCE(output_size == 1u,
                    "The size %d of outputs must be equal to 1.", output_size);
  IR_ENFORCE((*this)->result(0).type().isa<paddle::dialect::DenseTensorType>(),
                 "Type validation failed for the 0th output.");
  }
  VLOG(4) << "End Verifying for: Log10Op.";
}

void Log10Op::InferMeta( phi::InferMetaContext *infer_meta ) {
  auto fn = PD_INFER_META(phi::UnchangedInferMeta);
  fn(infer_meta);
}

phi::DataType Log10Op::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: Log10Op";
  


  return expected_kernel_dtype;
}

OpInfoTuple Log10_Op::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("x", "paddle::dialect::DenseTensorType", false, false, false, true) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = {  };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("out", "paddle::dialect::DenseTensorType", false, false) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("UnchangedInferMeta", {"x"}, "log10", {"x"}, {}, {}, {{"out", "x"}}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "log10");
}

void Log10_Op::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_) {
  VLOG(4) << "Start build Log10_Op";



  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);
  paddle::dialect::IrTensor dense_out;
  paddle::dialect::IrMetaTensor meta_out(&dense_out);

  phi::UnchangedInferMeta(meta_x, &meta_out);

  std::vector<pir::Type> argument_outputs;
  pir::Type out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_out.dtype()), dense_out.dims(), dense_out.layout(), dense_out.lod(), dense_out.offset());
  argument_outputs.push_back(out_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void Log10_Op::VerifySig() {
  VLOG(4) << "Start Verifying inputs, outputs and attributes for: Log10_Op.";
  VLOG(4) << "Verifying inputs:";
  {
  auto input_size = num_operands();
  IR_ENFORCE(input_size == 1u,
                    "The size %d of inputs must be equal to 1.", input_size);
  IR_ENFORCE((*this)->operand_source(0).type().isa<paddle::dialect::DenseTensorType>(),
                  "Type validation failed for the 0th input, got %s.", (*this)->operand_source(0).type());
  }
  VLOG(4) << "Verifying attributes:";
  {
  // Attributes num is 0, not need to check attributes type.
  }
  VLOG(4) << "Verifying outputs:";
  {
  auto output_size = num_results();
  IR_ENFORCE(output_size == 1u,
                    "The size %d of outputs must be equal to 1.", output_size);
  IR_ENFORCE((*this)->result(0).type().isa<paddle::dialect::DenseTensorType>(),
                 "Type validation failed for the 0th output.");
  }
  VLOG(4) << "End Verifying for: Log10_Op.";
}

void Log10_Op::InferMeta( phi::InferMetaContext *infer_meta ) {
  auto fn = PD_INFER_META(phi::UnchangedInferMeta);
  fn(infer_meta);
}

phi::DataType Log10_Op::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: Log10_Op";
  


  return expected_kernel_dtype;
}

OpInfoTuple Log1pOp::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("x", "paddle::dialect::DenseTensorType", false, false, false, true) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = {  };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("out", "paddle::dialect::DenseTensorType", false, false) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("UnchangedInferMeta", {"x"}, "log1p", {"x"}, {}, {}, {}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "log1p");
}

void Log1pOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_) {
  VLOG(4) << "Start build Log1pOp";



  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);
  paddle::dialect::IrTensor dense_out;
  paddle::dialect::IrMetaTensor meta_out(&dense_out);

  phi::UnchangedInferMeta(meta_x, &meta_out);

  std::vector<pir::Type> argument_outputs;
  pir::Type out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_out.dtype()), dense_out.dims(), dense_out.layout(), dense_out.lod(), dense_out.offset());
  argument_outputs.push_back(out_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void Log1pOp::VerifySig() {
  VLOG(4) << "Start Verifying inputs, outputs and attributes for: Log1pOp.";
  VLOG(4) << "Verifying inputs:";
  {
  auto input_size = num_operands();
  IR_ENFORCE(input_size == 1u,
                    "The size %d of inputs must be equal to 1.", input_size);
  IR_ENFORCE((*this)->operand_source(0).type().isa<paddle::dialect::DenseTensorType>(),
                  "Type validation failed for the 0th input, got %s.", (*this)->operand_source(0).type());
  }
  VLOG(4) << "Verifying attributes:";
  {
  // Attributes num is 0, not need to check attributes type.
  }
  VLOG(4) << "Verifying outputs:";
  {
  auto output_size = num_results();
  IR_ENFORCE(output_size == 1u,
                    "The size %d of outputs must be equal to 1.", output_size);
  IR_ENFORCE((*this)->result(0).type().isa<paddle::dialect::DenseTensorType>(),
                 "Type validation failed for the 0th output.");
  }
  VLOG(4) << "End Verifying for: Log1pOp.";
}

void Log1pOp::InferMeta( phi::InferMetaContext *infer_meta ) {
  auto fn = PD_INFER_META(phi::UnchangedInferMeta);
  fn(infer_meta);
}

phi::DataType Log1pOp::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: Log1pOp";
  


  return expected_kernel_dtype;
}

OpInfoTuple Log1p_Op::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("x", "paddle::dialect::DenseTensorType", false, false, false, true) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = {  };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("out", "paddle::dialect::DenseTensorType", false, false) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("UnchangedInferMeta", {"x"}, "log1p", {"x"}, {}, {}, {{"out", "x"}}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "log1p");
}

void Log1p_Op::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_) {
  VLOG(4) << "Start build Log1p_Op";



  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);
  paddle::dialect::IrTensor dense_out;
  paddle::dialect::IrMetaTensor meta_out(&dense_out);

  phi::UnchangedInferMeta(meta_x, &meta_out);

  std::vector<pir::Type> argument_outputs;
  pir::Type out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_out.dtype()), dense_out.dims(), dense_out.layout(), dense_out.lod(), dense_out.offset());
  argument_outputs.push_back(out_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void Log1p_Op::VerifySig() {
  VLOG(4) << "Start Verifying inputs, outputs and attributes for: Log1p_Op.";
  VLOG(4) << "Verifying inputs:";
  {
  auto input_size = num_operands();
  IR_ENFORCE(input_size == 1u,
                    "The size %d of inputs must be equal to 1.", input_size);
  IR_ENFORCE((*this)->operand_source(0).type().isa<paddle::dialect::DenseTensorType>(),
                  "Type validation failed for the 0th input, got %s.", (*this)->operand_source(0).type());
  }
  VLOG(4) << "Verifying attributes:";
  {
  // Attributes num is 0, not need to check attributes type.
  }
  VLOG(4) << "Verifying outputs:";
  {
  auto output_size = num_results();
  IR_ENFORCE(output_size == 1u,
                    "The size %d of outputs must be equal to 1.", output_size);
  IR_ENFORCE((*this)->result(0).type().isa<paddle::dialect::DenseTensorType>(),
                 "Type validation failed for the 0th output.");
  }
  VLOG(4) << "End Verifying for: Log1p_Op.";
}

void Log1p_Op::InferMeta( phi::InferMetaContext *infer_meta ) {
  auto fn = PD_INFER_META(phi::UnchangedInferMeta);
  fn(infer_meta);
}

phi::DataType Log1p_Op::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: Log1p_Op";
  


  return expected_kernel_dtype;
}

OpInfoTuple Log2Op::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("x", "paddle::dialect::DenseTensorType", false, false, false, true) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = {  };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("out", "paddle::dialect::DenseTensorType", false, false) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("UnchangedInferMeta", {"x"}, "log2", {"x"}, {}, {}, {}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "log2");
}

void Log2Op::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_) {
  VLOG(4) << "Start build Log2Op";



  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);
  paddle::dialect::IrTensor dense_out;
  paddle::dialect::IrMetaTensor meta_out(&dense_out);

  phi::UnchangedInferMeta(meta_x, &meta_out);

  std::vector<pir::Type> argument_outputs;
  pir::Type out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_out.dtype()), dense_out.dims(), dense_out.layout(), dense_out.lod(), dense_out.offset());
  argument_outputs.push_back(out_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void Log2Op::VerifySig() {
  VLOG(4) << "Start Verifying inputs, outputs and attributes for: Log2Op.";
  VLOG(4) << "Verifying inputs:";
  {
  auto input_size = num_operands();
  IR_ENFORCE(input_size == 1u,
                    "The size %d of inputs must be equal to 1.", input_size);
  IR_ENFORCE((*this)->operand_source(0).type().isa<paddle::dialect::DenseTensorType>(),
                  "Type validation failed for the 0th input, got %s.", (*this)->operand_source(0).type());
  }
  VLOG(4) << "Verifying attributes:";
  {
  // Attributes num is 0, not need to check attributes type.
  }
  VLOG(4) << "Verifying outputs:";
  {
  auto output_size = num_results();
  IR_ENFORCE(output_size == 1u,
                    "The size %d of outputs must be equal to 1.", output_size);
  IR_ENFORCE((*this)->result(0).type().isa<paddle::dialect::DenseTensorType>(),
                 "Type validation failed for the 0th output.");
  }
  VLOG(4) << "End Verifying for: Log2Op.";
}

void Log2Op::InferMeta( phi::InferMetaContext *infer_meta ) {
  auto fn = PD_INFER_META(phi::UnchangedInferMeta);
  fn(infer_meta);
}

phi::DataType Log2Op::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: Log2Op";
  


  return expected_kernel_dtype;
}

OpInfoTuple Log2_Op::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("x", "paddle::dialect::DenseTensorType", false, false, false, true) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = {  };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("out", "paddle::dialect::DenseTensorType", false, false) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("UnchangedInferMeta", {"x"}, "log2", {"x"}, {}, {}, {{"out", "x"}}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "log2");
}

void Log2_Op::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_) {
  VLOG(4) << "Start build Log2_Op";



  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);
  paddle::dialect::IrTensor dense_out;
  paddle::dialect::IrMetaTensor meta_out(&dense_out);

  phi::UnchangedInferMeta(meta_x, &meta_out);

  std::vector<pir::Type> argument_outputs;
  pir::Type out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_out.dtype()), dense_out.dims(), dense_out.layout(), dense_out.lod(), dense_out.offset());
  argument_outputs.push_back(out_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void Log2_Op::VerifySig() {
  VLOG(4) << "Start Verifying inputs, outputs and attributes for: Log2_Op.";
  VLOG(4) << "Verifying inputs:";
  {
  auto input_size = num_operands();
  IR_ENFORCE(input_size == 1u,
                    "The size %d of inputs must be equal to 1.", input_size);
  IR_ENFORCE((*this)->operand_source(0).type().isa<paddle::dialect::DenseTensorType>(),
                  "Type validation failed for the 0th input, got %s.", (*this)->operand_source(0).type());
  }
  VLOG(4) << "Verifying attributes:";
  {
  // Attributes num is 0, not need to check attributes type.
  }
  VLOG(4) << "Verifying outputs:";
  {
  auto output_size = num_results();
  IR_ENFORCE(output_size == 1u,
                    "The size %d of outputs must be equal to 1.", output_size);
  IR_ENFORCE((*this)->result(0).type().isa<paddle::dialect::DenseTensorType>(),
                 "Type validation failed for the 0th output.");
  }
  VLOG(4) << "End Verifying for: Log2_Op.";
}

void Log2_Op::InferMeta( phi::InferMetaContext *infer_meta ) {
  auto fn = PD_INFER_META(phi::UnchangedInferMeta);
  fn(infer_meta);
}

phi::DataType Log2_Op::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: Log2_Op";
  


  return expected_kernel_dtype;
}

const char *LogLossOp::attributes_name[1] = { "epsilon" };

OpInfoTuple LogLossOp::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("input", "paddle::dialect::DenseTensorType", false, false, false, true), paddle::dialect::OpInputInfo("label", "paddle::dialect::DenseTensorType", false, false, false, false) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = { paddle::dialect::OpAttributeInfo("epsilon", "pir::FloatAttribute", "") };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("out", "paddle::dialect::DenseTensorType", false, false) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("LogLossInferMeta", {"input", "label", "epsilon"}, "log_loss", {"input", "label", "epsilon"}, {}, {}, {}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "log_loss");
}

void LogLossOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value input_, pir::Value label_, float epsilon) {
  VLOG(4) << "Start build LogLossOp";



  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {input_, label_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_epsilon = pir::FloatAttribute::get(pir::IrContext::Instance(), epsilon);
  argument.AddAttribute("epsilon", attr_epsilon);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType input = input_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)input;
  paddle::dialect::DenseTensorType label = label_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)label;

  VLOG(4) << "Builder construction  dense_input";
  paddle::dialect::IrTensor ir_tensor_input(paddle::dialect::TransToPhiDataType(input.dtype()),
                                                      input.dims(),
                                                      input.data_layout(),
                                                      input.lod(),
                                                      input.offset());
  VLOG(4) << "Builder construction  meta_input";
  paddle::dialect::IrMetaTensor meta_input(&ir_tensor_input);

  VLOG(4) << "Builder construction  dense_label";
  paddle::dialect::IrTensor ir_tensor_label(paddle::dialect::TransToPhiDataType(label.dtype()),
                                                      label.dims(),
                                                      label.data_layout(),
                                                      label.lod(),
                                                      label.offset());
  VLOG(4) << "Builder construction  meta_label";
  paddle::dialect::IrMetaTensor meta_label(&ir_tensor_label);
  paddle::dialect::IrTensor dense_out;
  paddle::dialect::IrMetaTensor meta_out(&dense_out);

  phi::LogLossInferMeta(meta_input, meta_label, epsilon, &meta_out);

  std::vector<pir::Type> argument_outputs;
  pir::Type out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_out.dtype()), dense_out.dims(), dense_out.layout(), dense_out.lod(), dense_out.offset());
  argument_outputs.push_back(out_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void LogLossOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value input_, pir::Value label_, pir::AttributeMap attributes) {
  VLOG(4) << "Start build LogLossOp";


  IR_ENFORCE(
      attributes.find("epsilon") != attributes.end(),
          "'epsilon' Attribute is expected for LogLossOp. ");
  float epsilon = attributes.at("epsilon").dyn_cast<pir::FloatAttribute>().data();


  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {input_, label_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_epsilon = pir::FloatAttribute::get(pir::IrContext::Instance(), epsilon);
  argument.AddAttribute("epsilon", attr_epsilon);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType input = input_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)input;
  paddle::dialect::DenseTensorType label = label_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)label;

  VLOG(4) << "Builder construction  dense_input";
  paddle::dialect::IrTensor ir_tensor_input(paddle::dialect::TransToPhiDataType(input.dtype()),
                                                      input.dims(),
                                                      input.data_layout(),
                                                      input.lod(),
                                                      input.offset());
  VLOG(4) << "Builder construction  meta_input";
  paddle::dialect::IrMetaTensor meta_input(&ir_tensor_input);

  VLOG(4) << "Builder construction  dense_label";
  paddle::dialect::IrTensor ir_tensor_label(paddle::dialect::TransToPhiDataType(label.dtype()),
                                                      label.dims(),
                                                      label.data_layout(),
                                                      label.lod(),
                                                      label.offset());
  VLOG(4) << "Builder construction  meta_label";
  paddle::dialect::IrMetaTensor meta_label(&ir_tensor_label);
  paddle::dialect::IrTensor dense_out;
  paddle::dialect::IrMetaTensor meta_out(&dense_out);

  phi::LogLossInferMeta(meta_input, meta_label, epsilon, &meta_out);

  std::vector<pir::Type> argument_outputs;
  pir::Type out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_out.dtype()), dense_out.dims(), dense_out.layout(), dense_out.lod(), dense_out.offset());
  argument_outputs.push_back(out_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void LogLossOp::VerifySig() {
  VLOG(4) << "Start Verifying inputs, outputs and attributes for: LogLossOp.";
  VLOG(4) << "Verifying inputs:";
  {
  auto input_size = num_operands();
  IR_ENFORCE(input_size == 2u,
                    "The size %d of inputs must be equal to 2.", input_size);
  IR_ENFORCE((*this)->operand_source(0).type().isa<paddle::dialect::DenseTensorType>(),
                  "Type validation failed for the 0th input, got %s.", (*this)->operand_source(0).type());
  IR_ENFORCE((*this)->operand_source(1).type().isa<paddle::dialect::DenseTensorType>(),
                  "Type validation failed for the 1th input, got %s.", (*this)->operand_source(1).type());
  }
  VLOG(4) << "Verifying attributes:";
  {
  auto& attributes = this->attributes();
  IR_ENFORCE(attributes.count("epsilon")>0,
                 "epsilon does not exist.");
  IR_ENFORCE(attributes.at("epsilon").isa<pir::FloatAttribute>(),
                 "Type of attribute: epsilon is not pir::FloatAttribute.");

  }
  VLOG(4) << "Verifying outputs:";
  {
  auto output_size = num_results();
  IR_ENFORCE(output_size == 1u,
                    "The size %d of outputs must be equal to 1.", output_size);
  IR_ENFORCE((*this)->result(0).type().isa<paddle::dialect::DenseTensorType>(),
                 "Type validation failed for the 0th output.");
  }
  VLOG(4) << "End Verifying for: LogLossOp.";
}

void LogLossOp::InferMeta( phi::InferMetaContext *infer_meta ) {
  auto fn = PD_INFER_META(phi::LogLossInferMeta);
  fn(infer_meta);
}

phi::DataType LogLossOp::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: LogLossOp";
  


  return expected_kernel_dtype;
}

const char *LogSoftmaxOp::attributes_name[1] = { "axis" };

OpInfoTuple LogSoftmaxOp::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("x", "paddle::dialect::DenseTensorType", false, false, false, true) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = { paddle::dialect::OpAttributeInfo("axis", "pir::Int32Attribute", "") };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("out", "paddle::dialect::DenseTensorType", false, false) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("UnchangedInferMetaCheckAxis", {"x", "axis"}, "log_softmax", {"x", "axis"}, {"x"}, {}, {}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "log_softmax");
}

void LogSoftmaxOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, int axis) {
  VLOG(4) << "Start build LogSoftmaxOp";



  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_axis = pir::Int32Attribute::get(pir::IrContext::Instance(), axis);
  argument.AddAttribute("axis", attr_axis);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);
  paddle::dialect::IrTensor dense_out;
  paddle::dialect::IrMetaTensor meta_out(&dense_out);

  phi::UnchangedInferMetaCheckAxis(meta_x, axis, &meta_out);

  std::vector<pir::Type> argument_outputs;
  pir::Type out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_out.dtype()), dense_out.dims(), dense_out.layout(), dense_out.lod(), dense_out.offset());
  argument_outputs.push_back(out_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void LogSoftmaxOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, pir::AttributeMap attributes) {
  VLOG(4) << "Start build LogSoftmaxOp";


  IR_ENFORCE(
      attributes.find("axis") != attributes.end(),
          "'axis' Attribute is expected for LogSoftmaxOp. ");
  int axis = attributes.at("axis").dyn_cast<pir::Int32Attribute>().data();


  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_axis = pir::Int32Attribute::get(pir::IrContext::Instance(), axis);
  argument.AddAttribute("axis", attr_axis);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);
  paddle::dialect::IrTensor dense_out;
  paddle::dialect::IrMetaTensor meta_out(&dense_out);

  phi::UnchangedInferMetaCheckAxis(meta_x, axis, &meta_out);

  std::vector<pir::Type> argument_outputs;
  pir::Type out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_out.dtype()), dense_out.dims(), dense_out.layout(), dense_out.lod(), dense_out.offset());
  argument_outputs.push_back(out_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void LogSoftmaxOp::VerifySig() {
  VLOG(4) << "Start Verifying inputs, outputs and attributes for: LogSoftmaxOp.";
  VLOG(4) << "Verifying inputs:";
  {
  auto input_size = num_operands();
  IR_ENFORCE(input_size == 1u,
                    "The size %d of inputs must be equal to 1.", input_size);
  IR_ENFORCE((*this)->operand_source(0).type().isa<paddle::dialect::DenseTensorType>(),
                  "Type validation failed for the 0th input, got %s.", (*this)->operand_source(0).type());
  }
  VLOG(4) << "Verifying attributes:";
  {
  auto& attributes = this->attributes();
  IR_ENFORCE(attributes.count("axis")>0,
                 "axis does not exist.");
  IR_ENFORCE(attributes.at("axis").isa<pir::Int32Attribute>(),
                 "Type of attribute: axis is not pir::Int32Attribute.");

  }
  VLOG(4) << "Verifying outputs:";
  {
  auto output_size = num_results();
  IR_ENFORCE(output_size == 1u,
                    "The size %d of outputs must be equal to 1.", output_size);
  IR_ENFORCE((*this)->result(0).type().isa<paddle::dialect::DenseTensorType>(),
                 "Type validation failed for the 0th output.");
  }
  VLOG(4) << "End Verifying for: LogSoftmaxOp.";
}

void LogSoftmaxOp::InferMeta( phi::InferMetaContext *infer_meta ) {
  auto fn = PD_INFER_META(phi::UnchangedInferMetaCheckAxis);
  fn(infer_meta);
}

phi::DataType LogSoftmaxOp::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: LogSoftmaxOp";
  


  return expected_kernel_dtype;
}

const char *LogcumsumexpOp::attributes_name[4] = { "axis", "flatten", "exclusive", "reverse" };

OpInfoTuple LogcumsumexpOp::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("x", "paddle::dialect::DenseTensorType", false, false, false, true) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = { paddle::dialect::OpAttributeInfo("axis", "pir::Int32Attribute", ""), paddle::dialect::OpAttributeInfo("flatten", "pir::BoolAttribute", ""), paddle::dialect::OpAttributeInfo("exclusive", "pir::BoolAttribute", ""), paddle::dialect::OpAttributeInfo("reverse", "pir::BoolAttribute", "") };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("out", "paddle::dialect::DenseTensorType", false, false) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("CumInferMeta", {"x", "axis", "flatten", "exclusive", "reverse"}, "logcumsumexp", {"x", "axis", "flatten", "exclusive", "reverse"}, {}, {}, {}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "logcumsumexp");
}

void LogcumsumexpOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, int axis, bool flatten, bool exclusive, bool reverse) {
  VLOG(4) << "Start build LogcumsumexpOp";



  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_axis = pir::Int32Attribute::get(pir::IrContext::Instance(), axis);
  argument.AddAttribute("axis", attr_axis);
  pir::Attribute attr_flatten = pir::BoolAttribute::get(pir::IrContext::Instance(), flatten);
  argument.AddAttribute("flatten", attr_flatten);
  pir::Attribute attr_exclusive = pir::BoolAttribute::get(pir::IrContext::Instance(), exclusive);
  argument.AddAttribute("exclusive", attr_exclusive);
  pir::Attribute attr_reverse = pir::BoolAttribute::get(pir::IrContext::Instance(), reverse);
  argument.AddAttribute("reverse", attr_reverse);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);
  paddle::dialect::IrTensor dense_out;
  paddle::dialect::IrMetaTensor meta_out(&dense_out);

  phi::CumInferMeta(meta_x, axis, flatten, exclusive, reverse, &meta_out);

  std::vector<pir::Type> argument_outputs;
  pir::Type out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_out.dtype()), dense_out.dims(), dense_out.layout(), dense_out.lod(), dense_out.offset());
  argument_outputs.push_back(out_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void LogcumsumexpOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, pir::AttributeMap attributes) {
  VLOG(4) << "Start build LogcumsumexpOp";


  IR_ENFORCE(
      attributes.find("axis") != attributes.end(),
          "'axis' Attribute is expected for LogcumsumexpOp. ");
  int axis = attributes.at("axis").dyn_cast<pir::Int32Attribute>().data();

  IR_ENFORCE(
      attributes.find("flatten") != attributes.end(),
          "'flatten' Attribute is expected for LogcumsumexpOp. ");
  bool flatten = attributes.at("flatten").dyn_cast<pir::BoolAttribute>().data();

  IR_ENFORCE(
      attributes.find("exclusive") != attributes.end(),
          "'exclusive' Attribute is expected for LogcumsumexpOp. ");
  bool exclusive = attributes.at("exclusive").dyn_cast<pir::BoolAttribute>().data();

  IR_ENFORCE(
      attributes.find("reverse") != attributes.end(),
          "'reverse' Attribute is expected for LogcumsumexpOp. ");
  bool reverse = attributes.at("reverse").dyn_cast<pir::BoolAttribute>().data();


  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_axis = pir::Int32Attribute::get(pir::IrContext::Instance(), axis);
  argument.AddAttribute("axis", attr_axis);
  pir::Attribute attr_flatten = pir::BoolAttribute::get(pir::IrContext::Instance(), flatten);
  argument.AddAttribute("flatten", attr_flatten);
  pir::Attribute attr_exclusive = pir::BoolAttribute::get(pir::IrContext::Instance(), exclusive);
  argument.AddAttribute("exclusive", attr_exclusive);
  pir::Attribute attr_reverse = pir::BoolAttribute::get(pir::IrContext::Instance(), reverse);
  argument.AddAttribute("reverse", attr_reverse);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);
  paddle::dialect::IrTensor dense_out;
  paddle::dialect::IrMetaTensor meta_out(&dense_out);

  phi::CumInferMeta(meta_x, axis, flatten, exclusive, reverse, &meta_out);

  std::vector<pir::Type> argument_outputs;
  pir::Type out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_out.dtype()), dense_out.dims(), dense_out.layout(), dense_out.lod(), dense_out.offset());
  argument_outputs.push_back(out_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void LogcumsumexpOp::VerifySig() {
  VLOG(4) << "Start Verifying inputs, outputs and attributes for: LogcumsumexpOp.";
  VLOG(4) << "Verifying inputs:";
  {
  auto input_size = num_operands();
  IR_ENFORCE(input_size == 1u,
                    "The size %d of inputs must be equal to 1.", input_size);
  IR_ENFORCE((*this)->operand_source(0).type().isa<paddle::dialect::DenseTensorType>(),
                  "Type validation failed for the 0th input, got %s.", (*this)->operand_source(0).type());
  }
  VLOG(4) << "Verifying attributes:";
  {
  auto& attributes = this->attributes();
  IR_ENFORCE(attributes.count("axis")>0,
                 "axis does not exist.");
  IR_ENFORCE(attributes.at("axis").isa<pir::Int32Attribute>(),
                 "Type of attribute: axis is not pir::Int32Attribute.");

  IR_ENFORCE(attributes.count("flatten")>0,
                 "flatten does not exist.");
  IR_ENFORCE(attributes.at("flatten").isa<pir::BoolAttribute>(),
                 "Type of attribute: flatten is not pir::BoolAttribute.");

  IR_ENFORCE(attributes.count("exclusive")>0,
                 "exclusive does not exist.");
  IR_ENFORCE(attributes.at("exclusive").isa<pir::BoolAttribute>(),
                 "Type of attribute: exclusive is not pir::BoolAttribute.");

  IR_ENFORCE(attributes.count("reverse")>0,
                 "reverse does not exist.");
  IR_ENFORCE(attributes.at("reverse").isa<pir::BoolAttribute>(),
                 "Type of attribute: reverse is not pir::BoolAttribute.");

  }
  VLOG(4) << "Verifying outputs:";
  {
  auto output_size = num_results();
  IR_ENFORCE(output_size == 1u,
                    "The size %d of outputs must be equal to 1.", output_size);
  IR_ENFORCE((*this)->result(0).type().isa<paddle::dialect::DenseTensorType>(),
                 "Type validation failed for the 0th output.");
  }
  VLOG(4) << "End Verifying for: LogcumsumexpOp.";
}

void LogcumsumexpOp::InferMeta( phi::InferMetaContext *infer_meta ) {
  auto fn = PD_INFER_META(phi::CumInferMeta);
  fn(infer_meta);
}

phi::DataType LogcumsumexpOp::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: LogcumsumexpOp";
  


  return expected_kernel_dtype;
}

OpInfoTuple LogicalAndOp::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("x", "paddle::dialect::DenseTensorType", false, false, false, false), paddle::dialect::OpInputInfo("y", "paddle::dialect::DenseTensorType", false, false, false, false) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = {  };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("out", "paddle::dialect::DenseTensorType", false, false) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("LogicalBinaryInferMeta", {"x", "y"}, "logical_and", {"x", "y"}, {"x"}, {"x"}, {}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "logical_and");
}

void LogicalAndOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, pir::Value y_) {
  VLOG(4) << "Start build LogicalAndOp";



  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_, y_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;
  paddle::dialect::DenseTensorType y = y_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)y;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);

  VLOG(4) << "Builder construction  dense_y";
  paddle::dialect::IrTensor ir_tensor_y(paddle::dialect::TransToPhiDataType(y.dtype()),
                                                      y.dims(),
                                                      y.data_layout(),
                                                      y.lod(),
                                                      y.offset());
  VLOG(4) << "Builder construction  meta_y";
  paddle::dialect::IrMetaTensor meta_y(&ir_tensor_y);
  paddle::dialect::IrTensor dense_out;
  paddle::dialect::IrMetaTensor meta_out(&dense_out);

  phi::LogicalBinaryInferMeta(meta_x, meta_y, &meta_out);

  std::vector<pir::Type> argument_outputs;
  pir::Type out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_out.dtype()), dense_out.dims(), dense_out.layout(), dense_out.lod(), dense_out.offset());
  argument_outputs.push_back(out_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void LogicalAndOp::VerifySig() {
  VLOG(4) << "Start Verifying inputs, outputs and attributes for: LogicalAndOp.";
  VLOG(4) << "Verifying inputs:";
  {
  auto input_size = num_operands();
  IR_ENFORCE(input_size == 2u,
                    "The size %d of inputs must be equal to 2.", input_size);
  IR_ENFORCE((*this)->operand_source(0).type().isa<paddle::dialect::DenseTensorType>(),
                  "Type validation failed for the 0th input, got %s.", (*this)->operand_source(0).type());
  IR_ENFORCE((*this)->operand_source(1).type().isa<paddle::dialect::DenseTensorType>(),
                  "Type validation failed for the 1th input, got %s.", (*this)->operand_source(1).type());
  }
  VLOG(4) << "Verifying attributes:";
  {
  // Attributes num is 0, not need to check attributes type.
  }
  VLOG(4) << "Verifying outputs:";
  {
  auto output_size = num_results();
  IR_ENFORCE(output_size == 1u,
                    "The size %d of outputs must be equal to 1.", output_size);
  IR_ENFORCE((*this)->result(0).type().isa<paddle::dialect::DenseTensorType>(),
                 "Type validation failed for the 0th output.");
  }
  VLOG(4) << "End Verifying for: LogicalAndOp.";
}

void LogicalAndOp::InferMeta( phi::InferMetaContext *infer_meta ) {
  auto fn = PD_INFER_META(phi::LogicalBinaryInferMeta);
  fn(infer_meta);
}

phi::DataType LogicalAndOp::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: LogicalAndOp";
  


  return expected_kernel_dtype;
}

OpInfoTuple LogicalAnd_Op::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("x", "paddle::dialect::DenseTensorType", false, false, false, false), paddle::dialect::OpInputInfo("y", "paddle::dialect::DenseTensorType", false, false, false, false) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = {  };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("out", "paddle::dialect::DenseTensorType", false, false) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("LogicalBinaryInferMeta", {"x", "y"}, "logical_and", {"x", "y"}, {"x"}, {"x"}, {{"out", "x"}}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "logical_and");
}

void LogicalAnd_Op::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, pir::Value y_) {
  VLOG(4) << "Start build LogicalAnd_Op";



  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_, y_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;
  paddle::dialect::DenseTensorType y = y_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)y;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);

  VLOG(4) << "Builder construction  dense_y";
  paddle::dialect::IrTensor ir_tensor_y(paddle::dialect::TransToPhiDataType(y.dtype()),
                                                      y.dims(),
                                                      y.data_layout(),
                                                      y.lod(),
                                                      y.offset());
  VLOG(4) << "Builder construction  meta_y";
  paddle::dialect::IrMetaTensor meta_y(&ir_tensor_y);
  paddle::dialect::IrTensor dense_out;
  paddle::dialect::IrMetaTensor meta_out(&dense_out);

  phi::LogicalBinaryInferMeta(meta_x, meta_y, &meta_out);

  std::vector<pir::Type> argument_outputs;
  pir::Type out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_out.dtype()), dense_out.dims(), dense_out.layout(), dense_out.lod(), dense_out.offset());
  argument_outputs.push_back(out_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void LogicalAnd_Op::VerifySig() {
  VLOG(4) << "Start Verifying inputs, outputs and attributes for: LogicalAnd_Op.";
  VLOG(4) << "Verifying inputs:";
  {
  auto input_size = num_operands();
  IR_ENFORCE(input_size == 2u,
                    "The size %d of inputs must be equal to 2.", input_size);
  IR_ENFORCE((*this)->operand_source(0).type().isa<paddle::dialect::DenseTensorType>(),
                  "Type validation failed for the 0th input, got %s.", (*this)->operand_source(0).type());
  IR_ENFORCE((*this)->operand_source(1).type().isa<paddle::dialect::DenseTensorType>(),
                  "Type validation failed for the 1th input, got %s.", (*this)->operand_source(1).type());
  }
  VLOG(4) << "Verifying attributes:";
  {
  // Attributes num is 0, not need to check attributes type.
  }
  VLOG(4) << "Verifying outputs:";
  {
  auto output_size = num_results();
  IR_ENFORCE(output_size == 1u,
                    "The size %d of outputs must be equal to 1.", output_size);
  IR_ENFORCE((*this)->result(0).type().isa<paddle::dialect::DenseTensorType>(),
                 "Type validation failed for the 0th output.");
  }
  VLOG(4) << "End Verifying for: LogicalAnd_Op.";
}

void LogicalAnd_Op::InferMeta( phi::InferMetaContext *infer_meta ) {
  auto fn = PD_INFER_META(phi::LogicalBinaryInferMeta);
  fn(infer_meta);
}

phi::DataType LogicalAnd_Op::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: LogicalAnd_Op";
  


  return expected_kernel_dtype;
}

OpInfoTuple LogicalNotOp::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("x", "paddle::dialect::DenseTensorType", false, false, false, false) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = {  };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("out", "paddle::dialect::DenseTensorType", false, false) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("LogicalNotInfermeta", {"x"}, "logical_not", {"x"}, {"x"}, {"x"}, {}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "logical_not");
}

void LogicalNotOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_) {
  VLOG(4) << "Start build LogicalNotOp";



  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);
  paddle::dialect::IrTensor dense_out;
  paddle::dialect::IrMetaTensor meta_out(&dense_out);

  phi::LogicalNotInfermeta(meta_x, &meta_out);

  std::vector<pir::Type> argument_outputs;
  pir::Type out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_out.dtype()), dense_out.dims(), dense_out.layout(), dense_out.lod(), dense_out.offset());
  argument_outputs.push_back(out_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void LogicalNotOp::VerifySig() {
  VLOG(4) << "Start Verifying inputs, outputs and attributes for: LogicalNotOp.";
  VLOG(4) << "Verifying inputs:";
  {
  auto input_size = num_operands();
  IR_ENFORCE(input_size == 1u,
                    "The size %d of inputs must be equal to 1.", input_size);
  IR_ENFORCE((*this)->operand_source(0).type().isa<paddle::dialect::DenseTensorType>(),
                  "Type validation failed for the 0th input, got %s.", (*this)->operand_source(0).type());
  }
  VLOG(4) << "Verifying attributes:";
  {
  // Attributes num is 0, not need to check attributes type.
  }
  VLOG(4) << "Verifying outputs:";
  {
  auto output_size = num_results();
  IR_ENFORCE(output_size == 1u,
                    "The size %d of outputs must be equal to 1.", output_size);
  IR_ENFORCE((*this)->result(0).type().isa<paddle::dialect::DenseTensorType>(),
                 "Type validation failed for the 0th output.");
  }
  VLOG(4) << "End Verifying for: LogicalNotOp.";
}

void LogicalNotOp::InferMeta( phi::InferMetaContext *infer_meta ) {
  auto fn = PD_INFER_META(phi::LogicalNotInfermeta);
  fn(infer_meta);
}

phi::DataType LogicalNotOp::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: LogicalNotOp";
  


  return expected_kernel_dtype;
}

OpInfoTuple LogicalNot_Op::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("x", "paddle::dialect::DenseTensorType", false, false, false, false) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = {  };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("out", "paddle::dialect::DenseTensorType", false, false) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("LogicalNotInfermeta", {"x"}, "logical_not", {"x"}, {"x"}, {"x"}, {{"out", "x"}}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "logical_not");
}

void LogicalNot_Op::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_) {
  VLOG(4) << "Start build LogicalNot_Op";



  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);
  paddle::dialect::IrTensor dense_out;
  paddle::dialect::IrMetaTensor meta_out(&dense_out);

  phi::LogicalNotInfermeta(meta_x, &meta_out);

  std::vector<pir::Type> argument_outputs;
  pir::Type out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_out.dtype()), dense_out.dims(), dense_out.layout(), dense_out.lod(), dense_out.offset());
  argument_outputs.push_back(out_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void LogicalNot_Op::VerifySig() {
  VLOG(4) << "Start Verifying inputs, outputs and attributes for: LogicalNot_Op.";
  VLOG(4) << "Verifying inputs:";
  {
  auto input_size = num_operands();
  IR_ENFORCE(input_size == 1u,
                    "The size %d of inputs must be equal to 1.", input_size);
  IR_ENFORCE((*this)->operand_source(0).type().isa<paddle::dialect::DenseTensorType>(),
                  "Type validation failed for the 0th input, got %s.", (*this)->operand_source(0).type());
  }
  VLOG(4) << "Verifying attributes:";
  {
  // Attributes num is 0, not need to check attributes type.
  }
  VLOG(4) << "Verifying outputs:";
  {
  auto output_size = num_results();
  IR_ENFORCE(output_size == 1u,
                    "The size %d of outputs must be equal to 1.", output_size);
  IR_ENFORCE((*this)->result(0).type().isa<paddle::dialect::DenseTensorType>(),
                 "Type validation failed for the 0th output.");
  }
  VLOG(4) << "End Verifying for: LogicalNot_Op.";
}

void LogicalNot_Op::InferMeta( phi::InferMetaContext *infer_meta ) {
  auto fn = PD_INFER_META(phi::LogicalNotInfermeta);
  fn(infer_meta);
}

phi::DataType LogicalNot_Op::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: LogicalNot_Op";
  


  return expected_kernel_dtype;
}

OpInfoTuple LogicalOrOp::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("x", "paddle::dialect::DenseTensorType", false, false, false, false), paddle::dialect::OpInputInfo("y", "paddle::dialect::DenseTensorType", false, false, false, false) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = {  };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("out", "paddle::dialect::DenseTensorType", false, false) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("LogicalBinaryInferMeta", {"x", "y"}, "logical_or", {"x", "y"}, {"x"}, {"x"}, {}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "logical_or");
}

void LogicalOrOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, pir::Value y_) {
  VLOG(4) << "Start build LogicalOrOp";



  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_, y_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;
  paddle::dialect::DenseTensorType y = y_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)y;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);

  VLOG(4) << "Builder construction  dense_y";
  paddle::dialect::IrTensor ir_tensor_y(paddle::dialect::TransToPhiDataType(y.dtype()),
                                                      y.dims(),
                                                      y.data_layout(),
                                                      y.lod(),
                                                      y.offset());
  VLOG(4) << "Builder construction  meta_y";
  paddle::dialect::IrMetaTensor meta_y(&ir_tensor_y);
  paddle::dialect::IrTensor dense_out;
  paddle::dialect::IrMetaTensor meta_out(&dense_out);

  phi::LogicalBinaryInferMeta(meta_x, meta_y, &meta_out);

  std::vector<pir::Type> argument_outputs;
  pir::Type out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_out.dtype()), dense_out.dims(), dense_out.layout(), dense_out.lod(), dense_out.offset());
  argument_outputs.push_back(out_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void LogicalOrOp::VerifySig() {
  VLOG(4) << "Start Verifying inputs, outputs and attributes for: LogicalOrOp.";
  VLOG(4) << "Verifying inputs:";
  {
  auto input_size = num_operands();
  IR_ENFORCE(input_size == 2u,
                    "The size %d of inputs must be equal to 2.", input_size);
  IR_ENFORCE((*this)->operand_source(0).type().isa<paddle::dialect::DenseTensorType>(),
                  "Type validation failed for the 0th input, got %s.", (*this)->operand_source(0).type());
  IR_ENFORCE((*this)->operand_source(1).type().isa<paddle::dialect::DenseTensorType>(),
                  "Type validation failed for the 1th input, got %s.", (*this)->operand_source(1).type());
  }
  VLOG(4) << "Verifying attributes:";
  {
  // Attributes num is 0, not need to check attributes type.
  }
  VLOG(4) << "Verifying outputs:";
  {
  auto output_size = num_results();
  IR_ENFORCE(output_size == 1u,
                    "The size %d of outputs must be equal to 1.", output_size);
  IR_ENFORCE((*this)->result(0).type().isa<paddle::dialect::DenseTensorType>(),
                 "Type validation failed for the 0th output.");
  }
  VLOG(4) << "End Verifying for: LogicalOrOp.";
}

void LogicalOrOp::InferMeta( phi::InferMetaContext *infer_meta ) {
  auto fn = PD_INFER_META(phi::LogicalBinaryInferMeta);
  fn(infer_meta);
}

phi::DataType LogicalOrOp::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: LogicalOrOp";
  


  return expected_kernel_dtype;
}

OpInfoTuple LogicalOr_Op::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("x", "paddle::dialect::DenseTensorType", false, false, false, false), paddle::dialect::OpInputInfo("y", "paddle::dialect::DenseTensorType", false, false, false, false) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = {  };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("out", "paddle::dialect::DenseTensorType", false, false) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("LogicalBinaryInferMeta", {"x", "y"}, "logical_or", {"x", "y"}, {"x"}, {"x"}, {{"out", "x"}}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "logical_or");
}

void LogicalOr_Op::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, pir::Value y_) {
  VLOG(4) << "Start build LogicalOr_Op";



  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_, y_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;
  paddle::dialect::DenseTensorType y = y_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)y;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);

  VLOG(4) << "Builder construction  dense_y";
  paddle::dialect::IrTensor ir_tensor_y(paddle::dialect::TransToPhiDataType(y.dtype()),
                                                      y.dims(),
                                                      y.data_layout(),
                                                      y.lod(),
                                                      y.offset());
  VLOG(4) << "Builder construction  meta_y";
  paddle::dialect::IrMetaTensor meta_y(&ir_tensor_y);
  paddle::dialect::IrTensor dense_out;
  paddle::dialect::IrMetaTensor meta_out(&dense_out);

  phi::LogicalBinaryInferMeta(meta_x, meta_y, &meta_out);

  std::vector<pir::Type> argument_outputs;
  pir::Type out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_out.dtype()), dense_out.dims(), dense_out.layout(), dense_out.lod(), dense_out.offset());
  argument_outputs.push_back(out_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void LogicalOr_Op::VerifySig() {
  VLOG(4) << "Start Verifying inputs, outputs and attributes for: LogicalOr_Op.";
  VLOG(4) << "Verifying inputs:";
  {
  auto input_size = num_operands();
  IR_ENFORCE(input_size == 2u,
                    "The size %d of inputs must be equal to 2.", input_size);
  IR_ENFORCE((*this)->operand_source(0).type().isa<paddle::dialect::DenseTensorType>(),
                  "Type validation failed for the 0th input, got %s.", (*this)->operand_source(0).type());
  IR_ENFORCE((*this)->operand_source(1).type().isa<paddle::dialect::DenseTensorType>(),
                  "Type validation failed for the 1th input, got %s.", (*this)->operand_source(1).type());
  }
  VLOG(4) << "Verifying attributes:";
  {
  // Attributes num is 0, not need to check attributes type.
  }
  VLOG(4) << "Verifying outputs:";
  {
  auto output_size = num_results();
  IR_ENFORCE(output_size == 1u,
                    "The size %d of outputs must be equal to 1.", output_size);
  IR_ENFORCE((*this)->result(0).type().isa<paddle::dialect::DenseTensorType>(),
                 "Type validation failed for the 0th output.");
  }
  VLOG(4) << "End Verifying for: LogicalOr_Op.";
}

void LogicalOr_Op::InferMeta( phi::InferMetaContext *infer_meta ) {
  auto fn = PD_INFER_META(phi::LogicalBinaryInferMeta);
  fn(infer_meta);
}

phi::DataType LogicalOr_Op::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: LogicalOr_Op";
  


  return expected_kernel_dtype;
}

OpInfoTuple LogicalXorOp::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("x", "paddle::dialect::DenseTensorType", false, false, false, false), paddle::dialect::OpInputInfo("y", "paddle::dialect::DenseTensorType", false, false, false, false) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = {  };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("out", "paddle::dialect::DenseTensorType", false, false) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("LogicalBinaryInferMeta", {"x", "y"}, "logical_xor", {"x", "y"}, {"x"}, {"x"}, {}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "logical_xor");
}

void LogicalXorOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, pir::Value y_) {
  VLOG(4) << "Start build LogicalXorOp";



  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_, y_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;
  paddle::dialect::DenseTensorType y = y_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)y;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);

  VLOG(4) << "Builder construction  dense_y";
  paddle::dialect::IrTensor ir_tensor_y(paddle::dialect::TransToPhiDataType(y.dtype()),
                                                      y.dims(),
                                                      y.data_layout(),
                                                      y.lod(),
                                                      y.offset());
  VLOG(4) << "Builder construction  meta_y";
  paddle::dialect::IrMetaTensor meta_y(&ir_tensor_y);
  paddle::dialect::IrTensor dense_out;
  paddle::dialect::IrMetaTensor meta_out(&dense_out);

  phi::LogicalBinaryInferMeta(meta_x, meta_y, &meta_out);

  std::vector<pir::Type> argument_outputs;
  pir::Type out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_out.dtype()), dense_out.dims(), dense_out.layout(), dense_out.lod(), dense_out.offset());
  argument_outputs.push_back(out_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void LogicalXorOp::VerifySig() {
  VLOG(4) << "Start Verifying inputs, outputs and attributes for: LogicalXorOp.";
  VLOG(4) << "Verifying inputs:";
  {
  auto input_size = num_operands();
  IR_ENFORCE(input_size == 2u,
                    "The size %d of inputs must be equal to 2.", input_size);
  IR_ENFORCE((*this)->operand_source(0).type().isa<paddle::dialect::DenseTensorType>(),
                  "Type validation failed for the 0th input, got %s.", (*this)->operand_source(0).type());
  IR_ENFORCE((*this)->operand_source(1).type().isa<paddle::dialect::DenseTensorType>(),
                  "Type validation failed for the 1th input, got %s.", (*this)->operand_source(1).type());
  }
  VLOG(4) << "Verifying attributes:";
  {
  // Attributes num is 0, not need to check attributes type.
  }
  VLOG(4) << "Verifying outputs:";
  {
  auto output_size = num_results();
  IR_ENFORCE(output_size == 1u,
                    "The size %d of outputs must be equal to 1.", output_size);
  IR_ENFORCE((*this)->result(0).type().isa<paddle::dialect::DenseTensorType>(),
                 "Type validation failed for the 0th output.");
  }
  VLOG(4) << "End Verifying for: LogicalXorOp.";
}

void LogicalXorOp::InferMeta( phi::InferMetaContext *infer_meta ) {
  auto fn = PD_INFER_META(phi::LogicalBinaryInferMeta);
  fn(infer_meta);
}

phi::DataType LogicalXorOp::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: LogicalXorOp";
  


  return expected_kernel_dtype;
}

OpInfoTuple LogicalXor_Op::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("x", "paddle::dialect::DenseTensorType", false, false, false, false), paddle::dialect::OpInputInfo("y", "paddle::dialect::DenseTensorType", false, false, false, false) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = {  };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("out", "paddle::dialect::DenseTensorType", false, false) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("LogicalBinaryInferMeta", {"x", "y"}, "logical_xor", {"x", "y"}, {"x"}, {"x"}, {{"out", "x"}}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "logical_xor");
}

void LogicalXor_Op::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, pir::Value y_) {
  VLOG(4) << "Start build LogicalXor_Op";



  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_, y_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;
  paddle::dialect::DenseTensorType y = y_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)y;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);

  VLOG(4) << "Builder construction  dense_y";
  paddle::dialect::IrTensor ir_tensor_y(paddle::dialect::TransToPhiDataType(y.dtype()),
                                                      y.dims(),
                                                      y.data_layout(),
                                                      y.lod(),
                                                      y.offset());
  VLOG(4) << "Builder construction  meta_y";
  paddle::dialect::IrMetaTensor meta_y(&ir_tensor_y);
  paddle::dialect::IrTensor dense_out;
  paddle::dialect::IrMetaTensor meta_out(&dense_out);

  phi::LogicalBinaryInferMeta(meta_x, meta_y, &meta_out);

  std::vector<pir::Type> argument_outputs;
  pir::Type out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_out.dtype()), dense_out.dims(), dense_out.layout(), dense_out.lod(), dense_out.offset());
  argument_outputs.push_back(out_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void LogicalXor_Op::VerifySig() {
  VLOG(4) << "Start Verifying inputs, outputs and attributes for: LogicalXor_Op.";
  VLOG(4) << "Verifying inputs:";
  {
  auto input_size = num_operands();
  IR_ENFORCE(input_size == 2u,
                    "The size %d of inputs must be equal to 2.", input_size);
  IR_ENFORCE((*this)->operand_source(0).type().isa<paddle::dialect::DenseTensorType>(),
                  "Type validation failed for the 0th input, got %s.", (*this)->operand_source(0).type());
  IR_ENFORCE((*this)->operand_source(1).type().isa<paddle::dialect::DenseTensorType>(),
                  "Type validation failed for the 1th input, got %s.", (*this)->operand_source(1).type());
  }
  VLOG(4) << "Verifying attributes:";
  {
  // Attributes num is 0, not need to check attributes type.
  }
  VLOG(4) << "Verifying outputs:";
  {
  auto output_size = num_results();
  IR_ENFORCE(output_size == 1u,
                    "The size %d of outputs must be equal to 1.", output_size);
  IR_ENFORCE((*this)->result(0).type().isa<paddle::dialect::DenseTensorType>(),
                 "Type validation failed for the 0th output.");
  }
  VLOG(4) << "End Verifying for: LogicalXor_Op.";
}

void LogicalXor_Op::InferMeta( phi::InferMetaContext *infer_meta ) {
  auto fn = PD_INFER_META(phi::LogicalBinaryInferMeta);
  fn(infer_meta);
}

phi::DataType LogicalXor_Op::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: LogicalXor_Op";
  


  return expected_kernel_dtype;
}

const char *LogitOp::attributes_name[1] = { "eps" };

OpInfoTuple LogitOp::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("x", "paddle::dialect::DenseTensorType", false, false, false, true) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = { paddle::dialect::OpAttributeInfo("eps", "pir::FloatAttribute", "") };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("out", "paddle::dialect::DenseTensorType", false, false) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("UnchangedInferMeta", {"x"}, "logit", {"x", "eps"}, {}, {}, {}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "logit");
}

void LogitOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, float eps) {
  VLOG(4) << "Start build LogitOp";



  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_eps = pir::FloatAttribute::get(pir::IrContext::Instance(), eps);
  argument.AddAttribute("eps", attr_eps);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);
  paddle::dialect::IrTensor dense_out;
  paddle::dialect::IrMetaTensor meta_out(&dense_out);

  phi::UnchangedInferMeta(meta_x, &meta_out);

  std::vector<pir::Type> argument_outputs;
  pir::Type out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_out.dtype()), dense_out.dims(), dense_out.layout(), dense_out.lod(), dense_out.offset());
  argument_outputs.push_back(out_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void LogitOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, pir::AttributeMap attributes) {
  VLOG(4) << "Start build LogitOp";


  IR_ENFORCE(
      attributes.find("eps") != attributes.end(),
          "'eps' Attribute is expected for LogitOp. ");
  float eps = attributes.at("eps").dyn_cast<pir::FloatAttribute>().data();


  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_eps = pir::FloatAttribute::get(pir::IrContext::Instance(), eps);
  argument.AddAttribute("eps", attr_eps);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);
  paddle::dialect::IrTensor dense_out;
  paddle::dialect::IrMetaTensor meta_out(&dense_out);

  phi::UnchangedInferMeta(meta_x, &meta_out);

  std::vector<pir::Type> argument_outputs;
  pir::Type out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_out.dtype()), dense_out.dims(), dense_out.layout(), dense_out.lod(), dense_out.offset());
  argument_outputs.push_back(out_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void LogitOp::VerifySig() {
  VLOG(4) << "Start Verifying inputs, outputs and attributes for: LogitOp.";
  VLOG(4) << "Verifying inputs:";
  {
  auto input_size = num_operands();
  IR_ENFORCE(input_size == 1u,
                    "The size %d of inputs must be equal to 1.", input_size);
  IR_ENFORCE((*this)->operand_source(0).type().isa<paddle::dialect::DenseTensorType>(),
                  "Type validation failed for the 0th input, got %s.", (*this)->operand_source(0).type());
  }
  VLOG(4) << "Verifying attributes:";
  {
  auto& attributes = this->attributes();
  IR_ENFORCE(attributes.count("eps")>0,
                 "eps does not exist.");
  IR_ENFORCE(attributes.at("eps").isa<pir::FloatAttribute>(),
                 "Type of attribute: eps is not pir::FloatAttribute.");

  }
  VLOG(4) << "Verifying outputs:";
  {
  auto output_size = num_results();
  IR_ENFORCE(output_size == 1u,
                    "The size %d of outputs must be equal to 1.", output_size);
  IR_ENFORCE((*this)->result(0).type().isa<paddle::dialect::DenseTensorType>(),
                 "Type validation failed for the 0th output.");
  }
  VLOG(4) << "End Verifying for: LogitOp.";
}

void LogitOp::InferMeta( phi::InferMetaContext *infer_meta ) {
  auto fn = PD_INFER_META(phi::UnchangedInferMeta);
  fn(infer_meta);
}

phi::DataType LogitOp::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: LogitOp";
  


  return expected_kernel_dtype;
}

const char *Logit_Op::attributes_name[1] = { "eps" };

OpInfoTuple Logit_Op::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("x", "paddle::dialect::DenseTensorType", false, false, false, true) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = { paddle::dialect::OpAttributeInfo("eps", "pir::FloatAttribute", "") };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("out", "paddle::dialect::DenseTensorType", false, false) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("UnchangedInferMeta", {"x"}, "logit", {"x", "eps"}, {}, {}, {{"out", "x"}}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "logit");
}

void Logit_Op::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, float eps) {
  VLOG(4) << "Start build Logit_Op";



  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_eps = pir::FloatAttribute::get(pir::IrContext::Instance(), eps);
  argument.AddAttribute("eps", attr_eps);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);
  paddle::dialect::IrTensor dense_out;
  paddle::dialect::IrMetaTensor meta_out(&dense_out);

  phi::UnchangedInferMeta(meta_x, &meta_out);

  std::vector<pir::Type> argument_outputs;
  pir::Type out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_out.dtype()), dense_out.dims(), dense_out.layout(), dense_out.lod(), dense_out.offset());
  argument_outputs.push_back(out_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void Logit_Op::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, pir::AttributeMap attributes) {
  VLOG(4) << "Start build Logit_Op";


  IR_ENFORCE(
      attributes.find("eps") != attributes.end(),
          "'eps' Attribute is expected for Logit_Op. ");
  float eps = attributes.at("eps").dyn_cast<pir::FloatAttribute>().data();


  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_eps = pir::FloatAttribute::get(pir::IrContext::Instance(), eps);
  argument.AddAttribute("eps", attr_eps);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);
  paddle::dialect::IrTensor dense_out;
  paddle::dialect::IrMetaTensor meta_out(&dense_out);

  phi::UnchangedInferMeta(meta_x, &meta_out);

  std::vector<pir::Type> argument_outputs;
  pir::Type out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_out.dtype()), dense_out.dims(), dense_out.layout(), dense_out.lod(), dense_out.offset());
  argument_outputs.push_back(out_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void Logit_Op::VerifySig() {
  VLOG(4) << "Start Verifying inputs, outputs and attributes for: Logit_Op.";
  VLOG(4) << "Verifying inputs:";
  {
  auto input_size = num_operands();
  IR_ENFORCE(input_size == 1u,
                    "The size %d of inputs must be equal to 1.", input_size);
  IR_ENFORCE((*this)->operand_source(0).type().isa<paddle::dialect::DenseTensorType>(),
                  "Type validation failed for the 0th input, got %s.", (*this)->operand_source(0).type());
  }
  VLOG(4) << "Verifying attributes:";
  {
  auto& attributes = this->attributes();
  IR_ENFORCE(attributes.count("eps")>0,
                 "eps does not exist.");
  IR_ENFORCE(attributes.at("eps").isa<pir::FloatAttribute>(),
                 "Type of attribute: eps is not pir::FloatAttribute.");

  }
  VLOG(4) << "Verifying outputs:";
  {
  auto output_size = num_results();
  IR_ENFORCE(output_size == 1u,
                    "The size %d of outputs must be equal to 1.", output_size);
  IR_ENFORCE((*this)->result(0).type().isa<paddle::dialect::DenseTensorType>(),
                 "Type validation failed for the 0th output.");
  }
  VLOG(4) << "End Verifying for: Logit_Op.";
}

void Logit_Op::InferMeta( phi::InferMetaContext *infer_meta ) {
  auto fn = PD_INFER_META(phi::UnchangedInferMeta);
  fn(infer_meta);
}

phi::DataType Logit_Op::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: Logit_Op";
  


  return expected_kernel_dtype;
}

OpInfoTuple LogsigmoidOp::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("x", "paddle::dialect::DenseTensorType", false, false, false, true) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = {  };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("out", "paddle::dialect::DenseTensorType", false, false) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("UnchangedInferMeta", {"x"}, "logsigmoid", {"x"}, {}, {}, {}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "logsigmoid");
}

void LogsigmoidOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_) {
  VLOG(4) << "Start build LogsigmoidOp";



  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);
  paddle::dialect::IrTensor dense_out;
  paddle::dialect::IrMetaTensor meta_out(&dense_out);

  phi::UnchangedInferMeta(meta_x, &meta_out);

  std::vector<pir::Type> argument_outputs;
  pir::Type out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_out.dtype()), dense_out.dims(), dense_out.layout(), dense_out.lod(), dense_out.offset());
  argument_outputs.push_back(out_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void LogsigmoidOp::VerifySig() {
  VLOG(4) << "Start Verifying inputs, outputs and attributes for: LogsigmoidOp.";
  VLOG(4) << "Verifying inputs:";
  {
  auto input_size = num_operands();
  IR_ENFORCE(input_size == 1u,
                    "The size %d of inputs must be equal to 1.", input_size);
  IR_ENFORCE((*this)->operand_source(0).type().isa<paddle::dialect::DenseTensorType>(),
                  "Type validation failed for the 0th input, got %s.", (*this)->operand_source(0).type());
  }
  VLOG(4) << "Verifying attributes:";
  {
  // Attributes num is 0, not need to check attributes type.
  }
  VLOG(4) << "Verifying outputs:";
  {
  auto output_size = num_results();
  IR_ENFORCE(output_size == 1u,
                    "The size %d of outputs must be equal to 1.", output_size);
  IR_ENFORCE((*this)->result(0).type().isa<paddle::dialect::DenseTensorType>(),
                 "Type validation failed for the 0th output.");
  }
  VLOG(4) << "End Verifying for: LogsigmoidOp.";
}

void LogsigmoidOp::InferMeta( phi::InferMetaContext *infer_meta ) {
  auto fn = PD_INFER_META(phi::UnchangedInferMeta);
  fn(infer_meta);
}

phi::DataType LogsigmoidOp::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: LogsigmoidOp";
  


  return expected_kernel_dtype;
}

const char *LstsqOp::attributes_name[1] = { "driver" };

OpInfoTuple LstsqOp::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("x", "paddle::dialect::DenseTensorType", false, false, false, false), paddle::dialect::OpInputInfo("y", "paddle::dialect::DenseTensorType", false, false, false, false), paddle::dialect::OpInputInfo("rcond", "paddle::dialect::ScalarAttribute", false, false, true, false) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = { paddle::dialect::OpAttributeInfo("driver", "pir::StrAttribute", "") };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("solution", "paddle::dialect::DenseTensorType", false, false), paddle::dialect::OpOutputInfo("residuals", "paddle::dialect::DenseTensorType", true, false), paddle::dialect::OpOutputInfo("rank", "paddle::dialect::DenseTensorType", false, false), paddle::dialect::OpOutputInfo("singular_values", "paddle::dialect::DenseTensorType", false, false) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("LstsqInferMeta", {"x", "y", "rcond", "driver"}, "lstsq", {"x", "y", "rcond", "driver"}, {"x"}, {}, {}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "lstsq");
}

void LstsqOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, pir::Value y_, float rcond, const std::string& driver) {
  VLOG(4) << "Start build LstsqOp";


  // Generate scalar mutable attribute: rcond
  paddle::dialect::FullOp full_rcond_op = builder.Build<paddle::dialect::FullOp>(std::vector<int64_t>{1}, rcond, phi::DataType::FLOAT32, phi::CPUPlace());
  pir::OpResult rcond_ = full_rcond_op->result(0);
    
  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_, y_, rcond_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_driver = pir::StrAttribute::get(pir::IrContext::Instance(), driver);
  argument.AddAttribute("driver", attr_driver);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;
  paddle::dialect::DenseTensorType y = y_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)y;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);

  VLOG(4) << "Builder construction  dense_y";
  paddle::dialect::IrTensor ir_tensor_y(paddle::dialect::TransToPhiDataType(y.dtype()),
                                                      y.dims(),
                                                      y.data_layout(),
                                                      y.lod(),
                                                      y.offset());
  VLOG(4) << "Builder construction  meta_y";
  paddle::dialect::IrMetaTensor meta_y(&ir_tensor_y);
  paddle::dialect::IrTensor dense_solution;
  paddle::dialect::IrMetaTensor meta_solution(&dense_solution);
  paddle::dialect::IrTensor dense_residuals;
  paddle::dialect::IrMetaTensor meta_residuals(&dense_residuals);
  paddle::dialect::IrTensor dense_rank;
  paddle::dialect::IrMetaTensor meta_rank(&dense_rank);
  paddle::dialect::IrTensor dense_singular_values;
  paddle::dialect::IrMetaTensor meta_singular_values(&dense_singular_values);

  phi::LstsqInferMeta(meta_x, meta_y, rcond, driver, &meta_solution, &meta_residuals, &meta_rank, &meta_singular_values);

  std::vector<pir::Type> argument_outputs;
  pir::Type solution_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_solution.dtype()), dense_solution.dims(), dense_solution.layout(), dense_solution.lod(), dense_solution.offset());
  argument_outputs.push_back(solution_dense_tensor_type);

  pir::Type residuals_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_residuals.dtype()), dense_residuals.dims(), dense_residuals.layout(), dense_residuals.lod(), dense_residuals.offset());
  argument_outputs.push_back(residuals_dense_tensor_type);

  pir::Type rank_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_rank.dtype()), dense_rank.dims(), dense_rank.layout(), dense_rank.lod(), dense_rank.offset());
  argument_outputs.push_back(rank_dense_tensor_type);

  pir::Type singular_values_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_singular_values.dtype()), dense_singular_values.dims(), dense_singular_values.layout(), dense_singular_values.lod(), dense_singular_values.offset());
  argument_outputs.push_back(singular_values_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void LstsqOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, pir::Value y_, pir::AttributeMap attributes) {
  VLOG(4) << "Start build LstsqOp";


  IR_ENFORCE(
      attributes.find("rcond") != attributes.end(),
          "'rcond' Attribute is expected for LstsqOp. ");
  float rcond = attributes.at("rcond").dyn_cast<paddle::dialect::ScalarAttribute>().data().to<float>();

  IR_ENFORCE(
      attributes.find("driver") != attributes.end(),
          "'driver' Attribute is expected for LstsqOp. ");
  std::string driver = attributes.at("driver").dyn_cast<pir::StrAttribute>().AsString();

  // Generate scalar mutable attribute: rcond
  paddle::dialect::FullOp full_rcond_op = builder.Build<paddle::dialect::FullOp>(std::vector<int64_t>{1}, rcond, phi::DataType::FLOAT32, phi::CPUPlace());
  pir::OpResult rcond_ = full_rcond_op->result(0);
    
  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_, y_, rcond_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_driver = pir::StrAttribute::get(pir::IrContext::Instance(), driver);
  argument.AddAttribute("driver", attr_driver);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;
  paddle::dialect::DenseTensorType y = y_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)y;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);

  VLOG(4) << "Builder construction  dense_y";
  paddle::dialect::IrTensor ir_tensor_y(paddle::dialect::TransToPhiDataType(y.dtype()),
                                                      y.dims(),
                                                      y.data_layout(),
                                                      y.lod(),
                                                      y.offset());
  VLOG(4) << "Builder construction  meta_y";
  paddle::dialect::IrMetaTensor meta_y(&ir_tensor_y);
  paddle::dialect::IrTensor dense_solution;
  paddle::dialect::IrMetaTensor meta_solution(&dense_solution);
  paddle::dialect::IrTensor dense_residuals;
  paddle::dialect::IrMetaTensor meta_residuals(&dense_residuals);
  paddle::dialect::IrTensor dense_rank;
  paddle::dialect::IrMetaTensor meta_rank(&dense_rank);
  paddle::dialect::IrTensor dense_singular_values;
  paddle::dialect::IrMetaTensor meta_singular_values(&dense_singular_values);

  phi::LstsqInferMeta(meta_x, meta_y, rcond, driver, &meta_solution, &meta_residuals, &meta_rank, &meta_singular_values);

  std::vector<pir::Type> argument_outputs;
  pir::Type solution_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_solution.dtype()), dense_solution.dims(), dense_solution.layout(), dense_solution.lod(), dense_solution.offset());
  argument_outputs.push_back(solution_dense_tensor_type);

  pir::Type residuals_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_residuals.dtype()), dense_residuals.dims(), dense_residuals.layout(), dense_residuals.lod(), dense_residuals.offset());
  argument_outputs.push_back(residuals_dense_tensor_type);

  pir::Type rank_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_rank.dtype()), dense_rank.dims(), dense_rank.layout(), dense_rank.lod(), dense_rank.offset());
  argument_outputs.push_back(rank_dense_tensor_type);

  pir::Type singular_values_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_singular_values.dtype()), dense_singular_values.dims(), dense_singular_values.layout(), dense_singular_values.lod(), dense_singular_values.offset());
  argument_outputs.push_back(singular_values_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void LstsqOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, pir::Value y_, pir::Value rcond_, const std::string& driver) {
  VLOG(4) << "Start build LstsqOp";



  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_, y_, rcond_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_driver = pir::StrAttribute::get(pir::IrContext::Instance(), driver);
  argument.AddAttribute("driver", attr_driver);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;
  paddle::dialect::DenseTensorType y = y_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)y;
  phi::Scalar rcond;
  if (rcond_.dyn_cast<pir::OpResult>() && rcond_.dyn_cast<pir::OpResult>().owner()->isa<paddle::dialect::FullOp>()) {
    rcond = std::move(phi::Scalar(rcond_.dyn_cast<pir::OpResult>().owner()
                                  ->dyn_cast<paddle::dialect::FullOp>()
                                  .attribute("value")
                                  .dyn_cast<paddle::dialect::ScalarAttribute>()
                                  .data()
                                  .to<int>()));
  }
  else {
    rcond = std::move(phi::Scalar(-1));
    rcond.SetFromTensor(true);
  }


  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);

  VLOG(4) << "Builder construction  dense_y";
  paddle::dialect::IrTensor ir_tensor_y(paddle::dialect::TransToPhiDataType(y.dtype()),
                                                      y.dims(),
                                                      y.data_layout(),
                                                      y.lod(),
                                                      y.offset());
  VLOG(4) << "Builder construction  meta_y";
  paddle::dialect::IrMetaTensor meta_y(&ir_tensor_y);
  paddle::dialect::IrTensor dense_solution;
  paddle::dialect::IrMetaTensor meta_solution(&dense_solution);
  paddle::dialect::IrTensor dense_residuals;
  paddle::dialect::IrMetaTensor meta_residuals(&dense_residuals);
  paddle::dialect::IrTensor dense_rank;
  paddle::dialect::IrMetaTensor meta_rank(&dense_rank);
  paddle::dialect::IrTensor dense_singular_values;
  paddle::dialect::IrMetaTensor meta_singular_values(&dense_singular_values);

  phi::LstsqInferMeta(meta_x, meta_y, rcond, driver, &meta_solution, &meta_residuals, &meta_rank, &meta_singular_values);

  std::vector<pir::Type> argument_outputs;
  pir::Type solution_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_solution.dtype()), dense_solution.dims(), dense_solution.layout(), dense_solution.lod(), dense_solution.offset());
  argument_outputs.push_back(solution_dense_tensor_type);

  pir::Type residuals_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_residuals.dtype()), dense_residuals.dims(), dense_residuals.layout(), dense_residuals.lod(), dense_residuals.offset());
  argument_outputs.push_back(residuals_dense_tensor_type);

  pir::Type rank_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_rank.dtype()), dense_rank.dims(), dense_rank.layout(), dense_rank.lod(), dense_rank.offset());
  argument_outputs.push_back(rank_dense_tensor_type);

  pir::Type singular_values_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_singular_values.dtype()), dense_singular_values.dims(), dense_singular_values.layout(), dense_singular_values.lod(), dense_singular_values.offset());
  argument_outputs.push_back(singular_values_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void LstsqOp::VerifySig() {
  VLOG(4) << "Start Verifying inputs, outputs and attributes for: LstsqOp.";
  VLOG(4) << "Verifying inputs:";
  {
  auto input_size = num_operands();
  IR_ENFORCE(input_size == 3u,
                    "The size %d of inputs must be equal to 3.", input_size);
  IR_ENFORCE((*this)->operand_source(0).type().isa<paddle::dialect::DenseTensorType>(),
                  "Type validation failed for the 0th input, got %s.", (*this)->operand_source(0).type());
  IR_ENFORCE((*this)->operand_source(1).type().isa<paddle::dialect::DenseTensorType>(),
                  "Type validation failed for the 1th input, got %s.", (*this)->operand_source(1).type());
  IR_ENFORCE((*this)->operand_source(2).type().isa<paddle::dialect::DenseTensorType>(),
                  "Type validation failed for the 2th input, got %s.", (*this)->operand_source(2).type());
  }
  VLOG(4) << "Verifying attributes:";
  {
  auto& attributes = this->attributes();
  IR_ENFORCE(attributes.count("driver")>0,
                 "driver does not exist.");
  IR_ENFORCE(attributes.at("driver").isa<pir::StrAttribute>(),
                 "Type of attribute: driver is not pir::StrAttribute.");

  }
  VLOG(4) << "Verifying outputs:";
  {
  auto output_size = num_results();
  IR_ENFORCE(output_size == 4u,
                    "The size %d of outputs must be equal to 4.", output_size);
  IR_ENFORCE((*this)->result(0).type().isa<paddle::dialect::DenseTensorType>(),
                 "Type validation failed for the 0th output.");
  if (auto output_1_type = (*this)->result(1).type()) {
    IR_ENFORCE(output_1_type.isa<paddle::dialect::DenseTensorType>(),
                   "Type validation failed for the 1th output.");
  }
  IR_ENFORCE((*this)->result(2).type().isa<paddle::dialect::DenseTensorType>(),
                 "Type validation failed for the 2th output.");
  IR_ENFORCE((*this)->result(3).type().isa<paddle::dialect::DenseTensorType>(),
                 "Type validation failed for the 3th output.");
  }
  VLOG(4) << "End Verifying for: LstsqOp.";
}

void LstsqOp::InferMeta( phi::InferMetaContext *infer_meta ) {
  auto fn = PD_INFER_META(phi::LstsqInferMeta);
  fn(infer_meta);
}

phi::DataType LstsqOp::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: LstsqOp";
  


  return expected_kernel_dtype;
}

const char *LuOp::attributes_name[1] = { "pivot" };

OpInfoTuple LuOp::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("x", "paddle::dialect::DenseTensorType", false, false, false, true) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = { paddle::dialect::OpAttributeInfo("pivot", "pir::BoolAttribute", "") };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("out", "paddle::dialect::DenseTensorType", false, false), paddle::dialect::OpOutputInfo("pivots", "paddle::dialect::DenseTensorType", false, false), paddle::dialect::OpOutputInfo("infos", "paddle::dialect::DenseTensorType", false, false) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("LUInferMeta", {"x", "pivot"}, "lu", {"x", "pivot"}, {"x"}, {}, {}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "lu");
}

void LuOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, bool pivot) {
  VLOG(4) << "Start build LuOp";



  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_pivot = pir::BoolAttribute::get(pir::IrContext::Instance(), pivot);
  argument.AddAttribute("pivot", attr_pivot);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);
  paddle::dialect::IrTensor dense_out;
  paddle::dialect::IrMetaTensor meta_out(&dense_out);
  paddle::dialect::IrTensor dense_pivots;
  paddle::dialect::IrMetaTensor meta_pivots(&dense_pivots);
  paddle::dialect::IrTensor dense_infos;
  paddle::dialect::IrMetaTensor meta_infos(&dense_infos);

  phi::LUInferMeta(meta_x, pivot, &meta_out, &meta_pivots, &meta_infos);

  std::vector<pir::Type> argument_outputs;
  pir::Type out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_out.dtype()), dense_out.dims(), dense_out.layout(), dense_out.lod(), dense_out.offset());
  argument_outputs.push_back(out_dense_tensor_type);

  pir::Type pivots_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_pivots.dtype()), dense_pivots.dims(), dense_pivots.layout(), dense_pivots.lod(), dense_pivots.offset());
  argument_outputs.push_back(pivots_dense_tensor_type);

  pir::Type infos_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_infos.dtype()), dense_infos.dims(), dense_infos.layout(), dense_infos.lod(), dense_infos.offset());
  argument_outputs.push_back(infos_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void LuOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, pir::AttributeMap attributes) {
  VLOG(4) << "Start build LuOp";


  IR_ENFORCE(
      attributes.find("pivot") != attributes.end(),
          "'pivot' Attribute is expected for LuOp. ");
  bool pivot = attributes.at("pivot").dyn_cast<pir::BoolAttribute>().data();


  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_pivot = pir::BoolAttribute::get(pir::IrContext::Instance(), pivot);
  argument.AddAttribute("pivot", attr_pivot);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);
  paddle::dialect::IrTensor dense_out;
  paddle::dialect::IrMetaTensor meta_out(&dense_out);
  paddle::dialect::IrTensor dense_pivots;
  paddle::dialect::IrMetaTensor meta_pivots(&dense_pivots);
  paddle::dialect::IrTensor dense_infos;
  paddle::dialect::IrMetaTensor meta_infos(&dense_infos);

  phi::LUInferMeta(meta_x, pivot, &meta_out, &meta_pivots, &meta_infos);

  std::vector<pir::Type> argument_outputs;
  pir::Type out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_out.dtype()), dense_out.dims(), dense_out.layout(), dense_out.lod(), dense_out.offset());
  argument_outputs.push_back(out_dense_tensor_type);

  pir::Type pivots_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_pivots.dtype()), dense_pivots.dims(), dense_pivots.layout(), dense_pivots.lod(), dense_pivots.offset());
  argument_outputs.push_back(pivots_dense_tensor_type);

  pir::Type infos_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_infos.dtype()), dense_infos.dims(), dense_infos.layout(), dense_infos.lod(), dense_infos.offset());
  argument_outputs.push_back(infos_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void LuOp::VerifySig() {
  VLOG(4) << "Start Verifying inputs, outputs and attributes for: LuOp.";
  VLOG(4) << "Verifying inputs:";
  {
  auto input_size = num_operands();
  IR_ENFORCE(input_size == 1u,
                    "The size %d of inputs must be equal to 1.", input_size);
  IR_ENFORCE((*this)->operand_source(0).type().isa<paddle::dialect::DenseTensorType>(),
                  "Type validation failed for the 0th input, got %s.", (*this)->operand_source(0).type());
  }
  VLOG(4) << "Verifying attributes:";
  {
  auto& attributes = this->attributes();
  IR_ENFORCE(attributes.count("pivot")>0,
                 "pivot does not exist.");
  IR_ENFORCE(attributes.at("pivot").isa<pir::BoolAttribute>(),
                 "Type of attribute: pivot is not pir::BoolAttribute.");

  }
  VLOG(4) << "Verifying outputs:";
  {
  auto output_size = num_results();
  IR_ENFORCE(output_size == 3u,
                    "The size %d of outputs must be equal to 3.", output_size);
  IR_ENFORCE((*this)->result(0).type().isa<paddle::dialect::DenseTensorType>(),
                 "Type validation failed for the 0th output.");
  IR_ENFORCE((*this)->result(1).type().isa<paddle::dialect::DenseTensorType>(),
                 "Type validation failed for the 1th output.");
  IR_ENFORCE((*this)->result(2).type().isa<paddle::dialect::DenseTensorType>(),
                 "Type validation failed for the 2th output.");
  }
  VLOG(4) << "End Verifying for: LuOp.";
}

void LuOp::InferMeta( phi::InferMetaContext *infer_meta ) {
  auto fn = PD_INFER_META(phi::LUInferMeta);
  fn(infer_meta);
}

phi::DataType LuOp::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: LuOp";
  


  return expected_kernel_dtype;
}

const char *Lu_Op::attributes_name[1] = { "pivot" };

OpInfoTuple Lu_Op::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("x", "paddle::dialect::DenseTensorType", false, false, false, true) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = { paddle::dialect::OpAttributeInfo("pivot", "pir::BoolAttribute", "") };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("out", "paddle::dialect::DenseTensorType", false, false), paddle::dialect::OpOutputInfo("pivots", "paddle::dialect::DenseTensorType", false, false), paddle::dialect::OpOutputInfo("infos", "paddle::dialect::DenseTensorType", false, false) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("LUInferMeta", {"x", "pivot"}, "lu", {"x", "pivot"}, {"x"}, {}, {{"out", "x"}}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "lu");
}

void Lu_Op::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, bool pivot) {
  VLOG(4) << "Start build Lu_Op";



  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_pivot = pir::BoolAttribute::get(pir::IrContext::Instance(), pivot);
  argument.AddAttribute("pivot", attr_pivot);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);
  paddle::dialect::IrTensor dense_out;
  paddle::dialect::IrMetaTensor meta_out(&dense_out);
  paddle::dialect::IrTensor dense_pivots;
  paddle::dialect::IrMetaTensor meta_pivots(&dense_pivots);
  paddle::dialect::IrTensor dense_infos;
  paddle::dialect::IrMetaTensor meta_infos(&dense_infos);

  phi::LUInferMeta(meta_x, pivot, &meta_out, &meta_pivots, &meta_infos);

  std::vector<pir::Type> argument_outputs;
  pir::Type out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_out.dtype()), dense_out.dims(), dense_out.layout(), dense_out.lod(), dense_out.offset());
  argument_outputs.push_back(out_dense_tensor_type);

  pir::Type pivots_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_pivots.dtype()), dense_pivots.dims(), dense_pivots.layout(), dense_pivots.lod(), dense_pivots.offset());
  argument_outputs.push_back(pivots_dense_tensor_type);

  pir::Type infos_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_infos.dtype()), dense_infos.dims(), dense_infos.layout(), dense_infos.lod(), dense_infos.offset());
  argument_outputs.push_back(infos_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void Lu_Op::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, pir::AttributeMap attributes) {
  VLOG(4) << "Start build Lu_Op";


  IR_ENFORCE(
      attributes.find("pivot") != attributes.end(),
          "'pivot' Attribute is expected for Lu_Op. ");
  bool pivot = attributes.at("pivot").dyn_cast<pir::BoolAttribute>().data();


  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_pivot = pir::BoolAttribute::get(pir::IrContext::Instance(), pivot);
  argument.AddAttribute("pivot", attr_pivot);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);
  paddle::dialect::IrTensor dense_out;
  paddle::dialect::IrMetaTensor meta_out(&dense_out);
  paddle::dialect::IrTensor dense_pivots;
  paddle::dialect::IrMetaTensor meta_pivots(&dense_pivots);
  paddle::dialect::IrTensor dense_infos;
  paddle::dialect::IrMetaTensor meta_infos(&dense_infos);

  phi::LUInferMeta(meta_x, pivot, &meta_out, &meta_pivots, &meta_infos);

  std::vector<pir::Type> argument_outputs;
  pir::Type out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_out.dtype()), dense_out.dims(), dense_out.layout(), dense_out.lod(), dense_out.offset());
  argument_outputs.push_back(out_dense_tensor_type);

  pir::Type pivots_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_pivots.dtype()), dense_pivots.dims(), dense_pivots.layout(), dense_pivots.lod(), dense_pivots.offset());
  argument_outputs.push_back(pivots_dense_tensor_type);

  pir::Type infos_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_infos.dtype()), dense_infos.dims(), dense_infos.layout(), dense_infos.lod(), dense_infos.offset());
  argument_outputs.push_back(infos_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void Lu_Op::VerifySig() {
  VLOG(4) << "Start Verifying inputs, outputs and attributes for: Lu_Op.";
  VLOG(4) << "Verifying inputs:";
  {
  auto input_size = num_operands();
  IR_ENFORCE(input_size == 1u,
                    "The size %d of inputs must be equal to 1.", input_size);
  IR_ENFORCE((*this)->operand_source(0).type().isa<paddle::dialect::DenseTensorType>(),
                  "Type validation failed for the 0th input, got %s.", (*this)->operand_source(0).type());
  }
  VLOG(4) << "Verifying attributes:";
  {
  auto& attributes = this->attributes();
  IR_ENFORCE(attributes.count("pivot")>0,
                 "pivot does not exist.");
  IR_ENFORCE(attributes.at("pivot").isa<pir::BoolAttribute>(),
                 "Type of attribute: pivot is not pir::BoolAttribute.");

  }
  VLOG(4) << "Verifying outputs:";
  {
  auto output_size = num_results();
  IR_ENFORCE(output_size == 3u,
                    "The size %d of outputs must be equal to 3.", output_size);
  IR_ENFORCE((*this)->result(0).type().isa<paddle::dialect::DenseTensorType>(),
                 "Type validation failed for the 0th output.");
  IR_ENFORCE((*this)->result(1).type().isa<paddle::dialect::DenseTensorType>(),
                 "Type validation failed for the 1th output.");
  IR_ENFORCE((*this)->result(2).type().isa<paddle::dialect::DenseTensorType>(),
                 "Type validation failed for the 2th output.");
  }
  VLOG(4) << "End Verifying for: Lu_Op.";
}

void Lu_Op::InferMeta( phi::InferMetaContext *infer_meta ) {
  auto fn = PD_INFER_META(phi::LUInferMeta);
  fn(infer_meta);
}

phi::DataType Lu_Op::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: Lu_Op";
  


  return expected_kernel_dtype;
}

const char *LuUnpackOp::attributes_name[2] = { "unpack_ludata", "unpack_pivots" };

OpInfoTuple LuUnpackOp::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("x", "paddle::dialect::DenseTensorType", false, false, false, true), paddle::dialect::OpInputInfo("y", "paddle::dialect::DenseTensorType", false, false, false, false) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = { paddle::dialect::OpAttributeInfo("unpack_ludata", "pir::BoolAttribute", ""), paddle::dialect::OpAttributeInfo("unpack_pivots", "pir::BoolAttribute", "") };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("pmat", "paddle::dialect::DenseTensorType", false, false), paddle::dialect::OpOutputInfo("l", "paddle::dialect::DenseTensorType", false, false), paddle::dialect::OpOutputInfo("u", "paddle::dialect::DenseTensorType", false, false) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("LUUnpackInferMeta", {"x", "y", "unpack_ludata", "unpack_pivots"}, "lu_unpack", {"x", "y", "unpack_ludata", "unpack_pivots"}, {"x"}, {}, {}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "lu_unpack");
}

void LuUnpackOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, pir::Value y_, bool unpack_ludata, bool unpack_pivots) {
  VLOG(4) << "Start build LuUnpackOp";



  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_, y_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_unpack_ludata = pir::BoolAttribute::get(pir::IrContext::Instance(), unpack_ludata);
  argument.AddAttribute("unpack_ludata", attr_unpack_ludata);
  pir::Attribute attr_unpack_pivots = pir::BoolAttribute::get(pir::IrContext::Instance(), unpack_pivots);
  argument.AddAttribute("unpack_pivots", attr_unpack_pivots);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;
  paddle::dialect::DenseTensorType y = y_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)y;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);

  VLOG(4) << "Builder construction  dense_y";
  paddle::dialect::IrTensor ir_tensor_y(paddle::dialect::TransToPhiDataType(y.dtype()),
                                                      y.dims(),
                                                      y.data_layout(),
                                                      y.lod(),
                                                      y.offset());
  VLOG(4) << "Builder construction  meta_y";
  paddle::dialect::IrMetaTensor meta_y(&ir_tensor_y);
  paddle::dialect::IrTensor dense_pmat;
  paddle::dialect::IrMetaTensor meta_pmat(&dense_pmat);
  paddle::dialect::IrTensor dense_l;
  paddle::dialect::IrMetaTensor meta_l(&dense_l);
  paddle::dialect::IrTensor dense_u;
  paddle::dialect::IrMetaTensor meta_u(&dense_u);

  phi::LUUnpackInferMeta(meta_x, meta_y, unpack_ludata, unpack_pivots, &meta_pmat, &meta_l, &meta_u);

  std::vector<pir::Type> argument_outputs;
  pir::Type pmat_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_pmat.dtype()), dense_pmat.dims(), dense_pmat.layout(), dense_pmat.lod(), dense_pmat.offset());
  argument_outputs.push_back(pmat_dense_tensor_type);

  pir::Type l_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_l.dtype()), dense_l.dims(), dense_l.layout(), dense_l.lod(), dense_l.offset());
  argument_outputs.push_back(l_dense_tensor_type);

  pir::Type u_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_u.dtype()), dense_u.dims(), dense_u.layout(), dense_u.lod(), dense_u.offset());
  argument_outputs.push_back(u_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void LuUnpackOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, pir::Value y_, pir::AttributeMap attributes) {
  VLOG(4) << "Start build LuUnpackOp";


  IR_ENFORCE(
      attributes.find("unpack_ludata") != attributes.end(),
          "'unpack_ludata' Attribute is expected for LuUnpackOp. ");
  bool unpack_ludata = attributes.at("unpack_ludata").dyn_cast<pir::BoolAttribute>().data();

  IR_ENFORCE(
      attributes.find("unpack_pivots") != attributes.end(),
          "'unpack_pivots' Attribute is expected for LuUnpackOp. ");
  bool unpack_pivots = attributes.at("unpack_pivots").dyn_cast<pir::BoolAttribute>().data();


  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_, y_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_unpack_ludata = pir::BoolAttribute::get(pir::IrContext::Instance(), unpack_ludata);
  argument.AddAttribute("unpack_ludata", attr_unpack_ludata);
  pir::Attribute attr_unpack_pivots = pir::BoolAttribute::get(pir::IrContext::Instance(), unpack_pivots);
  argument.AddAttribute("unpack_pivots", attr_unpack_pivots);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;
  paddle::dialect::DenseTensorType y = y_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)y;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);

  VLOG(4) << "Builder construction  dense_y";
  paddle::dialect::IrTensor ir_tensor_y(paddle::dialect::TransToPhiDataType(y.dtype()),
                                                      y.dims(),
                                                      y.data_layout(),
                                                      y.lod(),
                                                      y.offset());
  VLOG(4) << "Builder construction  meta_y";
  paddle::dialect::IrMetaTensor meta_y(&ir_tensor_y);
  paddle::dialect::IrTensor dense_pmat;
  paddle::dialect::IrMetaTensor meta_pmat(&dense_pmat);
  paddle::dialect::IrTensor dense_l;
  paddle::dialect::IrMetaTensor meta_l(&dense_l);
  paddle::dialect::IrTensor dense_u;
  paddle::dialect::IrMetaTensor meta_u(&dense_u);

  phi::LUUnpackInferMeta(meta_x, meta_y, unpack_ludata, unpack_pivots, &meta_pmat, &meta_l, &meta_u);

  std::vector<pir::Type> argument_outputs;
  pir::Type pmat_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_pmat.dtype()), dense_pmat.dims(), dense_pmat.layout(), dense_pmat.lod(), dense_pmat.offset());
  argument_outputs.push_back(pmat_dense_tensor_type);

  pir::Type l_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_l.dtype()), dense_l.dims(), dense_l.layout(), dense_l.lod(), dense_l.offset());
  argument_outputs.push_back(l_dense_tensor_type);

  pir::Type u_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_u.dtype()), dense_u.dims(), dense_u.layout(), dense_u.lod(), dense_u.offset());
  argument_outputs.push_back(u_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void LuUnpackOp::VerifySig() {
  VLOG(4) << "Start Verifying inputs, outputs and attributes for: LuUnpackOp.";
  VLOG(4) << "Verifying inputs:";
  {
  auto input_size = num_operands();
  IR_ENFORCE(input_size == 2u,
                    "The size %d of inputs must be equal to 2.", input_size);
  IR_ENFORCE((*this)->operand_source(0).type().isa<paddle::dialect::DenseTensorType>(),
                  "Type validation failed for the 0th input, got %s.", (*this)->operand_source(0).type());
  IR_ENFORCE((*this)->operand_source(1).type().isa<paddle::dialect::DenseTensorType>(),
                  "Type validation failed for the 1th input, got %s.", (*this)->operand_source(1).type());
  }
  VLOG(4) << "Verifying attributes:";
  {
  auto& attributes = this->attributes();
  IR_ENFORCE(attributes.count("unpack_ludata")>0,
                 "unpack_ludata does not exist.");
  IR_ENFORCE(attributes.at("unpack_ludata").isa<pir::BoolAttribute>(),
                 "Type of attribute: unpack_ludata is not pir::BoolAttribute.");

  IR_ENFORCE(attributes.count("unpack_pivots")>0,
                 "unpack_pivots does not exist.");
  IR_ENFORCE(attributes.at("unpack_pivots").isa<pir::BoolAttribute>(),
                 "Type of attribute: unpack_pivots is not pir::BoolAttribute.");

  }
  VLOG(4) << "Verifying outputs:";
  {
  auto output_size = num_results();
  IR_ENFORCE(output_size == 3u,
                    "The size %d of outputs must be equal to 3.", output_size);
  IR_ENFORCE((*this)->result(0).type().isa<paddle::dialect::DenseTensorType>(),
                 "Type validation failed for the 0th output.");
  IR_ENFORCE((*this)->result(1).type().isa<paddle::dialect::DenseTensorType>(),
                 "Type validation failed for the 1th output.");
  IR_ENFORCE((*this)->result(2).type().isa<paddle::dialect::DenseTensorType>(),
                 "Type validation failed for the 2th output.");
  }
  VLOG(4) << "End Verifying for: LuUnpackOp.";
}

void LuUnpackOp::InferMeta( phi::InferMetaContext *infer_meta ) {
  auto fn = PD_INFER_META(phi::LUUnpackInferMeta);
  fn(infer_meta);
}

phi::DataType LuUnpackOp::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: LuUnpackOp";
  


  return expected_kernel_dtype;
}

const char *MarginCrossEntropyOp::attributes_name[8] = { "return_softmax", "ring_id", "rank", "nranks", "margin1", "margin2", "margin3", "scale" };

OpInfoTuple MarginCrossEntropyOp::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("logits", "paddle::dialect::DenseTensorType", false, false, false, true), paddle::dialect::OpInputInfo("label", "paddle::dialect::DenseTensorType", false, false, false, false) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = { paddle::dialect::OpAttributeInfo("return_softmax", "pir::BoolAttribute", ""), paddle::dialect::OpAttributeInfo("ring_id", "pir::Int32Attribute", ""), paddle::dialect::OpAttributeInfo("rank", "pir::Int32Attribute", ""), paddle::dialect::OpAttributeInfo("nranks", "pir::Int32Attribute", ""), paddle::dialect::OpAttributeInfo("margin1", "pir::FloatAttribute", ""), paddle::dialect::OpAttributeInfo("margin2", "pir::FloatAttribute", ""), paddle::dialect::OpAttributeInfo("margin3", "pir::FloatAttribute", ""), paddle::dialect::OpAttributeInfo("scale", "pir::FloatAttribute", "") };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("softmax", "paddle::dialect::DenseTensorType", false, false), paddle::dialect::OpOutputInfo("loss", "paddle::dialect::DenseTensorType", false, false) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("MarginCrossEntropyInferMeta", {"logits", "label", "return_softmax", "ring_id", "rank", "nranks", "margin1", "margin2", "margin3", "scale"}, "margin_cross_entropy", {"logits", "label", "return_softmax", "ring_id", "rank", "nranks", "margin1", "margin2", "margin3", "scale"}, {"logits"}, {}, {}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "margin_cross_entropy");
}

void MarginCrossEntropyOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value logits_, pir::Value label_, bool return_softmax, int ring_id, int rank, int nranks, float margin1, float margin2, float margin3, float scale) {
  VLOG(4) << "Start build MarginCrossEntropyOp";



  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {logits_, label_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_return_softmax = pir::BoolAttribute::get(pir::IrContext::Instance(), return_softmax);
  argument.AddAttribute("return_softmax", attr_return_softmax);
  pir::Attribute attr_ring_id = pir::Int32Attribute::get(pir::IrContext::Instance(), ring_id);
  argument.AddAttribute("ring_id", attr_ring_id);
  pir::Attribute attr_rank = pir::Int32Attribute::get(pir::IrContext::Instance(), rank);
  argument.AddAttribute("rank", attr_rank);
  pir::Attribute attr_nranks = pir::Int32Attribute::get(pir::IrContext::Instance(), nranks);
  argument.AddAttribute("nranks", attr_nranks);
  pir::Attribute attr_margin1 = pir::FloatAttribute::get(pir::IrContext::Instance(), margin1);
  argument.AddAttribute("margin1", attr_margin1);
  pir::Attribute attr_margin2 = pir::FloatAttribute::get(pir::IrContext::Instance(), margin2);
  argument.AddAttribute("margin2", attr_margin2);
  pir::Attribute attr_margin3 = pir::FloatAttribute::get(pir::IrContext::Instance(), margin3);
  argument.AddAttribute("margin3", attr_margin3);
  pir::Attribute attr_scale = pir::FloatAttribute::get(pir::IrContext::Instance(), scale);
  argument.AddAttribute("scale", attr_scale);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType logits = logits_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)logits;
  paddle::dialect::DenseTensorType label = label_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)label;

  VLOG(4) << "Builder construction  dense_logits";
  paddle::dialect::IrTensor ir_tensor_logits(paddle::dialect::TransToPhiDataType(logits.dtype()),
                                                      logits.dims(),
                                                      logits.data_layout(),
                                                      logits.lod(),
                                                      logits.offset());
  VLOG(4) << "Builder construction  meta_logits";
  paddle::dialect::IrMetaTensor meta_logits(&ir_tensor_logits);

  VLOG(4) << "Builder construction  dense_label";
  paddle::dialect::IrTensor ir_tensor_label(paddle::dialect::TransToPhiDataType(label.dtype()),
                                                      label.dims(),
                                                      label.data_layout(),
                                                      label.lod(),
                                                      label.offset());
  VLOG(4) << "Builder construction  meta_label";
  paddle::dialect::IrMetaTensor meta_label(&ir_tensor_label);
  paddle::dialect::IrTensor dense_softmax;
  paddle::dialect::IrMetaTensor meta_softmax(&dense_softmax);
  paddle::dialect::IrTensor dense_loss;
  paddle::dialect::IrMetaTensor meta_loss(&dense_loss);

  phi::MarginCrossEntropyInferMeta(meta_logits, meta_label, return_softmax, ring_id, rank, nranks, margin1, margin2, margin3, scale, &meta_softmax, &meta_loss);

  std::vector<pir::Type> argument_outputs;
  pir::Type softmax_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_softmax.dtype()), dense_softmax.dims(), dense_softmax.layout(), dense_softmax.lod(), dense_softmax.offset());
  argument_outputs.push_back(softmax_dense_tensor_type);

  pir::Type loss_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_loss.dtype()), dense_loss.dims(), dense_loss.layout(), dense_loss.lod(), dense_loss.offset());
  argument_outputs.push_back(loss_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void MarginCrossEntropyOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value logits_, pir::Value label_, pir::AttributeMap attributes) {
  VLOG(4) << "Start build MarginCrossEntropyOp";


  IR_ENFORCE(
      attributes.find("return_softmax") != attributes.end(),
          "'return_softmax' Attribute is expected for MarginCrossEntropyOp. ");
  bool return_softmax = attributes.at("return_softmax").dyn_cast<pir::BoolAttribute>().data();

  IR_ENFORCE(
      attributes.find("ring_id") != attributes.end(),
          "'ring_id' Attribute is expected for MarginCrossEntropyOp. ");
  int ring_id = attributes.at("ring_id").dyn_cast<pir::Int32Attribute>().data();

  IR_ENFORCE(
      attributes.find("rank") != attributes.end(),
          "'rank' Attribute is expected for MarginCrossEntropyOp. ");
  int rank = attributes.at("rank").dyn_cast<pir::Int32Attribute>().data();

  IR_ENFORCE(
      attributes.find("nranks") != attributes.end(),
          "'nranks' Attribute is expected for MarginCrossEntropyOp. ");
  int nranks = attributes.at("nranks").dyn_cast<pir::Int32Attribute>().data();

  IR_ENFORCE(
      attributes.find("margin1") != attributes.end(),
          "'margin1' Attribute is expected for MarginCrossEntropyOp. ");
  float margin1 = attributes.at("margin1").dyn_cast<pir::FloatAttribute>().data();

  IR_ENFORCE(
      attributes.find("margin2") != attributes.end(),
          "'margin2' Attribute is expected for MarginCrossEntropyOp. ");
  float margin2 = attributes.at("margin2").dyn_cast<pir::FloatAttribute>().data();

  IR_ENFORCE(
      attributes.find("margin3") != attributes.end(),
          "'margin3' Attribute is expected for MarginCrossEntropyOp. ");
  float margin3 = attributes.at("margin3").dyn_cast<pir::FloatAttribute>().data();

  IR_ENFORCE(
      attributes.find("scale") != attributes.end(),
          "'scale' Attribute is expected for MarginCrossEntropyOp. ");
  float scale = attributes.at("scale").dyn_cast<pir::FloatAttribute>().data();


  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {logits_, label_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_return_softmax = pir::BoolAttribute::get(pir::IrContext::Instance(), return_softmax);
  argument.AddAttribute("return_softmax", attr_return_softmax);
  pir::Attribute attr_ring_id = pir::Int32Attribute::get(pir::IrContext::Instance(), ring_id);
  argument.AddAttribute("ring_id", attr_ring_id);
  pir::Attribute attr_rank = pir::Int32Attribute::get(pir::IrContext::Instance(), rank);
  argument.AddAttribute("rank", attr_rank);
  pir::Attribute attr_nranks = pir::Int32Attribute::get(pir::IrContext::Instance(), nranks);
  argument.AddAttribute("nranks", attr_nranks);
  pir::Attribute attr_margin1 = pir::FloatAttribute::get(pir::IrContext::Instance(), margin1);
  argument.AddAttribute("margin1", attr_margin1);
  pir::Attribute attr_margin2 = pir::FloatAttribute::get(pir::IrContext::Instance(), margin2);
  argument.AddAttribute("margin2", attr_margin2);
  pir::Attribute attr_margin3 = pir::FloatAttribute::get(pir::IrContext::Instance(), margin3);
  argument.AddAttribute("margin3", attr_margin3);
  pir::Attribute attr_scale = pir::FloatAttribute::get(pir::IrContext::Instance(), scale);
  argument.AddAttribute("scale", attr_scale);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType logits = logits_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)logits;
  paddle::dialect::DenseTensorType label = label_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)label;

  VLOG(4) << "Builder construction  dense_logits";
  paddle::dialect::IrTensor ir_tensor_logits(paddle::dialect::TransToPhiDataType(logits.dtype()),
                                                      logits.dims(),
                                                      logits.data_layout(),
                                                      logits.lod(),
                                                      logits.offset());
  VLOG(4) << "Builder construction  meta_logits";
  paddle::dialect::IrMetaTensor meta_logits(&ir_tensor_logits);

  VLOG(4) << "Builder construction  dense_label";
  paddle::dialect::IrTensor ir_tensor_label(paddle::dialect::TransToPhiDataType(label.dtype()),
                                                      label.dims(),
                                                      label.data_layout(),
                                                      label.lod(),
                                                      label.offset());
  VLOG(4) << "Builder construction  meta_label";
  paddle::dialect::IrMetaTensor meta_label(&ir_tensor_label);
  paddle::dialect::IrTensor dense_softmax;
  paddle::dialect::IrMetaTensor meta_softmax(&dense_softmax);
  paddle::dialect::IrTensor dense_loss;
  paddle::dialect::IrMetaTensor meta_loss(&dense_loss);

  phi::MarginCrossEntropyInferMeta(meta_logits, meta_label, return_softmax, ring_id, rank, nranks, margin1, margin2, margin3, scale, &meta_softmax, &meta_loss);

  std::vector<pir::Type> argument_outputs;
  pir::Type softmax_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_softmax.dtype()), dense_softmax.dims(), dense_softmax.layout(), dense_softmax.lod(), dense_softmax.offset());
  argument_outputs.push_back(softmax_dense_tensor_type);

  pir::Type loss_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_loss.dtype()), dense_loss.dims(), dense_loss.layout(), dense_loss.lod(), dense_loss.offset());
  argument_outputs.push_back(loss_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void MarginCrossEntropyOp::VerifySig() {
  VLOG(4) << "Start Verifying inputs, outputs and attributes for: MarginCrossEntropyOp.";
  VLOG(4) << "Verifying inputs:";
  {
  auto input_size = num_operands();
  IR_ENFORCE(input_size == 2u,
                    "The size %d of inputs must be equal to 2.", input_size);
  IR_ENFORCE((*this)->operand_source(0).type().isa<paddle::dialect::DenseTensorType>(),
                  "Type validation failed for the 0th input, got %s.", (*this)->operand_source(0).type());
  IR_ENFORCE((*this)->operand_source(1).type().isa<paddle::dialect::DenseTensorType>(),
                  "Type validation failed for the 1th input, got %s.", (*this)->operand_source(1).type());
  }
  VLOG(4) << "Verifying attributes:";
  {
  auto& attributes = this->attributes();
  IR_ENFORCE(attributes.count("return_softmax")>0,
                 "return_softmax does not exist.");
  IR_ENFORCE(attributes.at("return_softmax").isa<pir::BoolAttribute>(),
                 "Type of attribute: return_softmax is not pir::BoolAttribute.");

  IR_ENFORCE(attributes.count("ring_id")>0,
                 "ring_id does not exist.");
  IR_ENFORCE(attributes.at("ring_id").isa<pir::Int32Attribute>(),
                 "Type of attribute: ring_id is not pir::Int32Attribute.");

  IR_ENFORCE(attributes.count("rank")>0,
                 "rank does not exist.");
  IR_ENFORCE(attributes.at("rank").isa<pir::Int32Attribute>(),
                 "Type of attribute: rank is not pir::Int32Attribute.");

  IR_ENFORCE(attributes.count("nranks")>0,
                 "nranks does not exist.");
  IR_ENFORCE(attributes.at("nranks").isa<pir::Int32Attribute>(),
                 "Type of attribute: nranks is not pir::Int32Attribute.");

  IR_ENFORCE(attributes.count("margin1")>0,
                 "margin1 does not exist.");
  IR_ENFORCE(attributes.at("margin1").isa<pir::FloatAttribute>(),
                 "Type of attribute: margin1 is not pir::FloatAttribute.");

  IR_ENFORCE(attributes.count("margin2")>0,
                 "margin2 does not exist.");
  IR_ENFORCE(attributes.at("margin2").isa<pir::FloatAttribute>(),
                 "Type of attribute: margin2 is not pir::FloatAttribute.");

  IR_ENFORCE(attributes.count("margin3")>0,
                 "margin3 does not exist.");
  IR_ENFORCE(attributes.at("margin3").isa<pir::FloatAttribute>(),
                 "Type of attribute: margin3 is not pir::FloatAttribute.");

  IR_ENFORCE(attributes.count("scale")>0,
                 "scale does not exist.");
  IR_ENFORCE(attributes.at("scale").isa<pir::FloatAttribute>(),
                 "Type of attribute: scale is not pir::FloatAttribute.");

  }
  VLOG(4) << "Verifying outputs:";
  {
  auto output_size = num_results();
  IR_ENFORCE(output_size == 2u,
                    "The size %d of outputs must be equal to 2.", output_size);
  IR_ENFORCE((*this)->result(0).type().isa<paddle::dialect::DenseTensorType>(),
                 "Type validation failed for the 0th output.");
  IR_ENFORCE((*this)->result(1).type().isa<paddle::dialect::DenseTensorType>(),
                 "Type validation failed for the 1th output.");
  }
  VLOG(4) << "End Verifying for: MarginCrossEntropyOp.";
}

void MarginCrossEntropyOp::InferMeta( phi::InferMetaContext *infer_meta ) {
  auto fn = PD_INFER_META(phi::MarginCrossEntropyInferMeta);
  fn(infer_meta);
}

phi::DataType MarginCrossEntropyOp::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: MarginCrossEntropyOp";
  


  return expected_kernel_dtype;
}

const char *MaskedMultiheadAttention_Op::attributes_name[8] = { "seq_len", "rotary_emb_dims", "use_neox_rotary_style", "compute_dtype", "out_scale", "quant_round_type", "quant_max_bound", "quant_min_bound" };

OpInfoTuple MaskedMultiheadAttention_Op::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("x", "paddle::dialect::DenseTensorType", false, false, false, false), paddle::dialect::OpInputInfo("cache_kv", "paddle::dialect::DenseTensorType", false, false, false, false), paddle::dialect::OpInputInfo("bias", "paddle::dialect::DenseTensorType", true, false, false, false), paddle::dialect::OpInputInfo("src_mask", "paddle::dialect::DenseTensorType", true, false, false, false), paddle::dialect::OpInputInfo("cum_offsets", "paddle::dialect::DenseTensorType", true, false, false, false), paddle::dialect::OpInputInfo("sequence_lengths", "paddle::dialect::DenseTensorType", true, false, false, false), paddle::dialect::OpInputInfo("rotary_tensor", "paddle::dialect::DenseTensorType", true, false, false, false), paddle::dialect::OpInputInfo("beam_cache_offset", "paddle::dialect::DenseTensorType", true, false, false, false), paddle::dialect::OpInputInfo("qkv_out_scale", "paddle::dialect::DenseTensorType", true, false, false, false), paddle::dialect::OpInputInfo("out_shift", "paddle::dialect::DenseTensorType", true, false, false, false), paddle::dialect::OpInputInfo("out_smooth", "paddle::dialect::DenseTensorType", true, false, false, false) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = { paddle::dialect::OpAttributeInfo("seq_len", "pir::Int32Attribute", ""), paddle::dialect::OpAttributeInfo("rotary_emb_dims", "pir::Int32Attribute", ""), paddle::dialect::OpAttributeInfo("use_neox_rotary_style", "pir::BoolAttribute", ""), paddle::dialect::OpAttributeInfo("compute_dtype", "pir::StrAttribute", ""), paddle::dialect::OpAttributeInfo("out_scale", "pir::FloatAttribute", ""), paddle::dialect::OpAttributeInfo("quant_round_type", "pir::Int32Attribute", ""), paddle::dialect::OpAttributeInfo("quant_max_bound", "pir::FloatAttribute", ""), paddle::dialect::OpAttributeInfo("quant_min_bound", "pir::FloatAttribute", "") };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("out", "paddle::dialect::DenseTensorType", false, false), paddle::dialect::OpOutputInfo("cache_kv_out", "paddle::dialect::DenseTensorType", false, false), paddle::dialect::OpOutputInfo("beam_cache_offset_out", "paddle::dialect::DenseTensorType", false, false) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("MaskedMultiheadAttentionInferMeta", {"x", "cache_kv", "bias", "src_mask", "cum_offsets", "sequence_lengths", "rotary_tensor", "beam_cache_offset", "qkv_out_scale", "out_shift", "out_smooth", "seq_len", "rotary_emb_dims", "use_neox_rotary_style", "compute_dtype", "out_scale", "quant_round_type", "quant_max_bound", "quant_min_bound"}, "masked_multihead_attention", {"x", "cache_kv", "bias", "src_mask", "cum_offsets", "sequence_lengths", "rotary_tensor", "beam_cache_offset", "qkv_out_scale", "out_shift", "out_smooth", "seq_len", "rotary_emb_dims", "use_neox_rotary_style", "compute_dtype", "out_scale", "quant_round_type", "quant_max_bound", "quant_min_bound"}, {"x"}, {}, {{"cache_kv_out", "cache_kv"},{"beam_cache_offset_out", "beam_cache_offset"}}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "masked_multihead_attention_");
}

void MaskedMultiheadAttention_Op::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, pir::Value cache_kv_, pir::Value bias_, pir::Value src_mask_, pir::Value cum_offsets_, pir::Value sequence_lengths_, pir::Value rotary_tensor_, pir::Value beam_cache_offset_, pir::Value qkv_out_scale_, pir::Value out_shift_, pir::Value out_smooth_, int seq_len, int rotary_emb_dims, bool use_neox_rotary_style, const std::string& compute_dtype, float out_scale, int quant_round_type, float quant_max_bound, float quant_min_bound) {
  VLOG(4) << "Start build MaskedMultiheadAttention_Op";



  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_, cache_kv_, bias_, src_mask_, cum_offsets_, sequence_lengths_, rotary_tensor_, beam_cache_offset_, qkv_out_scale_, out_shift_, out_smooth_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_seq_len = pir::Int32Attribute::get(pir::IrContext::Instance(), seq_len);
  argument.AddAttribute("seq_len", attr_seq_len);
  pir::Attribute attr_rotary_emb_dims = pir::Int32Attribute::get(pir::IrContext::Instance(), rotary_emb_dims);
  argument.AddAttribute("rotary_emb_dims", attr_rotary_emb_dims);
  pir::Attribute attr_use_neox_rotary_style = pir::BoolAttribute::get(pir::IrContext::Instance(), use_neox_rotary_style);
  argument.AddAttribute("use_neox_rotary_style", attr_use_neox_rotary_style);
  pir::Attribute attr_compute_dtype = pir::StrAttribute::get(pir::IrContext::Instance(), compute_dtype);
  argument.AddAttribute("compute_dtype", attr_compute_dtype);
  pir::Attribute attr_out_scale = pir::FloatAttribute::get(pir::IrContext::Instance(), out_scale);
  argument.AddAttribute("out_scale", attr_out_scale);
  pir::Attribute attr_quant_round_type = pir::Int32Attribute::get(pir::IrContext::Instance(), quant_round_type);
  argument.AddAttribute("quant_round_type", attr_quant_round_type);
  pir::Attribute attr_quant_max_bound = pir::FloatAttribute::get(pir::IrContext::Instance(), quant_max_bound);
  argument.AddAttribute("quant_max_bound", attr_quant_max_bound);
  pir::Attribute attr_quant_min_bound = pir::FloatAttribute::get(pir::IrContext::Instance(), quant_min_bound);
  argument.AddAttribute("quant_min_bound", attr_quant_min_bound);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;
  paddle::dialect::DenseTensorType cache_kv = cache_kv_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)cache_kv;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);

  VLOG(4) << "Builder construction  dense_cache_kv";
  paddle::dialect::IrTensor ir_tensor_cache_kv(paddle::dialect::TransToPhiDataType(cache_kv.dtype()),
                                                      cache_kv.dims(),
                                                      cache_kv.data_layout(),
                                                      cache_kv.lod(),
                                                      cache_kv.offset());
  VLOG(4) << "Builder construction  meta_cache_kv";
  paddle::dialect::IrMetaTensor meta_cache_kv(&ir_tensor_cache_kv);

  paddle::dialect::IrMetaTensor meta_bias;
  paddle::dialect::IrTensor ir_tensor_bias;
  if (bias_.impl() != nullptr) {
    paddle::dialect::DenseTensorType bias = bias_.type().dyn_cast<paddle::dialect::DenseTensorType>();
    VLOG(4) << "Builder construction  dense_bias";
    ir_tensor_bias = paddle::dialect::IrTensor(paddle::dialect::TransToPhiDataType(bias.dtype()),
                                                        bias.dims(),
                                                        bias.data_layout(),
                                                        bias.lod(),
                                                        bias.offset());
    VLOG(4) << "Builder construction  meta_bias";
    meta_bias = paddle::dialect::IrMetaTensor(&ir_tensor_bias);
  }


  paddle::dialect::IrMetaTensor meta_src_mask;
  paddle::dialect::IrTensor ir_tensor_src_mask;
  if (src_mask_.impl() != nullptr) {
    paddle::dialect::DenseTensorType src_mask = src_mask_.type().dyn_cast<paddle::dialect::DenseTensorType>();
    VLOG(4) << "Builder construction  dense_src_mask";
    ir_tensor_src_mask = paddle::dialect::IrTensor(paddle::dialect::TransToPhiDataType(src_mask.dtype()),
                                                        src_mask.dims(),
                                                        src_mask.data_layout(),
                                                        src_mask.lod(),
                                                        src_mask.offset());
    VLOG(4) << "Builder construction  meta_src_mask";
    meta_src_mask = paddle::dialect::IrMetaTensor(&ir_tensor_src_mask);
  }


  paddle::dialect::IrMetaTensor meta_cum_offsets;
  paddle::dialect::IrTensor ir_tensor_cum_offsets;
  if (cum_offsets_.impl() != nullptr) {
    paddle::dialect::DenseTensorType cum_offsets = cum_offsets_.type().dyn_cast<paddle::dialect::DenseTensorType>();
    VLOG(4) << "Builder construction  dense_cum_offsets";
    ir_tensor_cum_offsets = paddle::dialect::IrTensor(paddle::dialect::TransToPhiDataType(cum_offsets.dtype()),
                                                        cum_offsets.dims(),
                                                        cum_offsets.data_layout(),
                                                        cum_offsets.lod(),
                                                        cum_offsets.offset());
    VLOG(4) << "Builder construction  meta_cum_offsets";
    meta_cum_offsets = paddle::dialect::IrMetaTensor(&ir_tensor_cum_offsets);
  }


  paddle::dialect::IrMetaTensor meta_sequence_lengths;
  paddle::dialect::IrTensor ir_tensor_sequence_lengths;
  if (sequence_lengths_.impl() != nullptr) {
    paddle::dialect::DenseTensorType sequence_lengths = sequence_lengths_.type().dyn_cast<paddle::dialect::DenseTensorType>();
    VLOG(4) << "Builder construction  dense_sequence_lengths";
    ir_tensor_sequence_lengths = paddle::dialect::IrTensor(paddle::dialect::TransToPhiDataType(sequence_lengths.dtype()),
                                                        sequence_lengths.dims(),
                                                        sequence_lengths.data_layout(),
                                                        sequence_lengths.lod(),
                                                        sequence_lengths.offset());
    VLOG(4) << "Builder construction  meta_sequence_lengths";
    meta_sequence_lengths = paddle::dialect::IrMetaTensor(&ir_tensor_sequence_lengths);
  }


  paddle::dialect::IrMetaTensor meta_rotary_tensor;
  paddle::dialect::IrTensor ir_tensor_rotary_tensor;
  if (rotary_tensor_.impl() != nullptr) {
    paddle::dialect::DenseTensorType rotary_tensor = rotary_tensor_.type().dyn_cast<paddle::dialect::DenseTensorType>();
    VLOG(4) << "Builder construction  dense_rotary_tensor";
    ir_tensor_rotary_tensor = paddle::dialect::IrTensor(paddle::dialect::TransToPhiDataType(rotary_tensor.dtype()),
                                                        rotary_tensor.dims(),
                                                        rotary_tensor.data_layout(),
                                                        rotary_tensor.lod(),
                                                        rotary_tensor.offset());
    VLOG(4) << "Builder construction  meta_rotary_tensor";
    meta_rotary_tensor = paddle::dialect::IrMetaTensor(&ir_tensor_rotary_tensor);
  }


  paddle::dialect::IrMetaTensor meta_beam_cache_offset;
  paddle::dialect::IrTensor ir_tensor_beam_cache_offset;
  if (beam_cache_offset_.impl() != nullptr) {
    paddle::dialect::DenseTensorType beam_cache_offset = beam_cache_offset_.type().dyn_cast<paddle::dialect::DenseTensorType>();
    VLOG(4) << "Builder construction  dense_beam_cache_offset";
    ir_tensor_beam_cache_offset = paddle::dialect::IrTensor(paddle::dialect::TransToPhiDataType(beam_cache_offset.dtype()),
                                                        beam_cache_offset.dims(),
                                                        beam_cache_offset.data_layout(),
                                                        beam_cache_offset.lod(),
                                                        beam_cache_offset.offset());
    VLOG(4) << "Builder construction  meta_beam_cache_offset";
    meta_beam_cache_offset = paddle::dialect::IrMetaTensor(&ir_tensor_beam_cache_offset);
  }


  paddle::dialect::IrMetaTensor meta_qkv_out_scale;
  paddle::dialect::IrTensor ir_tensor_qkv_out_scale;
  if (qkv_out_scale_.impl() != nullptr) {
    paddle::dialect::DenseTensorType qkv_out_scale = qkv_out_scale_.type().dyn_cast<paddle::dialect::DenseTensorType>();
    VLOG(4) << "Builder construction  dense_qkv_out_scale";
    ir_tensor_qkv_out_scale = paddle::dialect::IrTensor(paddle::dialect::TransToPhiDataType(qkv_out_scale.dtype()),
                                                        qkv_out_scale.dims(),
                                                        qkv_out_scale.data_layout(),
                                                        qkv_out_scale.lod(),
                                                        qkv_out_scale.offset());
    VLOG(4) << "Builder construction  meta_qkv_out_scale";
    meta_qkv_out_scale = paddle::dialect::IrMetaTensor(&ir_tensor_qkv_out_scale);
  }


  paddle::dialect::IrMetaTensor meta_out_shift;
  paddle::dialect::IrTensor ir_tensor_out_shift;
  if (out_shift_.impl() != nullptr) {
    paddle::dialect::DenseTensorType out_shift = out_shift_.type().dyn_cast<paddle::dialect::DenseTensorType>();
    VLOG(4) << "Builder construction  dense_out_shift";
    ir_tensor_out_shift = paddle::dialect::IrTensor(paddle::dialect::TransToPhiDataType(out_shift.dtype()),
                                                        out_shift.dims(),
                                                        out_shift.data_layout(),
                                                        out_shift.lod(),
                                                        out_shift.offset());
    VLOG(4) << "Builder construction  meta_out_shift";
    meta_out_shift = paddle::dialect::IrMetaTensor(&ir_tensor_out_shift);
  }


  paddle::dialect::IrMetaTensor meta_out_smooth;
  paddle::dialect::IrTensor ir_tensor_out_smooth;
  if (out_smooth_.impl() != nullptr) {
    paddle::dialect::DenseTensorType out_smooth = out_smooth_.type().dyn_cast<paddle::dialect::DenseTensorType>();
    VLOG(4) << "Builder construction  dense_out_smooth";
    ir_tensor_out_smooth = paddle::dialect::IrTensor(paddle::dialect::TransToPhiDataType(out_smooth.dtype()),
                                                        out_smooth.dims(),
                                                        out_smooth.data_layout(),
                                                        out_smooth.lod(),
                                                        out_smooth.offset());
    VLOG(4) << "Builder construction  meta_out_smooth";
    meta_out_smooth = paddle::dialect::IrMetaTensor(&ir_tensor_out_smooth);
  }

  paddle::dialect::IrTensor dense_out;
  paddle::dialect::IrMetaTensor meta_out(&dense_out);
  paddle::dialect::IrTensor dense_cache_kv_out;
  paddle::dialect::IrMetaTensor meta_cache_kv_out(&dense_cache_kv_out);
  paddle::dialect::IrTensor dense_beam_cache_offset_out;
  paddle::dialect::IrMetaTensor meta_beam_cache_offset_out(&dense_beam_cache_offset_out);

  phi::MaskedMultiheadAttentionInferMeta(meta_x, meta_cache_kv, meta_bias, meta_src_mask, meta_cum_offsets, meta_sequence_lengths, meta_rotary_tensor, meta_beam_cache_offset, meta_qkv_out_scale, meta_out_shift, meta_out_smooth, seq_len, rotary_emb_dims, use_neox_rotary_style, compute_dtype, out_scale, quant_round_type, quant_max_bound, quant_min_bound, &meta_out, &meta_cache_kv_out, &meta_beam_cache_offset_out);

  std::vector<pir::Type> argument_outputs;
  pir::Type out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_out.dtype()), dense_out.dims(), dense_out.layout(), dense_out.lod(), dense_out.offset());
  argument_outputs.push_back(out_dense_tensor_type);

  pir::Type cache_kv_out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_cache_kv_out.dtype()), dense_cache_kv_out.dims(), dense_cache_kv_out.layout(), dense_cache_kv_out.lod(), dense_cache_kv_out.offset());
  argument_outputs.push_back(cache_kv_out_dense_tensor_type);

  pir::Type beam_cache_offset_out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_beam_cache_offset_out.dtype()), dense_beam_cache_offset_out.dims(), dense_beam_cache_offset_out.layout(), dense_beam_cache_offset_out.lod(), dense_beam_cache_offset_out.offset());
  argument_outputs.push_back(beam_cache_offset_out_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void MaskedMultiheadAttention_Op::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, pir::Value cache_kv_, pir::Value bias_, pir::Value src_mask_, pir::Value cum_offsets_, pir::Value sequence_lengths_, pir::Value rotary_tensor_, pir::Value beam_cache_offset_, pir::Value qkv_out_scale_, pir::Value out_shift_, pir::Value out_smooth_, pir::AttributeMap attributes) {
  VLOG(4) << "Start build MaskedMultiheadAttention_Op";


  IR_ENFORCE(
      attributes.find("seq_len") != attributes.end(),
          "'seq_len' Attribute is expected for MaskedMultiheadAttention_Op. ");
  int seq_len = attributes.at("seq_len").dyn_cast<pir::Int32Attribute>().data();

  IR_ENFORCE(
      attributes.find("rotary_emb_dims") != attributes.end(),
          "'rotary_emb_dims' Attribute is expected for MaskedMultiheadAttention_Op. ");
  int rotary_emb_dims = attributes.at("rotary_emb_dims").dyn_cast<pir::Int32Attribute>().data();

  IR_ENFORCE(
      attributes.find("use_neox_rotary_style") != attributes.end(),
          "'use_neox_rotary_style' Attribute is expected for MaskedMultiheadAttention_Op. ");
  bool use_neox_rotary_style = attributes.at("use_neox_rotary_style").dyn_cast<pir::BoolAttribute>().data();

  IR_ENFORCE(
      attributes.find("compute_dtype") != attributes.end(),
          "'compute_dtype' Attribute is expected for MaskedMultiheadAttention_Op. ");
  std::string compute_dtype = attributes.at("compute_dtype").dyn_cast<pir::StrAttribute>().AsString();

  IR_ENFORCE(
      attributes.find("out_scale") != attributes.end(),
          "'out_scale' Attribute is expected for MaskedMultiheadAttention_Op. ");
  float out_scale = attributes.at("out_scale").dyn_cast<pir::FloatAttribute>().data();

  IR_ENFORCE(
      attributes.find("quant_round_type") != attributes.end(),
          "'quant_round_type' Attribute is expected for MaskedMultiheadAttention_Op. ");
  int quant_round_type = attributes.at("quant_round_type").dyn_cast<pir::Int32Attribute>().data();

  IR_ENFORCE(
      attributes.find("quant_max_bound") != attributes.end(),
          "'quant_max_bound' Attribute is expected for MaskedMultiheadAttention_Op. ");
  float quant_max_bound = attributes.at("quant_max_bound").dyn_cast<pir::FloatAttribute>().data();

  IR_ENFORCE(
      attributes.find("quant_min_bound") != attributes.end(),
          "'quant_min_bound' Attribute is expected for MaskedMultiheadAttention_Op. ");
  float quant_min_bound = attributes.at("quant_min_bound").dyn_cast<pir::FloatAttribute>().data();


  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_, cache_kv_, bias_, src_mask_, cum_offsets_, sequence_lengths_, rotary_tensor_, beam_cache_offset_, qkv_out_scale_, out_shift_, out_smooth_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_seq_len = pir::Int32Attribute::get(pir::IrContext::Instance(), seq_len);
  argument.AddAttribute("seq_len", attr_seq_len);
  pir::Attribute attr_rotary_emb_dims = pir::Int32Attribute::get(pir::IrContext::Instance(), rotary_emb_dims);
  argument.AddAttribute("rotary_emb_dims", attr_rotary_emb_dims);
  pir::Attribute attr_use_neox_rotary_style = pir::BoolAttribute::get(pir::IrContext::Instance(), use_neox_rotary_style);
  argument.AddAttribute("use_neox_rotary_style", attr_use_neox_rotary_style);
  pir::Attribute attr_compute_dtype = pir::StrAttribute::get(pir::IrContext::Instance(), compute_dtype);
  argument.AddAttribute("compute_dtype", attr_compute_dtype);
  pir::Attribute attr_out_scale = pir::FloatAttribute::get(pir::IrContext::Instance(), out_scale);
  argument.AddAttribute("out_scale", attr_out_scale);
  pir::Attribute attr_quant_round_type = pir::Int32Attribute::get(pir::IrContext::Instance(), quant_round_type);
  argument.AddAttribute("quant_round_type", attr_quant_round_type);
  pir::Attribute attr_quant_max_bound = pir::FloatAttribute::get(pir::IrContext::Instance(), quant_max_bound);
  argument.AddAttribute("quant_max_bound", attr_quant_max_bound);
  pir::Attribute attr_quant_min_bound = pir::FloatAttribute::get(pir::IrContext::Instance(), quant_min_bound);
  argument.AddAttribute("quant_min_bound", attr_quant_min_bound);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;
  paddle::dialect::DenseTensorType cache_kv = cache_kv_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)cache_kv;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);

  VLOG(4) << "Builder construction  dense_cache_kv";
  paddle::dialect::IrTensor ir_tensor_cache_kv(paddle::dialect::TransToPhiDataType(cache_kv.dtype()),
                                                      cache_kv.dims(),
                                                      cache_kv.data_layout(),
                                                      cache_kv.lod(),
                                                      cache_kv.offset());
  VLOG(4) << "Builder construction  meta_cache_kv";
  paddle::dialect::IrMetaTensor meta_cache_kv(&ir_tensor_cache_kv);

  paddle::dialect::IrMetaTensor meta_bias;
  paddle::dialect::IrTensor ir_tensor_bias;
  if (bias_.impl() != nullptr) {
    paddle::dialect::DenseTensorType bias = bias_.type().dyn_cast<paddle::dialect::DenseTensorType>();
    VLOG(4) << "Builder construction  dense_bias";
    ir_tensor_bias = paddle::dialect::IrTensor(paddle::dialect::TransToPhiDataType(bias.dtype()),
                                                        bias.dims(),
                                                        bias.data_layout(),
                                                        bias.lod(),
                                                        bias.offset());
    VLOG(4) << "Builder construction  meta_bias";
    meta_bias = paddle::dialect::IrMetaTensor(&ir_tensor_bias);
  }


  paddle::dialect::IrMetaTensor meta_src_mask;
  paddle::dialect::IrTensor ir_tensor_src_mask;
  if (src_mask_.impl() != nullptr) {
    paddle::dialect::DenseTensorType src_mask = src_mask_.type().dyn_cast<paddle::dialect::DenseTensorType>();
    VLOG(4) << "Builder construction  dense_src_mask";
    ir_tensor_src_mask = paddle::dialect::IrTensor(paddle::dialect::TransToPhiDataType(src_mask.dtype()),
                                                        src_mask.dims(),
                                                        src_mask.data_layout(),
                                                        src_mask.lod(),
                                                        src_mask.offset());
    VLOG(4) << "Builder construction  meta_src_mask";
    meta_src_mask = paddle::dialect::IrMetaTensor(&ir_tensor_src_mask);
  }


  paddle::dialect::IrMetaTensor meta_cum_offsets;
  paddle::dialect::IrTensor ir_tensor_cum_offsets;
  if (cum_offsets_.impl() != nullptr) {
    paddle::dialect::DenseTensorType cum_offsets = cum_offsets_.type().dyn_cast<paddle::dialect::DenseTensorType>();
    VLOG(4) << "Builder construction  dense_cum_offsets";
    ir_tensor_cum_offsets = paddle::dialect::IrTensor(paddle::dialect::TransToPhiDataType(cum_offsets.dtype()),
                                                        cum_offsets.dims(),
                                                        cum_offsets.data_layout(),
                                                        cum_offsets.lod(),
                                                        cum_offsets.offset());
    VLOG(4) << "Builder construction  meta_cum_offsets";
    meta_cum_offsets = paddle::dialect::IrMetaTensor(&ir_tensor_cum_offsets);
  }


  paddle::dialect::IrMetaTensor meta_sequence_lengths;
  paddle::dialect::IrTensor ir_tensor_sequence_lengths;
  if (sequence_lengths_.impl() != nullptr) {
    paddle::dialect::DenseTensorType sequence_lengths = sequence_lengths_.type().dyn_cast<paddle::dialect::DenseTensorType>();
    VLOG(4) << "Builder construction  dense_sequence_lengths";
    ir_tensor_sequence_lengths = paddle::dialect::IrTensor(paddle::dialect::TransToPhiDataType(sequence_lengths.dtype()),
                                                        sequence_lengths.dims(),
                                                        sequence_lengths.data_layout(),
                                                        sequence_lengths.lod(),
                                                        sequence_lengths.offset());
    VLOG(4) << "Builder construction  meta_sequence_lengths";
    meta_sequence_lengths = paddle::dialect::IrMetaTensor(&ir_tensor_sequence_lengths);
  }


  paddle::dialect::IrMetaTensor meta_rotary_tensor;
  paddle::dialect::IrTensor ir_tensor_rotary_tensor;
  if (rotary_tensor_.impl() != nullptr) {
    paddle::dialect::DenseTensorType rotary_tensor = rotary_tensor_.type().dyn_cast<paddle::dialect::DenseTensorType>();
    VLOG(4) << "Builder construction  dense_rotary_tensor";
    ir_tensor_rotary_tensor = paddle::dialect::IrTensor(paddle::dialect::TransToPhiDataType(rotary_tensor.dtype()),
                                                        rotary_tensor.dims(),
                                                        rotary_tensor.data_layout(),
                                                        rotary_tensor.lod(),
                                                        rotary_tensor.offset());
    VLOG(4) << "Builder construction  meta_rotary_tensor";
    meta_rotary_tensor = paddle::dialect::IrMetaTensor(&ir_tensor_rotary_tensor);
  }


  paddle::dialect::IrMetaTensor meta_beam_cache_offset;
  paddle::dialect::IrTensor ir_tensor_beam_cache_offset;
  if (beam_cache_offset_.impl() != nullptr) {
    paddle::dialect::DenseTensorType beam_cache_offset = beam_cache_offset_.type().dyn_cast<paddle::dialect::DenseTensorType>();
    VLOG(4) << "Builder construction  dense_beam_cache_offset";
    ir_tensor_beam_cache_offset = paddle::dialect::IrTensor(paddle::dialect::TransToPhiDataType(beam_cache_offset.dtype()),
                                                        beam_cache_offset.dims(),
                                                        beam_cache_offset.data_layout(),
                                                        beam_cache_offset.lod(),
                                                        beam_cache_offset.offset());
    VLOG(4) << "Builder construction  meta_beam_cache_offset";
    meta_beam_cache_offset = paddle::dialect::IrMetaTensor(&ir_tensor_beam_cache_offset);
  }


  paddle::dialect::IrMetaTensor meta_qkv_out_scale;
  paddle::dialect::IrTensor ir_tensor_qkv_out_scale;
  if (qkv_out_scale_.impl() != nullptr) {
    paddle::dialect::DenseTensorType qkv_out_scale = qkv_out_scale_.type().dyn_cast<paddle::dialect::DenseTensorType>();
    VLOG(4) << "Builder construction  dense_qkv_out_scale";
    ir_tensor_qkv_out_scale = paddle::dialect::IrTensor(paddle::dialect::TransToPhiDataType(qkv_out_scale.dtype()),
                                                        qkv_out_scale.dims(),
                                                        qkv_out_scale.data_layout(),
                                                        qkv_out_scale.lod(),
                                                        qkv_out_scale.offset());
    VLOG(4) << "Builder construction  meta_qkv_out_scale";
    meta_qkv_out_scale = paddle::dialect::IrMetaTensor(&ir_tensor_qkv_out_scale);
  }


  paddle::dialect::IrMetaTensor meta_out_shift;
  paddle::dialect::IrTensor ir_tensor_out_shift;
  if (out_shift_.impl() != nullptr) {
    paddle::dialect::DenseTensorType out_shift = out_shift_.type().dyn_cast<paddle::dialect::DenseTensorType>();
    VLOG(4) << "Builder construction  dense_out_shift";
    ir_tensor_out_shift = paddle::dialect::IrTensor(paddle::dialect::TransToPhiDataType(out_shift.dtype()),
                                                        out_shift.dims(),
                                                        out_shift.data_layout(),
                                                        out_shift.lod(),
                                                        out_shift.offset());
    VLOG(4) << "Builder construction  meta_out_shift";
    meta_out_shift = paddle::dialect::IrMetaTensor(&ir_tensor_out_shift);
  }


  paddle::dialect::IrMetaTensor meta_out_smooth;
  paddle::dialect::IrTensor ir_tensor_out_smooth;
  if (out_smooth_.impl() != nullptr) {
    paddle::dialect::DenseTensorType out_smooth = out_smooth_.type().dyn_cast<paddle::dialect::DenseTensorType>();
    VLOG(4) << "Builder construction  dense_out_smooth";
    ir_tensor_out_smooth = paddle::dialect::IrTensor(paddle::dialect::TransToPhiDataType(out_smooth.dtype()),
                                                        out_smooth.dims(),
                                                        out_smooth.data_layout(),
                                                        out_smooth.lod(),
                                                        out_smooth.offset());
    VLOG(4) << "Builder construction  meta_out_smooth";
    meta_out_smooth = paddle::dialect::IrMetaTensor(&ir_tensor_out_smooth);
  }

  paddle::dialect::IrTensor dense_out;
  paddle::dialect::IrMetaTensor meta_out(&dense_out);
  paddle::dialect::IrTensor dense_cache_kv_out;
  paddle::dialect::IrMetaTensor meta_cache_kv_out(&dense_cache_kv_out);
  paddle::dialect::IrTensor dense_beam_cache_offset_out;
  paddle::dialect::IrMetaTensor meta_beam_cache_offset_out(&dense_beam_cache_offset_out);

  phi::MaskedMultiheadAttentionInferMeta(meta_x, meta_cache_kv, meta_bias, meta_src_mask, meta_cum_offsets, meta_sequence_lengths, meta_rotary_tensor, meta_beam_cache_offset, meta_qkv_out_scale, meta_out_shift, meta_out_smooth, seq_len, rotary_emb_dims, use_neox_rotary_style, compute_dtype, out_scale, quant_round_type, quant_max_bound, quant_min_bound, &meta_out, &meta_cache_kv_out, &meta_beam_cache_offset_out);

  std::vector<pir::Type> argument_outputs;
  pir::Type out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_out.dtype()), dense_out.dims(), dense_out.layout(), dense_out.lod(), dense_out.offset());
  argument_outputs.push_back(out_dense_tensor_type);

  pir::Type cache_kv_out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_cache_kv_out.dtype()), dense_cache_kv_out.dims(), dense_cache_kv_out.layout(), dense_cache_kv_out.lod(), dense_cache_kv_out.offset());
  argument_outputs.push_back(cache_kv_out_dense_tensor_type);

  pir::Type beam_cache_offset_out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_beam_cache_offset_out.dtype()), dense_beam_cache_offset_out.dims(), dense_beam_cache_offset_out.layout(), dense_beam_cache_offset_out.lod(), dense_beam_cache_offset_out.offset());
  argument_outputs.push_back(beam_cache_offset_out_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void MaskedMultiheadAttention_Op::VerifySig() {
  VLOG(4) << "Start Verifying inputs, outputs and attributes for: MaskedMultiheadAttention_Op.";
  VLOG(4) << "Verifying inputs:";
  {
  auto input_size = num_operands();
  IR_ENFORCE(input_size == 11u,
                    "The size %d of inputs must be equal to 11.", input_size);
  IR_ENFORCE((*this)->operand_source(0).type().isa<paddle::dialect::DenseTensorType>(),
                  "Type validation failed for the 0th input, got %s.", (*this)->operand_source(0).type());
  IR_ENFORCE((*this)->operand_source(1).type().isa<paddle::dialect::DenseTensorType>(),
                  "Type validation failed for the 1th input, got %s.", (*this)->operand_source(1).type());
  if (auto val = (*this)->operand(2)) {
    IR_ENFORCE(val.type().isa<paddle::dialect::DenseTensorType>(),
                   "Type validation failed for the 2th input, got %s.", (*this)->operand_source(2).type());
  }
  if (auto val = (*this)->operand(3)) {
    IR_ENFORCE(val.type().isa<paddle::dialect::DenseTensorType>(),
                   "Type validation failed for the 3th input, got %s.", (*this)->operand_source(3).type());
  }
  if (auto val = (*this)->operand(4)) {
    IR_ENFORCE(val.type().isa<paddle::dialect::DenseTensorType>(),
                   "Type validation failed for the 4th input, got %s.", (*this)->operand_source(4).type());
  }
  if (auto val = (*this)->operand(5)) {
    IR_ENFORCE(val.type().isa<paddle::dialect::DenseTensorType>(),
                   "Type validation failed for the 5th input, got %s.", (*this)->operand_source(5).type());
  }
  if (auto val = (*this)->operand(6)) {
    IR_ENFORCE(val.type().isa<paddle::dialect::DenseTensorType>(),
                   "Type validation failed for the 6th input, got %s.", (*this)->operand_source(6).type());
  }
  if (auto val = (*this)->operand(7)) {
    IR_ENFORCE(val.type().isa<paddle::dialect::DenseTensorType>(),
                   "Type validation failed for the 7th input, got %s.", (*this)->operand_source(7).type());
  }
  if (auto val = (*this)->operand(8)) {
    IR_ENFORCE(val.type().isa<paddle::dialect::DenseTensorType>(),
                   "Type validation failed for the 8th input, got %s.", (*this)->operand_source(8).type());
  }
  if (auto val = (*this)->operand(9)) {
    IR_ENFORCE(val.type().isa<paddle::dialect::DenseTensorType>(),
                   "Type validation failed for the 9th input, got %s.", (*this)->operand_source(9).type());
  }
  if (auto val = (*this)->operand(10)) {
    IR_ENFORCE(val.type().isa<paddle::dialect::DenseTensorType>(),
                   "Type validation failed for the 10th input, got %s.", (*this)->operand_source(10).type());
  }
  }
  VLOG(4) << "Verifying attributes:";
  {
  auto& attributes = this->attributes();
  IR_ENFORCE(attributes.count("seq_len")>0,
                 "seq_len does not exist.");
  IR_ENFORCE(attributes.at("seq_len").isa<pir::Int32Attribute>(),
                 "Type of attribute: seq_len is not pir::Int32Attribute.");

  IR_ENFORCE(attributes.count("rotary_emb_dims")>0,
                 "rotary_emb_dims does not exist.");
  IR_ENFORCE(attributes.at("rotary_emb_dims").isa<pir::Int32Attribute>(),
                 "Type of attribute: rotary_emb_dims is not pir::Int32Attribute.");

  IR_ENFORCE(attributes.count("use_neox_rotary_style")>0,
                 "use_neox_rotary_style does not exist.");
  IR_ENFORCE(attributes.at("use_neox_rotary_style").isa<pir::BoolAttribute>(),
                 "Type of attribute: use_neox_rotary_style is not pir::BoolAttribute.");

  IR_ENFORCE(attributes.count("compute_dtype")>0,
                 "compute_dtype does not exist.");
  IR_ENFORCE(attributes.at("compute_dtype").isa<pir::StrAttribute>(),
                 "Type of attribute: compute_dtype is not pir::StrAttribute.");

  IR_ENFORCE(attributes.count("out_scale")>0,
                 "out_scale does not exist.");
  IR_ENFORCE(attributes.at("out_scale").isa<pir::FloatAttribute>(),
                 "Type of attribute: out_scale is not pir::FloatAttribute.");

  IR_ENFORCE(attributes.count("quant_round_type")>0,
                 "quant_round_type does not exist.");
  IR_ENFORCE(attributes.at("quant_round_type").isa<pir::Int32Attribute>(),
                 "Type of attribute: quant_round_type is not pir::Int32Attribute.");

  IR_ENFORCE(attributes.count("quant_max_bound")>0,
                 "quant_max_bound does not exist.");
  IR_ENFORCE(attributes.at("quant_max_bound").isa<pir::FloatAttribute>(),
                 "Type of attribute: quant_max_bound is not pir::FloatAttribute.");

  IR_ENFORCE(attributes.count("quant_min_bound")>0,
                 "quant_min_bound does not exist.");
  IR_ENFORCE(attributes.at("quant_min_bound").isa<pir::FloatAttribute>(),
                 "Type of attribute: quant_min_bound is not pir::FloatAttribute.");

  }
  VLOG(4) << "Verifying outputs:";
  {
  auto output_size = num_results();
  IR_ENFORCE(output_size == 3u,
                    "The size %d of outputs must be equal to 3.", output_size);
  IR_ENFORCE((*this)->result(0).type().isa<paddle::dialect::DenseTensorType>(),
                 "Type validation failed for the 0th output.");
  IR_ENFORCE((*this)->result(1).type().isa<paddle::dialect::DenseTensorType>(),
                 "Type validation failed for the 1th output.");
  IR_ENFORCE((*this)->result(2).type().isa<paddle::dialect::DenseTensorType>(),
                 "Type validation failed for the 2th output.");
  }
  VLOG(4) << "End Verifying for: MaskedMultiheadAttention_Op.";
}

void MaskedMultiheadAttention_Op::InferMeta( phi::InferMetaContext *infer_meta ) {
  auto fn = PD_INFER_META(phi::MaskedMultiheadAttentionInferMeta);
  fn(infer_meta);
}

phi::DataType MaskedMultiheadAttention_Op::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: MaskedMultiheadAttention_Op";
  


  return expected_kernel_dtype;
}

OpInfoTuple MaskedSelectOp::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("x", "paddle::dialect::DenseTensorType", false, false, false, true), paddle::dialect::OpInputInfo("mask", "paddle::dialect::DenseTensorType", false, false, false, false) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = {  };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("out", "paddle::dialect::DenseTensorType", false, false) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("MaskedSelectInferMeta", {"x", "mask"}, "masked_select", {"x", "mask"}, {"x"}, {}, {}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "masked_select");
}

void MaskedSelectOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, pir::Value mask_) {
  VLOG(4) << "Start build MaskedSelectOp";



  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_, mask_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;
  paddle::dialect::DenseTensorType mask = mask_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)mask;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);

  VLOG(4) << "Builder construction  dense_mask";
  paddle::dialect::IrTensor ir_tensor_mask(paddle::dialect::TransToPhiDataType(mask.dtype()),
                                                      mask.dims(),
                                                      mask.data_layout(),
                                                      mask.lod(),
                                                      mask.offset());
  VLOG(4) << "Builder construction  meta_mask";
  paddle::dialect::IrMetaTensor meta_mask(&ir_tensor_mask);
  paddle::dialect::IrTensor dense_out;
  paddle::dialect::IrMetaTensor meta_out(&dense_out);

  phi::MaskedSelectInferMeta(meta_x, meta_mask, &meta_out);

  std::vector<pir::Type> argument_outputs;
  pir::Type out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_out.dtype()), dense_out.dims(), dense_out.layout(), dense_out.lod(), dense_out.offset());
  argument_outputs.push_back(out_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void MaskedSelectOp::VerifySig() {
  VLOG(4) << "Start Verifying inputs, outputs and attributes for: MaskedSelectOp.";
  VLOG(4) << "Verifying inputs:";
  {
  auto input_size = num_operands();
  IR_ENFORCE(input_size == 2u,
                    "The size %d of inputs must be equal to 2.", input_size);
  IR_ENFORCE((*this)->operand_source(0).type().isa<paddle::dialect::DenseTensorType>(),
                  "Type validation failed for the 0th input, got %s.", (*this)->operand_source(0).type());
  IR_ENFORCE((*this)->operand_source(1).type().isa<paddle::dialect::DenseTensorType>(),
                  "Type validation failed for the 1th input, got %s.", (*this)->operand_source(1).type());
  }
  VLOG(4) << "Verifying attributes:";
  {
  // Attributes num is 0, not need to check attributes type.
  }
  VLOG(4) << "Verifying outputs:";
  {
  auto output_size = num_results();
  IR_ENFORCE(output_size == 1u,
                    "The size %d of outputs must be equal to 1.", output_size);
  IR_ENFORCE((*this)->result(0).type().isa<paddle::dialect::DenseTensorType>(),
                 "Type validation failed for the 0th output.");
  }
  VLOG(4) << "End Verifying for: MaskedSelectOp.";
}

void MaskedSelectOp::InferMeta( phi::InferMetaContext *infer_meta ) {
  auto fn = PD_INFER_META(phi::MaskedSelectInferMeta);
  fn(infer_meta);
}

phi::DataType MaskedSelectOp::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: MaskedSelectOp";
  


  return expected_kernel_dtype;
}

const char *MatrixNmsOp::attributes_name[8] = { "score_threshold", "nms_top_k", "keep_top_k", "post_threshold", "use_gaussian", "gaussian_sigma", "background_label", "normalized" };

OpInfoTuple MatrixNmsOp::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("bboxes", "paddle::dialect::DenseTensorType", false, false, false, false), paddle::dialect::OpInputInfo("scores", "paddle::dialect::DenseTensorType", false, false, false, false) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = { paddle::dialect::OpAttributeInfo("score_threshold", "pir::FloatAttribute", ""), paddle::dialect::OpAttributeInfo("nms_top_k", "pir::Int32Attribute", ""), paddle::dialect::OpAttributeInfo("keep_top_k", "pir::Int32Attribute", ""), paddle::dialect::OpAttributeInfo("post_threshold", "pir::FloatAttribute", ""), paddle::dialect::OpAttributeInfo("use_gaussian", "pir::BoolAttribute", ""), paddle::dialect::OpAttributeInfo("gaussian_sigma", "pir::FloatAttribute", ""), paddle::dialect::OpAttributeInfo("background_label", "pir::Int32Attribute", ""), paddle::dialect::OpAttributeInfo("normalized", "pir::BoolAttribute", "") };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("out", "paddle::dialect::DenseTensorType", false, false), paddle::dialect::OpOutputInfo("index", "paddle::dialect::DenseTensorType", false, false), paddle::dialect::OpOutputInfo("roisnum", "paddle::dialect::DenseTensorType", true, false) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("MatrixNMSInferMeta", {"bboxes", "scores", "score_threshold", "nms_top_k", "keep_top_k", "post_threshold", "use_gaussian", "gaussian_sigma", "background_label", "normalized"}, "matrix_nms", {"bboxes", "scores", "score_threshold", "nms_top_k", "keep_top_k", "post_threshold", "use_gaussian", "gaussian_sigma", "background_label", "normalized"}, {}, {}, {}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "matrix_nms");
}

void MatrixNmsOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value bboxes_, pir::Value scores_, float score_threshold, int nms_top_k, int keep_top_k, float post_threshold, bool use_gaussian, float gaussian_sigma, int background_label, bool normalized) {
  VLOG(4) << "Start build MatrixNmsOp";



  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {bboxes_, scores_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_score_threshold = pir::FloatAttribute::get(pir::IrContext::Instance(), score_threshold);
  argument.AddAttribute("score_threshold", attr_score_threshold);
  pir::Attribute attr_nms_top_k = pir::Int32Attribute::get(pir::IrContext::Instance(), nms_top_k);
  argument.AddAttribute("nms_top_k", attr_nms_top_k);
  pir::Attribute attr_keep_top_k = pir::Int32Attribute::get(pir::IrContext::Instance(), keep_top_k);
  argument.AddAttribute("keep_top_k", attr_keep_top_k);
  pir::Attribute attr_post_threshold = pir::FloatAttribute::get(pir::IrContext::Instance(), post_threshold);
  argument.AddAttribute("post_threshold", attr_post_threshold);
  pir::Attribute attr_use_gaussian = pir::BoolAttribute::get(pir::IrContext::Instance(), use_gaussian);
  argument.AddAttribute("use_gaussian", attr_use_gaussian);
  pir::Attribute attr_gaussian_sigma = pir::FloatAttribute::get(pir::IrContext::Instance(), gaussian_sigma);
  argument.AddAttribute("gaussian_sigma", attr_gaussian_sigma);
  pir::Attribute attr_background_label = pir::Int32Attribute::get(pir::IrContext::Instance(), background_label);
  argument.AddAttribute("background_label", attr_background_label);
  pir::Attribute attr_normalized = pir::BoolAttribute::get(pir::IrContext::Instance(), normalized);
  argument.AddAttribute("normalized", attr_normalized);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType bboxes = bboxes_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)bboxes;
  paddle::dialect::DenseTensorType scores = scores_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)scores;

  VLOG(4) << "Builder construction  dense_bboxes";
  paddle::dialect::IrTensor ir_tensor_bboxes(paddle::dialect::TransToPhiDataType(bboxes.dtype()),
                                                      bboxes.dims(),
                                                      bboxes.data_layout(),
                                                      bboxes.lod(),
                                                      bboxes.offset());
  VLOG(4) << "Builder construction  meta_bboxes";
  paddle::dialect::IrMetaTensor meta_bboxes(&ir_tensor_bboxes);

  VLOG(4) << "Builder construction  dense_scores";
  paddle::dialect::IrTensor ir_tensor_scores(paddle::dialect::TransToPhiDataType(scores.dtype()),
                                                      scores.dims(),
                                                      scores.data_layout(),
                                                      scores.lod(),
                                                      scores.offset());
  VLOG(4) << "Builder construction  meta_scores";
  paddle::dialect::IrMetaTensor meta_scores(&ir_tensor_scores);
  paddle::dialect::IrTensor dense_out;
  paddle::dialect::IrMetaTensor meta_out(&dense_out);
  paddle::dialect::IrTensor dense_index;
  paddle::dialect::IrMetaTensor meta_index(&dense_index);
  paddle::dialect::IrTensor dense_roisnum;
  paddle::dialect::IrMetaTensor meta_roisnum(&dense_roisnum);

  phi::MatrixNMSInferMeta(meta_bboxes, meta_scores, score_threshold, nms_top_k, keep_top_k, post_threshold, use_gaussian, gaussian_sigma, background_label, normalized, &meta_out, &meta_index, &meta_roisnum);

  std::vector<pir::Type> argument_outputs;
  pir::Type out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_out.dtype()), dense_out.dims(), dense_out.layout(), dense_out.lod(), dense_out.offset());
  argument_outputs.push_back(out_dense_tensor_type);

  pir::Type index_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_index.dtype()), dense_index.dims(), dense_index.layout(), dense_index.lod(), dense_index.offset());
  argument_outputs.push_back(index_dense_tensor_type);

  pir::Type roisnum_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_roisnum.dtype()), dense_roisnum.dims(), dense_roisnum.layout(), dense_roisnum.lod(), dense_roisnum.offset());
  argument_outputs.push_back(roisnum_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void MatrixNmsOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value bboxes_, pir::Value scores_, pir::AttributeMap attributes) {
  VLOG(4) << "Start build MatrixNmsOp";


  IR_ENFORCE(
      attributes.find("score_threshold") != attributes.end(),
          "'score_threshold' Attribute is expected for MatrixNmsOp. ");
  float score_threshold = attributes.at("score_threshold").dyn_cast<pir::FloatAttribute>().data();

  IR_ENFORCE(
      attributes.find("nms_top_k") != attributes.end(),
          "'nms_top_k' Attribute is expected for MatrixNmsOp. ");
  int nms_top_k = attributes.at("nms_top_k").dyn_cast<pir::Int32Attribute>().data();

  IR_ENFORCE(
      attributes.find("keep_top_k") != attributes.end(),
          "'keep_top_k' Attribute is expected for MatrixNmsOp. ");
  int keep_top_k = attributes.at("keep_top_k").dyn_cast<pir::Int32Attribute>().data();

  IR_ENFORCE(
      attributes.find("post_threshold") != attributes.end(),
          "'post_threshold' Attribute is expected for MatrixNmsOp. ");
  float post_threshold = attributes.at("post_threshold").dyn_cast<pir::FloatAttribute>().data();

  IR_ENFORCE(
      attributes.find("use_gaussian") != attributes.end(),
          "'use_gaussian' Attribute is expected for MatrixNmsOp. ");
  bool use_gaussian = attributes.at("use_gaussian").dyn_cast<pir::BoolAttribute>().data();

  IR_ENFORCE(
      attributes.find("gaussian_sigma") != attributes.end(),
          "'gaussian_sigma' Attribute is expected for MatrixNmsOp. ");
  float gaussian_sigma = attributes.at("gaussian_sigma").dyn_cast<pir::FloatAttribute>().data();

  IR_ENFORCE(
      attributes.find("background_label") != attributes.end(),
          "'background_label' Attribute is expected for MatrixNmsOp. ");
  int background_label = attributes.at("background_label").dyn_cast<pir::Int32Attribute>().data();

  IR_ENFORCE(
      attributes.find("normalized") != attributes.end(),
          "'normalized' Attribute is expected for MatrixNmsOp. ");
  bool normalized = attributes.at("normalized").dyn_cast<pir::BoolAttribute>().data();


  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {bboxes_, scores_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_score_threshold = pir::FloatAttribute::get(pir::IrContext::Instance(), score_threshold);
  argument.AddAttribute("score_threshold", attr_score_threshold);
  pir::Attribute attr_nms_top_k = pir::Int32Attribute::get(pir::IrContext::Instance(), nms_top_k);
  argument.AddAttribute("nms_top_k", attr_nms_top_k);
  pir::Attribute attr_keep_top_k = pir::Int32Attribute::get(pir::IrContext::Instance(), keep_top_k);
  argument.AddAttribute("keep_top_k", attr_keep_top_k);
  pir::Attribute attr_post_threshold = pir::FloatAttribute::get(pir::IrContext::Instance(), post_threshold);
  argument.AddAttribute("post_threshold", attr_post_threshold);
  pir::Attribute attr_use_gaussian = pir::BoolAttribute::get(pir::IrContext::Instance(), use_gaussian);
  argument.AddAttribute("use_gaussian", attr_use_gaussian);
  pir::Attribute attr_gaussian_sigma = pir::FloatAttribute::get(pir::IrContext::Instance(), gaussian_sigma);
  argument.AddAttribute("gaussian_sigma", attr_gaussian_sigma);
  pir::Attribute attr_background_label = pir::Int32Attribute::get(pir::IrContext::Instance(), background_label);
  argument.AddAttribute("background_label", attr_background_label);
  pir::Attribute attr_normalized = pir::BoolAttribute::get(pir::IrContext::Instance(), normalized);
  argument.AddAttribute("normalized", attr_normalized);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType bboxes = bboxes_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)bboxes;
  paddle::dialect::DenseTensorType scores = scores_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)scores;

  VLOG(4) << "Builder construction  dense_bboxes";
  paddle::dialect::IrTensor ir_tensor_bboxes(paddle::dialect::TransToPhiDataType(bboxes.dtype()),
                                                      bboxes.dims(),
                                                      bboxes.data_layout(),
                                                      bboxes.lod(),
                                                      bboxes.offset());
  VLOG(4) << "Builder construction  meta_bboxes";
  paddle::dialect::IrMetaTensor meta_bboxes(&ir_tensor_bboxes);

  VLOG(4) << "Builder construction  dense_scores";
  paddle::dialect::IrTensor ir_tensor_scores(paddle::dialect::TransToPhiDataType(scores.dtype()),
                                                      scores.dims(),
                                                      scores.data_layout(),
                                                      scores.lod(),
                                                      scores.offset());
  VLOG(4) << "Builder construction  meta_scores";
  paddle::dialect::IrMetaTensor meta_scores(&ir_tensor_scores);
  paddle::dialect::IrTensor dense_out;
  paddle::dialect::IrMetaTensor meta_out(&dense_out);
  paddle::dialect::IrTensor dense_index;
  paddle::dialect::IrMetaTensor meta_index(&dense_index);
  paddle::dialect::IrTensor dense_roisnum;
  paddle::dialect::IrMetaTensor meta_roisnum(&dense_roisnum);

  phi::MatrixNMSInferMeta(meta_bboxes, meta_scores, score_threshold, nms_top_k, keep_top_k, post_threshold, use_gaussian, gaussian_sigma, background_label, normalized, &meta_out, &meta_index, &meta_roisnum);

  std::vector<pir::Type> argument_outputs;
  pir::Type out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_out.dtype()), dense_out.dims(), dense_out.layout(), dense_out.lod(), dense_out.offset());
  argument_outputs.push_back(out_dense_tensor_type);

  pir::Type index_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_index.dtype()), dense_index.dims(), dense_index.layout(), dense_index.lod(), dense_index.offset());
  argument_outputs.push_back(index_dense_tensor_type);

  pir::Type roisnum_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_roisnum.dtype()), dense_roisnum.dims(), dense_roisnum.layout(), dense_roisnum.lod(), dense_roisnum.offset());
  argument_outputs.push_back(roisnum_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void MatrixNmsOp::VerifySig() {
  VLOG(4) << "Start Verifying inputs, outputs and attributes for: MatrixNmsOp.";
  VLOG(4) << "Verifying inputs:";
  {
  auto input_size = num_operands();
  IR_ENFORCE(input_size == 2u,
                    "The size %d of inputs must be equal to 2.", input_size);
  IR_ENFORCE((*this)->operand_source(0).type().isa<paddle::dialect::DenseTensorType>(),
                  "Type validation failed for the 0th input, got %s.", (*this)->operand_source(0).type());
  IR_ENFORCE((*this)->operand_source(1).type().isa<paddle::dialect::DenseTensorType>(),
                  "Type validation failed for the 1th input, got %s.", (*this)->operand_source(1).type());
  }
  VLOG(4) << "Verifying attributes:";
  {
  auto& attributes = this->attributes();
  IR_ENFORCE(attributes.count("score_threshold")>0,
                 "score_threshold does not exist.");
  IR_ENFORCE(attributes.at("score_threshold").isa<pir::FloatAttribute>(),
                 "Type of attribute: score_threshold is not pir::FloatAttribute.");

  IR_ENFORCE(attributes.count("nms_top_k")>0,
                 "nms_top_k does not exist.");
  IR_ENFORCE(attributes.at("nms_top_k").isa<pir::Int32Attribute>(),
                 "Type of attribute: nms_top_k is not pir::Int32Attribute.");

  IR_ENFORCE(attributes.count("keep_top_k")>0,
                 "keep_top_k does not exist.");
  IR_ENFORCE(attributes.at("keep_top_k").isa<pir::Int32Attribute>(),
                 "Type of attribute: keep_top_k is not pir::Int32Attribute.");

  IR_ENFORCE(attributes.count("post_threshold")>0,
                 "post_threshold does not exist.");
  IR_ENFORCE(attributes.at("post_threshold").isa<pir::FloatAttribute>(),
                 "Type of attribute: post_threshold is not pir::FloatAttribute.");

  IR_ENFORCE(attributes.count("use_gaussian")>0,
                 "use_gaussian does not exist.");
  IR_ENFORCE(attributes.at("use_gaussian").isa<pir::BoolAttribute>(),
                 "Type of attribute: use_gaussian is not pir::BoolAttribute.");

  IR_ENFORCE(attributes.count("gaussian_sigma")>0,
                 "gaussian_sigma does not exist.");
  IR_ENFORCE(attributes.at("gaussian_sigma").isa<pir::FloatAttribute>(),
                 "Type of attribute: gaussian_sigma is not pir::FloatAttribute.");

  IR_ENFORCE(attributes.count("background_label")>0,
                 "background_label does not exist.");
  IR_ENFORCE(attributes.at("background_label").isa<pir::Int32Attribute>(),
                 "Type of attribute: background_label is not pir::Int32Attribute.");

  IR_ENFORCE(attributes.count("normalized")>0,
                 "normalized does not exist.");
  IR_ENFORCE(attributes.at("normalized").isa<pir::BoolAttribute>(),
                 "Type of attribute: normalized is not pir::BoolAttribute.");

  }
  VLOG(4) << "Verifying outputs:";
  {
  auto output_size = num_results();
  IR_ENFORCE(output_size == 3u,
                    "The size %d of outputs must be equal to 3.", output_size);
  IR_ENFORCE((*this)->result(0).type().isa<paddle::dialect::DenseTensorType>(),
                 "Type validation failed for the 0th output.");
  IR_ENFORCE((*this)->result(1).type().isa<paddle::dialect::DenseTensorType>(),
                 "Type validation failed for the 1th output.");
  if (auto output_2_type = (*this)->result(2).type()) {
    IR_ENFORCE(output_2_type.isa<paddle::dialect::DenseTensorType>(),
                   "Type validation failed for the 2th output.");
  }
  }
  VLOG(4) << "End Verifying for: MatrixNmsOp.";
}

void MatrixNmsOp::InferMeta( phi::InferMetaContext *infer_meta ) {
  auto fn = PD_INFER_META(phi::MatrixNMSInferMeta);
  fn(infer_meta);
}

phi::DataType MatrixNmsOp::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: MatrixNmsOp";
  


  return expected_kernel_dtype;
}

const char *MatrixPowerOp::attributes_name[1] = { "n" };

OpInfoTuple MatrixPowerOp::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("x", "paddle::dialect::DenseTensorType", false, false, false, true) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = { paddle::dialect::OpAttributeInfo("n", "pir::Int32Attribute", "") };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("out", "paddle::dialect::DenseTensorType", false, false) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("MatrixPowerInferMeta", {"x", "n"}, "matrix_power", {"x", "n"}, {}, {}, {}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "matrix_power");
}

void MatrixPowerOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, int n) {
  VLOG(4) << "Start build MatrixPowerOp";



  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_n = pir::Int32Attribute::get(pir::IrContext::Instance(), n);
  argument.AddAttribute("n", attr_n);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);
  paddle::dialect::IrTensor dense_out;
  paddle::dialect::IrMetaTensor meta_out(&dense_out);

  phi::MatrixPowerInferMeta(meta_x, n, &meta_out);

  std::vector<pir::Type> argument_outputs;
  pir::Type out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_out.dtype()), dense_out.dims(), dense_out.layout(), dense_out.lod(), dense_out.offset());
  argument_outputs.push_back(out_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void MatrixPowerOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, pir::AttributeMap attributes) {
  VLOG(4) << "Start build MatrixPowerOp";


  IR_ENFORCE(
      attributes.find("n") != attributes.end(),
          "'n' Attribute is expected for MatrixPowerOp. ");
  int n = attributes.at("n").dyn_cast<pir::Int32Attribute>().data();


  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_n = pir::Int32Attribute::get(pir::IrContext::Instance(), n);
  argument.AddAttribute("n", attr_n);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);
  paddle::dialect::IrTensor dense_out;
  paddle::dialect::IrMetaTensor meta_out(&dense_out);

  phi::MatrixPowerInferMeta(meta_x, n, &meta_out);

  std::vector<pir::Type> argument_outputs;
  pir::Type out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_out.dtype()), dense_out.dims(), dense_out.layout(), dense_out.lod(), dense_out.offset());
  argument_outputs.push_back(out_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void MatrixPowerOp::VerifySig() {
  VLOG(4) << "Start Verifying inputs, outputs and attributes for: MatrixPowerOp.";
  VLOG(4) << "Verifying inputs:";
  {
  auto input_size = num_operands();
  IR_ENFORCE(input_size == 1u,
                    "The size %d of inputs must be equal to 1.", input_size);
  IR_ENFORCE((*this)->operand_source(0).type().isa<paddle::dialect::DenseTensorType>(),
                  "Type validation failed for the 0th input, got %s.", (*this)->operand_source(0).type());
  }
  VLOG(4) << "Verifying attributes:";
  {
  auto& attributes = this->attributes();
  IR_ENFORCE(attributes.count("n")>0,
                 "n does not exist.");
  IR_ENFORCE(attributes.at("n").isa<pir::Int32Attribute>(),
                 "Type of attribute: n is not pir::Int32Attribute.");

  }
  VLOG(4) << "Verifying outputs:";
  {
  auto output_size = num_results();
  IR_ENFORCE(output_size == 1u,
                    "The size %d of outputs must be equal to 1.", output_size);
  IR_ENFORCE((*this)->result(0).type().isa<paddle::dialect::DenseTensorType>(),
                 "Type validation failed for the 0th output.");
  }
  VLOG(4) << "End Verifying for: MatrixPowerOp.";
}

void MatrixPowerOp::InferMeta( phi::InferMetaContext *infer_meta ) {
  auto fn = PD_INFER_META(phi::MatrixPowerInferMeta);
  fn(infer_meta);
}

phi::DataType MatrixPowerOp::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: MatrixPowerOp";
  


  return expected_kernel_dtype;
}

const char *MaxPool2dWithIndexOp::attributes_name[5] = { "kernel_size", "strides", "paddings", "global_pooling", "adaptive" };

OpInfoTuple MaxPool2dWithIndexOp::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("x", "paddle::dialect::DenseTensorType", false, false, false, true) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = { paddle::dialect::OpAttributeInfo("kernel_size", "pir::ArrayAttribute<pir::Int32Attribute>", ""), paddle::dialect::OpAttributeInfo("strides", "pir::ArrayAttribute<pir::Int32Attribute>", ""), paddle::dialect::OpAttributeInfo("paddings", "pir::ArrayAttribute<pir::Int32Attribute>", ""), paddle::dialect::OpAttributeInfo("global_pooling", "pir::BoolAttribute", ""), paddle::dialect::OpAttributeInfo("adaptive", "pir::BoolAttribute", "") };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("out", "paddle::dialect::DenseTensorType", false, false), paddle::dialect::OpOutputInfo("mask", "paddle::dialect::DenseTensorType", false, false) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("MaxPoolWithIndexInferMeta", {"x", "kernel_size", "strides", "paddings", "global_pooling", "adaptive"}, "max_pool2d_with_index", {"x", "kernel_size", "strides", "paddings", "global_pooling", "adaptive"}, {}, {}, {}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "max_pool2d_with_index");
}

void MaxPool2dWithIndexOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, const std::vector<int>& kernel_size, const std::vector<int>& strides, const std::vector<int>& paddings, bool global_pooling, bool adaptive) {
  VLOG(4) << "Start build MaxPool2dWithIndexOp";



  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  std::vector<pir::Attribute> vec_kernel_size;
  for (size_t i = 0; i < static_cast<size_t>(kernel_size.size()); i++) {
      pir::Attribute attr_kernel_size = pir::Int32Attribute::get(pir::IrContext::Instance(), kernel_size[i]);

    vec_kernel_size.push_back(attr_kernel_size);
  }
  pir::Attribute attr_kernel_size = pir::ArrayAttribute::get(pir::IrContext::Instance(), vec_kernel_size);
  argument.AddAttribute("kernel_size", attr_kernel_size);
  std::vector<pir::Attribute> vec_strides;
  for (size_t i = 0; i < static_cast<size_t>(strides.size()); i++) {
      pir::Attribute attr_strides = pir::Int32Attribute::get(pir::IrContext::Instance(), strides[i]);

    vec_strides.push_back(attr_strides);
  }
  pir::Attribute attr_strides = pir::ArrayAttribute::get(pir::IrContext::Instance(), vec_strides);
  argument.AddAttribute("strides", attr_strides);
  std::vector<pir::Attribute> vec_paddings;
  for (size_t i = 0; i < static_cast<size_t>(paddings.size()); i++) {
      pir::Attribute attr_paddings = pir::Int32Attribute::get(pir::IrContext::Instance(), paddings[i]);

    vec_paddings.push_back(attr_paddings);
  }
  pir::Attribute attr_paddings = pir::ArrayAttribute::get(pir::IrContext::Instance(), vec_paddings);
  argument.AddAttribute("paddings", attr_paddings);
  pir::Attribute attr_global_pooling = pir::BoolAttribute::get(pir::IrContext::Instance(), global_pooling);
  argument.AddAttribute("global_pooling", attr_global_pooling);
  pir::Attribute attr_adaptive = pir::BoolAttribute::get(pir::IrContext::Instance(), adaptive);
  argument.AddAttribute("adaptive", attr_adaptive);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);
  paddle::dialect::IrTensor dense_out;
  paddle::dialect::IrMetaTensor meta_out(&dense_out);
  paddle::dialect::IrTensor dense_mask;
  paddle::dialect::IrMetaTensor meta_mask(&dense_mask);

  phi::MaxPoolWithIndexInferMeta(meta_x, kernel_size, strides, paddings, global_pooling, adaptive, &meta_out, &meta_mask);

  std::vector<pir::Type> argument_outputs;
  pir::Type out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_out.dtype()), dense_out.dims(), dense_out.layout(), dense_out.lod(), dense_out.offset());
  argument_outputs.push_back(out_dense_tensor_type);

  pir::Type mask_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_mask.dtype()), dense_mask.dims(), dense_mask.layout(), dense_mask.lod(), dense_mask.offset());
  argument_outputs.push_back(mask_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void MaxPool2dWithIndexOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, pir::AttributeMap attributes) {
  VLOG(4) << "Start build MaxPool2dWithIndexOp";


  IR_ENFORCE(
      attributes.find("kernel_size") != attributes.end(),
          "'kernel_size' Attribute is expected for MaxPool2dWithIndexOp. ");
  std::vector<int> kernel_size;
  for (size_t i = 0; i < attributes.at("kernel_size").dyn_cast<pir::ArrayAttribute>().size(); i++) {
    kernel_size.push_back(attributes.at("kernel_size").dyn_cast<pir::ArrayAttribute>().at(i).dyn_cast<pir::Int32Attribute>().data());
  }

  IR_ENFORCE(
      attributes.find("strides") != attributes.end(),
          "'strides' Attribute is expected for MaxPool2dWithIndexOp. ");
  std::vector<int> strides;
  for (size_t i = 0; i < attributes.at("strides").dyn_cast<pir::ArrayAttribute>().size(); i++) {
    strides.push_back(attributes.at("strides").dyn_cast<pir::ArrayAttribute>().at(i).dyn_cast<pir::Int32Attribute>().data());
  }

  IR_ENFORCE(
      attributes.find("paddings") != attributes.end(),
          "'paddings' Attribute is expected for MaxPool2dWithIndexOp. ");
  std::vector<int> paddings;
  for (size_t i = 0; i < attributes.at("paddings").dyn_cast<pir::ArrayAttribute>().size(); i++) {
    paddings.push_back(attributes.at("paddings").dyn_cast<pir::ArrayAttribute>().at(i).dyn_cast<pir::Int32Attribute>().data());
  }

  IR_ENFORCE(
      attributes.find("global_pooling") != attributes.end(),
          "'global_pooling' Attribute is expected for MaxPool2dWithIndexOp. ");
  bool global_pooling = attributes.at("global_pooling").dyn_cast<pir::BoolAttribute>().data();

  IR_ENFORCE(
      attributes.find("adaptive") != attributes.end(),
          "'adaptive' Attribute is expected for MaxPool2dWithIndexOp. ");
  bool adaptive = attributes.at("adaptive").dyn_cast<pir::BoolAttribute>().data();


  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  std::vector<pir::Attribute> vec_kernel_size;
  for (size_t i = 0; i < static_cast<size_t>(kernel_size.size()); i++) {
      pir::Attribute attr_kernel_size = pir::Int32Attribute::get(pir::IrContext::Instance(), kernel_size[i]);

    vec_kernel_size.push_back(attr_kernel_size);
  }
  pir::Attribute attr_kernel_size = pir::ArrayAttribute::get(pir::IrContext::Instance(), vec_kernel_size);
  argument.AddAttribute("kernel_size", attr_kernel_size);
  std::vector<pir::Attribute> vec_strides;
  for (size_t i = 0; i < static_cast<size_t>(strides.size()); i++) {
      pir::Attribute attr_strides = pir::Int32Attribute::get(pir::IrContext::Instance(), strides[i]);

    vec_strides.push_back(attr_strides);
  }
  pir::Attribute attr_strides = pir::ArrayAttribute::get(pir::IrContext::Instance(), vec_strides);
  argument.AddAttribute("strides", attr_strides);
  std::vector<pir::Attribute> vec_paddings;
  for (size_t i = 0; i < static_cast<size_t>(paddings.size()); i++) {
      pir::Attribute attr_paddings = pir::Int32Attribute::get(pir::IrContext::Instance(), paddings[i]);

    vec_paddings.push_back(attr_paddings);
  }
  pir::Attribute attr_paddings = pir::ArrayAttribute::get(pir::IrContext::Instance(), vec_paddings);
  argument.AddAttribute("paddings", attr_paddings);
  pir::Attribute attr_global_pooling = pir::BoolAttribute::get(pir::IrContext::Instance(), global_pooling);
  argument.AddAttribute("global_pooling", attr_global_pooling);
  pir::Attribute attr_adaptive = pir::BoolAttribute::get(pir::IrContext::Instance(), adaptive);
  argument.AddAttribute("adaptive", attr_adaptive);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);
  paddle::dialect::IrTensor dense_out;
  paddle::dialect::IrMetaTensor meta_out(&dense_out);
  paddle::dialect::IrTensor dense_mask;
  paddle::dialect::IrMetaTensor meta_mask(&dense_mask);

  phi::MaxPoolWithIndexInferMeta(meta_x, kernel_size, strides, paddings, global_pooling, adaptive, &meta_out, &meta_mask);

  std::vector<pir::Type> argument_outputs;
  pir::Type out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_out.dtype()), dense_out.dims(), dense_out.layout(), dense_out.lod(), dense_out.offset());
  argument_outputs.push_back(out_dense_tensor_type);

  pir::Type mask_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_mask.dtype()), dense_mask.dims(), dense_mask.layout(), dense_mask.lod(), dense_mask.offset());
  argument_outputs.push_back(mask_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void MaxPool2dWithIndexOp::VerifySig() {
  VLOG(4) << "Start Verifying inputs, outputs and attributes for: MaxPool2dWithIndexOp.";
  VLOG(4) << "Verifying inputs:";
  {
  auto input_size = num_operands();
  IR_ENFORCE(input_size == 1u,
                    "The size %d of inputs must be equal to 1.", input_size);
  IR_ENFORCE((*this)->operand_source(0).type().isa<paddle::dialect::DenseTensorType>(),
                  "Type validation failed for the 0th input, got %s.", (*this)->operand_source(0).type());
  }
  VLOG(4) << "Verifying attributes:";
  {
  auto& attributes = this->attributes();
  IR_ENFORCE(attributes.count("kernel_size")>0,
                 "kernel_size does not exist.");
  IR_ENFORCE(attributes.at("kernel_size").isa<pir::ArrayAttribute>(),
                 "Type of attribute: kernel_size is not pir::ArrayAttribute.");
  for (size_t i = 0; i < attributes.at("kernel_size").dyn_cast<pir::ArrayAttribute>().size(); i++) {
    IR_ENFORCE(attributes.at("kernel_size").dyn_cast<pir::ArrayAttribute>().at(i).isa<pir::Int32Attribute>(),
                   "Type of attribute: kernel_size is not right.");
  }
  IR_ENFORCE(attributes.count("strides")>0,
                 "strides does not exist.");
  IR_ENFORCE(attributes.at("strides").isa<pir::ArrayAttribute>(),
                 "Type of attribute: strides is not pir::ArrayAttribute.");
  for (size_t i = 0; i < attributes.at("strides").dyn_cast<pir::ArrayAttribute>().size(); i++) {
    IR_ENFORCE(attributes.at("strides").dyn_cast<pir::ArrayAttribute>().at(i).isa<pir::Int32Attribute>(),
                   "Type of attribute: strides is not right.");
  }
  IR_ENFORCE(attributes.count("paddings")>0,
                 "paddings does not exist.");
  IR_ENFORCE(attributes.at("paddings").isa<pir::ArrayAttribute>(),
                 "Type of attribute: paddings is not pir::ArrayAttribute.");
  for (size_t i = 0; i < attributes.at("paddings").dyn_cast<pir::ArrayAttribute>().size(); i++) {
    IR_ENFORCE(attributes.at("paddings").dyn_cast<pir::ArrayAttribute>().at(i).isa<pir::Int32Attribute>(),
                   "Type of attribute: paddings is not right.");
  }
  IR_ENFORCE(attributes.count("global_pooling")>0,
                 "global_pooling does not exist.");
  IR_ENFORCE(attributes.at("global_pooling").isa<pir::BoolAttribute>(),
                 "Type of attribute: global_pooling is not pir::BoolAttribute.");

  IR_ENFORCE(attributes.count("adaptive")>0,
                 "adaptive does not exist.");
  IR_ENFORCE(attributes.at("adaptive").isa<pir::BoolAttribute>(),
                 "Type of attribute: adaptive is not pir::BoolAttribute.");

  }
  VLOG(4) << "Verifying outputs:";
  {
  auto output_size = num_results();
  IR_ENFORCE(output_size == 2u,
                    "The size %d of outputs must be equal to 2.", output_size);
  IR_ENFORCE((*this)->result(0).type().isa<paddle::dialect::DenseTensorType>(),
                 "Type validation failed for the 0th output.");
  IR_ENFORCE((*this)->result(1).type().isa<paddle::dialect::DenseTensorType>(),
                 "Type validation failed for the 1th output.");
  }
  VLOG(4) << "End Verifying for: MaxPool2dWithIndexOp.";
}

void MaxPool2dWithIndexOp::InferMeta( phi::InferMetaContext *infer_meta ) {
  auto fn = PD_INFER_META(phi::MaxPoolWithIndexInferMeta);
  fn(infer_meta);
}

phi::DataType MaxPool2dWithIndexOp::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: MaxPool2dWithIndexOp";
  


  return expected_kernel_dtype;
}

const char *MaxPool3dWithIndexOp::attributes_name[5] = { "kernel_size", "strides", "paddings", "global_pooling", "adaptive" };

OpInfoTuple MaxPool3dWithIndexOp::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("x", "paddle::dialect::DenseTensorType", false, false, false, true) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = { paddle::dialect::OpAttributeInfo("kernel_size", "pir::ArrayAttribute<pir::Int32Attribute>", ""), paddle::dialect::OpAttributeInfo("strides", "pir::ArrayAttribute<pir::Int32Attribute>", ""), paddle::dialect::OpAttributeInfo("paddings", "pir::ArrayAttribute<pir::Int32Attribute>", ""), paddle::dialect::OpAttributeInfo("global_pooling", "pir::BoolAttribute", ""), paddle::dialect::OpAttributeInfo("adaptive", "pir::BoolAttribute", "") };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("out", "paddle::dialect::DenseTensorType", false, false), paddle::dialect::OpOutputInfo("mask", "paddle::dialect::DenseTensorType", false, false) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("MaxPoolWithIndexInferMeta", {"x", "kernel_size", "strides", "paddings", "global_pooling", "adaptive"}, "max_pool3d_with_index", {"x", "kernel_size", "strides", "paddings", "global_pooling", "adaptive"}, {}, {}, {}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "max_pool3d_with_index");
}

void MaxPool3dWithIndexOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, const std::vector<int>& kernel_size, const std::vector<int>& strides, const std::vector<int>& paddings, bool global_pooling, bool adaptive) {
  VLOG(4) << "Start build MaxPool3dWithIndexOp";



  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  std::vector<pir::Attribute> vec_kernel_size;
  for (size_t i = 0; i < static_cast<size_t>(kernel_size.size()); i++) {
      pir::Attribute attr_kernel_size = pir::Int32Attribute::get(pir::IrContext::Instance(), kernel_size[i]);

    vec_kernel_size.push_back(attr_kernel_size);
  }
  pir::Attribute attr_kernel_size = pir::ArrayAttribute::get(pir::IrContext::Instance(), vec_kernel_size);
  argument.AddAttribute("kernel_size", attr_kernel_size);
  std::vector<pir::Attribute> vec_strides;
  for (size_t i = 0; i < static_cast<size_t>(strides.size()); i++) {
      pir::Attribute attr_strides = pir::Int32Attribute::get(pir::IrContext::Instance(), strides[i]);

    vec_strides.push_back(attr_strides);
  }
  pir::Attribute attr_strides = pir::ArrayAttribute::get(pir::IrContext::Instance(), vec_strides);
  argument.AddAttribute("strides", attr_strides);
  std::vector<pir::Attribute> vec_paddings;
  for (size_t i = 0; i < static_cast<size_t>(paddings.size()); i++) {
      pir::Attribute attr_paddings = pir::Int32Attribute::get(pir::IrContext::Instance(), paddings[i]);

    vec_paddings.push_back(attr_paddings);
  }
  pir::Attribute attr_paddings = pir::ArrayAttribute::get(pir::IrContext::Instance(), vec_paddings);
  argument.AddAttribute("paddings", attr_paddings);
  pir::Attribute attr_global_pooling = pir::BoolAttribute::get(pir::IrContext::Instance(), global_pooling);
  argument.AddAttribute("global_pooling", attr_global_pooling);
  pir::Attribute attr_adaptive = pir::BoolAttribute::get(pir::IrContext::Instance(), adaptive);
  argument.AddAttribute("adaptive", attr_adaptive);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);
  paddle::dialect::IrTensor dense_out;
  paddle::dialect::IrMetaTensor meta_out(&dense_out);
  paddle::dialect::IrTensor dense_mask;
  paddle::dialect::IrMetaTensor meta_mask(&dense_mask);

  phi::MaxPoolWithIndexInferMeta(meta_x, kernel_size, strides, paddings, global_pooling, adaptive, &meta_out, &meta_mask);

  std::vector<pir::Type> argument_outputs;
  pir::Type out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_out.dtype()), dense_out.dims(), dense_out.layout(), dense_out.lod(), dense_out.offset());
  argument_outputs.push_back(out_dense_tensor_type);

  pir::Type mask_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_mask.dtype()), dense_mask.dims(), dense_mask.layout(), dense_mask.lod(), dense_mask.offset());
  argument_outputs.push_back(mask_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void MaxPool3dWithIndexOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, pir::AttributeMap attributes) {
  VLOG(4) << "Start build MaxPool3dWithIndexOp";


  IR_ENFORCE(
      attributes.find("kernel_size") != attributes.end(),
          "'kernel_size' Attribute is expected for MaxPool3dWithIndexOp. ");
  std::vector<int> kernel_size;
  for (size_t i = 0; i < attributes.at("kernel_size").dyn_cast<pir::ArrayAttribute>().size(); i++) {
    kernel_size.push_back(attributes.at("kernel_size").dyn_cast<pir::ArrayAttribute>().at(i).dyn_cast<pir::Int32Attribute>().data());
  }

  IR_ENFORCE(
      attributes.find("strides") != attributes.end(),
          "'strides' Attribute is expected for MaxPool3dWithIndexOp. ");
  std::vector<int> strides;
  for (size_t i = 0; i < attributes.at("strides").dyn_cast<pir::ArrayAttribute>().size(); i++) {
    strides.push_back(attributes.at("strides").dyn_cast<pir::ArrayAttribute>().at(i).dyn_cast<pir::Int32Attribute>().data());
  }

  IR_ENFORCE(
      attributes.find("paddings") != attributes.end(),
          "'paddings' Attribute is expected for MaxPool3dWithIndexOp. ");
  std::vector<int> paddings;
  for (size_t i = 0; i < attributes.at("paddings").dyn_cast<pir::ArrayAttribute>().size(); i++) {
    paddings.push_back(attributes.at("paddings").dyn_cast<pir::ArrayAttribute>().at(i).dyn_cast<pir::Int32Attribute>().data());
  }

  IR_ENFORCE(
      attributes.find("global_pooling") != attributes.end(),
          "'global_pooling' Attribute is expected for MaxPool3dWithIndexOp. ");
  bool global_pooling = attributes.at("global_pooling").dyn_cast<pir::BoolAttribute>().data();

  IR_ENFORCE(
      attributes.find("adaptive") != attributes.end(),
          "'adaptive' Attribute is expected for MaxPool3dWithIndexOp. ");
  bool adaptive = attributes.at("adaptive").dyn_cast<pir::BoolAttribute>().data();


  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  std::vector<pir::Attribute> vec_kernel_size;
  for (size_t i = 0; i < static_cast<size_t>(kernel_size.size()); i++) {
      pir::Attribute attr_kernel_size = pir::Int32Attribute::get(pir::IrContext::Instance(), kernel_size[i]);

    vec_kernel_size.push_back(attr_kernel_size);
  }
  pir::Attribute attr_kernel_size = pir::ArrayAttribute::get(pir::IrContext::Instance(), vec_kernel_size);
  argument.AddAttribute("kernel_size", attr_kernel_size);
  std::vector<pir::Attribute> vec_strides;
  for (size_t i = 0; i < static_cast<size_t>(strides.size()); i++) {
      pir::Attribute attr_strides = pir::Int32Attribute::get(pir::IrContext::Instance(), strides[i]);

    vec_strides.push_back(attr_strides);
  }
  pir::Attribute attr_strides = pir::ArrayAttribute::get(pir::IrContext::Instance(), vec_strides);
  argument.AddAttribute("strides", attr_strides);
  std::vector<pir::Attribute> vec_paddings;
  for (size_t i = 0; i < static_cast<size_t>(paddings.size()); i++) {
      pir::Attribute attr_paddings = pir::Int32Attribute::get(pir::IrContext::Instance(), paddings[i]);

    vec_paddings.push_back(attr_paddings);
  }
  pir::Attribute attr_paddings = pir::ArrayAttribute::get(pir::IrContext::Instance(), vec_paddings);
  argument.AddAttribute("paddings", attr_paddings);
  pir::Attribute attr_global_pooling = pir::BoolAttribute::get(pir::IrContext::Instance(), global_pooling);
  argument.AddAttribute("global_pooling", attr_global_pooling);
  pir::Attribute attr_adaptive = pir::BoolAttribute::get(pir::IrContext::Instance(), adaptive);
  argument.AddAttribute("adaptive", attr_adaptive);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);
  paddle::dialect::IrTensor dense_out;
  paddle::dialect::IrMetaTensor meta_out(&dense_out);
  paddle::dialect::IrTensor dense_mask;
  paddle::dialect::IrMetaTensor meta_mask(&dense_mask);

  phi::MaxPoolWithIndexInferMeta(meta_x, kernel_size, strides, paddings, global_pooling, adaptive, &meta_out, &meta_mask);

  std::vector<pir::Type> argument_outputs;
  pir::Type out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_out.dtype()), dense_out.dims(), dense_out.layout(), dense_out.lod(), dense_out.offset());
  argument_outputs.push_back(out_dense_tensor_type);

  pir::Type mask_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_mask.dtype()), dense_mask.dims(), dense_mask.layout(), dense_mask.lod(), dense_mask.offset());
  argument_outputs.push_back(mask_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void MaxPool3dWithIndexOp::VerifySig() {
  VLOG(4) << "Start Verifying inputs, outputs and attributes for: MaxPool3dWithIndexOp.";
  VLOG(4) << "Verifying inputs:";
  {
  auto input_size = num_operands();
  IR_ENFORCE(input_size == 1u,
                    "The size %d of inputs must be equal to 1.", input_size);
  IR_ENFORCE((*this)->operand_source(0).type().isa<paddle::dialect::DenseTensorType>(),
                  "Type validation failed for the 0th input, got %s.", (*this)->operand_source(0).type());
  }
  VLOG(4) << "Verifying attributes:";
  {
  auto& attributes = this->attributes();
  IR_ENFORCE(attributes.count("kernel_size")>0,
                 "kernel_size does not exist.");
  IR_ENFORCE(attributes.at("kernel_size").isa<pir::ArrayAttribute>(),
                 "Type of attribute: kernel_size is not pir::ArrayAttribute.");
  for (size_t i = 0; i < attributes.at("kernel_size").dyn_cast<pir::ArrayAttribute>().size(); i++) {
    IR_ENFORCE(attributes.at("kernel_size").dyn_cast<pir::ArrayAttribute>().at(i).isa<pir::Int32Attribute>(),
                   "Type of attribute: kernel_size is not right.");
  }
  IR_ENFORCE(attributes.count("strides")>0,
                 "strides does not exist.");
  IR_ENFORCE(attributes.at("strides").isa<pir::ArrayAttribute>(),
                 "Type of attribute: strides is not pir::ArrayAttribute.");
  for (size_t i = 0; i < attributes.at("strides").dyn_cast<pir::ArrayAttribute>().size(); i++) {
    IR_ENFORCE(attributes.at("strides").dyn_cast<pir::ArrayAttribute>().at(i).isa<pir::Int32Attribute>(),
                   "Type of attribute: strides is not right.");
  }
  IR_ENFORCE(attributes.count("paddings")>0,
                 "paddings does not exist.");
  IR_ENFORCE(attributes.at("paddings").isa<pir::ArrayAttribute>(),
                 "Type of attribute: paddings is not pir::ArrayAttribute.");
  for (size_t i = 0; i < attributes.at("paddings").dyn_cast<pir::ArrayAttribute>().size(); i++) {
    IR_ENFORCE(attributes.at("paddings").dyn_cast<pir::ArrayAttribute>().at(i).isa<pir::Int32Attribute>(),
                   "Type of attribute: paddings is not right.");
  }
  IR_ENFORCE(attributes.count("global_pooling")>0,
                 "global_pooling does not exist.");
  IR_ENFORCE(attributes.at("global_pooling").isa<pir::BoolAttribute>(),
                 "Type of attribute: global_pooling is not pir::BoolAttribute.");

  IR_ENFORCE(attributes.count("adaptive")>0,
                 "adaptive does not exist.");
  IR_ENFORCE(attributes.at("adaptive").isa<pir::BoolAttribute>(),
                 "Type of attribute: adaptive is not pir::BoolAttribute.");

  }
  VLOG(4) << "Verifying outputs:";
  {
  auto output_size = num_results();
  IR_ENFORCE(output_size == 2u,
                    "The size %d of outputs must be equal to 2.", output_size);
  IR_ENFORCE((*this)->result(0).type().isa<paddle::dialect::DenseTensorType>(),
                 "Type validation failed for the 0th output.");
  IR_ENFORCE((*this)->result(1).type().isa<paddle::dialect::DenseTensorType>(),
                 "Type validation failed for the 1th output.");
  }
  VLOG(4) << "End Verifying for: MaxPool3dWithIndexOp.";
}

void MaxPool3dWithIndexOp::InferMeta( phi::InferMetaContext *infer_meta ) {
  auto fn = PD_INFER_META(phi::MaxPoolWithIndexInferMeta);
  fn(infer_meta);
}

phi::DataType MaxPool3dWithIndexOp::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: MaxPool3dWithIndexOp";
  


  return expected_kernel_dtype;
}

const char *MaxoutOp::attributes_name[2] = { "groups", "axis" };

OpInfoTuple MaxoutOp::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("x", "paddle::dialect::DenseTensorType", false, false, false, true) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = { paddle::dialect::OpAttributeInfo("groups", "pir::Int32Attribute", ""), paddle::dialect::OpAttributeInfo("axis", "pir::Int32Attribute", "") };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("out", "paddle::dialect::DenseTensorType", false, false) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("MaxOutInferMeta", {"x", "groups", "axis"}, "maxout", {"x", "groups", "axis"}, {}, {}, {}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "maxout");
}

void MaxoutOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, int groups, int axis) {
  VLOG(4) << "Start build MaxoutOp";



  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_groups = pir::Int32Attribute::get(pir::IrContext::Instance(), groups);
  argument.AddAttribute("groups", attr_groups);
  pir::Attribute attr_axis = pir::Int32Attribute::get(pir::IrContext::Instance(), axis);
  argument.AddAttribute("axis", attr_axis);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);
  paddle::dialect::IrTensor dense_out;
  paddle::dialect::IrMetaTensor meta_out(&dense_out);

  phi::MaxOutInferMeta(meta_x, groups, axis, &meta_out);

  std::vector<pir::Type> argument_outputs;
  pir::Type out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_out.dtype()), dense_out.dims(), dense_out.layout(), dense_out.lod(), dense_out.offset());
  argument_outputs.push_back(out_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void MaxoutOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, pir::AttributeMap attributes) {
  VLOG(4) << "Start build MaxoutOp";


  IR_ENFORCE(
      attributes.find("groups") != attributes.end(),
          "'groups' Attribute is expected for MaxoutOp. ");
  int groups = attributes.at("groups").dyn_cast<pir::Int32Attribute>().data();

  IR_ENFORCE(
      attributes.find("axis") != attributes.end(),
          "'axis' Attribute is expected for MaxoutOp. ");
  int axis = attributes.at("axis").dyn_cast<pir::Int32Attribute>().data();


  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_groups = pir::Int32Attribute::get(pir::IrContext::Instance(), groups);
  argument.AddAttribute("groups", attr_groups);
  pir::Attribute attr_axis = pir::Int32Attribute::get(pir::IrContext::Instance(), axis);
  argument.AddAttribute("axis", attr_axis);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);
  paddle::dialect::IrTensor dense_out;
  paddle::dialect::IrMetaTensor meta_out(&dense_out);

  phi::MaxOutInferMeta(meta_x, groups, axis, &meta_out);

  std::vector<pir::Type> argument_outputs;
  pir::Type out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_out.dtype()), dense_out.dims(), dense_out.layout(), dense_out.lod(), dense_out.offset());
  argument_outputs.push_back(out_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void MaxoutOp::VerifySig() {
  VLOG(4) << "Start Verifying inputs, outputs and attributes for: MaxoutOp.";
  VLOG(4) << "Verifying inputs:";
  {
  auto input_size = num_operands();
  IR_ENFORCE(input_size == 1u,
                    "The size %d of inputs must be equal to 1.", input_size);
  IR_ENFORCE((*this)->operand_source(0).type().isa<paddle::dialect::DenseTensorType>(),
                  "Type validation failed for the 0th input, got %s.", (*this)->operand_source(0).type());
  }
  VLOG(4) << "Verifying attributes:";
  {
  auto& attributes = this->attributes();
  IR_ENFORCE(attributes.count("groups")>0,
                 "groups does not exist.");
  IR_ENFORCE(attributes.at("groups").isa<pir::Int32Attribute>(),
                 "Type of attribute: groups is not pir::Int32Attribute.");

  IR_ENFORCE(attributes.count("axis")>0,
                 "axis does not exist.");
  IR_ENFORCE(attributes.at("axis").isa<pir::Int32Attribute>(),
                 "Type of attribute: axis is not pir::Int32Attribute.");

  }
  VLOG(4) << "Verifying outputs:";
  {
  auto output_size = num_results();
  IR_ENFORCE(output_size == 1u,
                    "The size %d of outputs must be equal to 1.", output_size);
  IR_ENFORCE((*this)->result(0).type().isa<paddle::dialect::DenseTensorType>(),
                 "Type validation failed for the 0th output.");
  }
  VLOG(4) << "End Verifying for: MaxoutOp.";
}

void MaxoutOp::InferMeta( phi::InferMetaContext *infer_meta ) {
  auto fn = PD_INFER_META(phi::MaxOutInferMeta);
  fn(infer_meta);
}

phi::DataType MaxoutOp::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: MaxoutOp";
  


  return expected_kernel_dtype;
}

OpInfoTuple MeanAllOp::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("x", "paddle::dialect::DenseTensorType", false, false, false, true) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = {  };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("out", "paddle::dialect::DenseTensorType", false, false) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("MeanAllInferMeta", {"x"}, "mean_all", {"x"}, {}, {}, {}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "mean_all");
}

void MeanAllOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_) {
  VLOG(4) << "Start build MeanAllOp";



  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);
  paddle::dialect::IrTensor dense_out;
  paddle::dialect::IrMetaTensor meta_out(&dense_out);

  phi::MeanAllInferMeta(meta_x, &meta_out);

  std::vector<pir::Type> argument_outputs;
  pir::Type out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_out.dtype()), dense_out.dims(), dense_out.layout(), dense_out.lod(), dense_out.offset());
  argument_outputs.push_back(out_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void MeanAllOp::VerifySig() {
  VLOG(4) << "Start Verifying inputs, outputs and attributes for: MeanAllOp.";
  VLOG(4) << "Verifying inputs:";
  {
  auto input_size = num_operands();
  IR_ENFORCE(input_size == 1u,
                    "The size %d of inputs must be equal to 1.", input_size);
  IR_ENFORCE((*this)->operand_source(0).type().isa<paddle::dialect::DenseTensorType>(),
                  "Type validation failed for the 0th input, got %s.", (*this)->operand_source(0).type());
  }
  VLOG(4) << "Verifying attributes:";
  {
  // Attributes num is 0, not need to check attributes type.
  }
  VLOG(4) << "Verifying outputs:";
  {
  auto output_size = num_results();
  IR_ENFORCE(output_size == 1u,
                    "The size %d of outputs must be equal to 1.", output_size);
  IR_ENFORCE((*this)->result(0).type().isa<paddle::dialect::DenseTensorType>(),
                 "Type validation failed for the 0th output.");
  }
  VLOG(4) << "End Verifying for: MeanAllOp.";
}

void MeanAllOp::InferMeta( phi::InferMetaContext *infer_meta ) {
  auto fn = PD_INFER_META(phi::MeanAllInferMeta);
  fn(infer_meta);
}

phi::DataType MeanAllOp::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: MeanAllOp";
  


  return expected_kernel_dtype;
}

const char *MemoryEfficientAttentionOp::attributes_name[6] = { "max_seqlen_q", "max_seqlen_k", "causal", "dropout_p", "scale", "is_test" };

OpInfoTuple MemoryEfficientAttentionOp::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("query", "paddle::dialect::DenseTensorType", false, false, false, true), paddle::dialect::OpInputInfo("key", "paddle::dialect::DenseTensorType", false, false, false, true), paddle::dialect::OpInputInfo("value", "paddle::dialect::DenseTensorType", false, false, false, true), paddle::dialect::OpInputInfo("bias", "paddle::dialect::DenseTensorType", true, false, false, true), paddle::dialect::OpInputInfo("cu_seqlens_q", "paddle::dialect::DenseTensorType", true, false, false, false), paddle::dialect::OpInputInfo("cu_seqlens_k", "paddle::dialect::DenseTensorType", true, false, false, false), paddle::dialect::OpInputInfo("causal_diagonal", "paddle::dialect::DenseTensorType", true, false, false, false), paddle::dialect::OpInputInfo("seqlen_k", "paddle::dialect::DenseTensorType", true, false, false, false) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = { paddle::dialect::OpAttributeInfo("max_seqlen_q", "paddle::dialect::ScalarAttribute", "float"), paddle::dialect::OpAttributeInfo("max_seqlen_k", "paddle::dialect::ScalarAttribute", "float"), paddle::dialect::OpAttributeInfo("causal", "pir::BoolAttribute", ""), paddle::dialect::OpAttributeInfo("dropout_p", "pir::DoubleAttribute", ""), paddle::dialect::OpAttributeInfo("scale", "pir::FloatAttribute", ""), paddle::dialect::OpAttributeInfo("is_test", "pir::BoolAttribute", "") };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("output", "paddle::dialect::DenseTensorType", false, false), paddle::dialect::OpOutputInfo("logsumexp", "paddle::dialect::DenseTensorType", false, false), paddle::dialect::OpOutputInfo("seed_and_offset", "paddle::dialect::DenseTensorType", false, false) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("MemoryEfficientAttentionInferMeta", {"query", "key", "value", "bias", "cu_seqlens_q", "cu_seqlens_k", "causal_diagonal", "seqlen_k", "max_seqlen_q", "max_seqlen_k", "causal", "dropout_p", "scale", "is_test"}, "memory_efficient_attention", {"query", "key", "value", "bias", "cu_seqlens_q", "cu_seqlens_k", "causal_diagonal", "seqlen_k", "max_seqlen_q", "max_seqlen_k", "causal", "dropout_p", "scale", "is_test"}, {"query"}, {}, {}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "memory_efficient_attention");
}

void MemoryEfficientAttentionOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value query_, pir::Value key_, pir::Value value_, pir::Value bias_, pir::Value cu_seqlens_q_, pir::Value cu_seqlens_k_, pir::Value causal_diagonal_, pir::Value seqlen_k_, float max_seqlen_q, float max_seqlen_k, bool causal, double dropout_p, float scale, bool is_test) {
  VLOG(4) << "Start build MemoryEfficientAttentionOp";



  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {query_, key_, value_, bias_, cu_seqlens_q_, cu_seqlens_k_, causal_diagonal_, seqlen_k_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_max_seqlen_q = paddle::dialect::TransToIrAttribute(max_seqlen_q, pir::IrContext::Instance());
  argument.AddAttribute("max_seqlen_q", attr_max_seqlen_q);
  pir::Attribute attr_max_seqlen_k = paddle::dialect::TransToIrAttribute(max_seqlen_k, pir::IrContext::Instance());
  argument.AddAttribute("max_seqlen_k", attr_max_seqlen_k);
  pir::Attribute attr_causal = pir::BoolAttribute::get(pir::IrContext::Instance(), causal);
  argument.AddAttribute("causal", attr_causal);
  pir::Attribute attr_dropout_p = pir::DoubleAttribute::get(pir::IrContext::Instance(), dropout_p);
  argument.AddAttribute("dropout_p", attr_dropout_p);
  pir::Attribute attr_scale = pir::FloatAttribute::get(pir::IrContext::Instance(), scale);
  argument.AddAttribute("scale", attr_scale);
  pir::Attribute attr_is_test = pir::BoolAttribute::get(pir::IrContext::Instance(), is_test);
  argument.AddAttribute("is_test", attr_is_test);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType query = query_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)query;
  paddle::dialect::DenseTensorType key = key_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)key;
  paddle::dialect::DenseTensorType value = value_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)value;

  VLOG(4) << "Builder construction  dense_query";
  paddle::dialect::IrTensor ir_tensor_query(paddle::dialect::TransToPhiDataType(query.dtype()),
                                                      query.dims(),
                                                      query.data_layout(),
                                                      query.lod(),
                                                      query.offset());
  VLOG(4) << "Builder construction  meta_query";
  paddle::dialect::IrMetaTensor meta_query(&ir_tensor_query);

  VLOG(4) << "Builder construction  dense_key";
  paddle::dialect::IrTensor ir_tensor_key(paddle::dialect::TransToPhiDataType(key.dtype()),
                                                      key.dims(),
                                                      key.data_layout(),
                                                      key.lod(),
                                                      key.offset());
  VLOG(4) << "Builder construction  meta_key";
  paddle::dialect::IrMetaTensor meta_key(&ir_tensor_key);

  VLOG(4) << "Builder construction  dense_value";
  paddle::dialect::IrTensor ir_tensor_value(paddle::dialect::TransToPhiDataType(value.dtype()),
                                                      value.dims(),
                                                      value.data_layout(),
                                                      value.lod(),
                                                      value.offset());
  VLOG(4) << "Builder construction  meta_value";
  paddle::dialect::IrMetaTensor meta_value(&ir_tensor_value);

  paddle::dialect::IrMetaTensor meta_bias;
  paddle::dialect::IrTensor ir_tensor_bias;
  if (bias_.impl() != nullptr) {
    paddle::dialect::DenseTensorType bias = bias_.type().dyn_cast<paddle::dialect::DenseTensorType>();
    VLOG(4) << "Builder construction  dense_bias";
    ir_tensor_bias = paddle::dialect::IrTensor(paddle::dialect::TransToPhiDataType(bias.dtype()),
                                                        bias.dims(),
                                                        bias.data_layout(),
                                                        bias.lod(),
                                                        bias.offset());
    VLOG(4) << "Builder construction  meta_bias";
    meta_bias = paddle::dialect::IrMetaTensor(&ir_tensor_bias);
  }


  paddle::dialect::IrMetaTensor meta_cu_seqlens_q;
  paddle::dialect::IrTensor ir_tensor_cu_seqlens_q;
  if (cu_seqlens_q_.impl() != nullptr) {
    paddle::dialect::DenseTensorType cu_seqlens_q = cu_seqlens_q_.type().dyn_cast<paddle::dialect::DenseTensorType>();
    VLOG(4) << "Builder construction  dense_cu_seqlens_q";
    ir_tensor_cu_seqlens_q = paddle::dialect::IrTensor(paddle::dialect::TransToPhiDataType(cu_seqlens_q.dtype()),
                                                        cu_seqlens_q.dims(),
                                                        cu_seqlens_q.data_layout(),
                                                        cu_seqlens_q.lod(),
                                                        cu_seqlens_q.offset());
    VLOG(4) << "Builder construction  meta_cu_seqlens_q";
    meta_cu_seqlens_q = paddle::dialect::IrMetaTensor(&ir_tensor_cu_seqlens_q);
  }


  paddle::dialect::IrMetaTensor meta_cu_seqlens_k;
  paddle::dialect::IrTensor ir_tensor_cu_seqlens_k;
  if (cu_seqlens_k_.impl() != nullptr) {
    paddle::dialect::DenseTensorType cu_seqlens_k = cu_seqlens_k_.type().dyn_cast<paddle::dialect::DenseTensorType>();
    VLOG(4) << "Builder construction  dense_cu_seqlens_k";
    ir_tensor_cu_seqlens_k = paddle::dialect::IrTensor(paddle::dialect::TransToPhiDataType(cu_seqlens_k.dtype()),
                                                        cu_seqlens_k.dims(),
                                                        cu_seqlens_k.data_layout(),
                                                        cu_seqlens_k.lod(),
                                                        cu_seqlens_k.offset());
    VLOG(4) << "Builder construction  meta_cu_seqlens_k";
    meta_cu_seqlens_k = paddle::dialect::IrMetaTensor(&ir_tensor_cu_seqlens_k);
  }


  paddle::dialect::IrMetaTensor meta_causal_diagonal;
  paddle::dialect::IrTensor ir_tensor_causal_diagonal;
  if (causal_diagonal_.impl() != nullptr) {
    paddle::dialect::DenseTensorType causal_diagonal = causal_diagonal_.type().dyn_cast<paddle::dialect::DenseTensorType>();
    VLOG(4) << "Builder construction  dense_causal_diagonal";
    ir_tensor_causal_diagonal = paddle::dialect::IrTensor(paddle::dialect::TransToPhiDataType(causal_diagonal.dtype()),
                                                        causal_diagonal.dims(),
                                                        causal_diagonal.data_layout(),
                                                        causal_diagonal.lod(),
                                                        causal_diagonal.offset());
    VLOG(4) << "Builder construction  meta_causal_diagonal";
    meta_causal_diagonal = paddle::dialect::IrMetaTensor(&ir_tensor_causal_diagonal);
  }


  paddle::dialect::IrMetaTensor meta_seqlen_k;
  paddle::dialect::IrTensor ir_tensor_seqlen_k;
  if (seqlen_k_.impl() != nullptr) {
    paddle::dialect::DenseTensorType seqlen_k = seqlen_k_.type().dyn_cast<paddle::dialect::DenseTensorType>();
    VLOG(4) << "Builder construction  dense_seqlen_k";
    ir_tensor_seqlen_k = paddle::dialect::IrTensor(paddle::dialect::TransToPhiDataType(seqlen_k.dtype()),
                                                        seqlen_k.dims(),
                                                        seqlen_k.data_layout(),
                                                        seqlen_k.lod(),
                                                        seqlen_k.offset());
    VLOG(4) << "Builder construction  meta_seqlen_k";
    meta_seqlen_k = paddle::dialect::IrMetaTensor(&ir_tensor_seqlen_k);
  }

  paddle::dialect::IrTensor dense_output;
  paddle::dialect::IrMetaTensor meta_output(&dense_output);
  paddle::dialect::IrTensor dense_logsumexp;
  paddle::dialect::IrMetaTensor meta_logsumexp(&dense_logsumexp);
  paddle::dialect::IrTensor dense_seed_and_offset;
  paddle::dialect::IrMetaTensor meta_seed_and_offset(&dense_seed_and_offset);

  phi::MemoryEfficientAttentionInferMeta(meta_query, meta_key, meta_value, meta_bias, meta_cu_seqlens_q, meta_cu_seqlens_k, meta_causal_diagonal, meta_seqlen_k, max_seqlen_q, max_seqlen_k, causal, dropout_p, scale, is_test, &meta_output, &meta_logsumexp, &meta_seed_and_offset);

  std::vector<pir::Type> argument_outputs;
  pir::Type output_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_output.dtype()), dense_output.dims(), dense_output.layout(), dense_output.lod(), dense_output.offset());
  argument_outputs.push_back(output_dense_tensor_type);

  pir::Type logsumexp_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_logsumexp.dtype()), dense_logsumexp.dims(), dense_logsumexp.layout(), dense_logsumexp.lod(), dense_logsumexp.offset());
  argument_outputs.push_back(logsumexp_dense_tensor_type);

  pir::Type seed_and_offset_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_seed_and_offset.dtype()), dense_seed_and_offset.dims(), dense_seed_and_offset.layout(), dense_seed_and_offset.lod(), dense_seed_and_offset.offset());
  argument_outputs.push_back(seed_and_offset_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void MemoryEfficientAttentionOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value query_, pir::Value key_, pir::Value value_, pir::Value bias_, pir::Value cu_seqlens_q_, pir::Value cu_seqlens_k_, pir::Value causal_diagonal_, pir::Value seqlen_k_, pir::AttributeMap attributes) {
  VLOG(4) << "Start build MemoryEfficientAttentionOp";


  IR_ENFORCE(
      attributes.find("max_seqlen_q") != attributes.end(),
          "'max_seqlen_q' Attribute is expected for MemoryEfficientAttentionOp. ");
  float max_seqlen_q = attributes.at("max_seqlen_q").dyn_cast<paddle::dialect::ScalarAttribute>().data().to<float>();

  IR_ENFORCE(
      attributes.find("max_seqlen_k") != attributes.end(),
          "'max_seqlen_k' Attribute is expected for MemoryEfficientAttentionOp. ");
  float max_seqlen_k = attributes.at("max_seqlen_k").dyn_cast<paddle::dialect::ScalarAttribute>().data().to<float>();

  IR_ENFORCE(
      attributes.find("causal") != attributes.end(),
          "'causal' Attribute is expected for MemoryEfficientAttentionOp. ");
  bool causal = attributes.at("causal").dyn_cast<pir::BoolAttribute>().data();

  IR_ENFORCE(
      attributes.find("dropout_p") != attributes.end(),
          "'dropout_p' Attribute is expected for MemoryEfficientAttentionOp. ");
  double dropout_p = attributes.at("dropout_p").dyn_cast<pir::DoubleAttribute>().data();

  IR_ENFORCE(
      attributes.find("scale") != attributes.end(),
          "'scale' Attribute is expected for MemoryEfficientAttentionOp. ");
  float scale = attributes.at("scale").dyn_cast<pir::FloatAttribute>().data();

  IR_ENFORCE(
      attributes.find("is_test") != attributes.end(),
          "'is_test' Attribute is expected for MemoryEfficientAttentionOp. ");
  bool is_test = attributes.at("is_test").dyn_cast<pir::BoolAttribute>().data();


  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {query_, key_, value_, bias_, cu_seqlens_q_, cu_seqlens_k_, causal_diagonal_, seqlen_k_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_max_seqlen_q = paddle::dialect::TransToIrAttribute(max_seqlen_q, pir::IrContext::Instance());
  argument.AddAttribute("max_seqlen_q", attr_max_seqlen_q);
  pir::Attribute attr_max_seqlen_k = paddle::dialect::TransToIrAttribute(max_seqlen_k, pir::IrContext::Instance());
  argument.AddAttribute("max_seqlen_k", attr_max_seqlen_k);
  pir::Attribute attr_causal = pir::BoolAttribute::get(pir::IrContext::Instance(), causal);
  argument.AddAttribute("causal", attr_causal);
  pir::Attribute attr_dropout_p = pir::DoubleAttribute::get(pir::IrContext::Instance(), dropout_p);
  argument.AddAttribute("dropout_p", attr_dropout_p);
  pir::Attribute attr_scale = pir::FloatAttribute::get(pir::IrContext::Instance(), scale);
  argument.AddAttribute("scale", attr_scale);
  pir::Attribute attr_is_test = pir::BoolAttribute::get(pir::IrContext::Instance(), is_test);
  argument.AddAttribute("is_test", attr_is_test);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType query = query_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)query;
  paddle::dialect::DenseTensorType key = key_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)key;
  paddle::dialect::DenseTensorType value = value_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)value;

  VLOG(4) << "Builder construction  dense_query";
  paddle::dialect::IrTensor ir_tensor_query(paddle::dialect::TransToPhiDataType(query.dtype()),
                                                      query.dims(),
                                                      query.data_layout(),
                                                      query.lod(),
                                                      query.offset());
  VLOG(4) << "Builder construction  meta_query";
  paddle::dialect::IrMetaTensor meta_query(&ir_tensor_query);

  VLOG(4) << "Builder construction  dense_key";
  paddle::dialect::IrTensor ir_tensor_key(paddle::dialect::TransToPhiDataType(key.dtype()),
                                                      key.dims(),
                                                      key.data_layout(),
                                                      key.lod(),
                                                      key.offset());
  VLOG(4) << "Builder construction  meta_key";
  paddle::dialect::IrMetaTensor meta_key(&ir_tensor_key);

  VLOG(4) << "Builder construction  dense_value";
  paddle::dialect::IrTensor ir_tensor_value(paddle::dialect::TransToPhiDataType(value.dtype()),
                                                      value.dims(),
                                                      value.data_layout(),
                                                      value.lod(),
                                                      value.offset());
  VLOG(4) << "Builder construction  meta_value";
  paddle::dialect::IrMetaTensor meta_value(&ir_tensor_value);

  paddle::dialect::IrMetaTensor meta_bias;
  paddle::dialect::IrTensor ir_tensor_bias;
  if (bias_.impl() != nullptr) {
    paddle::dialect::DenseTensorType bias = bias_.type().dyn_cast<paddle::dialect::DenseTensorType>();
    VLOG(4) << "Builder construction  dense_bias";
    ir_tensor_bias = paddle::dialect::IrTensor(paddle::dialect::TransToPhiDataType(bias.dtype()),
                                                        bias.dims(),
                                                        bias.data_layout(),
                                                        bias.lod(),
                                                        bias.offset());
    VLOG(4) << "Builder construction  meta_bias";
    meta_bias = paddle::dialect::IrMetaTensor(&ir_tensor_bias);
  }


  paddle::dialect::IrMetaTensor meta_cu_seqlens_q;
  paddle::dialect::IrTensor ir_tensor_cu_seqlens_q;
  if (cu_seqlens_q_.impl() != nullptr) {
    paddle::dialect::DenseTensorType cu_seqlens_q = cu_seqlens_q_.type().dyn_cast<paddle::dialect::DenseTensorType>();
    VLOG(4) << "Builder construction  dense_cu_seqlens_q";
    ir_tensor_cu_seqlens_q = paddle::dialect::IrTensor(paddle::dialect::TransToPhiDataType(cu_seqlens_q.dtype()),
                                                        cu_seqlens_q.dims(),
                                                        cu_seqlens_q.data_layout(),
                                                        cu_seqlens_q.lod(),
                                                        cu_seqlens_q.offset());
    VLOG(4) << "Builder construction  meta_cu_seqlens_q";
    meta_cu_seqlens_q = paddle::dialect::IrMetaTensor(&ir_tensor_cu_seqlens_q);
  }


  paddle::dialect::IrMetaTensor meta_cu_seqlens_k;
  paddle::dialect::IrTensor ir_tensor_cu_seqlens_k;
  if (cu_seqlens_k_.impl() != nullptr) {
    paddle::dialect::DenseTensorType cu_seqlens_k = cu_seqlens_k_.type().dyn_cast<paddle::dialect::DenseTensorType>();
    VLOG(4) << "Builder construction  dense_cu_seqlens_k";
    ir_tensor_cu_seqlens_k = paddle::dialect::IrTensor(paddle::dialect::TransToPhiDataType(cu_seqlens_k.dtype()),
                                                        cu_seqlens_k.dims(),
                                                        cu_seqlens_k.data_layout(),
                                                        cu_seqlens_k.lod(),
                                                        cu_seqlens_k.offset());
    VLOG(4) << "Builder construction  meta_cu_seqlens_k";
    meta_cu_seqlens_k = paddle::dialect::IrMetaTensor(&ir_tensor_cu_seqlens_k);
  }


  paddle::dialect::IrMetaTensor meta_causal_diagonal;
  paddle::dialect::IrTensor ir_tensor_causal_diagonal;
  if (causal_diagonal_.impl() != nullptr) {
    paddle::dialect::DenseTensorType causal_diagonal = causal_diagonal_.type().dyn_cast<paddle::dialect::DenseTensorType>();
    VLOG(4) << "Builder construction  dense_causal_diagonal";
    ir_tensor_causal_diagonal = paddle::dialect::IrTensor(paddle::dialect::TransToPhiDataType(causal_diagonal.dtype()),
                                                        causal_diagonal.dims(),
                                                        causal_diagonal.data_layout(),
                                                        causal_diagonal.lod(),
                                                        causal_diagonal.offset());
    VLOG(4) << "Builder construction  meta_causal_diagonal";
    meta_causal_diagonal = paddle::dialect::IrMetaTensor(&ir_tensor_causal_diagonal);
  }


  paddle::dialect::IrMetaTensor meta_seqlen_k;
  paddle::dialect::IrTensor ir_tensor_seqlen_k;
  if (seqlen_k_.impl() != nullptr) {
    paddle::dialect::DenseTensorType seqlen_k = seqlen_k_.type().dyn_cast<paddle::dialect::DenseTensorType>();
    VLOG(4) << "Builder construction  dense_seqlen_k";
    ir_tensor_seqlen_k = paddle::dialect::IrTensor(paddle::dialect::TransToPhiDataType(seqlen_k.dtype()),
                                                        seqlen_k.dims(),
                                                        seqlen_k.data_layout(),
                                                        seqlen_k.lod(),
                                                        seqlen_k.offset());
    VLOG(4) << "Builder construction  meta_seqlen_k";
    meta_seqlen_k = paddle::dialect::IrMetaTensor(&ir_tensor_seqlen_k);
  }

  paddle::dialect::IrTensor dense_output;
  paddle::dialect::IrMetaTensor meta_output(&dense_output);
  paddle::dialect::IrTensor dense_logsumexp;
  paddle::dialect::IrMetaTensor meta_logsumexp(&dense_logsumexp);
  paddle::dialect::IrTensor dense_seed_and_offset;
  paddle::dialect::IrMetaTensor meta_seed_and_offset(&dense_seed_and_offset);

  phi::MemoryEfficientAttentionInferMeta(meta_query, meta_key, meta_value, meta_bias, meta_cu_seqlens_q, meta_cu_seqlens_k, meta_causal_diagonal, meta_seqlen_k, max_seqlen_q, max_seqlen_k, causal, dropout_p, scale, is_test, &meta_output, &meta_logsumexp, &meta_seed_and_offset);

  std::vector<pir::Type> argument_outputs;
  pir::Type output_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_output.dtype()), dense_output.dims(), dense_output.layout(), dense_output.lod(), dense_output.offset());
  argument_outputs.push_back(output_dense_tensor_type);

  pir::Type logsumexp_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_logsumexp.dtype()), dense_logsumexp.dims(), dense_logsumexp.layout(), dense_logsumexp.lod(), dense_logsumexp.offset());
  argument_outputs.push_back(logsumexp_dense_tensor_type);

  pir::Type seed_and_offset_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_seed_and_offset.dtype()), dense_seed_and_offset.dims(), dense_seed_and_offset.layout(), dense_seed_and_offset.lod(), dense_seed_and_offset.offset());
  argument_outputs.push_back(seed_and_offset_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void MemoryEfficientAttentionOp::VerifySig() {
  VLOG(4) << "Start Verifying inputs, outputs and attributes for: MemoryEfficientAttentionOp.";
  VLOG(4) << "Verifying inputs:";
  {
  auto input_size = num_operands();
  IR_ENFORCE(input_size == 8u,
                    "The size %d of inputs must be equal to 8.", input_size);
  IR_ENFORCE((*this)->operand_source(0).type().isa<paddle::dialect::DenseTensorType>(),
                  "Type validation failed for the 0th input, got %s.", (*this)->operand_source(0).type());
  IR_ENFORCE((*this)->operand_source(1).type().isa<paddle::dialect::DenseTensorType>(),
                  "Type validation failed for the 1th input, got %s.", (*this)->operand_source(1).type());
  IR_ENFORCE((*this)->operand_source(2).type().isa<paddle::dialect::DenseTensorType>(),
                  "Type validation failed for the 2th input, got %s.", (*this)->operand_source(2).type());
  if (auto val = (*this)->operand(3)) {
    IR_ENFORCE(val.type().isa<paddle::dialect::DenseTensorType>(),
                   "Type validation failed for the 3th input, got %s.", (*this)->operand_source(3).type());
  }
  if (auto val = (*this)->operand(4)) {
    IR_ENFORCE(val.type().isa<paddle::dialect::DenseTensorType>(),
                   "Type validation failed for the 4th input, got %s.", (*this)->operand_source(4).type());
  }
  if (auto val = (*this)->operand(5)) {
    IR_ENFORCE(val.type().isa<paddle::dialect::DenseTensorType>(),
                   "Type validation failed for the 5th input, got %s.", (*this)->operand_source(5).type());
  }
  if (auto val = (*this)->operand(6)) {
    IR_ENFORCE(val.type().isa<paddle::dialect::DenseTensorType>(),
                   "Type validation failed for the 6th input, got %s.", (*this)->operand_source(6).type());
  }
  if (auto val = (*this)->operand(7)) {
    IR_ENFORCE(val.type().isa<paddle::dialect::DenseTensorType>(),
                   "Type validation failed for the 7th input, got %s.", (*this)->operand_source(7).type());
  }
  }
  VLOG(4) << "Verifying attributes:";
  {
  auto& attributes = this->attributes();
  IR_ENFORCE(attributes.count("max_seqlen_q")>0,
                 "max_seqlen_q does not exist.");
  IR_ENFORCE(attributes.at("max_seqlen_q").isa<paddle::dialect::ScalarAttribute>(),
                 "Type of attribute: max_seqlen_q is not paddle::dialect::ScalarAttribute.");

  IR_ENFORCE(attributes.count("max_seqlen_k")>0,
                 "max_seqlen_k does not exist.");
  IR_ENFORCE(attributes.at("max_seqlen_k").isa<paddle::dialect::ScalarAttribute>(),
                 "Type of attribute: max_seqlen_k is not paddle::dialect::ScalarAttribute.");

  IR_ENFORCE(attributes.count("causal")>0,
                 "causal does not exist.");
  IR_ENFORCE(attributes.at("causal").isa<pir::BoolAttribute>(),
                 "Type of attribute: causal is not pir::BoolAttribute.");

  IR_ENFORCE(attributes.count("dropout_p")>0,
                 "dropout_p does not exist.");
  IR_ENFORCE(attributes.at("dropout_p").isa<pir::DoubleAttribute>(),
                 "Type of attribute: dropout_p is not pir::DoubleAttribute.");

  IR_ENFORCE(attributes.count("scale")>0,
                 "scale does not exist.");
  IR_ENFORCE(attributes.at("scale").isa<pir::FloatAttribute>(),
                 "Type of attribute: scale is not pir::FloatAttribute.");

  IR_ENFORCE(attributes.count("is_test")>0,
                 "is_test does not exist.");
  IR_ENFORCE(attributes.at("is_test").isa<pir::BoolAttribute>(),
                 "Type of attribute: is_test is not pir::BoolAttribute.");

  }
  VLOG(4) << "Verifying outputs:";
  {
  auto output_size = num_results();
  IR_ENFORCE(output_size == 3u,
                    "The size %d of outputs must be equal to 3.", output_size);
  IR_ENFORCE((*this)->result(0).type().isa<paddle::dialect::DenseTensorType>(),
                 "Type validation failed for the 0th output.");
  IR_ENFORCE((*this)->result(1).type().isa<paddle::dialect::DenseTensorType>(),
                 "Type validation failed for the 1th output.");
  IR_ENFORCE((*this)->result(2).type().isa<paddle::dialect::DenseTensorType>(),
                 "Type validation failed for the 2th output.");
  }
  VLOG(4) << "End Verifying for: MemoryEfficientAttentionOp.";
}

void MemoryEfficientAttentionOp::InferMeta( phi::InferMetaContext *infer_meta ) {
  auto fn = PD_INFER_META(phi::MemoryEfficientAttentionInferMeta);
  fn(infer_meta);
}

phi::DataType MemoryEfficientAttentionOp::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: MemoryEfficientAttentionOp";
  


  return expected_kernel_dtype;
}

OpInfoTuple MergeSelectedRowsOp::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("x", "paddle::dialect::SelectedRowsType", false, false, false, false) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = {  };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("out", "paddle::dialect::SelectedRowsType", false, false) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("UnchangedInferMeta", {"x"}, "merge_selected_rows", {"x"}, {}, {}, {}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "merge_selected_rows");
}

void MergeSelectedRowsOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_) {
  VLOG(4) << "Start build MergeSelectedRowsOp";



  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::SelectedRowsType x = x_.type().dyn_cast<paddle::dialect::SelectedRowsType>(); (void)x;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);
  paddle::dialect::IrSelectedRows dense_out;
  paddle::dialect::IrMetaTensor meta_out(&dense_out);

  phi::UnchangedInferMeta(meta_x, &meta_out);

  std::vector<pir::Type> argument_outputs;
  pir::Type out_dense_tensor_type = paddle::dialect::SelectedRowsType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_out.dtype()), dense_out.dims(), dense_out.layout(), dense_out.lod(), dense_out.offset());
  argument_outputs.push_back(out_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void MergeSelectedRowsOp::VerifySig() {
  VLOG(4) << "Start Verifying inputs, outputs and attributes for: MergeSelectedRowsOp.";
  VLOG(4) << "Verifying inputs:";
  {
  auto input_size = num_operands();
  IR_ENFORCE(input_size == 1u,
                    "The size %d of inputs must be equal to 1.", input_size);
  IR_ENFORCE((*this)->operand_source(0).type().isa<paddle::dialect::SelectedRowsType>(),
                  "Type validation failed for the 0th input, got %s.", (*this)->operand_source(0).type());
  }
  VLOG(4) << "Verifying attributes:";
  {
  // Attributes num is 0, not need to check attributes type.
  }
  VLOG(4) << "Verifying outputs:";
  {
  auto output_size = num_results();
  IR_ENFORCE(output_size == 1u,
                    "The size %d of outputs must be equal to 1.", output_size);
  IR_ENFORCE((*this)->result(0).type().isa<paddle::dialect::SelectedRowsType>(),
                 "Type validation failed for the 0th output.");
  }
  VLOG(4) << "End Verifying for: MergeSelectedRowsOp.";
}

void MergeSelectedRowsOp::InferMeta( phi::InferMetaContext *infer_meta ) {
  auto fn = PD_INFER_META(phi::UnchangedInferMeta);
  fn(infer_meta);
}

phi::DataType MergeSelectedRowsOp::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: MergeSelectedRowsOp";
  


  return expected_kernel_dtype;
}

const char *MergedAdam_Op::attributes_name[2] = { "multi_precision", "use_global_beta_pow" };

OpInfoTuple MergedAdam_Op::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("param", "pir::VectorType<paddle::dialect::DenseTensorType>", false, false, false, false), paddle::dialect::OpInputInfo("grad", "pir::VectorType<paddle::dialect::DenseTensorType>", false, false, false, false), paddle::dialect::OpInputInfo("learning_rate", "pir::VectorType<paddle::dialect::DenseTensorType>", false, false, false, false), paddle::dialect::OpInputInfo("moment1", "pir::VectorType<paddle::dialect::DenseTensorType>", false, false, false, false), paddle::dialect::OpInputInfo("moment2", "pir::VectorType<paddle::dialect::DenseTensorType>", false, false, false, false), paddle::dialect::OpInputInfo("beta1_pow", "pir::VectorType<paddle::dialect::DenseTensorType>", false, false, false, false), paddle::dialect::OpInputInfo("beta2_pow", "pir::VectorType<paddle::dialect::DenseTensorType>", false, false, false, false), paddle::dialect::OpInputInfo("master_param", "pir::VectorType<paddle::dialect::DenseTensorType>", true, false, false, false), paddle::dialect::OpInputInfo("beta1", "paddle::dialect::ScalarAttribute", false, false, true, false), paddle::dialect::OpInputInfo("beta2", "paddle::dialect::ScalarAttribute", false, false, true, false), paddle::dialect::OpInputInfo("epsilon", "paddle::dialect::ScalarAttribute", false, false, true, false) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = { paddle::dialect::OpAttributeInfo("multi_precision", "pir::BoolAttribute", ""), paddle::dialect::OpAttributeInfo("use_global_beta_pow", "pir::BoolAttribute", "") };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("param_out", "pir::VectorType<paddle::dialect::DenseTensorType>", false, false), paddle::dialect::OpOutputInfo("moment1_out", "pir::VectorType<paddle::dialect::DenseTensorType>", false, false), paddle::dialect::OpOutputInfo("moment2_out", "pir::VectorType<paddle::dialect::DenseTensorType>", false, false), paddle::dialect::OpOutputInfo("beta1_pow_out", "pir::VectorType<paddle::dialect::DenseTensorType>", false, false), paddle::dialect::OpOutputInfo("beta2_pow_out", "pir::VectorType<paddle::dialect::DenseTensorType>", false, false), paddle::dialect::OpOutputInfo("master_param_out", "pir::VectorType<paddle::dialect::DenseTensorType>", true, false) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("MergedAdamInferMeta", {"param", "grad", "learning_rate", "moment1", "moment2", "beta1_pow", "beta2_pow", "master_param", "beta1", "beta2", "epsilon", "multi_precision", "use_global_beta_pow"}, "merged_adam", {"param", "grad", "learning_rate", "moment1", "moment2", "beta1_pow", "beta2_pow", "master_param", "beta1", "beta2", "epsilon", "multi_precision", "use_global_beta_pow"}, {"param"}, {}, {{"param_out", "param"},{"moment1_out", "moment1"},{"moment2_out", "moment2"},{"beta1_pow_out", "beta1_pow"},{"beta2_pow_out", "beta2_pow"},{"master_param_out", "master_param"}}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "merged_adam_");
}

void MergedAdam_Op::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value param_, pir::Value grad_, pir::Value learning_rate_, pir::Value moment1_, pir::Value moment2_, pir::Value beta1_pow_, pir::Value beta2_pow_, pir::Value master_param_, float beta1, float beta2, float epsilon, bool multi_precision, bool use_global_beta_pow) {
  VLOG(4) << "Start build MergedAdam_Op";


  // Generate scalar mutable attribute: beta1
  paddle::dialect::FullOp full_beta1_op = builder.Build<paddle::dialect::FullOp>(std::vector<int64_t>{1}, beta1, phi::DataType::FLOAT32, phi::CPUPlace());
  pir::OpResult beta1_ = full_beta1_op->result(0);
      // Generate scalar mutable attribute: beta2
  paddle::dialect::FullOp full_beta2_op = builder.Build<paddle::dialect::FullOp>(std::vector<int64_t>{1}, beta2, phi::DataType::FLOAT32, phi::CPUPlace());
  pir::OpResult beta2_ = full_beta2_op->result(0);
      // Generate scalar mutable attribute: epsilon
  paddle::dialect::FullOp full_epsilon_op = builder.Build<paddle::dialect::FullOp>(std::vector<int64_t>{1}, epsilon, phi::DataType::FLOAT32, phi::CPUPlace());
  pir::OpResult epsilon_ = full_epsilon_op->result(0);
    
  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {param_, grad_, learning_rate_, moment1_, moment2_, beta1_pow_, beta2_pow_, master_param_, beta1_, beta2_, epsilon_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_multi_precision = pir::BoolAttribute::get(pir::IrContext::Instance(), multi_precision);
  argument.AddAttribute("multi_precision", attr_multi_precision);
  pir::Attribute attr_use_global_beta_pow = pir::BoolAttribute::get(pir::IrContext::Instance(), use_global_beta_pow);
  argument.AddAttribute("use_global_beta_pow", attr_use_global_beta_pow);

  VLOG(4) << "Builder construction outputs";
  pir::VectorType param = param_.type().dyn_cast<pir::VectorType>(); (void)param;
  pir::VectorType grad = grad_.type().dyn_cast<pir::VectorType>(); (void)grad;
  pir::VectorType learning_rate = learning_rate_.type().dyn_cast<pir::VectorType>(); (void)learning_rate;
  pir::VectorType moment1 = moment1_.type().dyn_cast<pir::VectorType>(); (void)moment1;
  pir::VectorType moment2 = moment2_.type().dyn_cast<pir::VectorType>(); (void)moment2;
  pir::VectorType beta1_pow = beta1_pow_.type().dyn_cast<pir::VectorType>(); (void)beta1_pow;
  pir::VectorType beta2_pow = beta2_pow_.type().dyn_cast<pir::VectorType>(); (void)beta2_pow;
  std::vector<paddle::dialect::IrTensor> vec_ir_tensor_param;
  for (size_t i=0; i < static_cast<size_t>(param.size()); i++) {
    vec_ir_tensor_param.push_back(paddle::dialect::IrTensor(paddle::dialect::TransToPhiDataType(param[i].dyn_cast<paddle::dialect::DenseTensorType>().dtype()),
                                                                     param[i].dyn_cast<paddle::dialect::DenseTensorType>().dims(),
                                                                     param[i].dyn_cast<paddle::dialect::DenseTensorType>().data_layout(),
                                                                     param[i].dyn_cast<paddle::dialect::DenseTensorType>().lod(),
                                                                     param[i].dyn_cast<paddle::dialect::DenseTensorType>().offset()));
  }
  std::vector<paddle::dialect::IrMetaTensor> vec_meta_param;
  for (size_t i=0; i < vec_ir_tensor_param.size(); i++) {
    vec_meta_param.push_back(paddle::dialect::IrMetaTensor(&vec_ir_tensor_param[i]));
  }

  std::vector<const phi::MetaTensor*> meta_param;
  for (size_t i=0; i < static_cast<size_t>(vec_meta_param.size()); i++) {
    meta_param.push_back(&vec_meta_param[i]);
  }
   std::vector<paddle::dialect::IrTensor> vec_ir_tensor_grad;
  for (size_t i=0; i < static_cast<size_t>(grad.size()); i++) {
    vec_ir_tensor_grad.push_back(paddle::dialect::IrTensor(paddle::dialect::TransToPhiDataType(grad[i].dyn_cast<paddle::dialect::DenseTensorType>().dtype()),
                                                                     grad[i].dyn_cast<paddle::dialect::DenseTensorType>().dims(),
                                                                     grad[i].dyn_cast<paddle::dialect::DenseTensorType>().data_layout(),
                                                                     grad[i].dyn_cast<paddle::dialect::DenseTensorType>().lod(),
                                                                     grad[i].dyn_cast<paddle::dialect::DenseTensorType>().offset()));
  }
  std::vector<paddle::dialect::IrMetaTensor> vec_meta_grad;
  for (size_t i=0; i < vec_ir_tensor_grad.size(); i++) {
    vec_meta_grad.push_back(paddle::dialect::IrMetaTensor(&vec_ir_tensor_grad[i]));
  }

  std::vector<const phi::MetaTensor*> meta_grad;
  for (size_t i=0; i < static_cast<size_t>(vec_meta_grad.size()); i++) {
    meta_grad.push_back(&vec_meta_grad[i]);
  }
   std::vector<paddle::dialect::IrTensor> vec_ir_tensor_learning_rate;
  for (size_t i=0; i < static_cast<size_t>(learning_rate.size()); i++) {
    vec_ir_tensor_learning_rate.push_back(paddle::dialect::IrTensor(paddle::dialect::TransToPhiDataType(learning_rate[i].dyn_cast<paddle::dialect::DenseTensorType>().dtype()),
                                                                     learning_rate[i].dyn_cast<paddle::dialect::DenseTensorType>().dims(),
                                                                     learning_rate[i].dyn_cast<paddle::dialect::DenseTensorType>().data_layout(),
                                                                     learning_rate[i].dyn_cast<paddle::dialect::DenseTensorType>().lod(),
                                                                     learning_rate[i].dyn_cast<paddle::dialect::DenseTensorType>().offset()));
  }
  std::vector<paddle::dialect::IrMetaTensor> vec_meta_learning_rate;
  for (size_t i=0; i < vec_ir_tensor_learning_rate.size(); i++) {
    vec_meta_learning_rate.push_back(paddle::dialect::IrMetaTensor(&vec_ir_tensor_learning_rate[i]));
  }

  std::vector<const phi::MetaTensor*> meta_learning_rate;
  for (size_t i=0; i < static_cast<size_t>(vec_meta_learning_rate.size()); i++) {
    meta_learning_rate.push_back(&vec_meta_learning_rate[i]);
  }
   std::vector<paddle::dialect::IrTensor> vec_ir_tensor_moment1;
  for (size_t i=0; i < static_cast<size_t>(moment1.size()); i++) {
    vec_ir_tensor_moment1.push_back(paddle::dialect::IrTensor(paddle::dialect::TransToPhiDataType(moment1[i].dyn_cast<paddle::dialect::DenseTensorType>().dtype()),
                                                                     moment1[i].dyn_cast<paddle::dialect::DenseTensorType>().dims(),
                                                                     moment1[i].dyn_cast<paddle::dialect::DenseTensorType>().data_layout(),
                                                                     moment1[i].dyn_cast<paddle::dialect::DenseTensorType>().lod(),
                                                                     moment1[i].dyn_cast<paddle::dialect::DenseTensorType>().offset()));
  }
  std::vector<paddle::dialect::IrMetaTensor> vec_meta_moment1;
  for (size_t i=0; i < vec_ir_tensor_moment1.size(); i++) {
    vec_meta_moment1.push_back(paddle::dialect::IrMetaTensor(&vec_ir_tensor_moment1[i]));
  }

  std::vector<const phi::MetaTensor*> meta_moment1;
  for (size_t i=0; i < static_cast<size_t>(vec_meta_moment1.size()); i++) {
    meta_moment1.push_back(&vec_meta_moment1[i]);
  }
   std::vector<paddle::dialect::IrTensor> vec_ir_tensor_moment2;
  for (size_t i=0; i < static_cast<size_t>(moment2.size()); i++) {
    vec_ir_tensor_moment2.push_back(paddle::dialect::IrTensor(paddle::dialect::TransToPhiDataType(moment2[i].dyn_cast<paddle::dialect::DenseTensorType>().dtype()),
                                                                     moment2[i].dyn_cast<paddle::dialect::DenseTensorType>().dims(),
                                                                     moment2[i].dyn_cast<paddle::dialect::DenseTensorType>().data_layout(),
                                                                     moment2[i].dyn_cast<paddle::dialect::DenseTensorType>().lod(),
                                                                     moment2[i].dyn_cast<paddle::dialect::DenseTensorType>().offset()));
  }
  std::vector<paddle::dialect::IrMetaTensor> vec_meta_moment2;
  for (size_t i=0; i < vec_ir_tensor_moment2.size(); i++) {
    vec_meta_moment2.push_back(paddle::dialect::IrMetaTensor(&vec_ir_tensor_moment2[i]));
  }

  std::vector<const phi::MetaTensor*> meta_moment2;
  for (size_t i=0; i < static_cast<size_t>(vec_meta_moment2.size()); i++) {
    meta_moment2.push_back(&vec_meta_moment2[i]);
  }
   std::vector<paddle::dialect::IrTensor> vec_ir_tensor_beta1_pow;
  for (size_t i=0; i < static_cast<size_t>(beta1_pow.size()); i++) {
    vec_ir_tensor_beta1_pow.push_back(paddle::dialect::IrTensor(paddle::dialect::TransToPhiDataType(beta1_pow[i].dyn_cast<paddle::dialect::DenseTensorType>().dtype()),
                                                                     beta1_pow[i].dyn_cast<paddle::dialect::DenseTensorType>().dims(),
                                                                     beta1_pow[i].dyn_cast<paddle::dialect::DenseTensorType>().data_layout(),
                                                                     beta1_pow[i].dyn_cast<paddle::dialect::DenseTensorType>().lod(),
                                                                     beta1_pow[i].dyn_cast<paddle::dialect::DenseTensorType>().offset()));
  }
  std::vector<paddle::dialect::IrMetaTensor> vec_meta_beta1_pow;
  for (size_t i=0; i < vec_ir_tensor_beta1_pow.size(); i++) {
    vec_meta_beta1_pow.push_back(paddle::dialect::IrMetaTensor(&vec_ir_tensor_beta1_pow[i]));
  }

  std::vector<const phi::MetaTensor*> meta_beta1_pow;
  for (size_t i=0; i < static_cast<size_t>(vec_meta_beta1_pow.size()); i++) {
    meta_beta1_pow.push_back(&vec_meta_beta1_pow[i]);
  }
   std::vector<paddle::dialect::IrTensor> vec_ir_tensor_beta2_pow;
  for (size_t i=0; i < static_cast<size_t>(beta2_pow.size()); i++) {
    vec_ir_tensor_beta2_pow.push_back(paddle::dialect::IrTensor(paddle::dialect::TransToPhiDataType(beta2_pow[i].dyn_cast<paddle::dialect::DenseTensorType>().dtype()),
                                                                     beta2_pow[i].dyn_cast<paddle::dialect::DenseTensorType>().dims(),
                                                                     beta2_pow[i].dyn_cast<paddle::dialect::DenseTensorType>().data_layout(),
                                                                     beta2_pow[i].dyn_cast<paddle::dialect::DenseTensorType>().lod(),
                                                                     beta2_pow[i].dyn_cast<paddle::dialect::DenseTensorType>().offset()));
  }
  std::vector<paddle::dialect::IrMetaTensor> vec_meta_beta2_pow;
  for (size_t i=0; i < vec_ir_tensor_beta2_pow.size(); i++) {
    vec_meta_beta2_pow.push_back(paddle::dialect::IrMetaTensor(&vec_ir_tensor_beta2_pow[i]));
  }

  std::vector<const phi::MetaTensor*> meta_beta2_pow;
  for (size_t i=0; i < static_cast<size_t>(vec_meta_beta2_pow.size()); i++) {
    meta_beta2_pow.push_back(&vec_meta_beta2_pow[i]);
  }
   std::vector<paddle::dialect::IrTensor> vec_ir_tensor_master_param;
  if (master_param_.impl() != nullptr) {
    pir::VectorType master_param = master_param_.type().dyn_cast<pir::VectorType>();
    for (size_t i=0; i < static_cast<size_t>(master_param.size()); i++) {
        vec_ir_tensor_master_param.push_back(paddle::dialect::IrTensor(paddle::dialect::TransToPhiDataType(master_param[i].dyn_cast<paddle::dialect::DenseTensorType>().dtype()),
                                                                        master_param[i].dyn_cast<paddle::dialect::DenseTensorType>().dims(),
                                                                        master_param[i].dyn_cast<paddle::dialect::DenseTensorType>().data_layout(),
                                                                        master_param[i].dyn_cast<paddle::dialect::DenseTensorType>().lod(),
                                                                        master_param[i].dyn_cast<paddle::dialect::DenseTensorType>().offset()));
    }
  }

  std::vector<paddle::dialect::IrMetaTensor> vec_meta_master_param;
  for (size_t i=0; i < vec_ir_tensor_master_param.size(); i++) {
    vec_meta_master_param.push_back(paddle::dialect::IrMetaTensor(&vec_ir_tensor_master_param[i]));
  }

  std::vector<const phi::MetaTensor*> meta_master_param;
  for (size_t i=0; i < static_cast<size_t>(vec_meta_master_param.size()); i++) {
    meta_master_param.push_back(&vec_meta_master_param[i]);
  }

  std::vector<paddle::dialect::IrTensor> vec_dense_param_out((param.size()), paddle::dialect::IrTensor());
  std::vector<paddle::dialect::IrMetaTensor> vec_meta_param_out;
  for (size_t i=0; i < static_cast<size_t>(param.size()); i++) {
    vec_meta_param_out.push_back(paddle::dialect::IrMetaTensor(&vec_dense_param_out[i]));
  }
  std::vector<phi::MetaTensor*> meta_param_out;
  for (size_t i=0; i < static_cast<size_t>(vec_meta_param_out.size()); i++) {
    meta_param_out.push_back(&vec_meta_param_out[i]);
  }
  std::vector<paddle::dialect::IrTensor> vec_dense_moment1_out((param.size()), paddle::dialect::IrTensor());
  std::vector<paddle::dialect::IrMetaTensor> vec_meta_moment1_out;
  for (size_t i=0; i < static_cast<size_t>(param.size()); i++) {
    vec_meta_moment1_out.push_back(paddle::dialect::IrMetaTensor(&vec_dense_moment1_out[i]));
  }
  std::vector<phi::MetaTensor*> meta_moment1_out;
  for (size_t i=0; i < static_cast<size_t>(vec_meta_moment1_out.size()); i++) {
    meta_moment1_out.push_back(&vec_meta_moment1_out[i]);
  }
  std::vector<paddle::dialect::IrTensor> vec_dense_moment2_out((param.size()), paddle::dialect::IrTensor());
  std::vector<paddle::dialect::IrMetaTensor> vec_meta_moment2_out;
  for (size_t i=0; i < static_cast<size_t>(param.size()); i++) {
    vec_meta_moment2_out.push_back(paddle::dialect::IrMetaTensor(&vec_dense_moment2_out[i]));
  }
  std::vector<phi::MetaTensor*> meta_moment2_out;
  for (size_t i=0; i < static_cast<size_t>(vec_meta_moment2_out.size()); i++) {
    meta_moment2_out.push_back(&vec_meta_moment2_out[i]);
  }
  std::vector<paddle::dialect::IrTensor> vec_dense_beta1_pow_out((param.size()), paddle::dialect::IrTensor());
  std::vector<paddle::dialect::IrMetaTensor> vec_meta_beta1_pow_out;
  for (size_t i=0; i < static_cast<size_t>(param.size()); i++) {
    vec_meta_beta1_pow_out.push_back(paddle::dialect::IrMetaTensor(&vec_dense_beta1_pow_out[i]));
  }
  std::vector<phi::MetaTensor*> meta_beta1_pow_out;
  for (size_t i=0; i < static_cast<size_t>(vec_meta_beta1_pow_out.size()); i++) {
    meta_beta1_pow_out.push_back(&vec_meta_beta1_pow_out[i]);
  }
  std::vector<paddle::dialect::IrTensor> vec_dense_beta2_pow_out((param.size()), paddle::dialect::IrTensor());
  std::vector<paddle::dialect::IrMetaTensor> vec_meta_beta2_pow_out;
  for (size_t i=0; i < static_cast<size_t>(param.size()); i++) {
    vec_meta_beta2_pow_out.push_back(paddle::dialect::IrMetaTensor(&vec_dense_beta2_pow_out[i]));
  }
  std::vector<phi::MetaTensor*> meta_beta2_pow_out;
  for (size_t i=0; i < static_cast<size_t>(vec_meta_beta2_pow_out.size()); i++) {
    meta_beta2_pow_out.push_back(&vec_meta_beta2_pow_out[i]);
  }
  std::vector<paddle::dialect::IrTensor> vec_dense_master_param_out((param.size()), paddle::dialect::IrTensor());
  std::vector<paddle::dialect::IrMetaTensor> vec_meta_master_param_out;
  for (size_t i=0; i < static_cast<size_t>(param.size()); i++) {
    vec_meta_master_param_out.push_back(paddle::dialect::IrMetaTensor(&vec_dense_master_param_out[i]));
  }
  std::vector<phi::MetaTensor*> meta_master_param_out;
  for (size_t i=0; i < static_cast<size_t>(vec_meta_master_param_out.size()); i++) {
    meta_master_param_out.push_back(&vec_meta_master_param_out[i]);
  }

  phi::MergedAdamInferMeta(meta_param, meta_grad, meta_learning_rate, meta_moment1, meta_moment2, meta_beta1_pow, meta_beta2_pow, meta_master_param, beta1, beta2, epsilon, multi_precision, use_global_beta_pow, meta_param_out, meta_moment1_out, meta_moment2_out, meta_beta1_pow_out, meta_beta2_pow_out, meta_master_param_out);

  std::vector<pir::Type> argument_outputs;
  std::vector<pir::Type> param_out_types;
  for (size_t i=0; i < static_cast<size_t>(param.size()); i++) {
    param_out_types.push_back(paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(vec_dense_param_out[i].dtype()), vec_dense_param_out[i].dims(), vec_dense_param_out[i].layout(), vec_dense_param_out[i].lod(), vec_dense_param_out[i].offset()));
  }
  pir::Type param_out_vector_type = pir::VectorType::get(pir::IrContext::Instance(), param_out_types);
  argument_outputs.push_back(param_out_vector_type);

  std::vector<pir::Type> moment1_out_types;
  for (size_t i=0; i < static_cast<size_t>(param.size()); i++) {
    moment1_out_types.push_back(paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(vec_dense_moment1_out[i].dtype()), vec_dense_moment1_out[i].dims(), vec_dense_moment1_out[i].layout(), vec_dense_moment1_out[i].lod(), vec_dense_moment1_out[i].offset()));
  }
  pir::Type moment1_out_vector_type = pir::VectorType::get(pir::IrContext::Instance(), moment1_out_types);
  argument_outputs.push_back(moment1_out_vector_type);

  std::vector<pir::Type> moment2_out_types;
  for (size_t i=0; i < static_cast<size_t>(param.size()); i++) {
    moment2_out_types.push_back(paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(vec_dense_moment2_out[i].dtype()), vec_dense_moment2_out[i].dims(), vec_dense_moment2_out[i].layout(), vec_dense_moment2_out[i].lod(), vec_dense_moment2_out[i].offset()));
  }
  pir::Type moment2_out_vector_type = pir::VectorType::get(pir::IrContext::Instance(), moment2_out_types);
  argument_outputs.push_back(moment2_out_vector_type);

  std::vector<pir::Type> beta1_pow_out_types;
  for (size_t i=0; i < static_cast<size_t>(param.size()); i++) {
    beta1_pow_out_types.push_back(paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(vec_dense_beta1_pow_out[i].dtype()), vec_dense_beta1_pow_out[i].dims(), vec_dense_beta1_pow_out[i].layout(), vec_dense_beta1_pow_out[i].lod(), vec_dense_beta1_pow_out[i].offset()));
  }
  pir::Type beta1_pow_out_vector_type = pir::VectorType::get(pir::IrContext::Instance(), beta1_pow_out_types);
  argument_outputs.push_back(beta1_pow_out_vector_type);

  std::vector<pir::Type> beta2_pow_out_types;
  for (size_t i=0; i < static_cast<size_t>(param.size()); i++) {
    beta2_pow_out_types.push_back(paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(vec_dense_beta2_pow_out[i].dtype()), vec_dense_beta2_pow_out[i].dims(), vec_dense_beta2_pow_out[i].layout(), vec_dense_beta2_pow_out[i].lod(), vec_dense_beta2_pow_out[i].offset()));
  }
  pir::Type beta2_pow_out_vector_type = pir::VectorType::get(pir::IrContext::Instance(), beta2_pow_out_types);
  argument_outputs.push_back(beta2_pow_out_vector_type);

  std::vector<pir::Type> master_param_out_types;
  for (size_t i=0; i < static_cast<size_t>(param.size()); i++) {
    master_param_out_types.push_back(paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(vec_dense_master_param_out[i].dtype()), vec_dense_master_param_out[i].dims(), vec_dense_master_param_out[i].layout(), vec_dense_master_param_out[i].lod(), vec_dense_master_param_out[i].offset()));
  }
  pir::Type master_param_out_vector_type = pir::VectorType::get(pir::IrContext::Instance(), master_param_out_types);
  argument_outputs.push_back(master_param_out_vector_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void MergedAdam_Op::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value param_, pir::Value grad_, pir::Value learning_rate_, pir::Value moment1_, pir::Value moment2_, pir::Value beta1_pow_, pir::Value beta2_pow_, pir::Value master_param_, pir::AttributeMap attributes) {
  VLOG(4) << "Start build MergedAdam_Op";


  IR_ENFORCE(
      attributes.find("beta1") != attributes.end(),
          "'beta1' Attribute is expected for MergedAdam_Op. ");
  float beta1 = attributes.at("beta1").dyn_cast<paddle::dialect::ScalarAttribute>().data().to<float>();

  IR_ENFORCE(
      attributes.find("beta2") != attributes.end(),
          "'beta2' Attribute is expected for MergedAdam_Op. ");
  float beta2 = attributes.at("beta2").dyn_cast<paddle::dialect::ScalarAttribute>().data().to<float>();

  IR_ENFORCE(
      attributes.find("epsilon") != attributes.end(),
          "'epsilon' Attribute is expected for MergedAdam_Op. ");
  float epsilon = attributes.at("epsilon").dyn_cast<paddle::dialect::ScalarAttribute>().data().to<float>();

  IR_ENFORCE(
      attributes.find("multi_precision") != attributes.end(),
          "'multi_precision' Attribute is expected for MergedAdam_Op. ");
  bool multi_precision = attributes.at("multi_precision").dyn_cast<pir::BoolAttribute>().data();

  IR_ENFORCE(
      attributes.find("use_global_beta_pow") != attributes.end(),
          "'use_global_beta_pow' Attribute is expected for MergedAdam_Op. ");
  bool use_global_beta_pow = attributes.at("use_global_beta_pow").dyn_cast<pir::BoolAttribute>().data();

  // Generate scalar mutable attribute: beta1
  paddle::dialect::FullOp full_beta1_op = builder.Build<paddle::dialect::FullOp>(std::vector<int64_t>{1}, beta1, phi::DataType::FLOAT32, phi::CPUPlace());
  pir::OpResult beta1_ = full_beta1_op->result(0);
      // Generate scalar mutable attribute: beta2
  paddle::dialect::FullOp full_beta2_op = builder.Build<paddle::dialect::FullOp>(std::vector<int64_t>{1}, beta2, phi::DataType::FLOAT32, phi::CPUPlace());
  pir::OpResult beta2_ = full_beta2_op->result(0);
      // Generate scalar mutable attribute: epsilon
  paddle::dialect::FullOp full_epsilon_op = builder.Build<paddle::dialect::FullOp>(std::vector<int64_t>{1}, epsilon, phi::DataType::FLOAT32, phi::CPUPlace());
  pir::OpResult epsilon_ = full_epsilon_op->result(0);
    
  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {param_, grad_, learning_rate_, moment1_, moment2_, beta1_pow_, beta2_pow_, master_param_, beta1_, beta2_, epsilon_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_multi_precision = pir::BoolAttribute::get(pir::IrContext::Instance(), multi_precision);
  argument.AddAttribute("multi_precision", attr_multi_precision);
  pir::Attribute attr_use_global_beta_pow = pir::BoolAttribute::get(pir::IrContext::Instance(), use_global_beta_pow);
  argument.AddAttribute("use_global_beta_pow", attr_use_global_beta_pow);

  VLOG(4) << "Builder construction outputs";
  pir::VectorType param = param_.type().dyn_cast<pir::VectorType>(); (void)param;
  pir::VectorType grad = grad_.type().dyn_cast<pir::VectorType>(); (void)grad;
  pir::VectorType learning_rate = learning_rate_.type().dyn_cast<pir::VectorType>(); (void)learning_rate;
  pir::VectorType moment1 = moment1_.type().dyn_cast<pir::VectorType>(); (void)moment1;
  pir::VectorType moment2 = moment2_.type().dyn_cast<pir::VectorType>(); (void)moment2;
  pir::VectorType beta1_pow = beta1_pow_.type().dyn_cast<pir::VectorType>(); (void)beta1_pow;
  pir::VectorType beta2_pow = beta2_pow_.type().dyn_cast<pir::VectorType>(); (void)beta2_pow;
  std::vector<paddle::dialect::IrTensor> vec_ir_tensor_param;
  for (size_t i=0; i < static_cast<size_t>(param.size()); i++) {
    vec_ir_tensor_param.push_back(paddle::dialect::IrTensor(paddle::dialect::TransToPhiDataType(param[i].dyn_cast<paddle::dialect::DenseTensorType>().dtype()),
                                                                     param[i].dyn_cast<paddle::dialect::DenseTensorType>().dims(),
                                                                     param[i].dyn_cast<paddle::dialect::DenseTensorType>().data_layout(),
                                                                     param[i].dyn_cast<paddle::dialect::DenseTensorType>().lod(),
                                                                     param[i].dyn_cast<paddle::dialect::DenseTensorType>().offset()));
  }
  std::vector<paddle::dialect::IrMetaTensor> vec_meta_param;
  for (size_t i=0; i < vec_ir_tensor_param.size(); i++) {
    vec_meta_param.push_back(paddle::dialect::IrMetaTensor(&vec_ir_tensor_param[i]));
  }

  std::vector<const phi::MetaTensor*> meta_param;
  for (size_t i=0; i < static_cast<size_t>(vec_meta_param.size()); i++) {
    meta_param.push_back(&vec_meta_param[i]);
  }
   std::vector<paddle::dialect::IrTensor> vec_ir_tensor_grad;
  for (size_t i=0; i < static_cast<size_t>(grad.size()); i++) {
    vec_ir_tensor_grad.push_back(paddle::dialect::IrTensor(paddle::dialect::TransToPhiDataType(grad[i].dyn_cast<paddle::dialect::DenseTensorType>().dtype()),
                                                                     grad[i].dyn_cast<paddle::dialect::DenseTensorType>().dims(),
                                                                     grad[i].dyn_cast<paddle::dialect::DenseTensorType>().data_layout(),
                                                                     grad[i].dyn_cast<paddle::dialect::DenseTensorType>().lod(),
                                                                     grad[i].dyn_cast<paddle::dialect::DenseTensorType>().offset()));
  }
  std::vector<paddle::dialect::IrMetaTensor> vec_meta_grad;
  for (size_t i=0; i < vec_ir_tensor_grad.size(); i++) {
    vec_meta_grad.push_back(paddle::dialect::IrMetaTensor(&vec_ir_tensor_grad[i]));
  }

  std::vector<const phi::MetaTensor*> meta_grad;
  for (size_t i=0; i < static_cast<size_t>(vec_meta_grad.size()); i++) {
    meta_grad.push_back(&vec_meta_grad[i]);
  }
   std::vector<paddle::dialect::IrTensor> vec_ir_tensor_learning_rate;
  for (size_t i=0; i < static_cast<size_t>(learning_rate.size()); i++) {
    vec_ir_tensor_learning_rate.push_back(paddle::dialect::IrTensor(paddle::dialect::TransToPhiDataType(learning_rate[i].dyn_cast<paddle::dialect::DenseTensorType>().dtype()),
                                                                     learning_rate[i].dyn_cast<paddle::dialect::DenseTensorType>().dims(),
                                                                     learning_rate[i].dyn_cast<paddle::dialect::DenseTensorType>().data_layout(),
                                                                     learning_rate[i].dyn_cast<paddle::dialect::DenseTensorType>().lod(),
                                                                     learning_rate[i].dyn_cast<paddle::dialect::DenseTensorType>().offset()));
  }
  std::vector<paddle::dialect::IrMetaTensor> vec_meta_learning_rate;
  for (size_t i=0; i < vec_ir_tensor_learning_rate.size(); i++) {
    vec_meta_learning_rate.push_back(paddle::dialect::IrMetaTensor(&vec_ir_tensor_learning_rate[i]));
  }

  std::vector<const phi::MetaTensor*> meta_learning_rate;
  for (size_t i=0; i < static_cast<size_t>(vec_meta_learning_rate.size()); i++) {
    meta_learning_rate.push_back(&vec_meta_learning_rate[i]);
  }
   std::vector<paddle::dialect::IrTensor> vec_ir_tensor_moment1;
  for (size_t i=0; i < static_cast<size_t>(moment1.size()); i++) {
    vec_ir_tensor_moment1.push_back(paddle::dialect::IrTensor(paddle::dialect::TransToPhiDataType(moment1[i].dyn_cast<paddle::dialect::DenseTensorType>().dtype()),
                                                                     moment1[i].dyn_cast<paddle::dialect::DenseTensorType>().dims(),
                                                                     moment1[i].dyn_cast<paddle::dialect::DenseTensorType>().data_layout(),
                                                                     moment1[i].dyn_cast<paddle::dialect::DenseTensorType>().lod(),
                                                                     moment1[i].dyn_cast<paddle::dialect::DenseTensorType>().offset()));
  }
  std::vector<paddle::dialect::IrMetaTensor> vec_meta_moment1;
  for (size_t i=0; i < vec_ir_tensor_moment1.size(); i++) {
    vec_meta_moment1.push_back(paddle::dialect::IrMetaTensor(&vec_ir_tensor_moment1[i]));
  }

  std::vector<const phi::MetaTensor*> meta_moment1;
  for (size_t i=0; i < static_cast<size_t>(vec_meta_moment1.size()); i++) {
    meta_moment1.push_back(&vec_meta_moment1[i]);
  }
   std::vector<paddle::dialect::IrTensor> vec_ir_tensor_moment2;
  for (size_t i=0; i < static_cast<size_t>(moment2.size()); i++) {
    vec_ir_tensor_moment2.push_back(paddle::dialect::IrTensor(paddle::dialect::TransToPhiDataType(moment2[i].dyn_cast<paddle::dialect::DenseTensorType>().dtype()),
                                                                     moment2[i].dyn_cast<paddle::dialect::DenseTensorType>().dims(),
                                                                     moment2[i].dyn_cast<paddle::dialect::DenseTensorType>().data_layout(),
                                                                     moment2[i].dyn_cast<paddle::dialect::DenseTensorType>().lod(),
                                                                     moment2[i].dyn_cast<paddle::dialect::DenseTensorType>().offset()));
  }
  std::vector<paddle::dialect::IrMetaTensor> vec_meta_moment2;
  for (size_t i=0; i < vec_ir_tensor_moment2.size(); i++) {
    vec_meta_moment2.push_back(paddle::dialect::IrMetaTensor(&vec_ir_tensor_moment2[i]));
  }

  std::vector<const phi::MetaTensor*> meta_moment2;
  for (size_t i=0; i < static_cast<size_t>(vec_meta_moment2.size()); i++) {
    meta_moment2.push_back(&vec_meta_moment2[i]);
  }
   std::vector<paddle::dialect::IrTensor> vec_ir_tensor_beta1_pow;
  for (size_t i=0; i < static_cast<size_t>(beta1_pow.size()); i++) {
    vec_ir_tensor_beta1_pow.push_back(paddle::dialect::IrTensor(paddle::dialect::TransToPhiDataType(beta1_pow[i].dyn_cast<paddle::dialect::DenseTensorType>().dtype()),
                                                                     beta1_pow[i].dyn_cast<paddle::dialect::DenseTensorType>().dims(),
                                                                     beta1_pow[i].dyn_cast<paddle::dialect::DenseTensorType>().data_layout(),
                                                                     beta1_pow[i].dyn_cast<paddle::dialect::DenseTensorType>().lod(),
                                                                     beta1_pow[i].dyn_cast<paddle::dialect::DenseTensorType>().offset()));
  }
  std::vector<paddle::dialect::IrMetaTensor> vec_meta_beta1_pow;
  for (size_t i=0; i < vec_ir_tensor_beta1_pow.size(); i++) {
    vec_meta_beta1_pow.push_back(paddle::dialect::IrMetaTensor(&vec_ir_tensor_beta1_pow[i]));
  }

  std::vector<const phi::MetaTensor*> meta_beta1_pow;
  for (size_t i=0; i < static_cast<size_t>(vec_meta_beta1_pow.size()); i++) {
    meta_beta1_pow.push_back(&vec_meta_beta1_pow[i]);
  }
   std::vector<paddle::dialect::IrTensor> vec_ir_tensor_beta2_pow;
  for (size_t i=0; i < static_cast<size_t>(beta2_pow.size()); i++) {
    vec_ir_tensor_beta2_pow.push_back(paddle::dialect::IrTensor(paddle::dialect::TransToPhiDataType(beta2_pow[i].dyn_cast<paddle::dialect::DenseTensorType>().dtype()),
                                                                     beta2_pow[i].dyn_cast<paddle::dialect::DenseTensorType>().dims(),
                                                                     beta2_pow[i].dyn_cast<paddle::dialect::DenseTensorType>().data_layout(),
                                                                     beta2_pow[i].dyn_cast<paddle::dialect::DenseTensorType>().lod(),
                                                                     beta2_pow[i].dyn_cast<paddle::dialect::DenseTensorType>().offset()));
  }
  std::vector<paddle::dialect::IrMetaTensor> vec_meta_beta2_pow;
  for (size_t i=0; i < vec_ir_tensor_beta2_pow.size(); i++) {
    vec_meta_beta2_pow.push_back(paddle::dialect::IrMetaTensor(&vec_ir_tensor_beta2_pow[i]));
  }

  std::vector<const phi::MetaTensor*> meta_beta2_pow;
  for (size_t i=0; i < static_cast<size_t>(vec_meta_beta2_pow.size()); i++) {
    meta_beta2_pow.push_back(&vec_meta_beta2_pow[i]);
  }
   std::vector<paddle::dialect::IrTensor> vec_ir_tensor_master_param;
  if (master_param_.impl() != nullptr) {
    pir::VectorType master_param = master_param_.type().dyn_cast<pir::VectorType>();
    for (size_t i=0; i < static_cast<size_t>(master_param.size()); i++) {
        vec_ir_tensor_master_param.push_back(paddle::dialect::IrTensor(paddle::dialect::TransToPhiDataType(master_param[i].dyn_cast<paddle::dialect::DenseTensorType>().dtype()),
                                                                        master_param[i].dyn_cast<paddle::dialect::DenseTensorType>().dims(),
                                                                        master_param[i].dyn_cast<paddle::dialect::DenseTensorType>().data_layout(),
                                                                        master_param[i].dyn_cast<paddle::dialect::DenseTensorType>().lod(),
                                                                        master_param[i].dyn_cast<paddle::dialect::DenseTensorType>().offset()));
    }
  }

  std::vector<paddle::dialect::IrMetaTensor> vec_meta_master_param;
  for (size_t i=0; i < vec_ir_tensor_master_param.size(); i++) {
    vec_meta_master_param.push_back(paddle::dialect::IrMetaTensor(&vec_ir_tensor_master_param[i]));
  }

  std::vector<const phi::MetaTensor*> meta_master_param;
  for (size_t i=0; i < static_cast<size_t>(vec_meta_master_param.size()); i++) {
    meta_master_param.push_back(&vec_meta_master_param[i]);
  }

  std::vector<paddle::dialect::IrTensor> vec_dense_param_out((param.size()), paddle::dialect::IrTensor());
  std::vector<paddle::dialect::IrMetaTensor> vec_meta_param_out;
  for (size_t i=0; i < static_cast<size_t>(param.size()); i++) {
    vec_meta_param_out.push_back(paddle::dialect::IrMetaTensor(&vec_dense_param_out[i]));
  }
  std::vector<phi::MetaTensor*> meta_param_out;
  for (size_t i=0; i < static_cast<size_t>(vec_meta_param_out.size()); i++) {
    meta_param_out.push_back(&vec_meta_param_out[i]);
  }
  std::vector<paddle::dialect::IrTensor> vec_dense_moment1_out((param.size()), paddle::dialect::IrTensor());
  std::vector<paddle::dialect::IrMetaTensor> vec_meta_moment1_out;
  for (size_t i=0; i < static_cast<size_t>(param.size()); i++) {
    vec_meta_moment1_out.push_back(paddle::dialect::IrMetaTensor(&vec_dense_moment1_out[i]));
  }
  std::vector<phi::MetaTensor*> meta_moment1_out;
  for (size_t i=0; i < static_cast<size_t>(vec_meta_moment1_out.size()); i++) {
    meta_moment1_out.push_back(&vec_meta_moment1_out[i]);
  }
  std::vector<paddle::dialect::IrTensor> vec_dense_moment2_out((param.size()), paddle::dialect::IrTensor());
  std::vector<paddle::dialect::IrMetaTensor> vec_meta_moment2_out;
  for (size_t i=0; i < static_cast<size_t>(param.size()); i++) {
    vec_meta_moment2_out.push_back(paddle::dialect::IrMetaTensor(&vec_dense_moment2_out[i]));
  }
  std::vector<phi::MetaTensor*> meta_moment2_out;
  for (size_t i=0; i < static_cast<size_t>(vec_meta_moment2_out.size()); i++) {
    meta_moment2_out.push_back(&vec_meta_moment2_out[i]);
  }
  std::vector<paddle::dialect::IrTensor> vec_dense_beta1_pow_out((param.size()), paddle::dialect::IrTensor());
  std::vector<paddle::dialect::IrMetaTensor> vec_meta_beta1_pow_out;
  for (size_t i=0; i < static_cast<size_t>(param.size()); i++) {
    vec_meta_beta1_pow_out.push_back(paddle::dialect::IrMetaTensor(&vec_dense_beta1_pow_out[i]));
  }
  std::vector<phi::MetaTensor*> meta_beta1_pow_out;
  for (size_t i=0; i < static_cast<size_t>(vec_meta_beta1_pow_out.size()); i++) {
    meta_beta1_pow_out.push_back(&vec_meta_beta1_pow_out[i]);
  }
  std::vector<paddle::dialect::IrTensor> vec_dense_beta2_pow_out((param.size()), paddle::dialect::IrTensor());
  std::vector<paddle::dialect::IrMetaTensor> vec_meta_beta2_pow_out;
  for (size_t i=0; i < static_cast<size_t>(param.size()); i++) {
    vec_meta_beta2_pow_out.push_back(paddle::dialect::IrMetaTensor(&vec_dense_beta2_pow_out[i]));
  }
  std::vector<phi::MetaTensor*> meta_beta2_pow_out;
  for (size_t i=0; i < static_cast<size_t>(vec_meta_beta2_pow_out.size()); i++) {
    meta_beta2_pow_out.push_back(&vec_meta_beta2_pow_out[i]);
  }
  std::vector<paddle::dialect::IrTensor> vec_dense_master_param_out((param.size()), paddle::dialect::IrTensor());
  std::vector<paddle::dialect::IrMetaTensor> vec_meta_master_param_out;
  for (size_t i=0; i < static_cast<size_t>(param.size()); i++) {
    vec_meta_master_param_out.push_back(paddle::dialect::IrMetaTensor(&vec_dense_master_param_out[i]));
  }
  std::vector<phi::MetaTensor*> meta_master_param_out;
  for (size_t i=0; i < static_cast<size_t>(vec_meta_master_param_out.size()); i++) {
    meta_master_param_out.push_back(&vec_meta_master_param_out[i]);
  }

  phi::MergedAdamInferMeta(meta_param, meta_grad, meta_learning_rate, meta_moment1, meta_moment2, meta_beta1_pow, meta_beta2_pow, meta_master_param, beta1, beta2, epsilon, multi_precision, use_global_beta_pow, meta_param_out, meta_moment1_out, meta_moment2_out, meta_beta1_pow_out, meta_beta2_pow_out, meta_master_param_out);

  std::vector<pir::Type> argument_outputs;
  std::vector<pir::Type> param_out_types;
  for (size_t i=0; i < static_cast<size_t>(param.size()); i++) {
    param_out_types.push_back(paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(vec_dense_param_out[i].dtype()), vec_dense_param_out[i].dims(), vec_dense_param_out[i].layout(), vec_dense_param_out[i].lod(), vec_dense_param_out[i].offset()));
  }
  pir::Type param_out_vector_type = pir::VectorType::get(pir::IrContext::Instance(), param_out_types);
  argument_outputs.push_back(param_out_vector_type);

  std::vector<pir::Type> moment1_out_types;
  for (size_t i=0; i < static_cast<size_t>(param.size()); i++) {
    moment1_out_types.push_back(paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(vec_dense_moment1_out[i].dtype()), vec_dense_moment1_out[i].dims(), vec_dense_moment1_out[i].layout(), vec_dense_moment1_out[i].lod(), vec_dense_moment1_out[i].offset()));
  }
  pir::Type moment1_out_vector_type = pir::VectorType::get(pir::IrContext::Instance(), moment1_out_types);
  argument_outputs.push_back(moment1_out_vector_type);

  std::vector<pir::Type> moment2_out_types;
  for (size_t i=0; i < static_cast<size_t>(param.size()); i++) {
    moment2_out_types.push_back(paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(vec_dense_moment2_out[i].dtype()), vec_dense_moment2_out[i].dims(), vec_dense_moment2_out[i].layout(), vec_dense_moment2_out[i].lod(), vec_dense_moment2_out[i].offset()));
  }
  pir::Type moment2_out_vector_type = pir::VectorType::get(pir::IrContext::Instance(), moment2_out_types);
  argument_outputs.push_back(moment2_out_vector_type);

  std::vector<pir::Type> beta1_pow_out_types;
  for (size_t i=0; i < static_cast<size_t>(param.size()); i++) {
    beta1_pow_out_types.push_back(paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(vec_dense_beta1_pow_out[i].dtype()), vec_dense_beta1_pow_out[i].dims(), vec_dense_beta1_pow_out[i].layout(), vec_dense_beta1_pow_out[i].lod(), vec_dense_beta1_pow_out[i].offset()));
  }
  pir::Type beta1_pow_out_vector_type = pir::VectorType::get(pir::IrContext::Instance(), beta1_pow_out_types);
  argument_outputs.push_back(beta1_pow_out_vector_type);

  std::vector<pir::Type> beta2_pow_out_types;
  for (size_t i=0; i < static_cast<size_t>(param.size()); i++) {
    beta2_pow_out_types.push_back(paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(vec_dense_beta2_pow_out[i].dtype()), vec_dense_beta2_pow_out[i].dims(), vec_dense_beta2_pow_out[i].layout(), vec_dense_beta2_pow_out[i].lod(), vec_dense_beta2_pow_out[i].offset()));
  }
  pir::Type beta2_pow_out_vector_type = pir::VectorType::get(pir::IrContext::Instance(), beta2_pow_out_types);
  argument_outputs.push_back(beta2_pow_out_vector_type);

  std::vector<pir::Type> master_param_out_types;
  for (size_t i=0; i < static_cast<size_t>(param.size()); i++) {
    master_param_out_types.push_back(paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(vec_dense_master_param_out[i].dtype()), vec_dense_master_param_out[i].dims(), vec_dense_master_param_out[i].layout(), vec_dense_master_param_out[i].lod(), vec_dense_master_param_out[i].offset()));
  }
  pir::Type master_param_out_vector_type = pir::VectorType::get(pir::IrContext::Instance(), master_param_out_types);
  argument_outputs.push_back(master_param_out_vector_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void MergedAdam_Op::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value param_, pir::Value grad_, pir::Value learning_rate_, pir::Value moment1_, pir::Value moment2_, pir::Value beta1_pow_, pir::Value beta2_pow_, pir::Value master_param_, pir::Value beta1_, pir::Value beta2_, pir::Value epsilon_, bool multi_precision, bool use_global_beta_pow) {
  VLOG(4) << "Start build MergedAdam_Op";



  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {param_, grad_, learning_rate_, moment1_, moment2_, beta1_pow_, beta2_pow_, master_param_, beta1_, beta2_, epsilon_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_multi_precision = pir::BoolAttribute::get(pir::IrContext::Instance(), multi_precision);
  argument.AddAttribute("multi_precision", attr_multi_precision);
  pir::Attribute attr_use_global_beta_pow = pir::BoolAttribute::get(pir::IrContext::Instance(), use_global_beta_pow);
  argument.AddAttribute("use_global_beta_pow", attr_use_global_beta_pow);

  VLOG(4) << "Builder construction outputs";
  pir::VectorType param = param_.type().dyn_cast<pir::VectorType>(); (void)param;
  pir::VectorType grad = grad_.type().dyn_cast<pir::VectorType>(); (void)grad;
  pir::VectorType learning_rate = learning_rate_.type().dyn_cast<pir::VectorType>(); (void)learning_rate;
  pir::VectorType moment1 = moment1_.type().dyn_cast<pir::VectorType>(); (void)moment1;
  pir::VectorType moment2 = moment2_.type().dyn_cast<pir::VectorType>(); (void)moment2;
  pir::VectorType beta1_pow = beta1_pow_.type().dyn_cast<pir::VectorType>(); (void)beta1_pow;
  pir::VectorType beta2_pow = beta2_pow_.type().dyn_cast<pir::VectorType>(); (void)beta2_pow;
  phi::Scalar beta1;
  if (beta1_.dyn_cast<pir::OpResult>() && beta1_.dyn_cast<pir::OpResult>().owner()->isa<paddle::dialect::FullOp>()) {
    beta1 = std::move(phi::Scalar(beta1_.dyn_cast<pir::OpResult>().owner()
                                  ->dyn_cast<paddle::dialect::FullOp>()
                                  .attribute("value")
                                  .dyn_cast<paddle::dialect::ScalarAttribute>()
                                  .data()
                                  .to<int>()));
  }
  else {
    beta1 = std::move(phi::Scalar(-1));
    beta1.SetFromTensor(true);
  }
  phi::Scalar beta2;
  if (beta2_.dyn_cast<pir::OpResult>() && beta2_.dyn_cast<pir::OpResult>().owner()->isa<paddle::dialect::FullOp>()) {
    beta2 = std::move(phi::Scalar(beta2_.dyn_cast<pir::OpResult>().owner()
                                  ->dyn_cast<paddle::dialect::FullOp>()
                                  .attribute("value")
                                  .dyn_cast<paddle::dialect::ScalarAttribute>()
                                  .data()
                                  .to<int>()));
  }
  else {
    beta2 = std::move(phi::Scalar(-1));
    beta2.SetFromTensor(true);
  }
  phi::Scalar epsilon;
  if (epsilon_.dyn_cast<pir::OpResult>() && epsilon_.dyn_cast<pir::OpResult>().owner()->isa<paddle::dialect::FullOp>()) {
    epsilon = std::move(phi::Scalar(epsilon_.dyn_cast<pir::OpResult>().owner()
                                  ->dyn_cast<paddle::dialect::FullOp>()
                                  .attribute("value")
                                  .dyn_cast<paddle::dialect::ScalarAttribute>()
                                  .data()
                                  .to<int>()));
  }
  else {
    epsilon = std::move(phi::Scalar(-1));
    epsilon.SetFromTensor(true);
  }

  std::vector<paddle::dialect::IrTensor> vec_ir_tensor_param;
  for (size_t i=0; i < static_cast<size_t>(param.size()); i++) {
    vec_ir_tensor_param.push_back(paddle::dialect::IrTensor(paddle::dialect::TransToPhiDataType(param[i].dyn_cast<paddle::dialect::DenseTensorType>().dtype()),
                                                                     param[i].dyn_cast<paddle::dialect::DenseTensorType>().dims(),
                                                                     param[i].dyn_cast<paddle::dialect::DenseTensorType>().data_layout(),
                                                                     param[i].dyn_cast<paddle::dialect::DenseTensorType>().lod(),
                                                                     param[i].dyn_cast<paddle::dialect::DenseTensorType>().offset()));
  }
  std::vector<paddle::dialect::IrMetaTensor> vec_meta_param;
  for (size_t i=0; i < vec_ir_tensor_param.size(); i++) {
    vec_meta_param.push_back(paddle::dialect::IrMetaTensor(&vec_ir_tensor_param[i]));
  }

  std::vector<const phi::MetaTensor*> meta_param;
  for (size_t i=0; i < static_cast<size_t>(vec_meta_param.size()); i++) {
    meta_param.push_back(&vec_meta_param[i]);
  }
   std::vector<paddle::dialect::IrTensor> vec_ir_tensor_grad;
  for (size_t i=0; i < static_cast<size_t>(grad.size()); i++) {
    vec_ir_tensor_grad.push_back(paddle::dialect::IrTensor(paddle::dialect::TransToPhiDataType(grad[i].dyn_cast<paddle::dialect::DenseTensorType>().dtype()),
                                                                     grad[i].dyn_cast<paddle::dialect::DenseTensorType>().dims(),
                                                                     grad[i].dyn_cast<paddle::dialect::DenseTensorType>().data_layout(),
                                                                     grad[i].dyn_cast<paddle::dialect::DenseTensorType>().lod(),
                                                                     grad[i].dyn_cast<paddle::dialect::DenseTensorType>().offset()));
  }
  std::vector<paddle::dialect::IrMetaTensor> vec_meta_grad;
  for (size_t i=0; i < vec_ir_tensor_grad.size(); i++) {
    vec_meta_grad.push_back(paddle::dialect::IrMetaTensor(&vec_ir_tensor_grad[i]));
  }

  std::vector<const phi::MetaTensor*> meta_grad;
  for (size_t i=0; i < static_cast<size_t>(vec_meta_grad.size()); i++) {
    meta_grad.push_back(&vec_meta_grad[i]);
  }
   std::vector<paddle::dialect::IrTensor> vec_ir_tensor_learning_rate;
  for (size_t i=0; i < static_cast<size_t>(learning_rate.size()); i++) {
    vec_ir_tensor_learning_rate.push_back(paddle::dialect::IrTensor(paddle::dialect::TransToPhiDataType(learning_rate[i].dyn_cast<paddle::dialect::DenseTensorType>().dtype()),
                                                                     learning_rate[i].dyn_cast<paddle::dialect::DenseTensorType>().dims(),
                                                                     learning_rate[i].dyn_cast<paddle::dialect::DenseTensorType>().data_layout(),
                                                                     learning_rate[i].dyn_cast<paddle::dialect::DenseTensorType>().lod(),
                                                                     learning_rate[i].dyn_cast<paddle::dialect::DenseTensorType>().offset()));
  }
  std::vector<paddle::dialect::IrMetaTensor> vec_meta_learning_rate;
  for (size_t i=0; i < vec_ir_tensor_learning_rate.size(); i++) {
    vec_meta_learning_rate.push_back(paddle::dialect::IrMetaTensor(&vec_ir_tensor_learning_rate[i]));
  }

  std::vector<const phi::MetaTensor*> meta_learning_rate;
  for (size_t i=0; i < static_cast<size_t>(vec_meta_learning_rate.size()); i++) {
    meta_learning_rate.push_back(&vec_meta_learning_rate[i]);
  }
   std::vector<paddle::dialect::IrTensor> vec_ir_tensor_moment1;
  for (size_t i=0; i < static_cast<size_t>(moment1.size()); i++) {
    vec_ir_tensor_moment1.push_back(paddle::dialect::IrTensor(paddle::dialect::TransToPhiDataType(moment1[i].dyn_cast<paddle::dialect::DenseTensorType>().dtype()),
                                                                     moment1[i].dyn_cast<paddle::dialect::DenseTensorType>().dims(),
                                                                     moment1[i].dyn_cast<paddle::dialect::DenseTensorType>().data_layout(),
                                                                     moment1[i].dyn_cast<paddle::dialect::DenseTensorType>().lod(),
                                                                     moment1[i].dyn_cast<paddle::dialect::DenseTensorType>().offset()));
  }
  std::vector<paddle::dialect::IrMetaTensor> vec_meta_moment1;
  for (size_t i=0; i < vec_ir_tensor_moment1.size(); i++) {
    vec_meta_moment1.push_back(paddle::dialect::IrMetaTensor(&vec_ir_tensor_moment1[i]));
  }

  std::vector<const phi::MetaTensor*> meta_moment1;
  for (size_t i=0; i < static_cast<size_t>(vec_meta_moment1.size()); i++) {
    meta_moment1.push_back(&vec_meta_moment1[i]);
  }
   std::vector<paddle::dialect::IrTensor> vec_ir_tensor_moment2;
  for (size_t i=0; i < static_cast<size_t>(moment2.size()); i++) {
    vec_ir_tensor_moment2.push_back(paddle::dialect::IrTensor(paddle::dialect::TransToPhiDataType(moment2[i].dyn_cast<paddle::dialect::DenseTensorType>().dtype()),
                                                                     moment2[i].dyn_cast<paddle::dialect::DenseTensorType>().dims(),
                                                                     moment2[i].dyn_cast<paddle::dialect::DenseTensorType>().data_layout(),
                                                                     moment2[i].dyn_cast<paddle::dialect::DenseTensorType>().lod(),
                                                                     moment2[i].dyn_cast<paddle::dialect::DenseTensorType>().offset()));
  }
  std::vector<paddle::dialect::IrMetaTensor> vec_meta_moment2;
  for (size_t i=0; i < vec_ir_tensor_moment2.size(); i++) {
    vec_meta_moment2.push_back(paddle::dialect::IrMetaTensor(&vec_ir_tensor_moment2[i]));
  }

  std::vector<const phi::MetaTensor*> meta_moment2;
  for (size_t i=0; i < static_cast<size_t>(vec_meta_moment2.size()); i++) {
    meta_moment2.push_back(&vec_meta_moment2[i]);
  }
   std::vector<paddle::dialect::IrTensor> vec_ir_tensor_beta1_pow;
  for (size_t i=0; i < static_cast<size_t>(beta1_pow.size()); i++) {
    vec_ir_tensor_beta1_pow.push_back(paddle::dialect::IrTensor(paddle::dialect::TransToPhiDataType(beta1_pow[i].dyn_cast<paddle::dialect::DenseTensorType>().dtype()),
                                                                     beta1_pow[i].dyn_cast<paddle::dialect::DenseTensorType>().dims(),
                                                                     beta1_pow[i].dyn_cast<paddle::dialect::DenseTensorType>().data_layout(),
                                                                     beta1_pow[i].dyn_cast<paddle::dialect::DenseTensorType>().lod(),
                                                                     beta1_pow[i].dyn_cast<paddle::dialect::DenseTensorType>().offset()));
  }
  std::vector<paddle::dialect::IrMetaTensor> vec_meta_beta1_pow;
  for (size_t i=0; i < vec_ir_tensor_beta1_pow.size(); i++) {
    vec_meta_beta1_pow.push_back(paddle::dialect::IrMetaTensor(&vec_ir_tensor_beta1_pow[i]));
  }

  std::vector<const phi::MetaTensor*> meta_beta1_pow;
  for (size_t i=0; i < static_cast<size_t>(vec_meta_beta1_pow.size()); i++) {
    meta_beta1_pow.push_back(&vec_meta_beta1_pow[i]);
  }
   std::vector<paddle::dialect::IrTensor> vec_ir_tensor_beta2_pow;
  for (size_t i=0; i < static_cast<size_t>(beta2_pow.size()); i++) {
    vec_ir_tensor_beta2_pow.push_back(paddle::dialect::IrTensor(paddle::dialect::TransToPhiDataType(beta2_pow[i].dyn_cast<paddle::dialect::DenseTensorType>().dtype()),
                                                                     beta2_pow[i].dyn_cast<paddle::dialect::DenseTensorType>().dims(),
                                                                     beta2_pow[i].dyn_cast<paddle::dialect::DenseTensorType>().data_layout(),
                                                                     beta2_pow[i].dyn_cast<paddle::dialect::DenseTensorType>().lod(),
                                                                     beta2_pow[i].dyn_cast<paddle::dialect::DenseTensorType>().offset()));
  }
  std::vector<paddle::dialect::IrMetaTensor> vec_meta_beta2_pow;
  for (size_t i=0; i < vec_ir_tensor_beta2_pow.size(); i++) {
    vec_meta_beta2_pow.push_back(paddle::dialect::IrMetaTensor(&vec_ir_tensor_beta2_pow[i]));
  }

  std::vector<const phi::MetaTensor*> meta_beta2_pow;
  for (size_t i=0; i < static_cast<size_t>(vec_meta_beta2_pow.size()); i++) {
    meta_beta2_pow.push_back(&vec_meta_beta2_pow[i]);
  }
   std::vector<paddle::dialect::IrTensor> vec_ir_tensor_master_param;
  if (master_param_.impl() != nullptr) {
    pir::VectorType master_param = master_param_.type().dyn_cast<pir::VectorType>();
    for (size_t i=0; i < static_cast<size_t>(master_param.size()); i++) {
        vec_ir_tensor_master_param.push_back(paddle::dialect::IrTensor(paddle::dialect::TransToPhiDataType(master_param[i].dyn_cast<paddle::dialect::DenseTensorType>().dtype()),
                                                                        master_param[i].dyn_cast<paddle::dialect::DenseTensorType>().dims(),
                                                                        master_param[i].dyn_cast<paddle::dialect::DenseTensorType>().data_layout(),
                                                                        master_param[i].dyn_cast<paddle::dialect::DenseTensorType>().lod(),
                                                                        master_param[i].dyn_cast<paddle::dialect::DenseTensorType>().offset()));
    }
  }

  std::vector<paddle::dialect::IrMetaTensor> vec_meta_master_param;
  for (size_t i=0; i < vec_ir_tensor_master_param.size(); i++) {
    vec_meta_master_param.push_back(paddle::dialect::IrMetaTensor(&vec_ir_tensor_master_param[i]));
  }

  std::vector<const phi::MetaTensor*> meta_master_param;
  for (size_t i=0; i < static_cast<size_t>(vec_meta_master_param.size()); i++) {
    meta_master_param.push_back(&vec_meta_master_param[i]);
  }

  std::vector<paddle::dialect::IrTensor> vec_dense_param_out((param.size()), paddle::dialect::IrTensor());
  std::vector<paddle::dialect::IrMetaTensor> vec_meta_param_out;
  for (size_t i=0; i < static_cast<size_t>(param.size()); i++) {
    vec_meta_param_out.push_back(paddle::dialect::IrMetaTensor(&vec_dense_param_out[i]));
  }
  std::vector<phi::MetaTensor*> meta_param_out;
  for (size_t i=0; i < static_cast<size_t>(vec_meta_param_out.size()); i++) {
    meta_param_out.push_back(&vec_meta_param_out[i]);
  }
  std::vector<paddle::dialect::IrTensor> vec_dense_moment1_out((param.size()), paddle::dialect::IrTensor());
  std::vector<paddle::dialect::IrMetaTensor> vec_meta_moment1_out;
  for (size_t i=0; i < static_cast<size_t>(param.size()); i++) {
    vec_meta_moment1_out.push_back(paddle::dialect::IrMetaTensor(&vec_dense_moment1_out[i]));
  }
  std::vector<phi::MetaTensor*> meta_moment1_out;
  for (size_t i=0; i < static_cast<size_t>(vec_meta_moment1_out.size()); i++) {
    meta_moment1_out.push_back(&vec_meta_moment1_out[i]);
  }
  std::vector<paddle::dialect::IrTensor> vec_dense_moment2_out((param.size()), paddle::dialect::IrTensor());
  std::vector<paddle::dialect::IrMetaTensor> vec_meta_moment2_out;
  for (size_t i=0; i < static_cast<size_t>(param.size()); i++) {
    vec_meta_moment2_out.push_back(paddle::dialect::IrMetaTensor(&vec_dense_moment2_out[i]));
  }
  std::vector<phi::MetaTensor*> meta_moment2_out;
  for (size_t i=0; i < static_cast<size_t>(vec_meta_moment2_out.size()); i++) {
    meta_moment2_out.push_back(&vec_meta_moment2_out[i]);
  }
  std::vector<paddle::dialect::IrTensor> vec_dense_beta1_pow_out((param.size()), paddle::dialect::IrTensor());
  std::vector<paddle::dialect::IrMetaTensor> vec_meta_beta1_pow_out;
  for (size_t i=0; i < static_cast<size_t>(param.size()); i++) {
    vec_meta_beta1_pow_out.push_back(paddle::dialect::IrMetaTensor(&vec_dense_beta1_pow_out[i]));
  }
  std::vector<phi::MetaTensor*> meta_beta1_pow_out;
  for (size_t i=0; i < static_cast<size_t>(vec_meta_beta1_pow_out.size()); i++) {
    meta_beta1_pow_out.push_back(&vec_meta_beta1_pow_out[i]);
  }
  std::vector<paddle::dialect::IrTensor> vec_dense_beta2_pow_out((param.size()), paddle::dialect::IrTensor());
  std::vector<paddle::dialect::IrMetaTensor> vec_meta_beta2_pow_out;
  for (size_t i=0; i < static_cast<size_t>(param.size()); i++) {
    vec_meta_beta2_pow_out.push_back(paddle::dialect::IrMetaTensor(&vec_dense_beta2_pow_out[i]));
  }
  std::vector<phi::MetaTensor*> meta_beta2_pow_out;
  for (size_t i=0; i < static_cast<size_t>(vec_meta_beta2_pow_out.size()); i++) {
    meta_beta2_pow_out.push_back(&vec_meta_beta2_pow_out[i]);
  }
  std::vector<paddle::dialect::IrTensor> vec_dense_master_param_out((param.size()), paddle::dialect::IrTensor());
  std::vector<paddle::dialect::IrMetaTensor> vec_meta_master_param_out;
  for (size_t i=0; i < static_cast<size_t>(param.size()); i++) {
    vec_meta_master_param_out.push_back(paddle::dialect::IrMetaTensor(&vec_dense_master_param_out[i]));
  }
  std::vector<phi::MetaTensor*> meta_master_param_out;
  for (size_t i=0; i < static_cast<size_t>(vec_meta_master_param_out.size()); i++) {
    meta_master_param_out.push_back(&vec_meta_master_param_out[i]);
  }

  phi::MergedAdamInferMeta(meta_param, meta_grad, meta_learning_rate, meta_moment1, meta_moment2, meta_beta1_pow, meta_beta2_pow, meta_master_param, beta1, beta2, epsilon, multi_precision, use_global_beta_pow, meta_param_out, meta_moment1_out, meta_moment2_out, meta_beta1_pow_out, meta_beta2_pow_out, meta_master_param_out);

  std::vector<pir::Type> argument_outputs;
  std::vector<pir::Type> param_out_types;
  for (size_t i=0; i < static_cast<size_t>(param.size()); i++) {
    param_out_types.push_back(paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(vec_dense_param_out[i].dtype()), vec_dense_param_out[i].dims(), vec_dense_param_out[i].layout(), vec_dense_param_out[i].lod(), vec_dense_param_out[i].offset()));
  }
  pir::Type param_out_vector_type = pir::VectorType::get(pir::IrContext::Instance(), param_out_types);
  argument_outputs.push_back(param_out_vector_type);

  std::vector<pir::Type> moment1_out_types;
  for (size_t i=0; i < static_cast<size_t>(param.size()); i++) {
    moment1_out_types.push_back(paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(vec_dense_moment1_out[i].dtype()), vec_dense_moment1_out[i].dims(), vec_dense_moment1_out[i].layout(), vec_dense_moment1_out[i].lod(), vec_dense_moment1_out[i].offset()));
  }
  pir::Type moment1_out_vector_type = pir::VectorType::get(pir::IrContext::Instance(), moment1_out_types);
  argument_outputs.push_back(moment1_out_vector_type);

  std::vector<pir::Type> moment2_out_types;
  for (size_t i=0; i < static_cast<size_t>(param.size()); i++) {
    moment2_out_types.push_back(paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(vec_dense_moment2_out[i].dtype()), vec_dense_moment2_out[i].dims(), vec_dense_moment2_out[i].layout(), vec_dense_moment2_out[i].lod(), vec_dense_moment2_out[i].offset()));
  }
  pir::Type moment2_out_vector_type = pir::VectorType::get(pir::IrContext::Instance(), moment2_out_types);
  argument_outputs.push_back(moment2_out_vector_type);

  std::vector<pir::Type> beta1_pow_out_types;
  for (size_t i=0; i < static_cast<size_t>(param.size()); i++) {
    beta1_pow_out_types.push_back(paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(vec_dense_beta1_pow_out[i].dtype()), vec_dense_beta1_pow_out[i].dims(), vec_dense_beta1_pow_out[i].layout(), vec_dense_beta1_pow_out[i].lod(), vec_dense_beta1_pow_out[i].offset()));
  }
  pir::Type beta1_pow_out_vector_type = pir::VectorType::get(pir::IrContext::Instance(), beta1_pow_out_types);
  argument_outputs.push_back(beta1_pow_out_vector_type);

  std::vector<pir::Type> beta2_pow_out_types;
  for (size_t i=0; i < static_cast<size_t>(param.size()); i++) {
    beta2_pow_out_types.push_back(paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(vec_dense_beta2_pow_out[i].dtype()), vec_dense_beta2_pow_out[i].dims(), vec_dense_beta2_pow_out[i].layout(), vec_dense_beta2_pow_out[i].lod(), vec_dense_beta2_pow_out[i].offset()));
  }
  pir::Type beta2_pow_out_vector_type = pir::VectorType::get(pir::IrContext::Instance(), beta2_pow_out_types);
  argument_outputs.push_back(beta2_pow_out_vector_type);

  std::vector<pir::Type> master_param_out_types;
  for (size_t i=0; i < static_cast<size_t>(param.size()); i++) {
    master_param_out_types.push_back(paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(vec_dense_master_param_out[i].dtype()), vec_dense_master_param_out[i].dims(), vec_dense_master_param_out[i].layout(), vec_dense_master_param_out[i].lod(), vec_dense_master_param_out[i].offset()));
  }
  pir::Type master_param_out_vector_type = pir::VectorType::get(pir::IrContext::Instance(), master_param_out_types);
  argument_outputs.push_back(master_param_out_vector_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void MergedAdam_Op::VerifySig() {
  VLOG(4) << "Start Verifying inputs, outputs and attributes for: MergedAdam_Op.";
  VLOG(4) << "Verifying inputs:";
  {
  auto input_size = num_operands();
  IR_ENFORCE(input_size == 11u,
                    "The size %d of inputs must be equal to 11.", input_size);
  if (auto vec_type = (*this)->operand_source(0).type().dyn_cast<pir::VectorType>()) {
      for (size_t i = 0; i < vec_type.size(); ++i) {
        IR_ENFORCE(vec_type[i].isa<paddle::dialect::DenseTensorType>(),
                       "Type validation failed for the 0th input, got %s.", (*this)->operand_source(0).type());
      }
  }
  else {
    IR_ENFORCE((*this)->operand_source(0).type().isa<paddle::dialect::DenseTensorType>(),
                   "Type validation failed for the 0th input, got %s.", (*this)->operand_source(0).type());
  }
  if (auto vec_type = (*this)->operand_source(1).type().dyn_cast<pir::VectorType>()) {
      for (size_t i = 0; i < vec_type.size(); ++i) {
        IR_ENFORCE(vec_type[i].isa<paddle::dialect::DenseTensorType>(),
                       "Type validation failed for the 1th input, got %s.", (*this)->operand_source(1).type());
      }
  }
  else {
    IR_ENFORCE((*this)->operand_source(1).type().isa<paddle::dialect::DenseTensorType>(),
                   "Type validation failed for the 1th input, got %s.", (*this)->operand_source(1).type());
  }
  if (auto vec_type = (*this)->operand_source(2).type().dyn_cast<pir::VectorType>()) {
      for (size_t i = 0; i < vec_type.size(); ++i) {
        IR_ENFORCE(vec_type[i].isa<paddle::dialect::DenseTensorType>(),
                       "Type validation failed for the 2th input, got %s.", (*this)->operand_source(2).type());
      }
  }
  else {
    IR_ENFORCE((*this)->operand_source(2).type().isa<paddle::dialect::DenseTensorType>(),
                   "Type validation failed for the 2th input, got %s.", (*this)->operand_source(2).type());
  }
  if (auto vec_type = (*this)->operand_source(3).type().dyn_cast<pir::VectorType>()) {
      for (size_t i = 0; i < vec_type.size(); ++i) {
        IR_ENFORCE(vec_type[i].isa<paddle::dialect::DenseTensorType>(),
                       "Type validation failed for the 3th input, got %s.", (*this)->operand_source(3).type());
      }
  }
  else {
    IR_ENFORCE((*this)->operand_source(3).type().isa<paddle::dialect::DenseTensorType>(),
                   "Type validation failed for the 3th input, got %s.", (*this)->operand_source(3).type());
  }
  if (auto vec_type = (*this)->operand_source(4).type().dyn_cast<pir::VectorType>()) {
      for (size_t i = 0; i < vec_type.size(); ++i) {
        IR_ENFORCE(vec_type[i].isa<paddle::dialect::DenseTensorType>(),
                       "Type validation failed for the 4th input, got %s.", (*this)->operand_source(4).type());
      }
  }
  else {
    IR_ENFORCE((*this)->operand_source(4).type().isa<paddle::dialect::DenseTensorType>(),
                   "Type validation failed for the 4th input, got %s.", (*this)->operand_source(4).type());
  }
  if (auto vec_type = (*this)->operand_source(5).type().dyn_cast<pir::VectorType>()) {
      for (size_t i = 0; i < vec_type.size(); ++i) {
        IR_ENFORCE(vec_type[i].isa<paddle::dialect::DenseTensorType>(),
                       "Type validation failed for the 5th input, got %s.", (*this)->operand_source(5).type());
      }
  }
  else {
    IR_ENFORCE((*this)->operand_source(5).type().isa<paddle::dialect::DenseTensorType>(),
                   "Type validation failed for the 5th input, got %s.", (*this)->operand_source(5).type());
  }
  if (auto vec_type = (*this)->operand_source(6).type().dyn_cast<pir::VectorType>()) {
      for (size_t i = 0; i < vec_type.size(); ++i) {
        IR_ENFORCE(vec_type[i].isa<paddle::dialect::DenseTensorType>(),
                       "Type validation failed for the 6th input, got %s.", (*this)->operand_source(6).type());
      }
  }
  else {
    IR_ENFORCE((*this)->operand_source(6).type().isa<paddle::dialect::DenseTensorType>(),
                   "Type validation failed for the 6th input, got %s.", (*this)->operand_source(6).type());
  }
  if (auto val =  (*this)->operand(7)) {
    if (auto vec_type = val.type().dyn_cast<pir::VectorType>()) {
      for (size_t i = 0; i < vec_type.size(); i++) {
        IR_ENFORCE(vec_type[i].isa<paddle::dialect::DenseTensorType>(),
                          "Type validation failed for the 7th input, got %s.", (*this)->operand_source(7).type());
      }
    }
    else {
      IR_ENFORCE(val.type().isa<paddle::dialect::DenseTensorType>(),
                        "Type validation failed for the 7th input, got %s.", (*this)->operand_source(7).type());
    }
  }
  IR_ENFORCE((*this)->operand_source(8).type().isa<paddle::dialect::DenseTensorType>(),
                  "Type validation failed for the 8th input, got %s.", (*this)->operand_source(8).type());
  IR_ENFORCE((*this)->operand_source(9).type().isa<paddle::dialect::DenseTensorType>(),
                  "Type validation failed for the 9th input, got %s.", (*this)->operand_source(9).type());
  IR_ENFORCE((*this)->operand_source(10).type().isa<paddle::dialect::DenseTensorType>(),
                  "Type validation failed for the 10th input, got %s.", (*this)->operand_source(10).type());
  }
  VLOG(4) << "Verifying attributes:";
  {
  auto& attributes = this->attributes();
  IR_ENFORCE(attributes.count("multi_precision")>0,
                 "multi_precision does not exist.");
  IR_ENFORCE(attributes.at("multi_precision").isa<pir::BoolAttribute>(),
                 "Type of attribute: multi_precision is not pir::BoolAttribute.");

  IR_ENFORCE(attributes.count("use_global_beta_pow")>0,
                 "use_global_beta_pow does not exist.");
  IR_ENFORCE(attributes.at("use_global_beta_pow").isa<pir::BoolAttribute>(),
                 "Type of attribute: use_global_beta_pow is not pir::BoolAttribute.");

  }
  VLOG(4) << "Verifying outputs:";
  {
  auto output_size = num_results();
  IR_ENFORCE(output_size == 6u,
                    "The size %d of outputs must be equal to 6.", output_size);
  auto output_0_type = (*this)->result(0).type();
  if (auto vec_type = output_0_type.dyn_cast<pir::VectorType>()) {
    for (size_t i = 0; i < vec_type.size(); i++) {
      IR_ENFORCE(vec_type[i].isa<paddle::dialect::DenseTensorType>(),
                     "Type validation failed for the 0th output.");
    }
  }
  else {
    IR_ENFORCE(output_0_type.isa<paddle::dialect::DenseTensorType>(),
                   "Type validation failed for the 0th output.");
  }
  auto output_1_type = (*this)->result(1).type();
  if (auto vec_type = output_1_type.dyn_cast<pir::VectorType>()) {
    for (size_t i = 0; i < vec_type.size(); i++) {
      IR_ENFORCE(vec_type[i].isa<paddle::dialect::DenseTensorType>(),
                     "Type validation failed for the 1th output.");
    }
  }
  else {
    IR_ENFORCE(output_1_type.isa<paddle::dialect::DenseTensorType>(),
                   "Type validation failed for the 1th output.");
  }
  auto output_2_type = (*this)->result(2).type();
  if (auto vec_type = output_2_type.dyn_cast<pir::VectorType>()) {
    for (size_t i = 0; i < vec_type.size(); i++) {
      IR_ENFORCE(vec_type[i].isa<paddle::dialect::DenseTensorType>(),
                     "Type validation failed for the 2th output.");
    }
  }
  else {
    IR_ENFORCE(output_2_type.isa<paddle::dialect::DenseTensorType>(),
                   "Type validation failed for the 2th output.");
  }
  auto output_3_type = (*this)->result(3).type();
  if (auto vec_type = output_3_type.dyn_cast<pir::VectorType>()) {
    for (size_t i = 0; i < vec_type.size(); i++) {
      IR_ENFORCE(vec_type[i].isa<paddle::dialect::DenseTensorType>(),
                     "Type validation failed for the 3th output.");
    }
  }
  else {
    IR_ENFORCE(output_3_type.isa<paddle::dialect::DenseTensorType>(),
                   "Type validation failed for the 3th output.");
  }
  auto output_4_type = (*this)->result(4).type();
  if (auto vec_type = output_4_type.dyn_cast<pir::VectorType>()) {
    for (size_t i = 0; i < vec_type.size(); i++) {
      IR_ENFORCE(vec_type[i].isa<paddle::dialect::DenseTensorType>(),
                     "Type validation failed for the 4th output.");
    }
  }
  else {
    IR_ENFORCE(output_4_type.isa<paddle::dialect::DenseTensorType>(),
                   "Type validation failed for the 4th output.");
  }
  if (auto output_5_type = (*this)->result(5).type()) {
    if (auto vec_type = output_5_type.dyn_cast<pir::VectorType>()) {
      for (size_t i = 0; i < vec_type.size(); ++i) {
        IR_ENFORCE(vec_type[i].isa<paddle::dialect::DenseTensorType>(),
                       "Type validation failed for the 5th output.");
      }
    }
    else {
      IR_ENFORCE(output_5_type.isa<paddle::dialect::DenseTensorType>(),
                     "Type validation failed for the 5th output.");
    }
  }
  }
  VLOG(4) << "End Verifying for: MergedAdam_Op.";
}

void MergedAdam_Op::InferMeta( phi::InferMetaContext *infer_meta ) {
  auto fn = PD_INFER_META(phi::MergedAdamInferMeta);
  fn(infer_meta);
}

phi::DataType MergedAdam_Op::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: MergedAdam_Op";
  


  return expected_kernel_dtype;
}

const char *MergedMomentum_Op::attributes_name[6] = { "mu", "use_nesterov", "regularization_method", "regularization_coeff", "multi_precision", "rescale_grad" };

OpInfoTuple MergedMomentum_Op::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("param", "pir::VectorType<paddle::dialect::DenseTensorType>", false, false, false, false), paddle::dialect::OpInputInfo("grad", "pir::VectorType<paddle::dialect::DenseTensorType>", false, false, false, false), paddle::dialect::OpInputInfo("velocity", "pir::VectorType<paddle::dialect::DenseTensorType>", false, false, false, false), paddle::dialect::OpInputInfo("learning_rate", "pir::VectorType<paddle::dialect::DenseTensorType>", false, false, false, false), paddle::dialect::OpInputInfo("master_param", "pir::VectorType<paddle::dialect::DenseTensorType>", true, false, false, false) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = { paddle::dialect::OpAttributeInfo("mu", "pir::FloatAttribute", ""), paddle::dialect::OpAttributeInfo("use_nesterov", "pir::BoolAttribute", ""), paddle::dialect::OpAttributeInfo("regularization_method", "pir::ArrayAttribute<pir::StrAttribute>", ""), paddle::dialect::OpAttributeInfo("regularization_coeff", "pir::ArrayAttribute<pir::FloatAttribute>", ""), paddle::dialect::OpAttributeInfo("multi_precision", "pir::BoolAttribute", ""), paddle::dialect::OpAttributeInfo("rescale_grad", "pir::FloatAttribute", "") };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("param_out", "pir::VectorType<paddle::dialect::DenseTensorType>", false, false), paddle::dialect::OpOutputInfo("velocity_out", "pir::VectorType<paddle::dialect::DenseTensorType>", false, false), paddle::dialect::OpOutputInfo("master_param_out", "pir::VectorType<paddle::dialect::DenseTensorType>", true, false) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("MergedMomentumInferMeta", {"param", "grad", "velocity", "learning_rate", "master_param", "mu", "use_nesterov", "regularization_method", "regularization_coeff", "multi_precision", "rescale_grad"}, "merged_momentum", {"param", "grad", "velocity", "learning_rate", "master_param", "mu", "use_nesterov", "regularization_method", "regularization_coeff", "multi_precision", "rescale_grad"}, {"param"}, {}, {{"param_out", "param"},{"velocity_out", "velocity"},{"master_param_out", "master_param"}}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "merged_momentum_");
}

void MergedMomentum_Op::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value param_, pir::Value grad_, pir::Value velocity_, pir::Value learning_rate_, pir::Value master_param_, float mu, bool use_nesterov, const std::vector<std::string>& regularization_method, const std::vector<float>& regularization_coeff, bool multi_precision, float rescale_grad) {
  VLOG(4) << "Start build MergedMomentum_Op";



  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {param_, grad_, velocity_, learning_rate_, master_param_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_mu = pir::FloatAttribute::get(pir::IrContext::Instance(), mu);
  argument.AddAttribute("mu", attr_mu);
  pir::Attribute attr_use_nesterov = pir::BoolAttribute::get(pir::IrContext::Instance(), use_nesterov);
  argument.AddAttribute("use_nesterov", attr_use_nesterov);
  std::vector<pir::Attribute> vec_regularization_method;
  for (size_t i = 0; i < static_cast<size_t>(regularization_method.size()); i++) {
      pir::Attribute attr_regularization_method = pir::StrAttribute::get(pir::IrContext::Instance(), regularization_method[i]);

    vec_regularization_method.push_back(attr_regularization_method);
  }
  pir::Attribute attr_regularization_method = pir::ArrayAttribute::get(pir::IrContext::Instance(), vec_regularization_method);
  argument.AddAttribute("regularization_method", attr_regularization_method);
  std::vector<pir::Attribute> vec_regularization_coeff;
  for (size_t i = 0; i < static_cast<size_t>(regularization_coeff.size()); i++) {
      pir::Attribute attr_regularization_coeff = pir::FloatAttribute::get(pir::IrContext::Instance(), regularization_coeff[i]);

    vec_regularization_coeff.push_back(attr_regularization_coeff);
  }
  pir::Attribute attr_regularization_coeff = pir::ArrayAttribute::get(pir::IrContext::Instance(), vec_regularization_coeff);
  argument.AddAttribute("regularization_coeff", attr_regularization_coeff);
  pir::Attribute attr_multi_precision = pir::BoolAttribute::get(pir::IrContext::Instance(), multi_precision);
  argument.AddAttribute("multi_precision", attr_multi_precision);
  pir::Attribute attr_rescale_grad = pir::FloatAttribute::get(pir::IrContext::Instance(), rescale_grad);
  argument.AddAttribute("rescale_grad", attr_rescale_grad);

  VLOG(4) << "Builder construction outputs";
  pir::VectorType param = param_.type().dyn_cast<pir::VectorType>(); (void)param;
  pir::VectorType grad = grad_.type().dyn_cast<pir::VectorType>(); (void)grad;
  pir::VectorType velocity = velocity_.type().dyn_cast<pir::VectorType>(); (void)velocity;
  pir::VectorType learning_rate = learning_rate_.type().dyn_cast<pir::VectorType>(); (void)learning_rate;
  std::vector<paddle::dialect::IrTensor> vec_ir_tensor_param;
  for (size_t i=0; i < static_cast<size_t>(param.size()); i++) {
    vec_ir_tensor_param.push_back(paddle::dialect::IrTensor(paddle::dialect::TransToPhiDataType(param[i].dyn_cast<paddle::dialect::DenseTensorType>().dtype()),
                                                                     param[i].dyn_cast<paddle::dialect::DenseTensorType>().dims(),
                                                                     param[i].dyn_cast<paddle::dialect::DenseTensorType>().data_layout(),
                                                                     param[i].dyn_cast<paddle::dialect::DenseTensorType>().lod(),
                                                                     param[i].dyn_cast<paddle::dialect::DenseTensorType>().offset()));
  }
  std::vector<paddle::dialect::IrMetaTensor> vec_meta_param;
  for (size_t i=0; i < vec_ir_tensor_param.size(); i++) {
    vec_meta_param.push_back(paddle::dialect::IrMetaTensor(&vec_ir_tensor_param[i]));
  }

  std::vector<const phi::MetaTensor*> meta_param;
  for (size_t i=0; i < static_cast<size_t>(vec_meta_param.size()); i++) {
    meta_param.push_back(&vec_meta_param[i]);
  }
   std::vector<paddle::dialect::IrTensor> vec_ir_tensor_grad;
  for (size_t i=0; i < static_cast<size_t>(grad.size()); i++) {
    vec_ir_tensor_grad.push_back(paddle::dialect::IrTensor(paddle::dialect::TransToPhiDataType(grad[i].dyn_cast<paddle::dialect::DenseTensorType>().dtype()),
                                                                     grad[i].dyn_cast<paddle::dialect::DenseTensorType>().dims(),
                                                                     grad[i].dyn_cast<paddle::dialect::DenseTensorType>().data_layout(),
                                                                     grad[i].dyn_cast<paddle::dialect::DenseTensorType>().lod(),
                                                                     grad[i].dyn_cast<paddle::dialect::DenseTensorType>().offset()));
  }
  std::vector<paddle::dialect::IrMetaTensor> vec_meta_grad;
  for (size_t i=0; i < vec_ir_tensor_grad.size(); i++) {
    vec_meta_grad.push_back(paddle::dialect::IrMetaTensor(&vec_ir_tensor_grad[i]));
  }

  std::vector<const phi::MetaTensor*> meta_grad;
  for (size_t i=0; i < static_cast<size_t>(vec_meta_grad.size()); i++) {
    meta_grad.push_back(&vec_meta_grad[i]);
  }
   std::vector<paddle::dialect::IrTensor> vec_ir_tensor_velocity;
  for (size_t i=0; i < static_cast<size_t>(velocity.size()); i++) {
    vec_ir_tensor_velocity.push_back(paddle::dialect::IrTensor(paddle::dialect::TransToPhiDataType(velocity[i].dyn_cast<paddle::dialect::DenseTensorType>().dtype()),
                                                                     velocity[i].dyn_cast<paddle::dialect::DenseTensorType>().dims(),
                                                                     velocity[i].dyn_cast<paddle::dialect::DenseTensorType>().data_layout(),
                                                                     velocity[i].dyn_cast<paddle::dialect::DenseTensorType>().lod(),
                                                                     velocity[i].dyn_cast<paddle::dialect::DenseTensorType>().offset()));
  }
  std::vector<paddle::dialect::IrMetaTensor> vec_meta_velocity;
  for (size_t i=0; i < vec_ir_tensor_velocity.size(); i++) {
    vec_meta_velocity.push_back(paddle::dialect::IrMetaTensor(&vec_ir_tensor_velocity[i]));
  }

  std::vector<const phi::MetaTensor*> meta_velocity;
  for (size_t i=0; i < static_cast<size_t>(vec_meta_velocity.size()); i++) {
    meta_velocity.push_back(&vec_meta_velocity[i]);
  }
   std::vector<paddle::dialect::IrTensor> vec_ir_tensor_learning_rate;
  for (size_t i=0; i < static_cast<size_t>(learning_rate.size()); i++) {
    vec_ir_tensor_learning_rate.push_back(paddle::dialect::IrTensor(paddle::dialect::TransToPhiDataType(learning_rate[i].dyn_cast<paddle::dialect::DenseTensorType>().dtype()),
                                                                     learning_rate[i].dyn_cast<paddle::dialect::DenseTensorType>().dims(),
                                                                     learning_rate[i].dyn_cast<paddle::dialect::DenseTensorType>().data_layout(),
                                                                     learning_rate[i].dyn_cast<paddle::dialect::DenseTensorType>().lod(),
                                                                     learning_rate[i].dyn_cast<paddle::dialect::DenseTensorType>().offset()));
  }
  std::vector<paddle::dialect::IrMetaTensor> vec_meta_learning_rate;
  for (size_t i=0; i < vec_ir_tensor_learning_rate.size(); i++) {
    vec_meta_learning_rate.push_back(paddle::dialect::IrMetaTensor(&vec_ir_tensor_learning_rate[i]));
  }

  std::vector<const phi::MetaTensor*> meta_learning_rate;
  for (size_t i=0; i < static_cast<size_t>(vec_meta_learning_rate.size()); i++) {
    meta_learning_rate.push_back(&vec_meta_learning_rate[i]);
  }
   std::vector<paddle::dialect::IrTensor> vec_ir_tensor_master_param;
  if (master_param_.impl() != nullptr) {
    pir::VectorType master_param = master_param_.type().dyn_cast<pir::VectorType>();
    for (size_t i=0; i < static_cast<size_t>(master_param.size()); i++) {
        vec_ir_tensor_master_param.push_back(paddle::dialect::IrTensor(paddle::dialect::TransToPhiDataType(master_param[i].dyn_cast<paddle::dialect::DenseTensorType>().dtype()),
                                                                        master_param[i].dyn_cast<paddle::dialect::DenseTensorType>().dims(),
                                                                        master_param[i].dyn_cast<paddle::dialect::DenseTensorType>().data_layout(),
                                                                        master_param[i].dyn_cast<paddle::dialect::DenseTensorType>().lod(),
                                                                        master_param[i].dyn_cast<paddle::dialect::DenseTensorType>().offset()));
    }
  }

  std::vector<paddle::dialect::IrMetaTensor> vec_meta_master_param;
  for (size_t i=0; i < vec_ir_tensor_master_param.size(); i++) {
    vec_meta_master_param.push_back(paddle::dialect::IrMetaTensor(&vec_ir_tensor_master_param[i]));
  }

  std::vector<const phi::MetaTensor*> meta_master_param;
  for (size_t i=0; i < static_cast<size_t>(vec_meta_master_param.size()); i++) {
    meta_master_param.push_back(&vec_meta_master_param[i]);
  }

  std::vector<paddle::dialect::IrTensor> vec_dense_param_out((param.size()), paddle::dialect::IrTensor());
  std::vector<paddle::dialect::IrMetaTensor> vec_meta_param_out;
  for (size_t i=0; i < static_cast<size_t>(param.size()); i++) {
    vec_meta_param_out.push_back(paddle::dialect::IrMetaTensor(&vec_dense_param_out[i]));
  }
  std::vector<phi::MetaTensor*> meta_param_out;
  for (size_t i=0; i < static_cast<size_t>(vec_meta_param_out.size()); i++) {
    meta_param_out.push_back(&vec_meta_param_out[i]);
  }
  std::vector<paddle::dialect::IrTensor> vec_dense_velocity_out((param.size()), paddle::dialect::IrTensor());
  std::vector<paddle::dialect::IrMetaTensor> vec_meta_velocity_out;
  for (size_t i=0; i < static_cast<size_t>(param.size()); i++) {
    vec_meta_velocity_out.push_back(paddle::dialect::IrMetaTensor(&vec_dense_velocity_out[i]));
  }
  std::vector<phi::MetaTensor*> meta_velocity_out;
  for (size_t i=0; i < static_cast<size_t>(vec_meta_velocity_out.size()); i++) {
    meta_velocity_out.push_back(&vec_meta_velocity_out[i]);
  }
  std::vector<paddle::dialect::IrTensor> vec_dense_master_param_out((param.size()), paddle::dialect::IrTensor());
  std::vector<paddle::dialect::IrMetaTensor> vec_meta_master_param_out;
  for (size_t i=0; i < static_cast<size_t>(param.size()); i++) {
    vec_meta_master_param_out.push_back(paddle::dialect::IrMetaTensor(&vec_dense_master_param_out[i]));
  }
  std::vector<phi::MetaTensor*> meta_master_param_out;
  for (size_t i=0; i < static_cast<size_t>(vec_meta_master_param_out.size()); i++) {
    meta_master_param_out.push_back(&vec_meta_master_param_out[i]);
  }

  phi::MergedMomentumInferMeta(meta_param, meta_grad, meta_velocity, meta_learning_rate, meta_master_param, mu, use_nesterov, regularization_method, regularization_coeff, multi_precision, rescale_grad, meta_param_out, meta_velocity_out, meta_master_param_out);

  std::vector<pir::Type> argument_outputs;
  std::vector<pir::Type> param_out_types;
  for (size_t i=0; i < static_cast<size_t>(param.size()); i++) {
    param_out_types.push_back(paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(vec_dense_param_out[i].dtype()), vec_dense_param_out[i].dims(), vec_dense_param_out[i].layout(), vec_dense_param_out[i].lod(), vec_dense_param_out[i].offset()));
  }
  pir::Type param_out_vector_type = pir::VectorType::get(pir::IrContext::Instance(), param_out_types);
  argument_outputs.push_back(param_out_vector_type);

  std::vector<pir::Type> velocity_out_types;
  for (size_t i=0; i < static_cast<size_t>(param.size()); i++) {
    velocity_out_types.push_back(paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(vec_dense_velocity_out[i].dtype()), vec_dense_velocity_out[i].dims(), vec_dense_velocity_out[i].layout(), vec_dense_velocity_out[i].lod(), vec_dense_velocity_out[i].offset()));
  }
  pir::Type velocity_out_vector_type = pir::VectorType::get(pir::IrContext::Instance(), velocity_out_types);
  argument_outputs.push_back(velocity_out_vector_type);

  std::vector<pir::Type> master_param_out_types;
  for (size_t i=0; i < static_cast<size_t>(param.size()); i++) {
    master_param_out_types.push_back(paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(vec_dense_master_param_out[i].dtype()), vec_dense_master_param_out[i].dims(), vec_dense_master_param_out[i].layout(), vec_dense_master_param_out[i].lod(), vec_dense_master_param_out[i].offset()));
  }
  pir::Type master_param_out_vector_type = pir::VectorType::get(pir::IrContext::Instance(), master_param_out_types);
  argument_outputs.push_back(master_param_out_vector_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void MergedMomentum_Op::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value param_, pir::Value grad_, pir::Value velocity_, pir::Value learning_rate_, pir::Value master_param_, pir::AttributeMap attributes) {
  VLOG(4) << "Start build MergedMomentum_Op";


  IR_ENFORCE(
      attributes.find("mu") != attributes.end(),
          "'mu' Attribute is expected for MergedMomentum_Op. ");
  float mu = attributes.at("mu").dyn_cast<pir::FloatAttribute>().data();

  IR_ENFORCE(
      attributes.find("use_nesterov") != attributes.end(),
          "'use_nesterov' Attribute is expected for MergedMomentum_Op. ");
  bool use_nesterov = attributes.at("use_nesterov").dyn_cast<pir::BoolAttribute>().data();

  IR_ENFORCE(
      attributes.find("regularization_method") != attributes.end(),
          "'regularization_method' Attribute is expected for MergedMomentum_Op. ");
  std::vector<std::string> regularization_method;
  for (size_t i = 0; i < attributes.at("regularization_method").dyn_cast<pir::ArrayAttribute>().size(); i++) {
    regularization_method.push_back(attributes.at("regularization_method").dyn_cast<pir::ArrayAttribute>().at(i).dyn_cast<pir::StrAttribute>().AsString());
  }

  IR_ENFORCE(
      attributes.find("regularization_coeff") != attributes.end(),
          "'regularization_coeff' Attribute is expected for MergedMomentum_Op. ");
  std::vector<float> regularization_coeff;
  for (size_t i = 0; i < attributes.at("regularization_coeff").dyn_cast<pir::ArrayAttribute>().size(); i++) {
    regularization_coeff.push_back(attributes.at("regularization_coeff").dyn_cast<pir::ArrayAttribute>().at(i).dyn_cast<pir::FloatAttribute>().data());
  }

  IR_ENFORCE(
      attributes.find("multi_precision") != attributes.end(),
          "'multi_precision' Attribute is expected for MergedMomentum_Op. ");
  bool multi_precision = attributes.at("multi_precision").dyn_cast<pir::BoolAttribute>().data();

  IR_ENFORCE(
      attributes.find("rescale_grad") != attributes.end(),
          "'rescale_grad' Attribute is expected for MergedMomentum_Op. ");
  float rescale_grad = attributes.at("rescale_grad").dyn_cast<pir::FloatAttribute>().data();


  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {param_, grad_, velocity_, learning_rate_, master_param_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_mu = pir::FloatAttribute::get(pir::IrContext::Instance(), mu);
  argument.AddAttribute("mu", attr_mu);
  pir::Attribute attr_use_nesterov = pir::BoolAttribute::get(pir::IrContext::Instance(), use_nesterov);
  argument.AddAttribute("use_nesterov", attr_use_nesterov);
  std::vector<pir::Attribute> vec_regularization_method;
  for (size_t i = 0; i < static_cast<size_t>(regularization_method.size()); i++) {
      pir::Attribute attr_regularization_method = pir::StrAttribute::get(pir::IrContext::Instance(), regularization_method[i]);

    vec_regularization_method.push_back(attr_regularization_method);
  }
  pir::Attribute attr_regularization_method = pir::ArrayAttribute::get(pir::IrContext::Instance(), vec_regularization_method);
  argument.AddAttribute("regularization_method", attr_regularization_method);
  std::vector<pir::Attribute> vec_regularization_coeff;
  for (size_t i = 0; i < static_cast<size_t>(regularization_coeff.size()); i++) {
      pir::Attribute attr_regularization_coeff = pir::FloatAttribute::get(pir::IrContext::Instance(), regularization_coeff[i]);

    vec_regularization_coeff.push_back(attr_regularization_coeff);
  }
  pir::Attribute attr_regularization_coeff = pir::ArrayAttribute::get(pir::IrContext::Instance(), vec_regularization_coeff);
  argument.AddAttribute("regularization_coeff", attr_regularization_coeff);
  pir::Attribute attr_multi_precision = pir::BoolAttribute::get(pir::IrContext::Instance(), multi_precision);
  argument.AddAttribute("multi_precision", attr_multi_precision);
  pir::Attribute attr_rescale_grad = pir::FloatAttribute::get(pir::IrContext::Instance(), rescale_grad);
  argument.AddAttribute("rescale_grad", attr_rescale_grad);

  VLOG(4) << "Builder construction outputs";
  pir::VectorType param = param_.type().dyn_cast<pir::VectorType>(); (void)param;
  pir::VectorType grad = grad_.type().dyn_cast<pir::VectorType>(); (void)grad;
  pir::VectorType velocity = velocity_.type().dyn_cast<pir::VectorType>(); (void)velocity;
  pir::VectorType learning_rate = learning_rate_.type().dyn_cast<pir::VectorType>(); (void)learning_rate;
  std::vector<paddle::dialect::IrTensor> vec_ir_tensor_param;
  for (size_t i=0; i < static_cast<size_t>(param.size()); i++) {
    vec_ir_tensor_param.push_back(paddle::dialect::IrTensor(paddle::dialect::TransToPhiDataType(param[i].dyn_cast<paddle::dialect::DenseTensorType>().dtype()),
                                                                     param[i].dyn_cast<paddle::dialect::DenseTensorType>().dims(),
                                                                     param[i].dyn_cast<paddle::dialect::DenseTensorType>().data_layout(),
                                                                     param[i].dyn_cast<paddle::dialect::DenseTensorType>().lod(),
                                                                     param[i].dyn_cast<paddle::dialect::DenseTensorType>().offset()));
  }
  std::vector<paddle::dialect::IrMetaTensor> vec_meta_param;
  for (size_t i=0; i < vec_ir_tensor_param.size(); i++) {
    vec_meta_param.push_back(paddle::dialect::IrMetaTensor(&vec_ir_tensor_param[i]));
  }

  std::vector<const phi::MetaTensor*> meta_param;
  for (size_t i=0; i < static_cast<size_t>(vec_meta_param.size()); i++) {
    meta_param.push_back(&vec_meta_param[i]);
  }
   std::vector<paddle::dialect::IrTensor> vec_ir_tensor_grad;
  for (size_t i=0; i < static_cast<size_t>(grad.size()); i++) {
    vec_ir_tensor_grad.push_back(paddle::dialect::IrTensor(paddle::dialect::TransToPhiDataType(grad[i].dyn_cast<paddle::dialect::DenseTensorType>().dtype()),
                                                                     grad[i].dyn_cast<paddle::dialect::DenseTensorType>().dims(),
                                                                     grad[i].dyn_cast<paddle::dialect::DenseTensorType>().data_layout(),
                                                                     grad[i].dyn_cast<paddle::dialect::DenseTensorType>().lod(),
                                                                     grad[i].dyn_cast<paddle::dialect::DenseTensorType>().offset()));
  }
  std::vector<paddle::dialect::IrMetaTensor> vec_meta_grad;
  for (size_t i=0; i < vec_ir_tensor_grad.size(); i++) {
    vec_meta_grad.push_back(paddle::dialect::IrMetaTensor(&vec_ir_tensor_grad[i]));
  }

  std::vector<const phi::MetaTensor*> meta_grad;
  for (size_t i=0; i < static_cast<size_t>(vec_meta_grad.size()); i++) {
    meta_grad.push_back(&vec_meta_grad[i]);
  }
   std::vector<paddle::dialect::IrTensor> vec_ir_tensor_velocity;
  for (size_t i=0; i < static_cast<size_t>(velocity.size()); i++) {
    vec_ir_tensor_velocity.push_back(paddle::dialect::IrTensor(paddle::dialect::TransToPhiDataType(velocity[i].dyn_cast<paddle::dialect::DenseTensorType>().dtype()),
                                                                     velocity[i].dyn_cast<paddle::dialect::DenseTensorType>().dims(),
                                                                     velocity[i].dyn_cast<paddle::dialect::DenseTensorType>().data_layout(),
                                                                     velocity[i].dyn_cast<paddle::dialect::DenseTensorType>().lod(),
                                                                     velocity[i].dyn_cast<paddle::dialect::DenseTensorType>().offset()));
  }
  std::vector<paddle::dialect::IrMetaTensor> vec_meta_velocity;
  for (size_t i=0; i < vec_ir_tensor_velocity.size(); i++) {
    vec_meta_velocity.push_back(paddle::dialect::IrMetaTensor(&vec_ir_tensor_velocity[i]));
  }

  std::vector<const phi::MetaTensor*> meta_velocity;
  for (size_t i=0; i < static_cast<size_t>(vec_meta_velocity.size()); i++) {
    meta_velocity.push_back(&vec_meta_velocity[i]);
  }
   std::vector<paddle::dialect::IrTensor> vec_ir_tensor_learning_rate;
  for (size_t i=0; i < static_cast<size_t>(learning_rate.size()); i++) {
    vec_ir_tensor_learning_rate.push_back(paddle::dialect::IrTensor(paddle::dialect::TransToPhiDataType(learning_rate[i].dyn_cast<paddle::dialect::DenseTensorType>().dtype()),
                                                                     learning_rate[i].dyn_cast<paddle::dialect::DenseTensorType>().dims(),
                                                                     learning_rate[i].dyn_cast<paddle::dialect::DenseTensorType>().data_layout(),
                                                                     learning_rate[i].dyn_cast<paddle::dialect::DenseTensorType>().lod(),
                                                                     learning_rate[i].dyn_cast<paddle::dialect::DenseTensorType>().offset()));
  }
  std::vector<paddle::dialect::IrMetaTensor> vec_meta_learning_rate;
  for (size_t i=0; i < vec_ir_tensor_learning_rate.size(); i++) {
    vec_meta_learning_rate.push_back(paddle::dialect::IrMetaTensor(&vec_ir_tensor_learning_rate[i]));
  }

  std::vector<const phi::MetaTensor*> meta_learning_rate;
  for (size_t i=0; i < static_cast<size_t>(vec_meta_learning_rate.size()); i++) {
    meta_learning_rate.push_back(&vec_meta_learning_rate[i]);
  }
   std::vector<paddle::dialect::IrTensor> vec_ir_tensor_master_param;
  if (master_param_.impl() != nullptr) {
    pir::VectorType master_param = master_param_.type().dyn_cast<pir::VectorType>();
    for (size_t i=0; i < static_cast<size_t>(master_param.size()); i++) {
        vec_ir_tensor_master_param.push_back(paddle::dialect::IrTensor(paddle::dialect::TransToPhiDataType(master_param[i].dyn_cast<paddle::dialect::DenseTensorType>().dtype()),
                                                                        master_param[i].dyn_cast<paddle::dialect::DenseTensorType>().dims(),
                                                                        master_param[i].dyn_cast<paddle::dialect::DenseTensorType>().data_layout(),
                                                                        master_param[i].dyn_cast<paddle::dialect::DenseTensorType>().lod(),
                                                                        master_param[i].dyn_cast<paddle::dialect::DenseTensorType>().offset()));
    }
  }

  std::vector<paddle::dialect::IrMetaTensor> vec_meta_master_param;
  for (size_t i=0; i < vec_ir_tensor_master_param.size(); i++) {
    vec_meta_master_param.push_back(paddle::dialect::IrMetaTensor(&vec_ir_tensor_master_param[i]));
  }

  std::vector<const phi::MetaTensor*> meta_master_param;
  for (size_t i=0; i < static_cast<size_t>(vec_meta_master_param.size()); i++) {
    meta_master_param.push_back(&vec_meta_master_param[i]);
  }

  std::vector<paddle::dialect::IrTensor> vec_dense_param_out((param.size()), paddle::dialect::IrTensor());
  std::vector<paddle::dialect::IrMetaTensor> vec_meta_param_out;
  for (size_t i=0; i < static_cast<size_t>(param.size()); i++) {
    vec_meta_param_out.push_back(paddle::dialect::IrMetaTensor(&vec_dense_param_out[i]));
  }
  std::vector<phi::MetaTensor*> meta_param_out;
  for (size_t i=0; i < static_cast<size_t>(vec_meta_param_out.size()); i++) {
    meta_param_out.push_back(&vec_meta_param_out[i]);
  }
  std::vector<paddle::dialect::IrTensor> vec_dense_velocity_out((param.size()), paddle::dialect::IrTensor());
  std::vector<paddle::dialect::IrMetaTensor> vec_meta_velocity_out;
  for (size_t i=0; i < static_cast<size_t>(param.size()); i++) {
    vec_meta_velocity_out.push_back(paddle::dialect::IrMetaTensor(&vec_dense_velocity_out[i]));
  }
  std::vector<phi::MetaTensor*> meta_velocity_out;
  for (size_t i=0; i < static_cast<size_t>(vec_meta_velocity_out.size()); i++) {
    meta_velocity_out.push_back(&vec_meta_velocity_out[i]);
  }
  std::vector<paddle::dialect::IrTensor> vec_dense_master_param_out((param.size()), paddle::dialect::IrTensor());
  std::vector<paddle::dialect::IrMetaTensor> vec_meta_master_param_out;
  for (size_t i=0; i < static_cast<size_t>(param.size()); i++) {
    vec_meta_master_param_out.push_back(paddle::dialect::IrMetaTensor(&vec_dense_master_param_out[i]));
  }
  std::vector<phi::MetaTensor*> meta_master_param_out;
  for (size_t i=0; i < static_cast<size_t>(vec_meta_master_param_out.size()); i++) {
    meta_master_param_out.push_back(&vec_meta_master_param_out[i]);
  }

  phi::MergedMomentumInferMeta(meta_param, meta_grad, meta_velocity, meta_learning_rate, meta_master_param, mu, use_nesterov, regularization_method, regularization_coeff, multi_precision, rescale_grad, meta_param_out, meta_velocity_out, meta_master_param_out);

  std::vector<pir::Type> argument_outputs;
  std::vector<pir::Type> param_out_types;
  for (size_t i=0; i < static_cast<size_t>(param.size()); i++) {
    param_out_types.push_back(paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(vec_dense_param_out[i].dtype()), vec_dense_param_out[i].dims(), vec_dense_param_out[i].layout(), vec_dense_param_out[i].lod(), vec_dense_param_out[i].offset()));
  }
  pir::Type param_out_vector_type = pir::VectorType::get(pir::IrContext::Instance(), param_out_types);
  argument_outputs.push_back(param_out_vector_type);

  std::vector<pir::Type> velocity_out_types;
  for (size_t i=0; i < static_cast<size_t>(param.size()); i++) {
    velocity_out_types.push_back(paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(vec_dense_velocity_out[i].dtype()), vec_dense_velocity_out[i].dims(), vec_dense_velocity_out[i].layout(), vec_dense_velocity_out[i].lod(), vec_dense_velocity_out[i].offset()));
  }
  pir::Type velocity_out_vector_type = pir::VectorType::get(pir::IrContext::Instance(), velocity_out_types);
  argument_outputs.push_back(velocity_out_vector_type);

  std::vector<pir::Type> master_param_out_types;
  for (size_t i=0; i < static_cast<size_t>(param.size()); i++) {
    master_param_out_types.push_back(paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(vec_dense_master_param_out[i].dtype()), vec_dense_master_param_out[i].dims(), vec_dense_master_param_out[i].layout(), vec_dense_master_param_out[i].lod(), vec_dense_master_param_out[i].offset()));
  }
  pir::Type master_param_out_vector_type = pir::VectorType::get(pir::IrContext::Instance(), master_param_out_types);
  argument_outputs.push_back(master_param_out_vector_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void MergedMomentum_Op::VerifySig() {
  VLOG(4) << "Start Verifying inputs, outputs and attributes for: MergedMomentum_Op.";
  VLOG(4) << "Verifying inputs:";
  {
  auto input_size = num_operands();
  IR_ENFORCE(input_size == 5u,
                    "The size %d of inputs must be equal to 5.", input_size);
  if (auto vec_type = (*this)->operand_source(0).type().dyn_cast<pir::VectorType>()) {
      for (size_t i = 0; i < vec_type.size(); ++i) {
        IR_ENFORCE(vec_type[i].isa<paddle::dialect::DenseTensorType>(),
                       "Type validation failed for the 0th input, got %s.", (*this)->operand_source(0).type());
      }
  }
  else {
    IR_ENFORCE((*this)->operand_source(0).type().isa<paddle::dialect::DenseTensorType>(),
                   "Type validation failed for the 0th input, got %s.", (*this)->operand_source(0).type());
  }
  if (auto vec_type = (*this)->operand_source(1).type().dyn_cast<pir::VectorType>()) {
      for (size_t i = 0; i < vec_type.size(); ++i) {
        IR_ENFORCE(vec_type[i].isa<paddle::dialect::DenseTensorType>(),
                       "Type validation failed for the 1th input, got %s.", (*this)->operand_source(1).type());
      }
  }
  else {
    IR_ENFORCE((*this)->operand_source(1).type().isa<paddle::dialect::DenseTensorType>(),
                   "Type validation failed for the 1th input, got %s.", (*this)->operand_source(1).type());
  }
  if (auto vec_type = (*this)->operand_source(2).type().dyn_cast<pir::VectorType>()) {
      for (size_t i = 0; i < vec_type.size(); ++i) {
        IR_ENFORCE(vec_type[i].isa<paddle::dialect::DenseTensorType>(),
                       "Type validation failed for the 2th input, got %s.", (*this)->operand_source(2).type());
      }
  }
  else {
    IR_ENFORCE((*this)->operand_source(2).type().isa<paddle::dialect::DenseTensorType>(),
                   "Type validation failed for the 2th input, got %s.", (*this)->operand_source(2).type());
  }
  if (auto vec_type = (*this)->operand_source(3).type().dyn_cast<pir::VectorType>()) {
      for (size_t i = 0; i < vec_type.size(); ++i) {
        IR_ENFORCE(vec_type[i].isa<paddle::dialect::DenseTensorType>(),
                       "Type validation failed for the 3th input, got %s.", (*this)->operand_source(3).type());
      }
  }
  else {
    IR_ENFORCE((*this)->operand_source(3).type().isa<paddle::dialect::DenseTensorType>(),
                   "Type validation failed for the 3th input, got %s.", (*this)->operand_source(3).type());
  }
  if (auto val =  (*this)->operand(4)) {
    if (auto vec_type = val.type().dyn_cast<pir::VectorType>()) {
      for (size_t i = 0; i < vec_type.size(); i++) {
        IR_ENFORCE(vec_type[i].isa<paddle::dialect::DenseTensorType>(),
                          "Type validation failed for the 4th input, got %s.", (*this)->operand_source(4).type());
      }
    }
    else {
      IR_ENFORCE(val.type().isa<paddle::dialect::DenseTensorType>(),
                        "Type validation failed for the 4th input, got %s.", (*this)->operand_source(4).type());
    }
  }
  }
  VLOG(4) << "Verifying attributes:";
  {
  auto& attributes = this->attributes();
  IR_ENFORCE(attributes.count("mu")>0,
                 "mu does not exist.");
  IR_ENFORCE(attributes.at("mu").isa<pir::FloatAttribute>(),
                 "Type of attribute: mu is not pir::FloatAttribute.");

  IR_ENFORCE(attributes.count("use_nesterov")>0,
                 "use_nesterov does not exist.");
  IR_ENFORCE(attributes.at("use_nesterov").isa<pir::BoolAttribute>(),
                 "Type of attribute: use_nesterov is not pir::BoolAttribute.");

  IR_ENFORCE(attributes.count("regularization_method")>0,
                 "regularization_method does not exist.");
  IR_ENFORCE(attributes.at("regularization_method").isa<pir::ArrayAttribute>(),
                 "Type of attribute: regularization_method is not pir::ArrayAttribute.");
  for (size_t i = 0; i < attributes.at("regularization_method").dyn_cast<pir::ArrayAttribute>().size(); i++) {
    IR_ENFORCE(attributes.at("regularization_method").dyn_cast<pir::ArrayAttribute>().at(i).isa<pir::StrAttribute>(),
                   "Type of attribute: regularization_method is not right.");
  }
  IR_ENFORCE(attributes.count("regularization_coeff")>0,
                 "regularization_coeff does not exist.");
  IR_ENFORCE(attributes.at("regularization_coeff").isa<pir::ArrayAttribute>(),
                 "Type of attribute: regularization_coeff is not pir::ArrayAttribute.");
  for (size_t i = 0; i < attributes.at("regularization_coeff").dyn_cast<pir::ArrayAttribute>().size(); i++) {
    IR_ENFORCE(attributes.at("regularization_coeff").dyn_cast<pir::ArrayAttribute>().at(i).isa<pir::FloatAttribute>(),
                   "Type of attribute: regularization_coeff is not right.");
  }
  IR_ENFORCE(attributes.count("multi_precision")>0,
                 "multi_precision does not exist.");
  IR_ENFORCE(attributes.at("multi_precision").isa<pir::BoolAttribute>(),
                 "Type of attribute: multi_precision is not pir::BoolAttribute.");

  IR_ENFORCE(attributes.count("rescale_grad")>0,
                 "rescale_grad does not exist.");
  IR_ENFORCE(attributes.at("rescale_grad").isa<pir::FloatAttribute>(),
                 "Type of attribute: rescale_grad is not pir::FloatAttribute.");

  }
  VLOG(4) << "Verifying outputs:";
  {
  auto output_size = num_results();
  IR_ENFORCE(output_size == 3u,
                    "The size %d of outputs must be equal to 3.", output_size);
  auto output_0_type = (*this)->result(0).type();
  if (auto vec_type = output_0_type.dyn_cast<pir::VectorType>()) {
    for (size_t i = 0; i < vec_type.size(); i++) {
      IR_ENFORCE(vec_type[i].isa<paddle::dialect::DenseTensorType>(),
                     "Type validation failed for the 0th output.");
    }
  }
  else {
    IR_ENFORCE(output_0_type.isa<paddle::dialect::DenseTensorType>(),
                   "Type validation failed for the 0th output.");
  }
  auto output_1_type = (*this)->result(1).type();
  if (auto vec_type = output_1_type.dyn_cast<pir::VectorType>()) {
    for (size_t i = 0; i < vec_type.size(); i++) {
      IR_ENFORCE(vec_type[i].isa<paddle::dialect::DenseTensorType>(),
                     "Type validation failed for the 1th output.");
    }
  }
  else {
    IR_ENFORCE(output_1_type.isa<paddle::dialect::DenseTensorType>(),
                   "Type validation failed for the 1th output.");
  }
  if (auto output_2_type = (*this)->result(2).type()) {
    if (auto vec_type = output_2_type.dyn_cast<pir::VectorType>()) {
      for (size_t i = 0; i < vec_type.size(); ++i) {
        IR_ENFORCE(vec_type[i].isa<paddle::dialect::DenseTensorType>(),
                       "Type validation failed for the 2th output.");
      }
    }
    else {
      IR_ENFORCE(output_2_type.isa<paddle::dialect::DenseTensorType>(),
                     "Type validation failed for the 2th output.");
    }
  }
  }
  VLOG(4) << "End Verifying for: MergedMomentum_Op.";
}

void MergedMomentum_Op::InferMeta( phi::InferMetaContext *infer_meta ) {
  auto fn = PD_INFER_META(phi::MergedMomentumInferMeta);
  fn(infer_meta);
}

phi::DataType MergedMomentum_Op::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: MergedMomentum_Op";
  


  return expected_kernel_dtype;
}

OpInfoTuple MeshgridOp::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("inputs", "pir::VectorType<paddle::dialect::DenseTensorType>", false, false, false, true) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = {  };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("out", "pir::VectorType<paddle::dialect::DenseTensorType>", false, false) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("MeshgridInferMeta", {"inputs"}, "meshgrid", {"inputs"}, {"inputs"}, {}, {}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "meshgrid");
}

void MeshgridOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value inputs_) {
  VLOG(4) << "Start build MeshgridOp";



  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {inputs_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";

  VLOG(4) << "Builder construction outputs";
  pir::VectorType inputs = inputs_.type().dyn_cast<pir::VectorType>(); (void)inputs;
  std::vector<paddle::dialect::IrTensor> vec_ir_tensor_inputs;
  for (size_t i=0; i < static_cast<size_t>(inputs.size()); i++) {
    vec_ir_tensor_inputs.push_back(paddle::dialect::IrTensor(paddle::dialect::TransToPhiDataType(inputs[i].dyn_cast<paddle::dialect::DenseTensorType>().dtype()),
                                                                     inputs[i].dyn_cast<paddle::dialect::DenseTensorType>().dims(),
                                                                     inputs[i].dyn_cast<paddle::dialect::DenseTensorType>().data_layout(),
                                                                     inputs[i].dyn_cast<paddle::dialect::DenseTensorType>().lod(),
                                                                     inputs[i].dyn_cast<paddle::dialect::DenseTensorType>().offset()));
  }
  std::vector<paddle::dialect::IrMetaTensor> vec_meta_inputs;
  for (size_t i=0; i < vec_ir_tensor_inputs.size(); i++) {
    vec_meta_inputs.push_back(paddle::dialect::IrMetaTensor(&vec_ir_tensor_inputs[i]));
  }

  std::vector<const phi::MetaTensor*> meta_inputs;
  for (size_t i=0; i < static_cast<size_t>(vec_meta_inputs.size()); i++) {
    meta_inputs.push_back(&vec_meta_inputs[i]);
  }
   std::vector<paddle::dialect::IrTensor> vec_dense_out((inputs.size()), paddle::dialect::IrTensor());
  std::vector<paddle::dialect::IrMetaTensor> vec_meta_out;
  for (size_t i=0; i < static_cast<size_t>(inputs.size()); i++) {
    vec_meta_out.push_back(paddle::dialect::IrMetaTensor(&vec_dense_out[i]));
  }
  std::vector<phi::MetaTensor*> meta_out;
  for (size_t i=0; i < static_cast<size_t>(vec_meta_out.size()); i++) {
    meta_out.push_back(&vec_meta_out[i]);
  }

  phi::MeshgridInferMeta(meta_inputs, meta_out);

  std::vector<pir::Type> argument_outputs;
  std::vector<pir::Type> out_types;
  for (size_t i=0; i < static_cast<size_t>(inputs.size()); i++) {
    out_types.push_back(paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(vec_dense_out[i].dtype()), vec_dense_out[i].dims(), vec_dense_out[i].layout(), vec_dense_out[i].lod(), vec_dense_out[i].offset()));
  }
  pir::Type out_vector_type = pir::VectorType::get(pir::IrContext::Instance(), out_types);
  argument_outputs.push_back(out_vector_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void MeshgridOp::VerifySig() {
  VLOG(4) << "Start Verifying inputs, outputs and attributes for: MeshgridOp.";
  VLOG(4) << "Verifying inputs:";
  {
  auto input_size = num_operands();
  IR_ENFORCE(input_size == 1u,
                    "The size %d of inputs must be equal to 1.", input_size);
  if (auto vec_type = (*this)->operand_source(0).type().dyn_cast<pir::VectorType>()) {
      for (size_t i = 0; i < vec_type.size(); ++i) {
        IR_ENFORCE(vec_type[i].isa<paddle::dialect::DenseTensorType>(),
                       "Type validation failed for the 0th input, got %s.", (*this)->operand_source(0).type());
      }
  }
  else {
    IR_ENFORCE((*this)->operand_source(0).type().isa<paddle::dialect::DenseTensorType>(),
                   "Type validation failed for the 0th input, got %s.", (*this)->operand_source(0).type());
  }
  }
  VLOG(4) << "Verifying attributes:";
  {
  // Attributes num is 0, not need to check attributes type.
  }
  VLOG(4) << "Verifying outputs:";
  {
  auto output_size = num_results();
  IR_ENFORCE(output_size == 1u,
                    "The size %d of outputs must be equal to 1.", output_size);
  auto output_0_type = (*this)->result(0).type();
  if (auto vec_type = output_0_type.dyn_cast<pir::VectorType>()) {
    for (size_t i = 0; i < vec_type.size(); i++) {
      IR_ENFORCE(vec_type[i].isa<paddle::dialect::DenseTensorType>(),
                     "Type validation failed for the 0th output.");
    }
  }
  else {
    IR_ENFORCE(output_0_type.isa<paddle::dialect::DenseTensorType>(),
                   "Type validation failed for the 0th output.");
  }
  }
  VLOG(4) << "End Verifying for: MeshgridOp.";
}

void MeshgridOp::InferMeta( phi::InferMetaContext *infer_meta ) {
  auto fn = PD_INFER_META(phi::MeshgridInferMeta);
  fn(infer_meta);
}

phi::DataType MeshgridOp::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: MeshgridOp";
  


  return expected_kernel_dtype;
}

const char *ModeOp::attributes_name[2] = { "axis", "keepdim" };

OpInfoTuple ModeOp::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("x", "paddle::dialect::DenseTensorType", false, false, false, true) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = { paddle::dialect::OpAttributeInfo("axis", "pir::Int32Attribute", ""), paddle::dialect::OpAttributeInfo("keepdim", "pir::BoolAttribute", "") };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("out", "paddle::dialect::DenseTensorType", false, false), paddle::dialect::OpOutputInfo("indices", "paddle::dialect::DenseTensorType", false, false) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("ModeInferMeta", {"x", "axis", "keepdim"}, "mode", {"x", "axis", "keepdim"}, {}, {}, {}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "mode");
}

void ModeOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, int axis, bool keepdim) {
  VLOG(4) << "Start build ModeOp";



  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_axis = pir::Int32Attribute::get(pir::IrContext::Instance(), axis);
  argument.AddAttribute("axis", attr_axis);
  pir::Attribute attr_keepdim = pir::BoolAttribute::get(pir::IrContext::Instance(), keepdim);
  argument.AddAttribute("keepdim", attr_keepdim);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);
  paddle::dialect::IrTensor dense_out;
  paddle::dialect::IrMetaTensor meta_out(&dense_out);
  paddle::dialect::IrTensor dense_indices;
  paddle::dialect::IrMetaTensor meta_indices(&dense_indices);

  phi::ModeInferMeta(meta_x, axis, keepdim, &meta_out, &meta_indices);

  std::vector<pir::Type> argument_outputs;
  pir::Type out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_out.dtype()), dense_out.dims(), dense_out.layout(), dense_out.lod(), dense_out.offset());
  argument_outputs.push_back(out_dense_tensor_type);

  pir::Type indices_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_indices.dtype()), dense_indices.dims(), dense_indices.layout(), dense_indices.lod(), dense_indices.offset());
  argument_outputs.push_back(indices_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void ModeOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, pir::AttributeMap attributes) {
  VLOG(4) << "Start build ModeOp";


  IR_ENFORCE(
      attributes.find("axis") != attributes.end(),
          "'axis' Attribute is expected for ModeOp. ");
  int axis = attributes.at("axis").dyn_cast<pir::Int32Attribute>().data();

  IR_ENFORCE(
      attributes.find("keepdim") != attributes.end(),
          "'keepdim' Attribute is expected for ModeOp. ");
  bool keepdim = attributes.at("keepdim").dyn_cast<pir::BoolAttribute>().data();


  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_axis = pir::Int32Attribute::get(pir::IrContext::Instance(), axis);
  argument.AddAttribute("axis", attr_axis);
  pir::Attribute attr_keepdim = pir::BoolAttribute::get(pir::IrContext::Instance(), keepdim);
  argument.AddAttribute("keepdim", attr_keepdim);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);
  paddle::dialect::IrTensor dense_out;
  paddle::dialect::IrMetaTensor meta_out(&dense_out);
  paddle::dialect::IrTensor dense_indices;
  paddle::dialect::IrMetaTensor meta_indices(&dense_indices);

  phi::ModeInferMeta(meta_x, axis, keepdim, &meta_out, &meta_indices);

  std::vector<pir::Type> argument_outputs;
  pir::Type out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_out.dtype()), dense_out.dims(), dense_out.layout(), dense_out.lod(), dense_out.offset());
  argument_outputs.push_back(out_dense_tensor_type);

  pir::Type indices_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_indices.dtype()), dense_indices.dims(), dense_indices.layout(), dense_indices.lod(), dense_indices.offset());
  argument_outputs.push_back(indices_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void ModeOp::VerifySig() {
  VLOG(4) << "Start Verifying inputs, outputs and attributes for: ModeOp.";
  VLOG(4) << "Verifying inputs:";
  {
  auto input_size = num_operands();
  IR_ENFORCE(input_size == 1u,
                    "The size %d of inputs must be equal to 1.", input_size);
  IR_ENFORCE((*this)->operand_source(0).type().isa<paddle::dialect::DenseTensorType>(),
                  "Type validation failed for the 0th input, got %s.", (*this)->operand_source(0).type());
  }
  VLOG(4) << "Verifying attributes:";
  {
  auto& attributes = this->attributes();
  IR_ENFORCE(attributes.count("axis")>0,
                 "axis does not exist.");
  IR_ENFORCE(attributes.at("axis").isa<pir::Int32Attribute>(),
                 "Type of attribute: axis is not pir::Int32Attribute.");

  IR_ENFORCE(attributes.count("keepdim")>0,
                 "keepdim does not exist.");
  IR_ENFORCE(attributes.at("keepdim").isa<pir::BoolAttribute>(),
                 "Type of attribute: keepdim is not pir::BoolAttribute.");

  }
  VLOG(4) << "Verifying outputs:";
  {
  auto output_size = num_results();
  IR_ENFORCE(output_size == 2u,
                    "The size %d of outputs must be equal to 2.", output_size);
  IR_ENFORCE((*this)->result(0).type().isa<paddle::dialect::DenseTensorType>(),
                 "Type validation failed for the 0th output.");
  IR_ENFORCE((*this)->result(1).type().isa<paddle::dialect::DenseTensorType>(),
                 "Type validation failed for the 1th output.");
  }
  VLOG(4) << "End Verifying for: ModeOp.";
}

void ModeOp::InferMeta( phi::InferMetaContext *infer_meta ) {
  auto fn = PD_INFER_META(phi::ModeInferMeta);
  fn(infer_meta);
}

phi::DataType ModeOp::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: ModeOp";
  


  return expected_kernel_dtype;
}

const char *Momentum_Op::attributes_name[6] = { "mu", "use_nesterov", "regularization_method", "regularization_coeff", "multi_precision", "rescale_grad" };

OpInfoTuple Momentum_Op::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("param", "paddle::dialect::DenseTensorType", false, false, false, false), paddle::dialect::OpInputInfo("grad", "paddle::dialect::DenseTensorType", false, false, false, false), paddle::dialect::OpInputInfo("velocity", "paddle::dialect::DenseTensorType", false, false, false, false), paddle::dialect::OpInputInfo("learning_rate", "paddle::dialect::DenseTensorType", false, false, false, false), paddle::dialect::OpInputInfo("master_param", "paddle::dialect::DenseTensorType", true, false, false, false) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = { paddle::dialect::OpAttributeInfo("mu", "pir::FloatAttribute", ""), paddle::dialect::OpAttributeInfo("use_nesterov", "pir::BoolAttribute", ""), paddle::dialect::OpAttributeInfo("regularization_method", "pir::StrAttribute", ""), paddle::dialect::OpAttributeInfo("regularization_coeff", "pir::FloatAttribute", ""), paddle::dialect::OpAttributeInfo("multi_precision", "pir::BoolAttribute", ""), paddle::dialect::OpAttributeInfo("rescale_grad", "pir::FloatAttribute", "") };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("param_out", "paddle::dialect::DenseTensorType", false, false), paddle::dialect::OpOutputInfo("velocity_out", "paddle::dialect::DenseTensorType", false, false), paddle::dialect::OpOutputInfo("master_param_out", "paddle::dialect::DenseTensorType", true, false) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("MomentumInferMeta", {"param", "grad", "velocity", "learning_rate", "master_param", "mu", "use_nesterov", "regularization_method", "regularization_coeff", "multi_precision", "rescale_grad"}, "momentum", {"param", "grad", "velocity", "learning_rate", "master_param", "mu", "use_nesterov", "regularization_method", "regularization_coeff", "multi_precision", "rescale_grad"}, {"param"}, {}, {{"param_out", "param"},{"velocity_out", "velocity"},{"master_param_out", "master_param"}}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "momentum_");
}

void Momentum_Op::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value param_, pir::Value grad_, pir::Value velocity_, pir::Value learning_rate_, pir::Value master_param_, float mu, bool use_nesterov, const std::string& regularization_method, float regularization_coeff, bool multi_precision, float rescale_grad) {
  VLOG(4) << "Start build Momentum_Op";



  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {param_, grad_, velocity_, learning_rate_, master_param_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_mu = pir::FloatAttribute::get(pir::IrContext::Instance(), mu);
  argument.AddAttribute("mu", attr_mu);
  pir::Attribute attr_use_nesterov = pir::BoolAttribute::get(pir::IrContext::Instance(), use_nesterov);
  argument.AddAttribute("use_nesterov", attr_use_nesterov);
  pir::Attribute attr_regularization_method = pir::StrAttribute::get(pir::IrContext::Instance(), regularization_method);
  argument.AddAttribute("regularization_method", attr_regularization_method);
  pir::Attribute attr_regularization_coeff = pir::FloatAttribute::get(pir::IrContext::Instance(), regularization_coeff);
  argument.AddAttribute("regularization_coeff", attr_regularization_coeff);
  pir::Attribute attr_multi_precision = pir::BoolAttribute::get(pir::IrContext::Instance(), multi_precision);
  argument.AddAttribute("multi_precision", attr_multi_precision);
  pir::Attribute attr_rescale_grad = pir::FloatAttribute::get(pir::IrContext::Instance(), rescale_grad);
  argument.AddAttribute("rescale_grad", attr_rescale_grad);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType param = param_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)param;
  paddle::dialect::DenseTensorType grad = grad_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)grad;
  paddle::dialect::DenseTensorType velocity = velocity_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)velocity;
  paddle::dialect::DenseTensorType learning_rate = learning_rate_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)learning_rate;

  VLOG(4) << "Builder construction  dense_param";
  paddle::dialect::IrTensor ir_tensor_param(paddle::dialect::TransToPhiDataType(param.dtype()),
                                                      param.dims(),
                                                      param.data_layout(),
                                                      param.lod(),
                                                      param.offset());
  VLOG(4) << "Builder construction  meta_param";
  paddle::dialect::IrMetaTensor meta_param(&ir_tensor_param);

  VLOG(4) << "Builder construction  dense_grad";
  paddle::dialect::IrTensor ir_tensor_grad(paddle::dialect::TransToPhiDataType(grad.dtype()),
                                                      grad.dims(),
                                                      grad.data_layout(),
                                                      grad.lod(),
                                                      grad.offset());
  VLOG(4) << "Builder construction  meta_grad";
  paddle::dialect::IrMetaTensor meta_grad(&ir_tensor_grad);

  VLOG(4) << "Builder construction  dense_velocity";
  paddle::dialect::IrTensor ir_tensor_velocity(paddle::dialect::TransToPhiDataType(velocity.dtype()),
                                                      velocity.dims(),
                                                      velocity.data_layout(),
                                                      velocity.lod(),
                                                      velocity.offset());
  VLOG(4) << "Builder construction  meta_velocity";
  paddle::dialect::IrMetaTensor meta_velocity(&ir_tensor_velocity);

  VLOG(4) << "Builder construction  dense_learning_rate";
  paddle::dialect::IrTensor ir_tensor_learning_rate(paddle::dialect::TransToPhiDataType(learning_rate.dtype()),
                                                      learning_rate.dims(),
                                                      learning_rate.data_layout(),
                                                      learning_rate.lod(),
                                                      learning_rate.offset());
  VLOG(4) << "Builder construction  meta_learning_rate";
  paddle::dialect::IrMetaTensor meta_learning_rate(&ir_tensor_learning_rate);

  paddle::dialect::IrMetaTensor meta_master_param;
  paddle::dialect::IrTensor ir_tensor_master_param;
  if (master_param_.impl() != nullptr) {
    paddle::dialect::DenseTensorType master_param = master_param_.type().dyn_cast<paddle::dialect::DenseTensorType>();
    VLOG(4) << "Builder construction  dense_master_param";
    ir_tensor_master_param = paddle::dialect::IrTensor(paddle::dialect::TransToPhiDataType(master_param.dtype()),
                                                        master_param.dims(),
                                                        master_param.data_layout(),
                                                        master_param.lod(),
                                                        master_param.offset());
    VLOG(4) << "Builder construction  meta_master_param";
    meta_master_param = paddle::dialect::IrMetaTensor(&ir_tensor_master_param);
  }

  paddle::dialect::IrTensor dense_param_out;
  paddle::dialect::IrMetaTensor meta_param_out(&dense_param_out);
  paddle::dialect::IrTensor dense_velocity_out;
  paddle::dialect::IrMetaTensor meta_velocity_out(&dense_velocity_out);
  paddle::dialect::IrTensor dense_master_param_out;
  paddle::dialect::IrMetaTensor meta_master_param_out(&dense_master_param_out);

  phi::MomentumInferMeta(meta_param, meta_grad, meta_velocity, meta_learning_rate, meta_master_param, mu, use_nesterov, regularization_method, regularization_coeff, multi_precision, rescale_grad, &meta_param_out, &meta_velocity_out, &meta_master_param_out);

  std::vector<pir::Type> argument_outputs;
  pir::Type param_out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_param_out.dtype()), dense_param_out.dims(), dense_param_out.layout(), dense_param_out.lod(), dense_param_out.offset());
  argument_outputs.push_back(param_out_dense_tensor_type);

  pir::Type velocity_out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_velocity_out.dtype()), dense_velocity_out.dims(), dense_velocity_out.layout(), dense_velocity_out.lod(), dense_velocity_out.offset());
  argument_outputs.push_back(velocity_out_dense_tensor_type);

  if (master_param_.impl() != nullptr) {
    pir::Type master_param_out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_master_param_out.dtype()), dense_master_param_out.dims(), dense_master_param_out.layout(), dense_master_param_out.lod(), dense_master_param_out.offset());
    argument_outputs.push_back(master_param_out_dense_tensor_type);
  } else {
    pir::Type master_param_out_type;
    argument_outputs.push_back(master_param_out_type);
  }

  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void Momentum_Op::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value param_, pir::Value grad_, pir::Value velocity_, pir::Value learning_rate_, pir::Value master_param_, pir::AttributeMap attributes) {
  VLOG(4) << "Start build Momentum_Op";


  IR_ENFORCE(
      attributes.find("mu") != attributes.end(),
          "'mu' Attribute is expected for Momentum_Op. ");
  float mu = attributes.at("mu").dyn_cast<pir::FloatAttribute>().data();

  IR_ENFORCE(
      attributes.find("use_nesterov") != attributes.end(),
          "'use_nesterov' Attribute is expected for Momentum_Op. ");
  bool use_nesterov = attributes.at("use_nesterov").dyn_cast<pir::BoolAttribute>().data();

  IR_ENFORCE(
      attributes.find("regularization_method") != attributes.end(),
          "'regularization_method' Attribute is expected for Momentum_Op. ");
  std::string regularization_method = attributes.at("regularization_method").dyn_cast<pir::StrAttribute>().AsString();

  IR_ENFORCE(
      attributes.find("regularization_coeff") != attributes.end(),
          "'regularization_coeff' Attribute is expected for Momentum_Op. ");
  float regularization_coeff = attributes.at("regularization_coeff").dyn_cast<pir::FloatAttribute>().data();

  IR_ENFORCE(
      attributes.find("multi_precision") != attributes.end(),
          "'multi_precision' Attribute is expected for Momentum_Op. ");
  bool multi_precision = attributes.at("multi_precision").dyn_cast<pir::BoolAttribute>().data();

  IR_ENFORCE(
      attributes.find("rescale_grad") != attributes.end(),
          "'rescale_grad' Attribute is expected for Momentum_Op. ");
  float rescale_grad = attributes.at("rescale_grad").dyn_cast<pir::FloatAttribute>().data();


  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {param_, grad_, velocity_, learning_rate_, master_param_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_mu = pir::FloatAttribute::get(pir::IrContext::Instance(), mu);
  argument.AddAttribute("mu", attr_mu);
  pir::Attribute attr_use_nesterov = pir::BoolAttribute::get(pir::IrContext::Instance(), use_nesterov);
  argument.AddAttribute("use_nesterov", attr_use_nesterov);
  pir::Attribute attr_regularization_method = pir::StrAttribute::get(pir::IrContext::Instance(), regularization_method);
  argument.AddAttribute("regularization_method", attr_regularization_method);
  pir::Attribute attr_regularization_coeff = pir::FloatAttribute::get(pir::IrContext::Instance(), regularization_coeff);
  argument.AddAttribute("regularization_coeff", attr_regularization_coeff);
  pir::Attribute attr_multi_precision = pir::BoolAttribute::get(pir::IrContext::Instance(), multi_precision);
  argument.AddAttribute("multi_precision", attr_multi_precision);
  pir::Attribute attr_rescale_grad = pir::FloatAttribute::get(pir::IrContext::Instance(), rescale_grad);
  argument.AddAttribute("rescale_grad", attr_rescale_grad);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType param = param_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)param;
  paddle::dialect::DenseTensorType grad = grad_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)grad;
  paddle::dialect::DenseTensorType velocity = velocity_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)velocity;
  paddle::dialect::DenseTensorType learning_rate = learning_rate_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)learning_rate;

  VLOG(4) << "Builder construction  dense_param";
  paddle::dialect::IrTensor ir_tensor_param(paddle::dialect::TransToPhiDataType(param.dtype()),
                                                      param.dims(),
                                                      param.data_layout(),
                                                      param.lod(),
                                                      param.offset());
  VLOG(4) << "Builder construction  meta_param";
  paddle::dialect::IrMetaTensor meta_param(&ir_tensor_param);

  VLOG(4) << "Builder construction  dense_grad";
  paddle::dialect::IrTensor ir_tensor_grad(paddle::dialect::TransToPhiDataType(grad.dtype()),
                                                      grad.dims(),
                                                      grad.data_layout(),
                                                      grad.lod(),
                                                      grad.offset());
  VLOG(4) << "Builder construction  meta_grad";
  paddle::dialect::IrMetaTensor meta_grad(&ir_tensor_grad);

  VLOG(4) << "Builder construction  dense_velocity";
  paddle::dialect::IrTensor ir_tensor_velocity(paddle::dialect::TransToPhiDataType(velocity.dtype()),
                                                      velocity.dims(),
                                                      velocity.data_layout(),
                                                      velocity.lod(),
                                                      velocity.offset());
  VLOG(4) << "Builder construction  meta_velocity";
  paddle::dialect::IrMetaTensor meta_velocity(&ir_tensor_velocity);

  VLOG(4) << "Builder construction  dense_learning_rate";
  paddle::dialect::IrTensor ir_tensor_learning_rate(paddle::dialect::TransToPhiDataType(learning_rate.dtype()),
                                                      learning_rate.dims(),
                                                      learning_rate.data_layout(),
                                                      learning_rate.lod(),
                                                      learning_rate.offset());
  VLOG(4) << "Builder construction  meta_learning_rate";
  paddle::dialect::IrMetaTensor meta_learning_rate(&ir_tensor_learning_rate);

  paddle::dialect::IrMetaTensor meta_master_param;
  paddle::dialect::IrTensor ir_tensor_master_param;
  if (master_param_.impl() != nullptr) {
    paddle::dialect::DenseTensorType master_param = master_param_.type().dyn_cast<paddle::dialect::DenseTensorType>();
    VLOG(4) << "Builder construction  dense_master_param";
    ir_tensor_master_param = paddle::dialect::IrTensor(paddle::dialect::TransToPhiDataType(master_param.dtype()),
                                                        master_param.dims(),
                                                        master_param.data_layout(),
                                                        master_param.lod(),
                                                        master_param.offset());
    VLOG(4) << "Builder construction  meta_master_param";
    meta_master_param = paddle::dialect::IrMetaTensor(&ir_tensor_master_param);
  }

  paddle::dialect::IrTensor dense_param_out;
  paddle::dialect::IrMetaTensor meta_param_out(&dense_param_out);
  paddle::dialect::IrTensor dense_velocity_out;
  paddle::dialect::IrMetaTensor meta_velocity_out(&dense_velocity_out);
  paddle::dialect::IrTensor dense_master_param_out;
  paddle::dialect::IrMetaTensor meta_master_param_out(&dense_master_param_out);

  phi::MomentumInferMeta(meta_param, meta_grad, meta_velocity, meta_learning_rate, meta_master_param, mu, use_nesterov, regularization_method, regularization_coeff, multi_precision, rescale_grad, &meta_param_out, &meta_velocity_out, &meta_master_param_out);

  std::vector<pir::Type> argument_outputs;
  pir::Type param_out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_param_out.dtype()), dense_param_out.dims(), dense_param_out.layout(), dense_param_out.lod(), dense_param_out.offset());
  argument_outputs.push_back(param_out_dense_tensor_type);

  pir::Type velocity_out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_velocity_out.dtype()), dense_velocity_out.dims(), dense_velocity_out.layout(), dense_velocity_out.lod(), dense_velocity_out.offset());
  argument_outputs.push_back(velocity_out_dense_tensor_type);

  if (master_param_.impl() != nullptr) {
    pir::Type master_param_out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_master_param_out.dtype()), dense_master_param_out.dims(), dense_master_param_out.layout(), dense_master_param_out.lod(), dense_master_param_out.offset());
    argument_outputs.push_back(master_param_out_dense_tensor_type);
  } else {
    pir::Type master_param_out_type;
    argument_outputs.push_back(master_param_out_type);
  }

  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void Momentum_Op::VerifySig() {
  VLOG(4) << "Start Verifying inputs, outputs and attributes for: Momentum_Op.";
  VLOG(4) << "Verifying inputs:";
  {
  auto input_size = num_operands();
  IR_ENFORCE(input_size == 5u,
                    "The size %d of inputs must be equal to 5.", input_size);
  IR_ENFORCE((*this)->operand_source(0).type().isa<paddle::dialect::DenseTensorType>(),
                  "Type validation failed for the 0th input, got %s.", (*this)->operand_source(0).type());
  IR_ENFORCE((*this)->operand_source(1).type().isa<paddle::dialect::DenseTensorType>(),
                  "Type validation failed for the 1th input, got %s.", (*this)->operand_source(1).type());
  IR_ENFORCE((*this)->operand_source(2).type().isa<paddle::dialect::DenseTensorType>(),
                  "Type validation failed for the 2th input, got %s.", (*this)->operand_source(2).type());
  IR_ENFORCE((*this)->operand_source(3).type().isa<paddle::dialect::DenseTensorType>(),
                  "Type validation failed for the 3th input, got %s.", (*this)->operand_source(3).type());
  if (auto val = (*this)->operand(4)) {
    IR_ENFORCE(val.type().isa<paddle::dialect::DenseTensorType>(),
                   "Type validation failed for the 4th input, got %s.", (*this)->operand_source(4).type());
  }
  }
  VLOG(4) << "Verifying attributes:";
  {
  auto& attributes = this->attributes();
  IR_ENFORCE(attributes.count("mu")>0,
                 "mu does not exist.");
  IR_ENFORCE(attributes.at("mu").isa<pir::FloatAttribute>(),
                 "Type of attribute: mu is not pir::FloatAttribute.");

  IR_ENFORCE(attributes.count("use_nesterov")>0,
                 "use_nesterov does not exist.");
  IR_ENFORCE(attributes.at("use_nesterov").isa<pir::BoolAttribute>(),
                 "Type of attribute: use_nesterov is not pir::BoolAttribute.");

  IR_ENFORCE(attributes.count("regularization_method")>0,
                 "regularization_method does not exist.");
  IR_ENFORCE(attributes.at("regularization_method").isa<pir::StrAttribute>(),
                 "Type of attribute: regularization_method is not pir::StrAttribute.");

  IR_ENFORCE(attributes.count("regularization_coeff")>0,
                 "regularization_coeff does not exist.");
  IR_ENFORCE(attributes.at("regularization_coeff").isa<pir::FloatAttribute>(),
                 "Type of attribute: regularization_coeff is not pir::FloatAttribute.");

  IR_ENFORCE(attributes.count("multi_precision")>0,
                 "multi_precision does not exist.");
  IR_ENFORCE(attributes.at("multi_precision").isa<pir::BoolAttribute>(),
                 "Type of attribute: multi_precision is not pir::BoolAttribute.");

  IR_ENFORCE(attributes.count("rescale_grad")>0,
                 "rescale_grad does not exist.");
  IR_ENFORCE(attributes.at("rescale_grad").isa<pir::FloatAttribute>(),
                 "Type of attribute: rescale_grad is not pir::FloatAttribute.");

  }
  VLOG(4) << "Verifying outputs:";
  {
  auto output_size = num_results();
  IR_ENFORCE(output_size == 3u,
                    "The size %d of outputs must be equal to 3.", output_size);
  IR_ENFORCE((*this)->result(0).type().isa<paddle::dialect::DenseTensorType>(),
                 "Type validation failed for the 0th output.");
  IR_ENFORCE((*this)->result(1).type().isa<paddle::dialect::DenseTensorType>(),
                 "Type validation failed for the 1th output.");
  if (auto output_2_type = (*this)->result(2).type()) {
    IR_ENFORCE(output_2_type.isa<paddle::dialect::DenseTensorType>(),
                   "Type validation failed for the 2th output.");
  }
  }
  VLOG(4) << "End Verifying for: Momentum_Op.";
}

void Momentum_Op::InferMeta( phi::InferMetaContext *infer_meta ) {
  auto fn = PD_INFER_META(phi::MomentumInferMeta);
  fn(infer_meta);
}

phi::DataType Momentum_Op::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: Momentum_Op";
  


  return expected_kernel_dtype;
}

const char *MomentumDenseParamSparseGrad_Op::attributes_name[6] = { "mu", "use_nesterov", "regularization_method", "regularization_coeff", "multi_precision", "rescale_grad" };

OpInfoTuple MomentumDenseParamSparseGrad_Op::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("param", "paddle::dialect::DenseTensorType", false, false, false, false), paddle::dialect::OpInputInfo("grad", "paddle::dialect::SelectedRowsType", false, false, false, false), paddle::dialect::OpInputInfo("velocity", "paddle::dialect::DenseTensorType", false, false, false, false), paddle::dialect::OpInputInfo("learning_rate", "paddle::dialect::DenseTensorType", false, false, false, false), paddle::dialect::OpInputInfo("master_param", "paddle::dialect::DenseTensorType", true, false, false, false) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = { paddle::dialect::OpAttributeInfo("mu", "pir::FloatAttribute", ""), paddle::dialect::OpAttributeInfo("use_nesterov", "pir::BoolAttribute", ""), paddle::dialect::OpAttributeInfo("regularization_method", "pir::StrAttribute", ""), paddle::dialect::OpAttributeInfo("regularization_coeff", "pir::FloatAttribute", ""), paddle::dialect::OpAttributeInfo("multi_precision", "pir::BoolAttribute", ""), paddle::dialect::OpAttributeInfo("rescale_grad", "pir::FloatAttribute", "") };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("param_out", "paddle::dialect::DenseTensorType", false, false), paddle::dialect::OpOutputInfo("velocity_out", "paddle::dialect::DenseTensorType", false, false), paddle::dialect::OpOutputInfo("master_param_out", "paddle::dialect::DenseTensorType", true, false) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("MomentumInferMeta", {"param", "grad", "velocity", "learning_rate", "master_param", "mu", "use_nesterov", "regularization_method", "regularization_coeff", "multi_precision", "rescale_grad"}, "momentum_dense_param_sparse_grad", {"param", "grad", "velocity", "learning_rate", "master_param", "mu", "use_nesterov", "regularization_method", "regularization_coeff", "multi_precision", "rescale_grad"}, {"param"}, {}, {{"param_out", "param"},{"velocity_out", "velocity"},{"master_param_out", "master_param"}}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "momentum_");
}

void MomentumDenseParamSparseGrad_Op::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value param_, pir::Value grad_, pir::Value velocity_, pir::Value learning_rate_, pir::Value master_param_, float mu, bool use_nesterov, const std::string& regularization_method, float regularization_coeff, bool multi_precision, float rescale_grad) {
  VLOG(4) << "Start build MomentumDenseParamSparseGrad_Op";



  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {param_, grad_, velocity_, learning_rate_, master_param_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_mu = pir::FloatAttribute::get(pir::IrContext::Instance(), mu);
  argument.AddAttribute("mu", attr_mu);
  pir::Attribute attr_use_nesterov = pir::BoolAttribute::get(pir::IrContext::Instance(), use_nesterov);
  argument.AddAttribute("use_nesterov", attr_use_nesterov);
  pir::Attribute attr_regularization_method = pir::StrAttribute::get(pir::IrContext::Instance(), regularization_method);
  argument.AddAttribute("regularization_method", attr_regularization_method);
  pir::Attribute attr_regularization_coeff = pir::FloatAttribute::get(pir::IrContext::Instance(), regularization_coeff);
  argument.AddAttribute("regularization_coeff", attr_regularization_coeff);
  pir::Attribute attr_multi_precision = pir::BoolAttribute::get(pir::IrContext::Instance(), multi_precision);
  argument.AddAttribute("multi_precision", attr_multi_precision);
  pir::Attribute attr_rescale_grad = pir::FloatAttribute::get(pir::IrContext::Instance(), rescale_grad);
  argument.AddAttribute("rescale_grad", attr_rescale_grad);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType param = param_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)param;
  paddle::dialect::SelectedRowsType grad = grad_.type().dyn_cast<paddle::dialect::SelectedRowsType>(); (void)grad;
  paddle::dialect::DenseTensorType velocity = velocity_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)velocity;
  paddle::dialect::DenseTensorType learning_rate = learning_rate_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)learning_rate;

  VLOG(4) << "Builder construction  dense_param";
  paddle::dialect::IrTensor ir_tensor_param(paddle::dialect::TransToPhiDataType(param.dtype()),
                                                      param.dims(),
                                                      param.data_layout(),
                                                      param.lod(),
                                                      param.offset());
  VLOG(4) << "Builder construction  meta_param";
  paddle::dialect::IrMetaTensor meta_param(&ir_tensor_param);

  VLOG(4) << "Builder construction  dense_grad";
  paddle::dialect::IrTensor ir_tensor_grad(paddle::dialect::TransToPhiDataType(grad.dtype()),
                                                      grad.dims(),
                                                      grad.data_layout(),
                                                      grad.lod(),
                                                      grad.offset());
  VLOG(4) << "Builder construction  meta_grad";
  paddle::dialect::IrMetaTensor meta_grad(&ir_tensor_grad);

  VLOG(4) << "Builder construction  dense_velocity";
  paddle::dialect::IrTensor ir_tensor_velocity(paddle::dialect::TransToPhiDataType(velocity.dtype()),
                                                      velocity.dims(),
                                                      velocity.data_layout(),
                                                      velocity.lod(),
                                                      velocity.offset());
  VLOG(4) << "Builder construction  meta_velocity";
  paddle::dialect::IrMetaTensor meta_velocity(&ir_tensor_velocity);

  VLOG(4) << "Builder construction  dense_learning_rate";
  paddle::dialect::IrTensor ir_tensor_learning_rate(paddle::dialect::TransToPhiDataType(learning_rate.dtype()),
                                                      learning_rate.dims(),
                                                      learning_rate.data_layout(),
                                                      learning_rate.lod(),
                                                      learning_rate.offset());
  VLOG(4) << "Builder construction  meta_learning_rate";
  paddle::dialect::IrMetaTensor meta_learning_rate(&ir_tensor_learning_rate);

  paddle::dialect::IrMetaTensor meta_master_param;
  paddle::dialect::IrTensor ir_tensor_master_param;
  if (master_param_.impl() != nullptr) {
    paddle::dialect::DenseTensorType master_param = master_param_.type().dyn_cast<paddle::dialect::DenseTensorType>();
    VLOG(4) << "Builder construction  dense_master_param";
    ir_tensor_master_param = paddle::dialect::IrTensor(paddle::dialect::TransToPhiDataType(master_param.dtype()),
                                                        master_param.dims(),
                                                        master_param.data_layout(),
                                                        master_param.lod(),
                                                        master_param.offset());
    VLOG(4) << "Builder construction  meta_master_param";
    meta_master_param = paddle::dialect::IrMetaTensor(&ir_tensor_master_param);
  }

  paddle::dialect::IrTensor dense_param_out;
  paddle::dialect::IrMetaTensor meta_param_out(&dense_param_out);
  paddle::dialect::IrTensor dense_velocity_out;
  paddle::dialect::IrMetaTensor meta_velocity_out(&dense_velocity_out);
  paddle::dialect::IrTensor dense_master_param_out;
  paddle::dialect::IrMetaTensor meta_master_param_out(&dense_master_param_out);

  phi::MomentumInferMeta(meta_param, meta_grad, meta_velocity, meta_learning_rate, meta_master_param, mu, use_nesterov, regularization_method, regularization_coeff, multi_precision, rescale_grad, &meta_param_out, &meta_velocity_out, &meta_master_param_out);

  std::vector<pir::Type> argument_outputs;
  pir::Type param_out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_param_out.dtype()), dense_param_out.dims(), dense_param_out.layout(), dense_param_out.lod(), dense_param_out.offset());
  argument_outputs.push_back(param_out_dense_tensor_type);

  pir::Type velocity_out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_velocity_out.dtype()), dense_velocity_out.dims(), dense_velocity_out.layout(), dense_velocity_out.lod(), dense_velocity_out.offset());
  argument_outputs.push_back(velocity_out_dense_tensor_type);

  if (master_param_.impl() != nullptr) {
    pir::Type master_param_out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_master_param_out.dtype()), dense_master_param_out.dims(), dense_master_param_out.layout(), dense_master_param_out.lod(), dense_master_param_out.offset());
    argument_outputs.push_back(master_param_out_dense_tensor_type);
  } else {
    pir::Type master_param_out_type;
    argument_outputs.push_back(master_param_out_type);
  }

  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void MomentumDenseParamSparseGrad_Op::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value param_, pir::Value grad_, pir::Value velocity_, pir::Value learning_rate_, pir::Value master_param_, pir::AttributeMap attributes) {
  VLOG(4) << "Start build MomentumDenseParamSparseGrad_Op";


  IR_ENFORCE(
      attributes.find("mu") != attributes.end(),
          "'mu' Attribute is expected for MomentumDenseParamSparseGrad_Op. ");
  float mu = attributes.at("mu").dyn_cast<pir::FloatAttribute>().data();

  IR_ENFORCE(
      attributes.find("use_nesterov") != attributes.end(),
          "'use_nesterov' Attribute is expected for MomentumDenseParamSparseGrad_Op. ");
  bool use_nesterov = attributes.at("use_nesterov").dyn_cast<pir::BoolAttribute>().data();

  IR_ENFORCE(
      attributes.find("regularization_method") != attributes.end(),
          "'regularization_method' Attribute is expected for MomentumDenseParamSparseGrad_Op. ");
  std::string regularization_method = attributes.at("regularization_method").dyn_cast<pir::StrAttribute>().AsString();

  IR_ENFORCE(
      attributes.find("regularization_coeff") != attributes.end(),
          "'regularization_coeff' Attribute is expected for MomentumDenseParamSparseGrad_Op. ");
  float regularization_coeff = attributes.at("regularization_coeff").dyn_cast<pir::FloatAttribute>().data();

  IR_ENFORCE(
      attributes.find("multi_precision") != attributes.end(),
          "'multi_precision' Attribute is expected for MomentumDenseParamSparseGrad_Op. ");
  bool multi_precision = attributes.at("multi_precision").dyn_cast<pir::BoolAttribute>().data();

  IR_ENFORCE(
      attributes.find("rescale_grad") != attributes.end(),
          "'rescale_grad' Attribute is expected for MomentumDenseParamSparseGrad_Op. ");
  float rescale_grad = attributes.at("rescale_grad").dyn_cast<pir::FloatAttribute>().data();


  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {param_, grad_, velocity_, learning_rate_, master_param_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_mu = pir::FloatAttribute::get(pir::IrContext::Instance(), mu);
  argument.AddAttribute("mu", attr_mu);
  pir::Attribute attr_use_nesterov = pir::BoolAttribute::get(pir::IrContext::Instance(), use_nesterov);
  argument.AddAttribute("use_nesterov", attr_use_nesterov);
  pir::Attribute attr_regularization_method = pir::StrAttribute::get(pir::IrContext::Instance(), regularization_method);
  argument.AddAttribute("regularization_method", attr_regularization_method);
  pir::Attribute attr_regularization_coeff = pir::FloatAttribute::get(pir::IrContext::Instance(), regularization_coeff);
  argument.AddAttribute("regularization_coeff", attr_regularization_coeff);
  pir::Attribute attr_multi_precision = pir::BoolAttribute::get(pir::IrContext::Instance(), multi_precision);
  argument.AddAttribute("multi_precision", attr_multi_precision);
  pir::Attribute attr_rescale_grad = pir::FloatAttribute::get(pir::IrContext::Instance(), rescale_grad);
  argument.AddAttribute("rescale_grad", attr_rescale_grad);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType param = param_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)param;
  paddle::dialect::SelectedRowsType grad = grad_.type().dyn_cast<paddle::dialect::SelectedRowsType>(); (void)grad;
  paddle::dialect::DenseTensorType velocity = velocity_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)velocity;
  paddle::dialect::DenseTensorType learning_rate = learning_rate_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)learning_rate;

  VLOG(4) << "Builder construction  dense_param";
  paddle::dialect::IrTensor ir_tensor_param(paddle::dialect::TransToPhiDataType(param.dtype()),
                                                      param.dims(),
                                                      param.data_layout(),
                                                      param.lod(),
                                                      param.offset());
  VLOG(4) << "Builder construction  meta_param";
  paddle::dialect::IrMetaTensor meta_param(&ir_tensor_param);

  VLOG(4) << "Builder construction  dense_grad";
  paddle::dialect::IrTensor ir_tensor_grad(paddle::dialect::TransToPhiDataType(grad.dtype()),
                                                      grad.dims(),
                                                      grad.data_layout(),
                                                      grad.lod(),
                                                      grad.offset());
  VLOG(4) << "Builder construction  meta_grad";
  paddle::dialect::IrMetaTensor meta_grad(&ir_tensor_grad);

  VLOG(4) << "Builder construction  dense_velocity";
  paddle::dialect::IrTensor ir_tensor_velocity(paddle::dialect::TransToPhiDataType(velocity.dtype()),
                                                      velocity.dims(),
                                                      velocity.data_layout(),
                                                      velocity.lod(),
                                                      velocity.offset());
  VLOG(4) << "Builder construction  meta_velocity";
  paddle::dialect::IrMetaTensor meta_velocity(&ir_tensor_velocity);

  VLOG(4) << "Builder construction  dense_learning_rate";
  paddle::dialect::IrTensor ir_tensor_learning_rate(paddle::dialect::TransToPhiDataType(learning_rate.dtype()),
                                                      learning_rate.dims(),
                                                      learning_rate.data_layout(),
                                                      learning_rate.lod(),
                                                      learning_rate.offset());
  VLOG(4) << "Builder construction  meta_learning_rate";
  paddle::dialect::IrMetaTensor meta_learning_rate(&ir_tensor_learning_rate);

  paddle::dialect::IrMetaTensor meta_master_param;
  paddle::dialect::IrTensor ir_tensor_master_param;
  if (master_param_.impl() != nullptr) {
    paddle::dialect::DenseTensorType master_param = master_param_.type().dyn_cast<paddle::dialect::DenseTensorType>();
    VLOG(4) << "Builder construction  dense_master_param";
    ir_tensor_master_param = paddle::dialect::IrTensor(paddle::dialect::TransToPhiDataType(master_param.dtype()),
                                                        master_param.dims(),
                                                        master_param.data_layout(),
                                                        master_param.lod(),
                                                        master_param.offset());
    VLOG(4) << "Builder construction  meta_master_param";
    meta_master_param = paddle::dialect::IrMetaTensor(&ir_tensor_master_param);
  }

  paddle::dialect::IrTensor dense_param_out;
  paddle::dialect::IrMetaTensor meta_param_out(&dense_param_out);
  paddle::dialect::IrTensor dense_velocity_out;
  paddle::dialect::IrMetaTensor meta_velocity_out(&dense_velocity_out);
  paddle::dialect::IrTensor dense_master_param_out;
  paddle::dialect::IrMetaTensor meta_master_param_out(&dense_master_param_out);

  phi::MomentumInferMeta(meta_param, meta_grad, meta_velocity, meta_learning_rate, meta_master_param, mu, use_nesterov, regularization_method, regularization_coeff, multi_precision, rescale_grad, &meta_param_out, &meta_velocity_out, &meta_master_param_out);

  std::vector<pir::Type> argument_outputs;
  pir::Type param_out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_param_out.dtype()), dense_param_out.dims(), dense_param_out.layout(), dense_param_out.lod(), dense_param_out.offset());
  argument_outputs.push_back(param_out_dense_tensor_type);

  pir::Type velocity_out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_velocity_out.dtype()), dense_velocity_out.dims(), dense_velocity_out.layout(), dense_velocity_out.lod(), dense_velocity_out.offset());
  argument_outputs.push_back(velocity_out_dense_tensor_type);

  if (master_param_.impl() != nullptr) {
    pir::Type master_param_out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_master_param_out.dtype()), dense_master_param_out.dims(), dense_master_param_out.layout(), dense_master_param_out.lod(), dense_master_param_out.offset());
    argument_outputs.push_back(master_param_out_dense_tensor_type);
  } else {
    pir::Type master_param_out_type;
    argument_outputs.push_back(master_param_out_type);
  }

  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void MomentumDenseParamSparseGrad_Op::VerifySig() {}

void MomentumDenseParamSparseGrad_Op::InferMeta( phi::InferMetaContext *infer_meta ) {
  auto fn = PD_INFER_META(phi::MomentumInferMeta);
  fn(infer_meta);
}

phi::DataType MomentumDenseParamSparseGrad_Op::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: MomentumDenseParamSparseGrad_Op";
  


  return expected_kernel_dtype;
}

OpInfoTuple MultiDotOp::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("x", "pir::VectorType<paddle::dialect::DenseTensorType>", false, false, false, true) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = {  };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("out", "paddle::dialect::DenseTensorType", false, false) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("MultiDotInferMeta", {"x"}, "multi_dot", {"x"}, {}, {}, {}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "multi_dot");
}

void MultiDotOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_) {
  VLOG(4) << "Start build MultiDotOp";



  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";

  VLOG(4) << "Builder construction outputs";
  pir::VectorType x = x_.type().dyn_cast<pir::VectorType>(); (void)x;
  std::vector<paddle::dialect::IrTensor> vec_ir_tensor_x;
  for (size_t i=0; i < static_cast<size_t>(x.size()); i++) {
    vec_ir_tensor_x.push_back(paddle::dialect::IrTensor(paddle::dialect::TransToPhiDataType(x[i].dyn_cast<paddle::dialect::DenseTensorType>().dtype()),
                                                                     x[i].dyn_cast<paddle::dialect::DenseTensorType>().dims(),
                                                                     x[i].dyn_cast<paddle::dialect::DenseTensorType>().data_layout(),
                                                                     x[i].dyn_cast<paddle::dialect::DenseTensorType>().lod(),
                                                                     x[i].dyn_cast<paddle::dialect::DenseTensorType>().offset()));
  }
  std::vector<paddle::dialect::IrMetaTensor> vec_meta_x;
  for (size_t i=0; i < vec_ir_tensor_x.size(); i++) {
    vec_meta_x.push_back(paddle::dialect::IrMetaTensor(&vec_ir_tensor_x[i]));
  }

  std::vector<const phi::MetaTensor*> meta_x;
  for (size_t i=0; i < static_cast<size_t>(vec_meta_x.size()); i++) {
    meta_x.push_back(&vec_meta_x[i]);
  }
   paddle::dialect::IrTensor dense_out;
  paddle::dialect::IrMetaTensor meta_out(&dense_out);

  phi::MultiDotInferMeta(meta_x, &meta_out);

  std::vector<pir::Type> argument_outputs;
  pir::Type out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_out.dtype()), dense_out.dims(), dense_out.layout(), dense_out.lod(), dense_out.offset());
  argument_outputs.push_back(out_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void MultiDotOp::VerifySig() {
  VLOG(4) << "Start Verifying inputs, outputs and attributes for: MultiDotOp.";
  VLOG(4) << "Verifying inputs:";
  {
  auto input_size = num_operands();
  IR_ENFORCE(input_size == 1u,
                    "The size %d of inputs must be equal to 1.", input_size);
  if (auto vec_type = (*this)->operand_source(0).type().dyn_cast<pir::VectorType>()) {
      for (size_t i = 0; i < vec_type.size(); ++i) {
        IR_ENFORCE(vec_type[i].isa<paddle::dialect::DenseTensorType>(),
                       "Type validation failed for the 0th input, got %s.", (*this)->operand_source(0).type());
      }
  }
  else {
    IR_ENFORCE((*this)->operand_source(0).type().isa<paddle::dialect::DenseTensorType>(),
                   "Type validation failed for the 0th input, got %s.", (*this)->operand_source(0).type());
  }
  }
  VLOG(4) << "Verifying attributes:";
  {
  // Attributes num is 0, not need to check attributes type.
  }
  VLOG(4) << "Verifying outputs:";
  {
  auto output_size = num_results();
  IR_ENFORCE(output_size == 1u,
                    "The size %d of outputs must be equal to 1.", output_size);
  IR_ENFORCE((*this)->result(0).type().isa<paddle::dialect::DenseTensorType>(),
                 "Type validation failed for the 0th output.");
  }
  VLOG(4) << "End Verifying for: MultiDotOp.";
}

void MultiDotOp::InferMeta( phi::InferMetaContext *infer_meta ) {
  auto fn = PD_INFER_META(phi::MultiDotInferMeta);
  fn(infer_meta);
}

phi::DataType MultiDotOp::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: MultiDotOp";
  


  return expected_kernel_dtype;
}

const char *MulticlassNms3Op::attributes_name[7] = { "score_threshold", "nms_top_k", "keep_top_k", "nms_threshold", "normalized", "nms_eta", "background_label" };

OpInfoTuple MulticlassNms3Op::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("bboxes", "paddle::dialect::DenseTensorType", false, false, false, false), paddle::dialect::OpInputInfo("scores", "paddle::dialect::DenseTensorType", false, false, false, false), paddle::dialect::OpInputInfo("rois_num", "paddle::dialect::DenseTensorType", true, false, false, false) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = { paddle::dialect::OpAttributeInfo("score_threshold", "pir::FloatAttribute", ""), paddle::dialect::OpAttributeInfo("nms_top_k", "pir::Int32Attribute", ""), paddle::dialect::OpAttributeInfo("keep_top_k", "pir::Int32Attribute", ""), paddle::dialect::OpAttributeInfo("nms_threshold", "pir::FloatAttribute", ""), paddle::dialect::OpAttributeInfo("normalized", "pir::BoolAttribute", ""), paddle::dialect::OpAttributeInfo("nms_eta", "pir::FloatAttribute", ""), paddle::dialect::OpAttributeInfo("background_label", "pir::Int32Attribute", "") };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("out", "paddle::dialect::DenseTensorType", false, false), paddle::dialect::OpOutputInfo("index", "paddle::dialect::DenseTensorType", false, false), paddle::dialect::OpOutputInfo("nms_rois_num", "paddle::dialect::DenseTensorType", true, false) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("MultiClassNMSInferMeta", {"bboxes", "scores", "rois_num", "score_threshold", "nms_top_k", "keep_top_k", "nms_threshold", "normalized", "nms_eta", "background_label"}, "multiclass_nms3", {"bboxes", "scores", "rois_num", "score_threshold", "nms_top_k", "keep_top_k", "nms_threshold", "normalized", "nms_eta", "background_label"}, {"scores"}, {}, {}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "multiclass_nms3");
}

void MulticlassNms3Op::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value bboxes_, pir::Value scores_, pir::Value rois_num_, float score_threshold, int nms_top_k, int keep_top_k, float nms_threshold, bool normalized, float nms_eta, int background_label) {
  VLOG(4) << "Start build MulticlassNms3Op";



  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {bboxes_, scores_, rois_num_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_score_threshold = pir::FloatAttribute::get(pir::IrContext::Instance(), score_threshold);
  argument.AddAttribute("score_threshold", attr_score_threshold);
  pir::Attribute attr_nms_top_k = pir::Int32Attribute::get(pir::IrContext::Instance(), nms_top_k);
  argument.AddAttribute("nms_top_k", attr_nms_top_k);
  pir::Attribute attr_keep_top_k = pir::Int32Attribute::get(pir::IrContext::Instance(), keep_top_k);
  argument.AddAttribute("keep_top_k", attr_keep_top_k);
  pir::Attribute attr_nms_threshold = pir::FloatAttribute::get(pir::IrContext::Instance(), nms_threshold);
  argument.AddAttribute("nms_threshold", attr_nms_threshold);
  pir::Attribute attr_normalized = pir::BoolAttribute::get(pir::IrContext::Instance(), normalized);
  argument.AddAttribute("normalized", attr_normalized);
  pir::Attribute attr_nms_eta = pir::FloatAttribute::get(pir::IrContext::Instance(), nms_eta);
  argument.AddAttribute("nms_eta", attr_nms_eta);
  pir::Attribute attr_background_label = pir::Int32Attribute::get(pir::IrContext::Instance(), background_label);
  argument.AddAttribute("background_label", attr_background_label);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType bboxes = bboxes_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)bboxes;
  paddle::dialect::DenseTensorType scores = scores_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)scores;

  VLOG(4) << "Builder construction  dense_bboxes";
  paddle::dialect::IrTensor ir_tensor_bboxes(paddle::dialect::TransToPhiDataType(bboxes.dtype()),
                                                      bboxes.dims(),
                                                      bboxes.data_layout(),
                                                      bboxes.lod(),
                                                      bboxes.offset());
  VLOG(4) << "Builder construction  meta_bboxes";
  paddle::dialect::IrMetaTensor meta_bboxes(&ir_tensor_bboxes);

  VLOG(4) << "Builder construction  dense_scores";
  paddle::dialect::IrTensor ir_tensor_scores(paddle::dialect::TransToPhiDataType(scores.dtype()),
                                                      scores.dims(),
                                                      scores.data_layout(),
                                                      scores.lod(),
                                                      scores.offset());
  VLOG(4) << "Builder construction  meta_scores";
  paddle::dialect::IrMetaTensor meta_scores(&ir_tensor_scores);

  paddle::dialect::IrMetaTensor meta_rois_num;
  paddle::dialect::IrTensor ir_tensor_rois_num;
  if (rois_num_.impl() != nullptr) {
    paddle::dialect::DenseTensorType rois_num = rois_num_.type().dyn_cast<paddle::dialect::DenseTensorType>();
    VLOG(4) << "Builder construction  dense_rois_num";
    ir_tensor_rois_num = paddle::dialect::IrTensor(paddle::dialect::TransToPhiDataType(rois_num.dtype()),
                                                        rois_num.dims(),
                                                        rois_num.data_layout(),
                                                        rois_num.lod(),
                                                        rois_num.offset());
    VLOG(4) << "Builder construction  meta_rois_num";
    meta_rois_num = paddle::dialect::IrMetaTensor(&ir_tensor_rois_num);
  }

  paddle::dialect::IrTensor dense_out;
  paddle::dialect::IrMetaTensor meta_out(&dense_out);
  paddle::dialect::IrTensor dense_index;
  paddle::dialect::IrMetaTensor meta_index(&dense_index);
  paddle::dialect::IrTensor dense_nms_rois_num;
  paddle::dialect::IrMetaTensor meta_nms_rois_num(&dense_nms_rois_num);

  phi::MultiClassNMSInferMeta(meta_bboxes, meta_scores, meta_rois_num, score_threshold, nms_top_k, keep_top_k, nms_threshold, normalized, nms_eta, background_label, &meta_out, &meta_index, &meta_nms_rois_num);

  std::vector<pir::Type> argument_outputs;
  pir::Type out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_out.dtype()), dense_out.dims(), dense_out.layout(), dense_out.lod(), dense_out.offset());
  argument_outputs.push_back(out_dense_tensor_type);

  pir::Type index_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_index.dtype()), dense_index.dims(), dense_index.layout(), dense_index.lod(), dense_index.offset());
  argument_outputs.push_back(index_dense_tensor_type);

  pir::Type nms_rois_num_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_nms_rois_num.dtype()), dense_nms_rois_num.dims(), dense_nms_rois_num.layout(), dense_nms_rois_num.lod(), dense_nms_rois_num.offset());
  argument_outputs.push_back(nms_rois_num_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void MulticlassNms3Op::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value bboxes_, pir::Value scores_, pir::Value rois_num_, pir::AttributeMap attributes) {
  VLOG(4) << "Start build MulticlassNms3Op";


  IR_ENFORCE(
      attributes.find("score_threshold") != attributes.end(),
          "'score_threshold' Attribute is expected for MulticlassNms3Op. ");
  float score_threshold = attributes.at("score_threshold").dyn_cast<pir::FloatAttribute>().data();

  IR_ENFORCE(
      attributes.find("nms_top_k") != attributes.end(),
          "'nms_top_k' Attribute is expected for MulticlassNms3Op. ");
  int nms_top_k = attributes.at("nms_top_k").dyn_cast<pir::Int32Attribute>().data();

  IR_ENFORCE(
      attributes.find("keep_top_k") != attributes.end(),
          "'keep_top_k' Attribute is expected for MulticlassNms3Op. ");
  int keep_top_k = attributes.at("keep_top_k").dyn_cast<pir::Int32Attribute>().data();

  IR_ENFORCE(
      attributes.find("nms_threshold") != attributes.end(),
          "'nms_threshold' Attribute is expected for MulticlassNms3Op. ");
  float nms_threshold = attributes.at("nms_threshold").dyn_cast<pir::FloatAttribute>().data();

  IR_ENFORCE(
      attributes.find("normalized") != attributes.end(),
          "'normalized' Attribute is expected for MulticlassNms3Op. ");
  bool normalized = attributes.at("normalized").dyn_cast<pir::BoolAttribute>().data();

  IR_ENFORCE(
      attributes.find("nms_eta") != attributes.end(),
          "'nms_eta' Attribute is expected for MulticlassNms3Op. ");
  float nms_eta = attributes.at("nms_eta").dyn_cast<pir::FloatAttribute>().data();

  IR_ENFORCE(
      attributes.find("background_label") != attributes.end(),
          "'background_label' Attribute is expected for MulticlassNms3Op. ");
  int background_label = attributes.at("background_label").dyn_cast<pir::Int32Attribute>().data();


  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {bboxes_, scores_, rois_num_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_score_threshold = pir::FloatAttribute::get(pir::IrContext::Instance(), score_threshold);
  argument.AddAttribute("score_threshold", attr_score_threshold);
  pir::Attribute attr_nms_top_k = pir::Int32Attribute::get(pir::IrContext::Instance(), nms_top_k);
  argument.AddAttribute("nms_top_k", attr_nms_top_k);
  pir::Attribute attr_keep_top_k = pir::Int32Attribute::get(pir::IrContext::Instance(), keep_top_k);
  argument.AddAttribute("keep_top_k", attr_keep_top_k);
  pir::Attribute attr_nms_threshold = pir::FloatAttribute::get(pir::IrContext::Instance(), nms_threshold);
  argument.AddAttribute("nms_threshold", attr_nms_threshold);
  pir::Attribute attr_normalized = pir::BoolAttribute::get(pir::IrContext::Instance(), normalized);
  argument.AddAttribute("normalized", attr_normalized);
  pir::Attribute attr_nms_eta = pir::FloatAttribute::get(pir::IrContext::Instance(), nms_eta);
  argument.AddAttribute("nms_eta", attr_nms_eta);
  pir::Attribute attr_background_label = pir::Int32Attribute::get(pir::IrContext::Instance(), background_label);
  argument.AddAttribute("background_label", attr_background_label);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType bboxes = bboxes_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)bboxes;
  paddle::dialect::DenseTensorType scores = scores_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)scores;

  VLOG(4) << "Builder construction  dense_bboxes";
  paddle::dialect::IrTensor ir_tensor_bboxes(paddle::dialect::TransToPhiDataType(bboxes.dtype()),
                                                      bboxes.dims(),
                                                      bboxes.data_layout(),
                                                      bboxes.lod(),
                                                      bboxes.offset());
  VLOG(4) << "Builder construction  meta_bboxes";
  paddle::dialect::IrMetaTensor meta_bboxes(&ir_tensor_bboxes);

  VLOG(4) << "Builder construction  dense_scores";
  paddle::dialect::IrTensor ir_tensor_scores(paddle::dialect::TransToPhiDataType(scores.dtype()),
                                                      scores.dims(),
                                                      scores.data_layout(),
                                                      scores.lod(),
                                                      scores.offset());
  VLOG(4) << "Builder construction  meta_scores";
  paddle::dialect::IrMetaTensor meta_scores(&ir_tensor_scores);

  paddle::dialect::IrMetaTensor meta_rois_num;
  paddle::dialect::IrTensor ir_tensor_rois_num;
  if (rois_num_.impl() != nullptr) {
    paddle::dialect::DenseTensorType rois_num = rois_num_.type().dyn_cast<paddle::dialect::DenseTensorType>();
    VLOG(4) << "Builder construction  dense_rois_num";
    ir_tensor_rois_num = paddle::dialect::IrTensor(paddle::dialect::TransToPhiDataType(rois_num.dtype()),
                                                        rois_num.dims(),
                                                        rois_num.data_layout(),
                                                        rois_num.lod(),
                                                        rois_num.offset());
    VLOG(4) << "Builder construction  meta_rois_num";
    meta_rois_num = paddle::dialect::IrMetaTensor(&ir_tensor_rois_num);
  }

  paddle::dialect::IrTensor dense_out;
  paddle::dialect::IrMetaTensor meta_out(&dense_out);
  paddle::dialect::IrTensor dense_index;
  paddle::dialect::IrMetaTensor meta_index(&dense_index);
  paddle::dialect::IrTensor dense_nms_rois_num;
  paddle::dialect::IrMetaTensor meta_nms_rois_num(&dense_nms_rois_num);

  phi::MultiClassNMSInferMeta(meta_bboxes, meta_scores, meta_rois_num, score_threshold, nms_top_k, keep_top_k, nms_threshold, normalized, nms_eta, background_label, &meta_out, &meta_index, &meta_nms_rois_num);

  std::vector<pir::Type> argument_outputs;
  pir::Type out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_out.dtype()), dense_out.dims(), dense_out.layout(), dense_out.lod(), dense_out.offset());
  argument_outputs.push_back(out_dense_tensor_type);

  pir::Type index_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_index.dtype()), dense_index.dims(), dense_index.layout(), dense_index.lod(), dense_index.offset());
  argument_outputs.push_back(index_dense_tensor_type);

  pir::Type nms_rois_num_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_nms_rois_num.dtype()), dense_nms_rois_num.dims(), dense_nms_rois_num.layout(), dense_nms_rois_num.lod(), dense_nms_rois_num.offset());
  argument_outputs.push_back(nms_rois_num_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void MulticlassNms3Op::VerifySig() {
  VLOG(4) << "Start Verifying inputs, outputs and attributes for: MulticlassNms3Op.";
  VLOG(4) << "Verifying inputs:";
  {
  auto input_size = num_operands();
  IR_ENFORCE(input_size == 3u,
                    "The size %d of inputs must be equal to 3.", input_size);
  IR_ENFORCE((*this)->operand_source(0).type().isa<paddle::dialect::DenseTensorType>(),
                  "Type validation failed for the 0th input, got %s.", (*this)->operand_source(0).type());
  IR_ENFORCE((*this)->operand_source(1).type().isa<paddle::dialect::DenseTensorType>(),
                  "Type validation failed for the 1th input, got %s.", (*this)->operand_source(1).type());
  if (auto val = (*this)->operand(2)) {
    IR_ENFORCE(val.type().isa<paddle::dialect::DenseTensorType>(),
                   "Type validation failed for the 2th input, got %s.", (*this)->operand_source(2).type());
  }
  }
  VLOG(4) << "Verifying attributes:";
  {
  auto& attributes = this->attributes();
  IR_ENFORCE(attributes.count("score_threshold")>0,
                 "score_threshold does not exist.");
  IR_ENFORCE(attributes.at("score_threshold").isa<pir::FloatAttribute>(),
                 "Type of attribute: score_threshold is not pir::FloatAttribute.");

  IR_ENFORCE(attributes.count("nms_top_k")>0,
                 "nms_top_k does not exist.");
  IR_ENFORCE(attributes.at("nms_top_k").isa<pir::Int32Attribute>(),
                 "Type of attribute: nms_top_k is not pir::Int32Attribute.");

  IR_ENFORCE(attributes.count("keep_top_k")>0,
                 "keep_top_k does not exist.");
  IR_ENFORCE(attributes.at("keep_top_k").isa<pir::Int32Attribute>(),
                 "Type of attribute: keep_top_k is not pir::Int32Attribute.");

  IR_ENFORCE(attributes.count("nms_threshold")>0,
                 "nms_threshold does not exist.");
  IR_ENFORCE(attributes.at("nms_threshold").isa<pir::FloatAttribute>(),
                 "Type of attribute: nms_threshold is not pir::FloatAttribute.");

  IR_ENFORCE(attributes.count("normalized")>0,
                 "normalized does not exist.");
  IR_ENFORCE(attributes.at("normalized").isa<pir::BoolAttribute>(),
                 "Type of attribute: normalized is not pir::BoolAttribute.");

  IR_ENFORCE(attributes.count("nms_eta")>0,
                 "nms_eta does not exist.");
  IR_ENFORCE(attributes.at("nms_eta").isa<pir::FloatAttribute>(),
                 "Type of attribute: nms_eta is not pir::FloatAttribute.");

  IR_ENFORCE(attributes.count("background_label")>0,
                 "background_label does not exist.");
  IR_ENFORCE(attributes.at("background_label").isa<pir::Int32Attribute>(),
                 "Type of attribute: background_label is not pir::Int32Attribute.");

  }
  VLOG(4) << "Verifying outputs:";
  {
  auto output_size = num_results();
  IR_ENFORCE(output_size == 3u,
                    "The size %d of outputs must be equal to 3.", output_size);
  IR_ENFORCE((*this)->result(0).type().isa<paddle::dialect::DenseTensorType>(),
                 "Type validation failed for the 0th output.");
  IR_ENFORCE((*this)->result(1).type().isa<paddle::dialect::DenseTensorType>(),
                 "Type validation failed for the 1th output.");
  if (auto output_2_type = (*this)->result(2).type()) {
    IR_ENFORCE(output_2_type.isa<paddle::dialect::DenseTensorType>(),
                   "Type validation failed for the 2th output.");
  }
  }
  VLOG(4) << "End Verifying for: MulticlassNms3Op.";
}

void MulticlassNms3Op::InferMeta( phi::InferMetaContext *infer_meta ) {
  auto fn = PD_INFER_META(phi::MultiClassNMSInferMeta);
  fn(infer_meta);
}

phi::DataType MulticlassNms3Op::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: MulticlassNms3Op";
  


  return expected_kernel_dtype;
}

const char *MultinomialOp::attributes_name[1] = { "replacement" };

OpInfoTuple MultinomialOp::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("x", "paddle::dialect::DenseTensorType", false, false, false, false), paddle::dialect::OpInputInfo("num_samples", "paddle::dialect::ScalarAttribute", false, false, true, false) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = { paddle::dialect::OpAttributeInfo("replacement", "pir::BoolAttribute", "") };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("out", "paddle::dialect::DenseTensorType", false, false) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("MultinomialInferMeta", {"x", "num_samples", "replacement"}, "multinomial", {"x", "num_samples", "replacement"}, {"x"}, {}, {}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "multinomial");
}

void MultinomialOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, int num_samples, bool replacement) {
  VLOG(4) << "Start build MultinomialOp";


  // Generate scalar mutable attribute: num_samples
  paddle::dialect::FullOp full_num_samples_op = builder.Build<paddle::dialect::FullOp>(std::vector<int64_t>{1}, num_samples, phi::DataType::INT32, phi::CPUPlace());
  pir::OpResult num_samples_ = full_num_samples_op->result(0);
    
  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_, num_samples_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_replacement = pir::BoolAttribute::get(pir::IrContext::Instance(), replacement);
  argument.AddAttribute("replacement", attr_replacement);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);
  paddle::dialect::IrTensor dense_out;
  paddle::dialect::IrMetaTensor meta_out(&dense_out);

  phi::MultinomialInferMeta(meta_x, num_samples, replacement, &meta_out);

  std::vector<pir::Type> argument_outputs;
  pir::Type out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_out.dtype()), dense_out.dims(), dense_out.layout(), dense_out.lod(), dense_out.offset());
  argument_outputs.push_back(out_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void MultinomialOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, pir::AttributeMap attributes) {
  VLOG(4) << "Start build MultinomialOp";


  IR_ENFORCE(
      attributes.find("num_samples") != attributes.end(),
          "'num_samples' Attribute is expected for MultinomialOp. ");
  int num_samples = attributes.at("num_samples").dyn_cast<pir::Int32Attribute>().data();

  IR_ENFORCE(
      attributes.find("replacement") != attributes.end(),
          "'replacement' Attribute is expected for MultinomialOp. ");
  bool replacement = attributes.at("replacement").dyn_cast<pir::BoolAttribute>().data();

  // Generate scalar mutable attribute: num_samples
  paddle::dialect::FullOp full_num_samples_op = builder.Build<paddle::dialect::FullOp>(std::vector<int64_t>{1}, num_samples, phi::DataType::INT32, phi::CPUPlace());
  pir::OpResult num_samples_ = full_num_samples_op->result(0);
    
  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_, num_samples_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_replacement = pir::BoolAttribute::get(pir::IrContext::Instance(), replacement);
  argument.AddAttribute("replacement", attr_replacement);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);
  paddle::dialect::IrTensor dense_out;
  paddle::dialect::IrMetaTensor meta_out(&dense_out);

  phi::MultinomialInferMeta(meta_x, num_samples, replacement, &meta_out);

  std::vector<pir::Type> argument_outputs;
  pir::Type out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_out.dtype()), dense_out.dims(), dense_out.layout(), dense_out.lod(), dense_out.offset());
  argument_outputs.push_back(out_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void MultinomialOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, pir::Value num_samples_, bool replacement) {
  VLOG(4) << "Start build MultinomialOp";



  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_, num_samples_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_replacement = pir::BoolAttribute::get(pir::IrContext::Instance(), replacement);
  argument.AddAttribute("replacement", attr_replacement);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;
  phi::Scalar num_samples;
  if (num_samples_.dyn_cast<pir::OpResult>() && num_samples_.dyn_cast<pir::OpResult>().owner()->isa<paddle::dialect::FullOp>()) {
    num_samples = std::move(phi::Scalar(num_samples_.dyn_cast<pir::OpResult>().owner()
                                  ->dyn_cast<paddle::dialect::FullOp>()
                                  .attribute("value")
                                  .dyn_cast<paddle::dialect::ScalarAttribute>()
                                  .data()
                                  .to<int>()));
  }
  else {
    num_samples = std::move(phi::Scalar(-1));
    num_samples.SetFromTensor(true);
  }


  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);
  paddle::dialect::IrTensor dense_out;
  paddle::dialect::IrMetaTensor meta_out(&dense_out);

  phi::MultinomialInferMeta(meta_x, num_samples, replacement, &meta_out);

  std::vector<pir::Type> argument_outputs;
  pir::Type out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_out.dtype()), dense_out.dims(), dense_out.layout(), dense_out.lod(), dense_out.offset());
  argument_outputs.push_back(out_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void MultinomialOp::VerifySig() {
  VLOG(4) << "Start Verifying inputs, outputs and attributes for: MultinomialOp.";
  VLOG(4) << "Verifying inputs:";
  {
  auto input_size = num_operands();
  IR_ENFORCE(input_size == 2u,
                    "The size %d of inputs must be equal to 2.", input_size);
  IR_ENFORCE((*this)->operand_source(0).type().isa<paddle::dialect::DenseTensorType>(),
                  "Type validation failed for the 0th input, got %s.", (*this)->operand_source(0).type());
  IR_ENFORCE((*this)->operand_source(1).type().isa<paddle::dialect::DenseTensorType>(),
                  "Type validation failed for the 1th input, got %s.", (*this)->operand_source(1).type());
  }
  VLOG(4) << "Verifying attributes:";
  {
  auto& attributes = this->attributes();
  IR_ENFORCE(attributes.count("replacement")>0,
                 "replacement does not exist.");
  IR_ENFORCE(attributes.at("replacement").isa<pir::BoolAttribute>(),
                 "Type of attribute: replacement is not pir::BoolAttribute.");

  }
  VLOG(4) << "Verifying outputs:";
  {
  auto output_size = num_results();
  IR_ENFORCE(output_size == 1u,
                    "The size %d of outputs must be equal to 1.", output_size);
  IR_ENFORCE((*this)->result(0).type().isa<paddle::dialect::DenseTensorType>(),
                 "Type validation failed for the 0th output.");
  }
  VLOG(4) << "End Verifying for: MultinomialOp.";
}

void MultinomialOp::InferMeta( phi::InferMetaContext *infer_meta ) {
  auto fn = PD_INFER_META(phi::MultinomialInferMeta);
  fn(infer_meta);
}

phi::DataType MultinomialOp::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: MultinomialOp";
  


  return expected_kernel_dtype;
}

OpInfoTuple MultiplexOp::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("inputs", "pir::VectorType<paddle::dialect::DenseTensorType>", false, false, false, true), paddle::dialect::OpInputInfo("index", "paddle::dialect::DenseTensorType", false, false, false, false) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = {  };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("out", "paddle::dialect::DenseTensorType", false, false) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("MultiplexInferMeta", {"inputs", "index"}, "multiplex", {"inputs", "index"}, {"inputs"}, {}, {}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "multiplex");
}

void MultiplexOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value inputs_, pir::Value index_) {
  VLOG(4) << "Start build MultiplexOp";



  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {inputs_, index_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";

  VLOG(4) << "Builder construction outputs";
  pir::VectorType inputs = inputs_.type().dyn_cast<pir::VectorType>(); (void)inputs;
  paddle::dialect::DenseTensorType index = index_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)index;
  std::vector<paddle::dialect::IrTensor> vec_ir_tensor_inputs;
  for (size_t i=0; i < static_cast<size_t>(inputs.size()); i++) {
    vec_ir_tensor_inputs.push_back(paddle::dialect::IrTensor(paddle::dialect::TransToPhiDataType(inputs[i].dyn_cast<paddle::dialect::DenseTensorType>().dtype()),
                                                                     inputs[i].dyn_cast<paddle::dialect::DenseTensorType>().dims(),
                                                                     inputs[i].dyn_cast<paddle::dialect::DenseTensorType>().data_layout(),
                                                                     inputs[i].dyn_cast<paddle::dialect::DenseTensorType>().lod(),
                                                                     inputs[i].dyn_cast<paddle::dialect::DenseTensorType>().offset()));
  }
  std::vector<paddle::dialect::IrMetaTensor> vec_meta_inputs;
  for (size_t i=0; i < vec_ir_tensor_inputs.size(); i++) {
    vec_meta_inputs.push_back(paddle::dialect::IrMetaTensor(&vec_ir_tensor_inputs[i]));
  }

  std::vector<const phi::MetaTensor*> meta_inputs;
  for (size_t i=0; i < static_cast<size_t>(vec_meta_inputs.size()); i++) {
    meta_inputs.push_back(&vec_meta_inputs[i]);
  }
 
  VLOG(4) << "Builder construction  dense_index";
  paddle::dialect::IrTensor ir_tensor_index(paddle::dialect::TransToPhiDataType(index.dtype()),
                                                      index.dims(),
                                                      index.data_layout(),
                                                      index.lod(),
                                                      index.offset());
  VLOG(4) << "Builder construction  meta_index";
  paddle::dialect::IrMetaTensor meta_index(&ir_tensor_index);
  paddle::dialect::IrTensor dense_out;
  paddle::dialect::IrMetaTensor meta_out(&dense_out);

  phi::MultiplexInferMeta(meta_inputs, meta_index, &meta_out);

  std::vector<pir::Type> argument_outputs;
  pir::Type out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_out.dtype()), dense_out.dims(), dense_out.layout(), dense_out.lod(), dense_out.offset());
  argument_outputs.push_back(out_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void MultiplexOp::VerifySig() {
  VLOG(4) << "Start Verifying inputs, outputs and attributes for: MultiplexOp.";
  VLOG(4) << "Verifying inputs:";
  {
  auto input_size = num_operands();
  IR_ENFORCE(input_size == 2u,
                    "The size %d of inputs must be equal to 2.", input_size);
  if (auto vec_type = (*this)->operand_source(0).type().dyn_cast<pir::VectorType>()) {
      for (size_t i = 0; i < vec_type.size(); ++i) {
        IR_ENFORCE(vec_type[i].isa<paddle::dialect::DenseTensorType>(),
                       "Type validation failed for the 0th input, got %s.", (*this)->operand_source(0).type());
      }
  }
  else {
    IR_ENFORCE((*this)->operand_source(0).type().isa<paddle::dialect::DenseTensorType>(),
                   "Type validation failed for the 0th input, got %s.", (*this)->operand_source(0).type());
  }
  IR_ENFORCE((*this)->operand_source(1).type().isa<paddle::dialect::DenseTensorType>(),
                  "Type validation failed for the 1th input, got %s.", (*this)->operand_source(1).type());
  }
  VLOG(4) << "Verifying attributes:";
  {
  // Attributes num is 0, not need to check attributes type.
  }
  VLOG(4) << "Verifying outputs:";
  {
  auto output_size = num_results();
  IR_ENFORCE(output_size == 1u,
                    "The size %d of outputs must be equal to 1.", output_size);
  IR_ENFORCE((*this)->result(0).type().isa<paddle::dialect::DenseTensorType>(),
                 "Type validation failed for the 0th output.");
  }
  VLOG(4) << "End Verifying for: MultiplexOp.";
}

void MultiplexOp::InferMeta( phi::InferMetaContext *infer_meta ) {
  auto fn = PD_INFER_META(phi::MultiplexInferMeta);
  fn(infer_meta);
}

phi::DataType MultiplexOp::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: MultiplexOp";
  

  // deal skip data transform
  if (var_name == "index"){
    return expected_kernel_dtype;
  }


  return expected_kernel_dtype;
}

OpInfoTuple MvOp::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("x", "paddle::dialect::DenseTensorType", false, false, false, true), paddle::dialect::OpInputInfo("vec", "paddle::dialect::DenseTensorType", false, false, false, true) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = {  };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("out", "paddle::dialect::DenseTensorType", false, false) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("MvInferMeta", {"x", "vec"}, "mv", {"x", "vec"}, {}, {}, {}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "mv");
}

void MvOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, pir::Value vec_) {
  VLOG(4) << "Start build MvOp";



  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_, vec_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;
  paddle::dialect::DenseTensorType vec = vec_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)vec;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);

  VLOG(4) << "Builder construction  dense_vec";
  paddle::dialect::IrTensor ir_tensor_vec(paddle::dialect::TransToPhiDataType(vec.dtype()),
                                                      vec.dims(),
                                                      vec.data_layout(),
                                                      vec.lod(),
                                                      vec.offset());
  VLOG(4) << "Builder construction  meta_vec";
  paddle::dialect::IrMetaTensor meta_vec(&ir_tensor_vec);
  paddle::dialect::IrTensor dense_out;
  paddle::dialect::IrMetaTensor meta_out(&dense_out);

  phi::MvInferMeta(meta_x, meta_vec, &meta_out);

  std::vector<pir::Type> argument_outputs;
  pir::Type out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_out.dtype()), dense_out.dims(), dense_out.layout(), dense_out.lod(), dense_out.offset());
  argument_outputs.push_back(out_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void MvOp::VerifySig() {
  VLOG(4) << "Start Verifying inputs, outputs and attributes for: MvOp.";
  VLOG(4) << "Verifying inputs:";
  {
  auto input_size = num_operands();
  IR_ENFORCE(input_size == 2u,
                    "The size %d of inputs must be equal to 2.", input_size);
  IR_ENFORCE((*this)->operand_source(0).type().isa<paddle::dialect::DenseTensorType>(),
                  "Type validation failed for the 0th input, got %s.", (*this)->operand_source(0).type());
  IR_ENFORCE((*this)->operand_source(1).type().isa<paddle::dialect::DenseTensorType>(),
                  "Type validation failed for the 1th input, got %s.", (*this)->operand_source(1).type());
  }
  VLOG(4) << "Verifying attributes:";
  {
  // Attributes num is 0, not need to check attributes type.
  }
  VLOG(4) << "Verifying outputs:";
  {
  auto output_size = num_results();
  IR_ENFORCE(output_size == 1u,
                    "The size %d of outputs must be equal to 1.", output_size);
  IR_ENFORCE((*this)->result(0).type().isa<paddle::dialect::DenseTensorType>(),
                 "Type validation failed for the 0th output.");
  }
  VLOG(4) << "End Verifying for: MvOp.";
}

void MvOp::InferMeta( phi::InferMetaContext *infer_meta ) {
  auto fn = PD_INFER_META(phi::MvInferMeta);
  fn(infer_meta);
}

phi::DataType MvOp::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: MvOp";
  


  return expected_kernel_dtype;
}

const char *NanmedianOp::attributes_name[2] = { "axis", "keepdim" };

OpInfoTuple NanmedianOp::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("x", "paddle::dialect::DenseTensorType", false, false, false, true) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = { paddle::dialect::OpAttributeInfo("axis", "paddle::dialect::IntArrayAttribute", "std::vector<int64_t>"), paddle::dialect::OpAttributeInfo("keepdim", "pir::BoolAttribute", "") };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("out", "paddle::dialect::DenseTensorType", false, false), paddle::dialect::OpOutputInfo("medians", "paddle::dialect::DenseTensorType", true, true) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("NanmedianInferMeta", {"x", "axis", "keepdim"}, "nanmedian", {"x", "axis", "keepdim"}, {}, {}, {}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "nanmedian");
}

void NanmedianOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, const std::vector<int64_t>& axis, bool keepdim) {
  VLOG(4) << "Start build NanmedianOp";



  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_axis = paddle::dialect::IntArrayAttribute::get(pir::IrContext::Instance(), phi::IntArray(axis));
  argument.AddAttribute("axis", attr_axis);
  pir::Attribute attr_keepdim = pir::BoolAttribute::get(pir::IrContext::Instance(), keepdim);
  argument.AddAttribute("keepdim", attr_keepdim);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);
  paddle::dialect::IrTensor dense_out;
  paddle::dialect::IrMetaTensor meta_out(&dense_out);
  paddle::dialect::IrTensor dense_medians;
  paddle::dialect::IrMetaTensor meta_medians(&dense_medians);

  phi::NanmedianInferMeta(meta_x, axis, keepdim, &meta_out, &meta_medians);

  std::vector<pir::Type> argument_outputs;
  pir::Type out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_out.dtype()), dense_out.dims(), dense_out.layout(), dense_out.lod(), dense_out.offset());
  argument_outputs.push_back(out_dense_tensor_type);

  pir::Type medians_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_medians.dtype()), dense_medians.dims(), dense_medians.layout(), dense_medians.lod(), dense_medians.offset());
  argument_outputs.push_back(medians_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void NanmedianOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, pir::AttributeMap attributes) {
  VLOG(4) << "Start build NanmedianOp";


  IR_ENFORCE(
      attributes.find("axis") != attributes.end(),
          "'axis' Attribute is expected for NanmedianOp. ");
  std::vector<int64_t> axis = attributes.at("axis").dyn_cast<paddle::dialect::IntArrayAttribute>().data().GetData();

  IR_ENFORCE(
      attributes.find("keepdim") != attributes.end(),
          "'keepdim' Attribute is expected for NanmedianOp. ");
  bool keepdim = attributes.at("keepdim").dyn_cast<pir::BoolAttribute>().data();


  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_axis = paddle::dialect::IntArrayAttribute::get(pir::IrContext::Instance(), phi::IntArray(axis));
  argument.AddAttribute("axis", attr_axis);
  pir::Attribute attr_keepdim = pir::BoolAttribute::get(pir::IrContext::Instance(), keepdim);
  argument.AddAttribute("keepdim", attr_keepdim);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);
  paddle::dialect::IrTensor dense_out;
  paddle::dialect::IrMetaTensor meta_out(&dense_out);
  paddle::dialect::IrTensor dense_medians;
  paddle::dialect::IrMetaTensor meta_medians(&dense_medians);

  phi::NanmedianInferMeta(meta_x, axis, keepdim, &meta_out, &meta_medians);

  std::vector<pir::Type> argument_outputs;
  pir::Type out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_out.dtype()), dense_out.dims(), dense_out.layout(), dense_out.lod(), dense_out.offset());
  argument_outputs.push_back(out_dense_tensor_type);

  pir::Type medians_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_medians.dtype()), dense_medians.dims(), dense_medians.layout(), dense_medians.lod(), dense_medians.offset());
  argument_outputs.push_back(medians_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void NanmedianOp::VerifySig() {
  VLOG(4) << "Start Verifying inputs, outputs and attributes for: NanmedianOp.";
  VLOG(4) << "Verifying inputs:";
  {
  auto input_size = num_operands();
  IR_ENFORCE(input_size == 1u,
                    "The size %d of inputs must be equal to 1.", input_size);
  IR_ENFORCE((*this)->operand_source(0).type().isa<paddle::dialect::DenseTensorType>(),
                  "Type validation failed for the 0th input, got %s.", (*this)->operand_source(0).type());
  }
  VLOG(4) << "Verifying attributes:";
  {
  auto& attributes = this->attributes();
  IR_ENFORCE(attributes.count("axis")>0,
                 "axis does not exist.");
  IR_ENFORCE(attributes.at("axis").isa<paddle::dialect::IntArrayAttribute>(),
                 "Type of attribute: axis is not paddle::dialect::IntArrayAttribute.");

  IR_ENFORCE(attributes.count("keepdim")>0,
                 "keepdim does not exist.");
  IR_ENFORCE(attributes.at("keepdim").isa<pir::BoolAttribute>(),
                 "Type of attribute: keepdim is not pir::BoolAttribute.");

  }
  VLOG(4) << "Verifying outputs:";
  {
  auto output_size = num_results();
  IR_ENFORCE(output_size == 2u,
                    "The size %d of outputs must be equal to 2.", output_size);
  IR_ENFORCE((*this)->result(0).type().isa<paddle::dialect::DenseTensorType>(),
                 "Type validation failed for the 0th output.");
  if (auto output_1_type = (*this)->result(1).type()) {
    IR_ENFORCE(output_1_type.isa<paddle::dialect::DenseTensorType>(),
                   "Type validation failed for the 1th output.");
  }
  }
  VLOG(4) << "End Verifying for: NanmedianOp.";
}

void NanmedianOp::InferMeta( phi::InferMetaContext *infer_meta ) {
  auto fn = PD_INFER_META(phi::NanmedianInferMeta);
  fn(infer_meta);
}

phi::DataType NanmedianOp::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: NanmedianOp";
  


  return expected_kernel_dtype;
}

const char *NearestInterpOp::attributes_name[8] = { "data_layout", "out_d", "out_h", "out_w", "scale", "interp_method", "align_corners", "align_mode" };

OpInfoTuple NearestInterpOp::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("x", "paddle::dialect::DenseTensorType", false, false, false, true), paddle::dialect::OpInputInfo("out_size", "paddle::dialect::DenseTensorType", true, false, false, false), paddle::dialect::OpInputInfo("size_tensor", "pir::VectorType<paddle::dialect::DenseTensorType>", true, false, false, false), paddle::dialect::OpInputInfo("scale_tensor", "paddle::dialect::DenseTensorType", true, false, false, false) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = { paddle::dialect::OpAttributeInfo("data_layout", "pir::StrAttribute", ""), paddle::dialect::OpAttributeInfo("out_d", "pir::Int32Attribute", ""), paddle::dialect::OpAttributeInfo("out_h", "pir::Int32Attribute", ""), paddle::dialect::OpAttributeInfo("out_w", "pir::Int32Attribute", ""), paddle::dialect::OpAttributeInfo("scale", "pir::ArrayAttribute<pir::FloatAttribute>", ""), paddle::dialect::OpAttributeInfo("interp_method", "pir::StrAttribute", ""), paddle::dialect::OpAttributeInfo("align_corners", "pir::BoolAttribute", ""), paddle::dialect::OpAttributeInfo("align_mode", "pir::Int32Attribute", "") };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("output", "paddle::dialect::DenseTensorType", false, false) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("InterpolateInferMeta", {"x", "out_size", "size_tensor", "scale_tensor", "data_layout", "out_d", "out_h", "out_w", "scale", "interp_method", "align_corners", "align_mode"}, "nearest_interp", {"x", "out_size", "size_tensor", "scale_tensor", "data_layout", "out_d", "out_h", "out_w", "scale", "interp_method", "align_corners", "align_mode"}, {"x"}, {}, {}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "nearest_interp");
}

void NearestInterpOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, pir::Value out_size_, pir::Value size_tensor_, pir::Value scale_tensor_, const std::string& data_layout, int out_d, int out_h, int out_w, const std::vector<float>& scale, const std::string& interp_method, bool align_corners, int align_mode) {
  VLOG(4) << "Start build NearestInterpOp";



  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_, out_size_, size_tensor_, scale_tensor_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_data_layout = pir::StrAttribute::get(pir::IrContext::Instance(), data_layout);
  argument.AddAttribute("data_layout", attr_data_layout);
  pir::Attribute attr_out_d = pir::Int32Attribute::get(pir::IrContext::Instance(), out_d);
  argument.AddAttribute("out_d", attr_out_d);
  pir::Attribute attr_out_h = pir::Int32Attribute::get(pir::IrContext::Instance(), out_h);
  argument.AddAttribute("out_h", attr_out_h);
  pir::Attribute attr_out_w = pir::Int32Attribute::get(pir::IrContext::Instance(), out_w);
  argument.AddAttribute("out_w", attr_out_w);
  std::vector<pir::Attribute> vec_scale;
  for (size_t i = 0; i < static_cast<size_t>(scale.size()); i++) {
      pir::Attribute attr_scale = pir::FloatAttribute::get(pir::IrContext::Instance(), scale[i]);

    vec_scale.push_back(attr_scale);
  }
  pir::Attribute attr_scale = pir::ArrayAttribute::get(pir::IrContext::Instance(), vec_scale);
  argument.AddAttribute("scale", attr_scale);
  pir::Attribute attr_interp_method = pir::StrAttribute::get(pir::IrContext::Instance(), interp_method);
  argument.AddAttribute("interp_method", attr_interp_method);
  pir::Attribute attr_align_corners = pir::BoolAttribute::get(pir::IrContext::Instance(), align_corners);
  argument.AddAttribute("align_corners", attr_align_corners);
  pir::Attribute attr_align_mode = pir::Int32Attribute::get(pir::IrContext::Instance(), align_mode);
  argument.AddAttribute("align_mode", attr_align_mode);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);

  paddle::dialect::IrMetaTensor meta_out_size;
  paddle::dialect::IrTensor ir_tensor_out_size;
  if (out_size_.impl() != nullptr) {
    paddle::dialect::DenseTensorType out_size = out_size_.type().dyn_cast<paddle::dialect::DenseTensorType>();
    VLOG(4) << "Builder construction  dense_out_size";
    ir_tensor_out_size = paddle::dialect::IrTensor(paddle::dialect::TransToPhiDataType(out_size.dtype()),
                                                        out_size.dims(),
                                                        out_size.data_layout(),
                                                        out_size.lod(),
                                                        out_size.offset());
    VLOG(4) << "Builder construction  meta_out_size";
    meta_out_size = paddle::dialect::IrMetaTensor(&ir_tensor_out_size);
  }

  std::vector<paddle::dialect::IrTensor> vec_ir_tensor_size_tensor;
  if (size_tensor_.impl() != nullptr) {
    pir::VectorType size_tensor = size_tensor_.type().dyn_cast<pir::VectorType>();
    for (size_t i=0; i < static_cast<size_t>(size_tensor.size()); i++) {
        vec_ir_tensor_size_tensor.push_back(paddle::dialect::IrTensor(paddle::dialect::TransToPhiDataType(size_tensor[i].dyn_cast<paddle::dialect::DenseTensorType>().dtype()),
                                                                        size_tensor[i].dyn_cast<paddle::dialect::DenseTensorType>().dims(),
                                                                        size_tensor[i].dyn_cast<paddle::dialect::DenseTensorType>().data_layout(),
                                                                        size_tensor[i].dyn_cast<paddle::dialect::DenseTensorType>().lod(),
                                                                        size_tensor[i].dyn_cast<paddle::dialect::DenseTensorType>().offset()));
    }
  }

  std::vector<paddle::dialect::IrMetaTensor> vec_meta_size_tensor;
  for (size_t i=0; i < vec_ir_tensor_size_tensor.size(); i++) {
    vec_meta_size_tensor.push_back(paddle::dialect::IrMetaTensor(&vec_ir_tensor_size_tensor[i]));
  }

  std::vector<const phi::MetaTensor*> meta_size_tensor;
  for (size_t i=0; i < static_cast<size_t>(vec_meta_size_tensor.size()); i++) {
    meta_size_tensor.push_back(&vec_meta_size_tensor[i]);
  }


  paddle::dialect::IrMetaTensor meta_scale_tensor;
  paddle::dialect::IrTensor ir_tensor_scale_tensor;
  if (scale_tensor_.impl() != nullptr) {
    paddle::dialect::DenseTensorType scale_tensor = scale_tensor_.type().dyn_cast<paddle::dialect::DenseTensorType>();
    VLOG(4) << "Builder construction  dense_scale_tensor";
    ir_tensor_scale_tensor = paddle::dialect::IrTensor(paddle::dialect::TransToPhiDataType(scale_tensor.dtype()),
                                                        scale_tensor.dims(),
                                                        scale_tensor.data_layout(),
                                                        scale_tensor.lod(),
                                                        scale_tensor.offset());
    VLOG(4) << "Builder construction  meta_scale_tensor";
    meta_scale_tensor = paddle::dialect::IrMetaTensor(&ir_tensor_scale_tensor);
  }

  paddle::dialect::IrTensor dense_output;
  paddle::dialect::IrMetaTensor meta_output(&dense_output);

  phi::InterpolateInferMeta(meta_x, meta_out_size, meta_size_tensor, meta_scale_tensor, data_layout, out_d, out_h, out_w, scale, interp_method, align_corners, align_mode, &meta_output, phi::MetaConfig(false, false));

  std::vector<pir::Type> argument_outputs;
  pir::Type output_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_output.dtype()), dense_output.dims(), dense_output.layout(), dense_output.lod(), dense_output.offset());
  argument_outputs.push_back(output_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void NearestInterpOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, pir::Value out_size_, pir::Value size_tensor_, pir::Value scale_tensor_, pir::AttributeMap attributes) {
  VLOG(4) << "Start build NearestInterpOp";


  IR_ENFORCE(
      attributes.find("data_layout") != attributes.end(),
          "'data_layout' Attribute is expected for NearestInterpOp. ");
  std::string data_layout = attributes.at("data_layout").dyn_cast<pir::StrAttribute>().AsString();

  IR_ENFORCE(
      attributes.find("out_d") != attributes.end(),
          "'out_d' Attribute is expected for NearestInterpOp. ");
  int out_d = attributes.at("out_d").dyn_cast<pir::Int32Attribute>().data();

  IR_ENFORCE(
      attributes.find("out_h") != attributes.end(),
          "'out_h' Attribute is expected for NearestInterpOp. ");
  int out_h = attributes.at("out_h").dyn_cast<pir::Int32Attribute>().data();

  IR_ENFORCE(
      attributes.find("out_w") != attributes.end(),
          "'out_w' Attribute is expected for NearestInterpOp. ");
  int out_w = attributes.at("out_w").dyn_cast<pir::Int32Attribute>().data();

  IR_ENFORCE(
      attributes.find("scale") != attributes.end(),
          "'scale' Attribute is expected for NearestInterpOp. ");
  std::vector<float> scale;
  for (size_t i = 0; i < attributes.at("scale").dyn_cast<pir::ArrayAttribute>().size(); i++) {
    scale.push_back(attributes.at("scale").dyn_cast<pir::ArrayAttribute>().at(i).dyn_cast<pir::FloatAttribute>().data());
  }

  IR_ENFORCE(
      attributes.find("interp_method") != attributes.end(),
          "'interp_method' Attribute is expected for NearestInterpOp. ");
  std::string interp_method = attributes.at("interp_method").dyn_cast<pir::StrAttribute>().AsString();

  IR_ENFORCE(
      attributes.find("align_corners") != attributes.end(),
          "'align_corners' Attribute is expected for NearestInterpOp. ");
  bool align_corners = attributes.at("align_corners").dyn_cast<pir::BoolAttribute>().data();

  IR_ENFORCE(
      attributes.find("align_mode") != attributes.end(),
          "'align_mode' Attribute is expected for NearestInterpOp. ");
  int align_mode = attributes.at("align_mode").dyn_cast<pir::Int32Attribute>().data();


  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_, out_size_, size_tensor_, scale_tensor_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_data_layout = pir::StrAttribute::get(pir::IrContext::Instance(), data_layout);
  argument.AddAttribute("data_layout", attr_data_layout);
  pir::Attribute attr_out_d = pir::Int32Attribute::get(pir::IrContext::Instance(), out_d);
  argument.AddAttribute("out_d", attr_out_d);
  pir::Attribute attr_out_h = pir::Int32Attribute::get(pir::IrContext::Instance(), out_h);
  argument.AddAttribute("out_h", attr_out_h);
  pir::Attribute attr_out_w = pir::Int32Attribute::get(pir::IrContext::Instance(), out_w);
  argument.AddAttribute("out_w", attr_out_w);
  std::vector<pir::Attribute> vec_scale;
  for (size_t i = 0; i < static_cast<size_t>(scale.size()); i++) {
      pir::Attribute attr_scale = pir::FloatAttribute::get(pir::IrContext::Instance(), scale[i]);

    vec_scale.push_back(attr_scale);
  }
  pir::Attribute attr_scale = pir::ArrayAttribute::get(pir::IrContext::Instance(), vec_scale);
  argument.AddAttribute("scale", attr_scale);
  pir::Attribute attr_interp_method = pir::StrAttribute::get(pir::IrContext::Instance(), interp_method);
  argument.AddAttribute("interp_method", attr_interp_method);
  pir::Attribute attr_align_corners = pir::BoolAttribute::get(pir::IrContext::Instance(), align_corners);
  argument.AddAttribute("align_corners", attr_align_corners);
  pir::Attribute attr_align_mode = pir::Int32Attribute::get(pir::IrContext::Instance(), align_mode);
  argument.AddAttribute("align_mode", attr_align_mode);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);

  paddle::dialect::IrMetaTensor meta_out_size;
  paddle::dialect::IrTensor ir_tensor_out_size;
  if (out_size_.impl() != nullptr) {
    paddle::dialect::DenseTensorType out_size = out_size_.type().dyn_cast<paddle::dialect::DenseTensorType>();
    VLOG(4) << "Builder construction  dense_out_size";
    ir_tensor_out_size = paddle::dialect::IrTensor(paddle::dialect::TransToPhiDataType(out_size.dtype()),
                                                        out_size.dims(),
                                                        out_size.data_layout(),
                                                        out_size.lod(),
                                                        out_size.offset());
    VLOG(4) << "Builder construction  meta_out_size";
    meta_out_size = paddle::dialect::IrMetaTensor(&ir_tensor_out_size);
  }

  std::vector<paddle::dialect::IrTensor> vec_ir_tensor_size_tensor;
  if (size_tensor_.impl() != nullptr) {
    pir::VectorType size_tensor = size_tensor_.type().dyn_cast<pir::VectorType>();
    for (size_t i=0; i < static_cast<size_t>(size_tensor.size()); i++) {
        vec_ir_tensor_size_tensor.push_back(paddle::dialect::IrTensor(paddle::dialect::TransToPhiDataType(size_tensor[i].dyn_cast<paddle::dialect::DenseTensorType>().dtype()),
                                                                        size_tensor[i].dyn_cast<paddle::dialect::DenseTensorType>().dims(),
                                                                        size_tensor[i].dyn_cast<paddle::dialect::DenseTensorType>().data_layout(),
                                                                        size_tensor[i].dyn_cast<paddle::dialect::DenseTensorType>().lod(),
                                                                        size_tensor[i].dyn_cast<paddle::dialect::DenseTensorType>().offset()));
    }
  }

  std::vector<paddle::dialect::IrMetaTensor> vec_meta_size_tensor;
  for (size_t i=0; i < vec_ir_tensor_size_tensor.size(); i++) {
    vec_meta_size_tensor.push_back(paddle::dialect::IrMetaTensor(&vec_ir_tensor_size_tensor[i]));
  }

  std::vector<const phi::MetaTensor*> meta_size_tensor;
  for (size_t i=0; i < static_cast<size_t>(vec_meta_size_tensor.size()); i++) {
    meta_size_tensor.push_back(&vec_meta_size_tensor[i]);
  }


  paddle::dialect::IrMetaTensor meta_scale_tensor;
  paddle::dialect::IrTensor ir_tensor_scale_tensor;
  if (scale_tensor_.impl() != nullptr) {
    paddle::dialect::DenseTensorType scale_tensor = scale_tensor_.type().dyn_cast<paddle::dialect::DenseTensorType>();
    VLOG(4) << "Builder construction  dense_scale_tensor";
    ir_tensor_scale_tensor = paddle::dialect::IrTensor(paddle::dialect::TransToPhiDataType(scale_tensor.dtype()),
                                                        scale_tensor.dims(),
                                                        scale_tensor.data_layout(),
                                                        scale_tensor.lod(),
                                                        scale_tensor.offset());
    VLOG(4) << "Builder construction  meta_scale_tensor";
    meta_scale_tensor = paddle::dialect::IrMetaTensor(&ir_tensor_scale_tensor);
  }

  paddle::dialect::IrTensor dense_output;
  paddle::dialect::IrMetaTensor meta_output(&dense_output);

  phi::InterpolateInferMeta(meta_x, meta_out_size, meta_size_tensor, meta_scale_tensor, data_layout, out_d, out_h, out_w, scale, interp_method, align_corners, align_mode, &meta_output, phi::MetaConfig(false, false));

  std::vector<pir::Type> argument_outputs;
  pir::Type output_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_output.dtype()), dense_output.dims(), dense_output.layout(), dense_output.lod(), dense_output.offset());
  argument_outputs.push_back(output_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void NearestInterpOp::VerifySig() {
  VLOG(4) << "Start Verifying inputs, outputs and attributes for: NearestInterpOp.";
  VLOG(4) << "Verifying inputs:";
  {
  auto input_size = num_operands();
  IR_ENFORCE(input_size == 4u,
                    "The size %d of inputs must be equal to 4.", input_size);
  IR_ENFORCE((*this)->operand_source(0).type().isa<paddle::dialect::DenseTensorType>(),
                  "Type validation failed for the 0th input, got %s.", (*this)->operand_source(0).type());
  if (auto val = (*this)->operand(1)) {
    IR_ENFORCE(val.type().isa<paddle::dialect::DenseTensorType>(),
                   "Type validation failed for the 1th input, got %s.", (*this)->operand_source(1).type());
  }
  if (auto val =  (*this)->operand(2)) {
    if (auto vec_type = val.type().dyn_cast<pir::VectorType>()) {
      for (size_t i = 0; i < vec_type.size(); i++) {
        IR_ENFORCE(vec_type[i].isa<paddle::dialect::DenseTensorType>(),
                          "Type validation failed for the 2th input, got %s.", (*this)->operand_source(2).type());
      }
    }
    else {
      IR_ENFORCE(val.type().isa<paddle::dialect::DenseTensorType>(),
                        "Type validation failed for the 2th input, got %s.", (*this)->operand_source(2).type());
    }
  }
  if (auto val = (*this)->operand(3)) {
    IR_ENFORCE(val.type().isa<paddle::dialect::DenseTensorType>(),
                   "Type validation failed for the 3th input, got %s.", (*this)->operand_source(3).type());
  }
  }
  VLOG(4) << "Verifying attributes:";
  {
  auto& attributes = this->attributes();
  IR_ENFORCE(attributes.count("data_layout")>0,
                 "data_layout does not exist.");
  IR_ENFORCE(attributes.at("data_layout").isa<pir::StrAttribute>(),
                 "Type of attribute: data_layout is not pir::StrAttribute.");

  IR_ENFORCE(attributes.count("out_d")>0,
                 "out_d does not exist.");
  IR_ENFORCE(attributes.at("out_d").isa<pir::Int32Attribute>(),
                 "Type of attribute: out_d is not pir::Int32Attribute.");

  IR_ENFORCE(attributes.count("out_h")>0,
                 "out_h does not exist.");
  IR_ENFORCE(attributes.at("out_h").isa<pir::Int32Attribute>(),
                 "Type of attribute: out_h is not pir::Int32Attribute.");

  IR_ENFORCE(attributes.count("out_w")>0,
                 "out_w does not exist.");
  IR_ENFORCE(attributes.at("out_w").isa<pir::Int32Attribute>(),
                 "Type of attribute: out_w is not pir::Int32Attribute.");

  IR_ENFORCE(attributes.count("scale")>0,
                 "scale does not exist.");
  IR_ENFORCE(attributes.at("scale").isa<pir::ArrayAttribute>(),
                 "Type of attribute: scale is not pir::ArrayAttribute.");
  for (size_t i = 0; i < attributes.at("scale").dyn_cast<pir::ArrayAttribute>().size(); i++) {
    IR_ENFORCE(attributes.at("scale").dyn_cast<pir::ArrayAttribute>().at(i).isa<pir::FloatAttribute>(),
                   "Type of attribute: scale is not right.");
  }
  IR_ENFORCE(attributes.count("interp_method")>0,
                 "interp_method does not exist.");
  IR_ENFORCE(attributes.at("interp_method").isa<pir::StrAttribute>(),
                 "Type of attribute: interp_method is not pir::StrAttribute.");

  IR_ENFORCE(attributes.count("align_corners")>0,
                 "align_corners does not exist.");
  IR_ENFORCE(attributes.at("align_corners").isa<pir::BoolAttribute>(),
                 "Type of attribute: align_corners is not pir::BoolAttribute.");

  IR_ENFORCE(attributes.count("align_mode")>0,
                 "align_mode does not exist.");
  IR_ENFORCE(attributes.at("align_mode").isa<pir::Int32Attribute>(),
                 "Type of attribute: align_mode is not pir::Int32Attribute.");

  }
  VLOG(4) << "Verifying outputs:";
  {
  auto output_size = num_results();
  IR_ENFORCE(output_size == 1u,
                    "The size %d of outputs must be equal to 1.", output_size);
  IR_ENFORCE((*this)->result(0).type().isa<paddle::dialect::DenseTensorType>(),
                 "Type validation failed for the 0th output.");
  }
  VLOG(4) << "End Verifying for: NearestInterpOp.";
}

void NearestInterpOp::InferMeta( phi::InferMetaContext *infer_meta ) {
  auto fn = PD_INFER_META(phi::InterpolateInferMeta);
  fn(infer_meta);
}

phi::DataType NearestInterpOp::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: NearestInterpOp";
  

  // deal skip data transform
  if (var_name == "out_size" || var_name == "size_tensor" || var_name == "scale_tensor"){
    return expected_kernel_dtype;
  }


  return expected_kernel_dtype;
}

OpInfoTuple NextafterOp::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("x", "paddle::dialect::DenseTensorType", false, false, false, false), paddle::dialect::OpInputInfo("y", "paddle::dialect::DenseTensorType", false, false, false, false) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = {  };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("out", "paddle::dialect::DenseTensorType", false, false) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("ElementwiseInferMeta", {"x", "y"}, "nextafter", {"x", "y"}, {"x"}, {}, {}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "nextafter");
}

void NextafterOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, pir::Value y_) {
  VLOG(4) << "Start build NextafterOp";



  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_, y_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;
  paddle::dialect::DenseTensorType y = y_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)y;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);

  VLOG(4) << "Builder construction  dense_y";
  paddle::dialect::IrTensor ir_tensor_y(paddle::dialect::TransToPhiDataType(y.dtype()),
                                                      y.dims(),
                                                      y.data_layout(),
                                                      y.lod(),
                                                      y.offset());
  VLOG(4) << "Builder construction  meta_y";
  paddle::dialect::IrMetaTensor meta_y(&ir_tensor_y);
  paddle::dialect::IrTensor dense_out;
  paddle::dialect::IrMetaTensor meta_out(&dense_out);

  phi::ElementwiseInferMeta(meta_x, meta_y, &meta_out);

  std::vector<pir::Type> argument_outputs;
  pir::Type out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_out.dtype()), dense_out.dims(), dense_out.layout(), dense_out.lod(), dense_out.offset());
  argument_outputs.push_back(out_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void NextafterOp::VerifySig() {
  VLOG(4) << "Start Verifying inputs, outputs and attributes for: NextafterOp.";
  VLOG(4) << "Verifying inputs:";
  {
  auto input_size = num_operands();
  IR_ENFORCE(input_size == 2u,
                    "The size %d of inputs must be equal to 2.", input_size);
  IR_ENFORCE((*this)->operand_source(0).type().isa<paddle::dialect::DenseTensorType>(),
                  "Type validation failed for the 0th input, got %s.", (*this)->operand_source(0).type());
  IR_ENFORCE((*this)->operand_source(1).type().isa<paddle::dialect::DenseTensorType>(),
                  "Type validation failed for the 1th input, got %s.", (*this)->operand_source(1).type());
  }
  VLOG(4) << "Verifying attributes:";
  {
  // Attributes num is 0, not need to check attributes type.
  }
  VLOG(4) << "Verifying outputs:";
  {
  auto output_size = num_results();
  IR_ENFORCE(output_size == 1u,
                    "The size %d of outputs must be equal to 1.", output_size);
  IR_ENFORCE((*this)->result(0).type().isa<paddle::dialect::DenseTensorType>(),
                 "Type validation failed for the 0th output.");
  }
  VLOG(4) << "End Verifying for: NextafterOp.";
}

void NextafterOp::InferMeta( phi::InferMetaContext *infer_meta ) {
  auto fn = PD_INFER_META(phi::ElementwiseInferMeta);
  fn(infer_meta);
}

phi::DataType NextafterOp::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: NextafterOp";
  


  return expected_kernel_dtype;
}

const char *NllLossOp::attributes_name[2] = { "ignore_index", "reduction" };

OpInfoTuple NllLossOp::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("input", "paddle::dialect::DenseTensorType", false, false, false, true), paddle::dialect::OpInputInfo("label", "paddle::dialect::DenseTensorType", false, false, false, false), paddle::dialect::OpInputInfo("weight", "paddle::dialect::DenseTensorType", true, false, false, false) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = { paddle::dialect::OpAttributeInfo("ignore_index", "pir::Int64Attribute", ""), paddle::dialect::OpAttributeInfo("reduction", "pir::StrAttribute", "") };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("out", "paddle::dialect::DenseTensorType", false, false), paddle::dialect::OpOutputInfo("total_weight", "paddle::dialect::DenseTensorType", false, false) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("NllLossRawInferMeta", {"input", "label", "weight", "ignore_index", "reduction"}, "nll_loss", {"input", "label", "weight", "ignore_index", "reduction"}, {"input"}, {}, {}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "nll_loss");
}

void NllLossOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value input_, pir::Value label_, pir::Value weight_, int64_t ignore_index, const std::string& reduction) {
  VLOG(4) << "Start build NllLossOp";



  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {input_, label_, weight_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_ignore_index = pir::Int64Attribute::get(pir::IrContext::Instance(), ignore_index);
  argument.AddAttribute("ignore_index", attr_ignore_index);
  pir::Attribute attr_reduction = pir::StrAttribute::get(pir::IrContext::Instance(), reduction);
  argument.AddAttribute("reduction", attr_reduction);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType input = input_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)input;
  paddle::dialect::DenseTensorType label = label_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)label;

  VLOG(4) << "Builder construction  dense_input";
  paddle::dialect::IrTensor ir_tensor_input(paddle::dialect::TransToPhiDataType(input.dtype()),
                                                      input.dims(),
                                                      input.data_layout(),
                                                      input.lod(),
                                                      input.offset());
  VLOG(4) << "Builder construction  meta_input";
  paddle::dialect::IrMetaTensor meta_input(&ir_tensor_input);

  VLOG(4) << "Builder construction  dense_label";
  paddle::dialect::IrTensor ir_tensor_label(paddle::dialect::TransToPhiDataType(label.dtype()),
                                                      label.dims(),
                                                      label.data_layout(),
                                                      label.lod(),
                                                      label.offset());
  VLOG(4) << "Builder construction  meta_label";
  paddle::dialect::IrMetaTensor meta_label(&ir_tensor_label);

  paddle::dialect::IrMetaTensor meta_weight;
  paddle::dialect::IrTensor ir_tensor_weight;
  if (weight_.impl() != nullptr) {
    paddle::dialect::DenseTensorType weight = weight_.type().dyn_cast<paddle::dialect::DenseTensorType>();
    VLOG(4) << "Builder construction  dense_weight";
    ir_tensor_weight = paddle::dialect::IrTensor(paddle::dialect::TransToPhiDataType(weight.dtype()),
                                                        weight.dims(),
                                                        weight.data_layout(),
                                                        weight.lod(),
                                                        weight.offset());
    VLOG(4) << "Builder construction  meta_weight";
    meta_weight = paddle::dialect::IrMetaTensor(&ir_tensor_weight);
  }

  paddle::dialect::IrTensor dense_out;
  paddle::dialect::IrMetaTensor meta_out(&dense_out);
  paddle::dialect::IrTensor dense_total_weight;
  paddle::dialect::IrMetaTensor meta_total_weight(&dense_total_weight);

  phi::NllLossRawInferMeta(meta_input, meta_label, meta_weight, ignore_index, reduction, &meta_out, &meta_total_weight);

  std::vector<pir::Type> argument_outputs;
  pir::Type out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_out.dtype()), dense_out.dims(), dense_out.layout(), dense_out.lod(), dense_out.offset());
  argument_outputs.push_back(out_dense_tensor_type);

  pir::Type total_weight_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_total_weight.dtype()), dense_total_weight.dims(), dense_total_weight.layout(), dense_total_weight.lod(), dense_total_weight.offset());
  argument_outputs.push_back(total_weight_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void NllLossOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value input_, pir::Value label_, pir::Value weight_, pir::AttributeMap attributes) {
  VLOG(4) << "Start build NllLossOp";


  IR_ENFORCE(
      attributes.find("ignore_index") != attributes.end(),
          "'ignore_index' Attribute is expected for NllLossOp. ");
  int64_t ignore_index = attributes.at("ignore_index").dyn_cast<pir::Int64Attribute>().data();

  IR_ENFORCE(
      attributes.find("reduction") != attributes.end(),
          "'reduction' Attribute is expected for NllLossOp. ");
  std::string reduction = attributes.at("reduction").dyn_cast<pir::StrAttribute>().AsString();


  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {input_, label_, weight_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_ignore_index = pir::Int64Attribute::get(pir::IrContext::Instance(), ignore_index);
  argument.AddAttribute("ignore_index", attr_ignore_index);
  pir::Attribute attr_reduction = pir::StrAttribute::get(pir::IrContext::Instance(), reduction);
  argument.AddAttribute("reduction", attr_reduction);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType input = input_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)input;
  paddle::dialect::DenseTensorType label = label_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)label;

  VLOG(4) << "Builder construction  dense_input";
  paddle::dialect::IrTensor ir_tensor_input(paddle::dialect::TransToPhiDataType(input.dtype()),
                                                      input.dims(),
                                                      input.data_layout(),
                                                      input.lod(),
                                                      input.offset());
  VLOG(4) << "Builder construction  meta_input";
  paddle::dialect::IrMetaTensor meta_input(&ir_tensor_input);

  VLOG(4) << "Builder construction  dense_label";
  paddle::dialect::IrTensor ir_tensor_label(paddle::dialect::TransToPhiDataType(label.dtype()),
                                                      label.dims(),
                                                      label.data_layout(),
                                                      label.lod(),
                                                      label.offset());
  VLOG(4) << "Builder construction  meta_label";
  paddle::dialect::IrMetaTensor meta_label(&ir_tensor_label);

  paddle::dialect::IrMetaTensor meta_weight;
  paddle::dialect::IrTensor ir_tensor_weight;
  if (weight_.impl() != nullptr) {
    paddle::dialect::DenseTensorType weight = weight_.type().dyn_cast<paddle::dialect::DenseTensorType>();
    VLOG(4) << "Builder construction  dense_weight";
    ir_tensor_weight = paddle::dialect::IrTensor(paddle::dialect::TransToPhiDataType(weight.dtype()),
                                                        weight.dims(),
                                                        weight.data_layout(),
                                                        weight.lod(),
                                                        weight.offset());
    VLOG(4) << "Builder construction  meta_weight";
    meta_weight = paddle::dialect::IrMetaTensor(&ir_tensor_weight);
  }

  paddle::dialect::IrTensor dense_out;
  paddle::dialect::IrMetaTensor meta_out(&dense_out);
  paddle::dialect::IrTensor dense_total_weight;
  paddle::dialect::IrMetaTensor meta_total_weight(&dense_total_weight);

  phi::NllLossRawInferMeta(meta_input, meta_label, meta_weight, ignore_index, reduction, &meta_out, &meta_total_weight);

  std::vector<pir::Type> argument_outputs;
  pir::Type out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_out.dtype()), dense_out.dims(), dense_out.layout(), dense_out.lod(), dense_out.offset());
  argument_outputs.push_back(out_dense_tensor_type);

  pir::Type total_weight_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_total_weight.dtype()), dense_total_weight.dims(), dense_total_weight.layout(), dense_total_weight.lod(), dense_total_weight.offset());
  argument_outputs.push_back(total_weight_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void NllLossOp::VerifySig() {
  VLOG(4) << "Start Verifying inputs, outputs and attributes for: NllLossOp.";
  VLOG(4) << "Verifying inputs:";
  {
  auto input_size = num_operands();
  IR_ENFORCE(input_size == 3u,
                    "The size %d of inputs must be equal to 3.", input_size);
  IR_ENFORCE((*this)->operand_source(0).type().isa<paddle::dialect::DenseTensorType>(),
                  "Type validation failed for the 0th input, got %s.", (*this)->operand_source(0).type());
  IR_ENFORCE((*this)->operand_source(1).type().isa<paddle::dialect::DenseTensorType>(),
                  "Type validation failed for the 1th input, got %s.", (*this)->operand_source(1).type());
  if (auto val = (*this)->operand(2)) {
    IR_ENFORCE(val.type().isa<paddle::dialect::DenseTensorType>(),
                   "Type validation failed for the 2th input, got %s.", (*this)->operand_source(2).type());
  }
  }
  VLOG(4) << "Verifying attributes:";
  {
  auto& attributes = this->attributes();
  IR_ENFORCE(attributes.count("ignore_index")>0,
                 "ignore_index does not exist.");
  IR_ENFORCE(attributes.at("ignore_index").isa<pir::Int64Attribute>(),
                 "Type of attribute: ignore_index is not pir::Int64Attribute.");

  IR_ENFORCE(attributes.count("reduction")>0,
                 "reduction does not exist.");
  IR_ENFORCE(attributes.at("reduction").isa<pir::StrAttribute>(),
                 "Type of attribute: reduction is not pir::StrAttribute.");

  }
  VLOG(4) << "Verifying outputs:";
  {
  auto output_size = num_results();
  IR_ENFORCE(output_size == 2u,
                    "The size %d of outputs must be equal to 2.", output_size);
  IR_ENFORCE((*this)->result(0).type().isa<paddle::dialect::DenseTensorType>(),
                 "Type validation failed for the 0th output.");
  IR_ENFORCE((*this)->result(1).type().isa<paddle::dialect::DenseTensorType>(),
                 "Type validation failed for the 1th output.");
  }
  VLOG(4) << "End Verifying for: NllLossOp.";
}

void NllLossOp::InferMeta( phi::InferMetaContext *infer_meta ) {
  auto fn = PD_INFER_META(phi::NllLossRawInferMeta);
  fn(infer_meta);
}

phi::DataType NllLossOp::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: NllLossOp";
  


  return expected_kernel_dtype;
}

const char *NmsOp::attributes_name[1] = { "threshold" };

OpInfoTuple NmsOp::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("x", "paddle::dialect::DenseTensorType", false, false, false, false) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = { paddle::dialect::OpAttributeInfo("threshold", "pir::FloatAttribute", "") };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("out", "paddle::dialect::DenseTensorType", false, false) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("NMSInferMeta", {"x", "threshold"}, "nms", {"x", "threshold"}, {"x"}, {}, {}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "nms");
}

void NmsOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, float threshold) {
  VLOG(4) << "Start build NmsOp";



  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_threshold = pir::FloatAttribute::get(pir::IrContext::Instance(), threshold);
  argument.AddAttribute("threshold", attr_threshold);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);
  paddle::dialect::IrTensor dense_out;
  paddle::dialect::IrMetaTensor meta_out(&dense_out);

  phi::NMSInferMeta(meta_x, threshold, &meta_out);

  std::vector<pir::Type> argument_outputs;
  pir::Type out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_out.dtype()), dense_out.dims(), dense_out.layout(), dense_out.lod(), dense_out.offset());
  argument_outputs.push_back(out_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void NmsOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, pir::AttributeMap attributes) {
  VLOG(4) << "Start build NmsOp";


  IR_ENFORCE(
      attributes.find("threshold") != attributes.end(),
          "'threshold' Attribute is expected for NmsOp. ");
  float threshold = attributes.at("threshold").dyn_cast<pir::FloatAttribute>().data();


  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_threshold = pir::FloatAttribute::get(pir::IrContext::Instance(), threshold);
  argument.AddAttribute("threshold", attr_threshold);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);
  paddle::dialect::IrTensor dense_out;
  paddle::dialect::IrMetaTensor meta_out(&dense_out);

  phi::NMSInferMeta(meta_x, threshold, &meta_out);

  std::vector<pir::Type> argument_outputs;
  pir::Type out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_out.dtype()), dense_out.dims(), dense_out.layout(), dense_out.lod(), dense_out.offset());
  argument_outputs.push_back(out_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void NmsOp::VerifySig() {
  VLOG(4) << "Start Verifying inputs, outputs and attributes for: NmsOp.";
  VLOG(4) << "Verifying inputs:";
  {
  auto input_size = num_operands();
  IR_ENFORCE(input_size == 1u,
                    "The size %d of inputs must be equal to 1.", input_size);
  IR_ENFORCE((*this)->operand_source(0).type().isa<paddle::dialect::DenseTensorType>(),
                  "Type validation failed for the 0th input, got %s.", (*this)->operand_source(0).type());
  }
  VLOG(4) << "Verifying attributes:";
  {
  auto& attributes = this->attributes();
  IR_ENFORCE(attributes.count("threshold")>0,
                 "threshold does not exist.");
  IR_ENFORCE(attributes.at("threshold").isa<pir::FloatAttribute>(),
                 "Type of attribute: threshold is not pir::FloatAttribute.");

  }
  VLOG(4) << "Verifying outputs:";
  {
  auto output_size = num_results();
  IR_ENFORCE(output_size == 1u,
                    "The size %d of outputs must be equal to 1.", output_size);
  IR_ENFORCE((*this)->result(0).type().isa<paddle::dialect::DenseTensorType>(),
                 "Type validation failed for the 0th output.");
  }
  VLOG(4) << "End Verifying for: NmsOp.";
}

void NmsOp::InferMeta( phi::InferMetaContext *infer_meta ) {
  auto fn = PD_INFER_META(phi::NMSInferMeta);
  fn(infer_meta);
}

phi::DataType NmsOp::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: NmsOp";
  


  return expected_kernel_dtype;
}

OpInfoTuple NonzeroOp::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("condition", "paddle::dialect::DenseTensorType", false, false, false, false) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = {  };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("out", "paddle::dialect::DenseTensorType", false, false) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("NonZeroInferMeta", {"condition"}, "nonzero", {"condition"}, {"condition"}, {}, {}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "nonzero");
}

void NonzeroOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value condition_) {
  VLOG(4) << "Start build NonzeroOp";



  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {condition_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType condition = condition_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)condition;

  VLOG(4) << "Builder construction  dense_condition";
  paddle::dialect::IrTensor ir_tensor_condition(paddle::dialect::TransToPhiDataType(condition.dtype()),
                                                      condition.dims(),
                                                      condition.data_layout(),
                                                      condition.lod(),
                                                      condition.offset());
  VLOG(4) << "Builder construction  meta_condition";
  paddle::dialect::IrMetaTensor meta_condition(&ir_tensor_condition);
  paddle::dialect::IrTensor dense_out;
  paddle::dialect::IrMetaTensor meta_out(&dense_out);

  phi::NonZeroInferMeta(meta_condition, &meta_out);

  std::vector<pir::Type> argument_outputs;
  pir::Type out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_out.dtype()), dense_out.dims(), dense_out.layout(), dense_out.lod(), dense_out.offset());
  argument_outputs.push_back(out_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void NonzeroOp::VerifySig() {
  VLOG(4) << "Start Verifying inputs, outputs and attributes for: NonzeroOp.";
  VLOG(4) << "Verifying inputs:";
  {
  auto input_size = num_operands();
  IR_ENFORCE(input_size == 1u,
                    "The size %d of inputs must be equal to 1.", input_size);
  IR_ENFORCE((*this)->operand_source(0).type().isa<paddle::dialect::DenseTensorType>(),
                  "Type validation failed for the 0th input, got %s.", (*this)->operand_source(0).type());
  }
  VLOG(4) << "Verifying attributes:";
  {
  // Attributes num is 0, not need to check attributes type.
  }
  VLOG(4) << "Verifying outputs:";
  {
  auto output_size = num_results();
  IR_ENFORCE(output_size == 1u,
                    "The size %d of outputs must be equal to 1.", output_size);
  IR_ENFORCE((*this)->result(0).type().isa<paddle::dialect::DenseTensorType>(),
                 "Type validation failed for the 0th output.");
  }
  VLOG(4) << "End Verifying for: NonzeroOp.";
}

void NonzeroOp::InferMeta( phi::InferMetaContext *infer_meta ) {
  auto fn = PD_INFER_META(phi::NonZeroInferMeta);
  fn(infer_meta);
}

phi::DataType NonzeroOp::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: NonzeroOp";
  


  return expected_kernel_dtype;
}

const char *NpuIdentityOp::attributes_name[1] = { "format" };

OpInfoTuple NpuIdentityOp::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("x", "paddle::dialect::DenseTensorType", false, false, false, false) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = { paddle::dialect::OpAttributeInfo("format", "pir::Int32Attribute", "") };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("out", "paddle::dialect::DenseTensorType", false, false) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("UnchangedInferMeta", {"x"}, "npu_identity", {"x", "format"}, {}, {}, {}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "npu_identity");
}

void NpuIdentityOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, int format) {
  VLOG(4) << "Start build NpuIdentityOp";



  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_format = pir::Int32Attribute::get(pir::IrContext::Instance(), format);
  argument.AddAttribute("format", attr_format);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);
  paddle::dialect::IrTensor dense_out;
  paddle::dialect::IrMetaTensor meta_out(&dense_out);

  phi::UnchangedInferMeta(meta_x, &meta_out);

  std::vector<pir::Type> argument_outputs;
  pir::Type out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_out.dtype()), dense_out.dims(), dense_out.layout(), dense_out.lod(), dense_out.offset());
  argument_outputs.push_back(out_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void NpuIdentityOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, pir::AttributeMap attributes) {
  VLOG(4) << "Start build NpuIdentityOp";


  IR_ENFORCE(
      attributes.find("format") != attributes.end(),
          "'format' Attribute is expected for NpuIdentityOp. ");
  int format = attributes.at("format").dyn_cast<pir::Int32Attribute>().data();


  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_format = pir::Int32Attribute::get(pir::IrContext::Instance(), format);
  argument.AddAttribute("format", attr_format);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);
  paddle::dialect::IrTensor dense_out;
  paddle::dialect::IrMetaTensor meta_out(&dense_out);

  phi::UnchangedInferMeta(meta_x, &meta_out);

  std::vector<pir::Type> argument_outputs;
  pir::Type out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_out.dtype()), dense_out.dims(), dense_out.layout(), dense_out.lod(), dense_out.offset());
  argument_outputs.push_back(out_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void NpuIdentityOp::VerifySig() {
  VLOG(4) << "Start Verifying inputs, outputs and attributes for: NpuIdentityOp.";
  VLOG(4) << "Verifying inputs:";
  {
  auto input_size = num_operands();
  IR_ENFORCE(input_size == 1u,
                    "The size %d of inputs must be equal to 1.", input_size);
  IR_ENFORCE((*this)->operand_source(0).type().isa<paddle::dialect::DenseTensorType>(),
                  "Type validation failed for the 0th input, got %s.", (*this)->operand_source(0).type());
  }
  VLOG(4) << "Verifying attributes:";
  {
  auto& attributes = this->attributes();
  IR_ENFORCE(attributes.count("format")>0,
                 "format does not exist.");
  IR_ENFORCE(attributes.at("format").isa<pir::Int32Attribute>(),
                 "Type of attribute: format is not pir::Int32Attribute.");

  }
  VLOG(4) << "Verifying outputs:";
  {
  auto output_size = num_results();
  IR_ENFORCE(output_size == 1u,
                    "The size %d of outputs must be equal to 1.", output_size);
  IR_ENFORCE((*this)->result(0).type().isa<paddle::dialect::DenseTensorType>(),
                 "Type validation failed for the 0th output.");
  }
  VLOG(4) << "End Verifying for: NpuIdentityOp.";
}

void NpuIdentityOp::InferMeta( phi::InferMetaContext *infer_meta ) {
  auto fn = PD_INFER_META(phi::UnchangedInferMeta);
  fn(infer_meta);
}

phi::DataType NpuIdentityOp::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: NpuIdentityOp";
  


  return expected_kernel_dtype;
}

OpInfoTuple NumelOp::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("x", "paddle::dialect::DenseTensorType", false, true, false, false) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = {  };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("size", "paddle::dialect::DenseTensorType", false, false) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("NumelInferMeta", {"x"}, "numel", {"x"}, {"x"}, {}, {}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "numel");
}

void NumelOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_) {
  VLOG(4) << "Start build NumelOp";



  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);
  paddle::dialect::IrTensor dense_size;
  paddle::dialect::IrMetaTensor meta_size(&dense_size);

  phi::NumelInferMeta(meta_x, &meta_size);

  std::vector<pir::Type> argument_outputs;
  pir::Type size_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_size.dtype()), dense_size.dims(), dense_size.layout(), dense_size.lod(), dense_size.offset());
  argument_outputs.push_back(size_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void NumelOp::VerifySig() {
  VLOG(4) << "Start Verifying inputs, outputs and attributes for: NumelOp.";
  VLOG(4) << "Verifying inputs:";
  {
  auto input_size = num_operands();
  IR_ENFORCE(input_size == 1u,
                    "The size %d of inputs must be equal to 1.", input_size);
  IR_ENFORCE((*this)->operand_source(0).type().isa<paddle::dialect::DenseTensorType>(),
                  "Type validation failed for the 0th input, got %s.", (*this)->operand_source(0).type());
  }
  VLOG(4) << "Verifying attributes:";
  {
  // Attributes num is 0, not need to check attributes type.
  }
  VLOG(4) << "Verifying outputs:";
  {
  auto output_size = num_results();
  IR_ENFORCE(output_size == 1u,
                    "The size %d of outputs must be equal to 1.", output_size);
  IR_ENFORCE((*this)->result(0).type().isa<paddle::dialect::DenseTensorType>(),
                 "Type validation failed for the 0th output.");
  }
  VLOG(4) << "End Verifying for: NumelOp.";
}

void NumelOp::InferMeta( phi::InferMetaContext *infer_meta ) {
  auto fn = PD_INFER_META(phi::NumelInferMeta);
  fn(infer_meta);
}

phi::DataType NumelOp::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: NumelOp";
  

  // deal skip data transform
  if (var_name == "x"){
    return expected_kernel_dtype;
  }


  return expected_kernel_dtype;
}

const char *OverlapAddOp::attributes_name[2] = { "hop_length", "axis" };

OpInfoTuple OverlapAddOp::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("x", "paddle::dialect::DenseTensorType", false, false, false, true) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = { paddle::dialect::OpAttributeInfo("hop_length", "pir::Int32Attribute", ""), paddle::dialect::OpAttributeInfo("axis", "pir::Int32Attribute", "") };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("out", "paddle::dialect::DenseTensorType", false, false) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("OverlapAddInferMeta", {"x", "hop_length", "axis"}, "overlap_add", {"x", "hop_length", "axis"}, {"x"}, {}, {}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "overlap_add");
}

void OverlapAddOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, int hop_length, int axis) {
  VLOG(4) << "Start build OverlapAddOp";



  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_hop_length = pir::Int32Attribute::get(pir::IrContext::Instance(), hop_length);
  argument.AddAttribute("hop_length", attr_hop_length);
  pir::Attribute attr_axis = pir::Int32Attribute::get(pir::IrContext::Instance(), axis);
  argument.AddAttribute("axis", attr_axis);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);
  paddle::dialect::IrTensor dense_out;
  paddle::dialect::IrMetaTensor meta_out(&dense_out);

  phi::OverlapAddInferMeta(meta_x, hop_length, axis, &meta_out);

  std::vector<pir::Type> argument_outputs;
  pir::Type out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_out.dtype()), dense_out.dims(), dense_out.layout(), dense_out.lod(), dense_out.offset());
  argument_outputs.push_back(out_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void OverlapAddOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, pir::AttributeMap attributes) {
  VLOG(4) << "Start build OverlapAddOp";


  IR_ENFORCE(
      attributes.find("hop_length") != attributes.end(),
          "'hop_length' Attribute is expected for OverlapAddOp. ");
  int hop_length = attributes.at("hop_length").dyn_cast<pir::Int32Attribute>().data();

  IR_ENFORCE(
      attributes.find("axis") != attributes.end(),
          "'axis' Attribute is expected for OverlapAddOp. ");
  int axis = attributes.at("axis").dyn_cast<pir::Int32Attribute>().data();


  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_hop_length = pir::Int32Attribute::get(pir::IrContext::Instance(), hop_length);
  argument.AddAttribute("hop_length", attr_hop_length);
  pir::Attribute attr_axis = pir::Int32Attribute::get(pir::IrContext::Instance(), axis);
  argument.AddAttribute("axis", attr_axis);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);
  paddle::dialect::IrTensor dense_out;
  paddle::dialect::IrMetaTensor meta_out(&dense_out);

  phi::OverlapAddInferMeta(meta_x, hop_length, axis, &meta_out);

  std::vector<pir::Type> argument_outputs;
  pir::Type out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_out.dtype()), dense_out.dims(), dense_out.layout(), dense_out.lod(), dense_out.offset());
  argument_outputs.push_back(out_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void OverlapAddOp::VerifySig() {
  VLOG(4) << "Start Verifying inputs, outputs and attributes for: OverlapAddOp.";
  VLOG(4) << "Verifying inputs:";
  {
  auto input_size = num_operands();
  IR_ENFORCE(input_size == 1u,
                    "The size %d of inputs must be equal to 1.", input_size);
  IR_ENFORCE((*this)->operand_source(0).type().isa<paddle::dialect::DenseTensorType>(),
                  "Type validation failed for the 0th input, got %s.", (*this)->operand_source(0).type());
  }
  VLOG(4) << "Verifying attributes:";
  {
  auto& attributes = this->attributes();
  IR_ENFORCE(attributes.count("hop_length")>0,
                 "hop_length does not exist.");
  IR_ENFORCE(attributes.at("hop_length").isa<pir::Int32Attribute>(),
                 "Type of attribute: hop_length is not pir::Int32Attribute.");

  IR_ENFORCE(attributes.count("axis")>0,
                 "axis does not exist.");
  IR_ENFORCE(attributes.at("axis").isa<pir::Int32Attribute>(),
                 "Type of attribute: axis is not pir::Int32Attribute.");

  }
  VLOG(4) << "Verifying outputs:";
  {
  auto output_size = num_results();
  IR_ENFORCE(output_size == 1u,
                    "The size %d of outputs must be equal to 1.", output_size);
  IR_ENFORCE((*this)->result(0).type().isa<paddle::dialect::DenseTensorType>(),
                 "Type validation failed for the 0th output.");
  }
  VLOG(4) << "End Verifying for: OverlapAddOp.";
}

void OverlapAddOp::InferMeta( phi::InferMetaContext *infer_meta ) {
  auto fn = PD_INFER_META(phi::OverlapAddInferMeta);
  fn(infer_meta);
}

phi::DataType OverlapAddOp::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: OverlapAddOp";
  


  return expected_kernel_dtype;
}

const char *PNormOp::attributes_name[5] = { "porder", "axis", "epsilon", "keepdim", "asvector" };

OpInfoTuple PNormOp::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("x", "paddle::dialect::DenseTensorType", false, false, false, true) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = { paddle::dialect::OpAttributeInfo("porder", "pir::FloatAttribute", ""), paddle::dialect::OpAttributeInfo("axis", "pir::Int32Attribute", ""), paddle::dialect::OpAttributeInfo("epsilon", "pir::FloatAttribute", ""), paddle::dialect::OpAttributeInfo("keepdim", "pir::BoolAttribute", ""), paddle::dialect::OpAttributeInfo("asvector", "pir::BoolAttribute", "") };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("out", "paddle::dialect::DenseTensorType", false, false) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("PNormInferMeta", {"x", "porder", "axis", "epsilon", "keepdim", "asvector"}, "p_norm", {"x", "porder", "axis", "epsilon", "keepdim", "asvector"}, {}, {}, {}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "p_norm");
}

void PNormOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, float porder, int axis, float epsilon, bool keepdim, bool asvector) {
  VLOG(4) << "Start build PNormOp";



  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_porder = pir::FloatAttribute::get(pir::IrContext::Instance(), porder);
  argument.AddAttribute("porder", attr_porder);
  pir::Attribute attr_axis = pir::Int32Attribute::get(pir::IrContext::Instance(), axis);
  argument.AddAttribute("axis", attr_axis);
  pir::Attribute attr_epsilon = pir::FloatAttribute::get(pir::IrContext::Instance(), epsilon);
  argument.AddAttribute("epsilon", attr_epsilon);
  pir::Attribute attr_keepdim = pir::BoolAttribute::get(pir::IrContext::Instance(), keepdim);
  argument.AddAttribute("keepdim", attr_keepdim);
  pir::Attribute attr_asvector = pir::BoolAttribute::get(pir::IrContext::Instance(), asvector);
  argument.AddAttribute("asvector", attr_asvector);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);
  paddle::dialect::IrTensor dense_out;
  paddle::dialect::IrMetaTensor meta_out(&dense_out);

  phi::PNormInferMeta(meta_x, porder, axis, epsilon, keepdim, asvector, &meta_out);

  std::vector<pir::Type> argument_outputs;
  pir::Type out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_out.dtype()), dense_out.dims(), dense_out.layout(), dense_out.lod(), dense_out.offset());
  argument_outputs.push_back(out_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void PNormOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, pir::AttributeMap attributes) {
  VLOG(4) << "Start build PNormOp";


  IR_ENFORCE(
      attributes.find("porder") != attributes.end(),
          "'porder' Attribute is expected for PNormOp. ");
  float porder = attributes.at("porder").dyn_cast<pir::FloatAttribute>().data();

  IR_ENFORCE(
      attributes.find("axis") != attributes.end(),
          "'axis' Attribute is expected for PNormOp. ");
  int axis = attributes.at("axis").dyn_cast<pir::Int32Attribute>().data();

  IR_ENFORCE(
      attributes.find("epsilon") != attributes.end(),
          "'epsilon' Attribute is expected for PNormOp. ");
  float epsilon = attributes.at("epsilon").dyn_cast<pir::FloatAttribute>().data();

  IR_ENFORCE(
      attributes.find("keepdim") != attributes.end(),
          "'keepdim' Attribute is expected for PNormOp. ");
  bool keepdim = attributes.at("keepdim").dyn_cast<pir::BoolAttribute>().data();

  IR_ENFORCE(
      attributes.find("asvector") != attributes.end(),
          "'asvector' Attribute is expected for PNormOp. ");
  bool asvector = attributes.at("asvector").dyn_cast<pir::BoolAttribute>().data();


  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_porder = pir::FloatAttribute::get(pir::IrContext::Instance(), porder);
  argument.AddAttribute("porder", attr_porder);
  pir::Attribute attr_axis = pir::Int32Attribute::get(pir::IrContext::Instance(), axis);
  argument.AddAttribute("axis", attr_axis);
  pir::Attribute attr_epsilon = pir::FloatAttribute::get(pir::IrContext::Instance(), epsilon);
  argument.AddAttribute("epsilon", attr_epsilon);
  pir::Attribute attr_keepdim = pir::BoolAttribute::get(pir::IrContext::Instance(), keepdim);
  argument.AddAttribute("keepdim", attr_keepdim);
  pir::Attribute attr_asvector = pir::BoolAttribute::get(pir::IrContext::Instance(), asvector);
  argument.AddAttribute("asvector", attr_asvector);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);
  paddle::dialect::IrTensor dense_out;
  paddle::dialect::IrMetaTensor meta_out(&dense_out);

  phi::PNormInferMeta(meta_x, porder, axis, epsilon, keepdim, asvector, &meta_out);

  std::vector<pir::Type> argument_outputs;
  pir::Type out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_out.dtype()), dense_out.dims(), dense_out.layout(), dense_out.lod(), dense_out.offset());
  argument_outputs.push_back(out_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void PNormOp::VerifySig() {
  VLOG(4) << "Start Verifying inputs, outputs and attributes for: PNormOp.";
  VLOG(4) << "Verifying inputs:";
  {
  auto input_size = num_operands();
  IR_ENFORCE(input_size == 1u,
                    "The size %d of inputs must be equal to 1.", input_size);
  IR_ENFORCE((*this)->operand_source(0).type().isa<paddle::dialect::DenseTensorType>(),
                  "Type validation failed for the 0th input, got %s.", (*this)->operand_source(0).type());
  }
  VLOG(4) << "Verifying attributes:";
  {
  auto& attributes = this->attributes();
  IR_ENFORCE(attributes.count("porder")>0,
                 "porder does not exist.");
  IR_ENFORCE(attributes.at("porder").isa<pir::FloatAttribute>(),
                 "Type of attribute: porder is not pir::FloatAttribute.");

  IR_ENFORCE(attributes.count("axis")>0,
                 "axis does not exist.");
  IR_ENFORCE(attributes.at("axis").isa<pir::Int32Attribute>(),
                 "Type of attribute: axis is not pir::Int32Attribute.");

  IR_ENFORCE(attributes.count("epsilon")>0,
                 "epsilon does not exist.");
  IR_ENFORCE(attributes.at("epsilon").isa<pir::FloatAttribute>(),
                 "Type of attribute: epsilon is not pir::FloatAttribute.");

  IR_ENFORCE(attributes.count("keepdim")>0,
                 "keepdim does not exist.");
  IR_ENFORCE(attributes.at("keepdim").isa<pir::BoolAttribute>(),
                 "Type of attribute: keepdim is not pir::BoolAttribute.");

  IR_ENFORCE(attributes.count("asvector")>0,
                 "asvector does not exist.");
  IR_ENFORCE(attributes.at("asvector").isa<pir::BoolAttribute>(),
                 "Type of attribute: asvector is not pir::BoolAttribute.");

  }
  VLOG(4) << "Verifying outputs:";
  {
  auto output_size = num_results();
  IR_ENFORCE(output_size == 1u,
                    "The size %d of outputs must be equal to 1.", output_size);
  IR_ENFORCE((*this)->result(0).type().isa<paddle::dialect::DenseTensorType>(),
                 "Type validation failed for the 0th output.");
  }
  VLOG(4) << "End Verifying for: PNormOp.";
}

void PNormOp::InferMeta( phi::InferMetaContext *infer_meta ) {
  auto fn = PD_INFER_META(phi::PNormInferMeta);
  fn(infer_meta);
}

phi::DataType PNormOp::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: PNormOp";
  


  return expected_kernel_dtype;
}

const char *Pad3dOp::attributes_name[3] = { "mode", "pad_value", "data_format" };

OpInfoTuple Pad3dOp::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("x", "paddle::dialect::DenseTensorType", false, false, false, true), paddle::dialect::OpInputInfo("paddings", "paddle::dialect::IntArrayAttribute", false, false, true, false) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = { paddle::dialect::OpAttributeInfo("mode", "pir::StrAttribute", ""), paddle::dialect::OpAttributeInfo("pad_value", "pir::FloatAttribute", ""), paddle::dialect::OpAttributeInfo("data_format", "pir::StrAttribute", "") };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("out", "paddle::dialect::DenseTensorType", false, false) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("Pad3dInferMeta", {"x", "paddings", "mode", "pad_value", "data_format"}, "pad3d", {"x", "paddings", "mode", "pad_value", "data_format"}, {}, {}, {}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "pad3d");
}

void Pad3dOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, const std::vector<int64_t>& paddings, const std::string& mode, float pad_value, const std::string& data_format) {
  VLOG(4) << "Start build Pad3dOp";


  // Generate int_array mutable attribute: paddings
  paddle::dialect::FullIntArrayOp full_paddings_op = builder.Build<paddle::dialect::FullIntArrayOp>(paddings, phi::DataType::INT64, phi::CPUPlace());
  pir::OpResult paddings_ = full_paddings_op->result(0);
    
  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_, paddings_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_mode = pir::StrAttribute::get(pir::IrContext::Instance(), mode);
  argument.AddAttribute("mode", attr_mode);
  pir::Attribute attr_pad_value = pir::FloatAttribute::get(pir::IrContext::Instance(), pad_value);
  argument.AddAttribute("pad_value", attr_pad_value);
  pir::Attribute attr_data_format = pir::StrAttribute::get(pir::IrContext::Instance(), data_format);
  argument.AddAttribute("data_format", attr_data_format);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);
  paddle::dialect::IrTensor dense_out;
  paddle::dialect::IrMetaTensor meta_out(&dense_out);

  phi::Pad3dInferMeta(meta_x, paddings, mode, pad_value, data_format, &meta_out);

  std::vector<pir::Type> argument_outputs;
  pir::Type out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_out.dtype()), dense_out.dims(), dense_out.layout(), dense_out.lod(), dense_out.offset());
  argument_outputs.push_back(out_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void Pad3dOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, pir::AttributeMap attributes) {
  VLOG(4) << "Start build Pad3dOp";


  IR_ENFORCE(
      attributes.find("paddings") != attributes.end(),
          "'paddings' Attribute is expected for Pad3dOp. ");
  std::vector<int64_t> paddings = attributes.at("paddings").dyn_cast<paddle::dialect::IntArrayAttribute>().data().GetData();

  IR_ENFORCE(
      attributes.find("mode") != attributes.end(),
          "'mode' Attribute is expected for Pad3dOp. ");
  std::string mode = attributes.at("mode").dyn_cast<pir::StrAttribute>().AsString();

  IR_ENFORCE(
      attributes.find("pad_value") != attributes.end(),
          "'pad_value' Attribute is expected for Pad3dOp. ");
  float pad_value = attributes.at("pad_value").dyn_cast<pir::FloatAttribute>().data();

  IR_ENFORCE(
      attributes.find("data_format") != attributes.end(),
          "'data_format' Attribute is expected for Pad3dOp. ");
  std::string data_format = attributes.at("data_format").dyn_cast<pir::StrAttribute>().AsString();

  // Generate int_array mutable attribute: paddings
  paddle::dialect::FullIntArrayOp full_paddings_op = builder.Build<paddle::dialect::FullIntArrayOp>(paddings, phi::DataType::INT64, phi::CPUPlace());
  pir::OpResult paddings_ = full_paddings_op->result(0);
    
  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_, paddings_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_mode = pir::StrAttribute::get(pir::IrContext::Instance(), mode);
  argument.AddAttribute("mode", attr_mode);
  pir::Attribute attr_pad_value = pir::FloatAttribute::get(pir::IrContext::Instance(), pad_value);
  argument.AddAttribute("pad_value", attr_pad_value);
  pir::Attribute attr_data_format = pir::StrAttribute::get(pir::IrContext::Instance(), data_format);
  argument.AddAttribute("data_format", attr_data_format);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);
  paddle::dialect::IrTensor dense_out;
  paddle::dialect::IrMetaTensor meta_out(&dense_out);

  phi::Pad3dInferMeta(meta_x, paddings, mode, pad_value, data_format, &meta_out);

  std::vector<pir::Type> argument_outputs;
  pir::Type out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_out.dtype()), dense_out.dims(), dense_out.layout(), dense_out.lod(), dense_out.offset());
  argument_outputs.push_back(out_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void Pad3dOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, pir::Value paddings_, const std::string& mode, float pad_value, const std::string& data_format) {
  VLOG(4) << "Start build Pad3dOp";



  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_, paddings_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_mode = pir::StrAttribute::get(pir::IrContext::Instance(), mode);
  argument.AddAttribute("mode", attr_mode);
  pir::Attribute attr_pad_value = pir::FloatAttribute::get(pir::IrContext::Instance(), pad_value);
  argument.AddAttribute("pad_value", attr_pad_value);
  pir::Attribute attr_data_format = pir::StrAttribute::get(pir::IrContext::Instance(), data_format);
  argument.AddAttribute("data_format", attr_data_format);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;
  phi::IntArray paddings;
  if (paddings_.dyn_cast<pir::OpResult>() && paddings_.dyn_cast<pir::OpResult>().owner()->isa<paddle::dialect::FullIntArrayOp>()) {
    paddings = std::move(phi::IntArray(paddle::dialect::GetInt64Vector(
                          paddings_.dyn_cast<pir::OpResult>().owner()
                          ->dyn_cast<paddle::dialect::FullIntArrayOp>()
                          .attribute("value"))));
  } else if (paddings_.type().isa<pir::VectorType>()) {
    size_t paddings_size = paddings_.type().dyn_cast<pir::VectorType>().size();
    paddings = std::move(phi::IntArray(std::vector<int64_t>(paddings_size, -1)));
    paddings.SetFromTensor(true);
  } else if (paddings_.type().isa<paddle::dialect::DenseTensorType>()) {
    common::DDim paddings_dim = paddings_.type().dyn_cast<paddle::dialect::DenseTensorType>().dims();
    size_t paddings_size = common::product(paddings_dim);
    if (common::contain_unknown_dim(paddings_dim)) {
      paddings_size = 1;
    }
    paddings = std::move(phi::IntArray(std::vector<int64_t>(paddings_size, -1)));
    paddings.SetFromTensor(true);
  } else {
    PADDLE_THROW(phi::errors::Unimplemented("Only support VectorType or DenseTensorType"));
  }


  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);
  paddle::dialect::IrTensor dense_out;
  paddle::dialect::IrMetaTensor meta_out(&dense_out);

  phi::Pad3dInferMeta(meta_x, paddings, mode, pad_value, data_format, &meta_out);

  std::vector<pir::Type> argument_outputs;
  pir::Type out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_out.dtype()), dense_out.dims(), dense_out.layout(), dense_out.lod(), dense_out.offset());
  argument_outputs.push_back(out_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void Pad3dOp::VerifySig() {
  VLOG(4) << "Start Verifying inputs, outputs and attributes for: Pad3dOp.";
  VLOG(4) << "Verifying inputs:";
  {
  auto input_size = num_operands();
  IR_ENFORCE(input_size == 2u,
                    "The size %d of inputs must be equal to 2.", input_size);
  IR_ENFORCE((*this)->operand_source(0).type().isa<paddle::dialect::DenseTensorType>(),
                  "Type validation failed for the 0th input, got %s.", (*this)->operand_source(0).type());
  if (auto vec_type = (*this)->operand_source(1).type().dyn_cast<pir::VectorType>()) {
      for (size_t i = 0; i < vec_type.size(); ++i) {
        IR_ENFORCE(vec_type[i].isa<paddle::dialect::DenseTensorType>(),
                       "Type validation failed for the 1th input, got %s.", (*this)->operand_source(1).type());
      }
  }
  else {
    IR_ENFORCE((*this)->operand_source(1).type().isa<paddle::dialect::DenseTensorType>(),
                   "Type validation failed for the 1th input, got %s.", (*this)->operand_source(1).type());
  }
  }
  VLOG(4) << "Verifying attributes:";
  {
  auto& attributes = this->attributes();
  IR_ENFORCE(attributes.count("mode")>0,
                 "mode does not exist.");
  IR_ENFORCE(attributes.at("mode").isa<pir::StrAttribute>(),
                 "Type of attribute: mode is not pir::StrAttribute.");

  IR_ENFORCE(attributes.count("pad_value")>0,
                 "pad_value does not exist.");
  IR_ENFORCE(attributes.at("pad_value").isa<pir::FloatAttribute>(),
                 "Type of attribute: pad_value is not pir::FloatAttribute.");

  IR_ENFORCE(attributes.count("data_format")>0,
                 "data_format does not exist.");
  IR_ENFORCE(attributes.at("data_format").isa<pir::StrAttribute>(),
                 "Type of attribute: data_format is not pir::StrAttribute.");

  }
  VLOG(4) << "Verifying outputs:";
  {
  auto output_size = num_results();
  IR_ENFORCE(output_size == 1u,
                    "The size %d of outputs must be equal to 1.", output_size);
  IR_ENFORCE((*this)->result(0).type().isa<paddle::dialect::DenseTensorType>(),
                 "Type validation failed for the 0th output.");
  }
  VLOG(4) << "End Verifying for: Pad3dOp.";
}

void Pad3dOp::InferMeta( phi::InferMetaContext *infer_meta ) {
  auto fn = PD_INFER_META(phi::Pad3dInferMeta);
  fn(infer_meta);
}

phi::DataType Pad3dOp::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: Pad3dOp";
  


  return expected_kernel_dtype;
}

const char *PixelShuffleOp::attributes_name[2] = { "upscale_factor", "data_format" };

OpInfoTuple PixelShuffleOp::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("x", "paddle::dialect::DenseTensorType", false, false, false, true) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = { paddle::dialect::OpAttributeInfo("upscale_factor", "pir::Int32Attribute", ""), paddle::dialect::OpAttributeInfo("data_format", "pir::StrAttribute", "") };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("out", "paddle::dialect::DenseTensorType", false, false) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("PixelShuffleInferMeta", {"x", "upscale_factor", "data_format"}, "pixel_shuffle", {"x", "upscale_factor", "data_format"}, {}, {}, {}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "pixel_shuffle");
}

void PixelShuffleOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, int upscale_factor, const std::string& data_format) {
  VLOG(4) << "Start build PixelShuffleOp";



  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_upscale_factor = pir::Int32Attribute::get(pir::IrContext::Instance(), upscale_factor);
  argument.AddAttribute("upscale_factor", attr_upscale_factor);
  pir::Attribute attr_data_format = pir::StrAttribute::get(pir::IrContext::Instance(), data_format);
  argument.AddAttribute("data_format", attr_data_format);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);
  paddle::dialect::IrTensor dense_out;
  paddle::dialect::IrMetaTensor meta_out(&dense_out);

  phi::PixelShuffleInferMeta(meta_x, upscale_factor, data_format, &meta_out);

  std::vector<pir::Type> argument_outputs;
  pir::Type out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_out.dtype()), dense_out.dims(), dense_out.layout(), dense_out.lod(), dense_out.offset());
  argument_outputs.push_back(out_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void PixelShuffleOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, pir::AttributeMap attributes) {
  VLOG(4) << "Start build PixelShuffleOp";


  IR_ENFORCE(
      attributes.find("upscale_factor") != attributes.end(),
          "'upscale_factor' Attribute is expected for PixelShuffleOp. ");
  int upscale_factor = attributes.at("upscale_factor").dyn_cast<pir::Int32Attribute>().data();

  IR_ENFORCE(
      attributes.find("data_format") != attributes.end(),
          "'data_format' Attribute is expected for PixelShuffleOp. ");
  std::string data_format = attributes.at("data_format").dyn_cast<pir::StrAttribute>().AsString();


  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_upscale_factor = pir::Int32Attribute::get(pir::IrContext::Instance(), upscale_factor);
  argument.AddAttribute("upscale_factor", attr_upscale_factor);
  pir::Attribute attr_data_format = pir::StrAttribute::get(pir::IrContext::Instance(), data_format);
  argument.AddAttribute("data_format", attr_data_format);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);
  paddle::dialect::IrTensor dense_out;
  paddle::dialect::IrMetaTensor meta_out(&dense_out);

  phi::PixelShuffleInferMeta(meta_x, upscale_factor, data_format, &meta_out);

  std::vector<pir::Type> argument_outputs;
  pir::Type out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_out.dtype()), dense_out.dims(), dense_out.layout(), dense_out.lod(), dense_out.offset());
  argument_outputs.push_back(out_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void PixelShuffleOp::VerifySig() {
  VLOG(4) << "Start Verifying inputs, outputs and attributes for: PixelShuffleOp.";
  VLOG(4) << "Verifying inputs:";
  {
  auto input_size = num_operands();
  IR_ENFORCE(input_size == 1u,
                    "The size %d of inputs must be equal to 1.", input_size);
  IR_ENFORCE((*this)->operand_source(0).type().isa<paddle::dialect::DenseTensorType>(),
                  "Type validation failed for the 0th input, got %s.", (*this)->operand_source(0).type());
  }
  VLOG(4) << "Verifying attributes:";
  {
  auto& attributes = this->attributes();
  IR_ENFORCE(attributes.count("upscale_factor")>0,
                 "upscale_factor does not exist.");
  IR_ENFORCE(attributes.at("upscale_factor").isa<pir::Int32Attribute>(),
                 "Type of attribute: upscale_factor is not pir::Int32Attribute.");

  IR_ENFORCE(attributes.count("data_format")>0,
                 "data_format does not exist.");
  IR_ENFORCE(attributes.at("data_format").isa<pir::StrAttribute>(),
                 "Type of attribute: data_format is not pir::StrAttribute.");

  }
  VLOG(4) << "Verifying outputs:";
  {
  auto output_size = num_results();
  IR_ENFORCE(output_size == 1u,
                    "The size %d of outputs must be equal to 1.", output_size);
  IR_ENFORCE((*this)->result(0).type().isa<paddle::dialect::DenseTensorType>(),
                 "Type validation failed for the 0th output.");
  }
  VLOG(4) << "End Verifying for: PixelShuffleOp.";
}

void PixelShuffleOp::InferMeta( phi::InferMetaContext *infer_meta ) {
  auto fn = PD_INFER_META(phi::PixelShuffleInferMeta);
  fn(infer_meta);
}

phi::DataType PixelShuffleOp::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: PixelShuffleOp";
  


  return expected_kernel_dtype;
}

const char *PixelUnshuffleOp::attributes_name[2] = { "downscale_factor", "data_format" };

OpInfoTuple PixelUnshuffleOp::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("x", "paddle::dialect::DenseTensorType", false, false, false, true) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = { paddle::dialect::OpAttributeInfo("downscale_factor", "pir::Int32Attribute", ""), paddle::dialect::OpAttributeInfo("data_format", "pir::StrAttribute", "") };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("out", "paddle::dialect::DenseTensorType", false, false) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("PixelUnshuffleInferMeta", {"x", "downscale_factor", "data_format"}, "pixel_unshuffle", {"x", "downscale_factor", "data_format"}, {}, {}, {}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "pixel_unshuffle");
}

void PixelUnshuffleOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, int downscale_factor, const std::string& data_format) {
  VLOG(4) << "Start build PixelUnshuffleOp";



  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_downscale_factor = pir::Int32Attribute::get(pir::IrContext::Instance(), downscale_factor);
  argument.AddAttribute("downscale_factor", attr_downscale_factor);
  pir::Attribute attr_data_format = pir::StrAttribute::get(pir::IrContext::Instance(), data_format);
  argument.AddAttribute("data_format", attr_data_format);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);
  paddle::dialect::IrTensor dense_out;
  paddle::dialect::IrMetaTensor meta_out(&dense_out);

  phi::PixelUnshuffleInferMeta(meta_x, downscale_factor, data_format, &meta_out);

  std::vector<pir::Type> argument_outputs;
  pir::Type out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_out.dtype()), dense_out.dims(), dense_out.layout(), dense_out.lod(), dense_out.offset());
  argument_outputs.push_back(out_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void PixelUnshuffleOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, pir::AttributeMap attributes) {
  VLOG(4) << "Start build PixelUnshuffleOp";


  IR_ENFORCE(
      attributes.find("downscale_factor") != attributes.end(),
          "'downscale_factor' Attribute is expected for PixelUnshuffleOp. ");
  int downscale_factor = attributes.at("downscale_factor").dyn_cast<pir::Int32Attribute>().data();

  IR_ENFORCE(
      attributes.find("data_format") != attributes.end(),
          "'data_format' Attribute is expected for PixelUnshuffleOp. ");
  std::string data_format = attributes.at("data_format").dyn_cast<pir::StrAttribute>().AsString();


  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_downscale_factor = pir::Int32Attribute::get(pir::IrContext::Instance(), downscale_factor);
  argument.AddAttribute("downscale_factor", attr_downscale_factor);
  pir::Attribute attr_data_format = pir::StrAttribute::get(pir::IrContext::Instance(), data_format);
  argument.AddAttribute("data_format", attr_data_format);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);
  paddle::dialect::IrTensor dense_out;
  paddle::dialect::IrMetaTensor meta_out(&dense_out);

  phi::PixelUnshuffleInferMeta(meta_x, downscale_factor, data_format, &meta_out);

  std::vector<pir::Type> argument_outputs;
  pir::Type out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_out.dtype()), dense_out.dims(), dense_out.layout(), dense_out.lod(), dense_out.offset());
  argument_outputs.push_back(out_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void PixelUnshuffleOp::VerifySig() {
  VLOG(4) << "Start Verifying inputs, outputs and attributes for: PixelUnshuffleOp.";
  VLOG(4) << "Verifying inputs:";
  {
  auto input_size = num_operands();
  IR_ENFORCE(input_size == 1u,
                    "The size %d of inputs must be equal to 1.", input_size);
  IR_ENFORCE((*this)->operand_source(0).type().isa<paddle::dialect::DenseTensorType>(),
                  "Type validation failed for the 0th input, got %s.", (*this)->operand_source(0).type());
  }
  VLOG(4) << "Verifying attributes:";
  {
  auto& attributes = this->attributes();
  IR_ENFORCE(attributes.count("downscale_factor")>0,
                 "downscale_factor does not exist.");
  IR_ENFORCE(attributes.at("downscale_factor").isa<pir::Int32Attribute>(),
                 "Type of attribute: downscale_factor is not pir::Int32Attribute.");

  IR_ENFORCE(attributes.count("data_format")>0,
                 "data_format does not exist.");
  IR_ENFORCE(attributes.at("data_format").isa<pir::StrAttribute>(),
                 "Type of attribute: data_format is not pir::StrAttribute.");

  }
  VLOG(4) << "Verifying outputs:";
  {
  auto output_size = num_results();
  IR_ENFORCE(output_size == 1u,
                    "The size %d of outputs must be equal to 1.", output_size);
  IR_ENFORCE((*this)->result(0).type().isa<paddle::dialect::DenseTensorType>(),
                 "Type validation failed for the 0th output.");
  }
  VLOG(4) << "End Verifying for: PixelUnshuffleOp.";
}

void PixelUnshuffleOp::InferMeta( phi::InferMetaContext *infer_meta ) {
  auto fn = PD_INFER_META(phi::PixelUnshuffleInferMeta);
  fn(infer_meta);
}

phi::DataType PixelUnshuffleOp::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: PixelUnshuffleOp";
  


  return expected_kernel_dtype;
}

OpInfoTuple PoissonOp::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("x", "paddle::dialect::DenseTensorType", false, false, false, true) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = {  };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("out", "paddle::dialect::DenseTensorType", false, false) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("UnchangedInferMeta", {"x"}, "poisson", {"x"}, {}, {}, {}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "poisson");
}

void PoissonOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_) {
  VLOG(4) << "Start build PoissonOp";



  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);
  paddle::dialect::IrTensor dense_out;
  paddle::dialect::IrMetaTensor meta_out(&dense_out);

  phi::UnchangedInferMeta(meta_x, &meta_out);

  std::vector<pir::Type> argument_outputs;
  pir::Type out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_out.dtype()), dense_out.dims(), dense_out.layout(), dense_out.lod(), dense_out.offset());
  argument_outputs.push_back(out_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void PoissonOp::VerifySig() {
  VLOG(4) << "Start Verifying inputs, outputs and attributes for: PoissonOp.";
  VLOG(4) << "Verifying inputs:";
  {
  auto input_size = num_operands();
  IR_ENFORCE(input_size == 1u,
                    "The size %d of inputs must be equal to 1.", input_size);
  IR_ENFORCE((*this)->operand_source(0).type().isa<paddle::dialect::DenseTensorType>(),
                  "Type validation failed for the 0th input, got %s.", (*this)->operand_source(0).type());
  }
  VLOG(4) << "Verifying attributes:";
  {
  // Attributes num is 0, not need to check attributes type.
  }
  VLOG(4) << "Verifying outputs:";
  {
  auto output_size = num_results();
  IR_ENFORCE(output_size == 1u,
                    "The size %d of outputs must be equal to 1.", output_size);
  IR_ENFORCE((*this)->result(0).type().isa<paddle::dialect::DenseTensorType>(),
                 "Type validation failed for the 0th output.");
  }
  VLOG(4) << "End Verifying for: PoissonOp.";
}

void PoissonOp::InferMeta( phi::InferMetaContext *infer_meta ) {
  auto fn = PD_INFER_META(phi::UnchangedInferMeta);
  fn(infer_meta);
}

phi::DataType PoissonOp::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: PoissonOp";
  


  return expected_kernel_dtype;
}

const char *PolygammaOp::attributes_name[1] = { "n" };

OpInfoTuple PolygammaOp::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("x", "paddle::dialect::DenseTensorType", false, false, false, true) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = { paddle::dialect::OpAttributeInfo("n", "pir::Int32Attribute", "") };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("out", "paddle::dialect::DenseTensorType", false, false) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("UnchangedInferMeta", {"x"}, "polygamma", {"x", "n"}, {}, {}, {}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "polygamma");
}

void PolygammaOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, int n) {
  VLOG(4) << "Start build PolygammaOp";



  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_n = pir::Int32Attribute::get(pir::IrContext::Instance(), n);
  argument.AddAttribute("n", attr_n);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);
  paddle::dialect::IrTensor dense_out;
  paddle::dialect::IrMetaTensor meta_out(&dense_out);

  phi::UnchangedInferMeta(meta_x, &meta_out);

  std::vector<pir::Type> argument_outputs;
  pir::Type out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_out.dtype()), dense_out.dims(), dense_out.layout(), dense_out.lod(), dense_out.offset());
  argument_outputs.push_back(out_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void PolygammaOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, pir::AttributeMap attributes) {
  VLOG(4) << "Start build PolygammaOp";


  IR_ENFORCE(
      attributes.find("n") != attributes.end(),
          "'n' Attribute is expected for PolygammaOp. ");
  int n = attributes.at("n").dyn_cast<pir::Int32Attribute>().data();


  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_n = pir::Int32Attribute::get(pir::IrContext::Instance(), n);
  argument.AddAttribute("n", attr_n);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);
  paddle::dialect::IrTensor dense_out;
  paddle::dialect::IrMetaTensor meta_out(&dense_out);

  phi::UnchangedInferMeta(meta_x, &meta_out);

  std::vector<pir::Type> argument_outputs;
  pir::Type out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_out.dtype()), dense_out.dims(), dense_out.layout(), dense_out.lod(), dense_out.offset());
  argument_outputs.push_back(out_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void PolygammaOp::VerifySig() {
  VLOG(4) << "Start Verifying inputs, outputs and attributes for: PolygammaOp.";
  VLOG(4) << "Verifying inputs:";
  {
  auto input_size = num_operands();
  IR_ENFORCE(input_size == 1u,
                    "The size %d of inputs must be equal to 1.", input_size);
  IR_ENFORCE((*this)->operand_source(0).type().isa<paddle::dialect::DenseTensorType>(),
                  "Type validation failed for the 0th input, got %s.", (*this)->operand_source(0).type());
  }
  VLOG(4) << "Verifying attributes:";
  {
  auto& attributes = this->attributes();
  IR_ENFORCE(attributes.count("n")>0,
                 "n does not exist.");
  IR_ENFORCE(attributes.at("n").isa<pir::Int32Attribute>(),
                 "Type of attribute: n is not pir::Int32Attribute.");

  }
  VLOG(4) << "Verifying outputs:";
  {
  auto output_size = num_results();
  IR_ENFORCE(output_size == 1u,
                    "The size %d of outputs must be equal to 1.", output_size);
  IR_ENFORCE((*this)->result(0).type().isa<paddle::dialect::DenseTensorType>(),
                 "Type validation failed for the 0th output.");
  }
  VLOG(4) << "End Verifying for: PolygammaOp.";
}

void PolygammaOp::InferMeta( phi::InferMetaContext *infer_meta ) {
  auto fn = PD_INFER_META(phi::UnchangedInferMeta);
  fn(infer_meta);
}

phi::DataType PolygammaOp::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: PolygammaOp";
  


  return expected_kernel_dtype;
}

const char *Polygamma_Op::attributes_name[1] = { "n" };

OpInfoTuple Polygamma_Op::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("x", "paddle::dialect::DenseTensorType", false, false, false, true) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = { paddle::dialect::OpAttributeInfo("n", "pir::Int32Attribute", "") };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("out", "paddle::dialect::DenseTensorType", false, false) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("UnchangedInferMeta", {"x"}, "polygamma", {"x", "n"}, {}, {}, {{"out", "x"}}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "polygamma");
}

void Polygamma_Op::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, int n) {
  VLOG(4) << "Start build Polygamma_Op";



  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_n = pir::Int32Attribute::get(pir::IrContext::Instance(), n);
  argument.AddAttribute("n", attr_n);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);
  paddle::dialect::IrTensor dense_out;
  paddle::dialect::IrMetaTensor meta_out(&dense_out);

  phi::UnchangedInferMeta(meta_x, &meta_out);

  std::vector<pir::Type> argument_outputs;
  pir::Type out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_out.dtype()), dense_out.dims(), dense_out.layout(), dense_out.lod(), dense_out.offset());
  argument_outputs.push_back(out_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void Polygamma_Op::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, pir::AttributeMap attributes) {
  VLOG(4) << "Start build Polygamma_Op";


  IR_ENFORCE(
      attributes.find("n") != attributes.end(),
          "'n' Attribute is expected for Polygamma_Op. ");
  int n = attributes.at("n").dyn_cast<pir::Int32Attribute>().data();


  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_n = pir::Int32Attribute::get(pir::IrContext::Instance(), n);
  argument.AddAttribute("n", attr_n);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);
  paddle::dialect::IrTensor dense_out;
  paddle::dialect::IrMetaTensor meta_out(&dense_out);

  phi::UnchangedInferMeta(meta_x, &meta_out);

  std::vector<pir::Type> argument_outputs;
  pir::Type out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_out.dtype()), dense_out.dims(), dense_out.layout(), dense_out.lod(), dense_out.offset());
  argument_outputs.push_back(out_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void Polygamma_Op::VerifySig() {
  VLOG(4) << "Start Verifying inputs, outputs and attributes for: Polygamma_Op.";
  VLOG(4) << "Verifying inputs:";
  {
  auto input_size = num_operands();
  IR_ENFORCE(input_size == 1u,
                    "The size %d of inputs must be equal to 1.", input_size);
  IR_ENFORCE((*this)->operand_source(0).type().isa<paddle::dialect::DenseTensorType>(),
                  "Type validation failed for the 0th input, got %s.", (*this)->operand_source(0).type());
  }
  VLOG(4) << "Verifying attributes:";
  {
  auto& attributes = this->attributes();
  IR_ENFORCE(attributes.count("n")>0,
                 "n does not exist.");
  IR_ENFORCE(attributes.at("n").isa<pir::Int32Attribute>(),
                 "Type of attribute: n is not pir::Int32Attribute.");

  }
  VLOG(4) << "Verifying outputs:";
  {
  auto output_size = num_results();
  IR_ENFORCE(output_size == 1u,
                    "The size %d of outputs must be equal to 1.", output_size);
  IR_ENFORCE((*this)->result(0).type().isa<paddle::dialect::DenseTensorType>(),
                 "Type validation failed for the 0th output.");
  }
  VLOG(4) << "End Verifying for: Polygamma_Op.";
}

void Polygamma_Op::InferMeta( phi::InferMetaContext *infer_meta ) {
  auto fn = PD_INFER_META(phi::UnchangedInferMeta);
  fn(infer_meta);
}

phi::DataType Polygamma_Op::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: Polygamma_Op";
  


  return expected_kernel_dtype;
}

const char *PowOp::attributes_name[1] = { "y" };

OpInfoTuple PowOp::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("x", "paddle::dialect::DenseTensorType", false, false, false, true) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = { paddle::dialect::OpAttributeInfo("y", "paddle::dialect::ScalarAttribute", "float") };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("out", "paddle::dialect::DenseTensorType", false, false) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("UnchangedInferMeta", {"x"}, "pow", {"x", "y"}, {"x"}, {}, {}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "pow");
}

void PowOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, float y) {
  VLOG(4) << "Start build PowOp";



  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_y = paddle::dialect::TransToIrAttribute(y, pir::IrContext::Instance());
  argument.AddAttribute("y", attr_y);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);
  paddle::dialect::IrTensor dense_out;
  paddle::dialect::IrMetaTensor meta_out(&dense_out);

  phi::UnchangedInferMeta(meta_x, &meta_out);

  std::vector<pir::Type> argument_outputs;
  pir::Type out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_out.dtype()), dense_out.dims(), dense_out.layout(), dense_out.lod(), dense_out.offset());
  argument_outputs.push_back(out_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void PowOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, pir::AttributeMap attributes) {
  VLOG(4) << "Start build PowOp";


  IR_ENFORCE(
      attributes.find("y") != attributes.end(),
          "'y' Attribute is expected for PowOp. ");
  float y = attributes.at("y").dyn_cast<paddle::dialect::ScalarAttribute>().data().to<float>();


  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_y = paddle::dialect::TransToIrAttribute(y, pir::IrContext::Instance());
  argument.AddAttribute("y", attr_y);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);
  paddle::dialect::IrTensor dense_out;
  paddle::dialect::IrMetaTensor meta_out(&dense_out);

  phi::UnchangedInferMeta(meta_x, &meta_out);

  std::vector<pir::Type> argument_outputs;
  pir::Type out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_out.dtype()), dense_out.dims(), dense_out.layout(), dense_out.lod(), dense_out.offset());
  argument_outputs.push_back(out_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void PowOp::VerifySig() {
  VLOG(4) << "Start Verifying inputs, outputs and attributes for: PowOp.";
  VLOG(4) << "Verifying inputs:";
  {
  auto input_size = num_operands();
  IR_ENFORCE(input_size == 1u,
                    "The size %d of inputs must be equal to 1.", input_size);
  IR_ENFORCE((*this)->operand_source(0).type().isa<paddle::dialect::DenseTensorType>(),
                  "Type validation failed for the 0th input, got %s.", (*this)->operand_source(0).type());
  }
  VLOG(4) << "Verifying attributes:";
  {
  auto& attributes = this->attributes();
  IR_ENFORCE(attributes.count("y")>0,
                 "y does not exist.");
  IR_ENFORCE(attributes.at("y").isa<paddle::dialect::ScalarAttribute>(),
                 "Type of attribute: y is not paddle::dialect::ScalarAttribute.");

  }
  VLOG(4) << "Verifying outputs:";
  {
  auto output_size = num_results();
  IR_ENFORCE(output_size == 1u,
                    "The size %d of outputs must be equal to 1.", output_size);
  IR_ENFORCE((*this)->result(0).type().isa<paddle::dialect::DenseTensorType>(),
                 "Type validation failed for the 0th output.");
  }
  VLOG(4) << "End Verifying for: PowOp.";
}

void PowOp::InferMeta( phi::InferMetaContext *infer_meta ) {
  auto fn = PD_INFER_META(phi::UnchangedInferMeta);
  fn(infer_meta);
}

phi::DataType PowOp::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: PowOp";
  


  return expected_kernel_dtype;
}

const char *Pow_Op::attributes_name[1] = { "y" };

OpInfoTuple Pow_Op::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("x", "paddle::dialect::DenseTensorType", false, false, false, true) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = { paddle::dialect::OpAttributeInfo("y", "paddle::dialect::ScalarAttribute", "float") };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("out", "paddle::dialect::DenseTensorType", false, false) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("UnchangedInferMeta", {"x"}, "pow", {"x", "y"}, {"x"}, {}, {{"out", "x"}}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "pow");
}

void Pow_Op::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, float y) {
  VLOG(4) << "Start build Pow_Op";



  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_y = paddle::dialect::TransToIrAttribute(y, pir::IrContext::Instance());
  argument.AddAttribute("y", attr_y);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);
  paddle::dialect::IrTensor dense_out;
  paddle::dialect::IrMetaTensor meta_out(&dense_out);

  phi::UnchangedInferMeta(meta_x, &meta_out);

  std::vector<pir::Type> argument_outputs;
  pir::Type out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_out.dtype()), dense_out.dims(), dense_out.layout(), dense_out.lod(), dense_out.offset());
  argument_outputs.push_back(out_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void Pow_Op::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, pir::AttributeMap attributes) {
  VLOG(4) << "Start build Pow_Op";


  IR_ENFORCE(
      attributes.find("y") != attributes.end(),
          "'y' Attribute is expected for Pow_Op. ");
  float y = attributes.at("y").dyn_cast<paddle::dialect::ScalarAttribute>().data().to<float>();


  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_y = paddle::dialect::TransToIrAttribute(y, pir::IrContext::Instance());
  argument.AddAttribute("y", attr_y);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);
  paddle::dialect::IrTensor dense_out;
  paddle::dialect::IrMetaTensor meta_out(&dense_out);

  phi::UnchangedInferMeta(meta_x, &meta_out);

  std::vector<pir::Type> argument_outputs;
  pir::Type out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_out.dtype()), dense_out.dims(), dense_out.layout(), dense_out.lod(), dense_out.offset());
  argument_outputs.push_back(out_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void Pow_Op::VerifySig() {
  VLOG(4) << "Start Verifying inputs, outputs and attributes for: Pow_Op.";
  VLOG(4) << "Verifying inputs:";
  {
  auto input_size = num_operands();
  IR_ENFORCE(input_size == 1u,
                    "The size %d of inputs must be equal to 1.", input_size);
  IR_ENFORCE((*this)->operand_source(0).type().isa<paddle::dialect::DenseTensorType>(),
                  "Type validation failed for the 0th input, got %s.", (*this)->operand_source(0).type());
  }
  VLOG(4) << "Verifying attributes:";
  {
  auto& attributes = this->attributes();
  IR_ENFORCE(attributes.count("y")>0,
                 "y does not exist.");
  IR_ENFORCE(attributes.at("y").isa<paddle::dialect::ScalarAttribute>(),
                 "Type of attribute: y is not paddle::dialect::ScalarAttribute.");

  }
  VLOG(4) << "Verifying outputs:";
  {
  auto output_size = num_results();
  IR_ENFORCE(output_size == 1u,
                    "The size %d of outputs must be equal to 1.", output_size);
  IR_ENFORCE((*this)->result(0).type().isa<paddle::dialect::DenseTensorType>(),
                 "Type validation failed for the 0th output.");
  }
  VLOG(4) << "End Verifying for: Pow_Op.";
}

void Pow_Op::InferMeta( phi::InferMetaContext *infer_meta ) {
  auto fn = PD_INFER_META(phi::UnchangedInferMeta);
  fn(infer_meta);
}

phi::DataType Pow_Op::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: Pow_Op";
  


  return expected_kernel_dtype;
}

const char *PreluOp::attributes_name[2] = { "data_format", "mode" };

OpInfoTuple PreluOp::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("x", "paddle::dialect::DenseTensorType", false, false, false, true), paddle::dialect::OpInputInfo("alpha", "paddle::dialect::DenseTensorType", false, false, false, true) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = { paddle::dialect::OpAttributeInfo("data_format", "pir::StrAttribute", ""), paddle::dialect::OpAttributeInfo("mode", "pir::StrAttribute", "") };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("out", "paddle::dialect::DenseTensorType", false, false) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("PReluInferMeta", {"x", "alpha", "data_format", "mode"}, "prelu", {"x", "alpha", "data_format", "mode"}, {"x"}, {}, {}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "prelu");
}

void PreluOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, pir::Value alpha_, const std::string& data_format, const std::string& mode) {
  VLOG(4) << "Start build PreluOp";



  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_, alpha_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_data_format = pir::StrAttribute::get(pir::IrContext::Instance(), data_format);
  argument.AddAttribute("data_format", attr_data_format);
  pir::Attribute attr_mode = pir::StrAttribute::get(pir::IrContext::Instance(), mode);
  argument.AddAttribute("mode", attr_mode);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;
  paddle::dialect::DenseTensorType alpha = alpha_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)alpha;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);

  VLOG(4) << "Builder construction  dense_alpha";
  paddle::dialect::IrTensor ir_tensor_alpha(paddle::dialect::TransToPhiDataType(alpha.dtype()),
                                                      alpha.dims(),
                                                      alpha.data_layout(),
                                                      alpha.lod(),
                                                      alpha.offset());
  VLOG(4) << "Builder construction  meta_alpha";
  paddle::dialect::IrMetaTensor meta_alpha(&ir_tensor_alpha);
  paddle::dialect::IrTensor dense_out;
  paddle::dialect::IrMetaTensor meta_out(&dense_out);

  phi::PReluInferMeta(meta_x, meta_alpha, data_format, mode, &meta_out);

  std::vector<pir::Type> argument_outputs;
  pir::Type out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_out.dtype()), dense_out.dims(), dense_out.layout(), dense_out.lod(), dense_out.offset());
  argument_outputs.push_back(out_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void PreluOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, pir::Value alpha_, pir::AttributeMap attributes) {
  VLOG(4) << "Start build PreluOp";


  IR_ENFORCE(
      attributes.find("data_format") != attributes.end(),
          "'data_format' Attribute is expected for PreluOp. ");
  std::string data_format = attributes.at("data_format").dyn_cast<pir::StrAttribute>().AsString();

  IR_ENFORCE(
      attributes.find("mode") != attributes.end(),
          "'mode' Attribute is expected for PreluOp. ");
  std::string mode = attributes.at("mode").dyn_cast<pir::StrAttribute>().AsString();


  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_, alpha_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_data_format = pir::StrAttribute::get(pir::IrContext::Instance(), data_format);
  argument.AddAttribute("data_format", attr_data_format);
  pir::Attribute attr_mode = pir::StrAttribute::get(pir::IrContext::Instance(), mode);
  argument.AddAttribute("mode", attr_mode);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;
  paddle::dialect::DenseTensorType alpha = alpha_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)alpha;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);

  VLOG(4) << "Builder construction  dense_alpha";
  paddle::dialect::IrTensor ir_tensor_alpha(paddle::dialect::TransToPhiDataType(alpha.dtype()),
                                                      alpha.dims(),
                                                      alpha.data_layout(),
                                                      alpha.lod(),
                                                      alpha.offset());
  VLOG(4) << "Builder construction  meta_alpha";
  paddle::dialect::IrMetaTensor meta_alpha(&ir_tensor_alpha);
  paddle::dialect::IrTensor dense_out;
  paddle::dialect::IrMetaTensor meta_out(&dense_out);

  phi::PReluInferMeta(meta_x, meta_alpha, data_format, mode, &meta_out);

  std::vector<pir::Type> argument_outputs;
  pir::Type out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_out.dtype()), dense_out.dims(), dense_out.layout(), dense_out.lod(), dense_out.offset());
  argument_outputs.push_back(out_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void PreluOp::VerifySig() {
  VLOG(4) << "Start Verifying inputs, outputs and attributes for: PreluOp.";
  VLOG(4) << "Verifying inputs:";
  {
  auto input_size = num_operands();
  IR_ENFORCE(input_size == 2u,
                    "The size %d of inputs must be equal to 2.", input_size);
  IR_ENFORCE((*this)->operand_source(0).type().isa<paddle::dialect::DenseTensorType>(),
                  "Type validation failed for the 0th input, got %s.", (*this)->operand_source(0).type());
  IR_ENFORCE((*this)->operand_source(1).type().isa<paddle::dialect::DenseTensorType>(),
                  "Type validation failed for the 1th input, got %s.", (*this)->operand_source(1).type());
  }
  VLOG(4) << "Verifying attributes:";
  {
  auto& attributes = this->attributes();
  IR_ENFORCE(attributes.count("data_format")>0,
                 "data_format does not exist.");
  IR_ENFORCE(attributes.at("data_format").isa<pir::StrAttribute>(),
                 "Type of attribute: data_format is not pir::StrAttribute.");

  IR_ENFORCE(attributes.count("mode")>0,
                 "mode does not exist.");
  IR_ENFORCE(attributes.at("mode").isa<pir::StrAttribute>(),
                 "Type of attribute: mode is not pir::StrAttribute.");

  }
  VLOG(4) << "Verifying outputs:";
  {
  auto output_size = num_results();
  IR_ENFORCE(output_size == 1u,
                    "The size %d of outputs must be equal to 1.", output_size);
  IR_ENFORCE((*this)->result(0).type().isa<paddle::dialect::DenseTensorType>(),
                 "Type validation failed for the 0th output.");
  }
  VLOG(4) << "End Verifying for: PreluOp.";
}

void PreluOp::InferMeta( phi::InferMetaContext *infer_meta ) {
  auto fn = PD_INFER_META(phi::PReluInferMeta);
  fn(infer_meta);
}

phi::DataType PreluOp::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: PreluOp";
  


  return expected_kernel_dtype;
}

const char *PriorBoxOp::attributes_name[10] = { "min_sizes", "max_sizes", "aspect_ratios", "variances", "flip", "clip", "step_w", "step_h", "offset", "min_max_aspect_ratios_order" };

OpInfoTuple PriorBoxOp::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("input", "paddle::dialect::DenseTensorType", false, false, false, false), paddle::dialect::OpInputInfo("image", "paddle::dialect::DenseTensorType", false, false, false, false) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = { paddle::dialect::OpAttributeInfo("min_sizes", "pir::ArrayAttribute<pir::FloatAttribute>", ""), paddle::dialect::OpAttributeInfo("max_sizes", "pir::ArrayAttribute<pir::FloatAttribute>", ""), paddle::dialect::OpAttributeInfo("aspect_ratios", "pir::ArrayAttribute<pir::FloatAttribute>", ""), paddle::dialect::OpAttributeInfo("variances", "pir::ArrayAttribute<pir::FloatAttribute>", ""), paddle::dialect::OpAttributeInfo("flip", "pir::BoolAttribute", ""), paddle::dialect::OpAttributeInfo("clip", "pir::BoolAttribute", ""), paddle::dialect::OpAttributeInfo("step_w", "pir::FloatAttribute", ""), paddle::dialect::OpAttributeInfo("step_h", "pir::FloatAttribute", ""), paddle::dialect::OpAttributeInfo("offset", "pir::FloatAttribute", ""), paddle::dialect::OpAttributeInfo("min_max_aspect_ratios_order", "pir::BoolAttribute", "") };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("out", "paddle::dialect::DenseTensorType", false, false), paddle::dialect::OpOutputInfo("var", "paddle::dialect::DenseTensorType", false, false) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("PriorBoxInferMeta", {"input", "image", "min_sizes", "max_sizes", "aspect_ratios", "variances", "flip", "clip", "step_w", "step_h", "offset", "min_max_aspect_ratios_order"}, "prior_box", {"input", "image", "min_sizes", "max_sizes", "aspect_ratios", "variances", "flip", "clip", "step_w", "step_h", "offset", "min_max_aspect_ratios_order"}, {"input"}, {}, {}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "prior_box");
}

void PriorBoxOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value input_, pir::Value image_, const std::vector<float>& min_sizes, const std::vector<float>& max_sizes, const std::vector<float>& aspect_ratios, const std::vector<float>& variances, bool flip, bool clip, float step_w, float step_h, float offset, bool min_max_aspect_ratios_order) {
  VLOG(4) << "Start build PriorBoxOp";



  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {input_, image_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  std::vector<pir::Attribute> vec_min_sizes;
  for (size_t i = 0; i < static_cast<size_t>(min_sizes.size()); i++) {
      pir::Attribute attr_min_sizes = pir::FloatAttribute::get(pir::IrContext::Instance(), min_sizes[i]);

    vec_min_sizes.push_back(attr_min_sizes);
  }
  pir::Attribute attr_min_sizes = pir::ArrayAttribute::get(pir::IrContext::Instance(), vec_min_sizes);
  argument.AddAttribute("min_sizes", attr_min_sizes);
  std::vector<pir::Attribute> vec_max_sizes;
  for (size_t i = 0; i < static_cast<size_t>(max_sizes.size()); i++) {
      pir::Attribute attr_max_sizes = pir::FloatAttribute::get(pir::IrContext::Instance(), max_sizes[i]);

    vec_max_sizes.push_back(attr_max_sizes);
  }
  pir::Attribute attr_max_sizes = pir::ArrayAttribute::get(pir::IrContext::Instance(), vec_max_sizes);
  argument.AddAttribute("max_sizes", attr_max_sizes);
  std::vector<pir::Attribute> vec_aspect_ratios;
  for (size_t i = 0; i < static_cast<size_t>(aspect_ratios.size()); i++) {
      pir::Attribute attr_aspect_ratios = pir::FloatAttribute::get(pir::IrContext::Instance(), aspect_ratios[i]);

    vec_aspect_ratios.push_back(attr_aspect_ratios);
  }
  pir::Attribute attr_aspect_ratios = pir::ArrayAttribute::get(pir::IrContext::Instance(), vec_aspect_ratios);
  argument.AddAttribute("aspect_ratios", attr_aspect_ratios);
  std::vector<pir::Attribute> vec_variances;
  for (size_t i = 0; i < static_cast<size_t>(variances.size()); i++) {
      pir::Attribute attr_variances = pir::FloatAttribute::get(pir::IrContext::Instance(), variances[i]);

    vec_variances.push_back(attr_variances);
  }
  pir::Attribute attr_variances = pir::ArrayAttribute::get(pir::IrContext::Instance(), vec_variances);
  argument.AddAttribute("variances", attr_variances);
  pir::Attribute attr_flip = pir::BoolAttribute::get(pir::IrContext::Instance(), flip);
  argument.AddAttribute("flip", attr_flip);
  pir::Attribute attr_clip = pir::BoolAttribute::get(pir::IrContext::Instance(), clip);
  argument.AddAttribute("clip", attr_clip);
  pir::Attribute attr_step_w = pir::FloatAttribute::get(pir::IrContext::Instance(), step_w);
  argument.AddAttribute("step_w", attr_step_w);
  pir::Attribute attr_step_h = pir::FloatAttribute::get(pir::IrContext::Instance(), step_h);
  argument.AddAttribute("step_h", attr_step_h);
  pir::Attribute attr_offset = pir::FloatAttribute::get(pir::IrContext::Instance(), offset);
  argument.AddAttribute("offset", attr_offset);
  pir::Attribute attr_min_max_aspect_ratios_order = pir::BoolAttribute::get(pir::IrContext::Instance(), min_max_aspect_ratios_order);
  argument.AddAttribute("min_max_aspect_ratios_order", attr_min_max_aspect_ratios_order);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType input = input_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)input;
  paddle::dialect::DenseTensorType image = image_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)image;

  VLOG(4) << "Builder construction  dense_input";
  paddle::dialect::IrTensor ir_tensor_input(paddle::dialect::TransToPhiDataType(input.dtype()),
                                                      input.dims(),
                                                      input.data_layout(),
                                                      input.lod(),
                                                      input.offset());
  VLOG(4) << "Builder construction  meta_input";
  paddle::dialect::IrMetaTensor meta_input(&ir_tensor_input);

  VLOG(4) << "Builder construction  dense_image";
  paddle::dialect::IrTensor ir_tensor_image(paddle::dialect::TransToPhiDataType(image.dtype()),
                                                      image.dims(),
                                                      image.data_layout(),
                                                      image.lod(),
                                                      image.offset());
  VLOG(4) << "Builder construction  meta_image";
  paddle::dialect::IrMetaTensor meta_image(&ir_tensor_image);
  paddle::dialect::IrTensor dense_out;
  paddle::dialect::IrMetaTensor meta_out(&dense_out);
  paddle::dialect::IrTensor dense_var;
  paddle::dialect::IrMetaTensor meta_var(&dense_var);

  phi::PriorBoxInferMeta(meta_input, meta_image, min_sizes, max_sizes, aspect_ratios, variances, flip, clip, step_w, step_h, offset, min_max_aspect_ratios_order, &meta_out, &meta_var);

  std::vector<pir::Type> argument_outputs;
  pir::Type out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_out.dtype()), dense_out.dims(), dense_out.layout(), dense_out.lod(), dense_out.offset());
  argument_outputs.push_back(out_dense_tensor_type);

  pir::Type var_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_var.dtype()), dense_var.dims(), dense_var.layout(), dense_var.lod(), dense_var.offset());
  argument_outputs.push_back(var_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void PriorBoxOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value input_, pir::Value image_, pir::AttributeMap attributes) {
  VLOG(4) << "Start build PriorBoxOp";


  IR_ENFORCE(
      attributes.find("min_sizes") != attributes.end(),
          "'min_sizes' Attribute is expected for PriorBoxOp. ");
  std::vector<float> min_sizes;
  for (size_t i = 0; i < attributes.at("min_sizes").dyn_cast<pir::ArrayAttribute>().size(); i++) {
    min_sizes.push_back(attributes.at("min_sizes").dyn_cast<pir::ArrayAttribute>().at(i).dyn_cast<pir::FloatAttribute>().data());
  }

  IR_ENFORCE(
      attributes.find("max_sizes") != attributes.end(),
          "'max_sizes' Attribute is expected for PriorBoxOp. ");
  std::vector<float> max_sizes;
  for (size_t i = 0; i < attributes.at("max_sizes").dyn_cast<pir::ArrayAttribute>().size(); i++) {
    max_sizes.push_back(attributes.at("max_sizes").dyn_cast<pir::ArrayAttribute>().at(i).dyn_cast<pir::FloatAttribute>().data());
  }

  IR_ENFORCE(
      attributes.find("aspect_ratios") != attributes.end(),
          "'aspect_ratios' Attribute is expected for PriorBoxOp. ");
  std::vector<float> aspect_ratios;
  for (size_t i = 0; i < attributes.at("aspect_ratios").dyn_cast<pir::ArrayAttribute>().size(); i++) {
    aspect_ratios.push_back(attributes.at("aspect_ratios").dyn_cast<pir::ArrayAttribute>().at(i).dyn_cast<pir::FloatAttribute>().data());
  }

  IR_ENFORCE(
      attributes.find("variances") != attributes.end(),
          "'variances' Attribute is expected for PriorBoxOp. ");
  std::vector<float> variances;
  for (size_t i = 0; i < attributes.at("variances").dyn_cast<pir::ArrayAttribute>().size(); i++) {
    variances.push_back(attributes.at("variances").dyn_cast<pir::ArrayAttribute>().at(i).dyn_cast<pir::FloatAttribute>().data());
  }

  IR_ENFORCE(
      attributes.find("flip") != attributes.end(),
          "'flip' Attribute is expected for PriorBoxOp. ");
  bool flip = attributes.at("flip").dyn_cast<pir::BoolAttribute>().data();

  IR_ENFORCE(
      attributes.find("clip") != attributes.end(),
          "'clip' Attribute is expected for PriorBoxOp. ");
  bool clip = attributes.at("clip").dyn_cast<pir::BoolAttribute>().data();

  IR_ENFORCE(
      attributes.find("step_w") != attributes.end(),
          "'step_w' Attribute is expected for PriorBoxOp. ");
  float step_w = attributes.at("step_w").dyn_cast<pir::FloatAttribute>().data();

  IR_ENFORCE(
      attributes.find("step_h") != attributes.end(),
          "'step_h' Attribute is expected for PriorBoxOp. ");
  float step_h = attributes.at("step_h").dyn_cast<pir::FloatAttribute>().data();

  IR_ENFORCE(
      attributes.find("offset") != attributes.end(),
          "'offset' Attribute is expected for PriorBoxOp. ");
  float offset = attributes.at("offset").dyn_cast<pir::FloatAttribute>().data();

  IR_ENFORCE(
      attributes.find("min_max_aspect_ratios_order") != attributes.end(),
          "'min_max_aspect_ratios_order' Attribute is expected for PriorBoxOp. ");
  bool min_max_aspect_ratios_order = attributes.at("min_max_aspect_ratios_order").dyn_cast<pir::BoolAttribute>().data();


  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {input_, image_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  std::vector<pir::Attribute> vec_min_sizes;
  for (size_t i = 0; i < static_cast<size_t>(min_sizes.size()); i++) {
      pir::Attribute attr_min_sizes = pir::FloatAttribute::get(pir::IrContext::Instance(), min_sizes[i]);

    vec_min_sizes.push_back(attr_min_sizes);
  }
  pir::Attribute attr_min_sizes = pir::ArrayAttribute::get(pir::IrContext::Instance(), vec_min_sizes);
  argument.AddAttribute("min_sizes", attr_min_sizes);
  std::vector<pir::Attribute> vec_max_sizes;
  for (size_t i = 0; i < static_cast<size_t>(max_sizes.size()); i++) {
      pir::Attribute attr_max_sizes = pir::FloatAttribute::get(pir::IrContext::Instance(), max_sizes[i]);

    vec_max_sizes.push_back(attr_max_sizes);
  }
  pir::Attribute attr_max_sizes = pir::ArrayAttribute::get(pir::IrContext::Instance(), vec_max_sizes);
  argument.AddAttribute("max_sizes", attr_max_sizes);
  std::vector<pir::Attribute> vec_aspect_ratios;
  for (size_t i = 0; i < static_cast<size_t>(aspect_ratios.size()); i++) {
      pir::Attribute attr_aspect_ratios = pir::FloatAttribute::get(pir::IrContext::Instance(), aspect_ratios[i]);

    vec_aspect_ratios.push_back(attr_aspect_ratios);
  }
  pir::Attribute attr_aspect_ratios = pir::ArrayAttribute::get(pir::IrContext::Instance(), vec_aspect_ratios);
  argument.AddAttribute("aspect_ratios", attr_aspect_ratios);
  std::vector<pir::Attribute> vec_variances;
  for (size_t i = 0; i < static_cast<size_t>(variances.size()); i++) {
      pir::Attribute attr_variances = pir::FloatAttribute::get(pir::IrContext::Instance(), variances[i]);

    vec_variances.push_back(attr_variances);
  }
  pir::Attribute attr_variances = pir::ArrayAttribute::get(pir::IrContext::Instance(), vec_variances);
  argument.AddAttribute("variances", attr_variances);
  pir::Attribute attr_flip = pir::BoolAttribute::get(pir::IrContext::Instance(), flip);
  argument.AddAttribute("flip", attr_flip);
  pir::Attribute attr_clip = pir::BoolAttribute::get(pir::IrContext::Instance(), clip);
  argument.AddAttribute("clip", attr_clip);
  pir::Attribute attr_step_w = pir::FloatAttribute::get(pir::IrContext::Instance(), step_w);
  argument.AddAttribute("step_w", attr_step_w);
  pir::Attribute attr_step_h = pir::FloatAttribute::get(pir::IrContext::Instance(), step_h);
  argument.AddAttribute("step_h", attr_step_h);
  pir::Attribute attr_offset = pir::FloatAttribute::get(pir::IrContext::Instance(), offset);
  argument.AddAttribute("offset", attr_offset);
  pir::Attribute attr_min_max_aspect_ratios_order = pir::BoolAttribute::get(pir::IrContext::Instance(), min_max_aspect_ratios_order);
  argument.AddAttribute("min_max_aspect_ratios_order", attr_min_max_aspect_ratios_order);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType input = input_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)input;
  paddle::dialect::DenseTensorType image = image_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)image;

  VLOG(4) << "Builder construction  dense_input";
  paddle::dialect::IrTensor ir_tensor_input(paddle::dialect::TransToPhiDataType(input.dtype()),
                                                      input.dims(),
                                                      input.data_layout(),
                                                      input.lod(),
                                                      input.offset());
  VLOG(4) << "Builder construction  meta_input";
  paddle::dialect::IrMetaTensor meta_input(&ir_tensor_input);

  VLOG(4) << "Builder construction  dense_image";
  paddle::dialect::IrTensor ir_tensor_image(paddle::dialect::TransToPhiDataType(image.dtype()),
                                                      image.dims(),
                                                      image.data_layout(),
                                                      image.lod(),
                                                      image.offset());
  VLOG(4) << "Builder construction  meta_image";
  paddle::dialect::IrMetaTensor meta_image(&ir_tensor_image);
  paddle::dialect::IrTensor dense_out;
  paddle::dialect::IrMetaTensor meta_out(&dense_out);
  paddle::dialect::IrTensor dense_var;
  paddle::dialect::IrMetaTensor meta_var(&dense_var);

  phi::PriorBoxInferMeta(meta_input, meta_image, min_sizes, max_sizes, aspect_ratios, variances, flip, clip, step_w, step_h, offset, min_max_aspect_ratios_order, &meta_out, &meta_var);

  std::vector<pir::Type> argument_outputs;
  pir::Type out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_out.dtype()), dense_out.dims(), dense_out.layout(), dense_out.lod(), dense_out.offset());
  argument_outputs.push_back(out_dense_tensor_type);

  pir::Type var_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_var.dtype()), dense_var.dims(), dense_var.layout(), dense_var.lod(), dense_var.offset());
  argument_outputs.push_back(var_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void PriorBoxOp::VerifySig() {
  VLOG(4) << "Start Verifying inputs, outputs and attributes for: PriorBoxOp.";
  VLOG(4) << "Verifying inputs:";
  {
  auto input_size = num_operands();
  IR_ENFORCE(input_size == 2u,
                    "The size %d of inputs must be equal to 2.", input_size);
  IR_ENFORCE((*this)->operand_source(0).type().isa<paddle::dialect::DenseTensorType>(),
                  "Type validation failed for the 0th input, got %s.", (*this)->operand_source(0).type());
  IR_ENFORCE((*this)->operand_source(1).type().isa<paddle::dialect::DenseTensorType>(),
                  "Type validation failed for the 1th input, got %s.", (*this)->operand_source(1).type());
  }
  VLOG(4) << "Verifying attributes:";
  {
  auto& attributes = this->attributes();
  IR_ENFORCE(attributes.count("min_sizes")>0,
                 "min_sizes does not exist.");
  IR_ENFORCE(attributes.at("min_sizes").isa<pir::ArrayAttribute>(),
                 "Type of attribute: min_sizes is not pir::ArrayAttribute.");
  for (size_t i = 0; i < attributes.at("min_sizes").dyn_cast<pir::ArrayAttribute>().size(); i++) {
    IR_ENFORCE(attributes.at("min_sizes").dyn_cast<pir::ArrayAttribute>().at(i).isa<pir::FloatAttribute>(),
                   "Type of attribute: min_sizes is not right.");
  }
  IR_ENFORCE(attributes.count("max_sizes")>0,
                 "max_sizes does not exist.");
  IR_ENFORCE(attributes.at("max_sizes").isa<pir::ArrayAttribute>(),
                 "Type of attribute: max_sizes is not pir::ArrayAttribute.");
  for (size_t i = 0; i < attributes.at("max_sizes").dyn_cast<pir::ArrayAttribute>().size(); i++) {
    IR_ENFORCE(attributes.at("max_sizes").dyn_cast<pir::ArrayAttribute>().at(i).isa<pir::FloatAttribute>(),
                   "Type of attribute: max_sizes is not right.");
  }
  IR_ENFORCE(attributes.count("aspect_ratios")>0,
                 "aspect_ratios does not exist.");
  IR_ENFORCE(attributes.at("aspect_ratios").isa<pir::ArrayAttribute>(),
                 "Type of attribute: aspect_ratios is not pir::ArrayAttribute.");
  for (size_t i = 0; i < attributes.at("aspect_ratios").dyn_cast<pir::ArrayAttribute>().size(); i++) {
    IR_ENFORCE(attributes.at("aspect_ratios").dyn_cast<pir::ArrayAttribute>().at(i).isa<pir::FloatAttribute>(),
                   "Type of attribute: aspect_ratios is not right.");
  }
  IR_ENFORCE(attributes.count("variances")>0,
                 "variances does not exist.");
  IR_ENFORCE(attributes.at("variances").isa<pir::ArrayAttribute>(),
                 "Type of attribute: variances is not pir::ArrayAttribute.");
  for (size_t i = 0; i < attributes.at("variances").dyn_cast<pir::ArrayAttribute>().size(); i++) {
    IR_ENFORCE(attributes.at("variances").dyn_cast<pir::ArrayAttribute>().at(i).isa<pir::FloatAttribute>(),
                   "Type of attribute: variances is not right.");
  }
  IR_ENFORCE(attributes.count("flip")>0,
                 "flip does not exist.");
  IR_ENFORCE(attributes.at("flip").isa<pir::BoolAttribute>(),
                 "Type of attribute: flip is not pir::BoolAttribute.");

  IR_ENFORCE(attributes.count("clip")>0,
                 "clip does not exist.");
  IR_ENFORCE(attributes.at("clip").isa<pir::BoolAttribute>(),
                 "Type of attribute: clip is not pir::BoolAttribute.");

  IR_ENFORCE(attributes.count("step_w")>0,
                 "step_w does not exist.");
  IR_ENFORCE(attributes.at("step_w").isa<pir::FloatAttribute>(),
                 "Type of attribute: step_w is not pir::FloatAttribute.");

  IR_ENFORCE(attributes.count("step_h")>0,
                 "step_h does not exist.");
  IR_ENFORCE(attributes.at("step_h").isa<pir::FloatAttribute>(),
                 "Type of attribute: step_h is not pir::FloatAttribute.");

  IR_ENFORCE(attributes.count("offset")>0,
                 "offset does not exist.");
  IR_ENFORCE(attributes.at("offset").isa<pir::FloatAttribute>(),
                 "Type of attribute: offset is not pir::FloatAttribute.");

  IR_ENFORCE(attributes.count("min_max_aspect_ratios_order")>0,
                 "min_max_aspect_ratios_order does not exist.");
  IR_ENFORCE(attributes.at("min_max_aspect_ratios_order").isa<pir::BoolAttribute>(),
                 "Type of attribute: min_max_aspect_ratios_order is not pir::BoolAttribute.");

  }
  VLOG(4) << "Verifying outputs:";
  {
  auto output_size = num_results();
  IR_ENFORCE(output_size == 2u,
                    "The size %d of outputs must be equal to 2.", output_size);
  IR_ENFORCE((*this)->result(0).type().isa<paddle::dialect::DenseTensorType>(),
                 "Type validation failed for the 0th output.");
  IR_ENFORCE((*this)->result(1).type().isa<paddle::dialect::DenseTensorType>(),
                 "Type validation failed for the 1th output.");
  }
  VLOG(4) << "End Verifying for: PriorBoxOp.";
}

void PriorBoxOp::InferMeta( phi::InferMetaContext *infer_meta ) {
  auto fn = PD_INFER_META(phi::PriorBoxInferMeta);
  fn(infer_meta);
}

phi::DataType PriorBoxOp::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: PriorBoxOp";
  


  return expected_kernel_dtype;
}

const char *PsroiPoolOp::attributes_name[4] = { "pooled_height", "pooled_width", "output_channels", "spatial_scale" };

OpInfoTuple PsroiPoolOp::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("x", "paddle::dialect::DenseTensorType", false, false, false, true), paddle::dialect::OpInputInfo("boxes", "paddle::dialect::DenseTensorType", false, false, false, false), paddle::dialect::OpInputInfo("boxes_num", "paddle::dialect::DenseTensorType", true, false, false, false) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = { paddle::dialect::OpAttributeInfo("pooled_height", "pir::Int32Attribute", ""), paddle::dialect::OpAttributeInfo("pooled_width", "pir::Int32Attribute", ""), paddle::dialect::OpAttributeInfo("output_channels", "pir::Int32Attribute", ""), paddle::dialect::OpAttributeInfo("spatial_scale", "pir::FloatAttribute", "") };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("out", "paddle::dialect::DenseTensorType", false, false) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("PsroiPoolInferMeta", {"x", "boxes", "boxes_num", "pooled_height", "pooled_width", "output_channels", "spatial_scale"}, "psroi_pool", {"x", "boxes", "boxes_num", "pooled_height", "pooled_width", "output_channels", "spatial_scale"}, {"x"}, {}, {}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "psroi_pool");
}

void PsroiPoolOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, pir::Value boxes_, pir::Value boxes_num_, int pooled_height, int pooled_width, int output_channels, float spatial_scale) {
  VLOG(4) << "Start build PsroiPoolOp";



  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_, boxes_, boxes_num_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_pooled_height = pir::Int32Attribute::get(pir::IrContext::Instance(), pooled_height);
  argument.AddAttribute("pooled_height", attr_pooled_height);
  pir::Attribute attr_pooled_width = pir::Int32Attribute::get(pir::IrContext::Instance(), pooled_width);
  argument.AddAttribute("pooled_width", attr_pooled_width);
  pir::Attribute attr_output_channels = pir::Int32Attribute::get(pir::IrContext::Instance(), output_channels);
  argument.AddAttribute("output_channels", attr_output_channels);
  pir::Attribute attr_spatial_scale = pir::FloatAttribute::get(pir::IrContext::Instance(), spatial_scale);
  argument.AddAttribute("spatial_scale", attr_spatial_scale);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;
  paddle::dialect::DenseTensorType boxes = boxes_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)boxes;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);

  VLOG(4) << "Builder construction  dense_boxes";
  paddle::dialect::IrTensor ir_tensor_boxes(paddle::dialect::TransToPhiDataType(boxes.dtype()),
                                                      boxes.dims(),
                                                      boxes.data_layout(),
                                                      boxes.lod(),
                                                      boxes.offset());
  VLOG(4) << "Builder construction  meta_boxes";
  paddle::dialect::IrMetaTensor meta_boxes(&ir_tensor_boxes);

  paddle::dialect::IrMetaTensor meta_boxes_num;
  paddle::dialect::IrTensor ir_tensor_boxes_num;
  if (boxes_num_.impl() != nullptr) {
    paddle::dialect::DenseTensorType boxes_num = boxes_num_.type().dyn_cast<paddle::dialect::DenseTensorType>();
    VLOG(4) << "Builder construction  dense_boxes_num";
    ir_tensor_boxes_num = paddle::dialect::IrTensor(paddle::dialect::TransToPhiDataType(boxes_num.dtype()),
                                                        boxes_num.dims(),
                                                        boxes_num.data_layout(),
                                                        boxes_num.lod(),
                                                        boxes_num.offset());
    VLOG(4) << "Builder construction  meta_boxes_num";
    meta_boxes_num = paddle::dialect::IrMetaTensor(&ir_tensor_boxes_num);
  }

  paddle::dialect::IrTensor dense_out;
  paddle::dialect::IrMetaTensor meta_out(&dense_out);

  phi::PsroiPoolInferMeta(meta_x, meta_boxes, meta_boxes_num, pooled_height, pooled_width, output_channels, spatial_scale, &meta_out);

  std::vector<pir::Type> argument_outputs;
  pir::Type out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_out.dtype()), dense_out.dims(), dense_out.layout(), dense_out.lod(), dense_out.offset());
  argument_outputs.push_back(out_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void PsroiPoolOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, pir::Value boxes_, pir::Value boxes_num_, pir::AttributeMap attributes) {
  VLOG(4) << "Start build PsroiPoolOp";


  IR_ENFORCE(
      attributes.find("pooled_height") != attributes.end(),
          "'pooled_height' Attribute is expected for PsroiPoolOp. ");
  int pooled_height = attributes.at("pooled_height").dyn_cast<pir::Int32Attribute>().data();

  IR_ENFORCE(
      attributes.find("pooled_width") != attributes.end(),
          "'pooled_width' Attribute is expected for PsroiPoolOp. ");
  int pooled_width = attributes.at("pooled_width").dyn_cast<pir::Int32Attribute>().data();

  IR_ENFORCE(
      attributes.find("output_channels") != attributes.end(),
          "'output_channels' Attribute is expected for PsroiPoolOp. ");
  int output_channels = attributes.at("output_channels").dyn_cast<pir::Int32Attribute>().data();

  IR_ENFORCE(
      attributes.find("spatial_scale") != attributes.end(),
          "'spatial_scale' Attribute is expected for PsroiPoolOp. ");
  float spatial_scale = attributes.at("spatial_scale").dyn_cast<pir::FloatAttribute>().data();


  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_, boxes_, boxes_num_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_pooled_height = pir::Int32Attribute::get(pir::IrContext::Instance(), pooled_height);
  argument.AddAttribute("pooled_height", attr_pooled_height);
  pir::Attribute attr_pooled_width = pir::Int32Attribute::get(pir::IrContext::Instance(), pooled_width);
  argument.AddAttribute("pooled_width", attr_pooled_width);
  pir::Attribute attr_output_channels = pir::Int32Attribute::get(pir::IrContext::Instance(), output_channels);
  argument.AddAttribute("output_channels", attr_output_channels);
  pir::Attribute attr_spatial_scale = pir::FloatAttribute::get(pir::IrContext::Instance(), spatial_scale);
  argument.AddAttribute("spatial_scale", attr_spatial_scale);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;
  paddle::dialect::DenseTensorType boxes = boxes_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)boxes;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);

  VLOG(4) << "Builder construction  dense_boxes";
  paddle::dialect::IrTensor ir_tensor_boxes(paddle::dialect::TransToPhiDataType(boxes.dtype()),
                                                      boxes.dims(),
                                                      boxes.data_layout(),
                                                      boxes.lod(),
                                                      boxes.offset());
  VLOG(4) << "Builder construction  meta_boxes";
  paddle::dialect::IrMetaTensor meta_boxes(&ir_tensor_boxes);

  paddle::dialect::IrMetaTensor meta_boxes_num;
  paddle::dialect::IrTensor ir_tensor_boxes_num;
  if (boxes_num_.impl() != nullptr) {
    paddle::dialect::DenseTensorType boxes_num = boxes_num_.type().dyn_cast<paddle::dialect::DenseTensorType>();
    VLOG(4) << "Builder construction  dense_boxes_num";
    ir_tensor_boxes_num = paddle::dialect::IrTensor(paddle::dialect::TransToPhiDataType(boxes_num.dtype()),
                                                        boxes_num.dims(),
                                                        boxes_num.data_layout(),
                                                        boxes_num.lod(),
                                                        boxes_num.offset());
    VLOG(4) << "Builder construction  meta_boxes_num";
    meta_boxes_num = paddle::dialect::IrMetaTensor(&ir_tensor_boxes_num);
  }

  paddle::dialect::IrTensor dense_out;
  paddle::dialect::IrMetaTensor meta_out(&dense_out);

  phi::PsroiPoolInferMeta(meta_x, meta_boxes, meta_boxes_num, pooled_height, pooled_width, output_channels, spatial_scale, &meta_out);

  std::vector<pir::Type> argument_outputs;
  pir::Type out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_out.dtype()), dense_out.dims(), dense_out.layout(), dense_out.lod(), dense_out.offset());
  argument_outputs.push_back(out_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void PsroiPoolOp::VerifySig() {
  VLOG(4) << "Start Verifying inputs, outputs and attributes for: PsroiPoolOp.";
  VLOG(4) << "Verifying inputs:";
  {
  auto input_size = num_operands();
  IR_ENFORCE(input_size == 3u,
                    "The size %d of inputs must be equal to 3.", input_size);
  IR_ENFORCE((*this)->operand_source(0).type().isa<paddle::dialect::DenseTensorType>(),
                  "Type validation failed for the 0th input, got %s.", (*this)->operand_source(0).type());
  IR_ENFORCE((*this)->operand_source(1).type().isa<paddle::dialect::DenseTensorType>(),
                  "Type validation failed for the 1th input, got %s.", (*this)->operand_source(1).type());
  if (auto val = (*this)->operand(2)) {
    IR_ENFORCE(val.type().isa<paddle::dialect::DenseTensorType>(),
                   "Type validation failed for the 2th input, got %s.", (*this)->operand_source(2).type());
  }
  }
  VLOG(4) << "Verifying attributes:";
  {
  auto& attributes = this->attributes();
  IR_ENFORCE(attributes.count("pooled_height")>0,
                 "pooled_height does not exist.");
  IR_ENFORCE(attributes.at("pooled_height").isa<pir::Int32Attribute>(),
                 "Type of attribute: pooled_height is not pir::Int32Attribute.");

  IR_ENFORCE(attributes.count("pooled_width")>0,
                 "pooled_width does not exist.");
  IR_ENFORCE(attributes.at("pooled_width").isa<pir::Int32Attribute>(),
                 "Type of attribute: pooled_width is not pir::Int32Attribute.");

  IR_ENFORCE(attributes.count("output_channels")>0,
                 "output_channels does not exist.");
  IR_ENFORCE(attributes.at("output_channels").isa<pir::Int32Attribute>(),
                 "Type of attribute: output_channels is not pir::Int32Attribute.");

  IR_ENFORCE(attributes.count("spatial_scale")>0,
                 "spatial_scale does not exist.");
  IR_ENFORCE(attributes.at("spatial_scale").isa<pir::FloatAttribute>(),
                 "Type of attribute: spatial_scale is not pir::FloatAttribute.");

  }
  VLOG(4) << "Verifying outputs:";
  {
  auto output_size = num_results();
  IR_ENFORCE(output_size == 1u,
                    "The size %d of outputs must be equal to 1.", output_size);
  IR_ENFORCE((*this)->result(0).type().isa<paddle::dialect::DenseTensorType>(),
                 "Type validation failed for the 0th output.");
  }
  VLOG(4) << "End Verifying for: PsroiPoolOp.";
}

void PsroiPoolOp::InferMeta( phi::InferMetaContext *infer_meta ) {
  auto fn = PD_INFER_META(phi::PsroiPoolInferMeta);
  fn(infer_meta);
}

phi::DataType PsroiPoolOp::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: PsroiPoolOp";
  


  return expected_kernel_dtype;
}

const char *PutAlongAxisOp::attributes_name[3] = { "axis", "reduce", "include_self" };

OpInfoTuple PutAlongAxisOp::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("arr", "paddle::dialect::DenseTensorType", false, false, false, true), paddle::dialect::OpInputInfo("indices", "paddle::dialect::DenseTensorType", false, false, false, false), paddle::dialect::OpInputInfo("values", "paddle::dialect::DenseTensorType", false, false, false, true) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = { paddle::dialect::OpAttributeInfo("axis", "pir::Int32Attribute", ""), paddle::dialect::OpAttributeInfo("reduce", "pir::StrAttribute", ""), paddle::dialect::OpAttributeInfo("include_self", "pir::BoolAttribute", "") };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("out", "paddle::dialect::DenseTensorType", false, false) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("UnchangedInferMeta", {"arr"}, "put_along_axis", {"arr", "indices", "values", "axis", "reduce", "include_self"}, {"arr"}, {}, {}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "put_along_axis");
}

void PutAlongAxisOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value arr_, pir::Value indices_, pir::Value values_, int axis, const std::string& reduce, bool include_self) {
  VLOG(4) << "Start build PutAlongAxisOp";



  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {arr_, indices_, values_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_axis = pir::Int32Attribute::get(pir::IrContext::Instance(), axis);
  argument.AddAttribute("axis", attr_axis);
  pir::Attribute attr_reduce = pir::StrAttribute::get(pir::IrContext::Instance(), reduce);
  argument.AddAttribute("reduce", attr_reduce);
  pir::Attribute attr_include_self = pir::BoolAttribute::get(pir::IrContext::Instance(), include_self);
  argument.AddAttribute("include_self", attr_include_self);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType arr = arr_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)arr;
  paddle::dialect::DenseTensorType indices = indices_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)indices;
  paddle::dialect::DenseTensorType values = values_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)values;

  VLOG(4) << "Builder construction  dense_arr";
  paddle::dialect::IrTensor ir_tensor_arr(paddle::dialect::TransToPhiDataType(arr.dtype()),
                                                      arr.dims(),
                                                      arr.data_layout(),
                                                      arr.lod(),
                                                      arr.offset());
  VLOG(4) << "Builder construction  meta_arr";
  paddle::dialect::IrMetaTensor meta_arr(&ir_tensor_arr);
  paddle::dialect::IrTensor dense_out;
  paddle::dialect::IrMetaTensor meta_out(&dense_out);

  phi::UnchangedInferMeta(meta_arr, &meta_out);

  std::vector<pir::Type> argument_outputs;
  pir::Type out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_out.dtype()), dense_out.dims(), dense_out.layout(), dense_out.lod(), dense_out.offset());
  argument_outputs.push_back(out_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void PutAlongAxisOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value arr_, pir::Value indices_, pir::Value values_, pir::AttributeMap attributes) {
  VLOG(4) << "Start build PutAlongAxisOp";


  IR_ENFORCE(
      attributes.find("axis") != attributes.end(),
          "'axis' Attribute is expected for PutAlongAxisOp. ");
  int axis = attributes.at("axis").dyn_cast<pir::Int32Attribute>().data();

  IR_ENFORCE(
      attributes.find("reduce") != attributes.end(),
          "'reduce' Attribute is expected for PutAlongAxisOp. ");
  std::string reduce = attributes.at("reduce").dyn_cast<pir::StrAttribute>().AsString();

  IR_ENFORCE(
      attributes.find("include_self") != attributes.end(),
          "'include_self' Attribute is expected for PutAlongAxisOp. ");
  bool include_self = attributes.at("include_self").dyn_cast<pir::BoolAttribute>().data();


  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {arr_, indices_, values_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_axis = pir::Int32Attribute::get(pir::IrContext::Instance(), axis);
  argument.AddAttribute("axis", attr_axis);
  pir::Attribute attr_reduce = pir::StrAttribute::get(pir::IrContext::Instance(), reduce);
  argument.AddAttribute("reduce", attr_reduce);
  pir::Attribute attr_include_self = pir::BoolAttribute::get(pir::IrContext::Instance(), include_self);
  argument.AddAttribute("include_self", attr_include_self);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType arr = arr_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)arr;
  paddle::dialect::DenseTensorType indices = indices_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)indices;
  paddle::dialect::DenseTensorType values = values_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)values;

  VLOG(4) << "Builder construction  dense_arr";
  paddle::dialect::IrTensor ir_tensor_arr(paddle::dialect::TransToPhiDataType(arr.dtype()),
                                                      arr.dims(),
                                                      arr.data_layout(),
                                                      arr.lod(),
                                                      arr.offset());
  VLOG(4) << "Builder construction  meta_arr";
  paddle::dialect::IrMetaTensor meta_arr(&ir_tensor_arr);
  paddle::dialect::IrTensor dense_out;
  paddle::dialect::IrMetaTensor meta_out(&dense_out);

  phi::UnchangedInferMeta(meta_arr, &meta_out);

  std::vector<pir::Type> argument_outputs;
  pir::Type out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_out.dtype()), dense_out.dims(), dense_out.layout(), dense_out.lod(), dense_out.offset());
  argument_outputs.push_back(out_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void PutAlongAxisOp::VerifySig() {
  VLOG(4) << "Start Verifying inputs, outputs and attributes for: PutAlongAxisOp.";
  VLOG(4) << "Verifying inputs:";
  {
  auto input_size = num_operands();
  IR_ENFORCE(input_size == 3u,
                    "The size %d of inputs must be equal to 3.", input_size);
  IR_ENFORCE((*this)->operand_source(0).type().isa<paddle::dialect::DenseTensorType>(),
                  "Type validation failed for the 0th input, got %s.", (*this)->operand_source(0).type());
  IR_ENFORCE((*this)->operand_source(1).type().isa<paddle::dialect::DenseTensorType>(),
                  "Type validation failed for the 1th input, got %s.", (*this)->operand_source(1).type());
  IR_ENFORCE((*this)->operand_source(2).type().isa<paddle::dialect::DenseTensorType>(),
                  "Type validation failed for the 2th input, got %s.", (*this)->operand_source(2).type());
  }
  VLOG(4) << "Verifying attributes:";
  {
  auto& attributes = this->attributes();
  IR_ENFORCE(attributes.count("axis")>0,
                 "axis does not exist.");
  IR_ENFORCE(attributes.at("axis").isa<pir::Int32Attribute>(),
                 "Type of attribute: axis is not pir::Int32Attribute.");

  IR_ENFORCE(attributes.count("reduce")>0,
                 "reduce does not exist.");
  IR_ENFORCE(attributes.at("reduce").isa<pir::StrAttribute>(),
                 "Type of attribute: reduce is not pir::StrAttribute.");

  IR_ENFORCE(attributes.count("include_self")>0,
                 "include_self does not exist.");
  IR_ENFORCE(attributes.at("include_self").isa<pir::BoolAttribute>(),
                 "Type of attribute: include_self is not pir::BoolAttribute.");

  }
  VLOG(4) << "Verifying outputs:";
  {
  auto output_size = num_results();
  IR_ENFORCE(output_size == 1u,
                    "The size %d of outputs must be equal to 1.", output_size);
  IR_ENFORCE((*this)->result(0).type().isa<paddle::dialect::DenseTensorType>(),
                 "Type validation failed for the 0th output.");
  }
  VLOG(4) << "End Verifying for: PutAlongAxisOp.";
}

void PutAlongAxisOp::InferMeta( phi::InferMetaContext *infer_meta ) {
  auto fn = PD_INFER_META(phi::UnchangedInferMeta);
  fn(infer_meta);
}

phi::DataType PutAlongAxisOp::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: PutAlongAxisOp";
  


  return expected_kernel_dtype;
}

const char *PutAlongAxis_Op::attributes_name[3] = { "axis", "reduce", "include_self" };

OpInfoTuple PutAlongAxis_Op::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("arr", "paddle::dialect::DenseTensorType", false, false, false, true), paddle::dialect::OpInputInfo("indices", "paddle::dialect::DenseTensorType", false, false, false, false), paddle::dialect::OpInputInfo("values", "paddle::dialect::DenseTensorType", false, false, false, true) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = { paddle::dialect::OpAttributeInfo("axis", "pir::Int32Attribute", ""), paddle::dialect::OpAttributeInfo("reduce", "pir::StrAttribute", ""), paddle::dialect::OpAttributeInfo("include_self", "pir::BoolAttribute", "") };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("out", "paddle::dialect::DenseTensorType", false, false) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("UnchangedInferMeta", {"arr"}, "put_along_axis", {"arr", "indices", "values", "axis", "reduce", "include_self"}, {"arr"}, {}, {{"out", "arr"}}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "put_along_axis");
}

void PutAlongAxis_Op::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value arr_, pir::Value indices_, pir::Value values_, int axis, const std::string& reduce, bool include_self) {
  VLOG(4) << "Start build PutAlongAxis_Op";



  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {arr_, indices_, values_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_axis = pir::Int32Attribute::get(pir::IrContext::Instance(), axis);
  argument.AddAttribute("axis", attr_axis);
  pir::Attribute attr_reduce = pir::StrAttribute::get(pir::IrContext::Instance(), reduce);
  argument.AddAttribute("reduce", attr_reduce);
  pir::Attribute attr_include_self = pir::BoolAttribute::get(pir::IrContext::Instance(), include_self);
  argument.AddAttribute("include_self", attr_include_self);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType arr = arr_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)arr;
  paddle::dialect::DenseTensorType indices = indices_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)indices;
  paddle::dialect::DenseTensorType values = values_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)values;

  VLOG(4) << "Builder construction  dense_arr";
  paddle::dialect::IrTensor ir_tensor_arr(paddle::dialect::TransToPhiDataType(arr.dtype()),
                                                      arr.dims(),
                                                      arr.data_layout(),
                                                      arr.lod(),
                                                      arr.offset());
  VLOG(4) << "Builder construction  meta_arr";
  paddle::dialect::IrMetaTensor meta_arr(&ir_tensor_arr);
  paddle::dialect::IrTensor dense_out;
  paddle::dialect::IrMetaTensor meta_out(&dense_out);

  phi::UnchangedInferMeta(meta_arr, &meta_out);

  std::vector<pir::Type> argument_outputs;
  pir::Type out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_out.dtype()), dense_out.dims(), dense_out.layout(), dense_out.lod(), dense_out.offset());
  argument_outputs.push_back(out_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void PutAlongAxis_Op::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value arr_, pir::Value indices_, pir::Value values_, pir::AttributeMap attributes) {
  VLOG(4) << "Start build PutAlongAxis_Op";


  IR_ENFORCE(
      attributes.find("axis") != attributes.end(),
          "'axis' Attribute is expected for PutAlongAxis_Op. ");
  int axis = attributes.at("axis").dyn_cast<pir::Int32Attribute>().data();

  IR_ENFORCE(
      attributes.find("reduce") != attributes.end(),
          "'reduce' Attribute is expected for PutAlongAxis_Op. ");
  std::string reduce = attributes.at("reduce").dyn_cast<pir::StrAttribute>().AsString();

  IR_ENFORCE(
      attributes.find("include_self") != attributes.end(),
          "'include_self' Attribute is expected for PutAlongAxis_Op. ");
  bool include_self = attributes.at("include_self").dyn_cast<pir::BoolAttribute>().data();


  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {arr_, indices_, values_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_axis = pir::Int32Attribute::get(pir::IrContext::Instance(), axis);
  argument.AddAttribute("axis", attr_axis);
  pir::Attribute attr_reduce = pir::StrAttribute::get(pir::IrContext::Instance(), reduce);
  argument.AddAttribute("reduce", attr_reduce);
  pir::Attribute attr_include_self = pir::BoolAttribute::get(pir::IrContext::Instance(), include_self);
  argument.AddAttribute("include_self", attr_include_self);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType arr = arr_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)arr;
  paddle::dialect::DenseTensorType indices = indices_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)indices;
  paddle::dialect::DenseTensorType values = values_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)values;

  VLOG(4) << "Builder construction  dense_arr";
  paddle::dialect::IrTensor ir_tensor_arr(paddle::dialect::TransToPhiDataType(arr.dtype()),
                                                      arr.dims(),
                                                      arr.data_layout(),
                                                      arr.lod(),
                                                      arr.offset());
  VLOG(4) << "Builder construction  meta_arr";
  paddle::dialect::IrMetaTensor meta_arr(&ir_tensor_arr);
  paddle::dialect::IrTensor dense_out;
  paddle::dialect::IrMetaTensor meta_out(&dense_out);

  phi::UnchangedInferMeta(meta_arr, &meta_out);

  std::vector<pir::Type> argument_outputs;
  pir::Type out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_out.dtype()), dense_out.dims(), dense_out.layout(), dense_out.lod(), dense_out.offset());
  argument_outputs.push_back(out_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void PutAlongAxis_Op::VerifySig() {
  VLOG(4) << "Start Verifying inputs, outputs and attributes for: PutAlongAxis_Op.";
  VLOG(4) << "Verifying inputs:";
  {
  auto input_size = num_operands();
  IR_ENFORCE(input_size == 3u,
                    "The size %d of inputs must be equal to 3.", input_size);
  IR_ENFORCE((*this)->operand_source(0).type().isa<paddle::dialect::DenseTensorType>(),
                  "Type validation failed for the 0th input, got %s.", (*this)->operand_source(0).type());
  IR_ENFORCE((*this)->operand_source(1).type().isa<paddle::dialect::DenseTensorType>(),
                  "Type validation failed for the 1th input, got %s.", (*this)->operand_source(1).type());
  IR_ENFORCE((*this)->operand_source(2).type().isa<paddle::dialect::DenseTensorType>(),
                  "Type validation failed for the 2th input, got %s.", (*this)->operand_source(2).type());
  }
  VLOG(4) << "Verifying attributes:";
  {
  auto& attributes = this->attributes();
  IR_ENFORCE(attributes.count("axis")>0,
                 "axis does not exist.");
  IR_ENFORCE(attributes.at("axis").isa<pir::Int32Attribute>(),
                 "Type of attribute: axis is not pir::Int32Attribute.");

  IR_ENFORCE(attributes.count("reduce")>0,
                 "reduce does not exist.");
  IR_ENFORCE(attributes.at("reduce").isa<pir::StrAttribute>(),
                 "Type of attribute: reduce is not pir::StrAttribute.");

  IR_ENFORCE(attributes.count("include_self")>0,
                 "include_self does not exist.");
  IR_ENFORCE(attributes.at("include_self").isa<pir::BoolAttribute>(),
                 "Type of attribute: include_self is not pir::BoolAttribute.");

  }
  VLOG(4) << "Verifying outputs:";
  {
  auto output_size = num_results();
  IR_ENFORCE(output_size == 1u,
                    "The size %d of outputs must be equal to 1.", output_size);
  IR_ENFORCE((*this)->result(0).type().isa<paddle::dialect::DenseTensorType>(),
                 "Type validation failed for the 0th output.");
  }
  VLOG(4) << "End Verifying for: PutAlongAxis_Op.";
}

void PutAlongAxis_Op::InferMeta( phi::InferMetaContext *infer_meta ) {
  auto fn = PD_INFER_META(phi::UnchangedInferMeta);
  fn(infer_meta);
}

phi::DataType PutAlongAxis_Op::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: PutAlongAxis_Op";
  


  return expected_kernel_dtype;
}

const char *QrOp::attributes_name[1] = { "mode" };

OpInfoTuple QrOp::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("x", "paddle::dialect::DenseTensorType", false, false, false, true) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = { paddle::dialect::OpAttributeInfo("mode", "pir::StrAttribute", "") };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("q", "paddle::dialect::DenseTensorType", false, false), paddle::dialect::OpOutputInfo("r", "paddle::dialect::DenseTensorType", false, false) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("QrInferMeta", {"x", "mode"}, "qr", {"x", "mode"}, {}, {}, {}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "qr");
}

void QrOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, const std::string& mode) {
  VLOG(4) << "Start build QrOp";



  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_mode = pir::StrAttribute::get(pir::IrContext::Instance(), mode);
  argument.AddAttribute("mode", attr_mode);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);
  paddle::dialect::IrTensor dense_q;
  paddle::dialect::IrMetaTensor meta_q(&dense_q);
  paddle::dialect::IrTensor dense_r;
  paddle::dialect::IrMetaTensor meta_r(&dense_r);

  phi::QrInferMeta(meta_x, mode, &meta_q, &meta_r);

  std::vector<pir::Type> argument_outputs;
  pir::Type q_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_q.dtype()), dense_q.dims(), dense_q.layout(), dense_q.lod(), dense_q.offset());
  argument_outputs.push_back(q_dense_tensor_type);

  pir::Type r_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_r.dtype()), dense_r.dims(), dense_r.layout(), dense_r.lod(), dense_r.offset());
  argument_outputs.push_back(r_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void QrOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, pir::AttributeMap attributes) {
  VLOG(4) << "Start build QrOp";


  IR_ENFORCE(
      attributes.find("mode") != attributes.end(),
          "'mode' Attribute is expected for QrOp. ");
  std::string mode = attributes.at("mode").dyn_cast<pir::StrAttribute>().AsString();


  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_mode = pir::StrAttribute::get(pir::IrContext::Instance(), mode);
  argument.AddAttribute("mode", attr_mode);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);
  paddle::dialect::IrTensor dense_q;
  paddle::dialect::IrMetaTensor meta_q(&dense_q);
  paddle::dialect::IrTensor dense_r;
  paddle::dialect::IrMetaTensor meta_r(&dense_r);

  phi::QrInferMeta(meta_x, mode, &meta_q, &meta_r);

  std::vector<pir::Type> argument_outputs;
  pir::Type q_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_q.dtype()), dense_q.dims(), dense_q.layout(), dense_q.lod(), dense_q.offset());
  argument_outputs.push_back(q_dense_tensor_type);

  pir::Type r_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_r.dtype()), dense_r.dims(), dense_r.layout(), dense_r.lod(), dense_r.offset());
  argument_outputs.push_back(r_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void QrOp::VerifySig() {
  VLOG(4) << "Start Verifying inputs, outputs and attributes for: QrOp.";
  VLOG(4) << "Verifying inputs:";
  {
  auto input_size = num_operands();
  IR_ENFORCE(input_size == 1u,
                    "The size %d of inputs must be equal to 1.", input_size);
  IR_ENFORCE((*this)->operand_source(0).type().isa<paddle::dialect::DenseTensorType>(),
                  "Type validation failed for the 0th input, got %s.", (*this)->operand_source(0).type());
  }
  VLOG(4) << "Verifying attributes:";
  {
  auto& attributes = this->attributes();
  IR_ENFORCE(attributes.count("mode")>0,
                 "mode does not exist.");
  IR_ENFORCE(attributes.at("mode").isa<pir::StrAttribute>(),
                 "Type of attribute: mode is not pir::StrAttribute.");

  }
  VLOG(4) << "Verifying outputs:";
  {
  auto output_size = num_results();
  IR_ENFORCE(output_size == 2u,
                    "The size %d of outputs must be equal to 2.", output_size);
  IR_ENFORCE((*this)->result(0).type().isa<paddle::dialect::DenseTensorType>(),
                 "Type validation failed for the 0th output.");
  IR_ENFORCE((*this)->result(1).type().isa<paddle::dialect::DenseTensorType>(),
                 "Type validation failed for the 1th output.");
  }
  VLOG(4) << "End Verifying for: QrOp.";
}

void QrOp::InferMeta( phi::InferMetaContext *infer_meta ) {
  auto fn = PD_INFER_META(phi::QrInferMeta);
  fn(infer_meta);
}

phi::DataType QrOp::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: QrOp";
  


  return expected_kernel_dtype;
}

OpInfoTuple RealOp::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("x", "paddle::dialect::DenseTensorType", false, false, false, true) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = {  };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("out", "paddle::dialect::DenseTensorType", false, false) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("RealAndImagInferMeta", {"x"}, "real", {"x"}, {}, {}, {}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "real");
}

void RealOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_) {
  VLOG(4) << "Start build RealOp";



  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);
  paddle::dialect::IrTensor dense_out;
  paddle::dialect::IrMetaTensor meta_out(&dense_out);

  phi::RealAndImagInferMeta(meta_x, &meta_out);

  std::vector<pir::Type> argument_outputs;
  pir::Type out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_out.dtype()), dense_out.dims(), dense_out.layout(), dense_out.lod(), dense_out.offset());
  argument_outputs.push_back(out_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void RealOp::VerifySig() {
  VLOG(4) << "Start Verifying inputs, outputs and attributes for: RealOp.";
  VLOG(4) << "Verifying inputs:";
  {
  auto input_size = num_operands();
  IR_ENFORCE(input_size == 1u,
                    "The size %d of inputs must be equal to 1.", input_size);
  IR_ENFORCE((*this)->operand_source(0).type().isa<paddle::dialect::DenseTensorType>(),
                  "Type validation failed for the 0th input, got %s.", (*this)->operand_source(0).type());
  }
  VLOG(4) << "Verifying attributes:";
  {
  // Attributes num is 0, not need to check attributes type.
  }
  VLOG(4) << "Verifying outputs:";
  {
  auto output_size = num_results();
  IR_ENFORCE(output_size == 1u,
                    "The size %d of outputs must be equal to 1.", output_size);
  IR_ENFORCE((*this)->result(0).type().isa<paddle::dialect::DenseTensorType>(),
                 "Type validation failed for the 0th output.");
  }
  VLOG(4) << "End Verifying for: RealOp.";
}

void RealOp::InferMeta( phi::InferMetaContext *infer_meta ) {
  auto fn = PD_INFER_META(phi::RealAndImagInferMeta);
  fn(infer_meta);
}

phi::DataType RealOp::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: RealOp";
  


  return expected_kernel_dtype;
}

OpInfoTuple ReciprocalOp::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("x", "paddle::dialect::DenseTensorType", false, false, false, true) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = {  };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("out", "paddle::dialect::DenseTensorType", false, false) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("UnchangedInferMeta", {"x"}, "reciprocal", {"x"}, {}, {}, {}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "reciprocal");
}

void ReciprocalOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_) {
  VLOG(4) << "Start build ReciprocalOp";



  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);
  paddle::dialect::IrTensor dense_out;
  paddle::dialect::IrMetaTensor meta_out(&dense_out);

  phi::UnchangedInferMeta(meta_x, &meta_out);

  std::vector<pir::Type> argument_outputs;
  pir::Type out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_out.dtype()), dense_out.dims(), dense_out.layout(), dense_out.lod(), dense_out.offset());
  argument_outputs.push_back(out_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void ReciprocalOp::VerifySig() {
  VLOG(4) << "Start Verifying inputs, outputs and attributes for: ReciprocalOp.";
  VLOG(4) << "Verifying inputs:";
  {
  auto input_size = num_operands();
  IR_ENFORCE(input_size == 1u,
                    "The size %d of inputs must be equal to 1.", input_size);
  IR_ENFORCE((*this)->operand_source(0).type().isa<paddle::dialect::DenseTensorType>(),
                  "Type validation failed for the 0th input, got %s.", (*this)->operand_source(0).type());
  }
  VLOG(4) << "Verifying attributes:";
  {
  // Attributes num is 0, not need to check attributes type.
  }
  VLOG(4) << "Verifying outputs:";
  {
  auto output_size = num_results();
  IR_ENFORCE(output_size == 1u,
                    "The size %d of outputs must be equal to 1.", output_size);
  IR_ENFORCE((*this)->result(0).type().isa<paddle::dialect::DenseTensorType>(),
                 "Type validation failed for the 0th output.");
  }
  VLOG(4) << "End Verifying for: ReciprocalOp.";
}

void ReciprocalOp::InferMeta( phi::InferMetaContext *infer_meta ) {
  auto fn = PD_INFER_META(phi::UnchangedInferMeta);
  fn(infer_meta);
}

phi::DataType ReciprocalOp::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: ReciprocalOp";
  


  return expected_kernel_dtype;
}

OpInfoTuple Reciprocal_Op::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("x", "paddle::dialect::DenseTensorType", false, false, false, true) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = {  };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("out", "paddle::dialect::DenseTensorType", false, false) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("UnchangedInferMeta", {"x"}, "reciprocal", {"x"}, {}, {}, {{"out", "x"}}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "reciprocal");
}

void Reciprocal_Op::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_) {
  VLOG(4) << "Start build Reciprocal_Op";



  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);
  paddle::dialect::IrTensor dense_out;
  paddle::dialect::IrMetaTensor meta_out(&dense_out);

  phi::UnchangedInferMeta(meta_x, &meta_out);

  std::vector<pir::Type> argument_outputs;
  pir::Type out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_out.dtype()), dense_out.dims(), dense_out.layout(), dense_out.lod(), dense_out.offset());
  argument_outputs.push_back(out_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void Reciprocal_Op::VerifySig() {
  VLOG(4) << "Start Verifying inputs, outputs and attributes for: Reciprocal_Op.";
  VLOG(4) << "Verifying inputs:";
  {
  auto input_size = num_operands();
  IR_ENFORCE(input_size == 1u,
                    "The size %d of inputs must be equal to 1.", input_size);
  IR_ENFORCE((*this)->operand_source(0).type().isa<paddle::dialect::DenseTensorType>(),
                  "Type validation failed for the 0th input, got %s.", (*this)->operand_source(0).type());
  }
  VLOG(4) << "Verifying attributes:";
  {
  // Attributes num is 0, not need to check attributes type.
  }
  VLOG(4) << "Verifying outputs:";
  {
  auto output_size = num_results();
  IR_ENFORCE(output_size == 1u,
                    "The size %d of outputs must be equal to 1.", output_size);
  IR_ENFORCE((*this)->result(0).type().isa<paddle::dialect::DenseTensorType>(),
                 "Type validation failed for the 0th output.");
  }
  VLOG(4) << "End Verifying for: Reciprocal_Op.";
}

void Reciprocal_Op::InferMeta( phi::InferMetaContext *infer_meta ) {
  auto fn = PD_INFER_META(phi::UnchangedInferMeta);
  fn(infer_meta);
}

phi::DataType Reciprocal_Op::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: Reciprocal_Op";
  


  return expected_kernel_dtype;
}

OpInfoTuple ReindexGraphOp::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("x", "paddle::dialect::DenseTensorType", false, false, false, false), paddle::dialect::OpInputInfo("neighbors", "paddle::dialect::DenseTensorType", false, false, false, false), paddle::dialect::OpInputInfo("count", "paddle::dialect::DenseTensorType", false, false, false, false), paddle::dialect::OpInputInfo("hashtable_value", "paddle::dialect::DenseTensorType", true, false, false, false), paddle::dialect::OpInputInfo("hashtable_index", "paddle::dialect::DenseTensorType", true, false, false, false) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = {  };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("reindex_src", "paddle::dialect::DenseTensorType", false, false), paddle::dialect::OpOutputInfo("reindex_dst", "paddle::dialect::DenseTensorType", false, false), paddle::dialect::OpOutputInfo("out_nodes", "paddle::dialect::DenseTensorType", false, false) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("GraphReindexInferMeta", {"x", "neighbors", "count", "hashtable_value", "hashtable_index"}, "graph_reindex", {"x", "neighbors", "count", "hashtable_value", "hashtable_index"}, {"x"}, {}, {}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "reindex_graph");
}

void ReindexGraphOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, pir::Value neighbors_, pir::Value count_, pir::Value hashtable_value_, pir::Value hashtable_index_) {
  VLOG(4) << "Start build ReindexGraphOp";



  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_, neighbors_, count_, hashtable_value_, hashtable_index_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;
  paddle::dialect::DenseTensorType neighbors = neighbors_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)neighbors;
  paddle::dialect::DenseTensorType count = count_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)count;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);

  VLOG(4) << "Builder construction  dense_neighbors";
  paddle::dialect::IrTensor ir_tensor_neighbors(paddle::dialect::TransToPhiDataType(neighbors.dtype()),
                                                      neighbors.dims(),
                                                      neighbors.data_layout(),
                                                      neighbors.lod(),
                                                      neighbors.offset());
  VLOG(4) << "Builder construction  meta_neighbors";
  paddle::dialect::IrMetaTensor meta_neighbors(&ir_tensor_neighbors);

  VLOG(4) << "Builder construction  dense_count";
  paddle::dialect::IrTensor ir_tensor_count(paddle::dialect::TransToPhiDataType(count.dtype()),
                                                      count.dims(),
                                                      count.data_layout(),
                                                      count.lod(),
                                                      count.offset());
  VLOG(4) << "Builder construction  meta_count";
  paddle::dialect::IrMetaTensor meta_count(&ir_tensor_count);

  paddle::dialect::IrMetaTensor meta_hashtable_value;
  paddle::dialect::IrTensor ir_tensor_hashtable_value;
  if (hashtable_value_.impl() != nullptr) {
    paddle::dialect::DenseTensorType hashtable_value = hashtable_value_.type().dyn_cast<paddle::dialect::DenseTensorType>();
    VLOG(4) << "Builder construction  dense_hashtable_value";
    ir_tensor_hashtable_value = paddle::dialect::IrTensor(paddle::dialect::TransToPhiDataType(hashtable_value.dtype()),
                                                        hashtable_value.dims(),
                                                        hashtable_value.data_layout(),
                                                        hashtable_value.lod(),
                                                        hashtable_value.offset());
    VLOG(4) << "Builder construction  meta_hashtable_value";
    meta_hashtable_value = paddle::dialect::IrMetaTensor(&ir_tensor_hashtable_value);
  }


  paddle::dialect::IrMetaTensor meta_hashtable_index;
  paddle::dialect::IrTensor ir_tensor_hashtable_index;
  if (hashtable_index_.impl() != nullptr) {
    paddle::dialect::DenseTensorType hashtable_index = hashtable_index_.type().dyn_cast<paddle::dialect::DenseTensorType>();
    VLOG(4) << "Builder construction  dense_hashtable_index";
    ir_tensor_hashtable_index = paddle::dialect::IrTensor(paddle::dialect::TransToPhiDataType(hashtable_index.dtype()),
                                                        hashtable_index.dims(),
                                                        hashtable_index.data_layout(),
                                                        hashtable_index.lod(),
                                                        hashtable_index.offset());
    VLOG(4) << "Builder construction  meta_hashtable_index";
    meta_hashtable_index = paddle::dialect::IrMetaTensor(&ir_tensor_hashtable_index);
  }

  paddle::dialect::IrTensor dense_reindex_src;
  paddle::dialect::IrMetaTensor meta_reindex_src(&dense_reindex_src);
  paddle::dialect::IrTensor dense_reindex_dst;
  paddle::dialect::IrMetaTensor meta_reindex_dst(&dense_reindex_dst);
  paddle::dialect::IrTensor dense_out_nodes;
  paddle::dialect::IrMetaTensor meta_out_nodes(&dense_out_nodes);

  phi::GraphReindexInferMeta(meta_x, meta_neighbors, meta_count, meta_hashtable_value, meta_hashtable_index, &meta_reindex_src, &meta_reindex_dst, &meta_out_nodes);

  std::vector<pir::Type> argument_outputs;
  pir::Type reindex_src_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_reindex_src.dtype()), dense_reindex_src.dims(), dense_reindex_src.layout(), dense_reindex_src.lod(), dense_reindex_src.offset());
  argument_outputs.push_back(reindex_src_dense_tensor_type);

  pir::Type reindex_dst_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_reindex_dst.dtype()), dense_reindex_dst.dims(), dense_reindex_dst.layout(), dense_reindex_dst.lod(), dense_reindex_dst.offset());
  argument_outputs.push_back(reindex_dst_dense_tensor_type);

  pir::Type out_nodes_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_out_nodes.dtype()), dense_out_nodes.dims(), dense_out_nodes.layout(), dense_out_nodes.lod(), dense_out_nodes.offset());
  argument_outputs.push_back(out_nodes_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void ReindexGraphOp::VerifySig() {
  VLOG(4) << "Start Verifying inputs, outputs and attributes for: ReindexGraphOp.";
  VLOG(4) << "Verifying inputs:";
  {
  auto input_size = num_operands();
  IR_ENFORCE(input_size == 5u,
                    "The size %d of inputs must be equal to 5.", input_size);
  IR_ENFORCE((*this)->operand_source(0).type().isa<paddle::dialect::DenseTensorType>(),
                  "Type validation failed for the 0th input, got %s.", (*this)->operand_source(0).type());
  IR_ENFORCE((*this)->operand_source(1).type().isa<paddle::dialect::DenseTensorType>(),
                  "Type validation failed for the 1th input, got %s.", (*this)->operand_source(1).type());
  IR_ENFORCE((*this)->operand_source(2).type().isa<paddle::dialect::DenseTensorType>(),
                  "Type validation failed for the 2th input, got %s.", (*this)->operand_source(2).type());
  if (auto val = (*this)->operand(3)) {
    IR_ENFORCE(val.type().isa<paddle::dialect::DenseTensorType>(),
                   "Type validation failed for the 3th input, got %s.", (*this)->operand_source(3).type());
  }
  if (auto val = (*this)->operand(4)) {
    IR_ENFORCE(val.type().isa<paddle::dialect::DenseTensorType>(),
                   "Type validation failed for the 4th input, got %s.", (*this)->operand_source(4).type());
  }
  }
  VLOG(4) << "Verifying attributes:";
  {
  // Attributes num is 0, not need to check attributes type.
  }
  VLOG(4) << "Verifying outputs:";
  {
  auto output_size = num_results();
  IR_ENFORCE(output_size == 3u,
                    "The size %d of outputs must be equal to 3.", output_size);
  IR_ENFORCE((*this)->result(0).type().isa<paddle::dialect::DenseTensorType>(),
                 "Type validation failed for the 0th output.");
  IR_ENFORCE((*this)->result(1).type().isa<paddle::dialect::DenseTensorType>(),
                 "Type validation failed for the 1th output.");
  IR_ENFORCE((*this)->result(2).type().isa<paddle::dialect::DenseTensorType>(),
                 "Type validation failed for the 2th output.");
  }
  VLOG(4) << "End Verifying for: ReindexGraphOp.";
}

void ReindexGraphOp::InferMeta( phi::InferMetaContext *infer_meta ) {
  auto fn = PD_INFER_META(phi::GraphReindexInferMeta);
  fn(infer_meta);
}

phi::DataType ReindexGraphOp::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: ReindexGraphOp";
  


  return expected_kernel_dtype;
}

OpInfoTuple ReluOp::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("x", "paddle::dialect::DenseTensorType", false, false, false, true) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = {  };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("out", "paddle::dialect::DenseTensorType", false, false) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("UnchangedInferMeta", {"x"}, "relu", {"x"}, {}, {}, {}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "relu");
}

void ReluOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_) {
  VLOG(4) << "Start build ReluOp";



  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);
  paddle::dialect::IrTensor dense_out;
  paddle::dialect::IrMetaTensor meta_out(&dense_out);

  phi::UnchangedInferMeta(meta_x, &meta_out);

  std::vector<pir::Type> argument_outputs;
  pir::Type out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_out.dtype()), dense_out.dims(), dense_out.layout(), dense_out.lod(), dense_out.offset());
  argument_outputs.push_back(out_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void ReluOp::VerifySig() {
  VLOG(4) << "Start Verifying inputs, outputs and attributes for: ReluOp.";
  VLOG(4) << "Verifying inputs:";
  {
  auto input_size = num_operands();
  IR_ENFORCE(input_size == 1u,
                    "The size %d of inputs must be equal to 1.", input_size);
  IR_ENFORCE((*this)->operand_source(0).type().isa<paddle::dialect::DenseTensorType>(),
                  "Type validation failed for the 0th input, got %s.", (*this)->operand_source(0).type());
  }
  VLOG(4) << "Verifying attributes:";
  {
  // Attributes num is 0, not need to check attributes type.
  }
  VLOG(4) << "Verifying outputs:";
  {
  auto output_size = num_results();
  IR_ENFORCE(output_size == 1u,
                    "The size %d of outputs must be equal to 1.", output_size);
  IR_ENFORCE((*this)->result(0).type().isa<paddle::dialect::DenseTensorType>(),
                 "Type validation failed for the 0th output.");
  }
  VLOG(4) << "End Verifying for: ReluOp.";
}

void ReluOp::InferMeta( phi::InferMetaContext *infer_meta ) {
  auto fn = PD_INFER_META(phi::UnchangedInferMeta);
  fn(infer_meta);
}

phi::DataType ReluOp::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: ReluOp";
  


  return expected_kernel_dtype;
}

OpInfoTuple Relu_Op::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("x", "paddle::dialect::DenseTensorType", false, false, false, true) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = {  };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("out", "paddle::dialect::DenseTensorType", false, false) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("UnchangedInferMeta", {"x"}, "relu", {"x"}, {}, {}, {{"out", "x"}}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "relu");
}

void Relu_Op::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_) {
  VLOG(4) << "Start build Relu_Op";



  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);
  paddle::dialect::IrTensor dense_out;
  paddle::dialect::IrMetaTensor meta_out(&dense_out);

  phi::UnchangedInferMeta(meta_x, &meta_out);

  std::vector<pir::Type> argument_outputs;
  pir::Type out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_out.dtype()), dense_out.dims(), dense_out.layout(), dense_out.lod(), dense_out.offset());
  argument_outputs.push_back(out_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void Relu_Op::VerifySig() {
  VLOG(4) << "Start Verifying inputs, outputs and attributes for: Relu_Op.";
  VLOG(4) << "Verifying inputs:";
  {
  auto input_size = num_operands();
  IR_ENFORCE(input_size == 1u,
                    "The size %d of inputs must be equal to 1.", input_size);
  IR_ENFORCE((*this)->operand_source(0).type().isa<paddle::dialect::DenseTensorType>(),
                  "Type validation failed for the 0th input, got %s.", (*this)->operand_source(0).type());
  }
  VLOG(4) << "Verifying attributes:";
  {
  // Attributes num is 0, not need to check attributes type.
  }
  VLOG(4) << "Verifying outputs:";
  {
  auto output_size = num_results();
  IR_ENFORCE(output_size == 1u,
                    "The size %d of outputs must be equal to 1.", output_size);
  IR_ENFORCE((*this)->result(0).type().isa<paddle::dialect::DenseTensorType>(),
                 "Type validation failed for the 0th output.");
  }
  VLOG(4) << "End Verifying for: Relu_Op.";
}

void Relu_Op::InferMeta( phi::InferMetaContext *infer_meta ) {
  auto fn = PD_INFER_META(phi::UnchangedInferMeta);
  fn(infer_meta);
}

phi::DataType Relu_Op::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: Relu_Op";
  


  return expected_kernel_dtype;
}

OpInfoTuple Relu6Op::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("x", "paddle::dialect::DenseTensorType", false, false, false, true) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = {  };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("out", "paddle::dialect::DenseTensorType", false, false) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("UnchangedInferMeta", {"x"}, "relu6", {"x"}, {}, {}, {}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "relu6");
}

void Relu6Op::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_) {
  VLOG(4) << "Start build Relu6Op";



  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);
  paddle::dialect::IrTensor dense_out;
  paddle::dialect::IrMetaTensor meta_out(&dense_out);

  phi::UnchangedInferMeta(meta_x, &meta_out);

  std::vector<pir::Type> argument_outputs;
  pir::Type out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_out.dtype()), dense_out.dims(), dense_out.layout(), dense_out.lod(), dense_out.offset());
  argument_outputs.push_back(out_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void Relu6Op::VerifySig() {
  VLOG(4) << "Start Verifying inputs, outputs and attributes for: Relu6Op.";
  VLOG(4) << "Verifying inputs:";
  {
  auto input_size = num_operands();
  IR_ENFORCE(input_size == 1u,
                    "The size %d of inputs must be equal to 1.", input_size);
  IR_ENFORCE((*this)->operand_source(0).type().isa<paddle::dialect::DenseTensorType>(),
                  "Type validation failed for the 0th input, got %s.", (*this)->operand_source(0).type());
  }
  VLOG(4) << "Verifying attributes:";
  {
  // Attributes num is 0, not need to check attributes type.
  }
  VLOG(4) << "Verifying outputs:";
  {
  auto output_size = num_results();
  IR_ENFORCE(output_size == 1u,
                    "The size %d of outputs must be equal to 1.", output_size);
  IR_ENFORCE((*this)->result(0).type().isa<paddle::dialect::DenseTensorType>(),
                 "Type validation failed for the 0th output.");
  }
  VLOG(4) << "End Verifying for: Relu6Op.";
}

void Relu6Op::InferMeta( phi::InferMetaContext *infer_meta ) {
  auto fn = PD_INFER_META(phi::UnchangedInferMeta);
  fn(infer_meta);
}

phi::DataType Relu6Op::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: Relu6Op";
  


  return expected_kernel_dtype;
}

const char *RenormOp::attributes_name[3] = { "p", "axis", "max_norm" };

OpInfoTuple RenormOp::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("x", "paddle::dialect::DenseTensorType", false, false, false, true) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = { paddle::dialect::OpAttributeInfo("p", "pir::FloatAttribute", ""), paddle::dialect::OpAttributeInfo("axis", "pir::Int32Attribute", ""), paddle::dialect::OpAttributeInfo("max_norm", "pir::FloatAttribute", "") };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("out", "paddle::dialect::DenseTensorType", false, false) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("UnchangedInferMeta", {"x"}, "renorm", {"x", "p", "axis", "max_norm"}, {}, {}, {}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "renorm");
}

void RenormOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, float p, int axis, float max_norm) {
  VLOG(4) << "Start build RenormOp";



  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_p = pir::FloatAttribute::get(pir::IrContext::Instance(), p);
  argument.AddAttribute("p", attr_p);
  pir::Attribute attr_axis = pir::Int32Attribute::get(pir::IrContext::Instance(), axis);
  argument.AddAttribute("axis", attr_axis);
  pir::Attribute attr_max_norm = pir::FloatAttribute::get(pir::IrContext::Instance(), max_norm);
  argument.AddAttribute("max_norm", attr_max_norm);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);
  paddle::dialect::IrTensor dense_out;
  paddle::dialect::IrMetaTensor meta_out(&dense_out);

  phi::UnchangedInferMeta(meta_x, &meta_out);

  std::vector<pir::Type> argument_outputs;
  pir::Type out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_out.dtype()), dense_out.dims(), dense_out.layout(), dense_out.lod(), dense_out.offset());
  argument_outputs.push_back(out_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void RenormOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, pir::AttributeMap attributes) {
  VLOG(4) << "Start build RenormOp";


  IR_ENFORCE(
      attributes.find("p") != attributes.end(),
          "'p' Attribute is expected for RenormOp. ");
  float p = attributes.at("p").dyn_cast<pir::FloatAttribute>().data();

  IR_ENFORCE(
      attributes.find("axis") != attributes.end(),
          "'axis' Attribute is expected for RenormOp. ");
  int axis = attributes.at("axis").dyn_cast<pir::Int32Attribute>().data();

  IR_ENFORCE(
      attributes.find("max_norm") != attributes.end(),
          "'max_norm' Attribute is expected for RenormOp. ");
  float max_norm = attributes.at("max_norm").dyn_cast<pir::FloatAttribute>().data();


  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_p = pir::FloatAttribute::get(pir::IrContext::Instance(), p);
  argument.AddAttribute("p", attr_p);
  pir::Attribute attr_axis = pir::Int32Attribute::get(pir::IrContext::Instance(), axis);
  argument.AddAttribute("axis", attr_axis);
  pir::Attribute attr_max_norm = pir::FloatAttribute::get(pir::IrContext::Instance(), max_norm);
  argument.AddAttribute("max_norm", attr_max_norm);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);
  paddle::dialect::IrTensor dense_out;
  paddle::dialect::IrMetaTensor meta_out(&dense_out);

  phi::UnchangedInferMeta(meta_x, &meta_out);

  std::vector<pir::Type> argument_outputs;
  pir::Type out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_out.dtype()), dense_out.dims(), dense_out.layout(), dense_out.lod(), dense_out.offset());
  argument_outputs.push_back(out_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void RenormOp::VerifySig() {
  VLOG(4) << "Start Verifying inputs, outputs and attributes for: RenormOp.";
  VLOG(4) << "Verifying inputs:";
  {
  auto input_size = num_operands();
  IR_ENFORCE(input_size == 1u,
                    "The size %d of inputs must be equal to 1.", input_size);
  IR_ENFORCE((*this)->operand_source(0).type().isa<paddle::dialect::DenseTensorType>(),
                  "Type validation failed for the 0th input, got %s.", (*this)->operand_source(0).type());
  }
  VLOG(4) << "Verifying attributes:";
  {
  auto& attributes = this->attributes();
  IR_ENFORCE(attributes.count("p")>0,
                 "p does not exist.");
  IR_ENFORCE(attributes.at("p").isa<pir::FloatAttribute>(),
                 "Type of attribute: p is not pir::FloatAttribute.");

  IR_ENFORCE(attributes.count("axis")>0,
                 "axis does not exist.");
  IR_ENFORCE(attributes.at("axis").isa<pir::Int32Attribute>(),
                 "Type of attribute: axis is not pir::Int32Attribute.");

  IR_ENFORCE(attributes.count("max_norm")>0,
                 "max_norm does not exist.");
  IR_ENFORCE(attributes.at("max_norm").isa<pir::FloatAttribute>(),
                 "Type of attribute: max_norm is not pir::FloatAttribute.");

  }
  VLOG(4) << "Verifying outputs:";
  {
  auto output_size = num_results();
  IR_ENFORCE(output_size == 1u,
                    "The size %d of outputs must be equal to 1.", output_size);
  IR_ENFORCE((*this)->result(0).type().isa<paddle::dialect::DenseTensorType>(),
                 "Type validation failed for the 0th output.");
  }
  VLOG(4) << "End Verifying for: RenormOp.";
}

void RenormOp::InferMeta( phi::InferMetaContext *infer_meta ) {
  auto fn = PD_INFER_META(phi::UnchangedInferMeta);
  fn(infer_meta);
}

phi::DataType RenormOp::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: RenormOp";
  


  return expected_kernel_dtype;
}

const char *Renorm_Op::attributes_name[3] = { "p", "axis", "max_norm" };

OpInfoTuple Renorm_Op::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("x", "paddle::dialect::DenseTensorType", false, false, false, true) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = { paddle::dialect::OpAttributeInfo("p", "pir::FloatAttribute", ""), paddle::dialect::OpAttributeInfo("axis", "pir::Int32Attribute", ""), paddle::dialect::OpAttributeInfo("max_norm", "pir::FloatAttribute", "") };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("out", "paddle::dialect::DenseTensorType", false, false) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("UnchangedInferMeta", {"x"}, "renorm", {"x", "p", "axis", "max_norm"}, {}, {}, {{"out", "x"}}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "renorm");
}

void Renorm_Op::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, float p, int axis, float max_norm) {
  VLOG(4) << "Start build Renorm_Op";



  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_p = pir::FloatAttribute::get(pir::IrContext::Instance(), p);
  argument.AddAttribute("p", attr_p);
  pir::Attribute attr_axis = pir::Int32Attribute::get(pir::IrContext::Instance(), axis);
  argument.AddAttribute("axis", attr_axis);
  pir::Attribute attr_max_norm = pir::FloatAttribute::get(pir::IrContext::Instance(), max_norm);
  argument.AddAttribute("max_norm", attr_max_norm);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);
  paddle::dialect::IrTensor dense_out;
  paddle::dialect::IrMetaTensor meta_out(&dense_out);

  phi::UnchangedInferMeta(meta_x, &meta_out);

  std::vector<pir::Type> argument_outputs;
  pir::Type out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_out.dtype()), dense_out.dims(), dense_out.layout(), dense_out.lod(), dense_out.offset());
  argument_outputs.push_back(out_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void Renorm_Op::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, pir::AttributeMap attributes) {
  VLOG(4) << "Start build Renorm_Op";


  IR_ENFORCE(
      attributes.find("p") != attributes.end(),
          "'p' Attribute is expected for Renorm_Op. ");
  float p = attributes.at("p").dyn_cast<pir::FloatAttribute>().data();

  IR_ENFORCE(
      attributes.find("axis") != attributes.end(),
          "'axis' Attribute is expected for Renorm_Op. ");
  int axis = attributes.at("axis").dyn_cast<pir::Int32Attribute>().data();

  IR_ENFORCE(
      attributes.find("max_norm") != attributes.end(),
          "'max_norm' Attribute is expected for Renorm_Op. ");
  float max_norm = attributes.at("max_norm").dyn_cast<pir::FloatAttribute>().data();


  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_p = pir::FloatAttribute::get(pir::IrContext::Instance(), p);
  argument.AddAttribute("p", attr_p);
  pir::Attribute attr_axis = pir::Int32Attribute::get(pir::IrContext::Instance(), axis);
  argument.AddAttribute("axis", attr_axis);
  pir::Attribute attr_max_norm = pir::FloatAttribute::get(pir::IrContext::Instance(), max_norm);
  argument.AddAttribute("max_norm", attr_max_norm);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);
  paddle::dialect::IrTensor dense_out;
  paddle::dialect::IrMetaTensor meta_out(&dense_out);

  phi::UnchangedInferMeta(meta_x, &meta_out);

  std::vector<pir::Type> argument_outputs;
  pir::Type out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_out.dtype()), dense_out.dims(), dense_out.layout(), dense_out.lod(), dense_out.offset());
  argument_outputs.push_back(out_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void Renorm_Op::VerifySig() {
  VLOG(4) << "Start Verifying inputs, outputs and attributes for: Renorm_Op.";
  VLOG(4) << "Verifying inputs:";
  {
  auto input_size = num_operands();
  IR_ENFORCE(input_size == 1u,
                    "The size %d of inputs must be equal to 1.", input_size);
  IR_ENFORCE((*this)->operand_source(0).type().isa<paddle::dialect::DenseTensorType>(),
                  "Type validation failed for the 0th input, got %s.", (*this)->operand_source(0).type());
  }
  VLOG(4) << "Verifying attributes:";
  {
  auto& attributes = this->attributes();
  IR_ENFORCE(attributes.count("p")>0,
                 "p does not exist.");
  IR_ENFORCE(attributes.at("p").isa<pir::FloatAttribute>(),
                 "Type of attribute: p is not pir::FloatAttribute.");

  IR_ENFORCE(attributes.count("axis")>0,
                 "axis does not exist.");
  IR_ENFORCE(attributes.at("axis").isa<pir::Int32Attribute>(),
                 "Type of attribute: axis is not pir::Int32Attribute.");

  IR_ENFORCE(attributes.count("max_norm")>0,
                 "max_norm does not exist.");
  IR_ENFORCE(attributes.at("max_norm").isa<pir::FloatAttribute>(),
                 "Type of attribute: max_norm is not pir::FloatAttribute.");

  }
  VLOG(4) << "Verifying outputs:";
  {
  auto output_size = num_results();
  IR_ENFORCE(output_size == 1u,
                    "The size %d of outputs must be equal to 1.", output_size);
  IR_ENFORCE((*this)->result(0).type().isa<paddle::dialect::DenseTensorType>(),
                 "Type validation failed for the 0th output.");
  }
  VLOG(4) << "End Verifying for: Renorm_Op.";
}

void Renorm_Op::InferMeta( phi::InferMetaContext *infer_meta ) {
  auto fn = PD_INFER_META(phi::UnchangedInferMeta);
  fn(infer_meta);
}

phi::DataType Renorm_Op::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: Renorm_Op";
  


  return expected_kernel_dtype;
}

OpInfoTuple ReverseOp::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("x", "paddle::dialect::DenseTensorType", false, false, false, true), paddle::dialect::OpInputInfo("axis", "paddle::dialect::IntArrayAttribute", false, false, true, false) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = {  };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("out", "paddle::dialect::DenseTensorType", false, false) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("ReverseInferMeta", {"x", "axis"}, "reverse", {"x", "axis"}, {"x"}, {}, {}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "reverse");
}

void ReverseOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, const std::vector<int64_t>& axis) {
  VLOG(4) << "Start build ReverseOp";


  // Generate int_array mutable attribute: axis
  paddle::dialect::FullIntArrayOp full_axis_op = builder.Build<paddle::dialect::FullIntArrayOp>(axis, phi::DataType::INT64, phi::CPUPlace());
  pir::OpResult axis_ = full_axis_op->result(0);
    
  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_, axis_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);
  paddle::dialect::IrTensor dense_out;
  paddle::dialect::IrMetaTensor meta_out(&dense_out);

  phi::ReverseInferMeta(meta_x, axis, &meta_out);

  std::vector<pir::Type> argument_outputs;
  pir::Type out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_out.dtype()), dense_out.dims(), dense_out.layout(), dense_out.lod(), dense_out.offset());
  argument_outputs.push_back(out_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void ReverseOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, pir::AttributeMap attributes) {
  VLOG(4) << "Start build ReverseOp";


  IR_ENFORCE(
      attributes.find("axis") != attributes.end(),
          "'axis' Attribute is expected for ReverseOp. ");
  std::vector<int64_t> axis = attributes.at("axis").dyn_cast<paddle::dialect::IntArrayAttribute>().data().GetData();

  // Generate int_array mutable attribute: axis
  paddle::dialect::FullIntArrayOp full_axis_op = builder.Build<paddle::dialect::FullIntArrayOp>(axis, phi::DataType::INT64, phi::CPUPlace());
  pir::OpResult axis_ = full_axis_op->result(0);
    
  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_, axis_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);
  paddle::dialect::IrTensor dense_out;
  paddle::dialect::IrMetaTensor meta_out(&dense_out);

  phi::ReverseInferMeta(meta_x, axis, &meta_out);

  std::vector<pir::Type> argument_outputs;
  pir::Type out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_out.dtype()), dense_out.dims(), dense_out.layout(), dense_out.lod(), dense_out.offset());
  argument_outputs.push_back(out_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void ReverseOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, pir::Value axis_) {
  VLOG(4) << "Start build ReverseOp";



  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_, axis_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;
  phi::IntArray axis;
  if (axis_.dyn_cast<pir::OpResult>() && axis_.dyn_cast<pir::OpResult>().owner()->isa<paddle::dialect::FullIntArrayOp>()) {
    axis = std::move(phi::IntArray(paddle::dialect::GetInt64Vector(
                          axis_.dyn_cast<pir::OpResult>().owner()
                          ->dyn_cast<paddle::dialect::FullIntArrayOp>()
                          .attribute("value"))));
  } else if (axis_.type().isa<pir::VectorType>()) {
    size_t axis_size = axis_.type().dyn_cast<pir::VectorType>().size();
    axis = std::move(phi::IntArray(std::vector<int64_t>(axis_size, -1)));
    axis.SetFromTensor(true);
  } else if (axis_.type().isa<paddle::dialect::DenseTensorType>()) {
    common::DDim axis_dim = axis_.type().dyn_cast<paddle::dialect::DenseTensorType>().dims();
    size_t axis_size = common::product(axis_dim);
    if (common::contain_unknown_dim(axis_dim)) {
      axis_size = 1;
    }
    axis = std::move(phi::IntArray(std::vector<int64_t>(axis_size, -1)));
    axis.SetFromTensor(true);
  } else {
    PADDLE_THROW(phi::errors::Unimplemented("Only support VectorType or DenseTensorType"));
  }


  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);
  paddle::dialect::IrTensor dense_out;
  paddle::dialect::IrMetaTensor meta_out(&dense_out);

  phi::ReverseInferMeta(meta_x, axis, &meta_out);

  std::vector<pir::Type> argument_outputs;
  pir::Type out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_out.dtype()), dense_out.dims(), dense_out.layout(), dense_out.lod(), dense_out.offset());
  argument_outputs.push_back(out_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void ReverseOp::VerifySig() {
  VLOG(4) << "Start Verifying inputs, outputs and attributes for: ReverseOp.";
  VLOG(4) << "Verifying inputs:";
  {
  auto input_size = num_operands();
  IR_ENFORCE(input_size == 2u,
                    "The size %d of inputs must be equal to 2.", input_size);
  IR_ENFORCE((*this)->operand_source(0).type().isa<paddle::dialect::DenseTensorType>(),
                  "Type validation failed for the 0th input, got %s.", (*this)->operand_source(0).type());
  if (auto vec_type = (*this)->operand_source(1).type().dyn_cast<pir::VectorType>()) {
      for (size_t i = 0; i < vec_type.size(); ++i) {
        IR_ENFORCE(vec_type[i].isa<paddle::dialect::DenseTensorType>(),
                       "Type validation failed for the 1th input, got %s.", (*this)->operand_source(1).type());
      }
  }
  else {
    IR_ENFORCE((*this)->operand_source(1).type().isa<paddle::dialect::DenseTensorType>(),
                   "Type validation failed for the 1th input, got %s.", (*this)->operand_source(1).type());
  }
  }
  VLOG(4) << "Verifying attributes:";
  {
  // Attributes num is 0, not need to check attributes type.
  }
  VLOG(4) << "Verifying outputs:";
  {
  auto output_size = num_results();
  IR_ENFORCE(output_size == 1u,
                    "The size %d of outputs must be equal to 1.", output_size);
  IR_ENFORCE((*this)->result(0).type().isa<paddle::dialect::DenseTensorType>(),
                 "Type validation failed for the 0th output.");
  }
  VLOG(4) << "End Verifying for: ReverseOp.";
}

void ReverseOp::InferMeta( phi::InferMetaContext *infer_meta ) {
  auto fn = PD_INFER_META(phi::ReverseInferMeta);
  fn(infer_meta);
}

phi::DataType ReverseOp::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: ReverseOp";
  


  return expected_kernel_dtype;
}

const char *RmsNormOp::attributes_name[6] = { "epsilon", "begin_norm_axis", "quant_scale", "quant_round_type", "quant_max_bound", "quant_min_bound" };

OpInfoTuple RmsNormOp::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("x", "paddle::dialect::DenseTensorType", false, false, false, false), paddle::dialect::OpInputInfo("bias", "paddle::dialect::DenseTensorType", true, false, false, false), paddle::dialect::OpInputInfo("residual", "paddle::dialect::DenseTensorType", true, false, false, false), paddle::dialect::OpInputInfo("norm_weight", "paddle::dialect::DenseTensorType", false, false, false, false), paddle::dialect::OpInputInfo("norm_bias", "paddle::dialect::DenseTensorType", true, false, false, false) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = { paddle::dialect::OpAttributeInfo("epsilon", "pir::FloatAttribute", ""), paddle::dialect::OpAttributeInfo("begin_norm_axis", "pir::Int32Attribute", ""), paddle::dialect::OpAttributeInfo("quant_scale", "pir::FloatAttribute", ""), paddle::dialect::OpAttributeInfo("quant_round_type", "pir::Int32Attribute", ""), paddle::dialect::OpAttributeInfo("quant_max_bound", "pir::FloatAttribute", ""), paddle::dialect::OpAttributeInfo("quant_min_bound", "pir::FloatAttribute", "") };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("out", "paddle::dialect::DenseTensorType", false, false), paddle::dialect::OpOutputInfo("residual_out", "paddle::dialect::DenseTensorType", true, false) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("RmsNormInferMeta", {"x", "bias", "residual", "norm_weight", "norm_bias", "epsilon", "begin_norm_axis", "quant_scale", "quant_round_type", "quant_max_bound", "quant_min_bound"}, "rms_norm", {"x", "bias", "residual", "norm_weight", "norm_bias", "epsilon", "begin_norm_axis", "quant_scale", "quant_round_type", "quant_max_bound", "quant_min_bound"}, {"x"}, {}, {}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "rms_norm");
}

void RmsNormOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, pir::Value bias_, pir::Value residual_, pir::Value norm_weight_, pir::Value norm_bias_, float epsilon, int begin_norm_axis, float quant_scale, int quant_round_type, float quant_max_bound, float quant_min_bound) {
  VLOG(4) << "Start build RmsNormOp";



  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_, bias_, residual_, norm_weight_, norm_bias_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_epsilon = pir::FloatAttribute::get(pir::IrContext::Instance(), epsilon);
  argument.AddAttribute("epsilon", attr_epsilon);
  pir::Attribute attr_begin_norm_axis = pir::Int32Attribute::get(pir::IrContext::Instance(), begin_norm_axis);
  argument.AddAttribute("begin_norm_axis", attr_begin_norm_axis);
  pir::Attribute attr_quant_scale = pir::FloatAttribute::get(pir::IrContext::Instance(), quant_scale);
  argument.AddAttribute("quant_scale", attr_quant_scale);
  pir::Attribute attr_quant_round_type = pir::Int32Attribute::get(pir::IrContext::Instance(), quant_round_type);
  argument.AddAttribute("quant_round_type", attr_quant_round_type);
  pir::Attribute attr_quant_max_bound = pir::FloatAttribute::get(pir::IrContext::Instance(), quant_max_bound);
  argument.AddAttribute("quant_max_bound", attr_quant_max_bound);
  pir::Attribute attr_quant_min_bound = pir::FloatAttribute::get(pir::IrContext::Instance(), quant_min_bound);
  argument.AddAttribute("quant_min_bound", attr_quant_min_bound);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;
  paddle::dialect::DenseTensorType norm_weight = norm_weight_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)norm_weight;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);

  paddle::dialect::IrMetaTensor meta_bias;
  paddle::dialect::IrTensor ir_tensor_bias;
  if (bias_.impl() != nullptr) {
    paddle::dialect::DenseTensorType bias = bias_.type().dyn_cast<paddle::dialect::DenseTensorType>();
    VLOG(4) << "Builder construction  dense_bias";
    ir_tensor_bias = paddle::dialect::IrTensor(paddle::dialect::TransToPhiDataType(bias.dtype()),
                                                        bias.dims(),
                                                        bias.data_layout(),
                                                        bias.lod(),
                                                        bias.offset());
    VLOG(4) << "Builder construction  meta_bias";
    meta_bias = paddle::dialect::IrMetaTensor(&ir_tensor_bias);
  }


  paddle::dialect::IrMetaTensor meta_residual;
  paddle::dialect::IrTensor ir_tensor_residual;
  if (residual_.impl() != nullptr) {
    paddle::dialect::DenseTensorType residual = residual_.type().dyn_cast<paddle::dialect::DenseTensorType>();
    VLOG(4) << "Builder construction  dense_residual";
    ir_tensor_residual = paddle::dialect::IrTensor(paddle::dialect::TransToPhiDataType(residual.dtype()),
                                                        residual.dims(),
                                                        residual.data_layout(),
                                                        residual.lod(),
                                                        residual.offset());
    VLOG(4) << "Builder construction  meta_residual";
    meta_residual = paddle::dialect::IrMetaTensor(&ir_tensor_residual);
  }


  VLOG(4) << "Builder construction  dense_norm_weight";
  paddle::dialect::IrTensor ir_tensor_norm_weight(paddle::dialect::TransToPhiDataType(norm_weight.dtype()),
                                                      norm_weight.dims(),
                                                      norm_weight.data_layout(),
                                                      norm_weight.lod(),
                                                      norm_weight.offset());
  VLOG(4) << "Builder construction  meta_norm_weight";
  paddle::dialect::IrMetaTensor meta_norm_weight(&ir_tensor_norm_weight);

  paddle::dialect::IrMetaTensor meta_norm_bias;
  paddle::dialect::IrTensor ir_tensor_norm_bias;
  if (norm_bias_.impl() != nullptr) {
    paddle::dialect::DenseTensorType norm_bias = norm_bias_.type().dyn_cast<paddle::dialect::DenseTensorType>();
    VLOG(4) << "Builder construction  dense_norm_bias";
    ir_tensor_norm_bias = paddle::dialect::IrTensor(paddle::dialect::TransToPhiDataType(norm_bias.dtype()),
                                                        norm_bias.dims(),
                                                        norm_bias.data_layout(),
                                                        norm_bias.lod(),
                                                        norm_bias.offset());
    VLOG(4) << "Builder construction  meta_norm_bias";
    meta_norm_bias = paddle::dialect::IrMetaTensor(&ir_tensor_norm_bias);
  }

  paddle::dialect::IrTensor dense_out;
  paddle::dialect::IrMetaTensor meta_out(&dense_out);
  paddle::dialect::IrTensor dense_residual_out;
  paddle::dialect::IrMetaTensor meta_residual_out(&dense_residual_out);

  phi::RmsNormInferMeta(meta_x, meta_bias, meta_residual, meta_norm_weight, meta_norm_bias, epsilon, begin_norm_axis, quant_scale, quant_round_type, quant_max_bound, quant_min_bound, &meta_out, &meta_residual_out);

  std::vector<pir::Type> argument_outputs;
  pir::Type out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_out.dtype()), dense_out.dims(), dense_out.layout(), dense_out.lod(), dense_out.offset());
  argument_outputs.push_back(out_dense_tensor_type);

  pir::Type residual_out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_residual_out.dtype()), dense_residual_out.dims(), dense_residual_out.layout(), dense_residual_out.lod(), dense_residual_out.offset());
  argument_outputs.push_back(residual_out_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void RmsNormOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, pir::Value bias_, pir::Value residual_, pir::Value norm_weight_, pir::Value norm_bias_, pir::AttributeMap attributes) {
  VLOG(4) << "Start build RmsNormOp";


  IR_ENFORCE(
      attributes.find("epsilon") != attributes.end(),
          "'epsilon' Attribute is expected for RmsNormOp. ");
  float epsilon = attributes.at("epsilon").dyn_cast<pir::FloatAttribute>().data();

  IR_ENFORCE(
      attributes.find("begin_norm_axis") != attributes.end(),
          "'begin_norm_axis' Attribute is expected for RmsNormOp. ");
  int begin_norm_axis = attributes.at("begin_norm_axis").dyn_cast<pir::Int32Attribute>().data();

  IR_ENFORCE(
      attributes.find("quant_scale") != attributes.end(),
          "'quant_scale' Attribute is expected for RmsNormOp. ");
  float quant_scale = attributes.at("quant_scale").dyn_cast<pir::FloatAttribute>().data();

  IR_ENFORCE(
      attributes.find("quant_round_type") != attributes.end(),
          "'quant_round_type' Attribute is expected for RmsNormOp. ");
  int quant_round_type = attributes.at("quant_round_type").dyn_cast<pir::Int32Attribute>().data();

  IR_ENFORCE(
      attributes.find("quant_max_bound") != attributes.end(),
          "'quant_max_bound' Attribute is expected for RmsNormOp. ");
  float quant_max_bound = attributes.at("quant_max_bound").dyn_cast<pir::FloatAttribute>().data();

  IR_ENFORCE(
      attributes.find("quant_min_bound") != attributes.end(),
          "'quant_min_bound' Attribute is expected for RmsNormOp. ");
  float quant_min_bound = attributes.at("quant_min_bound").dyn_cast<pir::FloatAttribute>().data();


  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_, bias_, residual_, norm_weight_, norm_bias_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_epsilon = pir::FloatAttribute::get(pir::IrContext::Instance(), epsilon);
  argument.AddAttribute("epsilon", attr_epsilon);
  pir::Attribute attr_begin_norm_axis = pir::Int32Attribute::get(pir::IrContext::Instance(), begin_norm_axis);
  argument.AddAttribute("begin_norm_axis", attr_begin_norm_axis);
  pir::Attribute attr_quant_scale = pir::FloatAttribute::get(pir::IrContext::Instance(), quant_scale);
  argument.AddAttribute("quant_scale", attr_quant_scale);
  pir::Attribute attr_quant_round_type = pir::Int32Attribute::get(pir::IrContext::Instance(), quant_round_type);
  argument.AddAttribute("quant_round_type", attr_quant_round_type);
  pir::Attribute attr_quant_max_bound = pir::FloatAttribute::get(pir::IrContext::Instance(), quant_max_bound);
  argument.AddAttribute("quant_max_bound", attr_quant_max_bound);
  pir::Attribute attr_quant_min_bound = pir::FloatAttribute::get(pir::IrContext::Instance(), quant_min_bound);
  argument.AddAttribute("quant_min_bound", attr_quant_min_bound);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;
  paddle::dialect::DenseTensorType norm_weight = norm_weight_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)norm_weight;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);

  paddle::dialect::IrMetaTensor meta_bias;
  paddle::dialect::IrTensor ir_tensor_bias;
  if (bias_.impl() != nullptr) {
    paddle::dialect::DenseTensorType bias = bias_.type().dyn_cast<paddle::dialect::DenseTensorType>();
    VLOG(4) << "Builder construction  dense_bias";
    ir_tensor_bias = paddle::dialect::IrTensor(paddle::dialect::TransToPhiDataType(bias.dtype()),
                                                        bias.dims(),
                                                        bias.data_layout(),
                                                        bias.lod(),
                                                        bias.offset());
    VLOG(4) << "Builder construction  meta_bias";
    meta_bias = paddle::dialect::IrMetaTensor(&ir_tensor_bias);
  }


  paddle::dialect::IrMetaTensor meta_residual;
  paddle::dialect::IrTensor ir_tensor_residual;
  if (residual_.impl() != nullptr) {
    paddle::dialect::DenseTensorType residual = residual_.type().dyn_cast<paddle::dialect::DenseTensorType>();
    VLOG(4) << "Builder construction  dense_residual";
    ir_tensor_residual = paddle::dialect::IrTensor(paddle::dialect::TransToPhiDataType(residual.dtype()),
                                                        residual.dims(),
                                                        residual.data_layout(),
                                                        residual.lod(),
                                                        residual.offset());
    VLOG(4) << "Builder construction  meta_residual";
    meta_residual = paddle::dialect::IrMetaTensor(&ir_tensor_residual);
  }


  VLOG(4) << "Builder construction  dense_norm_weight";
  paddle::dialect::IrTensor ir_tensor_norm_weight(paddle::dialect::TransToPhiDataType(norm_weight.dtype()),
                                                      norm_weight.dims(),
                                                      norm_weight.data_layout(),
                                                      norm_weight.lod(),
                                                      norm_weight.offset());
  VLOG(4) << "Builder construction  meta_norm_weight";
  paddle::dialect::IrMetaTensor meta_norm_weight(&ir_tensor_norm_weight);

  paddle::dialect::IrMetaTensor meta_norm_bias;
  paddle::dialect::IrTensor ir_tensor_norm_bias;
  if (norm_bias_.impl() != nullptr) {
    paddle::dialect::DenseTensorType norm_bias = norm_bias_.type().dyn_cast<paddle::dialect::DenseTensorType>();
    VLOG(4) << "Builder construction  dense_norm_bias";
    ir_tensor_norm_bias = paddle::dialect::IrTensor(paddle::dialect::TransToPhiDataType(norm_bias.dtype()),
                                                        norm_bias.dims(),
                                                        norm_bias.data_layout(),
                                                        norm_bias.lod(),
                                                        norm_bias.offset());
    VLOG(4) << "Builder construction  meta_norm_bias";
    meta_norm_bias = paddle::dialect::IrMetaTensor(&ir_tensor_norm_bias);
  }

  paddle::dialect::IrTensor dense_out;
  paddle::dialect::IrMetaTensor meta_out(&dense_out);
  paddle::dialect::IrTensor dense_residual_out;
  paddle::dialect::IrMetaTensor meta_residual_out(&dense_residual_out);

  phi::RmsNormInferMeta(meta_x, meta_bias, meta_residual, meta_norm_weight, meta_norm_bias, epsilon, begin_norm_axis, quant_scale, quant_round_type, quant_max_bound, quant_min_bound, &meta_out, &meta_residual_out);

  std::vector<pir::Type> argument_outputs;
  pir::Type out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_out.dtype()), dense_out.dims(), dense_out.layout(), dense_out.lod(), dense_out.offset());
  argument_outputs.push_back(out_dense_tensor_type);

  pir::Type residual_out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_residual_out.dtype()), dense_residual_out.dims(), dense_residual_out.layout(), dense_residual_out.lod(), dense_residual_out.offset());
  argument_outputs.push_back(residual_out_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void RmsNormOp::VerifySig() {
  VLOG(4) << "Start Verifying inputs, outputs and attributes for: RmsNormOp.";
  VLOG(4) << "Verifying inputs:";
  {
  auto input_size = num_operands();
  IR_ENFORCE(input_size == 5u,
                    "The size %d of inputs must be equal to 5.", input_size);
  IR_ENFORCE((*this)->operand_source(0).type().isa<paddle::dialect::DenseTensorType>(),
                  "Type validation failed for the 0th input, got %s.", (*this)->operand_source(0).type());
  if (auto val = (*this)->operand(1)) {
    IR_ENFORCE(val.type().isa<paddle::dialect::DenseTensorType>(),
                   "Type validation failed for the 1th input, got %s.", (*this)->operand_source(1).type());
  }
  if (auto val = (*this)->operand(2)) {
    IR_ENFORCE(val.type().isa<paddle::dialect::DenseTensorType>(),
                   "Type validation failed for the 2th input, got %s.", (*this)->operand_source(2).type());
  }
  IR_ENFORCE((*this)->operand_source(3).type().isa<paddle::dialect::DenseTensorType>(),
                  "Type validation failed for the 3th input, got %s.", (*this)->operand_source(3).type());
  if (auto val = (*this)->operand(4)) {
    IR_ENFORCE(val.type().isa<paddle::dialect::DenseTensorType>(),
                   "Type validation failed for the 4th input, got %s.", (*this)->operand_source(4).type());
  }
  }
  VLOG(4) << "Verifying attributes:";
  {
  auto& attributes = this->attributes();
  IR_ENFORCE(attributes.count("epsilon")>0,
                 "epsilon does not exist.");
  IR_ENFORCE(attributes.at("epsilon").isa<pir::FloatAttribute>(),
                 "Type of attribute: epsilon is not pir::FloatAttribute.");

  IR_ENFORCE(attributes.count("begin_norm_axis")>0,
                 "begin_norm_axis does not exist.");
  IR_ENFORCE(attributes.at("begin_norm_axis").isa<pir::Int32Attribute>(),
                 "Type of attribute: begin_norm_axis is not pir::Int32Attribute.");

  IR_ENFORCE(attributes.count("quant_scale")>0,
                 "quant_scale does not exist.");
  IR_ENFORCE(attributes.at("quant_scale").isa<pir::FloatAttribute>(),
                 "Type of attribute: quant_scale is not pir::FloatAttribute.");

  IR_ENFORCE(attributes.count("quant_round_type")>0,
                 "quant_round_type does not exist.");
  IR_ENFORCE(attributes.at("quant_round_type").isa<pir::Int32Attribute>(),
                 "Type of attribute: quant_round_type is not pir::Int32Attribute.");

  IR_ENFORCE(attributes.count("quant_max_bound")>0,
                 "quant_max_bound does not exist.");
  IR_ENFORCE(attributes.at("quant_max_bound").isa<pir::FloatAttribute>(),
                 "Type of attribute: quant_max_bound is not pir::FloatAttribute.");

  IR_ENFORCE(attributes.count("quant_min_bound")>0,
                 "quant_min_bound does not exist.");
  IR_ENFORCE(attributes.at("quant_min_bound").isa<pir::FloatAttribute>(),
                 "Type of attribute: quant_min_bound is not pir::FloatAttribute.");

  }
  VLOG(4) << "Verifying outputs:";
  {
  auto output_size = num_results();
  IR_ENFORCE(output_size == 2u,
                    "The size %d of outputs must be equal to 2.", output_size);
  IR_ENFORCE((*this)->result(0).type().isa<paddle::dialect::DenseTensorType>(),
                 "Type validation failed for the 0th output.");
  if (auto output_1_type = (*this)->result(1).type()) {
    IR_ENFORCE(output_1_type.isa<paddle::dialect::DenseTensorType>(),
                   "Type validation failed for the 1th output.");
  }
  }
  VLOG(4) << "End Verifying for: RmsNormOp.";
}

void RmsNormOp::InferMeta( phi::InferMetaContext *infer_meta ) {
  auto fn = PD_INFER_META(phi::RmsNormInferMeta);
  fn(infer_meta);
}

phi::DataType RmsNormOp::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: RmsNormOp";
  


  return expected_kernel_dtype;
}

const char *Rmsprop_Op::attributes_name[5] = { "epsilon", "decay", "momentum", "centered", "multi_precision" };

OpInfoTuple Rmsprop_Op::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("param", "paddle::dialect::DenseTensorType", false, false, false, false), paddle::dialect::OpInputInfo("mean_square", "paddle::dialect::DenseTensorType", false, false, false, false), paddle::dialect::OpInputInfo("grad", "paddle::dialect::DenseTensorType", false, false, false, false), paddle::dialect::OpInputInfo("moment", "paddle::dialect::DenseTensorType", false, false, false, false), paddle::dialect::OpInputInfo("learning_rate", "paddle::dialect::DenseTensorType", false, false, false, false), paddle::dialect::OpInputInfo("mean_grad", "paddle::dialect::DenseTensorType", true, false, false, false), paddle::dialect::OpInputInfo("master_param", "paddle::dialect::DenseTensorType", true, false, false, false) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = { paddle::dialect::OpAttributeInfo("epsilon", "pir::FloatAttribute", ""), paddle::dialect::OpAttributeInfo("decay", "pir::FloatAttribute", ""), paddle::dialect::OpAttributeInfo("momentum", "pir::FloatAttribute", ""), paddle::dialect::OpAttributeInfo("centered", "pir::BoolAttribute", ""), paddle::dialect::OpAttributeInfo("multi_precision", "pir::BoolAttribute", "") };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("param_out", "paddle::dialect::DenseTensorType", false, false), paddle::dialect::OpOutputInfo("moment_out", "paddle::dialect::DenseTensorType", false, false), paddle::dialect::OpOutputInfo("mean_square_out", "paddle::dialect::DenseTensorType", false, false), paddle::dialect::OpOutputInfo("mean_grad_out", "paddle::dialect::DenseTensorType", false, false), paddle::dialect::OpOutputInfo("master_param_outs", "paddle::dialect::DenseTensorType", true, false) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("RmspropInferMeta", {"param", "mean_square", "grad", "moment", "learning_rate", "mean_grad", "master_param", "epsilon", "decay", "momentum", "centered", "multi_precision"}, "rmsprop", {"param", "mean_square", "grad", "moment", "learning_rate", "mean_grad", "master_param", "epsilon", "decay", "momentum", "centered", "multi_precision"}, {"param"}, {}, {{"param_out", "param"},{"moment_out", "moment"},{"mean_square_out", "mean_square"},{"mean_grad_out", "mean_grad"},{"master_param_outs", "master_param"}}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "rmsprop_");
}

void Rmsprop_Op::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value param_, pir::Value mean_square_, pir::Value grad_, pir::Value moment_, pir::Value learning_rate_, pir::Value mean_grad_, pir::Value master_param_, float epsilon, float decay, float momentum, bool centered, bool multi_precision) {
  VLOG(4) << "Start build Rmsprop_Op";



  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {param_, mean_square_, grad_, moment_, learning_rate_, mean_grad_, master_param_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_epsilon = pir::FloatAttribute::get(pir::IrContext::Instance(), epsilon);
  argument.AddAttribute("epsilon", attr_epsilon);
  pir::Attribute attr_decay = pir::FloatAttribute::get(pir::IrContext::Instance(), decay);
  argument.AddAttribute("decay", attr_decay);
  pir::Attribute attr_momentum = pir::FloatAttribute::get(pir::IrContext::Instance(), momentum);
  argument.AddAttribute("momentum", attr_momentum);
  pir::Attribute attr_centered = pir::BoolAttribute::get(pir::IrContext::Instance(), centered);
  argument.AddAttribute("centered", attr_centered);
  pir::Attribute attr_multi_precision = pir::BoolAttribute::get(pir::IrContext::Instance(), multi_precision);
  argument.AddAttribute("multi_precision", attr_multi_precision);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType param = param_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)param;
  paddle::dialect::DenseTensorType mean_square = mean_square_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)mean_square;
  paddle::dialect::DenseTensorType grad = grad_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)grad;
  paddle::dialect::DenseTensorType moment = moment_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)moment;
  paddle::dialect::DenseTensorType learning_rate = learning_rate_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)learning_rate;

  VLOG(4) << "Builder construction  dense_param";
  paddle::dialect::IrTensor ir_tensor_param(paddle::dialect::TransToPhiDataType(param.dtype()),
                                                      param.dims(),
                                                      param.data_layout(),
                                                      param.lod(),
                                                      param.offset());
  VLOG(4) << "Builder construction  meta_param";
  paddle::dialect::IrMetaTensor meta_param(&ir_tensor_param);

  VLOG(4) << "Builder construction  dense_mean_square";
  paddle::dialect::IrTensor ir_tensor_mean_square(paddle::dialect::TransToPhiDataType(mean_square.dtype()),
                                                      mean_square.dims(),
                                                      mean_square.data_layout(),
                                                      mean_square.lod(),
                                                      mean_square.offset());
  VLOG(4) << "Builder construction  meta_mean_square";
  paddle::dialect::IrMetaTensor meta_mean_square(&ir_tensor_mean_square);

  VLOG(4) << "Builder construction  dense_grad";
  paddle::dialect::IrTensor ir_tensor_grad(paddle::dialect::TransToPhiDataType(grad.dtype()),
                                                      grad.dims(),
                                                      grad.data_layout(),
                                                      grad.lod(),
                                                      grad.offset());
  VLOG(4) << "Builder construction  meta_grad";
  paddle::dialect::IrMetaTensor meta_grad(&ir_tensor_grad);

  VLOG(4) << "Builder construction  dense_moment";
  paddle::dialect::IrTensor ir_tensor_moment(paddle::dialect::TransToPhiDataType(moment.dtype()),
                                                      moment.dims(),
                                                      moment.data_layout(),
                                                      moment.lod(),
                                                      moment.offset());
  VLOG(4) << "Builder construction  meta_moment";
  paddle::dialect::IrMetaTensor meta_moment(&ir_tensor_moment);

  VLOG(4) << "Builder construction  dense_learning_rate";
  paddle::dialect::IrTensor ir_tensor_learning_rate(paddle::dialect::TransToPhiDataType(learning_rate.dtype()),
                                                      learning_rate.dims(),
                                                      learning_rate.data_layout(),
                                                      learning_rate.lod(),
                                                      learning_rate.offset());
  VLOG(4) << "Builder construction  meta_learning_rate";
  paddle::dialect::IrMetaTensor meta_learning_rate(&ir_tensor_learning_rate);

  paddle::dialect::IrMetaTensor meta_mean_grad;
  paddle::dialect::IrTensor ir_tensor_mean_grad;
  if (mean_grad_.impl() != nullptr) {
    paddle::dialect::DenseTensorType mean_grad = mean_grad_.type().dyn_cast<paddle::dialect::DenseTensorType>();
    VLOG(4) << "Builder construction  dense_mean_grad";
    ir_tensor_mean_grad = paddle::dialect::IrTensor(paddle::dialect::TransToPhiDataType(mean_grad.dtype()),
                                                        mean_grad.dims(),
                                                        mean_grad.data_layout(),
                                                        mean_grad.lod(),
                                                        mean_grad.offset());
    VLOG(4) << "Builder construction  meta_mean_grad";
    meta_mean_grad = paddle::dialect::IrMetaTensor(&ir_tensor_mean_grad);
  }


  paddle::dialect::IrMetaTensor meta_master_param;
  paddle::dialect::IrTensor ir_tensor_master_param;
  if (master_param_.impl() != nullptr) {
    paddle::dialect::DenseTensorType master_param = master_param_.type().dyn_cast<paddle::dialect::DenseTensorType>();
    VLOG(4) << "Builder construction  dense_master_param";
    ir_tensor_master_param = paddle::dialect::IrTensor(paddle::dialect::TransToPhiDataType(master_param.dtype()),
                                                        master_param.dims(),
                                                        master_param.data_layout(),
                                                        master_param.lod(),
                                                        master_param.offset());
    VLOG(4) << "Builder construction  meta_master_param";
    meta_master_param = paddle::dialect::IrMetaTensor(&ir_tensor_master_param);
  }

  paddle::dialect::IrTensor dense_param_out;
  paddle::dialect::IrMetaTensor meta_param_out(&dense_param_out);
  paddle::dialect::IrTensor dense_moment_out;
  paddle::dialect::IrMetaTensor meta_moment_out(&dense_moment_out);
  paddle::dialect::IrTensor dense_mean_square_out;
  paddle::dialect::IrMetaTensor meta_mean_square_out(&dense_mean_square_out);
  paddle::dialect::IrTensor dense_mean_grad_out;
  paddle::dialect::IrMetaTensor meta_mean_grad_out(&dense_mean_grad_out);
  paddle::dialect::IrTensor dense_master_param_outs;
  paddle::dialect::IrMetaTensor meta_master_param_outs(&dense_master_param_outs);

  phi::RmspropInferMeta(meta_param, meta_mean_square, meta_grad, meta_moment, meta_learning_rate, meta_mean_grad, meta_master_param, epsilon, decay, momentum, centered, multi_precision, &meta_param_out, &meta_moment_out, &meta_mean_square_out, &meta_mean_grad_out, &meta_master_param_outs);

  std::vector<pir::Type> argument_outputs;
  pir::Type param_out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_param_out.dtype()), dense_param_out.dims(), dense_param_out.layout(), dense_param_out.lod(), dense_param_out.offset());
  argument_outputs.push_back(param_out_dense_tensor_type);

  pir::Type moment_out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_moment_out.dtype()), dense_moment_out.dims(), dense_moment_out.layout(), dense_moment_out.lod(), dense_moment_out.offset());
  argument_outputs.push_back(moment_out_dense_tensor_type);

  pir::Type mean_square_out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_mean_square_out.dtype()), dense_mean_square_out.dims(), dense_mean_square_out.layout(), dense_mean_square_out.lod(), dense_mean_square_out.offset());
  argument_outputs.push_back(mean_square_out_dense_tensor_type);

  pir::Type mean_grad_out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_mean_grad_out.dtype()), dense_mean_grad_out.dims(), dense_mean_grad_out.layout(), dense_mean_grad_out.lod(), dense_mean_grad_out.offset());
  argument_outputs.push_back(mean_grad_out_dense_tensor_type);

  if (master_param_.impl() != nullptr) {
    pir::Type master_param_outs_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_master_param_outs.dtype()), dense_master_param_outs.dims(), dense_master_param_outs.layout(), dense_master_param_outs.lod(), dense_master_param_outs.offset());
    argument_outputs.push_back(master_param_outs_dense_tensor_type);
  } else {
    pir::Type master_param_outs_type;
    argument_outputs.push_back(master_param_outs_type);
  }

  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void Rmsprop_Op::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value param_, pir::Value mean_square_, pir::Value grad_, pir::Value moment_, pir::Value learning_rate_, pir::Value mean_grad_, pir::Value master_param_, pir::AttributeMap attributes) {
  VLOG(4) << "Start build Rmsprop_Op";


  IR_ENFORCE(
      attributes.find("epsilon") != attributes.end(),
          "'epsilon' Attribute is expected for Rmsprop_Op. ");
  float epsilon = attributes.at("epsilon").dyn_cast<pir::FloatAttribute>().data();

  IR_ENFORCE(
      attributes.find("decay") != attributes.end(),
          "'decay' Attribute is expected for Rmsprop_Op. ");
  float decay = attributes.at("decay").dyn_cast<pir::FloatAttribute>().data();

  IR_ENFORCE(
      attributes.find("momentum") != attributes.end(),
          "'momentum' Attribute is expected for Rmsprop_Op. ");
  float momentum = attributes.at("momentum").dyn_cast<pir::FloatAttribute>().data();

  IR_ENFORCE(
      attributes.find("centered") != attributes.end(),
          "'centered' Attribute is expected for Rmsprop_Op. ");
  bool centered = attributes.at("centered").dyn_cast<pir::BoolAttribute>().data();

  IR_ENFORCE(
      attributes.find("multi_precision") != attributes.end(),
          "'multi_precision' Attribute is expected for Rmsprop_Op. ");
  bool multi_precision = attributes.at("multi_precision").dyn_cast<pir::BoolAttribute>().data();


  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {param_, mean_square_, grad_, moment_, learning_rate_, mean_grad_, master_param_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_epsilon = pir::FloatAttribute::get(pir::IrContext::Instance(), epsilon);
  argument.AddAttribute("epsilon", attr_epsilon);
  pir::Attribute attr_decay = pir::FloatAttribute::get(pir::IrContext::Instance(), decay);
  argument.AddAttribute("decay", attr_decay);
  pir::Attribute attr_momentum = pir::FloatAttribute::get(pir::IrContext::Instance(), momentum);
  argument.AddAttribute("momentum", attr_momentum);
  pir::Attribute attr_centered = pir::BoolAttribute::get(pir::IrContext::Instance(), centered);
  argument.AddAttribute("centered", attr_centered);
  pir::Attribute attr_multi_precision = pir::BoolAttribute::get(pir::IrContext::Instance(), multi_precision);
  argument.AddAttribute("multi_precision", attr_multi_precision);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType param = param_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)param;
  paddle::dialect::DenseTensorType mean_square = mean_square_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)mean_square;
  paddle::dialect::DenseTensorType grad = grad_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)grad;
  paddle::dialect::DenseTensorType moment = moment_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)moment;
  paddle::dialect::DenseTensorType learning_rate = learning_rate_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)learning_rate;

  VLOG(4) << "Builder construction  dense_param";
  paddle::dialect::IrTensor ir_tensor_param(paddle::dialect::TransToPhiDataType(param.dtype()),
                                                      param.dims(),
                                                      param.data_layout(),
                                                      param.lod(),
                                                      param.offset());
  VLOG(4) << "Builder construction  meta_param";
  paddle::dialect::IrMetaTensor meta_param(&ir_tensor_param);

  VLOG(4) << "Builder construction  dense_mean_square";
  paddle::dialect::IrTensor ir_tensor_mean_square(paddle::dialect::TransToPhiDataType(mean_square.dtype()),
                                                      mean_square.dims(),
                                                      mean_square.data_layout(),
                                                      mean_square.lod(),
                                                      mean_square.offset());
  VLOG(4) << "Builder construction  meta_mean_square";
  paddle::dialect::IrMetaTensor meta_mean_square(&ir_tensor_mean_square);

  VLOG(4) << "Builder construction  dense_grad";
  paddle::dialect::IrTensor ir_tensor_grad(paddle::dialect::TransToPhiDataType(grad.dtype()),
                                                      grad.dims(),
                                                      grad.data_layout(),
                                                      grad.lod(),
                                                      grad.offset());
  VLOG(4) << "Builder construction  meta_grad";
  paddle::dialect::IrMetaTensor meta_grad(&ir_tensor_grad);

  VLOG(4) << "Builder construction  dense_moment";
  paddle::dialect::IrTensor ir_tensor_moment(paddle::dialect::TransToPhiDataType(moment.dtype()),
                                                      moment.dims(),
                                                      moment.data_layout(),
                                                      moment.lod(),
                                                      moment.offset());
  VLOG(4) << "Builder construction  meta_moment";
  paddle::dialect::IrMetaTensor meta_moment(&ir_tensor_moment);

  VLOG(4) << "Builder construction  dense_learning_rate";
  paddle::dialect::IrTensor ir_tensor_learning_rate(paddle::dialect::TransToPhiDataType(learning_rate.dtype()),
                                                      learning_rate.dims(),
                                                      learning_rate.data_layout(),
                                                      learning_rate.lod(),
                                                      learning_rate.offset());
  VLOG(4) << "Builder construction  meta_learning_rate";
  paddle::dialect::IrMetaTensor meta_learning_rate(&ir_tensor_learning_rate);

  paddle::dialect::IrMetaTensor meta_mean_grad;
  paddle::dialect::IrTensor ir_tensor_mean_grad;
  if (mean_grad_.impl() != nullptr) {
    paddle::dialect::DenseTensorType mean_grad = mean_grad_.type().dyn_cast<paddle::dialect::DenseTensorType>();
    VLOG(4) << "Builder construction  dense_mean_grad";
    ir_tensor_mean_grad = paddle::dialect::IrTensor(paddle::dialect::TransToPhiDataType(mean_grad.dtype()),
                                                        mean_grad.dims(),
                                                        mean_grad.data_layout(),
                                                        mean_grad.lod(),
                                                        mean_grad.offset());
    VLOG(4) << "Builder construction  meta_mean_grad";
    meta_mean_grad = paddle::dialect::IrMetaTensor(&ir_tensor_mean_grad);
  }


  paddle::dialect::IrMetaTensor meta_master_param;
  paddle::dialect::IrTensor ir_tensor_master_param;
  if (master_param_.impl() != nullptr) {
    paddle::dialect::DenseTensorType master_param = master_param_.type().dyn_cast<paddle::dialect::DenseTensorType>();
    VLOG(4) << "Builder construction  dense_master_param";
    ir_tensor_master_param = paddle::dialect::IrTensor(paddle::dialect::TransToPhiDataType(master_param.dtype()),
                                                        master_param.dims(),
                                                        master_param.data_layout(),
                                                        master_param.lod(),
                                                        master_param.offset());
    VLOG(4) << "Builder construction  meta_master_param";
    meta_master_param = paddle::dialect::IrMetaTensor(&ir_tensor_master_param);
  }

  paddle::dialect::IrTensor dense_param_out;
  paddle::dialect::IrMetaTensor meta_param_out(&dense_param_out);
  paddle::dialect::IrTensor dense_moment_out;
  paddle::dialect::IrMetaTensor meta_moment_out(&dense_moment_out);
  paddle::dialect::IrTensor dense_mean_square_out;
  paddle::dialect::IrMetaTensor meta_mean_square_out(&dense_mean_square_out);
  paddle::dialect::IrTensor dense_mean_grad_out;
  paddle::dialect::IrMetaTensor meta_mean_grad_out(&dense_mean_grad_out);
  paddle::dialect::IrTensor dense_master_param_outs;
  paddle::dialect::IrMetaTensor meta_master_param_outs(&dense_master_param_outs);

  phi::RmspropInferMeta(meta_param, meta_mean_square, meta_grad, meta_moment, meta_learning_rate, meta_mean_grad, meta_master_param, epsilon, decay, momentum, centered, multi_precision, &meta_param_out, &meta_moment_out, &meta_mean_square_out, &meta_mean_grad_out, &meta_master_param_outs);

  std::vector<pir::Type> argument_outputs;
  pir::Type param_out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_param_out.dtype()), dense_param_out.dims(), dense_param_out.layout(), dense_param_out.lod(), dense_param_out.offset());
  argument_outputs.push_back(param_out_dense_tensor_type);

  pir::Type moment_out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_moment_out.dtype()), dense_moment_out.dims(), dense_moment_out.layout(), dense_moment_out.lod(), dense_moment_out.offset());
  argument_outputs.push_back(moment_out_dense_tensor_type);

  pir::Type mean_square_out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_mean_square_out.dtype()), dense_mean_square_out.dims(), dense_mean_square_out.layout(), dense_mean_square_out.lod(), dense_mean_square_out.offset());
  argument_outputs.push_back(mean_square_out_dense_tensor_type);

  pir::Type mean_grad_out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_mean_grad_out.dtype()), dense_mean_grad_out.dims(), dense_mean_grad_out.layout(), dense_mean_grad_out.lod(), dense_mean_grad_out.offset());
  argument_outputs.push_back(mean_grad_out_dense_tensor_type);

  if (master_param_.impl() != nullptr) {
    pir::Type master_param_outs_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_master_param_outs.dtype()), dense_master_param_outs.dims(), dense_master_param_outs.layout(), dense_master_param_outs.lod(), dense_master_param_outs.offset());
    argument_outputs.push_back(master_param_outs_dense_tensor_type);
  } else {
    pir::Type master_param_outs_type;
    argument_outputs.push_back(master_param_outs_type);
  }

  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void Rmsprop_Op::VerifySig() {
  VLOG(4) << "Start Verifying inputs, outputs and attributes for: Rmsprop_Op.";
  VLOG(4) << "Verifying inputs:";
  {
  auto input_size = num_operands();
  IR_ENFORCE(input_size == 7u,
                    "The size %d of inputs must be equal to 7.", input_size);
  IR_ENFORCE((*this)->operand_source(0).type().isa<paddle::dialect::DenseTensorType>(),
                  "Type validation failed for the 0th input, got %s.", (*this)->operand_source(0).type());
  IR_ENFORCE((*this)->operand_source(1).type().isa<paddle::dialect::DenseTensorType>(),
                  "Type validation failed for the 1th input, got %s.", (*this)->operand_source(1).type());
  IR_ENFORCE((*this)->operand_source(2).type().isa<paddle::dialect::DenseTensorType>(),
                  "Type validation failed for the 2th input, got %s.", (*this)->operand_source(2).type());
  IR_ENFORCE((*this)->operand_source(3).type().isa<paddle::dialect::DenseTensorType>(),
                  "Type validation failed for the 3th input, got %s.", (*this)->operand_source(3).type());
  IR_ENFORCE((*this)->operand_source(4).type().isa<paddle::dialect::DenseTensorType>(),
                  "Type validation failed for the 4th input, got %s.", (*this)->operand_source(4).type());
  if (auto val = (*this)->operand(5)) {
    IR_ENFORCE(val.type().isa<paddle::dialect::DenseTensorType>(),
                   "Type validation failed for the 5th input, got %s.", (*this)->operand_source(5).type());
  }
  if (auto val = (*this)->operand(6)) {
    IR_ENFORCE(val.type().isa<paddle::dialect::DenseTensorType>(),
                   "Type validation failed for the 6th input, got %s.", (*this)->operand_source(6).type());
  }
  }
  VLOG(4) << "Verifying attributes:";
  {
  auto& attributes = this->attributes();
  IR_ENFORCE(attributes.count("epsilon")>0,
                 "epsilon does not exist.");
  IR_ENFORCE(attributes.at("epsilon").isa<pir::FloatAttribute>(),
                 "Type of attribute: epsilon is not pir::FloatAttribute.");

  IR_ENFORCE(attributes.count("decay")>0,
                 "decay does not exist.");
  IR_ENFORCE(attributes.at("decay").isa<pir::FloatAttribute>(),
                 "Type of attribute: decay is not pir::FloatAttribute.");

  IR_ENFORCE(attributes.count("momentum")>0,
                 "momentum does not exist.");
  IR_ENFORCE(attributes.at("momentum").isa<pir::FloatAttribute>(),
                 "Type of attribute: momentum is not pir::FloatAttribute.");

  IR_ENFORCE(attributes.count("centered")>0,
                 "centered does not exist.");
  IR_ENFORCE(attributes.at("centered").isa<pir::BoolAttribute>(),
                 "Type of attribute: centered is not pir::BoolAttribute.");

  IR_ENFORCE(attributes.count("multi_precision")>0,
                 "multi_precision does not exist.");
  IR_ENFORCE(attributes.at("multi_precision").isa<pir::BoolAttribute>(),
                 "Type of attribute: multi_precision is not pir::BoolAttribute.");

  }
  VLOG(4) << "Verifying outputs:";
  {
  auto output_size = num_results();
  IR_ENFORCE(output_size == 5u,
                    "The size %d of outputs must be equal to 5.", output_size);
  IR_ENFORCE((*this)->result(0).type().isa<paddle::dialect::DenseTensorType>(),
                 "Type validation failed for the 0th output.");
  IR_ENFORCE((*this)->result(1).type().isa<paddle::dialect::DenseTensorType>(),
                 "Type validation failed for the 1th output.");
  IR_ENFORCE((*this)->result(2).type().isa<paddle::dialect::DenseTensorType>(),
                 "Type validation failed for the 2th output.");
  IR_ENFORCE((*this)->result(3).type().isa<paddle::dialect::DenseTensorType>(),
                 "Type validation failed for the 3th output.");
  if (auto output_4_type = (*this)->result(4).type()) {
    IR_ENFORCE(output_4_type.isa<paddle::dialect::DenseTensorType>(),
                   "Type validation failed for the 4th output.");
  }
  }
  VLOG(4) << "End Verifying for: Rmsprop_Op.";
}

void Rmsprop_Op::InferMeta( phi::InferMetaContext *infer_meta ) {
  auto fn = PD_INFER_META(phi::RmspropInferMeta);
  fn(infer_meta);
}

phi::DataType Rmsprop_Op::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: Rmsprop_Op";
  


  return expected_kernel_dtype;
}

const char *RmspropDenseParamSparseGrad_Op::attributes_name[5] = { "epsilon", "decay", "momentum", "centered", "multi_precision" };

OpInfoTuple RmspropDenseParamSparseGrad_Op::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("param", "paddle::dialect::DenseTensorType", false, false, false, false), paddle::dialect::OpInputInfo("mean_square", "paddle::dialect::DenseTensorType", false, false, false, false), paddle::dialect::OpInputInfo("grad", "paddle::dialect::SelectedRowsType", false, false, false, false), paddle::dialect::OpInputInfo("moment", "paddle::dialect::DenseTensorType", false, false, false, false), paddle::dialect::OpInputInfo("learning_rate", "paddle::dialect::DenseTensorType", false, false, false, false), paddle::dialect::OpInputInfo("mean_grad", "paddle::dialect::DenseTensorType", true, false, false, false), paddle::dialect::OpInputInfo("master_param", "paddle::dialect::DenseTensorType", true, false, false, false) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = { paddle::dialect::OpAttributeInfo("epsilon", "pir::FloatAttribute", ""), paddle::dialect::OpAttributeInfo("decay", "pir::FloatAttribute", ""), paddle::dialect::OpAttributeInfo("momentum", "pir::FloatAttribute", ""), paddle::dialect::OpAttributeInfo("centered", "pir::BoolAttribute", ""), paddle::dialect::OpAttributeInfo("multi_precision", "pir::BoolAttribute", "") };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("param_out", "paddle::dialect::DenseTensorType", false, false), paddle::dialect::OpOutputInfo("moment_out", "paddle::dialect::DenseTensorType", false, false), paddle::dialect::OpOutputInfo("mean_square_out", "paddle::dialect::DenseTensorType", false, false), paddle::dialect::OpOutputInfo("mean_grad_out", "paddle::dialect::DenseTensorType", false, false), paddle::dialect::OpOutputInfo("master_param_outs", "paddle::dialect::DenseTensorType", true, false) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("RmspropInferMeta", {"param", "mean_square", "grad", "moment", "learning_rate", "mean_grad", "master_param", "epsilon", "decay", "momentum", "centered", "multi_precision"}, "rmsprop_dense_param_sparse_grad", {"param", "mean_square", "grad", "moment", "learning_rate", "mean_grad", "master_param", "epsilon", "decay", "momentum", "centered", "multi_precision"}, {"param"}, {}, {{"param_out", "param"},{"moment_out", "moment"},{"mean_square_out", "mean_square"},{"mean_grad_out", "mean_grad"},{"master_param_outs", "master_param"}}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "rmsprop_");
}

void RmspropDenseParamSparseGrad_Op::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value param_, pir::Value mean_square_, pir::Value grad_, pir::Value moment_, pir::Value learning_rate_, pir::Value mean_grad_, pir::Value master_param_, float epsilon, float decay, float momentum, bool centered, bool multi_precision) {
  VLOG(4) << "Start build RmspropDenseParamSparseGrad_Op";



  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {param_, mean_square_, grad_, moment_, learning_rate_, mean_grad_, master_param_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_epsilon = pir::FloatAttribute::get(pir::IrContext::Instance(), epsilon);
  argument.AddAttribute("epsilon", attr_epsilon);
  pir::Attribute attr_decay = pir::FloatAttribute::get(pir::IrContext::Instance(), decay);
  argument.AddAttribute("decay", attr_decay);
  pir::Attribute attr_momentum = pir::FloatAttribute::get(pir::IrContext::Instance(), momentum);
  argument.AddAttribute("momentum", attr_momentum);
  pir::Attribute attr_centered = pir::BoolAttribute::get(pir::IrContext::Instance(), centered);
  argument.AddAttribute("centered", attr_centered);
  pir::Attribute attr_multi_precision = pir::BoolAttribute::get(pir::IrContext::Instance(), multi_precision);
  argument.AddAttribute("multi_precision", attr_multi_precision);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType param = param_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)param;
  paddle::dialect::DenseTensorType mean_square = mean_square_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)mean_square;
  paddle::dialect::SelectedRowsType grad = grad_.type().dyn_cast<paddle::dialect::SelectedRowsType>(); (void)grad;
  paddle::dialect::DenseTensorType moment = moment_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)moment;
  paddle::dialect::DenseTensorType learning_rate = learning_rate_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)learning_rate;

  VLOG(4) << "Builder construction  dense_param";
  paddle::dialect::IrTensor ir_tensor_param(paddle::dialect::TransToPhiDataType(param.dtype()),
                                                      param.dims(),
                                                      param.data_layout(),
                                                      param.lod(),
                                                      param.offset());
  VLOG(4) << "Builder construction  meta_param";
  paddle::dialect::IrMetaTensor meta_param(&ir_tensor_param);

  VLOG(4) << "Builder construction  dense_mean_square";
  paddle::dialect::IrTensor ir_tensor_mean_square(paddle::dialect::TransToPhiDataType(mean_square.dtype()),
                                                      mean_square.dims(),
                                                      mean_square.data_layout(),
                                                      mean_square.lod(),
                                                      mean_square.offset());
  VLOG(4) << "Builder construction  meta_mean_square";
  paddle::dialect::IrMetaTensor meta_mean_square(&ir_tensor_mean_square);

  VLOG(4) << "Builder construction  dense_grad";
  paddle::dialect::IrTensor ir_tensor_grad(paddle::dialect::TransToPhiDataType(grad.dtype()),
                                                      grad.dims(),
                                                      grad.data_layout(),
                                                      grad.lod(),
                                                      grad.offset());
  VLOG(4) << "Builder construction  meta_grad";
  paddle::dialect::IrMetaTensor meta_grad(&ir_tensor_grad);

  VLOG(4) << "Builder construction  dense_moment";
  paddle::dialect::IrTensor ir_tensor_moment(paddle::dialect::TransToPhiDataType(moment.dtype()),
                                                      moment.dims(),
                                                      moment.data_layout(),
                                                      moment.lod(),
                                                      moment.offset());
  VLOG(4) << "Builder construction  meta_moment";
  paddle::dialect::IrMetaTensor meta_moment(&ir_tensor_moment);

  VLOG(4) << "Builder construction  dense_learning_rate";
  paddle::dialect::IrTensor ir_tensor_learning_rate(paddle::dialect::TransToPhiDataType(learning_rate.dtype()),
                                                      learning_rate.dims(),
                                                      learning_rate.data_layout(),
                                                      learning_rate.lod(),
                                                      learning_rate.offset());
  VLOG(4) << "Builder construction  meta_learning_rate";
  paddle::dialect::IrMetaTensor meta_learning_rate(&ir_tensor_learning_rate);

  paddle::dialect::IrMetaTensor meta_mean_grad;
  paddle::dialect::IrTensor ir_tensor_mean_grad;
  if (mean_grad_.impl() != nullptr) {
    paddle::dialect::DenseTensorType mean_grad = mean_grad_.type().dyn_cast<paddle::dialect::DenseTensorType>();
    VLOG(4) << "Builder construction  dense_mean_grad";
    ir_tensor_mean_grad = paddle::dialect::IrTensor(paddle::dialect::TransToPhiDataType(mean_grad.dtype()),
                                                        mean_grad.dims(),
                                                        mean_grad.data_layout(),
                                                        mean_grad.lod(),
                                                        mean_grad.offset());
    VLOG(4) << "Builder construction  meta_mean_grad";
    meta_mean_grad = paddle::dialect::IrMetaTensor(&ir_tensor_mean_grad);
  }


  paddle::dialect::IrMetaTensor meta_master_param;
  paddle::dialect::IrTensor ir_tensor_master_param;
  if (master_param_.impl() != nullptr) {
    paddle::dialect::DenseTensorType master_param = master_param_.type().dyn_cast<paddle::dialect::DenseTensorType>();
    VLOG(4) << "Builder construction  dense_master_param";
    ir_tensor_master_param = paddle::dialect::IrTensor(paddle::dialect::TransToPhiDataType(master_param.dtype()),
                                                        master_param.dims(),
                                                        master_param.data_layout(),
                                                        master_param.lod(),
                                                        master_param.offset());
    VLOG(4) << "Builder construction  meta_master_param";
    meta_master_param = paddle::dialect::IrMetaTensor(&ir_tensor_master_param);
  }

  paddle::dialect::IrTensor dense_param_out;
  paddle::dialect::IrMetaTensor meta_param_out(&dense_param_out);
  paddle::dialect::IrTensor dense_moment_out;
  paddle::dialect::IrMetaTensor meta_moment_out(&dense_moment_out);
  paddle::dialect::IrTensor dense_mean_square_out;
  paddle::dialect::IrMetaTensor meta_mean_square_out(&dense_mean_square_out);
  paddle::dialect::IrTensor dense_mean_grad_out;
  paddle::dialect::IrMetaTensor meta_mean_grad_out(&dense_mean_grad_out);
  paddle::dialect::IrTensor dense_master_param_outs;
  paddle::dialect::IrMetaTensor meta_master_param_outs(&dense_master_param_outs);

  phi::RmspropInferMeta(meta_param, meta_mean_square, meta_grad, meta_moment, meta_learning_rate, meta_mean_grad, meta_master_param, epsilon, decay, momentum, centered, multi_precision, &meta_param_out, &meta_moment_out, &meta_mean_square_out, &meta_mean_grad_out, &meta_master_param_outs);

  std::vector<pir::Type> argument_outputs;
  pir::Type param_out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_param_out.dtype()), dense_param_out.dims(), dense_param_out.layout(), dense_param_out.lod(), dense_param_out.offset());
  argument_outputs.push_back(param_out_dense_tensor_type);

  pir::Type moment_out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_moment_out.dtype()), dense_moment_out.dims(), dense_moment_out.layout(), dense_moment_out.lod(), dense_moment_out.offset());
  argument_outputs.push_back(moment_out_dense_tensor_type);

  pir::Type mean_square_out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_mean_square_out.dtype()), dense_mean_square_out.dims(), dense_mean_square_out.layout(), dense_mean_square_out.lod(), dense_mean_square_out.offset());
  argument_outputs.push_back(mean_square_out_dense_tensor_type);

  pir::Type mean_grad_out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_mean_grad_out.dtype()), dense_mean_grad_out.dims(), dense_mean_grad_out.layout(), dense_mean_grad_out.lod(), dense_mean_grad_out.offset());
  argument_outputs.push_back(mean_grad_out_dense_tensor_type);

  if (master_param_.impl() != nullptr) {
    pir::Type master_param_outs_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_master_param_outs.dtype()), dense_master_param_outs.dims(), dense_master_param_outs.layout(), dense_master_param_outs.lod(), dense_master_param_outs.offset());
    argument_outputs.push_back(master_param_outs_dense_tensor_type);
  } else {
    pir::Type master_param_outs_type;
    argument_outputs.push_back(master_param_outs_type);
  }

  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void RmspropDenseParamSparseGrad_Op::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value param_, pir::Value mean_square_, pir::Value grad_, pir::Value moment_, pir::Value learning_rate_, pir::Value mean_grad_, pir::Value master_param_, pir::AttributeMap attributes) {
  VLOG(4) << "Start build RmspropDenseParamSparseGrad_Op";


  IR_ENFORCE(
      attributes.find("epsilon") != attributes.end(),
          "'epsilon' Attribute is expected for RmspropDenseParamSparseGrad_Op. ");
  float epsilon = attributes.at("epsilon").dyn_cast<pir::FloatAttribute>().data();

  IR_ENFORCE(
      attributes.find("decay") != attributes.end(),
          "'decay' Attribute is expected for RmspropDenseParamSparseGrad_Op. ");
  float decay = attributes.at("decay").dyn_cast<pir::FloatAttribute>().data();

  IR_ENFORCE(
      attributes.find("momentum") != attributes.end(),
          "'momentum' Attribute is expected for RmspropDenseParamSparseGrad_Op. ");
  float momentum = attributes.at("momentum").dyn_cast<pir::FloatAttribute>().data();

  IR_ENFORCE(
      attributes.find("centered") != attributes.end(),
          "'centered' Attribute is expected for RmspropDenseParamSparseGrad_Op. ");
  bool centered = attributes.at("centered").dyn_cast<pir::BoolAttribute>().data();

  IR_ENFORCE(
      attributes.find("multi_precision") != attributes.end(),
          "'multi_precision' Attribute is expected for RmspropDenseParamSparseGrad_Op. ");
  bool multi_precision = attributes.at("multi_precision").dyn_cast<pir::BoolAttribute>().data();


  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {param_, mean_square_, grad_, moment_, learning_rate_, mean_grad_, master_param_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_epsilon = pir::FloatAttribute::get(pir::IrContext::Instance(), epsilon);
  argument.AddAttribute("epsilon", attr_epsilon);
  pir::Attribute attr_decay = pir::FloatAttribute::get(pir::IrContext::Instance(), decay);
  argument.AddAttribute("decay", attr_decay);
  pir::Attribute attr_momentum = pir::FloatAttribute::get(pir::IrContext::Instance(), momentum);
  argument.AddAttribute("momentum", attr_momentum);
  pir::Attribute attr_centered = pir::BoolAttribute::get(pir::IrContext::Instance(), centered);
  argument.AddAttribute("centered", attr_centered);
  pir::Attribute attr_multi_precision = pir::BoolAttribute::get(pir::IrContext::Instance(), multi_precision);
  argument.AddAttribute("multi_precision", attr_multi_precision);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType param = param_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)param;
  paddle::dialect::DenseTensorType mean_square = mean_square_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)mean_square;
  paddle::dialect::SelectedRowsType grad = grad_.type().dyn_cast<paddle::dialect::SelectedRowsType>(); (void)grad;
  paddle::dialect::DenseTensorType moment = moment_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)moment;
  paddle::dialect::DenseTensorType learning_rate = learning_rate_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)learning_rate;

  VLOG(4) << "Builder construction  dense_param";
  paddle::dialect::IrTensor ir_tensor_param(paddle::dialect::TransToPhiDataType(param.dtype()),
                                                      param.dims(),
                                                      param.data_layout(),
                                                      param.lod(),
                                                      param.offset());
  VLOG(4) << "Builder construction  meta_param";
  paddle::dialect::IrMetaTensor meta_param(&ir_tensor_param);

  VLOG(4) << "Builder construction  dense_mean_square";
  paddle::dialect::IrTensor ir_tensor_mean_square(paddle::dialect::TransToPhiDataType(mean_square.dtype()),
                                                      mean_square.dims(),
                                                      mean_square.data_layout(),
                                                      mean_square.lod(),
                                                      mean_square.offset());
  VLOG(4) << "Builder construction  meta_mean_square";
  paddle::dialect::IrMetaTensor meta_mean_square(&ir_tensor_mean_square);

  VLOG(4) << "Builder construction  dense_grad";
  paddle::dialect::IrTensor ir_tensor_grad(paddle::dialect::TransToPhiDataType(grad.dtype()),
                                                      grad.dims(),
                                                      grad.data_layout(),
                                                      grad.lod(),
                                                      grad.offset());
  VLOG(4) << "Builder construction  meta_grad";
  paddle::dialect::IrMetaTensor meta_grad(&ir_tensor_grad);

  VLOG(4) << "Builder construction  dense_moment";
  paddle::dialect::IrTensor ir_tensor_moment(paddle::dialect::TransToPhiDataType(moment.dtype()),
                                                      moment.dims(),
                                                      moment.data_layout(),
                                                      moment.lod(),
                                                      moment.offset());
  VLOG(4) << "Builder construction  meta_moment";
  paddle::dialect::IrMetaTensor meta_moment(&ir_tensor_moment);

  VLOG(4) << "Builder construction  dense_learning_rate";
  paddle::dialect::IrTensor ir_tensor_learning_rate(paddle::dialect::TransToPhiDataType(learning_rate.dtype()),
                                                      learning_rate.dims(),
                                                      learning_rate.data_layout(),
                                                      learning_rate.lod(),
                                                      learning_rate.offset());
  VLOG(4) << "Builder construction  meta_learning_rate";
  paddle::dialect::IrMetaTensor meta_learning_rate(&ir_tensor_learning_rate);

  paddle::dialect::IrMetaTensor meta_mean_grad;
  paddle::dialect::IrTensor ir_tensor_mean_grad;
  if (mean_grad_.impl() != nullptr) {
    paddle::dialect::DenseTensorType mean_grad = mean_grad_.type().dyn_cast<paddle::dialect::DenseTensorType>();
    VLOG(4) << "Builder construction  dense_mean_grad";
    ir_tensor_mean_grad = paddle::dialect::IrTensor(paddle::dialect::TransToPhiDataType(mean_grad.dtype()),
                                                        mean_grad.dims(),
                                                        mean_grad.data_layout(),
                                                        mean_grad.lod(),
                                                        mean_grad.offset());
    VLOG(4) << "Builder construction  meta_mean_grad";
    meta_mean_grad = paddle::dialect::IrMetaTensor(&ir_tensor_mean_grad);
  }


  paddle::dialect::IrMetaTensor meta_master_param;
  paddle::dialect::IrTensor ir_tensor_master_param;
  if (master_param_.impl() != nullptr) {
    paddle::dialect::DenseTensorType master_param = master_param_.type().dyn_cast<paddle::dialect::DenseTensorType>();
    VLOG(4) << "Builder construction  dense_master_param";
    ir_tensor_master_param = paddle::dialect::IrTensor(paddle::dialect::TransToPhiDataType(master_param.dtype()),
                                                        master_param.dims(),
                                                        master_param.data_layout(),
                                                        master_param.lod(),
                                                        master_param.offset());
    VLOG(4) << "Builder construction  meta_master_param";
    meta_master_param = paddle::dialect::IrMetaTensor(&ir_tensor_master_param);
  }

  paddle::dialect::IrTensor dense_param_out;
  paddle::dialect::IrMetaTensor meta_param_out(&dense_param_out);
  paddle::dialect::IrTensor dense_moment_out;
  paddle::dialect::IrMetaTensor meta_moment_out(&dense_moment_out);
  paddle::dialect::IrTensor dense_mean_square_out;
  paddle::dialect::IrMetaTensor meta_mean_square_out(&dense_mean_square_out);
  paddle::dialect::IrTensor dense_mean_grad_out;
  paddle::dialect::IrMetaTensor meta_mean_grad_out(&dense_mean_grad_out);
  paddle::dialect::IrTensor dense_master_param_outs;
  paddle::dialect::IrMetaTensor meta_master_param_outs(&dense_master_param_outs);

  phi::RmspropInferMeta(meta_param, meta_mean_square, meta_grad, meta_moment, meta_learning_rate, meta_mean_grad, meta_master_param, epsilon, decay, momentum, centered, multi_precision, &meta_param_out, &meta_moment_out, &meta_mean_square_out, &meta_mean_grad_out, &meta_master_param_outs);

  std::vector<pir::Type> argument_outputs;
  pir::Type param_out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_param_out.dtype()), dense_param_out.dims(), dense_param_out.layout(), dense_param_out.lod(), dense_param_out.offset());
  argument_outputs.push_back(param_out_dense_tensor_type);

  pir::Type moment_out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_moment_out.dtype()), dense_moment_out.dims(), dense_moment_out.layout(), dense_moment_out.lod(), dense_moment_out.offset());
  argument_outputs.push_back(moment_out_dense_tensor_type);

  pir::Type mean_square_out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_mean_square_out.dtype()), dense_mean_square_out.dims(), dense_mean_square_out.layout(), dense_mean_square_out.lod(), dense_mean_square_out.offset());
  argument_outputs.push_back(mean_square_out_dense_tensor_type);

  pir::Type mean_grad_out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_mean_grad_out.dtype()), dense_mean_grad_out.dims(), dense_mean_grad_out.layout(), dense_mean_grad_out.lod(), dense_mean_grad_out.offset());
  argument_outputs.push_back(mean_grad_out_dense_tensor_type);

  if (master_param_.impl() != nullptr) {
    pir::Type master_param_outs_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_master_param_outs.dtype()), dense_master_param_outs.dims(), dense_master_param_outs.layout(), dense_master_param_outs.lod(), dense_master_param_outs.offset());
    argument_outputs.push_back(master_param_outs_dense_tensor_type);
  } else {
    pir::Type master_param_outs_type;
    argument_outputs.push_back(master_param_outs_type);
  }

  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void RmspropDenseParamSparseGrad_Op::VerifySig() {}

void RmspropDenseParamSparseGrad_Op::InferMeta( phi::InferMetaContext *infer_meta ) {
  auto fn = PD_INFER_META(phi::RmspropInferMeta);
  fn(infer_meta);
}

phi::DataType RmspropDenseParamSparseGrad_Op::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: RmspropDenseParamSparseGrad_Op";
  


  return expected_kernel_dtype;
}

const char *RoiAlignOp::attributes_name[5] = { "pooled_height", "pooled_width", "spatial_scale", "sampling_ratio", "aligned" };

OpInfoTuple RoiAlignOp::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("x", "paddle::dialect::DenseTensorType", false, false, false, true), paddle::dialect::OpInputInfo("boxes", "paddle::dialect::DenseTensorType", false, false, false, false), paddle::dialect::OpInputInfo("boxes_num", "paddle::dialect::DenseTensorType", true, false, false, false) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = { paddle::dialect::OpAttributeInfo("pooled_height", "pir::Int32Attribute", ""), paddle::dialect::OpAttributeInfo("pooled_width", "pir::Int32Attribute", ""), paddle::dialect::OpAttributeInfo("spatial_scale", "pir::FloatAttribute", ""), paddle::dialect::OpAttributeInfo("sampling_ratio", "pir::Int32Attribute", ""), paddle::dialect::OpAttributeInfo("aligned", "pir::BoolAttribute", "") };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("out", "paddle::dialect::DenseTensorType", false, false) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("RoiAlignInferMeta", {"x", "boxes", "boxes_num", "pooled_height", "pooled_width", "spatial_scale", "sampling_ratio", "aligned"}, "roi_align", {"x", "boxes", "boxes_num", "pooled_height", "pooled_width", "spatial_scale", "sampling_ratio", "aligned"}, {"x"}, {}, {}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "roi_align");
}

void RoiAlignOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, pir::Value boxes_, pir::Value boxes_num_, int pooled_height, int pooled_width, float spatial_scale, int sampling_ratio, bool aligned) {
  VLOG(4) << "Start build RoiAlignOp";



  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_, boxes_, boxes_num_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_pooled_height = pir::Int32Attribute::get(pir::IrContext::Instance(), pooled_height);
  argument.AddAttribute("pooled_height", attr_pooled_height);
  pir::Attribute attr_pooled_width = pir::Int32Attribute::get(pir::IrContext::Instance(), pooled_width);
  argument.AddAttribute("pooled_width", attr_pooled_width);
  pir::Attribute attr_spatial_scale = pir::FloatAttribute::get(pir::IrContext::Instance(), spatial_scale);
  argument.AddAttribute("spatial_scale", attr_spatial_scale);
  pir::Attribute attr_sampling_ratio = pir::Int32Attribute::get(pir::IrContext::Instance(), sampling_ratio);
  argument.AddAttribute("sampling_ratio", attr_sampling_ratio);
  pir::Attribute attr_aligned = pir::BoolAttribute::get(pir::IrContext::Instance(), aligned);
  argument.AddAttribute("aligned", attr_aligned);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;
  paddle::dialect::DenseTensorType boxes = boxes_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)boxes;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);

  VLOG(4) << "Builder construction  dense_boxes";
  paddle::dialect::IrTensor ir_tensor_boxes(paddle::dialect::TransToPhiDataType(boxes.dtype()),
                                                      boxes.dims(),
                                                      boxes.data_layout(),
                                                      boxes.lod(),
                                                      boxes.offset());
  VLOG(4) << "Builder construction  meta_boxes";
  paddle::dialect::IrMetaTensor meta_boxes(&ir_tensor_boxes);

  paddle::dialect::IrMetaTensor meta_boxes_num;
  paddle::dialect::IrTensor ir_tensor_boxes_num;
  if (boxes_num_.impl() != nullptr) {
    paddle::dialect::DenseTensorType boxes_num = boxes_num_.type().dyn_cast<paddle::dialect::DenseTensorType>();
    VLOG(4) << "Builder construction  dense_boxes_num";
    ir_tensor_boxes_num = paddle::dialect::IrTensor(paddle::dialect::TransToPhiDataType(boxes_num.dtype()),
                                                        boxes_num.dims(),
                                                        boxes_num.data_layout(),
                                                        boxes_num.lod(),
                                                        boxes_num.offset());
    VLOG(4) << "Builder construction  meta_boxes_num";
    meta_boxes_num = paddle::dialect::IrMetaTensor(&ir_tensor_boxes_num);
  }

  paddle::dialect::IrTensor dense_out;
  paddle::dialect::IrMetaTensor meta_out(&dense_out);

  phi::RoiAlignInferMeta(meta_x, meta_boxes, meta_boxes_num, pooled_height, pooled_width, spatial_scale, sampling_ratio, aligned, &meta_out);

  std::vector<pir::Type> argument_outputs;
  pir::Type out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_out.dtype()), dense_out.dims(), dense_out.layout(), dense_out.lod(), dense_out.offset());
  argument_outputs.push_back(out_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void RoiAlignOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, pir::Value boxes_, pir::Value boxes_num_, pir::AttributeMap attributes) {
  VLOG(4) << "Start build RoiAlignOp";


  IR_ENFORCE(
      attributes.find("pooled_height") != attributes.end(),
          "'pooled_height' Attribute is expected for RoiAlignOp. ");
  int pooled_height = attributes.at("pooled_height").dyn_cast<pir::Int32Attribute>().data();

  IR_ENFORCE(
      attributes.find("pooled_width") != attributes.end(),
          "'pooled_width' Attribute is expected for RoiAlignOp. ");
  int pooled_width = attributes.at("pooled_width").dyn_cast<pir::Int32Attribute>().data();

  IR_ENFORCE(
      attributes.find("spatial_scale") != attributes.end(),
          "'spatial_scale' Attribute is expected for RoiAlignOp. ");
  float spatial_scale = attributes.at("spatial_scale").dyn_cast<pir::FloatAttribute>().data();

  IR_ENFORCE(
      attributes.find("sampling_ratio") != attributes.end(),
          "'sampling_ratio' Attribute is expected for RoiAlignOp. ");
  int sampling_ratio = attributes.at("sampling_ratio").dyn_cast<pir::Int32Attribute>().data();

  IR_ENFORCE(
      attributes.find("aligned") != attributes.end(),
          "'aligned' Attribute is expected for RoiAlignOp. ");
  bool aligned = attributes.at("aligned").dyn_cast<pir::BoolAttribute>().data();


  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_, boxes_, boxes_num_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_pooled_height = pir::Int32Attribute::get(pir::IrContext::Instance(), pooled_height);
  argument.AddAttribute("pooled_height", attr_pooled_height);
  pir::Attribute attr_pooled_width = pir::Int32Attribute::get(pir::IrContext::Instance(), pooled_width);
  argument.AddAttribute("pooled_width", attr_pooled_width);
  pir::Attribute attr_spatial_scale = pir::FloatAttribute::get(pir::IrContext::Instance(), spatial_scale);
  argument.AddAttribute("spatial_scale", attr_spatial_scale);
  pir::Attribute attr_sampling_ratio = pir::Int32Attribute::get(pir::IrContext::Instance(), sampling_ratio);
  argument.AddAttribute("sampling_ratio", attr_sampling_ratio);
  pir::Attribute attr_aligned = pir::BoolAttribute::get(pir::IrContext::Instance(), aligned);
  argument.AddAttribute("aligned", attr_aligned);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;
  paddle::dialect::DenseTensorType boxes = boxes_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)boxes;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);

  VLOG(4) << "Builder construction  dense_boxes";
  paddle::dialect::IrTensor ir_tensor_boxes(paddle::dialect::TransToPhiDataType(boxes.dtype()),
                                                      boxes.dims(),
                                                      boxes.data_layout(),
                                                      boxes.lod(),
                                                      boxes.offset());
  VLOG(4) << "Builder construction  meta_boxes";
  paddle::dialect::IrMetaTensor meta_boxes(&ir_tensor_boxes);

  paddle::dialect::IrMetaTensor meta_boxes_num;
  paddle::dialect::IrTensor ir_tensor_boxes_num;
  if (boxes_num_.impl() != nullptr) {
    paddle::dialect::DenseTensorType boxes_num = boxes_num_.type().dyn_cast<paddle::dialect::DenseTensorType>();
    VLOG(4) << "Builder construction  dense_boxes_num";
    ir_tensor_boxes_num = paddle::dialect::IrTensor(paddle::dialect::TransToPhiDataType(boxes_num.dtype()),
                                                        boxes_num.dims(),
                                                        boxes_num.data_layout(),
                                                        boxes_num.lod(),
                                                        boxes_num.offset());
    VLOG(4) << "Builder construction  meta_boxes_num";
    meta_boxes_num = paddle::dialect::IrMetaTensor(&ir_tensor_boxes_num);
  }

  paddle::dialect::IrTensor dense_out;
  paddle::dialect::IrMetaTensor meta_out(&dense_out);

  phi::RoiAlignInferMeta(meta_x, meta_boxes, meta_boxes_num, pooled_height, pooled_width, spatial_scale, sampling_ratio, aligned, &meta_out);

  std::vector<pir::Type> argument_outputs;
  pir::Type out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_out.dtype()), dense_out.dims(), dense_out.layout(), dense_out.lod(), dense_out.offset());
  argument_outputs.push_back(out_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void RoiAlignOp::VerifySig() {
  VLOG(4) << "Start Verifying inputs, outputs and attributes for: RoiAlignOp.";
  VLOG(4) << "Verifying inputs:";
  {
  auto input_size = num_operands();
  IR_ENFORCE(input_size == 3u,
                    "The size %d of inputs must be equal to 3.", input_size);
  IR_ENFORCE((*this)->operand_source(0).type().isa<paddle::dialect::DenseTensorType>(),
                  "Type validation failed for the 0th input, got %s.", (*this)->operand_source(0).type());
  IR_ENFORCE((*this)->operand_source(1).type().isa<paddle::dialect::DenseTensorType>(),
                  "Type validation failed for the 1th input, got %s.", (*this)->operand_source(1).type());
  if (auto val = (*this)->operand(2)) {
    IR_ENFORCE(val.type().isa<paddle::dialect::DenseTensorType>(),
                   "Type validation failed for the 2th input, got %s.", (*this)->operand_source(2).type());
  }
  }
  VLOG(4) << "Verifying attributes:";
  {
  auto& attributes = this->attributes();
  IR_ENFORCE(attributes.count("pooled_height")>0,
                 "pooled_height does not exist.");
  IR_ENFORCE(attributes.at("pooled_height").isa<pir::Int32Attribute>(),
                 "Type of attribute: pooled_height is not pir::Int32Attribute.");

  IR_ENFORCE(attributes.count("pooled_width")>0,
                 "pooled_width does not exist.");
  IR_ENFORCE(attributes.at("pooled_width").isa<pir::Int32Attribute>(),
                 "Type of attribute: pooled_width is not pir::Int32Attribute.");

  IR_ENFORCE(attributes.count("spatial_scale")>0,
                 "spatial_scale does not exist.");
  IR_ENFORCE(attributes.at("spatial_scale").isa<pir::FloatAttribute>(),
                 "Type of attribute: spatial_scale is not pir::FloatAttribute.");

  IR_ENFORCE(attributes.count("sampling_ratio")>0,
                 "sampling_ratio does not exist.");
  IR_ENFORCE(attributes.at("sampling_ratio").isa<pir::Int32Attribute>(),
                 "Type of attribute: sampling_ratio is not pir::Int32Attribute.");

  IR_ENFORCE(attributes.count("aligned")>0,
                 "aligned does not exist.");
  IR_ENFORCE(attributes.at("aligned").isa<pir::BoolAttribute>(),
                 "Type of attribute: aligned is not pir::BoolAttribute.");

  }
  VLOG(4) << "Verifying outputs:";
  {
  auto output_size = num_results();
  IR_ENFORCE(output_size == 1u,
                    "The size %d of outputs must be equal to 1.", output_size);
  IR_ENFORCE((*this)->result(0).type().isa<paddle::dialect::DenseTensorType>(),
                 "Type validation failed for the 0th output.");
  }
  VLOG(4) << "End Verifying for: RoiAlignOp.";
}

void RoiAlignOp::InferMeta( phi::InferMetaContext *infer_meta ) {
  auto fn = PD_INFER_META(phi::RoiAlignInferMeta);
  fn(infer_meta);
}

phi::DataType RoiAlignOp::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: RoiAlignOp";
  


  return expected_kernel_dtype;
}

const char *RoiPoolOp::attributes_name[3] = { "pooled_height", "pooled_width", "spatial_scale" };

OpInfoTuple RoiPoolOp::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("x", "paddle::dialect::DenseTensorType", false, false, false, true), paddle::dialect::OpInputInfo("boxes", "paddle::dialect::DenseTensorType", false, false, false, false), paddle::dialect::OpInputInfo("boxes_num", "paddle::dialect::DenseTensorType", true, false, false, false) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = { paddle::dialect::OpAttributeInfo("pooled_height", "pir::Int32Attribute", ""), paddle::dialect::OpAttributeInfo("pooled_width", "pir::Int32Attribute", ""), paddle::dialect::OpAttributeInfo("spatial_scale", "pir::FloatAttribute", "") };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("out", "paddle::dialect::DenseTensorType", false, false), paddle::dialect::OpOutputInfo("arg_max", "paddle::dialect::DenseTensorType", true, true) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("RoiPoolInferMeta", {"x", "boxes", "boxes_num", "pooled_height", "pooled_width", "spatial_scale"}, "roi_pool", {"x", "boxes", "boxes_num", "pooled_height", "pooled_width", "spatial_scale"}, {"x"}, {}, {}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "roi_pool");
}

void RoiPoolOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, pir::Value boxes_, pir::Value boxes_num_, int pooled_height, int pooled_width, float spatial_scale) {
  VLOG(4) << "Start build RoiPoolOp";



  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_, boxes_, boxes_num_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_pooled_height = pir::Int32Attribute::get(pir::IrContext::Instance(), pooled_height);
  argument.AddAttribute("pooled_height", attr_pooled_height);
  pir::Attribute attr_pooled_width = pir::Int32Attribute::get(pir::IrContext::Instance(), pooled_width);
  argument.AddAttribute("pooled_width", attr_pooled_width);
  pir::Attribute attr_spatial_scale = pir::FloatAttribute::get(pir::IrContext::Instance(), spatial_scale);
  argument.AddAttribute("spatial_scale", attr_spatial_scale);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;
  paddle::dialect::DenseTensorType boxes = boxes_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)boxes;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);

  VLOG(4) << "Builder construction  dense_boxes";
  paddle::dialect::IrTensor ir_tensor_boxes(paddle::dialect::TransToPhiDataType(boxes.dtype()),
                                                      boxes.dims(),
                                                      boxes.data_layout(),
                                                      boxes.lod(),
                                                      boxes.offset());
  VLOG(4) << "Builder construction  meta_boxes";
  paddle::dialect::IrMetaTensor meta_boxes(&ir_tensor_boxes);

  paddle::dialect::IrMetaTensor meta_boxes_num;
  paddle::dialect::IrTensor ir_tensor_boxes_num;
  if (boxes_num_.impl() != nullptr) {
    paddle::dialect::DenseTensorType boxes_num = boxes_num_.type().dyn_cast<paddle::dialect::DenseTensorType>();
    VLOG(4) << "Builder construction  dense_boxes_num";
    ir_tensor_boxes_num = paddle::dialect::IrTensor(paddle::dialect::TransToPhiDataType(boxes_num.dtype()),
                                                        boxes_num.dims(),
                                                        boxes_num.data_layout(),
                                                        boxes_num.lod(),
                                                        boxes_num.offset());
    VLOG(4) << "Builder construction  meta_boxes_num";
    meta_boxes_num = paddle::dialect::IrMetaTensor(&ir_tensor_boxes_num);
  }

  paddle::dialect::IrTensor dense_out;
  paddle::dialect::IrMetaTensor meta_out(&dense_out);
  paddle::dialect::IrTensor dense_arg_max;
  paddle::dialect::IrMetaTensor meta_arg_max(&dense_arg_max);

  phi::RoiPoolInferMeta(meta_x, meta_boxes, meta_boxes_num, pooled_height, pooled_width, spatial_scale, &meta_out, &meta_arg_max);

  std::vector<pir::Type> argument_outputs;
  pir::Type out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_out.dtype()), dense_out.dims(), dense_out.layout(), dense_out.lod(), dense_out.offset());
  argument_outputs.push_back(out_dense_tensor_type);

  pir::Type arg_max_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_arg_max.dtype()), dense_arg_max.dims(), dense_arg_max.layout(), dense_arg_max.lod(), dense_arg_max.offset());
  argument_outputs.push_back(arg_max_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void RoiPoolOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, pir::Value boxes_, pir::Value boxes_num_, pir::AttributeMap attributes) {
  VLOG(4) << "Start build RoiPoolOp";


  IR_ENFORCE(
      attributes.find("pooled_height") != attributes.end(),
          "'pooled_height' Attribute is expected for RoiPoolOp. ");
  int pooled_height = attributes.at("pooled_height").dyn_cast<pir::Int32Attribute>().data();

  IR_ENFORCE(
      attributes.find("pooled_width") != attributes.end(),
          "'pooled_width' Attribute is expected for RoiPoolOp. ");
  int pooled_width = attributes.at("pooled_width").dyn_cast<pir::Int32Attribute>().data();

  IR_ENFORCE(
      attributes.find("spatial_scale") != attributes.end(),
          "'spatial_scale' Attribute is expected for RoiPoolOp. ");
  float spatial_scale = attributes.at("spatial_scale").dyn_cast<pir::FloatAttribute>().data();


  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_, boxes_, boxes_num_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_pooled_height = pir::Int32Attribute::get(pir::IrContext::Instance(), pooled_height);
  argument.AddAttribute("pooled_height", attr_pooled_height);
  pir::Attribute attr_pooled_width = pir::Int32Attribute::get(pir::IrContext::Instance(), pooled_width);
  argument.AddAttribute("pooled_width", attr_pooled_width);
  pir::Attribute attr_spatial_scale = pir::FloatAttribute::get(pir::IrContext::Instance(), spatial_scale);
  argument.AddAttribute("spatial_scale", attr_spatial_scale);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;
  paddle::dialect::DenseTensorType boxes = boxes_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)boxes;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);

  VLOG(4) << "Builder construction  dense_boxes";
  paddle::dialect::IrTensor ir_tensor_boxes(paddle::dialect::TransToPhiDataType(boxes.dtype()),
                                                      boxes.dims(),
                                                      boxes.data_layout(),
                                                      boxes.lod(),
                                                      boxes.offset());
  VLOG(4) << "Builder construction  meta_boxes";
  paddle::dialect::IrMetaTensor meta_boxes(&ir_tensor_boxes);

  paddle::dialect::IrMetaTensor meta_boxes_num;
  paddle::dialect::IrTensor ir_tensor_boxes_num;
  if (boxes_num_.impl() != nullptr) {
    paddle::dialect::DenseTensorType boxes_num = boxes_num_.type().dyn_cast<paddle::dialect::DenseTensorType>();
    VLOG(4) << "Builder construction  dense_boxes_num";
    ir_tensor_boxes_num = paddle::dialect::IrTensor(paddle::dialect::TransToPhiDataType(boxes_num.dtype()),
                                                        boxes_num.dims(),
                                                        boxes_num.data_layout(),
                                                        boxes_num.lod(),
                                                        boxes_num.offset());
    VLOG(4) << "Builder construction  meta_boxes_num";
    meta_boxes_num = paddle::dialect::IrMetaTensor(&ir_tensor_boxes_num);
  }

  paddle::dialect::IrTensor dense_out;
  paddle::dialect::IrMetaTensor meta_out(&dense_out);
  paddle::dialect::IrTensor dense_arg_max;
  paddle::dialect::IrMetaTensor meta_arg_max(&dense_arg_max);

  phi::RoiPoolInferMeta(meta_x, meta_boxes, meta_boxes_num, pooled_height, pooled_width, spatial_scale, &meta_out, &meta_arg_max);

  std::vector<pir::Type> argument_outputs;
  pir::Type out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_out.dtype()), dense_out.dims(), dense_out.layout(), dense_out.lod(), dense_out.offset());
  argument_outputs.push_back(out_dense_tensor_type);

  pir::Type arg_max_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_arg_max.dtype()), dense_arg_max.dims(), dense_arg_max.layout(), dense_arg_max.lod(), dense_arg_max.offset());
  argument_outputs.push_back(arg_max_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void RoiPoolOp::VerifySig() {
  VLOG(4) << "Start Verifying inputs, outputs and attributes for: RoiPoolOp.";
  VLOG(4) << "Verifying inputs:";
  {
  auto input_size = num_operands();
  IR_ENFORCE(input_size == 3u,
                    "The size %d of inputs must be equal to 3.", input_size);
  IR_ENFORCE((*this)->operand_source(0).type().isa<paddle::dialect::DenseTensorType>(),
                  "Type validation failed for the 0th input, got %s.", (*this)->operand_source(0).type());
  IR_ENFORCE((*this)->operand_source(1).type().isa<paddle::dialect::DenseTensorType>(),
                  "Type validation failed for the 1th input, got %s.", (*this)->operand_source(1).type());
  if (auto val = (*this)->operand(2)) {
    IR_ENFORCE(val.type().isa<paddle::dialect::DenseTensorType>(),
                   "Type validation failed for the 2th input, got %s.", (*this)->operand_source(2).type());
  }
  }
  VLOG(4) << "Verifying attributes:";
  {
  auto& attributes = this->attributes();
  IR_ENFORCE(attributes.count("pooled_height")>0,
                 "pooled_height does not exist.");
  IR_ENFORCE(attributes.at("pooled_height").isa<pir::Int32Attribute>(),
                 "Type of attribute: pooled_height is not pir::Int32Attribute.");

  IR_ENFORCE(attributes.count("pooled_width")>0,
                 "pooled_width does not exist.");
  IR_ENFORCE(attributes.at("pooled_width").isa<pir::Int32Attribute>(),
                 "Type of attribute: pooled_width is not pir::Int32Attribute.");

  IR_ENFORCE(attributes.count("spatial_scale")>0,
                 "spatial_scale does not exist.");
  IR_ENFORCE(attributes.at("spatial_scale").isa<pir::FloatAttribute>(),
                 "Type of attribute: spatial_scale is not pir::FloatAttribute.");

  }
  VLOG(4) << "Verifying outputs:";
  {
  auto output_size = num_results();
  IR_ENFORCE(output_size == 2u,
                    "The size %d of outputs must be equal to 2.", output_size);
  IR_ENFORCE((*this)->result(0).type().isa<paddle::dialect::DenseTensorType>(),
                 "Type validation failed for the 0th output.");
  if (auto output_1_type = (*this)->result(1).type()) {
    IR_ENFORCE(output_1_type.isa<paddle::dialect::DenseTensorType>(),
                   "Type validation failed for the 1th output.");
  }
  }
  VLOG(4) << "End Verifying for: RoiPoolOp.";
}

void RoiPoolOp::InferMeta( phi::InferMetaContext *infer_meta ) {
  auto fn = PD_INFER_META(phi::RoiPoolInferMeta);
  fn(infer_meta);
}

phi::DataType RoiPoolOp::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: RoiPoolOp";
  


  return expected_kernel_dtype;
}

const char *RollOp::attributes_name[1] = { "axis" };

OpInfoTuple RollOp::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("x", "paddle::dialect::DenseTensorType", false, false, false, true), paddle::dialect::OpInputInfo("shifts", "paddle::dialect::IntArrayAttribute", false, false, true, false) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = { paddle::dialect::OpAttributeInfo("axis", "pir::ArrayAttribute<pir::Int64Attribute>", "") };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("out", "paddle::dialect::DenseTensorType", false, false) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("RollInferMeta", {"x", "shifts", "axis"}, "roll", {"x", "shifts", "axis"}, {"x"}, {}, {}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "roll");
}

void RollOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, const std::vector<int64_t>& shifts, const std::vector<int64_t>& axis) {
  VLOG(4) << "Start build RollOp";


  // Generate int_array mutable attribute: shifts
  paddle::dialect::FullIntArrayOp full_shifts_op = builder.Build<paddle::dialect::FullIntArrayOp>(shifts, phi::DataType::INT64, phi::CPUPlace());
  pir::OpResult shifts_ = full_shifts_op->result(0);
    
  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_, shifts_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  std::vector<pir::Attribute> vec_axis;
  for (size_t i = 0; i < static_cast<size_t>(axis.size()); i++) {
      pir::Attribute attr_axis = pir::Int64Attribute::get(pir::IrContext::Instance(), axis[i]);

    vec_axis.push_back(attr_axis);
  }
  pir::Attribute attr_axis = pir::ArrayAttribute::get(pir::IrContext::Instance(), vec_axis);
  argument.AddAttribute("axis", attr_axis);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);
  paddle::dialect::IrTensor dense_out;
  paddle::dialect::IrMetaTensor meta_out(&dense_out);

  phi::RollInferMeta(meta_x, shifts, axis, &meta_out);

  std::vector<pir::Type> argument_outputs;
  pir::Type out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_out.dtype()), dense_out.dims(), dense_out.layout(), dense_out.lod(), dense_out.offset());
  argument_outputs.push_back(out_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void RollOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, pir::AttributeMap attributes) {
  VLOG(4) << "Start build RollOp";


  IR_ENFORCE(
      attributes.find("shifts") != attributes.end(),
          "'shifts' Attribute is expected for RollOp. ");
  std::vector<int64_t> shifts = attributes.at("shifts").dyn_cast<paddle::dialect::IntArrayAttribute>().data().GetData();

  IR_ENFORCE(
      attributes.find("axis") != attributes.end(),
          "'axis' Attribute is expected for RollOp. ");
  std::vector<int64_t> axis;
  for (size_t i = 0; i < attributes.at("axis").dyn_cast<pir::ArrayAttribute>().size(); i++) {
    axis.push_back(attributes.at("axis").dyn_cast<pir::ArrayAttribute>().at(i).dyn_cast<pir::Int64Attribute>().data());
  }

  // Generate int_array mutable attribute: shifts
  paddle::dialect::FullIntArrayOp full_shifts_op = builder.Build<paddle::dialect::FullIntArrayOp>(shifts, phi::DataType::INT64, phi::CPUPlace());
  pir::OpResult shifts_ = full_shifts_op->result(0);
    
  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_, shifts_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  std::vector<pir::Attribute> vec_axis;
  for (size_t i = 0; i < static_cast<size_t>(axis.size()); i++) {
      pir::Attribute attr_axis = pir::Int64Attribute::get(pir::IrContext::Instance(), axis[i]);

    vec_axis.push_back(attr_axis);
  }
  pir::Attribute attr_axis = pir::ArrayAttribute::get(pir::IrContext::Instance(), vec_axis);
  argument.AddAttribute("axis", attr_axis);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);
  paddle::dialect::IrTensor dense_out;
  paddle::dialect::IrMetaTensor meta_out(&dense_out);

  phi::RollInferMeta(meta_x, shifts, axis, &meta_out);

  std::vector<pir::Type> argument_outputs;
  pir::Type out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_out.dtype()), dense_out.dims(), dense_out.layout(), dense_out.lod(), dense_out.offset());
  argument_outputs.push_back(out_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void RollOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, pir::Value shifts_, const std::vector<int64_t>& axis) {
  VLOG(4) << "Start build RollOp";



  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_, shifts_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  std::vector<pir::Attribute> vec_axis;
  for (size_t i = 0; i < static_cast<size_t>(axis.size()); i++) {
      pir::Attribute attr_axis = pir::Int64Attribute::get(pir::IrContext::Instance(), axis[i]);

    vec_axis.push_back(attr_axis);
  }
  pir::Attribute attr_axis = pir::ArrayAttribute::get(pir::IrContext::Instance(), vec_axis);
  argument.AddAttribute("axis", attr_axis);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;
  phi::IntArray shifts;
  if (shifts_.dyn_cast<pir::OpResult>() && shifts_.dyn_cast<pir::OpResult>().owner()->isa<paddle::dialect::FullIntArrayOp>()) {
    shifts = std::move(phi::IntArray(paddle::dialect::GetInt64Vector(
                          shifts_.dyn_cast<pir::OpResult>().owner()
                          ->dyn_cast<paddle::dialect::FullIntArrayOp>()
                          .attribute("value"))));
  } else if (shifts_.type().isa<pir::VectorType>()) {
    size_t shifts_size = shifts_.type().dyn_cast<pir::VectorType>().size();
    shifts = std::move(phi::IntArray(std::vector<int64_t>(shifts_size, -1)));
    shifts.SetFromTensor(true);
  } else if (shifts_.type().isa<paddle::dialect::DenseTensorType>()) {
    common::DDim shifts_dim = shifts_.type().dyn_cast<paddle::dialect::DenseTensorType>().dims();
    size_t shifts_size = common::product(shifts_dim);
    if (common::contain_unknown_dim(shifts_dim)) {
      shifts_size = 1;
    }
    shifts = std::move(phi::IntArray(std::vector<int64_t>(shifts_size, -1)));
    shifts.SetFromTensor(true);
  } else {
    PADDLE_THROW(phi::errors::Unimplemented("Only support VectorType or DenseTensorType"));
  }


  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);
  paddle::dialect::IrTensor dense_out;
  paddle::dialect::IrMetaTensor meta_out(&dense_out);

  phi::RollInferMeta(meta_x, shifts, axis, &meta_out);

  std::vector<pir::Type> argument_outputs;
  pir::Type out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_out.dtype()), dense_out.dims(), dense_out.layout(), dense_out.lod(), dense_out.offset());
  argument_outputs.push_back(out_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void RollOp::VerifySig() {
  VLOG(4) << "Start Verifying inputs, outputs and attributes for: RollOp.";
  VLOG(4) << "Verifying inputs:";
  {
  auto input_size = num_operands();
  IR_ENFORCE(input_size == 2u,
                    "The size %d of inputs must be equal to 2.", input_size);
  IR_ENFORCE((*this)->operand_source(0).type().isa<paddle::dialect::DenseTensorType>(),
                  "Type validation failed for the 0th input, got %s.", (*this)->operand_source(0).type());
  if (auto vec_type = (*this)->operand_source(1).type().dyn_cast<pir::VectorType>()) {
      for (size_t i = 0; i < vec_type.size(); ++i) {
        IR_ENFORCE(vec_type[i].isa<paddle::dialect::DenseTensorType>(),
                       "Type validation failed for the 1th input, got %s.", (*this)->operand_source(1).type());
      }
  }
  else {
    IR_ENFORCE((*this)->operand_source(1).type().isa<paddle::dialect::DenseTensorType>(),
                   "Type validation failed for the 1th input, got %s.", (*this)->operand_source(1).type());
  }
  }
  VLOG(4) << "Verifying attributes:";
  {
  auto& attributes = this->attributes();
  IR_ENFORCE(attributes.count("axis")>0,
                 "axis does not exist.");
  IR_ENFORCE(attributes.at("axis").isa<pir::ArrayAttribute>(),
                 "Type of attribute: axis is not pir::ArrayAttribute.");
  for (size_t i = 0; i < attributes.at("axis").dyn_cast<pir::ArrayAttribute>().size(); i++) {
    IR_ENFORCE(attributes.at("axis").dyn_cast<pir::ArrayAttribute>().at(i).isa<pir::Int64Attribute>(),
                   "Type of attribute: axis is not right.");
  }
  }
  VLOG(4) << "Verifying outputs:";
  {
  auto output_size = num_results();
  IR_ENFORCE(output_size == 1u,
                    "The size %d of outputs must be equal to 1.", output_size);
  IR_ENFORCE((*this)->result(0).type().isa<paddle::dialect::DenseTensorType>(),
                 "Type validation failed for the 0th output.");
  }
  VLOG(4) << "End Verifying for: RollOp.";
}

void RollOp::InferMeta( phi::InferMetaContext *infer_meta ) {
  auto fn = PD_INFER_META(phi::RollInferMeta);
  fn(infer_meta);
}

phi::DataType RollOp::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: RollOp";
  


  return expected_kernel_dtype;
}

OpInfoTuple RoundOp::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("x", "paddle::dialect::DenseTensorType", false, false, false, true) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = {  };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("out", "paddle::dialect::DenseTensorType", false, false) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("UnchangedInferMeta", {"x"}, "round", {"x"}, {}, {}, {}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "round");
}

void RoundOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_) {
  VLOG(4) << "Start build RoundOp";



  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);
  paddle::dialect::IrTensor dense_out;
  paddle::dialect::IrMetaTensor meta_out(&dense_out);

  phi::UnchangedInferMeta(meta_x, &meta_out);

  std::vector<pir::Type> argument_outputs;
  pir::Type out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_out.dtype()), dense_out.dims(), dense_out.layout(), dense_out.lod(), dense_out.offset());
  argument_outputs.push_back(out_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void RoundOp::VerifySig() {
  VLOG(4) << "Start Verifying inputs, outputs and attributes for: RoundOp.";
  VLOG(4) << "Verifying inputs:";
  {
  auto input_size = num_operands();
  IR_ENFORCE(input_size == 1u,
                    "The size %d of inputs must be equal to 1.", input_size);
  IR_ENFORCE((*this)->operand_source(0).type().isa<paddle::dialect::DenseTensorType>(),
                  "Type validation failed for the 0th input, got %s.", (*this)->operand_source(0).type());
  }
  VLOG(4) << "Verifying attributes:";
  {
  // Attributes num is 0, not need to check attributes type.
  }
  VLOG(4) << "Verifying outputs:";
  {
  auto output_size = num_results();
  IR_ENFORCE(output_size == 1u,
                    "The size %d of outputs must be equal to 1.", output_size);
  IR_ENFORCE((*this)->result(0).type().isa<paddle::dialect::DenseTensorType>(),
                 "Type validation failed for the 0th output.");
  }
  VLOG(4) << "End Verifying for: RoundOp.";
}

void RoundOp::InferMeta( phi::InferMetaContext *infer_meta ) {
  auto fn = PD_INFER_META(phi::UnchangedInferMeta);
  fn(infer_meta);
}

phi::DataType RoundOp::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: RoundOp";
  


  return expected_kernel_dtype;
}

OpInfoTuple Round_Op::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("x", "paddle::dialect::DenseTensorType", false, false, false, true) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = {  };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("out", "paddle::dialect::DenseTensorType", false, false) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("UnchangedInferMeta", {"x"}, "round", {"x"}, {}, {}, {{"out", "x"}}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "round");
}

void Round_Op::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_) {
  VLOG(4) << "Start build Round_Op";



  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);
  paddle::dialect::IrTensor dense_out;
  paddle::dialect::IrMetaTensor meta_out(&dense_out);

  phi::UnchangedInferMeta(meta_x, &meta_out);

  std::vector<pir::Type> argument_outputs;
  pir::Type out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_out.dtype()), dense_out.dims(), dense_out.layout(), dense_out.lod(), dense_out.offset());
  argument_outputs.push_back(out_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void Round_Op::VerifySig() {
  VLOG(4) << "Start Verifying inputs, outputs and attributes for: Round_Op.";
  VLOG(4) << "Verifying inputs:";
  {
  auto input_size = num_operands();
  IR_ENFORCE(input_size == 1u,
                    "The size %d of inputs must be equal to 1.", input_size);
  IR_ENFORCE((*this)->operand_source(0).type().isa<paddle::dialect::DenseTensorType>(),
                  "Type validation failed for the 0th input, got %s.", (*this)->operand_source(0).type());
  }
  VLOG(4) << "Verifying attributes:";
  {
  // Attributes num is 0, not need to check attributes type.
  }
  VLOG(4) << "Verifying outputs:";
  {
  auto output_size = num_results();
  IR_ENFORCE(output_size == 1u,
                    "The size %d of outputs must be equal to 1.", output_size);
  IR_ENFORCE((*this)->result(0).type().isa<paddle::dialect::DenseTensorType>(),
                 "Type validation failed for the 0th output.");
  }
  VLOG(4) << "End Verifying for: Round_Op.";
}

void Round_Op::InferMeta( phi::InferMetaContext *infer_meta ) {
  auto fn = PD_INFER_META(phi::UnchangedInferMeta);
  fn(infer_meta);
}

phi::DataType Round_Op::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: Round_Op";
  


  return expected_kernel_dtype;
}

const char *Rprop_Op::attributes_name[1] = { "multi_precision" };

OpInfoTuple Rprop_Op::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("param", "paddle::dialect::DenseTensorType", false, false, false, false), paddle::dialect::OpInputInfo("grad", "paddle::dialect::DenseTensorType", false, false, false, false), paddle::dialect::OpInputInfo("prev", "paddle::dialect::DenseTensorType", false, false, false, false), paddle::dialect::OpInputInfo("learning_rate", "paddle::dialect::DenseTensorType", false, false, false, false), paddle::dialect::OpInputInfo("master_param", "paddle::dialect::DenseTensorType", true, false, false, false), paddle::dialect::OpInputInfo("learning_rate_range", "paddle::dialect::DenseTensorType", false, false, false, false), paddle::dialect::OpInputInfo("etas", "paddle::dialect::DenseTensorType", false, false, false, false) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = { paddle::dialect::OpAttributeInfo("multi_precision", "pir::BoolAttribute", "") };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("param_out", "paddle::dialect::DenseTensorType", false, false), paddle::dialect::OpOutputInfo("prev_out", "paddle::dialect::DenseTensorType", false, false), paddle::dialect::OpOutputInfo("learning_rate_out", "paddle::dialect::DenseTensorType", false, false), paddle::dialect::OpOutputInfo("master_param_out", "paddle::dialect::DenseTensorType", true, false) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("RpropInferMeta", {"param", "grad", "prev", "learning_rate", "master_param", "learning_rate_range", "etas", "multi_precision"}, "rprop", {"param", "grad", "prev", "learning_rate", "master_param", "learning_rate_range", "etas", "multi_precision"}, {"param"}, {}, {{"param_out", "param"},{"prev_out", "prev"},{"learning_rate_out", "learning_rate"},{"master_param_out", "master_param"}}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "rprop_");
}

void Rprop_Op::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value param_, pir::Value grad_, pir::Value prev_, pir::Value learning_rate_, pir::Value master_param_, pir::Value learning_rate_range_, pir::Value etas_, bool multi_precision) {
  VLOG(4) << "Start build Rprop_Op";



  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {param_, grad_, prev_, learning_rate_, master_param_, learning_rate_range_, etas_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_multi_precision = pir::BoolAttribute::get(pir::IrContext::Instance(), multi_precision);
  argument.AddAttribute("multi_precision", attr_multi_precision);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType param = param_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)param;
  paddle::dialect::DenseTensorType grad = grad_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)grad;
  paddle::dialect::DenseTensorType prev = prev_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)prev;
  paddle::dialect::DenseTensorType learning_rate = learning_rate_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)learning_rate;
  paddle::dialect::DenseTensorType learning_rate_range = learning_rate_range_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)learning_rate_range;
  paddle::dialect::DenseTensorType etas = etas_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)etas;

  VLOG(4) << "Builder construction  dense_param";
  paddle::dialect::IrTensor ir_tensor_param(paddle::dialect::TransToPhiDataType(param.dtype()),
                                                      param.dims(),
                                                      param.data_layout(),
                                                      param.lod(),
                                                      param.offset());
  VLOG(4) << "Builder construction  meta_param";
  paddle::dialect::IrMetaTensor meta_param(&ir_tensor_param);

  VLOG(4) << "Builder construction  dense_grad";
  paddle::dialect::IrTensor ir_tensor_grad(paddle::dialect::TransToPhiDataType(grad.dtype()),
                                                      grad.dims(),
                                                      grad.data_layout(),
                                                      grad.lod(),
                                                      grad.offset());
  VLOG(4) << "Builder construction  meta_grad";
  paddle::dialect::IrMetaTensor meta_grad(&ir_tensor_grad);

  VLOG(4) << "Builder construction  dense_prev";
  paddle::dialect::IrTensor ir_tensor_prev(paddle::dialect::TransToPhiDataType(prev.dtype()),
                                                      prev.dims(),
                                                      prev.data_layout(),
                                                      prev.lod(),
                                                      prev.offset());
  VLOG(4) << "Builder construction  meta_prev";
  paddle::dialect::IrMetaTensor meta_prev(&ir_tensor_prev);

  VLOG(4) << "Builder construction  dense_learning_rate";
  paddle::dialect::IrTensor ir_tensor_learning_rate(paddle::dialect::TransToPhiDataType(learning_rate.dtype()),
                                                      learning_rate.dims(),
                                                      learning_rate.data_layout(),
                                                      learning_rate.lod(),
                                                      learning_rate.offset());
  VLOG(4) << "Builder construction  meta_learning_rate";
  paddle::dialect::IrMetaTensor meta_learning_rate(&ir_tensor_learning_rate);

  paddle::dialect::IrMetaTensor meta_master_param;
  paddle::dialect::IrTensor ir_tensor_master_param;
  if (master_param_.impl() != nullptr) {
    paddle::dialect::DenseTensorType master_param = master_param_.type().dyn_cast<paddle::dialect::DenseTensorType>();
    VLOG(4) << "Builder construction  dense_master_param";
    ir_tensor_master_param = paddle::dialect::IrTensor(paddle::dialect::TransToPhiDataType(master_param.dtype()),
                                                        master_param.dims(),
                                                        master_param.data_layout(),
                                                        master_param.lod(),
                                                        master_param.offset());
    VLOG(4) << "Builder construction  meta_master_param";
    meta_master_param = paddle::dialect::IrMetaTensor(&ir_tensor_master_param);
  }


  VLOG(4) << "Builder construction  dense_learning_rate_range";
  paddle::dialect::IrTensor ir_tensor_learning_rate_range(paddle::dialect::TransToPhiDataType(learning_rate_range.dtype()),
                                                      learning_rate_range.dims(),
                                                      learning_rate_range.data_layout(),
                                                      learning_rate_range.lod(),
                                                      learning_rate_range.offset());
  VLOG(4) << "Builder construction  meta_learning_rate_range";
  paddle::dialect::IrMetaTensor meta_learning_rate_range(&ir_tensor_learning_rate_range);

  VLOG(4) << "Builder construction  dense_etas";
  paddle::dialect::IrTensor ir_tensor_etas(paddle::dialect::TransToPhiDataType(etas.dtype()),
                                                      etas.dims(),
                                                      etas.data_layout(),
                                                      etas.lod(),
                                                      etas.offset());
  VLOG(4) << "Builder construction  meta_etas";
  paddle::dialect::IrMetaTensor meta_etas(&ir_tensor_etas);
  paddle::dialect::IrTensor dense_param_out;
  paddle::dialect::IrMetaTensor meta_param_out(&dense_param_out);
  paddle::dialect::IrTensor dense_prev_out;
  paddle::dialect::IrMetaTensor meta_prev_out(&dense_prev_out);
  paddle::dialect::IrTensor dense_learning_rate_out;
  paddle::dialect::IrMetaTensor meta_learning_rate_out(&dense_learning_rate_out);
  paddle::dialect::IrTensor dense_master_param_out;
  paddle::dialect::IrMetaTensor meta_master_param_out(&dense_master_param_out);

  phi::RpropInferMeta(meta_param, meta_grad, meta_prev, meta_learning_rate, meta_master_param, meta_learning_rate_range, meta_etas, multi_precision, &meta_param_out, &meta_prev_out, &meta_learning_rate_out, &meta_master_param_out);

  std::vector<pir::Type> argument_outputs;
  pir::Type param_out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_param_out.dtype()), dense_param_out.dims(), dense_param_out.layout(), dense_param_out.lod(), dense_param_out.offset());
  argument_outputs.push_back(param_out_dense_tensor_type);

  pir::Type prev_out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_prev_out.dtype()), dense_prev_out.dims(), dense_prev_out.layout(), dense_prev_out.lod(), dense_prev_out.offset());
  argument_outputs.push_back(prev_out_dense_tensor_type);

  pir::Type learning_rate_out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_learning_rate_out.dtype()), dense_learning_rate_out.dims(), dense_learning_rate_out.layout(), dense_learning_rate_out.lod(), dense_learning_rate_out.offset());
  argument_outputs.push_back(learning_rate_out_dense_tensor_type);

  if (master_param_.impl() != nullptr) {
    pir::Type master_param_out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_master_param_out.dtype()), dense_master_param_out.dims(), dense_master_param_out.layout(), dense_master_param_out.lod(), dense_master_param_out.offset());
    argument_outputs.push_back(master_param_out_dense_tensor_type);
  } else {
    pir::Type master_param_out_type;
    argument_outputs.push_back(master_param_out_type);
  }

  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void Rprop_Op::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value param_, pir::Value grad_, pir::Value prev_, pir::Value learning_rate_, pir::Value master_param_, pir::Value learning_rate_range_, pir::Value etas_, pir::AttributeMap attributes) {
  VLOG(4) << "Start build Rprop_Op";


  IR_ENFORCE(
      attributes.find("multi_precision") != attributes.end(),
          "'multi_precision' Attribute is expected for Rprop_Op. ");
  bool multi_precision = attributes.at("multi_precision").dyn_cast<pir::BoolAttribute>().data();


  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {param_, grad_, prev_, learning_rate_, master_param_, learning_rate_range_, etas_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_multi_precision = pir::BoolAttribute::get(pir::IrContext::Instance(), multi_precision);
  argument.AddAttribute("multi_precision", attr_multi_precision);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType param = param_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)param;
  paddle::dialect::DenseTensorType grad = grad_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)grad;
  paddle::dialect::DenseTensorType prev = prev_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)prev;
  paddle::dialect::DenseTensorType learning_rate = learning_rate_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)learning_rate;
  paddle::dialect::DenseTensorType learning_rate_range = learning_rate_range_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)learning_rate_range;
  paddle::dialect::DenseTensorType etas = etas_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)etas;

  VLOG(4) << "Builder construction  dense_param";
  paddle::dialect::IrTensor ir_tensor_param(paddle::dialect::TransToPhiDataType(param.dtype()),
                                                      param.dims(),
                                                      param.data_layout(),
                                                      param.lod(),
                                                      param.offset());
  VLOG(4) << "Builder construction  meta_param";
  paddle::dialect::IrMetaTensor meta_param(&ir_tensor_param);

  VLOG(4) << "Builder construction  dense_grad";
  paddle::dialect::IrTensor ir_tensor_grad(paddle::dialect::TransToPhiDataType(grad.dtype()),
                                                      grad.dims(),
                                                      grad.data_layout(),
                                                      grad.lod(),
                                                      grad.offset());
  VLOG(4) << "Builder construction  meta_grad";
  paddle::dialect::IrMetaTensor meta_grad(&ir_tensor_grad);

  VLOG(4) << "Builder construction  dense_prev";
  paddle::dialect::IrTensor ir_tensor_prev(paddle::dialect::TransToPhiDataType(prev.dtype()),
                                                      prev.dims(),
                                                      prev.data_layout(),
                                                      prev.lod(),
                                                      prev.offset());
  VLOG(4) << "Builder construction  meta_prev";
  paddle::dialect::IrMetaTensor meta_prev(&ir_tensor_prev);

  VLOG(4) << "Builder construction  dense_learning_rate";
  paddle::dialect::IrTensor ir_tensor_learning_rate(paddle::dialect::TransToPhiDataType(learning_rate.dtype()),
                                                      learning_rate.dims(),
                                                      learning_rate.data_layout(),
                                                      learning_rate.lod(),
                                                      learning_rate.offset());
  VLOG(4) << "Builder construction  meta_learning_rate";
  paddle::dialect::IrMetaTensor meta_learning_rate(&ir_tensor_learning_rate);

  paddle::dialect::IrMetaTensor meta_master_param;
  paddle::dialect::IrTensor ir_tensor_master_param;
  if (master_param_.impl() != nullptr) {
    paddle::dialect::DenseTensorType master_param = master_param_.type().dyn_cast<paddle::dialect::DenseTensorType>();
    VLOG(4) << "Builder construction  dense_master_param";
    ir_tensor_master_param = paddle::dialect::IrTensor(paddle::dialect::TransToPhiDataType(master_param.dtype()),
                                                        master_param.dims(),
                                                        master_param.data_layout(),
                                                        master_param.lod(),
                                                        master_param.offset());
    VLOG(4) << "Builder construction  meta_master_param";
    meta_master_param = paddle::dialect::IrMetaTensor(&ir_tensor_master_param);
  }


  VLOG(4) << "Builder construction  dense_learning_rate_range";
  paddle::dialect::IrTensor ir_tensor_learning_rate_range(paddle::dialect::TransToPhiDataType(learning_rate_range.dtype()),
                                                      learning_rate_range.dims(),
                                                      learning_rate_range.data_layout(),
                                                      learning_rate_range.lod(),
                                                      learning_rate_range.offset());
  VLOG(4) << "Builder construction  meta_learning_rate_range";
  paddle::dialect::IrMetaTensor meta_learning_rate_range(&ir_tensor_learning_rate_range);

  VLOG(4) << "Builder construction  dense_etas";
  paddle::dialect::IrTensor ir_tensor_etas(paddle::dialect::TransToPhiDataType(etas.dtype()),
                                                      etas.dims(),
                                                      etas.data_layout(),
                                                      etas.lod(),
                                                      etas.offset());
  VLOG(4) << "Builder construction  meta_etas";
  paddle::dialect::IrMetaTensor meta_etas(&ir_tensor_etas);
  paddle::dialect::IrTensor dense_param_out;
  paddle::dialect::IrMetaTensor meta_param_out(&dense_param_out);
  paddle::dialect::IrTensor dense_prev_out;
  paddle::dialect::IrMetaTensor meta_prev_out(&dense_prev_out);
  paddle::dialect::IrTensor dense_learning_rate_out;
  paddle::dialect::IrMetaTensor meta_learning_rate_out(&dense_learning_rate_out);
  paddle::dialect::IrTensor dense_master_param_out;
  paddle::dialect::IrMetaTensor meta_master_param_out(&dense_master_param_out);

  phi::RpropInferMeta(meta_param, meta_grad, meta_prev, meta_learning_rate, meta_master_param, meta_learning_rate_range, meta_etas, multi_precision, &meta_param_out, &meta_prev_out, &meta_learning_rate_out, &meta_master_param_out);

  std::vector<pir::Type> argument_outputs;
  pir::Type param_out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_param_out.dtype()), dense_param_out.dims(), dense_param_out.layout(), dense_param_out.lod(), dense_param_out.offset());
  argument_outputs.push_back(param_out_dense_tensor_type);

  pir::Type prev_out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_prev_out.dtype()), dense_prev_out.dims(), dense_prev_out.layout(), dense_prev_out.lod(), dense_prev_out.offset());
  argument_outputs.push_back(prev_out_dense_tensor_type);

  pir::Type learning_rate_out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_learning_rate_out.dtype()), dense_learning_rate_out.dims(), dense_learning_rate_out.layout(), dense_learning_rate_out.lod(), dense_learning_rate_out.offset());
  argument_outputs.push_back(learning_rate_out_dense_tensor_type);

  if (master_param_.impl() != nullptr) {
    pir::Type master_param_out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_master_param_out.dtype()), dense_master_param_out.dims(), dense_master_param_out.layout(), dense_master_param_out.lod(), dense_master_param_out.offset());
    argument_outputs.push_back(master_param_out_dense_tensor_type);
  } else {
    pir::Type master_param_out_type;
    argument_outputs.push_back(master_param_out_type);
  }

  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void Rprop_Op::VerifySig() {
  VLOG(4) << "Start Verifying inputs, outputs and attributes for: Rprop_Op.";
  VLOG(4) << "Verifying inputs:";
  {
  auto input_size = num_operands();
  IR_ENFORCE(input_size == 7u,
                    "The size %d of inputs must be equal to 7.", input_size);
  IR_ENFORCE((*this)->operand_source(0).type().isa<paddle::dialect::DenseTensorType>(),
                  "Type validation failed for the 0th input, got %s.", (*this)->operand_source(0).type());
  IR_ENFORCE((*this)->operand_source(1).type().isa<paddle::dialect::DenseTensorType>(),
                  "Type validation failed for the 1th input, got %s.", (*this)->operand_source(1).type());
  IR_ENFORCE((*this)->operand_source(2).type().isa<paddle::dialect::DenseTensorType>(),
                  "Type validation failed for the 2th input, got %s.", (*this)->operand_source(2).type());
  IR_ENFORCE((*this)->operand_source(3).type().isa<paddle::dialect::DenseTensorType>(),
                  "Type validation failed for the 3th input, got %s.", (*this)->operand_source(3).type());
  if (auto val = (*this)->operand(4)) {
    IR_ENFORCE(val.type().isa<paddle::dialect::DenseTensorType>(),
                   "Type validation failed for the 4th input, got %s.", (*this)->operand_source(4).type());
  }
  IR_ENFORCE((*this)->operand_source(5).type().isa<paddle::dialect::DenseTensorType>(),
                  "Type validation failed for the 5th input, got %s.", (*this)->operand_source(5).type());
  IR_ENFORCE((*this)->operand_source(6).type().isa<paddle::dialect::DenseTensorType>(),
                  "Type validation failed for the 6th input, got %s.", (*this)->operand_source(6).type());
  }
  VLOG(4) << "Verifying attributes:";
  {
  auto& attributes = this->attributes();
  IR_ENFORCE(attributes.count("multi_precision")>0,
                 "multi_precision does not exist.");
  IR_ENFORCE(attributes.at("multi_precision").isa<pir::BoolAttribute>(),
                 "Type of attribute: multi_precision is not pir::BoolAttribute.");

  }
  VLOG(4) << "Verifying outputs:";
  {
  auto output_size = num_results();
  IR_ENFORCE(output_size == 4u,
                    "The size %d of outputs must be equal to 4.", output_size);
  IR_ENFORCE((*this)->result(0).type().isa<paddle::dialect::DenseTensorType>(),
                 "Type validation failed for the 0th output.");
  IR_ENFORCE((*this)->result(1).type().isa<paddle::dialect::DenseTensorType>(),
                 "Type validation failed for the 1th output.");
  IR_ENFORCE((*this)->result(2).type().isa<paddle::dialect::DenseTensorType>(),
                 "Type validation failed for the 2th output.");
  if (auto output_3_type = (*this)->result(3).type()) {
    IR_ENFORCE(output_3_type.isa<paddle::dialect::DenseTensorType>(),
                   "Type validation failed for the 3th output.");
  }
  }
  VLOG(4) << "End Verifying for: Rprop_Op.";
}

void Rprop_Op::InferMeta( phi::InferMetaContext *infer_meta ) {
  auto fn = PD_INFER_META(phi::RpropInferMeta);
  fn(infer_meta);
}

phi::DataType Rprop_Op::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: Rprop_Op";
  

  // deal support data transform
  VLOG(8) << "SUPPORT_TRANSFORM: " << "['learning_rate'];";
  return tensor_dtype;


  return expected_kernel_dtype;
}

OpInfoTuple RsqrtOp::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("x", "paddle::dialect::DenseTensorType", false, false, false, true) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = {  };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("out", "paddle::dialect::DenseTensorType", false, false) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("UnchangedInferMeta", {"x"}, "rsqrt", {"x"}, {}, {}, {}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "rsqrt");
}

void RsqrtOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_) {
  VLOG(4) << "Start build RsqrtOp";



  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);
  paddle::dialect::IrTensor dense_out;
  paddle::dialect::IrMetaTensor meta_out(&dense_out);

  phi::UnchangedInferMeta(meta_x, &meta_out);

  std::vector<pir::Type> argument_outputs;
  pir::Type out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_out.dtype()), dense_out.dims(), dense_out.layout(), dense_out.lod(), dense_out.offset());
  argument_outputs.push_back(out_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void RsqrtOp::VerifySig() {
  VLOG(4) << "Start Verifying inputs, outputs and attributes for: RsqrtOp.";
  VLOG(4) << "Verifying inputs:";
  {
  auto input_size = num_operands();
  IR_ENFORCE(input_size == 1u,
                    "The size %d of inputs must be equal to 1.", input_size);
  IR_ENFORCE((*this)->operand_source(0).type().isa<paddle::dialect::DenseTensorType>(),
                  "Type validation failed for the 0th input, got %s.", (*this)->operand_source(0).type());
  }
  VLOG(4) << "Verifying attributes:";
  {
  // Attributes num is 0, not need to check attributes type.
  }
  VLOG(4) << "Verifying outputs:";
  {
  auto output_size = num_results();
  IR_ENFORCE(output_size == 1u,
                    "The size %d of outputs must be equal to 1.", output_size);
  IR_ENFORCE((*this)->result(0).type().isa<paddle::dialect::DenseTensorType>(),
                 "Type validation failed for the 0th output.");
  }
  VLOG(4) << "End Verifying for: RsqrtOp.";
}

void RsqrtOp::InferMeta( phi::InferMetaContext *infer_meta ) {
  auto fn = PD_INFER_META(phi::UnchangedInferMeta);
  fn(infer_meta);
}

phi::DataType RsqrtOp::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: RsqrtOp";
  


  return expected_kernel_dtype;
}

OpInfoTuple Rsqrt_Op::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("x", "paddle::dialect::DenseTensorType", false, false, false, true) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = {  };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("out", "paddle::dialect::DenseTensorType", false, false) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("UnchangedInferMeta", {"x"}, "rsqrt", {"x"}, {}, {}, {{"out", "x"}}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "rsqrt");
}

void Rsqrt_Op::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_) {
  VLOG(4) << "Start build Rsqrt_Op";



  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);
  paddle::dialect::IrTensor dense_out;
  paddle::dialect::IrMetaTensor meta_out(&dense_out);

  phi::UnchangedInferMeta(meta_x, &meta_out);

  std::vector<pir::Type> argument_outputs;
  pir::Type out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_out.dtype()), dense_out.dims(), dense_out.layout(), dense_out.lod(), dense_out.offset());
  argument_outputs.push_back(out_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void Rsqrt_Op::VerifySig() {
  VLOG(4) << "Start Verifying inputs, outputs and attributes for: Rsqrt_Op.";
  VLOG(4) << "Verifying inputs:";
  {
  auto input_size = num_operands();
  IR_ENFORCE(input_size == 1u,
                    "The size %d of inputs must be equal to 1.", input_size);
  IR_ENFORCE((*this)->operand_source(0).type().isa<paddle::dialect::DenseTensorType>(),
                  "Type validation failed for the 0th input, got %s.", (*this)->operand_source(0).type());
  }
  VLOG(4) << "Verifying attributes:";
  {
  // Attributes num is 0, not need to check attributes type.
  }
  VLOG(4) << "Verifying outputs:";
  {
  auto output_size = num_results();
  IR_ENFORCE(output_size == 1u,
                    "The size %d of outputs must be equal to 1.", output_size);
  IR_ENFORCE((*this)->result(0).type().isa<paddle::dialect::DenseTensorType>(),
                 "Type validation failed for the 0th output.");
  }
  VLOG(4) << "End Verifying for: Rsqrt_Op.";
}

void Rsqrt_Op::InferMeta( phi::InferMetaContext *infer_meta ) {
  auto fn = PD_INFER_META(phi::UnchangedInferMeta);
  fn(infer_meta);
}

phi::DataType Rsqrt_Op::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: Rsqrt_Op";
  


  return expected_kernel_dtype;
}

const char *ScaleOp::attributes_name[2] = { "bias", "bias_after_scale" };

OpInfoTuple ScaleOp::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("x", "paddle::dialect::DenseTensorType", false, false, false, true), paddle::dialect::OpInputInfo("scale", "paddle::dialect::ScalarAttribute", false, false, true, false) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = { paddle::dialect::OpAttributeInfo("bias", "pir::FloatAttribute", ""), paddle::dialect::OpAttributeInfo("bias_after_scale", "pir::BoolAttribute", "") };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("out", "paddle::dialect::DenseTensorType", false, false) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("UnchangedInferMeta", {"x"}, "scale", {"x", "scale", "bias", "bias_after_scale"}, {"x"}, {}, {}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "scale");
}

void ScaleOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, float scale, float bias, bool bias_after_scale) {
  VLOG(4) << "Start build ScaleOp";


  // Generate scalar mutable attribute: scale
  paddle::dialect::FullOp full_scale_op = builder.Build<paddle::dialect::FullOp>(std::vector<int64_t>{1}, scale, phi::DataType::FLOAT32, phi::CPUPlace());
  pir::OpResult scale_ = full_scale_op->result(0);
    
  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_, scale_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_bias = pir::FloatAttribute::get(pir::IrContext::Instance(), bias);
  argument.AddAttribute("bias", attr_bias);
  pir::Attribute attr_bias_after_scale = pir::BoolAttribute::get(pir::IrContext::Instance(), bias_after_scale);
  argument.AddAttribute("bias_after_scale", attr_bias_after_scale);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);
  paddle::dialect::IrTensor dense_out;
  paddle::dialect::IrMetaTensor meta_out(&dense_out);

  phi::UnchangedInferMeta(meta_x, &meta_out);

  std::vector<pir::Type> argument_outputs;
  pir::Type out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_out.dtype()), dense_out.dims(), dense_out.layout(), dense_out.lod(), dense_out.offset());
  argument_outputs.push_back(out_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void ScaleOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, pir::AttributeMap attributes) {
  VLOG(4) << "Start build ScaleOp";


  IR_ENFORCE(
      attributes.find("scale") != attributes.end(),
          "'scale' Attribute is expected for ScaleOp. ");
  float scale = attributes.at("scale").dyn_cast<paddle::dialect::ScalarAttribute>().data().to<float>();

  IR_ENFORCE(
      attributes.find("bias") != attributes.end(),
          "'bias' Attribute is expected for ScaleOp. ");
  float bias = attributes.at("bias").dyn_cast<pir::FloatAttribute>().data();

  IR_ENFORCE(
      attributes.find("bias_after_scale") != attributes.end(),
          "'bias_after_scale' Attribute is expected for ScaleOp. ");
  bool bias_after_scale = attributes.at("bias_after_scale").dyn_cast<pir::BoolAttribute>().data();

  // Generate scalar mutable attribute: scale
  paddle::dialect::FullOp full_scale_op = builder.Build<paddle::dialect::FullOp>(std::vector<int64_t>{1}, scale, phi::DataType::FLOAT32, phi::CPUPlace());
  pir::OpResult scale_ = full_scale_op->result(0);
    
  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_, scale_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_bias = pir::FloatAttribute::get(pir::IrContext::Instance(), bias);
  argument.AddAttribute("bias", attr_bias);
  pir::Attribute attr_bias_after_scale = pir::BoolAttribute::get(pir::IrContext::Instance(), bias_after_scale);
  argument.AddAttribute("bias_after_scale", attr_bias_after_scale);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);
  paddle::dialect::IrTensor dense_out;
  paddle::dialect::IrMetaTensor meta_out(&dense_out);

  phi::UnchangedInferMeta(meta_x, &meta_out);

  std::vector<pir::Type> argument_outputs;
  pir::Type out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_out.dtype()), dense_out.dims(), dense_out.layout(), dense_out.lod(), dense_out.offset());
  argument_outputs.push_back(out_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void ScaleOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, pir::Value scale_, float bias, bool bias_after_scale) {
  VLOG(4) << "Start build ScaleOp";



  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_, scale_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_bias = pir::FloatAttribute::get(pir::IrContext::Instance(), bias);
  argument.AddAttribute("bias", attr_bias);
  pir::Attribute attr_bias_after_scale = pir::BoolAttribute::get(pir::IrContext::Instance(), bias_after_scale);
  argument.AddAttribute("bias_after_scale", attr_bias_after_scale);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;
  phi::Scalar scale;
  if (scale_.dyn_cast<pir::OpResult>() && scale_.dyn_cast<pir::OpResult>().owner()->isa<paddle::dialect::FullOp>()) {
    scale = std::move(phi::Scalar(scale_.dyn_cast<pir::OpResult>().owner()
                                  ->dyn_cast<paddle::dialect::FullOp>()
                                  .attribute("value")
                                  .dyn_cast<paddle::dialect::ScalarAttribute>()
                                  .data()
                                  .to<int>()));
  }
  else {
    scale = std::move(phi::Scalar(-1));
    scale.SetFromTensor(true);
  }


  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);
  paddle::dialect::IrTensor dense_out;
  paddle::dialect::IrMetaTensor meta_out(&dense_out);

  phi::UnchangedInferMeta(meta_x, &meta_out);

  std::vector<pir::Type> argument_outputs;
  pir::Type out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_out.dtype()), dense_out.dims(), dense_out.layout(), dense_out.lod(), dense_out.offset());
  argument_outputs.push_back(out_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void ScaleOp::VerifySig() {
  VLOG(4) << "Start Verifying inputs, outputs and attributes for: ScaleOp.";
  VLOG(4) << "Verifying inputs:";
  {
  auto input_size = num_operands();
  IR_ENFORCE(input_size == 2u,
                    "The size %d of inputs must be equal to 2.", input_size);
  IR_ENFORCE((*this)->operand_source(0).type().isa<paddle::dialect::DenseTensorType>(),
                  "Type validation failed for the 0th input, got %s.", (*this)->operand_source(0).type());
  IR_ENFORCE((*this)->operand_source(1).type().isa<paddle::dialect::DenseTensorType>(),
                  "Type validation failed for the 1th input, got %s.", (*this)->operand_source(1).type());
  }
  VLOG(4) << "Verifying attributes:";
  {
  auto& attributes = this->attributes();
  IR_ENFORCE(attributes.count("bias")>0,
                 "bias does not exist.");
  IR_ENFORCE(attributes.at("bias").isa<pir::FloatAttribute>(),
                 "Type of attribute: bias is not pir::FloatAttribute.");

  IR_ENFORCE(attributes.count("bias_after_scale")>0,
                 "bias_after_scale does not exist.");
  IR_ENFORCE(attributes.at("bias_after_scale").isa<pir::BoolAttribute>(),
                 "Type of attribute: bias_after_scale is not pir::BoolAttribute.");

  }
  VLOG(4) << "Verifying outputs:";
  {
  auto output_size = num_results();
  IR_ENFORCE(output_size == 1u,
                    "The size %d of outputs must be equal to 1.", output_size);
  IR_ENFORCE((*this)->result(0).type().isa<paddle::dialect::DenseTensorType>(),
                 "Type validation failed for the 0th output.");
  }
  VLOG(4) << "End Verifying for: ScaleOp.";
}

void ScaleOp::InferMeta( phi::InferMetaContext *infer_meta ) {
  auto fn = PD_INFER_META(phi::UnchangedInferMeta);
  fn(infer_meta);
}

phi::DataType ScaleOp::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: ScaleOp";
  


  return expected_kernel_dtype;
}

const char *ScaleSrOp::attributes_name[2] = { "bias", "bias_after_scale" };

OpInfoTuple ScaleSrOp::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("x", "paddle::dialect::SelectedRowsType", false, false, false, true), paddle::dialect::OpInputInfo("scale", "paddle::dialect::ScalarAttribute", false, false, true, false) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = { paddle::dialect::OpAttributeInfo("bias", "pir::FloatAttribute", ""), paddle::dialect::OpAttributeInfo("bias_after_scale", "pir::BoolAttribute", "") };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("out", "paddle::dialect::SelectedRowsType", false, false) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("UnchangedInferMeta", {"x"}, "scale_sr", {"x", "scale", "bias", "bias_after_scale"}, {"x"}, {}, {}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "scale");
}

void ScaleSrOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, float scale, float bias, bool bias_after_scale) {
  VLOG(4) << "Start build ScaleSrOp";


  // Generate scalar mutable attribute: scale
  paddle::dialect::FullOp full_scale_op = builder.Build<paddle::dialect::FullOp>(std::vector<int64_t>{1}, scale, phi::DataType::FLOAT32, phi::CPUPlace());
  pir::OpResult scale_ = full_scale_op->result(0);
    
  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_, scale_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_bias = pir::FloatAttribute::get(pir::IrContext::Instance(), bias);
  argument.AddAttribute("bias", attr_bias);
  pir::Attribute attr_bias_after_scale = pir::BoolAttribute::get(pir::IrContext::Instance(), bias_after_scale);
  argument.AddAttribute("bias_after_scale", attr_bias_after_scale);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::SelectedRowsType x = x_.type().dyn_cast<paddle::dialect::SelectedRowsType>(); (void)x;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);
  paddle::dialect::IrSelectedRows dense_out;
  paddle::dialect::IrMetaTensor meta_out(&dense_out);

  phi::UnchangedInferMeta(meta_x, &meta_out);

  std::vector<pir::Type> argument_outputs;
  pir::Type out_dense_tensor_type = paddle::dialect::SelectedRowsType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_out.dtype()), dense_out.dims(), dense_out.layout(), dense_out.lod(), dense_out.offset());
  argument_outputs.push_back(out_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void ScaleSrOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, pir::AttributeMap attributes) {
  VLOG(4) << "Start build ScaleSrOp";


  IR_ENFORCE(
      attributes.find("scale") != attributes.end(),
          "'scale' Attribute is expected for ScaleSrOp. ");
  float scale = attributes.at("scale").dyn_cast<paddle::dialect::ScalarAttribute>().data().to<float>();

  IR_ENFORCE(
      attributes.find("bias") != attributes.end(),
          "'bias' Attribute is expected for ScaleSrOp. ");
  float bias = attributes.at("bias").dyn_cast<pir::FloatAttribute>().data();

  IR_ENFORCE(
      attributes.find("bias_after_scale") != attributes.end(),
          "'bias_after_scale' Attribute is expected for ScaleSrOp. ");
  bool bias_after_scale = attributes.at("bias_after_scale").dyn_cast<pir::BoolAttribute>().data();

  // Generate scalar mutable attribute: scale
  paddle::dialect::FullOp full_scale_op = builder.Build<paddle::dialect::FullOp>(std::vector<int64_t>{1}, scale, phi::DataType::FLOAT32, phi::CPUPlace());
  pir::OpResult scale_ = full_scale_op->result(0);
    
  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_, scale_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_bias = pir::FloatAttribute::get(pir::IrContext::Instance(), bias);
  argument.AddAttribute("bias", attr_bias);
  pir::Attribute attr_bias_after_scale = pir::BoolAttribute::get(pir::IrContext::Instance(), bias_after_scale);
  argument.AddAttribute("bias_after_scale", attr_bias_after_scale);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::SelectedRowsType x = x_.type().dyn_cast<paddle::dialect::SelectedRowsType>(); (void)x;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);
  paddle::dialect::IrSelectedRows dense_out;
  paddle::dialect::IrMetaTensor meta_out(&dense_out);

  phi::UnchangedInferMeta(meta_x, &meta_out);

  std::vector<pir::Type> argument_outputs;
  pir::Type out_dense_tensor_type = paddle::dialect::SelectedRowsType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_out.dtype()), dense_out.dims(), dense_out.layout(), dense_out.lod(), dense_out.offset());
  argument_outputs.push_back(out_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void ScaleSrOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, pir::Value scale_, float bias, bool bias_after_scale) {
  VLOG(4) << "Start build ScaleSrOp";



  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_, scale_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_bias = pir::FloatAttribute::get(pir::IrContext::Instance(), bias);
  argument.AddAttribute("bias", attr_bias);
  pir::Attribute attr_bias_after_scale = pir::BoolAttribute::get(pir::IrContext::Instance(), bias_after_scale);
  argument.AddAttribute("bias_after_scale", attr_bias_after_scale);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::SelectedRowsType x = x_.type().dyn_cast<paddle::dialect::SelectedRowsType>(); (void)x;
  phi::Scalar scale;
  if (scale_.dyn_cast<pir::OpResult>() && scale_.dyn_cast<pir::OpResult>().owner()->isa<paddle::dialect::FullOp>()) {
    scale = std::move(phi::Scalar(scale_.dyn_cast<pir::OpResult>().owner()
                                  ->dyn_cast<paddle::dialect::FullOp>()
                                  .attribute("value")
                                  .dyn_cast<paddle::dialect::ScalarAttribute>()
                                  .data()
                                  .to<int>()));
  }
  else {
    scale = std::move(phi::Scalar(-1));
    scale.SetFromTensor(true);
  }


  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);
  paddle::dialect::IrSelectedRows dense_out;
  paddle::dialect::IrMetaTensor meta_out(&dense_out);

  phi::UnchangedInferMeta(meta_x, &meta_out);

  std::vector<pir::Type> argument_outputs;
  pir::Type out_dense_tensor_type = paddle::dialect::SelectedRowsType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_out.dtype()), dense_out.dims(), dense_out.layout(), dense_out.lod(), dense_out.offset());
  argument_outputs.push_back(out_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void ScaleSrOp::VerifySig() {
  VLOG(4) << "Start Verifying inputs, outputs and attributes for: ScaleSrOp.";
  VLOG(4) << "Verifying inputs:";
  {
  auto input_size = num_operands();
  IR_ENFORCE(input_size == 2u,
                    "The size %d of inputs must be equal to 2.", input_size);
  IR_ENFORCE((*this)->operand_source(0).type().isa<paddle::dialect::SelectedRowsType>(),
                  "Type validation failed for the 0th input, got %s.", (*this)->operand_source(0).type());
  IR_ENFORCE((*this)->operand_source(1).type().isa<paddle::dialect::DenseTensorType>(),
                  "Type validation failed for the 1th input, got %s.", (*this)->operand_source(1).type());
  }
  VLOG(4) << "Verifying attributes:";
  {
  auto& attributes = this->attributes();
  IR_ENFORCE(attributes.count("bias")>0,
                 "bias does not exist.");
  IR_ENFORCE(attributes.at("bias").isa<pir::FloatAttribute>(),
                 "Type of attribute: bias is not pir::FloatAttribute.");

  IR_ENFORCE(attributes.count("bias_after_scale")>0,
                 "bias_after_scale does not exist.");
  IR_ENFORCE(attributes.at("bias_after_scale").isa<pir::BoolAttribute>(),
                 "Type of attribute: bias_after_scale is not pir::BoolAttribute.");

  }
  VLOG(4) << "Verifying outputs:";
  {
  auto output_size = num_results();
  IR_ENFORCE(output_size == 1u,
                    "The size %d of outputs must be equal to 1.", output_size);
  IR_ENFORCE((*this)->result(0).type().isa<paddle::dialect::SelectedRowsType>(),
                 "Type validation failed for the 0th output.");
  }
  VLOG(4) << "End Verifying for: ScaleSrOp.";
}

void ScaleSrOp::InferMeta( phi::InferMetaContext *infer_meta ) {
  auto fn = PD_INFER_META(phi::UnchangedInferMeta);
  fn(infer_meta);
}

phi::DataType ScaleSrOp::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: ScaleSrOp";
  


  return expected_kernel_dtype;
}

const char *Scale_Op::attributes_name[2] = { "bias", "bias_after_scale" };

OpInfoTuple Scale_Op::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("x", "paddle::dialect::DenseTensorType", false, false, false, true), paddle::dialect::OpInputInfo("scale", "paddle::dialect::ScalarAttribute", false, false, true, false) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = { paddle::dialect::OpAttributeInfo("bias", "pir::FloatAttribute", ""), paddle::dialect::OpAttributeInfo("bias_after_scale", "pir::BoolAttribute", "") };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("out", "paddle::dialect::DenseTensorType", false, false) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("UnchangedInferMeta", {"x"}, "scale", {"x", "scale", "bias", "bias_after_scale"}, {"x"}, {}, {{"out", "x"}}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "scale");
}

void Scale_Op::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, float scale, float bias, bool bias_after_scale) {
  VLOG(4) << "Start build Scale_Op";


  // Generate scalar mutable attribute: scale
  paddle::dialect::FullOp full_scale_op = builder.Build<paddle::dialect::FullOp>(std::vector<int64_t>{1}, scale, phi::DataType::FLOAT32, phi::CPUPlace());
  pir::OpResult scale_ = full_scale_op->result(0);
    
  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_, scale_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_bias = pir::FloatAttribute::get(pir::IrContext::Instance(), bias);
  argument.AddAttribute("bias", attr_bias);
  pir::Attribute attr_bias_after_scale = pir::BoolAttribute::get(pir::IrContext::Instance(), bias_after_scale);
  argument.AddAttribute("bias_after_scale", attr_bias_after_scale);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);
  paddle::dialect::IrTensor dense_out;
  paddle::dialect::IrMetaTensor meta_out(&dense_out);

  phi::UnchangedInferMeta(meta_x, &meta_out);

  std::vector<pir::Type> argument_outputs;
  pir::Type out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_out.dtype()), dense_out.dims(), dense_out.layout(), dense_out.lod(), dense_out.offset());
  argument_outputs.push_back(out_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void Scale_Op::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, pir::AttributeMap attributes) {
  VLOG(4) << "Start build Scale_Op";


  IR_ENFORCE(
      attributes.find("scale") != attributes.end(),
          "'scale' Attribute is expected for Scale_Op. ");
  float scale = attributes.at("scale").dyn_cast<paddle::dialect::ScalarAttribute>().data().to<float>();

  IR_ENFORCE(
      attributes.find("bias") != attributes.end(),
          "'bias' Attribute is expected for Scale_Op. ");
  float bias = attributes.at("bias").dyn_cast<pir::FloatAttribute>().data();

  IR_ENFORCE(
      attributes.find("bias_after_scale") != attributes.end(),
          "'bias_after_scale' Attribute is expected for Scale_Op. ");
  bool bias_after_scale = attributes.at("bias_after_scale").dyn_cast<pir::BoolAttribute>().data();

  // Generate scalar mutable attribute: scale
  paddle::dialect::FullOp full_scale_op = builder.Build<paddle::dialect::FullOp>(std::vector<int64_t>{1}, scale, phi::DataType::FLOAT32, phi::CPUPlace());
  pir::OpResult scale_ = full_scale_op->result(0);
    
  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_, scale_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_bias = pir::FloatAttribute::get(pir::IrContext::Instance(), bias);
  argument.AddAttribute("bias", attr_bias);
  pir::Attribute attr_bias_after_scale = pir::BoolAttribute::get(pir::IrContext::Instance(), bias_after_scale);
  argument.AddAttribute("bias_after_scale", attr_bias_after_scale);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);
  paddle::dialect::IrTensor dense_out;
  paddle::dialect::IrMetaTensor meta_out(&dense_out);

  phi::UnchangedInferMeta(meta_x, &meta_out);

  std::vector<pir::Type> argument_outputs;
  pir::Type out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_out.dtype()), dense_out.dims(), dense_out.layout(), dense_out.lod(), dense_out.offset());
  argument_outputs.push_back(out_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void Scale_Op::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, pir::Value scale_, float bias, bool bias_after_scale) {
  VLOG(4) << "Start build Scale_Op";



  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_, scale_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_bias = pir::FloatAttribute::get(pir::IrContext::Instance(), bias);
  argument.AddAttribute("bias", attr_bias);
  pir::Attribute attr_bias_after_scale = pir::BoolAttribute::get(pir::IrContext::Instance(), bias_after_scale);
  argument.AddAttribute("bias_after_scale", attr_bias_after_scale);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;
  phi::Scalar scale;
  if (scale_.dyn_cast<pir::OpResult>() && scale_.dyn_cast<pir::OpResult>().owner()->isa<paddle::dialect::FullOp>()) {
    scale = std::move(phi::Scalar(scale_.dyn_cast<pir::OpResult>().owner()
                                  ->dyn_cast<paddle::dialect::FullOp>()
                                  .attribute("value")
                                  .dyn_cast<paddle::dialect::ScalarAttribute>()
                                  .data()
                                  .to<int>()));
  }
  else {
    scale = std::move(phi::Scalar(-1));
    scale.SetFromTensor(true);
  }


  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);
  paddle::dialect::IrTensor dense_out;
  paddle::dialect::IrMetaTensor meta_out(&dense_out);

  phi::UnchangedInferMeta(meta_x, &meta_out);

  std::vector<pir::Type> argument_outputs;
  pir::Type out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_out.dtype()), dense_out.dims(), dense_out.layout(), dense_out.lod(), dense_out.offset());
  argument_outputs.push_back(out_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void Scale_Op::VerifySig() {
  VLOG(4) << "Start Verifying inputs, outputs and attributes for: Scale_Op.";
  VLOG(4) << "Verifying inputs:";
  {
  auto input_size = num_operands();
  IR_ENFORCE(input_size == 2u,
                    "The size %d of inputs must be equal to 2.", input_size);
  IR_ENFORCE((*this)->operand_source(0).type().isa<paddle::dialect::DenseTensorType>(),
                  "Type validation failed for the 0th input, got %s.", (*this)->operand_source(0).type());
  IR_ENFORCE((*this)->operand_source(1).type().isa<paddle::dialect::DenseTensorType>(),
                  "Type validation failed for the 1th input, got %s.", (*this)->operand_source(1).type());
  }
  VLOG(4) << "Verifying attributes:";
  {
  auto& attributes = this->attributes();
  IR_ENFORCE(attributes.count("bias")>0,
                 "bias does not exist.");
  IR_ENFORCE(attributes.at("bias").isa<pir::FloatAttribute>(),
                 "Type of attribute: bias is not pir::FloatAttribute.");

  IR_ENFORCE(attributes.count("bias_after_scale")>0,
                 "bias_after_scale does not exist.");
  IR_ENFORCE(attributes.at("bias_after_scale").isa<pir::BoolAttribute>(),
                 "Type of attribute: bias_after_scale is not pir::BoolAttribute.");

  }
  VLOG(4) << "Verifying outputs:";
  {
  auto output_size = num_results();
  IR_ENFORCE(output_size == 1u,
                    "The size %d of outputs must be equal to 1.", output_size);
  IR_ENFORCE((*this)->result(0).type().isa<paddle::dialect::DenseTensorType>(),
                 "Type validation failed for the 0th output.");
  }
  VLOG(4) << "End Verifying for: Scale_Op.";
}

void Scale_Op::InferMeta( phi::InferMetaContext *infer_meta ) {
  auto fn = PD_INFER_META(phi::UnchangedInferMeta);
  fn(infer_meta);
}

phi::DataType Scale_Op::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: Scale_Op";
  


  return expected_kernel_dtype;
}

const char *ScaleSr_Op::attributes_name[2] = { "bias", "bias_after_scale" };

OpInfoTuple ScaleSr_Op::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("x", "paddle::dialect::SelectedRowsType", false, false, false, true), paddle::dialect::OpInputInfo("scale", "paddle::dialect::ScalarAttribute", false, false, true, false) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = { paddle::dialect::OpAttributeInfo("bias", "pir::FloatAttribute", ""), paddle::dialect::OpAttributeInfo("bias_after_scale", "pir::BoolAttribute", "") };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("out", "paddle::dialect::SelectedRowsType", false, false) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("UnchangedInferMeta", {"x"}, "scale_sr", {"x", "scale", "bias", "bias_after_scale"}, {"x"}, {}, {{"out", "x"}}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "scale");
}

void ScaleSr_Op::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, float scale, float bias, bool bias_after_scale) {
  VLOG(4) << "Start build ScaleSr_Op";


  // Generate scalar mutable attribute: scale
  paddle::dialect::FullOp full_scale_op = builder.Build<paddle::dialect::FullOp>(std::vector<int64_t>{1}, scale, phi::DataType::FLOAT32, phi::CPUPlace());
  pir::OpResult scale_ = full_scale_op->result(0);
    
  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_, scale_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_bias = pir::FloatAttribute::get(pir::IrContext::Instance(), bias);
  argument.AddAttribute("bias", attr_bias);
  pir::Attribute attr_bias_after_scale = pir::BoolAttribute::get(pir::IrContext::Instance(), bias_after_scale);
  argument.AddAttribute("bias_after_scale", attr_bias_after_scale);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::SelectedRowsType x = x_.type().dyn_cast<paddle::dialect::SelectedRowsType>(); (void)x;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);
  paddle::dialect::IrSelectedRows dense_out;
  paddle::dialect::IrMetaTensor meta_out(&dense_out);

  phi::UnchangedInferMeta(meta_x, &meta_out);

  std::vector<pir::Type> argument_outputs;
  pir::Type out_dense_tensor_type = paddle::dialect::SelectedRowsType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_out.dtype()), dense_out.dims(), dense_out.layout(), dense_out.lod(), dense_out.offset());
  argument_outputs.push_back(out_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void ScaleSr_Op::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, pir::AttributeMap attributes) {
  VLOG(4) << "Start build ScaleSr_Op";


  IR_ENFORCE(
      attributes.find("scale") != attributes.end(),
          "'scale' Attribute is expected for ScaleSr_Op. ");
  float scale = attributes.at("scale").dyn_cast<paddle::dialect::ScalarAttribute>().data().to<float>();

  IR_ENFORCE(
      attributes.find("bias") != attributes.end(),
          "'bias' Attribute is expected for ScaleSr_Op. ");
  float bias = attributes.at("bias").dyn_cast<pir::FloatAttribute>().data();

  IR_ENFORCE(
      attributes.find("bias_after_scale") != attributes.end(),
          "'bias_after_scale' Attribute is expected for ScaleSr_Op. ");
  bool bias_after_scale = attributes.at("bias_after_scale").dyn_cast<pir::BoolAttribute>().data();

  // Generate scalar mutable attribute: scale
  paddle::dialect::FullOp full_scale_op = builder.Build<paddle::dialect::FullOp>(std::vector<int64_t>{1}, scale, phi::DataType::FLOAT32, phi::CPUPlace());
  pir::OpResult scale_ = full_scale_op->result(0);
    
  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_, scale_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_bias = pir::FloatAttribute::get(pir::IrContext::Instance(), bias);
  argument.AddAttribute("bias", attr_bias);
  pir::Attribute attr_bias_after_scale = pir::BoolAttribute::get(pir::IrContext::Instance(), bias_after_scale);
  argument.AddAttribute("bias_after_scale", attr_bias_after_scale);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::SelectedRowsType x = x_.type().dyn_cast<paddle::dialect::SelectedRowsType>(); (void)x;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);
  paddle::dialect::IrSelectedRows dense_out;
  paddle::dialect::IrMetaTensor meta_out(&dense_out);

  phi::UnchangedInferMeta(meta_x, &meta_out);

  std::vector<pir::Type> argument_outputs;
  pir::Type out_dense_tensor_type = paddle::dialect::SelectedRowsType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_out.dtype()), dense_out.dims(), dense_out.layout(), dense_out.lod(), dense_out.offset());
  argument_outputs.push_back(out_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void ScaleSr_Op::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, pir::Value scale_, float bias, bool bias_after_scale) {
  VLOG(4) << "Start build ScaleSr_Op";



  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_, scale_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_bias = pir::FloatAttribute::get(pir::IrContext::Instance(), bias);
  argument.AddAttribute("bias", attr_bias);
  pir::Attribute attr_bias_after_scale = pir::BoolAttribute::get(pir::IrContext::Instance(), bias_after_scale);
  argument.AddAttribute("bias_after_scale", attr_bias_after_scale);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::SelectedRowsType x = x_.type().dyn_cast<paddle::dialect::SelectedRowsType>(); (void)x;
  phi::Scalar scale;
  if (scale_.dyn_cast<pir::OpResult>() && scale_.dyn_cast<pir::OpResult>().owner()->isa<paddle::dialect::FullOp>()) {
    scale = std::move(phi::Scalar(scale_.dyn_cast<pir::OpResult>().owner()
                                  ->dyn_cast<paddle::dialect::FullOp>()
                                  .attribute("value")
                                  .dyn_cast<paddle::dialect::ScalarAttribute>()
                                  .data()
                                  .to<int>()));
  }
  else {
    scale = std::move(phi::Scalar(-1));
    scale.SetFromTensor(true);
  }


  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);
  paddle::dialect::IrSelectedRows dense_out;
  paddle::dialect::IrMetaTensor meta_out(&dense_out);

  phi::UnchangedInferMeta(meta_x, &meta_out);

  std::vector<pir::Type> argument_outputs;
  pir::Type out_dense_tensor_type = paddle::dialect::SelectedRowsType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_out.dtype()), dense_out.dims(), dense_out.layout(), dense_out.lod(), dense_out.offset());
  argument_outputs.push_back(out_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void ScaleSr_Op::VerifySig() {
  VLOG(4) << "Start Verifying inputs, outputs and attributes for: ScaleSr_Op.";
  VLOG(4) << "Verifying inputs:";
  {
  auto input_size = num_operands();
  IR_ENFORCE(input_size == 2u,
                    "The size %d of inputs must be equal to 2.", input_size);
  IR_ENFORCE((*this)->operand_source(0).type().isa<paddle::dialect::SelectedRowsType>(),
                  "Type validation failed for the 0th input, got %s.", (*this)->operand_source(0).type());
  IR_ENFORCE((*this)->operand_source(1).type().isa<paddle::dialect::DenseTensorType>(),
                  "Type validation failed for the 1th input, got %s.", (*this)->operand_source(1).type());
  }
  VLOG(4) << "Verifying attributes:";
  {
  auto& attributes = this->attributes();
  IR_ENFORCE(attributes.count("bias")>0,
                 "bias does not exist.");
  IR_ENFORCE(attributes.at("bias").isa<pir::FloatAttribute>(),
                 "Type of attribute: bias is not pir::FloatAttribute.");

  IR_ENFORCE(attributes.count("bias_after_scale")>0,
                 "bias_after_scale does not exist.");
  IR_ENFORCE(attributes.at("bias_after_scale").isa<pir::BoolAttribute>(),
                 "Type of attribute: bias_after_scale is not pir::BoolAttribute.");

  }
  VLOG(4) << "Verifying outputs:";
  {
  auto output_size = num_results();
  IR_ENFORCE(output_size == 1u,
                    "The size %d of outputs must be equal to 1.", output_size);
  IR_ENFORCE((*this)->result(0).type().isa<paddle::dialect::SelectedRowsType>(),
                 "Type validation failed for the 0th output.");
  }
  VLOG(4) << "End Verifying for: ScaleSr_Op.";
}

void ScaleSr_Op::InferMeta( phi::InferMetaContext *infer_meta ) {
  auto fn = PD_INFER_META(phi::UnchangedInferMeta);
  fn(infer_meta);
}

phi::DataType ScaleSr_Op::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: ScaleSr_Op";
  


  return expected_kernel_dtype;
}

const char *ScatterOp::attributes_name[1] = { "overwrite" };

OpInfoTuple ScatterOp::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("x", "paddle::dialect::DenseTensorType", false, false, false, true), paddle::dialect::OpInputInfo("index", "paddle::dialect::DenseTensorType", false, false, false, false), paddle::dialect::OpInputInfo("updates", "paddle::dialect::DenseTensorType", false, false, false, true) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = { paddle::dialect::OpAttributeInfo("overwrite", "pir::BoolAttribute", "") };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("out", "paddle::dialect::DenseTensorType", false, false) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("ScatterInferMeta", {"x", "index", "updates", "overwrite"}, "scatter", {"x", "index", "updates", "overwrite"}, {"x"}, {}, {}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "scatter");
}

void ScatterOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, pir::Value index_, pir::Value updates_, bool overwrite) {
  VLOG(4) << "Start build ScatterOp";



  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_, index_, updates_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_overwrite = pir::BoolAttribute::get(pir::IrContext::Instance(), overwrite);
  argument.AddAttribute("overwrite", attr_overwrite);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;
  paddle::dialect::DenseTensorType index = index_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)index;
  paddle::dialect::DenseTensorType updates = updates_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)updates;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);

  VLOG(4) << "Builder construction  dense_index";
  paddle::dialect::IrTensor ir_tensor_index(paddle::dialect::TransToPhiDataType(index.dtype()),
                                                      index.dims(),
                                                      index.data_layout(),
                                                      index.lod(),
                                                      index.offset());
  VLOG(4) << "Builder construction  meta_index";
  paddle::dialect::IrMetaTensor meta_index(&ir_tensor_index);

  VLOG(4) << "Builder construction  dense_updates";
  paddle::dialect::IrTensor ir_tensor_updates(paddle::dialect::TransToPhiDataType(updates.dtype()),
                                                      updates.dims(),
                                                      updates.data_layout(),
                                                      updates.lod(),
                                                      updates.offset());
  VLOG(4) << "Builder construction  meta_updates";
  paddle::dialect::IrMetaTensor meta_updates(&ir_tensor_updates);
  paddle::dialect::IrTensor dense_out;
  paddle::dialect::IrMetaTensor meta_out(&dense_out);

  phi::ScatterInferMeta(meta_x, meta_index, meta_updates, overwrite, &meta_out);

  std::vector<pir::Type> argument_outputs;
  pir::Type out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_out.dtype()), dense_out.dims(), dense_out.layout(), dense_out.lod(), dense_out.offset());
  argument_outputs.push_back(out_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void ScatterOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, pir::Value index_, pir::Value updates_, pir::AttributeMap attributes) {
  VLOG(4) << "Start build ScatterOp";


  IR_ENFORCE(
      attributes.find("overwrite") != attributes.end(),
          "'overwrite' Attribute is expected for ScatterOp. ");
  bool overwrite = attributes.at("overwrite").dyn_cast<pir::BoolAttribute>().data();


  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_, index_, updates_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_overwrite = pir::BoolAttribute::get(pir::IrContext::Instance(), overwrite);
  argument.AddAttribute("overwrite", attr_overwrite);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;
  paddle::dialect::DenseTensorType index = index_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)index;
  paddle::dialect::DenseTensorType updates = updates_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)updates;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);

  VLOG(4) << "Builder construction  dense_index";
  paddle::dialect::IrTensor ir_tensor_index(paddle::dialect::TransToPhiDataType(index.dtype()),
                                                      index.dims(),
                                                      index.data_layout(),
                                                      index.lod(),
                                                      index.offset());
  VLOG(4) << "Builder construction  meta_index";
  paddle::dialect::IrMetaTensor meta_index(&ir_tensor_index);

  VLOG(4) << "Builder construction  dense_updates";
  paddle::dialect::IrTensor ir_tensor_updates(paddle::dialect::TransToPhiDataType(updates.dtype()),
                                                      updates.dims(),
                                                      updates.data_layout(),
                                                      updates.lod(),
                                                      updates.offset());
  VLOG(4) << "Builder construction  meta_updates";
  paddle::dialect::IrMetaTensor meta_updates(&ir_tensor_updates);
  paddle::dialect::IrTensor dense_out;
  paddle::dialect::IrMetaTensor meta_out(&dense_out);

  phi::ScatterInferMeta(meta_x, meta_index, meta_updates, overwrite, &meta_out);

  std::vector<pir::Type> argument_outputs;
  pir::Type out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_out.dtype()), dense_out.dims(), dense_out.layout(), dense_out.lod(), dense_out.offset());
  argument_outputs.push_back(out_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void ScatterOp::VerifySig() {
  VLOG(4) << "Start Verifying inputs, outputs and attributes for: ScatterOp.";
  VLOG(4) << "Verifying inputs:";
  {
  auto input_size = num_operands();
  IR_ENFORCE(input_size == 3u,
                    "The size %d of inputs must be equal to 3.", input_size);
  IR_ENFORCE((*this)->operand_source(0).type().isa<paddle::dialect::DenseTensorType>(),
                  "Type validation failed for the 0th input, got %s.", (*this)->operand_source(0).type());
  IR_ENFORCE((*this)->operand_source(1).type().isa<paddle::dialect::DenseTensorType>(),
                  "Type validation failed for the 1th input, got %s.", (*this)->operand_source(1).type());
  IR_ENFORCE((*this)->operand_source(2).type().isa<paddle::dialect::DenseTensorType>(),
                  "Type validation failed for the 2th input, got %s.", (*this)->operand_source(2).type());
  }
  VLOG(4) << "Verifying attributes:";
  {
  auto& attributes = this->attributes();
  IR_ENFORCE(attributes.count("overwrite")>0,
                 "overwrite does not exist.");
  IR_ENFORCE(attributes.at("overwrite").isa<pir::BoolAttribute>(),
                 "Type of attribute: overwrite is not pir::BoolAttribute.");

  }
  VLOG(4) << "Verifying outputs:";
  {
  auto output_size = num_results();
  IR_ENFORCE(output_size == 1u,
                    "The size %d of outputs must be equal to 1.", output_size);
  IR_ENFORCE((*this)->result(0).type().isa<paddle::dialect::DenseTensorType>(),
                 "Type validation failed for the 0th output.");
  }
  VLOG(4) << "End Verifying for: ScatterOp.";
}

void ScatterOp::InferMeta( phi::InferMetaContext *infer_meta ) {
  auto fn = PD_INFER_META(phi::ScatterInferMeta);
  fn(infer_meta);
}

phi::DataType ScatterOp::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: ScatterOp";
  


  return expected_kernel_dtype;
}

const char *Scatter_Op::attributes_name[1] = { "overwrite" };

OpInfoTuple Scatter_Op::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("x", "paddle::dialect::DenseTensorType", false, false, false, true), paddle::dialect::OpInputInfo("index", "paddle::dialect::DenseTensorType", false, false, false, false), paddle::dialect::OpInputInfo("updates", "paddle::dialect::DenseTensorType", false, false, false, true) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = { paddle::dialect::OpAttributeInfo("overwrite", "pir::BoolAttribute", "") };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("out", "paddle::dialect::DenseTensorType", false, false) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("ScatterInferMeta", {"x", "index", "updates", "overwrite"}, "scatter", {"x", "index", "updates", "overwrite"}, {"x"}, {}, {{"out", "x"}}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "scatter");
}

void Scatter_Op::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, pir::Value index_, pir::Value updates_, bool overwrite) {
  VLOG(4) << "Start build Scatter_Op";



  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_, index_, updates_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_overwrite = pir::BoolAttribute::get(pir::IrContext::Instance(), overwrite);
  argument.AddAttribute("overwrite", attr_overwrite);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;
  paddle::dialect::DenseTensorType index = index_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)index;
  paddle::dialect::DenseTensorType updates = updates_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)updates;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);

  VLOG(4) << "Builder construction  dense_index";
  paddle::dialect::IrTensor ir_tensor_index(paddle::dialect::TransToPhiDataType(index.dtype()),
                                                      index.dims(),
                                                      index.data_layout(),
                                                      index.lod(),
                                                      index.offset());
  VLOG(4) << "Builder construction  meta_index";
  paddle::dialect::IrMetaTensor meta_index(&ir_tensor_index);

  VLOG(4) << "Builder construction  dense_updates";
  paddle::dialect::IrTensor ir_tensor_updates(paddle::dialect::TransToPhiDataType(updates.dtype()),
                                                      updates.dims(),
                                                      updates.data_layout(),
                                                      updates.lod(),
                                                      updates.offset());
  VLOG(4) << "Builder construction  meta_updates";
  paddle::dialect::IrMetaTensor meta_updates(&ir_tensor_updates);
  paddle::dialect::IrTensor dense_out;
  paddle::dialect::IrMetaTensor meta_out(&dense_out);

  phi::ScatterInferMeta(meta_x, meta_index, meta_updates, overwrite, &meta_out);

  std::vector<pir::Type> argument_outputs;
  pir::Type out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_out.dtype()), dense_out.dims(), dense_out.layout(), dense_out.lod(), dense_out.offset());
  argument_outputs.push_back(out_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void Scatter_Op::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, pir::Value index_, pir::Value updates_, pir::AttributeMap attributes) {
  VLOG(4) << "Start build Scatter_Op";


  IR_ENFORCE(
      attributes.find("overwrite") != attributes.end(),
          "'overwrite' Attribute is expected for Scatter_Op. ");
  bool overwrite = attributes.at("overwrite").dyn_cast<pir::BoolAttribute>().data();


  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_, index_, updates_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_overwrite = pir::BoolAttribute::get(pir::IrContext::Instance(), overwrite);
  argument.AddAttribute("overwrite", attr_overwrite);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;
  paddle::dialect::DenseTensorType index = index_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)index;
  paddle::dialect::DenseTensorType updates = updates_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)updates;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);

  VLOG(4) << "Builder construction  dense_index";
  paddle::dialect::IrTensor ir_tensor_index(paddle::dialect::TransToPhiDataType(index.dtype()),
                                                      index.dims(),
                                                      index.data_layout(),
                                                      index.lod(),
                                                      index.offset());
  VLOG(4) << "Builder construction  meta_index";
  paddle::dialect::IrMetaTensor meta_index(&ir_tensor_index);

  VLOG(4) << "Builder construction  dense_updates";
  paddle::dialect::IrTensor ir_tensor_updates(paddle::dialect::TransToPhiDataType(updates.dtype()),
                                                      updates.dims(),
                                                      updates.data_layout(),
                                                      updates.lod(),
                                                      updates.offset());
  VLOG(4) << "Builder construction  meta_updates";
  paddle::dialect::IrMetaTensor meta_updates(&ir_tensor_updates);
  paddle::dialect::IrTensor dense_out;
  paddle::dialect::IrMetaTensor meta_out(&dense_out);

  phi::ScatterInferMeta(meta_x, meta_index, meta_updates, overwrite, &meta_out);

  std::vector<pir::Type> argument_outputs;
  pir::Type out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_out.dtype()), dense_out.dims(), dense_out.layout(), dense_out.lod(), dense_out.offset());
  argument_outputs.push_back(out_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void Scatter_Op::VerifySig() {
  VLOG(4) << "Start Verifying inputs, outputs and attributes for: Scatter_Op.";
  VLOG(4) << "Verifying inputs:";
  {
  auto input_size = num_operands();
  IR_ENFORCE(input_size == 3u,
                    "The size %d of inputs must be equal to 3.", input_size);
  IR_ENFORCE((*this)->operand_source(0).type().isa<paddle::dialect::DenseTensorType>(),
                  "Type validation failed for the 0th input, got %s.", (*this)->operand_source(0).type());
  IR_ENFORCE((*this)->operand_source(1).type().isa<paddle::dialect::DenseTensorType>(),
                  "Type validation failed for the 1th input, got %s.", (*this)->operand_source(1).type());
  IR_ENFORCE((*this)->operand_source(2).type().isa<paddle::dialect::DenseTensorType>(),
                  "Type validation failed for the 2th input, got %s.", (*this)->operand_source(2).type());
  }
  VLOG(4) << "Verifying attributes:";
  {
  auto& attributes = this->attributes();
  IR_ENFORCE(attributes.count("overwrite")>0,
                 "overwrite does not exist.");
  IR_ENFORCE(attributes.at("overwrite").isa<pir::BoolAttribute>(),
                 "Type of attribute: overwrite is not pir::BoolAttribute.");

  }
  VLOG(4) << "Verifying outputs:";
  {
  auto output_size = num_results();
  IR_ENFORCE(output_size == 1u,
                    "The size %d of outputs must be equal to 1.", output_size);
  IR_ENFORCE((*this)->result(0).type().isa<paddle::dialect::DenseTensorType>(),
                 "Type validation failed for the 0th output.");
  }
  VLOG(4) << "End Verifying for: Scatter_Op.";
}

void Scatter_Op::InferMeta( phi::InferMetaContext *infer_meta ) {
  auto fn = PD_INFER_META(phi::ScatterInferMeta);
  fn(infer_meta);
}

phi::DataType Scatter_Op::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: Scatter_Op";
  


  return expected_kernel_dtype;
}

OpInfoTuple ScatterNdAddOp::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("x", "paddle::dialect::DenseTensorType", false, false, false, true), paddle::dialect::OpInputInfo("index", "paddle::dialect::DenseTensorType", false, false, false, false), paddle::dialect::OpInputInfo("updates", "paddle::dialect::DenseTensorType", false, false, false, true) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = {  };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("out", "paddle::dialect::DenseTensorType", false, false) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("ScatterNdAddInferMeta", {"x", "index", "updates"}, "scatter_nd_add", {"x", "index", "updates"}, {"x"}, {}, {}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "scatter_nd_add");
}

void ScatterNdAddOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, pir::Value index_, pir::Value updates_) {
  VLOG(4) << "Start build ScatterNdAddOp";



  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_, index_, updates_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;
  paddle::dialect::DenseTensorType index = index_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)index;
  paddle::dialect::DenseTensorType updates = updates_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)updates;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);

  VLOG(4) << "Builder construction  dense_index";
  paddle::dialect::IrTensor ir_tensor_index(paddle::dialect::TransToPhiDataType(index.dtype()),
                                                      index.dims(),
                                                      index.data_layout(),
                                                      index.lod(),
                                                      index.offset());
  VLOG(4) << "Builder construction  meta_index";
  paddle::dialect::IrMetaTensor meta_index(&ir_tensor_index);

  VLOG(4) << "Builder construction  dense_updates";
  paddle::dialect::IrTensor ir_tensor_updates(paddle::dialect::TransToPhiDataType(updates.dtype()),
                                                      updates.dims(),
                                                      updates.data_layout(),
                                                      updates.lod(),
                                                      updates.offset());
  VLOG(4) << "Builder construction  meta_updates";
  paddle::dialect::IrMetaTensor meta_updates(&ir_tensor_updates);
  paddle::dialect::IrTensor dense_out;
  paddle::dialect::IrMetaTensor meta_out(&dense_out);

  phi::ScatterNdAddInferMeta(meta_x, meta_index, meta_updates, &meta_out);

  std::vector<pir::Type> argument_outputs;
  pir::Type out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_out.dtype()), dense_out.dims(), dense_out.layout(), dense_out.lod(), dense_out.offset());
  argument_outputs.push_back(out_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void ScatterNdAddOp::VerifySig() {
  VLOG(4) << "Start Verifying inputs, outputs and attributes for: ScatterNdAddOp.";
  VLOG(4) << "Verifying inputs:";
  {
  auto input_size = num_operands();
  IR_ENFORCE(input_size == 3u,
                    "The size %d of inputs must be equal to 3.", input_size);
  IR_ENFORCE((*this)->operand_source(0).type().isa<paddle::dialect::DenseTensorType>(),
                  "Type validation failed for the 0th input, got %s.", (*this)->operand_source(0).type());
  IR_ENFORCE((*this)->operand_source(1).type().isa<paddle::dialect::DenseTensorType>(),
                  "Type validation failed for the 1th input, got %s.", (*this)->operand_source(1).type());
  IR_ENFORCE((*this)->operand_source(2).type().isa<paddle::dialect::DenseTensorType>(),
                  "Type validation failed for the 2th input, got %s.", (*this)->operand_source(2).type());
  }
  VLOG(4) << "Verifying attributes:";
  {
  // Attributes num is 0, not need to check attributes type.
  }
  VLOG(4) << "Verifying outputs:";
  {
  auto output_size = num_results();
  IR_ENFORCE(output_size == 1u,
                    "The size %d of outputs must be equal to 1.", output_size);
  IR_ENFORCE((*this)->result(0).type().isa<paddle::dialect::DenseTensorType>(),
                 "Type validation failed for the 0th output.");
  }
  VLOG(4) << "End Verifying for: ScatterNdAddOp.";
}

void ScatterNdAddOp::InferMeta( phi::InferMetaContext *infer_meta ) {
  auto fn = PD_INFER_META(phi::ScatterNdAddInferMeta);
  fn(infer_meta);
}

phi::DataType ScatterNdAddOp::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: ScatterNdAddOp";
  


  return expected_kernel_dtype;
}

const char *SearchsortedOp::attributes_name[2] = { "out_int32", "right" };

OpInfoTuple SearchsortedOp::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("sorted_sequence", "paddle::dialect::DenseTensorType", false, false, false, false), paddle::dialect::OpInputInfo("values", "paddle::dialect::DenseTensorType", false, false, false, false) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = { paddle::dialect::OpAttributeInfo("out_int32", "pir::BoolAttribute", ""), paddle::dialect::OpAttributeInfo("right", "pir::BoolAttribute", "") };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("out", "paddle::dialect::DenseTensorType", false, false) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("SearchsortedInferMeta", {"sorted_sequence", "values", "out_int32", "right"}, "searchsorted", {"sorted_sequence", "values", "out_int32", "right"}, {"sorted_sequence"}, {}, {}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "searchsorted");
}

void SearchsortedOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value sorted_sequence_, pir::Value values_, bool out_int32, bool right) {
  VLOG(4) << "Start build SearchsortedOp";



  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {sorted_sequence_, values_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_out_int32 = pir::BoolAttribute::get(pir::IrContext::Instance(), out_int32);
  argument.AddAttribute("out_int32", attr_out_int32);
  pir::Attribute attr_right = pir::BoolAttribute::get(pir::IrContext::Instance(), right);
  argument.AddAttribute("right", attr_right);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType sorted_sequence = sorted_sequence_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)sorted_sequence;
  paddle::dialect::DenseTensorType values = values_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)values;

  VLOG(4) << "Builder construction  dense_sorted_sequence";
  paddle::dialect::IrTensor ir_tensor_sorted_sequence(paddle::dialect::TransToPhiDataType(sorted_sequence.dtype()),
                                                      sorted_sequence.dims(),
                                                      sorted_sequence.data_layout(),
                                                      sorted_sequence.lod(),
                                                      sorted_sequence.offset());
  VLOG(4) << "Builder construction  meta_sorted_sequence";
  paddle::dialect::IrMetaTensor meta_sorted_sequence(&ir_tensor_sorted_sequence);

  VLOG(4) << "Builder construction  dense_values";
  paddle::dialect::IrTensor ir_tensor_values(paddle::dialect::TransToPhiDataType(values.dtype()),
                                                      values.dims(),
                                                      values.data_layout(),
                                                      values.lod(),
                                                      values.offset());
  VLOG(4) << "Builder construction  meta_values";
  paddle::dialect::IrMetaTensor meta_values(&ir_tensor_values);
  paddle::dialect::IrTensor dense_out;
  paddle::dialect::IrMetaTensor meta_out(&dense_out);

  phi::SearchsortedInferMeta(meta_sorted_sequence, meta_values, out_int32, right, &meta_out);

  std::vector<pir::Type> argument_outputs;
  pir::Type out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_out.dtype()), dense_out.dims(), dense_out.layout(), dense_out.lod(), dense_out.offset());
  argument_outputs.push_back(out_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void SearchsortedOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value sorted_sequence_, pir::Value values_, pir::AttributeMap attributes) {
  VLOG(4) << "Start build SearchsortedOp";


  IR_ENFORCE(
      attributes.find("out_int32") != attributes.end(),
          "'out_int32' Attribute is expected for SearchsortedOp. ");
  bool out_int32 = attributes.at("out_int32").dyn_cast<pir::BoolAttribute>().data();

  IR_ENFORCE(
      attributes.find("right") != attributes.end(),
          "'right' Attribute is expected for SearchsortedOp. ");
  bool right = attributes.at("right").dyn_cast<pir::BoolAttribute>().data();


  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {sorted_sequence_, values_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_out_int32 = pir::BoolAttribute::get(pir::IrContext::Instance(), out_int32);
  argument.AddAttribute("out_int32", attr_out_int32);
  pir::Attribute attr_right = pir::BoolAttribute::get(pir::IrContext::Instance(), right);
  argument.AddAttribute("right", attr_right);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType sorted_sequence = sorted_sequence_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)sorted_sequence;
  paddle::dialect::DenseTensorType values = values_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)values;

  VLOG(4) << "Builder construction  dense_sorted_sequence";
  paddle::dialect::IrTensor ir_tensor_sorted_sequence(paddle::dialect::TransToPhiDataType(sorted_sequence.dtype()),
                                                      sorted_sequence.dims(),
                                                      sorted_sequence.data_layout(),
                                                      sorted_sequence.lod(),
                                                      sorted_sequence.offset());
  VLOG(4) << "Builder construction  meta_sorted_sequence";
  paddle::dialect::IrMetaTensor meta_sorted_sequence(&ir_tensor_sorted_sequence);

  VLOG(4) << "Builder construction  dense_values";
  paddle::dialect::IrTensor ir_tensor_values(paddle::dialect::TransToPhiDataType(values.dtype()),
                                                      values.dims(),
                                                      values.data_layout(),
                                                      values.lod(),
                                                      values.offset());
  VLOG(4) << "Builder construction  meta_values";
  paddle::dialect::IrMetaTensor meta_values(&ir_tensor_values);
  paddle::dialect::IrTensor dense_out;
  paddle::dialect::IrMetaTensor meta_out(&dense_out);

  phi::SearchsortedInferMeta(meta_sorted_sequence, meta_values, out_int32, right, &meta_out);

  std::vector<pir::Type> argument_outputs;
  pir::Type out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_out.dtype()), dense_out.dims(), dense_out.layout(), dense_out.lod(), dense_out.offset());
  argument_outputs.push_back(out_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void SearchsortedOp::VerifySig() {
  VLOG(4) << "Start Verifying inputs, outputs and attributes for: SearchsortedOp.";
  VLOG(4) << "Verifying inputs:";
  {
  auto input_size = num_operands();
  IR_ENFORCE(input_size == 2u,
                    "The size %d of inputs must be equal to 2.", input_size);
  IR_ENFORCE((*this)->operand_source(0).type().isa<paddle::dialect::DenseTensorType>(),
                  "Type validation failed for the 0th input, got %s.", (*this)->operand_source(0).type());
  IR_ENFORCE((*this)->operand_source(1).type().isa<paddle::dialect::DenseTensorType>(),
                  "Type validation failed for the 1th input, got %s.", (*this)->operand_source(1).type());
  }
  VLOG(4) << "Verifying attributes:";
  {
  auto& attributes = this->attributes();
  IR_ENFORCE(attributes.count("out_int32")>0,
                 "out_int32 does not exist.");
  IR_ENFORCE(attributes.at("out_int32").isa<pir::BoolAttribute>(),
                 "Type of attribute: out_int32 is not pir::BoolAttribute.");

  IR_ENFORCE(attributes.count("right")>0,
                 "right does not exist.");
  IR_ENFORCE(attributes.at("right").isa<pir::BoolAttribute>(),
                 "Type of attribute: right is not pir::BoolAttribute.");

  }
  VLOG(4) << "Verifying outputs:";
  {
  auto output_size = num_results();
  IR_ENFORCE(output_size == 1u,
                    "The size %d of outputs must be equal to 1.", output_size);
  IR_ENFORCE((*this)->result(0).type().isa<paddle::dialect::DenseTensorType>(),
                 "Type validation failed for the 0th output.");
  }
  VLOG(4) << "End Verifying for: SearchsortedOp.";
}

void SearchsortedOp::InferMeta( phi::InferMetaContext *infer_meta ) {
  auto fn = PD_INFER_META(phi::SearchsortedInferMeta);
  fn(infer_meta);
}

phi::DataType SearchsortedOp::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: SearchsortedOp";
  


  return expected_kernel_dtype;
}

const char *SegmentPoolOp::attributes_name[1] = { "pooltype" };

OpInfoTuple SegmentPoolOp::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("x", "paddle::dialect::DenseTensorType", false, false, false, true), paddle::dialect::OpInputInfo("segment_ids", "paddle::dialect::DenseTensorType", false, false, false, false) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = { paddle::dialect::OpAttributeInfo("pooltype", "pir::StrAttribute", "") };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("out", "paddle::dialect::DenseTensorType", false, false), paddle::dialect::OpOutputInfo("summed_ids", "paddle::dialect::DenseTensorType", true, true) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("SegmentPoolInferMeta", {"x", "segment_ids", "pooltype"}, "segment_pool", {"x", "segment_ids", "pooltype"}, {"x"}, {}, {}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "segment_pool");
}

void SegmentPoolOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, pir::Value segment_ids_, const std::string& pooltype) {
  VLOG(4) << "Start build SegmentPoolOp";



  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_, segment_ids_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_pooltype = pir::StrAttribute::get(pir::IrContext::Instance(), pooltype);
  argument.AddAttribute("pooltype", attr_pooltype);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;
  paddle::dialect::DenseTensorType segment_ids = segment_ids_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)segment_ids;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);

  VLOG(4) << "Builder construction  dense_segment_ids";
  paddle::dialect::IrTensor ir_tensor_segment_ids(paddle::dialect::TransToPhiDataType(segment_ids.dtype()),
                                                      segment_ids.dims(),
                                                      segment_ids.data_layout(),
                                                      segment_ids.lod(),
                                                      segment_ids.offset());
  VLOG(4) << "Builder construction  meta_segment_ids";
  paddle::dialect::IrMetaTensor meta_segment_ids(&ir_tensor_segment_ids);
  paddle::dialect::IrTensor dense_out;
  paddle::dialect::IrMetaTensor meta_out(&dense_out);
  paddle::dialect::IrTensor dense_summed_ids;
  paddle::dialect::IrMetaTensor meta_summed_ids(&dense_summed_ids);

  phi::SegmentPoolInferMeta(meta_x, meta_segment_ids, pooltype, &meta_out, &meta_summed_ids);

  std::vector<pir::Type> argument_outputs;
  pir::Type out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_out.dtype()), dense_out.dims(), dense_out.layout(), dense_out.lod(), dense_out.offset());
  argument_outputs.push_back(out_dense_tensor_type);

  pir::Type summed_ids_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_summed_ids.dtype()), dense_summed_ids.dims(), dense_summed_ids.layout(), dense_summed_ids.lod(), dense_summed_ids.offset());
  argument_outputs.push_back(summed_ids_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void SegmentPoolOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, pir::Value segment_ids_, pir::AttributeMap attributes) {
  VLOG(4) << "Start build SegmentPoolOp";


  IR_ENFORCE(
      attributes.find("pooltype") != attributes.end(),
          "'pooltype' Attribute is expected for SegmentPoolOp. ");
  std::string pooltype = attributes.at("pooltype").dyn_cast<pir::StrAttribute>().AsString();


  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_, segment_ids_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_pooltype = pir::StrAttribute::get(pir::IrContext::Instance(), pooltype);
  argument.AddAttribute("pooltype", attr_pooltype);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;
  paddle::dialect::DenseTensorType segment_ids = segment_ids_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)segment_ids;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);

  VLOG(4) << "Builder construction  dense_segment_ids";
  paddle::dialect::IrTensor ir_tensor_segment_ids(paddle::dialect::TransToPhiDataType(segment_ids.dtype()),
                                                      segment_ids.dims(),
                                                      segment_ids.data_layout(),
                                                      segment_ids.lod(),
                                                      segment_ids.offset());
  VLOG(4) << "Builder construction  meta_segment_ids";
  paddle::dialect::IrMetaTensor meta_segment_ids(&ir_tensor_segment_ids);
  paddle::dialect::IrTensor dense_out;
  paddle::dialect::IrMetaTensor meta_out(&dense_out);
  paddle::dialect::IrTensor dense_summed_ids;
  paddle::dialect::IrMetaTensor meta_summed_ids(&dense_summed_ids);

  phi::SegmentPoolInferMeta(meta_x, meta_segment_ids, pooltype, &meta_out, &meta_summed_ids);

  std::vector<pir::Type> argument_outputs;
  pir::Type out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_out.dtype()), dense_out.dims(), dense_out.layout(), dense_out.lod(), dense_out.offset());
  argument_outputs.push_back(out_dense_tensor_type);

  pir::Type summed_ids_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_summed_ids.dtype()), dense_summed_ids.dims(), dense_summed_ids.layout(), dense_summed_ids.lod(), dense_summed_ids.offset());
  argument_outputs.push_back(summed_ids_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void SegmentPoolOp::VerifySig() {
  VLOG(4) << "Start Verifying inputs, outputs and attributes for: SegmentPoolOp.";
  VLOG(4) << "Verifying inputs:";
  {
  auto input_size = num_operands();
  IR_ENFORCE(input_size == 2u,
                    "The size %d of inputs must be equal to 2.", input_size);
  IR_ENFORCE((*this)->operand_source(0).type().isa<paddle::dialect::DenseTensorType>(),
                  "Type validation failed for the 0th input, got %s.", (*this)->operand_source(0).type());
  IR_ENFORCE((*this)->operand_source(1).type().isa<paddle::dialect::DenseTensorType>(),
                  "Type validation failed for the 1th input, got %s.", (*this)->operand_source(1).type());
  }
  VLOG(4) << "Verifying attributes:";
  {
  auto& attributes = this->attributes();
  IR_ENFORCE(attributes.count("pooltype")>0,
                 "pooltype does not exist.");
  IR_ENFORCE(attributes.at("pooltype").isa<pir::StrAttribute>(),
                 "Type of attribute: pooltype is not pir::StrAttribute.");

  }
  VLOG(4) << "Verifying outputs:";
  {
  auto output_size = num_results();
  IR_ENFORCE(output_size == 2u,
                    "The size %d of outputs must be equal to 2.", output_size);
  IR_ENFORCE((*this)->result(0).type().isa<paddle::dialect::DenseTensorType>(),
                 "Type validation failed for the 0th output.");
  if (auto output_1_type = (*this)->result(1).type()) {
    IR_ENFORCE(output_1_type.isa<paddle::dialect::DenseTensorType>(),
                   "Type validation failed for the 1th output.");
  }
  }
  VLOG(4) << "End Verifying for: SegmentPoolOp.";
}

void SegmentPoolOp::InferMeta( phi::InferMetaContext *infer_meta ) {
  auto fn = PD_INFER_META(phi::SegmentPoolInferMeta);
  fn(infer_meta);
}

phi::DataType SegmentPoolOp::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: SegmentPoolOp";
  


  return expected_kernel_dtype;
}

const char *SeluOp::attributes_name[2] = { "scale", "alpha" };

OpInfoTuple SeluOp::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("x", "paddle::dialect::DenseTensorType", false, false, false, true) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = { paddle::dialect::OpAttributeInfo("scale", "pir::FloatAttribute", ""), paddle::dialect::OpAttributeInfo("alpha", "pir::FloatAttribute", "") };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("out", "paddle::dialect::DenseTensorType", false, false) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("UnchangedInferMeta", {"x"}, "selu", {"x", "scale", "alpha"}, {}, {}, {}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "selu");
}

void SeluOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, float scale, float alpha) {
  VLOG(4) << "Start build SeluOp";



  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_scale = pir::FloatAttribute::get(pir::IrContext::Instance(), scale);
  argument.AddAttribute("scale", attr_scale);
  pir::Attribute attr_alpha = pir::FloatAttribute::get(pir::IrContext::Instance(), alpha);
  argument.AddAttribute("alpha", attr_alpha);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);
  paddle::dialect::IrTensor dense_out;
  paddle::dialect::IrMetaTensor meta_out(&dense_out);

  phi::UnchangedInferMeta(meta_x, &meta_out);

  std::vector<pir::Type> argument_outputs;
  pir::Type out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_out.dtype()), dense_out.dims(), dense_out.layout(), dense_out.lod(), dense_out.offset());
  argument_outputs.push_back(out_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void SeluOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, pir::AttributeMap attributes) {
  VLOG(4) << "Start build SeluOp";


  IR_ENFORCE(
      attributes.find("scale") != attributes.end(),
          "'scale' Attribute is expected for SeluOp. ");
  float scale = attributes.at("scale").dyn_cast<pir::FloatAttribute>().data();

  IR_ENFORCE(
      attributes.find("alpha") != attributes.end(),
          "'alpha' Attribute is expected for SeluOp. ");
  float alpha = attributes.at("alpha").dyn_cast<pir::FloatAttribute>().data();


  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_scale = pir::FloatAttribute::get(pir::IrContext::Instance(), scale);
  argument.AddAttribute("scale", attr_scale);
  pir::Attribute attr_alpha = pir::FloatAttribute::get(pir::IrContext::Instance(), alpha);
  argument.AddAttribute("alpha", attr_alpha);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);
  paddle::dialect::IrTensor dense_out;
  paddle::dialect::IrMetaTensor meta_out(&dense_out);

  phi::UnchangedInferMeta(meta_x, &meta_out);

  std::vector<pir::Type> argument_outputs;
  pir::Type out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_out.dtype()), dense_out.dims(), dense_out.layout(), dense_out.lod(), dense_out.offset());
  argument_outputs.push_back(out_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void SeluOp::VerifySig() {
  VLOG(4) << "Start Verifying inputs, outputs and attributes for: SeluOp.";
  VLOG(4) << "Verifying inputs:";
  {
  auto input_size = num_operands();
  IR_ENFORCE(input_size == 1u,
                    "The size %d of inputs must be equal to 1.", input_size);
  IR_ENFORCE((*this)->operand_source(0).type().isa<paddle::dialect::DenseTensorType>(),
                  "Type validation failed for the 0th input, got %s.", (*this)->operand_source(0).type());
  }
  VLOG(4) << "Verifying attributes:";
  {
  auto& attributes = this->attributes();
  IR_ENFORCE(attributes.count("scale")>0,
                 "scale does not exist.");
  IR_ENFORCE(attributes.at("scale").isa<pir::FloatAttribute>(),
                 "Type of attribute: scale is not pir::FloatAttribute.");

  IR_ENFORCE(attributes.count("alpha")>0,
                 "alpha does not exist.");
  IR_ENFORCE(attributes.at("alpha").isa<pir::FloatAttribute>(),
                 "Type of attribute: alpha is not pir::FloatAttribute.");

  }
  VLOG(4) << "Verifying outputs:";
  {
  auto output_size = num_results();
  IR_ENFORCE(output_size == 1u,
                    "The size %d of outputs must be equal to 1.", output_size);
  IR_ENFORCE((*this)->result(0).type().isa<paddle::dialect::DenseTensorType>(),
                 "Type validation failed for the 0th output.");
  }
  VLOG(4) << "End Verifying for: SeluOp.";
}

void SeluOp::InferMeta( phi::InferMetaContext *infer_meta ) {
  auto fn = PD_INFER_META(phi::UnchangedInferMeta);
  fn(infer_meta);
}

phi::DataType SeluOp::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: SeluOp";
  


  return expected_kernel_dtype;
}

const char *SendURecvOp::attributes_name[1] = { "reduce_op" };

OpInfoTuple SendURecvOp::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("x", "paddle::dialect::DenseTensorType", false, false, false, true), paddle::dialect::OpInputInfo("src_index", "paddle::dialect::DenseTensorType", false, false, false, false), paddle::dialect::OpInputInfo("dst_index", "paddle::dialect::DenseTensorType", false, false, false, false), paddle::dialect::OpInputInfo("out_size", "paddle::dialect::IntArrayAttribute", false, false, true, false) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = { paddle::dialect::OpAttributeInfo("reduce_op", "pir::StrAttribute", "") };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("out", "paddle::dialect::DenseTensorType", false, false), paddle::dialect::OpOutputInfo("dst_count", "paddle::dialect::DenseTensorType", true, true) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("SendURecvInferMeta", {"x", "src_index", "dst_index", "reduce_op", "out_size"}, "send_u_recv", {"x", "src_index", "dst_index", "reduce_op", "out_size"}, {"x"}, {}, {}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "send_u_recv");
}

void SendURecvOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, pir::Value src_index_, pir::Value dst_index_, const std::string& reduce_op, const std::vector<int64_t>& out_size) {
  VLOG(4) << "Start build SendURecvOp";


  // Generate int_array mutable attribute: out_size
  paddle::dialect::FullIntArrayOp full_out_size_op = builder.Build<paddle::dialect::FullIntArrayOp>(out_size, phi::DataType::INT64, phi::CPUPlace());
  pir::OpResult out_size_ = full_out_size_op->result(0);
    
  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_, src_index_, dst_index_, out_size_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_reduce_op = pir::StrAttribute::get(pir::IrContext::Instance(), reduce_op);
  argument.AddAttribute("reduce_op", attr_reduce_op);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;
  paddle::dialect::DenseTensorType src_index = src_index_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)src_index;
  paddle::dialect::DenseTensorType dst_index = dst_index_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)dst_index;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);

  VLOG(4) << "Builder construction  dense_src_index";
  paddle::dialect::IrTensor ir_tensor_src_index(paddle::dialect::TransToPhiDataType(src_index.dtype()),
                                                      src_index.dims(),
                                                      src_index.data_layout(),
                                                      src_index.lod(),
                                                      src_index.offset());
  VLOG(4) << "Builder construction  meta_src_index";
  paddle::dialect::IrMetaTensor meta_src_index(&ir_tensor_src_index);

  VLOG(4) << "Builder construction  dense_dst_index";
  paddle::dialect::IrTensor ir_tensor_dst_index(paddle::dialect::TransToPhiDataType(dst_index.dtype()),
                                                      dst_index.dims(),
                                                      dst_index.data_layout(),
                                                      dst_index.lod(),
                                                      dst_index.offset());
  VLOG(4) << "Builder construction  meta_dst_index";
  paddle::dialect::IrMetaTensor meta_dst_index(&ir_tensor_dst_index);
  paddle::dialect::IrTensor dense_out;
  paddle::dialect::IrMetaTensor meta_out(&dense_out);
  paddle::dialect::IrTensor dense_dst_count;
  paddle::dialect::IrMetaTensor meta_dst_count(&dense_dst_count);

  phi::SendURecvInferMeta(meta_x, meta_src_index, meta_dst_index, reduce_op, out_size, &meta_out, &meta_dst_count);

  std::vector<pir::Type> argument_outputs;
  pir::Type out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_out.dtype()), dense_out.dims(), dense_out.layout(), dense_out.lod(), dense_out.offset());
  argument_outputs.push_back(out_dense_tensor_type);

  pir::Type dst_count_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_dst_count.dtype()), dense_dst_count.dims(), dense_dst_count.layout(), dense_dst_count.lod(), dense_dst_count.offset());
  argument_outputs.push_back(dst_count_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void SendURecvOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, pir::Value src_index_, pir::Value dst_index_, pir::AttributeMap attributes) {
  VLOG(4) << "Start build SendURecvOp";


  IR_ENFORCE(
      attributes.find("reduce_op") != attributes.end(),
          "'reduce_op' Attribute is expected for SendURecvOp. ");
  std::string reduce_op = attributes.at("reduce_op").dyn_cast<pir::StrAttribute>().AsString();

  IR_ENFORCE(
      attributes.find("out_size") != attributes.end(),
          "'out_size' Attribute is expected for SendURecvOp. ");
  std::vector<int64_t> out_size = attributes.at("out_size").dyn_cast<paddle::dialect::IntArrayAttribute>().data().GetData();

  // Generate int_array mutable attribute: out_size
  paddle::dialect::FullIntArrayOp full_out_size_op = builder.Build<paddle::dialect::FullIntArrayOp>(out_size, phi::DataType::INT64, phi::CPUPlace());
  pir::OpResult out_size_ = full_out_size_op->result(0);
    
  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_, src_index_, dst_index_, out_size_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_reduce_op = pir::StrAttribute::get(pir::IrContext::Instance(), reduce_op);
  argument.AddAttribute("reduce_op", attr_reduce_op);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;
  paddle::dialect::DenseTensorType src_index = src_index_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)src_index;
  paddle::dialect::DenseTensorType dst_index = dst_index_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)dst_index;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);

  VLOG(4) << "Builder construction  dense_src_index";
  paddle::dialect::IrTensor ir_tensor_src_index(paddle::dialect::TransToPhiDataType(src_index.dtype()),
                                                      src_index.dims(),
                                                      src_index.data_layout(),
                                                      src_index.lod(),
                                                      src_index.offset());
  VLOG(4) << "Builder construction  meta_src_index";
  paddle::dialect::IrMetaTensor meta_src_index(&ir_tensor_src_index);

  VLOG(4) << "Builder construction  dense_dst_index";
  paddle::dialect::IrTensor ir_tensor_dst_index(paddle::dialect::TransToPhiDataType(dst_index.dtype()),
                                                      dst_index.dims(),
                                                      dst_index.data_layout(),
                                                      dst_index.lod(),
                                                      dst_index.offset());
  VLOG(4) << "Builder construction  meta_dst_index";
  paddle::dialect::IrMetaTensor meta_dst_index(&ir_tensor_dst_index);
  paddle::dialect::IrTensor dense_out;
  paddle::dialect::IrMetaTensor meta_out(&dense_out);
  paddle::dialect::IrTensor dense_dst_count;
  paddle::dialect::IrMetaTensor meta_dst_count(&dense_dst_count);

  phi::SendURecvInferMeta(meta_x, meta_src_index, meta_dst_index, reduce_op, out_size, &meta_out, &meta_dst_count);

  std::vector<pir::Type> argument_outputs;
  pir::Type out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_out.dtype()), dense_out.dims(), dense_out.layout(), dense_out.lod(), dense_out.offset());
  argument_outputs.push_back(out_dense_tensor_type);

  pir::Type dst_count_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_dst_count.dtype()), dense_dst_count.dims(), dense_dst_count.layout(), dense_dst_count.lod(), dense_dst_count.offset());
  argument_outputs.push_back(dst_count_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void SendURecvOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, pir::Value src_index_, pir::Value dst_index_, pir::Value out_size_, const std::string& reduce_op) {
  VLOG(4) << "Start build SendURecvOp";



  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_, src_index_, dst_index_, out_size_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_reduce_op = pir::StrAttribute::get(pir::IrContext::Instance(), reduce_op);
  argument.AddAttribute("reduce_op", attr_reduce_op);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;
  paddle::dialect::DenseTensorType src_index = src_index_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)src_index;
  paddle::dialect::DenseTensorType dst_index = dst_index_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)dst_index;
  phi::IntArray out_size;
  if (out_size_.dyn_cast<pir::OpResult>() && out_size_.dyn_cast<pir::OpResult>().owner()->isa<paddle::dialect::FullIntArrayOp>()) {
    out_size = std::move(phi::IntArray(paddle::dialect::GetInt64Vector(
                          out_size_.dyn_cast<pir::OpResult>().owner()
                          ->dyn_cast<paddle::dialect::FullIntArrayOp>()
                          .attribute("value"))));
  } else if (out_size_.type().isa<pir::VectorType>()) {
    size_t out_size_size = out_size_.type().dyn_cast<pir::VectorType>().size();
    out_size = std::move(phi::IntArray(std::vector<int64_t>(out_size_size, -1)));
    out_size.SetFromTensor(true);
  } else if (out_size_.type().isa<paddle::dialect::DenseTensorType>()) {
    common::DDim out_size_dim = out_size_.type().dyn_cast<paddle::dialect::DenseTensorType>().dims();
    size_t out_size_size = common::product(out_size_dim);
    if (common::contain_unknown_dim(out_size_dim)) {
      out_size_size = 1;
    }
    out_size = std::move(phi::IntArray(std::vector<int64_t>(out_size_size, -1)));
    out_size.SetFromTensor(true);
  } else {
    PADDLE_THROW(phi::errors::Unimplemented("Only support VectorType or DenseTensorType"));
  }


  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);

  VLOG(4) << "Builder construction  dense_src_index";
  paddle::dialect::IrTensor ir_tensor_src_index(paddle::dialect::TransToPhiDataType(src_index.dtype()),
                                                      src_index.dims(),
                                                      src_index.data_layout(),
                                                      src_index.lod(),
                                                      src_index.offset());
  VLOG(4) << "Builder construction  meta_src_index";
  paddle::dialect::IrMetaTensor meta_src_index(&ir_tensor_src_index);

  VLOG(4) << "Builder construction  dense_dst_index";
  paddle::dialect::IrTensor ir_tensor_dst_index(paddle::dialect::TransToPhiDataType(dst_index.dtype()),
                                                      dst_index.dims(),
                                                      dst_index.data_layout(),
                                                      dst_index.lod(),
                                                      dst_index.offset());
  VLOG(4) << "Builder construction  meta_dst_index";
  paddle::dialect::IrMetaTensor meta_dst_index(&ir_tensor_dst_index);
  paddle::dialect::IrTensor dense_out;
  paddle::dialect::IrMetaTensor meta_out(&dense_out);
  paddle::dialect::IrTensor dense_dst_count;
  paddle::dialect::IrMetaTensor meta_dst_count(&dense_dst_count);

  phi::SendURecvInferMeta(meta_x, meta_src_index, meta_dst_index, reduce_op, out_size, &meta_out, &meta_dst_count);

  std::vector<pir::Type> argument_outputs;
  pir::Type out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_out.dtype()), dense_out.dims(), dense_out.layout(), dense_out.lod(), dense_out.offset());
  argument_outputs.push_back(out_dense_tensor_type);

  pir::Type dst_count_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_dst_count.dtype()), dense_dst_count.dims(), dense_dst_count.layout(), dense_dst_count.lod(), dense_dst_count.offset());
  argument_outputs.push_back(dst_count_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void SendURecvOp::VerifySig() {
  VLOG(4) << "Start Verifying inputs, outputs and attributes for: SendURecvOp.";
  VLOG(4) << "Verifying inputs:";
  {
  auto input_size = num_operands();
  IR_ENFORCE(input_size == 4u,
                    "The size %d of inputs must be equal to 4.", input_size);
  IR_ENFORCE((*this)->operand_source(0).type().isa<paddle::dialect::DenseTensorType>(),
                  "Type validation failed for the 0th input, got %s.", (*this)->operand_source(0).type());
  IR_ENFORCE((*this)->operand_source(1).type().isa<paddle::dialect::DenseTensorType>(),
                  "Type validation failed for the 1th input, got %s.", (*this)->operand_source(1).type());
  IR_ENFORCE((*this)->operand_source(2).type().isa<paddle::dialect::DenseTensorType>(),
                  "Type validation failed for the 2th input, got %s.", (*this)->operand_source(2).type());
  if (auto vec_type = (*this)->operand_source(3).type().dyn_cast<pir::VectorType>()) {
      for (size_t i = 0; i < vec_type.size(); ++i) {
        IR_ENFORCE(vec_type[i].isa<paddle::dialect::DenseTensorType>(),
                       "Type validation failed for the 3th input, got %s.", (*this)->operand_source(3).type());
      }
  }
  else {
    IR_ENFORCE((*this)->operand_source(3).type().isa<paddle::dialect::DenseTensorType>(),
                   "Type validation failed for the 3th input, got %s.", (*this)->operand_source(3).type());
  }
  }
  VLOG(4) << "Verifying attributes:";
  {
  auto& attributes = this->attributes();
  IR_ENFORCE(attributes.count("reduce_op")>0,
                 "reduce_op does not exist.");
  IR_ENFORCE(attributes.at("reduce_op").isa<pir::StrAttribute>(),
                 "Type of attribute: reduce_op is not pir::StrAttribute.");

  }
  VLOG(4) << "Verifying outputs:";
  {
  auto output_size = num_results();
  IR_ENFORCE(output_size == 2u,
                    "The size %d of outputs must be equal to 2.", output_size);
  IR_ENFORCE((*this)->result(0).type().isa<paddle::dialect::DenseTensorType>(),
                 "Type validation failed for the 0th output.");
  if (auto output_1_type = (*this)->result(1).type()) {
    IR_ENFORCE(output_1_type.isa<paddle::dialect::DenseTensorType>(),
                   "Type validation failed for the 1th output.");
  }
  }
  VLOG(4) << "End Verifying for: SendURecvOp.";
}

void SendURecvOp::InferMeta( phi::InferMetaContext *infer_meta ) {
  auto fn = PD_INFER_META(phi::SendURecvInferMeta);
  fn(infer_meta);
}

phi::DataType SendURecvOp::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: SendURecvOp";
  


  return expected_kernel_dtype;
}

const char *SendUeRecvOp::attributes_name[2] = { "message_op", "reduce_op" };

OpInfoTuple SendUeRecvOp::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("x", "paddle::dialect::DenseTensorType", false, false, false, true), paddle::dialect::OpInputInfo("y", "paddle::dialect::DenseTensorType", false, false, false, true), paddle::dialect::OpInputInfo("src_index", "paddle::dialect::DenseTensorType", false, false, false, false), paddle::dialect::OpInputInfo("dst_index", "paddle::dialect::DenseTensorType", false, false, false, false), paddle::dialect::OpInputInfo("out_size", "paddle::dialect::IntArrayAttribute", false, false, true, false) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = { paddle::dialect::OpAttributeInfo("message_op", "pir::StrAttribute", ""), paddle::dialect::OpAttributeInfo("reduce_op", "pir::StrAttribute", "") };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("out", "paddle::dialect::DenseTensorType", false, false), paddle::dialect::OpOutputInfo("dst_count", "paddle::dialect::DenseTensorType", true, true) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("SendUERecvInferMeta", {"x", "y", "src_index", "dst_index", "message_op", "reduce_op", "out_size"}, "send_ue_recv", {"x", "y", "src_index", "dst_index", "message_op", "reduce_op", "out_size"}, {"x"}, {}, {}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "send_ue_recv");
}

void SendUeRecvOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, pir::Value y_, pir::Value src_index_, pir::Value dst_index_, const std::string& message_op, const std::string& reduce_op, const std::vector<int64_t>& out_size) {
  VLOG(4) << "Start build SendUeRecvOp";


  // Generate int_array mutable attribute: out_size
  paddle::dialect::FullIntArrayOp full_out_size_op = builder.Build<paddle::dialect::FullIntArrayOp>(out_size, phi::DataType::INT64, phi::CPUPlace());
  pir::OpResult out_size_ = full_out_size_op->result(0);
    
  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_, y_, src_index_, dst_index_, out_size_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_message_op = pir::StrAttribute::get(pir::IrContext::Instance(), message_op);
  argument.AddAttribute("message_op", attr_message_op);
  pir::Attribute attr_reduce_op = pir::StrAttribute::get(pir::IrContext::Instance(), reduce_op);
  argument.AddAttribute("reduce_op", attr_reduce_op);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;
  paddle::dialect::DenseTensorType y = y_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)y;
  paddle::dialect::DenseTensorType src_index = src_index_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)src_index;
  paddle::dialect::DenseTensorType dst_index = dst_index_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)dst_index;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);

  VLOG(4) << "Builder construction  dense_y";
  paddle::dialect::IrTensor ir_tensor_y(paddle::dialect::TransToPhiDataType(y.dtype()),
                                                      y.dims(),
                                                      y.data_layout(),
                                                      y.lod(),
                                                      y.offset());
  VLOG(4) << "Builder construction  meta_y";
  paddle::dialect::IrMetaTensor meta_y(&ir_tensor_y);

  VLOG(4) << "Builder construction  dense_src_index";
  paddle::dialect::IrTensor ir_tensor_src_index(paddle::dialect::TransToPhiDataType(src_index.dtype()),
                                                      src_index.dims(),
                                                      src_index.data_layout(),
                                                      src_index.lod(),
                                                      src_index.offset());
  VLOG(4) << "Builder construction  meta_src_index";
  paddle::dialect::IrMetaTensor meta_src_index(&ir_tensor_src_index);

  VLOG(4) << "Builder construction  dense_dst_index";
  paddle::dialect::IrTensor ir_tensor_dst_index(paddle::dialect::TransToPhiDataType(dst_index.dtype()),
                                                      dst_index.dims(),
                                                      dst_index.data_layout(),
                                                      dst_index.lod(),
                                                      dst_index.offset());
  VLOG(4) << "Builder construction  meta_dst_index";
  paddle::dialect::IrMetaTensor meta_dst_index(&ir_tensor_dst_index);
  paddle::dialect::IrTensor dense_out;
  paddle::dialect::IrMetaTensor meta_out(&dense_out);
  paddle::dialect::IrTensor dense_dst_count;
  paddle::dialect::IrMetaTensor meta_dst_count(&dense_dst_count);

  phi::SendUERecvInferMeta(meta_x, meta_y, meta_src_index, meta_dst_index, message_op, reduce_op, out_size, &meta_out, &meta_dst_count);

  std::vector<pir::Type> argument_outputs;
  pir::Type out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_out.dtype()), dense_out.dims(), dense_out.layout(), dense_out.lod(), dense_out.offset());
  argument_outputs.push_back(out_dense_tensor_type);

  pir::Type dst_count_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_dst_count.dtype()), dense_dst_count.dims(), dense_dst_count.layout(), dense_dst_count.lod(), dense_dst_count.offset());
  argument_outputs.push_back(dst_count_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void SendUeRecvOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, pir::Value y_, pir::Value src_index_, pir::Value dst_index_, pir::AttributeMap attributes) {
  VLOG(4) << "Start build SendUeRecvOp";


  IR_ENFORCE(
      attributes.find("message_op") != attributes.end(),
          "'message_op' Attribute is expected for SendUeRecvOp. ");
  std::string message_op = attributes.at("message_op").dyn_cast<pir::StrAttribute>().AsString();

  IR_ENFORCE(
      attributes.find("reduce_op") != attributes.end(),
          "'reduce_op' Attribute is expected for SendUeRecvOp. ");
  std::string reduce_op = attributes.at("reduce_op").dyn_cast<pir::StrAttribute>().AsString();

  IR_ENFORCE(
      attributes.find("out_size") != attributes.end(),
          "'out_size' Attribute is expected for SendUeRecvOp. ");
  std::vector<int64_t> out_size = attributes.at("out_size").dyn_cast<paddle::dialect::IntArrayAttribute>().data().GetData();

  // Generate int_array mutable attribute: out_size
  paddle::dialect::FullIntArrayOp full_out_size_op = builder.Build<paddle::dialect::FullIntArrayOp>(out_size, phi::DataType::INT64, phi::CPUPlace());
  pir::OpResult out_size_ = full_out_size_op->result(0);
    
  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_, y_, src_index_, dst_index_, out_size_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_message_op = pir::StrAttribute::get(pir::IrContext::Instance(), message_op);
  argument.AddAttribute("message_op", attr_message_op);
  pir::Attribute attr_reduce_op = pir::StrAttribute::get(pir::IrContext::Instance(), reduce_op);
  argument.AddAttribute("reduce_op", attr_reduce_op);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;
  paddle::dialect::DenseTensorType y = y_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)y;
  paddle::dialect::DenseTensorType src_index = src_index_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)src_index;
  paddle::dialect::DenseTensorType dst_index = dst_index_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)dst_index;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);

  VLOG(4) << "Builder construction  dense_y";
  paddle::dialect::IrTensor ir_tensor_y(paddle::dialect::TransToPhiDataType(y.dtype()),
                                                      y.dims(),
                                                      y.data_layout(),
                                                      y.lod(),
                                                      y.offset());
  VLOG(4) << "Builder construction  meta_y";
  paddle::dialect::IrMetaTensor meta_y(&ir_tensor_y);

  VLOG(4) << "Builder construction  dense_src_index";
  paddle::dialect::IrTensor ir_tensor_src_index(paddle::dialect::TransToPhiDataType(src_index.dtype()),
                                                      src_index.dims(),
                                                      src_index.data_layout(),
                                                      src_index.lod(),
                                                      src_index.offset());
  VLOG(4) << "Builder construction  meta_src_index";
  paddle::dialect::IrMetaTensor meta_src_index(&ir_tensor_src_index);

  VLOG(4) << "Builder construction  dense_dst_index";
  paddle::dialect::IrTensor ir_tensor_dst_index(paddle::dialect::TransToPhiDataType(dst_index.dtype()),
                                                      dst_index.dims(),
                                                      dst_index.data_layout(),
                                                      dst_index.lod(),
                                                      dst_index.offset());
  VLOG(4) << "Builder construction  meta_dst_index";
  paddle::dialect::IrMetaTensor meta_dst_index(&ir_tensor_dst_index);
  paddle::dialect::IrTensor dense_out;
  paddle::dialect::IrMetaTensor meta_out(&dense_out);
  paddle::dialect::IrTensor dense_dst_count;
  paddle::dialect::IrMetaTensor meta_dst_count(&dense_dst_count);

  phi::SendUERecvInferMeta(meta_x, meta_y, meta_src_index, meta_dst_index, message_op, reduce_op, out_size, &meta_out, &meta_dst_count);

  std::vector<pir::Type> argument_outputs;
  pir::Type out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_out.dtype()), dense_out.dims(), dense_out.layout(), dense_out.lod(), dense_out.offset());
  argument_outputs.push_back(out_dense_tensor_type);

  pir::Type dst_count_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_dst_count.dtype()), dense_dst_count.dims(), dense_dst_count.layout(), dense_dst_count.lod(), dense_dst_count.offset());
  argument_outputs.push_back(dst_count_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void SendUeRecvOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, pir::Value y_, pir::Value src_index_, pir::Value dst_index_, pir::Value out_size_, const std::string& message_op, const std::string& reduce_op) {
  VLOG(4) << "Start build SendUeRecvOp";



  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_, y_, src_index_, dst_index_, out_size_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_message_op = pir::StrAttribute::get(pir::IrContext::Instance(), message_op);
  argument.AddAttribute("message_op", attr_message_op);
  pir::Attribute attr_reduce_op = pir::StrAttribute::get(pir::IrContext::Instance(), reduce_op);
  argument.AddAttribute("reduce_op", attr_reduce_op);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;
  paddle::dialect::DenseTensorType y = y_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)y;
  paddle::dialect::DenseTensorType src_index = src_index_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)src_index;
  paddle::dialect::DenseTensorType dst_index = dst_index_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)dst_index;
  phi::IntArray out_size;
  if (out_size_.dyn_cast<pir::OpResult>() && out_size_.dyn_cast<pir::OpResult>().owner()->isa<paddle::dialect::FullIntArrayOp>()) {
    out_size = std::move(phi::IntArray(paddle::dialect::GetInt64Vector(
                          out_size_.dyn_cast<pir::OpResult>().owner()
                          ->dyn_cast<paddle::dialect::FullIntArrayOp>()
                          .attribute("value"))));
  } else if (out_size_.type().isa<pir::VectorType>()) {
    size_t out_size_size = out_size_.type().dyn_cast<pir::VectorType>().size();
    out_size = std::move(phi::IntArray(std::vector<int64_t>(out_size_size, -1)));
    out_size.SetFromTensor(true);
  } else if (out_size_.type().isa<paddle::dialect::DenseTensorType>()) {
    common::DDim out_size_dim = out_size_.type().dyn_cast<paddle::dialect::DenseTensorType>().dims();
    size_t out_size_size = common::product(out_size_dim);
    if (common::contain_unknown_dim(out_size_dim)) {
      out_size_size = 1;
    }
    out_size = std::move(phi::IntArray(std::vector<int64_t>(out_size_size, -1)));
    out_size.SetFromTensor(true);
  } else {
    PADDLE_THROW(phi::errors::Unimplemented("Only support VectorType or DenseTensorType"));
  }


  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);

  VLOG(4) << "Builder construction  dense_y";
  paddle::dialect::IrTensor ir_tensor_y(paddle::dialect::TransToPhiDataType(y.dtype()),
                                                      y.dims(),
                                                      y.data_layout(),
                                                      y.lod(),
                                                      y.offset());
  VLOG(4) << "Builder construction  meta_y";
  paddle::dialect::IrMetaTensor meta_y(&ir_tensor_y);

  VLOG(4) << "Builder construction  dense_src_index";
  paddle::dialect::IrTensor ir_tensor_src_index(paddle::dialect::TransToPhiDataType(src_index.dtype()),
                                                      src_index.dims(),
                                                      src_index.data_layout(),
                                                      src_index.lod(),
                                                      src_index.offset());
  VLOG(4) << "Builder construction  meta_src_index";
  paddle::dialect::IrMetaTensor meta_src_index(&ir_tensor_src_index);

  VLOG(4) << "Builder construction  dense_dst_index";
  paddle::dialect::IrTensor ir_tensor_dst_index(paddle::dialect::TransToPhiDataType(dst_index.dtype()),
                                                      dst_index.dims(),
                                                      dst_index.data_layout(),
                                                      dst_index.lod(),
                                                      dst_index.offset());
  VLOG(4) << "Builder construction  meta_dst_index";
  paddle::dialect::IrMetaTensor meta_dst_index(&ir_tensor_dst_index);
  paddle::dialect::IrTensor dense_out;
  paddle::dialect::IrMetaTensor meta_out(&dense_out);
  paddle::dialect::IrTensor dense_dst_count;
  paddle::dialect::IrMetaTensor meta_dst_count(&dense_dst_count);

  phi::SendUERecvInferMeta(meta_x, meta_y, meta_src_index, meta_dst_index, message_op, reduce_op, out_size, &meta_out, &meta_dst_count);

  std::vector<pir::Type> argument_outputs;
  pir::Type out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_out.dtype()), dense_out.dims(), dense_out.layout(), dense_out.lod(), dense_out.offset());
  argument_outputs.push_back(out_dense_tensor_type);

  pir::Type dst_count_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_dst_count.dtype()), dense_dst_count.dims(), dense_dst_count.layout(), dense_dst_count.lod(), dense_dst_count.offset());
  argument_outputs.push_back(dst_count_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void SendUeRecvOp::VerifySig() {
  VLOG(4) << "Start Verifying inputs, outputs and attributes for: SendUeRecvOp.";
  VLOG(4) << "Verifying inputs:";
  {
  auto input_size = num_operands();
  IR_ENFORCE(input_size == 5u,
                    "The size %d of inputs must be equal to 5.", input_size);
  IR_ENFORCE((*this)->operand_source(0).type().isa<paddle::dialect::DenseTensorType>(),
                  "Type validation failed for the 0th input, got %s.", (*this)->operand_source(0).type());
  IR_ENFORCE((*this)->operand_source(1).type().isa<paddle::dialect::DenseTensorType>(),
                  "Type validation failed for the 1th input, got %s.", (*this)->operand_source(1).type());
  IR_ENFORCE((*this)->operand_source(2).type().isa<paddle::dialect::DenseTensorType>(),
                  "Type validation failed for the 2th input, got %s.", (*this)->operand_source(2).type());
  IR_ENFORCE((*this)->operand_source(3).type().isa<paddle::dialect::DenseTensorType>(),
                  "Type validation failed for the 3th input, got %s.", (*this)->operand_source(3).type());
  if (auto vec_type = (*this)->operand_source(4).type().dyn_cast<pir::VectorType>()) {
      for (size_t i = 0; i < vec_type.size(); ++i) {
        IR_ENFORCE(vec_type[i].isa<paddle::dialect::DenseTensorType>(),
                       "Type validation failed for the 4th input, got %s.", (*this)->operand_source(4).type());
      }
  }
  else {
    IR_ENFORCE((*this)->operand_source(4).type().isa<paddle::dialect::DenseTensorType>(),
                   "Type validation failed for the 4th input, got %s.", (*this)->operand_source(4).type());
  }
  }
  VLOG(4) << "Verifying attributes:";
  {
  auto& attributes = this->attributes();
  IR_ENFORCE(attributes.count("message_op")>0,
                 "message_op does not exist.");
  IR_ENFORCE(attributes.at("message_op").isa<pir::StrAttribute>(),
                 "Type of attribute: message_op is not pir::StrAttribute.");

  IR_ENFORCE(attributes.count("reduce_op")>0,
                 "reduce_op does not exist.");
  IR_ENFORCE(attributes.at("reduce_op").isa<pir::StrAttribute>(),
                 "Type of attribute: reduce_op is not pir::StrAttribute.");

  }
  VLOG(4) << "Verifying outputs:";
  {
  auto output_size = num_results();
  IR_ENFORCE(output_size == 2u,
                    "The size %d of outputs must be equal to 2.", output_size);
  IR_ENFORCE((*this)->result(0).type().isa<paddle::dialect::DenseTensorType>(),
                 "Type validation failed for the 0th output.");
  if (auto output_1_type = (*this)->result(1).type()) {
    IR_ENFORCE(output_1_type.isa<paddle::dialect::DenseTensorType>(),
                   "Type validation failed for the 1th output.");
  }
  }
  VLOG(4) << "End Verifying for: SendUeRecvOp.";
}

void SendUeRecvOp::InferMeta( phi::InferMetaContext *infer_meta ) {
  auto fn = PD_INFER_META(phi::SendUERecvInferMeta);
  fn(infer_meta);
}

phi::DataType SendUeRecvOp::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: SendUeRecvOp";
  


  return expected_kernel_dtype;
}

const char *SendUvOp::attributes_name[1] = { "message_op" };

OpInfoTuple SendUvOp::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("x", "paddle::dialect::DenseTensorType", false, false, false, true), paddle::dialect::OpInputInfo("y", "paddle::dialect::DenseTensorType", false, false, false, true), paddle::dialect::OpInputInfo("src_index", "paddle::dialect::DenseTensorType", false, false, false, false), paddle::dialect::OpInputInfo("dst_index", "paddle::dialect::DenseTensorType", false, false, false, false) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = { paddle::dialect::OpAttributeInfo("message_op", "pir::StrAttribute", "") };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("out", "paddle::dialect::DenseTensorType", false, false) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("SendUVInferMeta", {"x", "y", "src_index", "dst_index", "message_op"}, "send_uv", {"x", "y", "src_index", "dst_index", "message_op"}, {"x"}, {}, {}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "send_uv");
}

void SendUvOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, pir::Value y_, pir::Value src_index_, pir::Value dst_index_, const std::string& message_op) {
  VLOG(4) << "Start build SendUvOp";



  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_, y_, src_index_, dst_index_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_message_op = pir::StrAttribute::get(pir::IrContext::Instance(), message_op);
  argument.AddAttribute("message_op", attr_message_op);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;
  paddle::dialect::DenseTensorType y = y_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)y;
  paddle::dialect::DenseTensorType src_index = src_index_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)src_index;
  paddle::dialect::DenseTensorType dst_index = dst_index_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)dst_index;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);

  VLOG(4) << "Builder construction  dense_y";
  paddle::dialect::IrTensor ir_tensor_y(paddle::dialect::TransToPhiDataType(y.dtype()),
                                                      y.dims(),
                                                      y.data_layout(),
                                                      y.lod(),
                                                      y.offset());
  VLOG(4) << "Builder construction  meta_y";
  paddle::dialect::IrMetaTensor meta_y(&ir_tensor_y);

  VLOG(4) << "Builder construction  dense_src_index";
  paddle::dialect::IrTensor ir_tensor_src_index(paddle::dialect::TransToPhiDataType(src_index.dtype()),
                                                      src_index.dims(),
                                                      src_index.data_layout(),
                                                      src_index.lod(),
                                                      src_index.offset());
  VLOG(4) << "Builder construction  meta_src_index";
  paddle::dialect::IrMetaTensor meta_src_index(&ir_tensor_src_index);

  VLOG(4) << "Builder construction  dense_dst_index";
  paddle::dialect::IrTensor ir_tensor_dst_index(paddle::dialect::TransToPhiDataType(dst_index.dtype()),
                                                      dst_index.dims(),
                                                      dst_index.data_layout(),
                                                      dst_index.lod(),
                                                      dst_index.offset());
  VLOG(4) << "Builder construction  meta_dst_index";
  paddle::dialect::IrMetaTensor meta_dst_index(&ir_tensor_dst_index);
  paddle::dialect::IrTensor dense_out;
  paddle::dialect::IrMetaTensor meta_out(&dense_out);

  phi::SendUVInferMeta(meta_x, meta_y, meta_src_index, meta_dst_index, message_op, &meta_out);

  std::vector<pir::Type> argument_outputs;
  pir::Type out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_out.dtype()), dense_out.dims(), dense_out.layout(), dense_out.lod(), dense_out.offset());
  argument_outputs.push_back(out_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void SendUvOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, pir::Value y_, pir::Value src_index_, pir::Value dst_index_, pir::AttributeMap attributes) {
  VLOG(4) << "Start build SendUvOp";


  IR_ENFORCE(
      attributes.find("message_op") != attributes.end(),
          "'message_op' Attribute is expected for SendUvOp. ");
  std::string message_op = attributes.at("message_op").dyn_cast<pir::StrAttribute>().AsString();


  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_, y_, src_index_, dst_index_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_message_op = pir::StrAttribute::get(pir::IrContext::Instance(), message_op);
  argument.AddAttribute("message_op", attr_message_op);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;
  paddle::dialect::DenseTensorType y = y_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)y;
  paddle::dialect::DenseTensorType src_index = src_index_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)src_index;
  paddle::dialect::DenseTensorType dst_index = dst_index_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)dst_index;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);

  VLOG(4) << "Builder construction  dense_y";
  paddle::dialect::IrTensor ir_tensor_y(paddle::dialect::TransToPhiDataType(y.dtype()),
                                                      y.dims(),
                                                      y.data_layout(),
                                                      y.lod(),
                                                      y.offset());
  VLOG(4) << "Builder construction  meta_y";
  paddle::dialect::IrMetaTensor meta_y(&ir_tensor_y);

  VLOG(4) << "Builder construction  dense_src_index";
  paddle::dialect::IrTensor ir_tensor_src_index(paddle::dialect::TransToPhiDataType(src_index.dtype()),
                                                      src_index.dims(),
                                                      src_index.data_layout(),
                                                      src_index.lod(),
                                                      src_index.offset());
  VLOG(4) << "Builder construction  meta_src_index";
  paddle::dialect::IrMetaTensor meta_src_index(&ir_tensor_src_index);

  VLOG(4) << "Builder construction  dense_dst_index";
  paddle::dialect::IrTensor ir_tensor_dst_index(paddle::dialect::TransToPhiDataType(dst_index.dtype()),
                                                      dst_index.dims(),
                                                      dst_index.data_layout(),
                                                      dst_index.lod(),
                                                      dst_index.offset());
  VLOG(4) << "Builder construction  meta_dst_index";
  paddle::dialect::IrMetaTensor meta_dst_index(&ir_tensor_dst_index);
  paddle::dialect::IrTensor dense_out;
  paddle::dialect::IrMetaTensor meta_out(&dense_out);

  phi::SendUVInferMeta(meta_x, meta_y, meta_src_index, meta_dst_index, message_op, &meta_out);

  std::vector<pir::Type> argument_outputs;
  pir::Type out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_out.dtype()), dense_out.dims(), dense_out.layout(), dense_out.lod(), dense_out.offset());
  argument_outputs.push_back(out_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void SendUvOp::VerifySig() {
  VLOG(4) << "Start Verifying inputs, outputs and attributes for: SendUvOp.";
  VLOG(4) << "Verifying inputs:";
  {
  auto input_size = num_operands();
  IR_ENFORCE(input_size == 4u,
                    "The size %d of inputs must be equal to 4.", input_size);
  IR_ENFORCE((*this)->operand_source(0).type().isa<paddle::dialect::DenseTensorType>(),
                  "Type validation failed for the 0th input, got %s.", (*this)->operand_source(0).type());
  IR_ENFORCE((*this)->operand_source(1).type().isa<paddle::dialect::DenseTensorType>(),
                  "Type validation failed for the 1th input, got %s.", (*this)->operand_source(1).type());
  IR_ENFORCE((*this)->operand_source(2).type().isa<paddle::dialect::DenseTensorType>(),
                  "Type validation failed for the 2th input, got %s.", (*this)->operand_source(2).type());
  IR_ENFORCE((*this)->operand_source(3).type().isa<paddle::dialect::DenseTensorType>(),
                  "Type validation failed for the 3th input, got %s.", (*this)->operand_source(3).type());
  }
  VLOG(4) << "Verifying attributes:";
  {
  auto& attributes = this->attributes();
  IR_ENFORCE(attributes.count("message_op")>0,
                 "message_op does not exist.");
  IR_ENFORCE(attributes.at("message_op").isa<pir::StrAttribute>(),
                 "Type of attribute: message_op is not pir::StrAttribute.");

  }
  VLOG(4) << "Verifying outputs:";
  {
  auto output_size = num_results();
  IR_ENFORCE(output_size == 1u,
                    "The size %d of outputs must be equal to 1.", output_size);
  IR_ENFORCE((*this)->result(0).type().isa<paddle::dialect::DenseTensorType>(),
                 "Type validation failed for the 0th output.");
  }
  VLOG(4) << "End Verifying for: SendUvOp.";
}

void SendUvOp::InferMeta( phi::InferMetaContext *infer_meta ) {
  auto fn = PD_INFER_META(phi::SendUVInferMeta);
  fn(infer_meta);
}

phi::DataType SendUvOp::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: SendUvOp";
  


  return expected_kernel_dtype;
}

const char *Sgd_Op::attributes_name[1] = { "multi_precision" };

OpInfoTuple Sgd_Op::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("param", "paddle::dialect::DenseTensorType", false, false, false, false), paddle::dialect::OpInputInfo("learning_rate", "paddle::dialect::DenseTensorType", false, false, false, false), paddle::dialect::OpInputInfo("grad", "paddle::dialect::DenseTensorType", false, false, false, false), paddle::dialect::OpInputInfo("master_param", "paddle::dialect::DenseTensorType", true, false, false, false) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = { paddle::dialect::OpAttributeInfo("multi_precision", "pir::BoolAttribute", "") };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("param_out", "paddle::dialect::DenseTensorType", false, false), paddle::dialect::OpOutputInfo("master_param_out", "paddle::dialect::DenseTensorType", true, false) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("SgdInferMeta", {"param", "learning_rate", "grad", "master_param", "multi_precision"}, "sgd", {"param", "learning_rate", "grad", "master_param", "multi_precision"}, {"param"}, {}, {{"param_out", "param"},{"master_param_out", "master_param"}}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "sgd_");
}

void Sgd_Op::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value param_, pir::Value learning_rate_, pir::Value grad_, pir::Value master_param_, bool multi_precision) {
  VLOG(4) << "Start build Sgd_Op";



  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {param_, learning_rate_, grad_, master_param_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_multi_precision = pir::BoolAttribute::get(pir::IrContext::Instance(), multi_precision);
  argument.AddAttribute("multi_precision", attr_multi_precision);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType param = param_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)param;
  paddle::dialect::DenseTensorType learning_rate = learning_rate_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)learning_rate;
  paddle::dialect::DenseTensorType grad = grad_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)grad;

  VLOG(4) << "Builder construction  dense_param";
  paddle::dialect::IrTensor ir_tensor_param(paddle::dialect::TransToPhiDataType(param.dtype()),
                                                      param.dims(),
                                                      param.data_layout(),
                                                      param.lod(),
                                                      param.offset());
  VLOG(4) << "Builder construction  meta_param";
  paddle::dialect::IrMetaTensor meta_param(&ir_tensor_param);

  VLOG(4) << "Builder construction  dense_learning_rate";
  paddle::dialect::IrTensor ir_tensor_learning_rate(paddle::dialect::TransToPhiDataType(learning_rate.dtype()),
                                                      learning_rate.dims(),
                                                      learning_rate.data_layout(),
                                                      learning_rate.lod(),
                                                      learning_rate.offset());
  VLOG(4) << "Builder construction  meta_learning_rate";
  paddle::dialect::IrMetaTensor meta_learning_rate(&ir_tensor_learning_rate);

  VLOG(4) << "Builder construction  dense_grad";
  paddle::dialect::IrTensor ir_tensor_grad(paddle::dialect::TransToPhiDataType(grad.dtype()),
                                                      grad.dims(),
                                                      grad.data_layout(),
                                                      grad.lod(),
                                                      grad.offset());
  VLOG(4) << "Builder construction  meta_grad";
  paddle::dialect::IrMetaTensor meta_grad(&ir_tensor_grad);

  paddle::dialect::IrMetaTensor meta_master_param;
  paddle::dialect::IrTensor ir_tensor_master_param;
  if (master_param_.impl() != nullptr) {
    paddle::dialect::DenseTensorType master_param = master_param_.type().dyn_cast<paddle::dialect::DenseTensorType>();
    VLOG(4) << "Builder construction  dense_master_param";
    ir_tensor_master_param = paddle::dialect::IrTensor(paddle::dialect::TransToPhiDataType(master_param.dtype()),
                                                        master_param.dims(),
                                                        master_param.data_layout(),
                                                        master_param.lod(),
                                                        master_param.offset());
    VLOG(4) << "Builder construction  meta_master_param";
    meta_master_param = paddle::dialect::IrMetaTensor(&ir_tensor_master_param);
  }

  paddle::dialect::IrTensor dense_param_out;
  paddle::dialect::IrMetaTensor meta_param_out(&dense_param_out);
  paddle::dialect::IrTensor dense_master_param_out;
  paddle::dialect::IrMetaTensor meta_master_param_out(&dense_master_param_out);

  phi::SgdInferMeta(meta_param, meta_learning_rate, meta_grad, meta_master_param, multi_precision, &meta_param_out, &meta_master_param_out);

  std::vector<pir::Type> argument_outputs;
  pir::Type param_out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_param_out.dtype()), dense_param_out.dims(), dense_param_out.layout(), dense_param_out.lod(), dense_param_out.offset());
  argument_outputs.push_back(param_out_dense_tensor_type);

  if (master_param_.impl() != nullptr) {
    pir::Type master_param_out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_master_param_out.dtype()), dense_master_param_out.dims(), dense_master_param_out.layout(), dense_master_param_out.lod(), dense_master_param_out.offset());
    argument_outputs.push_back(master_param_out_dense_tensor_type);
  } else {
    pir::Type master_param_out_type;
    argument_outputs.push_back(master_param_out_type);
  }

  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void Sgd_Op::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value param_, pir::Value learning_rate_, pir::Value grad_, pir::Value master_param_, pir::AttributeMap attributes) {
  VLOG(4) << "Start build Sgd_Op";


  IR_ENFORCE(
      attributes.find("multi_precision") != attributes.end(),
          "'multi_precision' Attribute is expected for Sgd_Op. ");
  bool multi_precision = attributes.at("multi_precision").dyn_cast<pir::BoolAttribute>().data();


  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {param_, learning_rate_, grad_, master_param_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_multi_precision = pir::BoolAttribute::get(pir::IrContext::Instance(), multi_precision);
  argument.AddAttribute("multi_precision", attr_multi_precision);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType param = param_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)param;
  paddle::dialect::DenseTensorType learning_rate = learning_rate_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)learning_rate;
  paddle::dialect::DenseTensorType grad = grad_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)grad;

  VLOG(4) << "Builder construction  dense_param";
  paddle::dialect::IrTensor ir_tensor_param(paddle::dialect::TransToPhiDataType(param.dtype()),
                                                      param.dims(),
                                                      param.data_layout(),
                                                      param.lod(),
                                                      param.offset());
  VLOG(4) << "Builder construction  meta_param";
  paddle::dialect::IrMetaTensor meta_param(&ir_tensor_param);

  VLOG(4) << "Builder construction  dense_learning_rate";
  paddle::dialect::IrTensor ir_tensor_learning_rate(paddle::dialect::TransToPhiDataType(learning_rate.dtype()),
                                                      learning_rate.dims(),
                                                      learning_rate.data_layout(),
                                                      learning_rate.lod(),
                                                      learning_rate.offset());
  VLOG(4) << "Builder construction  meta_learning_rate";
  paddle::dialect::IrMetaTensor meta_learning_rate(&ir_tensor_learning_rate);

  VLOG(4) << "Builder construction  dense_grad";
  paddle::dialect::IrTensor ir_tensor_grad(paddle::dialect::TransToPhiDataType(grad.dtype()),
                                                      grad.dims(),
                                                      grad.data_layout(),
                                                      grad.lod(),
                                                      grad.offset());
  VLOG(4) << "Builder construction  meta_grad";
  paddle::dialect::IrMetaTensor meta_grad(&ir_tensor_grad);

  paddle::dialect::IrMetaTensor meta_master_param;
  paddle::dialect::IrTensor ir_tensor_master_param;
  if (master_param_.impl() != nullptr) {
    paddle::dialect::DenseTensorType master_param = master_param_.type().dyn_cast<paddle::dialect::DenseTensorType>();
    VLOG(4) << "Builder construction  dense_master_param";
    ir_tensor_master_param = paddle::dialect::IrTensor(paddle::dialect::TransToPhiDataType(master_param.dtype()),
                                                        master_param.dims(),
                                                        master_param.data_layout(),
                                                        master_param.lod(),
                                                        master_param.offset());
    VLOG(4) << "Builder construction  meta_master_param";
    meta_master_param = paddle::dialect::IrMetaTensor(&ir_tensor_master_param);
  }

  paddle::dialect::IrTensor dense_param_out;
  paddle::dialect::IrMetaTensor meta_param_out(&dense_param_out);
  paddle::dialect::IrTensor dense_master_param_out;
  paddle::dialect::IrMetaTensor meta_master_param_out(&dense_master_param_out);

  phi::SgdInferMeta(meta_param, meta_learning_rate, meta_grad, meta_master_param, multi_precision, &meta_param_out, &meta_master_param_out);

  std::vector<pir::Type> argument_outputs;
  pir::Type param_out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_param_out.dtype()), dense_param_out.dims(), dense_param_out.layout(), dense_param_out.lod(), dense_param_out.offset());
  argument_outputs.push_back(param_out_dense_tensor_type);

  if (master_param_.impl() != nullptr) {
    pir::Type master_param_out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_master_param_out.dtype()), dense_master_param_out.dims(), dense_master_param_out.layout(), dense_master_param_out.lod(), dense_master_param_out.offset());
    argument_outputs.push_back(master_param_out_dense_tensor_type);
  } else {
    pir::Type master_param_out_type;
    argument_outputs.push_back(master_param_out_type);
  }

  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void Sgd_Op::VerifySig() {
  VLOG(4) << "Start Verifying inputs, outputs and attributes for: Sgd_Op.";
  VLOG(4) << "Verifying inputs:";
  {
  auto input_size = num_operands();
  IR_ENFORCE(input_size == 4u,
                    "The size %d of inputs must be equal to 4.", input_size);
  IR_ENFORCE((*this)->operand_source(0).type().isa<paddle::dialect::DenseTensorType>(),
                  "Type validation failed for the 0th input, got %s.", (*this)->operand_source(0).type());
  IR_ENFORCE((*this)->operand_source(1).type().isa<paddle::dialect::DenseTensorType>(),
                  "Type validation failed for the 1th input, got %s.", (*this)->operand_source(1).type());
  IR_ENFORCE((*this)->operand_source(2).type().isa<paddle::dialect::DenseTensorType>(),
                  "Type validation failed for the 2th input, got %s.", (*this)->operand_source(2).type());
  if (auto val = (*this)->operand(3)) {
    IR_ENFORCE(val.type().isa<paddle::dialect::DenseTensorType>(),
                   "Type validation failed for the 3th input, got %s.", (*this)->operand_source(3).type());
  }
  }
  VLOG(4) << "Verifying attributes:";
  {
  auto& attributes = this->attributes();
  IR_ENFORCE(attributes.count("multi_precision")>0,
                 "multi_precision does not exist.");
  IR_ENFORCE(attributes.at("multi_precision").isa<pir::BoolAttribute>(),
                 "Type of attribute: multi_precision is not pir::BoolAttribute.");

  }
  VLOG(4) << "Verifying outputs:";
  {
  auto output_size = num_results();
  IR_ENFORCE(output_size == 2u,
                    "The size %d of outputs must be equal to 2.", output_size);
  IR_ENFORCE((*this)->result(0).type().isa<paddle::dialect::DenseTensorType>(),
                 "Type validation failed for the 0th output.");
  if (auto output_1_type = (*this)->result(1).type()) {
    IR_ENFORCE(output_1_type.isa<paddle::dialect::DenseTensorType>(),
                   "Type validation failed for the 1th output.");
  }
  }
  VLOG(4) << "End Verifying for: Sgd_Op.";
}

void Sgd_Op::InferMeta( phi::InferMetaContext *infer_meta ) {
  auto fn = PD_INFER_META(phi::SgdInferMeta);
  fn(infer_meta);
}

phi::DataType Sgd_Op::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: Sgd_Op";
  

  // deal support data transform
  VLOG(8) << "SUPPORT_TRANSFORM: " << "['learning_rate'];";
  return tensor_dtype;


  return expected_kernel_dtype;
}

const char *SgdDenseParamSparseGrad_Op::attributes_name[1] = { "multi_precision" };

OpInfoTuple SgdDenseParamSparseGrad_Op::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("param", "paddle::dialect::DenseTensorType", false, false, false, false), paddle::dialect::OpInputInfo("learning_rate", "paddle::dialect::DenseTensorType", false, false, false, false), paddle::dialect::OpInputInfo("grad", "paddle::dialect::SelectedRowsType", false, false, false, false), paddle::dialect::OpInputInfo("master_param", "paddle::dialect::DenseTensorType", true, false, false, false) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = { paddle::dialect::OpAttributeInfo("multi_precision", "pir::BoolAttribute", "") };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("param_out", "paddle::dialect::DenseTensorType", false, false), paddle::dialect::OpOutputInfo("master_param_out", "paddle::dialect::DenseTensorType", true, false) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("SgdInferMeta", {"param", "learning_rate", "grad", "master_param", "multi_precision"}, "sgd_dense_param_sparse_grad", {"param", "learning_rate", "grad", "master_param", "multi_precision"}, {"param"}, {}, {{"param_out", "param"},{"master_param_out", "master_param"}}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "sgd_");
}

void SgdDenseParamSparseGrad_Op::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value param_, pir::Value learning_rate_, pir::Value grad_, pir::Value master_param_, bool multi_precision) {
  VLOG(4) << "Start build SgdDenseParamSparseGrad_Op";



  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {param_, learning_rate_, grad_, master_param_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_multi_precision = pir::BoolAttribute::get(pir::IrContext::Instance(), multi_precision);
  argument.AddAttribute("multi_precision", attr_multi_precision);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType param = param_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)param;
  paddle::dialect::DenseTensorType learning_rate = learning_rate_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)learning_rate;
  paddle::dialect::SelectedRowsType grad = grad_.type().dyn_cast<paddle::dialect::SelectedRowsType>(); (void)grad;

  VLOG(4) << "Builder construction  dense_param";
  paddle::dialect::IrTensor ir_tensor_param(paddle::dialect::TransToPhiDataType(param.dtype()),
                                                      param.dims(),
                                                      param.data_layout(),
                                                      param.lod(),
                                                      param.offset());
  VLOG(4) << "Builder construction  meta_param";
  paddle::dialect::IrMetaTensor meta_param(&ir_tensor_param);

  VLOG(4) << "Builder construction  dense_learning_rate";
  paddle::dialect::IrTensor ir_tensor_learning_rate(paddle::dialect::TransToPhiDataType(learning_rate.dtype()),
                                                      learning_rate.dims(),
                                                      learning_rate.data_layout(),
                                                      learning_rate.lod(),
                                                      learning_rate.offset());
  VLOG(4) << "Builder construction  meta_learning_rate";
  paddle::dialect::IrMetaTensor meta_learning_rate(&ir_tensor_learning_rate);

  VLOG(4) << "Builder construction  dense_grad";
  paddle::dialect::IrTensor ir_tensor_grad(paddle::dialect::TransToPhiDataType(grad.dtype()),
                                                      grad.dims(),
                                                      grad.data_layout(),
                                                      grad.lod(),
                                                      grad.offset());
  VLOG(4) << "Builder construction  meta_grad";
  paddle::dialect::IrMetaTensor meta_grad(&ir_tensor_grad);

  paddle::dialect::IrMetaTensor meta_master_param;
  paddle::dialect::IrTensor ir_tensor_master_param;
  if (master_param_.impl() != nullptr) {
    paddle::dialect::DenseTensorType master_param = master_param_.type().dyn_cast<paddle::dialect::DenseTensorType>();
    VLOG(4) << "Builder construction  dense_master_param";
    ir_tensor_master_param = paddle::dialect::IrTensor(paddle::dialect::TransToPhiDataType(master_param.dtype()),
                                                        master_param.dims(),
                                                        master_param.data_layout(),
                                                        master_param.lod(),
                                                        master_param.offset());
    VLOG(4) << "Builder construction  meta_master_param";
    meta_master_param = paddle::dialect::IrMetaTensor(&ir_tensor_master_param);
  }

  paddle::dialect::IrTensor dense_param_out;
  paddle::dialect::IrMetaTensor meta_param_out(&dense_param_out);
  paddle::dialect::IrTensor dense_master_param_out;
  paddle::dialect::IrMetaTensor meta_master_param_out(&dense_master_param_out);

  phi::SgdInferMeta(meta_param, meta_learning_rate, meta_grad, meta_master_param, multi_precision, &meta_param_out, &meta_master_param_out);

  std::vector<pir::Type> argument_outputs;
  pir::Type param_out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_param_out.dtype()), dense_param_out.dims(), dense_param_out.layout(), dense_param_out.lod(), dense_param_out.offset());
  argument_outputs.push_back(param_out_dense_tensor_type);

  if (master_param_.impl() != nullptr) {
    pir::Type master_param_out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_master_param_out.dtype()), dense_master_param_out.dims(), dense_master_param_out.layout(), dense_master_param_out.lod(), dense_master_param_out.offset());
    argument_outputs.push_back(master_param_out_dense_tensor_type);
  } else {
    pir::Type master_param_out_type;
    argument_outputs.push_back(master_param_out_type);
  }

  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void SgdDenseParamSparseGrad_Op::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value param_, pir::Value learning_rate_, pir::Value grad_, pir::Value master_param_, pir::AttributeMap attributes) {
  VLOG(4) << "Start build SgdDenseParamSparseGrad_Op";


  IR_ENFORCE(
      attributes.find("multi_precision") != attributes.end(),
          "'multi_precision' Attribute is expected for SgdDenseParamSparseGrad_Op. ");
  bool multi_precision = attributes.at("multi_precision").dyn_cast<pir::BoolAttribute>().data();


  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {param_, learning_rate_, grad_, master_param_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_multi_precision = pir::BoolAttribute::get(pir::IrContext::Instance(), multi_precision);
  argument.AddAttribute("multi_precision", attr_multi_precision);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType param = param_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)param;
  paddle::dialect::DenseTensorType learning_rate = learning_rate_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)learning_rate;
  paddle::dialect::SelectedRowsType grad = grad_.type().dyn_cast<paddle::dialect::SelectedRowsType>(); (void)grad;

  VLOG(4) << "Builder construction  dense_param";
  paddle::dialect::IrTensor ir_tensor_param(paddle::dialect::TransToPhiDataType(param.dtype()),
                                                      param.dims(),
                                                      param.data_layout(),
                                                      param.lod(),
                                                      param.offset());
  VLOG(4) << "Builder construction  meta_param";
  paddle::dialect::IrMetaTensor meta_param(&ir_tensor_param);

  VLOG(4) << "Builder construction  dense_learning_rate";
  paddle::dialect::IrTensor ir_tensor_learning_rate(paddle::dialect::TransToPhiDataType(learning_rate.dtype()),
                                                      learning_rate.dims(),
                                                      learning_rate.data_layout(),
                                                      learning_rate.lod(),
                                                      learning_rate.offset());
  VLOG(4) << "Builder construction  meta_learning_rate";
  paddle::dialect::IrMetaTensor meta_learning_rate(&ir_tensor_learning_rate);

  VLOG(4) << "Builder construction  dense_grad";
  paddle::dialect::IrTensor ir_tensor_grad(paddle::dialect::TransToPhiDataType(grad.dtype()),
                                                      grad.dims(),
                                                      grad.data_layout(),
                                                      grad.lod(),
                                                      grad.offset());
  VLOG(4) << "Builder construction  meta_grad";
  paddle::dialect::IrMetaTensor meta_grad(&ir_tensor_grad);

  paddle::dialect::IrMetaTensor meta_master_param;
  paddle::dialect::IrTensor ir_tensor_master_param;
  if (master_param_.impl() != nullptr) {
    paddle::dialect::DenseTensorType master_param = master_param_.type().dyn_cast<paddle::dialect::DenseTensorType>();
    VLOG(4) << "Builder construction  dense_master_param";
    ir_tensor_master_param = paddle::dialect::IrTensor(paddle::dialect::TransToPhiDataType(master_param.dtype()),
                                                        master_param.dims(),
                                                        master_param.data_layout(),
                                                        master_param.lod(),
                                                        master_param.offset());
    VLOG(4) << "Builder construction  meta_master_param";
    meta_master_param = paddle::dialect::IrMetaTensor(&ir_tensor_master_param);
  }

  paddle::dialect::IrTensor dense_param_out;
  paddle::dialect::IrMetaTensor meta_param_out(&dense_param_out);
  paddle::dialect::IrTensor dense_master_param_out;
  paddle::dialect::IrMetaTensor meta_master_param_out(&dense_master_param_out);

  phi::SgdInferMeta(meta_param, meta_learning_rate, meta_grad, meta_master_param, multi_precision, &meta_param_out, &meta_master_param_out);

  std::vector<pir::Type> argument_outputs;
  pir::Type param_out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_param_out.dtype()), dense_param_out.dims(), dense_param_out.layout(), dense_param_out.lod(), dense_param_out.offset());
  argument_outputs.push_back(param_out_dense_tensor_type);

  if (master_param_.impl() != nullptr) {
    pir::Type master_param_out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_master_param_out.dtype()), dense_master_param_out.dims(), dense_master_param_out.layout(), dense_master_param_out.lod(), dense_master_param_out.offset());
    argument_outputs.push_back(master_param_out_dense_tensor_type);
  } else {
    pir::Type master_param_out_type;
    argument_outputs.push_back(master_param_out_type);
  }

  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void SgdDenseParamSparseGrad_Op::VerifySig() {}

void SgdDenseParamSparseGrad_Op::InferMeta( phi::InferMetaContext *infer_meta ) {
  auto fn = PD_INFER_META(phi::SgdInferMeta);
  fn(infer_meta);
}

phi::DataType SgdDenseParamSparseGrad_Op::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: SgdDenseParamSparseGrad_Op";
  

  // deal support data transform
  VLOG(8) << "SUPPORT_TRANSFORM: " << "['learning_rate'];";
  return tensor_dtype;


  return expected_kernel_dtype;
}

const char *SgdSparseParamSparseGrad_Op::attributes_name[1] = { "multi_precision" };

OpInfoTuple SgdSparseParamSparseGrad_Op::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("param", "paddle::dialect::SelectedRowsType", false, false, false, false), paddle::dialect::OpInputInfo("learning_rate", "paddle::dialect::DenseTensorType", false, false, false, false), paddle::dialect::OpInputInfo("grad", "paddle::dialect::SelectedRowsType", false, false, false, false), paddle::dialect::OpInputInfo("master_param", "paddle::dialect::SelectedRowsType", true, false, false, false) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = { paddle::dialect::OpAttributeInfo("multi_precision", "pir::BoolAttribute", "") };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("param_out", "paddle::dialect::SelectedRowsType", false, false), paddle::dialect::OpOutputInfo("master_param_out", "paddle::dialect::SelectedRowsType", true, false) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("SgdInferMeta", {"param", "learning_rate", "grad", "master_param", "multi_precision"}, "sgd_sparse_param_sparse_grad", {"param", "learning_rate", "grad", "master_param", "multi_precision"}, {"param"}, {}, {{"param_out", "param"},{"master_param_out", "master_param"}}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "sgd_");
}

void SgdSparseParamSparseGrad_Op::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value param_, pir::Value learning_rate_, pir::Value grad_, pir::Value master_param_, bool multi_precision) {
  VLOG(4) << "Start build SgdSparseParamSparseGrad_Op";



  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {param_, learning_rate_, grad_, master_param_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_multi_precision = pir::BoolAttribute::get(pir::IrContext::Instance(), multi_precision);
  argument.AddAttribute("multi_precision", attr_multi_precision);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::SelectedRowsType param = param_.type().dyn_cast<paddle::dialect::SelectedRowsType>(); (void)param;
  paddle::dialect::DenseTensorType learning_rate = learning_rate_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)learning_rate;
  paddle::dialect::SelectedRowsType grad = grad_.type().dyn_cast<paddle::dialect::SelectedRowsType>(); (void)grad;

  VLOG(4) << "Builder construction  dense_param";
  paddle::dialect::IrTensor ir_tensor_param(paddle::dialect::TransToPhiDataType(param.dtype()),
                                                      param.dims(),
                                                      param.data_layout(),
                                                      param.lod(),
                                                      param.offset());
  VLOG(4) << "Builder construction  meta_param";
  paddle::dialect::IrMetaTensor meta_param(&ir_tensor_param);

  VLOG(4) << "Builder construction  dense_learning_rate";
  paddle::dialect::IrTensor ir_tensor_learning_rate(paddle::dialect::TransToPhiDataType(learning_rate.dtype()),
                                                      learning_rate.dims(),
                                                      learning_rate.data_layout(),
                                                      learning_rate.lod(),
                                                      learning_rate.offset());
  VLOG(4) << "Builder construction  meta_learning_rate";
  paddle::dialect::IrMetaTensor meta_learning_rate(&ir_tensor_learning_rate);

  VLOG(4) << "Builder construction  dense_grad";
  paddle::dialect::IrTensor ir_tensor_grad(paddle::dialect::TransToPhiDataType(grad.dtype()),
                                                      grad.dims(),
                                                      grad.data_layout(),
                                                      grad.lod(),
                                                      grad.offset());
  VLOG(4) << "Builder construction  meta_grad";
  paddle::dialect::IrMetaTensor meta_grad(&ir_tensor_grad);

  paddle::dialect::IrMetaTensor meta_master_param;
  paddle::dialect::IrTensor ir_tensor_master_param;
  if (master_param_.impl() != nullptr) {
    paddle::dialect::SelectedRowsType master_param = master_param_.type().dyn_cast<paddle::dialect::SelectedRowsType>();
    VLOG(4) << "Builder construction  dense_master_param";
    ir_tensor_master_param = paddle::dialect::IrTensor(paddle::dialect::TransToPhiDataType(master_param.dtype()),
                                                        master_param.dims(),
                                                        master_param.data_layout(),
                                                        master_param.lod(),
                                                        master_param.offset());
    VLOG(4) << "Builder construction  meta_master_param";
    meta_master_param = paddle::dialect::IrMetaTensor(&ir_tensor_master_param);
  }

  paddle::dialect::IrSelectedRows dense_param_out;
  paddle::dialect::IrMetaTensor meta_param_out(&dense_param_out);
  paddle::dialect::IrSelectedRows dense_master_param_out;
  paddle::dialect::IrMetaTensor meta_master_param_out(&dense_master_param_out);

  phi::SgdInferMeta(meta_param, meta_learning_rate, meta_grad, meta_master_param, multi_precision, &meta_param_out, &meta_master_param_out);

  std::vector<pir::Type> argument_outputs;
  pir::Type param_out_dense_tensor_type = paddle::dialect::SelectedRowsType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_param_out.dtype()), dense_param_out.dims(), dense_param_out.layout(), dense_param_out.lod(), dense_param_out.offset());
  argument_outputs.push_back(param_out_dense_tensor_type);

  if (master_param_.impl() != nullptr) {
    pir::Type master_param_out_dense_tensor_type = paddle::dialect::SelectedRowsType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_master_param_out.dtype()), dense_master_param_out.dims(), dense_master_param_out.layout(), dense_master_param_out.lod(), dense_master_param_out.offset());
    argument_outputs.push_back(master_param_out_dense_tensor_type);
  } else {
    pir::Type master_param_out_type;
    argument_outputs.push_back(master_param_out_type);
  }

  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void SgdSparseParamSparseGrad_Op::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value param_, pir::Value learning_rate_, pir::Value grad_, pir::Value master_param_, pir::AttributeMap attributes) {
  VLOG(4) << "Start build SgdSparseParamSparseGrad_Op";


  IR_ENFORCE(
      attributes.find("multi_precision") != attributes.end(),
          "'multi_precision' Attribute is expected for SgdSparseParamSparseGrad_Op. ");
  bool multi_precision = attributes.at("multi_precision").dyn_cast<pir::BoolAttribute>().data();


  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {param_, learning_rate_, grad_, master_param_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_multi_precision = pir::BoolAttribute::get(pir::IrContext::Instance(), multi_precision);
  argument.AddAttribute("multi_precision", attr_multi_precision);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::SelectedRowsType param = param_.type().dyn_cast<paddle::dialect::SelectedRowsType>(); (void)param;
  paddle::dialect::DenseTensorType learning_rate = learning_rate_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)learning_rate;
  paddle::dialect::SelectedRowsType grad = grad_.type().dyn_cast<paddle::dialect::SelectedRowsType>(); (void)grad;

  VLOG(4) << "Builder construction  dense_param";
  paddle::dialect::IrTensor ir_tensor_param(paddle::dialect::TransToPhiDataType(param.dtype()),
                                                      param.dims(),
                                                      param.data_layout(),
                                                      param.lod(),
                                                      param.offset());
  VLOG(4) << "Builder construction  meta_param";
  paddle::dialect::IrMetaTensor meta_param(&ir_tensor_param);

  VLOG(4) << "Builder construction  dense_learning_rate";
  paddle::dialect::IrTensor ir_tensor_learning_rate(paddle::dialect::TransToPhiDataType(learning_rate.dtype()),
                                                      learning_rate.dims(),
                                                      learning_rate.data_layout(),
                                                      learning_rate.lod(),
                                                      learning_rate.offset());
  VLOG(4) << "Builder construction  meta_learning_rate";
  paddle::dialect::IrMetaTensor meta_learning_rate(&ir_tensor_learning_rate);

  VLOG(4) << "Builder construction  dense_grad";
  paddle::dialect::IrTensor ir_tensor_grad(paddle::dialect::TransToPhiDataType(grad.dtype()),
                                                      grad.dims(),
                                                      grad.data_layout(),
                                                      grad.lod(),
                                                      grad.offset());
  VLOG(4) << "Builder construction  meta_grad";
  paddle::dialect::IrMetaTensor meta_grad(&ir_tensor_grad);

  paddle::dialect::IrMetaTensor meta_master_param;
  paddle::dialect::IrTensor ir_tensor_master_param;
  if (master_param_.impl() != nullptr) {
    paddle::dialect::SelectedRowsType master_param = master_param_.type().dyn_cast<paddle::dialect::SelectedRowsType>();
    VLOG(4) << "Builder construction  dense_master_param";
    ir_tensor_master_param = paddle::dialect::IrTensor(paddle::dialect::TransToPhiDataType(master_param.dtype()),
                                                        master_param.dims(),
                                                        master_param.data_layout(),
                                                        master_param.lod(),
                                                        master_param.offset());
    VLOG(4) << "Builder construction  meta_master_param";
    meta_master_param = paddle::dialect::IrMetaTensor(&ir_tensor_master_param);
  }

  paddle::dialect::IrSelectedRows dense_param_out;
  paddle::dialect::IrMetaTensor meta_param_out(&dense_param_out);
  paddle::dialect::IrSelectedRows dense_master_param_out;
  paddle::dialect::IrMetaTensor meta_master_param_out(&dense_master_param_out);

  phi::SgdInferMeta(meta_param, meta_learning_rate, meta_grad, meta_master_param, multi_precision, &meta_param_out, &meta_master_param_out);

  std::vector<pir::Type> argument_outputs;
  pir::Type param_out_dense_tensor_type = paddle::dialect::SelectedRowsType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_param_out.dtype()), dense_param_out.dims(), dense_param_out.layout(), dense_param_out.lod(), dense_param_out.offset());
  argument_outputs.push_back(param_out_dense_tensor_type);

  if (master_param_.impl() != nullptr) {
    pir::Type master_param_out_dense_tensor_type = paddle::dialect::SelectedRowsType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_master_param_out.dtype()), dense_master_param_out.dims(), dense_master_param_out.layout(), dense_master_param_out.lod(), dense_master_param_out.offset());
    argument_outputs.push_back(master_param_out_dense_tensor_type);
  } else {
    pir::Type master_param_out_type;
    argument_outputs.push_back(master_param_out_type);
  }

  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void SgdSparseParamSparseGrad_Op::VerifySig() {}

void SgdSparseParamSparseGrad_Op::InferMeta( phi::InferMetaContext *infer_meta ) {
  auto fn = PD_INFER_META(phi::SgdInferMeta);
  fn(infer_meta);
}

phi::DataType SgdSparseParamSparseGrad_Op::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: SgdSparseParamSparseGrad_Op";
  

  // deal support data transform
  VLOG(8) << "SUPPORT_TRANSFORM: " << "['learning_rate'];";
  return tensor_dtype;


  return expected_kernel_dtype;
}

OpInfoTuple ShapeOp::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("input", "paddle::dialect::DenseTensorType", false, false, false, false) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = {  };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("out", "paddle::dialect::DenseTensorType", false, false) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("ShapeInferMeta", {"input"}, "shape", {"input"}, {}, {}, {}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "shape");
}

void ShapeOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value input_) {
  VLOG(4) << "Start build ShapeOp";



  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {input_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType input = input_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)input;

  VLOG(4) << "Builder construction  dense_input";
  paddle::dialect::IrTensor ir_tensor_input(paddle::dialect::TransToPhiDataType(input.dtype()),
                                                      input.dims(),
                                                      input.data_layout(),
                                                      input.lod(),
                                                      input.offset());
  VLOG(4) << "Builder construction  meta_input";
  paddle::dialect::IrMetaTensor meta_input(&ir_tensor_input);
  paddle::dialect::IrTensor dense_out;
  paddle::dialect::IrMetaTensor meta_out(&dense_out);

  phi::ShapeInferMeta(meta_input, &meta_out);

  std::vector<pir::Type> argument_outputs;
  pir::Type out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_out.dtype()), dense_out.dims(), dense_out.layout(), dense_out.lod(), dense_out.offset());
  argument_outputs.push_back(out_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void ShapeOp::VerifySig() {
  VLOG(4) << "Start Verifying inputs, outputs and attributes for: ShapeOp.";
  VLOG(4) << "Verifying inputs:";
  {
  auto input_size = num_operands();
  IR_ENFORCE(input_size == 1u,
                    "The size %d of inputs must be equal to 1.", input_size);
  IR_ENFORCE((*this)->operand_source(0).type().isa<paddle::dialect::DenseTensorType>(),
                  "Type validation failed for the 0th input, got %s.", (*this)->operand_source(0).type());
  }
  VLOG(4) << "Verifying attributes:";
  {
  // Attributes num is 0, not need to check attributes type.
  }
  VLOG(4) << "Verifying outputs:";
  {
  auto output_size = num_results();
  IR_ENFORCE(output_size == 1u,
                    "The size %d of outputs must be equal to 1.", output_size);
  IR_ENFORCE((*this)->result(0).type().isa<paddle::dialect::DenseTensorType>(),
                 "Type validation failed for the 0th output.");
  }
  VLOG(4) << "End Verifying for: ShapeOp.";
}

void ShapeOp::InferMeta( phi::InferMetaContext *infer_meta ) {
  auto fn = PD_INFER_META(phi::ShapeInferMeta);
  fn(infer_meta);
}

phi::DataType ShapeOp::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: ShapeOp";
  

  // deal skip data transform
  if (var_name == "input"){
    return expected_kernel_dtype;
  }


  return expected_kernel_dtype;
}

bool ShapeOp::InferSymbolicShape(pir::ShapeConstraintIRAnalysis* shape_analysis) {
  VLOG(4) << "Infer symbolic shape for op: ShapeOp";
  return ShapeOpInferSymbolicShape(this->operation(), shape_analysis);
}

OpInfoTuple ShapeSrOp::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("input", "paddle::dialect::SelectedRowsType", false, false, false, false) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = {  };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("out", "paddle::dialect::DenseTensorType", false, false) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("ShapeInferMeta", {"input"}, "shape_sr", {"input"}, {}, {}, {}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "shape");
}

void ShapeSrOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value input_) {
  VLOG(4) << "Start build ShapeSrOp";



  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {input_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::SelectedRowsType input = input_.type().dyn_cast<paddle::dialect::SelectedRowsType>(); (void)input;

  VLOG(4) << "Builder construction  dense_input";
  paddle::dialect::IrTensor ir_tensor_input(paddle::dialect::TransToPhiDataType(input.dtype()),
                                                      input.dims(),
                                                      input.data_layout(),
                                                      input.lod(),
                                                      input.offset());
  VLOG(4) << "Builder construction  meta_input";
  paddle::dialect::IrMetaTensor meta_input(&ir_tensor_input);
  paddle::dialect::IrTensor dense_out;
  paddle::dialect::IrMetaTensor meta_out(&dense_out);

  phi::ShapeInferMeta(meta_input, &meta_out);

  std::vector<pir::Type> argument_outputs;
  pir::Type out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_out.dtype()), dense_out.dims(), dense_out.layout(), dense_out.lod(), dense_out.offset());
  argument_outputs.push_back(out_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void ShapeSrOp::VerifySig() {
  VLOG(4) << "Start Verifying inputs, outputs and attributes for: ShapeSrOp.";
  VLOG(4) << "Verifying inputs:";
  {
  auto input_size = num_operands();
  IR_ENFORCE(input_size == 1u,
                    "The size %d of inputs must be equal to 1.", input_size);
  IR_ENFORCE((*this)->operand_source(0).type().isa<paddle::dialect::SelectedRowsType>(),
                  "Type validation failed for the 0th input, got %s.", (*this)->operand_source(0).type());
  }
  VLOG(4) << "Verifying attributes:";
  {
  // Attributes num is 0, not need to check attributes type.
  }
  VLOG(4) << "Verifying outputs:";
  {
  auto output_size = num_results();
  IR_ENFORCE(output_size == 1u,
                    "The size %d of outputs must be equal to 1.", output_size);
  IR_ENFORCE((*this)->result(0).type().isa<paddle::dialect::DenseTensorType>(),
                 "Type validation failed for the 0th output.");
  }
  VLOG(4) << "End Verifying for: ShapeSrOp.";
}

void ShapeSrOp::InferMeta( phi::InferMetaContext *infer_meta ) {
  auto fn = PD_INFER_META(phi::ShapeInferMeta);
  fn(infer_meta);
}

phi::DataType ShapeSrOp::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: ShapeSrOp";
  

  // deal skip data transform
  if (var_name == "input"){
    return expected_kernel_dtype;
  }


  return expected_kernel_dtype;
}

bool ShapeSrOp::InferSymbolicShape(pir::ShapeConstraintIRAnalysis* shape_analysis) {
  VLOG(4) << "Infer symbolic shape for op: ShapeSrOp";
  return ShapeSrOpInferSymbolicShape(this->operation(), shape_analysis);
}

const char *ShardIndexOp::attributes_name[4] = { "index_num", "nshards", "shard_id", "ignore_value" };

OpInfoTuple ShardIndexOp::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("input", "paddle::dialect::DenseTensorType", false, false, false, false) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = { paddle::dialect::OpAttributeInfo("index_num", "pir::Int32Attribute", ""), paddle::dialect::OpAttributeInfo("nshards", "pir::Int32Attribute", ""), paddle::dialect::OpAttributeInfo("shard_id", "pir::Int32Attribute", ""), paddle::dialect::OpAttributeInfo("ignore_value", "pir::Int32Attribute", "") };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("out", "paddle::dialect::DenseTensorType", false, false) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("ShardIndexInferMeta", {"input", "index_num", "nshards", "shard_id", "ignore_value"}, "shard_index", {"input", "index_num", "nshards", "shard_id", "ignore_value"}, {}, {}, {}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "shard_index");
}

void ShardIndexOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value input_, int index_num, int nshards, int shard_id, int ignore_value) {
  VLOG(4) << "Start build ShardIndexOp";



  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {input_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_index_num = pir::Int32Attribute::get(pir::IrContext::Instance(), index_num);
  argument.AddAttribute("index_num", attr_index_num);
  pir::Attribute attr_nshards = pir::Int32Attribute::get(pir::IrContext::Instance(), nshards);
  argument.AddAttribute("nshards", attr_nshards);
  pir::Attribute attr_shard_id = pir::Int32Attribute::get(pir::IrContext::Instance(), shard_id);
  argument.AddAttribute("shard_id", attr_shard_id);
  pir::Attribute attr_ignore_value = pir::Int32Attribute::get(pir::IrContext::Instance(), ignore_value);
  argument.AddAttribute("ignore_value", attr_ignore_value);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType input = input_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)input;

  VLOG(4) << "Builder construction  dense_input";
  paddle::dialect::IrTensor ir_tensor_input(paddle::dialect::TransToPhiDataType(input.dtype()),
                                                      input.dims(),
                                                      input.data_layout(),
                                                      input.lod(),
                                                      input.offset());
  VLOG(4) << "Builder construction  meta_input";
  paddle::dialect::IrMetaTensor meta_input(&ir_tensor_input);
  paddle::dialect::IrTensor dense_out;
  paddle::dialect::IrMetaTensor meta_out(&dense_out);

  phi::ShardIndexInferMeta(meta_input, index_num, nshards, shard_id, ignore_value, &meta_out);

  std::vector<pir::Type> argument_outputs;
  pir::Type out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_out.dtype()), dense_out.dims(), dense_out.layout(), dense_out.lod(), dense_out.offset());
  argument_outputs.push_back(out_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void ShardIndexOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value input_, pir::AttributeMap attributes) {
  VLOG(4) << "Start build ShardIndexOp";


  IR_ENFORCE(
      attributes.find("index_num") != attributes.end(),
          "'index_num' Attribute is expected for ShardIndexOp. ");
  int index_num = attributes.at("index_num").dyn_cast<pir::Int32Attribute>().data();

  IR_ENFORCE(
      attributes.find("nshards") != attributes.end(),
          "'nshards' Attribute is expected for ShardIndexOp. ");
  int nshards = attributes.at("nshards").dyn_cast<pir::Int32Attribute>().data();

  IR_ENFORCE(
      attributes.find("shard_id") != attributes.end(),
          "'shard_id' Attribute is expected for ShardIndexOp. ");
  int shard_id = attributes.at("shard_id").dyn_cast<pir::Int32Attribute>().data();

  IR_ENFORCE(
      attributes.find("ignore_value") != attributes.end(),
          "'ignore_value' Attribute is expected for ShardIndexOp. ");
  int ignore_value = attributes.at("ignore_value").dyn_cast<pir::Int32Attribute>().data();


  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {input_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_index_num = pir::Int32Attribute::get(pir::IrContext::Instance(), index_num);
  argument.AddAttribute("index_num", attr_index_num);
  pir::Attribute attr_nshards = pir::Int32Attribute::get(pir::IrContext::Instance(), nshards);
  argument.AddAttribute("nshards", attr_nshards);
  pir::Attribute attr_shard_id = pir::Int32Attribute::get(pir::IrContext::Instance(), shard_id);
  argument.AddAttribute("shard_id", attr_shard_id);
  pir::Attribute attr_ignore_value = pir::Int32Attribute::get(pir::IrContext::Instance(), ignore_value);
  argument.AddAttribute("ignore_value", attr_ignore_value);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType input = input_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)input;

  VLOG(4) << "Builder construction  dense_input";
  paddle::dialect::IrTensor ir_tensor_input(paddle::dialect::TransToPhiDataType(input.dtype()),
                                                      input.dims(),
                                                      input.data_layout(),
                                                      input.lod(),
                                                      input.offset());
  VLOG(4) << "Builder construction  meta_input";
  paddle::dialect::IrMetaTensor meta_input(&ir_tensor_input);
  paddle::dialect::IrTensor dense_out;
  paddle::dialect::IrMetaTensor meta_out(&dense_out);

  phi::ShardIndexInferMeta(meta_input, index_num, nshards, shard_id, ignore_value, &meta_out);

  std::vector<pir::Type> argument_outputs;
  pir::Type out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_out.dtype()), dense_out.dims(), dense_out.layout(), dense_out.lod(), dense_out.offset());
  argument_outputs.push_back(out_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void ShardIndexOp::VerifySig() {
  VLOG(4) << "Start Verifying inputs, outputs and attributes for: ShardIndexOp.";
  VLOG(4) << "Verifying inputs:";
  {
  auto input_size = num_operands();
  IR_ENFORCE(input_size == 1u,
                    "The size %d of inputs must be equal to 1.", input_size);
  IR_ENFORCE((*this)->operand_source(0).type().isa<paddle::dialect::DenseTensorType>(),
                  "Type validation failed for the 0th input, got %s.", (*this)->operand_source(0).type());
  }
  VLOG(4) << "Verifying attributes:";
  {
  auto& attributes = this->attributes();
  IR_ENFORCE(attributes.count("index_num")>0,
                 "index_num does not exist.");
  IR_ENFORCE(attributes.at("index_num").isa<pir::Int32Attribute>(),
                 "Type of attribute: index_num is not pir::Int32Attribute.");

  IR_ENFORCE(attributes.count("nshards")>0,
                 "nshards does not exist.");
  IR_ENFORCE(attributes.at("nshards").isa<pir::Int32Attribute>(),
                 "Type of attribute: nshards is not pir::Int32Attribute.");

  IR_ENFORCE(attributes.count("shard_id")>0,
                 "shard_id does not exist.");
  IR_ENFORCE(attributes.at("shard_id").isa<pir::Int32Attribute>(),
                 "Type of attribute: shard_id is not pir::Int32Attribute.");

  IR_ENFORCE(attributes.count("ignore_value")>0,
                 "ignore_value does not exist.");
  IR_ENFORCE(attributes.at("ignore_value").isa<pir::Int32Attribute>(),
                 "Type of attribute: ignore_value is not pir::Int32Attribute.");

  }
  VLOG(4) << "Verifying outputs:";
  {
  auto output_size = num_results();
  IR_ENFORCE(output_size == 1u,
                    "The size %d of outputs must be equal to 1.", output_size);
  IR_ENFORCE((*this)->result(0).type().isa<paddle::dialect::DenseTensorType>(),
                 "Type validation failed for the 0th output.");
  }
  VLOG(4) << "End Verifying for: ShardIndexOp.";
}

void ShardIndexOp::InferMeta( phi::InferMetaContext *infer_meta ) {
  auto fn = PD_INFER_META(phi::ShardIndexInferMeta);
  fn(infer_meta);
}

phi::DataType ShardIndexOp::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: ShardIndexOp";
  


  return expected_kernel_dtype;
}

OpInfoTuple SigmoidOp::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("x", "paddle::dialect::DenseTensorType", false, false, false, true) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = {  };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("out", "paddle::dialect::DenseTensorType", false, false) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("UnchangedInferMeta", {"x"}, "sigmoid", {"x"}, {}, {}, {}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "sigmoid");
}

void SigmoidOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_) {
  VLOG(4) << "Start build SigmoidOp";



  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);
  paddle::dialect::IrTensor dense_out;
  paddle::dialect::IrMetaTensor meta_out(&dense_out);

  phi::UnchangedInferMeta(meta_x, &meta_out);

  std::vector<pir::Type> argument_outputs;
  pir::Type out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_out.dtype()), dense_out.dims(), dense_out.layout(), dense_out.lod(), dense_out.offset());
  argument_outputs.push_back(out_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void SigmoidOp::VerifySig() {
  VLOG(4) << "Start Verifying inputs, outputs and attributes for: SigmoidOp.";
  VLOG(4) << "Verifying inputs:";
  {
  auto input_size = num_operands();
  IR_ENFORCE(input_size == 1u,
                    "The size %d of inputs must be equal to 1.", input_size);
  IR_ENFORCE((*this)->operand_source(0).type().isa<paddle::dialect::DenseTensorType>(),
                  "Type validation failed for the 0th input, got %s.", (*this)->operand_source(0).type());
  }
  VLOG(4) << "Verifying attributes:";
  {
  // Attributes num is 0, not need to check attributes type.
  }
  VLOG(4) << "Verifying outputs:";
  {
  auto output_size = num_results();
  IR_ENFORCE(output_size == 1u,
                    "The size %d of outputs must be equal to 1.", output_size);
  IR_ENFORCE((*this)->result(0).type().isa<paddle::dialect::DenseTensorType>(),
                 "Type validation failed for the 0th output.");
  }
  VLOG(4) << "End Verifying for: SigmoidOp.";
}

void SigmoidOp::InferMeta( phi::InferMetaContext *infer_meta ) {
  auto fn = PD_INFER_META(phi::UnchangedInferMeta);
  fn(infer_meta);
}

phi::DataType SigmoidOp::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: SigmoidOp";
  


  return expected_kernel_dtype;
}

OpInfoTuple Sigmoid_Op::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("x", "paddle::dialect::DenseTensorType", false, false, false, true) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = {  };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("out", "paddle::dialect::DenseTensorType", false, false) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("UnchangedInferMeta", {"x"}, "sigmoid", {"x"}, {}, {}, {{"out", "x"}}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "sigmoid");
}

void Sigmoid_Op::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_) {
  VLOG(4) << "Start build Sigmoid_Op";



  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);
  paddle::dialect::IrTensor dense_out;
  paddle::dialect::IrMetaTensor meta_out(&dense_out);

  phi::UnchangedInferMeta(meta_x, &meta_out);

  std::vector<pir::Type> argument_outputs;
  pir::Type out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_out.dtype()), dense_out.dims(), dense_out.layout(), dense_out.lod(), dense_out.offset());
  argument_outputs.push_back(out_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void Sigmoid_Op::VerifySig() {
  VLOG(4) << "Start Verifying inputs, outputs and attributes for: Sigmoid_Op.";
  VLOG(4) << "Verifying inputs:";
  {
  auto input_size = num_operands();
  IR_ENFORCE(input_size == 1u,
                    "The size %d of inputs must be equal to 1.", input_size);
  IR_ENFORCE((*this)->operand_source(0).type().isa<paddle::dialect::DenseTensorType>(),
                  "Type validation failed for the 0th input, got %s.", (*this)->operand_source(0).type());
  }
  VLOG(4) << "Verifying attributes:";
  {
  // Attributes num is 0, not need to check attributes type.
  }
  VLOG(4) << "Verifying outputs:";
  {
  auto output_size = num_results();
  IR_ENFORCE(output_size == 1u,
                    "The size %d of outputs must be equal to 1.", output_size);
  IR_ENFORCE((*this)->result(0).type().isa<paddle::dialect::DenseTensorType>(),
                 "Type validation failed for the 0th output.");
  }
  VLOG(4) << "End Verifying for: Sigmoid_Op.";
}

void Sigmoid_Op::InferMeta( phi::InferMetaContext *infer_meta ) {
  auto fn = PD_INFER_META(phi::UnchangedInferMeta);
  fn(infer_meta);
}

phi::DataType Sigmoid_Op::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: Sigmoid_Op";
  


  return expected_kernel_dtype;
}

const char *SigmoidCrossEntropyWithLogitsOp::attributes_name[2] = { "normalize", "ignore_index" };

OpInfoTuple SigmoidCrossEntropyWithLogitsOp::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("x", "paddle::dialect::DenseTensorType", false, false, false, true), paddle::dialect::OpInputInfo("label", "paddle::dialect::DenseTensorType", false, false, false, false), paddle::dialect::OpInputInfo("pos_weight", "paddle::dialect::DenseTensorType", true, false, false, false) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = { paddle::dialect::OpAttributeInfo("normalize", "pir::BoolAttribute", ""), paddle::dialect::OpAttributeInfo("ignore_index", "pir::Int32Attribute", "") };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("out", "paddle::dialect::DenseTensorType", false, false) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("SigmoidCrossEntropyWithLogitsInferMeta", {"x", "label", "pos_weight", "normalize", "ignore_index"}, "sigmoid_cross_entropy_with_logits", {"x", "label", "pos_weight", "normalize", "ignore_index"}, {}, {}, {}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "sigmoid_cross_entropy_with_logits");
}

void SigmoidCrossEntropyWithLogitsOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, pir::Value label_, pir::Value pos_weight_, bool normalize, int ignore_index) {
  VLOG(4) << "Start build SigmoidCrossEntropyWithLogitsOp";



  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_, label_, pos_weight_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_normalize = pir::BoolAttribute::get(pir::IrContext::Instance(), normalize);
  argument.AddAttribute("normalize", attr_normalize);
  pir::Attribute attr_ignore_index = pir::Int32Attribute::get(pir::IrContext::Instance(), ignore_index);
  argument.AddAttribute("ignore_index", attr_ignore_index);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;
  paddle::dialect::DenseTensorType label = label_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)label;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);

  VLOG(4) << "Builder construction  dense_label";
  paddle::dialect::IrTensor ir_tensor_label(paddle::dialect::TransToPhiDataType(label.dtype()),
                                                      label.dims(),
                                                      label.data_layout(),
                                                      label.lod(),
                                                      label.offset());
  VLOG(4) << "Builder construction  meta_label";
  paddle::dialect::IrMetaTensor meta_label(&ir_tensor_label);

  paddle::dialect::IrMetaTensor meta_pos_weight;
  paddle::dialect::IrTensor ir_tensor_pos_weight;
  if (pos_weight_.impl() != nullptr) {
    paddle::dialect::DenseTensorType pos_weight = pos_weight_.type().dyn_cast<paddle::dialect::DenseTensorType>();
    VLOG(4) << "Builder construction  dense_pos_weight";
    ir_tensor_pos_weight = paddle::dialect::IrTensor(paddle::dialect::TransToPhiDataType(pos_weight.dtype()),
                                                        pos_weight.dims(),
                                                        pos_weight.data_layout(),
                                                        pos_weight.lod(),
                                                        pos_weight.offset());
    VLOG(4) << "Builder construction  meta_pos_weight";
    meta_pos_weight = paddle::dialect::IrMetaTensor(&ir_tensor_pos_weight);
  }

  paddle::dialect::IrTensor dense_out;
  paddle::dialect::IrMetaTensor meta_out(&dense_out);

  phi::SigmoidCrossEntropyWithLogitsInferMeta(meta_x, meta_label, meta_pos_weight, normalize, ignore_index, &meta_out);

  std::vector<pir::Type> argument_outputs;
  pir::Type out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_out.dtype()), dense_out.dims(), dense_out.layout(), dense_out.lod(), dense_out.offset());
  argument_outputs.push_back(out_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void SigmoidCrossEntropyWithLogitsOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, pir::Value label_, pir::Value pos_weight_, pir::AttributeMap attributes) {
  VLOG(4) << "Start build SigmoidCrossEntropyWithLogitsOp";


  IR_ENFORCE(
      attributes.find("normalize") != attributes.end(),
          "'normalize' Attribute is expected for SigmoidCrossEntropyWithLogitsOp. ");
  bool normalize = attributes.at("normalize").dyn_cast<pir::BoolAttribute>().data();

  IR_ENFORCE(
      attributes.find("ignore_index") != attributes.end(),
          "'ignore_index' Attribute is expected for SigmoidCrossEntropyWithLogitsOp. ");
  int ignore_index = attributes.at("ignore_index").dyn_cast<pir::Int32Attribute>().data();


  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_, label_, pos_weight_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_normalize = pir::BoolAttribute::get(pir::IrContext::Instance(), normalize);
  argument.AddAttribute("normalize", attr_normalize);
  pir::Attribute attr_ignore_index = pir::Int32Attribute::get(pir::IrContext::Instance(), ignore_index);
  argument.AddAttribute("ignore_index", attr_ignore_index);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;
  paddle::dialect::DenseTensorType label = label_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)label;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);

  VLOG(4) << "Builder construction  dense_label";
  paddle::dialect::IrTensor ir_tensor_label(paddle::dialect::TransToPhiDataType(label.dtype()),
                                                      label.dims(),
                                                      label.data_layout(),
                                                      label.lod(),
                                                      label.offset());
  VLOG(4) << "Builder construction  meta_label";
  paddle::dialect::IrMetaTensor meta_label(&ir_tensor_label);

  paddle::dialect::IrMetaTensor meta_pos_weight;
  paddle::dialect::IrTensor ir_tensor_pos_weight;
  if (pos_weight_.impl() != nullptr) {
    paddle::dialect::DenseTensorType pos_weight = pos_weight_.type().dyn_cast<paddle::dialect::DenseTensorType>();
    VLOG(4) << "Builder construction  dense_pos_weight";
    ir_tensor_pos_weight = paddle::dialect::IrTensor(paddle::dialect::TransToPhiDataType(pos_weight.dtype()),
                                                        pos_weight.dims(),
                                                        pos_weight.data_layout(),
                                                        pos_weight.lod(),
                                                        pos_weight.offset());
    VLOG(4) << "Builder construction  meta_pos_weight";
    meta_pos_weight = paddle::dialect::IrMetaTensor(&ir_tensor_pos_weight);
  }

  paddle::dialect::IrTensor dense_out;
  paddle::dialect::IrMetaTensor meta_out(&dense_out);

  phi::SigmoidCrossEntropyWithLogitsInferMeta(meta_x, meta_label, meta_pos_weight, normalize, ignore_index, &meta_out);

  std::vector<pir::Type> argument_outputs;
  pir::Type out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_out.dtype()), dense_out.dims(), dense_out.layout(), dense_out.lod(), dense_out.offset());
  argument_outputs.push_back(out_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void SigmoidCrossEntropyWithLogitsOp::VerifySig() {
  VLOG(4) << "Start Verifying inputs, outputs and attributes for: SigmoidCrossEntropyWithLogitsOp.";
  VLOG(4) << "Verifying inputs:";
  {
  auto input_size = num_operands();
  IR_ENFORCE(input_size == 3u,
                    "The size %d of inputs must be equal to 3.", input_size);
  IR_ENFORCE((*this)->operand_source(0).type().isa<paddle::dialect::DenseTensorType>(),
                  "Type validation failed for the 0th input, got %s.", (*this)->operand_source(0).type());
  IR_ENFORCE((*this)->operand_source(1).type().isa<paddle::dialect::DenseTensorType>(),
                  "Type validation failed for the 1th input, got %s.", (*this)->operand_source(1).type());
  if (auto val = (*this)->operand(2)) {
    IR_ENFORCE(val.type().isa<paddle::dialect::DenseTensorType>(),
                   "Type validation failed for the 2th input, got %s.", (*this)->operand_source(2).type());
  }
  }
  VLOG(4) << "Verifying attributes:";
  {
  auto& attributes = this->attributes();
  IR_ENFORCE(attributes.count("normalize")>0,
                 "normalize does not exist.");
  IR_ENFORCE(attributes.at("normalize").isa<pir::BoolAttribute>(),
                 "Type of attribute: normalize is not pir::BoolAttribute.");

  IR_ENFORCE(attributes.count("ignore_index")>0,
                 "ignore_index does not exist.");
  IR_ENFORCE(attributes.at("ignore_index").isa<pir::Int32Attribute>(),
                 "Type of attribute: ignore_index is not pir::Int32Attribute.");

  }
  VLOG(4) << "Verifying outputs:";
  {
  auto output_size = num_results();
  IR_ENFORCE(output_size == 1u,
                    "The size %d of outputs must be equal to 1.", output_size);
  IR_ENFORCE((*this)->result(0).type().isa<paddle::dialect::DenseTensorType>(),
                 "Type validation failed for the 0th output.");
  }
  VLOG(4) << "End Verifying for: SigmoidCrossEntropyWithLogitsOp.";
}

void SigmoidCrossEntropyWithLogitsOp::InferMeta( phi::InferMetaContext *infer_meta ) {
  auto fn = PD_INFER_META(phi::SigmoidCrossEntropyWithLogitsInferMeta);
  fn(infer_meta);
}

phi::DataType SigmoidCrossEntropyWithLogitsOp::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: SigmoidCrossEntropyWithLogitsOp";
  


  return expected_kernel_dtype;
}

const char *SigmoidCrossEntropyWithLogits_Op::attributes_name[2] = { "normalize", "ignore_index" };

OpInfoTuple SigmoidCrossEntropyWithLogits_Op::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("x", "paddle::dialect::DenseTensorType", false, false, false, true), paddle::dialect::OpInputInfo("label", "paddle::dialect::DenseTensorType", false, false, false, false), paddle::dialect::OpInputInfo("pos_weight", "paddle::dialect::DenseTensorType", true, false, false, false) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = { paddle::dialect::OpAttributeInfo("normalize", "pir::BoolAttribute", ""), paddle::dialect::OpAttributeInfo("ignore_index", "pir::Int32Attribute", "") };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("out", "paddle::dialect::DenseTensorType", false, false) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("SigmoidCrossEntropyWithLogitsInferMeta", {"x", "label", "pos_weight", "normalize", "ignore_index"}, "sigmoid_cross_entropy_with_logits", {"x", "label", "pos_weight", "normalize", "ignore_index"}, {}, {}, {{"out", "x"}}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "sigmoid_cross_entropy_with_logits");
}

void SigmoidCrossEntropyWithLogits_Op::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, pir::Value label_, pir::Value pos_weight_, bool normalize, int ignore_index) {
  VLOG(4) << "Start build SigmoidCrossEntropyWithLogits_Op";



  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_, label_, pos_weight_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_normalize = pir::BoolAttribute::get(pir::IrContext::Instance(), normalize);
  argument.AddAttribute("normalize", attr_normalize);
  pir::Attribute attr_ignore_index = pir::Int32Attribute::get(pir::IrContext::Instance(), ignore_index);
  argument.AddAttribute("ignore_index", attr_ignore_index);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;
  paddle::dialect::DenseTensorType label = label_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)label;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);

  VLOG(4) << "Builder construction  dense_label";
  paddle::dialect::IrTensor ir_tensor_label(paddle::dialect::TransToPhiDataType(label.dtype()),
                                                      label.dims(),
                                                      label.data_layout(),
                                                      label.lod(),
                                                      label.offset());
  VLOG(4) << "Builder construction  meta_label";
  paddle::dialect::IrMetaTensor meta_label(&ir_tensor_label);

  paddle::dialect::IrMetaTensor meta_pos_weight;
  paddle::dialect::IrTensor ir_tensor_pos_weight;
  if (pos_weight_.impl() != nullptr) {
    paddle::dialect::DenseTensorType pos_weight = pos_weight_.type().dyn_cast<paddle::dialect::DenseTensorType>();
    VLOG(4) << "Builder construction  dense_pos_weight";
    ir_tensor_pos_weight = paddle::dialect::IrTensor(paddle::dialect::TransToPhiDataType(pos_weight.dtype()),
                                                        pos_weight.dims(),
                                                        pos_weight.data_layout(),
                                                        pos_weight.lod(),
                                                        pos_weight.offset());
    VLOG(4) << "Builder construction  meta_pos_weight";
    meta_pos_weight = paddle::dialect::IrMetaTensor(&ir_tensor_pos_weight);
  }

  paddle::dialect::IrTensor dense_out;
  paddle::dialect::IrMetaTensor meta_out(&dense_out);

  phi::SigmoidCrossEntropyWithLogitsInferMeta(meta_x, meta_label, meta_pos_weight, normalize, ignore_index, &meta_out);

  std::vector<pir::Type> argument_outputs;
  pir::Type out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_out.dtype()), dense_out.dims(), dense_out.layout(), dense_out.lod(), dense_out.offset());
  argument_outputs.push_back(out_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void SigmoidCrossEntropyWithLogits_Op::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, pir::Value label_, pir::Value pos_weight_, pir::AttributeMap attributes) {
  VLOG(4) << "Start build SigmoidCrossEntropyWithLogits_Op";


  IR_ENFORCE(
      attributes.find("normalize") != attributes.end(),
          "'normalize' Attribute is expected for SigmoidCrossEntropyWithLogits_Op. ");
  bool normalize = attributes.at("normalize").dyn_cast<pir::BoolAttribute>().data();

  IR_ENFORCE(
      attributes.find("ignore_index") != attributes.end(),
          "'ignore_index' Attribute is expected for SigmoidCrossEntropyWithLogits_Op. ");
  int ignore_index = attributes.at("ignore_index").dyn_cast<pir::Int32Attribute>().data();


  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_, label_, pos_weight_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_normalize = pir::BoolAttribute::get(pir::IrContext::Instance(), normalize);
  argument.AddAttribute("normalize", attr_normalize);
  pir::Attribute attr_ignore_index = pir::Int32Attribute::get(pir::IrContext::Instance(), ignore_index);
  argument.AddAttribute("ignore_index", attr_ignore_index);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;
  paddle::dialect::DenseTensorType label = label_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)label;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);

  VLOG(4) << "Builder construction  dense_label";
  paddle::dialect::IrTensor ir_tensor_label(paddle::dialect::TransToPhiDataType(label.dtype()),
                                                      label.dims(),
                                                      label.data_layout(),
                                                      label.lod(),
                                                      label.offset());
  VLOG(4) << "Builder construction  meta_label";
  paddle::dialect::IrMetaTensor meta_label(&ir_tensor_label);

  paddle::dialect::IrMetaTensor meta_pos_weight;
  paddle::dialect::IrTensor ir_tensor_pos_weight;
  if (pos_weight_.impl() != nullptr) {
    paddle::dialect::DenseTensorType pos_weight = pos_weight_.type().dyn_cast<paddle::dialect::DenseTensorType>();
    VLOG(4) << "Builder construction  dense_pos_weight";
    ir_tensor_pos_weight = paddle::dialect::IrTensor(paddle::dialect::TransToPhiDataType(pos_weight.dtype()),
                                                        pos_weight.dims(),
                                                        pos_weight.data_layout(),
                                                        pos_weight.lod(),
                                                        pos_weight.offset());
    VLOG(4) << "Builder construction  meta_pos_weight";
    meta_pos_weight = paddle::dialect::IrMetaTensor(&ir_tensor_pos_weight);
  }

  paddle::dialect::IrTensor dense_out;
  paddle::dialect::IrMetaTensor meta_out(&dense_out);

  phi::SigmoidCrossEntropyWithLogitsInferMeta(meta_x, meta_label, meta_pos_weight, normalize, ignore_index, &meta_out);

  std::vector<pir::Type> argument_outputs;
  pir::Type out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_out.dtype()), dense_out.dims(), dense_out.layout(), dense_out.lod(), dense_out.offset());
  argument_outputs.push_back(out_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void SigmoidCrossEntropyWithLogits_Op::VerifySig() {
  VLOG(4) << "Start Verifying inputs, outputs and attributes for: SigmoidCrossEntropyWithLogits_Op.";
  VLOG(4) << "Verifying inputs:";
  {
  auto input_size = num_operands();
  IR_ENFORCE(input_size == 3u,
                    "The size %d of inputs must be equal to 3.", input_size);
  IR_ENFORCE((*this)->operand_source(0).type().isa<paddle::dialect::DenseTensorType>(),
                  "Type validation failed for the 0th input, got %s.", (*this)->operand_source(0).type());
  IR_ENFORCE((*this)->operand_source(1).type().isa<paddle::dialect::DenseTensorType>(),
                  "Type validation failed for the 1th input, got %s.", (*this)->operand_source(1).type());
  if (auto val = (*this)->operand(2)) {
    IR_ENFORCE(val.type().isa<paddle::dialect::DenseTensorType>(),
                   "Type validation failed for the 2th input, got %s.", (*this)->operand_source(2).type());
  }
  }
  VLOG(4) << "Verifying attributes:";
  {
  auto& attributes = this->attributes();
  IR_ENFORCE(attributes.count("normalize")>0,
                 "normalize does not exist.");
  IR_ENFORCE(attributes.at("normalize").isa<pir::BoolAttribute>(),
                 "Type of attribute: normalize is not pir::BoolAttribute.");

  IR_ENFORCE(attributes.count("ignore_index")>0,
                 "ignore_index does not exist.");
  IR_ENFORCE(attributes.at("ignore_index").isa<pir::Int32Attribute>(),
                 "Type of attribute: ignore_index is not pir::Int32Attribute.");

  }
  VLOG(4) << "Verifying outputs:";
  {
  auto output_size = num_results();
  IR_ENFORCE(output_size == 1u,
                    "The size %d of outputs must be equal to 1.", output_size);
  IR_ENFORCE((*this)->result(0).type().isa<paddle::dialect::DenseTensorType>(),
                 "Type validation failed for the 0th output.");
  }
  VLOG(4) << "End Verifying for: SigmoidCrossEntropyWithLogits_Op.";
}

void SigmoidCrossEntropyWithLogits_Op::InferMeta( phi::InferMetaContext *infer_meta ) {
  auto fn = PD_INFER_META(phi::SigmoidCrossEntropyWithLogitsInferMeta);
  fn(infer_meta);
}

phi::DataType SigmoidCrossEntropyWithLogits_Op::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: SigmoidCrossEntropyWithLogits_Op";
  


  return expected_kernel_dtype;
}

OpInfoTuple SignOp::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("x", "paddle::dialect::DenseTensorType", false, false, false, true) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = {  };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("out", "paddle::dialect::DenseTensorType", false, false) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("UnchangedInferMeta", {"x"}, "sign", {"x"}, {}, {}, {}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "sign");
}

void SignOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_) {
  VLOG(4) << "Start build SignOp";



  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);
  paddle::dialect::IrTensor dense_out;
  paddle::dialect::IrMetaTensor meta_out(&dense_out);

  phi::UnchangedInferMeta(meta_x, &meta_out);

  std::vector<pir::Type> argument_outputs;
  pir::Type out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_out.dtype()), dense_out.dims(), dense_out.layout(), dense_out.lod(), dense_out.offset());
  argument_outputs.push_back(out_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void SignOp::VerifySig() {
  VLOG(4) << "Start Verifying inputs, outputs and attributes for: SignOp.";
  VLOG(4) << "Verifying inputs:";
  {
  auto input_size = num_operands();
  IR_ENFORCE(input_size == 1u,
                    "The size %d of inputs must be equal to 1.", input_size);
  IR_ENFORCE((*this)->operand_source(0).type().isa<paddle::dialect::DenseTensorType>(),
                  "Type validation failed for the 0th input, got %s.", (*this)->operand_source(0).type());
  }
  VLOG(4) << "Verifying attributes:";
  {
  // Attributes num is 0, not need to check attributes type.
  }
  VLOG(4) << "Verifying outputs:";
  {
  auto output_size = num_results();
  IR_ENFORCE(output_size == 1u,
                    "The size %d of outputs must be equal to 1.", output_size);
  IR_ENFORCE((*this)->result(0).type().isa<paddle::dialect::DenseTensorType>(),
                 "Type validation failed for the 0th output.");
  }
  VLOG(4) << "End Verifying for: SignOp.";
}

void SignOp::InferMeta( phi::InferMetaContext *infer_meta ) {
  auto fn = PD_INFER_META(phi::UnchangedInferMeta);
  fn(infer_meta);
}

phi::DataType SignOp::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: SignOp";
  


  return expected_kernel_dtype;
}

OpInfoTuple SiluOp::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("x", "paddle::dialect::DenseTensorType", false, false, false, true) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = {  };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("out", "paddle::dialect::DenseTensorType", false, false) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("UnchangedInferMeta", {"x"}, "silu", {"x"}, {}, {}, {}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "silu");
}

void SiluOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_) {
  VLOG(4) << "Start build SiluOp";



  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);
  paddle::dialect::IrTensor dense_out;
  paddle::dialect::IrMetaTensor meta_out(&dense_out);

  phi::UnchangedInferMeta(meta_x, &meta_out);

  std::vector<pir::Type> argument_outputs;
  pir::Type out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_out.dtype()), dense_out.dims(), dense_out.layout(), dense_out.lod(), dense_out.offset());
  argument_outputs.push_back(out_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void SiluOp::VerifySig() {
  VLOG(4) << "Start Verifying inputs, outputs and attributes for: SiluOp.";
  VLOG(4) << "Verifying inputs:";
  {
  auto input_size = num_operands();
  IR_ENFORCE(input_size == 1u,
                    "The size %d of inputs must be equal to 1.", input_size);
  IR_ENFORCE((*this)->operand_source(0).type().isa<paddle::dialect::DenseTensorType>(),
                  "Type validation failed for the 0th input, got %s.", (*this)->operand_source(0).type());
  }
  VLOG(4) << "Verifying attributes:";
  {
  // Attributes num is 0, not need to check attributes type.
  }
  VLOG(4) << "Verifying outputs:";
  {
  auto output_size = num_results();
  IR_ENFORCE(output_size == 1u,
                    "The size %d of outputs must be equal to 1.", output_size);
  IR_ENFORCE((*this)->result(0).type().isa<paddle::dialect::DenseTensorType>(),
                 "Type validation failed for the 0th output.");
  }
  VLOG(4) << "End Verifying for: SiluOp.";
}

void SiluOp::InferMeta( phi::InferMetaContext *infer_meta ) {
  auto fn = PD_INFER_META(phi::UnchangedInferMeta);
  fn(infer_meta);
}

phi::DataType SiluOp::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: SiluOp";
  


  return expected_kernel_dtype;
}

OpInfoTuple SinOp::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("x", "paddle::dialect::DenseTensorType", false, false, false, true) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = {  };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("out", "paddle::dialect::DenseTensorType", false, false) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("UnchangedInferMeta", {"x"}, "sin", {"x"}, {}, {}, {}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "sin");
}

void SinOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_) {
  VLOG(4) << "Start build SinOp";



  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);
  paddle::dialect::IrTensor dense_out;
  paddle::dialect::IrMetaTensor meta_out(&dense_out);

  phi::UnchangedInferMeta(meta_x, &meta_out);

  std::vector<pir::Type> argument_outputs;
  pir::Type out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_out.dtype()), dense_out.dims(), dense_out.layout(), dense_out.lod(), dense_out.offset());
  argument_outputs.push_back(out_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void SinOp::VerifySig() {
  VLOG(4) << "Start Verifying inputs, outputs and attributes for: SinOp.";
  VLOG(4) << "Verifying inputs:";
  {
  auto input_size = num_operands();
  IR_ENFORCE(input_size == 1u,
                    "The size %d of inputs must be equal to 1.", input_size);
  IR_ENFORCE((*this)->operand_source(0).type().isa<paddle::dialect::DenseTensorType>(),
                  "Type validation failed for the 0th input, got %s.", (*this)->operand_source(0).type());
  }
  VLOG(4) << "Verifying attributes:";
  {
  // Attributes num is 0, not need to check attributes type.
  }
  VLOG(4) << "Verifying outputs:";
  {
  auto output_size = num_results();
  IR_ENFORCE(output_size == 1u,
                    "The size %d of outputs must be equal to 1.", output_size);
  IR_ENFORCE((*this)->result(0).type().isa<paddle::dialect::DenseTensorType>(),
                 "Type validation failed for the 0th output.");
  }
  VLOG(4) << "End Verifying for: SinOp.";
}

void SinOp::InferMeta( phi::InferMetaContext *infer_meta ) {
  auto fn = PD_INFER_META(phi::UnchangedInferMeta);
  fn(infer_meta);
}

phi::DataType SinOp::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: SinOp";
  


  return expected_kernel_dtype;
}

OpInfoTuple Sin_Op::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("x", "paddle::dialect::DenseTensorType", false, false, false, true) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = {  };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("out", "paddle::dialect::DenseTensorType", false, false) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("UnchangedInferMeta", {"x"}, "sin", {"x"}, {}, {}, {{"out", "x"}}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "sin");
}

void Sin_Op::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_) {
  VLOG(4) << "Start build Sin_Op";



  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);
  paddle::dialect::IrTensor dense_out;
  paddle::dialect::IrMetaTensor meta_out(&dense_out);

  phi::UnchangedInferMeta(meta_x, &meta_out);

  std::vector<pir::Type> argument_outputs;
  pir::Type out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_out.dtype()), dense_out.dims(), dense_out.layout(), dense_out.lod(), dense_out.offset());
  argument_outputs.push_back(out_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void Sin_Op::VerifySig() {
  VLOG(4) << "Start Verifying inputs, outputs and attributes for: Sin_Op.";
  VLOG(4) << "Verifying inputs:";
  {
  auto input_size = num_operands();
  IR_ENFORCE(input_size == 1u,
                    "The size %d of inputs must be equal to 1.", input_size);
  IR_ENFORCE((*this)->operand_source(0).type().isa<paddle::dialect::DenseTensorType>(),
                  "Type validation failed for the 0th input, got %s.", (*this)->operand_source(0).type());
  }
  VLOG(4) << "Verifying attributes:";
  {
  // Attributes num is 0, not need to check attributes type.
  }
  VLOG(4) << "Verifying outputs:";
  {
  auto output_size = num_results();
  IR_ENFORCE(output_size == 1u,
                    "The size %d of outputs must be equal to 1.", output_size);
  IR_ENFORCE((*this)->result(0).type().isa<paddle::dialect::DenseTensorType>(),
                 "Type validation failed for the 0th output.");
  }
  VLOG(4) << "End Verifying for: Sin_Op.";
}

void Sin_Op::InferMeta( phi::InferMetaContext *infer_meta ) {
  auto fn = PD_INFER_META(phi::UnchangedInferMeta);
  fn(infer_meta);
}

phi::DataType Sin_Op::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: Sin_Op";
  


  return expected_kernel_dtype;
}

OpInfoTuple SinhOp::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("x", "paddle::dialect::DenseTensorType", false, false, false, true) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = {  };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("out", "paddle::dialect::DenseTensorType", false, false) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("UnchangedInferMeta", {"x"}, "sinh", {"x"}, {}, {}, {}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "sinh");
}

void SinhOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_) {
  VLOG(4) << "Start build SinhOp";



  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);
  paddle::dialect::IrTensor dense_out;
  paddle::dialect::IrMetaTensor meta_out(&dense_out);

  phi::UnchangedInferMeta(meta_x, &meta_out);

  std::vector<pir::Type> argument_outputs;
  pir::Type out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_out.dtype()), dense_out.dims(), dense_out.layout(), dense_out.lod(), dense_out.offset());
  argument_outputs.push_back(out_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void SinhOp::VerifySig() {
  VLOG(4) << "Start Verifying inputs, outputs and attributes for: SinhOp.";
  VLOG(4) << "Verifying inputs:";
  {
  auto input_size = num_operands();
  IR_ENFORCE(input_size == 1u,
                    "The size %d of inputs must be equal to 1.", input_size);
  IR_ENFORCE((*this)->operand_source(0).type().isa<paddle::dialect::DenseTensorType>(),
                  "Type validation failed for the 0th input, got %s.", (*this)->operand_source(0).type());
  }
  VLOG(4) << "Verifying attributes:";
  {
  // Attributes num is 0, not need to check attributes type.
  }
  VLOG(4) << "Verifying outputs:";
  {
  auto output_size = num_results();
  IR_ENFORCE(output_size == 1u,
                    "The size %d of outputs must be equal to 1.", output_size);
  IR_ENFORCE((*this)->result(0).type().isa<paddle::dialect::DenseTensorType>(),
                 "Type validation failed for the 0th output.");
  }
  VLOG(4) << "End Verifying for: SinhOp.";
}

void SinhOp::InferMeta( phi::InferMetaContext *infer_meta ) {
  auto fn = PD_INFER_META(phi::UnchangedInferMeta);
  fn(infer_meta);
}

phi::DataType SinhOp::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: SinhOp";
  


  return expected_kernel_dtype;
}

OpInfoTuple Sinh_Op::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("x", "paddle::dialect::DenseTensorType", false, false, false, true) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = {  };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("out", "paddle::dialect::DenseTensorType", false, false) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("UnchangedInferMeta", {"x"}, "sinh", {"x"}, {}, {}, {{"out", "x"}}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "sinh");
}

void Sinh_Op::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_) {
  VLOG(4) << "Start build Sinh_Op";



  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);
  paddle::dialect::IrTensor dense_out;
  paddle::dialect::IrMetaTensor meta_out(&dense_out);

  phi::UnchangedInferMeta(meta_x, &meta_out);

  std::vector<pir::Type> argument_outputs;
  pir::Type out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_out.dtype()), dense_out.dims(), dense_out.layout(), dense_out.lod(), dense_out.offset());
  argument_outputs.push_back(out_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void Sinh_Op::VerifySig() {
  VLOG(4) << "Start Verifying inputs, outputs and attributes for: Sinh_Op.";
  VLOG(4) << "Verifying inputs:";
  {
  auto input_size = num_operands();
  IR_ENFORCE(input_size == 1u,
                    "The size %d of inputs must be equal to 1.", input_size);
  IR_ENFORCE((*this)->operand_source(0).type().isa<paddle::dialect::DenseTensorType>(),
                  "Type validation failed for the 0th input, got %s.", (*this)->operand_source(0).type());
  }
  VLOG(4) << "Verifying attributes:";
  {
  // Attributes num is 0, not need to check attributes type.
  }
  VLOG(4) << "Verifying outputs:";
  {
  auto output_size = num_results();
  IR_ENFORCE(output_size == 1u,
                    "The size %d of outputs must be equal to 1.", output_size);
  IR_ENFORCE((*this)->result(0).type().isa<paddle::dialect::DenseTensorType>(),
                 "Type validation failed for the 0th output.");
  }
  VLOG(4) << "End Verifying for: Sinh_Op.";
}

void Sinh_Op::InferMeta( phi::InferMetaContext *infer_meta ) {
  auto fn = PD_INFER_META(phi::UnchangedInferMeta);
  fn(infer_meta);
}

phi::DataType Sinh_Op::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: Sinh_Op";
  


  return expected_kernel_dtype;
}

OpInfoTuple SlogdetOp::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("x", "paddle::dialect::DenseTensorType", false, false, false, true) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = {  };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("out", "paddle::dialect::DenseTensorType", false, false) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("UnchangedInferMeta", {"x"}, "slogdet", {"x"}, {}, {}, {}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "slogdet");
}

void SlogdetOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_) {
  VLOG(4) << "Start build SlogdetOp";



  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);
  paddle::dialect::IrTensor dense_out;
  paddle::dialect::IrMetaTensor meta_out(&dense_out);

  phi::UnchangedInferMeta(meta_x, &meta_out);

  std::vector<pir::Type> argument_outputs;
  pir::Type out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_out.dtype()), dense_out.dims(), dense_out.layout(), dense_out.lod(), dense_out.offset());
  argument_outputs.push_back(out_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void SlogdetOp::VerifySig() {
  VLOG(4) << "Start Verifying inputs, outputs and attributes for: SlogdetOp.";
  VLOG(4) << "Verifying inputs:";
  {
  auto input_size = num_operands();
  IR_ENFORCE(input_size == 1u,
                    "The size %d of inputs must be equal to 1.", input_size);
  IR_ENFORCE((*this)->operand_source(0).type().isa<paddle::dialect::DenseTensorType>(),
                  "Type validation failed for the 0th input, got %s.", (*this)->operand_source(0).type());
  }
  VLOG(4) << "Verifying attributes:";
  {
  // Attributes num is 0, not need to check attributes type.
  }
  VLOG(4) << "Verifying outputs:";
  {
  auto output_size = num_results();
  IR_ENFORCE(output_size == 1u,
                    "The size %d of outputs must be equal to 1.", output_size);
  IR_ENFORCE((*this)->result(0).type().isa<paddle::dialect::DenseTensorType>(),
                 "Type validation failed for the 0th output.");
  }
  VLOG(4) << "End Verifying for: SlogdetOp.";
}

void SlogdetOp::InferMeta( phi::InferMetaContext *infer_meta ) {
  auto fn = PD_INFER_META(phi::UnchangedInferMeta);
  fn(infer_meta);
}

phi::DataType SlogdetOp::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: SlogdetOp";
  


  return expected_kernel_dtype;
}

const char *SoftplusOp::attributes_name[2] = { "beta", "threshold" };

OpInfoTuple SoftplusOp::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("x", "paddle::dialect::DenseTensorType", false, false, false, true) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = { paddle::dialect::OpAttributeInfo("beta", "pir::FloatAttribute", ""), paddle::dialect::OpAttributeInfo("threshold", "pir::FloatAttribute", "") };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("out", "paddle::dialect::DenseTensorType", false, false) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("UnchangedInferMeta", {"x"}, "softplus", {"x", "beta", "threshold"}, {}, {}, {}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "softplus");
}

void SoftplusOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, float beta, float threshold) {
  VLOG(4) << "Start build SoftplusOp";



  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_beta = pir::FloatAttribute::get(pir::IrContext::Instance(), beta);
  argument.AddAttribute("beta", attr_beta);
  pir::Attribute attr_threshold = pir::FloatAttribute::get(pir::IrContext::Instance(), threshold);
  argument.AddAttribute("threshold", attr_threshold);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);
  paddle::dialect::IrTensor dense_out;
  paddle::dialect::IrMetaTensor meta_out(&dense_out);

  phi::UnchangedInferMeta(meta_x, &meta_out);

  std::vector<pir::Type> argument_outputs;
  pir::Type out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_out.dtype()), dense_out.dims(), dense_out.layout(), dense_out.lod(), dense_out.offset());
  argument_outputs.push_back(out_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void SoftplusOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, pir::AttributeMap attributes) {
  VLOG(4) << "Start build SoftplusOp";


  IR_ENFORCE(
      attributes.find("beta") != attributes.end(),
          "'beta' Attribute is expected for SoftplusOp. ");
  float beta = attributes.at("beta").dyn_cast<pir::FloatAttribute>().data();

  IR_ENFORCE(
      attributes.find("threshold") != attributes.end(),
          "'threshold' Attribute is expected for SoftplusOp. ");
  float threshold = attributes.at("threshold").dyn_cast<pir::FloatAttribute>().data();


  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_beta = pir::FloatAttribute::get(pir::IrContext::Instance(), beta);
  argument.AddAttribute("beta", attr_beta);
  pir::Attribute attr_threshold = pir::FloatAttribute::get(pir::IrContext::Instance(), threshold);
  argument.AddAttribute("threshold", attr_threshold);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);
  paddle::dialect::IrTensor dense_out;
  paddle::dialect::IrMetaTensor meta_out(&dense_out);

  phi::UnchangedInferMeta(meta_x, &meta_out);

  std::vector<pir::Type> argument_outputs;
  pir::Type out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_out.dtype()), dense_out.dims(), dense_out.layout(), dense_out.lod(), dense_out.offset());
  argument_outputs.push_back(out_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void SoftplusOp::VerifySig() {
  VLOG(4) << "Start Verifying inputs, outputs and attributes for: SoftplusOp.";
  VLOG(4) << "Verifying inputs:";
  {
  auto input_size = num_operands();
  IR_ENFORCE(input_size == 1u,
                    "The size %d of inputs must be equal to 1.", input_size);
  IR_ENFORCE((*this)->operand_source(0).type().isa<paddle::dialect::DenseTensorType>(),
                  "Type validation failed for the 0th input, got %s.", (*this)->operand_source(0).type());
  }
  VLOG(4) << "Verifying attributes:";
  {
  auto& attributes = this->attributes();
  IR_ENFORCE(attributes.count("beta")>0,
                 "beta does not exist.");
  IR_ENFORCE(attributes.at("beta").isa<pir::FloatAttribute>(),
                 "Type of attribute: beta is not pir::FloatAttribute.");

  IR_ENFORCE(attributes.count("threshold")>0,
                 "threshold does not exist.");
  IR_ENFORCE(attributes.at("threshold").isa<pir::FloatAttribute>(),
                 "Type of attribute: threshold is not pir::FloatAttribute.");

  }
  VLOG(4) << "Verifying outputs:";
  {
  auto output_size = num_results();
  IR_ENFORCE(output_size == 1u,
                    "The size %d of outputs must be equal to 1.", output_size);
  IR_ENFORCE((*this)->result(0).type().isa<paddle::dialect::DenseTensorType>(),
                 "Type validation failed for the 0th output.");
  }
  VLOG(4) << "End Verifying for: SoftplusOp.";
}

void SoftplusOp::InferMeta( phi::InferMetaContext *infer_meta ) {
  auto fn = PD_INFER_META(phi::UnchangedInferMeta);
  fn(infer_meta);
}

phi::DataType SoftplusOp::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: SoftplusOp";
  


  return expected_kernel_dtype;
}

const char *SoftshrinkOp::attributes_name[1] = { "threshold" };

OpInfoTuple SoftshrinkOp::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("x", "paddle::dialect::DenseTensorType", false, false, false, true) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = { paddle::dialect::OpAttributeInfo("threshold", "pir::FloatAttribute", "") };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("out", "paddle::dialect::DenseTensorType", false, false) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("UnchangedInferMeta", {"x"}, "softshrink", {"x", "threshold"}, {}, {}, {}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "softshrink");
}

void SoftshrinkOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, float threshold) {
  VLOG(4) << "Start build SoftshrinkOp";



  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_threshold = pir::FloatAttribute::get(pir::IrContext::Instance(), threshold);
  argument.AddAttribute("threshold", attr_threshold);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);
  paddle::dialect::IrTensor dense_out;
  paddle::dialect::IrMetaTensor meta_out(&dense_out);

  phi::UnchangedInferMeta(meta_x, &meta_out);

  std::vector<pir::Type> argument_outputs;
  pir::Type out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_out.dtype()), dense_out.dims(), dense_out.layout(), dense_out.lod(), dense_out.offset());
  argument_outputs.push_back(out_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void SoftshrinkOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, pir::AttributeMap attributes) {
  VLOG(4) << "Start build SoftshrinkOp";


  IR_ENFORCE(
      attributes.find("threshold") != attributes.end(),
          "'threshold' Attribute is expected for SoftshrinkOp. ");
  float threshold = attributes.at("threshold").dyn_cast<pir::FloatAttribute>().data();


  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_threshold = pir::FloatAttribute::get(pir::IrContext::Instance(), threshold);
  argument.AddAttribute("threshold", attr_threshold);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);
  paddle::dialect::IrTensor dense_out;
  paddle::dialect::IrMetaTensor meta_out(&dense_out);

  phi::UnchangedInferMeta(meta_x, &meta_out);

  std::vector<pir::Type> argument_outputs;
  pir::Type out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_out.dtype()), dense_out.dims(), dense_out.layout(), dense_out.lod(), dense_out.offset());
  argument_outputs.push_back(out_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void SoftshrinkOp::VerifySig() {
  VLOG(4) << "Start Verifying inputs, outputs and attributes for: SoftshrinkOp.";
  VLOG(4) << "Verifying inputs:";
  {
  auto input_size = num_operands();
  IR_ENFORCE(input_size == 1u,
                    "The size %d of inputs must be equal to 1.", input_size);
  IR_ENFORCE((*this)->operand_source(0).type().isa<paddle::dialect::DenseTensorType>(),
                  "Type validation failed for the 0th input, got %s.", (*this)->operand_source(0).type());
  }
  VLOG(4) << "Verifying attributes:";
  {
  auto& attributes = this->attributes();
  IR_ENFORCE(attributes.count("threshold")>0,
                 "threshold does not exist.");
  IR_ENFORCE(attributes.at("threshold").isa<pir::FloatAttribute>(),
                 "Type of attribute: threshold is not pir::FloatAttribute.");

  }
  VLOG(4) << "Verifying outputs:";
  {
  auto output_size = num_results();
  IR_ENFORCE(output_size == 1u,
                    "The size %d of outputs must be equal to 1.", output_size);
  IR_ENFORCE((*this)->result(0).type().isa<paddle::dialect::DenseTensorType>(),
                 "Type validation failed for the 0th output.");
  }
  VLOG(4) << "End Verifying for: SoftshrinkOp.";
}

void SoftshrinkOp::InferMeta( phi::InferMetaContext *infer_meta ) {
  auto fn = PD_INFER_META(phi::UnchangedInferMeta);
  fn(infer_meta);
}

phi::DataType SoftshrinkOp::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: SoftshrinkOp";
  


  return expected_kernel_dtype;
}

OpInfoTuple SoftsignOp::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("x", "paddle::dialect::DenseTensorType", false, false, false, true) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = {  };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("out", "paddle::dialect::DenseTensorType", false, false) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("UnchangedInferMeta", {"x"}, "softsign", {"x"}, {}, {}, {}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "softsign");
}

void SoftsignOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_) {
  VLOG(4) << "Start build SoftsignOp";



  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);
  paddle::dialect::IrTensor dense_out;
  paddle::dialect::IrMetaTensor meta_out(&dense_out);

  phi::UnchangedInferMeta(meta_x, &meta_out);

  std::vector<pir::Type> argument_outputs;
  pir::Type out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_out.dtype()), dense_out.dims(), dense_out.layout(), dense_out.lod(), dense_out.offset());
  argument_outputs.push_back(out_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void SoftsignOp::VerifySig() {
  VLOG(4) << "Start Verifying inputs, outputs and attributes for: SoftsignOp.";
  VLOG(4) << "Verifying inputs:";
  {
  auto input_size = num_operands();
  IR_ENFORCE(input_size == 1u,
                    "The size %d of inputs must be equal to 1.", input_size);
  IR_ENFORCE((*this)->operand_source(0).type().isa<paddle::dialect::DenseTensorType>(),
                  "Type validation failed for the 0th input, got %s.", (*this)->operand_source(0).type());
  }
  VLOG(4) << "Verifying attributes:";
  {
  // Attributes num is 0, not need to check attributes type.
  }
  VLOG(4) << "Verifying outputs:";
  {
  auto output_size = num_results();
  IR_ENFORCE(output_size == 1u,
                    "The size %d of outputs must be equal to 1.", output_size);
  IR_ENFORCE((*this)->result(0).type().isa<paddle::dialect::DenseTensorType>(),
                 "Type validation failed for the 0th output.");
  }
  VLOG(4) << "End Verifying for: SoftsignOp.";
}

void SoftsignOp::InferMeta( phi::InferMetaContext *infer_meta ) {
  auto fn = PD_INFER_META(phi::UnchangedInferMeta);
  fn(infer_meta);
}

phi::DataType SoftsignOp::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: SoftsignOp";
  


  return expected_kernel_dtype;
}

OpInfoTuple SolveOp::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("x", "paddle::dialect::DenseTensorType", false, false, false, true), paddle::dialect::OpInputInfo("y", "paddle::dialect::DenseTensorType", false, false, false, true) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = {  };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("out", "paddle::dialect::DenseTensorType", false, false) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("SolveInferMeta", {"x", "y"}, "solve", {"x", "y"}, {"x"}, {}, {}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "solve");
}

void SolveOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, pir::Value y_) {
  VLOG(4) << "Start build SolveOp";



  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_, y_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;
  paddle::dialect::DenseTensorType y = y_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)y;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);

  VLOG(4) << "Builder construction  dense_y";
  paddle::dialect::IrTensor ir_tensor_y(paddle::dialect::TransToPhiDataType(y.dtype()),
                                                      y.dims(),
                                                      y.data_layout(),
                                                      y.lod(),
                                                      y.offset());
  VLOG(4) << "Builder construction  meta_y";
  paddle::dialect::IrMetaTensor meta_y(&ir_tensor_y);
  paddle::dialect::IrTensor dense_out;
  paddle::dialect::IrMetaTensor meta_out(&dense_out);

  phi::SolveInferMeta(meta_x, meta_y, &meta_out);

  std::vector<pir::Type> argument_outputs;
  pir::Type out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_out.dtype()), dense_out.dims(), dense_out.layout(), dense_out.lod(), dense_out.offset());
  argument_outputs.push_back(out_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void SolveOp::VerifySig() {
  VLOG(4) << "Start Verifying inputs, outputs and attributes for: SolveOp.";
  VLOG(4) << "Verifying inputs:";
  {
  auto input_size = num_operands();
  IR_ENFORCE(input_size == 2u,
                    "The size %d of inputs must be equal to 2.", input_size);
  IR_ENFORCE((*this)->operand_source(0).type().isa<paddle::dialect::DenseTensorType>(),
                  "Type validation failed for the 0th input, got %s.", (*this)->operand_source(0).type());
  IR_ENFORCE((*this)->operand_source(1).type().isa<paddle::dialect::DenseTensorType>(),
                  "Type validation failed for the 1th input, got %s.", (*this)->operand_source(1).type());
  }
  VLOG(4) << "Verifying attributes:";
  {
  // Attributes num is 0, not need to check attributes type.
  }
  VLOG(4) << "Verifying outputs:";
  {
  auto output_size = num_results();
  IR_ENFORCE(output_size == 1u,
                    "The size %d of outputs must be equal to 1.", output_size);
  IR_ENFORCE((*this)->result(0).type().isa<paddle::dialect::DenseTensorType>(),
                 "Type validation failed for the 0th output.");
  }
  VLOG(4) << "End Verifying for: SolveOp.";
}

void SolveOp::InferMeta( phi::InferMetaContext *infer_meta ) {
  auto fn = PD_INFER_META(phi::SolveInferMeta);
  fn(infer_meta);
}

phi::DataType SolveOp::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: SolveOp";
  


  return expected_kernel_dtype;
}

const char *SpectralNormOp::attributes_name[3] = { "dim", "power_iters", "eps" };

OpInfoTuple SpectralNormOp::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("weight", "paddle::dialect::DenseTensorType", false, false, false, true), paddle::dialect::OpInputInfo("u", "paddle::dialect::DenseTensorType", false, false, false, false), paddle::dialect::OpInputInfo("v", "paddle::dialect::DenseTensorType", false, false, false, false) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = { paddle::dialect::OpAttributeInfo("dim", "pir::Int32Attribute", ""), paddle::dialect::OpAttributeInfo("power_iters", "pir::Int32Attribute", ""), paddle::dialect::OpAttributeInfo("eps", "pir::FloatAttribute", "") };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("out", "paddle::dialect::DenseTensorType", false, false) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("SpectralNormInferMeta", {"weight", "u", "v", "dim", "power_iters", "eps"}, "spectral_norm", {"weight", "u", "v", "dim", "power_iters", "eps"}, {"weight"}, {}, {}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "spectral_norm");
}

void SpectralNormOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value weight_, pir::Value u_, pir::Value v_, int dim, int power_iters, float eps) {
  VLOG(4) << "Start build SpectralNormOp";



  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {weight_, u_, v_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_dim = pir::Int32Attribute::get(pir::IrContext::Instance(), dim);
  argument.AddAttribute("dim", attr_dim);
  pir::Attribute attr_power_iters = pir::Int32Attribute::get(pir::IrContext::Instance(), power_iters);
  argument.AddAttribute("power_iters", attr_power_iters);
  pir::Attribute attr_eps = pir::FloatAttribute::get(pir::IrContext::Instance(), eps);
  argument.AddAttribute("eps", attr_eps);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType weight = weight_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)weight;
  paddle::dialect::DenseTensorType u = u_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)u;
  paddle::dialect::DenseTensorType v = v_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)v;

  VLOG(4) << "Builder construction  dense_weight";
  paddle::dialect::IrTensor ir_tensor_weight(paddle::dialect::TransToPhiDataType(weight.dtype()),
                                                      weight.dims(),
                                                      weight.data_layout(),
                                                      weight.lod(),
                                                      weight.offset());
  VLOG(4) << "Builder construction  meta_weight";
  paddle::dialect::IrMetaTensor meta_weight(&ir_tensor_weight);

  VLOG(4) << "Builder construction  dense_u";
  paddle::dialect::IrTensor ir_tensor_u(paddle::dialect::TransToPhiDataType(u.dtype()),
                                                      u.dims(),
                                                      u.data_layout(),
                                                      u.lod(),
                                                      u.offset());
  VLOG(4) << "Builder construction  meta_u";
  paddle::dialect::IrMetaTensor meta_u(&ir_tensor_u);

  VLOG(4) << "Builder construction  dense_v";
  paddle::dialect::IrTensor ir_tensor_v(paddle::dialect::TransToPhiDataType(v.dtype()),
                                                      v.dims(),
                                                      v.data_layout(),
                                                      v.lod(),
                                                      v.offset());
  VLOG(4) << "Builder construction  meta_v";
  paddle::dialect::IrMetaTensor meta_v(&ir_tensor_v);
  paddle::dialect::IrTensor dense_out;
  paddle::dialect::IrMetaTensor meta_out(&dense_out);

  phi::SpectralNormInferMeta(meta_weight, meta_u, meta_v, dim, power_iters, eps, &meta_out);

  std::vector<pir::Type> argument_outputs;
  pir::Type out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_out.dtype()), dense_out.dims(), dense_out.layout(), dense_out.lod(), dense_out.offset());
  argument_outputs.push_back(out_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void SpectralNormOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value weight_, pir::Value u_, pir::Value v_, pir::AttributeMap attributes) {
  VLOG(4) << "Start build SpectralNormOp";


  IR_ENFORCE(
      attributes.find("dim") != attributes.end(),
          "'dim' Attribute is expected for SpectralNormOp. ");
  int dim = attributes.at("dim").dyn_cast<pir::Int32Attribute>().data();

  IR_ENFORCE(
      attributes.find("power_iters") != attributes.end(),
          "'power_iters' Attribute is expected for SpectralNormOp. ");
  int power_iters = attributes.at("power_iters").dyn_cast<pir::Int32Attribute>().data();

  IR_ENFORCE(
      attributes.find("eps") != attributes.end(),
          "'eps' Attribute is expected for SpectralNormOp. ");
  float eps = attributes.at("eps").dyn_cast<pir::FloatAttribute>().data();


  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {weight_, u_, v_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_dim = pir::Int32Attribute::get(pir::IrContext::Instance(), dim);
  argument.AddAttribute("dim", attr_dim);
  pir::Attribute attr_power_iters = pir::Int32Attribute::get(pir::IrContext::Instance(), power_iters);
  argument.AddAttribute("power_iters", attr_power_iters);
  pir::Attribute attr_eps = pir::FloatAttribute::get(pir::IrContext::Instance(), eps);
  argument.AddAttribute("eps", attr_eps);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType weight = weight_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)weight;
  paddle::dialect::DenseTensorType u = u_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)u;
  paddle::dialect::DenseTensorType v = v_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)v;

  VLOG(4) << "Builder construction  dense_weight";
  paddle::dialect::IrTensor ir_tensor_weight(paddle::dialect::TransToPhiDataType(weight.dtype()),
                                                      weight.dims(),
                                                      weight.data_layout(),
                                                      weight.lod(),
                                                      weight.offset());
  VLOG(4) << "Builder construction  meta_weight";
  paddle::dialect::IrMetaTensor meta_weight(&ir_tensor_weight);

  VLOG(4) << "Builder construction  dense_u";
  paddle::dialect::IrTensor ir_tensor_u(paddle::dialect::TransToPhiDataType(u.dtype()),
                                                      u.dims(),
                                                      u.data_layout(),
                                                      u.lod(),
                                                      u.offset());
  VLOG(4) << "Builder construction  meta_u";
  paddle::dialect::IrMetaTensor meta_u(&ir_tensor_u);

  VLOG(4) << "Builder construction  dense_v";
  paddle::dialect::IrTensor ir_tensor_v(paddle::dialect::TransToPhiDataType(v.dtype()),
                                                      v.dims(),
                                                      v.data_layout(),
                                                      v.lod(),
                                                      v.offset());
  VLOG(4) << "Builder construction  meta_v";
  paddle::dialect::IrMetaTensor meta_v(&ir_tensor_v);
  paddle::dialect::IrTensor dense_out;
  paddle::dialect::IrMetaTensor meta_out(&dense_out);

  phi::SpectralNormInferMeta(meta_weight, meta_u, meta_v, dim, power_iters, eps, &meta_out);

  std::vector<pir::Type> argument_outputs;
  pir::Type out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_out.dtype()), dense_out.dims(), dense_out.layout(), dense_out.lod(), dense_out.offset());
  argument_outputs.push_back(out_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void SpectralNormOp::VerifySig() {
  VLOG(4) << "Start Verifying inputs, outputs and attributes for: SpectralNormOp.";
  VLOG(4) << "Verifying inputs:";
  {
  auto input_size = num_operands();
  IR_ENFORCE(input_size == 3u,
                    "The size %d of inputs must be equal to 3.", input_size);
  IR_ENFORCE((*this)->operand_source(0).type().isa<paddle::dialect::DenseTensorType>(),
                  "Type validation failed for the 0th input, got %s.", (*this)->operand_source(0).type());
  IR_ENFORCE((*this)->operand_source(1).type().isa<paddle::dialect::DenseTensorType>(),
                  "Type validation failed for the 1th input, got %s.", (*this)->operand_source(1).type());
  IR_ENFORCE((*this)->operand_source(2).type().isa<paddle::dialect::DenseTensorType>(),
                  "Type validation failed for the 2th input, got %s.", (*this)->operand_source(2).type());
  }
  VLOG(4) << "Verifying attributes:";
  {
  auto& attributes = this->attributes();
  IR_ENFORCE(attributes.count("dim")>0,
                 "dim does not exist.");
  IR_ENFORCE(attributes.at("dim").isa<pir::Int32Attribute>(),
                 "Type of attribute: dim is not pir::Int32Attribute.");

  IR_ENFORCE(attributes.count("power_iters")>0,
                 "power_iters does not exist.");
  IR_ENFORCE(attributes.at("power_iters").isa<pir::Int32Attribute>(),
                 "Type of attribute: power_iters is not pir::Int32Attribute.");

  IR_ENFORCE(attributes.count("eps")>0,
                 "eps does not exist.");
  IR_ENFORCE(attributes.at("eps").isa<pir::FloatAttribute>(),
                 "Type of attribute: eps is not pir::FloatAttribute.");

  }
  VLOG(4) << "Verifying outputs:";
  {
  auto output_size = num_results();
  IR_ENFORCE(output_size == 1u,
                    "The size %d of outputs must be equal to 1.", output_size);
  IR_ENFORCE((*this)->result(0).type().isa<paddle::dialect::DenseTensorType>(),
                 "Type validation failed for the 0th output.");
  }
  VLOG(4) << "End Verifying for: SpectralNormOp.";
}

void SpectralNormOp::InferMeta( phi::InferMetaContext *infer_meta ) {
  auto fn = PD_INFER_META(phi::SpectralNormInferMeta);
  fn(infer_meta);
}

phi::DataType SpectralNormOp::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: SpectralNormOp";
  


  return expected_kernel_dtype;
}

OpInfoTuple SqrtOp::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("x", "paddle::dialect::DenseTensorType", false, false, false, true) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = {  };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("out", "paddle::dialect::DenseTensorType", false, false) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("UnchangedInferMeta", {"x"}, "sqrt", {"x"}, {}, {}, {}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "sqrt");
}

void SqrtOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_) {
  VLOG(4) << "Start build SqrtOp";



  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);
  paddle::dialect::IrTensor dense_out;
  paddle::dialect::IrMetaTensor meta_out(&dense_out);

  phi::UnchangedInferMeta(meta_x, &meta_out);

  std::vector<pir::Type> argument_outputs;
  pir::Type out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_out.dtype()), dense_out.dims(), dense_out.layout(), dense_out.lod(), dense_out.offset());
  argument_outputs.push_back(out_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void SqrtOp::VerifySig() {
  VLOG(4) << "Start Verifying inputs, outputs and attributes for: SqrtOp.";
  VLOG(4) << "Verifying inputs:";
  {
  auto input_size = num_operands();
  IR_ENFORCE(input_size == 1u,
                    "The size %d of inputs must be equal to 1.", input_size);
  IR_ENFORCE((*this)->operand_source(0).type().isa<paddle::dialect::DenseTensorType>(),
                  "Type validation failed for the 0th input, got %s.", (*this)->operand_source(0).type());
  }
  VLOG(4) << "Verifying attributes:";
  {
  // Attributes num is 0, not need to check attributes type.
  }
  VLOG(4) << "Verifying outputs:";
  {
  auto output_size = num_results();
  IR_ENFORCE(output_size == 1u,
                    "The size %d of outputs must be equal to 1.", output_size);
  IR_ENFORCE((*this)->result(0).type().isa<paddle::dialect::DenseTensorType>(),
                 "Type validation failed for the 0th output.");
  }
  VLOG(4) << "End Verifying for: SqrtOp.";
}

void SqrtOp::InferMeta( phi::InferMetaContext *infer_meta ) {
  auto fn = PD_INFER_META(phi::UnchangedInferMeta);
  fn(infer_meta);
}

phi::DataType SqrtOp::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: SqrtOp";
  


  return expected_kernel_dtype;
}

OpInfoTuple SqrtSrOp::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("x", "paddle::dialect::SelectedRowsType", false, false, false, true) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = {  };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("out", "paddle::dialect::SelectedRowsType", false, false) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("UnchangedInferMeta", {"x"}, "sqrt_sr", {"x"}, {}, {}, {}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "sqrt");
}

void SqrtSrOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_) {
  VLOG(4) << "Start build SqrtSrOp";



  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::SelectedRowsType x = x_.type().dyn_cast<paddle::dialect::SelectedRowsType>(); (void)x;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);
  paddle::dialect::IrSelectedRows dense_out;
  paddle::dialect::IrMetaTensor meta_out(&dense_out);

  phi::UnchangedInferMeta(meta_x, &meta_out);

  std::vector<pir::Type> argument_outputs;
  pir::Type out_dense_tensor_type = paddle::dialect::SelectedRowsType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_out.dtype()), dense_out.dims(), dense_out.layout(), dense_out.lod(), dense_out.offset());
  argument_outputs.push_back(out_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void SqrtSrOp::VerifySig() {
  VLOG(4) << "Start Verifying inputs, outputs and attributes for: SqrtSrOp.";
  VLOG(4) << "Verifying inputs:";
  {
  auto input_size = num_operands();
  IR_ENFORCE(input_size == 1u,
                    "The size %d of inputs must be equal to 1.", input_size);
  IR_ENFORCE((*this)->operand_source(0).type().isa<paddle::dialect::SelectedRowsType>(),
                  "Type validation failed for the 0th input, got %s.", (*this)->operand_source(0).type());
  }
  VLOG(4) << "Verifying attributes:";
  {
  // Attributes num is 0, not need to check attributes type.
  }
  VLOG(4) << "Verifying outputs:";
  {
  auto output_size = num_results();
  IR_ENFORCE(output_size == 1u,
                    "The size %d of outputs must be equal to 1.", output_size);
  IR_ENFORCE((*this)->result(0).type().isa<paddle::dialect::SelectedRowsType>(),
                 "Type validation failed for the 0th output.");
  }
  VLOG(4) << "End Verifying for: SqrtSrOp.";
}

void SqrtSrOp::InferMeta( phi::InferMetaContext *infer_meta ) {
  auto fn = PD_INFER_META(phi::UnchangedInferMeta);
  fn(infer_meta);
}

phi::DataType SqrtSrOp::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: SqrtSrOp";
  


  return expected_kernel_dtype;
}

OpInfoTuple Sqrt_Op::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("x", "paddle::dialect::DenseTensorType", false, false, false, true) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = {  };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("out", "paddle::dialect::DenseTensorType", false, false) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("UnchangedInferMeta", {"x"}, "sqrt", {"x"}, {}, {}, {{"out", "x"}}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "sqrt");
}

void Sqrt_Op::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_) {
  VLOG(4) << "Start build Sqrt_Op";



  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);
  paddle::dialect::IrTensor dense_out;
  paddle::dialect::IrMetaTensor meta_out(&dense_out);

  phi::UnchangedInferMeta(meta_x, &meta_out);

  std::vector<pir::Type> argument_outputs;
  pir::Type out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_out.dtype()), dense_out.dims(), dense_out.layout(), dense_out.lod(), dense_out.offset());
  argument_outputs.push_back(out_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void Sqrt_Op::VerifySig() {
  VLOG(4) << "Start Verifying inputs, outputs and attributes for: Sqrt_Op.";
  VLOG(4) << "Verifying inputs:";
  {
  auto input_size = num_operands();
  IR_ENFORCE(input_size == 1u,
                    "The size %d of inputs must be equal to 1.", input_size);
  IR_ENFORCE((*this)->operand_source(0).type().isa<paddle::dialect::DenseTensorType>(),
                  "Type validation failed for the 0th input, got %s.", (*this)->operand_source(0).type());
  }
  VLOG(4) << "Verifying attributes:";
  {
  // Attributes num is 0, not need to check attributes type.
  }
  VLOG(4) << "Verifying outputs:";
  {
  auto output_size = num_results();
  IR_ENFORCE(output_size == 1u,
                    "The size %d of outputs must be equal to 1.", output_size);
  IR_ENFORCE((*this)->result(0).type().isa<paddle::dialect::DenseTensorType>(),
                 "Type validation failed for the 0th output.");
  }
  VLOG(4) << "End Verifying for: Sqrt_Op.";
}

void Sqrt_Op::InferMeta( phi::InferMetaContext *infer_meta ) {
  auto fn = PD_INFER_META(phi::UnchangedInferMeta);
  fn(infer_meta);
}

phi::DataType Sqrt_Op::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: Sqrt_Op";
  


  return expected_kernel_dtype;
}

OpInfoTuple SqrtSr_Op::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("x", "paddle::dialect::SelectedRowsType", false, false, false, true) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = {  };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("out", "paddle::dialect::SelectedRowsType", false, false) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("UnchangedInferMeta", {"x"}, "sqrt_sr", {"x"}, {}, {}, {{"out", "x"}}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "sqrt");
}

void SqrtSr_Op::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_) {
  VLOG(4) << "Start build SqrtSr_Op";



  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::SelectedRowsType x = x_.type().dyn_cast<paddle::dialect::SelectedRowsType>(); (void)x;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);
  paddle::dialect::IrSelectedRows dense_out;
  paddle::dialect::IrMetaTensor meta_out(&dense_out);

  phi::UnchangedInferMeta(meta_x, &meta_out);

  std::vector<pir::Type> argument_outputs;
  pir::Type out_dense_tensor_type = paddle::dialect::SelectedRowsType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_out.dtype()), dense_out.dims(), dense_out.layout(), dense_out.lod(), dense_out.offset());
  argument_outputs.push_back(out_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void SqrtSr_Op::VerifySig() {
  VLOG(4) << "Start Verifying inputs, outputs and attributes for: SqrtSr_Op.";
  VLOG(4) << "Verifying inputs:";
  {
  auto input_size = num_operands();
  IR_ENFORCE(input_size == 1u,
                    "The size %d of inputs must be equal to 1.", input_size);
  IR_ENFORCE((*this)->operand_source(0).type().isa<paddle::dialect::SelectedRowsType>(),
                  "Type validation failed for the 0th input, got %s.", (*this)->operand_source(0).type());
  }
  VLOG(4) << "Verifying attributes:";
  {
  // Attributes num is 0, not need to check attributes type.
  }
  VLOG(4) << "Verifying outputs:";
  {
  auto output_size = num_results();
  IR_ENFORCE(output_size == 1u,
                    "The size %d of outputs must be equal to 1.", output_size);
  IR_ENFORCE((*this)->result(0).type().isa<paddle::dialect::SelectedRowsType>(),
                 "Type validation failed for the 0th output.");
  }
  VLOG(4) << "End Verifying for: SqrtSr_Op.";
}

void SqrtSr_Op::InferMeta( phi::InferMetaContext *infer_meta ) {
  auto fn = PD_INFER_META(phi::UnchangedInferMeta);
  fn(infer_meta);
}

phi::DataType SqrtSr_Op::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: SqrtSr_Op";
  


  return expected_kernel_dtype;
}

OpInfoTuple SquareOp::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("x", "paddle::dialect::DenseTensorType", false, false, false, true) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = {  };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("out", "paddle::dialect::DenseTensorType", false, false) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("UnchangedInferMeta", {"x"}, "square", {"x"}, {}, {}, {}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "square");
}

void SquareOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_) {
  VLOG(4) << "Start build SquareOp";



  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);
  paddle::dialect::IrTensor dense_out;
  paddle::dialect::IrMetaTensor meta_out(&dense_out);

  phi::UnchangedInferMeta(meta_x, &meta_out);

  std::vector<pir::Type> argument_outputs;
  pir::Type out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_out.dtype()), dense_out.dims(), dense_out.layout(), dense_out.lod(), dense_out.offset());
  argument_outputs.push_back(out_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void SquareOp::VerifySig() {
  VLOG(4) << "Start Verifying inputs, outputs and attributes for: SquareOp.";
  VLOG(4) << "Verifying inputs:";
  {
  auto input_size = num_operands();
  IR_ENFORCE(input_size == 1u,
                    "The size %d of inputs must be equal to 1.", input_size);
  IR_ENFORCE((*this)->operand_source(0).type().isa<paddle::dialect::DenseTensorType>(),
                  "Type validation failed for the 0th input, got %s.", (*this)->operand_source(0).type());
  }
  VLOG(4) << "Verifying attributes:";
  {
  // Attributes num is 0, not need to check attributes type.
  }
  VLOG(4) << "Verifying outputs:";
  {
  auto output_size = num_results();
  IR_ENFORCE(output_size == 1u,
                    "The size %d of outputs must be equal to 1.", output_size);
  IR_ENFORCE((*this)->result(0).type().isa<paddle::dialect::DenseTensorType>(),
                 "Type validation failed for the 0th output.");
  }
  VLOG(4) << "End Verifying for: SquareOp.";
}

void SquareOp::InferMeta( phi::InferMetaContext *infer_meta ) {
  auto fn = PD_INFER_META(phi::UnchangedInferMeta);
  fn(infer_meta);
}

phi::DataType SquareOp::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: SquareOp";
  


  return expected_kernel_dtype;
}

OpInfoTuple SquareSrOp::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("x", "paddle::dialect::SelectedRowsType", false, false, false, true) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = {  };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("out", "paddle::dialect::SelectedRowsType", false, false) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("UnchangedInferMeta", {"x"}, "square_sr", {"x"}, {}, {}, {}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "square");
}

void SquareSrOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_) {
  VLOG(4) << "Start build SquareSrOp";



  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::SelectedRowsType x = x_.type().dyn_cast<paddle::dialect::SelectedRowsType>(); (void)x;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);
  paddle::dialect::IrSelectedRows dense_out;
  paddle::dialect::IrMetaTensor meta_out(&dense_out);

  phi::UnchangedInferMeta(meta_x, &meta_out);

  std::vector<pir::Type> argument_outputs;
  pir::Type out_dense_tensor_type = paddle::dialect::SelectedRowsType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_out.dtype()), dense_out.dims(), dense_out.layout(), dense_out.lod(), dense_out.offset());
  argument_outputs.push_back(out_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void SquareSrOp::VerifySig() {
  VLOG(4) << "Start Verifying inputs, outputs and attributes for: SquareSrOp.";
  VLOG(4) << "Verifying inputs:";
  {
  auto input_size = num_operands();
  IR_ENFORCE(input_size == 1u,
                    "The size %d of inputs must be equal to 1.", input_size);
  IR_ENFORCE((*this)->operand_source(0).type().isa<paddle::dialect::SelectedRowsType>(),
                  "Type validation failed for the 0th input, got %s.", (*this)->operand_source(0).type());
  }
  VLOG(4) << "Verifying attributes:";
  {
  // Attributes num is 0, not need to check attributes type.
  }
  VLOG(4) << "Verifying outputs:";
  {
  auto output_size = num_results();
  IR_ENFORCE(output_size == 1u,
                    "The size %d of outputs must be equal to 1.", output_size);
  IR_ENFORCE((*this)->result(0).type().isa<paddle::dialect::SelectedRowsType>(),
                 "Type validation failed for the 0th output.");
  }
  VLOG(4) << "End Verifying for: SquareSrOp.";
}

void SquareSrOp::InferMeta( phi::InferMetaContext *infer_meta ) {
  auto fn = PD_INFER_META(phi::UnchangedInferMeta);
  fn(infer_meta);
}

phi::DataType SquareSrOp::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: SquareSrOp";
  


  return expected_kernel_dtype;
}

OpInfoTuple SquaredL2NormOp::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("x", "paddle::dialect::DenseTensorType", false, false, false, true) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = {  };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("out", "paddle::dialect::DenseTensorType", false, false) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("SquaredL2NormInferMeta", {"x"}, "squared_l2_norm", {"x"}, {}, {}, {}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "squared_l2_norm");
}

void SquaredL2NormOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_) {
  VLOG(4) << "Start build SquaredL2NormOp";



  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);
  paddle::dialect::IrTensor dense_out;
  paddle::dialect::IrMetaTensor meta_out(&dense_out);

  phi::SquaredL2NormInferMeta(meta_x, &meta_out);

  std::vector<pir::Type> argument_outputs;
  pir::Type out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_out.dtype()), dense_out.dims(), dense_out.layout(), dense_out.lod(), dense_out.offset());
  argument_outputs.push_back(out_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void SquaredL2NormOp::VerifySig() {
  VLOG(4) << "Start Verifying inputs, outputs and attributes for: SquaredL2NormOp.";
  VLOG(4) << "Verifying inputs:";
  {
  auto input_size = num_operands();
  IR_ENFORCE(input_size == 1u,
                    "The size %d of inputs must be equal to 1.", input_size);
  IR_ENFORCE((*this)->operand_source(0).type().isa<paddle::dialect::DenseTensorType>(),
                  "Type validation failed for the 0th input, got %s.", (*this)->operand_source(0).type());
  }
  VLOG(4) << "Verifying attributes:";
  {
  // Attributes num is 0, not need to check attributes type.
  }
  VLOG(4) << "Verifying outputs:";
  {
  auto output_size = num_results();
  IR_ENFORCE(output_size == 1u,
                    "The size %d of outputs must be equal to 1.", output_size);
  IR_ENFORCE((*this)->result(0).type().isa<paddle::dialect::DenseTensorType>(),
                 "Type validation failed for the 0th output.");
  }
  VLOG(4) << "End Verifying for: SquaredL2NormOp.";
}

void SquaredL2NormOp::InferMeta( phi::InferMetaContext *infer_meta ) {
  auto fn = PD_INFER_META(phi::SquaredL2NormInferMeta);
  fn(infer_meta);
}

phi::DataType SquaredL2NormOp::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: SquaredL2NormOp";
  


  return expected_kernel_dtype;
}

OpInfoTuple SqueezeOp::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("x", "paddle::dialect::DenseTensorType", false, false, false, true), paddle::dialect::OpInputInfo("axis", "paddle::dialect::IntArrayAttribute", false, false, true, false) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = {  };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("out", "paddle::dialect::DenseTensorType", false, false), paddle::dialect::OpOutputInfo("xshape", "paddle::dialect::DenseTensorType", true, true) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("SqueezeWithXShapeInferMeta", {"x", "axis"}, "squeeze", {"x", "axis"}, {"x"}, {}, {}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "squeeze");
}

void SqueezeOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, const std::vector<int64_t>& axis) {
  VLOG(4) << "Start build SqueezeOp";


  // Generate int_array mutable attribute: axis
  paddle::dialect::FullIntArrayOp full_axis_op = builder.Build<paddle::dialect::FullIntArrayOp>(axis, phi::DataType::INT64, phi::CPUPlace());
  pir::OpResult axis_ = full_axis_op->result(0);
    
  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_, axis_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);
  paddle::dialect::IrTensor dense_out;
  paddle::dialect::IrMetaTensor meta_out(&dense_out);
  paddle::dialect::IrTensor dense_xshape;
  paddle::dialect::IrMetaTensor meta_xshape(&dense_xshape);

  phi::SqueezeWithXShapeInferMeta(meta_x, axis, &meta_out, &meta_xshape);

  std::vector<pir::Type> argument_outputs;
  pir::Type out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_out.dtype()), dense_out.dims(), dense_out.layout(), dense_out.lod(), dense_out.offset());
  argument_outputs.push_back(out_dense_tensor_type);

  pir::Type xshape_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_xshape.dtype()), dense_xshape.dims(), dense_xshape.layout(), dense_xshape.lod(), dense_xshape.offset());
  argument_outputs.push_back(xshape_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void SqueezeOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, pir::AttributeMap attributes) {
  VLOG(4) << "Start build SqueezeOp";


  IR_ENFORCE(
      attributes.find("axis") != attributes.end(),
          "'axis' Attribute is expected for SqueezeOp. ");
  std::vector<int64_t> axis = attributes.at("axis").dyn_cast<paddle::dialect::IntArrayAttribute>().data().GetData();

  // Generate int_array mutable attribute: axis
  paddle::dialect::FullIntArrayOp full_axis_op = builder.Build<paddle::dialect::FullIntArrayOp>(axis, phi::DataType::INT64, phi::CPUPlace());
  pir::OpResult axis_ = full_axis_op->result(0);
    
  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_, axis_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);
  paddle::dialect::IrTensor dense_out;
  paddle::dialect::IrMetaTensor meta_out(&dense_out);
  paddle::dialect::IrTensor dense_xshape;
  paddle::dialect::IrMetaTensor meta_xshape(&dense_xshape);

  phi::SqueezeWithXShapeInferMeta(meta_x, axis, &meta_out, &meta_xshape);

  std::vector<pir::Type> argument_outputs;
  pir::Type out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_out.dtype()), dense_out.dims(), dense_out.layout(), dense_out.lod(), dense_out.offset());
  argument_outputs.push_back(out_dense_tensor_type);

  pir::Type xshape_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_xshape.dtype()), dense_xshape.dims(), dense_xshape.layout(), dense_xshape.lod(), dense_xshape.offset());
  argument_outputs.push_back(xshape_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void SqueezeOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, pir::Value axis_) {
  VLOG(4) << "Start build SqueezeOp";



  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_, axis_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;
  phi::IntArray axis;
  if (axis_.dyn_cast<pir::OpResult>() && axis_.dyn_cast<pir::OpResult>().owner()->isa<paddle::dialect::FullIntArrayOp>()) {
    axis = std::move(phi::IntArray(paddle::dialect::GetInt64Vector(
                          axis_.dyn_cast<pir::OpResult>().owner()
                          ->dyn_cast<paddle::dialect::FullIntArrayOp>()
                          .attribute("value"))));
  } else if (axis_.type().isa<pir::VectorType>()) {
    size_t axis_size = axis_.type().dyn_cast<pir::VectorType>().size();
    axis = std::move(phi::IntArray(std::vector<int64_t>(axis_size, -1)));
    axis.SetFromTensor(true);
  } else if (axis_.type().isa<paddle::dialect::DenseTensorType>()) {
    common::DDim axis_dim = axis_.type().dyn_cast<paddle::dialect::DenseTensorType>().dims();
    size_t axis_size = common::product(axis_dim);
    if (common::contain_unknown_dim(axis_dim)) {
      axis_size = 1;
    }
    axis = std::move(phi::IntArray(std::vector<int64_t>(axis_size, -1)));
    axis.SetFromTensor(true);
  } else {
    PADDLE_THROW(phi::errors::Unimplemented("Only support VectorType or DenseTensorType"));
  }


  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);
  paddle::dialect::IrTensor dense_out;
  paddle::dialect::IrMetaTensor meta_out(&dense_out);
  paddle::dialect::IrTensor dense_xshape;
  paddle::dialect::IrMetaTensor meta_xshape(&dense_xshape);

  phi::SqueezeWithXShapeInferMeta(meta_x, axis, &meta_out, &meta_xshape);

  std::vector<pir::Type> argument_outputs;
  pir::Type out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_out.dtype()), dense_out.dims(), dense_out.layout(), dense_out.lod(), dense_out.offset());
  argument_outputs.push_back(out_dense_tensor_type);

  pir::Type xshape_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_xshape.dtype()), dense_xshape.dims(), dense_xshape.layout(), dense_xshape.lod(), dense_xshape.offset());
  argument_outputs.push_back(xshape_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void SqueezeOp::VerifySig() {
  VLOG(4) << "Start Verifying inputs, outputs and attributes for: SqueezeOp.";
  VLOG(4) << "Verifying inputs:";
  {
  auto input_size = num_operands();
  IR_ENFORCE(input_size == 2u,
                    "The size %d of inputs must be equal to 2.", input_size);
  IR_ENFORCE((*this)->operand_source(0).type().isa<paddle::dialect::DenseTensorType>(),
                  "Type validation failed for the 0th input, got %s.", (*this)->operand_source(0).type());
  if (auto vec_type = (*this)->operand_source(1).type().dyn_cast<pir::VectorType>()) {
      for (size_t i = 0; i < vec_type.size(); ++i) {
        IR_ENFORCE(vec_type[i].isa<paddle::dialect::DenseTensorType>(),
                       "Type validation failed for the 1th input, got %s.", (*this)->operand_source(1).type());
      }
  }
  else {
    IR_ENFORCE((*this)->operand_source(1).type().isa<paddle::dialect::DenseTensorType>(),
                   "Type validation failed for the 1th input, got %s.", (*this)->operand_source(1).type());
  }
  }
  VLOG(4) << "Verifying attributes:";
  {
  // Attributes num is 0, not need to check attributes type.
  }
  VLOG(4) << "Verifying outputs:";
  {
  auto output_size = num_results();
  IR_ENFORCE(output_size == 2u,
                    "The size %d of outputs must be equal to 2.", output_size);
  IR_ENFORCE((*this)->result(0).type().isa<paddle::dialect::DenseTensorType>(),
                 "Type validation failed for the 0th output.");
  if (auto output_1_type = (*this)->result(1).type()) {
    IR_ENFORCE(output_1_type.isa<paddle::dialect::DenseTensorType>(),
                   "Type validation failed for the 1th output.");
  }
  }
  VLOG(4) << "End Verifying for: SqueezeOp.";
}

void SqueezeOp::InferMeta( phi::InferMetaContext *infer_meta ) {
  auto fn = PD_INFER_META(phi::SqueezeWithXShapeInferMeta);
  fn(infer_meta);
}

phi::DataType SqueezeOp::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: SqueezeOp";
  


  return expected_kernel_dtype;
}

OpInfoTuple Squeeze_Op::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("x", "paddle::dialect::DenseTensorType", false, false, false, true), paddle::dialect::OpInputInfo("axis", "paddle::dialect::IntArrayAttribute", false, false, true, false) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = {  };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("out", "paddle::dialect::DenseTensorType", false, false), paddle::dialect::OpOutputInfo("xshape", "paddle::dialect::DenseTensorType", true, true) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("SqueezeWithXShapeInferMeta", {"x", "axis"}, "squeeze", {"x", "axis"}, {"x"}, {}, {{"out", "x"}}, {{"out", "x"}});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "squeeze");
}

void Squeeze_Op::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, const std::vector<int64_t>& axis) {
  VLOG(4) << "Start build Squeeze_Op";


  // Generate int_array mutable attribute: axis
  paddle::dialect::FullIntArrayOp full_axis_op = builder.Build<paddle::dialect::FullIntArrayOp>(axis, phi::DataType::INT64, phi::CPUPlace());
  pir::OpResult axis_ = full_axis_op->result(0);
    
  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_, axis_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);
  paddle::dialect::IrTensor dense_out;
  paddle::dialect::IrMetaTensor meta_out(&dense_out);
  paddle::dialect::IrTensor dense_xshape;
  paddle::dialect::IrMetaTensor meta_xshape(&dense_xshape);

  phi::SqueezeWithXShapeInferMeta(meta_x, axis, &meta_out, &meta_xshape);

  std::vector<pir::Type> argument_outputs;
  pir::Type out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_out.dtype()), dense_out.dims(), dense_out.layout(), dense_out.lod(), dense_out.offset());
  argument_outputs.push_back(out_dense_tensor_type);

  pir::Type xshape_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_xshape.dtype()), dense_xshape.dims(), dense_xshape.layout(), dense_xshape.lod(), dense_xshape.offset());
  argument_outputs.push_back(xshape_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void Squeeze_Op::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, pir::AttributeMap attributes) {
  VLOG(4) << "Start build Squeeze_Op";


  IR_ENFORCE(
      attributes.find("axis") != attributes.end(),
          "'axis' Attribute is expected for Squeeze_Op. ");
  std::vector<int64_t> axis = attributes.at("axis").dyn_cast<paddle::dialect::IntArrayAttribute>().data().GetData();

  // Generate int_array mutable attribute: axis
  paddle::dialect::FullIntArrayOp full_axis_op = builder.Build<paddle::dialect::FullIntArrayOp>(axis, phi::DataType::INT64, phi::CPUPlace());
  pir::OpResult axis_ = full_axis_op->result(0);
    
  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_, axis_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);
  paddle::dialect::IrTensor dense_out;
  paddle::dialect::IrMetaTensor meta_out(&dense_out);
  paddle::dialect::IrTensor dense_xshape;
  paddle::dialect::IrMetaTensor meta_xshape(&dense_xshape);

  phi::SqueezeWithXShapeInferMeta(meta_x, axis, &meta_out, &meta_xshape);

  std::vector<pir::Type> argument_outputs;
  pir::Type out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_out.dtype()), dense_out.dims(), dense_out.layout(), dense_out.lod(), dense_out.offset());
  argument_outputs.push_back(out_dense_tensor_type);

  pir::Type xshape_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_xshape.dtype()), dense_xshape.dims(), dense_xshape.layout(), dense_xshape.lod(), dense_xshape.offset());
  argument_outputs.push_back(xshape_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void Squeeze_Op::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, pir::Value axis_) {
  VLOG(4) << "Start build Squeeze_Op";



  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_, axis_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;
  phi::IntArray axis;
  if (axis_.dyn_cast<pir::OpResult>() && axis_.dyn_cast<pir::OpResult>().owner()->isa<paddle::dialect::FullIntArrayOp>()) {
    axis = std::move(phi::IntArray(paddle::dialect::GetInt64Vector(
                          axis_.dyn_cast<pir::OpResult>().owner()
                          ->dyn_cast<paddle::dialect::FullIntArrayOp>()
                          .attribute("value"))));
  } else if (axis_.type().isa<pir::VectorType>()) {
    size_t axis_size = axis_.type().dyn_cast<pir::VectorType>().size();
    axis = std::move(phi::IntArray(std::vector<int64_t>(axis_size, -1)));
    axis.SetFromTensor(true);
  } else if (axis_.type().isa<paddle::dialect::DenseTensorType>()) {
    common::DDim axis_dim = axis_.type().dyn_cast<paddle::dialect::DenseTensorType>().dims();
    size_t axis_size = common::product(axis_dim);
    if (common::contain_unknown_dim(axis_dim)) {
      axis_size = 1;
    }
    axis = std::move(phi::IntArray(std::vector<int64_t>(axis_size, -1)));
    axis.SetFromTensor(true);
  } else {
    PADDLE_THROW(phi::errors::Unimplemented("Only support VectorType or DenseTensorType"));
  }


  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);
  paddle::dialect::IrTensor dense_out;
  paddle::dialect::IrMetaTensor meta_out(&dense_out);
  paddle::dialect::IrTensor dense_xshape;
  paddle::dialect::IrMetaTensor meta_xshape(&dense_xshape);

  phi::SqueezeWithXShapeInferMeta(meta_x, axis, &meta_out, &meta_xshape);

  std::vector<pir::Type> argument_outputs;
  pir::Type out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_out.dtype()), dense_out.dims(), dense_out.layout(), dense_out.lod(), dense_out.offset());
  argument_outputs.push_back(out_dense_tensor_type);

  pir::Type xshape_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_xshape.dtype()), dense_xshape.dims(), dense_xshape.layout(), dense_xshape.lod(), dense_xshape.offset());
  argument_outputs.push_back(xshape_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void Squeeze_Op::VerifySig() {
  VLOG(4) << "Start Verifying inputs, outputs and attributes for: Squeeze_Op.";
  VLOG(4) << "Verifying inputs:";
  {
  auto input_size = num_operands();
  IR_ENFORCE(input_size == 2u,
                    "The size %d of inputs must be equal to 2.", input_size);
  IR_ENFORCE((*this)->operand_source(0).type().isa<paddle::dialect::DenseTensorType>(),
                  "Type validation failed for the 0th input, got %s.", (*this)->operand_source(0).type());
  if (auto vec_type = (*this)->operand_source(1).type().dyn_cast<pir::VectorType>()) {
      for (size_t i = 0; i < vec_type.size(); ++i) {
        IR_ENFORCE(vec_type[i].isa<paddle::dialect::DenseTensorType>(),
                       "Type validation failed for the 1th input, got %s.", (*this)->operand_source(1).type());
      }
  }
  else {
    IR_ENFORCE((*this)->operand_source(1).type().isa<paddle::dialect::DenseTensorType>(),
                   "Type validation failed for the 1th input, got %s.", (*this)->operand_source(1).type());
  }
  }
  VLOG(4) << "Verifying attributes:";
  {
  // Attributes num is 0, not need to check attributes type.
  }
  VLOG(4) << "Verifying outputs:";
  {
  auto output_size = num_results();
  IR_ENFORCE(output_size == 2u,
                    "The size %d of outputs must be equal to 2.", output_size);
  IR_ENFORCE((*this)->result(0).type().isa<paddle::dialect::DenseTensorType>(),
                 "Type validation failed for the 0th output.");
  if (auto output_1_type = (*this)->result(1).type()) {
    IR_ENFORCE(output_1_type.isa<paddle::dialect::DenseTensorType>(),
                   "Type validation failed for the 1th output.");
  }
  }
  VLOG(4) << "End Verifying for: Squeeze_Op.";
}

void Squeeze_Op::InferMeta( phi::InferMetaContext *infer_meta ) {
  auto fn = PD_INFER_META(phi::SqueezeWithXShapeInferMeta);
  fn(infer_meta);
}

phi::DataType Squeeze_Op::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: Squeeze_Op";
  


  return expected_kernel_dtype;
}

const char *StackOp::attributes_name[1] = { "axis" };

OpInfoTuple StackOp::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("x", "pir::VectorType<paddle::dialect::DenseTensorType>", false, false, false, true) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = { paddle::dialect::OpAttributeInfo("axis", "pir::Int32Attribute", "") };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("out", "paddle::dialect::DenseTensorType", false, false) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("StackInferMeta", {"x", "axis"}, "stack", {"x", "axis"}, {}, {}, {}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "stack");
}

void StackOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, int axis) {
  VLOG(4) << "Start build StackOp";



  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_axis = pir::Int32Attribute::get(pir::IrContext::Instance(), axis);
  argument.AddAttribute("axis", attr_axis);

  VLOG(4) << "Builder construction outputs";
  pir::VectorType x = x_.type().dyn_cast<pir::VectorType>(); (void)x;
  std::vector<paddle::dialect::IrTensor> vec_ir_tensor_x;
  for (size_t i=0; i < static_cast<size_t>(x.size()); i++) {
    vec_ir_tensor_x.push_back(paddle::dialect::IrTensor(paddle::dialect::TransToPhiDataType(x[i].dyn_cast<paddle::dialect::DenseTensorType>().dtype()),
                                                                     x[i].dyn_cast<paddle::dialect::DenseTensorType>().dims(),
                                                                     x[i].dyn_cast<paddle::dialect::DenseTensorType>().data_layout(),
                                                                     x[i].dyn_cast<paddle::dialect::DenseTensorType>().lod(),
                                                                     x[i].dyn_cast<paddle::dialect::DenseTensorType>().offset()));
  }
  std::vector<paddle::dialect::IrMetaTensor> vec_meta_x;
  for (size_t i=0; i < vec_ir_tensor_x.size(); i++) {
    vec_meta_x.push_back(paddle::dialect::IrMetaTensor(&vec_ir_tensor_x[i]));
  }

  std::vector<const phi::MetaTensor*> meta_x;
  for (size_t i=0; i < static_cast<size_t>(vec_meta_x.size()); i++) {
    meta_x.push_back(&vec_meta_x[i]);
  }
   paddle::dialect::IrTensor dense_out;
  paddle::dialect::IrMetaTensor meta_out(&dense_out);

  phi::StackInferMeta(meta_x, axis, &meta_out, phi::MetaConfig(false, false));

  std::vector<pir::Type> argument_outputs;
  pir::Type out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_out.dtype()), dense_out.dims(), dense_out.layout(), dense_out.lod(), dense_out.offset());
  argument_outputs.push_back(out_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void StackOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, pir::AttributeMap attributes) {
  VLOG(4) << "Start build StackOp";


  IR_ENFORCE(
      attributes.find("axis") != attributes.end(),
          "'axis' Attribute is expected for StackOp. ");
  int axis = attributes.at("axis").dyn_cast<pir::Int32Attribute>().data();


  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_axis = pir::Int32Attribute::get(pir::IrContext::Instance(), axis);
  argument.AddAttribute("axis", attr_axis);

  VLOG(4) << "Builder construction outputs";
  pir::VectorType x = x_.type().dyn_cast<pir::VectorType>(); (void)x;
  std::vector<paddle::dialect::IrTensor> vec_ir_tensor_x;
  for (size_t i=0; i < static_cast<size_t>(x.size()); i++) {
    vec_ir_tensor_x.push_back(paddle::dialect::IrTensor(paddle::dialect::TransToPhiDataType(x[i].dyn_cast<paddle::dialect::DenseTensorType>().dtype()),
                                                                     x[i].dyn_cast<paddle::dialect::DenseTensorType>().dims(),
                                                                     x[i].dyn_cast<paddle::dialect::DenseTensorType>().data_layout(),
                                                                     x[i].dyn_cast<paddle::dialect::DenseTensorType>().lod(),
                                                                     x[i].dyn_cast<paddle::dialect::DenseTensorType>().offset()));
  }
  std::vector<paddle::dialect::IrMetaTensor> vec_meta_x;
  for (size_t i=0; i < vec_ir_tensor_x.size(); i++) {
    vec_meta_x.push_back(paddle::dialect::IrMetaTensor(&vec_ir_tensor_x[i]));
  }

  std::vector<const phi::MetaTensor*> meta_x;
  for (size_t i=0; i < static_cast<size_t>(vec_meta_x.size()); i++) {
    meta_x.push_back(&vec_meta_x[i]);
  }
   paddle::dialect::IrTensor dense_out;
  paddle::dialect::IrMetaTensor meta_out(&dense_out);

  phi::StackInferMeta(meta_x, axis, &meta_out, phi::MetaConfig(false, false));

  std::vector<pir::Type> argument_outputs;
  pir::Type out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_out.dtype()), dense_out.dims(), dense_out.layout(), dense_out.lod(), dense_out.offset());
  argument_outputs.push_back(out_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void StackOp::VerifySig() {
  VLOG(4) << "Start Verifying inputs, outputs and attributes for: StackOp.";
  VLOG(4) << "Verifying inputs:";
  {
  auto input_size = num_operands();
  IR_ENFORCE(input_size == 1u,
                    "The size %d of inputs must be equal to 1.", input_size);
  if (auto vec_type = (*this)->operand_source(0).type().dyn_cast<pir::VectorType>()) {
      for (size_t i = 0; i < vec_type.size(); ++i) {
        IR_ENFORCE(vec_type[i].isa<paddle::dialect::DenseTensorType>(),
                       "Type validation failed for the 0th input, got %s.", (*this)->operand_source(0).type());
      }
  }
  else {
    IR_ENFORCE((*this)->operand_source(0).type().isa<paddle::dialect::DenseTensorType>(),
                   "Type validation failed for the 0th input, got %s.", (*this)->operand_source(0).type());
  }
  }
  VLOG(4) << "Verifying attributes:";
  {
  auto& attributes = this->attributes();
  IR_ENFORCE(attributes.count("axis")>0,
                 "axis does not exist.");
  IR_ENFORCE(attributes.at("axis").isa<pir::Int32Attribute>(),
                 "Type of attribute: axis is not pir::Int32Attribute.");

  }
  VLOG(4) << "Verifying outputs:";
  {
  auto output_size = num_results();
  IR_ENFORCE(output_size == 1u,
                    "The size %d of outputs must be equal to 1.", output_size);
  IR_ENFORCE((*this)->result(0).type().isa<paddle::dialect::DenseTensorType>(),
                 "Type validation failed for the 0th output.");
  }
  VLOG(4) << "End Verifying for: StackOp.";
}

void StackOp::InferMeta( phi::InferMetaContext *infer_meta ) {
  auto fn = PD_INFER_META(phi::StackInferMeta);
  fn(infer_meta);
}

phi::DataType StackOp::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: StackOp";
  


  return expected_kernel_dtype;
}

bool StackOp::InferSymbolicShape(pir::ShapeConstraintIRAnalysis* shape_analysis) {
  VLOG(4) << "Infer symbolic shape for op: StackOp";
  return StackOpInferSymbolicShape(this->operation(), shape_analysis);
}

OpInfoTuple StandardGammaOp::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("x", "paddle::dialect::DenseTensorType", false, false, false, false) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = {  };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("out", "paddle::dialect::DenseTensorType", false, false) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("UnchangedInferMeta", {"x"}, "standard_gamma", {"x"}, {}, {}, {}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "standard_gamma");
}

void StandardGammaOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_) {
  VLOG(4) << "Start build StandardGammaOp";



  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);
  paddle::dialect::IrTensor dense_out;
  paddle::dialect::IrMetaTensor meta_out(&dense_out);

  phi::UnchangedInferMeta(meta_x, &meta_out);

  std::vector<pir::Type> argument_outputs;
  pir::Type out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_out.dtype()), dense_out.dims(), dense_out.layout(), dense_out.lod(), dense_out.offset());
  argument_outputs.push_back(out_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void StandardGammaOp::VerifySig() {
  VLOG(4) << "Start Verifying inputs, outputs and attributes for: StandardGammaOp.";
  VLOG(4) << "Verifying inputs:";
  {
  auto input_size = num_operands();
  IR_ENFORCE(input_size == 1u,
                    "The size %d of inputs must be equal to 1.", input_size);
  IR_ENFORCE((*this)->operand_source(0).type().isa<paddle::dialect::DenseTensorType>(),
                  "Type validation failed for the 0th input, got %s.", (*this)->operand_source(0).type());
  }
  VLOG(4) << "Verifying attributes:";
  {
  // Attributes num is 0, not need to check attributes type.
  }
  VLOG(4) << "Verifying outputs:";
  {
  auto output_size = num_results();
  IR_ENFORCE(output_size == 1u,
                    "The size %d of outputs must be equal to 1.", output_size);
  IR_ENFORCE((*this)->result(0).type().isa<paddle::dialect::DenseTensorType>(),
                 "Type validation failed for the 0th output.");
  }
  VLOG(4) << "End Verifying for: StandardGammaOp.";
}

void StandardGammaOp::InferMeta( phi::InferMetaContext *infer_meta ) {
  auto fn = PD_INFER_META(phi::UnchangedInferMeta);
  fn(infer_meta);
}

phi::DataType StandardGammaOp::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: StandardGammaOp";
  


  return expected_kernel_dtype;
}

const char *StanhOp::attributes_name[2] = { "scale_a", "scale_b" };

OpInfoTuple StanhOp::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("x", "paddle::dialect::DenseTensorType", false, false, false, true) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = { paddle::dialect::OpAttributeInfo("scale_a", "pir::FloatAttribute", ""), paddle::dialect::OpAttributeInfo("scale_b", "pir::FloatAttribute", "") };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("out", "paddle::dialect::DenseTensorType", false, false) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("UnchangedInferMeta", {"x"}, "stanh", {"x", "scale_a", "scale_b"}, {}, {}, {}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "stanh");
}

void StanhOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, float scale_a, float scale_b) {
  VLOG(4) << "Start build StanhOp";



  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_scale_a = pir::FloatAttribute::get(pir::IrContext::Instance(), scale_a);
  argument.AddAttribute("scale_a", attr_scale_a);
  pir::Attribute attr_scale_b = pir::FloatAttribute::get(pir::IrContext::Instance(), scale_b);
  argument.AddAttribute("scale_b", attr_scale_b);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);
  paddle::dialect::IrTensor dense_out;
  paddle::dialect::IrMetaTensor meta_out(&dense_out);

  phi::UnchangedInferMeta(meta_x, &meta_out);

  std::vector<pir::Type> argument_outputs;
  pir::Type out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_out.dtype()), dense_out.dims(), dense_out.layout(), dense_out.lod(), dense_out.offset());
  argument_outputs.push_back(out_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void StanhOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, pir::AttributeMap attributes) {
  VLOG(4) << "Start build StanhOp";


  IR_ENFORCE(
      attributes.find("scale_a") != attributes.end(),
          "'scale_a' Attribute is expected for StanhOp. ");
  float scale_a = attributes.at("scale_a").dyn_cast<pir::FloatAttribute>().data();

  IR_ENFORCE(
      attributes.find("scale_b") != attributes.end(),
          "'scale_b' Attribute is expected for StanhOp. ");
  float scale_b = attributes.at("scale_b").dyn_cast<pir::FloatAttribute>().data();


  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_scale_a = pir::FloatAttribute::get(pir::IrContext::Instance(), scale_a);
  argument.AddAttribute("scale_a", attr_scale_a);
  pir::Attribute attr_scale_b = pir::FloatAttribute::get(pir::IrContext::Instance(), scale_b);
  argument.AddAttribute("scale_b", attr_scale_b);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);
  paddle::dialect::IrTensor dense_out;
  paddle::dialect::IrMetaTensor meta_out(&dense_out);

  phi::UnchangedInferMeta(meta_x, &meta_out);

  std::vector<pir::Type> argument_outputs;
  pir::Type out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_out.dtype()), dense_out.dims(), dense_out.layout(), dense_out.lod(), dense_out.offset());
  argument_outputs.push_back(out_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void StanhOp::VerifySig() {
  VLOG(4) << "Start Verifying inputs, outputs and attributes for: StanhOp.";
  VLOG(4) << "Verifying inputs:";
  {
  auto input_size = num_operands();
  IR_ENFORCE(input_size == 1u,
                    "The size %d of inputs must be equal to 1.", input_size);
  IR_ENFORCE((*this)->operand_source(0).type().isa<paddle::dialect::DenseTensorType>(),
                  "Type validation failed for the 0th input, got %s.", (*this)->operand_source(0).type());
  }
  VLOG(4) << "Verifying attributes:";
  {
  auto& attributes = this->attributes();
  IR_ENFORCE(attributes.count("scale_a")>0,
                 "scale_a does not exist.");
  IR_ENFORCE(attributes.at("scale_a").isa<pir::FloatAttribute>(),
                 "Type of attribute: scale_a is not pir::FloatAttribute.");

  IR_ENFORCE(attributes.count("scale_b")>0,
                 "scale_b does not exist.");
  IR_ENFORCE(attributes.at("scale_b").isa<pir::FloatAttribute>(),
                 "Type of attribute: scale_b is not pir::FloatAttribute.");

  }
  VLOG(4) << "Verifying outputs:";
  {
  auto output_size = num_results();
  IR_ENFORCE(output_size == 1u,
                    "The size %d of outputs must be equal to 1.", output_size);
  IR_ENFORCE((*this)->result(0).type().isa<paddle::dialect::DenseTensorType>(),
                 "Type validation failed for the 0th output.");
  }
  VLOG(4) << "End Verifying for: StanhOp.";
}

void StanhOp::InferMeta( phi::InferMetaContext *infer_meta ) {
  auto fn = PD_INFER_META(phi::UnchangedInferMeta);
  fn(infer_meta);
}

phi::DataType StanhOp::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: StanhOp";
  


  return expected_kernel_dtype;
}

const char *SvdOp::attributes_name[1] = { "full_matrices" };

OpInfoTuple SvdOp::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("x", "paddle::dialect::DenseTensorType", false, false, false, true) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = { paddle::dialect::OpAttributeInfo("full_matrices", "pir::BoolAttribute", "") };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("u", "paddle::dialect::DenseTensorType", false, false), paddle::dialect::OpOutputInfo("s", "paddle::dialect::DenseTensorType", false, false), paddle::dialect::OpOutputInfo("vh", "paddle::dialect::DenseTensorType", false, false) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("SvdInferMeta", {"x", "full_matrices"}, "svd", {"x", "full_matrices"}, {}, {}, {}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "svd");
}

void SvdOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, bool full_matrices) {
  VLOG(4) << "Start build SvdOp";



  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_full_matrices = pir::BoolAttribute::get(pir::IrContext::Instance(), full_matrices);
  argument.AddAttribute("full_matrices", attr_full_matrices);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);
  paddle::dialect::IrTensor dense_u;
  paddle::dialect::IrMetaTensor meta_u(&dense_u);
  paddle::dialect::IrTensor dense_s;
  paddle::dialect::IrMetaTensor meta_s(&dense_s);
  paddle::dialect::IrTensor dense_vh;
  paddle::dialect::IrMetaTensor meta_vh(&dense_vh);

  phi::SvdInferMeta(meta_x, full_matrices, &meta_u, &meta_s, &meta_vh);

  std::vector<pir::Type> argument_outputs;
  pir::Type u_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_u.dtype()), dense_u.dims(), dense_u.layout(), dense_u.lod(), dense_u.offset());
  argument_outputs.push_back(u_dense_tensor_type);

  pir::Type s_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_s.dtype()), dense_s.dims(), dense_s.layout(), dense_s.lod(), dense_s.offset());
  argument_outputs.push_back(s_dense_tensor_type);

  pir::Type vh_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_vh.dtype()), dense_vh.dims(), dense_vh.layout(), dense_vh.lod(), dense_vh.offset());
  argument_outputs.push_back(vh_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void SvdOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, pir::AttributeMap attributes) {
  VLOG(4) << "Start build SvdOp";


  IR_ENFORCE(
      attributes.find("full_matrices") != attributes.end(),
          "'full_matrices' Attribute is expected for SvdOp. ");
  bool full_matrices = attributes.at("full_matrices").dyn_cast<pir::BoolAttribute>().data();


  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_full_matrices = pir::BoolAttribute::get(pir::IrContext::Instance(), full_matrices);
  argument.AddAttribute("full_matrices", attr_full_matrices);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);
  paddle::dialect::IrTensor dense_u;
  paddle::dialect::IrMetaTensor meta_u(&dense_u);
  paddle::dialect::IrTensor dense_s;
  paddle::dialect::IrMetaTensor meta_s(&dense_s);
  paddle::dialect::IrTensor dense_vh;
  paddle::dialect::IrMetaTensor meta_vh(&dense_vh);

  phi::SvdInferMeta(meta_x, full_matrices, &meta_u, &meta_s, &meta_vh);

  std::vector<pir::Type> argument_outputs;
  pir::Type u_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_u.dtype()), dense_u.dims(), dense_u.layout(), dense_u.lod(), dense_u.offset());
  argument_outputs.push_back(u_dense_tensor_type);

  pir::Type s_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_s.dtype()), dense_s.dims(), dense_s.layout(), dense_s.lod(), dense_s.offset());
  argument_outputs.push_back(s_dense_tensor_type);

  pir::Type vh_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_vh.dtype()), dense_vh.dims(), dense_vh.layout(), dense_vh.lod(), dense_vh.offset());
  argument_outputs.push_back(vh_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void SvdOp::VerifySig() {
  VLOG(4) << "Start Verifying inputs, outputs and attributes for: SvdOp.";
  VLOG(4) << "Verifying inputs:";
  {
  auto input_size = num_operands();
  IR_ENFORCE(input_size == 1u,
                    "The size %d of inputs must be equal to 1.", input_size);
  IR_ENFORCE((*this)->operand_source(0).type().isa<paddle::dialect::DenseTensorType>(),
                  "Type validation failed for the 0th input, got %s.", (*this)->operand_source(0).type());
  }
  VLOG(4) << "Verifying attributes:";
  {
  auto& attributes = this->attributes();
  IR_ENFORCE(attributes.count("full_matrices")>0,
                 "full_matrices does not exist.");
  IR_ENFORCE(attributes.at("full_matrices").isa<pir::BoolAttribute>(),
                 "Type of attribute: full_matrices is not pir::BoolAttribute.");

  }
  VLOG(4) << "Verifying outputs:";
  {
  auto output_size = num_results();
  IR_ENFORCE(output_size == 3u,
                    "The size %d of outputs must be equal to 3.", output_size);
  IR_ENFORCE((*this)->result(0).type().isa<paddle::dialect::DenseTensorType>(),
                 "Type validation failed for the 0th output.");
  IR_ENFORCE((*this)->result(1).type().isa<paddle::dialect::DenseTensorType>(),
                 "Type validation failed for the 1th output.");
  IR_ENFORCE((*this)->result(2).type().isa<paddle::dialect::DenseTensorType>(),
                 "Type validation failed for the 2th output.");
  }
  VLOG(4) << "End Verifying for: SvdOp.";
}

void SvdOp::InferMeta( phi::InferMetaContext *infer_meta ) {
  auto fn = PD_INFER_META(phi::SvdInferMeta);
  fn(infer_meta);
}

phi::DataType SvdOp::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: SvdOp";
  


  return expected_kernel_dtype;
}

const char *TakeAlongAxisOp::attributes_name[1] = { "axis" };

OpInfoTuple TakeAlongAxisOp::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("arr", "paddle::dialect::DenseTensorType", false, false, false, true), paddle::dialect::OpInputInfo("indices", "paddle::dialect::DenseTensorType", false, false, false, false) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = { paddle::dialect::OpAttributeInfo("axis", "pir::Int32Attribute", "") };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("out", "paddle::dialect::DenseTensorType", false, false) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("TakeAlongAxisInferMeta", {"arr", "indices", "axis"}, "take_along_axis", {"arr", "indices", "axis"}, {"arr"}, {}, {}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "take_along_axis");
}

void TakeAlongAxisOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value arr_, pir::Value indices_, int axis) {
  VLOG(4) << "Start build TakeAlongAxisOp";



  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {arr_, indices_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_axis = pir::Int32Attribute::get(pir::IrContext::Instance(), axis);
  argument.AddAttribute("axis", attr_axis);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType arr = arr_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)arr;
  paddle::dialect::DenseTensorType indices = indices_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)indices;

  VLOG(4) << "Builder construction  dense_arr";
  paddle::dialect::IrTensor ir_tensor_arr(paddle::dialect::TransToPhiDataType(arr.dtype()),
                                                      arr.dims(),
                                                      arr.data_layout(),
                                                      arr.lod(),
                                                      arr.offset());
  VLOG(4) << "Builder construction  meta_arr";
  paddle::dialect::IrMetaTensor meta_arr(&ir_tensor_arr);

  VLOG(4) << "Builder construction  dense_indices";
  paddle::dialect::IrTensor ir_tensor_indices(paddle::dialect::TransToPhiDataType(indices.dtype()),
                                                      indices.dims(),
                                                      indices.data_layout(),
                                                      indices.lod(),
                                                      indices.offset());
  VLOG(4) << "Builder construction  meta_indices";
  paddle::dialect::IrMetaTensor meta_indices(&ir_tensor_indices);
  paddle::dialect::IrTensor dense_out;
  paddle::dialect::IrMetaTensor meta_out(&dense_out);

  phi::TakeAlongAxisInferMeta(meta_arr, meta_indices, axis, &meta_out);

  std::vector<pir::Type> argument_outputs;
  pir::Type out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_out.dtype()), dense_out.dims(), dense_out.layout(), dense_out.lod(), dense_out.offset());
  argument_outputs.push_back(out_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void TakeAlongAxisOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value arr_, pir::Value indices_, pir::AttributeMap attributes) {
  VLOG(4) << "Start build TakeAlongAxisOp";


  IR_ENFORCE(
      attributes.find("axis") != attributes.end(),
          "'axis' Attribute is expected for TakeAlongAxisOp. ");
  int axis = attributes.at("axis").dyn_cast<pir::Int32Attribute>().data();


  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {arr_, indices_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_axis = pir::Int32Attribute::get(pir::IrContext::Instance(), axis);
  argument.AddAttribute("axis", attr_axis);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType arr = arr_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)arr;
  paddle::dialect::DenseTensorType indices = indices_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)indices;

  VLOG(4) << "Builder construction  dense_arr";
  paddle::dialect::IrTensor ir_tensor_arr(paddle::dialect::TransToPhiDataType(arr.dtype()),
                                                      arr.dims(),
                                                      arr.data_layout(),
                                                      arr.lod(),
                                                      arr.offset());
  VLOG(4) << "Builder construction  meta_arr";
  paddle::dialect::IrMetaTensor meta_arr(&ir_tensor_arr);

  VLOG(4) << "Builder construction  dense_indices";
  paddle::dialect::IrTensor ir_tensor_indices(paddle::dialect::TransToPhiDataType(indices.dtype()),
                                                      indices.dims(),
                                                      indices.data_layout(),
                                                      indices.lod(),
                                                      indices.offset());
  VLOG(4) << "Builder construction  meta_indices";
  paddle::dialect::IrMetaTensor meta_indices(&ir_tensor_indices);
  paddle::dialect::IrTensor dense_out;
  paddle::dialect::IrMetaTensor meta_out(&dense_out);

  phi::TakeAlongAxisInferMeta(meta_arr, meta_indices, axis, &meta_out);

  std::vector<pir::Type> argument_outputs;
  pir::Type out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_out.dtype()), dense_out.dims(), dense_out.layout(), dense_out.lod(), dense_out.offset());
  argument_outputs.push_back(out_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void TakeAlongAxisOp::VerifySig() {
  VLOG(4) << "Start Verifying inputs, outputs and attributes for: TakeAlongAxisOp.";
  VLOG(4) << "Verifying inputs:";
  {
  auto input_size = num_operands();
  IR_ENFORCE(input_size == 2u,
                    "The size %d of inputs must be equal to 2.", input_size);
  IR_ENFORCE((*this)->operand_source(0).type().isa<paddle::dialect::DenseTensorType>(),
                  "Type validation failed for the 0th input, got %s.", (*this)->operand_source(0).type());
  IR_ENFORCE((*this)->operand_source(1).type().isa<paddle::dialect::DenseTensorType>(),
                  "Type validation failed for the 1th input, got %s.", (*this)->operand_source(1).type());
  }
  VLOG(4) << "Verifying attributes:";
  {
  auto& attributes = this->attributes();
  IR_ENFORCE(attributes.count("axis")>0,
                 "axis does not exist.");
  IR_ENFORCE(attributes.at("axis").isa<pir::Int32Attribute>(),
                 "Type of attribute: axis is not pir::Int32Attribute.");

  }
  VLOG(4) << "Verifying outputs:";
  {
  auto output_size = num_results();
  IR_ENFORCE(output_size == 1u,
                    "The size %d of outputs must be equal to 1.", output_size);
  IR_ENFORCE((*this)->result(0).type().isa<paddle::dialect::DenseTensorType>(),
                 "Type validation failed for the 0th output.");
  }
  VLOG(4) << "End Verifying for: TakeAlongAxisOp.";
}

void TakeAlongAxisOp::InferMeta( phi::InferMetaContext *infer_meta ) {
  auto fn = PD_INFER_META(phi::TakeAlongAxisInferMeta);
  fn(infer_meta);
}

phi::DataType TakeAlongAxisOp::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: TakeAlongAxisOp";
  


  return expected_kernel_dtype;
}

OpInfoTuple TanOp::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("x", "paddle::dialect::DenseTensorType", false, false, false, true) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = {  };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("out", "paddle::dialect::DenseTensorType", false, false) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("UnchangedInferMeta", {"x"}, "tan", {"x"}, {}, {}, {}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "tan");
}

void TanOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_) {
  VLOG(4) << "Start build TanOp";



  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);
  paddle::dialect::IrTensor dense_out;
  paddle::dialect::IrMetaTensor meta_out(&dense_out);

  phi::UnchangedInferMeta(meta_x, &meta_out);

  std::vector<pir::Type> argument_outputs;
  pir::Type out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_out.dtype()), dense_out.dims(), dense_out.layout(), dense_out.lod(), dense_out.offset());
  argument_outputs.push_back(out_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void TanOp::VerifySig() {
  VLOG(4) << "Start Verifying inputs, outputs and attributes for: TanOp.";
  VLOG(4) << "Verifying inputs:";
  {
  auto input_size = num_operands();
  IR_ENFORCE(input_size == 1u,
                    "The size %d of inputs must be equal to 1.", input_size);
  IR_ENFORCE((*this)->operand_source(0).type().isa<paddle::dialect::DenseTensorType>(),
                  "Type validation failed for the 0th input, got %s.", (*this)->operand_source(0).type());
  }
  VLOG(4) << "Verifying attributes:";
  {
  // Attributes num is 0, not need to check attributes type.
  }
  VLOG(4) << "Verifying outputs:";
  {
  auto output_size = num_results();
  IR_ENFORCE(output_size == 1u,
                    "The size %d of outputs must be equal to 1.", output_size);
  IR_ENFORCE((*this)->result(0).type().isa<paddle::dialect::DenseTensorType>(),
                 "Type validation failed for the 0th output.");
  }
  VLOG(4) << "End Verifying for: TanOp.";
}

void TanOp::InferMeta( phi::InferMetaContext *infer_meta ) {
  auto fn = PD_INFER_META(phi::UnchangedInferMeta);
  fn(infer_meta);
}

phi::DataType TanOp::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: TanOp";
  


  return expected_kernel_dtype;
}

OpInfoTuple Tan_Op::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("x", "paddle::dialect::DenseTensorType", false, false, false, true) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = {  };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("out", "paddle::dialect::DenseTensorType", false, false) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("UnchangedInferMeta", {"x"}, "tan", {"x"}, {}, {}, {{"out", "x"}}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "tan");
}

void Tan_Op::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_) {
  VLOG(4) << "Start build Tan_Op";



  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);
  paddle::dialect::IrTensor dense_out;
  paddle::dialect::IrMetaTensor meta_out(&dense_out);

  phi::UnchangedInferMeta(meta_x, &meta_out);

  std::vector<pir::Type> argument_outputs;
  pir::Type out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_out.dtype()), dense_out.dims(), dense_out.layout(), dense_out.lod(), dense_out.offset());
  argument_outputs.push_back(out_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void Tan_Op::VerifySig() {
  VLOG(4) << "Start Verifying inputs, outputs and attributes for: Tan_Op.";
  VLOG(4) << "Verifying inputs:";
  {
  auto input_size = num_operands();
  IR_ENFORCE(input_size == 1u,
                    "The size %d of inputs must be equal to 1.", input_size);
  IR_ENFORCE((*this)->operand_source(0).type().isa<paddle::dialect::DenseTensorType>(),
                  "Type validation failed for the 0th input, got %s.", (*this)->operand_source(0).type());
  }
  VLOG(4) << "Verifying attributes:";
  {
  // Attributes num is 0, not need to check attributes type.
  }
  VLOG(4) << "Verifying outputs:";
  {
  auto output_size = num_results();
  IR_ENFORCE(output_size == 1u,
                    "The size %d of outputs must be equal to 1.", output_size);
  IR_ENFORCE((*this)->result(0).type().isa<paddle::dialect::DenseTensorType>(),
                 "Type validation failed for the 0th output.");
  }
  VLOG(4) << "End Verifying for: Tan_Op.";
}

void Tan_Op::InferMeta( phi::InferMetaContext *infer_meta ) {
  auto fn = PD_INFER_META(phi::UnchangedInferMeta);
  fn(infer_meta);
}

phi::DataType Tan_Op::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: Tan_Op";
  


  return expected_kernel_dtype;
}

OpInfoTuple TanhOp::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("x", "paddle::dialect::DenseTensorType", false, false, false, true) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = {  };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("out", "paddle::dialect::DenseTensorType", false, false) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("UnchangedInferMeta", {"x"}, "tanh", {"x"}, {}, {}, {}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "tanh");
}

void TanhOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_) {
  VLOG(4) << "Start build TanhOp";



  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);
  paddle::dialect::IrTensor dense_out;
  paddle::dialect::IrMetaTensor meta_out(&dense_out);

  phi::UnchangedInferMeta(meta_x, &meta_out);

  std::vector<pir::Type> argument_outputs;
  pir::Type out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_out.dtype()), dense_out.dims(), dense_out.layout(), dense_out.lod(), dense_out.offset());
  argument_outputs.push_back(out_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void TanhOp::VerifySig() {
  VLOG(4) << "Start Verifying inputs, outputs and attributes for: TanhOp.";
  VLOG(4) << "Verifying inputs:";
  {
  auto input_size = num_operands();
  IR_ENFORCE(input_size == 1u,
                    "The size %d of inputs must be equal to 1.", input_size);
  IR_ENFORCE((*this)->operand_source(0).type().isa<paddle::dialect::DenseTensorType>(),
                  "Type validation failed for the 0th input, got %s.", (*this)->operand_source(0).type());
  }
  VLOG(4) << "Verifying attributes:";
  {
  // Attributes num is 0, not need to check attributes type.
  }
  VLOG(4) << "Verifying outputs:";
  {
  auto output_size = num_results();
  IR_ENFORCE(output_size == 1u,
                    "The size %d of outputs must be equal to 1.", output_size);
  IR_ENFORCE((*this)->result(0).type().isa<paddle::dialect::DenseTensorType>(),
                 "Type validation failed for the 0th output.");
  }
  VLOG(4) << "End Verifying for: TanhOp.";
}

void TanhOp::InferMeta( phi::InferMetaContext *infer_meta ) {
  auto fn = PD_INFER_META(phi::UnchangedInferMeta);
  fn(infer_meta);
}

phi::DataType TanhOp::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: TanhOp";
  


  return expected_kernel_dtype;
}

OpInfoTuple Tanh_Op::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("x", "paddle::dialect::DenseTensorType", false, false, false, true) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = {  };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("out", "paddle::dialect::DenseTensorType", false, false) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("UnchangedInferMeta", {"x"}, "tanh", {"x"}, {}, {}, {{"out", "x"}}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "tanh");
}

void Tanh_Op::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_) {
  VLOG(4) << "Start build Tanh_Op";



  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);
  paddle::dialect::IrTensor dense_out;
  paddle::dialect::IrMetaTensor meta_out(&dense_out);

  phi::UnchangedInferMeta(meta_x, &meta_out);

  std::vector<pir::Type> argument_outputs;
  pir::Type out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_out.dtype()), dense_out.dims(), dense_out.layout(), dense_out.lod(), dense_out.offset());
  argument_outputs.push_back(out_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void Tanh_Op::VerifySig() {
  VLOG(4) << "Start Verifying inputs, outputs and attributes for: Tanh_Op.";
  VLOG(4) << "Verifying inputs:";
  {
  auto input_size = num_operands();
  IR_ENFORCE(input_size == 1u,
                    "The size %d of inputs must be equal to 1.", input_size);
  IR_ENFORCE((*this)->operand_source(0).type().isa<paddle::dialect::DenseTensorType>(),
                  "Type validation failed for the 0th input, got %s.", (*this)->operand_source(0).type());
  }
  VLOG(4) << "Verifying attributes:";
  {
  // Attributes num is 0, not need to check attributes type.
  }
  VLOG(4) << "Verifying outputs:";
  {
  auto output_size = num_results();
  IR_ENFORCE(output_size == 1u,
                    "The size %d of outputs must be equal to 1.", output_size);
  IR_ENFORCE((*this)->result(0).type().isa<paddle::dialect::DenseTensorType>(),
                 "Type validation failed for the 0th output.");
  }
  VLOG(4) << "End Verifying for: Tanh_Op.";
}

void Tanh_Op::InferMeta( phi::InferMetaContext *infer_meta ) {
  auto fn = PD_INFER_META(phi::UnchangedInferMeta);
  fn(infer_meta);
}

phi::DataType Tanh_Op::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: Tanh_Op";
  


  return expected_kernel_dtype;
}

OpInfoTuple TanhShrinkOp::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("x", "paddle::dialect::DenseTensorType", false, false, false, true) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = {  };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("out", "paddle::dialect::DenseTensorType", false, false) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("UnchangedInferMeta", {"x"}, "tanh_shrink", {"x"}, {}, {}, {}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "tanh_shrink");
}

void TanhShrinkOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_) {
  VLOG(4) << "Start build TanhShrinkOp";



  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);
  paddle::dialect::IrTensor dense_out;
  paddle::dialect::IrMetaTensor meta_out(&dense_out);

  phi::UnchangedInferMeta(meta_x, &meta_out);

  std::vector<pir::Type> argument_outputs;
  pir::Type out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_out.dtype()), dense_out.dims(), dense_out.layout(), dense_out.lod(), dense_out.offset());
  argument_outputs.push_back(out_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void TanhShrinkOp::VerifySig() {
  VLOG(4) << "Start Verifying inputs, outputs and attributes for: TanhShrinkOp.";
  VLOG(4) << "Verifying inputs:";
  {
  auto input_size = num_operands();
  IR_ENFORCE(input_size == 1u,
                    "The size %d of inputs must be equal to 1.", input_size);
  IR_ENFORCE((*this)->operand_source(0).type().isa<paddle::dialect::DenseTensorType>(),
                  "Type validation failed for the 0th input, got %s.", (*this)->operand_source(0).type());
  }
  VLOG(4) << "Verifying attributes:";
  {
  // Attributes num is 0, not need to check attributes type.
  }
  VLOG(4) << "Verifying outputs:";
  {
  auto output_size = num_results();
  IR_ENFORCE(output_size == 1u,
                    "The size %d of outputs must be equal to 1.", output_size);
  IR_ENFORCE((*this)->result(0).type().isa<paddle::dialect::DenseTensorType>(),
                 "Type validation failed for the 0th output.");
  }
  VLOG(4) << "End Verifying for: TanhShrinkOp.";
}

void TanhShrinkOp::InferMeta( phi::InferMetaContext *infer_meta ) {
  auto fn = PD_INFER_META(phi::UnchangedInferMeta);
  fn(infer_meta);
}

phi::DataType TanhShrinkOp::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: TanhShrinkOp";
  


  return expected_kernel_dtype;
}

const char *TemporalShiftOp::attributes_name[3] = { "seg_num", "shift_ratio", "data_format" };

OpInfoTuple TemporalShiftOp::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("x", "paddle::dialect::DenseTensorType", false, false, false, true) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = { paddle::dialect::OpAttributeInfo("seg_num", "pir::Int32Attribute", ""), paddle::dialect::OpAttributeInfo("shift_ratio", "pir::FloatAttribute", ""), paddle::dialect::OpAttributeInfo("data_format", "pir::StrAttribute", "") };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("out", "paddle::dialect::DenseTensorType", false, false) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("TemporalShiftInferMeta", {"x", "seg_num", "shift_ratio", "data_format"}, "temporal_shift", {"x", "seg_num", "shift_ratio", "data_format"}, {"x"}, {}, {}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "temporal_shift");
}

void TemporalShiftOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, int seg_num, float shift_ratio, const std::string& data_format) {
  VLOG(4) << "Start build TemporalShiftOp";



  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_seg_num = pir::Int32Attribute::get(pir::IrContext::Instance(), seg_num);
  argument.AddAttribute("seg_num", attr_seg_num);
  pir::Attribute attr_shift_ratio = pir::FloatAttribute::get(pir::IrContext::Instance(), shift_ratio);
  argument.AddAttribute("shift_ratio", attr_shift_ratio);
  pir::Attribute attr_data_format = pir::StrAttribute::get(pir::IrContext::Instance(), data_format);
  argument.AddAttribute("data_format", attr_data_format);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);
  paddle::dialect::IrTensor dense_out;
  paddle::dialect::IrMetaTensor meta_out(&dense_out);

  phi::TemporalShiftInferMeta(meta_x, seg_num, shift_ratio, data_format, &meta_out);

  std::vector<pir::Type> argument_outputs;
  pir::Type out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_out.dtype()), dense_out.dims(), dense_out.layout(), dense_out.lod(), dense_out.offset());
  argument_outputs.push_back(out_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void TemporalShiftOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, pir::AttributeMap attributes) {
  VLOG(4) << "Start build TemporalShiftOp";


  IR_ENFORCE(
      attributes.find("seg_num") != attributes.end(),
          "'seg_num' Attribute is expected for TemporalShiftOp. ");
  int seg_num = attributes.at("seg_num").dyn_cast<pir::Int32Attribute>().data();

  IR_ENFORCE(
      attributes.find("shift_ratio") != attributes.end(),
          "'shift_ratio' Attribute is expected for TemporalShiftOp. ");
  float shift_ratio = attributes.at("shift_ratio").dyn_cast<pir::FloatAttribute>().data();

  IR_ENFORCE(
      attributes.find("data_format") != attributes.end(),
          "'data_format' Attribute is expected for TemporalShiftOp. ");
  std::string data_format = attributes.at("data_format").dyn_cast<pir::StrAttribute>().AsString();


  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_seg_num = pir::Int32Attribute::get(pir::IrContext::Instance(), seg_num);
  argument.AddAttribute("seg_num", attr_seg_num);
  pir::Attribute attr_shift_ratio = pir::FloatAttribute::get(pir::IrContext::Instance(), shift_ratio);
  argument.AddAttribute("shift_ratio", attr_shift_ratio);
  pir::Attribute attr_data_format = pir::StrAttribute::get(pir::IrContext::Instance(), data_format);
  argument.AddAttribute("data_format", attr_data_format);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);
  paddle::dialect::IrTensor dense_out;
  paddle::dialect::IrMetaTensor meta_out(&dense_out);

  phi::TemporalShiftInferMeta(meta_x, seg_num, shift_ratio, data_format, &meta_out);

  std::vector<pir::Type> argument_outputs;
  pir::Type out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_out.dtype()), dense_out.dims(), dense_out.layout(), dense_out.lod(), dense_out.offset());
  argument_outputs.push_back(out_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void TemporalShiftOp::VerifySig() {
  VLOG(4) << "Start Verifying inputs, outputs and attributes for: TemporalShiftOp.";
  VLOG(4) << "Verifying inputs:";
  {
  auto input_size = num_operands();
  IR_ENFORCE(input_size == 1u,
                    "The size %d of inputs must be equal to 1.", input_size);
  IR_ENFORCE((*this)->operand_source(0).type().isa<paddle::dialect::DenseTensorType>(),
                  "Type validation failed for the 0th input, got %s.", (*this)->operand_source(0).type());
  }
  VLOG(4) << "Verifying attributes:";
  {
  auto& attributes = this->attributes();
  IR_ENFORCE(attributes.count("seg_num")>0,
                 "seg_num does not exist.");
  IR_ENFORCE(attributes.at("seg_num").isa<pir::Int32Attribute>(),
                 "Type of attribute: seg_num is not pir::Int32Attribute.");

  IR_ENFORCE(attributes.count("shift_ratio")>0,
                 "shift_ratio does not exist.");
  IR_ENFORCE(attributes.at("shift_ratio").isa<pir::FloatAttribute>(),
                 "Type of attribute: shift_ratio is not pir::FloatAttribute.");

  IR_ENFORCE(attributes.count("data_format")>0,
                 "data_format does not exist.");
  IR_ENFORCE(attributes.at("data_format").isa<pir::StrAttribute>(),
                 "Type of attribute: data_format is not pir::StrAttribute.");

  }
  VLOG(4) << "Verifying outputs:";
  {
  auto output_size = num_results();
  IR_ENFORCE(output_size == 1u,
                    "The size %d of outputs must be equal to 1.", output_size);
  IR_ENFORCE((*this)->result(0).type().isa<paddle::dialect::DenseTensorType>(),
                 "Type validation failed for the 0th output.");
  }
  VLOG(4) << "End Verifying for: TemporalShiftOp.";
}

void TemporalShiftOp::InferMeta( phi::InferMetaContext *infer_meta ) {
  auto fn = PD_INFER_META(phi::TemporalShiftInferMeta);
  fn(infer_meta);
}

phi::DataType TemporalShiftOp::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: TemporalShiftOp";
  


  return expected_kernel_dtype;
}

const char *TensorUnfoldOp::attributes_name[3] = { "axis", "size", "step" };

OpInfoTuple TensorUnfoldOp::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("input", "paddle::dialect::DenseTensorType", false, true, false, true) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = { paddle::dialect::OpAttributeInfo("axis", "pir::Int64Attribute", ""), paddle::dialect::OpAttributeInfo("size", "pir::Int64Attribute", ""), paddle::dialect::OpAttributeInfo("step", "pir::Int64Attribute", "") };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("out", "paddle::dialect::DenseTensorType", false, false) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("StridedUnChangedInferMeta", {"input"}, "tensor_unfold", {"input", "axis", "size", "step"}, {}, {}, {}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "tensor_unfold");
}

void TensorUnfoldOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value input_, int64_t axis, int64_t size, int64_t step) {
  VLOG(4) << "Start build TensorUnfoldOp";



  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {input_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_axis = pir::Int64Attribute::get(pir::IrContext::Instance(), axis);
  argument.AddAttribute("axis", attr_axis);
  pir::Attribute attr_size = pir::Int64Attribute::get(pir::IrContext::Instance(), size);
  argument.AddAttribute("size", attr_size);
  pir::Attribute attr_step = pir::Int64Attribute::get(pir::IrContext::Instance(), step);
  argument.AddAttribute("step", attr_step);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType input = input_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)input;

  VLOG(4) << "Builder construction  dense_input";
  paddle::dialect::IrTensor ir_tensor_input(paddle::dialect::TransToPhiDataType(input.dtype()),
                                                      input.dims(),
                                                      input.data_layout(),
                                                      input.lod(),
                                                      input.offset());
  VLOG(4) << "Builder construction  meta_input";
  paddle::dialect::IrMetaTensor meta_input(&ir_tensor_input);
  paddle::dialect::IrTensor dense_out;
  paddle::dialect::IrMetaTensor meta_out(&dense_out);

  phi::StridedUnChangedInferMeta(meta_input, &meta_out);

  std::vector<pir::Type> argument_outputs;
  pir::Type out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_out.dtype()), dense_out.dims(), dense_out.layout(), dense_out.lod(), dense_out.offset());
  argument_outputs.push_back(out_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void TensorUnfoldOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value input_, pir::AttributeMap attributes) {
  VLOG(4) << "Start build TensorUnfoldOp";


  IR_ENFORCE(
      attributes.find("axis") != attributes.end(),
          "'axis' Attribute is expected for TensorUnfoldOp. ");
  int64_t axis = attributes.at("axis").dyn_cast<pir::Int64Attribute>().data();

  IR_ENFORCE(
      attributes.find("size") != attributes.end(),
          "'size' Attribute is expected for TensorUnfoldOp. ");
  int64_t size = attributes.at("size").dyn_cast<pir::Int64Attribute>().data();

  IR_ENFORCE(
      attributes.find("step") != attributes.end(),
          "'step' Attribute is expected for TensorUnfoldOp. ");
  int64_t step = attributes.at("step").dyn_cast<pir::Int64Attribute>().data();


  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {input_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_axis = pir::Int64Attribute::get(pir::IrContext::Instance(), axis);
  argument.AddAttribute("axis", attr_axis);
  pir::Attribute attr_size = pir::Int64Attribute::get(pir::IrContext::Instance(), size);
  argument.AddAttribute("size", attr_size);
  pir::Attribute attr_step = pir::Int64Attribute::get(pir::IrContext::Instance(), step);
  argument.AddAttribute("step", attr_step);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType input = input_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)input;

  VLOG(4) << "Builder construction  dense_input";
  paddle::dialect::IrTensor ir_tensor_input(paddle::dialect::TransToPhiDataType(input.dtype()),
                                                      input.dims(),
                                                      input.data_layout(),
                                                      input.lod(),
                                                      input.offset());
  VLOG(4) << "Builder construction  meta_input";
  paddle::dialect::IrMetaTensor meta_input(&ir_tensor_input);
  paddle::dialect::IrTensor dense_out;
  paddle::dialect::IrMetaTensor meta_out(&dense_out);

  phi::StridedUnChangedInferMeta(meta_input, &meta_out);

  std::vector<pir::Type> argument_outputs;
  pir::Type out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_out.dtype()), dense_out.dims(), dense_out.layout(), dense_out.lod(), dense_out.offset());
  argument_outputs.push_back(out_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void TensorUnfoldOp::VerifySig() {
  VLOG(4) << "Start Verifying inputs, outputs and attributes for: TensorUnfoldOp.";
  VLOG(4) << "Verifying inputs:";
  {
  auto input_size = num_operands();
  IR_ENFORCE(input_size == 1u,
                    "The size %d of inputs must be equal to 1.", input_size);
  IR_ENFORCE((*this)->operand_source(0).type().isa<paddle::dialect::DenseTensorType>(),
                  "Type validation failed for the 0th input, got %s.", (*this)->operand_source(0).type());
  }
  VLOG(4) << "Verifying attributes:";
  {
  auto& attributes = this->attributes();
  IR_ENFORCE(attributes.count("axis")>0,
                 "axis does not exist.");
  IR_ENFORCE(attributes.at("axis").isa<pir::Int64Attribute>(),
                 "Type of attribute: axis is not pir::Int64Attribute.");

  IR_ENFORCE(attributes.count("size")>0,
                 "size does not exist.");
  IR_ENFORCE(attributes.at("size").isa<pir::Int64Attribute>(),
                 "Type of attribute: size is not pir::Int64Attribute.");

  IR_ENFORCE(attributes.count("step")>0,
                 "step does not exist.");
  IR_ENFORCE(attributes.at("step").isa<pir::Int64Attribute>(),
                 "Type of attribute: step is not pir::Int64Attribute.");

  }
  VLOG(4) << "Verifying outputs:";
  {
  auto output_size = num_results();
  IR_ENFORCE(output_size == 1u,
                    "The size %d of outputs must be equal to 1.", output_size);
  IR_ENFORCE((*this)->result(0).type().isa<paddle::dialect::DenseTensorType>(),
                 "Type validation failed for the 0th output.");
  }
  VLOG(4) << "End Verifying for: TensorUnfoldOp.";
}

void TensorUnfoldOp::InferMeta( phi::InferMetaContext *infer_meta ) {
  auto fn = PD_INFER_META(phi::StridedUnChangedInferMeta);
  fn(infer_meta);
}

phi::DataType TensorUnfoldOp::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: TensorUnfoldOp";
  


  return expected_kernel_dtype;
}

const char *ThresholdedReluOp::attributes_name[1] = { "threshold" };

OpInfoTuple ThresholdedReluOp::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("x", "paddle::dialect::DenseTensorType", false, false, false, true) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = { paddle::dialect::OpAttributeInfo("threshold", "pir::FloatAttribute", "") };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("out", "paddle::dialect::DenseTensorType", false, false) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("UnchangedInferMeta", {"x"}, "thresholded_relu", {"x", "threshold"}, {}, {}, {}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "thresholded_relu");
}

void ThresholdedReluOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, float threshold) {
  VLOG(4) << "Start build ThresholdedReluOp";



  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_threshold = pir::FloatAttribute::get(pir::IrContext::Instance(), threshold);
  argument.AddAttribute("threshold", attr_threshold);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);
  paddle::dialect::IrTensor dense_out;
  paddle::dialect::IrMetaTensor meta_out(&dense_out);

  phi::UnchangedInferMeta(meta_x, &meta_out);

  std::vector<pir::Type> argument_outputs;
  pir::Type out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_out.dtype()), dense_out.dims(), dense_out.layout(), dense_out.lod(), dense_out.offset());
  argument_outputs.push_back(out_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void ThresholdedReluOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, pir::AttributeMap attributes) {
  VLOG(4) << "Start build ThresholdedReluOp";


  IR_ENFORCE(
      attributes.find("threshold") != attributes.end(),
          "'threshold' Attribute is expected for ThresholdedReluOp. ");
  float threshold = attributes.at("threshold").dyn_cast<pir::FloatAttribute>().data();


  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_threshold = pir::FloatAttribute::get(pir::IrContext::Instance(), threshold);
  argument.AddAttribute("threshold", attr_threshold);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);
  paddle::dialect::IrTensor dense_out;
  paddle::dialect::IrMetaTensor meta_out(&dense_out);

  phi::UnchangedInferMeta(meta_x, &meta_out);

  std::vector<pir::Type> argument_outputs;
  pir::Type out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_out.dtype()), dense_out.dims(), dense_out.layout(), dense_out.lod(), dense_out.offset());
  argument_outputs.push_back(out_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void ThresholdedReluOp::VerifySig() {
  VLOG(4) << "Start Verifying inputs, outputs and attributes for: ThresholdedReluOp.";
  VLOG(4) << "Verifying inputs:";
  {
  auto input_size = num_operands();
  IR_ENFORCE(input_size == 1u,
                    "The size %d of inputs must be equal to 1.", input_size);
  IR_ENFORCE((*this)->operand_source(0).type().isa<paddle::dialect::DenseTensorType>(),
                  "Type validation failed for the 0th input, got %s.", (*this)->operand_source(0).type());
  }
  VLOG(4) << "Verifying attributes:";
  {
  auto& attributes = this->attributes();
  IR_ENFORCE(attributes.count("threshold")>0,
                 "threshold does not exist.");
  IR_ENFORCE(attributes.at("threshold").isa<pir::FloatAttribute>(),
                 "Type of attribute: threshold is not pir::FloatAttribute.");

  }
  VLOG(4) << "Verifying outputs:";
  {
  auto output_size = num_results();
  IR_ENFORCE(output_size == 1u,
                    "The size %d of outputs must be equal to 1.", output_size);
  IR_ENFORCE((*this)->result(0).type().isa<paddle::dialect::DenseTensorType>(),
                 "Type validation failed for the 0th output.");
  }
  VLOG(4) << "End Verifying for: ThresholdedReluOp.";
}

void ThresholdedReluOp::InferMeta( phi::InferMetaContext *infer_meta ) {
  auto fn = PD_INFER_META(phi::UnchangedInferMeta);
  fn(infer_meta);
}

phi::DataType ThresholdedReluOp::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: ThresholdedReluOp";
  


  return expected_kernel_dtype;
}

const char *ThresholdedRelu_Op::attributes_name[1] = { "threshold" };

OpInfoTuple ThresholdedRelu_Op::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("x", "paddle::dialect::DenseTensorType", false, false, false, true) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = { paddle::dialect::OpAttributeInfo("threshold", "pir::FloatAttribute", "") };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("out", "paddle::dialect::DenseTensorType", false, false) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("UnchangedInferMeta", {"x"}, "thresholded_relu", {"x", "threshold"}, {}, {}, {{"out", "x"}}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "thresholded_relu");
}

void ThresholdedRelu_Op::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, float threshold) {
  VLOG(4) << "Start build ThresholdedRelu_Op";



  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_threshold = pir::FloatAttribute::get(pir::IrContext::Instance(), threshold);
  argument.AddAttribute("threshold", attr_threshold);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);
  paddle::dialect::IrTensor dense_out;
  paddle::dialect::IrMetaTensor meta_out(&dense_out);

  phi::UnchangedInferMeta(meta_x, &meta_out);

  std::vector<pir::Type> argument_outputs;
  pir::Type out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_out.dtype()), dense_out.dims(), dense_out.layout(), dense_out.lod(), dense_out.offset());
  argument_outputs.push_back(out_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void ThresholdedRelu_Op::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, pir::AttributeMap attributes) {
  VLOG(4) << "Start build ThresholdedRelu_Op";


  IR_ENFORCE(
      attributes.find("threshold") != attributes.end(),
          "'threshold' Attribute is expected for ThresholdedRelu_Op. ");
  float threshold = attributes.at("threshold").dyn_cast<pir::FloatAttribute>().data();


  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_threshold = pir::FloatAttribute::get(pir::IrContext::Instance(), threshold);
  argument.AddAttribute("threshold", attr_threshold);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);
  paddle::dialect::IrTensor dense_out;
  paddle::dialect::IrMetaTensor meta_out(&dense_out);

  phi::UnchangedInferMeta(meta_x, &meta_out);

  std::vector<pir::Type> argument_outputs;
  pir::Type out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_out.dtype()), dense_out.dims(), dense_out.layout(), dense_out.lod(), dense_out.offset());
  argument_outputs.push_back(out_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void ThresholdedRelu_Op::VerifySig() {
  VLOG(4) << "Start Verifying inputs, outputs and attributes for: ThresholdedRelu_Op.";
  VLOG(4) << "Verifying inputs:";
  {
  auto input_size = num_operands();
  IR_ENFORCE(input_size == 1u,
                    "The size %d of inputs must be equal to 1.", input_size);
  IR_ENFORCE((*this)->operand_source(0).type().isa<paddle::dialect::DenseTensorType>(),
                  "Type validation failed for the 0th input, got %s.", (*this)->operand_source(0).type());
  }
  VLOG(4) << "Verifying attributes:";
  {
  auto& attributes = this->attributes();
  IR_ENFORCE(attributes.count("threshold")>0,
                 "threshold does not exist.");
  IR_ENFORCE(attributes.at("threshold").isa<pir::FloatAttribute>(),
                 "Type of attribute: threshold is not pir::FloatAttribute.");

  }
  VLOG(4) << "Verifying outputs:";
  {
  auto output_size = num_results();
  IR_ENFORCE(output_size == 1u,
                    "The size %d of outputs must be equal to 1.", output_size);
  IR_ENFORCE((*this)->result(0).type().isa<paddle::dialect::DenseTensorType>(),
                 "Type validation failed for the 0th output.");
  }
  VLOG(4) << "End Verifying for: ThresholdedRelu_Op.";
}

void ThresholdedRelu_Op::InferMeta( phi::InferMetaContext *infer_meta ) {
  auto fn = PD_INFER_META(phi::UnchangedInferMeta);
  fn(infer_meta);
}

phi::DataType ThresholdedRelu_Op::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: ThresholdedRelu_Op";
  


  return expected_kernel_dtype;
}

const char *TopPSamplingOp::attributes_name[1] = { "seed" };

OpInfoTuple TopPSamplingOp::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("x", "paddle::dialect::DenseTensorType", false, false, false, false), paddle::dialect::OpInputInfo("ps", "paddle::dialect::DenseTensorType", false, false, false, false), paddle::dialect::OpInputInfo("threshold", "paddle::dialect::DenseTensorType", true, false, false, false) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = { paddle::dialect::OpAttributeInfo("seed", "pir::Int32Attribute", "") };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("out", "paddle::dialect::DenseTensorType", false, false), paddle::dialect::OpOutputInfo("ids", "paddle::dialect::DenseTensorType", false, false) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("TopPSamplingInferMeta", {"x", "ps", "threshold", "seed"}, "top_p_sampling", {"x", "ps", "threshold", "seed"}, {"x"}, {}, {}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "top_p_sampling");
}

void TopPSamplingOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, pir::Value ps_, pir::Value threshold_, int seed) {
  VLOG(4) << "Start build TopPSamplingOp";



  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_, ps_, threshold_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_seed = pir::Int32Attribute::get(pir::IrContext::Instance(), seed);
  argument.AddAttribute("seed", attr_seed);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;
  paddle::dialect::DenseTensorType ps = ps_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)ps;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);

  VLOG(4) << "Builder construction  dense_ps";
  paddle::dialect::IrTensor ir_tensor_ps(paddle::dialect::TransToPhiDataType(ps.dtype()),
                                                      ps.dims(),
                                                      ps.data_layout(),
                                                      ps.lod(),
                                                      ps.offset());
  VLOG(4) << "Builder construction  meta_ps";
  paddle::dialect::IrMetaTensor meta_ps(&ir_tensor_ps);

  paddle::dialect::IrMetaTensor meta_threshold;
  paddle::dialect::IrTensor ir_tensor_threshold;
  if (threshold_.impl() != nullptr) {
    paddle::dialect::DenseTensorType threshold = threshold_.type().dyn_cast<paddle::dialect::DenseTensorType>();
    VLOG(4) << "Builder construction  dense_threshold";
    ir_tensor_threshold = paddle::dialect::IrTensor(paddle::dialect::TransToPhiDataType(threshold.dtype()),
                                                        threshold.dims(),
                                                        threshold.data_layout(),
                                                        threshold.lod(),
                                                        threshold.offset());
    VLOG(4) << "Builder construction  meta_threshold";
    meta_threshold = paddle::dialect::IrMetaTensor(&ir_tensor_threshold);
  }

  paddle::dialect::IrTensor dense_out;
  paddle::dialect::IrMetaTensor meta_out(&dense_out);
  paddle::dialect::IrTensor dense_ids;
  paddle::dialect::IrMetaTensor meta_ids(&dense_ids);

  phi::TopPSamplingInferMeta(meta_x, meta_ps, meta_threshold, seed, &meta_out, &meta_ids);

  std::vector<pir::Type> argument_outputs;
  pir::Type out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_out.dtype()), dense_out.dims(), dense_out.layout(), dense_out.lod(), dense_out.offset());
  argument_outputs.push_back(out_dense_tensor_type);

  pir::Type ids_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_ids.dtype()), dense_ids.dims(), dense_ids.layout(), dense_ids.lod(), dense_ids.offset());
  argument_outputs.push_back(ids_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void TopPSamplingOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, pir::Value ps_, pir::Value threshold_, pir::AttributeMap attributes) {
  VLOG(4) << "Start build TopPSamplingOp";


  IR_ENFORCE(
      attributes.find("seed") != attributes.end(),
          "'seed' Attribute is expected for TopPSamplingOp. ");
  int seed = attributes.at("seed").dyn_cast<pir::Int32Attribute>().data();


  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_, ps_, threshold_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_seed = pir::Int32Attribute::get(pir::IrContext::Instance(), seed);
  argument.AddAttribute("seed", attr_seed);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;
  paddle::dialect::DenseTensorType ps = ps_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)ps;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);

  VLOG(4) << "Builder construction  dense_ps";
  paddle::dialect::IrTensor ir_tensor_ps(paddle::dialect::TransToPhiDataType(ps.dtype()),
                                                      ps.dims(),
                                                      ps.data_layout(),
                                                      ps.lod(),
                                                      ps.offset());
  VLOG(4) << "Builder construction  meta_ps";
  paddle::dialect::IrMetaTensor meta_ps(&ir_tensor_ps);

  paddle::dialect::IrMetaTensor meta_threshold;
  paddle::dialect::IrTensor ir_tensor_threshold;
  if (threshold_.impl() != nullptr) {
    paddle::dialect::DenseTensorType threshold = threshold_.type().dyn_cast<paddle::dialect::DenseTensorType>();
    VLOG(4) << "Builder construction  dense_threshold";
    ir_tensor_threshold = paddle::dialect::IrTensor(paddle::dialect::TransToPhiDataType(threshold.dtype()),
                                                        threshold.dims(),
                                                        threshold.data_layout(),
                                                        threshold.lod(),
                                                        threshold.offset());
    VLOG(4) << "Builder construction  meta_threshold";
    meta_threshold = paddle::dialect::IrMetaTensor(&ir_tensor_threshold);
  }

  paddle::dialect::IrTensor dense_out;
  paddle::dialect::IrMetaTensor meta_out(&dense_out);
  paddle::dialect::IrTensor dense_ids;
  paddle::dialect::IrMetaTensor meta_ids(&dense_ids);

  phi::TopPSamplingInferMeta(meta_x, meta_ps, meta_threshold, seed, &meta_out, &meta_ids);

  std::vector<pir::Type> argument_outputs;
  pir::Type out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_out.dtype()), dense_out.dims(), dense_out.layout(), dense_out.lod(), dense_out.offset());
  argument_outputs.push_back(out_dense_tensor_type);

  pir::Type ids_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_ids.dtype()), dense_ids.dims(), dense_ids.layout(), dense_ids.lod(), dense_ids.offset());
  argument_outputs.push_back(ids_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void TopPSamplingOp::VerifySig() {
  VLOG(4) << "Start Verifying inputs, outputs and attributes for: TopPSamplingOp.";
  VLOG(4) << "Verifying inputs:";
  {
  auto input_size = num_operands();
  IR_ENFORCE(input_size == 3u,
                    "The size %d of inputs must be equal to 3.", input_size);
  IR_ENFORCE((*this)->operand_source(0).type().isa<paddle::dialect::DenseTensorType>(),
                  "Type validation failed for the 0th input, got %s.", (*this)->operand_source(0).type());
  IR_ENFORCE((*this)->operand_source(1).type().isa<paddle::dialect::DenseTensorType>(),
                  "Type validation failed for the 1th input, got %s.", (*this)->operand_source(1).type());
  if (auto val = (*this)->operand(2)) {
    IR_ENFORCE(val.type().isa<paddle::dialect::DenseTensorType>(),
                   "Type validation failed for the 2th input, got %s.", (*this)->operand_source(2).type());
  }
  }
  VLOG(4) << "Verifying attributes:";
  {
  auto& attributes = this->attributes();
  IR_ENFORCE(attributes.count("seed")>0,
                 "seed does not exist.");
  IR_ENFORCE(attributes.at("seed").isa<pir::Int32Attribute>(),
                 "Type of attribute: seed is not pir::Int32Attribute.");

  }
  VLOG(4) << "Verifying outputs:";
  {
  auto output_size = num_results();
  IR_ENFORCE(output_size == 2u,
                    "The size %d of outputs must be equal to 2.", output_size);
  IR_ENFORCE((*this)->result(0).type().isa<paddle::dialect::DenseTensorType>(),
                 "Type validation failed for the 0th output.");
  IR_ENFORCE((*this)->result(1).type().isa<paddle::dialect::DenseTensorType>(),
                 "Type validation failed for the 1th output.");
  }
  VLOG(4) << "End Verifying for: TopPSamplingOp.";
}

void TopPSamplingOp::InferMeta( phi::InferMetaContext *infer_meta ) {
  auto fn = PD_INFER_META(phi::TopPSamplingInferMeta);
  fn(infer_meta);
}

phi::DataType TopPSamplingOp::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: TopPSamplingOp";
  


  return expected_kernel_dtype;
}

const char *TopkOp::attributes_name[3] = { "axis", "largest", "sorted" };

OpInfoTuple TopkOp::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("x", "paddle::dialect::DenseTensorType", false, false, false, true), paddle::dialect::OpInputInfo("k", "paddle::dialect::ScalarAttribute", false, false, true, false) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = { paddle::dialect::OpAttributeInfo("axis", "pir::Int32Attribute", ""), paddle::dialect::OpAttributeInfo("largest", "pir::BoolAttribute", ""), paddle::dialect::OpAttributeInfo("sorted", "pir::BoolAttribute", "") };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("out", "paddle::dialect::DenseTensorType", false, false), paddle::dialect::OpOutputInfo("indices", "paddle::dialect::DenseTensorType", false, false) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("TopKInferMeta", {"x", "k", "axis", "largest", "sorted"}, "topk", {"x", "k", "axis", "largest", "sorted"}, {"x"}, {}, {}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "topk");
}

void TopkOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, int k, int axis, bool largest, bool sorted) {
  VLOG(4) << "Start build TopkOp";


  // Generate scalar mutable attribute: k
  paddle::dialect::FullOp full_k_op = builder.Build<paddle::dialect::FullOp>(std::vector<int64_t>{1}, k, phi::DataType::INT32, phi::CPUPlace());
  pir::OpResult k_ = full_k_op->result(0);
    
  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_, k_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_axis = pir::Int32Attribute::get(pir::IrContext::Instance(), axis);
  argument.AddAttribute("axis", attr_axis);
  pir::Attribute attr_largest = pir::BoolAttribute::get(pir::IrContext::Instance(), largest);
  argument.AddAttribute("largest", attr_largest);
  pir::Attribute attr_sorted = pir::BoolAttribute::get(pir::IrContext::Instance(), sorted);
  argument.AddAttribute("sorted", attr_sorted);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);
  paddle::dialect::IrTensor dense_out;
  paddle::dialect::IrMetaTensor meta_out(&dense_out);
  paddle::dialect::IrTensor dense_indices;
  paddle::dialect::IrMetaTensor meta_indices(&dense_indices);

  phi::TopKInferMeta(meta_x, k, axis, largest, sorted, &meta_out, &meta_indices);

  std::vector<pir::Type> argument_outputs;
  pir::Type out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_out.dtype()), dense_out.dims(), dense_out.layout(), dense_out.lod(), dense_out.offset());
  argument_outputs.push_back(out_dense_tensor_type);

  pir::Type indices_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_indices.dtype()), dense_indices.dims(), dense_indices.layout(), dense_indices.lod(), dense_indices.offset());
  argument_outputs.push_back(indices_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void TopkOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, pir::AttributeMap attributes) {
  VLOG(4) << "Start build TopkOp";


  IR_ENFORCE(
      attributes.find("k") != attributes.end(),
          "'k' Attribute is expected for TopkOp. ");
  int k = attributes.at("k").dyn_cast<pir::Int32Attribute>().data();

  IR_ENFORCE(
      attributes.find("axis") != attributes.end(),
          "'axis' Attribute is expected for TopkOp. ");
  int axis = attributes.at("axis").dyn_cast<pir::Int32Attribute>().data();

  IR_ENFORCE(
      attributes.find("largest") != attributes.end(),
          "'largest' Attribute is expected for TopkOp. ");
  bool largest = attributes.at("largest").dyn_cast<pir::BoolAttribute>().data();

  IR_ENFORCE(
      attributes.find("sorted") != attributes.end(),
          "'sorted' Attribute is expected for TopkOp. ");
  bool sorted = attributes.at("sorted").dyn_cast<pir::BoolAttribute>().data();

  // Generate scalar mutable attribute: k
  paddle::dialect::FullOp full_k_op = builder.Build<paddle::dialect::FullOp>(std::vector<int64_t>{1}, k, phi::DataType::INT32, phi::CPUPlace());
  pir::OpResult k_ = full_k_op->result(0);
    
  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_, k_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_axis = pir::Int32Attribute::get(pir::IrContext::Instance(), axis);
  argument.AddAttribute("axis", attr_axis);
  pir::Attribute attr_largest = pir::BoolAttribute::get(pir::IrContext::Instance(), largest);
  argument.AddAttribute("largest", attr_largest);
  pir::Attribute attr_sorted = pir::BoolAttribute::get(pir::IrContext::Instance(), sorted);
  argument.AddAttribute("sorted", attr_sorted);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);
  paddle::dialect::IrTensor dense_out;
  paddle::dialect::IrMetaTensor meta_out(&dense_out);
  paddle::dialect::IrTensor dense_indices;
  paddle::dialect::IrMetaTensor meta_indices(&dense_indices);

  phi::TopKInferMeta(meta_x, k, axis, largest, sorted, &meta_out, &meta_indices);

  std::vector<pir::Type> argument_outputs;
  pir::Type out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_out.dtype()), dense_out.dims(), dense_out.layout(), dense_out.lod(), dense_out.offset());
  argument_outputs.push_back(out_dense_tensor_type);

  pir::Type indices_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_indices.dtype()), dense_indices.dims(), dense_indices.layout(), dense_indices.lod(), dense_indices.offset());
  argument_outputs.push_back(indices_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void TopkOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, pir::Value k_, int axis, bool largest, bool sorted) {
  VLOG(4) << "Start build TopkOp";



  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_, k_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_axis = pir::Int32Attribute::get(pir::IrContext::Instance(), axis);
  argument.AddAttribute("axis", attr_axis);
  pir::Attribute attr_largest = pir::BoolAttribute::get(pir::IrContext::Instance(), largest);
  argument.AddAttribute("largest", attr_largest);
  pir::Attribute attr_sorted = pir::BoolAttribute::get(pir::IrContext::Instance(), sorted);
  argument.AddAttribute("sorted", attr_sorted);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;
  phi::Scalar k;
  if (k_.dyn_cast<pir::OpResult>() && k_.dyn_cast<pir::OpResult>().owner()->isa<paddle::dialect::FullOp>()) {
    k = std::move(phi::Scalar(k_.dyn_cast<pir::OpResult>().owner()
                                  ->dyn_cast<paddle::dialect::FullOp>()
                                  .attribute("value")
                                  .dyn_cast<paddle::dialect::ScalarAttribute>()
                                  .data()
                                  .to<int>()));
  }
  else {
    k = std::move(phi::Scalar(-1));
    k.SetFromTensor(true);
  }


  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);
  paddle::dialect::IrTensor dense_out;
  paddle::dialect::IrMetaTensor meta_out(&dense_out);
  paddle::dialect::IrTensor dense_indices;
  paddle::dialect::IrMetaTensor meta_indices(&dense_indices);

  phi::TopKInferMeta(meta_x, k, axis, largest, sorted, &meta_out, &meta_indices);

  std::vector<pir::Type> argument_outputs;
  pir::Type out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_out.dtype()), dense_out.dims(), dense_out.layout(), dense_out.lod(), dense_out.offset());
  argument_outputs.push_back(out_dense_tensor_type);

  pir::Type indices_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_indices.dtype()), dense_indices.dims(), dense_indices.layout(), dense_indices.lod(), dense_indices.offset());
  argument_outputs.push_back(indices_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void TopkOp::VerifySig() {
  VLOG(4) << "Start Verifying inputs, outputs and attributes for: TopkOp.";
  VLOG(4) << "Verifying inputs:";
  {
  auto input_size = num_operands();
  IR_ENFORCE(input_size == 2u,
                    "The size %d of inputs must be equal to 2.", input_size);
  IR_ENFORCE((*this)->operand_source(0).type().isa<paddle::dialect::DenseTensorType>(),
                  "Type validation failed for the 0th input, got %s.", (*this)->operand_source(0).type());
  IR_ENFORCE((*this)->operand_source(1).type().isa<paddle::dialect::DenseTensorType>(),
                  "Type validation failed for the 1th input, got %s.", (*this)->operand_source(1).type());
  }
  VLOG(4) << "Verifying attributes:";
  {
  auto& attributes = this->attributes();
  IR_ENFORCE(attributes.count("axis")>0,
                 "axis does not exist.");
  IR_ENFORCE(attributes.at("axis").isa<pir::Int32Attribute>(),
                 "Type of attribute: axis is not pir::Int32Attribute.");

  IR_ENFORCE(attributes.count("largest")>0,
                 "largest does not exist.");
  IR_ENFORCE(attributes.at("largest").isa<pir::BoolAttribute>(),
                 "Type of attribute: largest is not pir::BoolAttribute.");

  IR_ENFORCE(attributes.count("sorted")>0,
                 "sorted does not exist.");
  IR_ENFORCE(attributes.at("sorted").isa<pir::BoolAttribute>(),
                 "Type of attribute: sorted is not pir::BoolAttribute.");

  }
  VLOG(4) << "Verifying outputs:";
  {
  auto output_size = num_results();
  IR_ENFORCE(output_size == 2u,
                    "The size %d of outputs must be equal to 2.", output_size);
  IR_ENFORCE((*this)->result(0).type().isa<paddle::dialect::DenseTensorType>(),
                 "Type validation failed for the 0th output.");
  IR_ENFORCE((*this)->result(1).type().isa<paddle::dialect::DenseTensorType>(),
                 "Type validation failed for the 1th output.");
  }
  VLOG(4) << "End Verifying for: TopkOp.";
}

void TopkOp::InferMeta( phi::InferMetaContext *infer_meta ) {
  auto fn = PD_INFER_META(phi::TopKInferMeta);
  fn(infer_meta);
}

phi::DataType TopkOp::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: TopkOp";
  


  return expected_kernel_dtype;
}

const char *TraceOp::attributes_name[3] = { "offset", "axis1", "axis2" };

OpInfoTuple TraceOp::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("x", "paddle::dialect::DenseTensorType", false, false, false, true) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = { paddle::dialect::OpAttributeInfo("offset", "pir::Int32Attribute", ""), paddle::dialect::OpAttributeInfo("axis1", "pir::Int32Attribute", ""), paddle::dialect::OpAttributeInfo("axis2", "pir::Int32Attribute", "") };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("out", "paddle::dialect::DenseTensorType", false, false) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("TraceInferMeta", {"x", "offset", "axis1", "axis2"}, "trace", {"x", "offset", "axis1", "axis2"}, {}, {}, {}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "trace");
}

void TraceOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, int offset, int axis1, int axis2) {
  VLOG(4) << "Start build TraceOp";



  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_offset = pir::Int32Attribute::get(pir::IrContext::Instance(), offset);
  argument.AddAttribute("offset", attr_offset);
  pir::Attribute attr_axis1 = pir::Int32Attribute::get(pir::IrContext::Instance(), axis1);
  argument.AddAttribute("axis1", attr_axis1);
  pir::Attribute attr_axis2 = pir::Int32Attribute::get(pir::IrContext::Instance(), axis2);
  argument.AddAttribute("axis2", attr_axis2);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);
  paddle::dialect::IrTensor dense_out;
  paddle::dialect::IrMetaTensor meta_out(&dense_out);

  phi::TraceInferMeta(meta_x, offset, axis1, axis2, &meta_out);

  std::vector<pir::Type> argument_outputs;
  pir::Type out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_out.dtype()), dense_out.dims(), dense_out.layout(), dense_out.lod(), dense_out.offset());
  argument_outputs.push_back(out_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void TraceOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, pir::AttributeMap attributes) {
  VLOG(4) << "Start build TraceOp";


  IR_ENFORCE(
      attributes.find("offset") != attributes.end(),
          "'offset' Attribute is expected for TraceOp. ");
  int offset = attributes.at("offset").dyn_cast<pir::Int32Attribute>().data();

  IR_ENFORCE(
      attributes.find("axis1") != attributes.end(),
          "'axis1' Attribute is expected for TraceOp. ");
  int axis1 = attributes.at("axis1").dyn_cast<pir::Int32Attribute>().data();

  IR_ENFORCE(
      attributes.find("axis2") != attributes.end(),
          "'axis2' Attribute is expected for TraceOp. ");
  int axis2 = attributes.at("axis2").dyn_cast<pir::Int32Attribute>().data();


  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_offset = pir::Int32Attribute::get(pir::IrContext::Instance(), offset);
  argument.AddAttribute("offset", attr_offset);
  pir::Attribute attr_axis1 = pir::Int32Attribute::get(pir::IrContext::Instance(), axis1);
  argument.AddAttribute("axis1", attr_axis1);
  pir::Attribute attr_axis2 = pir::Int32Attribute::get(pir::IrContext::Instance(), axis2);
  argument.AddAttribute("axis2", attr_axis2);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);
  paddle::dialect::IrTensor dense_out;
  paddle::dialect::IrMetaTensor meta_out(&dense_out);

  phi::TraceInferMeta(meta_x, offset, axis1, axis2, &meta_out);

  std::vector<pir::Type> argument_outputs;
  pir::Type out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_out.dtype()), dense_out.dims(), dense_out.layout(), dense_out.lod(), dense_out.offset());
  argument_outputs.push_back(out_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void TraceOp::VerifySig() {
  VLOG(4) << "Start Verifying inputs, outputs and attributes for: TraceOp.";
  VLOG(4) << "Verifying inputs:";
  {
  auto input_size = num_operands();
  IR_ENFORCE(input_size == 1u,
                    "The size %d of inputs must be equal to 1.", input_size);
  IR_ENFORCE((*this)->operand_source(0).type().isa<paddle::dialect::DenseTensorType>(),
                  "Type validation failed for the 0th input, got %s.", (*this)->operand_source(0).type());
  }
  VLOG(4) << "Verifying attributes:";
  {
  auto& attributes = this->attributes();
  IR_ENFORCE(attributes.count("offset")>0,
                 "offset does not exist.");
  IR_ENFORCE(attributes.at("offset").isa<pir::Int32Attribute>(),
                 "Type of attribute: offset is not pir::Int32Attribute.");

  IR_ENFORCE(attributes.count("axis1")>0,
                 "axis1 does not exist.");
  IR_ENFORCE(attributes.at("axis1").isa<pir::Int32Attribute>(),
                 "Type of attribute: axis1 is not pir::Int32Attribute.");

  IR_ENFORCE(attributes.count("axis2")>0,
                 "axis2 does not exist.");
  IR_ENFORCE(attributes.at("axis2").isa<pir::Int32Attribute>(),
                 "Type of attribute: axis2 is not pir::Int32Attribute.");

  }
  VLOG(4) << "Verifying outputs:";
  {
  auto output_size = num_results();
  IR_ENFORCE(output_size == 1u,
                    "The size %d of outputs must be equal to 1.", output_size);
  IR_ENFORCE((*this)->result(0).type().isa<paddle::dialect::DenseTensorType>(),
                 "Type validation failed for the 0th output.");
  }
  VLOG(4) << "End Verifying for: TraceOp.";
}

void TraceOp::InferMeta( phi::InferMetaContext *infer_meta ) {
  auto fn = PD_INFER_META(phi::TraceInferMeta);
  fn(infer_meta);
}

phi::DataType TraceOp::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: TraceOp";
  


  return expected_kernel_dtype;
}

const char *TriangularSolveOp::attributes_name[3] = { "upper", "transpose", "unitriangular" };

OpInfoTuple TriangularSolveOp::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("x", "paddle::dialect::DenseTensorType", false, false, false, true), paddle::dialect::OpInputInfo("y", "paddle::dialect::DenseTensorType", false, false, false, true) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = { paddle::dialect::OpAttributeInfo("upper", "pir::BoolAttribute", ""), paddle::dialect::OpAttributeInfo("transpose", "pir::BoolAttribute", ""), paddle::dialect::OpAttributeInfo("unitriangular", "pir::BoolAttribute", "") };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("out", "paddle::dialect::DenseTensorType", false, false) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("TriangularSolveInferMeta", {"x", "y", "upper", "transpose", "unitriangular"}, "triangular_solve", {"x", "y", "upper", "transpose", "unitriangular"}, {"x"}, {}, {}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "triangular_solve");
}

void TriangularSolveOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, pir::Value y_, bool upper, bool transpose, bool unitriangular) {
  VLOG(4) << "Start build TriangularSolveOp";



  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_, y_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_upper = pir::BoolAttribute::get(pir::IrContext::Instance(), upper);
  argument.AddAttribute("upper", attr_upper);
  pir::Attribute attr_transpose = pir::BoolAttribute::get(pir::IrContext::Instance(), transpose);
  argument.AddAttribute("transpose", attr_transpose);
  pir::Attribute attr_unitriangular = pir::BoolAttribute::get(pir::IrContext::Instance(), unitriangular);
  argument.AddAttribute("unitriangular", attr_unitriangular);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;
  paddle::dialect::DenseTensorType y = y_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)y;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);

  VLOG(4) << "Builder construction  dense_y";
  paddle::dialect::IrTensor ir_tensor_y(paddle::dialect::TransToPhiDataType(y.dtype()),
                                                      y.dims(),
                                                      y.data_layout(),
                                                      y.lod(),
                                                      y.offset());
  VLOG(4) << "Builder construction  meta_y";
  paddle::dialect::IrMetaTensor meta_y(&ir_tensor_y);
  paddle::dialect::IrTensor dense_out;
  paddle::dialect::IrMetaTensor meta_out(&dense_out);

  phi::TriangularSolveInferMeta(meta_x, meta_y, upper, transpose, unitriangular, &meta_out);

  std::vector<pir::Type> argument_outputs;
  pir::Type out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_out.dtype()), dense_out.dims(), dense_out.layout(), dense_out.lod(), dense_out.offset());
  argument_outputs.push_back(out_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void TriangularSolveOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, pir::Value y_, pir::AttributeMap attributes) {
  VLOG(4) << "Start build TriangularSolveOp";


  IR_ENFORCE(
      attributes.find("upper") != attributes.end(),
          "'upper' Attribute is expected for TriangularSolveOp. ");
  bool upper = attributes.at("upper").dyn_cast<pir::BoolAttribute>().data();

  IR_ENFORCE(
      attributes.find("transpose") != attributes.end(),
          "'transpose' Attribute is expected for TriangularSolveOp. ");
  bool transpose = attributes.at("transpose").dyn_cast<pir::BoolAttribute>().data();

  IR_ENFORCE(
      attributes.find("unitriangular") != attributes.end(),
          "'unitriangular' Attribute is expected for TriangularSolveOp. ");
  bool unitriangular = attributes.at("unitriangular").dyn_cast<pir::BoolAttribute>().data();


  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_, y_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_upper = pir::BoolAttribute::get(pir::IrContext::Instance(), upper);
  argument.AddAttribute("upper", attr_upper);
  pir::Attribute attr_transpose = pir::BoolAttribute::get(pir::IrContext::Instance(), transpose);
  argument.AddAttribute("transpose", attr_transpose);
  pir::Attribute attr_unitriangular = pir::BoolAttribute::get(pir::IrContext::Instance(), unitriangular);
  argument.AddAttribute("unitriangular", attr_unitriangular);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;
  paddle::dialect::DenseTensorType y = y_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)y;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);

  VLOG(4) << "Builder construction  dense_y";
  paddle::dialect::IrTensor ir_tensor_y(paddle::dialect::TransToPhiDataType(y.dtype()),
                                                      y.dims(),
                                                      y.data_layout(),
                                                      y.lod(),
                                                      y.offset());
  VLOG(4) << "Builder construction  meta_y";
  paddle::dialect::IrMetaTensor meta_y(&ir_tensor_y);
  paddle::dialect::IrTensor dense_out;
  paddle::dialect::IrMetaTensor meta_out(&dense_out);

  phi::TriangularSolveInferMeta(meta_x, meta_y, upper, transpose, unitriangular, &meta_out);

  std::vector<pir::Type> argument_outputs;
  pir::Type out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_out.dtype()), dense_out.dims(), dense_out.layout(), dense_out.lod(), dense_out.offset());
  argument_outputs.push_back(out_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void TriangularSolveOp::VerifySig() {
  VLOG(4) << "Start Verifying inputs, outputs and attributes for: TriangularSolveOp.";
  VLOG(4) << "Verifying inputs:";
  {
  auto input_size = num_operands();
  IR_ENFORCE(input_size == 2u,
                    "The size %d of inputs must be equal to 2.", input_size);
  IR_ENFORCE((*this)->operand_source(0).type().isa<paddle::dialect::DenseTensorType>(),
                  "Type validation failed for the 0th input, got %s.", (*this)->operand_source(0).type());
  IR_ENFORCE((*this)->operand_source(1).type().isa<paddle::dialect::DenseTensorType>(),
                  "Type validation failed for the 1th input, got %s.", (*this)->operand_source(1).type());
  }
  VLOG(4) << "Verifying attributes:";
  {
  auto& attributes = this->attributes();
  IR_ENFORCE(attributes.count("upper")>0,
                 "upper does not exist.");
  IR_ENFORCE(attributes.at("upper").isa<pir::BoolAttribute>(),
                 "Type of attribute: upper is not pir::BoolAttribute.");

  IR_ENFORCE(attributes.count("transpose")>0,
                 "transpose does not exist.");
  IR_ENFORCE(attributes.at("transpose").isa<pir::BoolAttribute>(),
                 "Type of attribute: transpose is not pir::BoolAttribute.");

  IR_ENFORCE(attributes.count("unitriangular")>0,
                 "unitriangular does not exist.");
  IR_ENFORCE(attributes.at("unitriangular").isa<pir::BoolAttribute>(),
                 "Type of attribute: unitriangular is not pir::BoolAttribute.");

  }
  VLOG(4) << "Verifying outputs:";
  {
  auto output_size = num_results();
  IR_ENFORCE(output_size == 1u,
                    "The size %d of outputs must be equal to 1.", output_size);
  IR_ENFORCE((*this)->result(0).type().isa<paddle::dialect::DenseTensorType>(),
                 "Type validation failed for the 0th output.");
  }
  VLOG(4) << "End Verifying for: TriangularSolveOp.";
}

void TriangularSolveOp::InferMeta( phi::InferMetaContext *infer_meta ) {
  auto fn = PD_INFER_META(phi::TriangularSolveInferMeta);
  fn(infer_meta);
}

phi::DataType TriangularSolveOp::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: TriangularSolveOp";
  


  return expected_kernel_dtype;
}

const char *TrilinearInterpOp::attributes_name[8] = { "data_layout", "out_d", "out_h", "out_w", "scale", "interp_method", "align_corners", "align_mode" };

OpInfoTuple TrilinearInterpOp::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("x", "paddle::dialect::DenseTensorType", false, false, false, true), paddle::dialect::OpInputInfo("out_size", "paddle::dialect::DenseTensorType", true, false, false, false), paddle::dialect::OpInputInfo("size_tensor", "pir::VectorType<paddle::dialect::DenseTensorType>", true, false, false, false), paddle::dialect::OpInputInfo("scale_tensor", "paddle::dialect::DenseTensorType", true, false, false, false) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = { paddle::dialect::OpAttributeInfo("data_layout", "pir::StrAttribute", ""), paddle::dialect::OpAttributeInfo("out_d", "pir::Int32Attribute", ""), paddle::dialect::OpAttributeInfo("out_h", "pir::Int32Attribute", ""), paddle::dialect::OpAttributeInfo("out_w", "pir::Int32Attribute", ""), paddle::dialect::OpAttributeInfo("scale", "pir::ArrayAttribute<pir::FloatAttribute>", ""), paddle::dialect::OpAttributeInfo("interp_method", "pir::StrAttribute", ""), paddle::dialect::OpAttributeInfo("align_corners", "pir::BoolAttribute", ""), paddle::dialect::OpAttributeInfo("align_mode", "pir::Int32Attribute", "") };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("output", "paddle::dialect::DenseTensorType", false, false) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("InterpolateInferMeta", {"x", "out_size", "size_tensor", "scale_tensor", "data_layout", "out_d", "out_h", "out_w", "scale", "interp_method", "align_corners", "align_mode"}, "trilinear_interp", {"x", "out_size", "size_tensor", "scale_tensor", "data_layout", "out_d", "out_h", "out_w", "scale", "interp_method", "align_corners", "align_mode"}, {"x"}, {}, {}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "trilinear_interp");
}

void TrilinearInterpOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, pir::Value out_size_, pir::Value size_tensor_, pir::Value scale_tensor_, const std::string& data_layout, int out_d, int out_h, int out_w, const std::vector<float>& scale, const std::string& interp_method, bool align_corners, int align_mode) {
  VLOG(4) << "Start build TrilinearInterpOp";



  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_, out_size_, size_tensor_, scale_tensor_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_data_layout = pir::StrAttribute::get(pir::IrContext::Instance(), data_layout);
  argument.AddAttribute("data_layout", attr_data_layout);
  pir::Attribute attr_out_d = pir::Int32Attribute::get(pir::IrContext::Instance(), out_d);
  argument.AddAttribute("out_d", attr_out_d);
  pir::Attribute attr_out_h = pir::Int32Attribute::get(pir::IrContext::Instance(), out_h);
  argument.AddAttribute("out_h", attr_out_h);
  pir::Attribute attr_out_w = pir::Int32Attribute::get(pir::IrContext::Instance(), out_w);
  argument.AddAttribute("out_w", attr_out_w);
  std::vector<pir::Attribute> vec_scale;
  for (size_t i = 0; i < static_cast<size_t>(scale.size()); i++) {
      pir::Attribute attr_scale = pir::FloatAttribute::get(pir::IrContext::Instance(), scale[i]);

    vec_scale.push_back(attr_scale);
  }
  pir::Attribute attr_scale = pir::ArrayAttribute::get(pir::IrContext::Instance(), vec_scale);
  argument.AddAttribute("scale", attr_scale);
  pir::Attribute attr_interp_method = pir::StrAttribute::get(pir::IrContext::Instance(), interp_method);
  argument.AddAttribute("interp_method", attr_interp_method);
  pir::Attribute attr_align_corners = pir::BoolAttribute::get(pir::IrContext::Instance(), align_corners);
  argument.AddAttribute("align_corners", attr_align_corners);
  pir::Attribute attr_align_mode = pir::Int32Attribute::get(pir::IrContext::Instance(), align_mode);
  argument.AddAttribute("align_mode", attr_align_mode);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);

  paddle::dialect::IrMetaTensor meta_out_size;
  paddle::dialect::IrTensor ir_tensor_out_size;
  if (out_size_.impl() != nullptr) {
    paddle::dialect::DenseTensorType out_size = out_size_.type().dyn_cast<paddle::dialect::DenseTensorType>();
    VLOG(4) << "Builder construction  dense_out_size";
    ir_tensor_out_size = paddle::dialect::IrTensor(paddle::dialect::TransToPhiDataType(out_size.dtype()),
                                                        out_size.dims(),
                                                        out_size.data_layout(),
                                                        out_size.lod(),
                                                        out_size.offset());
    VLOG(4) << "Builder construction  meta_out_size";
    meta_out_size = paddle::dialect::IrMetaTensor(&ir_tensor_out_size);
  }

  std::vector<paddle::dialect::IrTensor> vec_ir_tensor_size_tensor;
  if (size_tensor_.impl() != nullptr) {
    pir::VectorType size_tensor = size_tensor_.type().dyn_cast<pir::VectorType>();
    for (size_t i=0; i < static_cast<size_t>(size_tensor.size()); i++) {
        vec_ir_tensor_size_tensor.push_back(paddle::dialect::IrTensor(paddle::dialect::TransToPhiDataType(size_tensor[i].dyn_cast<paddle::dialect::DenseTensorType>().dtype()),
                                                                        size_tensor[i].dyn_cast<paddle::dialect::DenseTensorType>().dims(),
                                                                        size_tensor[i].dyn_cast<paddle::dialect::DenseTensorType>().data_layout(),
                                                                        size_tensor[i].dyn_cast<paddle::dialect::DenseTensorType>().lod(),
                                                                        size_tensor[i].dyn_cast<paddle::dialect::DenseTensorType>().offset()));
    }
  }

  std::vector<paddle::dialect::IrMetaTensor> vec_meta_size_tensor;
  for (size_t i=0; i < vec_ir_tensor_size_tensor.size(); i++) {
    vec_meta_size_tensor.push_back(paddle::dialect::IrMetaTensor(&vec_ir_tensor_size_tensor[i]));
  }

  std::vector<const phi::MetaTensor*> meta_size_tensor;
  for (size_t i=0; i < static_cast<size_t>(vec_meta_size_tensor.size()); i++) {
    meta_size_tensor.push_back(&vec_meta_size_tensor[i]);
  }


  paddle::dialect::IrMetaTensor meta_scale_tensor;
  paddle::dialect::IrTensor ir_tensor_scale_tensor;
  if (scale_tensor_.impl() != nullptr) {
    paddle::dialect::DenseTensorType scale_tensor = scale_tensor_.type().dyn_cast<paddle::dialect::DenseTensorType>();
    VLOG(4) << "Builder construction  dense_scale_tensor";
    ir_tensor_scale_tensor = paddle::dialect::IrTensor(paddle::dialect::TransToPhiDataType(scale_tensor.dtype()),
                                                        scale_tensor.dims(),
                                                        scale_tensor.data_layout(),
                                                        scale_tensor.lod(),
                                                        scale_tensor.offset());
    VLOG(4) << "Builder construction  meta_scale_tensor";
    meta_scale_tensor = paddle::dialect::IrMetaTensor(&ir_tensor_scale_tensor);
  }

  paddle::dialect::IrTensor dense_output;
  paddle::dialect::IrMetaTensor meta_output(&dense_output);

  phi::InterpolateInferMeta(meta_x, meta_out_size, meta_size_tensor, meta_scale_tensor, data_layout, out_d, out_h, out_w, scale, interp_method, align_corners, align_mode, &meta_output, phi::MetaConfig(false, false));

  std::vector<pir::Type> argument_outputs;
  pir::Type output_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_output.dtype()), dense_output.dims(), dense_output.layout(), dense_output.lod(), dense_output.offset());
  argument_outputs.push_back(output_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void TrilinearInterpOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, pir::Value out_size_, pir::Value size_tensor_, pir::Value scale_tensor_, pir::AttributeMap attributes) {
  VLOG(4) << "Start build TrilinearInterpOp";


  IR_ENFORCE(
      attributes.find("data_layout") != attributes.end(),
          "'data_layout' Attribute is expected for TrilinearInterpOp. ");
  std::string data_layout = attributes.at("data_layout").dyn_cast<pir::StrAttribute>().AsString();

  IR_ENFORCE(
      attributes.find("out_d") != attributes.end(),
          "'out_d' Attribute is expected for TrilinearInterpOp. ");
  int out_d = attributes.at("out_d").dyn_cast<pir::Int32Attribute>().data();

  IR_ENFORCE(
      attributes.find("out_h") != attributes.end(),
          "'out_h' Attribute is expected for TrilinearInterpOp. ");
  int out_h = attributes.at("out_h").dyn_cast<pir::Int32Attribute>().data();

  IR_ENFORCE(
      attributes.find("out_w") != attributes.end(),
          "'out_w' Attribute is expected for TrilinearInterpOp. ");
  int out_w = attributes.at("out_w").dyn_cast<pir::Int32Attribute>().data();

  IR_ENFORCE(
      attributes.find("scale") != attributes.end(),
          "'scale' Attribute is expected for TrilinearInterpOp. ");
  std::vector<float> scale;
  for (size_t i = 0; i < attributes.at("scale").dyn_cast<pir::ArrayAttribute>().size(); i++) {
    scale.push_back(attributes.at("scale").dyn_cast<pir::ArrayAttribute>().at(i).dyn_cast<pir::FloatAttribute>().data());
  }

  IR_ENFORCE(
      attributes.find("interp_method") != attributes.end(),
          "'interp_method' Attribute is expected for TrilinearInterpOp. ");
  std::string interp_method = attributes.at("interp_method").dyn_cast<pir::StrAttribute>().AsString();

  IR_ENFORCE(
      attributes.find("align_corners") != attributes.end(),
          "'align_corners' Attribute is expected for TrilinearInterpOp. ");
  bool align_corners = attributes.at("align_corners").dyn_cast<pir::BoolAttribute>().data();

  IR_ENFORCE(
      attributes.find("align_mode") != attributes.end(),
          "'align_mode' Attribute is expected for TrilinearInterpOp. ");
  int align_mode = attributes.at("align_mode").dyn_cast<pir::Int32Attribute>().data();


  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_, out_size_, size_tensor_, scale_tensor_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_data_layout = pir::StrAttribute::get(pir::IrContext::Instance(), data_layout);
  argument.AddAttribute("data_layout", attr_data_layout);
  pir::Attribute attr_out_d = pir::Int32Attribute::get(pir::IrContext::Instance(), out_d);
  argument.AddAttribute("out_d", attr_out_d);
  pir::Attribute attr_out_h = pir::Int32Attribute::get(pir::IrContext::Instance(), out_h);
  argument.AddAttribute("out_h", attr_out_h);
  pir::Attribute attr_out_w = pir::Int32Attribute::get(pir::IrContext::Instance(), out_w);
  argument.AddAttribute("out_w", attr_out_w);
  std::vector<pir::Attribute> vec_scale;
  for (size_t i = 0; i < static_cast<size_t>(scale.size()); i++) {
      pir::Attribute attr_scale = pir::FloatAttribute::get(pir::IrContext::Instance(), scale[i]);

    vec_scale.push_back(attr_scale);
  }
  pir::Attribute attr_scale = pir::ArrayAttribute::get(pir::IrContext::Instance(), vec_scale);
  argument.AddAttribute("scale", attr_scale);
  pir::Attribute attr_interp_method = pir::StrAttribute::get(pir::IrContext::Instance(), interp_method);
  argument.AddAttribute("interp_method", attr_interp_method);
  pir::Attribute attr_align_corners = pir::BoolAttribute::get(pir::IrContext::Instance(), align_corners);
  argument.AddAttribute("align_corners", attr_align_corners);
  pir::Attribute attr_align_mode = pir::Int32Attribute::get(pir::IrContext::Instance(), align_mode);
  argument.AddAttribute("align_mode", attr_align_mode);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);

  paddle::dialect::IrMetaTensor meta_out_size;
  paddle::dialect::IrTensor ir_tensor_out_size;
  if (out_size_.impl() != nullptr) {
    paddle::dialect::DenseTensorType out_size = out_size_.type().dyn_cast<paddle::dialect::DenseTensorType>();
    VLOG(4) << "Builder construction  dense_out_size";
    ir_tensor_out_size = paddle::dialect::IrTensor(paddle::dialect::TransToPhiDataType(out_size.dtype()),
                                                        out_size.dims(),
                                                        out_size.data_layout(),
                                                        out_size.lod(),
                                                        out_size.offset());
    VLOG(4) << "Builder construction  meta_out_size";
    meta_out_size = paddle::dialect::IrMetaTensor(&ir_tensor_out_size);
  }

  std::vector<paddle::dialect::IrTensor> vec_ir_tensor_size_tensor;
  if (size_tensor_.impl() != nullptr) {
    pir::VectorType size_tensor = size_tensor_.type().dyn_cast<pir::VectorType>();
    for (size_t i=0; i < static_cast<size_t>(size_tensor.size()); i++) {
        vec_ir_tensor_size_tensor.push_back(paddle::dialect::IrTensor(paddle::dialect::TransToPhiDataType(size_tensor[i].dyn_cast<paddle::dialect::DenseTensorType>().dtype()),
                                                                        size_tensor[i].dyn_cast<paddle::dialect::DenseTensorType>().dims(),
                                                                        size_tensor[i].dyn_cast<paddle::dialect::DenseTensorType>().data_layout(),
                                                                        size_tensor[i].dyn_cast<paddle::dialect::DenseTensorType>().lod(),
                                                                        size_tensor[i].dyn_cast<paddle::dialect::DenseTensorType>().offset()));
    }
  }

  std::vector<paddle::dialect::IrMetaTensor> vec_meta_size_tensor;
  for (size_t i=0; i < vec_ir_tensor_size_tensor.size(); i++) {
    vec_meta_size_tensor.push_back(paddle::dialect::IrMetaTensor(&vec_ir_tensor_size_tensor[i]));
  }

  std::vector<const phi::MetaTensor*> meta_size_tensor;
  for (size_t i=0; i < static_cast<size_t>(vec_meta_size_tensor.size()); i++) {
    meta_size_tensor.push_back(&vec_meta_size_tensor[i]);
  }


  paddle::dialect::IrMetaTensor meta_scale_tensor;
  paddle::dialect::IrTensor ir_tensor_scale_tensor;
  if (scale_tensor_.impl() != nullptr) {
    paddle::dialect::DenseTensorType scale_tensor = scale_tensor_.type().dyn_cast<paddle::dialect::DenseTensorType>();
    VLOG(4) << "Builder construction  dense_scale_tensor";
    ir_tensor_scale_tensor = paddle::dialect::IrTensor(paddle::dialect::TransToPhiDataType(scale_tensor.dtype()),
                                                        scale_tensor.dims(),
                                                        scale_tensor.data_layout(),
                                                        scale_tensor.lod(),
                                                        scale_tensor.offset());
    VLOG(4) << "Builder construction  meta_scale_tensor";
    meta_scale_tensor = paddle::dialect::IrMetaTensor(&ir_tensor_scale_tensor);
  }

  paddle::dialect::IrTensor dense_output;
  paddle::dialect::IrMetaTensor meta_output(&dense_output);

  phi::InterpolateInferMeta(meta_x, meta_out_size, meta_size_tensor, meta_scale_tensor, data_layout, out_d, out_h, out_w, scale, interp_method, align_corners, align_mode, &meta_output, phi::MetaConfig(false, false));

  std::vector<pir::Type> argument_outputs;
  pir::Type output_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_output.dtype()), dense_output.dims(), dense_output.layout(), dense_output.lod(), dense_output.offset());
  argument_outputs.push_back(output_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void TrilinearInterpOp::VerifySig() {
  VLOG(4) << "Start Verifying inputs, outputs and attributes for: TrilinearInterpOp.";
  VLOG(4) << "Verifying inputs:";
  {
  auto input_size = num_operands();
  IR_ENFORCE(input_size == 4u,
                    "The size %d of inputs must be equal to 4.", input_size);
  IR_ENFORCE((*this)->operand_source(0).type().isa<paddle::dialect::DenseTensorType>(),
                  "Type validation failed for the 0th input, got %s.", (*this)->operand_source(0).type());
  if (auto val = (*this)->operand(1)) {
    IR_ENFORCE(val.type().isa<paddle::dialect::DenseTensorType>(),
                   "Type validation failed for the 1th input, got %s.", (*this)->operand_source(1).type());
  }
  if (auto val =  (*this)->operand(2)) {
    if (auto vec_type = val.type().dyn_cast<pir::VectorType>()) {
      for (size_t i = 0; i < vec_type.size(); i++) {
        IR_ENFORCE(vec_type[i].isa<paddle::dialect::DenseTensorType>(),
                          "Type validation failed for the 2th input, got %s.", (*this)->operand_source(2).type());
      }
    }
    else {
      IR_ENFORCE(val.type().isa<paddle::dialect::DenseTensorType>(),
                        "Type validation failed for the 2th input, got %s.", (*this)->operand_source(2).type());
    }
  }
  if (auto val = (*this)->operand(3)) {
    IR_ENFORCE(val.type().isa<paddle::dialect::DenseTensorType>(),
                   "Type validation failed for the 3th input, got %s.", (*this)->operand_source(3).type());
  }
  }
  VLOG(4) << "Verifying attributes:";
  {
  auto& attributes = this->attributes();
  IR_ENFORCE(attributes.count("data_layout")>0,
                 "data_layout does not exist.");
  IR_ENFORCE(attributes.at("data_layout").isa<pir::StrAttribute>(),
                 "Type of attribute: data_layout is not pir::StrAttribute.");

  IR_ENFORCE(attributes.count("out_d")>0,
                 "out_d does not exist.");
  IR_ENFORCE(attributes.at("out_d").isa<pir::Int32Attribute>(),
                 "Type of attribute: out_d is not pir::Int32Attribute.");

  IR_ENFORCE(attributes.count("out_h")>0,
                 "out_h does not exist.");
  IR_ENFORCE(attributes.at("out_h").isa<pir::Int32Attribute>(),
                 "Type of attribute: out_h is not pir::Int32Attribute.");

  IR_ENFORCE(attributes.count("out_w")>0,
                 "out_w does not exist.");
  IR_ENFORCE(attributes.at("out_w").isa<pir::Int32Attribute>(),
                 "Type of attribute: out_w is not pir::Int32Attribute.");

  IR_ENFORCE(attributes.count("scale")>0,
                 "scale does not exist.");
  IR_ENFORCE(attributes.at("scale").isa<pir::ArrayAttribute>(),
                 "Type of attribute: scale is not pir::ArrayAttribute.");
  for (size_t i = 0; i < attributes.at("scale").dyn_cast<pir::ArrayAttribute>().size(); i++) {
    IR_ENFORCE(attributes.at("scale").dyn_cast<pir::ArrayAttribute>().at(i).isa<pir::FloatAttribute>(),
                   "Type of attribute: scale is not right.");
  }
  IR_ENFORCE(attributes.count("interp_method")>0,
                 "interp_method does not exist.");
  IR_ENFORCE(attributes.at("interp_method").isa<pir::StrAttribute>(),
                 "Type of attribute: interp_method is not pir::StrAttribute.");

  IR_ENFORCE(attributes.count("align_corners")>0,
                 "align_corners does not exist.");
  IR_ENFORCE(attributes.at("align_corners").isa<pir::BoolAttribute>(),
                 "Type of attribute: align_corners is not pir::BoolAttribute.");

  IR_ENFORCE(attributes.count("align_mode")>0,
                 "align_mode does not exist.");
  IR_ENFORCE(attributes.at("align_mode").isa<pir::Int32Attribute>(),
                 "Type of attribute: align_mode is not pir::Int32Attribute.");

  }
  VLOG(4) << "Verifying outputs:";
  {
  auto output_size = num_results();
  IR_ENFORCE(output_size == 1u,
                    "The size %d of outputs must be equal to 1.", output_size);
  IR_ENFORCE((*this)->result(0).type().isa<paddle::dialect::DenseTensorType>(),
                 "Type validation failed for the 0th output.");
  }
  VLOG(4) << "End Verifying for: TrilinearInterpOp.";
}

void TrilinearInterpOp::InferMeta( phi::InferMetaContext *infer_meta ) {
  auto fn = PD_INFER_META(phi::InterpolateInferMeta);
  fn(infer_meta);
}

phi::DataType TrilinearInterpOp::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: TrilinearInterpOp";
  

  // deal skip data transform
  if (var_name == "out_size" || var_name == "size_tensor" || var_name == "scale_tensor"){
    return expected_kernel_dtype;
  }


  return expected_kernel_dtype;
}

OpInfoTuple TruncOp::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("input", "paddle::dialect::DenseTensorType", false, false, false, true) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = {  };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("out", "paddle::dialect::DenseTensorType", false, false) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("UnchangedInferMeta", {"input"}, "trunc", {"input"}, {}, {}, {}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "trunc");
}

void TruncOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value input_) {
  VLOG(4) << "Start build TruncOp";



  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {input_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType input = input_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)input;

  VLOG(4) << "Builder construction  dense_input";
  paddle::dialect::IrTensor ir_tensor_input(paddle::dialect::TransToPhiDataType(input.dtype()),
                                                      input.dims(),
                                                      input.data_layout(),
                                                      input.lod(),
                                                      input.offset());
  VLOG(4) << "Builder construction  meta_input";
  paddle::dialect::IrMetaTensor meta_input(&ir_tensor_input);
  paddle::dialect::IrTensor dense_out;
  paddle::dialect::IrMetaTensor meta_out(&dense_out);

  phi::UnchangedInferMeta(meta_input, &meta_out);

  std::vector<pir::Type> argument_outputs;
  pir::Type out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_out.dtype()), dense_out.dims(), dense_out.layout(), dense_out.lod(), dense_out.offset());
  argument_outputs.push_back(out_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void TruncOp::VerifySig() {
  VLOG(4) << "Start Verifying inputs, outputs and attributes for: TruncOp.";
  VLOG(4) << "Verifying inputs:";
  {
  auto input_size = num_operands();
  IR_ENFORCE(input_size == 1u,
                    "The size %d of inputs must be equal to 1.", input_size);
  IR_ENFORCE((*this)->operand_source(0).type().isa<paddle::dialect::DenseTensorType>(),
                  "Type validation failed for the 0th input, got %s.", (*this)->operand_source(0).type());
  }
  VLOG(4) << "Verifying attributes:";
  {
  // Attributes num is 0, not need to check attributes type.
  }
  VLOG(4) << "Verifying outputs:";
  {
  auto output_size = num_results();
  IR_ENFORCE(output_size == 1u,
                    "The size %d of outputs must be equal to 1.", output_size);
  IR_ENFORCE((*this)->result(0).type().isa<paddle::dialect::DenseTensorType>(),
                 "Type validation failed for the 0th output.");
  }
  VLOG(4) << "End Verifying for: TruncOp.";
}

void TruncOp::InferMeta( phi::InferMetaContext *infer_meta ) {
  auto fn = PD_INFER_META(phi::UnchangedInferMeta);
  fn(infer_meta);
}

phi::DataType TruncOp::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: TruncOp";
  


  return expected_kernel_dtype;
}

OpInfoTuple Trunc_Op::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("input", "paddle::dialect::DenseTensorType", false, false, false, true) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = {  };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("out", "paddle::dialect::DenseTensorType", false, false) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("UnchangedInferMeta", {"input"}, "trunc", {"input"}, {}, {}, {{"out", "input"}}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "trunc");
}

void Trunc_Op::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value input_) {
  VLOG(4) << "Start build Trunc_Op";



  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {input_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType input = input_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)input;

  VLOG(4) << "Builder construction  dense_input";
  paddle::dialect::IrTensor ir_tensor_input(paddle::dialect::TransToPhiDataType(input.dtype()),
                                                      input.dims(),
                                                      input.data_layout(),
                                                      input.lod(),
                                                      input.offset());
  VLOG(4) << "Builder construction  meta_input";
  paddle::dialect::IrMetaTensor meta_input(&ir_tensor_input);
  paddle::dialect::IrTensor dense_out;
  paddle::dialect::IrMetaTensor meta_out(&dense_out);

  phi::UnchangedInferMeta(meta_input, &meta_out);

  std::vector<pir::Type> argument_outputs;
  pir::Type out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_out.dtype()), dense_out.dims(), dense_out.layout(), dense_out.lod(), dense_out.offset());
  argument_outputs.push_back(out_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void Trunc_Op::VerifySig() {
  VLOG(4) << "Start Verifying inputs, outputs and attributes for: Trunc_Op.";
  VLOG(4) << "Verifying inputs:";
  {
  auto input_size = num_operands();
  IR_ENFORCE(input_size == 1u,
                    "The size %d of inputs must be equal to 1.", input_size);
  IR_ENFORCE((*this)->operand_source(0).type().isa<paddle::dialect::DenseTensorType>(),
                  "Type validation failed for the 0th input, got %s.", (*this)->operand_source(0).type());
  }
  VLOG(4) << "Verifying attributes:";
  {
  // Attributes num is 0, not need to check attributes type.
  }
  VLOG(4) << "Verifying outputs:";
  {
  auto output_size = num_results();
  IR_ENFORCE(output_size == 1u,
                    "The size %d of outputs must be equal to 1.", output_size);
  IR_ENFORCE((*this)->result(0).type().isa<paddle::dialect::DenseTensorType>(),
                 "Type validation failed for the 0th output.");
  }
  VLOG(4) << "End Verifying for: Trunc_Op.";
}

void Trunc_Op::InferMeta( phi::InferMetaContext *infer_meta ) {
  auto fn = PD_INFER_META(phi::UnchangedInferMeta);
  fn(infer_meta);
}

phi::DataType Trunc_Op::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: Trunc_Op";
  


  return expected_kernel_dtype;
}

const char *UnbindOp::attributes_name[1] = { "axis" };

OpInfoTuple UnbindOp::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("input", "paddle::dialect::DenseTensorType", false, false, false, true) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = { paddle::dialect::OpAttributeInfo("axis", "pir::Int32Attribute", "") };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("out", "pir::VectorType<paddle::dialect::DenseTensorType>", false, false) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("UnbindInferMeta", {"input", "axis"}, "unbind", {"input", "axis"}, {}, {}, {}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "unbind");
}

void UnbindOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value input_, int axis) {
  VLOG(4) << "Start build UnbindOp";



  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {input_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_axis = pir::Int32Attribute::get(pir::IrContext::Instance(), axis);
  argument.AddAttribute("axis", attr_axis);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType input = input_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)input;

  VLOG(4) << "Builder construction  dense_input";
  paddle::dialect::IrTensor ir_tensor_input(paddle::dialect::TransToPhiDataType(input.dtype()),
                                                      input.dims(),
                                                      input.data_layout(),
                                                      input.lod(),
                                                      input.offset());
  VLOG(4) << "Builder construction  meta_input";
  paddle::dialect::IrMetaTensor meta_input(&ir_tensor_input);
  std::vector<paddle::dialect::IrTensor> vec_dense_out((axis<0 ? input.dims()[input.dims().size()+axis]:input.dims()[axis]), paddle::dialect::IrTensor());
  std::vector<paddle::dialect::IrMetaTensor> vec_meta_out;
  for (size_t i=0; i < static_cast<size_t>(axis<0 ? input.dims()[input.dims().size()+axis]:input.dims()[axis]); i++) {
    vec_meta_out.push_back(paddle::dialect::IrMetaTensor(&vec_dense_out[i]));
  }
  std::vector<phi::MetaTensor*> meta_out;
  for (size_t i=0; i < static_cast<size_t>(vec_meta_out.size()); i++) {
    meta_out.push_back(&vec_meta_out[i]);
  }

  phi::UnbindInferMeta(meta_input, axis, meta_out);

  std::vector<pir::Type> argument_outputs;
  std::vector<pir::Type> out_types;
  for (size_t i=0; i < static_cast<size_t>(axis<0 ? input.dims()[input.dims().size()+axis]:input.dims()[axis]); i++) {
    out_types.push_back(paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(vec_dense_out[i].dtype()), vec_dense_out[i].dims(), vec_dense_out[i].layout(), vec_dense_out[i].lod(), vec_dense_out[i].offset()));
  }
  pir::Type out_vector_type = pir::VectorType::get(pir::IrContext::Instance(), out_types);
  argument_outputs.push_back(out_vector_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void UnbindOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value input_, pir::AttributeMap attributes) {
  VLOG(4) << "Start build UnbindOp";


  IR_ENFORCE(
      attributes.find("axis") != attributes.end(),
          "'axis' Attribute is expected for UnbindOp. ");
  int axis = attributes.at("axis").dyn_cast<pir::Int32Attribute>().data();


  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {input_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_axis = pir::Int32Attribute::get(pir::IrContext::Instance(), axis);
  argument.AddAttribute("axis", attr_axis);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType input = input_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)input;

  VLOG(4) << "Builder construction  dense_input";
  paddle::dialect::IrTensor ir_tensor_input(paddle::dialect::TransToPhiDataType(input.dtype()),
                                                      input.dims(),
                                                      input.data_layout(),
                                                      input.lod(),
                                                      input.offset());
  VLOG(4) << "Builder construction  meta_input";
  paddle::dialect::IrMetaTensor meta_input(&ir_tensor_input);
  std::vector<paddle::dialect::IrTensor> vec_dense_out((axis<0 ? input.dims()[input.dims().size()+axis]:input.dims()[axis]), paddle::dialect::IrTensor());
  std::vector<paddle::dialect::IrMetaTensor> vec_meta_out;
  for (size_t i=0; i < static_cast<size_t>(axis<0 ? input.dims()[input.dims().size()+axis]:input.dims()[axis]); i++) {
    vec_meta_out.push_back(paddle::dialect::IrMetaTensor(&vec_dense_out[i]));
  }
  std::vector<phi::MetaTensor*> meta_out;
  for (size_t i=0; i < static_cast<size_t>(vec_meta_out.size()); i++) {
    meta_out.push_back(&vec_meta_out[i]);
  }

  phi::UnbindInferMeta(meta_input, axis, meta_out);

  std::vector<pir::Type> argument_outputs;
  std::vector<pir::Type> out_types;
  for (size_t i=0; i < static_cast<size_t>(axis<0 ? input.dims()[input.dims().size()+axis]:input.dims()[axis]); i++) {
    out_types.push_back(paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(vec_dense_out[i].dtype()), vec_dense_out[i].dims(), vec_dense_out[i].layout(), vec_dense_out[i].lod(), vec_dense_out[i].offset()));
  }
  pir::Type out_vector_type = pir::VectorType::get(pir::IrContext::Instance(), out_types);
  argument_outputs.push_back(out_vector_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void UnbindOp::VerifySig() {
  VLOG(4) << "Start Verifying inputs, outputs and attributes for: UnbindOp.";
  VLOG(4) << "Verifying inputs:";
  {
  auto input_size = num_operands();
  IR_ENFORCE(input_size == 1u,
                    "The size %d of inputs must be equal to 1.", input_size);
  IR_ENFORCE((*this)->operand_source(0).type().isa<paddle::dialect::DenseTensorType>(),
                  "Type validation failed for the 0th input, got %s.", (*this)->operand_source(0).type());
  }
  VLOG(4) << "Verifying attributes:";
  {
  auto& attributes = this->attributes();
  IR_ENFORCE(attributes.count("axis")>0,
                 "axis does not exist.");
  IR_ENFORCE(attributes.at("axis").isa<pir::Int32Attribute>(),
                 "Type of attribute: axis is not pir::Int32Attribute.");

  }
  VLOG(4) << "Verifying outputs:";
  {
  auto output_size = num_results();
  IR_ENFORCE(output_size == 1u,
                    "The size %d of outputs must be equal to 1.", output_size);
  auto output_0_type = (*this)->result(0).type();
  if (auto vec_type = output_0_type.dyn_cast<pir::VectorType>()) {
    for (size_t i = 0; i < vec_type.size(); i++) {
      IR_ENFORCE(vec_type[i].isa<paddle::dialect::DenseTensorType>(),
                     "Type validation failed for the 0th output.");
    }
  }
  else {
    IR_ENFORCE(output_0_type.isa<paddle::dialect::DenseTensorType>(),
                   "Type validation failed for the 0th output.");
  }
  }
  VLOG(4) << "End Verifying for: UnbindOp.";
}

void UnbindOp::InferMeta( phi::InferMetaContext *infer_meta ) {
  auto fn = PD_INFER_META(phi::UnbindInferMeta);
  fn(infer_meta);
}

phi::DataType UnbindOp::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: UnbindOp";
  


  return expected_kernel_dtype;
}

const char *UnfoldOp::attributes_name[4] = { "kernel_sizes", "strides", "paddings", "dilations" };

OpInfoTuple UnfoldOp::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("x", "paddle::dialect::DenseTensorType", false, false, false, true) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = { paddle::dialect::OpAttributeInfo("kernel_sizes", "pir::ArrayAttribute<pir::Int32Attribute>", ""), paddle::dialect::OpAttributeInfo("strides", "pir::ArrayAttribute<pir::Int32Attribute>", ""), paddle::dialect::OpAttributeInfo("paddings", "pir::ArrayAttribute<pir::Int32Attribute>", ""), paddle::dialect::OpAttributeInfo("dilations", "pir::ArrayAttribute<pir::Int32Attribute>", "") };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("out", "paddle::dialect::DenseTensorType", false, false) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("UnfoldInferMeta", {"x", "kernel_sizes", "strides", "paddings", "dilations"}, "unfold", {"x", "kernel_sizes", "strides", "paddings", "dilations"}, {}, {}, {}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "unfold");
}

void UnfoldOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, const std::vector<int>& kernel_sizes, const std::vector<int>& strides, const std::vector<int>& paddings, const std::vector<int>& dilations) {
  VLOG(4) << "Start build UnfoldOp";



  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  std::vector<pir::Attribute> vec_kernel_sizes;
  for (size_t i = 0; i < static_cast<size_t>(kernel_sizes.size()); i++) {
      pir::Attribute attr_kernel_sizes = pir::Int32Attribute::get(pir::IrContext::Instance(), kernel_sizes[i]);

    vec_kernel_sizes.push_back(attr_kernel_sizes);
  }
  pir::Attribute attr_kernel_sizes = pir::ArrayAttribute::get(pir::IrContext::Instance(), vec_kernel_sizes);
  argument.AddAttribute("kernel_sizes", attr_kernel_sizes);
  std::vector<pir::Attribute> vec_strides;
  for (size_t i = 0; i < static_cast<size_t>(strides.size()); i++) {
      pir::Attribute attr_strides = pir::Int32Attribute::get(pir::IrContext::Instance(), strides[i]);

    vec_strides.push_back(attr_strides);
  }
  pir::Attribute attr_strides = pir::ArrayAttribute::get(pir::IrContext::Instance(), vec_strides);
  argument.AddAttribute("strides", attr_strides);
  std::vector<pir::Attribute> vec_paddings;
  for (size_t i = 0; i < static_cast<size_t>(paddings.size()); i++) {
      pir::Attribute attr_paddings = pir::Int32Attribute::get(pir::IrContext::Instance(), paddings[i]);

    vec_paddings.push_back(attr_paddings);
  }
  pir::Attribute attr_paddings = pir::ArrayAttribute::get(pir::IrContext::Instance(), vec_paddings);
  argument.AddAttribute("paddings", attr_paddings);
  std::vector<pir::Attribute> vec_dilations;
  for (size_t i = 0; i < static_cast<size_t>(dilations.size()); i++) {
      pir::Attribute attr_dilations = pir::Int32Attribute::get(pir::IrContext::Instance(), dilations[i]);

    vec_dilations.push_back(attr_dilations);
  }
  pir::Attribute attr_dilations = pir::ArrayAttribute::get(pir::IrContext::Instance(), vec_dilations);
  argument.AddAttribute("dilations", attr_dilations);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);
  paddle::dialect::IrTensor dense_out;
  paddle::dialect::IrMetaTensor meta_out(&dense_out);

  phi::UnfoldInferMeta(meta_x, kernel_sizes, strides, paddings, dilations, &meta_out);

  std::vector<pir::Type> argument_outputs;
  pir::Type out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_out.dtype()), dense_out.dims(), dense_out.layout(), dense_out.lod(), dense_out.offset());
  argument_outputs.push_back(out_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void UnfoldOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, pir::AttributeMap attributes) {
  VLOG(4) << "Start build UnfoldOp";


  IR_ENFORCE(
      attributes.find("kernel_sizes") != attributes.end(),
          "'kernel_sizes' Attribute is expected for UnfoldOp. ");
  std::vector<int> kernel_sizes;
  for (size_t i = 0; i < attributes.at("kernel_sizes").dyn_cast<pir::ArrayAttribute>().size(); i++) {
    kernel_sizes.push_back(attributes.at("kernel_sizes").dyn_cast<pir::ArrayAttribute>().at(i).dyn_cast<pir::Int32Attribute>().data());
  }

  IR_ENFORCE(
      attributes.find("strides") != attributes.end(),
          "'strides' Attribute is expected for UnfoldOp. ");
  std::vector<int> strides;
  for (size_t i = 0; i < attributes.at("strides").dyn_cast<pir::ArrayAttribute>().size(); i++) {
    strides.push_back(attributes.at("strides").dyn_cast<pir::ArrayAttribute>().at(i).dyn_cast<pir::Int32Attribute>().data());
  }

  IR_ENFORCE(
      attributes.find("paddings") != attributes.end(),
          "'paddings' Attribute is expected for UnfoldOp. ");
  std::vector<int> paddings;
  for (size_t i = 0; i < attributes.at("paddings").dyn_cast<pir::ArrayAttribute>().size(); i++) {
    paddings.push_back(attributes.at("paddings").dyn_cast<pir::ArrayAttribute>().at(i).dyn_cast<pir::Int32Attribute>().data());
  }

  IR_ENFORCE(
      attributes.find("dilations") != attributes.end(),
          "'dilations' Attribute is expected for UnfoldOp. ");
  std::vector<int> dilations;
  for (size_t i = 0; i < attributes.at("dilations").dyn_cast<pir::ArrayAttribute>().size(); i++) {
    dilations.push_back(attributes.at("dilations").dyn_cast<pir::ArrayAttribute>().at(i).dyn_cast<pir::Int32Attribute>().data());
  }


  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  std::vector<pir::Attribute> vec_kernel_sizes;
  for (size_t i = 0; i < static_cast<size_t>(kernel_sizes.size()); i++) {
      pir::Attribute attr_kernel_sizes = pir::Int32Attribute::get(pir::IrContext::Instance(), kernel_sizes[i]);

    vec_kernel_sizes.push_back(attr_kernel_sizes);
  }
  pir::Attribute attr_kernel_sizes = pir::ArrayAttribute::get(pir::IrContext::Instance(), vec_kernel_sizes);
  argument.AddAttribute("kernel_sizes", attr_kernel_sizes);
  std::vector<pir::Attribute> vec_strides;
  for (size_t i = 0; i < static_cast<size_t>(strides.size()); i++) {
      pir::Attribute attr_strides = pir::Int32Attribute::get(pir::IrContext::Instance(), strides[i]);

    vec_strides.push_back(attr_strides);
  }
  pir::Attribute attr_strides = pir::ArrayAttribute::get(pir::IrContext::Instance(), vec_strides);
  argument.AddAttribute("strides", attr_strides);
  std::vector<pir::Attribute> vec_paddings;
  for (size_t i = 0; i < static_cast<size_t>(paddings.size()); i++) {
      pir::Attribute attr_paddings = pir::Int32Attribute::get(pir::IrContext::Instance(), paddings[i]);

    vec_paddings.push_back(attr_paddings);
  }
  pir::Attribute attr_paddings = pir::ArrayAttribute::get(pir::IrContext::Instance(), vec_paddings);
  argument.AddAttribute("paddings", attr_paddings);
  std::vector<pir::Attribute> vec_dilations;
  for (size_t i = 0; i < static_cast<size_t>(dilations.size()); i++) {
      pir::Attribute attr_dilations = pir::Int32Attribute::get(pir::IrContext::Instance(), dilations[i]);

    vec_dilations.push_back(attr_dilations);
  }
  pir::Attribute attr_dilations = pir::ArrayAttribute::get(pir::IrContext::Instance(), vec_dilations);
  argument.AddAttribute("dilations", attr_dilations);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);
  paddle::dialect::IrTensor dense_out;
  paddle::dialect::IrMetaTensor meta_out(&dense_out);

  phi::UnfoldInferMeta(meta_x, kernel_sizes, strides, paddings, dilations, &meta_out);

  std::vector<pir::Type> argument_outputs;
  pir::Type out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_out.dtype()), dense_out.dims(), dense_out.layout(), dense_out.lod(), dense_out.offset());
  argument_outputs.push_back(out_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void UnfoldOp::VerifySig() {
  VLOG(4) << "Start Verifying inputs, outputs and attributes for: UnfoldOp.";
  VLOG(4) << "Verifying inputs:";
  {
  auto input_size = num_operands();
  IR_ENFORCE(input_size == 1u,
                    "The size %d of inputs must be equal to 1.", input_size);
  IR_ENFORCE((*this)->operand_source(0).type().isa<paddle::dialect::DenseTensorType>(),
                  "Type validation failed for the 0th input, got %s.", (*this)->operand_source(0).type());
  }
  VLOG(4) << "Verifying attributes:";
  {
  auto& attributes = this->attributes();
  IR_ENFORCE(attributes.count("kernel_sizes")>0,
                 "kernel_sizes does not exist.");
  IR_ENFORCE(attributes.at("kernel_sizes").isa<pir::ArrayAttribute>(),
                 "Type of attribute: kernel_sizes is not pir::ArrayAttribute.");
  for (size_t i = 0; i < attributes.at("kernel_sizes").dyn_cast<pir::ArrayAttribute>().size(); i++) {
    IR_ENFORCE(attributes.at("kernel_sizes").dyn_cast<pir::ArrayAttribute>().at(i).isa<pir::Int32Attribute>(),
                   "Type of attribute: kernel_sizes is not right.");
  }
  IR_ENFORCE(attributes.count("strides")>0,
                 "strides does not exist.");
  IR_ENFORCE(attributes.at("strides").isa<pir::ArrayAttribute>(),
                 "Type of attribute: strides is not pir::ArrayAttribute.");
  for (size_t i = 0; i < attributes.at("strides").dyn_cast<pir::ArrayAttribute>().size(); i++) {
    IR_ENFORCE(attributes.at("strides").dyn_cast<pir::ArrayAttribute>().at(i).isa<pir::Int32Attribute>(),
                   "Type of attribute: strides is not right.");
  }
  IR_ENFORCE(attributes.count("paddings")>0,
                 "paddings does not exist.");
  IR_ENFORCE(attributes.at("paddings").isa<pir::ArrayAttribute>(),
                 "Type of attribute: paddings is not pir::ArrayAttribute.");
  for (size_t i = 0; i < attributes.at("paddings").dyn_cast<pir::ArrayAttribute>().size(); i++) {
    IR_ENFORCE(attributes.at("paddings").dyn_cast<pir::ArrayAttribute>().at(i).isa<pir::Int32Attribute>(),
                   "Type of attribute: paddings is not right.");
  }
  IR_ENFORCE(attributes.count("dilations")>0,
                 "dilations does not exist.");
  IR_ENFORCE(attributes.at("dilations").isa<pir::ArrayAttribute>(),
                 "Type of attribute: dilations is not pir::ArrayAttribute.");
  for (size_t i = 0; i < attributes.at("dilations").dyn_cast<pir::ArrayAttribute>().size(); i++) {
    IR_ENFORCE(attributes.at("dilations").dyn_cast<pir::ArrayAttribute>().at(i).isa<pir::Int32Attribute>(),
                   "Type of attribute: dilations is not right.");
  }
  }
  VLOG(4) << "Verifying outputs:";
  {
  auto output_size = num_results();
  IR_ENFORCE(output_size == 1u,
                    "The size %d of outputs must be equal to 1.", output_size);
  IR_ENFORCE((*this)->result(0).type().isa<paddle::dialect::DenseTensorType>(),
                 "Type validation failed for the 0th output.");
  }
  VLOG(4) << "End Verifying for: UnfoldOp.";
}

void UnfoldOp::InferMeta( phi::InferMetaContext *infer_meta ) {
  auto fn = PD_INFER_META(phi::UnfoldInferMeta);
  fn(infer_meta);
}

phi::DataType UnfoldOp::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: UnfoldOp";
  


  return expected_kernel_dtype;
}

const char *UniformInplaceOp::attributes_name[6] = { "min", "max", "seed", "diag_num", "diag_step", "diag_val" };

OpInfoTuple UniformInplaceOp::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("x", "paddle::dialect::DenseTensorType", false, false, false, true) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = { paddle::dialect::OpAttributeInfo("min", "pir::FloatAttribute", ""), paddle::dialect::OpAttributeInfo("max", "pir::FloatAttribute", ""), paddle::dialect::OpAttributeInfo("seed", "pir::Int32Attribute", ""), paddle::dialect::OpAttributeInfo("diag_num", "pir::Int32Attribute", ""), paddle::dialect::OpAttributeInfo("diag_step", "pir::Int32Attribute", ""), paddle::dialect::OpAttributeInfo("diag_val", "pir::FloatAttribute", "") };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("out", "paddle::dialect::DenseTensorType", false, false) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("UniformRandomInplaceInferMeta", {"x", "min", "max", "seed", "diag_num", "diag_step", "diag_val"}, "uniform_inplace", {"x", "min", "max", "seed", "diag_num", "diag_step", "diag_val"}, {"x"}, {}, {}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "uniform_inplace");
}

void UniformInplaceOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, float min, float max, int seed, int diag_num, int diag_step, float diag_val) {
  VLOG(4) << "Start build UniformInplaceOp";



  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_min = pir::FloatAttribute::get(pir::IrContext::Instance(), min);
  argument.AddAttribute("min", attr_min);
  pir::Attribute attr_max = pir::FloatAttribute::get(pir::IrContext::Instance(), max);
  argument.AddAttribute("max", attr_max);
  pir::Attribute attr_seed = pir::Int32Attribute::get(pir::IrContext::Instance(), seed);
  argument.AddAttribute("seed", attr_seed);
  pir::Attribute attr_diag_num = pir::Int32Attribute::get(pir::IrContext::Instance(), diag_num);
  argument.AddAttribute("diag_num", attr_diag_num);
  pir::Attribute attr_diag_step = pir::Int32Attribute::get(pir::IrContext::Instance(), diag_step);
  argument.AddAttribute("diag_step", attr_diag_step);
  pir::Attribute attr_diag_val = pir::FloatAttribute::get(pir::IrContext::Instance(), diag_val);
  argument.AddAttribute("diag_val", attr_diag_val);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);
  paddle::dialect::IrTensor dense_out;
  paddle::dialect::IrMetaTensor meta_out(&dense_out);

  phi::UniformRandomInplaceInferMeta(meta_x, min, max, seed, diag_num, diag_step, diag_val, &meta_out);

  std::vector<pir::Type> argument_outputs;
  pir::Type out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_out.dtype()), dense_out.dims(), dense_out.layout(), dense_out.lod(), dense_out.offset());
  argument_outputs.push_back(out_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void UniformInplaceOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, pir::AttributeMap attributes) {
  VLOG(4) << "Start build UniformInplaceOp";


  IR_ENFORCE(
      attributes.find("min") != attributes.end(),
          "'min' Attribute is expected for UniformInplaceOp. ");
  float min = attributes.at("min").dyn_cast<pir::FloatAttribute>().data();

  IR_ENFORCE(
      attributes.find("max") != attributes.end(),
          "'max' Attribute is expected for UniformInplaceOp. ");
  float max = attributes.at("max").dyn_cast<pir::FloatAttribute>().data();

  IR_ENFORCE(
      attributes.find("seed") != attributes.end(),
          "'seed' Attribute is expected for UniformInplaceOp. ");
  int seed = attributes.at("seed").dyn_cast<pir::Int32Attribute>().data();

  IR_ENFORCE(
      attributes.find("diag_num") != attributes.end(),
          "'diag_num' Attribute is expected for UniformInplaceOp. ");
  int diag_num = attributes.at("diag_num").dyn_cast<pir::Int32Attribute>().data();

  IR_ENFORCE(
      attributes.find("diag_step") != attributes.end(),
          "'diag_step' Attribute is expected for UniformInplaceOp. ");
  int diag_step = attributes.at("diag_step").dyn_cast<pir::Int32Attribute>().data();

  IR_ENFORCE(
      attributes.find("diag_val") != attributes.end(),
          "'diag_val' Attribute is expected for UniformInplaceOp. ");
  float diag_val = attributes.at("diag_val").dyn_cast<pir::FloatAttribute>().data();


  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_min = pir::FloatAttribute::get(pir::IrContext::Instance(), min);
  argument.AddAttribute("min", attr_min);
  pir::Attribute attr_max = pir::FloatAttribute::get(pir::IrContext::Instance(), max);
  argument.AddAttribute("max", attr_max);
  pir::Attribute attr_seed = pir::Int32Attribute::get(pir::IrContext::Instance(), seed);
  argument.AddAttribute("seed", attr_seed);
  pir::Attribute attr_diag_num = pir::Int32Attribute::get(pir::IrContext::Instance(), diag_num);
  argument.AddAttribute("diag_num", attr_diag_num);
  pir::Attribute attr_diag_step = pir::Int32Attribute::get(pir::IrContext::Instance(), diag_step);
  argument.AddAttribute("diag_step", attr_diag_step);
  pir::Attribute attr_diag_val = pir::FloatAttribute::get(pir::IrContext::Instance(), diag_val);
  argument.AddAttribute("diag_val", attr_diag_val);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);
  paddle::dialect::IrTensor dense_out;
  paddle::dialect::IrMetaTensor meta_out(&dense_out);

  phi::UniformRandomInplaceInferMeta(meta_x, min, max, seed, diag_num, diag_step, diag_val, &meta_out);

  std::vector<pir::Type> argument_outputs;
  pir::Type out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_out.dtype()), dense_out.dims(), dense_out.layout(), dense_out.lod(), dense_out.offset());
  argument_outputs.push_back(out_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void UniformInplaceOp::VerifySig() {
  VLOG(4) << "Start Verifying inputs, outputs and attributes for: UniformInplaceOp.";
  VLOG(4) << "Verifying inputs:";
  {
  auto input_size = num_operands();
  IR_ENFORCE(input_size == 1u,
                    "The size %d of inputs must be equal to 1.", input_size);
  IR_ENFORCE((*this)->operand_source(0).type().isa<paddle::dialect::DenseTensorType>(),
                  "Type validation failed for the 0th input, got %s.", (*this)->operand_source(0).type());
  }
  VLOG(4) << "Verifying attributes:";
  {
  auto& attributes = this->attributes();
  IR_ENFORCE(attributes.count("min")>0,
                 "min does not exist.");
  IR_ENFORCE(attributes.at("min").isa<pir::FloatAttribute>(),
                 "Type of attribute: min is not pir::FloatAttribute.");

  IR_ENFORCE(attributes.count("max")>0,
                 "max does not exist.");
  IR_ENFORCE(attributes.at("max").isa<pir::FloatAttribute>(),
                 "Type of attribute: max is not pir::FloatAttribute.");

  IR_ENFORCE(attributes.count("seed")>0,
                 "seed does not exist.");
  IR_ENFORCE(attributes.at("seed").isa<pir::Int32Attribute>(),
                 "Type of attribute: seed is not pir::Int32Attribute.");

  IR_ENFORCE(attributes.count("diag_num")>0,
                 "diag_num does not exist.");
  IR_ENFORCE(attributes.at("diag_num").isa<pir::Int32Attribute>(),
                 "Type of attribute: diag_num is not pir::Int32Attribute.");

  IR_ENFORCE(attributes.count("diag_step")>0,
                 "diag_step does not exist.");
  IR_ENFORCE(attributes.at("diag_step").isa<pir::Int32Attribute>(),
                 "Type of attribute: diag_step is not pir::Int32Attribute.");

  IR_ENFORCE(attributes.count("diag_val")>0,
                 "diag_val does not exist.");
  IR_ENFORCE(attributes.at("diag_val").isa<pir::FloatAttribute>(),
                 "Type of attribute: diag_val is not pir::FloatAttribute.");

  }
  VLOG(4) << "Verifying outputs:";
  {
  auto output_size = num_results();
  IR_ENFORCE(output_size == 1u,
                    "The size %d of outputs must be equal to 1.", output_size);
  IR_ENFORCE((*this)->result(0).type().isa<paddle::dialect::DenseTensorType>(),
                 "Type validation failed for the 0th output.");
  }
  VLOG(4) << "End Verifying for: UniformInplaceOp.";
}

void UniformInplaceOp::InferMeta( phi::InferMetaContext *infer_meta ) {
  auto fn = PD_INFER_META(phi::UniformRandomInplaceInferMeta);
  fn(infer_meta);
}

phi::DataType UniformInplaceOp::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: UniformInplaceOp";
  


  return expected_kernel_dtype;
}

const char *UniformInplace_Op::attributes_name[6] = { "min", "max", "seed", "diag_num", "diag_step", "diag_val" };

OpInfoTuple UniformInplace_Op::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("x", "paddle::dialect::DenseTensorType", false, false, false, true) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = { paddle::dialect::OpAttributeInfo("min", "pir::FloatAttribute", ""), paddle::dialect::OpAttributeInfo("max", "pir::FloatAttribute", ""), paddle::dialect::OpAttributeInfo("seed", "pir::Int32Attribute", ""), paddle::dialect::OpAttributeInfo("diag_num", "pir::Int32Attribute", ""), paddle::dialect::OpAttributeInfo("diag_step", "pir::Int32Attribute", ""), paddle::dialect::OpAttributeInfo("diag_val", "pir::FloatAttribute", "") };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("out", "paddle::dialect::DenseTensorType", false, false) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("UniformRandomInplaceInferMeta", {"x", "min", "max", "seed", "diag_num", "diag_step", "diag_val"}, "uniform_inplace", {"x", "min", "max", "seed", "diag_num", "diag_step", "diag_val"}, {"x"}, {}, {{"out", "x"}}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "uniform_inplace");
}

void UniformInplace_Op::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, float min, float max, int seed, int diag_num, int diag_step, float diag_val) {
  VLOG(4) << "Start build UniformInplace_Op";



  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_min = pir::FloatAttribute::get(pir::IrContext::Instance(), min);
  argument.AddAttribute("min", attr_min);
  pir::Attribute attr_max = pir::FloatAttribute::get(pir::IrContext::Instance(), max);
  argument.AddAttribute("max", attr_max);
  pir::Attribute attr_seed = pir::Int32Attribute::get(pir::IrContext::Instance(), seed);
  argument.AddAttribute("seed", attr_seed);
  pir::Attribute attr_diag_num = pir::Int32Attribute::get(pir::IrContext::Instance(), diag_num);
  argument.AddAttribute("diag_num", attr_diag_num);
  pir::Attribute attr_diag_step = pir::Int32Attribute::get(pir::IrContext::Instance(), diag_step);
  argument.AddAttribute("diag_step", attr_diag_step);
  pir::Attribute attr_diag_val = pir::FloatAttribute::get(pir::IrContext::Instance(), diag_val);
  argument.AddAttribute("diag_val", attr_diag_val);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);
  paddle::dialect::IrTensor dense_out;
  paddle::dialect::IrMetaTensor meta_out(&dense_out);

  phi::UniformRandomInplaceInferMeta(meta_x, min, max, seed, diag_num, diag_step, diag_val, &meta_out);

  std::vector<pir::Type> argument_outputs;
  pir::Type out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_out.dtype()), dense_out.dims(), dense_out.layout(), dense_out.lod(), dense_out.offset());
  argument_outputs.push_back(out_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void UniformInplace_Op::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, pir::AttributeMap attributes) {
  VLOG(4) << "Start build UniformInplace_Op";


  IR_ENFORCE(
      attributes.find("min") != attributes.end(),
          "'min' Attribute is expected for UniformInplace_Op. ");
  float min = attributes.at("min").dyn_cast<pir::FloatAttribute>().data();

  IR_ENFORCE(
      attributes.find("max") != attributes.end(),
          "'max' Attribute is expected for UniformInplace_Op. ");
  float max = attributes.at("max").dyn_cast<pir::FloatAttribute>().data();

  IR_ENFORCE(
      attributes.find("seed") != attributes.end(),
          "'seed' Attribute is expected for UniformInplace_Op. ");
  int seed = attributes.at("seed").dyn_cast<pir::Int32Attribute>().data();

  IR_ENFORCE(
      attributes.find("diag_num") != attributes.end(),
          "'diag_num' Attribute is expected for UniformInplace_Op. ");
  int diag_num = attributes.at("diag_num").dyn_cast<pir::Int32Attribute>().data();

  IR_ENFORCE(
      attributes.find("diag_step") != attributes.end(),
          "'diag_step' Attribute is expected for UniformInplace_Op. ");
  int diag_step = attributes.at("diag_step").dyn_cast<pir::Int32Attribute>().data();

  IR_ENFORCE(
      attributes.find("diag_val") != attributes.end(),
          "'diag_val' Attribute is expected for UniformInplace_Op. ");
  float diag_val = attributes.at("diag_val").dyn_cast<pir::FloatAttribute>().data();


  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_min = pir::FloatAttribute::get(pir::IrContext::Instance(), min);
  argument.AddAttribute("min", attr_min);
  pir::Attribute attr_max = pir::FloatAttribute::get(pir::IrContext::Instance(), max);
  argument.AddAttribute("max", attr_max);
  pir::Attribute attr_seed = pir::Int32Attribute::get(pir::IrContext::Instance(), seed);
  argument.AddAttribute("seed", attr_seed);
  pir::Attribute attr_diag_num = pir::Int32Attribute::get(pir::IrContext::Instance(), diag_num);
  argument.AddAttribute("diag_num", attr_diag_num);
  pir::Attribute attr_diag_step = pir::Int32Attribute::get(pir::IrContext::Instance(), diag_step);
  argument.AddAttribute("diag_step", attr_diag_step);
  pir::Attribute attr_diag_val = pir::FloatAttribute::get(pir::IrContext::Instance(), diag_val);
  argument.AddAttribute("diag_val", attr_diag_val);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);
  paddle::dialect::IrTensor dense_out;
  paddle::dialect::IrMetaTensor meta_out(&dense_out);

  phi::UniformRandomInplaceInferMeta(meta_x, min, max, seed, diag_num, diag_step, diag_val, &meta_out);

  std::vector<pir::Type> argument_outputs;
  pir::Type out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_out.dtype()), dense_out.dims(), dense_out.layout(), dense_out.lod(), dense_out.offset());
  argument_outputs.push_back(out_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void UniformInplace_Op::VerifySig() {
  VLOG(4) << "Start Verifying inputs, outputs and attributes for: UniformInplace_Op.";
  VLOG(4) << "Verifying inputs:";
  {
  auto input_size = num_operands();
  IR_ENFORCE(input_size == 1u,
                    "The size %d of inputs must be equal to 1.", input_size);
  IR_ENFORCE((*this)->operand_source(0).type().isa<paddle::dialect::DenseTensorType>(),
                  "Type validation failed for the 0th input, got %s.", (*this)->operand_source(0).type());
  }
  VLOG(4) << "Verifying attributes:";
  {
  auto& attributes = this->attributes();
  IR_ENFORCE(attributes.count("min")>0,
                 "min does not exist.");
  IR_ENFORCE(attributes.at("min").isa<pir::FloatAttribute>(),
                 "Type of attribute: min is not pir::FloatAttribute.");

  IR_ENFORCE(attributes.count("max")>0,
                 "max does not exist.");
  IR_ENFORCE(attributes.at("max").isa<pir::FloatAttribute>(),
                 "Type of attribute: max is not pir::FloatAttribute.");

  IR_ENFORCE(attributes.count("seed")>0,
                 "seed does not exist.");
  IR_ENFORCE(attributes.at("seed").isa<pir::Int32Attribute>(),
                 "Type of attribute: seed is not pir::Int32Attribute.");

  IR_ENFORCE(attributes.count("diag_num")>0,
                 "diag_num does not exist.");
  IR_ENFORCE(attributes.at("diag_num").isa<pir::Int32Attribute>(),
                 "Type of attribute: diag_num is not pir::Int32Attribute.");

  IR_ENFORCE(attributes.count("diag_step")>0,
                 "diag_step does not exist.");
  IR_ENFORCE(attributes.at("diag_step").isa<pir::Int32Attribute>(),
                 "Type of attribute: diag_step is not pir::Int32Attribute.");

  IR_ENFORCE(attributes.count("diag_val")>0,
                 "diag_val does not exist.");
  IR_ENFORCE(attributes.at("diag_val").isa<pir::FloatAttribute>(),
                 "Type of attribute: diag_val is not pir::FloatAttribute.");

  }
  VLOG(4) << "Verifying outputs:";
  {
  auto output_size = num_results();
  IR_ENFORCE(output_size == 1u,
                    "The size %d of outputs must be equal to 1.", output_size);
  IR_ENFORCE((*this)->result(0).type().isa<paddle::dialect::DenseTensorType>(),
                 "Type validation failed for the 0th output.");
  }
  VLOG(4) << "End Verifying for: UniformInplace_Op.";
}

void UniformInplace_Op::InferMeta( phi::InferMetaContext *infer_meta ) {
  auto fn = PD_INFER_META(phi::UniformRandomInplaceInferMeta);
  fn(infer_meta);
}

phi::DataType UniformInplace_Op::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: UniformInplace_Op";
  


  return expected_kernel_dtype;
}

const char *UniqueConsecutiveOp::attributes_name[4] = { "return_inverse", "return_counts", "axis", "dtype" };

OpInfoTuple UniqueConsecutiveOp::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("x", "paddle::dialect::DenseTensorType", false, false, false, false) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = { paddle::dialect::OpAttributeInfo("return_inverse", "pir::BoolAttribute", ""), paddle::dialect::OpAttributeInfo("return_counts", "pir::BoolAttribute", ""), paddle::dialect::OpAttributeInfo("axis", "pir::ArrayAttribute<pir::Int32Attribute>", ""), paddle::dialect::OpAttributeInfo("dtype", "paddle::dialect::DataTypeAttribute", "") };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("out", "paddle::dialect::DenseTensorType", false, false), paddle::dialect::OpOutputInfo("index", "paddle::dialect::DenseTensorType", true, false), paddle::dialect::OpOutputInfo("counts", "paddle::dialect::DenseTensorType", true, false) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("UniqueConsecutiveInferMeta", {"x", "return_inverse", "return_counts", "axis", "dtype"}, "unique_consecutive", {"x", "return_inverse", "return_counts", "axis", "dtype"}, {"x"}, {}, {}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "unique_consecutive");
}

void UniqueConsecutiveOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, bool return_inverse, bool return_counts, const std::vector<int>& axis, phi::DataType dtype) {
  VLOG(4) << "Start build UniqueConsecutiveOp";



  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_return_inverse = pir::BoolAttribute::get(pir::IrContext::Instance(), return_inverse);
  argument.AddAttribute("return_inverse", attr_return_inverse);
  pir::Attribute attr_return_counts = pir::BoolAttribute::get(pir::IrContext::Instance(), return_counts);
  argument.AddAttribute("return_counts", attr_return_counts);
  std::vector<pir::Attribute> vec_axis;
  for (size_t i = 0; i < static_cast<size_t>(axis.size()); i++) {
      pir::Attribute attr_axis = pir::Int32Attribute::get(pir::IrContext::Instance(), axis[i]);

    vec_axis.push_back(attr_axis);
  }
  pir::Attribute attr_axis = pir::ArrayAttribute::get(pir::IrContext::Instance(), vec_axis);
  argument.AddAttribute("axis", attr_axis);
  pir::Attribute attr_dtype = paddle::dialect::DataTypeAttribute::get(pir::IrContext::Instance(), dtype);
  argument.AddAttribute("dtype", attr_dtype);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);
  paddle::dialect::IrTensor dense_out;
  paddle::dialect::IrMetaTensor meta_out(&dense_out);
  paddle::dialect::IrTensor dense_index;
  paddle::dialect::IrMetaTensor meta_index(&dense_index);
  paddle::dialect::IrTensor dense_counts;
  paddle::dialect::IrMetaTensor meta_counts(&dense_counts);

  phi::UniqueConsecutiveInferMeta(meta_x, return_inverse, return_counts, axis, dtype, &meta_out, &meta_index, &meta_counts);

  std::vector<pir::Type> argument_outputs;
  pir::Type out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_out.dtype()), dense_out.dims(), dense_out.layout(), dense_out.lod(), dense_out.offset());
  argument_outputs.push_back(out_dense_tensor_type);

  pir::Type index_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_index.dtype()), dense_index.dims(), dense_index.layout(), dense_index.lod(), dense_index.offset());
  argument_outputs.push_back(index_dense_tensor_type);

  pir::Type counts_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_counts.dtype()), dense_counts.dims(), dense_counts.layout(), dense_counts.lod(), dense_counts.offset());
  argument_outputs.push_back(counts_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void UniqueConsecutiveOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, pir::AttributeMap attributes) {
  VLOG(4) << "Start build UniqueConsecutiveOp";


  IR_ENFORCE(
      attributes.find("return_inverse") != attributes.end(),
          "'return_inverse' Attribute is expected for UniqueConsecutiveOp. ");
  bool return_inverse = attributes.at("return_inverse").dyn_cast<pir::BoolAttribute>().data();

  IR_ENFORCE(
      attributes.find("return_counts") != attributes.end(),
          "'return_counts' Attribute is expected for UniqueConsecutiveOp. ");
  bool return_counts = attributes.at("return_counts").dyn_cast<pir::BoolAttribute>().data();

  IR_ENFORCE(
      attributes.find("axis") != attributes.end(),
          "'axis' Attribute is expected for UniqueConsecutiveOp. ");
  std::vector<int> axis;
  for (size_t i = 0; i < attributes.at("axis").dyn_cast<pir::ArrayAttribute>().size(); i++) {
    axis.push_back(attributes.at("axis").dyn_cast<pir::ArrayAttribute>().at(i).dyn_cast<pir::Int32Attribute>().data());
  }

  IR_ENFORCE(
      attributes.find("dtype") != attributes.end(),
          "'dtype' Attribute is expected for UniqueConsecutiveOp. ");
  phi::DataType dtype = attributes.at("dtype").dyn_cast<paddle::dialect::DataTypeAttribute>().data();


  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_return_inverse = pir::BoolAttribute::get(pir::IrContext::Instance(), return_inverse);
  argument.AddAttribute("return_inverse", attr_return_inverse);
  pir::Attribute attr_return_counts = pir::BoolAttribute::get(pir::IrContext::Instance(), return_counts);
  argument.AddAttribute("return_counts", attr_return_counts);
  std::vector<pir::Attribute> vec_axis;
  for (size_t i = 0; i < static_cast<size_t>(axis.size()); i++) {
      pir::Attribute attr_axis = pir::Int32Attribute::get(pir::IrContext::Instance(), axis[i]);

    vec_axis.push_back(attr_axis);
  }
  pir::Attribute attr_axis = pir::ArrayAttribute::get(pir::IrContext::Instance(), vec_axis);
  argument.AddAttribute("axis", attr_axis);
  pir::Attribute attr_dtype = paddle::dialect::DataTypeAttribute::get(pir::IrContext::Instance(), dtype);
  argument.AddAttribute("dtype", attr_dtype);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);
  paddle::dialect::IrTensor dense_out;
  paddle::dialect::IrMetaTensor meta_out(&dense_out);
  paddle::dialect::IrTensor dense_index;
  paddle::dialect::IrMetaTensor meta_index(&dense_index);
  paddle::dialect::IrTensor dense_counts;
  paddle::dialect::IrMetaTensor meta_counts(&dense_counts);

  phi::UniqueConsecutiveInferMeta(meta_x, return_inverse, return_counts, axis, dtype, &meta_out, &meta_index, &meta_counts);

  std::vector<pir::Type> argument_outputs;
  pir::Type out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_out.dtype()), dense_out.dims(), dense_out.layout(), dense_out.lod(), dense_out.offset());
  argument_outputs.push_back(out_dense_tensor_type);

  pir::Type index_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_index.dtype()), dense_index.dims(), dense_index.layout(), dense_index.lod(), dense_index.offset());
  argument_outputs.push_back(index_dense_tensor_type);

  pir::Type counts_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_counts.dtype()), dense_counts.dims(), dense_counts.layout(), dense_counts.lod(), dense_counts.offset());
  argument_outputs.push_back(counts_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void UniqueConsecutiveOp::VerifySig() {
  VLOG(4) << "Start Verifying inputs, outputs and attributes for: UniqueConsecutiveOp.";
  VLOG(4) << "Verifying inputs:";
  {
  auto input_size = num_operands();
  IR_ENFORCE(input_size == 1u,
                    "The size %d of inputs must be equal to 1.", input_size);
  IR_ENFORCE((*this)->operand_source(0).type().isa<paddle::dialect::DenseTensorType>(),
                  "Type validation failed for the 0th input, got %s.", (*this)->operand_source(0).type());
  }
  VLOG(4) << "Verifying attributes:";
  {
  auto& attributes = this->attributes();
  IR_ENFORCE(attributes.count("return_inverse")>0,
                 "return_inverse does not exist.");
  IR_ENFORCE(attributes.at("return_inverse").isa<pir::BoolAttribute>(),
                 "Type of attribute: return_inverse is not pir::BoolAttribute.");

  IR_ENFORCE(attributes.count("return_counts")>0,
                 "return_counts does not exist.");
  IR_ENFORCE(attributes.at("return_counts").isa<pir::BoolAttribute>(),
                 "Type of attribute: return_counts is not pir::BoolAttribute.");

  IR_ENFORCE(attributes.count("axis")>0,
                 "axis does not exist.");
  IR_ENFORCE(attributes.at("axis").isa<pir::ArrayAttribute>(),
                 "Type of attribute: axis is not pir::ArrayAttribute.");
  for (size_t i = 0; i < attributes.at("axis").dyn_cast<pir::ArrayAttribute>().size(); i++) {
    IR_ENFORCE(attributes.at("axis").dyn_cast<pir::ArrayAttribute>().at(i).isa<pir::Int32Attribute>(),
                   "Type of attribute: axis is not right.");
  }
  IR_ENFORCE(attributes.count("dtype")>0,
                 "dtype does not exist.");
  IR_ENFORCE(attributes.at("dtype").isa<paddle::dialect::DataTypeAttribute>(),
                 "Type of attribute: dtype is not paddle::dialect::DataTypeAttribute.");

  }
  VLOG(4) << "Verifying outputs:";
  {
  auto output_size = num_results();
  IR_ENFORCE(output_size == 3u,
                    "The size %d of outputs must be equal to 3.", output_size);
  IR_ENFORCE((*this)->result(0).type().isa<paddle::dialect::DenseTensorType>(),
                 "Type validation failed for the 0th output.");
  if (auto output_1_type = (*this)->result(1).type()) {
    IR_ENFORCE(output_1_type.isa<paddle::dialect::DenseTensorType>(),
                   "Type validation failed for the 1th output.");
  }
  if (auto output_2_type = (*this)->result(2).type()) {
    IR_ENFORCE(output_2_type.isa<paddle::dialect::DenseTensorType>(),
                   "Type validation failed for the 2th output.");
  }
  }
  VLOG(4) << "End Verifying for: UniqueConsecutiveOp.";
}

void UniqueConsecutiveOp::InferMeta( phi::InferMetaContext *infer_meta ) {
  auto fn = PD_INFER_META(phi::UniqueConsecutiveInferMeta);
  fn(infer_meta);
}

phi::DataType UniqueConsecutiveOp::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: UniqueConsecutiveOp";
  


  return expected_kernel_dtype;
}

const char *Unpool3dOp::attributes_name[5] = { "ksize", "strides", "paddings", "output_size", "data_format" };

OpInfoTuple Unpool3dOp::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("x", "paddle::dialect::DenseTensorType", false, false, false, true), paddle::dialect::OpInputInfo("indices", "paddle::dialect::DenseTensorType", false, false, false, false) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = { paddle::dialect::OpAttributeInfo("ksize", "pir::ArrayAttribute<pir::Int32Attribute>", ""), paddle::dialect::OpAttributeInfo("strides", "pir::ArrayAttribute<pir::Int32Attribute>", ""), paddle::dialect::OpAttributeInfo("paddings", "pir::ArrayAttribute<pir::Int32Attribute>", ""), paddle::dialect::OpAttributeInfo("output_size", "pir::ArrayAttribute<pir::Int32Attribute>", ""), paddle::dialect::OpAttributeInfo("data_format", "pir::StrAttribute", "") };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("out", "paddle::dialect::DenseTensorType", false, false) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("Unpool3dInferMeta", {"x", "indices", "ksize", "strides", "paddings", "output_size", "data_format"}, "unpool3d", {"x", "indices", "ksize", "strides", "paddings", "output_size", "data_format"}, {"x"}, {}, {}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "unpool3d");
}

void Unpool3dOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, pir::Value indices_, const std::vector<int>& ksize, const std::vector<int>& strides, const std::vector<int>& paddings, const std::vector<int>& output_size, const std::string& data_format) {
  VLOG(4) << "Start build Unpool3dOp";



  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_, indices_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  std::vector<pir::Attribute> vec_ksize;
  for (size_t i = 0; i < static_cast<size_t>(ksize.size()); i++) {
      pir::Attribute attr_ksize = pir::Int32Attribute::get(pir::IrContext::Instance(), ksize[i]);

    vec_ksize.push_back(attr_ksize);
  }
  pir::Attribute attr_ksize = pir::ArrayAttribute::get(pir::IrContext::Instance(), vec_ksize);
  argument.AddAttribute("ksize", attr_ksize);
  std::vector<pir::Attribute> vec_strides;
  for (size_t i = 0; i < static_cast<size_t>(strides.size()); i++) {
      pir::Attribute attr_strides = pir::Int32Attribute::get(pir::IrContext::Instance(), strides[i]);

    vec_strides.push_back(attr_strides);
  }
  pir::Attribute attr_strides = pir::ArrayAttribute::get(pir::IrContext::Instance(), vec_strides);
  argument.AddAttribute("strides", attr_strides);
  std::vector<pir::Attribute> vec_paddings;
  for (size_t i = 0; i < static_cast<size_t>(paddings.size()); i++) {
      pir::Attribute attr_paddings = pir::Int32Attribute::get(pir::IrContext::Instance(), paddings[i]);

    vec_paddings.push_back(attr_paddings);
  }
  pir::Attribute attr_paddings = pir::ArrayAttribute::get(pir::IrContext::Instance(), vec_paddings);
  argument.AddAttribute("paddings", attr_paddings);
  std::vector<pir::Attribute> vec_output_size;
  for (size_t i = 0; i < static_cast<size_t>(output_size.size()); i++) {
      pir::Attribute attr_output_size = pir::Int32Attribute::get(pir::IrContext::Instance(), output_size[i]);

    vec_output_size.push_back(attr_output_size);
  }
  pir::Attribute attr_output_size = pir::ArrayAttribute::get(pir::IrContext::Instance(), vec_output_size);
  argument.AddAttribute("output_size", attr_output_size);
  pir::Attribute attr_data_format = pir::StrAttribute::get(pir::IrContext::Instance(), data_format);
  argument.AddAttribute("data_format", attr_data_format);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;
  paddle::dialect::DenseTensorType indices = indices_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)indices;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);

  VLOG(4) << "Builder construction  dense_indices";
  paddle::dialect::IrTensor ir_tensor_indices(paddle::dialect::TransToPhiDataType(indices.dtype()),
                                                      indices.dims(),
                                                      indices.data_layout(),
                                                      indices.lod(),
                                                      indices.offset());
  VLOG(4) << "Builder construction  meta_indices";
  paddle::dialect::IrMetaTensor meta_indices(&ir_tensor_indices);
  paddle::dialect::IrTensor dense_out;
  paddle::dialect::IrMetaTensor meta_out(&dense_out);

  phi::Unpool3dInferMeta(meta_x, meta_indices, ksize, strides, paddings, output_size, data_format, &meta_out);

  std::vector<pir::Type> argument_outputs;
  pir::Type out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_out.dtype()), dense_out.dims(), dense_out.layout(), dense_out.lod(), dense_out.offset());
  argument_outputs.push_back(out_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void Unpool3dOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, pir::Value indices_, pir::AttributeMap attributes) {
  VLOG(4) << "Start build Unpool3dOp";


  IR_ENFORCE(
      attributes.find("ksize") != attributes.end(),
          "'ksize' Attribute is expected for Unpool3dOp. ");
  std::vector<int> ksize;
  for (size_t i = 0; i < attributes.at("ksize").dyn_cast<pir::ArrayAttribute>().size(); i++) {
    ksize.push_back(attributes.at("ksize").dyn_cast<pir::ArrayAttribute>().at(i).dyn_cast<pir::Int32Attribute>().data());
  }

  IR_ENFORCE(
      attributes.find("strides") != attributes.end(),
          "'strides' Attribute is expected for Unpool3dOp. ");
  std::vector<int> strides;
  for (size_t i = 0; i < attributes.at("strides").dyn_cast<pir::ArrayAttribute>().size(); i++) {
    strides.push_back(attributes.at("strides").dyn_cast<pir::ArrayAttribute>().at(i).dyn_cast<pir::Int32Attribute>().data());
  }

  IR_ENFORCE(
      attributes.find("paddings") != attributes.end(),
          "'paddings' Attribute is expected for Unpool3dOp. ");
  std::vector<int> paddings;
  for (size_t i = 0; i < attributes.at("paddings").dyn_cast<pir::ArrayAttribute>().size(); i++) {
    paddings.push_back(attributes.at("paddings").dyn_cast<pir::ArrayAttribute>().at(i).dyn_cast<pir::Int32Attribute>().data());
  }

  IR_ENFORCE(
      attributes.find("output_size") != attributes.end(),
          "'output_size' Attribute is expected for Unpool3dOp. ");
  std::vector<int> output_size;
  for (size_t i = 0; i < attributes.at("output_size").dyn_cast<pir::ArrayAttribute>().size(); i++) {
    output_size.push_back(attributes.at("output_size").dyn_cast<pir::ArrayAttribute>().at(i).dyn_cast<pir::Int32Attribute>().data());
  }

  IR_ENFORCE(
      attributes.find("data_format") != attributes.end(),
          "'data_format' Attribute is expected for Unpool3dOp. ");
  std::string data_format = attributes.at("data_format").dyn_cast<pir::StrAttribute>().AsString();


  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_, indices_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  std::vector<pir::Attribute> vec_ksize;
  for (size_t i = 0; i < static_cast<size_t>(ksize.size()); i++) {
      pir::Attribute attr_ksize = pir::Int32Attribute::get(pir::IrContext::Instance(), ksize[i]);

    vec_ksize.push_back(attr_ksize);
  }
  pir::Attribute attr_ksize = pir::ArrayAttribute::get(pir::IrContext::Instance(), vec_ksize);
  argument.AddAttribute("ksize", attr_ksize);
  std::vector<pir::Attribute> vec_strides;
  for (size_t i = 0; i < static_cast<size_t>(strides.size()); i++) {
      pir::Attribute attr_strides = pir::Int32Attribute::get(pir::IrContext::Instance(), strides[i]);

    vec_strides.push_back(attr_strides);
  }
  pir::Attribute attr_strides = pir::ArrayAttribute::get(pir::IrContext::Instance(), vec_strides);
  argument.AddAttribute("strides", attr_strides);
  std::vector<pir::Attribute> vec_paddings;
  for (size_t i = 0; i < static_cast<size_t>(paddings.size()); i++) {
      pir::Attribute attr_paddings = pir::Int32Attribute::get(pir::IrContext::Instance(), paddings[i]);

    vec_paddings.push_back(attr_paddings);
  }
  pir::Attribute attr_paddings = pir::ArrayAttribute::get(pir::IrContext::Instance(), vec_paddings);
  argument.AddAttribute("paddings", attr_paddings);
  std::vector<pir::Attribute> vec_output_size;
  for (size_t i = 0; i < static_cast<size_t>(output_size.size()); i++) {
      pir::Attribute attr_output_size = pir::Int32Attribute::get(pir::IrContext::Instance(), output_size[i]);

    vec_output_size.push_back(attr_output_size);
  }
  pir::Attribute attr_output_size = pir::ArrayAttribute::get(pir::IrContext::Instance(), vec_output_size);
  argument.AddAttribute("output_size", attr_output_size);
  pir::Attribute attr_data_format = pir::StrAttribute::get(pir::IrContext::Instance(), data_format);
  argument.AddAttribute("data_format", attr_data_format);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;
  paddle::dialect::DenseTensorType indices = indices_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)indices;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);

  VLOG(4) << "Builder construction  dense_indices";
  paddle::dialect::IrTensor ir_tensor_indices(paddle::dialect::TransToPhiDataType(indices.dtype()),
                                                      indices.dims(),
                                                      indices.data_layout(),
                                                      indices.lod(),
                                                      indices.offset());
  VLOG(4) << "Builder construction  meta_indices";
  paddle::dialect::IrMetaTensor meta_indices(&ir_tensor_indices);
  paddle::dialect::IrTensor dense_out;
  paddle::dialect::IrMetaTensor meta_out(&dense_out);

  phi::Unpool3dInferMeta(meta_x, meta_indices, ksize, strides, paddings, output_size, data_format, &meta_out);

  std::vector<pir::Type> argument_outputs;
  pir::Type out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_out.dtype()), dense_out.dims(), dense_out.layout(), dense_out.lod(), dense_out.offset());
  argument_outputs.push_back(out_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void Unpool3dOp::VerifySig() {
  VLOG(4) << "Start Verifying inputs, outputs and attributes for: Unpool3dOp.";
  VLOG(4) << "Verifying inputs:";
  {
  auto input_size = num_operands();
  IR_ENFORCE(input_size == 2u,
                    "The size %d of inputs must be equal to 2.", input_size);
  IR_ENFORCE((*this)->operand_source(0).type().isa<paddle::dialect::DenseTensorType>(),
                  "Type validation failed for the 0th input, got %s.", (*this)->operand_source(0).type());
  IR_ENFORCE((*this)->operand_source(1).type().isa<paddle::dialect::DenseTensorType>(),
                  "Type validation failed for the 1th input, got %s.", (*this)->operand_source(1).type());
  }
  VLOG(4) << "Verifying attributes:";
  {
  auto& attributes = this->attributes();
  IR_ENFORCE(attributes.count("ksize")>0,
                 "ksize does not exist.");
  IR_ENFORCE(attributes.at("ksize").isa<pir::ArrayAttribute>(),
                 "Type of attribute: ksize is not pir::ArrayAttribute.");
  for (size_t i = 0; i < attributes.at("ksize").dyn_cast<pir::ArrayAttribute>().size(); i++) {
    IR_ENFORCE(attributes.at("ksize").dyn_cast<pir::ArrayAttribute>().at(i).isa<pir::Int32Attribute>(),
                   "Type of attribute: ksize is not right.");
  }
  IR_ENFORCE(attributes.count("strides")>0,
                 "strides does not exist.");
  IR_ENFORCE(attributes.at("strides").isa<pir::ArrayAttribute>(),
                 "Type of attribute: strides is not pir::ArrayAttribute.");
  for (size_t i = 0; i < attributes.at("strides").dyn_cast<pir::ArrayAttribute>().size(); i++) {
    IR_ENFORCE(attributes.at("strides").dyn_cast<pir::ArrayAttribute>().at(i).isa<pir::Int32Attribute>(),
                   "Type of attribute: strides is not right.");
  }
  IR_ENFORCE(attributes.count("paddings")>0,
                 "paddings does not exist.");
  IR_ENFORCE(attributes.at("paddings").isa<pir::ArrayAttribute>(),
                 "Type of attribute: paddings is not pir::ArrayAttribute.");
  for (size_t i = 0; i < attributes.at("paddings").dyn_cast<pir::ArrayAttribute>().size(); i++) {
    IR_ENFORCE(attributes.at("paddings").dyn_cast<pir::ArrayAttribute>().at(i).isa<pir::Int32Attribute>(),
                   "Type of attribute: paddings is not right.");
  }
  IR_ENFORCE(attributes.count("output_size")>0,
                 "output_size does not exist.");
  IR_ENFORCE(attributes.at("output_size").isa<pir::ArrayAttribute>(),
                 "Type of attribute: output_size is not pir::ArrayAttribute.");
  for (size_t i = 0; i < attributes.at("output_size").dyn_cast<pir::ArrayAttribute>().size(); i++) {
    IR_ENFORCE(attributes.at("output_size").dyn_cast<pir::ArrayAttribute>().at(i).isa<pir::Int32Attribute>(),
                   "Type of attribute: output_size is not right.");
  }
  IR_ENFORCE(attributes.count("data_format")>0,
                 "data_format does not exist.");
  IR_ENFORCE(attributes.at("data_format").isa<pir::StrAttribute>(),
                 "Type of attribute: data_format is not pir::StrAttribute.");

  }
  VLOG(4) << "Verifying outputs:";
  {
  auto output_size = num_results();
  IR_ENFORCE(output_size == 1u,
                    "The size %d of outputs must be equal to 1.", output_size);
  IR_ENFORCE((*this)->result(0).type().isa<paddle::dialect::DenseTensorType>(),
                 "Type validation failed for the 0th output.");
  }
  VLOG(4) << "End Verifying for: Unpool3dOp.";
}

void Unpool3dOp::InferMeta( phi::InferMetaContext *infer_meta ) {
  auto fn = PD_INFER_META(phi::Unpool3dInferMeta);
  fn(infer_meta);
}

phi::DataType Unpool3dOp::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: Unpool3dOp";
  


  return expected_kernel_dtype;
}

OpInfoTuple UnsqueezeOp::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("x", "paddle::dialect::DenseTensorType", false, false, false, true), paddle::dialect::OpInputInfo("axis", "paddle::dialect::IntArrayAttribute", false, false, true, false) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = {  };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("out", "paddle::dialect::DenseTensorType", false, false), paddle::dialect::OpOutputInfo("xshape", "paddle::dialect::DenseTensorType", true, true) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("UnsqueezeWithXShapeInferMeta", {"x", "axis"}, "unsqueeze", {"x", "axis"}, {"x"}, {}, {}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "unsqueeze");
}

void UnsqueezeOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, const std::vector<int64_t>& axis) {
  VLOG(4) << "Start build UnsqueezeOp";


  // Generate int_array mutable attribute: axis
  paddle::dialect::FullIntArrayOp full_axis_op = builder.Build<paddle::dialect::FullIntArrayOp>(axis, phi::DataType::INT64, phi::CPUPlace());
  pir::OpResult axis_ = full_axis_op->result(0);
    
  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_, axis_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);
  paddle::dialect::IrTensor dense_out;
  paddle::dialect::IrMetaTensor meta_out(&dense_out);
  paddle::dialect::IrTensor dense_xshape;
  paddle::dialect::IrMetaTensor meta_xshape(&dense_xshape);

  phi::UnsqueezeWithXShapeInferMeta(meta_x, axis, &meta_out, &meta_xshape);

  std::vector<pir::Type> argument_outputs;
  pir::Type out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_out.dtype()), dense_out.dims(), dense_out.layout(), dense_out.lod(), dense_out.offset());
  argument_outputs.push_back(out_dense_tensor_type);

  pir::Type xshape_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_xshape.dtype()), dense_xshape.dims(), dense_xshape.layout(), dense_xshape.lod(), dense_xshape.offset());
  argument_outputs.push_back(xshape_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void UnsqueezeOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, pir::AttributeMap attributes) {
  VLOG(4) << "Start build UnsqueezeOp";


  IR_ENFORCE(
      attributes.find("axis") != attributes.end(),
          "'axis' Attribute is expected for UnsqueezeOp. ");
  std::vector<int64_t> axis = attributes.at("axis").dyn_cast<paddle::dialect::IntArrayAttribute>().data().GetData();

  // Generate int_array mutable attribute: axis
  paddle::dialect::FullIntArrayOp full_axis_op = builder.Build<paddle::dialect::FullIntArrayOp>(axis, phi::DataType::INT64, phi::CPUPlace());
  pir::OpResult axis_ = full_axis_op->result(0);
    
  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_, axis_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);
  paddle::dialect::IrTensor dense_out;
  paddle::dialect::IrMetaTensor meta_out(&dense_out);
  paddle::dialect::IrTensor dense_xshape;
  paddle::dialect::IrMetaTensor meta_xshape(&dense_xshape);

  phi::UnsqueezeWithXShapeInferMeta(meta_x, axis, &meta_out, &meta_xshape);

  std::vector<pir::Type> argument_outputs;
  pir::Type out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_out.dtype()), dense_out.dims(), dense_out.layout(), dense_out.lod(), dense_out.offset());
  argument_outputs.push_back(out_dense_tensor_type);

  pir::Type xshape_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_xshape.dtype()), dense_xshape.dims(), dense_xshape.layout(), dense_xshape.lod(), dense_xshape.offset());
  argument_outputs.push_back(xshape_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void UnsqueezeOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, pir::Value axis_) {
  VLOG(4) << "Start build UnsqueezeOp";



  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_, axis_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;
  phi::IntArray axis;
  if (axis_.dyn_cast<pir::OpResult>() && axis_.dyn_cast<pir::OpResult>().owner()->isa<paddle::dialect::FullIntArrayOp>()) {
    axis = std::move(phi::IntArray(paddle::dialect::GetInt64Vector(
                          axis_.dyn_cast<pir::OpResult>().owner()
                          ->dyn_cast<paddle::dialect::FullIntArrayOp>()
                          .attribute("value"))));
  } else if (axis_.type().isa<pir::VectorType>()) {
    size_t axis_size = axis_.type().dyn_cast<pir::VectorType>().size();
    axis = std::move(phi::IntArray(std::vector<int64_t>(axis_size, -1)));
    axis.SetFromTensor(true);
  } else if (axis_.type().isa<paddle::dialect::DenseTensorType>()) {
    common::DDim axis_dim = axis_.type().dyn_cast<paddle::dialect::DenseTensorType>().dims();
    size_t axis_size = common::product(axis_dim);
    if (common::contain_unknown_dim(axis_dim)) {
      axis_size = 1;
    }
    axis = std::move(phi::IntArray(std::vector<int64_t>(axis_size, -1)));
    axis.SetFromTensor(true);
  } else {
    PADDLE_THROW(phi::errors::Unimplemented("Only support VectorType or DenseTensorType"));
  }


  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);
  paddle::dialect::IrTensor dense_out;
  paddle::dialect::IrMetaTensor meta_out(&dense_out);
  paddle::dialect::IrTensor dense_xshape;
  paddle::dialect::IrMetaTensor meta_xshape(&dense_xshape);

  phi::UnsqueezeWithXShapeInferMeta(meta_x, axis, &meta_out, &meta_xshape);

  std::vector<pir::Type> argument_outputs;
  pir::Type out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_out.dtype()), dense_out.dims(), dense_out.layout(), dense_out.lod(), dense_out.offset());
  argument_outputs.push_back(out_dense_tensor_type);

  pir::Type xshape_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_xshape.dtype()), dense_xshape.dims(), dense_xshape.layout(), dense_xshape.lod(), dense_xshape.offset());
  argument_outputs.push_back(xshape_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void UnsqueezeOp::VerifySig() {
  VLOG(4) << "Start Verifying inputs, outputs and attributes for: UnsqueezeOp.";
  VLOG(4) << "Verifying inputs:";
  {
  auto input_size = num_operands();
  IR_ENFORCE(input_size == 2u,
                    "The size %d of inputs must be equal to 2.", input_size);
  IR_ENFORCE((*this)->operand_source(0).type().isa<paddle::dialect::DenseTensorType>(),
                  "Type validation failed for the 0th input, got %s.", (*this)->operand_source(0).type());
  if (auto vec_type = (*this)->operand_source(1).type().dyn_cast<pir::VectorType>()) {
      for (size_t i = 0; i < vec_type.size(); ++i) {
        IR_ENFORCE(vec_type[i].isa<paddle::dialect::DenseTensorType>(),
                       "Type validation failed for the 1th input, got %s.", (*this)->operand_source(1).type());
      }
  }
  else {
    IR_ENFORCE((*this)->operand_source(1).type().isa<paddle::dialect::DenseTensorType>(),
                   "Type validation failed for the 1th input, got %s.", (*this)->operand_source(1).type());
  }
  }
  VLOG(4) << "Verifying attributes:";
  {
  // Attributes num is 0, not need to check attributes type.
  }
  VLOG(4) << "Verifying outputs:";
  {
  auto output_size = num_results();
  IR_ENFORCE(output_size == 2u,
                    "The size %d of outputs must be equal to 2.", output_size);
  IR_ENFORCE((*this)->result(0).type().isa<paddle::dialect::DenseTensorType>(),
                 "Type validation failed for the 0th output.");
  if (auto output_1_type = (*this)->result(1).type()) {
    IR_ENFORCE(output_1_type.isa<paddle::dialect::DenseTensorType>(),
                   "Type validation failed for the 1th output.");
  }
  }
  VLOG(4) << "End Verifying for: UnsqueezeOp.";
}

void UnsqueezeOp::InferMeta( phi::InferMetaContext *infer_meta ) {
  auto fn = PD_INFER_META(phi::UnsqueezeWithXShapeInferMeta);
  fn(infer_meta);
}

phi::DataType UnsqueezeOp::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: UnsqueezeOp";
  


  return expected_kernel_dtype;
}

OpInfoTuple Unsqueeze_Op::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("x", "paddle::dialect::DenseTensorType", false, false, false, true), paddle::dialect::OpInputInfo("axis", "paddle::dialect::IntArrayAttribute", false, false, true, false) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = {  };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("out", "paddle::dialect::DenseTensorType", false, false), paddle::dialect::OpOutputInfo("xshape", "paddle::dialect::DenseTensorType", true, true) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("UnsqueezeWithXShapeInferMeta", {"x", "axis"}, "unsqueeze", {"x", "axis"}, {"x"}, {}, {{"out", "x"}}, {{"out", "x"}});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "unsqueeze");
}

void Unsqueeze_Op::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, const std::vector<int64_t>& axis) {
  VLOG(4) << "Start build Unsqueeze_Op";


  // Generate int_array mutable attribute: axis
  paddle::dialect::FullIntArrayOp full_axis_op = builder.Build<paddle::dialect::FullIntArrayOp>(axis, phi::DataType::INT64, phi::CPUPlace());
  pir::OpResult axis_ = full_axis_op->result(0);
    
  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_, axis_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);
  paddle::dialect::IrTensor dense_out;
  paddle::dialect::IrMetaTensor meta_out(&dense_out);
  paddle::dialect::IrTensor dense_xshape;
  paddle::dialect::IrMetaTensor meta_xshape(&dense_xshape);

  phi::UnsqueezeWithXShapeInferMeta(meta_x, axis, &meta_out, &meta_xshape);

  std::vector<pir::Type> argument_outputs;
  pir::Type out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_out.dtype()), dense_out.dims(), dense_out.layout(), dense_out.lod(), dense_out.offset());
  argument_outputs.push_back(out_dense_tensor_type);

  pir::Type xshape_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_xshape.dtype()), dense_xshape.dims(), dense_xshape.layout(), dense_xshape.lod(), dense_xshape.offset());
  argument_outputs.push_back(xshape_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void Unsqueeze_Op::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, pir::AttributeMap attributes) {
  VLOG(4) << "Start build Unsqueeze_Op";


  IR_ENFORCE(
      attributes.find("axis") != attributes.end(),
          "'axis' Attribute is expected for Unsqueeze_Op. ");
  std::vector<int64_t> axis = attributes.at("axis").dyn_cast<paddle::dialect::IntArrayAttribute>().data().GetData();

  // Generate int_array mutable attribute: axis
  paddle::dialect::FullIntArrayOp full_axis_op = builder.Build<paddle::dialect::FullIntArrayOp>(axis, phi::DataType::INT64, phi::CPUPlace());
  pir::OpResult axis_ = full_axis_op->result(0);
    
  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_, axis_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);
  paddle::dialect::IrTensor dense_out;
  paddle::dialect::IrMetaTensor meta_out(&dense_out);
  paddle::dialect::IrTensor dense_xshape;
  paddle::dialect::IrMetaTensor meta_xshape(&dense_xshape);

  phi::UnsqueezeWithXShapeInferMeta(meta_x, axis, &meta_out, &meta_xshape);

  std::vector<pir::Type> argument_outputs;
  pir::Type out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_out.dtype()), dense_out.dims(), dense_out.layout(), dense_out.lod(), dense_out.offset());
  argument_outputs.push_back(out_dense_tensor_type);

  pir::Type xshape_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_xshape.dtype()), dense_xshape.dims(), dense_xshape.layout(), dense_xshape.lod(), dense_xshape.offset());
  argument_outputs.push_back(xshape_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void Unsqueeze_Op::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, pir::Value axis_) {
  VLOG(4) << "Start build Unsqueeze_Op";



  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_, axis_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;
  phi::IntArray axis;
  if (axis_.dyn_cast<pir::OpResult>() && axis_.dyn_cast<pir::OpResult>().owner()->isa<paddle::dialect::FullIntArrayOp>()) {
    axis = std::move(phi::IntArray(paddle::dialect::GetInt64Vector(
                          axis_.dyn_cast<pir::OpResult>().owner()
                          ->dyn_cast<paddle::dialect::FullIntArrayOp>()
                          .attribute("value"))));
  } else if (axis_.type().isa<pir::VectorType>()) {
    size_t axis_size = axis_.type().dyn_cast<pir::VectorType>().size();
    axis = std::move(phi::IntArray(std::vector<int64_t>(axis_size, -1)));
    axis.SetFromTensor(true);
  } else if (axis_.type().isa<paddle::dialect::DenseTensorType>()) {
    common::DDim axis_dim = axis_.type().dyn_cast<paddle::dialect::DenseTensorType>().dims();
    size_t axis_size = common::product(axis_dim);
    if (common::contain_unknown_dim(axis_dim)) {
      axis_size = 1;
    }
    axis = std::move(phi::IntArray(std::vector<int64_t>(axis_size, -1)));
    axis.SetFromTensor(true);
  } else {
    PADDLE_THROW(phi::errors::Unimplemented("Only support VectorType or DenseTensorType"));
  }


  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);
  paddle::dialect::IrTensor dense_out;
  paddle::dialect::IrMetaTensor meta_out(&dense_out);
  paddle::dialect::IrTensor dense_xshape;
  paddle::dialect::IrMetaTensor meta_xshape(&dense_xshape);

  phi::UnsqueezeWithXShapeInferMeta(meta_x, axis, &meta_out, &meta_xshape);

  std::vector<pir::Type> argument_outputs;
  pir::Type out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_out.dtype()), dense_out.dims(), dense_out.layout(), dense_out.lod(), dense_out.offset());
  argument_outputs.push_back(out_dense_tensor_type);

  pir::Type xshape_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_xshape.dtype()), dense_xshape.dims(), dense_xshape.layout(), dense_xshape.lod(), dense_xshape.offset());
  argument_outputs.push_back(xshape_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void Unsqueeze_Op::VerifySig() {
  VLOG(4) << "Start Verifying inputs, outputs and attributes for: Unsqueeze_Op.";
  VLOG(4) << "Verifying inputs:";
  {
  auto input_size = num_operands();
  IR_ENFORCE(input_size == 2u,
                    "The size %d of inputs must be equal to 2.", input_size);
  IR_ENFORCE((*this)->operand_source(0).type().isa<paddle::dialect::DenseTensorType>(),
                  "Type validation failed for the 0th input, got %s.", (*this)->operand_source(0).type());
  if (auto vec_type = (*this)->operand_source(1).type().dyn_cast<pir::VectorType>()) {
      for (size_t i = 0; i < vec_type.size(); ++i) {
        IR_ENFORCE(vec_type[i].isa<paddle::dialect::DenseTensorType>(),
                       "Type validation failed for the 1th input, got %s.", (*this)->operand_source(1).type());
      }
  }
  else {
    IR_ENFORCE((*this)->operand_source(1).type().isa<paddle::dialect::DenseTensorType>(),
                   "Type validation failed for the 1th input, got %s.", (*this)->operand_source(1).type());
  }
  }
  VLOG(4) << "Verifying attributes:";
  {
  // Attributes num is 0, not need to check attributes type.
  }
  VLOG(4) << "Verifying outputs:";
  {
  auto output_size = num_results();
  IR_ENFORCE(output_size == 2u,
                    "The size %d of outputs must be equal to 2.", output_size);
  IR_ENFORCE((*this)->result(0).type().isa<paddle::dialect::DenseTensorType>(),
                 "Type validation failed for the 0th output.");
  if (auto output_1_type = (*this)->result(1).type()) {
    IR_ENFORCE(output_1_type.isa<paddle::dialect::DenseTensorType>(),
                   "Type validation failed for the 1th output.");
  }
  }
  VLOG(4) << "End Verifying for: Unsqueeze_Op.";
}

void Unsqueeze_Op::InferMeta( phi::InferMetaContext *infer_meta ) {
  auto fn = PD_INFER_META(phi::UnsqueezeWithXShapeInferMeta);
  fn(infer_meta);
}

phi::DataType Unsqueeze_Op::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: Unsqueeze_Op";
  


  return expected_kernel_dtype;
}

const char *UnstackOp::attributes_name[2] = { "axis", "num" };

OpInfoTuple UnstackOp::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("x", "paddle::dialect::DenseTensorType", false, false, false, true) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = { paddle::dialect::OpAttributeInfo("axis", "pir::Int32Attribute", ""), paddle::dialect::OpAttributeInfo("num", "pir::Int32Attribute", "") };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("out", "pir::VectorType<paddle::dialect::DenseTensorType>", false, false) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("UnStackInferMeta", {"x", "axis", "num"}, "unstack", {"x", "axis", "num"}, {}, {}, {}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "unstack");
}

void UnstackOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, int axis, int num) {
  VLOG(4) << "Start build UnstackOp";



  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_axis = pir::Int32Attribute::get(pir::IrContext::Instance(), axis);
  argument.AddAttribute("axis", attr_axis);
  pir::Attribute attr_num = pir::Int32Attribute::get(pir::IrContext::Instance(), num);
  argument.AddAttribute("num", attr_num);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);
  std::vector<paddle::dialect::IrTensor> vec_dense_out((num), paddle::dialect::IrTensor());
  std::vector<paddle::dialect::IrMetaTensor> vec_meta_out;
  for (size_t i=0; i < static_cast<size_t>(num); i++) {
    vec_meta_out.push_back(paddle::dialect::IrMetaTensor(&vec_dense_out[i]));
  }
  std::vector<phi::MetaTensor*> meta_out;
  for (size_t i=0; i < static_cast<size_t>(vec_meta_out.size()); i++) {
    meta_out.push_back(&vec_meta_out[i]);
  }

  phi::UnStackInferMeta(meta_x, axis, num, meta_out);

  std::vector<pir::Type> argument_outputs;
  std::vector<pir::Type> out_types;
  for (size_t i=0; i < static_cast<size_t>(num); i++) {
    out_types.push_back(paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(vec_dense_out[i].dtype()), vec_dense_out[i].dims(), vec_dense_out[i].layout(), vec_dense_out[i].lod(), vec_dense_out[i].offset()));
  }
  pir::Type out_vector_type = pir::VectorType::get(pir::IrContext::Instance(), out_types);
  argument_outputs.push_back(out_vector_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void UnstackOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, pir::AttributeMap attributes) {
  VLOG(4) << "Start build UnstackOp";


  IR_ENFORCE(
      attributes.find("axis") != attributes.end(),
          "'axis' Attribute is expected for UnstackOp. ");
  int axis = attributes.at("axis").dyn_cast<pir::Int32Attribute>().data();

  IR_ENFORCE(
      attributes.find("num") != attributes.end(),
          "'num' Attribute is expected for UnstackOp. ");
  int num = attributes.at("num").dyn_cast<pir::Int32Attribute>().data();


  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_axis = pir::Int32Attribute::get(pir::IrContext::Instance(), axis);
  argument.AddAttribute("axis", attr_axis);
  pir::Attribute attr_num = pir::Int32Attribute::get(pir::IrContext::Instance(), num);
  argument.AddAttribute("num", attr_num);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);
  std::vector<paddle::dialect::IrTensor> vec_dense_out((num), paddle::dialect::IrTensor());
  std::vector<paddle::dialect::IrMetaTensor> vec_meta_out;
  for (size_t i=0; i < static_cast<size_t>(num); i++) {
    vec_meta_out.push_back(paddle::dialect::IrMetaTensor(&vec_dense_out[i]));
  }
  std::vector<phi::MetaTensor*> meta_out;
  for (size_t i=0; i < static_cast<size_t>(vec_meta_out.size()); i++) {
    meta_out.push_back(&vec_meta_out[i]);
  }

  phi::UnStackInferMeta(meta_x, axis, num, meta_out);

  std::vector<pir::Type> argument_outputs;
  std::vector<pir::Type> out_types;
  for (size_t i=0; i < static_cast<size_t>(num); i++) {
    out_types.push_back(paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(vec_dense_out[i].dtype()), vec_dense_out[i].dims(), vec_dense_out[i].layout(), vec_dense_out[i].lod(), vec_dense_out[i].offset()));
  }
  pir::Type out_vector_type = pir::VectorType::get(pir::IrContext::Instance(), out_types);
  argument_outputs.push_back(out_vector_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void UnstackOp::VerifySig() {
  VLOG(4) << "Start Verifying inputs, outputs and attributes for: UnstackOp.";
  VLOG(4) << "Verifying inputs:";
  {
  auto input_size = num_operands();
  IR_ENFORCE(input_size == 1u,
                    "The size %d of inputs must be equal to 1.", input_size);
  IR_ENFORCE((*this)->operand_source(0).type().isa<paddle::dialect::DenseTensorType>(),
                  "Type validation failed for the 0th input, got %s.", (*this)->operand_source(0).type());
  }
  VLOG(4) << "Verifying attributes:";
  {
  auto& attributes = this->attributes();
  IR_ENFORCE(attributes.count("axis")>0,
                 "axis does not exist.");
  IR_ENFORCE(attributes.at("axis").isa<pir::Int32Attribute>(),
                 "Type of attribute: axis is not pir::Int32Attribute.");

  IR_ENFORCE(attributes.count("num")>0,
                 "num does not exist.");
  IR_ENFORCE(attributes.at("num").isa<pir::Int32Attribute>(),
                 "Type of attribute: num is not pir::Int32Attribute.");

  }
  VLOG(4) << "Verifying outputs:";
  {
  auto output_size = num_results();
  IR_ENFORCE(output_size == 1u,
                    "The size %d of outputs must be equal to 1.", output_size);
  auto output_0_type = (*this)->result(0).type();
  if (auto vec_type = output_0_type.dyn_cast<pir::VectorType>()) {
    for (size_t i = 0; i < vec_type.size(); i++) {
      IR_ENFORCE(vec_type[i].isa<paddle::dialect::DenseTensorType>(),
                     "Type validation failed for the 0th output.");
    }
  }
  else {
    IR_ENFORCE(output_0_type.isa<paddle::dialect::DenseTensorType>(),
                   "Type validation failed for the 0th output.");
  }
  }
  VLOG(4) << "End Verifying for: UnstackOp.";
}

void UnstackOp::InferMeta( phi::InferMetaContext *infer_meta ) {
  auto fn = PD_INFER_META(phi::UnStackInferMeta);
  fn(infer_meta);
}

phi::DataType UnstackOp::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: UnstackOp";
  


  return expected_kernel_dtype;
}

const char *UpdateLossScaling_Op::attributes_name[4] = { "incr_every_n_steps", "decr_every_n_nan_or_inf", "incr_ratio", "decr_ratio" };

OpInfoTuple UpdateLossScaling_Op::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("x", "pir::VectorType<paddle::dialect::DenseTensorType>", false, false, false, false), paddle::dialect::OpInputInfo("found_infinite", "paddle::dialect::DenseTensorType", false, false, false, false), paddle::dialect::OpInputInfo("prev_loss_scaling", "paddle::dialect::DenseTensorType", false, false, false, false), paddle::dialect::OpInputInfo("in_good_steps", "paddle::dialect::DenseTensorType", false, false, false, false), paddle::dialect::OpInputInfo("in_bad_steps", "paddle::dialect::DenseTensorType", false, false, false, false), paddle::dialect::OpInputInfo("stop_update", "paddle::dialect::ScalarAttribute", false, false, true, false) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = { paddle::dialect::OpAttributeInfo("incr_every_n_steps", "pir::Int32Attribute", ""), paddle::dialect::OpAttributeInfo("decr_every_n_nan_or_inf", "pir::Int32Attribute", ""), paddle::dialect::OpAttributeInfo("incr_ratio", "pir::FloatAttribute", ""), paddle::dialect::OpAttributeInfo("decr_ratio", "pir::FloatAttribute", "") };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("out", "pir::VectorType<paddle::dialect::DenseTensorType>", false, false), paddle::dialect::OpOutputInfo("loss_scaling", "paddle::dialect::DenseTensorType", false, false), paddle::dialect::OpOutputInfo("out_good_steps", "paddle::dialect::DenseTensorType", false, false), paddle::dialect::OpOutputInfo("out_bad_steps", "paddle::dialect::DenseTensorType", false, false) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("UpdateLossScalingInferMeta", {"x", "found_infinite", "prev_loss_scaling", "in_good_steps", "in_bad_steps"}, "update_loss_scaling", {"x", "found_infinite", "prev_loss_scaling", "in_good_steps", "in_bad_steps", "incr_every_n_steps", "decr_every_n_nan_or_inf", "incr_ratio", "decr_ratio", "stop_update"}, {"x"}, {}, {{"out", "x"},{"loss_scaling", "prev_loss_scaling"},{"out_good_steps", "in_good_steps"},{"out_bad_steps", "in_bad_steps"}}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "update_loss_scaling_");
}

void UpdateLossScaling_Op::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, pir::Value found_infinite_, pir::Value prev_loss_scaling_, pir::Value in_good_steps_, pir::Value in_bad_steps_, int incr_every_n_steps, int decr_every_n_nan_or_inf, float incr_ratio, float decr_ratio, bool stop_update) {
  VLOG(4) << "Start build UpdateLossScaling_Op";


  // Generate scalar mutable attribute: stop_update
  paddle::dialect::FullOp full_stop_update_op = builder.Build<paddle::dialect::FullOp>(std::vector<int64_t>{1}, stop_update, phi::DataType::BOOL, phi::CPUPlace());
  pir::OpResult stop_update_ = full_stop_update_op->result(0);
    
  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_, found_infinite_, prev_loss_scaling_, in_good_steps_, in_bad_steps_, stop_update_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_incr_every_n_steps = pir::Int32Attribute::get(pir::IrContext::Instance(), incr_every_n_steps);
  argument.AddAttribute("incr_every_n_steps", attr_incr_every_n_steps);
  pir::Attribute attr_decr_every_n_nan_or_inf = pir::Int32Attribute::get(pir::IrContext::Instance(), decr_every_n_nan_or_inf);
  argument.AddAttribute("decr_every_n_nan_or_inf", attr_decr_every_n_nan_or_inf);
  pir::Attribute attr_incr_ratio = pir::FloatAttribute::get(pir::IrContext::Instance(), incr_ratio);
  argument.AddAttribute("incr_ratio", attr_incr_ratio);
  pir::Attribute attr_decr_ratio = pir::FloatAttribute::get(pir::IrContext::Instance(), decr_ratio);
  argument.AddAttribute("decr_ratio", attr_decr_ratio);

  VLOG(4) << "Builder construction outputs";
  pir::VectorType x = x_.type().dyn_cast<pir::VectorType>(); (void)x;
  paddle::dialect::DenseTensorType found_infinite = found_infinite_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)found_infinite;
  paddle::dialect::DenseTensorType prev_loss_scaling = prev_loss_scaling_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)prev_loss_scaling;
  paddle::dialect::DenseTensorType in_good_steps = in_good_steps_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)in_good_steps;
  paddle::dialect::DenseTensorType in_bad_steps = in_bad_steps_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)in_bad_steps;
  std::vector<paddle::dialect::IrTensor> vec_ir_tensor_x;
  for (size_t i=0; i < static_cast<size_t>(x.size()); i++) {
    vec_ir_tensor_x.push_back(paddle::dialect::IrTensor(paddle::dialect::TransToPhiDataType(x[i].dyn_cast<paddle::dialect::DenseTensorType>().dtype()),
                                                                     x[i].dyn_cast<paddle::dialect::DenseTensorType>().dims(),
                                                                     x[i].dyn_cast<paddle::dialect::DenseTensorType>().data_layout(),
                                                                     x[i].dyn_cast<paddle::dialect::DenseTensorType>().lod(),
                                                                     x[i].dyn_cast<paddle::dialect::DenseTensorType>().offset()));
  }
  std::vector<paddle::dialect::IrMetaTensor> vec_meta_x;
  for (size_t i=0; i < vec_ir_tensor_x.size(); i++) {
    vec_meta_x.push_back(paddle::dialect::IrMetaTensor(&vec_ir_tensor_x[i]));
  }

  std::vector<const phi::MetaTensor*> meta_x;
  for (size_t i=0; i < static_cast<size_t>(vec_meta_x.size()); i++) {
    meta_x.push_back(&vec_meta_x[i]);
  }
 
  VLOG(4) << "Builder construction  dense_found_infinite";
  paddle::dialect::IrTensor ir_tensor_found_infinite(paddle::dialect::TransToPhiDataType(found_infinite.dtype()),
                                                      found_infinite.dims(),
                                                      found_infinite.data_layout(),
                                                      found_infinite.lod(),
                                                      found_infinite.offset());
  VLOG(4) << "Builder construction  meta_found_infinite";
  paddle::dialect::IrMetaTensor meta_found_infinite(&ir_tensor_found_infinite);

  VLOG(4) << "Builder construction  dense_prev_loss_scaling";
  paddle::dialect::IrTensor ir_tensor_prev_loss_scaling(paddle::dialect::TransToPhiDataType(prev_loss_scaling.dtype()),
                                                      prev_loss_scaling.dims(),
                                                      prev_loss_scaling.data_layout(),
                                                      prev_loss_scaling.lod(),
                                                      prev_loss_scaling.offset());
  VLOG(4) << "Builder construction  meta_prev_loss_scaling";
  paddle::dialect::IrMetaTensor meta_prev_loss_scaling(&ir_tensor_prev_loss_scaling);

  VLOG(4) << "Builder construction  dense_in_good_steps";
  paddle::dialect::IrTensor ir_tensor_in_good_steps(paddle::dialect::TransToPhiDataType(in_good_steps.dtype()),
                                                      in_good_steps.dims(),
                                                      in_good_steps.data_layout(),
                                                      in_good_steps.lod(),
                                                      in_good_steps.offset());
  VLOG(4) << "Builder construction  meta_in_good_steps";
  paddle::dialect::IrMetaTensor meta_in_good_steps(&ir_tensor_in_good_steps);

  VLOG(4) << "Builder construction  dense_in_bad_steps";
  paddle::dialect::IrTensor ir_tensor_in_bad_steps(paddle::dialect::TransToPhiDataType(in_bad_steps.dtype()),
                                                      in_bad_steps.dims(),
                                                      in_bad_steps.data_layout(),
                                                      in_bad_steps.lod(),
                                                      in_bad_steps.offset());
  VLOG(4) << "Builder construction  meta_in_bad_steps";
  paddle::dialect::IrMetaTensor meta_in_bad_steps(&ir_tensor_in_bad_steps);
  std::vector<paddle::dialect::IrTensor> vec_dense_out((x.size()), paddle::dialect::IrTensor());
  std::vector<paddle::dialect::IrMetaTensor> vec_meta_out;
  for (size_t i=0; i < static_cast<size_t>(x.size()); i++) {
    vec_meta_out.push_back(paddle::dialect::IrMetaTensor(&vec_dense_out[i]));
  }
  std::vector<phi::MetaTensor*> meta_out;
  for (size_t i=0; i < static_cast<size_t>(vec_meta_out.size()); i++) {
    meta_out.push_back(&vec_meta_out[i]);
  }
  paddle::dialect::IrTensor dense_loss_scaling;
  paddle::dialect::IrMetaTensor meta_loss_scaling(&dense_loss_scaling);
  paddle::dialect::IrTensor dense_out_good_steps;
  paddle::dialect::IrMetaTensor meta_out_good_steps(&dense_out_good_steps);
  paddle::dialect::IrTensor dense_out_bad_steps;
  paddle::dialect::IrMetaTensor meta_out_bad_steps(&dense_out_bad_steps);

  phi::UpdateLossScalingInferMeta(meta_x, meta_found_infinite, meta_prev_loss_scaling, meta_in_good_steps, meta_in_bad_steps, meta_out, &meta_loss_scaling, &meta_out_good_steps, &meta_out_bad_steps);

  std::vector<pir::Type> argument_outputs;
  std::vector<pir::Type> out_types;
  for (size_t i=0; i < static_cast<size_t>(x.size()); i++) {
    out_types.push_back(paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(vec_dense_out[i].dtype()), vec_dense_out[i].dims(), vec_dense_out[i].layout(), vec_dense_out[i].lod(), vec_dense_out[i].offset()));
  }
  pir::Type out_vector_type = pir::VectorType::get(pir::IrContext::Instance(), out_types);
  argument_outputs.push_back(out_vector_type);

  pir::Type loss_scaling_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_loss_scaling.dtype()), dense_loss_scaling.dims(), dense_loss_scaling.layout(), dense_loss_scaling.lod(), dense_loss_scaling.offset());
  argument_outputs.push_back(loss_scaling_dense_tensor_type);

  pir::Type out_good_steps_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_out_good_steps.dtype()), dense_out_good_steps.dims(), dense_out_good_steps.layout(), dense_out_good_steps.lod(), dense_out_good_steps.offset());
  argument_outputs.push_back(out_good_steps_dense_tensor_type);

  pir::Type out_bad_steps_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_out_bad_steps.dtype()), dense_out_bad_steps.dims(), dense_out_bad_steps.layout(), dense_out_bad_steps.lod(), dense_out_bad_steps.offset());
  argument_outputs.push_back(out_bad_steps_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void UpdateLossScaling_Op::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, pir::Value found_infinite_, pir::Value prev_loss_scaling_, pir::Value in_good_steps_, pir::Value in_bad_steps_, pir::AttributeMap attributes) {
  VLOG(4) << "Start build UpdateLossScaling_Op";


  IR_ENFORCE(
      attributes.find("incr_every_n_steps") != attributes.end(),
          "'incr_every_n_steps' Attribute is expected for UpdateLossScaling_Op. ");
  int incr_every_n_steps = attributes.at("incr_every_n_steps").dyn_cast<pir::Int32Attribute>().data();

  IR_ENFORCE(
      attributes.find("decr_every_n_nan_or_inf") != attributes.end(),
          "'decr_every_n_nan_or_inf' Attribute is expected for UpdateLossScaling_Op. ");
  int decr_every_n_nan_or_inf = attributes.at("decr_every_n_nan_or_inf").dyn_cast<pir::Int32Attribute>().data();

  IR_ENFORCE(
      attributes.find("incr_ratio") != attributes.end(),
          "'incr_ratio' Attribute is expected for UpdateLossScaling_Op. ");
  float incr_ratio = attributes.at("incr_ratio").dyn_cast<pir::FloatAttribute>().data();

  IR_ENFORCE(
      attributes.find("decr_ratio") != attributes.end(),
          "'decr_ratio' Attribute is expected for UpdateLossScaling_Op. ");
  float decr_ratio = attributes.at("decr_ratio").dyn_cast<pir::FloatAttribute>().data();

  IR_ENFORCE(
      attributes.find("stop_update") != attributes.end(),
          "'stop_update' Attribute is expected for UpdateLossScaling_Op. ");
  bool stop_update = attributes.at("stop_update").dyn_cast<paddle::dialect::ScalarAttribute>().data().to<bool>();

  // Generate scalar mutable attribute: stop_update
  paddle::dialect::FullOp full_stop_update_op = builder.Build<paddle::dialect::FullOp>(std::vector<int64_t>{1}, stop_update, phi::DataType::BOOL, phi::CPUPlace());
  pir::OpResult stop_update_ = full_stop_update_op->result(0);
    
  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_, found_infinite_, prev_loss_scaling_, in_good_steps_, in_bad_steps_, stop_update_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_incr_every_n_steps = pir::Int32Attribute::get(pir::IrContext::Instance(), incr_every_n_steps);
  argument.AddAttribute("incr_every_n_steps", attr_incr_every_n_steps);
  pir::Attribute attr_decr_every_n_nan_or_inf = pir::Int32Attribute::get(pir::IrContext::Instance(), decr_every_n_nan_or_inf);
  argument.AddAttribute("decr_every_n_nan_or_inf", attr_decr_every_n_nan_or_inf);
  pir::Attribute attr_incr_ratio = pir::FloatAttribute::get(pir::IrContext::Instance(), incr_ratio);
  argument.AddAttribute("incr_ratio", attr_incr_ratio);
  pir::Attribute attr_decr_ratio = pir::FloatAttribute::get(pir::IrContext::Instance(), decr_ratio);
  argument.AddAttribute("decr_ratio", attr_decr_ratio);

  VLOG(4) << "Builder construction outputs";
  pir::VectorType x = x_.type().dyn_cast<pir::VectorType>(); (void)x;
  paddle::dialect::DenseTensorType found_infinite = found_infinite_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)found_infinite;
  paddle::dialect::DenseTensorType prev_loss_scaling = prev_loss_scaling_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)prev_loss_scaling;
  paddle::dialect::DenseTensorType in_good_steps = in_good_steps_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)in_good_steps;
  paddle::dialect::DenseTensorType in_bad_steps = in_bad_steps_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)in_bad_steps;
  std::vector<paddle::dialect::IrTensor> vec_ir_tensor_x;
  for (size_t i=0; i < static_cast<size_t>(x.size()); i++) {
    vec_ir_tensor_x.push_back(paddle::dialect::IrTensor(paddle::dialect::TransToPhiDataType(x[i].dyn_cast<paddle::dialect::DenseTensorType>().dtype()),
                                                                     x[i].dyn_cast<paddle::dialect::DenseTensorType>().dims(),
                                                                     x[i].dyn_cast<paddle::dialect::DenseTensorType>().data_layout(),
                                                                     x[i].dyn_cast<paddle::dialect::DenseTensorType>().lod(),
                                                                     x[i].dyn_cast<paddle::dialect::DenseTensorType>().offset()));
  }
  std::vector<paddle::dialect::IrMetaTensor> vec_meta_x;
  for (size_t i=0; i < vec_ir_tensor_x.size(); i++) {
    vec_meta_x.push_back(paddle::dialect::IrMetaTensor(&vec_ir_tensor_x[i]));
  }

  std::vector<const phi::MetaTensor*> meta_x;
  for (size_t i=0; i < static_cast<size_t>(vec_meta_x.size()); i++) {
    meta_x.push_back(&vec_meta_x[i]);
  }
 
  VLOG(4) << "Builder construction  dense_found_infinite";
  paddle::dialect::IrTensor ir_tensor_found_infinite(paddle::dialect::TransToPhiDataType(found_infinite.dtype()),
                                                      found_infinite.dims(),
                                                      found_infinite.data_layout(),
                                                      found_infinite.lod(),
                                                      found_infinite.offset());
  VLOG(4) << "Builder construction  meta_found_infinite";
  paddle::dialect::IrMetaTensor meta_found_infinite(&ir_tensor_found_infinite);

  VLOG(4) << "Builder construction  dense_prev_loss_scaling";
  paddle::dialect::IrTensor ir_tensor_prev_loss_scaling(paddle::dialect::TransToPhiDataType(prev_loss_scaling.dtype()),
                                                      prev_loss_scaling.dims(),
                                                      prev_loss_scaling.data_layout(),
                                                      prev_loss_scaling.lod(),
                                                      prev_loss_scaling.offset());
  VLOG(4) << "Builder construction  meta_prev_loss_scaling";
  paddle::dialect::IrMetaTensor meta_prev_loss_scaling(&ir_tensor_prev_loss_scaling);

  VLOG(4) << "Builder construction  dense_in_good_steps";
  paddle::dialect::IrTensor ir_tensor_in_good_steps(paddle::dialect::TransToPhiDataType(in_good_steps.dtype()),
                                                      in_good_steps.dims(),
                                                      in_good_steps.data_layout(),
                                                      in_good_steps.lod(),
                                                      in_good_steps.offset());
  VLOG(4) << "Builder construction  meta_in_good_steps";
  paddle::dialect::IrMetaTensor meta_in_good_steps(&ir_tensor_in_good_steps);

  VLOG(4) << "Builder construction  dense_in_bad_steps";
  paddle::dialect::IrTensor ir_tensor_in_bad_steps(paddle::dialect::TransToPhiDataType(in_bad_steps.dtype()),
                                                      in_bad_steps.dims(),
                                                      in_bad_steps.data_layout(),
                                                      in_bad_steps.lod(),
                                                      in_bad_steps.offset());
  VLOG(4) << "Builder construction  meta_in_bad_steps";
  paddle::dialect::IrMetaTensor meta_in_bad_steps(&ir_tensor_in_bad_steps);
  std::vector<paddle::dialect::IrTensor> vec_dense_out((x.size()), paddle::dialect::IrTensor());
  std::vector<paddle::dialect::IrMetaTensor> vec_meta_out;
  for (size_t i=0; i < static_cast<size_t>(x.size()); i++) {
    vec_meta_out.push_back(paddle::dialect::IrMetaTensor(&vec_dense_out[i]));
  }
  std::vector<phi::MetaTensor*> meta_out;
  for (size_t i=0; i < static_cast<size_t>(vec_meta_out.size()); i++) {
    meta_out.push_back(&vec_meta_out[i]);
  }
  paddle::dialect::IrTensor dense_loss_scaling;
  paddle::dialect::IrMetaTensor meta_loss_scaling(&dense_loss_scaling);
  paddle::dialect::IrTensor dense_out_good_steps;
  paddle::dialect::IrMetaTensor meta_out_good_steps(&dense_out_good_steps);
  paddle::dialect::IrTensor dense_out_bad_steps;
  paddle::dialect::IrMetaTensor meta_out_bad_steps(&dense_out_bad_steps);

  phi::UpdateLossScalingInferMeta(meta_x, meta_found_infinite, meta_prev_loss_scaling, meta_in_good_steps, meta_in_bad_steps, meta_out, &meta_loss_scaling, &meta_out_good_steps, &meta_out_bad_steps);

  std::vector<pir::Type> argument_outputs;
  std::vector<pir::Type> out_types;
  for (size_t i=0; i < static_cast<size_t>(x.size()); i++) {
    out_types.push_back(paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(vec_dense_out[i].dtype()), vec_dense_out[i].dims(), vec_dense_out[i].layout(), vec_dense_out[i].lod(), vec_dense_out[i].offset()));
  }
  pir::Type out_vector_type = pir::VectorType::get(pir::IrContext::Instance(), out_types);
  argument_outputs.push_back(out_vector_type);

  pir::Type loss_scaling_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_loss_scaling.dtype()), dense_loss_scaling.dims(), dense_loss_scaling.layout(), dense_loss_scaling.lod(), dense_loss_scaling.offset());
  argument_outputs.push_back(loss_scaling_dense_tensor_type);

  pir::Type out_good_steps_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_out_good_steps.dtype()), dense_out_good_steps.dims(), dense_out_good_steps.layout(), dense_out_good_steps.lod(), dense_out_good_steps.offset());
  argument_outputs.push_back(out_good_steps_dense_tensor_type);

  pir::Type out_bad_steps_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_out_bad_steps.dtype()), dense_out_bad_steps.dims(), dense_out_bad_steps.layout(), dense_out_bad_steps.lod(), dense_out_bad_steps.offset());
  argument_outputs.push_back(out_bad_steps_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void UpdateLossScaling_Op::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, pir::Value found_infinite_, pir::Value prev_loss_scaling_, pir::Value in_good_steps_, pir::Value in_bad_steps_, pir::Value stop_update_, int incr_every_n_steps, int decr_every_n_nan_or_inf, float incr_ratio, float decr_ratio) {
  VLOG(4) << "Start build UpdateLossScaling_Op";



  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_, found_infinite_, prev_loss_scaling_, in_good_steps_, in_bad_steps_, stop_update_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_incr_every_n_steps = pir::Int32Attribute::get(pir::IrContext::Instance(), incr_every_n_steps);
  argument.AddAttribute("incr_every_n_steps", attr_incr_every_n_steps);
  pir::Attribute attr_decr_every_n_nan_or_inf = pir::Int32Attribute::get(pir::IrContext::Instance(), decr_every_n_nan_or_inf);
  argument.AddAttribute("decr_every_n_nan_or_inf", attr_decr_every_n_nan_or_inf);
  pir::Attribute attr_incr_ratio = pir::FloatAttribute::get(pir::IrContext::Instance(), incr_ratio);
  argument.AddAttribute("incr_ratio", attr_incr_ratio);
  pir::Attribute attr_decr_ratio = pir::FloatAttribute::get(pir::IrContext::Instance(), decr_ratio);
  argument.AddAttribute("decr_ratio", attr_decr_ratio);

  VLOG(4) << "Builder construction outputs";
  pir::VectorType x = x_.type().dyn_cast<pir::VectorType>(); (void)x;
  paddle::dialect::DenseTensorType found_infinite = found_infinite_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)found_infinite;
  paddle::dialect::DenseTensorType prev_loss_scaling = prev_loss_scaling_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)prev_loss_scaling;
  paddle::dialect::DenseTensorType in_good_steps = in_good_steps_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)in_good_steps;
  paddle::dialect::DenseTensorType in_bad_steps = in_bad_steps_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)in_bad_steps;
  phi::Scalar stop_update;
  if (stop_update_.dyn_cast<pir::OpResult>() && stop_update_.dyn_cast<pir::OpResult>().owner()->isa<paddle::dialect::FullOp>()) {
    stop_update = std::move(phi::Scalar(stop_update_.dyn_cast<pir::OpResult>().owner()
                                  ->dyn_cast<paddle::dialect::FullOp>()
                                  .attribute("value")
                                  .dyn_cast<paddle::dialect::ScalarAttribute>()
                                  .data()
                                  .to<int>()));
  }
  else {
    stop_update = std::move(phi::Scalar(-1));
    stop_update.SetFromTensor(true);
  }

  std::vector<paddle::dialect::IrTensor> vec_ir_tensor_x;
  for (size_t i=0; i < static_cast<size_t>(x.size()); i++) {
    vec_ir_tensor_x.push_back(paddle::dialect::IrTensor(paddle::dialect::TransToPhiDataType(x[i].dyn_cast<paddle::dialect::DenseTensorType>().dtype()),
                                                                     x[i].dyn_cast<paddle::dialect::DenseTensorType>().dims(),
                                                                     x[i].dyn_cast<paddle::dialect::DenseTensorType>().data_layout(),
                                                                     x[i].dyn_cast<paddle::dialect::DenseTensorType>().lod(),
                                                                     x[i].dyn_cast<paddle::dialect::DenseTensorType>().offset()));
  }
  std::vector<paddle::dialect::IrMetaTensor> vec_meta_x;
  for (size_t i=0; i < vec_ir_tensor_x.size(); i++) {
    vec_meta_x.push_back(paddle::dialect::IrMetaTensor(&vec_ir_tensor_x[i]));
  }

  std::vector<const phi::MetaTensor*> meta_x;
  for (size_t i=0; i < static_cast<size_t>(vec_meta_x.size()); i++) {
    meta_x.push_back(&vec_meta_x[i]);
  }
 
  VLOG(4) << "Builder construction  dense_found_infinite";
  paddle::dialect::IrTensor ir_tensor_found_infinite(paddle::dialect::TransToPhiDataType(found_infinite.dtype()),
                                                      found_infinite.dims(),
                                                      found_infinite.data_layout(),
                                                      found_infinite.lod(),
                                                      found_infinite.offset());
  VLOG(4) << "Builder construction  meta_found_infinite";
  paddle::dialect::IrMetaTensor meta_found_infinite(&ir_tensor_found_infinite);

  VLOG(4) << "Builder construction  dense_prev_loss_scaling";
  paddle::dialect::IrTensor ir_tensor_prev_loss_scaling(paddle::dialect::TransToPhiDataType(prev_loss_scaling.dtype()),
                                                      prev_loss_scaling.dims(),
                                                      prev_loss_scaling.data_layout(),
                                                      prev_loss_scaling.lod(),
                                                      prev_loss_scaling.offset());
  VLOG(4) << "Builder construction  meta_prev_loss_scaling";
  paddle::dialect::IrMetaTensor meta_prev_loss_scaling(&ir_tensor_prev_loss_scaling);

  VLOG(4) << "Builder construction  dense_in_good_steps";
  paddle::dialect::IrTensor ir_tensor_in_good_steps(paddle::dialect::TransToPhiDataType(in_good_steps.dtype()),
                                                      in_good_steps.dims(),
                                                      in_good_steps.data_layout(),
                                                      in_good_steps.lod(),
                                                      in_good_steps.offset());
  VLOG(4) << "Builder construction  meta_in_good_steps";
  paddle::dialect::IrMetaTensor meta_in_good_steps(&ir_tensor_in_good_steps);

  VLOG(4) << "Builder construction  dense_in_bad_steps";
  paddle::dialect::IrTensor ir_tensor_in_bad_steps(paddle::dialect::TransToPhiDataType(in_bad_steps.dtype()),
                                                      in_bad_steps.dims(),
                                                      in_bad_steps.data_layout(),
                                                      in_bad_steps.lod(),
                                                      in_bad_steps.offset());
  VLOG(4) << "Builder construction  meta_in_bad_steps";
  paddle::dialect::IrMetaTensor meta_in_bad_steps(&ir_tensor_in_bad_steps);
  std::vector<paddle::dialect::IrTensor> vec_dense_out((x.size()), paddle::dialect::IrTensor());
  std::vector<paddle::dialect::IrMetaTensor> vec_meta_out;
  for (size_t i=0; i < static_cast<size_t>(x.size()); i++) {
    vec_meta_out.push_back(paddle::dialect::IrMetaTensor(&vec_dense_out[i]));
  }
  std::vector<phi::MetaTensor*> meta_out;
  for (size_t i=0; i < static_cast<size_t>(vec_meta_out.size()); i++) {
    meta_out.push_back(&vec_meta_out[i]);
  }
  paddle::dialect::IrTensor dense_loss_scaling;
  paddle::dialect::IrMetaTensor meta_loss_scaling(&dense_loss_scaling);
  paddle::dialect::IrTensor dense_out_good_steps;
  paddle::dialect::IrMetaTensor meta_out_good_steps(&dense_out_good_steps);
  paddle::dialect::IrTensor dense_out_bad_steps;
  paddle::dialect::IrMetaTensor meta_out_bad_steps(&dense_out_bad_steps);

  phi::UpdateLossScalingInferMeta(meta_x, meta_found_infinite, meta_prev_loss_scaling, meta_in_good_steps, meta_in_bad_steps, meta_out, &meta_loss_scaling, &meta_out_good_steps, &meta_out_bad_steps);

  std::vector<pir::Type> argument_outputs;
  std::vector<pir::Type> out_types;
  for (size_t i=0; i < static_cast<size_t>(x.size()); i++) {
    out_types.push_back(paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(vec_dense_out[i].dtype()), vec_dense_out[i].dims(), vec_dense_out[i].layout(), vec_dense_out[i].lod(), vec_dense_out[i].offset()));
  }
  pir::Type out_vector_type = pir::VectorType::get(pir::IrContext::Instance(), out_types);
  argument_outputs.push_back(out_vector_type);

  pir::Type loss_scaling_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_loss_scaling.dtype()), dense_loss_scaling.dims(), dense_loss_scaling.layout(), dense_loss_scaling.lod(), dense_loss_scaling.offset());
  argument_outputs.push_back(loss_scaling_dense_tensor_type);

  pir::Type out_good_steps_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_out_good_steps.dtype()), dense_out_good_steps.dims(), dense_out_good_steps.layout(), dense_out_good_steps.lod(), dense_out_good_steps.offset());
  argument_outputs.push_back(out_good_steps_dense_tensor_type);

  pir::Type out_bad_steps_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_out_bad_steps.dtype()), dense_out_bad_steps.dims(), dense_out_bad_steps.layout(), dense_out_bad_steps.lod(), dense_out_bad_steps.offset());
  argument_outputs.push_back(out_bad_steps_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void UpdateLossScaling_Op::VerifySig() {
  VLOG(4) << "Start Verifying inputs, outputs and attributes for: UpdateLossScaling_Op.";
  VLOG(4) << "Verifying inputs:";
  {
  auto input_size = num_operands();
  IR_ENFORCE(input_size == 6u,
                    "The size %d of inputs must be equal to 6.", input_size);
  if (auto vec_type = (*this)->operand_source(0).type().dyn_cast<pir::VectorType>()) {
      for (size_t i = 0; i < vec_type.size(); ++i) {
        IR_ENFORCE(vec_type[i].isa<paddle::dialect::DenseTensorType>(),
                       "Type validation failed for the 0th input, got %s.", (*this)->operand_source(0).type());
      }
  }
  else {
    IR_ENFORCE((*this)->operand_source(0).type().isa<paddle::dialect::DenseTensorType>(),
                   "Type validation failed for the 0th input, got %s.", (*this)->operand_source(0).type());
  }
  IR_ENFORCE((*this)->operand_source(1).type().isa<paddle::dialect::DenseTensorType>(),
                  "Type validation failed for the 1th input, got %s.", (*this)->operand_source(1).type());
  IR_ENFORCE((*this)->operand_source(2).type().isa<paddle::dialect::DenseTensorType>(),
                  "Type validation failed for the 2th input, got %s.", (*this)->operand_source(2).type());
  IR_ENFORCE((*this)->operand_source(3).type().isa<paddle::dialect::DenseTensorType>(),
                  "Type validation failed for the 3th input, got %s.", (*this)->operand_source(3).type());
  IR_ENFORCE((*this)->operand_source(4).type().isa<paddle::dialect::DenseTensorType>(),
                  "Type validation failed for the 4th input, got %s.", (*this)->operand_source(4).type());
  IR_ENFORCE((*this)->operand_source(5).type().isa<paddle::dialect::DenseTensorType>(),
                  "Type validation failed for the 5th input, got %s.", (*this)->operand_source(5).type());
  }
  VLOG(4) << "Verifying attributes:";
  {
  auto& attributes = this->attributes();
  IR_ENFORCE(attributes.count("incr_every_n_steps")>0,
                 "incr_every_n_steps does not exist.");
  IR_ENFORCE(attributes.at("incr_every_n_steps").isa<pir::Int32Attribute>(),
                 "Type of attribute: incr_every_n_steps is not pir::Int32Attribute.");

  IR_ENFORCE(attributes.count("decr_every_n_nan_or_inf")>0,
                 "decr_every_n_nan_or_inf does not exist.");
  IR_ENFORCE(attributes.at("decr_every_n_nan_or_inf").isa<pir::Int32Attribute>(),
                 "Type of attribute: decr_every_n_nan_or_inf is not pir::Int32Attribute.");

  IR_ENFORCE(attributes.count("incr_ratio")>0,
                 "incr_ratio does not exist.");
  IR_ENFORCE(attributes.at("incr_ratio").isa<pir::FloatAttribute>(),
                 "Type of attribute: incr_ratio is not pir::FloatAttribute.");

  IR_ENFORCE(attributes.count("decr_ratio")>0,
                 "decr_ratio does not exist.");
  IR_ENFORCE(attributes.at("decr_ratio").isa<pir::FloatAttribute>(),
                 "Type of attribute: decr_ratio is not pir::FloatAttribute.");

  }
  VLOG(4) << "Verifying outputs:";
  {
  auto output_size = num_results();
  IR_ENFORCE(output_size == 4u,
                    "The size %d of outputs must be equal to 4.", output_size);
  auto output_0_type = (*this)->result(0).type();
  if (auto vec_type = output_0_type.dyn_cast<pir::VectorType>()) {
    for (size_t i = 0; i < vec_type.size(); i++) {
      IR_ENFORCE(vec_type[i].isa<paddle::dialect::DenseTensorType>(),
                     "Type validation failed for the 0th output.");
    }
  }
  else {
    IR_ENFORCE(output_0_type.isa<paddle::dialect::DenseTensorType>(),
                   "Type validation failed for the 0th output.");
  }
  IR_ENFORCE((*this)->result(1).type().isa<paddle::dialect::DenseTensorType>(),
                 "Type validation failed for the 1th output.");
  IR_ENFORCE((*this)->result(2).type().isa<paddle::dialect::DenseTensorType>(),
                 "Type validation failed for the 2th output.");
  IR_ENFORCE((*this)->result(3).type().isa<paddle::dialect::DenseTensorType>(),
                 "Type validation failed for the 3th output.");
  }
  VLOG(4) << "End Verifying for: UpdateLossScaling_Op.";
}

void UpdateLossScaling_Op::InferMeta( phi::InferMetaContext *infer_meta ) {
  auto fn = PD_INFER_META(phi::UpdateLossScalingInferMeta);
  fn(infer_meta);
}

phi::DataType UpdateLossScaling_Op::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: UpdateLossScaling_Op";
  

  // deal skip data transform
  if (var_name == "found_infinite"){
    return expected_kernel_dtype;
  }


  return expected_kernel_dtype;
}

const char *ViewDtypeOp::attributes_name[1] = { "dtype" };

OpInfoTuple ViewDtypeOp::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("input", "paddle::dialect::DenseTensorType", false, true, false, true) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = { paddle::dialect::OpAttributeInfo("dtype", "paddle::dialect::DataTypeAttribute", "") };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("out", "paddle::dialect::DenseTensorType", false, false) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("StridedUnChangedInferMeta", {"input"}, "view_dtype", {"input", "dtype"}, {"input"}, {}, {}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "view_dtype");
}

void ViewDtypeOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value input_, phi::DataType dtype) {
  VLOG(4) << "Start build ViewDtypeOp";



  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {input_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_dtype = paddle::dialect::DataTypeAttribute::get(pir::IrContext::Instance(), dtype);
  argument.AddAttribute("dtype", attr_dtype);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType input = input_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)input;

  VLOG(4) << "Builder construction  dense_input";
  paddle::dialect::IrTensor ir_tensor_input(paddle::dialect::TransToPhiDataType(input.dtype()),
                                                      input.dims(),
                                                      input.data_layout(),
                                                      input.lod(),
                                                      input.offset());
  VLOG(4) << "Builder construction  meta_input";
  paddle::dialect::IrMetaTensor meta_input(&ir_tensor_input);
  paddle::dialect::IrTensor dense_out;
  paddle::dialect::IrMetaTensor meta_out(&dense_out);

  phi::StridedUnChangedInferMeta(meta_input, &meta_out);

  std::vector<pir::Type> argument_outputs;
  pir::Type out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_out.dtype()), dense_out.dims(), dense_out.layout(), dense_out.lod(), dense_out.offset());
  argument_outputs.push_back(out_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void ViewDtypeOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value input_, pir::AttributeMap attributes) {
  VLOG(4) << "Start build ViewDtypeOp";


  IR_ENFORCE(
      attributes.find("dtype") != attributes.end(),
          "'dtype' Attribute is expected for ViewDtypeOp. ");
  phi::DataType dtype = attributes.at("dtype").dyn_cast<paddle::dialect::DataTypeAttribute>().data();


  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {input_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_dtype = paddle::dialect::DataTypeAttribute::get(pir::IrContext::Instance(), dtype);
  argument.AddAttribute("dtype", attr_dtype);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType input = input_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)input;

  VLOG(4) << "Builder construction  dense_input";
  paddle::dialect::IrTensor ir_tensor_input(paddle::dialect::TransToPhiDataType(input.dtype()),
                                                      input.dims(),
                                                      input.data_layout(),
                                                      input.lod(),
                                                      input.offset());
  VLOG(4) << "Builder construction  meta_input";
  paddle::dialect::IrMetaTensor meta_input(&ir_tensor_input);
  paddle::dialect::IrTensor dense_out;
  paddle::dialect::IrMetaTensor meta_out(&dense_out);

  phi::StridedUnChangedInferMeta(meta_input, &meta_out);

  std::vector<pir::Type> argument_outputs;
  pir::Type out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_out.dtype()), dense_out.dims(), dense_out.layout(), dense_out.lod(), dense_out.offset());
  argument_outputs.push_back(out_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void ViewDtypeOp::VerifySig() {
  VLOG(4) << "Start Verifying inputs, outputs and attributes for: ViewDtypeOp.";
  VLOG(4) << "Verifying inputs:";
  {
  auto input_size = num_operands();
  IR_ENFORCE(input_size == 1u,
                    "The size %d of inputs must be equal to 1.", input_size);
  IR_ENFORCE((*this)->operand_source(0).type().isa<paddle::dialect::DenseTensorType>(),
                  "Type validation failed for the 0th input, got %s.", (*this)->operand_source(0).type());
  }
  VLOG(4) << "Verifying attributes:";
  {
  auto& attributes = this->attributes();
  IR_ENFORCE(attributes.count("dtype")>0,
                 "dtype does not exist.");
  IR_ENFORCE(attributes.at("dtype").isa<paddle::dialect::DataTypeAttribute>(),
                 "Type of attribute: dtype is not paddle::dialect::DataTypeAttribute.");

  }
  VLOG(4) << "Verifying outputs:";
  {
  auto output_size = num_results();
  IR_ENFORCE(output_size == 1u,
                    "The size %d of outputs must be equal to 1.", output_size);
  IR_ENFORCE((*this)->result(0).type().isa<paddle::dialect::DenseTensorType>(),
                 "Type validation failed for the 0th output.");
  }
  VLOG(4) << "End Verifying for: ViewDtypeOp.";
}

void ViewDtypeOp::InferMeta( phi::InferMetaContext *infer_meta ) {
  auto fn = PD_INFER_META(phi::StridedUnChangedInferMeta);
  fn(infer_meta);
}

phi::DataType ViewDtypeOp::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: ViewDtypeOp";
  


  return expected_kernel_dtype;
}

const char *ViewShapeOp::attributes_name[1] = { "dims" };

OpInfoTuple ViewShapeOp::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("input", "paddle::dialect::DenseTensorType", false, true, false, true) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = { paddle::dialect::OpAttributeInfo("dims", "pir::ArrayAttribute<pir::Int64Attribute>", "") };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("out", "paddle::dialect::DenseTensorType", false, false) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("StridedUnChangedInferMeta", {"input"}, "view_shape", {"input", "dims"}, {}, {}, {}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "view_shape");
}

void ViewShapeOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value input_, const std::vector<int64_t>& dims) {
  VLOG(4) << "Start build ViewShapeOp";



  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {input_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  std::vector<pir::Attribute> vec_dims;
  for (size_t i = 0; i < static_cast<size_t>(dims.size()); i++) {
      pir::Attribute attr_dims = pir::Int64Attribute::get(pir::IrContext::Instance(), dims[i]);

    vec_dims.push_back(attr_dims);
  }
  pir::Attribute attr_dims = pir::ArrayAttribute::get(pir::IrContext::Instance(), vec_dims);
  argument.AddAttribute("dims", attr_dims);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType input = input_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)input;

  VLOG(4) << "Builder construction  dense_input";
  paddle::dialect::IrTensor ir_tensor_input(paddle::dialect::TransToPhiDataType(input.dtype()),
                                                      input.dims(),
                                                      input.data_layout(),
                                                      input.lod(),
                                                      input.offset());
  VLOG(4) << "Builder construction  meta_input";
  paddle::dialect::IrMetaTensor meta_input(&ir_tensor_input);
  paddle::dialect::IrTensor dense_out;
  paddle::dialect::IrMetaTensor meta_out(&dense_out);

  phi::StridedUnChangedInferMeta(meta_input, &meta_out);

  std::vector<pir::Type> argument_outputs;
  pir::Type out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_out.dtype()), dense_out.dims(), dense_out.layout(), dense_out.lod(), dense_out.offset());
  argument_outputs.push_back(out_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void ViewShapeOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value input_, pir::AttributeMap attributes) {
  VLOG(4) << "Start build ViewShapeOp";


  IR_ENFORCE(
      attributes.find("dims") != attributes.end(),
          "'dims' Attribute is expected for ViewShapeOp. ");
  std::vector<int64_t> dims;
  for (size_t i = 0; i < attributes.at("dims").dyn_cast<pir::ArrayAttribute>().size(); i++) {
    dims.push_back(attributes.at("dims").dyn_cast<pir::ArrayAttribute>().at(i).dyn_cast<pir::Int64Attribute>().data());
  }


  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {input_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  std::vector<pir::Attribute> vec_dims;
  for (size_t i = 0; i < static_cast<size_t>(dims.size()); i++) {
      pir::Attribute attr_dims = pir::Int64Attribute::get(pir::IrContext::Instance(), dims[i]);

    vec_dims.push_back(attr_dims);
  }
  pir::Attribute attr_dims = pir::ArrayAttribute::get(pir::IrContext::Instance(), vec_dims);
  argument.AddAttribute("dims", attr_dims);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType input = input_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)input;

  VLOG(4) << "Builder construction  dense_input";
  paddle::dialect::IrTensor ir_tensor_input(paddle::dialect::TransToPhiDataType(input.dtype()),
                                                      input.dims(),
                                                      input.data_layout(),
                                                      input.lod(),
                                                      input.offset());
  VLOG(4) << "Builder construction  meta_input";
  paddle::dialect::IrMetaTensor meta_input(&ir_tensor_input);
  paddle::dialect::IrTensor dense_out;
  paddle::dialect::IrMetaTensor meta_out(&dense_out);

  phi::StridedUnChangedInferMeta(meta_input, &meta_out);

  std::vector<pir::Type> argument_outputs;
  pir::Type out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_out.dtype()), dense_out.dims(), dense_out.layout(), dense_out.lod(), dense_out.offset());
  argument_outputs.push_back(out_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void ViewShapeOp::VerifySig() {
  VLOG(4) << "Start Verifying inputs, outputs and attributes for: ViewShapeOp.";
  VLOG(4) << "Verifying inputs:";
  {
  auto input_size = num_operands();
  IR_ENFORCE(input_size == 1u,
                    "The size %d of inputs must be equal to 1.", input_size);
  IR_ENFORCE((*this)->operand_source(0).type().isa<paddle::dialect::DenseTensorType>(),
                  "Type validation failed for the 0th input, got %s.", (*this)->operand_source(0).type());
  }
  VLOG(4) << "Verifying attributes:";
  {
  auto& attributes = this->attributes();
  IR_ENFORCE(attributes.count("dims")>0,
                 "dims does not exist.");
  IR_ENFORCE(attributes.at("dims").isa<pir::ArrayAttribute>(),
                 "Type of attribute: dims is not pir::ArrayAttribute.");
  for (size_t i = 0; i < attributes.at("dims").dyn_cast<pir::ArrayAttribute>().size(); i++) {
    IR_ENFORCE(attributes.at("dims").dyn_cast<pir::ArrayAttribute>().at(i).isa<pir::Int64Attribute>(),
                   "Type of attribute: dims is not right.");
  }
  }
  VLOG(4) << "Verifying outputs:";
  {
  auto output_size = num_results();
  IR_ENFORCE(output_size == 1u,
                    "The size %d of outputs must be equal to 1.", output_size);
  IR_ENFORCE((*this)->result(0).type().isa<paddle::dialect::DenseTensorType>(),
                 "Type validation failed for the 0th output.");
  }
  VLOG(4) << "End Verifying for: ViewShapeOp.";
}

void ViewShapeOp::InferMeta( phi::InferMetaContext *infer_meta ) {
  auto fn = PD_INFER_META(phi::StridedUnChangedInferMeta);
  fn(infer_meta);
}

phi::DataType ViewShapeOp::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: ViewShapeOp";
  


  return expected_kernel_dtype;
}

const char *ViterbiDecodeOp::attributes_name[1] = { "include_bos_eos_tag" };

OpInfoTuple ViterbiDecodeOp::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("potentials", "paddle::dialect::DenseTensorType", false, false, false, false), paddle::dialect::OpInputInfo("transition_params", "paddle::dialect::DenseTensorType", false, false, false, false), paddle::dialect::OpInputInfo("lengths", "paddle::dialect::DenseTensorType", false, false, false, false) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = { paddle::dialect::OpAttributeInfo("include_bos_eos_tag", "pir::BoolAttribute", "") };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("scores", "paddle::dialect::DenseTensorType", false, false), paddle::dialect::OpOutputInfo("path", "paddle::dialect::DenseTensorType", false, false) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("ViterbiDecodeInferMeta", {"potentials", "transition_params", "lengths", "include_bos_eos_tag"}, "viterbi_decode", {"potentials", "transition_params", "lengths", "include_bos_eos_tag"}, {"potentials"}, {}, {}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "viterbi_decode");
}

void ViterbiDecodeOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value potentials_, pir::Value transition_params_, pir::Value lengths_, bool include_bos_eos_tag) {
  VLOG(4) << "Start build ViterbiDecodeOp";



  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {potentials_, transition_params_, lengths_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_include_bos_eos_tag = pir::BoolAttribute::get(pir::IrContext::Instance(), include_bos_eos_tag);
  argument.AddAttribute("include_bos_eos_tag", attr_include_bos_eos_tag);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType potentials = potentials_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)potentials;
  paddle::dialect::DenseTensorType transition_params = transition_params_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)transition_params;
  paddle::dialect::DenseTensorType lengths = lengths_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)lengths;

  VLOG(4) << "Builder construction  dense_potentials";
  paddle::dialect::IrTensor ir_tensor_potentials(paddle::dialect::TransToPhiDataType(potentials.dtype()),
                                                      potentials.dims(),
                                                      potentials.data_layout(),
                                                      potentials.lod(),
                                                      potentials.offset());
  VLOG(4) << "Builder construction  meta_potentials";
  paddle::dialect::IrMetaTensor meta_potentials(&ir_tensor_potentials);

  VLOG(4) << "Builder construction  dense_transition_params";
  paddle::dialect::IrTensor ir_tensor_transition_params(paddle::dialect::TransToPhiDataType(transition_params.dtype()),
                                                      transition_params.dims(),
                                                      transition_params.data_layout(),
                                                      transition_params.lod(),
                                                      transition_params.offset());
  VLOG(4) << "Builder construction  meta_transition_params";
  paddle::dialect::IrMetaTensor meta_transition_params(&ir_tensor_transition_params);

  VLOG(4) << "Builder construction  dense_lengths";
  paddle::dialect::IrTensor ir_tensor_lengths(paddle::dialect::TransToPhiDataType(lengths.dtype()),
                                                      lengths.dims(),
                                                      lengths.data_layout(),
                                                      lengths.lod(),
                                                      lengths.offset());
  VLOG(4) << "Builder construction  meta_lengths";
  paddle::dialect::IrMetaTensor meta_lengths(&ir_tensor_lengths);
  paddle::dialect::IrTensor dense_scores;
  paddle::dialect::IrMetaTensor meta_scores(&dense_scores);
  paddle::dialect::IrTensor dense_path;
  paddle::dialect::IrMetaTensor meta_path(&dense_path);

  phi::ViterbiDecodeInferMeta(meta_potentials, meta_transition_params, meta_lengths, include_bos_eos_tag, &meta_scores, &meta_path);

  std::vector<pir::Type> argument_outputs;
  pir::Type scores_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_scores.dtype()), dense_scores.dims(), dense_scores.layout(), dense_scores.lod(), dense_scores.offset());
  argument_outputs.push_back(scores_dense_tensor_type);

  pir::Type path_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_path.dtype()), dense_path.dims(), dense_path.layout(), dense_path.lod(), dense_path.offset());
  argument_outputs.push_back(path_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void ViterbiDecodeOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value potentials_, pir::Value transition_params_, pir::Value lengths_, pir::AttributeMap attributes) {
  VLOG(4) << "Start build ViterbiDecodeOp";


  IR_ENFORCE(
      attributes.find("include_bos_eos_tag") != attributes.end(),
          "'include_bos_eos_tag' Attribute is expected for ViterbiDecodeOp. ");
  bool include_bos_eos_tag = attributes.at("include_bos_eos_tag").dyn_cast<pir::BoolAttribute>().data();


  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {potentials_, transition_params_, lengths_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_include_bos_eos_tag = pir::BoolAttribute::get(pir::IrContext::Instance(), include_bos_eos_tag);
  argument.AddAttribute("include_bos_eos_tag", attr_include_bos_eos_tag);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType potentials = potentials_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)potentials;
  paddle::dialect::DenseTensorType transition_params = transition_params_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)transition_params;
  paddle::dialect::DenseTensorType lengths = lengths_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)lengths;

  VLOG(4) << "Builder construction  dense_potentials";
  paddle::dialect::IrTensor ir_tensor_potentials(paddle::dialect::TransToPhiDataType(potentials.dtype()),
                                                      potentials.dims(),
                                                      potentials.data_layout(),
                                                      potentials.lod(),
                                                      potentials.offset());
  VLOG(4) << "Builder construction  meta_potentials";
  paddle::dialect::IrMetaTensor meta_potentials(&ir_tensor_potentials);

  VLOG(4) << "Builder construction  dense_transition_params";
  paddle::dialect::IrTensor ir_tensor_transition_params(paddle::dialect::TransToPhiDataType(transition_params.dtype()),
                                                      transition_params.dims(),
                                                      transition_params.data_layout(),
                                                      transition_params.lod(),
                                                      transition_params.offset());
  VLOG(4) << "Builder construction  meta_transition_params";
  paddle::dialect::IrMetaTensor meta_transition_params(&ir_tensor_transition_params);

  VLOG(4) << "Builder construction  dense_lengths";
  paddle::dialect::IrTensor ir_tensor_lengths(paddle::dialect::TransToPhiDataType(lengths.dtype()),
                                                      lengths.dims(),
                                                      lengths.data_layout(),
                                                      lengths.lod(),
                                                      lengths.offset());
  VLOG(4) << "Builder construction  meta_lengths";
  paddle::dialect::IrMetaTensor meta_lengths(&ir_tensor_lengths);
  paddle::dialect::IrTensor dense_scores;
  paddle::dialect::IrMetaTensor meta_scores(&dense_scores);
  paddle::dialect::IrTensor dense_path;
  paddle::dialect::IrMetaTensor meta_path(&dense_path);

  phi::ViterbiDecodeInferMeta(meta_potentials, meta_transition_params, meta_lengths, include_bos_eos_tag, &meta_scores, &meta_path);

  std::vector<pir::Type> argument_outputs;
  pir::Type scores_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_scores.dtype()), dense_scores.dims(), dense_scores.layout(), dense_scores.lod(), dense_scores.offset());
  argument_outputs.push_back(scores_dense_tensor_type);

  pir::Type path_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_path.dtype()), dense_path.dims(), dense_path.layout(), dense_path.lod(), dense_path.offset());
  argument_outputs.push_back(path_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void ViterbiDecodeOp::VerifySig() {
  VLOG(4) << "Start Verifying inputs, outputs and attributes for: ViterbiDecodeOp.";
  VLOG(4) << "Verifying inputs:";
  {
  auto input_size = num_operands();
  IR_ENFORCE(input_size == 3u,
                    "The size %d of inputs must be equal to 3.", input_size);
  IR_ENFORCE((*this)->operand_source(0).type().isa<paddle::dialect::DenseTensorType>(),
                  "Type validation failed for the 0th input, got %s.", (*this)->operand_source(0).type());
  IR_ENFORCE((*this)->operand_source(1).type().isa<paddle::dialect::DenseTensorType>(),
                  "Type validation failed for the 1th input, got %s.", (*this)->operand_source(1).type());
  IR_ENFORCE((*this)->operand_source(2).type().isa<paddle::dialect::DenseTensorType>(),
                  "Type validation failed for the 2th input, got %s.", (*this)->operand_source(2).type());
  }
  VLOG(4) << "Verifying attributes:";
  {
  auto& attributes = this->attributes();
  IR_ENFORCE(attributes.count("include_bos_eos_tag")>0,
                 "include_bos_eos_tag does not exist.");
  IR_ENFORCE(attributes.at("include_bos_eos_tag").isa<pir::BoolAttribute>(),
                 "Type of attribute: include_bos_eos_tag is not pir::BoolAttribute.");

  }
  VLOG(4) << "Verifying outputs:";
  {
  auto output_size = num_results();
  IR_ENFORCE(output_size == 2u,
                    "The size %d of outputs must be equal to 2.", output_size);
  IR_ENFORCE((*this)->result(0).type().isa<paddle::dialect::DenseTensorType>(),
                 "Type validation failed for the 0th output.");
  IR_ENFORCE((*this)->result(1).type().isa<paddle::dialect::DenseTensorType>(),
                 "Type validation failed for the 1th output.");
  }
  VLOG(4) << "End Verifying for: ViterbiDecodeOp.";
}

void ViterbiDecodeOp::InferMeta( phi::InferMetaContext *infer_meta ) {
  auto fn = PD_INFER_META(phi::ViterbiDecodeInferMeta);
  fn(infer_meta);
}

phi::DataType ViterbiDecodeOp::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: ViterbiDecodeOp";
  


  return expected_kernel_dtype;
}

const char *WarpctcOp::attributes_name[2] = { "blank", "norm_by_times" };

OpInfoTuple WarpctcOp::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("logits", "paddle::dialect::DenseTensorType", false, false, false, true), paddle::dialect::OpInputInfo("label", "paddle::dialect::DenseTensorType", false, false, false, false), paddle::dialect::OpInputInfo("logits_length", "paddle::dialect::DenseTensorType", true, false, false, false), paddle::dialect::OpInputInfo("labels_length", "paddle::dialect::DenseTensorType", true, false, false, false) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = { paddle::dialect::OpAttributeInfo("blank", "pir::Int32Attribute", ""), paddle::dialect::OpAttributeInfo("norm_by_times", "pir::BoolAttribute", "") };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("loss", "paddle::dialect::DenseTensorType", false, false), paddle::dialect::OpOutputInfo("warpctcgrad", "paddle::dialect::DenseTensorType", true, true) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("WarpctcInferMeta", {"logits", "label", "logits_length", "labels_length", "blank", "norm_by_times"}, "warpctc", {"logits", "label", "logits_length", "labels_length", "blank", "norm_by_times"}, {"logits"}, {}, {}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "warpctc");
}

void WarpctcOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value logits_, pir::Value label_, pir::Value logits_length_, pir::Value labels_length_, int blank, bool norm_by_times) {
  VLOG(4) << "Start build WarpctcOp";



  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {logits_, label_, logits_length_, labels_length_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_blank = pir::Int32Attribute::get(pir::IrContext::Instance(), blank);
  argument.AddAttribute("blank", attr_blank);
  pir::Attribute attr_norm_by_times = pir::BoolAttribute::get(pir::IrContext::Instance(), norm_by_times);
  argument.AddAttribute("norm_by_times", attr_norm_by_times);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType logits = logits_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)logits;
  paddle::dialect::DenseTensorType label = label_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)label;

  VLOG(4) << "Builder construction  dense_logits";
  paddle::dialect::IrTensor ir_tensor_logits(paddle::dialect::TransToPhiDataType(logits.dtype()),
                                                      logits.dims(),
                                                      logits.data_layout(),
                                                      logits.lod(),
                                                      logits.offset());
  VLOG(4) << "Builder construction  meta_logits";
  paddle::dialect::IrMetaTensor meta_logits(&ir_tensor_logits);

  VLOG(4) << "Builder construction  dense_label";
  paddle::dialect::IrTensor ir_tensor_label(paddle::dialect::TransToPhiDataType(label.dtype()),
                                                      label.dims(),
                                                      label.data_layout(),
                                                      label.lod(),
                                                      label.offset());
  VLOG(4) << "Builder construction  meta_label";
  paddle::dialect::IrMetaTensor meta_label(&ir_tensor_label);

  paddle::dialect::IrMetaTensor meta_logits_length;
  paddle::dialect::IrTensor ir_tensor_logits_length;
  if (logits_length_.impl() != nullptr) {
    paddle::dialect::DenseTensorType logits_length = logits_length_.type().dyn_cast<paddle::dialect::DenseTensorType>();
    VLOG(4) << "Builder construction  dense_logits_length";
    ir_tensor_logits_length = paddle::dialect::IrTensor(paddle::dialect::TransToPhiDataType(logits_length.dtype()),
                                                        logits_length.dims(),
                                                        logits_length.data_layout(),
                                                        logits_length.lod(),
                                                        logits_length.offset());
    VLOG(4) << "Builder construction  meta_logits_length";
    meta_logits_length = paddle::dialect::IrMetaTensor(&ir_tensor_logits_length);
  }


  paddle::dialect::IrMetaTensor meta_labels_length;
  paddle::dialect::IrTensor ir_tensor_labels_length;
  if (labels_length_.impl() != nullptr) {
    paddle::dialect::DenseTensorType labels_length = labels_length_.type().dyn_cast<paddle::dialect::DenseTensorType>();
    VLOG(4) << "Builder construction  dense_labels_length";
    ir_tensor_labels_length = paddle::dialect::IrTensor(paddle::dialect::TransToPhiDataType(labels_length.dtype()),
                                                        labels_length.dims(),
                                                        labels_length.data_layout(),
                                                        labels_length.lod(),
                                                        labels_length.offset());
    VLOG(4) << "Builder construction  meta_labels_length";
    meta_labels_length = paddle::dialect::IrMetaTensor(&ir_tensor_labels_length);
  }

  paddle::dialect::IrTensor dense_loss;
  paddle::dialect::IrMetaTensor meta_loss(&dense_loss);
  paddle::dialect::IrTensor dense_warpctcgrad;
  paddle::dialect::IrMetaTensor meta_warpctcgrad(&dense_warpctcgrad);

  phi::WarpctcInferMeta(meta_logits, meta_label, meta_logits_length, meta_labels_length, blank, norm_by_times, &meta_loss, &meta_warpctcgrad);

  std::vector<pir::Type> argument_outputs;
  pir::Type loss_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_loss.dtype()), dense_loss.dims(), dense_loss.layout(), dense_loss.lod(), dense_loss.offset());
  argument_outputs.push_back(loss_dense_tensor_type);

  pir::Type warpctcgrad_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_warpctcgrad.dtype()), dense_warpctcgrad.dims(), dense_warpctcgrad.layout(), dense_warpctcgrad.lod(), dense_warpctcgrad.offset());
  argument_outputs.push_back(warpctcgrad_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void WarpctcOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value logits_, pir::Value label_, pir::Value logits_length_, pir::Value labels_length_, pir::AttributeMap attributes) {
  VLOG(4) << "Start build WarpctcOp";


  IR_ENFORCE(
      attributes.find("blank") != attributes.end(),
          "'blank' Attribute is expected for WarpctcOp. ");
  int blank = attributes.at("blank").dyn_cast<pir::Int32Attribute>().data();

  IR_ENFORCE(
      attributes.find("norm_by_times") != attributes.end(),
          "'norm_by_times' Attribute is expected for WarpctcOp. ");
  bool norm_by_times = attributes.at("norm_by_times").dyn_cast<pir::BoolAttribute>().data();


  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {logits_, label_, logits_length_, labels_length_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_blank = pir::Int32Attribute::get(pir::IrContext::Instance(), blank);
  argument.AddAttribute("blank", attr_blank);
  pir::Attribute attr_norm_by_times = pir::BoolAttribute::get(pir::IrContext::Instance(), norm_by_times);
  argument.AddAttribute("norm_by_times", attr_norm_by_times);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType logits = logits_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)logits;
  paddle::dialect::DenseTensorType label = label_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)label;

  VLOG(4) << "Builder construction  dense_logits";
  paddle::dialect::IrTensor ir_tensor_logits(paddle::dialect::TransToPhiDataType(logits.dtype()),
                                                      logits.dims(),
                                                      logits.data_layout(),
                                                      logits.lod(),
                                                      logits.offset());
  VLOG(4) << "Builder construction  meta_logits";
  paddle::dialect::IrMetaTensor meta_logits(&ir_tensor_logits);

  VLOG(4) << "Builder construction  dense_label";
  paddle::dialect::IrTensor ir_tensor_label(paddle::dialect::TransToPhiDataType(label.dtype()),
                                                      label.dims(),
                                                      label.data_layout(),
                                                      label.lod(),
                                                      label.offset());
  VLOG(4) << "Builder construction  meta_label";
  paddle::dialect::IrMetaTensor meta_label(&ir_tensor_label);

  paddle::dialect::IrMetaTensor meta_logits_length;
  paddle::dialect::IrTensor ir_tensor_logits_length;
  if (logits_length_.impl() != nullptr) {
    paddle::dialect::DenseTensorType logits_length = logits_length_.type().dyn_cast<paddle::dialect::DenseTensorType>();
    VLOG(4) << "Builder construction  dense_logits_length";
    ir_tensor_logits_length = paddle::dialect::IrTensor(paddle::dialect::TransToPhiDataType(logits_length.dtype()),
                                                        logits_length.dims(),
                                                        logits_length.data_layout(),
                                                        logits_length.lod(),
                                                        logits_length.offset());
    VLOG(4) << "Builder construction  meta_logits_length";
    meta_logits_length = paddle::dialect::IrMetaTensor(&ir_tensor_logits_length);
  }


  paddle::dialect::IrMetaTensor meta_labels_length;
  paddle::dialect::IrTensor ir_tensor_labels_length;
  if (labels_length_.impl() != nullptr) {
    paddle::dialect::DenseTensorType labels_length = labels_length_.type().dyn_cast<paddle::dialect::DenseTensorType>();
    VLOG(4) << "Builder construction  dense_labels_length";
    ir_tensor_labels_length = paddle::dialect::IrTensor(paddle::dialect::TransToPhiDataType(labels_length.dtype()),
                                                        labels_length.dims(),
                                                        labels_length.data_layout(),
                                                        labels_length.lod(),
                                                        labels_length.offset());
    VLOG(4) << "Builder construction  meta_labels_length";
    meta_labels_length = paddle::dialect::IrMetaTensor(&ir_tensor_labels_length);
  }

  paddle::dialect::IrTensor dense_loss;
  paddle::dialect::IrMetaTensor meta_loss(&dense_loss);
  paddle::dialect::IrTensor dense_warpctcgrad;
  paddle::dialect::IrMetaTensor meta_warpctcgrad(&dense_warpctcgrad);

  phi::WarpctcInferMeta(meta_logits, meta_label, meta_logits_length, meta_labels_length, blank, norm_by_times, &meta_loss, &meta_warpctcgrad);

  std::vector<pir::Type> argument_outputs;
  pir::Type loss_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_loss.dtype()), dense_loss.dims(), dense_loss.layout(), dense_loss.lod(), dense_loss.offset());
  argument_outputs.push_back(loss_dense_tensor_type);

  pir::Type warpctcgrad_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_warpctcgrad.dtype()), dense_warpctcgrad.dims(), dense_warpctcgrad.layout(), dense_warpctcgrad.lod(), dense_warpctcgrad.offset());
  argument_outputs.push_back(warpctcgrad_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void WarpctcOp::VerifySig() {
  VLOG(4) << "Start Verifying inputs, outputs and attributes for: WarpctcOp.";
  VLOG(4) << "Verifying inputs:";
  {
  auto input_size = num_operands();
  IR_ENFORCE(input_size == 4u,
                    "The size %d of inputs must be equal to 4.", input_size);
  IR_ENFORCE((*this)->operand_source(0).type().isa<paddle::dialect::DenseTensorType>(),
                  "Type validation failed for the 0th input, got %s.", (*this)->operand_source(0).type());
  IR_ENFORCE((*this)->operand_source(1).type().isa<paddle::dialect::DenseTensorType>(),
                  "Type validation failed for the 1th input, got %s.", (*this)->operand_source(1).type());
  if (auto val = (*this)->operand(2)) {
    IR_ENFORCE(val.type().isa<paddle::dialect::DenseTensorType>(),
                   "Type validation failed for the 2th input, got %s.", (*this)->operand_source(2).type());
  }
  if (auto val = (*this)->operand(3)) {
    IR_ENFORCE(val.type().isa<paddle::dialect::DenseTensorType>(),
                   "Type validation failed for the 3th input, got %s.", (*this)->operand_source(3).type());
  }
  }
  VLOG(4) << "Verifying attributes:";
  {
  auto& attributes = this->attributes();
  IR_ENFORCE(attributes.count("blank")>0,
                 "blank does not exist.");
  IR_ENFORCE(attributes.at("blank").isa<pir::Int32Attribute>(),
                 "Type of attribute: blank is not pir::Int32Attribute.");

  IR_ENFORCE(attributes.count("norm_by_times")>0,
                 "norm_by_times does not exist.");
  IR_ENFORCE(attributes.at("norm_by_times").isa<pir::BoolAttribute>(),
                 "Type of attribute: norm_by_times is not pir::BoolAttribute.");

  }
  VLOG(4) << "Verifying outputs:";
  {
  auto output_size = num_results();
  IR_ENFORCE(output_size == 2u,
                    "The size %d of outputs must be equal to 2.", output_size);
  IR_ENFORCE((*this)->result(0).type().isa<paddle::dialect::DenseTensorType>(),
                 "Type validation failed for the 0th output.");
  if (auto output_1_type = (*this)->result(1).type()) {
    IR_ENFORCE(output_1_type.isa<paddle::dialect::DenseTensorType>(),
                   "Type validation failed for the 1th output.");
  }
  }
  VLOG(4) << "End Verifying for: WarpctcOp.";
}

void WarpctcOp::InferMeta( phi::InferMetaContext *infer_meta ) {
  auto fn = PD_INFER_META(phi::WarpctcInferMeta);
  fn(infer_meta);
}

phi::DataType WarpctcOp::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: WarpctcOp";
  


  return expected_kernel_dtype;
}

const char *WarprnntOp::attributes_name[2] = { "blank", "fastemit_lambda" };

OpInfoTuple WarprnntOp::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("input", "paddle::dialect::DenseTensorType", false, false, false, true), paddle::dialect::OpInputInfo("label", "paddle::dialect::DenseTensorType", false, false, false, false), paddle::dialect::OpInputInfo("input_lengths", "paddle::dialect::DenseTensorType", false, false, false, false), paddle::dialect::OpInputInfo("label_lengths", "paddle::dialect::DenseTensorType", false, false, false, false) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = { paddle::dialect::OpAttributeInfo("blank", "pir::Int32Attribute", ""), paddle::dialect::OpAttributeInfo("fastemit_lambda", "pir::FloatAttribute", "") };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("loss", "paddle::dialect::DenseTensorType", false, false), paddle::dialect::OpOutputInfo("warprnntgrad", "paddle::dialect::DenseTensorType", true, true) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("WarprnntInferMeta", {"input", "label", "input_lengths", "label_lengths", "blank", "fastemit_lambda"}, "warprnnt", {"input", "label", "input_lengths", "label_lengths", "blank", "fastemit_lambda"}, {"input"}, {}, {}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "warprnnt");
}

void WarprnntOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value input_, pir::Value label_, pir::Value input_lengths_, pir::Value label_lengths_, int blank, float fastemit_lambda) {
  VLOG(4) << "Start build WarprnntOp";



  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {input_, label_, input_lengths_, label_lengths_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_blank = pir::Int32Attribute::get(pir::IrContext::Instance(), blank);
  argument.AddAttribute("blank", attr_blank);
  pir::Attribute attr_fastemit_lambda = pir::FloatAttribute::get(pir::IrContext::Instance(), fastemit_lambda);
  argument.AddAttribute("fastemit_lambda", attr_fastemit_lambda);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType input = input_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)input;
  paddle::dialect::DenseTensorType label = label_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)label;
  paddle::dialect::DenseTensorType input_lengths = input_lengths_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)input_lengths;
  paddle::dialect::DenseTensorType label_lengths = label_lengths_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)label_lengths;

  VLOG(4) << "Builder construction  dense_input";
  paddle::dialect::IrTensor ir_tensor_input(paddle::dialect::TransToPhiDataType(input.dtype()),
                                                      input.dims(),
                                                      input.data_layout(),
                                                      input.lod(),
                                                      input.offset());
  VLOG(4) << "Builder construction  meta_input";
  paddle::dialect::IrMetaTensor meta_input(&ir_tensor_input);

  VLOG(4) << "Builder construction  dense_label";
  paddle::dialect::IrTensor ir_tensor_label(paddle::dialect::TransToPhiDataType(label.dtype()),
                                                      label.dims(),
                                                      label.data_layout(),
                                                      label.lod(),
                                                      label.offset());
  VLOG(4) << "Builder construction  meta_label";
  paddle::dialect::IrMetaTensor meta_label(&ir_tensor_label);

  VLOG(4) << "Builder construction  dense_input_lengths";
  paddle::dialect::IrTensor ir_tensor_input_lengths(paddle::dialect::TransToPhiDataType(input_lengths.dtype()),
                                                      input_lengths.dims(),
                                                      input_lengths.data_layout(),
                                                      input_lengths.lod(),
                                                      input_lengths.offset());
  VLOG(4) << "Builder construction  meta_input_lengths";
  paddle::dialect::IrMetaTensor meta_input_lengths(&ir_tensor_input_lengths);

  VLOG(4) << "Builder construction  dense_label_lengths";
  paddle::dialect::IrTensor ir_tensor_label_lengths(paddle::dialect::TransToPhiDataType(label_lengths.dtype()),
                                                      label_lengths.dims(),
                                                      label_lengths.data_layout(),
                                                      label_lengths.lod(),
                                                      label_lengths.offset());
  VLOG(4) << "Builder construction  meta_label_lengths";
  paddle::dialect::IrMetaTensor meta_label_lengths(&ir_tensor_label_lengths);
  paddle::dialect::IrTensor dense_loss;
  paddle::dialect::IrMetaTensor meta_loss(&dense_loss);
  paddle::dialect::IrTensor dense_warprnntgrad;
  paddle::dialect::IrMetaTensor meta_warprnntgrad(&dense_warprnntgrad);

  phi::WarprnntInferMeta(meta_input, meta_label, meta_input_lengths, meta_label_lengths, blank, fastemit_lambda, &meta_loss, &meta_warprnntgrad);

  std::vector<pir::Type> argument_outputs;
  pir::Type loss_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_loss.dtype()), dense_loss.dims(), dense_loss.layout(), dense_loss.lod(), dense_loss.offset());
  argument_outputs.push_back(loss_dense_tensor_type);

  pir::Type warprnntgrad_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_warprnntgrad.dtype()), dense_warprnntgrad.dims(), dense_warprnntgrad.layout(), dense_warprnntgrad.lod(), dense_warprnntgrad.offset());
  argument_outputs.push_back(warprnntgrad_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void WarprnntOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value input_, pir::Value label_, pir::Value input_lengths_, pir::Value label_lengths_, pir::AttributeMap attributes) {
  VLOG(4) << "Start build WarprnntOp";


  IR_ENFORCE(
      attributes.find("blank") != attributes.end(),
          "'blank' Attribute is expected for WarprnntOp. ");
  int blank = attributes.at("blank").dyn_cast<pir::Int32Attribute>().data();

  IR_ENFORCE(
      attributes.find("fastemit_lambda") != attributes.end(),
          "'fastemit_lambda' Attribute is expected for WarprnntOp. ");
  float fastemit_lambda = attributes.at("fastemit_lambda").dyn_cast<pir::FloatAttribute>().data();


  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {input_, label_, input_lengths_, label_lengths_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_blank = pir::Int32Attribute::get(pir::IrContext::Instance(), blank);
  argument.AddAttribute("blank", attr_blank);
  pir::Attribute attr_fastemit_lambda = pir::FloatAttribute::get(pir::IrContext::Instance(), fastemit_lambda);
  argument.AddAttribute("fastemit_lambda", attr_fastemit_lambda);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType input = input_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)input;
  paddle::dialect::DenseTensorType label = label_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)label;
  paddle::dialect::DenseTensorType input_lengths = input_lengths_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)input_lengths;
  paddle::dialect::DenseTensorType label_lengths = label_lengths_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)label_lengths;

  VLOG(4) << "Builder construction  dense_input";
  paddle::dialect::IrTensor ir_tensor_input(paddle::dialect::TransToPhiDataType(input.dtype()),
                                                      input.dims(),
                                                      input.data_layout(),
                                                      input.lod(),
                                                      input.offset());
  VLOG(4) << "Builder construction  meta_input";
  paddle::dialect::IrMetaTensor meta_input(&ir_tensor_input);

  VLOG(4) << "Builder construction  dense_label";
  paddle::dialect::IrTensor ir_tensor_label(paddle::dialect::TransToPhiDataType(label.dtype()),
                                                      label.dims(),
                                                      label.data_layout(),
                                                      label.lod(),
                                                      label.offset());
  VLOG(4) << "Builder construction  meta_label";
  paddle::dialect::IrMetaTensor meta_label(&ir_tensor_label);

  VLOG(4) << "Builder construction  dense_input_lengths";
  paddle::dialect::IrTensor ir_tensor_input_lengths(paddle::dialect::TransToPhiDataType(input_lengths.dtype()),
                                                      input_lengths.dims(),
                                                      input_lengths.data_layout(),
                                                      input_lengths.lod(),
                                                      input_lengths.offset());
  VLOG(4) << "Builder construction  meta_input_lengths";
  paddle::dialect::IrMetaTensor meta_input_lengths(&ir_tensor_input_lengths);

  VLOG(4) << "Builder construction  dense_label_lengths";
  paddle::dialect::IrTensor ir_tensor_label_lengths(paddle::dialect::TransToPhiDataType(label_lengths.dtype()),
                                                      label_lengths.dims(),
                                                      label_lengths.data_layout(),
                                                      label_lengths.lod(),
                                                      label_lengths.offset());
  VLOG(4) << "Builder construction  meta_label_lengths";
  paddle::dialect::IrMetaTensor meta_label_lengths(&ir_tensor_label_lengths);
  paddle::dialect::IrTensor dense_loss;
  paddle::dialect::IrMetaTensor meta_loss(&dense_loss);
  paddle::dialect::IrTensor dense_warprnntgrad;
  paddle::dialect::IrMetaTensor meta_warprnntgrad(&dense_warprnntgrad);

  phi::WarprnntInferMeta(meta_input, meta_label, meta_input_lengths, meta_label_lengths, blank, fastemit_lambda, &meta_loss, &meta_warprnntgrad);

  std::vector<pir::Type> argument_outputs;
  pir::Type loss_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_loss.dtype()), dense_loss.dims(), dense_loss.layout(), dense_loss.lod(), dense_loss.offset());
  argument_outputs.push_back(loss_dense_tensor_type);

  pir::Type warprnntgrad_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_warprnntgrad.dtype()), dense_warprnntgrad.dims(), dense_warprnntgrad.layout(), dense_warprnntgrad.lod(), dense_warprnntgrad.offset());
  argument_outputs.push_back(warprnntgrad_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void WarprnntOp::VerifySig() {
  VLOG(4) << "Start Verifying inputs, outputs and attributes for: WarprnntOp.";
  VLOG(4) << "Verifying inputs:";
  {
  auto input_size = num_operands();
  IR_ENFORCE(input_size == 4u,
                    "The size %d of inputs must be equal to 4.", input_size);
  IR_ENFORCE((*this)->operand_source(0).type().isa<paddle::dialect::DenseTensorType>(),
                  "Type validation failed for the 0th input, got %s.", (*this)->operand_source(0).type());
  IR_ENFORCE((*this)->operand_source(1).type().isa<paddle::dialect::DenseTensorType>(),
                  "Type validation failed for the 1th input, got %s.", (*this)->operand_source(1).type());
  IR_ENFORCE((*this)->operand_source(2).type().isa<paddle::dialect::DenseTensorType>(),
                  "Type validation failed for the 2th input, got %s.", (*this)->operand_source(2).type());
  IR_ENFORCE((*this)->operand_source(3).type().isa<paddle::dialect::DenseTensorType>(),
                  "Type validation failed for the 3th input, got %s.", (*this)->operand_source(3).type());
  }
  VLOG(4) << "Verifying attributes:";
  {
  auto& attributes = this->attributes();
  IR_ENFORCE(attributes.count("blank")>0,
                 "blank does not exist.");
  IR_ENFORCE(attributes.at("blank").isa<pir::Int32Attribute>(),
                 "Type of attribute: blank is not pir::Int32Attribute.");

  IR_ENFORCE(attributes.count("fastemit_lambda")>0,
                 "fastemit_lambda does not exist.");
  IR_ENFORCE(attributes.at("fastemit_lambda").isa<pir::FloatAttribute>(),
                 "Type of attribute: fastemit_lambda is not pir::FloatAttribute.");

  }
  VLOG(4) << "Verifying outputs:";
  {
  auto output_size = num_results();
  IR_ENFORCE(output_size == 2u,
                    "The size %d of outputs must be equal to 2.", output_size);
  IR_ENFORCE((*this)->result(0).type().isa<paddle::dialect::DenseTensorType>(),
                 "Type validation failed for the 0th output.");
  if (auto output_1_type = (*this)->result(1).type()) {
    IR_ENFORCE(output_1_type.isa<paddle::dialect::DenseTensorType>(),
                   "Type validation failed for the 1th output.");
  }
  }
  VLOG(4) << "End Verifying for: WarprnntOp.";
}

void WarprnntOp::InferMeta( phi::InferMetaContext *infer_meta ) {
  auto fn = PD_INFER_META(phi::WarprnntInferMeta);
  fn(infer_meta);
}

phi::DataType WarprnntOp::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: WarprnntOp";
  


  return expected_kernel_dtype;
}

const char *WeightDequantizeOp::attributes_name[3] = { "algo", "out_dtype", "group_size" };

OpInfoTuple WeightDequantizeOp::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("x", "paddle::dialect::DenseTensorType", false, false, false, false), paddle::dialect::OpInputInfo("scale", "paddle::dialect::DenseTensorType", false, false, false, false) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = { paddle::dialect::OpAttributeInfo("algo", "pir::StrAttribute", ""), paddle::dialect::OpAttributeInfo("out_dtype", "paddle::dialect::DataTypeAttribute", ""), paddle::dialect::OpAttributeInfo("group_size", "pir::Int32Attribute", "") };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("out", "paddle::dialect::DenseTensorType", false, false) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("WeightDequantizeInferMeta", {"x", "scale", "algo", "out_dtype", "group_size"}, "weight_dequantize", {"x", "scale", "algo", "out_dtype", "group_size"}, {"out_dtype"}, {}, {}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "weight_dequantize");
}

void WeightDequantizeOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, pir::Value scale_, const std::string& algo, phi::DataType out_dtype, int group_size) {
  VLOG(4) << "Start build WeightDequantizeOp";



  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_, scale_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_algo = pir::StrAttribute::get(pir::IrContext::Instance(), algo);
  argument.AddAttribute("algo", attr_algo);
  pir::Attribute attr_out_dtype = paddle::dialect::DataTypeAttribute::get(pir::IrContext::Instance(), out_dtype);
  argument.AddAttribute("out_dtype", attr_out_dtype);
  pir::Attribute attr_group_size = pir::Int32Attribute::get(pir::IrContext::Instance(), group_size);
  argument.AddAttribute("group_size", attr_group_size);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;
  paddle::dialect::DenseTensorType scale = scale_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)scale;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);

  VLOG(4) << "Builder construction  dense_scale";
  paddle::dialect::IrTensor ir_tensor_scale(paddle::dialect::TransToPhiDataType(scale.dtype()),
                                                      scale.dims(),
                                                      scale.data_layout(),
                                                      scale.lod(),
                                                      scale.offset());
  VLOG(4) << "Builder construction  meta_scale";
  paddle::dialect::IrMetaTensor meta_scale(&ir_tensor_scale);
  paddle::dialect::IrTensor dense_out;
  paddle::dialect::IrMetaTensor meta_out(&dense_out);

  phi::WeightDequantizeInferMeta(meta_x, meta_scale, algo, out_dtype, group_size, &meta_out);

  std::vector<pir::Type> argument_outputs;
  pir::Type out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_out.dtype()), dense_out.dims(), dense_out.layout(), dense_out.lod(), dense_out.offset());
  argument_outputs.push_back(out_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void WeightDequantizeOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, pir::Value scale_, pir::AttributeMap attributes) {
  VLOG(4) << "Start build WeightDequantizeOp";


  IR_ENFORCE(
      attributes.find("algo") != attributes.end(),
          "'algo' Attribute is expected for WeightDequantizeOp. ");
  std::string algo = attributes.at("algo").dyn_cast<pir::StrAttribute>().AsString();

  IR_ENFORCE(
      attributes.find("out_dtype") != attributes.end(),
          "'out_dtype' Attribute is expected for WeightDequantizeOp. ");
  phi::DataType out_dtype = attributes.at("out_dtype").dyn_cast<paddle::dialect::DataTypeAttribute>().data();

  IR_ENFORCE(
      attributes.find("group_size") != attributes.end(),
          "'group_size' Attribute is expected for WeightDequantizeOp. ");
  int group_size = attributes.at("group_size").dyn_cast<pir::Int32Attribute>().data();


  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_, scale_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_algo = pir::StrAttribute::get(pir::IrContext::Instance(), algo);
  argument.AddAttribute("algo", attr_algo);
  pir::Attribute attr_out_dtype = paddle::dialect::DataTypeAttribute::get(pir::IrContext::Instance(), out_dtype);
  argument.AddAttribute("out_dtype", attr_out_dtype);
  pir::Attribute attr_group_size = pir::Int32Attribute::get(pir::IrContext::Instance(), group_size);
  argument.AddAttribute("group_size", attr_group_size);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;
  paddle::dialect::DenseTensorType scale = scale_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)scale;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);

  VLOG(4) << "Builder construction  dense_scale";
  paddle::dialect::IrTensor ir_tensor_scale(paddle::dialect::TransToPhiDataType(scale.dtype()),
                                                      scale.dims(),
                                                      scale.data_layout(),
                                                      scale.lod(),
                                                      scale.offset());
  VLOG(4) << "Builder construction  meta_scale";
  paddle::dialect::IrMetaTensor meta_scale(&ir_tensor_scale);
  paddle::dialect::IrTensor dense_out;
  paddle::dialect::IrMetaTensor meta_out(&dense_out);

  phi::WeightDequantizeInferMeta(meta_x, meta_scale, algo, out_dtype, group_size, &meta_out);

  std::vector<pir::Type> argument_outputs;
  pir::Type out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_out.dtype()), dense_out.dims(), dense_out.layout(), dense_out.lod(), dense_out.offset());
  argument_outputs.push_back(out_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void WeightDequantizeOp::VerifySig() {
  VLOG(4) << "Start Verifying inputs, outputs and attributes for: WeightDequantizeOp.";
  VLOG(4) << "Verifying inputs:";
  {
  auto input_size = num_operands();
  IR_ENFORCE(input_size == 2u,
                    "The size %d of inputs must be equal to 2.", input_size);
  IR_ENFORCE((*this)->operand_source(0).type().isa<paddle::dialect::DenseTensorType>(),
                  "Type validation failed for the 0th input, got %s.", (*this)->operand_source(0).type());
  IR_ENFORCE((*this)->operand_source(1).type().isa<paddle::dialect::DenseTensorType>(),
                  "Type validation failed for the 1th input, got %s.", (*this)->operand_source(1).type());
  }
  VLOG(4) << "Verifying attributes:";
  {
  auto& attributes = this->attributes();
  IR_ENFORCE(attributes.count("algo")>0,
                 "algo does not exist.");
  IR_ENFORCE(attributes.at("algo").isa<pir::StrAttribute>(),
                 "Type of attribute: algo is not pir::StrAttribute.");

  IR_ENFORCE(attributes.count("out_dtype")>0,
                 "out_dtype does not exist.");
  IR_ENFORCE(attributes.at("out_dtype").isa<paddle::dialect::DataTypeAttribute>(),
                 "Type of attribute: out_dtype is not paddle::dialect::DataTypeAttribute.");

  IR_ENFORCE(attributes.count("group_size")>0,
                 "group_size does not exist.");
  IR_ENFORCE(attributes.at("group_size").isa<pir::Int32Attribute>(),
                 "Type of attribute: group_size is not pir::Int32Attribute.");

  }
  VLOG(4) << "Verifying outputs:";
  {
  auto output_size = num_results();
  IR_ENFORCE(output_size == 1u,
                    "The size %d of outputs must be equal to 1.", output_size);
  IR_ENFORCE((*this)->result(0).type().isa<paddle::dialect::DenseTensorType>(),
                 "Type validation failed for the 0th output.");
  }
  VLOG(4) << "End Verifying for: WeightDequantizeOp.";
}

void WeightDequantizeOp::InferMeta( phi::InferMetaContext *infer_meta ) {
  auto fn = PD_INFER_META(phi::WeightDequantizeInferMeta);
  fn(infer_meta);
}

phi::DataType WeightDequantizeOp::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: WeightDequantizeOp";
  


  return expected_kernel_dtype;
}

const char *WeightOnlyLinearOp::attributes_name[3] = { "weight_dtype", "arch", "group_size" };

OpInfoTuple WeightOnlyLinearOp::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("x", "paddle::dialect::DenseTensorType", false, false, false, true), paddle::dialect::OpInputInfo("weight", "paddle::dialect::DenseTensorType", false, false, false, false), paddle::dialect::OpInputInfo("bias", "paddle::dialect::DenseTensorType", true, false, false, false), paddle::dialect::OpInputInfo("weight_scale", "paddle::dialect::DenseTensorType", false, false, false, false) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = { paddle::dialect::OpAttributeInfo("weight_dtype", "pir::StrAttribute", ""), paddle::dialect::OpAttributeInfo("arch", "pir::Int32Attribute", ""), paddle::dialect::OpAttributeInfo("group_size", "pir::Int32Attribute", "") };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("out", "paddle::dialect::DenseTensorType", false, false) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("WeightOnlyLinearInferMeta", {"x", "weight", "bias", "weight_scale", "weight_dtype", "arch", "group_size"}, "weight_only_linear", {"x", "weight", "bias", "weight_scale", "weight_dtype", "arch", "group_size"}, {"x"}, {}, {}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "weight_only_linear");
}

void WeightOnlyLinearOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, pir::Value weight_, pir::Value bias_, pir::Value weight_scale_, const std::string& weight_dtype, int arch, int group_size) {
  VLOG(4) << "Start build WeightOnlyLinearOp";



  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_, weight_, bias_, weight_scale_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_weight_dtype = pir::StrAttribute::get(pir::IrContext::Instance(), weight_dtype);
  argument.AddAttribute("weight_dtype", attr_weight_dtype);
  pir::Attribute attr_arch = pir::Int32Attribute::get(pir::IrContext::Instance(), arch);
  argument.AddAttribute("arch", attr_arch);
  pir::Attribute attr_group_size = pir::Int32Attribute::get(pir::IrContext::Instance(), group_size);
  argument.AddAttribute("group_size", attr_group_size);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;
  paddle::dialect::DenseTensorType weight = weight_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)weight;
  paddle::dialect::DenseTensorType weight_scale = weight_scale_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)weight_scale;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);

  VLOG(4) << "Builder construction  dense_weight";
  paddle::dialect::IrTensor ir_tensor_weight(paddle::dialect::TransToPhiDataType(weight.dtype()),
                                                      weight.dims(),
                                                      weight.data_layout(),
                                                      weight.lod(),
                                                      weight.offset());
  VLOG(4) << "Builder construction  meta_weight";
  paddle::dialect::IrMetaTensor meta_weight(&ir_tensor_weight);

  paddle::dialect::IrMetaTensor meta_bias;
  paddle::dialect::IrTensor ir_tensor_bias;
  if (bias_.impl() != nullptr) {
    paddle::dialect::DenseTensorType bias = bias_.type().dyn_cast<paddle::dialect::DenseTensorType>();
    VLOG(4) << "Builder construction  dense_bias";
    ir_tensor_bias = paddle::dialect::IrTensor(paddle::dialect::TransToPhiDataType(bias.dtype()),
                                                        bias.dims(),
                                                        bias.data_layout(),
                                                        bias.lod(),
                                                        bias.offset());
    VLOG(4) << "Builder construction  meta_bias";
    meta_bias = paddle::dialect::IrMetaTensor(&ir_tensor_bias);
  }


  VLOG(4) << "Builder construction  dense_weight_scale";
  paddle::dialect::IrTensor ir_tensor_weight_scale(paddle::dialect::TransToPhiDataType(weight_scale.dtype()),
                                                      weight_scale.dims(),
                                                      weight_scale.data_layout(),
                                                      weight_scale.lod(),
                                                      weight_scale.offset());
  VLOG(4) << "Builder construction  meta_weight_scale";
  paddle::dialect::IrMetaTensor meta_weight_scale(&ir_tensor_weight_scale);
  paddle::dialect::IrTensor dense_out;
  paddle::dialect::IrMetaTensor meta_out(&dense_out);

  phi::WeightOnlyLinearInferMeta(meta_x, meta_weight, meta_bias, meta_weight_scale, weight_dtype, arch, group_size, &meta_out);

  std::vector<pir::Type> argument_outputs;
  pir::Type out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_out.dtype()), dense_out.dims(), dense_out.layout(), dense_out.lod(), dense_out.offset());
  argument_outputs.push_back(out_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void WeightOnlyLinearOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, pir::Value weight_, pir::Value bias_, pir::Value weight_scale_, pir::AttributeMap attributes) {
  VLOG(4) << "Start build WeightOnlyLinearOp";


  IR_ENFORCE(
      attributes.find("weight_dtype") != attributes.end(),
          "'weight_dtype' Attribute is expected for WeightOnlyLinearOp. ");
  std::string weight_dtype = attributes.at("weight_dtype").dyn_cast<pir::StrAttribute>().AsString();

  IR_ENFORCE(
      attributes.find("arch") != attributes.end(),
          "'arch' Attribute is expected for WeightOnlyLinearOp. ");
  int arch = attributes.at("arch").dyn_cast<pir::Int32Attribute>().data();

  IR_ENFORCE(
      attributes.find("group_size") != attributes.end(),
          "'group_size' Attribute is expected for WeightOnlyLinearOp. ");
  int group_size = attributes.at("group_size").dyn_cast<pir::Int32Attribute>().data();


  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_, weight_, bias_, weight_scale_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_weight_dtype = pir::StrAttribute::get(pir::IrContext::Instance(), weight_dtype);
  argument.AddAttribute("weight_dtype", attr_weight_dtype);
  pir::Attribute attr_arch = pir::Int32Attribute::get(pir::IrContext::Instance(), arch);
  argument.AddAttribute("arch", attr_arch);
  pir::Attribute attr_group_size = pir::Int32Attribute::get(pir::IrContext::Instance(), group_size);
  argument.AddAttribute("group_size", attr_group_size);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;
  paddle::dialect::DenseTensorType weight = weight_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)weight;
  paddle::dialect::DenseTensorType weight_scale = weight_scale_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)weight_scale;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);

  VLOG(4) << "Builder construction  dense_weight";
  paddle::dialect::IrTensor ir_tensor_weight(paddle::dialect::TransToPhiDataType(weight.dtype()),
                                                      weight.dims(),
                                                      weight.data_layout(),
                                                      weight.lod(),
                                                      weight.offset());
  VLOG(4) << "Builder construction  meta_weight";
  paddle::dialect::IrMetaTensor meta_weight(&ir_tensor_weight);

  paddle::dialect::IrMetaTensor meta_bias;
  paddle::dialect::IrTensor ir_tensor_bias;
  if (bias_.impl() != nullptr) {
    paddle::dialect::DenseTensorType bias = bias_.type().dyn_cast<paddle::dialect::DenseTensorType>();
    VLOG(4) << "Builder construction  dense_bias";
    ir_tensor_bias = paddle::dialect::IrTensor(paddle::dialect::TransToPhiDataType(bias.dtype()),
                                                        bias.dims(),
                                                        bias.data_layout(),
                                                        bias.lod(),
                                                        bias.offset());
    VLOG(4) << "Builder construction  meta_bias";
    meta_bias = paddle::dialect::IrMetaTensor(&ir_tensor_bias);
  }


  VLOG(4) << "Builder construction  dense_weight_scale";
  paddle::dialect::IrTensor ir_tensor_weight_scale(paddle::dialect::TransToPhiDataType(weight_scale.dtype()),
                                                      weight_scale.dims(),
                                                      weight_scale.data_layout(),
                                                      weight_scale.lod(),
                                                      weight_scale.offset());
  VLOG(4) << "Builder construction  meta_weight_scale";
  paddle::dialect::IrMetaTensor meta_weight_scale(&ir_tensor_weight_scale);
  paddle::dialect::IrTensor dense_out;
  paddle::dialect::IrMetaTensor meta_out(&dense_out);

  phi::WeightOnlyLinearInferMeta(meta_x, meta_weight, meta_bias, meta_weight_scale, weight_dtype, arch, group_size, &meta_out);

  std::vector<pir::Type> argument_outputs;
  pir::Type out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_out.dtype()), dense_out.dims(), dense_out.layout(), dense_out.lod(), dense_out.offset());
  argument_outputs.push_back(out_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void WeightOnlyLinearOp::VerifySig() {
  VLOG(4) << "Start Verifying inputs, outputs and attributes for: WeightOnlyLinearOp.";
  VLOG(4) << "Verifying inputs:";
  {
  auto input_size = num_operands();
  IR_ENFORCE(input_size == 4u,
                    "The size %d of inputs must be equal to 4.", input_size);
  IR_ENFORCE((*this)->operand_source(0).type().isa<paddle::dialect::DenseTensorType>(),
                  "Type validation failed for the 0th input, got %s.", (*this)->operand_source(0).type());
  IR_ENFORCE((*this)->operand_source(1).type().isa<paddle::dialect::DenseTensorType>(),
                  "Type validation failed for the 1th input, got %s.", (*this)->operand_source(1).type());
  if (auto val = (*this)->operand(2)) {
    IR_ENFORCE(val.type().isa<paddle::dialect::DenseTensorType>(),
                   "Type validation failed for the 2th input, got %s.", (*this)->operand_source(2).type());
  }
  IR_ENFORCE((*this)->operand_source(3).type().isa<paddle::dialect::DenseTensorType>(),
                  "Type validation failed for the 3th input, got %s.", (*this)->operand_source(3).type());
  }
  VLOG(4) << "Verifying attributes:";
  {
  auto& attributes = this->attributes();
  IR_ENFORCE(attributes.count("weight_dtype")>0,
                 "weight_dtype does not exist.");
  IR_ENFORCE(attributes.at("weight_dtype").isa<pir::StrAttribute>(),
                 "Type of attribute: weight_dtype is not pir::StrAttribute.");

  IR_ENFORCE(attributes.count("arch")>0,
                 "arch does not exist.");
  IR_ENFORCE(attributes.at("arch").isa<pir::Int32Attribute>(),
                 "Type of attribute: arch is not pir::Int32Attribute.");

  IR_ENFORCE(attributes.count("group_size")>0,
                 "group_size does not exist.");
  IR_ENFORCE(attributes.at("group_size").isa<pir::Int32Attribute>(),
                 "Type of attribute: group_size is not pir::Int32Attribute.");

  }
  VLOG(4) << "Verifying outputs:";
  {
  auto output_size = num_results();
  IR_ENFORCE(output_size == 1u,
                    "The size %d of outputs must be equal to 1.", output_size);
  IR_ENFORCE((*this)->result(0).type().isa<paddle::dialect::DenseTensorType>(),
                 "Type validation failed for the 0th output.");
  }
  VLOG(4) << "End Verifying for: WeightOnlyLinearOp.";
}

void WeightOnlyLinearOp::InferMeta( phi::InferMetaContext *infer_meta ) {
  auto fn = PD_INFER_META(phi::WeightOnlyLinearInferMeta);
  fn(infer_meta);
}

phi::DataType WeightOnlyLinearOp::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: WeightOnlyLinearOp";
  


  return expected_kernel_dtype;
}

const char *WeightQuantizeOp::attributes_name[3] = { "algo", "arch", "group_size" };

OpInfoTuple WeightQuantizeOp::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("x", "paddle::dialect::DenseTensorType", false, false, false, false) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = { paddle::dialect::OpAttributeInfo("algo", "pir::StrAttribute", ""), paddle::dialect::OpAttributeInfo("arch", "pir::Int32Attribute", ""), paddle::dialect::OpAttributeInfo("group_size", "pir::Int32Attribute", "") };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("out", "paddle::dialect::DenseTensorType", false, false), paddle::dialect::OpOutputInfo("scale", "paddle::dialect::DenseTensorType", false, false) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("WeightQuantizeInferMeta", {"x", "algo", "arch", "group_size"}, "weight_quantize", {"x", "algo", "arch", "group_size"}, {"x"}, {}, {}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "weight_quantize");
}

void WeightQuantizeOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, const std::string& algo, int arch, int group_size) {
  VLOG(4) << "Start build WeightQuantizeOp";



  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_algo = pir::StrAttribute::get(pir::IrContext::Instance(), algo);
  argument.AddAttribute("algo", attr_algo);
  pir::Attribute attr_arch = pir::Int32Attribute::get(pir::IrContext::Instance(), arch);
  argument.AddAttribute("arch", attr_arch);
  pir::Attribute attr_group_size = pir::Int32Attribute::get(pir::IrContext::Instance(), group_size);
  argument.AddAttribute("group_size", attr_group_size);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);
  paddle::dialect::IrTensor dense_out;
  paddle::dialect::IrMetaTensor meta_out(&dense_out);
  paddle::dialect::IrTensor dense_scale;
  paddle::dialect::IrMetaTensor meta_scale(&dense_scale);

  phi::WeightQuantizeInferMeta(meta_x, algo, arch, group_size, &meta_out, &meta_scale);

  std::vector<pir::Type> argument_outputs;
  pir::Type out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_out.dtype()), dense_out.dims(), dense_out.layout(), dense_out.lod(), dense_out.offset());
  argument_outputs.push_back(out_dense_tensor_type);

  pir::Type scale_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_scale.dtype()), dense_scale.dims(), dense_scale.layout(), dense_scale.lod(), dense_scale.offset());
  argument_outputs.push_back(scale_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void WeightQuantizeOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, pir::AttributeMap attributes) {
  VLOG(4) << "Start build WeightQuantizeOp";


  IR_ENFORCE(
      attributes.find("algo") != attributes.end(),
          "'algo' Attribute is expected for WeightQuantizeOp. ");
  std::string algo = attributes.at("algo").dyn_cast<pir::StrAttribute>().AsString();

  IR_ENFORCE(
      attributes.find("arch") != attributes.end(),
          "'arch' Attribute is expected for WeightQuantizeOp. ");
  int arch = attributes.at("arch").dyn_cast<pir::Int32Attribute>().data();

  IR_ENFORCE(
      attributes.find("group_size") != attributes.end(),
          "'group_size' Attribute is expected for WeightQuantizeOp. ");
  int group_size = attributes.at("group_size").dyn_cast<pir::Int32Attribute>().data();


  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_algo = pir::StrAttribute::get(pir::IrContext::Instance(), algo);
  argument.AddAttribute("algo", attr_algo);
  pir::Attribute attr_arch = pir::Int32Attribute::get(pir::IrContext::Instance(), arch);
  argument.AddAttribute("arch", attr_arch);
  pir::Attribute attr_group_size = pir::Int32Attribute::get(pir::IrContext::Instance(), group_size);
  argument.AddAttribute("group_size", attr_group_size);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);
  paddle::dialect::IrTensor dense_out;
  paddle::dialect::IrMetaTensor meta_out(&dense_out);
  paddle::dialect::IrTensor dense_scale;
  paddle::dialect::IrMetaTensor meta_scale(&dense_scale);

  phi::WeightQuantizeInferMeta(meta_x, algo, arch, group_size, &meta_out, &meta_scale);

  std::vector<pir::Type> argument_outputs;
  pir::Type out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_out.dtype()), dense_out.dims(), dense_out.layout(), dense_out.lod(), dense_out.offset());
  argument_outputs.push_back(out_dense_tensor_type);

  pir::Type scale_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_scale.dtype()), dense_scale.dims(), dense_scale.layout(), dense_scale.lod(), dense_scale.offset());
  argument_outputs.push_back(scale_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void WeightQuantizeOp::VerifySig() {
  VLOG(4) << "Start Verifying inputs, outputs and attributes for: WeightQuantizeOp.";
  VLOG(4) << "Verifying inputs:";
  {
  auto input_size = num_operands();
  IR_ENFORCE(input_size == 1u,
                    "The size %d of inputs must be equal to 1.", input_size);
  IR_ENFORCE((*this)->operand_source(0).type().isa<paddle::dialect::DenseTensorType>(),
                  "Type validation failed for the 0th input, got %s.", (*this)->operand_source(0).type());
  }
  VLOG(4) << "Verifying attributes:";
  {
  auto& attributes = this->attributes();
  IR_ENFORCE(attributes.count("algo")>0,
                 "algo does not exist.");
  IR_ENFORCE(attributes.at("algo").isa<pir::StrAttribute>(),
                 "Type of attribute: algo is not pir::StrAttribute.");

  IR_ENFORCE(attributes.count("arch")>0,
                 "arch does not exist.");
  IR_ENFORCE(attributes.at("arch").isa<pir::Int32Attribute>(),
                 "Type of attribute: arch is not pir::Int32Attribute.");

  IR_ENFORCE(attributes.count("group_size")>0,
                 "group_size does not exist.");
  IR_ENFORCE(attributes.at("group_size").isa<pir::Int32Attribute>(),
                 "Type of attribute: group_size is not pir::Int32Attribute.");

  }
  VLOG(4) << "Verifying outputs:";
  {
  auto output_size = num_results();
  IR_ENFORCE(output_size == 2u,
                    "The size %d of outputs must be equal to 2.", output_size);
  IR_ENFORCE((*this)->result(0).type().isa<paddle::dialect::DenseTensorType>(),
                 "Type validation failed for the 0th output.");
  IR_ENFORCE((*this)->result(1).type().isa<paddle::dialect::DenseTensorType>(),
                 "Type validation failed for the 1th output.");
  }
  VLOG(4) << "End Verifying for: WeightQuantizeOp.";
}

void WeightQuantizeOp::InferMeta( phi::InferMetaContext *infer_meta ) {
  auto fn = PD_INFER_META(phi::WeightQuantizeInferMeta);
  fn(infer_meta);
}

phi::DataType WeightQuantizeOp::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: WeightQuantizeOp";
  


  return expected_kernel_dtype;
}

const char *WeightedSampleNeighborsOp::attributes_name[2] = { "sample_size", "return_eids" };

OpInfoTuple WeightedSampleNeighborsOp::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("row", "paddle::dialect::DenseTensorType", false, false, false, false), paddle::dialect::OpInputInfo("colptr", "paddle::dialect::DenseTensorType", false, false, false, false), paddle::dialect::OpInputInfo("edge_weight", "paddle::dialect::DenseTensorType", false, false, false, false), paddle::dialect::OpInputInfo("input_nodes", "paddle::dialect::DenseTensorType", false, false, false, false), paddle::dialect::OpInputInfo("eids", "paddle::dialect::DenseTensorType", true, false, false, false) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = { paddle::dialect::OpAttributeInfo("sample_size", "pir::Int32Attribute", ""), paddle::dialect::OpAttributeInfo("return_eids", "pir::BoolAttribute", "") };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("out_neighbors", "paddle::dialect::DenseTensorType", false, false), paddle::dialect::OpOutputInfo("out_count", "paddle::dialect::DenseTensorType", false, false), paddle::dialect::OpOutputInfo("out_eids", "paddle::dialect::DenseTensorType", false, false) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("WeightedSampleNeighborsInferMeta", {"row", "colptr", "edge_weight", "input_nodes", "eids", "sample_size", "return_eids"}, "weighted_sample_neighbors", {"row", "colptr", "edge_weight", "input_nodes", "eids", "sample_size", "return_eids"}, {}, {}, {}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "weighted_sample_neighbors");
}

void WeightedSampleNeighborsOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value row_, pir::Value colptr_, pir::Value edge_weight_, pir::Value input_nodes_, pir::Value eids_, int sample_size, bool return_eids) {
  VLOG(4) << "Start build WeightedSampleNeighborsOp";



  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {row_, colptr_, edge_weight_, input_nodes_, eids_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_sample_size = pir::Int32Attribute::get(pir::IrContext::Instance(), sample_size);
  argument.AddAttribute("sample_size", attr_sample_size);
  pir::Attribute attr_return_eids = pir::BoolAttribute::get(pir::IrContext::Instance(), return_eids);
  argument.AddAttribute("return_eids", attr_return_eids);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType row = row_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)row;
  paddle::dialect::DenseTensorType colptr = colptr_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)colptr;
  paddle::dialect::DenseTensorType edge_weight = edge_weight_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)edge_weight;
  paddle::dialect::DenseTensorType input_nodes = input_nodes_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)input_nodes;

  VLOG(4) << "Builder construction  dense_row";
  paddle::dialect::IrTensor ir_tensor_row(paddle::dialect::TransToPhiDataType(row.dtype()),
                                                      row.dims(),
                                                      row.data_layout(),
                                                      row.lod(),
                                                      row.offset());
  VLOG(4) << "Builder construction  meta_row";
  paddle::dialect::IrMetaTensor meta_row(&ir_tensor_row);

  VLOG(4) << "Builder construction  dense_colptr";
  paddle::dialect::IrTensor ir_tensor_colptr(paddle::dialect::TransToPhiDataType(colptr.dtype()),
                                                      colptr.dims(),
                                                      colptr.data_layout(),
                                                      colptr.lod(),
                                                      colptr.offset());
  VLOG(4) << "Builder construction  meta_colptr";
  paddle::dialect::IrMetaTensor meta_colptr(&ir_tensor_colptr);

  VLOG(4) << "Builder construction  dense_edge_weight";
  paddle::dialect::IrTensor ir_tensor_edge_weight(paddle::dialect::TransToPhiDataType(edge_weight.dtype()),
                                                      edge_weight.dims(),
                                                      edge_weight.data_layout(),
                                                      edge_weight.lod(),
                                                      edge_weight.offset());
  VLOG(4) << "Builder construction  meta_edge_weight";
  paddle::dialect::IrMetaTensor meta_edge_weight(&ir_tensor_edge_weight);

  VLOG(4) << "Builder construction  dense_input_nodes";
  paddle::dialect::IrTensor ir_tensor_input_nodes(paddle::dialect::TransToPhiDataType(input_nodes.dtype()),
                                                      input_nodes.dims(),
                                                      input_nodes.data_layout(),
                                                      input_nodes.lod(),
                                                      input_nodes.offset());
  VLOG(4) << "Builder construction  meta_input_nodes";
  paddle::dialect::IrMetaTensor meta_input_nodes(&ir_tensor_input_nodes);

  paddle::dialect::IrMetaTensor meta_eids;
  paddle::dialect::IrTensor ir_tensor_eids;
  if (eids_.impl() != nullptr) {
    paddle::dialect::DenseTensorType eids = eids_.type().dyn_cast<paddle::dialect::DenseTensorType>();
    VLOG(4) << "Builder construction  dense_eids";
    ir_tensor_eids = paddle::dialect::IrTensor(paddle::dialect::TransToPhiDataType(eids.dtype()),
                                                        eids.dims(),
                                                        eids.data_layout(),
                                                        eids.lod(),
                                                        eids.offset());
    VLOG(4) << "Builder construction  meta_eids";
    meta_eids = paddle::dialect::IrMetaTensor(&ir_tensor_eids);
  }

  paddle::dialect::IrTensor dense_out_neighbors;
  paddle::dialect::IrMetaTensor meta_out_neighbors(&dense_out_neighbors);
  paddle::dialect::IrTensor dense_out_count;
  paddle::dialect::IrMetaTensor meta_out_count(&dense_out_count);
  paddle::dialect::IrTensor dense_out_eids;
  paddle::dialect::IrMetaTensor meta_out_eids(&dense_out_eids);

  phi::WeightedSampleNeighborsInferMeta(meta_row, meta_colptr, meta_edge_weight, meta_input_nodes, meta_eids, sample_size, return_eids, &meta_out_neighbors, &meta_out_count, &meta_out_eids);

  std::vector<pir::Type> argument_outputs;
  pir::Type out_neighbors_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_out_neighbors.dtype()), dense_out_neighbors.dims(), dense_out_neighbors.layout(), dense_out_neighbors.lod(), dense_out_neighbors.offset());
  argument_outputs.push_back(out_neighbors_dense_tensor_type);

  pir::Type out_count_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_out_count.dtype()), dense_out_count.dims(), dense_out_count.layout(), dense_out_count.lod(), dense_out_count.offset());
  argument_outputs.push_back(out_count_dense_tensor_type);

  pir::Type out_eids_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_out_eids.dtype()), dense_out_eids.dims(), dense_out_eids.layout(), dense_out_eids.lod(), dense_out_eids.offset());
  argument_outputs.push_back(out_eids_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void WeightedSampleNeighborsOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value row_, pir::Value colptr_, pir::Value edge_weight_, pir::Value input_nodes_, pir::Value eids_, pir::AttributeMap attributes) {
  VLOG(4) << "Start build WeightedSampleNeighborsOp";


  IR_ENFORCE(
      attributes.find("sample_size") != attributes.end(),
          "'sample_size' Attribute is expected for WeightedSampleNeighborsOp. ");
  int sample_size = attributes.at("sample_size").dyn_cast<pir::Int32Attribute>().data();

  IR_ENFORCE(
      attributes.find("return_eids") != attributes.end(),
          "'return_eids' Attribute is expected for WeightedSampleNeighborsOp. ");
  bool return_eids = attributes.at("return_eids").dyn_cast<pir::BoolAttribute>().data();


  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {row_, colptr_, edge_weight_, input_nodes_, eids_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_sample_size = pir::Int32Attribute::get(pir::IrContext::Instance(), sample_size);
  argument.AddAttribute("sample_size", attr_sample_size);
  pir::Attribute attr_return_eids = pir::BoolAttribute::get(pir::IrContext::Instance(), return_eids);
  argument.AddAttribute("return_eids", attr_return_eids);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType row = row_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)row;
  paddle::dialect::DenseTensorType colptr = colptr_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)colptr;
  paddle::dialect::DenseTensorType edge_weight = edge_weight_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)edge_weight;
  paddle::dialect::DenseTensorType input_nodes = input_nodes_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)input_nodes;

  VLOG(4) << "Builder construction  dense_row";
  paddle::dialect::IrTensor ir_tensor_row(paddle::dialect::TransToPhiDataType(row.dtype()),
                                                      row.dims(),
                                                      row.data_layout(),
                                                      row.lod(),
                                                      row.offset());
  VLOG(4) << "Builder construction  meta_row";
  paddle::dialect::IrMetaTensor meta_row(&ir_tensor_row);

  VLOG(4) << "Builder construction  dense_colptr";
  paddle::dialect::IrTensor ir_tensor_colptr(paddle::dialect::TransToPhiDataType(colptr.dtype()),
                                                      colptr.dims(),
                                                      colptr.data_layout(),
                                                      colptr.lod(),
                                                      colptr.offset());
  VLOG(4) << "Builder construction  meta_colptr";
  paddle::dialect::IrMetaTensor meta_colptr(&ir_tensor_colptr);

  VLOG(4) << "Builder construction  dense_edge_weight";
  paddle::dialect::IrTensor ir_tensor_edge_weight(paddle::dialect::TransToPhiDataType(edge_weight.dtype()),
                                                      edge_weight.dims(),
                                                      edge_weight.data_layout(),
                                                      edge_weight.lod(),
                                                      edge_weight.offset());
  VLOG(4) << "Builder construction  meta_edge_weight";
  paddle::dialect::IrMetaTensor meta_edge_weight(&ir_tensor_edge_weight);

  VLOG(4) << "Builder construction  dense_input_nodes";
  paddle::dialect::IrTensor ir_tensor_input_nodes(paddle::dialect::TransToPhiDataType(input_nodes.dtype()),
                                                      input_nodes.dims(),
                                                      input_nodes.data_layout(),
                                                      input_nodes.lod(),
                                                      input_nodes.offset());
  VLOG(4) << "Builder construction  meta_input_nodes";
  paddle::dialect::IrMetaTensor meta_input_nodes(&ir_tensor_input_nodes);

  paddle::dialect::IrMetaTensor meta_eids;
  paddle::dialect::IrTensor ir_tensor_eids;
  if (eids_.impl() != nullptr) {
    paddle::dialect::DenseTensorType eids = eids_.type().dyn_cast<paddle::dialect::DenseTensorType>();
    VLOG(4) << "Builder construction  dense_eids";
    ir_tensor_eids = paddle::dialect::IrTensor(paddle::dialect::TransToPhiDataType(eids.dtype()),
                                                        eids.dims(),
                                                        eids.data_layout(),
                                                        eids.lod(),
                                                        eids.offset());
    VLOG(4) << "Builder construction  meta_eids";
    meta_eids = paddle::dialect::IrMetaTensor(&ir_tensor_eids);
  }

  paddle::dialect::IrTensor dense_out_neighbors;
  paddle::dialect::IrMetaTensor meta_out_neighbors(&dense_out_neighbors);
  paddle::dialect::IrTensor dense_out_count;
  paddle::dialect::IrMetaTensor meta_out_count(&dense_out_count);
  paddle::dialect::IrTensor dense_out_eids;
  paddle::dialect::IrMetaTensor meta_out_eids(&dense_out_eids);

  phi::WeightedSampleNeighborsInferMeta(meta_row, meta_colptr, meta_edge_weight, meta_input_nodes, meta_eids, sample_size, return_eids, &meta_out_neighbors, &meta_out_count, &meta_out_eids);

  std::vector<pir::Type> argument_outputs;
  pir::Type out_neighbors_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_out_neighbors.dtype()), dense_out_neighbors.dims(), dense_out_neighbors.layout(), dense_out_neighbors.lod(), dense_out_neighbors.offset());
  argument_outputs.push_back(out_neighbors_dense_tensor_type);

  pir::Type out_count_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_out_count.dtype()), dense_out_count.dims(), dense_out_count.layout(), dense_out_count.lod(), dense_out_count.offset());
  argument_outputs.push_back(out_count_dense_tensor_type);

  pir::Type out_eids_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_out_eids.dtype()), dense_out_eids.dims(), dense_out_eids.layout(), dense_out_eids.lod(), dense_out_eids.offset());
  argument_outputs.push_back(out_eids_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void WeightedSampleNeighborsOp::VerifySig() {
  VLOG(4) << "Start Verifying inputs, outputs and attributes for: WeightedSampleNeighborsOp.";
  VLOG(4) << "Verifying inputs:";
  {
  auto input_size = num_operands();
  IR_ENFORCE(input_size == 5u,
                    "The size %d of inputs must be equal to 5.", input_size);
  IR_ENFORCE((*this)->operand_source(0).type().isa<paddle::dialect::DenseTensorType>(),
                  "Type validation failed for the 0th input, got %s.", (*this)->operand_source(0).type());
  IR_ENFORCE((*this)->operand_source(1).type().isa<paddle::dialect::DenseTensorType>(),
                  "Type validation failed for the 1th input, got %s.", (*this)->operand_source(1).type());
  IR_ENFORCE((*this)->operand_source(2).type().isa<paddle::dialect::DenseTensorType>(),
                  "Type validation failed for the 2th input, got %s.", (*this)->operand_source(2).type());
  IR_ENFORCE((*this)->operand_source(3).type().isa<paddle::dialect::DenseTensorType>(),
                  "Type validation failed for the 3th input, got %s.", (*this)->operand_source(3).type());
  if (auto val = (*this)->operand(4)) {
    IR_ENFORCE(val.type().isa<paddle::dialect::DenseTensorType>(),
                   "Type validation failed for the 4th input, got %s.", (*this)->operand_source(4).type());
  }
  }
  VLOG(4) << "Verifying attributes:";
  {
  auto& attributes = this->attributes();
  IR_ENFORCE(attributes.count("sample_size")>0,
                 "sample_size does not exist.");
  IR_ENFORCE(attributes.at("sample_size").isa<pir::Int32Attribute>(),
                 "Type of attribute: sample_size is not pir::Int32Attribute.");

  IR_ENFORCE(attributes.count("return_eids")>0,
                 "return_eids does not exist.");
  IR_ENFORCE(attributes.at("return_eids").isa<pir::BoolAttribute>(),
                 "Type of attribute: return_eids is not pir::BoolAttribute.");

  }
  VLOG(4) << "Verifying outputs:";
  {
  auto output_size = num_results();
  IR_ENFORCE(output_size == 3u,
                    "The size %d of outputs must be equal to 3.", output_size);
  IR_ENFORCE((*this)->result(0).type().isa<paddle::dialect::DenseTensorType>(),
                 "Type validation failed for the 0th output.");
  IR_ENFORCE((*this)->result(1).type().isa<paddle::dialect::DenseTensorType>(),
                 "Type validation failed for the 1th output.");
  IR_ENFORCE((*this)->result(2).type().isa<paddle::dialect::DenseTensorType>(),
                 "Type validation failed for the 2th output.");
  }
  VLOG(4) << "End Verifying for: WeightedSampleNeighborsOp.";
}

void WeightedSampleNeighborsOp::InferMeta( phi::InferMetaContext *infer_meta ) {
  auto fn = PD_INFER_META(phi::WeightedSampleNeighborsInferMeta);
  fn(infer_meta);
}

phi::DataType WeightedSampleNeighborsOp::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: WeightedSampleNeighborsOp";
  


  return expected_kernel_dtype;
}

OpInfoTuple WhereOp::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("condition", "paddle::dialect::DenseTensorType", false, false, false, false), paddle::dialect::OpInputInfo("x", "paddle::dialect::DenseTensorType", false, false, false, true), paddle::dialect::OpInputInfo("y", "paddle::dialect::DenseTensorType", false, false, false, true) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = {  };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("out", "paddle::dialect::DenseTensorType", false, false) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("WhereInferMeta", {"condition", "x", "y"}, "where", {"condition", "x", "y"}, {}, {}, {}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "where");
}

void WhereOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value condition_, pir::Value x_, pir::Value y_) {
  VLOG(4) << "Start build WhereOp";



  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {condition_, x_, y_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType condition = condition_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)condition;
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;
  paddle::dialect::DenseTensorType y = y_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)y;

  VLOG(4) << "Builder construction  dense_condition";
  paddle::dialect::IrTensor ir_tensor_condition(paddle::dialect::TransToPhiDataType(condition.dtype()),
                                                      condition.dims(),
                                                      condition.data_layout(),
                                                      condition.lod(),
                                                      condition.offset());
  VLOG(4) << "Builder construction  meta_condition";
  paddle::dialect::IrMetaTensor meta_condition(&ir_tensor_condition);

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);

  VLOG(4) << "Builder construction  dense_y";
  paddle::dialect::IrTensor ir_tensor_y(paddle::dialect::TransToPhiDataType(y.dtype()),
                                                      y.dims(),
                                                      y.data_layout(),
                                                      y.lod(),
                                                      y.offset());
  VLOG(4) << "Builder construction  meta_y";
  paddle::dialect::IrMetaTensor meta_y(&ir_tensor_y);
  paddle::dialect::IrTensor dense_out;
  paddle::dialect::IrMetaTensor meta_out(&dense_out);

  phi::WhereInferMeta(meta_condition, meta_x, meta_y, &meta_out);

  std::vector<pir::Type> argument_outputs;
  pir::Type out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_out.dtype()), dense_out.dims(), dense_out.layout(), dense_out.lod(), dense_out.offset());
  argument_outputs.push_back(out_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void WhereOp::VerifySig() {
  VLOG(4) << "Start Verifying inputs, outputs and attributes for: WhereOp.";
  VLOG(4) << "Verifying inputs:";
  {
  auto input_size = num_operands();
  IR_ENFORCE(input_size == 3u,
                    "The size %d of inputs must be equal to 3.", input_size);
  IR_ENFORCE((*this)->operand_source(0).type().isa<paddle::dialect::DenseTensorType>(),
                  "Type validation failed for the 0th input, got %s.", (*this)->operand_source(0).type());
  IR_ENFORCE((*this)->operand_source(1).type().isa<paddle::dialect::DenseTensorType>(),
                  "Type validation failed for the 1th input, got %s.", (*this)->operand_source(1).type());
  IR_ENFORCE((*this)->operand_source(2).type().isa<paddle::dialect::DenseTensorType>(),
                  "Type validation failed for the 2th input, got %s.", (*this)->operand_source(2).type());
  }
  VLOG(4) << "Verifying attributes:";
  {
  // Attributes num is 0, not need to check attributes type.
  }
  VLOG(4) << "Verifying outputs:";
  {
  auto output_size = num_results();
  IR_ENFORCE(output_size == 1u,
                    "The size %d of outputs must be equal to 1.", output_size);
  IR_ENFORCE((*this)->result(0).type().isa<paddle::dialect::DenseTensorType>(),
                 "Type validation failed for the 0th output.");
  }
  VLOG(4) << "End Verifying for: WhereOp.";
}

void WhereOp::InferMeta( phi::InferMetaContext *infer_meta ) {
  auto fn = PD_INFER_META(phi::WhereInferMeta);
  fn(infer_meta);
}

phi::DataType WhereOp::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: WhereOp";
  


  return expected_kernel_dtype;
}

OpInfoTuple Where_Op::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("condition", "paddle::dialect::DenseTensorType", false, false, false, false), paddle::dialect::OpInputInfo("x", "paddle::dialect::DenseTensorType", false, false, false, true), paddle::dialect::OpInputInfo("y", "paddle::dialect::DenseTensorType", false, false, false, true) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = {  };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("out", "paddle::dialect::DenseTensorType", false, false) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("WhereInferMeta", {"condition", "x", "y"}, "where", {"condition", "x", "y"}, {}, {}, {{"out", "x"}}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "where");
}

void Where_Op::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value condition_, pir::Value x_, pir::Value y_) {
  VLOG(4) << "Start build Where_Op";



  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {condition_, x_, y_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType condition = condition_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)condition;
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;
  paddle::dialect::DenseTensorType y = y_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)y;

  VLOG(4) << "Builder construction  dense_condition";
  paddle::dialect::IrTensor ir_tensor_condition(paddle::dialect::TransToPhiDataType(condition.dtype()),
                                                      condition.dims(),
                                                      condition.data_layout(),
                                                      condition.lod(),
                                                      condition.offset());
  VLOG(4) << "Builder construction  meta_condition";
  paddle::dialect::IrMetaTensor meta_condition(&ir_tensor_condition);

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);

  VLOG(4) << "Builder construction  dense_y";
  paddle::dialect::IrTensor ir_tensor_y(paddle::dialect::TransToPhiDataType(y.dtype()),
                                                      y.dims(),
                                                      y.data_layout(),
                                                      y.lod(),
                                                      y.offset());
  VLOG(4) << "Builder construction  meta_y";
  paddle::dialect::IrMetaTensor meta_y(&ir_tensor_y);
  paddle::dialect::IrTensor dense_out;
  paddle::dialect::IrMetaTensor meta_out(&dense_out);

  phi::WhereInferMeta(meta_condition, meta_x, meta_y, &meta_out);

  std::vector<pir::Type> argument_outputs;
  pir::Type out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_out.dtype()), dense_out.dims(), dense_out.layout(), dense_out.lod(), dense_out.offset());
  argument_outputs.push_back(out_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void Where_Op::VerifySig() {
  VLOG(4) << "Start Verifying inputs, outputs and attributes for: Where_Op.";
  VLOG(4) << "Verifying inputs:";
  {
  auto input_size = num_operands();
  IR_ENFORCE(input_size == 3u,
                    "The size %d of inputs must be equal to 3.", input_size);
  IR_ENFORCE((*this)->operand_source(0).type().isa<paddle::dialect::DenseTensorType>(),
                  "Type validation failed for the 0th input, got %s.", (*this)->operand_source(0).type());
  IR_ENFORCE((*this)->operand_source(1).type().isa<paddle::dialect::DenseTensorType>(),
                  "Type validation failed for the 1th input, got %s.", (*this)->operand_source(1).type());
  IR_ENFORCE((*this)->operand_source(2).type().isa<paddle::dialect::DenseTensorType>(),
                  "Type validation failed for the 2th input, got %s.", (*this)->operand_source(2).type());
  }
  VLOG(4) << "Verifying attributes:";
  {
  // Attributes num is 0, not need to check attributes type.
  }
  VLOG(4) << "Verifying outputs:";
  {
  auto output_size = num_results();
  IR_ENFORCE(output_size == 1u,
                    "The size %d of outputs must be equal to 1.", output_size);
  IR_ENFORCE((*this)->result(0).type().isa<paddle::dialect::DenseTensorType>(),
                 "Type validation failed for the 0th output.");
  }
  VLOG(4) << "End Verifying for: Where_Op.";
}

void Where_Op::InferMeta( phi::InferMetaContext *infer_meta ) {
  auto fn = PD_INFER_META(phi::WhereInferMeta);
  fn(infer_meta);
}

phi::DataType Where_Op::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: Where_Op";
  


  return expected_kernel_dtype;
}

const char *YoloBoxOp::attributes_name[8] = { "anchors", "class_num", "conf_thresh", "downsample_ratio", "clip_bbox", "scale_x_y", "iou_aware", "iou_aware_factor" };

OpInfoTuple YoloBoxOp::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("x", "paddle::dialect::DenseTensorType", false, false, false, false), paddle::dialect::OpInputInfo("img_size", "paddle::dialect::DenseTensorType", false, false, false, false) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = { paddle::dialect::OpAttributeInfo("anchors", "pir::ArrayAttribute<pir::Int32Attribute>", ""), paddle::dialect::OpAttributeInfo("class_num", "pir::Int32Attribute", ""), paddle::dialect::OpAttributeInfo("conf_thresh", "pir::FloatAttribute", ""), paddle::dialect::OpAttributeInfo("downsample_ratio", "pir::Int32Attribute", ""), paddle::dialect::OpAttributeInfo("clip_bbox", "pir::BoolAttribute", ""), paddle::dialect::OpAttributeInfo("scale_x_y", "pir::FloatAttribute", ""), paddle::dialect::OpAttributeInfo("iou_aware", "pir::BoolAttribute", ""), paddle::dialect::OpAttributeInfo("iou_aware_factor", "pir::FloatAttribute", "") };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("boxes", "paddle::dialect::DenseTensorType", false, false), paddle::dialect::OpOutputInfo("scores", "paddle::dialect::DenseTensorType", false, false) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("YoloBoxInferMeta", {"x", "img_size", "anchors", "class_num", "conf_thresh", "downsample_ratio", "clip_bbox", "scale_x_y", "iou_aware", "iou_aware_factor"}, "yolo_box", {"x", "img_size", "anchors", "class_num", "conf_thresh", "downsample_ratio", "clip_bbox", "scale_x_y", "iou_aware", "iou_aware_factor"}, {"x"}, {}, {}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "yolo_box");
}

void YoloBoxOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, pir::Value img_size_, const std::vector<int>& anchors, int class_num, float conf_thresh, int downsample_ratio, bool clip_bbox, float scale_x_y, bool iou_aware, float iou_aware_factor) {
  VLOG(4) << "Start build YoloBoxOp";



  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_, img_size_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  std::vector<pir::Attribute> vec_anchors;
  for (size_t i = 0; i < static_cast<size_t>(anchors.size()); i++) {
      pir::Attribute attr_anchors = pir::Int32Attribute::get(pir::IrContext::Instance(), anchors[i]);

    vec_anchors.push_back(attr_anchors);
  }
  pir::Attribute attr_anchors = pir::ArrayAttribute::get(pir::IrContext::Instance(), vec_anchors);
  argument.AddAttribute("anchors", attr_anchors);
  pir::Attribute attr_class_num = pir::Int32Attribute::get(pir::IrContext::Instance(), class_num);
  argument.AddAttribute("class_num", attr_class_num);
  pir::Attribute attr_conf_thresh = pir::FloatAttribute::get(pir::IrContext::Instance(), conf_thresh);
  argument.AddAttribute("conf_thresh", attr_conf_thresh);
  pir::Attribute attr_downsample_ratio = pir::Int32Attribute::get(pir::IrContext::Instance(), downsample_ratio);
  argument.AddAttribute("downsample_ratio", attr_downsample_ratio);
  pir::Attribute attr_clip_bbox = pir::BoolAttribute::get(pir::IrContext::Instance(), clip_bbox);
  argument.AddAttribute("clip_bbox", attr_clip_bbox);
  pir::Attribute attr_scale_x_y = pir::FloatAttribute::get(pir::IrContext::Instance(), scale_x_y);
  argument.AddAttribute("scale_x_y", attr_scale_x_y);
  pir::Attribute attr_iou_aware = pir::BoolAttribute::get(pir::IrContext::Instance(), iou_aware);
  argument.AddAttribute("iou_aware", attr_iou_aware);
  pir::Attribute attr_iou_aware_factor = pir::FloatAttribute::get(pir::IrContext::Instance(), iou_aware_factor);
  argument.AddAttribute("iou_aware_factor", attr_iou_aware_factor);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;
  paddle::dialect::DenseTensorType img_size = img_size_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)img_size;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);

  VLOG(4) << "Builder construction  dense_img_size";
  paddle::dialect::IrTensor ir_tensor_img_size(paddle::dialect::TransToPhiDataType(img_size.dtype()),
                                                      img_size.dims(),
                                                      img_size.data_layout(),
                                                      img_size.lod(),
                                                      img_size.offset());
  VLOG(4) << "Builder construction  meta_img_size";
  paddle::dialect::IrMetaTensor meta_img_size(&ir_tensor_img_size);
  paddle::dialect::IrTensor dense_boxes;
  paddle::dialect::IrMetaTensor meta_boxes(&dense_boxes);
  paddle::dialect::IrTensor dense_scores;
  paddle::dialect::IrMetaTensor meta_scores(&dense_scores);

  phi::YoloBoxInferMeta(meta_x, meta_img_size, anchors, class_num, conf_thresh, downsample_ratio, clip_bbox, scale_x_y, iou_aware, iou_aware_factor, &meta_boxes, &meta_scores);

  std::vector<pir::Type> argument_outputs;
  pir::Type boxes_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_boxes.dtype()), dense_boxes.dims(), dense_boxes.layout(), dense_boxes.lod(), dense_boxes.offset());
  argument_outputs.push_back(boxes_dense_tensor_type);

  pir::Type scores_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_scores.dtype()), dense_scores.dims(), dense_scores.layout(), dense_scores.lod(), dense_scores.offset());
  argument_outputs.push_back(scores_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void YoloBoxOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, pir::Value img_size_, pir::AttributeMap attributes) {
  VLOG(4) << "Start build YoloBoxOp";


  IR_ENFORCE(
      attributes.find("anchors") != attributes.end(),
          "'anchors' Attribute is expected for YoloBoxOp. ");
  std::vector<int> anchors;
  for (size_t i = 0; i < attributes.at("anchors").dyn_cast<pir::ArrayAttribute>().size(); i++) {
    anchors.push_back(attributes.at("anchors").dyn_cast<pir::ArrayAttribute>().at(i).dyn_cast<pir::Int32Attribute>().data());
  }

  IR_ENFORCE(
      attributes.find("class_num") != attributes.end(),
          "'class_num' Attribute is expected for YoloBoxOp. ");
  int class_num = attributes.at("class_num").dyn_cast<pir::Int32Attribute>().data();

  IR_ENFORCE(
      attributes.find("conf_thresh") != attributes.end(),
          "'conf_thresh' Attribute is expected for YoloBoxOp. ");
  float conf_thresh = attributes.at("conf_thresh").dyn_cast<pir::FloatAttribute>().data();

  IR_ENFORCE(
      attributes.find("downsample_ratio") != attributes.end(),
          "'downsample_ratio' Attribute is expected for YoloBoxOp. ");
  int downsample_ratio = attributes.at("downsample_ratio").dyn_cast<pir::Int32Attribute>().data();

  IR_ENFORCE(
      attributes.find("clip_bbox") != attributes.end(),
          "'clip_bbox' Attribute is expected for YoloBoxOp. ");
  bool clip_bbox = attributes.at("clip_bbox").dyn_cast<pir::BoolAttribute>().data();

  IR_ENFORCE(
      attributes.find("scale_x_y") != attributes.end(),
          "'scale_x_y' Attribute is expected for YoloBoxOp. ");
  float scale_x_y = attributes.at("scale_x_y").dyn_cast<pir::FloatAttribute>().data();

  IR_ENFORCE(
      attributes.find("iou_aware") != attributes.end(),
          "'iou_aware' Attribute is expected for YoloBoxOp. ");
  bool iou_aware = attributes.at("iou_aware").dyn_cast<pir::BoolAttribute>().data();

  IR_ENFORCE(
      attributes.find("iou_aware_factor") != attributes.end(),
          "'iou_aware_factor' Attribute is expected for YoloBoxOp. ");
  float iou_aware_factor = attributes.at("iou_aware_factor").dyn_cast<pir::FloatAttribute>().data();


  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_, img_size_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  std::vector<pir::Attribute> vec_anchors;
  for (size_t i = 0; i < static_cast<size_t>(anchors.size()); i++) {
      pir::Attribute attr_anchors = pir::Int32Attribute::get(pir::IrContext::Instance(), anchors[i]);

    vec_anchors.push_back(attr_anchors);
  }
  pir::Attribute attr_anchors = pir::ArrayAttribute::get(pir::IrContext::Instance(), vec_anchors);
  argument.AddAttribute("anchors", attr_anchors);
  pir::Attribute attr_class_num = pir::Int32Attribute::get(pir::IrContext::Instance(), class_num);
  argument.AddAttribute("class_num", attr_class_num);
  pir::Attribute attr_conf_thresh = pir::FloatAttribute::get(pir::IrContext::Instance(), conf_thresh);
  argument.AddAttribute("conf_thresh", attr_conf_thresh);
  pir::Attribute attr_downsample_ratio = pir::Int32Attribute::get(pir::IrContext::Instance(), downsample_ratio);
  argument.AddAttribute("downsample_ratio", attr_downsample_ratio);
  pir::Attribute attr_clip_bbox = pir::BoolAttribute::get(pir::IrContext::Instance(), clip_bbox);
  argument.AddAttribute("clip_bbox", attr_clip_bbox);
  pir::Attribute attr_scale_x_y = pir::FloatAttribute::get(pir::IrContext::Instance(), scale_x_y);
  argument.AddAttribute("scale_x_y", attr_scale_x_y);
  pir::Attribute attr_iou_aware = pir::BoolAttribute::get(pir::IrContext::Instance(), iou_aware);
  argument.AddAttribute("iou_aware", attr_iou_aware);
  pir::Attribute attr_iou_aware_factor = pir::FloatAttribute::get(pir::IrContext::Instance(), iou_aware_factor);
  argument.AddAttribute("iou_aware_factor", attr_iou_aware_factor);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;
  paddle::dialect::DenseTensorType img_size = img_size_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)img_size;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);

  VLOG(4) << "Builder construction  dense_img_size";
  paddle::dialect::IrTensor ir_tensor_img_size(paddle::dialect::TransToPhiDataType(img_size.dtype()),
                                                      img_size.dims(),
                                                      img_size.data_layout(),
                                                      img_size.lod(),
                                                      img_size.offset());
  VLOG(4) << "Builder construction  meta_img_size";
  paddle::dialect::IrMetaTensor meta_img_size(&ir_tensor_img_size);
  paddle::dialect::IrTensor dense_boxes;
  paddle::dialect::IrMetaTensor meta_boxes(&dense_boxes);
  paddle::dialect::IrTensor dense_scores;
  paddle::dialect::IrMetaTensor meta_scores(&dense_scores);

  phi::YoloBoxInferMeta(meta_x, meta_img_size, anchors, class_num, conf_thresh, downsample_ratio, clip_bbox, scale_x_y, iou_aware, iou_aware_factor, &meta_boxes, &meta_scores);

  std::vector<pir::Type> argument_outputs;
  pir::Type boxes_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_boxes.dtype()), dense_boxes.dims(), dense_boxes.layout(), dense_boxes.lod(), dense_boxes.offset());
  argument_outputs.push_back(boxes_dense_tensor_type);

  pir::Type scores_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_scores.dtype()), dense_scores.dims(), dense_scores.layout(), dense_scores.lod(), dense_scores.offset());
  argument_outputs.push_back(scores_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void YoloBoxOp::VerifySig() {
  VLOG(4) << "Start Verifying inputs, outputs and attributes for: YoloBoxOp.";
  VLOG(4) << "Verifying inputs:";
  {
  auto input_size = num_operands();
  IR_ENFORCE(input_size == 2u,
                    "The size %d of inputs must be equal to 2.", input_size);
  IR_ENFORCE((*this)->operand_source(0).type().isa<paddle::dialect::DenseTensorType>(),
                  "Type validation failed for the 0th input, got %s.", (*this)->operand_source(0).type());
  IR_ENFORCE((*this)->operand_source(1).type().isa<paddle::dialect::DenseTensorType>(),
                  "Type validation failed for the 1th input, got %s.", (*this)->operand_source(1).type());
  }
  VLOG(4) << "Verifying attributes:";
  {
  auto& attributes = this->attributes();
  IR_ENFORCE(attributes.count("anchors")>0,
                 "anchors does not exist.");
  IR_ENFORCE(attributes.at("anchors").isa<pir::ArrayAttribute>(),
                 "Type of attribute: anchors is not pir::ArrayAttribute.");
  for (size_t i = 0; i < attributes.at("anchors").dyn_cast<pir::ArrayAttribute>().size(); i++) {
    IR_ENFORCE(attributes.at("anchors").dyn_cast<pir::ArrayAttribute>().at(i).isa<pir::Int32Attribute>(),
                   "Type of attribute: anchors is not right.");
  }
  IR_ENFORCE(attributes.count("class_num")>0,
                 "class_num does not exist.");
  IR_ENFORCE(attributes.at("class_num").isa<pir::Int32Attribute>(),
                 "Type of attribute: class_num is not pir::Int32Attribute.");

  IR_ENFORCE(attributes.count("conf_thresh")>0,
                 "conf_thresh does not exist.");
  IR_ENFORCE(attributes.at("conf_thresh").isa<pir::FloatAttribute>(),
                 "Type of attribute: conf_thresh is not pir::FloatAttribute.");

  IR_ENFORCE(attributes.count("downsample_ratio")>0,
                 "downsample_ratio does not exist.");
  IR_ENFORCE(attributes.at("downsample_ratio").isa<pir::Int32Attribute>(),
                 "Type of attribute: downsample_ratio is not pir::Int32Attribute.");

  IR_ENFORCE(attributes.count("clip_bbox")>0,
                 "clip_bbox does not exist.");
  IR_ENFORCE(attributes.at("clip_bbox").isa<pir::BoolAttribute>(),
                 "Type of attribute: clip_bbox is not pir::BoolAttribute.");

  IR_ENFORCE(attributes.count("scale_x_y")>0,
                 "scale_x_y does not exist.");
  IR_ENFORCE(attributes.at("scale_x_y").isa<pir::FloatAttribute>(),
                 "Type of attribute: scale_x_y is not pir::FloatAttribute.");

  IR_ENFORCE(attributes.count("iou_aware")>0,
                 "iou_aware does not exist.");
  IR_ENFORCE(attributes.at("iou_aware").isa<pir::BoolAttribute>(),
                 "Type of attribute: iou_aware is not pir::BoolAttribute.");

  IR_ENFORCE(attributes.count("iou_aware_factor")>0,
                 "iou_aware_factor does not exist.");
  IR_ENFORCE(attributes.at("iou_aware_factor").isa<pir::FloatAttribute>(),
                 "Type of attribute: iou_aware_factor is not pir::FloatAttribute.");

  }
  VLOG(4) << "Verifying outputs:";
  {
  auto output_size = num_results();
  IR_ENFORCE(output_size == 2u,
                    "The size %d of outputs must be equal to 2.", output_size);
  IR_ENFORCE((*this)->result(0).type().isa<paddle::dialect::DenseTensorType>(),
                 "Type validation failed for the 0th output.");
  IR_ENFORCE((*this)->result(1).type().isa<paddle::dialect::DenseTensorType>(),
                 "Type validation failed for the 1th output.");
  }
  VLOG(4) << "End Verifying for: YoloBoxOp.";
}

void YoloBoxOp::InferMeta( phi::InferMetaContext *infer_meta ) {
  auto fn = PD_INFER_META(phi::YoloBoxInferMeta);
  fn(infer_meta);
}

phi::DataType YoloBoxOp::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: YoloBoxOp";
  


  return expected_kernel_dtype;
}

const char *YoloLossOp::attributes_name[7] = { "anchors", "anchor_mask", "class_num", "ignore_thresh", "downsample_ratio", "use_label_smooth", "scale_x_y" };

OpInfoTuple YoloLossOp::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("x", "paddle::dialect::DenseTensorType", false, false, false, true), paddle::dialect::OpInputInfo("gt_box", "paddle::dialect::DenseTensorType", false, false, false, true), paddle::dialect::OpInputInfo("gt_label", "paddle::dialect::DenseTensorType", false, false, false, true), paddle::dialect::OpInputInfo("gt_score", "paddle::dialect::DenseTensorType", true, false, false, true) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = { paddle::dialect::OpAttributeInfo("anchors", "pir::ArrayAttribute<pir::Int32Attribute>", ""), paddle::dialect::OpAttributeInfo("anchor_mask", "pir::ArrayAttribute<pir::Int32Attribute>", ""), paddle::dialect::OpAttributeInfo("class_num", "pir::Int32Attribute", ""), paddle::dialect::OpAttributeInfo("ignore_thresh", "pir::FloatAttribute", ""), paddle::dialect::OpAttributeInfo("downsample_ratio", "pir::Int32Attribute", ""), paddle::dialect::OpAttributeInfo("use_label_smooth", "pir::BoolAttribute", ""), paddle::dialect::OpAttributeInfo("scale_x_y", "pir::FloatAttribute", "") };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("loss", "paddle::dialect::DenseTensorType", false, false), paddle::dialect::OpOutputInfo("objectness_mask", "paddle::dialect::DenseTensorType", true, true), paddle::dialect::OpOutputInfo("gt_match_mask", "paddle::dialect::DenseTensorType", true, true) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("YoloLossInferMeta", {"x", "gt_box", "gt_label", "gt_score", "anchors", "anchor_mask", "class_num", "ignore_thresh", "downsample_ratio", "use_label_smooth", "scale_x_y"}, "yolo_loss", {"x", "gt_box", "gt_label", "gt_score", "anchors", "anchor_mask", "class_num", "ignore_thresh", "downsample_ratio", "use_label_smooth", "scale_x_y"}, {"x"}, {}, {}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "yolo_loss");
}

void YoloLossOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, pir::Value gt_box_, pir::Value gt_label_, pir::Value gt_score_, const std::vector<int>& anchors, const std::vector<int>& anchor_mask, int class_num, float ignore_thresh, int downsample_ratio, bool use_label_smooth, float scale_x_y) {
  VLOG(4) << "Start build YoloLossOp";



  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_, gt_box_, gt_label_, gt_score_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  std::vector<pir::Attribute> vec_anchors;
  for (size_t i = 0; i < static_cast<size_t>(anchors.size()); i++) {
      pir::Attribute attr_anchors = pir::Int32Attribute::get(pir::IrContext::Instance(), anchors[i]);

    vec_anchors.push_back(attr_anchors);
  }
  pir::Attribute attr_anchors = pir::ArrayAttribute::get(pir::IrContext::Instance(), vec_anchors);
  argument.AddAttribute("anchors", attr_anchors);
  std::vector<pir::Attribute> vec_anchor_mask;
  for (size_t i = 0; i < static_cast<size_t>(anchor_mask.size()); i++) {
      pir::Attribute attr_anchor_mask = pir::Int32Attribute::get(pir::IrContext::Instance(), anchor_mask[i]);

    vec_anchor_mask.push_back(attr_anchor_mask);
  }
  pir::Attribute attr_anchor_mask = pir::ArrayAttribute::get(pir::IrContext::Instance(), vec_anchor_mask);
  argument.AddAttribute("anchor_mask", attr_anchor_mask);
  pir::Attribute attr_class_num = pir::Int32Attribute::get(pir::IrContext::Instance(), class_num);
  argument.AddAttribute("class_num", attr_class_num);
  pir::Attribute attr_ignore_thresh = pir::FloatAttribute::get(pir::IrContext::Instance(), ignore_thresh);
  argument.AddAttribute("ignore_thresh", attr_ignore_thresh);
  pir::Attribute attr_downsample_ratio = pir::Int32Attribute::get(pir::IrContext::Instance(), downsample_ratio);
  argument.AddAttribute("downsample_ratio", attr_downsample_ratio);
  pir::Attribute attr_use_label_smooth = pir::BoolAttribute::get(pir::IrContext::Instance(), use_label_smooth);
  argument.AddAttribute("use_label_smooth", attr_use_label_smooth);
  pir::Attribute attr_scale_x_y = pir::FloatAttribute::get(pir::IrContext::Instance(), scale_x_y);
  argument.AddAttribute("scale_x_y", attr_scale_x_y);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;
  paddle::dialect::DenseTensorType gt_box = gt_box_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)gt_box;
  paddle::dialect::DenseTensorType gt_label = gt_label_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)gt_label;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);

  VLOG(4) << "Builder construction  dense_gt_box";
  paddle::dialect::IrTensor ir_tensor_gt_box(paddle::dialect::TransToPhiDataType(gt_box.dtype()),
                                                      gt_box.dims(),
                                                      gt_box.data_layout(),
                                                      gt_box.lod(),
                                                      gt_box.offset());
  VLOG(4) << "Builder construction  meta_gt_box";
  paddle::dialect::IrMetaTensor meta_gt_box(&ir_tensor_gt_box);

  VLOG(4) << "Builder construction  dense_gt_label";
  paddle::dialect::IrTensor ir_tensor_gt_label(paddle::dialect::TransToPhiDataType(gt_label.dtype()),
                                                      gt_label.dims(),
                                                      gt_label.data_layout(),
                                                      gt_label.lod(),
                                                      gt_label.offset());
  VLOG(4) << "Builder construction  meta_gt_label";
  paddle::dialect::IrMetaTensor meta_gt_label(&ir_tensor_gt_label);

  paddle::dialect::IrMetaTensor meta_gt_score;
  paddle::dialect::IrTensor ir_tensor_gt_score;
  if (gt_score_.impl() != nullptr) {
    paddle::dialect::DenseTensorType gt_score = gt_score_.type().dyn_cast<paddle::dialect::DenseTensorType>();
    VLOG(4) << "Builder construction  dense_gt_score";
    ir_tensor_gt_score = paddle::dialect::IrTensor(paddle::dialect::TransToPhiDataType(gt_score.dtype()),
                                                        gt_score.dims(),
                                                        gt_score.data_layout(),
                                                        gt_score.lod(),
                                                        gt_score.offset());
    VLOG(4) << "Builder construction  meta_gt_score";
    meta_gt_score = paddle::dialect::IrMetaTensor(&ir_tensor_gt_score);
  }

  paddle::dialect::IrTensor dense_loss;
  paddle::dialect::IrMetaTensor meta_loss(&dense_loss);
  paddle::dialect::IrTensor dense_objectness_mask;
  paddle::dialect::IrMetaTensor meta_objectness_mask(&dense_objectness_mask);
  paddle::dialect::IrTensor dense_gt_match_mask;
  paddle::dialect::IrMetaTensor meta_gt_match_mask(&dense_gt_match_mask);

  phi::YoloLossInferMeta(meta_x, meta_gt_box, meta_gt_label, meta_gt_score, anchors, anchor_mask, class_num, ignore_thresh, downsample_ratio, use_label_smooth, scale_x_y, &meta_loss, &meta_objectness_mask, &meta_gt_match_mask);

  std::vector<pir::Type> argument_outputs;
  pir::Type loss_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_loss.dtype()), dense_loss.dims(), dense_loss.layout(), dense_loss.lod(), dense_loss.offset());
  argument_outputs.push_back(loss_dense_tensor_type);

  pir::Type objectness_mask_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_objectness_mask.dtype()), dense_objectness_mask.dims(), dense_objectness_mask.layout(), dense_objectness_mask.lod(), dense_objectness_mask.offset());
  argument_outputs.push_back(objectness_mask_dense_tensor_type);

  pir::Type gt_match_mask_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_gt_match_mask.dtype()), dense_gt_match_mask.dims(), dense_gt_match_mask.layout(), dense_gt_match_mask.lod(), dense_gt_match_mask.offset());
  argument_outputs.push_back(gt_match_mask_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void YoloLossOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, pir::Value gt_box_, pir::Value gt_label_, pir::Value gt_score_, pir::AttributeMap attributes) {
  VLOG(4) << "Start build YoloLossOp";


  IR_ENFORCE(
      attributes.find("anchors") != attributes.end(),
          "'anchors' Attribute is expected for YoloLossOp. ");
  std::vector<int> anchors;
  for (size_t i = 0; i < attributes.at("anchors").dyn_cast<pir::ArrayAttribute>().size(); i++) {
    anchors.push_back(attributes.at("anchors").dyn_cast<pir::ArrayAttribute>().at(i).dyn_cast<pir::Int32Attribute>().data());
  }

  IR_ENFORCE(
      attributes.find("anchor_mask") != attributes.end(),
          "'anchor_mask' Attribute is expected for YoloLossOp. ");
  std::vector<int> anchor_mask;
  for (size_t i = 0; i < attributes.at("anchor_mask").dyn_cast<pir::ArrayAttribute>().size(); i++) {
    anchor_mask.push_back(attributes.at("anchor_mask").dyn_cast<pir::ArrayAttribute>().at(i).dyn_cast<pir::Int32Attribute>().data());
  }

  IR_ENFORCE(
      attributes.find("class_num") != attributes.end(),
          "'class_num' Attribute is expected for YoloLossOp. ");
  int class_num = attributes.at("class_num").dyn_cast<pir::Int32Attribute>().data();

  IR_ENFORCE(
      attributes.find("ignore_thresh") != attributes.end(),
          "'ignore_thresh' Attribute is expected for YoloLossOp. ");
  float ignore_thresh = attributes.at("ignore_thresh").dyn_cast<pir::FloatAttribute>().data();

  IR_ENFORCE(
      attributes.find("downsample_ratio") != attributes.end(),
          "'downsample_ratio' Attribute is expected for YoloLossOp. ");
  int downsample_ratio = attributes.at("downsample_ratio").dyn_cast<pir::Int32Attribute>().data();

  IR_ENFORCE(
      attributes.find("use_label_smooth") != attributes.end(),
          "'use_label_smooth' Attribute is expected for YoloLossOp. ");
  bool use_label_smooth = attributes.at("use_label_smooth").dyn_cast<pir::BoolAttribute>().data();

  IR_ENFORCE(
      attributes.find("scale_x_y") != attributes.end(),
          "'scale_x_y' Attribute is expected for YoloLossOp. ");
  float scale_x_y = attributes.at("scale_x_y").dyn_cast<pir::FloatAttribute>().data();


  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_, gt_box_, gt_label_, gt_score_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  std::vector<pir::Attribute> vec_anchors;
  for (size_t i = 0; i < static_cast<size_t>(anchors.size()); i++) {
      pir::Attribute attr_anchors = pir::Int32Attribute::get(pir::IrContext::Instance(), anchors[i]);

    vec_anchors.push_back(attr_anchors);
  }
  pir::Attribute attr_anchors = pir::ArrayAttribute::get(pir::IrContext::Instance(), vec_anchors);
  argument.AddAttribute("anchors", attr_anchors);
  std::vector<pir::Attribute> vec_anchor_mask;
  for (size_t i = 0; i < static_cast<size_t>(anchor_mask.size()); i++) {
      pir::Attribute attr_anchor_mask = pir::Int32Attribute::get(pir::IrContext::Instance(), anchor_mask[i]);

    vec_anchor_mask.push_back(attr_anchor_mask);
  }
  pir::Attribute attr_anchor_mask = pir::ArrayAttribute::get(pir::IrContext::Instance(), vec_anchor_mask);
  argument.AddAttribute("anchor_mask", attr_anchor_mask);
  pir::Attribute attr_class_num = pir::Int32Attribute::get(pir::IrContext::Instance(), class_num);
  argument.AddAttribute("class_num", attr_class_num);
  pir::Attribute attr_ignore_thresh = pir::FloatAttribute::get(pir::IrContext::Instance(), ignore_thresh);
  argument.AddAttribute("ignore_thresh", attr_ignore_thresh);
  pir::Attribute attr_downsample_ratio = pir::Int32Attribute::get(pir::IrContext::Instance(), downsample_ratio);
  argument.AddAttribute("downsample_ratio", attr_downsample_ratio);
  pir::Attribute attr_use_label_smooth = pir::BoolAttribute::get(pir::IrContext::Instance(), use_label_smooth);
  argument.AddAttribute("use_label_smooth", attr_use_label_smooth);
  pir::Attribute attr_scale_x_y = pir::FloatAttribute::get(pir::IrContext::Instance(), scale_x_y);
  argument.AddAttribute("scale_x_y", attr_scale_x_y);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;
  paddle::dialect::DenseTensorType gt_box = gt_box_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)gt_box;
  paddle::dialect::DenseTensorType gt_label = gt_label_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)gt_label;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);

  VLOG(4) << "Builder construction  dense_gt_box";
  paddle::dialect::IrTensor ir_tensor_gt_box(paddle::dialect::TransToPhiDataType(gt_box.dtype()),
                                                      gt_box.dims(),
                                                      gt_box.data_layout(),
                                                      gt_box.lod(),
                                                      gt_box.offset());
  VLOG(4) << "Builder construction  meta_gt_box";
  paddle::dialect::IrMetaTensor meta_gt_box(&ir_tensor_gt_box);

  VLOG(4) << "Builder construction  dense_gt_label";
  paddle::dialect::IrTensor ir_tensor_gt_label(paddle::dialect::TransToPhiDataType(gt_label.dtype()),
                                                      gt_label.dims(),
                                                      gt_label.data_layout(),
                                                      gt_label.lod(),
                                                      gt_label.offset());
  VLOG(4) << "Builder construction  meta_gt_label";
  paddle::dialect::IrMetaTensor meta_gt_label(&ir_tensor_gt_label);

  paddle::dialect::IrMetaTensor meta_gt_score;
  paddle::dialect::IrTensor ir_tensor_gt_score;
  if (gt_score_.impl() != nullptr) {
    paddle::dialect::DenseTensorType gt_score = gt_score_.type().dyn_cast<paddle::dialect::DenseTensorType>();
    VLOG(4) << "Builder construction  dense_gt_score";
    ir_tensor_gt_score = paddle::dialect::IrTensor(paddle::dialect::TransToPhiDataType(gt_score.dtype()),
                                                        gt_score.dims(),
                                                        gt_score.data_layout(),
                                                        gt_score.lod(),
                                                        gt_score.offset());
    VLOG(4) << "Builder construction  meta_gt_score";
    meta_gt_score = paddle::dialect::IrMetaTensor(&ir_tensor_gt_score);
  }

  paddle::dialect::IrTensor dense_loss;
  paddle::dialect::IrMetaTensor meta_loss(&dense_loss);
  paddle::dialect::IrTensor dense_objectness_mask;
  paddle::dialect::IrMetaTensor meta_objectness_mask(&dense_objectness_mask);
  paddle::dialect::IrTensor dense_gt_match_mask;
  paddle::dialect::IrMetaTensor meta_gt_match_mask(&dense_gt_match_mask);

  phi::YoloLossInferMeta(meta_x, meta_gt_box, meta_gt_label, meta_gt_score, anchors, anchor_mask, class_num, ignore_thresh, downsample_ratio, use_label_smooth, scale_x_y, &meta_loss, &meta_objectness_mask, &meta_gt_match_mask);

  std::vector<pir::Type> argument_outputs;
  pir::Type loss_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_loss.dtype()), dense_loss.dims(), dense_loss.layout(), dense_loss.lod(), dense_loss.offset());
  argument_outputs.push_back(loss_dense_tensor_type);

  pir::Type objectness_mask_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_objectness_mask.dtype()), dense_objectness_mask.dims(), dense_objectness_mask.layout(), dense_objectness_mask.lod(), dense_objectness_mask.offset());
  argument_outputs.push_back(objectness_mask_dense_tensor_type);

  pir::Type gt_match_mask_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_gt_match_mask.dtype()), dense_gt_match_mask.dims(), dense_gt_match_mask.layout(), dense_gt_match_mask.lod(), dense_gt_match_mask.offset());
  argument_outputs.push_back(gt_match_mask_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void YoloLossOp::VerifySig() {
  VLOG(4) << "Start Verifying inputs, outputs and attributes for: YoloLossOp.";
  VLOG(4) << "Verifying inputs:";
  {
  auto input_size = num_operands();
  IR_ENFORCE(input_size == 4u,
                    "The size %d of inputs must be equal to 4.", input_size);
  IR_ENFORCE((*this)->operand_source(0).type().isa<paddle::dialect::DenseTensorType>(),
                  "Type validation failed for the 0th input, got %s.", (*this)->operand_source(0).type());
  IR_ENFORCE((*this)->operand_source(1).type().isa<paddle::dialect::DenseTensorType>(),
                  "Type validation failed for the 1th input, got %s.", (*this)->operand_source(1).type());
  IR_ENFORCE((*this)->operand_source(2).type().isa<paddle::dialect::DenseTensorType>(),
                  "Type validation failed for the 2th input, got %s.", (*this)->operand_source(2).type());
  if (auto val = (*this)->operand(3)) {
    IR_ENFORCE(val.type().isa<paddle::dialect::DenseTensorType>(),
                   "Type validation failed for the 3th input, got %s.", (*this)->operand_source(3).type());
  }
  }
  VLOG(4) << "Verifying attributes:";
  {
  auto& attributes = this->attributes();
  IR_ENFORCE(attributes.count("anchors")>0,
                 "anchors does not exist.");
  IR_ENFORCE(attributes.at("anchors").isa<pir::ArrayAttribute>(),
                 "Type of attribute: anchors is not pir::ArrayAttribute.");
  for (size_t i = 0; i < attributes.at("anchors").dyn_cast<pir::ArrayAttribute>().size(); i++) {
    IR_ENFORCE(attributes.at("anchors").dyn_cast<pir::ArrayAttribute>().at(i).isa<pir::Int32Attribute>(),
                   "Type of attribute: anchors is not right.");
  }
  IR_ENFORCE(attributes.count("anchor_mask")>0,
                 "anchor_mask does not exist.");
  IR_ENFORCE(attributes.at("anchor_mask").isa<pir::ArrayAttribute>(),
                 "Type of attribute: anchor_mask is not pir::ArrayAttribute.");
  for (size_t i = 0; i < attributes.at("anchor_mask").dyn_cast<pir::ArrayAttribute>().size(); i++) {
    IR_ENFORCE(attributes.at("anchor_mask").dyn_cast<pir::ArrayAttribute>().at(i).isa<pir::Int32Attribute>(),
                   "Type of attribute: anchor_mask is not right.");
  }
  IR_ENFORCE(attributes.count("class_num")>0,
                 "class_num does not exist.");
  IR_ENFORCE(attributes.at("class_num").isa<pir::Int32Attribute>(),
                 "Type of attribute: class_num is not pir::Int32Attribute.");

  IR_ENFORCE(attributes.count("ignore_thresh")>0,
                 "ignore_thresh does not exist.");
  IR_ENFORCE(attributes.at("ignore_thresh").isa<pir::FloatAttribute>(),
                 "Type of attribute: ignore_thresh is not pir::FloatAttribute.");

  IR_ENFORCE(attributes.count("downsample_ratio")>0,
                 "downsample_ratio does not exist.");
  IR_ENFORCE(attributes.at("downsample_ratio").isa<pir::Int32Attribute>(),
                 "Type of attribute: downsample_ratio is not pir::Int32Attribute.");

  IR_ENFORCE(attributes.count("use_label_smooth")>0,
                 "use_label_smooth does not exist.");
  IR_ENFORCE(attributes.at("use_label_smooth").isa<pir::BoolAttribute>(),
                 "Type of attribute: use_label_smooth is not pir::BoolAttribute.");

  IR_ENFORCE(attributes.count("scale_x_y")>0,
                 "scale_x_y does not exist.");
  IR_ENFORCE(attributes.at("scale_x_y").isa<pir::FloatAttribute>(),
                 "Type of attribute: scale_x_y is not pir::FloatAttribute.");

  }
  VLOG(4) << "Verifying outputs:";
  {
  auto output_size = num_results();
  IR_ENFORCE(output_size == 3u,
                    "The size %d of outputs must be equal to 3.", output_size);
  IR_ENFORCE((*this)->result(0).type().isa<paddle::dialect::DenseTensorType>(),
                 "Type validation failed for the 0th output.");
  if (auto output_1_type = (*this)->result(1).type()) {
    IR_ENFORCE(output_1_type.isa<paddle::dialect::DenseTensorType>(),
                   "Type validation failed for the 1th output.");
  }
  if (auto output_2_type = (*this)->result(2).type()) {
    IR_ENFORCE(output_2_type.isa<paddle::dialect::DenseTensorType>(),
                   "Type validation failed for the 2th output.");
  }
  }
  VLOG(4) << "End Verifying for: YoloLossOp.";
}

void YoloLossOp::InferMeta( phi::InferMetaContext *infer_meta ) {
  auto fn = PD_INFER_META(phi::YoloLossInferMeta);
  fn(infer_meta);
}

phi::DataType YoloLossOp::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: YoloLossOp";
  


  return expected_kernel_dtype;
}

} // namespace dialect
} // namespace paddle


IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::AbsOp)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::Abs_Op)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::AccuracyOp)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::AcosOp)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::Acos_Op)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::AcoshOp)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::Acosh_Op)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::Adagrad_Op)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::AdagradDenseParamSparseGrad_Op)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::Adam_Op)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::AdamDenseParamSparseGrad_Op)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::Adamax_Op)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::Adamw_Op)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::AddmmOp)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::Addmm_Op)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::AffineGridOp)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::AllcloseOp)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::AngleOp)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::ArgmaxOp)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::ArgminOp)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::ArgsortOp)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::AsComplexOp)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::AsRealOp)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::AsStridedOp)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::AsinOp)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::Asin_Op)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::AsinhOp)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::Asinh_Op)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::AtanOp)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::Atan_Op)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::Atan2Op)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::AtanhOp)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::Atanh_Op)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::AucOp)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::AverageAccumulates_Op)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::BceLossOp)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::BceLoss_Op)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::BernoulliOp)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::BicubicInterpOp)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::BilinearOp)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::BilinearInterpOp)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::BincountOp)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::BinomialOp)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::BitwiseAndOp)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::BitwiseAnd_Op)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::BitwiseNotOp)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::BitwiseNot_Op)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::BitwiseOrOp)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::BitwiseOr_Op)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::BitwiseXorOp)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::BitwiseXor_Op)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::BmmOp)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::BoxCoderOp)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::BroadcastTensorsOp)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::CeilOp)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::Ceil_Op)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::CeluOp)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::CheckFiniteAndUnscale_Op)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::CheckNumericsOp)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::CholeskyOp)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::CholeskySolveOp)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::ClassCenterSampleOp)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::ClipOp)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::Clip_Op)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::ClipByNormOp)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::ClipByNormSrOp)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::CoalesceTensorOp)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::ComplexOp)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::ConcatOp)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::ConjOp)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::Conv2dOp)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::Conv3dOp)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::Conv3dTransposeOp)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::CosOp)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::Cos_Op)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::CoshOp)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::Cosh_Op)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::CropOp)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::CrossOp)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::CrossEntropyWithSoftmaxOp)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::CrossEntropyWithSoftmax_Op)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::CummaxOp)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::CumminOp)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::CumprodOp)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::Cumprod_Op)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::CumsumOp)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::Cumsum_Op)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::DataOp)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::DepthwiseConv2dOp)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::DetOp)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::DiagOp)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::DiagEmbedOp)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::DiagonalOp)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::DigammaOp)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::Digamma_Op)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::DirichletOp)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::DistOp)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::DotOp)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::EditDistanceOp)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::EigOp)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::EighOp)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::EigvalsOp)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::EigvalshOp)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::EluOp)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::Elu_Op)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::EqualAllOp)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::ErfOp)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::Erf_Op)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::ErfinvOp)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::Erfinv_Op)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::ExpOp)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::Exp_Op)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::ExpandAsOp)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::Expm1Op)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::Expm1_Op)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::FftC2cOp)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::FftC2rOp)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::FftR2cOp)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::FillOp)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::Fill_Op)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::FillDiagonalOp)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::FillDiagonal_Op)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::FillDiagonalTensorOp)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::FillDiagonalTensor_Op)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::FlashAttnOp)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::FlashAttnUnpaddedOp)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::FlattenOp)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::Flatten_Op)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::FlipOp)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::FloorOp)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::Floor_Op)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::FmaxOp)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::FminOp)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::FoldOp)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::FrameOp)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::FullIntArrayOp)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::GammalnOp)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::Gammaln_Op)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::GatherOp)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::GatherNdOp)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::GatherTreeOp)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::GaussianInplaceOp)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::GaussianInplace_Op)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::GeluOp)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::GenerateProposalsOp)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::GridSampleOp)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::GroupNormOp)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::GumbelSoftmaxOp)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::HardshrinkOp)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::HardsigmoidOp)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::HardtanhOp)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::Hardtanh_Op)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::HeavisideOp)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::HistogramOp)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::HuberLossOp)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::I0Op)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::I0_Op)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::I0eOp)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::I1Op)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::I1eOp)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::IdentityLossOp)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::IdentityLoss_Op)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::ImagOp)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::IndexAddOp)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::IndexAdd_Op)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::IndexPutOp)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::IndexPut_Op)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::IndexSampleOp)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::IndexSelectOp)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::IndexSelectStridedOp)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::InstanceNormOp)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::InverseOp)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::IsEmptyOp)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::IscloseOp)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::IsfiniteOp)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::IsfiniteSrOp)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::IsinfOp)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::IsinfSrOp)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::IsnanOp)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::IsnanSrOp)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::KldivLossOp)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::KronOp)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::KthvalueOp)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::LabelSmoothOp)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::Lamb_Op)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::LambSr_Op)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::LayerNormOp)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::LeakyReluOp)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::LeakyRelu_Op)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::LerpOp)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::Lerp_Op)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::LgammaOp)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::Lgamma_Op)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::LinearInterpOp)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::LlmInt8LinearOp)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::LogOp)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::Log_Op)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::Log10Op)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::Log10_Op)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::Log1pOp)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::Log1p_Op)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::Log2Op)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::Log2_Op)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::LogLossOp)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::LogSoftmaxOp)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::LogcumsumexpOp)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::LogicalAndOp)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::LogicalAnd_Op)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::LogicalNotOp)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::LogicalNot_Op)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::LogicalOrOp)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::LogicalOr_Op)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::LogicalXorOp)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::LogicalXor_Op)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::LogitOp)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::Logit_Op)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::LogsigmoidOp)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::LstsqOp)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::LuOp)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::Lu_Op)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::LuUnpackOp)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::MarginCrossEntropyOp)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::MaskedMultiheadAttention_Op)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::MaskedSelectOp)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::MatrixNmsOp)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::MatrixPowerOp)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::MaxPool2dWithIndexOp)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::MaxPool3dWithIndexOp)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::MaxoutOp)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::MeanAllOp)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::MemoryEfficientAttentionOp)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::MergeSelectedRowsOp)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::MergedAdam_Op)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::MergedMomentum_Op)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::MeshgridOp)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::ModeOp)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::Momentum_Op)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::MomentumDenseParamSparseGrad_Op)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::MultiDotOp)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::MulticlassNms3Op)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::MultinomialOp)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::MultiplexOp)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::MvOp)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::NanmedianOp)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::NearestInterpOp)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::NextafterOp)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::NllLossOp)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::NmsOp)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::NonzeroOp)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::NpuIdentityOp)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::NumelOp)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::OverlapAddOp)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::PNormOp)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::Pad3dOp)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::PixelShuffleOp)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::PixelUnshuffleOp)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::PoissonOp)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::PolygammaOp)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::Polygamma_Op)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::PowOp)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::Pow_Op)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::PreluOp)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::PriorBoxOp)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::PsroiPoolOp)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::PutAlongAxisOp)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::PutAlongAxis_Op)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::QrOp)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::RealOp)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::ReciprocalOp)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::Reciprocal_Op)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::ReindexGraphOp)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::ReluOp)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::Relu_Op)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::Relu6Op)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::RenormOp)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::Renorm_Op)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::ReverseOp)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::RmsNormOp)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::Rmsprop_Op)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::RmspropDenseParamSparseGrad_Op)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::RoiAlignOp)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::RoiPoolOp)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::RollOp)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::RoundOp)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::Round_Op)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::Rprop_Op)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::RsqrtOp)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::Rsqrt_Op)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::ScaleOp)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::ScaleSrOp)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::Scale_Op)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::ScaleSr_Op)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::ScatterOp)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::Scatter_Op)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::ScatterNdAddOp)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::SearchsortedOp)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::SegmentPoolOp)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::SeluOp)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::SendURecvOp)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::SendUeRecvOp)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::SendUvOp)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::Sgd_Op)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::SgdDenseParamSparseGrad_Op)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::SgdSparseParamSparseGrad_Op)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::ShapeOp)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::ShapeSrOp)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::ShardIndexOp)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::SigmoidOp)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::Sigmoid_Op)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::SigmoidCrossEntropyWithLogitsOp)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::SigmoidCrossEntropyWithLogits_Op)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::SignOp)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::SiluOp)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::SinOp)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::Sin_Op)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::SinhOp)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::Sinh_Op)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::SlogdetOp)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::SoftplusOp)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::SoftshrinkOp)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::SoftsignOp)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::SolveOp)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::SpectralNormOp)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::SqrtOp)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::SqrtSrOp)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::Sqrt_Op)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::SqrtSr_Op)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::SquareOp)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::SquareSrOp)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::SquaredL2NormOp)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::SqueezeOp)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::Squeeze_Op)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::StackOp)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::StandardGammaOp)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::StanhOp)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::SvdOp)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::TakeAlongAxisOp)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::TanOp)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::Tan_Op)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::TanhOp)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::Tanh_Op)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::TanhShrinkOp)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::TemporalShiftOp)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::TensorUnfoldOp)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::ThresholdedReluOp)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::ThresholdedRelu_Op)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::TopPSamplingOp)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::TopkOp)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::TraceOp)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::TriangularSolveOp)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::TrilinearInterpOp)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::TruncOp)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::Trunc_Op)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::UnbindOp)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::UnfoldOp)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::UniformInplaceOp)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::UniformInplace_Op)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::UniqueConsecutiveOp)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::Unpool3dOp)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::UnsqueezeOp)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::Unsqueeze_Op)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::UnstackOp)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::UpdateLossScaling_Op)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::ViewDtypeOp)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::ViewShapeOp)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::ViterbiDecodeOp)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::WarpctcOp)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::WarprnntOp)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::WeightDequantizeOp)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::WeightOnlyLinearOp)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::WeightQuantizeOp)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::WeightedSampleNeighborsOp)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::WhereOp)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::Where_Op)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::YoloBoxOp)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::YoloLossOp)

