// This file is generated by "paddle/fluid/pir/dialect/op_generator/op_gen.py"
#include "/home/aistudio/fix_op/Paddle/paddle/fluid/pir/dialect/operator/ir/pd_op.h"
#include "paddle/fluid/pir/dialect/operator/ir/op_type.h"
#include "paddle/fluid/pir/dialect/operator/ir/op_attribute.h"
#include "paddle/fluid/pir/dialect/operator/ir/ir_tensor.h"
#include "paddle/fluid/pir/dialect/operator/ir/ir_selected_rows.h"
#include "paddle/fluid/pir/dialect/operator/ir/ir_meta_tensor.h"
#include "paddle/pir/core/builtin_attribute.h"
#include "paddle/pir/core/builtin_type.h"
#include "paddle/pir/core/builtin_op.h"
#include "paddle/pir/core/ir_context.h"
#include "paddle/phi/core/enforce.h"
#include "paddle/phi/core/dense_tensor.h"
#include "paddle/phi/infermeta/binary.h"
#include "paddle/phi/infermeta/multiary.h"
#include "paddle/phi/infermeta/nullary.h"
#include "paddle/phi/infermeta/unary.h"
#include "paddle/phi/infermeta/ternary.h"
#include "paddle/phi/infermeta/backward.h"
#include "paddle/phi/infermeta/fusion.h"
#include "paddle/phi/api/lib/utils/allocator.h"
#include "paddle/fluid/primitive/rule/vjp/vjp.h"
#include "paddle/pir/core/op_base.h"

namespace paddle {
namespace dialect {

const char *AddDoubleGradOp::attributes_name[1] = { "axis" };

OpInfoTuple AddDoubleGradOp::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("y", "paddle::dialect::DenseTensorType", false, false, false, false), paddle::dialect::OpInputInfo("grad_out", "paddle::dialect::DenseTensorType", false, false, false, false), paddle::dialect::OpInputInfo("grad_x_grad", "paddle::dialect::DenseTensorType", true, false, false, true), paddle::dialect::OpInputInfo("grad_y_grad", "paddle::dialect::DenseTensorType", true, false, false, true) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = { paddle::dialect::OpAttributeInfo("axis", "pir::Int32Attribute", "") };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("grad_out_grad", "paddle::dialect::DenseTensorType", false, false) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("UnchangedInferMeta", {"grad_out"}, "add_double_grad", {"y", "grad_out", "grad_x_grad", "grad_y_grad", "axis"}, {}, {}, {}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "add_double_grad");
}

void AddDoubleGradOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value y_, pir::Value grad_out_, pir::Value grad_x_grad_, pir::Value grad_y_grad_, int axis) {
  VLOG(4) << "Start build AddDoubleGradOp";



  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {y_, grad_out_, grad_x_grad_, grad_y_grad_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_axis = pir::Int32Attribute::get(pir::IrContext::Instance(), axis);
  argument.AddAttribute("axis", attr_axis);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType y = y_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)y;
  paddle::dialect::DenseTensorType grad_out = grad_out_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)grad_out;

  VLOG(4) << "Builder construction  dense_grad_out";
  paddle::dialect::IrTensor ir_tensor_grad_out(paddle::dialect::TransToPhiDataType(grad_out.dtype()),
                                                      grad_out.dims(),
                                                      grad_out.data_layout(),
                                                      grad_out.lod(),
                                                      grad_out.offset());
  VLOG(4) << "Builder construction  meta_grad_out";
  paddle::dialect::IrMetaTensor meta_grad_out(&ir_tensor_grad_out);
  paddle::dialect::IrTensor dense_grad_out_grad;
  paddle::dialect::IrMetaTensor meta_grad_out_grad(&dense_grad_out_grad);

  phi::UnchangedInferMeta(meta_grad_out, &meta_grad_out_grad);

  std::vector<pir::Type> argument_outputs;
  pir::Type grad_out_grad_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_grad_out_grad.dtype()), dense_grad_out_grad.dims(), dense_grad_out_grad.layout(), dense_grad_out_grad.lod(), dense_grad_out_grad.offset());
  argument_outputs.push_back(grad_out_grad_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void AddDoubleGradOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value y_, pir::Value grad_out_, pir::Value grad_x_grad_, pir::Value grad_y_grad_, pir::AttributeMap attributes) {
  VLOG(4) << "Start build AddDoubleGradOp";


  IR_ENFORCE(
      attributes.find("axis") != attributes.end(),
          "'axis' Attribute is expected for AddDoubleGradOp. ");
  int axis = attributes.at("axis").dyn_cast<pir::Int32Attribute>().data();


  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {y_, grad_out_, grad_x_grad_, grad_y_grad_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_axis = pir::Int32Attribute::get(pir::IrContext::Instance(), axis);
  argument.AddAttribute("axis", attr_axis);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType y = y_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)y;
  paddle::dialect::DenseTensorType grad_out = grad_out_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)grad_out;

  VLOG(4) << "Builder construction  dense_grad_out";
  paddle::dialect::IrTensor ir_tensor_grad_out(paddle::dialect::TransToPhiDataType(grad_out.dtype()),
                                                      grad_out.dims(),
                                                      grad_out.data_layout(),
                                                      grad_out.lod(),
                                                      grad_out.offset());
  VLOG(4) << "Builder construction  meta_grad_out";
  paddle::dialect::IrMetaTensor meta_grad_out(&ir_tensor_grad_out);
  paddle::dialect::IrTensor dense_grad_out_grad;
  paddle::dialect::IrMetaTensor meta_grad_out_grad(&dense_grad_out_grad);

  phi::UnchangedInferMeta(meta_grad_out, &meta_grad_out_grad);

  std::vector<pir::Type> argument_outputs;
  pir::Type grad_out_grad_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_grad_out_grad.dtype()), dense_grad_out_grad.dims(), dense_grad_out_grad.layout(), dense_grad_out_grad.lod(), dense_grad_out_grad.offset());
  argument_outputs.push_back(grad_out_grad_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void AddDoubleGradOp::VerifySig() {}

void AddDoubleGradOp::InferMeta( phi::InferMetaContext *infer_meta ) {
  auto fn = PD_INFER_META(phi::UnchangedInferMeta);
  fn(infer_meta);
}

phi::DataType AddDoubleGradOp::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: AddDoubleGradOp";
  


  // deal complex_promote
  if (framework::IsComplexType(expected_kernel_dtype)) {{
    // only promote inputsâ€™s types when contains complex input
    return tensor_dtype;
  }}

  return expected_kernel_dtype;
}

const char *AddDoubleGrad_Op::attributes_name[1] = { "axis" };

OpInfoTuple AddDoubleGrad_Op::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("y", "paddle::dialect::DenseTensorType", false, false, false, false), paddle::dialect::OpInputInfo("grad_out", "paddle::dialect::DenseTensorType", false, false, false, false), paddle::dialect::OpInputInfo("grad_x_grad", "paddle::dialect::DenseTensorType", true, false, false, true), paddle::dialect::OpInputInfo("grad_y_grad", "paddle::dialect::DenseTensorType", true, false, false, true) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = { paddle::dialect::OpAttributeInfo("axis", "pir::Int32Attribute", "") };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("grad_out_grad", "paddle::dialect::DenseTensorType", false, false) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("UnchangedInferMeta", {"grad_out"}, "add_double_grad", {"y", "grad_out", "grad_x_grad", "grad_y_grad", "axis"}, {}, {}, {{"grad_out_grad", "grad_x_grad"}}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "add_double_grad");
}

void AddDoubleGrad_Op::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value y_, pir::Value grad_out_, pir::Value grad_x_grad_, pir::Value grad_y_grad_, int axis) {
  VLOG(4) << "Start build AddDoubleGrad_Op";



  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {y_, grad_out_, grad_x_grad_, grad_y_grad_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_axis = pir::Int32Attribute::get(pir::IrContext::Instance(), axis);
  argument.AddAttribute("axis", attr_axis);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType y = y_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)y;
  paddle::dialect::DenseTensorType grad_out = grad_out_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)grad_out;

  VLOG(4) << "Builder construction  dense_grad_out";
  paddle::dialect::IrTensor ir_tensor_grad_out(paddle::dialect::TransToPhiDataType(grad_out.dtype()),
                                                      grad_out.dims(),
                                                      grad_out.data_layout(),
                                                      grad_out.lod(),
                                                      grad_out.offset());
  VLOG(4) << "Builder construction  meta_grad_out";
  paddle::dialect::IrMetaTensor meta_grad_out(&ir_tensor_grad_out);
  paddle::dialect::IrTensor dense_grad_out_grad;
  paddle::dialect::IrMetaTensor meta_grad_out_grad(&dense_grad_out_grad);

  phi::UnchangedInferMeta(meta_grad_out, &meta_grad_out_grad);

  std::vector<pir::Type> argument_outputs;
  pir::Type grad_out_grad_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_grad_out_grad.dtype()), dense_grad_out_grad.dims(), dense_grad_out_grad.layout(), dense_grad_out_grad.lod(), dense_grad_out_grad.offset());
  argument_outputs.push_back(grad_out_grad_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void AddDoubleGrad_Op::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value y_, pir::Value grad_out_, pir::Value grad_x_grad_, pir::Value grad_y_grad_, pir::AttributeMap attributes) {
  VLOG(4) << "Start build AddDoubleGrad_Op";


  IR_ENFORCE(
      attributes.find("axis") != attributes.end(),
          "'axis' Attribute is expected for AddDoubleGrad_Op. ");
  int axis = attributes.at("axis").dyn_cast<pir::Int32Attribute>().data();


  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {y_, grad_out_, grad_x_grad_, grad_y_grad_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_axis = pir::Int32Attribute::get(pir::IrContext::Instance(), axis);
  argument.AddAttribute("axis", attr_axis);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType y = y_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)y;
  paddle::dialect::DenseTensorType grad_out = grad_out_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)grad_out;

  VLOG(4) << "Builder construction  dense_grad_out";
  paddle::dialect::IrTensor ir_tensor_grad_out(paddle::dialect::TransToPhiDataType(grad_out.dtype()),
                                                      grad_out.dims(),
                                                      grad_out.data_layout(),
                                                      grad_out.lod(),
                                                      grad_out.offset());
  VLOG(4) << "Builder construction  meta_grad_out";
  paddle::dialect::IrMetaTensor meta_grad_out(&ir_tensor_grad_out);
  paddle::dialect::IrTensor dense_grad_out_grad;
  paddle::dialect::IrMetaTensor meta_grad_out_grad(&dense_grad_out_grad);

  phi::UnchangedInferMeta(meta_grad_out, &meta_grad_out_grad);

  std::vector<pir::Type> argument_outputs;
  pir::Type grad_out_grad_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_grad_out_grad.dtype()), dense_grad_out_grad.dims(), dense_grad_out_grad.layout(), dense_grad_out_grad.lod(), dense_grad_out_grad.offset());
  argument_outputs.push_back(grad_out_grad_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void AddDoubleGrad_Op::VerifySig() {}

void AddDoubleGrad_Op::InferMeta( phi::InferMetaContext *infer_meta ) {
  auto fn = PD_INFER_META(phi::UnchangedInferMeta);
  fn(infer_meta);
}

phi::DataType AddDoubleGrad_Op::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: AddDoubleGrad_Op";
  


  // deal complex_promote
  if (framework::IsComplexType(expected_kernel_dtype)) {{
    // only promote inputsâ€™s types when contains complex input
    return tensor_dtype;
  }}

  return expected_kernel_dtype;
}

const char *AddGradOp::attributes_name[1] = { "axis" };

OpInfoTuple AddGradOp::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("x", "paddle::dialect::DenseTensorType", false, true, false, false), paddle::dialect::OpInputInfo("y", "paddle::dialect::DenseTensorType", false, true, false, false), paddle::dialect::OpInputInfo("out_grad", "paddle::dialect::DenseTensorType", false, false, false, true) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = { paddle::dialect::OpAttributeInfo("axis", "pir::Int32Attribute", "") };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("x_grad", "paddle::dialect::DenseTensorType", false, false), paddle::dialect::OpOutputInfo("y_grad", "paddle::dialect::DenseTensorType", false, false) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("GeneralBinaryGradInferMeta", {"x", "y"}, "add_grad", {"x", "y", "out_grad", "axis"}, {}, {}, {}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "add_grad");
}

void AddGradOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, pir::Value y_, pir::Value out_grad_, int axis) {
  VLOG(4) << "Start build AddGradOp";



  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_, y_, out_grad_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_axis = pir::Int32Attribute::get(pir::IrContext::Instance(), axis);
  argument.AddAttribute("axis", attr_axis);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;
  paddle::dialect::DenseTensorType y = y_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)y;
  paddle::dialect::DenseTensorType out_grad = out_grad_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)out_grad;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);

  VLOG(4) << "Builder construction  dense_y";
  paddle::dialect::IrTensor ir_tensor_y(paddle::dialect::TransToPhiDataType(y.dtype()),
                                                      y.dims(),
                                                      y.data_layout(),
                                                      y.lod(),
                                                      y.offset());
  VLOG(4) << "Builder construction  meta_y";
  paddle::dialect::IrMetaTensor meta_y(&ir_tensor_y);
  paddle::dialect::IrTensor dense_x_grad;
  paddle::dialect::IrMetaTensor meta_x_grad(&dense_x_grad);
  paddle::dialect::IrTensor dense_y_grad;
  paddle::dialect::IrMetaTensor meta_y_grad(&dense_y_grad);

  phi::GeneralBinaryGradInferMeta(meta_x, meta_y, &meta_x_grad, &meta_y_grad);

  std::vector<pir::Type> argument_outputs;
  pir::Type x_grad_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_x_grad.dtype()), dense_x_grad.dims(), dense_x_grad.layout(), dense_x_grad.lod(), dense_x_grad.offset());
  argument_outputs.push_back(x_grad_dense_tensor_type);

  pir::Type y_grad_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_y_grad.dtype()), dense_y_grad.dims(), dense_y_grad.layout(), dense_y_grad.lod(), dense_y_grad.offset());
  argument_outputs.push_back(y_grad_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void AddGradOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, pir::Value y_, pir::Value out_grad_, pir::AttributeMap attributes) {
  VLOG(4) << "Start build AddGradOp";


  IR_ENFORCE(
      attributes.find("axis") != attributes.end(),
          "'axis' Attribute is expected for AddGradOp. ");
  int axis = attributes.at("axis").dyn_cast<pir::Int32Attribute>().data();


  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_, y_, out_grad_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_axis = pir::Int32Attribute::get(pir::IrContext::Instance(), axis);
  argument.AddAttribute("axis", attr_axis);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;
  paddle::dialect::DenseTensorType y = y_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)y;
  paddle::dialect::DenseTensorType out_grad = out_grad_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)out_grad;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);

  VLOG(4) << "Builder construction  dense_y";
  paddle::dialect::IrTensor ir_tensor_y(paddle::dialect::TransToPhiDataType(y.dtype()),
                                                      y.dims(),
                                                      y.data_layout(),
                                                      y.lod(),
                                                      y.offset());
  VLOG(4) << "Builder construction  meta_y";
  paddle::dialect::IrMetaTensor meta_y(&ir_tensor_y);
  paddle::dialect::IrTensor dense_x_grad;
  paddle::dialect::IrMetaTensor meta_x_grad(&dense_x_grad);
  paddle::dialect::IrTensor dense_y_grad;
  paddle::dialect::IrMetaTensor meta_y_grad(&dense_y_grad);

  phi::GeneralBinaryGradInferMeta(meta_x, meta_y, &meta_x_grad, &meta_y_grad);

  std::vector<pir::Type> argument_outputs;
  pir::Type x_grad_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_x_grad.dtype()), dense_x_grad.dims(), dense_x_grad.layout(), dense_x_grad.lod(), dense_x_grad.offset());
  argument_outputs.push_back(x_grad_dense_tensor_type);

  pir::Type y_grad_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_y_grad.dtype()), dense_y_grad.dims(), dense_y_grad.layout(), dense_y_grad.lod(), dense_y_grad.offset());
  argument_outputs.push_back(y_grad_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void AddGradOp::VerifySig() {}

void AddGradOp::InferMeta( phi::InferMetaContext *infer_meta ) {
  auto fn = PD_INFER_META(phi::GeneralBinaryGradInferMeta);
  fn(infer_meta);
}

phi::DataType AddGradOp::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: AddGradOp";
  


  // deal complex_promote
  if (framework::IsComplexType(expected_kernel_dtype)) {{
    // only promote inputsâ€™s types when contains complex input
    return tensor_dtype;
  }}

  return expected_kernel_dtype;
}

const char *AddGrad_Op::attributes_name[1] = { "axis" };

OpInfoTuple AddGrad_Op::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("x", "paddle::dialect::DenseTensorType", false, true, false, false), paddle::dialect::OpInputInfo("y", "paddle::dialect::DenseTensorType", false, true, false, false), paddle::dialect::OpInputInfo("out_grad", "paddle::dialect::DenseTensorType", false, false, false, true) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = { paddle::dialect::OpAttributeInfo("axis", "pir::Int32Attribute", "") };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("x_grad", "paddle::dialect::DenseTensorType", false, false), paddle::dialect::OpOutputInfo("y_grad", "paddle::dialect::DenseTensorType", false, false) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("GeneralBinaryGradInferMeta", {"x", "y"}, "add_grad", {"x", "y", "out_grad", "axis"}, {}, {}, {{"x_grad", "out_grad"}}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "add_grad");
}

void AddGrad_Op::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, pir::Value y_, pir::Value out_grad_, int axis) {
  VLOG(4) << "Start build AddGrad_Op";



  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_, y_, out_grad_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_axis = pir::Int32Attribute::get(pir::IrContext::Instance(), axis);
  argument.AddAttribute("axis", attr_axis);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;
  paddle::dialect::DenseTensorType y = y_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)y;
  paddle::dialect::DenseTensorType out_grad = out_grad_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)out_grad;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);

  VLOG(4) << "Builder construction  dense_y";
  paddle::dialect::IrTensor ir_tensor_y(paddle::dialect::TransToPhiDataType(y.dtype()),
                                                      y.dims(),
                                                      y.data_layout(),
                                                      y.lod(),
                                                      y.offset());
  VLOG(4) << "Builder construction  meta_y";
  paddle::dialect::IrMetaTensor meta_y(&ir_tensor_y);
  paddle::dialect::IrTensor dense_x_grad;
  paddle::dialect::IrMetaTensor meta_x_grad(&dense_x_grad);
  paddle::dialect::IrTensor dense_y_grad;
  paddle::dialect::IrMetaTensor meta_y_grad(&dense_y_grad);

  phi::GeneralBinaryGradInferMeta(meta_x, meta_y, &meta_x_grad, &meta_y_grad);

  std::vector<pir::Type> argument_outputs;
  pir::Type x_grad_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_x_grad.dtype()), dense_x_grad.dims(), dense_x_grad.layout(), dense_x_grad.lod(), dense_x_grad.offset());
  argument_outputs.push_back(x_grad_dense_tensor_type);

  pir::Type y_grad_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_y_grad.dtype()), dense_y_grad.dims(), dense_y_grad.layout(), dense_y_grad.lod(), dense_y_grad.offset());
  argument_outputs.push_back(y_grad_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void AddGrad_Op::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, pir::Value y_, pir::Value out_grad_, pir::AttributeMap attributes) {
  VLOG(4) << "Start build AddGrad_Op";


  IR_ENFORCE(
      attributes.find("axis") != attributes.end(),
          "'axis' Attribute is expected for AddGrad_Op. ");
  int axis = attributes.at("axis").dyn_cast<pir::Int32Attribute>().data();


  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_, y_, out_grad_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_axis = pir::Int32Attribute::get(pir::IrContext::Instance(), axis);
  argument.AddAttribute("axis", attr_axis);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;
  paddle::dialect::DenseTensorType y = y_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)y;
  paddle::dialect::DenseTensorType out_grad = out_grad_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)out_grad;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);

  VLOG(4) << "Builder construction  dense_y";
  paddle::dialect::IrTensor ir_tensor_y(paddle::dialect::TransToPhiDataType(y.dtype()),
                                                      y.dims(),
                                                      y.data_layout(),
                                                      y.lod(),
                                                      y.offset());
  VLOG(4) << "Builder construction  meta_y";
  paddle::dialect::IrMetaTensor meta_y(&ir_tensor_y);
  paddle::dialect::IrTensor dense_x_grad;
  paddle::dialect::IrMetaTensor meta_x_grad(&dense_x_grad);
  paddle::dialect::IrTensor dense_y_grad;
  paddle::dialect::IrMetaTensor meta_y_grad(&dense_y_grad);

  phi::GeneralBinaryGradInferMeta(meta_x, meta_y, &meta_x_grad, &meta_y_grad);

  std::vector<pir::Type> argument_outputs;
  pir::Type x_grad_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_x_grad.dtype()), dense_x_grad.dims(), dense_x_grad.layout(), dense_x_grad.lod(), dense_x_grad.offset());
  argument_outputs.push_back(x_grad_dense_tensor_type);

  pir::Type y_grad_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_y_grad.dtype()), dense_y_grad.dims(), dense_y_grad.layout(), dense_y_grad.lod(), dense_y_grad.offset());
  argument_outputs.push_back(y_grad_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void AddGrad_Op::VerifySig() {}

void AddGrad_Op::InferMeta( phi::InferMetaContext *infer_meta ) {
  auto fn = PD_INFER_META(phi::GeneralBinaryGradInferMeta);
  fn(infer_meta);
}

phi::DataType AddGrad_Op::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: AddGrad_Op";
  


  // deal complex_promote
  if (framework::IsComplexType(expected_kernel_dtype)) {{
    // only promote inputsâ€™s types when contains complex input
    return tensor_dtype;
  }}

  return expected_kernel_dtype;
}

const char *AddTripleGradOp::attributes_name[1] = { "axis" };

OpInfoTuple AddTripleGradOp::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("grad_grad_x", "paddle::dialect::DenseTensorType", false, false, false, false), paddle::dialect::OpInputInfo("grad_grad_y", "paddle::dialect::DenseTensorType", false, false, false, false), paddle::dialect::OpInputInfo("grad_grad_out_grad", "paddle::dialect::DenseTensorType", false, false, false, false) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = { paddle::dialect::OpAttributeInfo("axis", "pir::Int32Attribute", "") };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("grad_grad_x_grad", "paddle::dialect::DenseTensorType", false, false), paddle::dialect::OpOutputInfo("grad_grad_y_grad", "paddle::dialect::DenseTensorType", false, false) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("GeneralBinaryGradInferMeta", {"grad_grad_x", "grad_grad_y"}, "add_triple_grad", {"grad_grad_x", "grad_grad_y", "grad_grad_out_grad", "axis"}, {}, {}, {}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "add_triple_grad");
}

void AddTripleGradOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value grad_grad_x_, pir::Value grad_grad_y_, pir::Value grad_grad_out_grad_, int axis) {
  VLOG(4) << "Start build AddTripleGradOp";



  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {grad_grad_x_, grad_grad_y_, grad_grad_out_grad_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_axis = pir::Int32Attribute::get(pir::IrContext::Instance(), axis);
  argument.AddAttribute("axis", attr_axis);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType grad_grad_x = grad_grad_x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)grad_grad_x;
  paddle::dialect::DenseTensorType grad_grad_y = grad_grad_y_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)grad_grad_y;
  paddle::dialect::DenseTensorType grad_grad_out_grad = grad_grad_out_grad_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)grad_grad_out_grad;

  VLOG(4) << "Builder construction  dense_grad_grad_x";
  paddle::dialect::IrTensor ir_tensor_grad_grad_x(paddle::dialect::TransToPhiDataType(grad_grad_x.dtype()),
                                                      grad_grad_x.dims(),
                                                      grad_grad_x.data_layout(),
                                                      grad_grad_x.lod(),
                                                      grad_grad_x.offset());
  VLOG(4) << "Builder construction  meta_grad_grad_x";
  paddle::dialect::IrMetaTensor meta_grad_grad_x(&ir_tensor_grad_grad_x);

  VLOG(4) << "Builder construction  dense_grad_grad_y";
  paddle::dialect::IrTensor ir_tensor_grad_grad_y(paddle::dialect::TransToPhiDataType(grad_grad_y.dtype()),
                                                      grad_grad_y.dims(),
                                                      grad_grad_y.data_layout(),
                                                      grad_grad_y.lod(),
                                                      grad_grad_y.offset());
  VLOG(4) << "Builder construction  meta_grad_grad_y";
  paddle::dialect::IrMetaTensor meta_grad_grad_y(&ir_tensor_grad_grad_y);
  paddle::dialect::IrTensor dense_grad_grad_x_grad;
  paddle::dialect::IrMetaTensor meta_grad_grad_x_grad(&dense_grad_grad_x_grad);
  paddle::dialect::IrTensor dense_grad_grad_y_grad;
  paddle::dialect::IrMetaTensor meta_grad_grad_y_grad(&dense_grad_grad_y_grad);

  phi::GeneralBinaryGradInferMeta(meta_grad_grad_x, meta_grad_grad_y, &meta_grad_grad_x_grad, &meta_grad_grad_y_grad);

  std::vector<pir::Type> argument_outputs;
  pir::Type grad_grad_x_grad_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_grad_grad_x_grad.dtype()), dense_grad_grad_x_grad.dims(), dense_grad_grad_x_grad.layout(), dense_grad_grad_x_grad.lod(), dense_grad_grad_x_grad.offset());
  argument_outputs.push_back(grad_grad_x_grad_dense_tensor_type);

  pir::Type grad_grad_y_grad_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_grad_grad_y_grad.dtype()), dense_grad_grad_y_grad.dims(), dense_grad_grad_y_grad.layout(), dense_grad_grad_y_grad.lod(), dense_grad_grad_y_grad.offset());
  argument_outputs.push_back(grad_grad_y_grad_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void AddTripleGradOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value grad_grad_x_, pir::Value grad_grad_y_, pir::Value grad_grad_out_grad_, pir::AttributeMap attributes) {
  VLOG(4) << "Start build AddTripleGradOp";


  IR_ENFORCE(
      attributes.find("axis") != attributes.end(),
          "'axis' Attribute is expected for AddTripleGradOp. ");
  int axis = attributes.at("axis").dyn_cast<pir::Int32Attribute>().data();


  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {grad_grad_x_, grad_grad_y_, grad_grad_out_grad_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_axis = pir::Int32Attribute::get(pir::IrContext::Instance(), axis);
  argument.AddAttribute("axis", attr_axis);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType grad_grad_x = grad_grad_x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)grad_grad_x;
  paddle::dialect::DenseTensorType grad_grad_y = grad_grad_y_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)grad_grad_y;
  paddle::dialect::DenseTensorType grad_grad_out_grad = grad_grad_out_grad_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)grad_grad_out_grad;

  VLOG(4) << "Builder construction  dense_grad_grad_x";
  paddle::dialect::IrTensor ir_tensor_grad_grad_x(paddle::dialect::TransToPhiDataType(grad_grad_x.dtype()),
                                                      grad_grad_x.dims(),
                                                      grad_grad_x.data_layout(),
                                                      grad_grad_x.lod(),
                                                      grad_grad_x.offset());
  VLOG(4) << "Builder construction  meta_grad_grad_x";
  paddle::dialect::IrMetaTensor meta_grad_grad_x(&ir_tensor_grad_grad_x);

  VLOG(4) << "Builder construction  dense_grad_grad_y";
  paddle::dialect::IrTensor ir_tensor_grad_grad_y(paddle::dialect::TransToPhiDataType(grad_grad_y.dtype()),
                                                      grad_grad_y.dims(),
                                                      grad_grad_y.data_layout(),
                                                      grad_grad_y.lod(),
                                                      grad_grad_y.offset());
  VLOG(4) << "Builder construction  meta_grad_grad_y";
  paddle::dialect::IrMetaTensor meta_grad_grad_y(&ir_tensor_grad_grad_y);
  paddle::dialect::IrTensor dense_grad_grad_x_grad;
  paddle::dialect::IrMetaTensor meta_grad_grad_x_grad(&dense_grad_grad_x_grad);
  paddle::dialect::IrTensor dense_grad_grad_y_grad;
  paddle::dialect::IrMetaTensor meta_grad_grad_y_grad(&dense_grad_grad_y_grad);

  phi::GeneralBinaryGradInferMeta(meta_grad_grad_x, meta_grad_grad_y, &meta_grad_grad_x_grad, &meta_grad_grad_y_grad);

  std::vector<pir::Type> argument_outputs;
  pir::Type grad_grad_x_grad_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_grad_grad_x_grad.dtype()), dense_grad_grad_x_grad.dims(), dense_grad_grad_x_grad.layout(), dense_grad_grad_x_grad.lod(), dense_grad_grad_x_grad.offset());
  argument_outputs.push_back(grad_grad_x_grad_dense_tensor_type);

  pir::Type grad_grad_y_grad_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_grad_grad_y_grad.dtype()), dense_grad_grad_y_grad.dims(), dense_grad_grad_y_grad.layout(), dense_grad_grad_y_grad.lod(), dense_grad_grad_y_grad.offset());
  argument_outputs.push_back(grad_grad_y_grad_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void AddTripleGradOp::VerifySig() {}

void AddTripleGradOp::InferMeta( phi::InferMetaContext *infer_meta ) {
  auto fn = PD_INFER_META(phi::GeneralBinaryGradInferMeta);
  fn(infer_meta);
}

phi::DataType AddTripleGradOp::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: AddTripleGradOp";
  


  // deal complex_promote
  if (framework::IsComplexType(expected_kernel_dtype)) {{
    // only promote inputsâ€™s types when contains complex input
    return tensor_dtype;
  }}

  return expected_kernel_dtype;
}

const char *AddTripleGrad_Op::attributes_name[1] = { "axis" };

OpInfoTuple AddTripleGrad_Op::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("grad_grad_x", "paddle::dialect::DenseTensorType", false, false, false, false), paddle::dialect::OpInputInfo("grad_grad_y", "paddle::dialect::DenseTensorType", false, false, false, false), paddle::dialect::OpInputInfo("grad_grad_out_grad", "paddle::dialect::DenseTensorType", false, false, false, false) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = { paddle::dialect::OpAttributeInfo("axis", "pir::Int32Attribute", "") };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("grad_grad_x_grad", "paddle::dialect::DenseTensorType", false, false), paddle::dialect::OpOutputInfo("grad_grad_y_grad", "paddle::dialect::DenseTensorType", false, false) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("GeneralBinaryGradInferMeta", {"grad_grad_x", "grad_grad_y"}, "add_triple_grad", {"grad_grad_x", "grad_grad_y", "grad_grad_out_grad", "axis"}, {}, {}, {{"grad_grad_x_grad", "grad_grad_out_grad"}}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "add_triple_grad");
}

void AddTripleGrad_Op::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value grad_grad_x_, pir::Value grad_grad_y_, pir::Value grad_grad_out_grad_, int axis) {
  VLOG(4) << "Start build AddTripleGrad_Op";



  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {grad_grad_x_, grad_grad_y_, grad_grad_out_grad_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_axis = pir::Int32Attribute::get(pir::IrContext::Instance(), axis);
  argument.AddAttribute("axis", attr_axis);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType grad_grad_x = grad_grad_x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)grad_grad_x;
  paddle::dialect::DenseTensorType grad_grad_y = grad_grad_y_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)grad_grad_y;
  paddle::dialect::DenseTensorType grad_grad_out_grad = grad_grad_out_grad_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)grad_grad_out_grad;

  VLOG(4) << "Builder construction  dense_grad_grad_x";
  paddle::dialect::IrTensor ir_tensor_grad_grad_x(paddle::dialect::TransToPhiDataType(grad_grad_x.dtype()),
                                                      grad_grad_x.dims(),
                                                      grad_grad_x.data_layout(),
                                                      grad_grad_x.lod(),
                                                      grad_grad_x.offset());
  VLOG(4) << "Builder construction  meta_grad_grad_x";
  paddle::dialect::IrMetaTensor meta_grad_grad_x(&ir_tensor_grad_grad_x);

  VLOG(4) << "Builder construction  dense_grad_grad_y";
  paddle::dialect::IrTensor ir_tensor_grad_grad_y(paddle::dialect::TransToPhiDataType(grad_grad_y.dtype()),
                                                      grad_grad_y.dims(),
                                                      grad_grad_y.data_layout(),
                                                      grad_grad_y.lod(),
                                                      grad_grad_y.offset());
  VLOG(4) << "Builder construction  meta_grad_grad_y";
  paddle::dialect::IrMetaTensor meta_grad_grad_y(&ir_tensor_grad_grad_y);
  paddle::dialect::IrTensor dense_grad_grad_x_grad;
  paddle::dialect::IrMetaTensor meta_grad_grad_x_grad(&dense_grad_grad_x_grad);
  paddle::dialect::IrTensor dense_grad_grad_y_grad;
  paddle::dialect::IrMetaTensor meta_grad_grad_y_grad(&dense_grad_grad_y_grad);

  phi::GeneralBinaryGradInferMeta(meta_grad_grad_x, meta_grad_grad_y, &meta_grad_grad_x_grad, &meta_grad_grad_y_grad);

  std::vector<pir::Type> argument_outputs;
  pir::Type grad_grad_x_grad_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_grad_grad_x_grad.dtype()), dense_grad_grad_x_grad.dims(), dense_grad_grad_x_grad.layout(), dense_grad_grad_x_grad.lod(), dense_grad_grad_x_grad.offset());
  argument_outputs.push_back(grad_grad_x_grad_dense_tensor_type);

  pir::Type grad_grad_y_grad_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_grad_grad_y_grad.dtype()), dense_grad_grad_y_grad.dims(), dense_grad_grad_y_grad.layout(), dense_grad_grad_y_grad.lod(), dense_grad_grad_y_grad.offset());
  argument_outputs.push_back(grad_grad_y_grad_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void AddTripleGrad_Op::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value grad_grad_x_, pir::Value grad_grad_y_, pir::Value grad_grad_out_grad_, pir::AttributeMap attributes) {
  VLOG(4) << "Start build AddTripleGrad_Op";


  IR_ENFORCE(
      attributes.find("axis") != attributes.end(),
          "'axis' Attribute is expected for AddTripleGrad_Op. ");
  int axis = attributes.at("axis").dyn_cast<pir::Int32Attribute>().data();


  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {grad_grad_x_, grad_grad_y_, grad_grad_out_grad_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_axis = pir::Int32Attribute::get(pir::IrContext::Instance(), axis);
  argument.AddAttribute("axis", attr_axis);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType grad_grad_x = grad_grad_x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)grad_grad_x;
  paddle::dialect::DenseTensorType grad_grad_y = grad_grad_y_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)grad_grad_y;
  paddle::dialect::DenseTensorType grad_grad_out_grad = grad_grad_out_grad_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)grad_grad_out_grad;

  VLOG(4) << "Builder construction  dense_grad_grad_x";
  paddle::dialect::IrTensor ir_tensor_grad_grad_x(paddle::dialect::TransToPhiDataType(grad_grad_x.dtype()),
                                                      grad_grad_x.dims(),
                                                      grad_grad_x.data_layout(),
                                                      grad_grad_x.lod(),
                                                      grad_grad_x.offset());
  VLOG(4) << "Builder construction  meta_grad_grad_x";
  paddle::dialect::IrMetaTensor meta_grad_grad_x(&ir_tensor_grad_grad_x);

  VLOG(4) << "Builder construction  dense_grad_grad_y";
  paddle::dialect::IrTensor ir_tensor_grad_grad_y(paddle::dialect::TransToPhiDataType(grad_grad_y.dtype()),
                                                      grad_grad_y.dims(),
                                                      grad_grad_y.data_layout(),
                                                      grad_grad_y.lod(),
                                                      grad_grad_y.offset());
  VLOG(4) << "Builder construction  meta_grad_grad_y";
  paddle::dialect::IrMetaTensor meta_grad_grad_y(&ir_tensor_grad_grad_y);
  paddle::dialect::IrTensor dense_grad_grad_x_grad;
  paddle::dialect::IrMetaTensor meta_grad_grad_x_grad(&dense_grad_grad_x_grad);
  paddle::dialect::IrTensor dense_grad_grad_y_grad;
  paddle::dialect::IrMetaTensor meta_grad_grad_y_grad(&dense_grad_grad_y_grad);

  phi::GeneralBinaryGradInferMeta(meta_grad_grad_x, meta_grad_grad_y, &meta_grad_grad_x_grad, &meta_grad_grad_y_grad);

  std::vector<pir::Type> argument_outputs;
  pir::Type grad_grad_x_grad_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_grad_grad_x_grad.dtype()), dense_grad_grad_x_grad.dims(), dense_grad_grad_x_grad.layout(), dense_grad_grad_x_grad.lod(), dense_grad_grad_x_grad.offset());
  argument_outputs.push_back(grad_grad_x_grad_dense_tensor_type);

  pir::Type grad_grad_y_grad_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_grad_grad_y_grad.dtype()), dense_grad_grad_y_grad.dims(), dense_grad_grad_y_grad.layout(), dense_grad_grad_y_grad.lod(), dense_grad_grad_y_grad.offset());
  argument_outputs.push_back(grad_grad_y_grad_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void AddTripleGrad_Op::VerifySig() {}

void AddTripleGrad_Op::InferMeta( phi::InferMetaContext *infer_meta ) {
  auto fn = PD_INFER_META(phi::GeneralBinaryGradInferMeta);
  fn(infer_meta);
}

phi::DataType AddTripleGrad_Op::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: AddTripleGrad_Op";
  


  // deal complex_promote
  if (framework::IsComplexType(expected_kernel_dtype)) {{
    // only promote inputsâ€™s types when contains complex input
    return tensor_dtype;
  }}

  return expected_kernel_dtype;
}

const char *AmaxGradOp::attributes_name[3] = { "axis", "keepdim", "reduce_all" };

OpInfoTuple AmaxGradOp::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("x", "paddle::dialect::DenseTensorType", false, false, false, false), paddle::dialect::OpInputInfo("out", "paddle::dialect::DenseTensorType", false, false, false, false), paddle::dialect::OpInputInfo("out_grad", "paddle::dialect::DenseTensorType", false, false, false, false) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = { paddle::dialect::OpAttributeInfo("axis", "pir::ArrayAttribute<pir::Int64Attribute>", ""), paddle::dialect::OpAttributeInfo("keepdim", "pir::BoolAttribute", ""), paddle::dialect::OpAttributeInfo("reduce_all", "pir::BoolAttribute", "") };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("x_grad", "paddle::dialect::DenseTensorType", false, false) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("UnchangedInferMeta", {"x"}, "amax_grad", {"x", "out", "out_grad", "axis", "keepdim", "reduce_all"}, {}, {}, {}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "amax_grad");
}

void AmaxGradOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, pir::Value out_, pir::Value out_grad_, const std::vector<int64_t>& axis, bool keepdim, bool reduce_all) {
  VLOG(4) << "Start build AmaxGradOp";



  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_, out_, out_grad_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  std::vector<pir::Attribute> vec_axis;
  for (size_t i = 0; i < static_cast<size_t>(axis.size()); i++) {
      pir::Attribute attr_axis = pir::Int64Attribute::get(pir::IrContext::Instance(), axis[i]);

    vec_axis.push_back(attr_axis);
  }
  pir::Attribute attr_axis = pir::ArrayAttribute::get(pir::IrContext::Instance(), vec_axis);
  argument.AddAttribute("axis", attr_axis);
  pir::Attribute attr_keepdim = pir::BoolAttribute::get(pir::IrContext::Instance(), keepdim);
  argument.AddAttribute("keepdim", attr_keepdim);
  pir::Attribute attr_reduce_all = pir::BoolAttribute::get(pir::IrContext::Instance(), reduce_all);
  argument.AddAttribute("reduce_all", attr_reduce_all);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;
  paddle::dialect::DenseTensorType out = out_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)out;
  paddle::dialect::DenseTensorType out_grad = out_grad_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)out_grad;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);
  paddle::dialect::IrTensor dense_x_grad;
  paddle::dialect::IrMetaTensor meta_x_grad(&dense_x_grad);

  phi::UnchangedInferMeta(meta_x, &meta_x_grad);

  std::vector<pir::Type> argument_outputs;
  pir::Type x_grad_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_x_grad.dtype()), dense_x_grad.dims(), dense_x_grad.layout(), dense_x_grad.lod(), dense_x_grad.offset());
  argument_outputs.push_back(x_grad_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void AmaxGradOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, pir::Value out_, pir::Value out_grad_, pir::AttributeMap attributes) {
  VLOG(4) << "Start build AmaxGradOp";


  IR_ENFORCE(
      attributes.find("axis") != attributes.end(),
          "'axis' Attribute is expected for AmaxGradOp. ");
  std::vector<int64_t> axis;
  for (size_t i = 0; i < attributes.at("axis").dyn_cast<pir::ArrayAttribute>().size(); i++) {
    axis.push_back(attributes.at("axis").dyn_cast<pir::ArrayAttribute>().at(i).dyn_cast<pir::Int64Attribute>().data());
  }

  IR_ENFORCE(
      attributes.find("keepdim") != attributes.end(),
          "'keepdim' Attribute is expected for AmaxGradOp. ");
  bool keepdim = attributes.at("keepdim").dyn_cast<pir::BoolAttribute>().data();

  IR_ENFORCE(
      attributes.find("reduce_all") != attributes.end(),
          "'reduce_all' Attribute is expected for AmaxGradOp. ");
  bool reduce_all = attributes.at("reduce_all").dyn_cast<pir::BoolAttribute>().data();


  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_, out_, out_grad_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  std::vector<pir::Attribute> vec_axis;
  for (size_t i = 0; i < static_cast<size_t>(axis.size()); i++) {
      pir::Attribute attr_axis = pir::Int64Attribute::get(pir::IrContext::Instance(), axis[i]);

    vec_axis.push_back(attr_axis);
  }
  pir::Attribute attr_axis = pir::ArrayAttribute::get(pir::IrContext::Instance(), vec_axis);
  argument.AddAttribute("axis", attr_axis);
  pir::Attribute attr_keepdim = pir::BoolAttribute::get(pir::IrContext::Instance(), keepdim);
  argument.AddAttribute("keepdim", attr_keepdim);
  pir::Attribute attr_reduce_all = pir::BoolAttribute::get(pir::IrContext::Instance(), reduce_all);
  argument.AddAttribute("reduce_all", attr_reduce_all);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;
  paddle::dialect::DenseTensorType out = out_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)out;
  paddle::dialect::DenseTensorType out_grad = out_grad_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)out_grad;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);
  paddle::dialect::IrTensor dense_x_grad;
  paddle::dialect::IrMetaTensor meta_x_grad(&dense_x_grad);

  phi::UnchangedInferMeta(meta_x, &meta_x_grad);

  std::vector<pir::Type> argument_outputs;
  pir::Type x_grad_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_x_grad.dtype()), dense_x_grad.dims(), dense_x_grad.layout(), dense_x_grad.lod(), dense_x_grad.offset());
  argument_outputs.push_back(x_grad_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void AmaxGradOp::VerifySig() {}

void AmaxGradOp::InferMeta( phi::InferMetaContext *infer_meta ) {
  auto fn = PD_INFER_META(phi::UnchangedInferMeta);
  fn(infer_meta);
}

phi::DataType AmaxGradOp::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: AmaxGradOp";
  


  return expected_kernel_dtype;
}

const char *AminGradOp::attributes_name[3] = { "axis", "keepdim", "reduce_all" };

OpInfoTuple AminGradOp::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("x", "paddle::dialect::DenseTensorType", false, false, false, false), paddle::dialect::OpInputInfo("out", "paddle::dialect::DenseTensorType", false, false, false, false), paddle::dialect::OpInputInfo("out_grad", "paddle::dialect::DenseTensorType", false, false, false, false) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = { paddle::dialect::OpAttributeInfo("axis", "pir::ArrayAttribute<pir::Int64Attribute>", ""), paddle::dialect::OpAttributeInfo("keepdim", "pir::BoolAttribute", ""), paddle::dialect::OpAttributeInfo("reduce_all", "pir::BoolAttribute", "") };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("x_grad", "paddle::dialect::DenseTensorType", false, false) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("UnchangedInferMeta", {"x"}, "amin_grad", {"x", "out", "out_grad", "axis", "keepdim", "reduce_all"}, {}, {}, {}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "amin_grad");
}

void AminGradOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, pir::Value out_, pir::Value out_grad_, const std::vector<int64_t>& axis, bool keepdim, bool reduce_all) {
  VLOG(4) << "Start build AminGradOp";



  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_, out_, out_grad_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  std::vector<pir::Attribute> vec_axis;
  for (size_t i = 0; i < static_cast<size_t>(axis.size()); i++) {
      pir::Attribute attr_axis = pir::Int64Attribute::get(pir::IrContext::Instance(), axis[i]);

    vec_axis.push_back(attr_axis);
  }
  pir::Attribute attr_axis = pir::ArrayAttribute::get(pir::IrContext::Instance(), vec_axis);
  argument.AddAttribute("axis", attr_axis);
  pir::Attribute attr_keepdim = pir::BoolAttribute::get(pir::IrContext::Instance(), keepdim);
  argument.AddAttribute("keepdim", attr_keepdim);
  pir::Attribute attr_reduce_all = pir::BoolAttribute::get(pir::IrContext::Instance(), reduce_all);
  argument.AddAttribute("reduce_all", attr_reduce_all);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;
  paddle::dialect::DenseTensorType out = out_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)out;
  paddle::dialect::DenseTensorType out_grad = out_grad_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)out_grad;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);
  paddle::dialect::IrTensor dense_x_grad;
  paddle::dialect::IrMetaTensor meta_x_grad(&dense_x_grad);

  phi::UnchangedInferMeta(meta_x, &meta_x_grad);

  std::vector<pir::Type> argument_outputs;
  pir::Type x_grad_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_x_grad.dtype()), dense_x_grad.dims(), dense_x_grad.layout(), dense_x_grad.lod(), dense_x_grad.offset());
  argument_outputs.push_back(x_grad_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void AminGradOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, pir::Value out_, pir::Value out_grad_, pir::AttributeMap attributes) {
  VLOG(4) << "Start build AminGradOp";


  IR_ENFORCE(
      attributes.find("axis") != attributes.end(),
          "'axis' Attribute is expected for AminGradOp. ");
  std::vector<int64_t> axis;
  for (size_t i = 0; i < attributes.at("axis").dyn_cast<pir::ArrayAttribute>().size(); i++) {
    axis.push_back(attributes.at("axis").dyn_cast<pir::ArrayAttribute>().at(i).dyn_cast<pir::Int64Attribute>().data());
  }

  IR_ENFORCE(
      attributes.find("keepdim") != attributes.end(),
          "'keepdim' Attribute is expected for AminGradOp. ");
  bool keepdim = attributes.at("keepdim").dyn_cast<pir::BoolAttribute>().data();

  IR_ENFORCE(
      attributes.find("reduce_all") != attributes.end(),
          "'reduce_all' Attribute is expected for AminGradOp. ");
  bool reduce_all = attributes.at("reduce_all").dyn_cast<pir::BoolAttribute>().data();


  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_, out_, out_grad_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  std::vector<pir::Attribute> vec_axis;
  for (size_t i = 0; i < static_cast<size_t>(axis.size()); i++) {
      pir::Attribute attr_axis = pir::Int64Attribute::get(pir::IrContext::Instance(), axis[i]);

    vec_axis.push_back(attr_axis);
  }
  pir::Attribute attr_axis = pir::ArrayAttribute::get(pir::IrContext::Instance(), vec_axis);
  argument.AddAttribute("axis", attr_axis);
  pir::Attribute attr_keepdim = pir::BoolAttribute::get(pir::IrContext::Instance(), keepdim);
  argument.AddAttribute("keepdim", attr_keepdim);
  pir::Attribute attr_reduce_all = pir::BoolAttribute::get(pir::IrContext::Instance(), reduce_all);
  argument.AddAttribute("reduce_all", attr_reduce_all);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;
  paddle::dialect::DenseTensorType out = out_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)out;
  paddle::dialect::DenseTensorType out_grad = out_grad_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)out_grad;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);
  paddle::dialect::IrTensor dense_x_grad;
  paddle::dialect::IrMetaTensor meta_x_grad(&dense_x_grad);

  phi::UnchangedInferMeta(meta_x, &meta_x_grad);

  std::vector<pir::Type> argument_outputs;
  pir::Type x_grad_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_x_grad.dtype()), dense_x_grad.dims(), dense_x_grad.layout(), dense_x_grad.lod(), dense_x_grad.offset());
  argument_outputs.push_back(x_grad_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void AminGradOp::VerifySig() {}

void AminGradOp::InferMeta( phi::InferMetaContext *infer_meta ) {
  auto fn = PD_INFER_META(phi::UnchangedInferMeta);
  fn(infer_meta);
}

phi::DataType AminGradOp::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: AminGradOp";
  


  return expected_kernel_dtype;
}

OpInfoTuple AssignGradOp::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("out_grad", "paddle::dialect::DenseTensorType", false, false, false, false) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = {  };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("x_grad", "paddle::dialect::DenseTensorType", false, false) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("", {""}, "", {""}, {}, {}, {}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "assign_grad");
}

void AssignGradOp::VerifySig() {}

phi::DataType AssignGradOp::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: AssignGradOp";
  


  return expected_kernel_dtype;
}

OpInfoTuple AssignOutGradOp::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("out_grad", "paddle::dialect::DenseTensorType", false, false, false, false) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = {  };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("x_grad", "paddle::dialect::DenseTensorType", false, false) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("UnchangedInferMeta", {"out_grad"}, "assign", {"out_grad"}, {}, {}, {}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "assign_out__grad");
}

void AssignOutGradOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value out_grad_) {
  VLOG(4) << "Start build AssignOutGradOp";



  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {out_grad_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType out_grad = out_grad_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)out_grad;

  VLOG(4) << "Builder construction  dense_out_grad";
  paddle::dialect::IrTensor ir_tensor_out_grad(paddle::dialect::TransToPhiDataType(out_grad.dtype()),
                                                      out_grad.dims(),
                                                      out_grad.data_layout(),
                                                      out_grad.lod(),
                                                      out_grad.offset());
  VLOG(4) << "Builder construction  meta_out_grad";
  paddle::dialect::IrMetaTensor meta_out_grad(&ir_tensor_out_grad);
  paddle::dialect::IrTensor dense_x_grad;
  paddle::dialect::IrMetaTensor meta_x_grad(&dense_x_grad);

  phi::UnchangedInferMeta(meta_out_grad, &meta_x_grad);

  std::vector<pir::Type> argument_outputs;
  pir::Type x_grad_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_x_grad.dtype()), dense_x_grad.dims(), dense_x_grad.layout(), dense_x_grad.lod(), dense_x_grad.offset());
  argument_outputs.push_back(x_grad_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void AssignOutGradOp::VerifySig() {}

void AssignOutGradOp::InferMeta( phi::InferMetaContext *infer_meta ) {
  auto fn = PD_INFER_META(phi::UnchangedInferMeta);
  fn(infer_meta);
}

phi::DataType AssignOutGradOp::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: AssignOutGradOp";
  


  return expected_kernel_dtype;
}

OpInfoTuple AssignOutGrad_Op::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("out_grad", "paddle::dialect::DenseTensorType", false, false, false, false) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = {  };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("x_grad", "paddle::dialect::DenseTensorType", false, false) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("UnchangedInferMeta", {"out_grad"}, "assign", {"out_grad"}, {}, {}, {{"x_grad", "out_grad"}}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "assign_out__grad");
}

void AssignOutGrad_Op::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value out_grad_) {
  VLOG(4) << "Start build AssignOutGrad_Op";



  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {out_grad_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType out_grad = out_grad_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)out_grad;

  VLOG(4) << "Builder construction  dense_out_grad";
  paddle::dialect::IrTensor ir_tensor_out_grad(paddle::dialect::TransToPhiDataType(out_grad.dtype()),
                                                      out_grad.dims(),
                                                      out_grad.data_layout(),
                                                      out_grad.lod(),
                                                      out_grad.offset());
  VLOG(4) << "Builder construction  meta_out_grad";
  paddle::dialect::IrMetaTensor meta_out_grad(&ir_tensor_out_grad);
  paddle::dialect::IrTensor dense_x_grad;
  paddle::dialect::IrMetaTensor meta_x_grad(&dense_x_grad);

  phi::UnchangedInferMeta(meta_out_grad, &meta_x_grad);

  std::vector<pir::Type> argument_outputs;
  pir::Type x_grad_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_x_grad.dtype()), dense_x_grad.dims(), dense_x_grad.layout(), dense_x_grad.lod(), dense_x_grad.offset());
  argument_outputs.push_back(x_grad_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void AssignOutGrad_Op::VerifySig() {}

void AssignOutGrad_Op::InferMeta( phi::InferMetaContext *infer_meta ) {
  auto fn = PD_INFER_META(phi::UnchangedInferMeta);
  fn(infer_meta);
}

phi::DataType AssignOutGrad_Op::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: AssignOutGrad_Op";
  


  return expected_kernel_dtype;
}

const char *BatchNormDoubleGradOp::attributes_name[6] = { "momentum", "epsilon", "data_layout", "is_test", "use_global_stats", "trainable_statistics" };

OpInfoTuple BatchNormDoubleGradOp::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("x", "paddle::dialect::DenseTensorType", false, false, false, false), paddle::dialect::OpInputInfo("scale", "paddle::dialect::DenseTensorType", true, false, false, false), paddle::dialect::OpInputInfo("out_mean", "paddle::dialect::DenseTensorType", true, false, false, false), paddle::dialect::OpInputInfo("out_variance", "paddle::dialect::DenseTensorType", true, false, false, false), paddle::dialect::OpInputInfo("saved_mean", "paddle::dialect::DenseTensorType", false, false, false, false), paddle::dialect::OpInputInfo("saved_variance", "paddle::dialect::DenseTensorType", false, false, false, false), paddle::dialect::OpInputInfo("grad_out", "paddle::dialect::DenseTensorType", false, false, false, false), paddle::dialect::OpInputInfo("grad_x_grad", "paddle::dialect::DenseTensorType", true, false, false, false), paddle::dialect::OpInputInfo("grad_scale_grad", "paddle::dialect::DenseTensorType", true, false, false, false), paddle::dialect::OpInputInfo("grad_bias_grad", "paddle::dialect::DenseTensorType", true, false, false, false) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = { paddle::dialect::OpAttributeInfo("momentum", "pir::FloatAttribute", ""), paddle::dialect::OpAttributeInfo("epsilon", "pir::FloatAttribute", ""), paddle::dialect::OpAttributeInfo("data_layout", "pir::StrAttribute", ""), paddle::dialect::OpAttributeInfo("is_test", "pir::BoolAttribute", ""), paddle::dialect::OpAttributeInfo("use_global_stats", "pir::BoolAttribute", ""), paddle::dialect::OpAttributeInfo("trainable_statistics", "pir::BoolAttribute", "") };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("x_grad", "paddle::dialect::DenseTensorType", false, false), paddle::dialect::OpOutputInfo("scale_grad", "paddle::dialect::DenseTensorType", false, false), paddle::dialect::OpOutputInfo("grad_out_grad", "paddle::dialect::DenseTensorType", false, false) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("GeneralTernaryGradInferMeta", {"x", "scale", "x"}, "batch_norm_double_grad", {"x", "scale", "out_mean", "out_variance", "saved_mean", "saved_variance", "grad_out", "grad_x_grad", "grad_scale_grad", "grad_bias_grad", "momentum", "epsilon", "data_layout", "is_test", "use_global_stats", "trainable_statistics"}, {"x"}, {}, {}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "batch_norm_double_grad");
}

void BatchNormDoubleGradOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, pir::Value scale_, pir::Value out_mean_, pir::Value out_variance_, pir::Value saved_mean_, pir::Value saved_variance_, pir::Value grad_out_, pir::Value grad_x_grad_, pir::Value grad_scale_grad_, pir::Value grad_bias_grad_, float momentum, float epsilon, const std::string& data_layout, bool is_test, bool use_global_stats, bool trainable_statistics) {
  VLOG(4) << "Start build BatchNormDoubleGradOp";



  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_, scale_, out_mean_, out_variance_, saved_mean_, saved_variance_, grad_out_, grad_x_grad_, grad_scale_grad_, grad_bias_grad_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_momentum = pir::FloatAttribute::get(pir::IrContext::Instance(), momentum);
  argument.AddAttribute("momentum", attr_momentum);
  pir::Attribute attr_epsilon = pir::FloatAttribute::get(pir::IrContext::Instance(), epsilon);
  argument.AddAttribute("epsilon", attr_epsilon);
  pir::Attribute attr_data_layout = pir::StrAttribute::get(pir::IrContext::Instance(), data_layout);
  argument.AddAttribute("data_layout", attr_data_layout);
  pir::Attribute attr_is_test = pir::BoolAttribute::get(pir::IrContext::Instance(), is_test);
  argument.AddAttribute("is_test", attr_is_test);
  pir::Attribute attr_use_global_stats = pir::BoolAttribute::get(pir::IrContext::Instance(), use_global_stats);
  argument.AddAttribute("use_global_stats", attr_use_global_stats);
  pir::Attribute attr_trainable_statistics = pir::BoolAttribute::get(pir::IrContext::Instance(), trainable_statistics);
  argument.AddAttribute("trainable_statistics", attr_trainable_statistics);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;
  paddle::dialect::DenseTensorType saved_mean = saved_mean_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)saved_mean;
  paddle::dialect::DenseTensorType saved_variance = saved_variance_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)saved_variance;
  paddle::dialect::DenseTensorType grad_out = grad_out_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)grad_out;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);

  paddle::dialect::IrMetaTensor meta_scale;
  paddle::dialect::IrTensor ir_tensor_scale;
  if (scale_.impl() != nullptr) {
    paddle::dialect::DenseTensorType scale = scale_.type().dyn_cast<paddle::dialect::DenseTensorType>();
    VLOG(4) << "Builder construction  dense_scale";
    ir_tensor_scale = paddle::dialect::IrTensor(paddle::dialect::TransToPhiDataType(scale.dtype()),
                                                        scale.dims(),
                                                        scale.data_layout(),
                                                        scale.lod(),
                                                        scale.offset());
    VLOG(4) << "Builder construction  meta_scale";
    meta_scale = paddle::dialect::IrMetaTensor(&ir_tensor_scale);
  }

  paddle::dialect::IrTensor dense_x_grad;
  paddle::dialect::IrMetaTensor meta_x_grad(&dense_x_grad);
  paddle::dialect::IrTensor dense_scale_grad;
  paddle::dialect::IrMetaTensor meta_scale_grad(&dense_scale_grad);
  paddle::dialect::IrTensor dense_grad_out_grad;
  paddle::dialect::IrMetaTensor meta_grad_out_grad(&dense_grad_out_grad);

  phi::GeneralTernaryGradInferMeta(meta_x, meta_scale, meta_x, &meta_x_grad, &meta_scale_grad, &meta_grad_out_grad);

  std::vector<pir::Type> argument_outputs;
  pir::Type x_grad_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_x_grad.dtype()), dense_x_grad.dims(), dense_x_grad.layout(), dense_x_grad.lod(), dense_x_grad.offset());
  argument_outputs.push_back(x_grad_dense_tensor_type);

  pir::Type scale_grad_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_scale_grad.dtype()), dense_scale_grad.dims(), dense_scale_grad.layout(), dense_scale_grad.lod(), dense_scale_grad.offset());
  argument_outputs.push_back(scale_grad_dense_tensor_type);

  pir::Type grad_out_grad_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_grad_out_grad.dtype()), dense_grad_out_grad.dims(), dense_grad_out_grad.layout(), dense_grad_out_grad.lod(), dense_grad_out_grad.offset());
  argument_outputs.push_back(grad_out_grad_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void BatchNormDoubleGradOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, pir::Value scale_, pir::Value out_mean_, pir::Value out_variance_, pir::Value saved_mean_, pir::Value saved_variance_, pir::Value grad_out_, pir::Value grad_x_grad_, pir::Value grad_scale_grad_, pir::Value grad_bias_grad_, pir::AttributeMap attributes) {
  VLOG(4) << "Start build BatchNormDoubleGradOp";


  IR_ENFORCE(
      attributes.find("momentum") != attributes.end(),
          "'momentum' Attribute is expected for BatchNormDoubleGradOp. ");
  float momentum = attributes.at("momentum").dyn_cast<pir::FloatAttribute>().data();

  IR_ENFORCE(
      attributes.find("epsilon") != attributes.end(),
          "'epsilon' Attribute is expected for BatchNormDoubleGradOp. ");
  float epsilon = attributes.at("epsilon").dyn_cast<pir::FloatAttribute>().data();

  IR_ENFORCE(
      attributes.find("data_layout") != attributes.end(),
          "'data_layout' Attribute is expected for BatchNormDoubleGradOp. ");
  std::string data_layout = attributes.at("data_layout").dyn_cast<pir::StrAttribute>().AsString();

  IR_ENFORCE(
      attributes.find("is_test") != attributes.end(),
          "'is_test' Attribute is expected for BatchNormDoubleGradOp. ");
  bool is_test = attributes.at("is_test").dyn_cast<pir::BoolAttribute>().data();

  IR_ENFORCE(
      attributes.find("use_global_stats") != attributes.end(),
          "'use_global_stats' Attribute is expected for BatchNormDoubleGradOp. ");
  bool use_global_stats = attributes.at("use_global_stats").dyn_cast<pir::BoolAttribute>().data();

  IR_ENFORCE(
      attributes.find("trainable_statistics") != attributes.end(),
          "'trainable_statistics' Attribute is expected for BatchNormDoubleGradOp. ");
  bool trainable_statistics = attributes.at("trainable_statistics").dyn_cast<pir::BoolAttribute>().data();


  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_, scale_, out_mean_, out_variance_, saved_mean_, saved_variance_, grad_out_, grad_x_grad_, grad_scale_grad_, grad_bias_grad_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_momentum = pir::FloatAttribute::get(pir::IrContext::Instance(), momentum);
  argument.AddAttribute("momentum", attr_momentum);
  pir::Attribute attr_epsilon = pir::FloatAttribute::get(pir::IrContext::Instance(), epsilon);
  argument.AddAttribute("epsilon", attr_epsilon);
  pir::Attribute attr_data_layout = pir::StrAttribute::get(pir::IrContext::Instance(), data_layout);
  argument.AddAttribute("data_layout", attr_data_layout);
  pir::Attribute attr_is_test = pir::BoolAttribute::get(pir::IrContext::Instance(), is_test);
  argument.AddAttribute("is_test", attr_is_test);
  pir::Attribute attr_use_global_stats = pir::BoolAttribute::get(pir::IrContext::Instance(), use_global_stats);
  argument.AddAttribute("use_global_stats", attr_use_global_stats);
  pir::Attribute attr_trainable_statistics = pir::BoolAttribute::get(pir::IrContext::Instance(), trainable_statistics);
  argument.AddAttribute("trainable_statistics", attr_trainable_statistics);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;
  paddle::dialect::DenseTensorType saved_mean = saved_mean_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)saved_mean;
  paddle::dialect::DenseTensorType saved_variance = saved_variance_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)saved_variance;
  paddle::dialect::DenseTensorType grad_out = grad_out_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)grad_out;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);

  paddle::dialect::IrMetaTensor meta_scale;
  paddle::dialect::IrTensor ir_tensor_scale;
  if (scale_.impl() != nullptr) {
    paddle::dialect::DenseTensorType scale = scale_.type().dyn_cast<paddle::dialect::DenseTensorType>();
    VLOG(4) << "Builder construction  dense_scale";
    ir_tensor_scale = paddle::dialect::IrTensor(paddle::dialect::TransToPhiDataType(scale.dtype()),
                                                        scale.dims(),
                                                        scale.data_layout(),
                                                        scale.lod(),
                                                        scale.offset());
    VLOG(4) << "Builder construction  meta_scale";
    meta_scale = paddle::dialect::IrMetaTensor(&ir_tensor_scale);
  }

  paddle::dialect::IrTensor dense_x_grad;
  paddle::dialect::IrMetaTensor meta_x_grad(&dense_x_grad);
  paddle::dialect::IrTensor dense_scale_grad;
  paddle::dialect::IrMetaTensor meta_scale_grad(&dense_scale_grad);
  paddle::dialect::IrTensor dense_grad_out_grad;
  paddle::dialect::IrMetaTensor meta_grad_out_grad(&dense_grad_out_grad);

  phi::GeneralTernaryGradInferMeta(meta_x, meta_scale, meta_x, &meta_x_grad, &meta_scale_grad, &meta_grad_out_grad);

  std::vector<pir::Type> argument_outputs;
  pir::Type x_grad_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_x_grad.dtype()), dense_x_grad.dims(), dense_x_grad.layout(), dense_x_grad.lod(), dense_x_grad.offset());
  argument_outputs.push_back(x_grad_dense_tensor_type);

  pir::Type scale_grad_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_scale_grad.dtype()), dense_scale_grad.dims(), dense_scale_grad.layout(), dense_scale_grad.lod(), dense_scale_grad.offset());
  argument_outputs.push_back(scale_grad_dense_tensor_type);

  pir::Type grad_out_grad_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_grad_out_grad.dtype()), dense_grad_out_grad.dims(), dense_grad_out_grad.layout(), dense_grad_out_grad.lod(), dense_grad_out_grad.offset());
  argument_outputs.push_back(grad_out_grad_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void BatchNormDoubleGradOp::VerifySig() {}

void BatchNormDoubleGradOp::InferMeta( phi::InferMetaContext *infer_meta ) {
  auto fn = PD_INFER_META(phi::GeneralTernaryGradInferMeta);
  fn(infer_meta);
}

phi::DataType BatchNormDoubleGradOp::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: BatchNormDoubleGradOp";
  


  return expected_kernel_dtype;
}

const char *BatchNormDoubleGrad_Op::attributes_name[6] = { "momentum", "epsilon", "data_layout", "is_test", "use_global_stats", "trainable_statistics" };

OpInfoTuple BatchNormDoubleGrad_Op::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("x", "paddle::dialect::DenseTensorType", false, false, false, false), paddle::dialect::OpInputInfo("scale", "paddle::dialect::DenseTensorType", true, false, false, false), paddle::dialect::OpInputInfo("out_mean", "paddle::dialect::DenseTensorType", true, false, false, false), paddle::dialect::OpInputInfo("out_variance", "paddle::dialect::DenseTensorType", true, false, false, false), paddle::dialect::OpInputInfo("saved_mean", "paddle::dialect::DenseTensorType", false, false, false, false), paddle::dialect::OpInputInfo("saved_variance", "paddle::dialect::DenseTensorType", false, false, false, false), paddle::dialect::OpInputInfo("grad_out", "paddle::dialect::DenseTensorType", false, false, false, false), paddle::dialect::OpInputInfo("grad_x_grad", "paddle::dialect::DenseTensorType", true, false, false, false), paddle::dialect::OpInputInfo("grad_scale_grad", "paddle::dialect::DenseTensorType", true, false, false, false), paddle::dialect::OpInputInfo("grad_bias_grad", "paddle::dialect::DenseTensorType", true, false, false, false) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = { paddle::dialect::OpAttributeInfo("momentum", "pir::FloatAttribute", ""), paddle::dialect::OpAttributeInfo("epsilon", "pir::FloatAttribute", ""), paddle::dialect::OpAttributeInfo("data_layout", "pir::StrAttribute", ""), paddle::dialect::OpAttributeInfo("is_test", "pir::BoolAttribute", ""), paddle::dialect::OpAttributeInfo("use_global_stats", "pir::BoolAttribute", ""), paddle::dialect::OpAttributeInfo("trainable_statistics", "pir::BoolAttribute", "") };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("x_grad", "paddle::dialect::DenseTensorType", false, false), paddle::dialect::OpOutputInfo("scale_grad", "paddle::dialect::DenseTensorType", false, false), paddle::dialect::OpOutputInfo("grad_out_grad", "paddle::dialect::DenseTensorType", false, false) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("GeneralTernaryGradInferMeta", {"x", "scale", "x"}, "batch_norm_double_grad", {"x", "scale", "out_mean", "out_variance", "saved_mean", "saved_variance", "grad_out", "grad_x_grad", "grad_scale_grad", "grad_bias_grad", "momentum", "epsilon", "data_layout", "is_test", "use_global_stats", "trainable_statistics"}, {"x"}, {}, {{"grad_out_grad", "grad_out"}}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "batch_norm_double_grad");
}

void BatchNormDoubleGrad_Op::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, pir::Value scale_, pir::Value out_mean_, pir::Value out_variance_, pir::Value saved_mean_, pir::Value saved_variance_, pir::Value grad_out_, pir::Value grad_x_grad_, pir::Value grad_scale_grad_, pir::Value grad_bias_grad_, float momentum, float epsilon, const std::string& data_layout, bool is_test, bool use_global_stats, bool trainable_statistics) {
  VLOG(4) << "Start build BatchNormDoubleGrad_Op";



  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_, scale_, out_mean_, out_variance_, saved_mean_, saved_variance_, grad_out_, grad_x_grad_, grad_scale_grad_, grad_bias_grad_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_momentum = pir::FloatAttribute::get(pir::IrContext::Instance(), momentum);
  argument.AddAttribute("momentum", attr_momentum);
  pir::Attribute attr_epsilon = pir::FloatAttribute::get(pir::IrContext::Instance(), epsilon);
  argument.AddAttribute("epsilon", attr_epsilon);
  pir::Attribute attr_data_layout = pir::StrAttribute::get(pir::IrContext::Instance(), data_layout);
  argument.AddAttribute("data_layout", attr_data_layout);
  pir::Attribute attr_is_test = pir::BoolAttribute::get(pir::IrContext::Instance(), is_test);
  argument.AddAttribute("is_test", attr_is_test);
  pir::Attribute attr_use_global_stats = pir::BoolAttribute::get(pir::IrContext::Instance(), use_global_stats);
  argument.AddAttribute("use_global_stats", attr_use_global_stats);
  pir::Attribute attr_trainable_statistics = pir::BoolAttribute::get(pir::IrContext::Instance(), trainable_statistics);
  argument.AddAttribute("trainable_statistics", attr_trainable_statistics);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;
  paddle::dialect::DenseTensorType saved_mean = saved_mean_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)saved_mean;
  paddle::dialect::DenseTensorType saved_variance = saved_variance_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)saved_variance;
  paddle::dialect::DenseTensorType grad_out = grad_out_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)grad_out;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);

  paddle::dialect::IrMetaTensor meta_scale;
  paddle::dialect::IrTensor ir_tensor_scale;
  if (scale_.impl() != nullptr) {
    paddle::dialect::DenseTensorType scale = scale_.type().dyn_cast<paddle::dialect::DenseTensorType>();
    VLOG(4) << "Builder construction  dense_scale";
    ir_tensor_scale = paddle::dialect::IrTensor(paddle::dialect::TransToPhiDataType(scale.dtype()),
                                                        scale.dims(),
                                                        scale.data_layout(),
                                                        scale.lod(),
                                                        scale.offset());
    VLOG(4) << "Builder construction  meta_scale";
    meta_scale = paddle::dialect::IrMetaTensor(&ir_tensor_scale);
  }

  paddle::dialect::IrTensor dense_x_grad;
  paddle::dialect::IrMetaTensor meta_x_grad(&dense_x_grad);
  paddle::dialect::IrTensor dense_scale_grad;
  paddle::dialect::IrMetaTensor meta_scale_grad(&dense_scale_grad);
  paddle::dialect::IrTensor dense_grad_out_grad;
  paddle::dialect::IrMetaTensor meta_grad_out_grad(&dense_grad_out_grad);

  phi::GeneralTernaryGradInferMeta(meta_x, meta_scale, meta_x, &meta_x_grad, &meta_scale_grad, &meta_grad_out_grad);

  std::vector<pir::Type> argument_outputs;
  pir::Type x_grad_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_x_grad.dtype()), dense_x_grad.dims(), dense_x_grad.layout(), dense_x_grad.lod(), dense_x_grad.offset());
  argument_outputs.push_back(x_grad_dense_tensor_type);

  pir::Type scale_grad_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_scale_grad.dtype()), dense_scale_grad.dims(), dense_scale_grad.layout(), dense_scale_grad.lod(), dense_scale_grad.offset());
  argument_outputs.push_back(scale_grad_dense_tensor_type);

  pir::Type grad_out_grad_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_grad_out_grad.dtype()), dense_grad_out_grad.dims(), dense_grad_out_grad.layout(), dense_grad_out_grad.lod(), dense_grad_out_grad.offset());
  argument_outputs.push_back(grad_out_grad_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void BatchNormDoubleGrad_Op::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, pir::Value scale_, pir::Value out_mean_, pir::Value out_variance_, pir::Value saved_mean_, pir::Value saved_variance_, pir::Value grad_out_, pir::Value grad_x_grad_, pir::Value grad_scale_grad_, pir::Value grad_bias_grad_, pir::AttributeMap attributes) {
  VLOG(4) << "Start build BatchNormDoubleGrad_Op";


  IR_ENFORCE(
      attributes.find("momentum") != attributes.end(),
          "'momentum' Attribute is expected for BatchNormDoubleGrad_Op. ");
  float momentum = attributes.at("momentum").dyn_cast<pir::FloatAttribute>().data();

  IR_ENFORCE(
      attributes.find("epsilon") != attributes.end(),
          "'epsilon' Attribute is expected for BatchNormDoubleGrad_Op. ");
  float epsilon = attributes.at("epsilon").dyn_cast<pir::FloatAttribute>().data();

  IR_ENFORCE(
      attributes.find("data_layout") != attributes.end(),
          "'data_layout' Attribute is expected for BatchNormDoubleGrad_Op. ");
  std::string data_layout = attributes.at("data_layout").dyn_cast<pir::StrAttribute>().AsString();

  IR_ENFORCE(
      attributes.find("is_test") != attributes.end(),
          "'is_test' Attribute is expected for BatchNormDoubleGrad_Op. ");
  bool is_test = attributes.at("is_test").dyn_cast<pir::BoolAttribute>().data();

  IR_ENFORCE(
      attributes.find("use_global_stats") != attributes.end(),
          "'use_global_stats' Attribute is expected for BatchNormDoubleGrad_Op. ");
  bool use_global_stats = attributes.at("use_global_stats").dyn_cast<pir::BoolAttribute>().data();

  IR_ENFORCE(
      attributes.find("trainable_statistics") != attributes.end(),
          "'trainable_statistics' Attribute is expected for BatchNormDoubleGrad_Op. ");
  bool trainable_statistics = attributes.at("trainable_statistics").dyn_cast<pir::BoolAttribute>().data();


  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_, scale_, out_mean_, out_variance_, saved_mean_, saved_variance_, grad_out_, grad_x_grad_, grad_scale_grad_, grad_bias_grad_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_momentum = pir::FloatAttribute::get(pir::IrContext::Instance(), momentum);
  argument.AddAttribute("momentum", attr_momentum);
  pir::Attribute attr_epsilon = pir::FloatAttribute::get(pir::IrContext::Instance(), epsilon);
  argument.AddAttribute("epsilon", attr_epsilon);
  pir::Attribute attr_data_layout = pir::StrAttribute::get(pir::IrContext::Instance(), data_layout);
  argument.AddAttribute("data_layout", attr_data_layout);
  pir::Attribute attr_is_test = pir::BoolAttribute::get(pir::IrContext::Instance(), is_test);
  argument.AddAttribute("is_test", attr_is_test);
  pir::Attribute attr_use_global_stats = pir::BoolAttribute::get(pir::IrContext::Instance(), use_global_stats);
  argument.AddAttribute("use_global_stats", attr_use_global_stats);
  pir::Attribute attr_trainable_statistics = pir::BoolAttribute::get(pir::IrContext::Instance(), trainable_statistics);
  argument.AddAttribute("trainable_statistics", attr_trainable_statistics);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;
  paddle::dialect::DenseTensorType saved_mean = saved_mean_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)saved_mean;
  paddle::dialect::DenseTensorType saved_variance = saved_variance_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)saved_variance;
  paddle::dialect::DenseTensorType grad_out = grad_out_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)grad_out;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);

  paddle::dialect::IrMetaTensor meta_scale;
  paddle::dialect::IrTensor ir_tensor_scale;
  if (scale_.impl() != nullptr) {
    paddle::dialect::DenseTensorType scale = scale_.type().dyn_cast<paddle::dialect::DenseTensorType>();
    VLOG(4) << "Builder construction  dense_scale";
    ir_tensor_scale = paddle::dialect::IrTensor(paddle::dialect::TransToPhiDataType(scale.dtype()),
                                                        scale.dims(),
                                                        scale.data_layout(),
                                                        scale.lod(),
                                                        scale.offset());
    VLOG(4) << "Builder construction  meta_scale";
    meta_scale = paddle::dialect::IrMetaTensor(&ir_tensor_scale);
  }

  paddle::dialect::IrTensor dense_x_grad;
  paddle::dialect::IrMetaTensor meta_x_grad(&dense_x_grad);
  paddle::dialect::IrTensor dense_scale_grad;
  paddle::dialect::IrMetaTensor meta_scale_grad(&dense_scale_grad);
  paddle::dialect::IrTensor dense_grad_out_grad;
  paddle::dialect::IrMetaTensor meta_grad_out_grad(&dense_grad_out_grad);

  phi::GeneralTernaryGradInferMeta(meta_x, meta_scale, meta_x, &meta_x_grad, &meta_scale_grad, &meta_grad_out_grad);

  std::vector<pir::Type> argument_outputs;
  pir::Type x_grad_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_x_grad.dtype()), dense_x_grad.dims(), dense_x_grad.layout(), dense_x_grad.lod(), dense_x_grad.offset());
  argument_outputs.push_back(x_grad_dense_tensor_type);

  pir::Type scale_grad_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_scale_grad.dtype()), dense_scale_grad.dims(), dense_scale_grad.layout(), dense_scale_grad.lod(), dense_scale_grad.offset());
  argument_outputs.push_back(scale_grad_dense_tensor_type);

  pir::Type grad_out_grad_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_grad_out_grad.dtype()), dense_grad_out_grad.dims(), dense_grad_out_grad.layout(), dense_grad_out_grad.lod(), dense_grad_out_grad.offset());
  argument_outputs.push_back(grad_out_grad_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void BatchNormDoubleGrad_Op::VerifySig() {}

void BatchNormDoubleGrad_Op::InferMeta( phi::InferMetaContext *infer_meta ) {
  auto fn = PD_INFER_META(phi::GeneralTernaryGradInferMeta);
  fn(infer_meta);
}

phi::DataType BatchNormDoubleGrad_Op::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: BatchNormDoubleGrad_Op";
  


  return expected_kernel_dtype;
}

const char *BatchNormGradOp::attributes_name[6] = { "momentum", "epsilon", "data_layout", "is_test", "use_global_stats", "trainable_statistics" };

OpInfoTuple BatchNormGradOp::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("x", "paddle::dialect::DenseTensorType", false, false, false, true), paddle::dialect::OpInputInfo("scale", "paddle::dialect::DenseTensorType", true, false, false, true), paddle::dialect::OpInputInfo("bias", "paddle::dialect::DenseTensorType", true, false, false, false), paddle::dialect::OpInputInfo("mean_out", "paddle::dialect::DenseTensorType", true, false, false, false), paddle::dialect::OpInputInfo("variance_out", "paddle::dialect::DenseTensorType", true, false, false, false), paddle::dialect::OpInputInfo("saved_mean", "paddle::dialect::DenseTensorType", false, false, false, false), paddle::dialect::OpInputInfo("saved_variance", "paddle::dialect::DenseTensorType", false, false, false, false), paddle::dialect::OpInputInfo("reserve_space", "paddle::dialect::DenseTensorType", true, false, false, false), paddle::dialect::OpInputInfo("out_grad", "paddle::dialect::DenseTensorType", false, false, false, true) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = { paddle::dialect::OpAttributeInfo("momentum", "pir::FloatAttribute", ""), paddle::dialect::OpAttributeInfo("epsilon", "pir::FloatAttribute", ""), paddle::dialect::OpAttributeInfo("data_layout", "pir::StrAttribute", ""), paddle::dialect::OpAttributeInfo("is_test", "pir::BoolAttribute", ""), paddle::dialect::OpAttributeInfo("use_global_stats", "pir::BoolAttribute", ""), paddle::dialect::OpAttributeInfo("trainable_statistics", "pir::BoolAttribute", "") };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("x_grad", "paddle::dialect::DenseTensorType", false, false), paddle::dialect::OpOutputInfo("scale_grad", "paddle::dialect::DenseTensorType", false, false), paddle::dialect::OpOutputInfo("bias_grad", "paddle::dialect::DenseTensorType", false, false) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("GeneralTernaryGradInferMeta", {"x", "scale", "bias"}, "batch_norm_grad", {"x", "scale", "bias", "mean_out", "variance_out", "saved_mean", "saved_variance", "reserve_space", "out_grad", "momentum", "epsilon", "data_layout", "is_test", "use_global_stats", "trainable_statistics"}, {"out_grad"}, {}, {}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "batch_norm_grad");
}

void BatchNormGradOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, pir::Value scale_, pir::Value bias_, pir::Value mean_out_, pir::Value variance_out_, pir::Value saved_mean_, pir::Value saved_variance_, pir::Value reserve_space_, pir::Value out_grad_, float momentum, float epsilon, const std::string& data_layout, bool is_test, bool use_global_stats, bool trainable_statistics) {
  VLOG(4) << "Start build BatchNormGradOp";



  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_, scale_, bias_, mean_out_, variance_out_, saved_mean_, saved_variance_, reserve_space_, out_grad_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_momentum = pir::FloatAttribute::get(pir::IrContext::Instance(), momentum);
  argument.AddAttribute("momentum", attr_momentum);
  pir::Attribute attr_epsilon = pir::FloatAttribute::get(pir::IrContext::Instance(), epsilon);
  argument.AddAttribute("epsilon", attr_epsilon);
  pir::Attribute attr_data_layout = pir::StrAttribute::get(pir::IrContext::Instance(), data_layout);
  argument.AddAttribute("data_layout", attr_data_layout);
  pir::Attribute attr_is_test = pir::BoolAttribute::get(pir::IrContext::Instance(), is_test);
  argument.AddAttribute("is_test", attr_is_test);
  pir::Attribute attr_use_global_stats = pir::BoolAttribute::get(pir::IrContext::Instance(), use_global_stats);
  argument.AddAttribute("use_global_stats", attr_use_global_stats);
  pir::Attribute attr_trainable_statistics = pir::BoolAttribute::get(pir::IrContext::Instance(), trainable_statistics);
  argument.AddAttribute("trainable_statistics", attr_trainable_statistics);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;
  paddle::dialect::DenseTensorType saved_mean = saved_mean_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)saved_mean;
  paddle::dialect::DenseTensorType saved_variance = saved_variance_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)saved_variance;
  paddle::dialect::DenseTensorType out_grad = out_grad_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)out_grad;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);

  paddle::dialect::IrMetaTensor meta_scale;
  paddle::dialect::IrTensor ir_tensor_scale;
  if (scale_.impl() != nullptr) {
    paddle::dialect::DenseTensorType scale = scale_.type().dyn_cast<paddle::dialect::DenseTensorType>();
    VLOG(4) << "Builder construction  dense_scale";
    ir_tensor_scale = paddle::dialect::IrTensor(paddle::dialect::TransToPhiDataType(scale.dtype()),
                                                        scale.dims(),
                                                        scale.data_layout(),
                                                        scale.lod(),
                                                        scale.offset());
    VLOG(4) << "Builder construction  meta_scale";
    meta_scale = paddle::dialect::IrMetaTensor(&ir_tensor_scale);
  }


  paddle::dialect::IrMetaTensor meta_bias;
  paddle::dialect::IrTensor ir_tensor_bias;
  if (bias_.impl() != nullptr) {
    paddle::dialect::DenseTensorType bias = bias_.type().dyn_cast<paddle::dialect::DenseTensorType>();
    VLOG(4) << "Builder construction  dense_bias";
    ir_tensor_bias = paddle::dialect::IrTensor(paddle::dialect::TransToPhiDataType(bias.dtype()),
                                                        bias.dims(),
                                                        bias.data_layout(),
                                                        bias.lod(),
                                                        bias.offset());
    VLOG(4) << "Builder construction  meta_bias";
    meta_bias = paddle::dialect::IrMetaTensor(&ir_tensor_bias);
  }

  paddle::dialect::IrTensor dense_x_grad;
  paddle::dialect::IrMetaTensor meta_x_grad(&dense_x_grad);
  paddle::dialect::IrTensor dense_scale_grad;
  paddle::dialect::IrMetaTensor meta_scale_grad(&dense_scale_grad);
  paddle::dialect::IrTensor dense_bias_grad;
  paddle::dialect::IrMetaTensor meta_bias_grad(&dense_bias_grad);

  phi::GeneralTernaryGradInferMeta(meta_x, meta_scale, meta_bias, &meta_x_grad, &meta_scale_grad, &meta_bias_grad);

  std::vector<pir::Type> argument_outputs;
  pir::Type x_grad_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_x_grad.dtype()), dense_x_grad.dims(), dense_x_grad.layout(), dense_x_grad.lod(), dense_x_grad.offset());
  argument_outputs.push_back(x_grad_dense_tensor_type);

  pir::Type scale_grad_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_scale_grad.dtype()), dense_scale_grad.dims(), dense_scale_grad.layout(), dense_scale_grad.lod(), dense_scale_grad.offset());
  argument_outputs.push_back(scale_grad_dense_tensor_type);

  pir::Type bias_grad_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_bias_grad.dtype()), dense_bias_grad.dims(), dense_bias_grad.layout(), dense_bias_grad.lod(), dense_bias_grad.offset());
  argument_outputs.push_back(bias_grad_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void BatchNormGradOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, pir::Value scale_, pir::Value bias_, pir::Value mean_out_, pir::Value variance_out_, pir::Value saved_mean_, pir::Value saved_variance_, pir::Value reserve_space_, pir::Value out_grad_, pir::AttributeMap attributes) {
  VLOG(4) << "Start build BatchNormGradOp";


  IR_ENFORCE(
      attributes.find("momentum") != attributes.end(),
          "'momentum' Attribute is expected for BatchNormGradOp. ");
  float momentum = attributes.at("momentum").dyn_cast<pir::FloatAttribute>().data();

  IR_ENFORCE(
      attributes.find("epsilon") != attributes.end(),
          "'epsilon' Attribute is expected for BatchNormGradOp. ");
  float epsilon = attributes.at("epsilon").dyn_cast<pir::FloatAttribute>().data();

  IR_ENFORCE(
      attributes.find("data_layout") != attributes.end(),
          "'data_layout' Attribute is expected for BatchNormGradOp. ");
  std::string data_layout = attributes.at("data_layout").dyn_cast<pir::StrAttribute>().AsString();

  IR_ENFORCE(
      attributes.find("is_test") != attributes.end(),
          "'is_test' Attribute is expected for BatchNormGradOp. ");
  bool is_test = attributes.at("is_test").dyn_cast<pir::BoolAttribute>().data();

  IR_ENFORCE(
      attributes.find("use_global_stats") != attributes.end(),
          "'use_global_stats' Attribute is expected for BatchNormGradOp. ");
  bool use_global_stats = attributes.at("use_global_stats").dyn_cast<pir::BoolAttribute>().data();

  IR_ENFORCE(
      attributes.find("trainable_statistics") != attributes.end(),
          "'trainable_statistics' Attribute is expected for BatchNormGradOp. ");
  bool trainable_statistics = attributes.at("trainable_statistics").dyn_cast<pir::BoolAttribute>().data();


  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_, scale_, bias_, mean_out_, variance_out_, saved_mean_, saved_variance_, reserve_space_, out_grad_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_momentum = pir::FloatAttribute::get(pir::IrContext::Instance(), momentum);
  argument.AddAttribute("momentum", attr_momentum);
  pir::Attribute attr_epsilon = pir::FloatAttribute::get(pir::IrContext::Instance(), epsilon);
  argument.AddAttribute("epsilon", attr_epsilon);
  pir::Attribute attr_data_layout = pir::StrAttribute::get(pir::IrContext::Instance(), data_layout);
  argument.AddAttribute("data_layout", attr_data_layout);
  pir::Attribute attr_is_test = pir::BoolAttribute::get(pir::IrContext::Instance(), is_test);
  argument.AddAttribute("is_test", attr_is_test);
  pir::Attribute attr_use_global_stats = pir::BoolAttribute::get(pir::IrContext::Instance(), use_global_stats);
  argument.AddAttribute("use_global_stats", attr_use_global_stats);
  pir::Attribute attr_trainable_statistics = pir::BoolAttribute::get(pir::IrContext::Instance(), trainable_statistics);
  argument.AddAttribute("trainable_statistics", attr_trainable_statistics);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;
  paddle::dialect::DenseTensorType saved_mean = saved_mean_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)saved_mean;
  paddle::dialect::DenseTensorType saved_variance = saved_variance_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)saved_variance;
  paddle::dialect::DenseTensorType out_grad = out_grad_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)out_grad;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);

  paddle::dialect::IrMetaTensor meta_scale;
  paddle::dialect::IrTensor ir_tensor_scale;
  if (scale_.impl() != nullptr) {
    paddle::dialect::DenseTensorType scale = scale_.type().dyn_cast<paddle::dialect::DenseTensorType>();
    VLOG(4) << "Builder construction  dense_scale";
    ir_tensor_scale = paddle::dialect::IrTensor(paddle::dialect::TransToPhiDataType(scale.dtype()),
                                                        scale.dims(),
                                                        scale.data_layout(),
                                                        scale.lod(),
                                                        scale.offset());
    VLOG(4) << "Builder construction  meta_scale";
    meta_scale = paddle::dialect::IrMetaTensor(&ir_tensor_scale);
  }


  paddle::dialect::IrMetaTensor meta_bias;
  paddle::dialect::IrTensor ir_tensor_bias;
  if (bias_.impl() != nullptr) {
    paddle::dialect::DenseTensorType bias = bias_.type().dyn_cast<paddle::dialect::DenseTensorType>();
    VLOG(4) << "Builder construction  dense_bias";
    ir_tensor_bias = paddle::dialect::IrTensor(paddle::dialect::TransToPhiDataType(bias.dtype()),
                                                        bias.dims(),
                                                        bias.data_layout(),
                                                        bias.lod(),
                                                        bias.offset());
    VLOG(4) << "Builder construction  meta_bias";
    meta_bias = paddle::dialect::IrMetaTensor(&ir_tensor_bias);
  }

  paddle::dialect::IrTensor dense_x_grad;
  paddle::dialect::IrMetaTensor meta_x_grad(&dense_x_grad);
  paddle::dialect::IrTensor dense_scale_grad;
  paddle::dialect::IrMetaTensor meta_scale_grad(&dense_scale_grad);
  paddle::dialect::IrTensor dense_bias_grad;
  paddle::dialect::IrMetaTensor meta_bias_grad(&dense_bias_grad);

  phi::GeneralTernaryGradInferMeta(meta_x, meta_scale, meta_bias, &meta_x_grad, &meta_scale_grad, &meta_bias_grad);

  std::vector<pir::Type> argument_outputs;
  pir::Type x_grad_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_x_grad.dtype()), dense_x_grad.dims(), dense_x_grad.layout(), dense_x_grad.lod(), dense_x_grad.offset());
  argument_outputs.push_back(x_grad_dense_tensor_type);

  pir::Type scale_grad_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_scale_grad.dtype()), dense_scale_grad.dims(), dense_scale_grad.layout(), dense_scale_grad.lod(), dense_scale_grad.offset());
  argument_outputs.push_back(scale_grad_dense_tensor_type);

  pir::Type bias_grad_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_bias_grad.dtype()), dense_bias_grad.dims(), dense_bias_grad.layout(), dense_bias_grad.lod(), dense_bias_grad.offset());
  argument_outputs.push_back(bias_grad_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void BatchNormGradOp::VerifySig() {}

void BatchNormGradOp::InferMeta( phi::InferMetaContext *infer_meta ) {
  auto fn = PD_INFER_META(phi::GeneralTernaryGradInferMeta);
  fn(infer_meta);
}

phi::DataType BatchNormGradOp::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: BatchNormGradOp";
  


  return expected_kernel_dtype;
}

const char *CEmbeddingGradOp::attributes_name[1] = { "start_index" };

OpInfoTuple CEmbeddingGradOp::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("weight", "paddle::dialect::DenseTensorType", false, true, false, false), paddle::dialect::OpInputInfo("x", "paddle::dialect::DenseTensorType", false, false, false, false), paddle::dialect::OpInputInfo("out_grad", "paddle::dialect::DenseTensorType", false, false, false, false) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = { paddle::dialect::OpAttributeInfo("start_index", "pir::Int64Attribute", "") };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("weight_grad", "paddle::dialect::DenseTensorType", false, false) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("EmbeddingGradInferMeta", {"x", "weight"}, "c_embedding_grad", {"weight", "x", "out_grad", "start_index"}, {}, {}, {}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "c_embedding_grad");
}

void CEmbeddingGradOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value weight_, pir::Value x_, pir::Value out_grad_, int64_t start_index) {
  VLOG(4) << "Start build CEmbeddingGradOp";



  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {weight_, x_, out_grad_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_start_index = pir::Int64Attribute::get(pir::IrContext::Instance(), start_index);
  argument.AddAttribute("start_index", attr_start_index);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType weight = weight_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)weight;
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;
  paddle::dialect::DenseTensorType out_grad = out_grad_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)out_grad;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);

  VLOG(4) << "Builder construction  dense_weight";
  paddle::dialect::IrTensor ir_tensor_weight(paddle::dialect::TransToPhiDataType(weight.dtype()),
                                                      weight.dims(),
                                                      weight.data_layout(),
                                                      weight.lod(),
                                                      weight.offset());
  VLOG(4) << "Builder construction  meta_weight";
  paddle::dialect::IrMetaTensor meta_weight(&ir_tensor_weight);
  paddle::dialect::IrTensor dense_weight_grad;
  paddle::dialect::IrMetaTensor meta_weight_grad(&dense_weight_grad);

  phi::EmbeddingGradInferMeta(meta_x, meta_weight, &meta_weight_grad);

  std::vector<pir::Type> argument_outputs;
  pir::Type weight_grad_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_weight_grad.dtype()), dense_weight_grad.dims(), dense_weight_grad.layout(), dense_weight_grad.lod(), dense_weight_grad.offset());
  argument_outputs.push_back(weight_grad_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void CEmbeddingGradOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value weight_, pir::Value x_, pir::Value out_grad_, pir::AttributeMap attributes) {
  VLOG(4) << "Start build CEmbeddingGradOp";


  IR_ENFORCE(
      attributes.find("start_index") != attributes.end(),
          "'start_index' Attribute is expected for CEmbeddingGradOp. ");
  int64_t start_index = attributes.at("start_index").dyn_cast<pir::Int64Attribute>().data();


  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {weight_, x_, out_grad_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_start_index = pir::Int64Attribute::get(pir::IrContext::Instance(), start_index);
  argument.AddAttribute("start_index", attr_start_index);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType weight = weight_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)weight;
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;
  paddle::dialect::DenseTensorType out_grad = out_grad_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)out_grad;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);

  VLOG(4) << "Builder construction  dense_weight";
  paddle::dialect::IrTensor ir_tensor_weight(paddle::dialect::TransToPhiDataType(weight.dtype()),
                                                      weight.dims(),
                                                      weight.data_layout(),
                                                      weight.lod(),
                                                      weight.offset());
  VLOG(4) << "Builder construction  meta_weight";
  paddle::dialect::IrMetaTensor meta_weight(&ir_tensor_weight);
  paddle::dialect::IrTensor dense_weight_grad;
  paddle::dialect::IrMetaTensor meta_weight_grad(&dense_weight_grad);

  phi::EmbeddingGradInferMeta(meta_x, meta_weight, &meta_weight_grad);

  std::vector<pir::Type> argument_outputs;
  pir::Type weight_grad_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_weight_grad.dtype()), dense_weight_grad.dims(), dense_weight_grad.layout(), dense_weight_grad.lod(), dense_weight_grad.offset());
  argument_outputs.push_back(weight_grad_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void CEmbeddingGradOp::VerifySig() {}

void CEmbeddingGradOp::InferMeta( phi::InferMetaContext *infer_meta ) {
  auto fn = PD_INFER_META(phi::EmbeddingGradInferMeta);
  fn(infer_meta);
}

phi::DataType CEmbeddingGradOp::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: CEmbeddingGradOp";
  


  return expected_kernel_dtype;
}

const char *CSoftmaxWithCrossEntropyGradOp::attributes_name[4] = { "ignore_index", "ring_id", "rank", "nranks" };

OpInfoTuple CSoftmaxWithCrossEntropyGradOp::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("softmax", "paddle::dialect::DenseTensorType", false, false, false, false), paddle::dialect::OpInputInfo("label", "paddle::dialect::DenseTensorType", false, false, false, false), paddle::dialect::OpInputInfo("loss_grad", "paddle::dialect::DenseTensorType", false, false, false, false) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = { paddle::dialect::OpAttributeInfo("ignore_index", "pir::Int64Attribute", ""), paddle::dialect::OpAttributeInfo("ring_id", "pir::Int32Attribute", ""), paddle::dialect::OpAttributeInfo("rank", "pir::Int32Attribute", ""), paddle::dialect::OpAttributeInfo("nranks", "pir::Int32Attribute", "") };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("logits_grad", "paddle::dialect::DenseTensorType", false, false) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("CSoftmaxWithCrossEntropyGradInferMeta", {"softmax", "label", "loss_grad", "ignore_index", "ring_id", "rank", "nranks"}, "c_softmax_with_cross_entropy_grad", {"softmax", "label", "loss_grad", "ignore_index", "ring_id", "rank", "nranks"}, {"loss_grad"}, {}, {}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "c_softmax_with_cross_entropy_grad");
}

void CSoftmaxWithCrossEntropyGradOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value softmax_, pir::Value label_, pir::Value loss_grad_, int64_t ignore_index, int ring_id, int rank, int nranks) {
  VLOG(4) << "Start build CSoftmaxWithCrossEntropyGradOp";



  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {softmax_, label_, loss_grad_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_ignore_index = pir::Int64Attribute::get(pir::IrContext::Instance(), ignore_index);
  argument.AddAttribute("ignore_index", attr_ignore_index);
  pir::Attribute attr_ring_id = pir::Int32Attribute::get(pir::IrContext::Instance(), ring_id);
  argument.AddAttribute("ring_id", attr_ring_id);
  pir::Attribute attr_rank = pir::Int32Attribute::get(pir::IrContext::Instance(), rank);
  argument.AddAttribute("rank", attr_rank);
  pir::Attribute attr_nranks = pir::Int32Attribute::get(pir::IrContext::Instance(), nranks);
  argument.AddAttribute("nranks", attr_nranks);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType softmax = softmax_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)softmax;
  paddle::dialect::DenseTensorType label = label_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)label;
  paddle::dialect::DenseTensorType loss_grad = loss_grad_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)loss_grad;

  VLOG(4) << "Builder construction  dense_softmax";
  paddle::dialect::IrTensor ir_tensor_softmax(paddle::dialect::TransToPhiDataType(softmax.dtype()),
                                                      softmax.dims(),
                                                      softmax.data_layout(),
                                                      softmax.lod(),
                                                      softmax.offset());
  VLOG(4) << "Builder construction  meta_softmax";
  paddle::dialect::IrMetaTensor meta_softmax(&ir_tensor_softmax);

  VLOG(4) << "Builder construction  dense_label";
  paddle::dialect::IrTensor ir_tensor_label(paddle::dialect::TransToPhiDataType(label.dtype()),
                                                      label.dims(),
                                                      label.data_layout(),
                                                      label.lod(),
                                                      label.offset());
  VLOG(4) << "Builder construction  meta_label";
  paddle::dialect::IrMetaTensor meta_label(&ir_tensor_label);

  VLOG(4) << "Builder construction  dense_loss_grad";
  paddle::dialect::IrTensor ir_tensor_loss_grad(paddle::dialect::TransToPhiDataType(loss_grad.dtype()),
                                                      loss_grad.dims(),
                                                      loss_grad.data_layout(),
                                                      loss_grad.lod(),
                                                      loss_grad.offset());
  VLOG(4) << "Builder construction  meta_loss_grad";
  paddle::dialect::IrMetaTensor meta_loss_grad(&ir_tensor_loss_grad);
  paddle::dialect::IrTensor dense_logits_grad;
  paddle::dialect::IrMetaTensor meta_logits_grad(&dense_logits_grad);

  phi::CSoftmaxWithCrossEntropyGradInferMeta(meta_softmax, meta_label, meta_loss_grad, ignore_index, ring_id, rank, nranks, &meta_logits_grad);

  std::vector<pir::Type> argument_outputs;
  pir::Type logits_grad_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_logits_grad.dtype()), dense_logits_grad.dims(), dense_logits_grad.layout(), dense_logits_grad.lod(), dense_logits_grad.offset());
  argument_outputs.push_back(logits_grad_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void CSoftmaxWithCrossEntropyGradOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value softmax_, pir::Value label_, pir::Value loss_grad_, pir::AttributeMap attributes) {
  VLOG(4) << "Start build CSoftmaxWithCrossEntropyGradOp";


  IR_ENFORCE(
      attributes.find("ignore_index") != attributes.end(),
          "'ignore_index' Attribute is expected for CSoftmaxWithCrossEntropyGradOp. ");
  int64_t ignore_index = attributes.at("ignore_index").dyn_cast<pir::Int64Attribute>().data();

  IR_ENFORCE(
      attributes.find("ring_id") != attributes.end(),
          "'ring_id' Attribute is expected for CSoftmaxWithCrossEntropyGradOp. ");
  int ring_id = attributes.at("ring_id").dyn_cast<pir::Int32Attribute>().data();

  IR_ENFORCE(
      attributes.find("rank") != attributes.end(),
          "'rank' Attribute is expected for CSoftmaxWithCrossEntropyGradOp. ");
  int rank = attributes.at("rank").dyn_cast<pir::Int32Attribute>().data();

  IR_ENFORCE(
      attributes.find("nranks") != attributes.end(),
          "'nranks' Attribute is expected for CSoftmaxWithCrossEntropyGradOp. ");
  int nranks = attributes.at("nranks").dyn_cast<pir::Int32Attribute>().data();


  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {softmax_, label_, loss_grad_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_ignore_index = pir::Int64Attribute::get(pir::IrContext::Instance(), ignore_index);
  argument.AddAttribute("ignore_index", attr_ignore_index);
  pir::Attribute attr_ring_id = pir::Int32Attribute::get(pir::IrContext::Instance(), ring_id);
  argument.AddAttribute("ring_id", attr_ring_id);
  pir::Attribute attr_rank = pir::Int32Attribute::get(pir::IrContext::Instance(), rank);
  argument.AddAttribute("rank", attr_rank);
  pir::Attribute attr_nranks = pir::Int32Attribute::get(pir::IrContext::Instance(), nranks);
  argument.AddAttribute("nranks", attr_nranks);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType softmax = softmax_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)softmax;
  paddle::dialect::DenseTensorType label = label_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)label;
  paddle::dialect::DenseTensorType loss_grad = loss_grad_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)loss_grad;

  VLOG(4) << "Builder construction  dense_softmax";
  paddle::dialect::IrTensor ir_tensor_softmax(paddle::dialect::TransToPhiDataType(softmax.dtype()),
                                                      softmax.dims(),
                                                      softmax.data_layout(),
                                                      softmax.lod(),
                                                      softmax.offset());
  VLOG(4) << "Builder construction  meta_softmax";
  paddle::dialect::IrMetaTensor meta_softmax(&ir_tensor_softmax);

  VLOG(4) << "Builder construction  dense_label";
  paddle::dialect::IrTensor ir_tensor_label(paddle::dialect::TransToPhiDataType(label.dtype()),
                                                      label.dims(),
                                                      label.data_layout(),
                                                      label.lod(),
                                                      label.offset());
  VLOG(4) << "Builder construction  meta_label";
  paddle::dialect::IrMetaTensor meta_label(&ir_tensor_label);

  VLOG(4) << "Builder construction  dense_loss_grad";
  paddle::dialect::IrTensor ir_tensor_loss_grad(paddle::dialect::TransToPhiDataType(loss_grad.dtype()),
                                                      loss_grad.dims(),
                                                      loss_grad.data_layout(),
                                                      loss_grad.lod(),
                                                      loss_grad.offset());
  VLOG(4) << "Builder construction  meta_loss_grad";
  paddle::dialect::IrMetaTensor meta_loss_grad(&ir_tensor_loss_grad);
  paddle::dialect::IrTensor dense_logits_grad;
  paddle::dialect::IrMetaTensor meta_logits_grad(&dense_logits_grad);

  phi::CSoftmaxWithCrossEntropyGradInferMeta(meta_softmax, meta_label, meta_loss_grad, ignore_index, ring_id, rank, nranks, &meta_logits_grad);

  std::vector<pir::Type> argument_outputs;
  pir::Type logits_grad_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_logits_grad.dtype()), dense_logits_grad.dims(), dense_logits_grad.layout(), dense_logits_grad.lod(), dense_logits_grad.offset());
  argument_outputs.push_back(logits_grad_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void CSoftmaxWithCrossEntropyGradOp::VerifySig() {}

void CSoftmaxWithCrossEntropyGradOp::InferMeta( phi::InferMetaContext *infer_meta ) {
  auto fn = PD_INFER_META(phi::CSoftmaxWithCrossEntropyGradInferMeta);
  fn(infer_meta);
}

phi::DataType CSoftmaxWithCrossEntropyGradOp::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: CSoftmaxWithCrossEntropyGradOp";
  


  return expected_kernel_dtype;
}

OpInfoTuple CastGradOp::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("x", "paddle::dialect::DenseTensorType", false, true, false, false), paddle::dialect::OpInputInfo("out_grad", "paddle::dialect::DenseTensorType", false, false, false, false) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = {  };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("x_grad", "paddle::dialect::DenseTensorType", false, false) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("", {""}, "", {""}, {}, {}, {}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "cast_grad");
}

void CastGradOp::VerifySig() {}

phi::DataType CastGradOp::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: CastGradOp";
  


  return expected_kernel_dtype;
}

const char *ChannelShuffleGradOp::attributes_name[2] = { "groups", "data_format" };

OpInfoTuple ChannelShuffleGradOp::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("out_grad", "paddle::dialect::DenseTensorType", false, false, false, false) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = { paddle::dialect::OpAttributeInfo("groups", "pir::Int32Attribute", ""), paddle::dialect::OpAttributeInfo("data_format", "pir::StrAttribute", "") };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("x_grad", "paddle::dialect::DenseTensorType", false, false) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("ChannelShuffleGradInferMeta", {"out_grad", "groups", "data_format"}, "channel_shuffle_grad", {"out_grad", "groups", "data_format"}, {}, {}, {}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "channel_shuffle_grad");
}

void ChannelShuffleGradOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value out_grad_, int groups, const std::string& data_format) {
  VLOG(4) << "Start build ChannelShuffleGradOp";



  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {out_grad_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_groups = pir::Int32Attribute::get(pir::IrContext::Instance(), groups);
  argument.AddAttribute("groups", attr_groups);
  pir::Attribute attr_data_format = pir::StrAttribute::get(pir::IrContext::Instance(), data_format);
  argument.AddAttribute("data_format", attr_data_format);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType out_grad = out_grad_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)out_grad;

  VLOG(4) << "Builder construction  dense_out_grad";
  paddle::dialect::IrTensor ir_tensor_out_grad(paddle::dialect::TransToPhiDataType(out_grad.dtype()),
                                                      out_grad.dims(),
                                                      out_grad.data_layout(),
                                                      out_grad.lod(),
                                                      out_grad.offset());
  VLOG(4) << "Builder construction  meta_out_grad";
  paddle::dialect::IrMetaTensor meta_out_grad(&ir_tensor_out_grad);
  paddle::dialect::IrTensor dense_x_grad;
  paddle::dialect::IrMetaTensor meta_x_grad(&dense_x_grad);

  phi::ChannelShuffleGradInferMeta(meta_out_grad, groups, data_format, &meta_x_grad);

  std::vector<pir::Type> argument_outputs;
  pir::Type x_grad_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_x_grad.dtype()), dense_x_grad.dims(), dense_x_grad.layout(), dense_x_grad.lod(), dense_x_grad.offset());
  argument_outputs.push_back(x_grad_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void ChannelShuffleGradOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value out_grad_, pir::AttributeMap attributes) {
  VLOG(4) << "Start build ChannelShuffleGradOp";


  IR_ENFORCE(
      attributes.find("groups") != attributes.end(),
          "'groups' Attribute is expected for ChannelShuffleGradOp. ");
  int groups = attributes.at("groups").dyn_cast<pir::Int32Attribute>().data();

  IR_ENFORCE(
      attributes.find("data_format") != attributes.end(),
          "'data_format' Attribute is expected for ChannelShuffleGradOp. ");
  std::string data_format = attributes.at("data_format").dyn_cast<pir::StrAttribute>().AsString();


  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {out_grad_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_groups = pir::Int32Attribute::get(pir::IrContext::Instance(), groups);
  argument.AddAttribute("groups", attr_groups);
  pir::Attribute attr_data_format = pir::StrAttribute::get(pir::IrContext::Instance(), data_format);
  argument.AddAttribute("data_format", attr_data_format);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType out_grad = out_grad_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)out_grad;

  VLOG(4) << "Builder construction  dense_out_grad";
  paddle::dialect::IrTensor ir_tensor_out_grad(paddle::dialect::TransToPhiDataType(out_grad.dtype()),
                                                      out_grad.dims(),
                                                      out_grad.data_layout(),
                                                      out_grad.lod(),
                                                      out_grad.offset());
  VLOG(4) << "Builder construction  meta_out_grad";
  paddle::dialect::IrMetaTensor meta_out_grad(&ir_tensor_out_grad);
  paddle::dialect::IrTensor dense_x_grad;
  paddle::dialect::IrMetaTensor meta_x_grad(&dense_x_grad);

  phi::ChannelShuffleGradInferMeta(meta_out_grad, groups, data_format, &meta_x_grad);

  std::vector<pir::Type> argument_outputs;
  pir::Type x_grad_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_x_grad.dtype()), dense_x_grad.dims(), dense_x_grad.layout(), dense_x_grad.lod(), dense_x_grad.offset());
  argument_outputs.push_back(x_grad_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void ChannelShuffleGradOp::VerifySig() {}

void ChannelShuffleGradOp::InferMeta( phi::InferMetaContext *infer_meta ) {
  auto fn = PD_INFER_META(phi::ChannelShuffleGradInferMeta);
  fn(infer_meta);
}

phi::DataType ChannelShuffleGradOp::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: ChannelShuffleGradOp";
  


  return expected_kernel_dtype;
}

const char *Conv2dTransposeDoubleGradOp::attributes_name[7] = { "strides", "paddings", "output_padding", "padding_algorithm", "groups", "dilations", "data_format" };

OpInfoTuple Conv2dTransposeDoubleGradOp::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("x", "paddle::dialect::DenseTensorType", false, false, false, false), paddle::dialect::OpInputInfo("filter", "paddle::dialect::DenseTensorType", false, false, false, false), paddle::dialect::OpInputInfo("grad_out", "paddle::dialect::DenseTensorType", false, false, false, false), paddle::dialect::OpInputInfo("grad_x_grad", "paddle::dialect::DenseTensorType", false, false, false, false), paddle::dialect::OpInputInfo("grad_filter_grad", "paddle::dialect::DenseTensorType", false, false, false, false), paddle::dialect::OpInputInfo("output_size", "paddle::dialect::IntArrayAttribute", false, false, true, false) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = { paddle::dialect::OpAttributeInfo("strides", "pir::ArrayAttribute<pir::Int32Attribute>", ""), paddle::dialect::OpAttributeInfo("paddings", "pir::ArrayAttribute<pir::Int32Attribute>", ""), paddle::dialect::OpAttributeInfo("output_padding", "pir::ArrayAttribute<pir::Int32Attribute>", ""), paddle::dialect::OpAttributeInfo("padding_algorithm", "pir::StrAttribute", ""), paddle::dialect::OpAttributeInfo("groups", "pir::Int32Attribute", ""), paddle::dialect::OpAttributeInfo("dilations", "pir::ArrayAttribute<pir::Int32Attribute>", ""), paddle::dialect::OpAttributeInfo("data_format", "pir::StrAttribute", "") };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("x_grad", "paddle::dialect::DenseTensorType", false, false), paddle::dialect::OpOutputInfo("filter_grad", "paddle::dialect::DenseTensorType", false, false), paddle::dialect::OpOutputInfo("grad_out_grad", "paddle::dialect::DenseTensorType", false, false) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("Conv2dTransposeDoubleGradInferMeta", {"x", "filter", "grad_out", "grad_x_grad", "grad_filter_grad", "strides", "paddings", "output_padding", "output_size", "padding_algorithm", "groups", "dilations", "data_format"}, "conv2d_transpose_double_grad", {"x", "filter", "grad_out", "grad_x_grad", "grad_filter_grad", "strides", "paddings", "output_padding", "output_size", "padding_algorithm", "groups", "dilations", "data_format"}, {"x"}, {}, {}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "conv2d_transpose_double_grad");
}

void Conv2dTransposeDoubleGradOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, pir::Value filter_, pir::Value grad_out_, pir::Value grad_x_grad_, pir::Value grad_filter_grad_, const std::vector<int>& strides, const std::vector<int>& paddings, const std::vector<int>& output_padding, const std::vector<int64_t>& output_size, const std::string& padding_algorithm, int groups, const std::vector<int>& dilations, const std::string& data_format) {
  VLOG(4) << "Start build Conv2dTransposeDoubleGradOp";


  // Generate int_array mutable attribute: output_size
  paddle::dialect::FullIntArrayOp full_output_size_op = builder.Build<paddle::dialect::FullIntArrayOp>(output_size, phi::DataType::INT64, phi::CPUPlace());
  pir::OpResult output_size_ = full_output_size_op->result(0);
    
  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_, filter_, grad_out_, grad_x_grad_, grad_filter_grad_, output_size_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  std::vector<pir::Attribute> vec_strides;
  for (size_t i = 0; i < static_cast<size_t>(strides.size()); i++) {
      pir::Attribute attr_strides = pir::Int32Attribute::get(pir::IrContext::Instance(), strides[i]);

    vec_strides.push_back(attr_strides);
  }
  pir::Attribute attr_strides = pir::ArrayAttribute::get(pir::IrContext::Instance(), vec_strides);
  argument.AddAttribute("strides", attr_strides);
  std::vector<pir::Attribute> vec_paddings;
  for (size_t i = 0; i < static_cast<size_t>(paddings.size()); i++) {
      pir::Attribute attr_paddings = pir::Int32Attribute::get(pir::IrContext::Instance(), paddings[i]);

    vec_paddings.push_back(attr_paddings);
  }
  pir::Attribute attr_paddings = pir::ArrayAttribute::get(pir::IrContext::Instance(), vec_paddings);
  argument.AddAttribute("paddings", attr_paddings);
  std::vector<pir::Attribute> vec_output_padding;
  for (size_t i = 0; i < static_cast<size_t>(output_padding.size()); i++) {
      pir::Attribute attr_output_padding = pir::Int32Attribute::get(pir::IrContext::Instance(), output_padding[i]);

    vec_output_padding.push_back(attr_output_padding);
  }
  pir::Attribute attr_output_padding = pir::ArrayAttribute::get(pir::IrContext::Instance(), vec_output_padding);
  argument.AddAttribute("output_padding", attr_output_padding);
  pir::Attribute attr_padding_algorithm = pir::StrAttribute::get(pir::IrContext::Instance(), padding_algorithm);
  argument.AddAttribute("padding_algorithm", attr_padding_algorithm);
  pir::Attribute attr_groups = pir::Int32Attribute::get(pir::IrContext::Instance(), groups);
  argument.AddAttribute("groups", attr_groups);
  std::vector<pir::Attribute> vec_dilations;
  for (size_t i = 0; i < static_cast<size_t>(dilations.size()); i++) {
      pir::Attribute attr_dilations = pir::Int32Attribute::get(pir::IrContext::Instance(), dilations[i]);

    vec_dilations.push_back(attr_dilations);
  }
  pir::Attribute attr_dilations = pir::ArrayAttribute::get(pir::IrContext::Instance(), vec_dilations);
  argument.AddAttribute("dilations", attr_dilations);
  pir::Attribute attr_data_format = pir::StrAttribute::get(pir::IrContext::Instance(), data_format);
  argument.AddAttribute("data_format", attr_data_format);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;
  paddle::dialect::DenseTensorType filter = filter_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)filter;
  paddle::dialect::DenseTensorType grad_out = grad_out_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)grad_out;
  paddle::dialect::DenseTensorType grad_x_grad = grad_x_grad_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)grad_x_grad;
  paddle::dialect::DenseTensorType grad_filter_grad = grad_filter_grad_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)grad_filter_grad;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);

  VLOG(4) << "Builder construction  dense_filter";
  paddle::dialect::IrTensor ir_tensor_filter(paddle::dialect::TransToPhiDataType(filter.dtype()),
                                                      filter.dims(),
                                                      filter.data_layout(),
                                                      filter.lod(),
                                                      filter.offset());
  VLOG(4) << "Builder construction  meta_filter";
  paddle::dialect::IrMetaTensor meta_filter(&ir_tensor_filter);

  VLOG(4) << "Builder construction  dense_grad_out";
  paddle::dialect::IrTensor ir_tensor_grad_out(paddle::dialect::TransToPhiDataType(grad_out.dtype()),
                                                      grad_out.dims(),
                                                      grad_out.data_layout(),
                                                      grad_out.lod(),
                                                      grad_out.offset());
  VLOG(4) << "Builder construction  meta_grad_out";
  paddle::dialect::IrMetaTensor meta_grad_out(&ir_tensor_grad_out);

  VLOG(4) << "Builder construction  dense_grad_x_grad";
  paddle::dialect::IrTensor ir_tensor_grad_x_grad(paddle::dialect::TransToPhiDataType(grad_x_grad.dtype()),
                                                      grad_x_grad.dims(),
                                                      grad_x_grad.data_layout(),
                                                      grad_x_grad.lod(),
                                                      grad_x_grad.offset());
  VLOG(4) << "Builder construction  meta_grad_x_grad";
  paddle::dialect::IrMetaTensor meta_grad_x_grad(&ir_tensor_grad_x_grad);

  VLOG(4) << "Builder construction  dense_grad_filter_grad";
  paddle::dialect::IrTensor ir_tensor_grad_filter_grad(paddle::dialect::TransToPhiDataType(grad_filter_grad.dtype()),
                                                      grad_filter_grad.dims(),
                                                      grad_filter_grad.data_layout(),
                                                      grad_filter_grad.lod(),
                                                      grad_filter_grad.offset());
  VLOG(4) << "Builder construction  meta_grad_filter_grad";
  paddle::dialect::IrMetaTensor meta_grad_filter_grad(&ir_tensor_grad_filter_grad);
  paddle::dialect::IrTensor dense_x_grad;
  paddle::dialect::IrMetaTensor meta_x_grad(&dense_x_grad);
  paddle::dialect::IrTensor dense_filter_grad;
  paddle::dialect::IrMetaTensor meta_filter_grad(&dense_filter_grad);
  paddle::dialect::IrTensor dense_grad_out_grad;
  paddle::dialect::IrMetaTensor meta_grad_out_grad(&dense_grad_out_grad);

  phi::Conv2dTransposeDoubleGradInferMeta(meta_x, meta_filter, meta_grad_out, meta_grad_x_grad, meta_grad_filter_grad, strides, paddings, output_padding, output_size, padding_algorithm, groups, dilations, data_format, &meta_x_grad, &meta_filter_grad, &meta_grad_out_grad);

  std::vector<pir::Type> argument_outputs;
  pir::Type x_grad_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_x_grad.dtype()), dense_x_grad.dims(), dense_x_grad.layout(), dense_x_grad.lod(), dense_x_grad.offset());
  argument_outputs.push_back(x_grad_dense_tensor_type);

  pir::Type filter_grad_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_filter_grad.dtype()), dense_filter_grad.dims(), dense_filter_grad.layout(), dense_filter_grad.lod(), dense_filter_grad.offset());
  argument_outputs.push_back(filter_grad_dense_tensor_type);

  pir::Type grad_out_grad_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_grad_out_grad.dtype()), dense_grad_out_grad.dims(), dense_grad_out_grad.layout(), dense_grad_out_grad.lod(), dense_grad_out_grad.offset());
  argument_outputs.push_back(grad_out_grad_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void Conv2dTransposeDoubleGradOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, pir::Value filter_, pir::Value grad_out_, pir::Value grad_x_grad_, pir::Value grad_filter_grad_, pir::AttributeMap attributes) {
  VLOG(4) << "Start build Conv2dTransposeDoubleGradOp";


  IR_ENFORCE(
      attributes.find("strides") != attributes.end(),
          "'strides' Attribute is expected for Conv2dTransposeDoubleGradOp. ");
  std::vector<int> strides;
  for (size_t i = 0; i < attributes.at("strides").dyn_cast<pir::ArrayAttribute>().size(); i++) {
    strides.push_back(attributes.at("strides").dyn_cast<pir::ArrayAttribute>().at(i).dyn_cast<pir::Int32Attribute>().data());
  }

  IR_ENFORCE(
      attributes.find("paddings") != attributes.end(),
          "'paddings' Attribute is expected for Conv2dTransposeDoubleGradOp. ");
  std::vector<int> paddings;
  for (size_t i = 0; i < attributes.at("paddings").dyn_cast<pir::ArrayAttribute>().size(); i++) {
    paddings.push_back(attributes.at("paddings").dyn_cast<pir::ArrayAttribute>().at(i).dyn_cast<pir::Int32Attribute>().data());
  }

  IR_ENFORCE(
      attributes.find("output_padding") != attributes.end(),
          "'output_padding' Attribute is expected for Conv2dTransposeDoubleGradOp. ");
  std::vector<int> output_padding;
  for (size_t i = 0; i < attributes.at("output_padding").dyn_cast<pir::ArrayAttribute>().size(); i++) {
    output_padding.push_back(attributes.at("output_padding").dyn_cast<pir::ArrayAttribute>().at(i).dyn_cast<pir::Int32Attribute>().data());
  }

  IR_ENFORCE(
      attributes.find("output_size") != attributes.end(),
          "'output_size' Attribute is expected for Conv2dTransposeDoubleGradOp. ");
  std::vector<int64_t> output_size = attributes.at("output_size").dyn_cast<paddle::dialect::IntArrayAttribute>().data().GetData();

  IR_ENFORCE(
      attributes.find("padding_algorithm") != attributes.end(),
          "'padding_algorithm' Attribute is expected for Conv2dTransposeDoubleGradOp. ");
  std::string padding_algorithm = attributes.at("padding_algorithm").dyn_cast<pir::StrAttribute>().AsString();

  IR_ENFORCE(
      attributes.find("groups") != attributes.end(),
          "'groups' Attribute is expected for Conv2dTransposeDoubleGradOp. ");
  int groups = attributes.at("groups").dyn_cast<pir::Int32Attribute>().data();

  IR_ENFORCE(
      attributes.find("dilations") != attributes.end(),
          "'dilations' Attribute is expected for Conv2dTransposeDoubleGradOp. ");
  std::vector<int> dilations;
  for (size_t i = 0; i < attributes.at("dilations").dyn_cast<pir::ArrayAttribute>().size(); i++) {
    dilations.push_back(attributes.at("dilations").dyn_cast<pir::ArrayAttribute>().at(i).dyn_cast<pir::Int32Attribute>().data());
  }

  IR_ENFORCE(
      attributes.find("data_format") != attributes.end(),
          "'data_format' Attribute is expected for Conv2dTransposeDoubleGradOp. ");
  std::string data_format = attributes.at("data_format").dyn_cast<pir::StrAttribute>().AsString();

  // Generate int_array mutable attribute: output_size
  paddle::dialect::FullIntArrayOp full_output_size_op = builder.Build<paddle::dialect::FullIntArrayOp>(output_size, phi::DataType::INT64, phi::CPUPlace());
  pir::OpResult output_size_ = full_output_size_op->result(0);
    
  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_, filter_, grad_out_, grad_x_grad_, grad_filter_grad_, output_size_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  std::vector<pir::Attribute> vec_strides;
  for (size_t i = 0; i < static_cast<size_t>(strides.size()); i++) {
      pir::Attribute attr_strides = pir::Int32Attribute::get(pir::IrContext::Instance(), strides[i]);

    vec_strides.push_back(attr_strides);
  }
  pir::Attribute attr_strides = pir::ArrayAttribute::get(pir::IrContext::Instance(), vec_strides);
  argument.AddAttribute("strides", attr_strides);
  std::vector<pir::Attribute> vec_paddings;
  for (size_t i = 0; i < static_cast<size_t>(paddings.size()); i++) {
      pir::Attribute attr_paddings = pir::Int32Attribute::get(pir::IrContext::Instance(), paddings[i]);

    vec_paddings.push_back(attr_paddings);
  }
  pir::Attribute attr_paddings = pir::ArrayAttribute::get(pir::IrContext::Instance(), vec_paddings);
  argument.AddAttribute("paddings", attr_paddings);
  std::vector<pir::Attribute> vec_output_padding;
  for (size_t i = 0; i < static_cast<size_t>(output_padding.size()); i++) {
      pir::Attribute attr_output_padding = pir::Int32Attribute::get(pir::IrContext::Instance(), output_padding[i]);

    vec_output_padding.push_back(attr_output_padding);
  }
  pir::Attribute attr_output_padding = pir::ArrayAttribute::get(pir::IrContext::Instance(), vec_output_padding);
  argument.AddAttribute("output_padding", attr_output_padding);
  pir::Attribute attr_padding_algorithm = pir::StrAttribute::get(pir::IrContext::Instance(), padding_algorithm);
  argument.AddAttribute("padding_algorithm", attr_padding_algorithm);
  pir::Attribute attr_groups = pir::Int32Attribute::get(pir::IrContext::Instance(), groups);
  argument.AddAttribute("groups", attr_groups);
  std::vector<pir::Attribute> vec_dilations;
  for (size_t i = 0; i < static_cast<size_t>(dilations.size()); i++) {
      pir::Attribute attr_dilations = pir::Int32Attribute::get(pir::IrContext::Instance(), dilations[i]);

    vec_dilations.push_back(attr_dilations);
  }
  pir::Attribute attr_dilations = pir::ArrayAttribute::get(pir::IrContext::Instance(), vec_dilations);
  argument.AddAttribute("dilations", attr_dilations);
  pir::Attribute attr_data_format = pir::StrAttribute::get(pir::IrContext::Instance(), data_format);
  argument.AddAttribute("data_format", attr_data_format);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;
  paddle::dialect::DenseTensorType filter = filter_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)filter;
  paddle::dialect::DenseTensorType grad_out = grad_out_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)grad_out;
  paddle::dialect::DenseTensorType grad_x_grad = grad_x_grad_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)grad_x_grad;
  paddle::dialect::DenseTensorType grad_filter_grad = grad_filter_grad_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)grad_filter_grad;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);

  VLOG(4) << "Builder construction  dense_filter";
  paddle::dialect::IrTensor ir_tensor_filter(paddle::dialect::TransToPhiDataType(filter.dtype()),
                                                      filter.dims(),
                                                      filter.data_layout(),
                                                      filter.lod(),
                                                      filter.offset());
  VLOG(4) << "Builder construction  meta_filter";
  paddle::dialect::IrMetaTensor meta_filter(&ir_tensor_filter);

  VLOG(4) << "Builder construction  dense_grad_out";
  paddle::dialect::IrTensor ir_tensor_grad_out(paddle::dialect::TransToPhiDataType(grad_out.dtype()),
                                                      grad_out.dims(),
                                                      grad_out.data_layout(),
                                                      grad_out.lod(),
                                                      grad_out.offset());
  VLOG(4) << "Builder construction  meta_grad_out";
  paddle::dialect::IrMetaTensor meta_grad_out(&ir_tensor_grad_out);

  VLOG(4) << "Builder construction  dense_grad_x_grad";
  paddle::dialect::IrTensor ir_tensor_grad_x_grad(paddle::dialect::TransToPhiDataType(grad_x_grad.dtype()),
                                                      grad_x_grad.dims(),
                                                      grad_x_grad.data_layout(),
                                                      grad_x_grad.lod(),
                                                      grad_x_grad.offset());
  VLOG(4) << "Builder construction  meta_grad_x_grad";
  paddle::dialect::IrMetaTensor meta_grad_x_grad(&ir_tensor_grad_x_grad);

  VLOG(4) << "Builder construction  dense_grad_filter_grad";
  paddle::dialect::IrTensor ir_tensor_grad_filter_grad(paddle::dialect::TransToPhiDataType(grad_filter_grad.dtype()),
                                                      grad_filter_grad.dims(),
                                                      grad_filter_grad.data_layout(),
                                                      grad_filter_grad.lod(),
                                                      grad_filter_grad.offset());
  VLOG(4) << "Builder construction  meta_grad_filter_grad";
  paddle::dialect::IrMetaTensor meta_grad_filter_grad(&ir_tensor_grad_filter_grad);
  paddle::dialect::IrTensor dense_x_grad;
  paddle::dialect::IrMetaTensor meta_x_grad(&dense_x_grad);
  paddle::dialect::IrTensor dense_filter_grad;
  paddle::dialect::IrMetaTensor meta_filter_grad(&dense_filter_grad);
  paddle::dialect::IrTensor dense_grad_out_grad;
  paddle::dialect::IrMetaTensor meta_grad_out_grad(&dense_grad_out_grad);

  phi::Conv2dTransposeDoubleGradInferMeta(meta_x, meta_filter, meta_grad_out, meta_grad_x_grad, meta_grad_filter_grad, strides, paddings, output_padding, output_size, padding_algorithm, groups, dilations, data_format, &meta_x_grad, &meta_filter_grad, &meta_grad_out_grad);

  std::vector<pir::Type> argument_outputs;
  pir::Type x_grad_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_x_grad.dtype()), dense_x_grad.dims(), dense_x_grad.layout(), dense_x_grad.lod(), dense_x_grad.offset());
  argument_outputs.push_back(x_grad_dense_tensor_type);

  pir::Type filter_grad_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_filter_grad.dtype()), dense_filter_grad.dims(), dense_filter_grad.layout(), dense_filter_grad.lod(), dense_filter_grad.offset());
  argument_outputs.push_back(filter_grad_dense_tensor_type);

  pir::Type grad_out_grad_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_grad_out_grad.dtype()), dense_grad_out_grad.dims(), dense_grad_out_grad.layout(), dense_grad_out_grad.lod(), dense_grad_out_grad.offset());
  argument_outputs.push_back(grad_out_grad_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void Conv2dTransposeDoubleGradOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, pir::Value filter_, pir::Value grad_out_, pir::Value grad_x_grad_, pir::Value grad_filter_grad_, pir::Value output_size_, const std::vector<int>& strides, const std::vector<int>& paddings, const std::vector<int>& output_padding, const std::string& padding_algorithm, int groups, const std::vector<int>& dilations, const std::string& data_format) {
  VLOG(4) << "Start build Conv2dTransposeDoubleGradOp";



  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_, filter_, grad_out_, grad_x_grad_, grad_filter_grad_, output_size_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  std::vector<pir::Attribute> vec_strides;
  for (size_t i = 0; i < static_cast<size_t>(strides.size()); i++) {
      pir::Attribute attr_strides = pir::Int32Attribute::get(pir::IrContext::Instance(), strides[i]);

    vec_strides.push_back(attr_strides);
  }
  pir::Attribute attr_strides = pir::ArrayAttribute::get(pir::IrContext::Instance(), vec_strides);
  argument.AddAttribute("strides", attr_strides);
  std::vector<pir::Attribute> vec_paddings;
  for (size_t i = 0; i < static_cast<size_t>(paddings.size()); i++) {
      pir::Attribute attr_paddings = pir::Int32Attribute::get(pir::IrContext::Instance(), paddings[i]);

    vec_paddings.push_back(attr_paddings);
  }
  pir::Attribute attr_paddings = pir::ArrayAttribute::get(pir::IrContext::Instance(), vec_paddings);
  argument.AddAttribute("paddings", attr_paddings);
  std::vector<pir::Attribute> vec_output_padding;
  for (size_t i = 0; i < static_cast<size_t>(output_padding.size()); i++) {
      pir::Attribute attr_output_padding = pir::Int32Attribute::get(pir::IrContext::Instance(), output_padding[i]);

    vec_output_padding.push_back(attr_output_padding);
  }
  pir::Attribute attr_output_padding = pir::ArrayAttribute::get(pir::IrContext::Instance(), vec_output_padding);
  argument.AddAttribute("output_padding", attr_output_padding);
  pir::Attribute attr_padding_algorithm = pir::StrAttribute::get(pir::IrContext::Instance(), padding_algorithm);
  argument.AddAttribute("padding_algorithm", attr_padding_algorithm);
  pir::Attribute attr_groups = pir::Int32Attribute::get(pir::IrContext::Instance(), groups);
  argument.AddAttribute("groups", attr_groups);
  std::vector<pir::Attribute> vec_dilations;
  for (size_t i = 0; i < static_cast<size_t>(dilations.size()); i++) {
      pir::Attribute attr_dilations = pir::Int32Attribute::get(pir::IrContext::Instance(), dilations[i]);

    vec_dilations.push_back(attr_dilations);
  }
  pir::Attribute attr_dilations = pir::ArrayAttribute::get(pir::IrContext::Instance(), vec_dilations);
  argument.AddAttribute("dilations", attr_dilations);
  pir::Attribute attr_data_format = pir::StrAttribute::get(pir::IrContext::Instance(), data_format);
  argument.AddAttribute("data_format", attr_data_format);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;
  paddle::dialect::DenseTensorType filter = filter_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)filter;
  paddle::dialect::DenseTensorType grad_out = grad_out_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)grad_out;
  paddle::dialect::DenseTensorType grad_x_grad = grad_x_grad_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)grad_x_grad;
  paddle::dialect::DenseTensorType grad_filter_grad = grad_filter_grad_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)grad_filter_grad;
  phi::IntArray output_size;
  if (output_size_.dyn_cast<pir::OpResult>() && output_size_.dyn_cast<pir::OpResult>().owner()->isa<paddle::dialect::FullIntArrayOp>()) {
    output_size = std::move(phi::IntArray(paddle::dialect::GetInt64Vector(
                          output_size_.dyn_cast<pir::OpResult>().owner()
                          ->dyn_cast<paddle::dialect::FullIntArrayOp>()
                          .attribute("value"))));
  } else if (output_size_.type().isa<pir::VectorType>()) {
    size_t output_size_size = output_size_.type().dyn_cast<pir::VectorType>().size();
    output_size = std::move(phi::IntArray(std::vector<int64_t>(output_size_size, -1)));
    output_size.SetFromTensor(true);
  } else if (output_size_.type().isa<paddle::dialect::DenseTensorType>()) {
    common::DDim output_size_dim = output_size_.type().dyn_cast<paddle::dialect::DenseTensorType>().dims();
    size_t output_size_size = common::product(output_size_dim);
    if (common::contain_unknown_dim(output_size_dim)) {
      output_size_size = 1;
    }
    output_size = std::move(phi::IntArray(std::vector<int64_t>(output_size_size, -1)));
    output_size.SetFromTensor(true);
  } else {
    PADDLE_THROW(phi::errors::Unimplemented("Only support VectorType or DenseTensorType"));
  }


  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);

  VLOG(4) << "Builder construction  dense_filter";
  paddle::dialect::IrTensor ir_tensor_filter(paddle::dialect::TransToPhiDataType(filter.dtype()),
                                                      filter.dims(),
                                                      filter.data_layout(),
                                                      filter.lod(),
                                                      filter.offset());
  VLOG(4) << "Builder construction  meta_filter";
  paddle::dialect::IrMetaTensor meta_filter(&ir_tensor_filter);

  VLOG(4) << "Builder construction  dense_grad_out";
  paddle::dialect::IrTensor ir_tensor_grad_out(paddle::dialect::TransToPhiDataType(grad_out.dtype()),
                                                      grad_out.dims(),
                                                      grad_out.data_layout(),
                                                      grad_out.lod(),
                                                      grad_out.offset());
  VLOG(4) << "Builder construction  meta_grad_out";
  paddle::dialect::IrMetaTensor meta_grad_out(&ir_tensor_grad_out);

  VLOG(4) << "Builder construction  dense_grad_x_grad";
  paddle::dialect::IrTensor ir_tensor_grad_x_grad(paddle::dialect::TransToPhiDataType(grad_x_grad.dtype()),
                                                      grad_x_grad.dims(),
                                                      grad_x_grad.data_layout(),
                                                      grad_x_grad.lod(),
                                                      grad_x_grad.offset());
  VLOG(4) << "Builder construction  meta_grad_x_grad";
  paddle::dialect::IrMetaTensor meta_grad_x_grad(&ir_tensor_grad_x_grad);

  VLOG(4) << "Builder construction  dense_grad_filter_grad";
  paddle::dialect::IrTensor ir_tensor_grad_filter_grad(paddle::dialect::TransToPhiDataType(grad_filter_grad.dtype()),
                                                      grad_filter_grad.dims(),
                                                      grad_filter_grad.data_layout(),
                                                      grad_filter_grad.lod(),
                                                      grad_filter_grad.offset());
  VLOG(4) << "Builder construction  meta_grad_filter_grad";
  paddle::dialect::IrMetaTensor meta_grad_filter_grad(&ir_tensor_grad_filter_grad);
  paddle::dialect::IrTensor dense_x_grad;
  paddle::dialect::IrMetaTensor meta_x_grad(&dense_x_grad);
  paddle::dialect::IrTensor dense_filter_grad;
  paddle::dialect::IrMetaTensor meta_filter_grad(&dense_filter_grad);
  paddle::dialect::IrTensor dense_grad_out_grad;
  paddle::dialect::IrMetaTensor meta_grad_out_grad(&dense_grad_out_grad);

  phi::Conv2dTransposeDoubleGradInferMeta(meta_x, meta_filter, meta_grad_out, meta_grad_x_grad, meta_grad_filter_grad, strides, paddings, output_padding, output_size, padding_algorithm, groups, dilations, data_format, &meta_x_grad, &meta_filter_grad, &meta_grad_out_grad);

  std::vector<pir::Type> argument_outputs;
  pir::Type x_grad_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_x_grad.dtype()), dense_x_grad.dims(), dense_x_grad.layout(), dense_x_grad.lod(), dense_x_grad.offset());
  argument_outputs.push_back(x_grad_dense_tensor_type);

  pir::Type filter_grad_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_filter_grad.dtype()), dense_filter_grad.dims(), dense_filter_grad.layout(), dense_filter_grad.lod(), dense_filter_grad.offset());
  argument_outputs.push_back(filter_grad_dense_tensor_type);

  pir::Type grad_out_grad_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_grad_out_grad.dtype()), dense_grad_out_grad.dims(), dense_grad_out_grad.layout(), dense_grad_out_grad.lod(), dense_grad_out_grad.offset());
  argument_outputs.push_back(grad_out_grad_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void Conv2dTransposeDoubleGradOp::VerifySig() {}

void Conv2dTransposeDoubleGradOp::InferMeta( phi::InferMetaContext *infer_meta ) {
  auto fn = PD_INFER_META(phi::Conv2dTransposeDoubleGradInferMeta);
  fn(infer_meta);
}

phi::DataType Conv2dTransposeDoubleGradOp::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: Conv2dTransposeDoubleGradOp";
  


  return expected_kernel_dtype;
}

const char *Conv2dTransposeGradOp::attributes_name[7] = { "strides", "paddings", "output_padding", "padding_algorithm", "groups", "dilations", "data_format" };

OpInfoTuple Conv2dTransposeGradOp::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("x", "paddle::dialect::DenseTensorType", false, false, false, true), paddle::dialect::OpInputInfo("filter", "paddle::dialect::DenseTensorType", false, false, false, true), paddle::dialect::OpInputInfo("out_grad", "paddle::dialect::DenseTensorType", false, false, false, true), paddle::dialect::OpInputInfo("output_size", "paddle::dialect::IntArrayAttribute", false, false, true, false) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = { paddle::dialect::OpAttributeInfo("strides", "pir::ArrayAttribute<pir::Int32Attribute>", ""), paddle::dialect::OpAttributeInfo("paddings", "pir::ArrayAttribute<pir::Int32Attribute>", ""), paddle::dialect::OpAttributeInfo("output_padding", "pir::ArrayAttribute<pir::Int32Attribute>", ""), paddle::dialect::OpAttributeInfo("padding_algorithm", "pir::StrAttribute", ""), paddle::dialect::OpAttributeInfo("groups", "pir::Int32Attribute", ""), paddle::dialect::OpAttributeInfo("dilations", "pir::ArrayAttribute<pir::Int32Attribute>", ""), paddle::dialect::OpAttributeInfo("data_format", "pir::StrAttribute", "") };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("x_grad", "paddle::dialect::DenseTensorType", false, false), paddle::dialect::OpOutputInfo("filter_grad", "paddle::dialect::DenseTensorType", false, false) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("Conv2dTransposeGradInferMeta", {"x", "filter", "out_grad", "strides", "paddings", "output_padding", "output_size", "padding_algorithm", "groups", "dilations", "data_format"}, "conv2d_transpose_grad", {"x", "filter", "out_grad", "strides", "paddings", "output_padding", "output_size", "padding_algorithm", "groups", "dilations", "data_format"}, {"x"}, {}, {}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "conv2d_transpose_grad");
}

void Conv2dTransposeGradOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, pir::Value filter_, pir::Value out_grad_, const std::vector<int>& strides, const std::vector<int>& paddings, const std::vector<int>& output_padding, const std::vector<int64_t>& output_size, const std::string& padding_algorithm, int groups, const std::vector<int>& dilations, const std::string& data_format) {
  VLOG(4) << "Start build Conv2dTransposeGradOp";


  // Generate int_array mutable attribute: output_size
  paddle::dialect::FullIntArrayOp full_output_size_op = builder.Build<paddle::dialect::FullIntArrayOp>(output_size, phi::DataType::INT64, phi::CPUPlace());
  pir::OpResult output_size_ = full_output_size_op->result(0);
    
  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_, filter_, out_grad_, output_size_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  std::vector<pir::Attribute> vec_strides;
  for (size_t i = 0; i < static_cast<size_t>(strides.size()); i++) {
      pir::Attribute attr_strides = pir::Int32Attribute::get(pir::IrContext::Instance(), strides[i]);

    vec_strides.push_back(attr_strides);
  }
  pir::Attribute attr_strides = pir::ArrayAttribute::get(pir::IrContext::Instance(), vec_strides);
  argument.AddAttribute("strides", attr_strides);
  std::vector<pir::Attribute> vec_paddings;
  for (size_t i = 0; i < static_cast<size_t>(paddings.size()); i++) {
      pir::Attribute attr_paddings = pir::Int32Attribute::get(pir::IrContext::Instance(), paddings[i]);

    vec_paddings.push_back(attr_paddings);
  }
  pir::Attribute attr_paddings = pir::ArrayAttribute::get(pir::IrContext::Instance(), vec_paddings);
  argument.AddAttribute("paddings", attr_paddings);
  std::vector<pir::Attribute> vec_output_padding;
  for (size_t i = 0; i < static_cast<size_t>(output_padding.size()); i++) {
      pir::Attribute attr_output_padding = pir::Int32Attribute::get(pir::IrContext::Instance(), output_padding[i]);

    vec_output_padding.push_back(attr_output_padding);
  }
  pir::Attribute attr_output_padding = pir::ArrayAttribute::get(pir::IrContext::Instance(), vec_output_padding);
  argument.AddAttribute("output_padding", attr_output_padding);
  pir::Attribute attr_padding_algorithm = pir::StrAttribute::get(pir::IrContext::Instance(), padding_algorithm);
  argument.AddAttribute("padding_algorithm", attr_padding_algorithm);
  pir::Attribute attr_groups = pir::Int32Attribute::get(pir::IrContext::Instance(), groups);
  argument.AddAttribute("groups", attr_groups);
  std::vector<pir::Attribute> vec_dilations;
  for (size_t i = 0; i < static_cast<size_t>(dilations.size()); i++) {
      pir::Attribute attr_dilations = pir::Int32Attribute::get(pir::IrContext::Instance(), dilations[i]);

    vec_dilations.push_back(attr_dilations);
  }
  pir::Attribute attr_dilations = pir::ArrayAttribute::get(pir::IrContext::Instance(), vec_dilations);
  argument.AddAttribute("dilations", attr_dilations);
  pir::Attribute attr_data_format = pir::StrAttribute::get(pir::IrContext::Instance(), data_format);
  argument.AddAttribute("data_format", attr_data_format);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;
  paddle::dialect::DenseTensorType filter = filter_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)filter;
  paddle::dialect::DenseTensorType out_grad = out_grad_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)out_grad;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);

  VLOG(4) << "Builder construction  dense_filter";
  paddle::dialect::IrTensor ir_tensor_filter(paddle::dialect::TransToPhiDataType(filter.dtype()),
                                                      filter.dims(),
                                                      filter.data_layout(),
                                                      filter.lod(),
                                                      filter.offset());
  VLOG(4) << "Builder construction  meta_filter";
  paddle::dialect::IrMetaTensor meta_filter(&ir_tensor_filter);

  VLOG(4) << "Builder construction  dense_out_grad";
  paddle::dialect::IrTensor ir_tensor_out_grad(paddle::dialect::TransToPhiDataType(out_grad.dtype()),
                                                      out_grad.dims(),
                                                      out_grad.data_layout(),
                                                      out_grad.lod(),
                                                      out_grad.offset());
  VLOG(4) << "Builder construction  meta_out_grad";
  paddle::dialect::IrMetaTensor meta_out_grad(&ir_tensor_out_grad);
  paddle::dialect::IrTensor dense_x_grad;
  paddle::dialect::IrMetaTensor meta_x_grad(&dense_x_grad);
  paddle::dialect::IrTensor dense_filter_grad;
  paddle::dialect::IrMetaTensor meta_filter_grad(&dense_filter_grad);

  phi::Conv2dTransposeGradInferMeta(meta_x, meta_filter, meta_out_grad, strides, paddings, output_padding, output_size, padding_algorithm, groups, dilations, data_format, &meta_x_grad, &meta_filter_grad);

  std::vector<pir::Type> argument_outputs;
  pir::Type x_grad_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_x_grad.dtype()), dense_x_grad.dims(), dense_x_grad.layout(), dense_x_grad.lod(), dense_x_grad.offset());
  argument_outputs.push_back(x_grad_dense_tensor_type);

  pir::Type filter_grad_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_filter_grad.dtype()), dense_filter_grad.dims(), dense_filter_grad.layout(), dense_filter_grad.lod(), dense_filter_grad.offset());
  argument_outputs.push_back(filter_grad_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void Conv2dTransposeGradOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, pir::Value filter_, pir::Value out_grad_, pir::AttributeMap attributes) {
  VLOG(4) << "Start build Conv2dTransposeGradOp";


  IR_ENFORCE(
      attributes.find("strides") != attributes.end(),
          "'strides' Attribute is expected for Conv2dTransposeGradOp. ");
  std::vector<int> strides;
  for (size_t i = 0; i < attributes.at("strides").dyn_cast<pir::ArrayAttribute>().size(); i++) {
    strides.push_back(attributes.at("strides").dyn_cast<pir::ArrayAttribute>().at(i).dyn_cast<pir::Int32Attribute>().data());
  }

  IR_ENFORCE(
      attributes.find("paddings") != attributes.end(),
          "'paddings' Attribute is expected for Conv2dTransposeGradOp. ");
  std::vector<int> paddings;
  for (size_t i = 0; i < attributes.at("paddings").dyn_cast<pir::ArrayAttribute>().size(); i++) {
    paddings.push_back(attributes.at("paddings").dyn_cast<pir::ArrayAttribute>().at(i).dyn_cast<pir::Int32Attribute>().data());
  }

  IR_ENFORCE(
      attributes.find("output_padding") != attributes.end(),
          "'output_padding' Attribute is expected for Conv2dTransposeGradOp. ");
  std::vector<int> output_padding;
  for (size_t i = 0; i < attributes.at("output_padding").dyn_cast<pir::ArrayAttribute>().size(); i++) {
    output_padding.push_back(attributes.at("output_padding").dyn_cast<pir::ArrayAttribute>().at(i).dyn_cast<pir::Int32Attribute>().data());
  }

  IR_ENFORCE(
      attributes.find("output_size") != attributes.end(),
          "'output_size' Attribute is expected for Conv2dTransposeGradOp. ");
  std::vector<int64_t> output_size = attributes.at("output_size").dyn_cast<paddle::dialect::IntArrayAttribute>().data().GetData();

  IR_ENFORCE(
      attributes.find("padding_algorithm") != attributes.end(),
          "'padding_algorithm' Attribute is expected for Conv2dTransposeGradOp. ");
  std::string padding_algorithm = attributes.at("padding_algorithm").dyn_cast<pir::StrAttribute>().AsString();

  IR_ENFORCE(
      attributes.find("groups") != attributes.end(),
          "'groups' Attribute is expected for Conv2dTransposeGradOp. ");
  int groups = attributes.at("groups").dyn_cast<pir::Int32Attribute>().data();

  IR_ENFORCE(
      attributes.find("dilations") != attributes.end(),
          "'dilations' Attribute is expected for Conv2dTransposeGradOp. ");
  std::vector<int> dilations;
  for (size_t i = 0; i < attributes.at("dilations").dyn_cast<pir::ArrayAttribute>().size(); i++) {
    dilations.push_back(attributes.at("dilations").dyn_cast<pir::ArrayAttribute>().at(i).dyn_cast<pir::Int32Attribute>().data());
  }

  IR_ENFORCE(
      attributes.find("data_format") != attributes.end(),
          "'data_format' Attribute is expected for Conv2dTransposeGradOp. ");
  std::string data_format = attributes.at("data_format").dyn_cast<pir::StrAttribute>().AsString();

  // Generate int_array mutable attribute: output_size
  paddle::dialect::FullIntArrayOp full_output_size_op = builder.Build<paddle::dialect::FullIntArrayOp>(output_size, phi::DataType::INT64, phi::CPUPlace());
  pir::OpResult output_size_ = full_output_size_op->result(0);
    
  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_, filter_, out_grad_, output_size_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  std::vector<pir::Attribute> vec_strides;
  for (size_t i = 0; i < static_cast<size_t>(strides.size()); i++) {
      pir::Attribute attr_strides = pir::Int32Attribute::get(pir::IrContext::Instance(), strides[i]);

    vec_strides.push_back(attr_strides);
  }
  pir::Attribute attr_strides = pir::ArrayAttribute::get(pir::IrContext::Instance(), vec_strides);
  argument.AddAttribute("strides", attr_strides);
  std::vector<pir::Attribute> vec_paddings;
  for (size_t i = 0; i < static_cast<size_t>(paddings.size()); i++) {
      pir::Attribute attr_paddings = pir::Int32Attribute::get(pir::IrContext::Instance(), paddings[i]);

    vec_paddings.push_back(attr_paddings);
  }
  pir::Attribute attr_paddings = pir::ArrayAttribute::get(pir::IrContext::Instance(), vec_paddings);
  argument.AddAttribute("paddings", attr_paddings);
  std::vector<pir::Attribute> vec_output_padding;
  for (size_t i = 0; i < static_cast<size_t>(output_padding.size()); i++) {
      pir::Attribute attr_output_padding = pir::Int32Attribute::get(pir::IrContext::Instance(), output_padding[i]);

    vec_output_padding.push_back(attr_output_padding);
  }
  pir::Attribute attr_output_padding = pir::ArrayAttribute::get(pir::IrContext::Instance(), vec_output_padding);
  argument.AddAttribute("output_padding", attr_output_padding);
  pir::Attribute attr_padding_algorithm = pir::StrAttribute::get(pir::IrContext::Instance(), padding_algorithm);
  argument.AddAttribute("padding_algorithm", attr_padding_algorithm);
  pir::Attribute attr_groups = pir::Int32Attribute::get(pir::IrContext::Instance(), groups);
  argument.AddAttribute("groups", attr_groups);
  std::vector<pir::Attribute> vec_dilations;
  for (size_t i = 0; i < static_cast<size_t>(dilations.size()); i++) {
      pir::Attribute attr_dilations = pir::Int32Attribute::get(pir::IrContext::Instance(), dilations[i]);

    vec_dilations.push_back(attr_dilations);
  }
  pir::Attribute attr_dilations = pir::ArrayAttribute::get(pir::IrContext::Instance(), vec_dilations);
  argument.AddAttribute("dilations", attr_dilations);
  pir::Attribute attr_data_format = pir::StrAttribute::get(pir::IrContext::Instance(), data_format);
  argument.AddAttribute("data_format", attr_data_format);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;
  paddle::dialect::DenseTensorType filter = filter_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)filter;
  paddle::dialect::DenseTensorType out_grad = out_grad_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)out_grad;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);

  VLOG(4) << "Builder construction  dense_filter";
  paddle::dialect::IrTensor ir_tensor_filter(paddle::dialect::TransToPhiDataType(filter.dtype()),
                                                      filter.dims(),
                                                      filter.data_layout(),
                                                      filter.lod(),
                                                      filter.offset());
  VLOG(4) << "Builder construction  meta_filter";
  paddle::dialect::IrMetaTensor meta_filter(&ir_tensor_filter);

  VLOG(4) << "Builder construction  dense_out_grad";
  paddle::dialect::IrTensor ir_tensor_out_grad(paddle::dialect::TransToPhiDataType(out_grad.dtype()),
                                                      out_grad.dims(),
                                                      out_grad.data_layout(),
                                                      out_grad.lod(),
                                                      out_grad.offset());
  VLOG(4) << "Builder construction  meta_out_grad";
  paddle::dialect::IrMetaTensor meta_out_grad(&ir_tensor_out_grad);
  paddle::dialect::IrTensor dense_x_grad;
  paddle::dialect::IrMetaTensor meta_x_grad(&dense_x_grad);
  paddle::dialect::IrTensor dense_filter_grad;
  paddle::dialect::IrMetaTensor meta_filter_grad(&dense_filter_grad);

  phi::Conv2dTransposeGradInferMeta(meta_x, meta_filter, meta_out_grad, strides, paddings, output_padding, output_size, padding_algorithm, groups, dilations, data_format, &meta_x_grad, &meta_filter_grad);

  std::vector<pir::Type> argument_outputs;
  pir::Type x_grad_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_x_grad.dtype()), dense_x_grad.dims(), dense_x_grad.layout(), dense_x_grad.lod(), dense_x_grad.offset());
  argument_outputs.push_back(x_grad_dense_tensor_type);

  pir::Type filter_grad_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_filter_grad.dtype()), dense_filter_grad.dims(), dense_filter_grad.layout(), dense_filter_grad.lod(), dense_filter_grad.offset());
  argument_outputs.push_back(filter_grad_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void Conv2dTransposeGradOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, pir::Value filter_, pir::Value out_grad_, pir::Value output_size_, const std::vector<int>& strides, const std::vector<int>& paddings, const std::vector<int>& output_padding, const std::string& padding_algorithm, int groups, const std::vector<int>& dilations, const std::string& data_format) {
  VLOG(4) << "Start build Conv2dTransposeGradOp";



  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_, filter_, out_grad_, output_size_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  std::vector<pir::Attribute> vec_strides;
  for (size_t i = 0; i < static_cast<size_t>(strides.size()); i++) {
      pir::Attribute attr_strides = pir::Int32Attribute::get(pir::IrContext::Instance(), strides[i]);

    vec_strides.push_back(attr_strides);
  }
  pir::Attribute attr_strides = pir::ArrayAttribute::get(pir::IrContext::Instance(), vec_strides);
  argument.AddAttribute("strides", attr_strides);
  std::vector<pir::Attribute> vec_paddings;
  for (size_t i = 0; i < static_cast<size_t>(paddings.size()); i++) {
      pir::Attribute attr_paddings = pir::Int32Attribute::get(pir::IrContext::Instance(), paddings[i]);

    vec_paddings.push_back(attr_paddings);
  }
  pir::Attribute attr_paddings = pir::ArrayAttribute::get(pir::IrContext::Instance(), vec_paddings);
  argument.AddAttribute("paddings", attr_paddings);
  std::vector<pir::Attribute> vec_output_padding;
  for (size_t i = 0; i < static_cast<size_t>(output_padding.size()); i++) {
      pir::Attribute attr_output_padding = pir::Int32Attribute::get(pir::IrContext::Instance(), output_padding[i]);

    vec_output_padding.push_back(attr_output_padding);
  }
  pir::Attribute attr_output_padding = pir::ArrayAttribute::get(pir::IrContext::Instance(), vec_output_padding);
  argument.AddAttribute("output_padding", attr_output_padding);
  pir::Attribute attr_padding_algorithm = pir::StrAttribute::get(pir::IrContext::Instance(), padding_algorithm);
  argument.AddAttribute("padding_algorithm", attr_padding_algorithm);
  pir::Attribute attr_groups = pir::Int32Attribute::get(pir::IrContext::Instance(), groups);
  argument.AddAttribute("groups", attr_groups);
  std::vector<pir::Attribute> vec_dilations;
  for (size_t i = 0; i < static_cast<size_t>(dilations.size()); i++) {
      pir::Attribute attr_dilations = pir::Int32Attribute::get(pir::IrContext::Instance(), dilations[i]);

    vec_dilations.push_back(attr_dilations);
  }
  pir::Attribute attr_dilations = pir::ArrayAttribute::get(pir::IrContext::Instance(), vec_dilations);
  argument.AddAttribute("dilations", attr_dilations);
  pir::Attribute attr_data_format = pir::StrAttribute::get(pir::IrContext::Instance(), data_format);
  argument.AddAttribute("data_format", attr_data_format);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;
  paddle::dialect::DenseTensorType filter = filter_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)filter;
  paddle::dialect::DenseTensorType out_grad = out_grad_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)out_grad;
  phi::IntArray output_size;
  if (output_size_.dyn_cast<pir::OpResult>() && output_size_.dyn_cast<pir::OpResult>().owner()->isa<paddle::dialect::FullIntArrayOp>()) {
    output_size = std::move(phi::IntArray(paddle::dialect::GetInt64Vector(
                          output_size_.dyn_cast<pir::OpResult>().owner()
                          ->dyn_cast<paddle::dialect::FullIntArrayOp>()
                          .attribute("value"))));
  } else if (output_size_.type().isa<pir::VectorType>()) {
    size_t output_size_size = output_size_.type().dyn_cast<pir::VectorType>().size();
    output_size = std::move(phi::IntArray(std::vector<int64_t>(output_size_size, -1)));
    output_size.SetFromTensor(true);
  } else if (output_size_.type().isa<paddle::dialect::DenseTensorType>()) {
    common::DDim output_size_dim = output_size_.type().dyn_cast<paddle::dialect::DenseTensorType>().dims();
    size_t output_size_size = common::product(output_size_dim);
    if (common::contain_unknown_dim(output_size_dim)) {
      output_size_size = 1;
    }
    output_size = std::move(phi::IntArray(std::vector<int64_t>(output_size_size, -1)));
    output_size.SetFromTensor(true);
  } else {
    PADDLE_THROW(phi::errors::Unimplemented("Only support VectorType or DenseTensorType"));
  }


  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);

  VLOG(4) << "Builder construction  dense_filter";
  paddle::dialect::IrTensor ir_tensor_filter(paddle::dialect::TransToPhiDataType(filter.dtype()),
                                                      filter.dims(),
                                                      filter.data_layout(),
                                                      filter.lod(),
                                                      filter.offset());
  VLOG(4) << "Builder construction  meta_filter";
  paddle::dialect::IrMetaTensor meta_filter(&ir_tensor_filter);

  VLOG(4) << "Builder construction  dense_out_grad";
  paddle::dialect::IrTensor ir_tensor_out_grad(paddle::dialect::TransToPhiDataType(out_grad.dtype()),
                                                      out_grad.dims(),
                                                      out_grad.data_layout(),
                                                      out_grad.lod(),
                                                      out_grad.offset());
  VLOG(4) << "Builder construction  meta_out_grad";
  paddle::dialect::IrMetaTensor meta_out_grad(&ir_tensor_out_grad);
  paddle::dialect::IrTensor dense_x_grad;
  paddle::dialect::IrMetaTensor meta_x_grad(&dense_x_grad);
  paddle::dialect::IrTensor dense_filter_grad;
  paddle::dialect::IrMetaTensor meta_filter_grad(&dense_filter_grad);

  phi::Conv2dTransposeGradInferMeta(meta_x, meta_filter, meta_out_grad, strides, paddings, output_padding, output_size, padding_algorithm, groups, dilations, data_format, &meta_x_grad, &meta_filter_grad);

  std::vector<pir::Type> argument_outputs;
  pir::Type x_grad_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_x_grad.dtype()), dense_x_grad.dims(), dense_x_grad.layout(), dense_x_grad.lod(), dense_x_grad.offset());
  argument_outputs.push_back(x_grad_dense_tensor_type);

  pir::Type filter_grad_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_filter_grad.dtype()), dense_filter_grad.dims(), dense_filter_grad.layout(), dense_filter_grad.lod(), dense_filter_grad.offset());
  argument_outputs.push_back(filter_grad_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void Conv2dTransposeGradOp::VerifySig() {}

void Conv2dTransposeGradOp::InferMeta( phi::InferMetaContext *infer_meta ) {
  auto fn = PD_INFER_META(phi::Conv2dTransposeGradInferMeta);
  fn(infer_meta);
}

phi::DataType Conv2dTransposeGradOp::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: Conv2dTransposeGradOp";
  


  return expected_kernel_dtype;
}

const char *DeformableConvGradOp::attributes_name[6] = { "strides", "paddings", "dilations", "deformable_groups", "groups", "im2col_step" };

OpInfoTuple DeformableConvGradOp::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("x", "paddle::dialect::DenseTensorType", false, false, false, false), paddle::dialect::OpInputInfo("offset", "paddle::dialect::DenseTensorType", false, false, false, false), paddle::dialect::OpInputInfo("filter", "paddle::dialect::DenseTensorType", false, false, false, false), paddle::dialect::OpInputInfo("mask", "paddle::dialect::DenseTensorType", true, false, false, false), paddle::dialect::OpInputInfo("out_grad", "paddle::dialect::DenseTensorType", false, false, false, false) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = { paddle::dialect::OpAttributeInfo("strides", "pir::ArrayAttribute<pir::Int32Attribute>", ""), paddle::dialect::OpAttributeInfo("paddings", "pir::ArrayAttribute<pir::Int32Attribute>", ""), paddle::dialect::OpAttributeInfo("dilations", "pir::ArrayAttribute<pir::Int32Attribute>", ""), paddle::dialect::OpAttributeInfo("deformable_groups", "pir::Int32Attribute", ""), paddle::dialect::OpAttributeInfo("groups", "pir::Int32Attribute", ""), paddle::dialect::OpAttributeInfo("im2col_step", "pir::Int32Attribute", "") };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("x_grad", "paddle::dialect::DenseTensorType", false, false), paddle::dialect::OpOutputInfo("offset_grad", "paddle::dialect::DenseTensorType", false, false), paddle::dialect::OpOutputInfo("filter_grad", "paddle::dialect::DenseTensorType", false, false), paddle::dialect::OpOutputInfo("mask_grad", "paddle::dialect::DenseTensorType", false, false) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("DeformableConvGradInferMeta", {"x", "offset", "filter", "mask", "out_grad", "strides", "paddings", "dilations", "deformable_groups", "groups", "im2col_step"}, "deformable_conv_grad", {"x", "offset", "filter", "mask", "out_grad", "strides", "paddings", "dilations", "deformable_groups", "groups", "im2col_step"}, {"x"}, {}, {}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "deformable_conv_grad");
}

void DeformableConvGradOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, pir::Value offset_, pir::Value filter_, pir::Value mask_, pir::Value out_grad_, const std::vector<int>& strides, const std::vector<int>& paddings, const std::vector<int>& dilations, int deformable_groups, int groups, int im2col_step) {
  VLOG(4) << "Start build DeformableConvGradOp";



  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_, offset_, filter_, mask_, out_grad_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  std::vector<pir::Attribute> vec_strides;
  for (size_t i = 0; i < static_cast<size_t>(strides.size()); i++) {
      pir::Attribute attr_strides = pir::Int32Attribute::get(pir::IrContext::Instance(), strides[i]);

    vec_strides.push_back(attr_strides);
  }
  pir::Attribute attr_strides = pir::ArrayAttribute::get(pir::IrContext::Instance(), vec_strides);
  argument.AddAttribute("strides", attr_strides);
  std::vector<pir::Attribute> vec_paddings;
  for (size_t i = 0; i < static_cast<size_t>(paddings.size()); i++) {
      pir::Attribute attr_paddings = pir::Int32Attribute::get(pir::IrContext::Instance(), paddings[i]);

    vec_paddings.push_back(attr_paddings);
  }
  pir::Attribute attr_paddings = pir::ArrayAttribute::get(pir::IrContext::Instance(), vec_paddings);
  argument.AddAttribute("paddings", attr_paddings);
  std::vector<pir::Attribute> vec_dilations;
  for (size_t i = 0; i < static_cast<size_t>(dilations.size()); i++) {
      pir::Attribute attr_dilations = pir::Int32Attribute::get(pir::IrContext::Instance(), dilations[i]);

    vec_dilations.push_back(attr_dilations);
  }
  pir::Attribute attr_dilations = pir::ArrayAttribute::get(pir::IrContext::Instance(), vec_dilations);
  argument.AddAttribute("dilations", attr_dilations);
  pir::Attribute attr_deformable_groups = pir::Int32Attribute::get(pir::IrContext::Instance(), deformable_groups);
  argument.AddAttribute("deformable_groups", attr_deformable_groups);
  pir::Attribute attr_groups = pir::Int32Attribute::get(pir::IrContext::Instance(), groups);
  argument.AddAttribute("groups", attr_groups);
  pir::Attribute attr_im2col_step = pir::Int32Attribute::get(pir::IrContext::Instance(), im2col_step);
  argument.AddAttribute("im2col_step", attr_im2col_step);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;
  paddle::dialect::DenseTensorType offset = offset_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)offset;
  paddle::dialect::DenseTensorType filter = filter_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)filter;
  paddle::dialect::DenseTensorType out_grad = out_grad_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)out_grad;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);

  VLOG(4) << "Builder construction  dense_offset";
  paddle::dialect::IrTensor ir_tensor_offset(paddle::dialect::TransToPhiDataType(offset.dtype()),
                                                      offset.dims(),
                                                      offset.data_layout(),
                                                      offset.lod(),
                                                      offset.offset());
  VLOG(4) << "Builder construction  meta_offset";
  paddle::dialect::IrMetaTensor meta_offset(&ir_tensor_offset);

  VLOG(4) << "Builder construction  dense_filter";
  paddle::dialect::IrTensor ir_tensor_filter(paddle::dialect::TransToPhiDataType(filter.dtype()),
                                                      filter.dims(),
                                                      filter.data_layout(),
                                                      filter.lod(),
                                                      filter.offset());
  VLOG(4) << "Builder construction  meta_filter";
  paddle::dialect::IrMetaTensor meta_filter(&ir_tensor_filter);

  paddle::dialect::IrMetaTensor meta_mask;
  paddle::dialect::IrTensor ir_tensor_mask;
  if (mask_.impl() != nullptr) {
    paddle::dialect::DenseTensorType mask = mask_.type().dyn_cast<paddle::dialect::DenseTensorType>();
    VLOG(4) << "Builder construction  dense_mask";
    ir_tensor_mask = paddle::dialect::IrTensor(paddle::dialect::TransToPhiDataType(mask.dtype()),
                                                        mask.dims(),
                                                        mask.data_layout(),
                                                        mask.lod(),
                                                        mask.offset());
    VLOG(4) << "Builder construction  meta_mask";
    meta_mask = paddle::dialect::IrMetaTensor(&ir_tensor_mask);
  }


  VLOG(4) << "Builder construction  dense_out_grad";
  paddle::dialect::IrTensor ir_tensor_out_grad(paddle::dialect::TransToPhiDataType(out_grad.dtype()),
                                                      out_grad.dims(),
                                                      out_grad.data_layout(),
                                                      out_grad.lod(),
                                                      out_grad.offset());
  VLOG(4) << "Builder construction  meta_out_grad";
  paddle::dialect::IrMetaTensor meta_out_grad(&ir_tensor_out_grad);
  paddle::dialect::IrTensor dense_x_grad;
  paddle::dialect::IrMetaTensor meta_x_grad(&dense_x_grad);
  paddle::dialect::IrTensor dense_offset_grad;
  paddle::dialect::IrMetaTensor meta_offset_grad(&dense_offset_grad);
  paddle::dialect::IrTensor dense_filter_grad;
  paddle::dialect::IrMetaTensor meta_filter_grad(&dense_filter_grad);
  paddle::dialect::IrTensor dense_mask_grad;
  paddle::dialect::IrMetaTensor meta_mask_grad(&dense_mask_grad);

  phi::DeformableConvGradInferMeta(meta_x, meta_offset, meta_filter, meta_mask, meta_out_grad, strides, paddings, dilations, deformable_groups, groups, im2col_step, &meta_x_grad, &meta_offset_grad, &meta_filter_grad, &meta_mask_grad);

  std::vector<pir::Type> argument_outputs;
  pir::Type x_grad_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_x_grad.dtype()), dense_x_grad.dims(), dense_x_grad.layout(), dense_x_grad.lod(), dense_x_grad.offset());
  argument_outputs.push_back(x_grad_dense_tensor_type);

  pir::Type offset_grad_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_offset_grad.dtype()), dense_offset_grad.dims(), dense_offset_grad.layout(), dense_offset_grad.lod(), dense_offset_grad.offset());
  argument_outputs.push_back(offset_grad_dense_tensor_type);

  pir::Type filter_grad_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_filter_grad.dtype()), dense_filter_grad.dims(), dense_filter_grad.layout(), dense_filter_grad.lod(), dense_filter_grad.offset());
  argument_outputs.push_back(filter_grad_dense_tensor_type);

  pir::Type mask_grad_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_mask_grad.dtype()), dense_mask_grad.dims(), dense_mask_grad.layout(), dense_mask_grad.lod(), dense_mask_grad.offset());
  argument_outputs.push_back(mask_grad_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void DeformableConvGradOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, pir::Value offset_, pir::Value filter_, pir::Value mask_, pir::Value out_grad_, pir::AttributeMap attributes) {
  VLOG(4) << "Start build DeformableConvGradOp";


  IR_ENFORCE(
      attributes.find("strides") != attributes.end(),
          "'strides' Attribute is expected for DeformableConvGradOp. ");
  std::vector<int> strides;
  for (size_t i = 0; i < attributes.at("strides").dyn_cast<pir::ArrayAttribute>().size(); i++) {
    strides.push_back(attributes.at("strides").dyn_cast<pir::ArrayAttribute>().at(i).dyn_cast<pir::Int32Attribute>().data());
  }

  IR_ENFORCE(
      attributes.find("paddings") != attributes.end(),
          "'paddings' Attribute is expected for DeformableConvGradOp. ");
  std::vector<int> paddings;
  for (size_t i = 0; i < attributes.at("paddings").dyn_cast<pir::ArrayAttribute>().size(); i++) {
    paddings.push_back(attributes.at("paddings").dyn_cast<pir::ArrayAttribute>().at(i).dyn_cast<pir::Int32Attribute>().data());
  }

  IR_ENFORCE(
      attributes.find("dilations") != attributes.end(),
          "'dilations' Attribute is expected for DeformableConvGradOp. ");
  std::vector<int> dilations;
  for (size_t i = 0; i < attributes.at("dilations").dyn_cast<pir::ArrayAttribute>().size(); i++) {
    dilations.push_back(attributes.at("dilations").dyn_cast<pir::ArrayAttribute>().at(i).dyn_cast<pir::Int32Attribute>().data());
  }

  IR_ENFORCE(
      attributes.find("deformable_groups") != attributes.end(),
          "'deformable_groups' Attribute is expected for DeformableConvGradOp. ");
  int deformable_groups = attributes.at("deformable_groups").dyn_cast<pir::Int32Attribute>().data();

  IR_ENFORCE(
      attributes.find("groups") != attributes.end(),
          "'groups' Attribute is expected for DeformableConvGradOp. ");
  int groups = attributes.at("groups").dyn_cast<pir::Int32Attribute>().data();

  IR_ENFORCE(
      attributes.find("im2col_step") != attributes.end(),
          "'im2col_step' Attribute is expected for DeformableConvGradOp. ");
  int im2col_step = attributes.at("im2col_step").dyn_cast<pir::Int32Attribute>().data();


  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_, offset_, filter_, mask_, out_grad_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  std::vector<pir::Attribute> vec_strides;
  for (size_t i = 0; i < static_cast<size_t>(strides.size()); i++) {
      pir::Attribute attr_strides = pir::Int32Attribute::get(pir::IrContext::Instance(), strides[i]);

    vec_strides.push_back(attr_strides);
  }
  pir::Attribute attr_strides = pir::ArrayAttribute::get(pir::IrContext::Instance(), vec_strides);
  argument.AddAttribute("strides", attr_strides);
  std::vector<pir::Attribute> vec_paddings;
  for (size_t i = 0; i < static_cast<size_t>(paddings.size()); i++) {
      pir::Attribute attr_paddings = pir::Int32Attribute::get(pir::IrContext::Instance(), paddings[i]);

    vec_paddings.push_back(attr_paddings);
  }
  pir::Attribute attr_paddings = pir::ArrayAttribute::get(pir::IrContext::Instance(), vec_paddings);
  argument.AddAttribute("paddings", attr_paddings);
  std::vector<pir::Attribute> vec_dilations;
  for (size_t i = 0; i < static_cast<size_t>(dilations.size()); i++) {
      pir::Attribute attr_dilations = pir::Int32Attribute::get(pir::IrContext::Instance(), dilations[i]);

    vec_dilations.push_back(attr_dilations);
  }
  pir::Attribute attr_dilations = pir::ArrayAttribute::get(pir::IrContext::Instance(), vec_dilations);
  argument.AddAttribute("dilations", attr_dilations);
  pir::Attribute attr_deformable_groups = pir::Int32Attribute::get(pir::IrContext::Instance(), deformable_groups);
  argument.AddAttribute("deformable_groups", attr_deformable_groups);
  pir::Attribute attr_groups = pir::Int32Attribute::get(pir::IrContext::Instance(), groups);
  argument.AddAttribute("groups", attr_groups);
  pir::Attribute attr_im2col_step = pir::Int32Attribute::get(pir::IrContext::Instance(), im2col_step);
  argument.AddAttribute("im2col_step", attr_im2col_step);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;
  paddle::dialect::DenseTensorType offset = offset_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)offset;
  paddle::dialect::DenseTensorType filter = filter_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)filter;
  paddle::dialect::DenseTensorType out_grad = out_grad_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)out_grad;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);

  VLOG(4) << "Builder construction  dense_offset";
  paddle::dialect::IrTensor ir_tensor_offset(paddle::dialect::TransToPhiDataType(offset.dtype()),
                                                      offset.dims(),
                                                      offset.data_layout(),
                                                      offset.lod(),
                                                      offset.offset());
  VLOG(4) << "Builder construction  meta_offset";
  paddle::dialect::IrMetaTensor meta_offset(&ir_tensor_offset);

  VLOG(4) << "Builder construction  dense_filter";
  paddle::dialect::IrTensor ir_tensor_filter(paddle::dialect::TransToPhiDataType(filter.dtype()),
                                                      filter.dims(),
                                                      filter.data_layout(),
                                                      filter.lod(),
                                                      filter.offset());
  VLOG(4) << "Builder construction  meta_filter";
  paddle::dialect::IrMetaTensor meta_filter(&ir_tensor_filter);

  paddle::dialect::IrMetaTensor meta_mask;
  paddle::dialect::IrTensor ir_tensor_mask;
  if (mask_.impl() != nullptr) {
    paddle::dialect::DenseTensorType mask = mask_.type().dyn_cast<paddle::dialect::DenseTensorType>();
    VLOG(4) << "Builder construction  dense_mask";
    ir_tensor_mask = paddle::dialect::IrTensor(paddle::dialect::TransToPhiDataType(mask.dtype()),
                                                        mask.dims(),
                                                        mask.data_layout(),
                                                        mask.lod(),
                                                        mask.offset());
    VLOG(4) << "Builder construction  meta_mask";
    meta_mask = paddle::dialect::IrMetaTensor(&ir_tensor_mask);
  }


  VLOG(4) << "Builder construction  dense_out_grad";
  paddle::dialect::IrTensor ir_tensor_out_grad(paddle::dialect::TransToPhiDataType(out_grad.dtype()),
                                                      out_grad.dims(),
                                                      out_grad.data_layout(),
                                                      out_grad.lod(),
                                                      out_grad.offset());
  VLOG(4) << "Builder construction  meta_out_grad";
  paddle::dialect::IrMetaTensor meta_out_grad(&ir_tensor_out_grad);
  paddle::dialect::IrTensor dense_x_grad;
  paddle::dialect::IrMetaTensor meta_x_grad(&dense_x_grad);
  paddle::dialect::IrTensor dense_offset_grad;
  paddle::dialect::IrMetaTensor meta_offset_grad(&dense_offset_grad);
  paddle::dialect::IrTensor dense_filter_grad;
  paddle::dialect::IrMetaTensor meta_filter_grad(&dense_filter_grad);
  paddle::dialect::IrTensor dense_mask_grad;
  paddle::dialect::IrMetaTensor meta_mask_grad(&dense_mask_grad);

  phi::DeformableConvGradInferMeta(meta_x, meta_offset, meta_filter, meta_mask, meta_out_grad, strides, paddings, dilations, deformable_groups, groups, im2col_step, &meta_x_grad, &meta_offset_grad, &meta_filter_grad, &meta_mask_grad);

  std::vector<pir::Type> argument_outputs;
  pir::Type x_grad_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_x_grad.dtype()), dense_x_grad.dims(), dense_x_grad.layout(), dense_x_grad.lod(), dense_x_grad.offset());
  argument_outputs.push_back(x_grad_dense_tensor_type);

  pir::Type offset_grad_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_offset_grad.dtype()), dense_offset_grad.dims(), dense_offset_grad.layout(), dense_offset_grad.lod(), dense_offset_grad.offset());
  argument_outputs.push_back(offset_grad_dense_tensor_type);

  pir::Type filter_grad_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_filter_grad.dtype()), dense_filter_grad.dims(), dense_filter_grad.layout(), dense_filter_grad.lod(), dense_filter_grad.offset());
  argument_outputs.push_back(filter_grad_dense_tensor_type);

  pir::Type mask_grad_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_mask_grad.dtype()), dense_mask_grad.dims(), dense_mask_grad.layout(), dense_mask_grad.lod(), dense_mask_grad.offset());
  argument_outputs.push_back(mask_grad_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void DeformableConvGradOp::VerifySig() {}

void DeformableConvGradOp::InferMeta( phi::InferMetaContext *infer_meta ) {
  auto fn = PD_INFER_META(phi::DeformableConvGradInferMeta);
  fn(infer_meta);
}

phi::DataType DeformableConvGradOp::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: DeformableConvGradOp";
  


  return expected_kernel_dtype;
}

const char *DepthwiseConv2dTransposeGradOp::attributes_name[7] = { "strides", "paddings", "output_padding", "padding_algorithm", "groups", "dilations", "data_format" };

OpInfoTuple DepthwiseConv2dTransposeGradOp::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("x", "paddle::dialect::DenseTensorType", false, false, false, false), paddle::dialect::OpInputInfo("filter", "paddle::dialect::DenseTensorType", false, false, false, false), paddle::dialect::OpInputInfo("out_grad", "paddle::dialect::DenseTensorType", false, false, false, false), paddle::dialect::OpInputInfo("output_size", "paddle::dialect::IntArrayAttribute", false, false, true, false) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = { paddle::dialect::OpAttributeInfo("strides", "pir::ArrayAttribute<pir::Int32Attribute>", ""), paddle::dialect::OpAttributeInfo("paddings", "pir::ArrayAttribute<pir::Int32Attribute>", ""), paddle::dialect::OpAttributeInfo("output_padding", "pir::ArrayAttribute<pir::Int32Attribute>", ""), paddle::dialect::OpAttributeInfo("padding_algorithm", "pir::StrAttribute", ""), paddle::dialect::OpAttributeInfo("groups", "pir::Int32Attribute", ""), paddle::dialect::OpAttributeInfo("dilations", "pir::ArrayAttribute<pir::Int32Attribute>", ""), paddle::dialect::OpAttributeInfo("data_format", "pir::StrAttribute", "") };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("x_grad", "paddle::dialect::DenseTensorType", false, false), paddle::dialect::OpOutputInfo("filter_grad", "paddle::dialect::DenseTensorType", false, false) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("Conv2dTransposeGradInferMeta", {"x", "filter", "out_grad", "strides", "paddings", "output_padding", "output_size", "padding_algorithm", "groups", "dilations", "data_format"}, "depthwise_conv2d_transpose_grad", {"x", "filter", "out_grad", "strides", "paddings", "output_padding", "output_size", "padding_algorithm", "groups", "dilations", "data_format"}, {"x"}, {}, {}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "depthwise_conv2d_transpose_grad");
}

void DepthwiseConv2dTransposeGradOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, pir::Value filter_, pir::Value out_grad_, const std::vector<int>& strides, const std::vector<int>& paddings, const std::vector<int>& output_padding, const std::vector<int64_t>& output_size, const std::string& padding_algorithm, int groups, const std::vector<int>& dilations, const std::string& data_format) {
  VLOG(4) << "Start build DepthwiseConv2dTransposeGradOp";


  // Generate int_array mutable attribute: output_size
  paddle::dialect::FullIntArrayOp full_output_size_op = builder.Build<paddle::dialect::FullIntArrayOp>(output_size, phi::DataType::INT64, phi::CPUPlace());
  pir::OpResult output_size_ = full_output_size_op->result(0);
    
  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_, filter_, out_grad_, output_size_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  std::vector<pir::Attribute> vec_strides;
  for (size_t i = 0; i < static_cast<size_t>(strides.size()); i++) {
      pir::Attribute attr_strides = pir::Int32Attribute::get(pir::IrContext::Instance(), strides[i]);

    vec_strides.push_back(attr_strides);
  }
  pir::Attribute attr_strides = pir::ArrayAttribute::get(pir::IrContext::Instance(), vec_strides);
  argument.AddAttribute("strides", attr_strides);
  std::vector<pir::Attribute> vec_paddings;
  for (size_t i = 0; i < static_cast<size_t>(paddings.size()); i++) {
      pir::Attribute attr_paddings = pir::Int32Attribute::get(pir::IrContext::Instance(), paddings[i]);

    vec_paddings.push_back(attr_paddings);
  }
  pir::Attribute attr_paddings = pir::ArrayAttribute::get(pir::IrContext::Instance(), vec_paddings);
  argument.AddAttribute("paddings", attr_paddings);
  std::vector<pir::Attribute> vec_output_padding;
  for (size_t i = 0; i < static_cast<size_t>(output_padding.size()); i++) {
      pir::Attribute attr_output_padding = pir::Int32Attribute::get(pir::IrContext::Instance(), output_padding[i]);

    vec_output_padding.push_back(attr_output_padding);
  }
  pir::Attribute attr_output_padding = pir::ArrayAttribute::get(pir::IrContext::Instance(), vec_output_padding);
  argument.AddAttribute("output_padding", attr_output_padding);
  pir::Attribute attr_padding_algorithm = pir::StrAttribute::get(pir::IrContext::Instance(), padding_algorithm);
  argument.AddAttribute("padding_algorithm", attr_padding_algorithm);
  pir::Attribute attr_groups = pir::Int32Attribute::get(pir::IrContext::Instance(), groups);
  argument.AddAttribute("groups", attr_groups);
  std::vector<pir::Attribute> vec_dilations;
  for (size_t i = 0; i < static_cast<size_t>(dilations.size()); i++) {
      pir::Attribute attr_dilations = pir::Int32Attribute::get(pir::IrContext::Instance(), dilations[i]);

    vec_dilations.push_back(attr_dilations);
  }
  pir::Attribute attr_dilations = pir::ArrayAttribute::get(pir::IrContext::Instance(), vec_dilations);
  argument.AddAttribute("dilations", attr_dilations);
  pir::Attribute attr_data_format = pir::StrAttribute::get(pir::IrContext::Instance(), data_format);
  argument.AddAttribute("data_format", attr_data_format);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;
  paddle::dialect::DenseTensorType filter = filter_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)filter;
  paddle::dialect::DenseTensorType out_grad = out_grad_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)out_grad;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);

  VLOG(4) << "Builder construction  dense_filter";
  paddle::dialect::IrTensor ir_tensor_filter(paddle::dialect::TransToPhiDataType(filter.dtype()),
                                                      filter.dims(),
                                                      filter.data_layout(),
                                                      filter.lod(),
                                                      filter.offset());
  VLOG(4) << "Builder construction  meta_filter";
  paddle::dialect::IrMetaTensor meta_filter(&ir_tensor_filter);

  VLOG(4) << "Builder construction  dense_out_grad";
  paddle::dialect::IrTensor ir_tensor_out_grad(paddle::dialect::TransToPhiDataType(out_grad.dtype()),
                                                      out_grad.dims(),
                                                      out_grad.data_layout(),
                                                      out_grad.lod(),
                                                      out_grad.offset());
  VLOG(4) << "Builder construction  meta_out_grad";
  paddle::dialect::IrMetaTensor meta_out_grad(&ir_tensor_out_grad);
  paddle::dialect::IrTensor dense_x_grad;
  paddle::dialect::IrMetaTensor meta_x_grad(&dense_x_grad);
  paddle::dialect::IrTensor dense_filter_grad;
  paddle::dialect::IrMetaTensor meta_filter_grad(&dense_filter_grad);

  phi::Conv2dTransposeGradInferMeta(meta_x, meta_filter, meta_out_grad, strides, paddings, output_padding, output_size, padding_algorithm, groups, dilations, data_format, &meta_x_grad, &meta_filter_grad);

  std::vector<pir::Type> argument_outputs;
  pir::Type x_grad_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_x_grad.dtype()), dense_x_grad.dims(), dense_x_grad.layout(), dense_x_grad.lod(), dense_x_grad.offset());
  argument_outputs.push_back(x_grad_dense_tensor_type);

  pir::Type filter_grad_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_filter_grad.dtype()), dense_filter_grad.dims(), dense_filter_grad.layout(), dense_filter_grad.lod(), dense_filter_grad.offset());
  argument_outputs.push_back(filter_grad_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void DepthwiseConv2dTransposeGradOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, pir::Value filter_, pir::Value out_grad_, pir::AttributeMap attributes) {
  VLOG(4) << "Start build DepthwiseConv2dTransposeGradOp";


  IR_ENFORCE(
      attributes.find("strides") != attributes.end(),
          "'strides' Attribute is expected for DepthwiseConv2dTransposeGradOp. ");
  std::vector<int> strides;
  for (size_t i = 0; i < attributes.at("strides").dyn_cast<pir::ArrayAttribute>().size(); i++) {
    strides.push_back(attributes.at("strides").dyn_cast<pir::ArrayAttribute>().at(i).dyn_cast<pir::Int32Attribute>().data());
  }

  IR_ENFORCE(
      attributes.find("paddings") != attributes.end(),
          "'paddings' Attribute is expected for DepthwiseConv2dTransposeGradOp. ");
  std::vector<int> paddings;
  for (size_t i = 0; i < attributes.at("paddings").dyn_cast<pir::ArrayAttribute>().size(); i++) {
    paddings.push_back(attributes.at("paddings").dyn_cast<pir::ArrayAttribute>().at(i).dyn_cast<pir::Int32Attribute>().data());
  }

  IR_ENFORCE(
      attributes.find("output_padding") != attributes.end(),
          "'output_padding' Attribute is expected for DepthwiseConv2dTransposeGradOp. ");
  std::vector<int> output_padding;
  for (size_t i = 0; i < attributes.at("output_padding").dyn_cast<pir::ArrayAttribute>().size(); i++) {
    output_padding.push_back(attributes.at("output_padding").dyn_cast<pir::ArrayAttribute>().at(i).dyn_cast<pir::Int32Attribute>().data());
  }

  IR_ENFORCE(
      attributes.find("output_size") != attributes.end(),
          "'output_size' Attribute is expected for DepthwiseConv2dTransposeGradOp. ");
  std::vector<int64_t> output_size = attributes.at("output_size").dyn_cast<paddle::dialect::IntArrayAttribute>().data().GetData();

  IR_ENFORCE(
      attributes.find("padding_algorithm") != attributes.end(),
          "'padding_algorithm' Attribute is expected for DepthwiseConv2dTransposeGradOp. ");
  std::string padding_algorithm = attributes.at("padding_algorithm").dyn_cast<pir::StrAttribute>().AsString();

  IR_ENFORCE(
      attributes.find("groups") != attributes.end(),
          "'groups' Attribute is expected for DepthwiseConv2dTransposeGradOp. ");
  int groups = attributes.at("groups").dyn_cast<pir::Int32Attribute>().data();

  IR_ENFORCE(
      attributes.find("dilations") != attributes.end(),
          "'dilations' Attribute is expected for DepthwiseConv2dTransposeGradOp. ");
  std::vector<int> dilations;
  for (size_t i = 0; i < attributes.at("dilations").dyn_cast<pir::ArrayAttribute>().size(); i++) {
    dilations.push_back(attributes.at("dilations").dyn_cast<pir::ArrayAttribute>().at(i).dyn_cast<pir::Int32Attribute>().data());
  }

  IR_ENFORCE(
      attributes.find("data_format") != attributes.end(),
          "'data_format' Attribute is expected for DepthwiseConv2dTransposeGradOp. ");
  std::string data_format = attributes.at("data_format").dyn_cast<pir::StrAttribute>().AsString();

  // Generate int_array mutable attribute: output_size
  paddle::dialect::FullIntArrayOp full_output_size_op = builder.Build<paddle::dialect::FullIntArrayOp>(output_size, phi::DataType::INT64, phi::CPUPlace());
  pir::OpResult output_size_ = full_output_size_op->result(0);
    
  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_, filter_, out_grad_, output_size_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  std::vector<pir::Attribute> vec_strides;
  for (size_t i = 0; i < static_cast<size_t>(strides.size()); i++) {
      pir::Attribute attr_strides = pir::Int32Attribute::get(pir::IrContext::Instance(), strides[i]);

    vec_strides.push_back(attr_strides);
  }
  pir::Attribute attr_strides = pir::ArrayAttribute::get(pir::IrContext::Instance(), vec_strides);
  argument.AddAttribute("strides", attr_strides);
  std::vector<pir::Attribute> vec_paddings;
  for (size_t i = 0; i < static_cast<size_t>(paddings.size()); i++) {
      pir::Attribute attr_paddings = pir::Int32Attribute::get(pir::IrContext::Instance(), paddings[i]);

    vec_paddings.push_back(attr_paddings);
  }
  pir::Attribute attr_paddings = pir::ArrayAttribute::get(pir::IrContext::Instance(), vec_paddings);
  argument.AddAttribute("paddings", attr_paddings);
  std::vector<pir::Attribute> vec_output_padding;
  for (size_t i = 0; i < static_cast<size_t>(output_padding.size()); i++) {
      pir::Attribute attr_output_padding = pir::Int32Attribute::get(pir::IrContext::Instance(), output_padding[i]);

    vec_output_padding.push_back(attr_output_padding);
  }
  pir::Attribute attr_output_padding = pir::ArrayAttribute::get(pir::IrContext::Instance(), vec_output_padding);
  argument.AddAttribute("output_padding", attr_output_padding);
  pir::Attribute attr_padding_algorithm = pir::StrAttribute::get(pir::IrContext::Instance(), padding_algorithm);
  argument.AddAttribute("padding_algorithm", attr_padding_algorithm);
  pir::Attribute attr_groups = pir::Int32Attribute::get(pir::IrContext::Instance(), groups);
  argument.AddAttribute("groups", attr_groups);
  std::vector<pir::Attribute> vec_dilations;
  for (size_t i = 0; i < static_cast<size_t>(dilations.size()); i++) {
      pir::Attribute attr_dilations = pir::Int32Attribute::get(pir::IrContext::Instance(), dilations[i]);

    vec_dilations.push_back(attr_dilations);
  }
  pir::Attribute attr_dilations = pir::ArrayAttribute::get(pir::IrContext::Instance(), vec_dilations);
  argument.AddAttribute("dilations", attr_dilations);
  pir::Attribute attr_data_format = pir::StrAttribute::get(pir::IrContext::Instance(), data_format);
  argument.AddAttribute("data_format", attr_data_format);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;
  paddle::dialect::DenseTensorType filter = filter_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)filter;
  paddle::dialect::DenseTensorType out_grad = out_grad_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)out_grad;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);

  VLOG(4) << "Builder construction  dense_filter";
  paddle::dialect::IrTensor ir_tensor_filter(paddle::dialect::TransToPhiDataType(filter.dtype()),
                                                      filter.dims(),
                                                      filter.data_layout(),
                                                      filter.lod(),
                                                      filter.offset());
  VLOG(4) << "Builder construction  meta_filter";
  paddle::dialect::IrMetaTensor meta_filter(&ir_tensor_filter);

  VLOG(4) << "Builder construction  dense_out_grad";
  paddle::dialect::IrTensor ir_tensor_out_grad(paddle::dialect::TransToPhiDataType(out_grad.dtype()),
                                                      out_grad.dims(),
                                                      out_grad.data_layout(),
                                                      out_grad.lod(),
                                                      out_grad.offset());
  VLOG(4) << "Builder construction  meta_out_grad";
  paddle::dialect::IrMetaTensor meta_out_grad(&ir_tensor_out_grad);
  paddle::dialect::IrTensor dense_x_grad;
  paddle::dialect::IrMetaTensor meta_x_grad(&dense_x_grad);
  paddle::dialect::IrTensor dense_filter_grad;
  paddle::dialect::IrMetaTensor meta_filter_grad(&dense_filter_grad);

  phi::Conv2dTransposeGradInferMeta(meta_x, meta_filter, meta_out_grad, strides, paddings, output_padding, output_size, padding_algorithm, groups, dilations, data_format, &meta_x_grad, &meta_filter_grad);

  std::vector<pir::Type> argument_outputs;
  pir::Type x_grad_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_x_grad.dtype()), dense_x_grad.dims(), dense_x_grad.layout(), dense_x_grad.lod(), dense_x_grad.offset());
  argument_outputs.push_back(x_grad_dense_tensor_type);

  pir::Type filter_grad_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_filter_grad.dtype()), dense_filter_grad.dims(), dense_filter_grad.layout(), dense_filter_grad.lod(), dense_filter_grad.offset());
  argument_outputs.push_back(filter_grad_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void DepthwiseConv2dTransposeGradOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, pir::Value filter_, pir::Value out_grad_, pir::Value output_size_, const std::vector<int>& strides, const std::vector<int>& paddings, const std::vector<int>& output_padding, const std::string& padding_algorithm, int groups, const std::vector<int>& dilations, const std::string& data_format) {
  VLOG(4) << "Start build DepthwiseConv2dTransposeGradOp";



  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_, filter_, out_grad_, output_size_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  std::vector<pir::Attribute> vec_strides;
  for (size_t i = 0; i < static_cast<size_t>(strides.size()); i++) {
      pir::Attribute attr_strides = pir::Int32Attribute::get(pir::IrContext::Instance(), strides[i]);

    vec_strides.push_back(attr_strides);
  }
  pir::Attribute attr_strides = pir::ArrayAttribute::get(pir::IrContext::Instance(), vec_strides);
  argument.AddAttribute("strides", attr_strides);
  std::vector<pir::Attribute> vec_paddings;
  for (size_t i = 0; i < static_cast<size_t>(paddings.size()); i++) {
      pir::Attribute attr_paddings = pir::Int32Attribute::get(pir::IrContext::Instance(), paddings[i]);

    vec_paddings.push_back(attr_paddings);
  }
  pir::Attribute attr_paddings = pir::ArrayAttribute::get(pir::IrContext::Instance(), vec_paddings);
  argument.AddAttribute("paddings", attr_paddings);
  std::vector<pir::Attribute> vec_output_padding;
  for (size_t i = 0; i < static_cast<size_t>(output_padding.size()); i++) {
      pir::Attribute attr_output_padding = pir::Int32Attribute::get(pir::IrContext::Instance(), output_padding[i]);

    vec_output_padding.push_back(attr_output_padding);
  }
  pir::Attribute attr_output_padding = pir::ArrayAttribute::get(pir::IrContext::Instance(), vec_output_padding);
  argument.AddAttribute("output_padding", attr_output_padding);
  pir::Attribute attr_padding_algorithm = pir::StrAttribute::get(pir::IrContext::Instance(), padding_algorithm);
  argument.AddAttribute("padding_algorithm", attr_padding_algorithm);
  pir::Attribute attr_groups = pir::Int32Attribute::get(pir::IrContext::Instance(), groups);
  argument.AddAttribute("groups", attr_groups);
  std::vector<pir::Attribute> vec_dilations;
  for (size_t i = 0; i < static_cast<size_t>(dilations.size()); i++) {
      pir::Attribute attr_dilations = pir::Int32Attribute::get(pir::IrContext::Instance(), dilations[i]);

    vec_dilations.push_back(attr_dilations);
  }
  pir::Attribute attr_dilations = pir::ArrayAttribute::get(pir::IrContext::Instance(), vec_dilations);
  argument.AddAttribute("dilations", attr_dilations);
  pir::Attribute attr_data_format = pir::StrAttribute::get(pir::IrContext::Instance(), data_format);
  argument.AddAttribute("data_format", attr_data_format);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;
  paddle::dialect::DenseTensorType filter = filter_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)filter;
  paddle::dialect::DenseTensorType out_grad = out_grad_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)out_grad;
  phi::IntArray output_size;
  if (output_size_.dyn_cast<pir::OpResult>() && output_size_.dyn_cast<pir::OpResult>().owner()->isa<paddle::dialect::FullIntArrayOp>()) {
    output_size = std::move(phi::IntArray(paddle::dialect::GetInt64Vector(
                          output_size_.dyn_cast<pir::OpResult>().owner()
                          ->dyn_cast<paddle::dialect::FullIntArrayOp>()
                          .attribute("value"))));
  } else if (output_size_.type().isa<pir::VectorType>()) {
    size_t output_size_size = output_size_.type().dyn_cast<pir::VectorType>().size();
    output_size = std::move(phi::IntArray(std::vector<int64_t>(output_size_size, -1)));
    output_size.SetFromTensor(true);
  } else if (output_size_.type().isa<paddle::dialect::DenseTensorType>()) {
    common::DDim output_size_dim = output_size_.type().dyn_cast<paddle::dialect::DenseTensorType>().dims();
    size_t output_size_size = common::product(output_size_dim);
    if (common::contain_unknown_dim(output_size_dim)) {
      output_size_size = 1;
    }
    output_size = std::move(phi::IntArray(std::vector<int64_t>(output_size_size, -1)));
    output_size.SetFromTensor(true);
  } else {
    PADDLE_THROW(phi::errors::Unimplemented("Only support VectorType or DenseTensorType"));
  }


  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);

  VLOG(4) << "Builder construction  dense_filter";
  paddle::dialect::IrTensor ir_tensor_filter(paddle::dialect::TransToPhiDataType(filter.dtype()),
                                                      filter.dims(),
                                                      filter.data_layout(),
                                                      filter.lod(),
                                                      filter.offset());
  VLOG(4) << "Builder construction  meta_filter";
  paddle::dialect::IrMetaTensor meta_filter(&ir_tensor_filter);

  VLOG(4) << "Builder construction  dense_out_grad";
  paddle::dialect::IrTensor ir_tensor_out_grad(paddle::dialect::TransToPhiDataType(out_grad.dtype()),
                                                      out_grad.dims(),
                                                      out_grad.data_layout(),
                                                      out_grad.lod(),
                                                      out_grad.offset());
  VLOG(4) << "Builder construction  meta_out_grad";
  paddle::dialect::IrMetaTensor meta_out_grad(&ir_tensor_out_grad);
  paddle::dialect::IrTensor dense_x_grad;
  paddle::dialect::IrMetaTensor meta_x_grad(&dense_x_grad);
  paddle::dialect::IrTensor dense_filter_grad;
  paddle::dialect::IrMetaTensor meta_filter_grad(&dense_filter_grad);

  phi::Conv2dTransposeGradInferMeta(meta_x, meta_filter, meta_out_grad, strides, paddings, output_padding, output_size, padding_algorithm, groups, dilations, data_format, &meta_x_grad, &meta_filter_grad);

  std::vector<pir::Type> argument_outputs;
  pir::Type x_grad_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_x_grad.dtype()), dense_x_grad.dims(), dense_x_grad.layout(), dense_x_grad.lod(), dense_x_grad.offset());
  argument_outputs.push_back(x_grad_dense_tensor_type);

  pir::Type filter_grad_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_filter_grad.dtype()), dense_filter_grad.dims(), dense_filter_grad.layout(), dense_filter_grad.lod(), dense_filter_grad.offset());
  argument_outputs.push_back(filter_grad_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void DepthwiseConv2dTransposeGradOp::VerifySig() {}

void DepthwiseConv2dTransposeGradOp::InferMeta( phi::InferMetaContext *infer_meta ) {
  auto fn = PD_INFER_META(phi::Conv2dTransposeGradInferMeta);
  fn(infer_meta);
}

phi::DataType DepthwiseConv2dTransposeGradOp::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: DepthwiseConv2dTransposeGradOp";
  


  return expected_kernel_dtype;
}

const char *DivideDoubleGradOp::attributes_name[1] = { "axis" };

OpInfoTuple DivideDoubleGradOp::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("y", "paddle::dialect::DenseTensorType", false, false, false, false), paddle::dialect::OpInputInfo("out", "paddle::dialect::DenseTensorType", false, false, false, false), paddle::dialect::OpInputInfo("grad_x", "paddle::dialect::DenseTensorType", false, false, false, false), paddle::dialect::OpInputInfo("grad_x_grad", "paddle::dialect::DenseTensorType", true, false, false, false), paddle::dialect::OpInputInfo("grad_y_grad", "paddle::dialect::DenseTensorType", true, false, false, false) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = { paddle::dialect::OpAttributeInfo("axis", "pir::Int32Attribute", "") };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("y_grad", "paddle::dialect::DenseTensorType", false, false), paddle::dialect::OpOutputInfo("out_grad", "paddle::dialect::DenseTensorType", false, false), paddle::dialect::OpOutputInfo("grad_out_grad", "paddle::dialect::DenseTensorType", false, false) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("GeneralTernaryGradInferMeta", {"y", "grad_x", "grad_x"}, "divide_double_grad", {"y", "out", "grad_x", "grad_x_grad", "grad_y_grad", "axis"}, {"out"}, {}, {}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "divide_double_grad");
}

void DivideDoubleGradOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value y_, pir::Value out_, pir::Value grad_x_, pir::Value grad_x_grad_, pir::Value grad_y_grad_, int axis) {
  VLOG(4) << "Start build DivideDoubleGradOp";



  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {y_, out_, grad_x_, grad_x_grad_, grad_y_grad_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_axis = pir::Int32Attribute::get(pir::IrContext::Instance(), axis);
  argument.AddAttribute("axis", attr_axis);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType y = y_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)y;
  paddle::dialect::DenseTensorType out = out_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)out;
  paddle::dialect::DenseTensorType grad_x = grad_x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)grad_x;

  VLOG(4) << "Builder construction  dense_y";
  paddle::dialect::IrTensor ir_tensor_y(paddle::dialect::TransToPhiDataType(y.dtype()),
                                                      y.dims(),
                                                      y.data_layout(),
                                                      y.lod(),
                                                      y.offset());
  VLOG(4) << "Builder construction  meta_y";
  paddle::dialect::IrMetaTensor meta_y(&ir_tensor_y);

  VLOG(4) << "Builder construction  dense_grad_x";
  paddle::dialect::IrTensor ir_tensor_grad_x(paddle::dialect::TransToPhiDataType(grad_x.dtype()),
                                                      grad_x.dims(),
                                                      grad_x.data_layout(),
                                                      grad_x.lod(),
                                                      grad_x.offset());
  VLOG(4) << "Builder construction  meta_grad_x";
  paddle::dialect::IrMetaTensor meta_grad_x(&ir_tensor_grad_x);
  paddle::dialect::IrTensor dense_y_grad;
  paddle::dialect::IrMetaTensor meta_y_grad(&dense_y_grad);
  paddle::dialect::IrTensor dense_out_grad;
  paddle::dialect::IrMetaTensor meta_out_grad(&dense_out_grad);
  paddle::dialect::IrTensor dense_grad_out_grad;
  paddle::dialect::IrMetaTensor meta_grad_out_grad(&dense_grad_out_grad);

  phi::GeneralTernaryGradInferMeta(meta_y, meta_grad_x, meta_grad_x, &meta_y_grad, &meta_out_grad, &meta_grad_out_grad);

  std::vector<pir::Type> argument_outputs;
  pir::Type y_grad_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_y_grad.dtype()), dense_y_grad.dims(), dense_y_grad.layout(), dense_y_grad.lod(), dense_y_grad.offset());
  argument_outputs.push_back(y_grad_dense_tensor_type);

  pir::Type out_grad_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_out_grad.dtype()), dense_out_grad.dims(), dense_out_grad.layout(), dense_out_grad.lod(), dense_out_grad.offset());
  argument_outputs.push_back(out_grad_dense_tensor_type);

  pir::Type grad_out_grad_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_grad_out_grad.dtype()), dense_grad_out_grad.dims(), dense_grad_out_grad.layout(), dense_grad_out_grad.lod(), dense_grad_out_grad.offset());
  argument_outputs.push_back(grad_out_grad_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void DivideDoubleGradOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value y_, pir::Value out_, pir::Value grad_x_, pir::Value grad_x_grad_, pir::Value grad_y_grad_, pir::AttributeMap attributes) {
  VLOG(4) << "Start build DivideDoubleGradOp";


  IR_ENFORCE(
      attributes.find("axis") != attributes.end(),
          "'axis' Attribute is expected for DivideDoubleGradOp. ");
  int axis = attributes.at("axis").dyn_cast<pir::Int32Attribute>().data();


  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {y_, out_, grad_x_, grad_x_grad_, grad_y_grad_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_axis = pir::Int32Attribute::get(pir::IrContext::Instance(), axis);
  argument.AddAttribute("axis", attr_axis);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType y = y_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)y;
  paddle::dialect::DenseTensorType out = out_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)out;
  paddle::dialect::DenseTensorType grad_x = grad_x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)grad_x;

  VLOG(4) << "Builder construction  dense_y";
  paddle::dialect::IrTensor ir_tensor_y(paddle::dialect::TransToPhiDataType(y.dtype()),
                                                      y.dims(),
                                                      y.data_layout(),
                                                      y.lod(),
                                                      y.offset());
  VLOG(4) << "Builder construction  meta_y";
  paddle::dialect::IrMetaTensor meta_y(&ir_tensor_y);

  VLOG(4) << "Builder construction  dense_grad_x";
  paddle::dialect::IrTensor ir_tensor_grad_x(paddle::dialect::TransToPhiDataType(grad_x.dtype()),
                                                      grad_x.dims(),
                                                      grad_x.data_layout(),
                                                      grad_x.lod(),
                                                      grad_x.offset());
  VLOG(4) << "Builder construction  meta_grad_x";
  paddle::dialect::IrMetaTensor meta_grad_x(&ir_tensor_grad_x);
  paddle::dialect::IrTensor dense_y_grad;
  paddle::dialect::IrMetaTensor meta_y_grad(&dense_y_grad);
  paddle::dialect::IrTensor dense_out_grad;
  paddle::dialect::IrMetaTensor meta_out_grad(&dense_out_grad);
  paddle::dialect::IrTensor dense_grad_out_grad;
  paddle::dialect::IrMetaTensor meta_grad_out_grad(&dense_grad_out_grad);

  phi::GeneralTernaryGradInferMeta(meta_y, meta_grad_x, meta_grad_x, &meta_y_grad, &meta_out_grad, &meta_grad_out_grad);

  std::vector<pir::Type> argument_outputs;
  pir::Type y_grad_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_y_grad.dtype()), dense_y_grad.dims(), dense_y_grad.layout(), dense_y_grad.lod(), dense_y_grad.offset());
  argument_outputs.push_back(y_grad_dense_tensor_type);

  pir::Type out_grad_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_out_grad.dtype()), dense_out_grad.dims(), dense_out_grad.layout(), dense_out_grad.lod(), dense_out_grad.offset());
  argument_outputs.push_back(out_grad_dense_tensor_type);

  pir::Type grad_out_grad_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_grad_out_grad.dtype()), dense_grad_out_grad.dims(), dense_grad_out_grad.layout(), dense_grad_out_grad.lod(), dense_grad_out_grad.offset());
  argument_outputs.push_back(grad_out_grad_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void DivideDoubleGradOp::VerifySig() {}

void DivideDoubleGradOp::InferMeta( phi::InferMetaContext *infer_meta ) {
  auto fn = PD_INFER_META(phi::GeneralTernaryGradInferMeta);
  fn(infer_meta);
}

phi::DataType DivideDoubleGradOp::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: DivideDoubleGradOp";
  


  return expected_kernel_dtype;
}

const char *DivideDoubleGrad_Op::attributes_name[1] = { "axis" };

OpInfoTuple DivideDoubleGrad_Op::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("y", "paddle::dialect::DenseTensorType", false, false, false, false), paddle::dialect::OpInputInfo("out", "paddle::dialect::DenseTensorType", false, false, false, false), paddle::dialect::OpInputInfo("grad_x", "paddle::dialect::DenseTensorType", false, false, false, false), paddle::dialect::OpInputInfo("grad_x_grad", "paddle::dialect::DenseTensorType", true, false, false, false), paddle::dialect::OpInputInfo("grad_y_grad", "paddle::dialect::DenseTensorType", true, false, false, false) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = { paddle::dialect::OpAttributeInfo("axis", "pir::Int32Attribute", "") };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("y_grad", "paddle::dialect::DenseTensorType", false, false), paddle::dialect::OpOutputInfo("out_grad", "paddle::dialect::DenseTensorType", false, false), paddle::dialect::OpOutputInfo("grad_out_grad", "paddle::dialect::DenseTensorType", false, false) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("GeneralTernaryGradInferMeta", {"y", "grad_x", "grad_x"}, "divide_double_grad", {"y", "out", "grad_x", "grad_x_grad", "grad_y_grad", "axis"}, {"out"}, {}, {{"grad_out_grad", "grad_x_grad"}}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "divide_double_grad");
}

void DivideDoubleGrad_Op::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value y_, pir::Value out_, pir::Value grad_x_, pir::Value grad_x_grad_, pir::Value grad_y_grad_, int axis) {
  VLOG(4) << "Start build DivideDoubleGrad_Op";



  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {y_, out_, grad_x_, grad_x_grad_, grad_y_grad_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_axis = pir::Int32Attribute::get(pir::IrContext::Instance(), axis);
  argument.AddAttribute("axis", attr_axis);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType y = y_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)y;
  paddle::dialect::DenseTensorType out = out_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)out;
  paddle::dialect::DenseTensorType grad_x = grad_x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)grad_x;

  VLOG(4) << "Builder construction  dense_y";
  paddle::dialect::IrTensor ir_tensor_y(paddle::dialect::TransToPhiDataType(y.dtype()),
                                                      y.dims(),
                                                      y.data_layout(),
                                                      y.lod(),
                                                      y.offset());
  VLOG(4) << "Builder construction  meta_y";
  paddle::dialect::IrMetaTensor meta_y(&ir_tensor_y);

  VLOG(4) << "Builder construction  dense_grad_x";
  paddle::dialect::IrTensor ir_tensor_grad_x(paddle::dialect::TransToPhiDataType(grad_x.dtype()),
                                                      grad_x.dims(),
                                                      grad_x.data_layout(),
                                                      grad_x.lod(),
                                                      grad_x.offset());
  VLOG(4) << "Builder construction  meta_grad_x";
  paddle::dialect::IrMetaTensor meta_grad_x(&ir_tensor_grad_x);
  paddle::dialect::IrTensor dense_y_grad;
  paddle::dialect::IrMetaTensor meta_y_grad(&dense_y_grad);
  paddle::dialect::IrTensor dense_out_grad;
  paddle::dialect::IrMetaTensor meta_out_grad(&dense_out_grad);
  paddle::dialect::IrTensor dense_grad_out_grad;
  paddle::dialect::IrMetaTensor meta_grad_out_grad(&dense_grad_out_grad);

  phi::GeneralTernaryGradInferMeta(meta_y, meta_grad_x, meta_grad_x, &meta_y_grad, &meta_out_grad, &meta_grad_out_grad);

  std::vector<pir::Type> argument_outputs;
  pir::Type y_grad_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_y_grad.dtype()), dense_y_grad.dims(), dense_y_grad.layout(), dense_y_grad.lod(), dense_y_grad.offset());
  argument_outputs.push_back(y_grad_dense_tensor_type);

  pir::Type out_grad_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_out_grad.dtype()), dense_out_grad.dims(), dense_out_grad.layout(), dense_out_grad.lod(), dense_out_grad.offset());
  argument_outputs.push_back(out_grad_dense_tensor_type);

  pir::Type grad_out_grad_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_grad_out_grad.dtype()), dense_grad_out_grad.dims(), dense_grad_out_grad.layout(), dense_grad_out_grad.lod(), dense_grad_out_grad.offset());
  argument_outputs.push_back(grad_out_grad_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void DivideDoubleGrad_Op::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value y_, pir::Value out_, pir::Value grad_x_, pir::Value grad_x_grad_, pir::Value grad_y_grad_, pir::AttributeMap attributes) {
  VLOG(4) << "Start build DivideDoubleGrad_Op";


  IR_ENFORCE(
      attributes.find("axis") != attributes.end(),
          "'axis' Attribute is expected for DivideDoubleGrad_Op. ");
  int axis = attributes.at("axis").dyn_cast<pir::Int32Attribute>().data();


  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {y_, out_, grad_x_, grad_x_grad_, grad_y_grad_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_axis = pir::Int32Attribute::get(pir::IrContext::Instance(), axis);
  argument.AddAttribute("axis", attr_axis);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType y = y_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)y;
  paddle::dialect::DenseTensorType out = out_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)out;
  paddle::dialect::DenseTensorType grad_x = grad_x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)grad_x;

  VLOG(4) << "Builder construction  dense_y";
  paddle::dialect::IrTensor ir_tensor_y(paddle::dialect::TransToPhiDataType(y.dtype()),
                                                      y.dims(),
                                                      y.data_layout(),
                                                      y.lod(),
                                                      y.offset());
  VLOG(4) << "Builder construction  meta_y";
  paddle::dialect::IrMetaTensor meta_y(&ir_tensor_y);

  VLOG(4) << "Builder construction  dense_grad_x";
  paddle::dialect::IrTensor ir_tensor_grad_x(paddle::dialect::TransToPhiDataType(grad_x.dtype()),
                                                      grad_x.dims(),
                                                      grad_x.data_layout(),
                                                      grad_x.lod(),
                                                      grad_x.offset());
  VLOG(4) << "Builder construction  meta_grad_x";
  paddle::dialect::IrMetaTensor meta_grad_x(&ir_tensor_grad_x);
  paddle::dialect::IrTensor dense_y_grad;
  paddle::dialect::IrMetaTensor meta_y_grad(&dense_y_grad);
  paddle::dialect::IrTensor dense_out_grad;
  paddle::dialect::IrMetaTensor meta_out_grad(&dense_out_grad);
  paddle::dialect::IrTensor dense_grad_out_grad;
  paddle::dialect::IrMetaTensor meta_grad_out_grad(&dense_grad_out_grad);

  phi::GeneralTernaryGradInferMeta(meta_y, meta_grad_x, meta_grad_x, &meta_y_grad, &meta_out_grad, &meta_grad_out_grad);

  std::vector<pir::Type> argument_outputs;
  pir::Type y_grad_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_y_grad.dtype()), dense_y_grad.dims(), dense_y_grad.layout(), dense_y_grad.lod(), dense_y_grad.offset());
  argument_outputs.push_back(y_grad_dense_tensor_type);

  pir::Type out_grad_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_out_grad.dtype()), dense_out_grad.dims(), dense_out_grad.layout(), dense_out_grad.lod(), dense_out_grad.offset());
  argument_outputs.push_back(out_grad_dense_tensor_type);

  pir::Type grad_out_grad_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_grad_out_grad.dtype()), dense_grad_out_grad.dims(), dense_grad_out_grad.layout(), dense_grad_out_grad.lod(), dense_grad_out_grad.offset());
  argument_outputs.push_back(grad_out_grad_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void DivideDoubleGrad_Op::VerifySig() {}

void DivideDoubleGrad_Op::InferMeta( phi::InferMetaContext *infer_meta ) {
  auto fn = PD_INFER_META(phi::GeneralTernaryGradInferMeta);
  fn(infer_meta);
}

phi::DataType DivideDoubleGrad_Op::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: DivideDoubleGrad_Op";
  


  return expected_kernel_dtype;
}

const char *DivideGradOp::attributes_name[1] = { "axis" };

OpInfoTuple DivideGradOp::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("x", "paddle::dialect::DenseTensorType", false, false, false, false), paddle::dialect::OpInputInfo("y", "paddle::dialect::DenseTensorType", false, false, false, true), paddle::dialect::OpInputInfo("out", "paddle::dialect::DenseTensorType", false, false, false, true), paddle::dialect::OpInputInfo("out_grad", "paddle::dialect::DenseTensorType", false, false, false, true) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = { paddle::dialect::OpAttributeInfo("axis", "pir::Int32Attribute", "") };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("x_grad", "paddle::dialect::DenseTensorType", false, false), paddle::dialect::OpOutputInfo("y_grad", "paddle::dialect::DenseTensorType", false, false) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("GeneralBinaryGradInferMeta", {"x", "y"}, "divide_grad", {"x", "y", "out", "out_grad", "axis"}, {}, {}, {}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "divide_grad");
}

void DivideGradOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, pir::Value y_, pir::Value out_, pir::Value out_grad_, int axis) {
  VLOG(4) << "Start build DivideGradOp";



  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_, y_, out_, out_grad_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_axis = pir::Int32Attribute::get(pir::IrContext::Instance(), axis);
  argument.AddAttribute("axis", attr_axis);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;
  paddle::dialect::DenseTensorType y = y_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)y;
  paddle::dialect::DenseTensorType out = out_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)out;
  paddle::dialect::DenseTensorType out_grad = out_grad_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)out_grad;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);

  VLOG(4) << "Builder construction  dense_y";
  paddle::dialect::IrTensor ir_tensor_y(paddle::dialect::TransToPhiDataType(y.dtype()),
                                                      y.dims(),
                                                      y.data_layout(),
                                                      y.lod(),
                                                      y.offset());
  VLOG(4) << "Builder construction  meta_y";
  paddle::dialect::IrMetaTensor meta_y(&ir_tensor_y);
  paddle::dialect::IrTensor dense_x_grad;
  paddle::dialect::IrMetaTensor meta_x_grad(&dense_x_grad);
  paddle::dialect::IrTensor dense_y_grad;
  paddle::dialect::IrMetaTensor meta_y_grad(&dense_y_grad);

  phi::GeneralBinaryGradInferMeta(meta_x, meta_y, &meta_x_grad, &meta_y_grad);

  std::vector<pir::Type> argument_outputs;
  pir::Type x_grad_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_x_grad.dtype()), dense_x_grad.dims(), dense_x_grad.layout(), dense_x_grad.lod(), dense_x_grad.offset());
  argument_outputs.push_back(x_grad_dense_tensor_type);

  pir::Type y_grad_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_y_grad.dtype()), dense_y_grad.dims(), dense_y_grad.layout(), dense_y_grad.lod(), dense_y_grad.offset());
  argument_outputs.push_back(y_grad_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void DivideGradOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, pir::Value y_, pir::Value out_, pir::Value out_grad_, pir::AttributeMap attributes) {
  VLOG(4) << "Start build DivideGradOp";


  IR_ENFORCE(
      attributes.find("axis") != attributes.end(),
          "'axis' Attribute is expected for DivideGradOp. ");
  int axis = attributes.at("axis").dyn_cast<pir::Int32Attribute>().data();


  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_, y_, out_, out_grad_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_axis = pir::Int32Attribute::get(pir::IrContext::Instance(), axis);
  argument.AddAttribute("axis", attr_axis);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;
  paddle::dialect::DenseTensorType y = y_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)y;
  paddle::dialect::DenseTensorType out = out_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)out;
  paddle::dialect::DenseTensorType out_grad = out_grad_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)out_grad;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);

  VLOG(4) << "Builder construction  dense_y";
  paddle::dialect::IrTensor ir_tensor_y(paddle::dialect::TransToPhiDataType(y.dtype()),
                                                      y.dims(),
                                                      y.data_layout(),
                                                      y.lod(),
                                                      y.offset());
  VLOG(4) << "Builder construction  meta_y";
  paddle::dialect::IrMetaTensor meta_y(&ir_tensor_y);
  paddle::dialect::IrTensor dense_x_grad;
  paddle::dialect::IrMetaTensor meta_x_grad(&dense_x_grad);
  paddle::dialect::IrTensor dense_y_grad;
  paddle::dialect::IrMetaTensor meta_y_grad(&dense_y_grad);

  phi::GeneralBinaryGradInferMeta(meta_x, meta_y, &meta_x_grad, &meta_y_grad);

  std::vector<pir::Type> argument_outputs;
  pir::Type x_grad_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_x_grad.dtype()), dense_x_grad.dims(), dense_x_grad.layout(), dense_x_grad.lod(), dense_x_grad.offset());
  argument_outputs.push_back(x_grad_dense_tensor_type);

  pir::Type y_grad_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_y_grad.dtype()), dense_y_grad.dims(), dense_y_grad.layout(), dense_y_grad.lod(), dense_y_grad.offset());
  argument_outputs.push_back(y_grad_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void DivideGradOp::VerifySig() {}

void DivideGradOp::InferMeta( phi::InferMetaContext *infer_meta ) {
  auto fn = PD_INFER_META(phi::GeneralBinaryGradInferMeta);
  fn(infer_meta);
}

phi::DataType DivideGradOp::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: DivideGradOp";
  


  return expected_kernel_dtype;
}

const char *DropoutGradOp::attributes_name[3] = { "p", "is_test", "mode" };

OpInfoTuple DropoutGradOp::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("mask", "paddle::dialect::DenseTensorType", false, false, false, false), paddle::dialect::OpInputInfo("out_grad", "paddle::dialect::DenseTensorType", false, false, false, false) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = { paddle::dialect::OpAttributeInfo("p", "paddle::dialect::ScalarAttribute", "float"), paddle::dialect::OpAttributeInfo("is_test", "pir::BoolAttribute", ""), paddle::dialect::OpAttributeInfo("mode", "pir::StrAttribute", "") };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("x_grad", "paddle::dialect::DenseTensorType", false, false) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("UnchangedInferMeta", {"out_grad"}, "dropout_grad", {"mask", "out_grad", "p", "is_test", "mode"}, {}, {}, {}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "dropout_grad");
}

void DropoutGradOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value mask_, pir::Value out_grad_, float p, bool is_test, const std::string& mode) {
  VLOG(4) << "Start build DropoutGradOp";



  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {mask_, out_grad_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_p = paddle::dialect::TransToIrAttribute(p, pir::IrContext::Instance());
  argument.AddAttribute("p", attr_p);
  pir::Attribute attr_is_test = pir::BoolAttribute::get(pir::IrContext::Instance(), is_test);
  argument.AddAttribute("is_test", attr_is_test);
  pir::Attribute attr_mode = pir::StrAttribute::get(pir::IrContext::Instance(), mode);
  argument.AddAttribute("mode", attr_mode);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType mask = mask_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)mask;
  paddle::dialect::DenseTensorType out_grad = out_grad_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)out_grad;

  VLOG(4) << "Builder construction  dense_out_grad";
  paddle::dialect::IrTensor ir_tensor_out_grad(paddle::dialect::TransToPhiDataType(out_grad.dtype()),
                                                      out_grad.dims(),
                                                      out_grad.data_layout(),
                                                      out_grad.lod(),
                                                      out_grad.offset());
  VLOG(4) << "Builder construction  meta_out_grad";
  paddle::dialect::IrMetaTensor meta_out_grad(&ir_tensor_out_grad);
  paddle::dialect::IrTensor dense_x_grad;
  paddle::dialect::IrMetaTensor meta_x_grad(&dense_x_grad);

  phi::UnchangedInferMeta(meta_out_grad, &meta_x_grad);

  std::vector<pir::Type> argument_outputs;
  pir::Type x_grad_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_x_grad.dtype()), dense_x_grad.dims(), dense_x_grad.layout(), dense_x_grad.lod(), dense_x_grad.offset());
  argument_outputs.push_back(x_grad_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void DropoutGradOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value mask_, pir::Value out_grad_, pir::AttributeMap attributes) {
  VLOG(4) << "Start build DropoutGradOp";


  IR_ENFORCE(
      attributes.find("p") != attributes.end(),
          "'p' Attribute is expected for DropoutGradOp. ");
  float p = attributes.at("p").dyn_cast<paddle::dialect::ScalarAttribute>().data().to<float>();

  IR_ENFORCE(
      attributes.find("is_test") != attributes.end(),
          "'is_test' Attribute is expected for DropoutGradOp. ");
  bool is_test = attributes.at("is_test").dyn_cast<pir::BoolAttribute>().data();

  IR_ENFORCE(
      attributes.find("mode") != attributes.end(),
          "'mode' Attribute is expected for DropoutGradOp. ");
  std::string mode = attributes.at("mode").dyn_cast<pir::StrAttribute>().AsString();


  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {mask_, out_grad_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_p = paddle::dialect::TransToIrAttribute(p, pir::IrContext::Instance());
  argument.AddAttribute("p", attr_p);
  pir::Attribute attr_is_test = pir::BoolAttribute::get(pir::IrContext::Instance(), is_test);
  argument.AddAttribute("is_test", attr_is_test);
  pir::Attribute attr_mode = pir::StrAttribute::get(pir::IrContext::Instance(), mode);
  argument.AddAttribute("mode", attr_mode);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType mask = mask_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)mask;
  paddle::dialect::DenseTensorType out_grad = out_grad_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)out_grad;

  VLOG(4) << "Builder construction  dense_out_grad";
  paddle::dialect::IrTensor ir_tensor_out_grad(paddle::dialect::TransToPhiDataType(out_grad.dtype()),
                                                      out_grad.dims(),
                                                      out_grad.data_layout(),
                                                      out_grad.lod(),
                                                      out_grad.offset());
  VLOG(4) << "Builder construction  meta_out_grad";
  paddle::dialect::IrMetaTensor meta_out_grad(&ir_tensor_out_grad);
  paddle::dialect::IrTensor dense_x_grad;
  paddle::dialect::IrMetaTensor meta_x_grad(&dense_x_grad);

  phi::UnchangedInferMeta(meta_out_grad, &meta_x_grad);

  std::vector<pir::Type> argument_outputs;
  pir::Type x_grad_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_x_grad.dtype()), dense_x_grad.dims(), dense_x_grad.layout(), dense_x_grad.lod(), dense_x_grad.offset());
  argument_outputs.push_back(x_grad_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void DropoutGradOp::VerifySig() {}

void DropoutGradOp::InferMeta( phi::InferMetaContext *infer_meta ) {
  auto fn = PD_INFER_META(phi::UnchangedInferMeta);
  fn(infer_meta);
}

phi::DataType DropoutGradOp::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: DropoutGradOp";
  


  return expected_kernel_dtype;
}

const char *EinsumGradOp::attributes_name[1] = { "equation" };

OpInfoTuple EinsumGradOp::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("x_shape", "pir::VectorType<paddle::dialect::DenseTensorType>", false, false, false, false), paddle::dialect::OpInputInfo("inner_cache", "pir::VectorType<paddle::dialect::DenseTensorType>", false, false, false, false), paddle::dialect::OpInputInfo("out_grad", "paddle::dialect::DenseTensorType", false, false, false, false) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = { paddle::dialect::OpAttributeInfo("equation", "pir::StrAttribute", "") };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("x_grad", "pir::VectorType<paddle::dialect::DenseTensorType>", false, false) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("UnchangedMultiInferMeta", {"x_shape"}, "einsum_grad", {"x_shape", "inner_cache", "out_grad", "equation"}, {}, {}, {}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "einsum_grad");
}

void EinsumGradOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_shape_, pir::Value inner_cache_, pir::Value out_grad_, const std::string& equation) {
  VLOG(4) << "Start build EinsumGradOp";



  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_shape_, inner_cache_, out_grad_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_equation = pir::StrAttribute::get(pir::IrContext::Instance(), equation);
  argument.AddAttribute("equation", attr_equation);

  VLOG(4) << "Builder construction outputs";
  pir::VectorType x_shape = x_shape_.type().dyn_cast<pir::VectorType>(); (void)x_shape;
  pir::VectorType inner_cache = inner_cache_.type().dyn_cast<pir::VectorType>(); (void)inner_cache;
  paddle::dialect::DenseTensorType out_grad = out_grad_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)out_grad;
  std::vector<paddle::dialect::IrTensor> vec_ir_tensor_x_shape;
  for (size_t i=0; i < static_cast<size_t>(x_shape.size()); i++) {
    vec_ir_tensor_x_shape.push_back(paddle::dialect::IrTensor(paddle::dialect::TransToPhiDataType(x_shape[i].dyn_cast<paddle::dialect::DenseTensorType>().dtype()),
                                                                     x_shape[i].dyn_cast<paddle::dialect::DenseTensorType>().dims(),
                                                                     x_shape[i].dyn_cast<paddle::dialect::DenseTensorType>().data_layout(),
                                                                     x_shape[i].dyn_cast<paddle::dialect::DenseTensorType>().lod(),
                                                                     x_shape[i].dyn_cast<paddle::dialect::DenseTensorType>().offset()));
  }
  std::vector<paddle::dialect::IrMetaTensor> vec_meta_x_shape;
  for (size_t i=0; i < vec_ir_tensor_x_shape.size(); i++) {
    vec_meta_x_shape.push_back(paddle::dialect::IrMetaTensor(&vec_ir_tensor_x_shape[i]));
  }

  std::vector<const phi::MetaTensor*> meta_x_shape;
  for (size_t i=0; i < static_cast<size_t>(vec_meta_x_shape.size()); i++) {
    meta_x_shape.push_back(&vec_meta_x_shape[i]);
  }
   std::vector<paddle::dialect::IrTensor> vec_dense_x_grad((x_shape.size()), paddle::dialect::IrTensor());
  std::vector<paddle::dialect::IrMetaTensor> vec_meta_x_grad;
  for (size_t i=0; i < static_cast<size_t>(x_shape.size()); i++) {
    vec_meta_x_grad.push_back(paddle::dialect::IrMetaTensor(&vec_dense_x_grad[i]));
  }
  std::vector<phi::MetaTensor*> meta_x_grad;
  for (size_t i=0; i < static_cast<size_t>(vec_meta_x_grad.size()); i++) {
    meta_x_grad.push_back(&vec_meta_x_grad[i]);
  }

  phi::UnchangedMultiInferMeta(meta_x_shape, meta_x_grad);

  std::vector<pir::Type> argument_outputs;
  std::vector<pir::Type> x_grad_types;
  for (size_t i=0; i < static_cast<size_t>(x_shape.size()); i++) {
    x_grad_types.push_back(paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(vec_dense_x_grad[i].dtype()), vec_dense_x_grad[i].dims(), vec_dense_x_grad[i].layout(), vec_dense_x_grad[i].lod(), vec_dense_x_grad[i].offset()));
  }
  pir::Type x_grad_vector_type = pir::VectorType::get(pir::IrContext::Instance(), x_grad_types);
  argument_outputs.push_back(x_grad_vector_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void EinsumGradOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_shape_, pir::Value inner_cache_, pir::Value out_grad_, pir::AttributeMap attributes) {
  VLOG(4) << "Start build EinsumGradOp";


  IR_ENFORCE(
      attributes.find("equation") != attributes.end(),
          "'equation' Attribute is expected for EinsumGradOp. ");
  std::string equation = attributes.at("equation").dyn_cast<pir::StrAttribute>().AsString();


  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_shape_, inner_cache_, out_grad_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_equation = pir::StrAttribute::get(pir::IrContext::Instance(), equation);
  argument.AddAttribute("equation", attr_equation);

  VLOG(4) << "Builder construction outputs";
  pir::VectorType x_shape = x_shape_.type().dyn_cast<pir::VectorType>(); (void)x_shape;
  pir::VectorType inner_cache = inner_cache_.type().dyn_cast<pir::VectorType>(); (void)inner_cache;
  paddle::dialect::DenseTensorType out_grad = out_grad_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)out_grad;
  std::vector<paddle::dialect::IrTensor> vec_ir_tensor_x_shape;
  for (size_t i=0; i < static_cast<size_t>(x_shape.size()); i++) {
    vec_ir_tensor_x_shape.push_back(paddle::dialect::IrTensor(paddle::dialect::TransToPhiDataType(x_shape[i].dyn_cast<paddle::dialect::DenseTensorType>().dtype()),
                                                                     x_shape[i].dyn_cast<paddle::dialect::DenseTensorType>().dims(),
                                                                     x_shape[i].dyn_cast<paddle::dialect::DenseTensorType>().data_layout(),
                                                                     x_shape[i].dyn_cast<paddle::dialect::DenseTensorType>().lod(),
                                                                     x_shape[i].dyn_cast<paddle::dialect::DenseTensorType>().offset()));
  }
  std::vector<paddle::dialect::IrMetaTensor> vec_meta_x_shape;
  for (size_t i=0; i < vec_ir_tensor_x_shape.size(); i++) {
    vec_meta_x_shape.push_back(paddle::dialect::IrMetaTensor(&vec_ir_tensor_x_shape[i]));
  }

  std::vector<const phi::MetaTensor*> meta_x_shape;
  for (size_t i=0; i < static_cast<size_t>(vec_meta_x_shape.size()); i++) {
    meta_x_shape.push_back(&vec_meta_x_shape[i]);
  }
   std::vector<paddle::dialect::IrTensor> vec_dense_x_grad((x_shape.size()), paddle::dialect::IrTensor());
  std::vector<paddle::dialect::IrMetaTensor> vec_meta_x_grad;
  for (size_t i=0; i < static_cast<size_t>(x_shape.size()); i++) {
    vec_meta_x_grad.push_back(paddle::dialect::IrMetaTensor(&vec_dense_x_grad[i]));
  }
  std::vector<phi::MetaTensor*> meta_x_grad;
  for (size_t i=0; i < static_cast<size_t>(vec_meta_x_grad.size()); i++) {
    meta_x_grad.push_back(&vec_meta_x_grad[i]);
  }

  phi::UnchangedMultiInferMeta(meta_x_shape, meta_x_grad);

  std::vector<pir::Type> argument_outputs;
  std::vector<pir::Type> x_grad_types;
  for (size_t i=0; i < static_cast<size_t>(x_shape.size()); i++) {
    x_grad_types.push_back(paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(vec_dense_x_grad[i].dtype()), vec_dense_x_grad[i].dims(), vec_dense_x_grad[i].layout(), vec_dense_x_grad[i].lod(), vec_dense_x_grad[i].offset()));
  }
  pir::Type x_grad_vector_type = pir::VectorType::get(pir::IrContext::Instance(), x_grad_types);
  argument_outputs.push_back(x_grad_vector_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void EinsumGradOp::VerifySig() {}

void EinsumGradOp::InferMeta( phi::InferMetaContext *infer_meta ) {
  auto fn = PD_INFER_META(phi::UnchangedMultiInferMeta);
  fn(infer_meta);
}

phi::DataType EinsumGradOp::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: EinsumGradOp";
  


  return expected_kernel_dtype;
}

OpInfoTuple ElementwisePowGradOp::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("x", "paddle::dialect::DenseTensorType", false, false, false, false), paddle::dialect::OpInputInfo("y", "paddle::dialect::DenseTensorType", false, false, false, false), paddle::dialect::OpInputInfo("out_grad", "paddle::dialect::DenseTensorType", false, false, false, false) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = {  };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("x_grad", "paddle::dialect::DenseTensorType", false, false), paddle::dialect::OpOutputInfo("y_grad", "paddle::dialect::DenseTensorType", false, false) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("GeneralBinaryGradInferMeta", {"x", "y"}, "elementwise_pow_grad", {"x", "y", "out_grad"}, {}, {}, {}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "elementwise_pow_grad");
}

void ElementwisePowGradOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, pir::Value y_, pir::Value out_grad_) {
  VLOG(4) << "Start build ElementwisePowGradOp";



  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_, y_, out_grad_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;
  paddle::dialect::DenseTensorType y = y_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)y;
  paddle::dialect::DenseTensorType out_grad = out_grad_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)out_grad;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);

  VLOG(4) << "Builder construction  dense_y";
  paddle::dialect::IrTensor ir_tensor_y(paddle::dialect::TransToPhiDataType(y.dtype()),
                                                      y.dims(),
                                                      y.data_layout(),
                                                      y.lod(),
                                                      y.offset());
  VLOG(4) << "Builder construction  meta_y";
  paddle::dialect::IrMetaTensor meta_y(&ir_tensor_y);
  paddle::dialect::IrTensor dense_x_grad;
  paddle::dialect::IrMetaTensor meta_x_grad(&dense_x_grad);
  paddle::dialect::IrTensor dense_y_grad;
  paddle::dialect::IrMetaTensor meta_y_grad(&dense_y_grad);

  phi::GeneralBinaryGradInferMeta(meta_x, meta_y, &meta_x_grad, &meta_y_grad);

  std::vector<pir::Type> argument_outputs;
  pir::Type x_grad_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_x_grad.dtype()), dense_x_grad.dims(), dense_x_grad.layout(), dense_x_grad.lod(), dense_x_grad.offset());
  argument_outputs.push_back(x_grad_dense_tensor_type);

  pir::Type y_grad_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_y_grad.dtype()), dense_y_grad.dims(), dense_y_grad.layout(), dense_y_grad.lod(), dense_y_grad.offset());
  argument_outputs.push_back(y_grad_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void ElementwisePowGradOp::VerifySig() {}

void ElementwisePowGradOp::InferMeta( phi::InferMetaContext *infer_meta ) {
  auto fn = PD_INFER_META(phi::GeneralBinaryGradInferMeta);
  fn(infer_meta);
}

phi::DataType ElementwisePowGradOp::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: ElementwisePowGradOp";
  


  // deal complex_promote
  if (framework::IsComplexType(expected_kernel_dtype)) {{
    // only promote inputsâ€™s types when contains complex input
    return tensor_dtype;
  }}

  return expected_kernel_dtype;
}

const char *EmbeddingGradOp::attributes_name[1] = { "padding_idx" };

OpInfoTuple EmbeddingGradOp::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("x", "paddle::dialect::DenseTensorType", false, false, false, false), paddle::dialect::OpInputInfo("weight", "paddle::dialect::DenseTensorType", false, true, false, false), paddle::dialect::OpInputInfo("out_grad", "paddle::dialect::DenseTensorType", false, false, false, false) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = { paddle::dialect::OpAttributeInfo("padding_idx", "pir::Int64Attribute", "") };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("weight_grad", "paddle::dialect::DenseTensorType", false, false) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("EmbeddingGradSparseInferMeta", {"x", "weight"}, "embedding_grad", {"x", "weight", "out_grad", "padding_idx"}, {"out_grad"}, {}, {}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "embedding_grad");
}

void EmbeddingGradOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, pir::Value weight_, pir::Value out_grad_, int64_t padding_idx) {
  VLOG(4) << "Start build EmbeddingGradOp";



  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_, weight_, out_grad_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_padding_idx = pir::Int64Attribute::get(pir::IrContext::Instance(), padding_idx);
  argument.AddAttribute("padding_idx", attr_padding_idx);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;
  paddle::dialect::DenseTensorType weight = weight_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)weight;
  paddle::dialect::DenseTensorType out_grad = out_grad_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)out_grad;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);

  VLOG(4) << "Builder construction  dense_weight";
  paddle::dialect::IrTensor ir_tensor_weight(paddle::dialect::TransToPhiDataType(weight.dtype()),
                                                      weight.dims(),
                                                      weight.data_layout(),
                                                      weight.lod(),
                                                      weight.offset());
  VLOG(4) << "Builder construction  meta_weight";
  paddle::dialect::IrMetaTensor meta_weight(&ir_tensor_weight);
  paddle::dialect::IrTensor dense_weight_grad;
  paddle::dialect::IrMetaTensor meta_weight_grad(&dense_weight_grad);

  phi::EmbeddingGradSparseInferMeta(meta_x, meta_weight, &meta_weight_grad);

  std::vector<pir::Type> argument_outputs;
  pir::Type weight_grad_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_weight_grad.dtype()), dense_weight_grad.dims(), dense_weight_grad.layout(), dense_weight_grad.lod(), dense_weight_grad.offset());
  argument_outputs.push_back(weight_grad_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void EmbeddingGradOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, pir::Value weight_, pir::Value out_grad_, pir::AttributeMap attributes) {
  VLOG(4) << "Start build EmbeddingGradOp";


  IR_ENFORCE(
      attributes.find("padding_idx") != attributes.end(),
          "'padding_idx' Attribute is expected for EmbeddingGradOp. ");
  int64_t padding_idx = attributes.at("padding_idx").dyn_cast<pir::Int64Attribute>().data();


  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_, weight_, out_grad_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_padding_idx = pir::Int64Attribute::get(pir::IrContext::Instance(), padding_idx);
  argument.AddAttribute("padding_idx", attr_padding_idx);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;
  paddle::dialect::DenseTensorType weight = weight_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)weight;
  paddle::dialect::DenseTensorType out_grad = out_grad_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)out_grad;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);

  VLOG(4) << "Builder construction  dense_weight";
  paddle::dialect::IrTensor ir_tensor_weight(paddle::dialect::TransToPhiDataType(weight.dtype()),
                                                      weight.dims(),
                                                      weight.data_layout(),
                                                      weight.lod(),
                                                      weight.offset());
  VLOG(4) << "Builder construction  meta_weight";
  paddle::dialect::IrMetaTensor meta_weight(&ir_tensor_weight);
  paddle::dialect::IrTensor dense_weight_grad;
  paddle::dialect::IrMetaTensor meta_weight_grad(&dense_weight_grad);

  phi::EmbeddingGradSparseInferMeta(meta_x, meta_weight, &meta_weight_grad);

  std::vector<pir::Type> argument_outputs;
  pir::Type weight_grad_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_weight_grad.dtype()), dense_weight_grad.dims(), dense_weight_grad.layout(), dense_weight_grad.lod(), dense_weight_grad.offset());
  argument_outputs.push_back(weight_grad_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void EmbeddingGradOp::VerifySig() {}

void EmbeddingGradOp::InferMeta( phi::InferMetaContext *infer_meta ) {
  auto fn = PD_INFER_META(phi::EmbeddingGradSparseInferMeta);
  fn(infer_meta);
}

phi::DataType EmbeddingGradOp::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: EmbeddingGradOp";
  


  return expected_kernel_dtype;
}

const char *EmbeddingSparseGradOp::attributes_name[1] = { "padding_idx" };

OpInfoTuple EmbeddingSparseGradOp::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("x", "paddle::dialect::DenseTensorType", false, false, false, false), paddle::dialect::OpInputInfo("weight", "paddle::dialect::DenseTensorType", false, true, false, false), paddle::dialect::OpInputInfo("out_grad", "paddle::dialect::DenseTensorType", false, false, false, false) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = { paddle::dialect::OpAttributeInfo("padding_idx", "pir::Int64Attribute", "") };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("weight_grad", "paddle::dialect::SelectedRowsType", false, false) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("EmbeddingGradSparseInferMeta", {"x", "weight"}, "embedding_sparse_grad", {"x", "weight", "out_grad", "padding_idx"}, {"out_grad"}, {}, {}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "embedding_grad");
}

void EmbeddingSparseGradOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, pir::Value weight_, pir::Value out_grad_, int64_t padding_idx) {
  VLOG(4) << "Start build EmbeddingSparseGradOp";



  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_, weight_, out_grad_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_padding_idx = pir::Int64Attribute::get(pir::IrContext::Instance(), padding_idx);
  argument.AddAttribute("padding_idx", attr_padding_idx);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;
  paddle::dialect::DenseTensorType weight = weight_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)weight;
  paddle::dialect::DenseTensorType out_grad = out_grad_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)out_grad;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);

  VLOG(4) << "Builder construction  dense_weight";
  paddle::dialect::IrTensor ir_tensor_weight(paddle::dialect::TransToPhiDataType(weight.dtype()),
                                                      weight.dims(),
                                                      weight.data_layout(),
                                                      weight.lod(),
                                                      weight.offset());
  VLOG(4) << "Builder construction  meta_weight";
  paddle::dialect::IrMetaTensor meta_weight(&ir_tensor_weight);
  paddle::dialect::IrSelectedRows dense_weight_grad;
  paddle::dialect::IrMetaTensor meta_weight_grad(&dense_weight_grad);

  phi::EmbeddingGradSparseInferMeta(meta_x, meta_weight, &meta_weight_grad);

  std::vector<pir::Type> argument_outputs;
  pir::Type weight_grad_dense_tensor_type = paddle::dialect::SelectedRowsType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_weight_grad.dtype()), dense_weight_grad.dims(), dense_weight_grad.layout(), dense_weight_grad.lod(), dense_weight_grad.offset());
  argument_outputs.push_back(weight_grad_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void EmbeddingSparseGradOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, pir::Value weight_, pir::Value out_grad_, pir::AttributeMap attributes) {
  VLOG(4) << "Start build EmbeddingSparseGradOp";


  IR_ENFORCE(
      attributes.find("padding_idx") != attributes.end(),
          "'padding_idx' Attribute is expected for EmbeddingSparseGradOp. ");
  int64_t padding_idx = attributes.at("padding_idx").dyn_cast<pir::Int64Attribute>().data();


  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_, weight_, out_grad_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_padding_idx = pir::Int64Attribute::get(pir::IrContext::Instance(), padding_idx);
  argument.AddAttribute("padding_idx", attr_padding_idx);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;
  paddle::dialect::DenseTensorType weight = weight_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)weight;
  paddle::dialect::DenseTensorType out_grad = out_grad_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)out_grad;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);

  VLOG(4) << "Builder construction  dense_weight";
  paddle::dialect::IrTensor ir_tensor_weight(paddle::dialect::TransToPhiDataType(weight.dtype()),
                                                      weight.dims(),
                                                      weight.data_layout(),
                                                      weight.lod(),
                                                      weight.offset());
  VLOG(4) << "Builder construction  meta_weight";
  paddle::dialect::IrMetaTensor meta_weight(&ir_tensor_weight);
  paddle::dialect::IrSelectedRows dense_weight_grad;
  paddle::dialect::IrMetaTensor meta_weight_grad(&dense_weight_grad);

  phi::EmbeddingGradSparseInferMeta(meta_x, meta_weight, &meta_weight_grad);

  std::vector<pir::Type> argument_outputs;
  pir::Type weight_grad_dense_tensor_type = paddle::dialect::SelectedRowsType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_weight_grad.dtype()), dense_weight_grad.dims(), dense_weight_grad.layout(), dense_weight_grad.lod(), dense_weight_grad.offset());
  argument_outputs.push_back(weight_grad_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void EmbeddingSparseGradOp::VerifySig() {}

void EmbeddingSparseGradOp::InferMeta( phi::InferMetaContext *infer_meta ) {
  auto fn = PD_INFER_META(phi::EmbeddingGradSparseInferMeta);
  fn(infer_meta);
}

phi::DataType EmbeddingSparseGradOp::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: EmbeddingSparseGradOp";
  


  return expected_kernel_dtype;
}

const char *SparseWeightEmbeddingGradOp::attributes_name[1] = { "padding_idx" };

OpInfoTuple SparseWeightEmbeddingGradOp::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("x", "paddle::dialect::SelectedRowsType", false, false, false, false), paddle::dialect::OpInputInfo("weight", "paddle::dialect::DenseTensorType", false, true, false, false), paddle::dialect::OpInputInfo("out_grad", "paddle::dialect::DenseTensorType", false, false, false, false) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = { paddle::dialect::OpAttributeInfo("padding_idx", "pir::Int64Attribute", "") };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("weight_grad", "paddle::dialect::DenseTensorType", false, false) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("EmbeddingGradSparseInferMeta", {"x", "weight"}, "sparse_weight_embedding_grad", {"x", "weight", "out_grad", "padding_idx"}, {"out_grad"}, {}, {}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "embedding_grad");
}

void SparseWeightEmbeddingGradOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, pir::Value weight_, pir::Value out_grad_, int64_t padding_idx) {
  VLOG(4) << "Start build SparseWeightEmbeddingGradOp";



  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_, weight_, out_grad_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_padding_idx = pir::Int64Attribute::get(pir::IrContext::Instance(), padding_idx);
  argument.AddAttribute("padding_idx", attr_padding_idx);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::SelectedRowsType x = x_.type().dyn_cast<paddle::dialect::SelectedRowsType>(); (void)x;
  paddle::dialect::DenseTensorType weight = weight_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)weight;
  paddle::dialect::DenseTensorType out_grad = out_grad_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)out_grad;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);

  VLOG(4) << "Builder construction  dense_weight";
  paddle::dialect::IrTensor ir_tensor_weight(paddle::dialect::TransToPhiDataType(weight.dtype()),
                                                      weight.dims(),
                                                      weight.data_layout(),
                                                      weight.lod(),
                                                      weight.offset());
  VLOG(4) << "Builder construction  meta_weight";
  paddle::dialect::IrMetaTensor meta_weight(&ir_tensor_weight);
  paddle::dialect::IrTensor dense_weight_grad;
  paddle::dialect::IrMetaTensor meta_weight_grad(&dense_weight_grad);

  phi::EmbeddingGradSparseInferMeta(meta_x, meta_weight, &meta_weight_grad);

  std::vector<pir::Type> argument_outputs;
  pir::Type weight_grad_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_weight_grad.dtype()), dense_weight_grad.dims(), dense_weight_grad.layout(), dense_weight_grad.lod(), dense_weight_grad.offset());
  argument_outputs.push_back(weight_grad_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void SparseWeightEmbeddingGradOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, pir::Value weight_, pir::Value out_grad_, pir::AttributeMap attributes) {
  VLOG(4) << "Start build SparseWeightEmbeddingGradOp";


  IR_ENFORCE(
      attributes.find("padding_idx") != attributes.end(),
          "'padding_idx' Attribute is expected for SparseWeightEmbeddingGradOp. ");
  int64_t padding_idx = attributes.at("padding_idx").dyn_cast<pir::Int64Attribute>().data();


  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_, weight_, out_grad_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_padding_idx = pir::Int64Attribute::get(pir::IrContext::Instance(), padding_idx);
  argument.AddAttribute("padding_idx", attr_padding_idx);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::SelectedRowsType x = x_.type().dyn_cast<paddle::dialect::SelectedRowsType>(); (void)x;
  paddle::dialect::DenseTensorType weight = weight_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)weight;
  paddle::dialect::DenseTensorType out_grad = out_grad_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)out_grad;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);

  VLOG(4) << "Builder construction  dense_weight";
  paddle::dialect::IrTensor ir_tensor_weight(paddle::dialect::TransToPhiDataType(weight.dtype()),
                                                      weight.dims(),
                                                      weight.data_layout(),
                                                      weight.lod(),
                                                      weight.offset());
  VLOG(4) << "Builder construction  meta_weight";
  paddle::dialect::IrMetaTensor meta_weight(&ir_tensor_weight);
  paddle::dialect::IrTensor dense_weight_grad;
  paddle::dialect::IrMetaTensor meta_weight_grad(&dense_weight_grad);

  phi::EmbeddingGradSparseInferMeta(meta_x, meta_weight, &meta_weight_grad);

  std::vector<pir::Type> argument_outputs;
  pir::Type weight_grad_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_weight_grad.dtype()), dense_weight_grad.dims(), dense_weight_grad.layout(), dense_weight_grad.lod(), dense_weight_grad.offset());
  argument_outputs.push_back(weight_grad_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void SparseWeightEmbeddingGradOp::VerifySig() {}

void SparseWeightEmbeddingGradOp::InferMeta( phi::InferMetaContext *infer_meta ) {
  auto fn = PD_INFER_META(phi::EmbeddingGradSparseInferMeta);
  fn(infer_meta);
}

phi::DataType SparseWeightEmbeddingGradOp::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: SparseWeightEmbeddingGradOp";
  


  return expected_kernel_dtype;
}

const char *SparseWeightEmbeddingSparseGradOp::attributes_name[1] = { "padding_idx" };

OpInfoTuple SparseWeightEmbeddingSparseGradOp::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("x", "paddle::dialect::SelectedRowsType", false, false, false, false), paddle::dialect::OpInputInfo("weight", "paddle::dialect::DenseTensorType", false, true, false, false), paddle::dialect::OpInputInfo("out_grad", "paddle::dialect::DenseTensorType", false, false, false, false) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = { paddle::dialect::OpAttributeInfo("padding_idx", "pir::Int64Attribute", "") };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("weight_grad", "paddle::dialect::SelectedRowsType", false, false) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("EmbeddingGradSparseInferMeta", {"x", "weight"}, "sparse_weight_embedding_sparse_grad", {"x", "weight", "out_grad", "padding_idx"}, {"out_grad"}, {}, {}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "embedding_grad");
}

void SparseWeightEmbeddingSparseGradOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, pir::Value weight_, pir::Value out_grad_, int64_t padding_idx) {
  VLOG(4) << "Start build SparseWeightEmbeddingSparseGradOp";



  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_, weight_, out_grad_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_padding_idx = pir::Int64Attribute::get(pir::IrContext::Instance(), padding_idx);
  argument.AddAttribute("padding_idx", attr_padding_idx);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::SelectedRowsType x = x_.type().dyn_cast<paddle::dialect::SelectedRowsType>(); (void)x;
  paddle::dialect::DenseTensorType weight = weight_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)weight;
  paddle::dialect::DenseTensorType out_grad = out_grad_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)out_grad;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);

  VLOG(4) << "Builder construction  dense_weight";
  paddle::dialect::IrTensor ir_tensor_weight(paddle::dialect::TransToPhiDataType(weight.dtype()),
                                                      weight.dims(),
                                                      weight.data_layout(),
                                                      weight.lod(),
                                                      weight.offset());
  VLOG(4) << "Builder construction  meta_weight";
  paddle::dialect::IrMetaTensor meta_weight(&ir_tensor_weight);
  paddle::dialect::IrSelectedRows dense_weight_grad;
  paddle::dialect::IrMetaTensor meta_weight_grad(&dense_weight_grad);

  phi::EmbeddingGradSparseInferMeta(meta_x, meta_weight, &meta_weight_grad);

  std::vector<pir::Type> argument_outputs;
  pir::Type weight_grad_dense_tensor_type = paddle::dialect::SelectedRowsType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_weight_grad.dtype()), dense_weight_grad.dims(), dense_weight_grad.layout(), dense_weight_grad.lod(), dense_weight_grad.offset());
  argument_outputs.push_back(weight_grad_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void SparseWeightEmbeddingSparseGradOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, pir::Value weight_, pir::Value out_grad_, pir::AttributeMap attributes) {
  VLOG(4) << "Start build SparseWeightEmbeddingSparseGradOp";


  IR_ENFORCE(
      attributes.find("padding_idx") != attributes.end(),
          "'padding_idx' Attribute is expected for SparseWeightEmbeddingSparseGradOp. ");
  int64_t padding_idx = attributes.at("padding_idx").dyn_cast<pir::Int64Attribute>().data();


  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_, weight_, out_grad_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_padding_idx = pir::Int64Attribute::get(pir::IrContext::Instance(), padding_idx);
  argument.AddAttribute("padding_idx", attr_padding_idx);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::SelectedRowsType x = x_.type().dyn_cast<paddle::dialect::SelectedRowsType>(); (void)x;
  paddle::dialect::DenseTensorType weight = weight_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)weight;
  paddle::dialect::DenseTensorType out_grad = out_grad_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)out_grad;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);

  VLOG(4) << "Builder construction  dense_weight";
  paddle::dialect::IrTensor ir_tensor_weight(paddle::dialect::TransToPhiDataType(weight.dtype()),
                                                      weight.dims(),
                                                      weight.data_layout(),
                                                      weight.lod(),
                                                      weight.offset());
  VLOG(4) << "Builder construction  meta_weight";
  paddle::dialect::IrMetaTensor meta_weight(&ir_tensor_weight);
  paddle::dialect::IrSelectedRows dense_weight_grad;
  paddle::dialect::IrMetaTensor meta_weight_grad(&dense_weight_grad);

  phi::EmbeddingGradSparseInferMeta(meta_x, meta_weight, &meta_weight_grad);

  std::vector<pir::Type> argument_outputs;
  pir::Type weight_grad_dense_tensor_type = paddle::dialect::SelectedRowsType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_weight_grad.dtype()), dense_weight_grad.dims(), dense_weight_grad.layout(), dense_weight_grad.lod(), dense_weight_grad.offset());
  argument_outputs.push_back(weight_grad_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void SparseWeightEmbeddingSparseGradOp::VerifySig() {}

void SparseWeightEmbeddingSparseGradOp::InferMeta( phi::InferMetaContext *infer_meta ) {
  auto fn = PD_INFER_META(phi::EmbeddingGradSparseInferMeta);
  fn(infer_meta);
}

phi::DataType SparseWeightEmbeddingSparseGradOp::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: SparseWeightEmbeddingSparseGradOp";
  


  return expected_kernel_dtype;
}

OpInfoTuple ExponentialGradOp::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("out_grad", "paddle::dialect::DenseTensorType", false, false, false, false) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = {  };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("x_grad", "paddle::dialect::DenseTensorType", false, false) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("", {""}, "", {""}, {}, {}, {}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "exponential__grad");
}

void ExponentialGradOp::VerifySig() {}

phi::DataType ExponentialGradOp::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: ExponentialGradOp";
  


  return expected_kernel_dtype;
}

const char *FrobeniusNormGradOp::attributes_name[2] = { "keep_dim", "reduce_all" };

OpInfoTuple FrobeniusNormGradOp::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("x", "paddle::dialect::DenseTensorType", false, false, false, false), paddle::dialect::OpInputInfo("out", "paddle::dialect::DenseTensorType", false, false, false, false), paddle::dialect::OpInputInfo("out_grad", "paddle::dialect::DenseTensorType", false, false, false, false), paddle::dialect::OpInputInfo("axis", "paddle::dialect::IntArrayAttribute", false, false, true, false) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = { paddle::dialect::OpAttributeInfo("keep_dim", "pir::BoolAttribute", ""), paddle::dialect::OpAttributeInfo("reduce_all", "pir::BoolAttribute", "") };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("x_grad", "paddle::dialect::DenseTensorType", false, false) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("UnchangedInferMeta", {"x"}, "frobenius_norm_grad", {"x", "out", "out_grad", "axis", "keep_dim", "reduce_all"}, {}, {}, {}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "frobenius_norm_grad");
}

void FrobeniusNormGradOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, pir::Value out_, pir::Value out_grad_, const std::vector<int64_t>& axis, bool keep_dim, bool reduce_all) {
  VLOG(4) << "Start build FrobeniusNormGradOp";


  // Generate int_array mutable attribute: axis
  paddle::dialect::FullIntArrayOp full_axis_op = builder.Build<paddle::dialect::FullIntArrayOp>(axis, phi::DataType::INT64, phi::CPUPlace());
  pir::OpResult axis_ = full_axis_op->result(0);
    
  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_, out_, out_grad_, axis_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_keep_dim = pir::BoolAttribute::get(pir::IrContext::Instance(), keep_dim);
  argument.AddAttribute("keep_dim", attr_keep_dim);
  pir::Attribute attr_reduce_all = pir::BoolAttribute::get(pir::IrContext::Instance(), reduce_all);
  argument.AddAttribute("reduce_all", attr_reduce_all);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;
  paddle::dialect::DenseTensorType out = out_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)out;
  paddle::dialect::DenseTensorType out_grad = out_grad_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)out_grad;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);
  paddle::dialect::IrTensor dense_x_grad;
  paddle::dialect::IrMetaTensor meta_x_grad(&dense_x_grad);

  phi::UnchangedInferMeta(meta_x, &meta_x_grad);

  std::vector<pir::Type> argument_outputs;
  pir::Type x_grad_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_x_grad.dtype()), dense_x_grad.dims(), dense_x_grad.layout(), dense_x_grad.lod(), dense_x_grad.offset());
  argument_outputs.push_back(x_grad_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void FrobeniusNormGradOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, pir::Value out_, pir::Value out_grad_, pir::AttributeMap attributes) {
  VLOG(4) << "Start build FrobeniusNormGradOp";


  IR_ENFORCE(
      attributes.find("axis") != attributes.end(),
          "'axis' Attribute is expected for FrobeniusNormGradOp. ");
  std::vector<int64_t> axis = attributes.at("axis").dyn_cast<paddle::dialect::IntArrayAttribute>().data().GetData();

  IR_ENFORCE(
      attributes.find("keep_dim") != attributes.end(),
          "'keep_dim' Attribute is expected for FrobeniusNormGradOp. ");
  bool keep_dim = attributes.at("keep_dim").dyn_cast<pir::BoolAttribute>().data();

  IR_ENFORCE(
      attributes.find("reduce_all") != attributes.end(),
          "'reduce_all' Attribute is expected for FrobeniusNormGradOp. ");
  bool reduce_all = attributes.at("reduce_all").dyn_cast<pir::BoolAttribute>().data();

  // Generate int_array mutable attribute: axis
  paddle::dialect::FullIntArrayOp full_axis_op = builder.Build<paddle::dialect::FullIntArrayOp>(axis, phi::DataType::INT64, phi::CPUPlace());
  pir::OpResult axis_ = full_axis_op->result(0);
    
  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_, out_, out_grad_, axis_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_keep_dim = pir::BoolAttribute::get(pir::IrContext::Instance(), keep_dim);
  argument.AddAttribute("keep_dim", attr_keep_dim);
  pir::Attribute attr_reduce_all = pir::BoolAttribute::get(pir::IrContext::Instance(), reduce_all);
  argument.AddAttribute("reduce_all", attr_reduce_all);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;
  paddle::dialect::DenseTensorType out = out_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)out;
  paddle::dialect::DenseTensorType out_grad = out_grad_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)out_grad;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);
  paddle::dialect::IrTensor dense_x_grad;
  paddle::dialect::IrMetaTensor meta_x_grad(&dense_x_grad);

  phi::UnchangedInferMeta(meta_x, &meta_x_grad);

  std::vector<pir::Type> argument_outputs;
  pir::Type x_grad_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_x_grad.dtype()), dense_x_grad.dims(), dense_x_grad.layout(), dense_x_grad.lod(), dense_x_grad.offset());
  argument_outputs.push_back(x_grad_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void FrobeniusNormGradOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, pir::Value out_, pir::Value out_grad_, pir::Value axis_, bool keep_dim, bool reduce_all) {
  VLOG(4) << "Start build FrobeniusNormGradOp";



  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_, out_, out_grad_, axis_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_keep_dim = pir::BoolAttribute::get(pir::IrContext::Instance(), keep_dim);
  argument.AddAttribute("keep_dim", attr_keep_dim);
  pir::Attribute attr_reduce_all = pir::BoolAttribute::get(pir::IrContext::Instance(), reduce_all);
  argument.AddAttribute("reduce_all", attr_reduce_all);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;
  paddle::dialect::DenseTensorType out = out_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)out;
  paddle::dialect::DenseTensorType out_grad = out_grad_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)out_grad;
  phi::IntArray axis;
  if (axis_.dyn_cast<pir::OpResult>() && axis_.dyn_cast<pir::OpResult>().owner()->isa<paddle::dialect::FullIntArrayOp>()) {
    axis = std::move(phi::IntArray(paddle::dialect::GetInt64Vector(
                          axis_.dyn_cast<pir::OpResult>().owner()
                          ->dyn_cast<paddle::dialect::FullIntArrayOp>()
                          .attribute("value"))));
  } else if (axis_.type().isa<pir::VectorType>()) {
    size_t axis_size = axis_.type().dyn_cast<pir::VectorType>().size();
    axis = std::move(phi::IntArray(std::vector<int64_t>(axis_size, -1)));
    axis.SetFromTensor(true);
  } else if (axis_.type().isa<paddle::dialect::DenseTensorType>()) {
    common::DDim axis_dim = axis_.type().dyn_cast<paddle::dialect::DenseTensorType>().dims();
    size_t axis_size = common::product(axis_dim);
    if (common::contain_unknown_dim(axis_dim)) {
      axis_size = 1;
    }
    axis = std::move(phi::IntArray(std::vector<int64_t>(axis_size, -1)));
    axis.SetFromTensor(true);
  } else {
    PADDLE_THROW(phi::errors::Unimplemented("Only support VectorType or DenseTensorType"));
  }


  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);
  paddle::dialect::IrTensor dense_x_grad;
  paddle::dialect::IrMetaTensor meta_x_grad(&dense_x_grad);

  phi::UnchangedInferMeta(meta_x, &meta_x_grad);

  std::vector<pir::Type> argument_outputs;
  pir::Type x_grad_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_x_grad.dtype()), dense_x_grad.dims(), dense_x_grad.layout(), dense_x_grad.lod(), dense_x_grad.offset());
  argument_outputs.push_back(x_grad_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void FrobeniusNormGradOp::VerifySig() {}

void FrobeniusNormGradOp::InferMeta( phi::InferMetaContext *infer_meta ) {
  auto fn = PD_INFER_META(phi::UnchangedInferMeta);
  fn(infer_meta);
}

phi::DataType FrobeniusNormGradOp::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: FrobeniusNormGradOp";
  


  return expected_kernel_dtype;
}

const char *FusedAttentionGradOp::attributes_name[16] = { "num_heads", "transpose_qkv_wb", "pre_layer_norm", "epsilon", "attn_dropout_rate", "is_test", "attn_dropout_fix_seed", "attn_dropout_seed", "attn_dropout_implementation", "dropout_rate", "dropout_fix_seed", "dropout_seed", "dropout_implementation", "ln_epsilon", "add_residual", "ring_id" };

OpInfoTuple FusedAttentionGradOp::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("out_grad", "paddle::dialect::DenseTensorType", false, false, false, false), paddle::dialect::OpInputInfo("x", "paddle::dialect::DenseTensorType", false, false, false, false), paddle::dialect::OpInputInfo("qkv_weight", "paddle::dialect::DenseTensorType", false, false, false, false), paddle::dialect::OpInputInfo("qkv_bias", "paddle::dialect::DenseTensorType", true, false, false, false), paddle::dialect::OpInputInfo("qkv_bias_out", "paddle::dialect::DenseTensorType", true, true, false, false), paddle::dialect::OpInputInfo("src_mask", "paddle::dialect::DenseTensorType", true, true, false, false), paddle::dialect::OpInputInfo("src_mask_out", "paddle::dialect::DenseTensorType", true, false, false, false), paddle::dialect::OpInputInfo("out_linear_weight", "paddle::dialect::DenseTensorType", false, false, false, false), paddle::dialect::OpInputInfo("out_linear_bias", "paddle::dialect::DenseTensorType", true, false, false, false), paddle::dialect::OpInputInfo("ln_scale", "paddle::dialect::DenseTensorType", true, false, false, false), paddle::dialect::OpInputInfo("ln_bias", "paddle::dialect::DenseTensorType", true, false, false, false), paddle::dialect::OpInputInfo("ln_scale_2", "paddle::dialect::DenseTensorType", true, false, false, false), paddle::dialect::OpInputInfo("ln_bias_2", "paddle::dialect::DenseTensorType", true, false, false, false), paddle::dialect::OpInputInfo("ln_out", "paddle::dialect::DenseTensorType", true, false, false, false), paddle::dialect::OpInputInfo("ln_mean", "paddle::dialect::DenseTensorType", true, false, false, false), paddle::dialect::OpInputInfo("ln_var", "paddle::dialect::DenseTensorType", true, false, false, false), paddle::dialect::OpInputInfo("ln_mean_2", "paddle::dialect::DenseTensorType", true, false, false, false), paddle::dialect::OpInputInfo("ln_var_2", "paddle::dialect::DenseTensorType", true, false, false, false), paddle::dialect::OpInputInfo("bias_dropout_residual_out", "paddle::dialect::DenseTensorType", true, false, false, false), paddle::dialect::OpInputInfo("qkv_out", "paddle::dialect::DenseTensorType", false, true, false, false), paddle::dialect::OpInputInfo("transpose_out_2", "paddle::dialect::DenseTensorType", false, false, false, false), paddle::dialect::OpInputInfo("qk_out", "paddle::dialect::DenseTensorType", false, true, false, false), paddle::dialect::OpInputInfo("qktv_out", "paddle::dialect::DenseTensorType", false, true, false, false), paddle::dialect::OpInputInfo("softmax_out", "paddle::dialect::DenseTensorType", false, false, false, false), paddle::dialect::OpInputInfo("attn_dropout_mask_out", "paddle::dialect::DenseTensorType", false, false, false, false), paddle::dialect::OpInputInfo("attn_dropout_out", "paddle::dialect::DenseTensorType", false, false, false, false), paddle::dialect::OpInputInfo("fmha_out", "paddle::dialect::DenseTensorType", false, false, false, false), paddle::dialect::OpInputInfo("out_linear_out", "paddle::dialect::DenseTensorType", false, true, false, false), paddle::dialect::OpInputInfo("dropout_mask_out", "paddle::dialect::DenseTensorType", false, false, false, false) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = { paddle::dialect::OpAttributeInfo("num_heads", "pir::Int32Attribute", ""), paddle::dialect::OpAttributeInfo("transpose_qkv_wb", "pir::BoolAttribute", ""), paddle::dialect::OpAttributeInfo("pre_layer_norm", "pir::BoolAttribute", ""), paddle::dialect::OpAttributeInfo("epsilon", "pir::FloatAttribute", ""), paddle::dialect::OpAttributeInfo("attn_dropout_rate", "pir::FloatAttribute", ""), paddle::dialect::OpAttributeInfo("is_test", "pir::BoolAttribute", ""), paddle::dialect::OpAttributeInfo("attn_dropout_fix_seed", "pir::BoolAttribute", ""), paddle::dialect::OpAttributeInfo("attn_dropout_seed", "pir::Int32Attribute", ""), paddle::dialect::OpAttributeInfo("attn_dropout_implementation", "pir::StrAttribute", ""), paddle::dialect::OpAttributeInfo("dropout_rate", "pir::FloatAttribute", ""), paddle::dialect::OpAttributeInfo("dropout_fix_seed", "pir::BoolAttribute", ""), paddle::dialect::OpAttributeInfo("dropout_seed", "pir::Int32Attribute", ""), paddle::dialect::OpAttributeInfo("dropout_implementation", "pir::StrAttribute", ""), paddle::dialect::OpAttributeInfo("ln_epsilon", "pir::FloatAttribute", ""), paddle::dialect::OpAttributeInfo("add_residual", "pir::BoolAttribute", ""), paddle::dialect::OpAttributeInfo("ring_id", "pir::Int32Attribute", "") };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("qkv_bias_grad", "paddle::dialect::DenseTensorType", true, false), paddle::dialect::OpOutputInfo("qkv_bias_out_grad", "paddle::dialect::DenseTensorType", true, false), paddle::dialect::OpOutputInfo("src_mask_out_grad", "paddle::dialect::DenseTensorType", true, false), paddle::dialect::OpOutputInfo("out_linear_bias_grad", "paddle::dialect::DenseTensorType", true, false), paddle::dialect::OpOutputInfo("ln_scale_grad", "paddle::dialect::DenseTensorType", true, false), paddle::dialect::OpOutputInfo("ln_bias_grad", "paddle::dialect::DenseTensorType", true, false), paddle::dialect::OpOutputInfo("ln_scale_2_grad", "paddle::dialect::DenseTensorType", true, false), paddle::dialect::OpOutputInfo("ln_bias_2_grad", "paddle::dialect::DenseTensorType", true, false), paddle::dialect::OpOutputInfo("x_grad", "paddle::dialect::DenseTensorType", false, false), paddle::dialect::OpOutputInfo("qkv_weight_grad", "paddle::dialect::DenseTensorType", false, false), paddle::dialect::OpOutputInfo("out_linear_weight_grad", "paddle::dialect::DenseTensorType", false, false), paddle::dialect::OpOutputInfo("ln_out_grad", "paddle::dialect::DenseTensorType", true, false), paddle::dialect::OpOutputInfo("bias_dropout_residual_out_grad", "paddle::dialect::DenseTensorType", true, false), paddle::dialect::OpOutputInfo("qkv_out_grad", "paddle::dialect::DenseTensorType", false, false), paddle::dialect::OpOutputInfo("qktv_out_grad", "paddle::dialect::DenseTensorType", false, false), paddle::dialect::OpOutputInfo("transpose_out_2_grad", "paddle::dialect::DenseTensorType", false, false), paddle::dialect::OpOutputInfo("qk_out_grad", "paddle::dialect::DenseTensorType", false, false), paddle::dialect::OpOutputInfo("softmax_out_grad", "paddle::dialect::DenseTensorType", false, false), paddle::dialect::OpOutputInfo("attn_dropout_out_grad", "paddle::dialect::DenseTensorType", false, false), paddle::dialect::OpOutputInfo("fmha_out_grad", "paddle::dialect::DenseTensorType", false, false), paddle::dialect::OpOutputInfo("out_linear_out_grad", "paddle::dialect::DenseTensorType", false, false) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("FusedAttentionGradInferMeta", {"out_grad", "x", "qkv_weight", "qkv_bias", "qkv_bias_out", "src_mask", "src_mask_out", "out_linear_weight", "out_linear_bias", "ln_scale", "ln_bias", "ln_scale_2", "ln_bias_2", "ln_out", "ln_mean", "ln_var", "ln_mean_2", "ln_var_2", "bias_dropout_residual_out", "qkv_out", "transpose_out_2", "qk_out", "qktv_out", "softmax_out", "attn_dropout_mask_out", "attn_dropout_out", "fmha_out", "out_linear_out", "dropout_mask_out", "num_heads", "transpose_qkv_wb", "pre_layer_norm", "epsilon", "attn_dropout_rate", "is_test", "attn_dropout_fix_seed", "attn_dropout_seed", "attn_dropout_implementation", "dropout_rate", "dropout_fix_seed", "dropout_seed", "dropout_implementation", "ln_epsilon", "add_residual", "ring_id"}, "fused_attention_grad", {"out_grad", "x", "qkv_weight", "qkv_bias", "qkv_bias_out", "src_mask", "src_mask_out", "out_linear_weight", "out_linear_bias", "ln_scale", "ln_bias", "ln_scale_2", "ln_bias_2", "ln_out", "ln_mean", "ln_var", "ln_mean_2", "ln_var_2", "bias_dropout_residual_out", "qkv_out", "transpose_out_2", "qk_out", "qktv_out", "softmax_out", "attn_dropout_mask_out", "attn_dropout_out", "fmha_out", "out_linear_out", "dropout_mask_out", "num_heads", "transpose_qkv_wb", "pre_layer_norm", "epsilon", "attn_dropout_rate", "is_test", "attn_dropout_fix_seed", "attn_dropout_seed", "attn_dropout_implementation", "dropout_rate", "dropout_fix_seed", "dropout_seed", "dropout_implementation", "ln_epsilon", "add_residual", "ring_id"}, {"x"}, {}, {}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "fused_attention_grad");
}

void FusedAttentionGradOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value out_grad_, pir::Value x_, pir::Value qkv_weight_, pir::Value qkv_bias_, pir::Value qkv_bias_out_, pir::Value src_mask_, pir::Value src_mask_out_, pir::Value out_linear_weight_, pir::Value out_linear_bias_, pir::Value ln_scale_, pir::Value ln_bias_, pir::Value ln_scale_2_, pir::Value ln_bias_2_, pir::Value ln_out_, pir::Value ln_mean_, pir::Value ln_var_, pir::Value ln_mean_2_, pir::Value ln_var_2_, pir::Value bias_dropout_residual_out_, pir::Value qkv_out_, pir::Value transpose_out_2_, pir::Value qk_out_, pir::Value qktv_out_, pir::Value softmax_out_, pir::Value attn_dropout_mask_out_, pir::Value attn_dropout_out_, pir::Value fmha_out_, pir::Value out_linear_out_, pir::Value dropout_mask_out_, int num_heads, bool transpose_qkv_wb, bool pre_layer_norm, float epsilon, float attn_dropout_rate, bool is_test, bool attn_dropout_fix_seed, int attn_dropout_seed, const std::string& attn_dropout_implementation, float dropout_rate, bool dropout_fix_seed, int dropout_seed, const std::string& dropout_implementation, float ln_epsilon, bool add_residual, int ring_id) {
  VLOG(4) << "Start build FusedAttentionGradOp";



  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {out_grad_, x_, qkv_weight_, qkv_bias_, qkv_bias_out_, src_mask_, src_mask_out_, out_linear_weight_, out_linear_bias_, ln_scale_, ln_bias_, ln_scale_2_, ln_bias_2_, ln_out_, ln_mean_, ln_var_, ln_mean_2_, ln_var_2_, bias_dropout_residual_out_, qkv_out_, transpose_out_2_, qk_out_, qktv_out_, softmax_out_, attn_dropout_mask_out_, attn_dropout_out_, fmha_out_, out_linear_out_, dropout_mask_out_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_num_heads = pir::Int32Attribute::get(pir::IrContext::Instance(), num_heads);
  argument.AddAttribute("num_heads", attr_num_heads);
  pir::Attribute attr_transpose_qkv_wb = pir::BoolAttribute::get(pir::IrContext::Instance(), transpose_qkv_wb);
  argument.AddAttribute("transpose_qkv_wb", attr_transpose_qkv_wb);
  pir::Attribute attr_pre_layer_norm = pir::BoolAttribute::get(pir::IrContext::Instance(), pre_layer_norm);
  argument.AddAttribute("pre_layer_norm", attr_pre_layer_norm);
  pir::Attribute attr_epsilon = pir::FloatAttribute::get(pir::IrContext::Instance(), epsilon);
  argument.AddAttribute("epsilon", attr_epsilon);
  pir::Attribute attr_attn_dropout_rate = pir::FloatAttribute::get(pir::IrContext::Instance(), attn_dropout_rate);
  argument.AddAttribute("attn_dropout_rate", attr_attn_dropout_rate);
  pir::Attribute attr_is_test = pir::BoolAttribute::get(pir::IrContext::Instance(), is_test);
  argument.AddAttribute("is_test", attr_is_test);
  pir::Attribute attr_attn_dropout_fix_seed = pir::BoolAttribute::get(pir::IrContext::Instance(), attn_dropout_fix_seed);
  argument.AddAttribute("attn_dropout_fix_seed", attr_attn_dropout_fix_seed);
  pir::Attribute attr_attn_dropout_seed = pir::Int32Attribute::get(pir::IrContext::Instance(), attn_dropout_seed);
  argument.AddAttribute("attn_dropout_seed", attr_attn_dropout_seed);
  pir::Attribute attr_attn_dropout_implementation = pir::StrAttribute::get(pir::IrContext::Instance(), attn_dropout_implementation);
  argument.AddAttribute("attn_dropout_implementation", attr_attn_dropout_implementation);
  pir::Attribute attr_dropout_rate = pir::FloatAttribute::get(pir::IrContext::Instance(), dropout_rate);
  argument.AddAttribute("dropout_rate", attr_dropout_rate);
  pir::Attribute attr_dropout_fix_seed = pir::BoolAttribute::get(pir::IrContext::Instance(), dropout_fix_seed);
  argument.AddAttribute("dropout_fix_seed", attr_dropout_fix_seed);
  pir::Attribute attr_dropout_seed = pir::Int32Attribute::get(pir::IrContext::Instance(), dropout_seed);
  argument.AddAttribute("dropout_seed", attr_dropout_seed);
  pir::Attribute attr_dropout_implementation = pir::StrAttribute::get(pir::IrContext::Instance(), dropout_implementation);
  argument.AddAttribute("dropout_implementation", attr_dropout_implementation);
  pir::Attribute attr_ln_epsilon = pir::FloatAttribute::get(pir::IrContext::Instance(), ln_epsilon);
  argument.AddAttribute("ln_epsilon", attr_ln_epsilon);
  pir::Attribute attr_add_residual = pir::BoolAttribute::get(pir::IrContext::Instance(), add_residual);
  argument.AddAttribute("add_residual", attr_add_residual);
  pir::Attribute attr_ring_id = pir::Int32Attribute::get(pir::IrContext::Instance(), ring_id);
  argument.AddAttribute("ring_id", attr_ring_id);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType out_grad = out_grad_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)out_grad;
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;
  paddle::dialect::DenseTensorType qkv_weight = qkv_weight_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)qkv_weight;
  paddle::dialect::DenseTensorType out_linear_weight = out_linear_weight_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)out_linear_weight;
  paddle::dialect::DenseTensorType qkv_out = qkv_out_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)qkv_out;
  paddle::dialect::DenseTensorType transpose_out_2 = transpose_out_2_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)transpose_out_2;
  paddle::dialect::DenseTensorType qk_out = qk_out_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)qk_out;
  paddle::dialect::DenseTensorType qktv_out = qktv_out_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)qktv_out;
  paddle::dialect::DenseTensorType softmax_out = softmax_out_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)softmax_out;
  paddle::dialect::DenseTensorType attn_dropout_mask_out = attn_dropout_mask_out_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)attn_dropout_mask_out;
  paddle::dialect::DenseTensorType attn_dropout_out = attn_dropout_out_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)attn_dropout_out;
  paddle::dialect::DenseTensorType fmha_out = fmha_out_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)fmha_out;
  paddle::dialect::DenseTensorType out_linear_out = out_linear_out_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)out_linear_out;
  paddle::dialect::DenseTensorType dropout_mask_out = dropout_mask_out_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)dropout_mask_out;

  VLOG(4) << "Builder construction  dense_out_grad";
  paddle::dialect::IrTensor ir_tensor_out_grad(paddle::dialect::TransToPhiDataType(out_grad.dtype()),
                                                      out_grad.dims(),
                                                      out_grad.data_layout(),
                                                      out_grad.lod(),
                                                      out_grad.offset());
  VLOG(4) << "Builder construction  meta_out_grad";
  paddle::dialect::IrMetaTensor meta_out_grad(&ir_tensor_out_grad);

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);

  VLOG(4) << "Builder construction  dense_qkv_weight";
  paddle::dialect::IrTensor ir_tensor_qkv_weight(paddle::dialect::TransToPhiDataType(qkv_weight.dtype()),
                                                      qkv_weight.dims(),
                                                      qkv_weight.data_layout(),
                                                      qkv_weight.lod(),
                                                      qkv_weight.offset());
  VLOG(4) << "Builder construction  meta_qkv_weight";
  paddle::dialect::IrMetaTensor meta_qkv_weight(&ir_tensor_qkv_weight);

  paddle::dialect::IrMetaTensor meta_qkv_bias;
  paddle::dialect::IrTensor ir_tensor_qkv_bias;
  if (qkv_bias_.impl() != nullptr) {
    paddle::dialect::DenseTensorType qkv_bias = qkv_bias_.type().dyn_cast<paddle::dialect::DenseTensorType>();
    VLOG(4) << "Builder construction  dense_qkv_bias";
    ir_tensor_qkv_bias = paddle::dialect::IrTensor(paddle::dialect::TransToPhiDataType(qkv_bias.dtype()),
                                                        qkv_bias.dims(),
                                                        qkv_bias.data_layout(),
                                                        qkv_bias.lod(),
                                                        qkv_bias.offset());
    VLOG(4) << "Builder construction  meta_qkv_bias";
    meta_qkv_bias = paddle::dialect::IrMetaTensor(&ir_tensor_qkv_bias);
  }


  paddle::dialect::IrMetaTensor meta_qkv_bias_out;
  paddle::dialect::IrTensor ir_tensor_qkv_bias_out;
  if (qkv_bias_out_.impl() != nullptr) {
    paddle::dialect::DenseTensorType qkv_bias_out = qkv_bias_out_.type().dyn_cast<paddle::dialect::DenseTensorType>();
    VLOG(4) << "Builder construction  dense_qkv_bias_out";
    ir_tensor_qkv_bias_out = paddle::dialect::IrTensor(paddle::dialect::TransToPhiDataType(qkv_bias_out.dtype()),
                                                        qkv_bias_out.dims(),
                                                        qkv_bias_out.data_layout(),
                                                        qkv_bias_out.lod(),
                                                        qkv_bias_out.offset());
    VLOG(4) << "Builder construction  meta_qkv_bias_out";
    meta_qkv_bias_out = paddle::dialect::IrMetaTensor(&ir_tensor_qkv_bias_out);
  }


  paddle::dialect::IrMetaTensor meta_src_mask;
  paddle::dialect::IrTensor ir_tensor_src_mask;
  if (src_mask_.impl() != nullptr) {
    paddle::dialect::DenseTensorType src_mask = src_mask_.type().dyn_cast<paddle::dialect::DenseTensorType>();
    VLOG(4) << "Builder construction  dense_src_mask";
    ir_tensor_src_mask = paddle::dialect::IrTensor(paddle::dialect::TransToPhiDataType(src_mask.dtype()),
                                                        src_mask.dims(),
                                                        src_mask.data_layout(),
                                                        src_mask.lod(),
                                                        src_mask.offset());
    VLOG(4) << "Builder construction  meta_src_mask";
    meta_src_mask = paddle::dialect::IrMetaTensor(&ir_tensor_src_mask);
  }


  paddle::dialect::IrMetaTensor meta_src_mask_out;
  paddle::dialect::IrTensor ir_tensor_src_mask_out;
  if (src_mask_out_.impl() != nullptr) {
    paddle::dialect::DenseTensorType src_mask_out = src_mask_out_.type().dyn_cast<paddle::dialect::DenseTensorType>();
    VLOG(4) << "Builder construction  dense_src_mask_out";
    ir_tensor_src_mask_out = paddle::dialect::IrTensor(paddle::dialect::TransToPhiDataType(src_mask_out.dtype()),
                                                        src_mask_out.dims(),
                                                        src_mask_out.data_layout(),
                                                        src_mask_out.lod(),
                                                        src_mask_out.offset());
    VLOG(4) << "Builder construction  meta_src_mask_out";
    meta_src_mask_out = paddle::dialect::IrMetaTensor(&ir_tensor_src_mask_out);
  }


  VLOG(4) << "Builder construction  dense_out_linear_weight";
  paddle::dialect::IrTensor ir_tensor_out_linear_weight(paddle::dialect::TransToPhiDataType(out_linear_weight.dtype()),
                                                      out_linear_weight.dims(),
                                                      out_linear_weight.data_layout(),
                                                      out_linear_weight.lod(),
                                                      out_linear_weight.offset());
  VLOG(4) << "Builder construction  meta_out_linear_weight";
  paddle::dialect::IrMetaTensor meta_out_linear_weight(&ir_tensor_out_linear_weight);

  paddle::dialect::IrMetaTensor meta_out_linear_bias;
  paddle::dialect::IrTensor ir_tensor_out_linear_bias;
  if (out_linear_bias_.impl() != nullptr) {
    paddle::dialect::DenseTensorType out_linear_bias = out_linear_bias_.type().dyn_cast<paddle::dialect::DenseTensorType>();
    VLOG(4) << "Builder construction  dense_out_linear_bias";
    ir_tensor_out_linear_bias = paddle::dialect::IrTensor(paddle::dialect::TransToPhiDataType(out_linear_bias.dtype()),
                                                        out_linear_bias.dims(),
                                                        out_linear_bias.data_layout(),
                                                        out_linear_bias.lod(),
                                                        out_linear_bias.offset());
    VLOG(4) << "Builder construction  meta_out_linear_bias";
    meta_out_linear_bias = paddle::dialect::IrMetaTensor(&ir_tensor_out_linear_bias);
  }


  paddle::dialect::IrMetaTensor meta_ln_scale;
  paddle::dialect::IrTensor ir_tensor_ln_scale;
  if (ln_scale_.impl() != nullptr) {
    paddle::dialect::DenseTensorType ln_scale = ln_scale_.type().dyn_cast<paddle::dialect::DenseTensorType>();
    VLOG(4) << "Builder construction  dense_ln_scale";
    ir_tensor_ln_scale = paddle::dialect::IrTensor(paddle::dialect::TransToPhiDataType(ln_scale.dtype()),
                                                        ln_scale.dims(),
                                                        ln_scale.data_layout(),
                                                        ln_scale.lod(),
                                                        ln_scale.offset());
    VLOG(4) << "Builder construction  meta_ln_scale";
    meta_ln_scale = paddle::dialect::IrMetaTensor(&ir_tensor_ln_scale);
  }


  paddle::dialect::IrMetaTensor meta_ln_bias;
  paddle::dialect::IrTensor ir_tensor_ln_bias;
  if (ln_bias_.impl() != nullptr) {
    paddle::dialect::DenseTensorType ln_bias = ln_bias_.type().dyn_cast<paddle::dialect::DenseTensorType>();
    VLOG(4) << "Builder construction  dense_ln_bias";
    ir_tensor_ln_bias = paddle::dialect::IrTensor(paddle::dialect::TransToPhiDataType(ln_bias.dtype()),
                                                        ln_bias.dims(),
                                                        ln_bias.data_layout(),
                                                        ln_bias.lod(),
                                                        ln_bias.offset());
    VLOG(4) << "Builder construction  meta_ln_bias";
    meta_ln_bias = paddle::dialect::IrMetaTensor(&ir_tensor_ln_bias);
  }


  paddle::dialect::IrMetaTensor meta_ln_scale_2;
  paddle::dialect::IrTensor ir_tensor_ln_scale_2;
  if (ln_scale_2_.impl() != nullptr) {
    paddle::dialect::DenseTensorType ln_scale_2 = ln_scale_2_.type().dyn_cast<paddle::dialect::DenseTensorType>();
    VLOG(4) << "Builder construction  dense_ln_scale_2";
    ir_tensor_ln_scale_2 = paddle::dialect::IrTensor(paddle::dialect::TransToPhiDataType(ln_scale_2.dtype()),
                                                        ln_scale_2.dims(),
                                                        ln_scale_2.data_layout(),
                                                        ln_scale_2.lod(),
                                                        ln_scale_2.offset());
    VLOG(4) << "Builder construction  meta_ln_scale_2";
    meta_ln_scale_2 = paddle::dialect::IrMetaTensor(&ir_tensor_ln_scale_2);
  }


  paddle::dialect::IrMetaTensor meta_ln_bias_2;
  paddle::dialect::IrTensor ir_tensor_ln_bias_2;
  if (ln_bias_2_.impl() != nullptr) {
    paddle::dialect::DenseTensorType ln_bias_2 = ln_bias_2_.type().dyn_cast<paddle::dialect::DenseTensorType>();
    VLOG(4) << "Builder construction  dense_ln_bias_2";
    ir_tensor_ln_bias_2 = paddle::dialect::IrTensor(paddle::dialect::TransToPhiDataType(ln_bias_2.dtype()),
                                                        ln_bias_2.dims(),
                                                        ln_bias_2.data_layout(),
                                                        ln_bias_2.lod(),
                                                        ln_bias_2.offset());
    VLOG(4) << "Builder construction  meta_ln_bias_2";
    meta_ln_bias_2 = paddle::dialect::IrMetaTensor(&ir_tensor_ln_bias_2);
  }


  paddle::dialect::IrMetaTensor meta_ln_out;
  paddle::dialect::IrTensor ir_tensor_ln_out;
  if (ln_out_.impl() != nullptr) {
    paddle::dialect::DenseTensorType ln_out = ln_out_.type().dyn_cast<paddle::dialect::DenseTensorType>();
    VLOG(4) << "Builder construction  dense_ln_out";
    ir_tensor_ln_out = paddle::dialect::IrTensor(paddle::dialect::TransToPhiDataType(ln_out.dtype()),
                                                        ln_out.dims(),
                                                        ln_out.data_layout(),
                                                        ln_out.lod(),
                                                        ln_out.offset());
    VLOG(4) << "Builder construction  meta_ln_out";
    meta_ln_out = paddle::dialect::IrMetaTensor(&ir_tensor_ln_out);
  }


  paddle::dialect::IrMetaTensor meta_ln_mean;
  paddle::dialect::IrTensor ir_tensor_ln_mean;
  if (ln_mean_.impl() != nullptr) {
    paddle::dialect::DenseTensorType ln_mean = ln_mean_.type().dyn_cast<paddle::dialect::DenseTensorType>();
    VLOG(4) << "Builder construction  dense_ln_mean";
    ir_tensor_ln_mean = paddle::dialect::IrTensor(paddle::dialect::TransToPhiDataType(ln_mean.dtype()),
                                                        ln_mean.dims(),
                                                        ln_mean.data_layout(),
                                                        ln_mean.lod(),
                                                        ln_mean.offset());
    VLOG(4) << "Builder construction  meta_ln_mean";
    meta_ln_mean = paddle::dialect::IrMetaTensor(&ir_tensor_ln_mean);
  }


  paddle::dialect::IrMetaTensor meta_ln_var;
  paddle::dialect::IrTensor ir_tensor_ln_var;
  if (ln_var_.impl() != nullptr) {
    paddle::dialect::DenseTensorType ln_var = ln_var_.type().dyn_cast<paddle::dialect::DenseTensorType>();
    VLOG(4) << "Builder construction  dense_ln_var";
    ir_tensor_ln_var = paddle::dialect::IrTensor(paddle::dialect::TransToPhiDataType(ln_var.dtype()),
                                                        ln_var.dims(),
                                                        ln_var.data_layout(),
                                                        ln_var.lod(),
                                                        ln_var.offset());
    VLOG(4) << "Builder construction  meta_ln_var";
    meta_ln_var = paddle::dialect::IrMetaTensor(&ir_tensor_ln_var);
  }


  paddle::dialect::IrMetaTensor meta_ln_mean_2;
  paddle::dialect::IrTensor ir_tensor_ln_mean_2;
  if (ln_mean_2_.impl() != nullptr) {
    paddle::dialect::DenseTensorType ln_mean_2 = ln_mean_2_.type().dyn_cast<paddle::dialect::DenseTensorType>();
    VLOG(4) << "Builder construction  dense_ln_mean_2";
    ir_tensor_ln_mean_2 = paddle::dialect::IrTensor(paddle::dialect::TransToPhiDataType(ln_mean_2.dtype()),
                                                        ln_mean_2.dims(),
                                                        ln_mean_2.data_layout(),
                                                        ln_mean_2.lod(),
                                                        ln_mean_2.offset());
    VLOG(4) << "Builder construction  meta_ln_mean_2";
    meta_ln_mean_2 = paddle::dialect::IrMetaTensor(&ir_tensor_ln_mean_2);
  }


  paddle::dialect::IrMetaTensor meta_ln_var_2;
  paddle::dialect::IrTensor ir_tensor_ln_var_2;
  if (ln_var_2_.impl() != nullptr) {
    paddle::dialect::DenseTensorType ln_var_2 = ln_var_2_.type().dyn_cast<paddle::dialect::DenseTensorType>();
    VLOG(4) << "Builder construction  dense_ln_var_2";
    ir_tensor_ln_var_2 = paddle::dialect::IrTensor(paddle::dialect::TransToPhiDataType(ln_var_2.dtype()),
                                                        ln_var_2.dims(),
                                                        ln_var_2.data_layout(),
                                                        ln_var_2.lod(),
                                                        ln_var_2.offset());
    VLOG(4) << "Builder construction  meta_ln_var_2";
    meta_ln_var_2 = paddle::dialect::IrMetaTensor(&ir_tensor_ln_var_2);
  }


  paddle::dialect::IrMetaTensor meta_bias_dropout_residual_out;
  paddle::dialect::IrTensor ir_tensor_bias_dropout_residual_out;
  if (bias_dropout_residual_out_.impl() != nullptr) {
    paddle::dialect::DenseTensorType bias_dropout_residual_out = bias_dropout_residual_out_.type().dyn_cast<paddle::dialect::DenseTensorType>();
    VLOG(4) << "Builder construction  dense_bias_dropout_residual_out";
    ir_tensor_bias_dropout_residual_out = paddle::dialect::IrTensor(paddle::dialect::TransToPhiDataType(bias_dropout_residual_out.dtype()),
                                                        bias_dropout_residual_out.dims(),
                                                        bias_dropout_residual_out.data_layout(),
                                                        bias_dropout_residual_out.lod(),
                                                        bias_dropout_residual_out.offset());
    VLOG(4) << "Builder construction  meta_bias_dropout_residual_out";
    meta_bias_dropout_residual_out = paddle::dialect::IrMetaTensor(&ir_tensor_bias_dropout_residual_out);
  }


  VLOG(4) << "Builder construction  dense_qkv_out";
  paddle::dialect::IrTensor ir_tensor_qkv_out(paddle::dialect::TransToPhiDataType(qkv_out.dtype()),
                                                      qkv_out.dims(),
                                                      qkv_out.data_layout(),
                                                      qkv_out.lod(),
                                                      qkv_out.offset());
  VLOG(4) << "Builder construction  meta_qkv_out";
  paddle::dialect::IrMetaTensor meta_qkv_out(&ir_tensor_qkv_out);

  VLOG(4) << "Builder construction  dense_transpose_out_2";
  paddle::dialect::IrTensor ir_tensor_transpose_out_2(paddle::dialect::TransToPhiDataType(transpose_out_2.dtype()),
                                                      transpose_out_2.dims(),
                                                      transpose_out_2.data_layout(),
                                                      transpose_out_2.lod(),
                                                      transpose_out_2.offset());
  VLOG(4) << "Builder construction  meta_transpose_out_2";
  paddle::dialect::IrMetaTensor meta_transpose_out_2(&ir_tensor_transpose_out_2);

  VLOG(4) << "Builder construction  dense_qk_out";
  paddle::dialect::IrTensor ir_tensor_qk_out(paddle::dialect::TransToPhiDataType(qk_out.dtype()),
                                                      qk_out.dims(),
                                                      qk_out.data_layout(),
                                                      qk_out.lod(),
                                                      qk_out.offset());
  VLOG(4) << "Builder construction  meta_qk_out";
  paddle::dialect::IrMetaTensor meta_qk_out(&ir_tensor_qk_out);

  VLOG(4) << "Builder construction  dense_qktv_out";
  paddle::dialect::IrTensor ir_tensor_qktv_out(paddle::dialect::TransToPhiDataType(qktv_out.dtype()),
                                                      qktv_out.dims(),
                                                      qktv_out.data_layout(),
                                                      qktv_out.lod(),
                                                      qktv_out.offset());
  VLOG(4) << "Builder construction  meta_qktv_out";
  paddle::dialect::IrMetaTensor meta_qktv_out(&ir_tensor_qktv_out);

  VLOG(4) << "Builder construction  dense_softmax_out";
  paddle::dialect::IrTensor ir_tensor_softmax_out(paddle::dialect::TransToPhiDataType(softmax_out.dtype()),
                                                      softmax_out.dims(),
                                                      softmax_out.data_layout(),
                                                      softmax_out.lod(),
                                                      softmax_out.offset());
  VLOG(4) << "Builder construction  meta_softmax_out";
  paddle::dialect::IrMetaTensor meta_softmax_out(&ir_tensor_softmax_out);

  VLOG(4) << "Builder construction  dense_attn_dropout_mask_out";
  paddle::dialect::IrTensor ir_tensor_attn_dropout_mask_out(paddle::dialect::TransToPhiDataType(attn_dropout_mask_out.dtype()),
                                                      attn_dropout_mask_out.dims(),
                                                      attn_dropout_mask_out.data_layout(),
                                                      attn_dropout_mask_out.lod(),
                                                      attn_dropout_mask_out.offset());
  VLOG(4) << "Builder construction  meta_attn_dropout_mask_out";
  paddle::dialect::IrMetaTensor meta_attn_dropout_mask_out(&ir_tensor_attn_dropout_mask_out);

  VLOG(4) << "Builder construction  dense_attn_dropout_out";
  paddle::dialect::IrTensor ir_tensor_attn_dropout_out(paddle::dialect::TransToPhiDataType(attn_dropout_out.dtype()),
                                                      attn_dropout_out.dims(),
                                                      attn_dropout_out.data_layout(),
                                                      attn_dropout_out.lod(),
                                                      attn_dropout_out.offset());
  VLOG(4) << "Builder construction  meta_attn_dropout_out";
  paddle::dialect::IrMetaTensor meta_attn_dropout_out(&ir_tensor_attn_dropout_out);

  VLOG(4) << "Builder construction  dense_fmha_out";
  paddle::dialect::IrTensor ir_tensor_fmha_out(paddle::dialect::TransToPhiDataType(fmha_out.dtype()),
                                                      fmha_out.dims(),
                                                      fmha_out.data_layout(),
                                                      fmha_out.lod(),
                                                      fmha_out.offset());
  VLOG(4) << "Builder construction  meta_fmha_out";
  paddle::dialect::IrMetaTensor meta_fmha_out(&ir_tensor_fmha_out);

  VLOG(4) << "Builder construction  dense_out_linear_out";
  paddle::dialect::IrTensor ir_tensor_out_linear_out(paddle::dialect::TransToPhiDataType(out_linear_out.dtype()),
                                                      out_linear_out.dims(),
                                                      out_linear_out.data_layout(),
                                                      out_linear_out.lod(),
                                                      out_linear_out.offset());
  VLOG(4) << "Builder construction  meta_out_linear_out";
  paddle::dialect::IrMetaTensor meta_out_linear_out(&ir_tensor_out_linear_out);

  VLOG(4) << "Builder construction  dense_dropout_mask_out";
  paddle::dialect::IrTensor ir_tensor_dropout_mask_out(paddle::dialect::TransToPhiDataType(dropout_mask_out.dtype()),
                                                      dropout_mask_out.dims(),
                                                      dropout_mask_out.data_layout(),
                                                      dropout_mask_out.lod(),
                                                      dropout_mask_out.offset());
  VLOG(4) << "Builder construction  meta_dropout_mask_out";
  paddle::dialect::IrMetaTensor meta_dropout_mask_out(&ir_tensor_dropout_mask_out);
  paddle::dialect::IrTensor dense_qkv_bias_grad;
  paddle::dialect::IrMetaTensor meta_qkv_bias_grad(&dense_qkv_bias_grad);
  paddle::dialect::IrTensor dense_qkv_bias_out_grad;
  paddle::dialect::IrMetaTensor meta_qkv_bias_out_grad(&dense_qkv_bias_out_grad);
  paddle::dialect::IrTensor dense_src_mask_out_grad;
  paddle::dialect::IrMetaTensor meta_src_mask_out_grad(&dense_src_mask_out_grad);
  paddle::dialect::IrTensor dense_out_linear_bias_grad;
  paddle::dialect::IrMetaTensor meta_out_linear_bias_grad(&dense_out_linear_bias_grad);
  paddle::dialect::IrTensor dense_ln_scale_grad;
  paddle::dialect::IrMetaTensor meta_ln_scale_grad(&dense_ln_scale_grad);
  paddle::dialect::IrTensor dense_ln_bias_grad;
  paddle::dialect::IrMetaTensor meta_ln_bias_grad(&dense_ln_bias_grad);
  paddle::dialect::IrTensor dense_ln_scale_2_grad;
  paddle::dialect::IrMetaTensor meta_ln_scale_2_grad(&dense_ln_scale_2_grad);
  paddle::dialect::IrTensor dense_ln_bias_2_grad;
  paddle::dialect::IrMetaTensor meta_ln_bias_2_grad(&dense_ln_bias_2_grad);
  paddle::dialect::IrTensor dense_x_grad;
  paddle::dialect::IrMetaTensor meta_x_grad(&dense_x_grad);
  paddle::dialect::IrTensor dense_qkv_weight_grad;
  paddle::dialect::IrMetaTensor meta_qkv_weight_grad(&dense_qkv_weight_grad);
  paddle::dialect::IrTensor dense_out_linear_weight_grad;
  paddle::dialect::IrMetaTensor meta_out_linear_weight_grad(&dense_out_linear_weight_grad);
  paddle::dialect::IrTensor dense_ln_out_grad;
  paddle::dialect::IrMetaTensor meta_ln_out_grad(&dense_ln_out_grad);
  paddle::dialect::IrTensor dense_bias_dropout_residual_out_grad;
  paddle::dialect::IrMetaTensor meta_bias_dropout_residual_out_grad(&dense_bias_dropout_residual_out_grad);
  paddle::dialect::IrTensor dense_qkv_out_grad;
  paddle::dialect::IrMetaTensor meta_qkv_out_grad(&dense_qkv_out_grad);
  paddle::dialect::IrTensor dense_qktv_out_grad;
  paddle::dialect::IrMetaTensor meta_qktv_out_grad(&dense_qktv_out_grad);
  paddle::dialect::IrTensor dense_transpose_out_2_grad;
  paddle::dialect::IrMetaTensor meta_transpose_out_2_grad(&dense_transpose_out_2_grad);
  paddle::dialect::IrTensor dense_qk_out_grad;
  paddle::dialect::IrMetaTensor meta_qk_out_grad(&dense_qk_out_grad);
  paddle::dialect::IrTensor dense_softmax_out_grad;
  paddle::dialect::IrMetaTensor meta_softmax_out_grad(&dense_softmax_out_grad);
  paddle::dialect::IrTensor dense_attn_dropout_out_grad;
  paddle::dialect::IrMetaTensor meta_attn_dropout_out_grad(&dense_attn_dropout_out_grad);
  paddle::dialect::IrTensor dense_fmha_out_grad;
  paddle::dialect::IrMetaTensor meta_fmha_out_grad(&dense_fmha_out_grad);
  paddle::dialect::IrTensor dense_out_linear_out_grad;
  paddle::dialect::IrMetaTensor meta_out_linear_out_grad(&dense_out_linear_out_grad);

  phi::FusedAttentionGradInferMeta(meta_out_grad, meta_x, meta_qkv_weight, meta_qkv_bias, meta_qkv_bias_out, meta_src_mask, meta_src_mask_out, meta_out_linear_weight, meta_out_linear_bias, meta_ln_scale, meta_ln_bias, meta_ln_scale_2, meta_ln_bias_2, meta_ln_out, meta_ln_mean, meta_ln_var, meta_ln_mean_2, meta_ln_var_2, meta_bias_dropout_residual_out, meta_qkv_out, meta_transpose_out_2, meta_qk_out, meta_qktv_out, meta_softmax_out, meta_attn_dropout_mask_out, meta_attn_dropout_out, meta_fmha_out, meta_out_linear_out, meta_dropout_mask_out, num_heads, transpose_qkv_wb, pre_layer_norm, epsilon, attn_dropout_rate, is_test, attn_dropout_fix_seed, attn_dropout_seed, attn_dropout_implementation, dropout_rate, dropout_fix_seed, dropout_seed, dropout_implementation, ln_epsilon, add_residual, ring_id, &meta_qkv_bias_grad, &meta_qkv_bias_out_grad, &meta_src_mask_out_grad, &meta_out_linear_bias_grad, &meta_ln_scale_grad, &meta_ln_bias_grad, &meta_ln_scale_2_grad, &meta_ln_bias_2_grad, &meta_x_grad, &meta_qkv_weight_grad, &meta_out_linear_weight_grad, &meta_ln_out_grad, &meta_bias_dropout_residual_out_grad, &meta_qkv_out_grad, &meta_qktv_out_grad, &meta_transpose_out_2_grad, &meta_qk_out_grad, &meta_softmax_out_grad, &meta_attn_dropout_out_grad, &meta_fmha_out_grad, &meta_out_linear_out_grad);

  std::vector<pir::Type> argument_outputs;
  pir::Type qkv_bias_grad_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_qkv_bias_grad.dtype()), dense_qkv_bias_grad.dims(), dense_qkv_bias_grad.layout(), dense_qkv_bias_grad.lod(), dense_qkv_bias_grad.offset());
  argument_outputs.push_back(qkv_bias_grad_dense_tensor_type);

  pir::Type qkv_bias_out_grad_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_qkv_bias_out_grad.dtype()), dense_qkv_bias_out_grad.dims(), dense_qkv_bias_out_grad.layout(), dense_qkv_bias_out_grad.lod(), dense_qkv_bias_out_grad.offset());
  argument_outputs.push_back(qkv_bias_out_grad_dense_tensor_type);

  pir::Type src_mask_out_grad_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_src_mask_out_grad.dtype()), dense_src_mask_out_grad.dims(), dense_src_mask_out_grad.layout(), dense_src_mask_out_grad.lod(), dense_src_mask_out_grad.offset());
  argument_outputs.push_back(src_mask_out_grad_dense_tensor_type);

  pir::Type out_linear_bias_grad_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_out_linear_bias_grad.dtype()), dense_out_linear_bias_grad.dims(), dense_out_linear_bias_grad.layout(), dense_out_linear_bias_grad.lod(), dense_out_linear_bias_grad.offset());
  argument_outputs.push_back(out_linear_bias_grad_dense_tensor_type);

  pir::Type ln_scale_grad_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_ln_scale_grad.dtype()), dense_ln_scale_grad.dims(), dense_ln_scale_grad.layout(), dense_ln_scale_grad.lod(), dense_ln_scale_grad.offset());
  argument_outputs.push_back(ln_scale_grad_dense_tensor_type);

  pir::Type ln_bias_grad_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_ln_bias_grad.dtype()), dense_ln_bias_grad.dims(), dense_ln_bias_grad.layout(), dense_ln_bias_grad.lod(), dense_ln_bias_grad.offset());
  argument_outputs.push_back(ln_bias_grad_dense_tensor_type);

  pir::Type ln_scale_2_grad_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_ln_scale_2_grad.dtype()), dense_ln_scale_2_grad.dims(), dense_ln_scale_2_grad.layout(), dense_ln_scale_2_grad.lod(), dense_ln_scale_2_grad.offset());
  argument_outputs.push_back(ln_scale_2_grad_dense_tensor_type);

  pir::Type ln_bias_2_grad_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_ln_bias_2_grad.dtype()), dense_ln_bias_2_grad.dims(), dense_ln_bias_2_grad.layout(), dense_ln_bias_2_grad.lod(), dense_ln_bias_2_grad.offset());
  argument_outputs.push_back(ln_bias_2_grad_dense_tensor_type);

  pir::Type x_grad_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_x_grad.dtype()), dense_x_grad.dims(), dense_x_grad.layout(), dense_x_grad.lod(), dense_x_grad.offset());
  argument_outputs.push_back(x_grad_dense_tensor_type);

  pir::Type qkv_weight_grad_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_qkv_weight_grad.dtype()), dense_qkv_weight_grad.dims(), dense_qkv_weight_grad.layout(), dense_qkv_weight_grad.lod(), dense_qkv_weight_grad.offset());
  argument_outputs.push_back(qkv_weight_grad_dense_tensor_type);

  pir::Type out_linear_weight_grad_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_out_linear_weight_grad.dtype()), dense_out_linear_weight_grad.dims(), dense_out_linear_weight_grad.layout(), dense_out_linear_weight_grad.lod(), dense_out_linear_weight_grad.offset());
  argument_outputs.push_back(out_linear_weight_grad_dense_tensor_type);

  pir::Type ln_out_grad_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_ln_out_grad.dtype()), dense_ln_out_grad.dims(), dense_ln_out_grad.layout(), dense_ln_out_grad.lod(), dense_ln_out_grad.offset());
  argument_outputs.push_back(ln_out_grad_dense_tensor_type);

  pir::Type bias_dropout_residual_out_grad_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_bias_dropout_residual_out_grad.dtype()), dense_bias_dropout_residual_out_grad.dims(), dense_bias_dropout_residual_out_grad.layout(), dense_bias_dropout_residual_out_grad.lod(), dense_bias_dropout_residual_out_grad.offset());
  argument_outputs.push_back(bias_dropout_residual_out_grad_dense_tensor_type);

  pir::Type qkv_out_grad_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_qkv_out_grad.dtype()), dense_qkv_out_grad.dims(), dense_qkv_out_grad.layout(), dense_qkv_out_grad.lod(), dense_qkv_out_grad.offset());
  argument_outputs.push_back(qkv_out_grad_dense_tensor_type);

  pir::Type qktv_out_grad_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_qktv_out_grad.dtype()), dense_qktv_out_grad.dims(), dense_qktv_out_grad.layout(), dense_qktv_out_grad.lod(), dense_qktv_out_grad.offset());
  argument_outputs.push_back(qktv_out_grad_dense_tensor_type);

  pir::Type transpose_out_2_grad_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_transpose_out_2_grad.dtype()), dense_transpose_out_2_grad.dims(), dense_transpose_out_2_grad.layout(), dense_transpose_out_2_grad.lod(), dense_transpose_out_2_grad.offset());
  argument_outputs.push_back(transpose_out_2_grad_dense_tensor_type);

  pir::Type qk_out_grad_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_qk_out_grad.dtype()), dense_qk_out_grad.dims(), dense_qk_out_grad.layout(), dense_qk_out_grad.lod(), dense_qk_out_grad.offset());
  argument_outputs.push_back(qk_out_grad_dense_tensor_type);

  pir::Type softmax_out_grad_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_softmax_out_grad.dtype()), dense_softmax_out_grad.dims(), dense_softmax_out_grad.layout(), dense_softmax_out_grad.lod(), dense_softmax_out_grad.offset());
  argument_outputs.push_back(softmax_out_grad_dense_tensor_type);

  pir::Type attn_dropout_out_grad_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_attn_dropout_out_grad.dtype()), dense_attn_dropout_out_grad.dims(), dense_attn_dropout_out_grad.layout(), dense_attn_dropout_out_grad.lod(), dense_attn_dropout_out_grad.offset());
  argument_outputs.push_back(attn_dropout_out_grad_dense_tensor_type);

  pir::Type fmha_out_grad_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_fmha_out_grad.dtype()), dense_fmha_out_grad.dims(), dense_fmha_out_grad.layout(), dense_fmha_out_grad.lod(), dense_fmha_out_grad.offset());
  argument_outputs.push_back(fmha_out_grad_dense_tensor_type);

  pir::Type out_linear_out_grad_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_out_linear_out_grad.dtype()), dense_out_linear_out_grad.dims(), dense_out_linear_out_grad.layout(), dense_out_linear_out_grad.lod(), dense_out_linear_out_grad.offset());
  argument_outputs.push_back(out_linear_out_grad_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void FusedAttentionGradOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value out_grad_, pir::Value x_, pir::Value qkv_weight_, pir::Value qkv_bias_, pir::Value qkv_bias_out_, pir::Value src_mask_, pir::Value src_mask_out_, pir::Value out_linear_weight_, pir::Value out_linear_bias_, pir::Value ln_scale_, pir::Value ln_bias_, pir::Value ln_scale_2_, pir::Value ln_bias_2_, pir::Value ln_out_, pir::Value ln_mean_, pir::Value ln_var_, pir::Value ln_mean_2_, pir::Value ln_var_2_, pir::Value bias_dropout_residual_out_, pir::Value qkv_out_, pir::Value transpose_out_2_, pir::Value qk_out_, pir::Value qktv_out_, pir::Value softmax_out_, pir::Value attn_dropout_mask_out_, pir::Value attn_dropout_out_, pir::Value fmha_out_, pir::Value out_linear_out_, pir::Value dropout_mask_out_, pir::AttributeMap attributes) {
  VLOG(4) << "Start build FusedAttentionGradOp";


  IR_ENFORCE(
      attributes.find("num_heads") != attributes.end(),
          "'num_heads' Attribute is expected for FusedAttentionGradOp. ");
  int num_heads = attributes.at("num_heads").dyn_cast<pir::Int32Attribute>().data();

  IR_ENFORCE(
      attributes.find("transpose_qkv_wb") != attributes.end(),
          "'transpose_qkv_wb' Attribute is expected for FusedAttentionGradOp. ");
  bool transpose_qkv_wb = attributes.at("transpose_qkv_wb").dyn_cast<pir::BoolAttribute>().data();

  IR_ENFORCE(
      attributes.find("pre_layer_norm") != attributes.end(),
          "'pre_layer_norm' Attribute is expected for FusedAttentionGradOp. ");
  bool pre_layer_norm = attributes.at("pre_layer_norm").dyn_cast<pir::BoolAttribute>().data();

  IR_ENFORCE(
      attributes.find("epsilon") != attributes.end(),
          "'epsilon' Attribute is expected for FusedAttentionGradOp. ");
  float epsilon = attributes.at("epsilon").dyn_cast<pir::FloatAttribute>().data();

  IR_ENFORCE(
      attributes.find("attn_dropout_rate") != attributes.end(),
          "'attn_dropout_rate' Attribute is expected for FusedAttentionGradOp. ");
  float attn_dropout_rate = attributes.at("attn_dropout_rate").dyn_cast<pir::FloatAttribute>().data();

  IR_ENFORCE(
      attributes.find("is_test") != attributes.end(),
          "'is_test' Attribute is expected for FusedAttentionGradOp. ");
  bool is_test = attributes.at("is_test").dyn_cast<pir::BoolAttribute>().data();

  IR_ENFORCE(
      attributes.find("attn_dropout_fix_seed") != attributes.end(),
          "'attn_dropout_fix_seed' Attribute is expected for FusedAttentionGradOp. ");
  bool attn_dropout_fix_seed = attributes.at("attn_dropout_fix_seed").dyn_cast<pir::BoolAttribute>().data();

  IR_ENFORCE(
      attributes.find("attn_dropout_seed") != attributes.end(),
          "'attn_dropout_seed' Attribute is expected for FusedAttentionGradOp. ");
  int attn_dropout_seed = attributes.at("attn_dropout_seed").dyn_cast<pir::Int32Attribute>().data();

  IR_ENFORCE(
      attributes.find("attn_dropout_implementation") != attributes.end(),
          "'attn_dropout_implementation' Attribute is expected for FusedAttentionGradOp. ");
  std::string attn_dropout_implementation = attributes.at("attn_dropout_implementation").dyn_cast<pir::StrAttribute>().AsString();

  IR_ENFORCE(
      attributes.find("dropout_rate") != attributes.end(),
          "'dropout_rate' Attribute is expected for FusedAttentionGradOp. ");
  float dropout_rate = attributes.at("dropout_rate").dyn_cast<pir::FloatAttribute>().data();

  IR_ENFORCE(
      attributes.find("dropout_fix_seed") != attributes.end(),
          "'dropout_fix_seed' Attribute is expected for FusedAttentionGradOp. ");
  bool dropout_fix_seed = attributes.at("dropout_fix_seed").dyn_cast<pir::BoolAttribute>().data();

  IR_ENFORCE(
      attributes.find("dropout_seed") != attributes.end(),
          "'dropout_seed' Attribute is expected for FusedAttentionGradOp. ");
  int dropout_seed = attributes.at("dropout_seed").dyn_cast<pir::Int32Attribute>().data();

  IR_ENFORCE(
      attributes.find("dropout_implementation") != attributes.end(),
          "'dropout_implementation' Attribute is expected for FusedAttentionGradOp. ");
  std::string dropout_implementation = attributes.at("dropout_implementation").dyn_cast<pir::StrAttribute>().AsString();

  IR_ENFORCE(
      attributes.find("ln_epsilon") != attributes.end(),
          "'ln_epsilon' Attribute is expected for FusedAttentionGradOp. ");
  float ln_epsilon = attributes.at("ln_epsilon").dyn_cast<pir::FloatAttribute>().data();

  IR_ENFORCE(
      attributes.find("add_residual") != attributes.end(),
          "'add_residual' Attribute is expected for FusedAttentionGradOp. ");
  bool add_residual = attributes.at("add_residual").dyn_cast<pir::BoolAttribute>().data();

  IR_ENFORCE(
      attributes.find("ring_id") != attributes.end(),
          "'ring_id' Attribute is expected for FusedAttentionGradOp. ");
  int ring_id = attributes.at("ring_id").dyn_cast<pir::Int32Attribute>().data();


  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {out_grad_, x_, qkv_weight_, qkv_bias_, qkv_bias_out_, src_mask_, src_mask_out_, out_linear_weight_, out_linear_bias_, ln_scale_, ln_bias_, ln_scale_2_, ln_bias_2_, ln_out_, ln_mean_, ln_var_, ln_mean_2_, ln_var_2_, bias_dropout_residual_out_, qkv_out_, transpose_out_2_, qk_out_, qktv_out_, softmax_out_, attn_dropout_mask_out_, attn_dropout_out_, fmha_out_, out_linear_out_, dropout_mask_out_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_num_heads = pir::Int32Attribute::get(pir::IrContext::Instance(), num_heads);
  argument.AddAttribute("num_heads", attr_num_heads);
  pir::Attribute attr_transpose_qkv_wb = pir::BoolAttribute::get(pir::IrContext::Instance(), transpose_qkv_wb);
  argument.AddAttribute("transpose_qkv_wb", attr_transpose_qkv_wb);
  pir::Attribute attr_pre_layer_norm = pir::BoolAttribute::get(pir::IrContext::Instance(), pre_layer_norm);
  argument.AddAttribute("pre_layer_norm", attr_pre_layer_norm);
  pir::Attribute attr_epsilon = pir::FloatAttribute::get(pir::IrContext::Instance(), epsilon);
  argument.AddAttribute("epsilon", attr_epsilon);
  pir::Attribute attr_attn_dropout_rate = pir::FloatAttribute::get(pir::IrContext::Instance(), attn_dropout_rate);
  argument.AddAttribute("attn_dropout_rate", attr_attn_dropout_rate);
  pir::Attribute attr_is_test = pir::BoolAttribute::get(pir::IrContext::Instance(), is_test);
  argument.AddAttribute("is_test", attr_is_test);
  pir::Attribute attr_attn_dropout_fix_seed = pir::BoolAttribute::get(pir::IrContext::Instance(), attn_dropout_fix_seed);
  argument.AddAttribute("attn_dropout_fix_seed", attr_attn_dropout_fix_seed);
  pir::Attribute attr_attn_dropout_seed = pir::Int32Attribute::get(pir::IrContext::Instance(), attn_dropout_seed);
  argument.AddAttribute("attn_dropout_seed", attr_attn_dropout_seed);
  pir::Attribute attr_attn_dropout_implementation = pir::StrAttribute::get(pir::IrContext::Instance(), attn_dropout_implementation);
  argument.AddAttribute("attn_dropout_implementation", attr_attn_dropout_implementation);
  pir::Attribute attr_dropout_rate = pir::FloatAttribute::get(pir::IrContext::Instance(), dropout_rate);
  argument.AddAttribute("dropout_rate", attr_dropout_rate);
  pir::Attribute attr_dropout_fix_seed = pir::BoolAttribute::get(pir::IrContext::Instance(), dropout_fix_seed);
  argument.AddAttribute("dropout_fix_seed", attr_dropout_fix_seed);
  pir::Attribute attr_dropout_seed = pir::Int32Attribute::get(pir::IrContext::Instance(), dropout_seed);
  argument.AddAttribute("dropout_seed", attr_dropout_seed);
  pir::Attribute attr_dropout_implementation = pir::StrAttribute::get(pir::IrContext::Instance(), dropout_implementation);
  argument.AddAttribute("dropout_implementation", attr_dropout_implementation);
  pir::Attribute attr_ln_epsilon = pir::FloatAttribute::get(pir::IrContext::Instance(), ln_epsilon);
  argument.AddAttribute("ln_epsilon", attr_ln_epsilon);
  pir::Attribute attr_add_residual = pir::BoolAttribute::get(pir::IrContext::Instance(), add_residual);
  argument.AddAttribute("add_residual", attr_add_residual);
  pir::Attribute attr_ring_id = pir::Int32Attribute::get(pir::IrContext::Instance(), ring_id);
  argument.AddAttribute("ring_id", attr_ring_id);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType out_grad = out_grad_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)out_grad;
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;
  paddle::dialect::DenseTensorType qkv_weight = qkv_weight_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)qkv_weight;
  paddle::dialect::DenseTensorType out_linear_weight = out_linear_weight_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)out_linear_weight;
  paddle::dialect::DenseTensorType qkv_out = qkv_out_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)qkv_out;
  paddle::dialect::DenseTensorType transpose_out_2 = transpose_out_2_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)transpose_out_2;
  paddle::dialect::DenseTensorType qk_out = qk_out_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)qk_out;
  paddle::dialect::DenseTensorType qktv_out = qktv_out_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)qktv_out;
  paddle::dialect::DenseTensorType softmax_out = softmax_out_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)softmax_out;
  paddle::dialect::DenseTensorType attn_dropout_mask_out = attn_dropout_mask_out_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)attn_dropout_mask_out;
  paddle::dialect::DenseTensorType attn_dropout_out = attn_dropout_out_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)attn_dropout_out;
  paddle::dialect::DenseTensorType fmha_out = fmha_out_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)fmha_out;
  paddle::dialect::DenseTensorType out_linear_out = out_linear_out_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)out_linear_out;
  paddle::dialect::DenseTensorType dropout_mask_out = dropout_mask_out_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)dropout_mask_out;

  VLOG(4) << "Builder construction  dense_out_grad";
  paddle::dialect::IrTensor ir_tensor_out_grad(paddle::dialect::TransToPhiDataType(out_grad.dtype()),
                                                      out_grad.dims(),
                                                      out_grad.data_layout(),
                                                      out_grad.lod(),
                                                      out_grad.offset());
  VLOG(4) << "Builder construction  meta_out_grad";
  paddle::dialect::IrMetaTensor meta_out_grad(&ir_tensor_out_grad);

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);

  VLOG(4) << "Builder construction  dense_qkv_weight";
  paddle::dialect::IrTensor ir_tensor_qkv_weight(paddle::dialect::TransToPhiDataType(qkv_weight.dtype()),
                                                      qkv_weight.dims(),
                                                      qkv_weight.data_layout(),
                                                      qkv_weight.lod(),
                                                      qkv_weight.offset());
  VLOG(4) << "Builder construction  meta_qkv_weight";
  paddle::dialect::IrMetaTensor meta_qkv_weight(&ir_tensor_qkv_weight);

  paddle::dialect::IrMetaTensor meta_qkv_bias;
  paddle::dialect::IrTensor ir_tensor_qkv_bias;
  if (qkv_bias_.impl() != nullptr) {
    paddle::dialect::DenseTensorType qkv_bias = qkv_bias_.type().dyn_cast<paddle::dialect::DenseTensorType>();
    VLOG(4) << "Builder construction  dense_qkv_bias";
    ir_tensor_qkv_bias = paddle::dialect::IrTensor(paddle::dialect::TransToPhiDataType(qkv_bias.dtype()),
                                                        qkv_bias.dims(),
                                                        qkv_bias.data_layout(),
                                                        qkv_bias.lod(),
                                                        qkv_bias.offset());
    VLOG(4) << "Builder construction  meta_qkv_bias";
    meta_qkv_bias = paddle::dialect::IrMetaTensor(&ir_tensor_qkv_bias);
  }


  paddle::dialect::IrMetaTensor meta_qkv_bias_out;
  paddle::dialect::IrTensor ir_tensor_qkv_bias_out;
  if (qkv_bias_out_.impl() != nullptr) {
    paddle::dialect::DenseTensorType qkv_bias_out = qkv_bias_out_.type().dyn_cast<paddle::dialect::DenseTensorType>();
    VLOG(4) << "Builder construction  dense_qkv_bias_out";
    ir_tensor_qkv_bias_out = paddle::dialect::IrTensor(paddle::dialect::TransToPhiDataType(qkv_bias_out.dtype()),
                                                        qkv_bias_out.dims(),
                                                        qkv_bias_out.data_layout(),
                                                        qkv_bias_out.lod(),
                                                        qkv_bias_out.offset());
    VLOG(4) << "Builder construction  meta_qkv_bias_out";
    meta_qkv_bias_out = paddle::dialect::IrMetaTensor(&ir_tensor_qkv_bias_out);
  }


  paddle::dialect::IrMetaTensor meta_src_mask;
  paddle::dialect::IrTensor ir_tensor_src_mask;
  if (src_mask_.impl() != nullptr) {
    paddle::dialect::DenseTensorType src_mask = src_mask_.type().dyn_cast<paddle::dialect::DenseTensorType>();
    VLOG(4) << "Builder construction  dense_src_mask";
    ir_tensor_src_mask = paddle::dialect::IrTensor(paddle::dialect::TransToPhiDataType(src_mask.dtype()),
                                                        src_mask.dims(),
                                                        src_mask.data_layout(),
                                                        src_mask.lod(),
                                                        src_mask.offset());
    VLOG(4) << "Builder construction  meta_src_mask";
    meta_src_mask = paddle::dialect::IrMetaTensor(&ir_tensor_src_mask);
  }


  paddle::dialect::IrMetaTensor meta_src_mask_out;
  paddle::dialect::IrTensor ir_tensor_src_mask_out;
  if (src_mask_out_.impl() != nullptr) {
    paddle::dialect::DenseTensorType src_mask_out = src_mask_out_.type().dyn_cast<paddle::dialect::DenseTensorType>();
    VLOG(4) << "Builder construction  dense_src_mask_out";
    ir_tensor_src_mask_out = paddle::dialect::IrTensor(paddle::dialect::TransToPhiDataType(src_mask_out.dtype()),
                                                        src_mask_out.dims(),
                                                        src_mask_out.data_layout(),
                                                        src_mask_out.lod(),
                                                        src_mask_out.offset());
    VLOG(4) << "Builder construction  meta_src_mask_out";
    meta_src_mask_out = paddle::dialect::IrMetaTensor(&ir_tensor_src_mask_out);
  }


  VLOG(4) << "Builder construction  dense_out_linear_weight";
  paddle::dialect::IrTensor ir_tensor_out_linear_weight(paddle::dialect::TransToPhiDataType(out_linear_weight.dtype()),
                                                      out_linear_weight.dims(),
                                                      out_linear_weight.data_layout(),
                                                      out_linear_weight.lod(),
                                                      out_linear_weight.offset());
  VLOG(4) << "Builder construction  meta_out_linear_weight";
  paddle::dialect::IrMetaTensor meta_out_linear_weight(&ir_tensor_out_linear_weight);

  paddle::dialect::IrMetaTensor meta_out_linear_bias;
  paddle::dialect::IrTensor ir_tensor_out_linear_bias;
  if (out_linear_bias_.impl() != nullptr) {
    paddle::dialect::DenseTensorType out_linear_bias = out_linear_bias_.type().dyn_cast<paddle::dialect::DenseTensorType>();
    VLOG(4) << "Builder construction  dense_out_linear_bias";
    ir_tensor_out_linear_bias = paddle::dialect::IrTensor(paddle::dialect::TransToPhiDataType(out_linear_bias.dtype()),
                                                        out_linear_bias.dims(),
                                                        out_linear_bias.data_layout(),
                                                        out_linear_bias.lod(),
                                                        out_linear_bias.offset());
    VLOG(4) << "Builder construction  meta_out_linear_bias";
    meta_out_linear_bias = paddle::dialect::IrMetaTensor(&ir_tensor_out_linear_bias);
  }


  paddle::dialect::IrMetaTensor meta_ln_scale;
  paddle::dialect::IrTensor ir_tensor_ln_scale;
  if (ln_scale_.impl() != nullptr) {
    paddle::dialect::DenseTensorType ln_scale = ln_scale_.type().dyn_cast<paddle::dialect::DenseTensorType>();
    VLOG(4) << "Builder construction  dense_ln_scale";
    ir_tensor_ln_scale = paddle::dialect::IrTensor(paddle::dialect::TransToPhiDataType(ln_scale.dtype()),
                                                        ln_scale.dims(),
                                                        ln_scale.data_layout(),
                                                        ln_scale.lod(),
                                                        ln_scale.offset());
    VLOG(4) << "Builder construction  meta_ln_scale";
    meta_ln_scale = paddle::dialect::IrMetaTensor(&ir_tensor_ln_scale);
  }


  paddle::dialect::IrMetaTensor meta_ln_bias;
  paddle::dialect::IrTensor ir_tensor_ln_bias;
  if (ln_bias_.impl() != nullptr) {
    paddle::dialect::DenseTensorType ln_bias = ln_bias_.type().dyn_cast<paddle::dialect::DenseTensorType>();
    VLOG(4) << "Builder construction  dense_ln_bias";
    ir_tensor_ln_bias = paddle::dialect::IrTensor(paddle::dialect::TransToPhiDataType(ln_bias.dtype()),
                                                        ln_bias.dims(),
                                                        ln_bias.data_layout(),
                                                        ln_bias.lod(),
                                                        ln_bias.offset());
    VLOG(4) << "Builder construction  meta_ln_bias";
    meta_ln_bias = paddle::dialect::IrMetaTensor(&ir_tensor_ln_bias);
  }


  paddle::dialect::IrMetaTensor meta_ln_scale_2;
  paddle::dialect::IrTensor ir_tensor_ln_scale_2;
  if (ln_scale_2_.impl() != nullptr) {
    paddle::dialect::DenseTensorType ln_scale_2 = ln_scale_2_.type().dyn_cast<paddle::dialect::DenseTensorType>();
    VLOG(4) << "Builder construction  dense_ln_scale_2";
    ir_tensor_ln_scale_2 = paddle::dialect::IrTensor(paddle::dialect::TransToPhiDataType(ln_scale_2.dtype()),
                                                        ln_scale_2.dims(),
                                                        ln_scale_2.data_layout(),
                                                        ln_scale_2.lod(),
                                                        ln_scale_2.offset());
    VLOG(4) << "Builder construction  meta_ln_scale_2";
    meta_ln_scale_2 = paddle::dialect::IrMetaTensor(&ir_tensor_ln_scale_2);
  }


  paddle::dialect::IrMetaTensor meta_ln_bias_2;
  paddle::dialect::IrTensor ir_tensor_ln_bias_2;
  if (ln_bias_2_.impl() != nullptr) {
    paddle::dialect::DenseTensorType ln_bias_2 = ln_bias_2_.type().dyn_cast<paddle::dialect::DenseTensorType>();
    VLOG(4) << "Builder construction  dense_ln_bias_2";
    ir_tensor_ln_bias_2 = paddle::dialect::IrTensor(paddle::dialect::TransToPhiDataType(ln_bias_2.dtype()),
                                                        ln_bias_2.dims(),
                                                        ln_bias_2.data_layout(),
                                                        ln_bias_2.lod(),
                                                        ln_bias_2.offset());
    VLOG(4) << "Builder construction  meta_ln_bias_2";
    meta_ln_bias_2 = paddle::dialect::IrMetaTensor(&ir_tensor_ln_bias_2);
  }


  paddle::dialect::IrMetaTensor meta_ln_out;
  paddle::dialect::IrTensor ir_tensor_ln_out;
  if (ln_out_.impl() != nullptr) {
    paddle::dialect::DenseTensorType ln_out = ln_out_.type().dyn_cast<paddle::dialect::DenseTensorType>();
    VLOG(4) << "Builder construction  dense_ln_out";
    ir_tensor_ln_out = paddle::dialect::IrTensor(paddle::dialect::TransToPhiDataType(ln_out.dtype()),
                                                        ln_out.dims(),
                                                        ln_out.data_layout(),
                                                        ln_out.lod(),
                                                        ln_out.offset());
    VLOG(4) << "Builder construction  meta_ln_out";
    meta_ln_out = paddle::dialect::IrMetaTensor(&ir_tensor_ln_out);
  }


  paddle::dialect::IrMetaTensor meta_ln_mean;
  paddle::dialect::IrTensor ir_tensor_ln_mean;
  if (ln_mean_.impl() != nullptr) {
    paddle::dialect::DenseTensorType ln_mean = ln_mean_.type().dyn_cast<paddle::dialect::DenseTensorType>();
    VLOG(4) << "Builder construction  dense_ln_mean";
    ir_tensor_ln_mean = paddle::dialect::IrTensor(paddle::dialect::TransToPhiDataType(ln_mean.dtype()),
                                                        ln_mean.dims(),
                                                        ln_mean.data_layout(),
                                                        ln_mean.lod(),
                                                        ln_mean.offset());
    VLOG(4) << "Builder construction  meta_ln_mean";
    meta_ln_mean = paddle::dialect::IrMetaTensor(&ir_tensor_ln_mean);
  }


  paddle::dialect::IrMetaTensor meta_ln_var;
  paddle::dialect::IrTensor ir_tensor_ln_var;
  if (ln_var_.impl() != nullptr) {
    paddle::dialect::DenseTensorType ln_var = ln_var_.type().dyn_cast<paddle::dialect::DenseTensorType>();
    VLOG(4) << "Builder construction  dense_ln_var";
    ir_tensor_ln_var = paddle::dialect::IrTensor(paddle::dialect::TransToPhiDataType(ln_var.dtype()),
                                                        ln_var.dims(),
                                                        ln_var.data_layout(),
                                                        ln_var.lod(),
                                                        ln_var.offset());
    VLOG(4) << "Builder construction  meta_ln_var";
    meta_ln_var = paddle::dialect::IrMetaTensor(&ir_tensor_ln_var);
  }


  paddle::dialect::IrMetaTensor meta_ln_mean_2;
  paddle::dialect::IrTensor ir_tensor_ln_mean_2;
  if (ln_mean_2_.impl() != nullptr) {
    paddle::dialect::DenseTensorType ln_mean_2 = ln_mean_2_.type().dyn_cast<paddle::dialect::DenseTensorType>();
    VLOG(4) << "Builder construction  dense_ln_mean_2";
    ir_tensor_ln_mean_2 = paddle::dialect::IrTensor(paddle::dialect::TransToPhiDataType(ln_mean_2.dtype()),
                                                        ln_mean_2.dims(),
                                                        ln_mean_2.data_layout(),
                                                        ln_mean_2.lod(),
                                                        ln_mean_2.offset());
    VLOG(4) << "Builder construction  meta_ln_mean_2";
    meta_ln_mean_2 = paddle::dialect::IrMetaTensor(&ir_tensor_ln_mean_2);
  }


  paddle::dialect::IrMetaTensor meta_ln_var_2;
  paddle::dialect::IrTensor ir_tensor_ln_var_2;
  if (ln_var_2_.impl() != nullptr) {
    paddle::dialect::DenseTensorType ln_var_2 = ln_var_2_.type().dyn_cast<paddle::dialect::DenseTensorType>();
    VLOG(4) << "Builder construction  dense_ln_var_2";
    ir_tensor_ln_var_2 = paddle::dialect::IrTensor(paddle::dialect::TransToPhiDataType(ln_var_2.dtype()),
                                                        ln_var_2.dims(),
                                                        ln_var_2.data_layout(),
                                                        ln_var_2.lod(),
                                                        ln_var_2.offset());
    VLOG(4) << "Builder construction  meta_ln_var_2";
    meta_ln_var_2 = paddle::dialect::IrMetaTensor(&ir_tensor_ln_var_2);
  }


  paddle::dialect::IrMetaTensor meta_bias_dropout_residual_out;
  paddle::dialect::IrTensor ir_tensor_bias_dropout_residual_out;
  if (bias_dropout_residual_out_.impl() != nullptr) {
    paddle::dialect::DenseTensorType bias_dropout_residual_out = bias_dropout_residual_out_.type().dyn_cast<paddle::dialect::DenseTensorType>();
    VLOG(4) << "Builder construction  dense_bias_dropout_residual_out";
    ir_tensor_bias_dropout_residual_out = paddle::dialect::IrTensor(paddle::dialect::TransToPhiDataType(bias_dropout_residual_out.dtype()),
                                                        bias_dropout_residual_out.dims(),
                                                        bias_dropout_residual_out.data_layout(),
                                                        bias_dropout_residual_out.lod(),
                                                        bias_dropout_residual_out.offset());
    VLOG(4) << "Builder construction  meta_bias_dropout_residual_out";
    meta_bias_dropout_residual_out = paddle::dialect::IrMetaTensor(&ir_tensor_bias_dropout_residual_out);
  }


  VLOG(4) << "Builder construction  dense_qkv_out";
  paddle::dialect::IrTensor ir_tensor_qkv_out(paddle::dialect::TransToPhiDataType(qkv_out.dtype()),
                                                      qkv_out.dims(),
                                                      qkv_out.data_layout(),
                                                      qkv_out.lod(),
                                                      qkv_out.offset());
  VLOG(4) << "Builder construction  meta_qkv_out";
  paddle::dialect::IrMetaTensor meta_qkv_out(&ir_tensor_qkv_out);

  VLOG(4) << "Builder construction  dense_transpose_out_2";
  paddle::dialect::IrTensor ir_tensor_transpose_out_2(paddle::dialect::TransToPhiDataType(transpose_out_2.dtype()),
                                                      transpose_out_2.dims(),
                                                      transpose_out_2.data_layout(),
                                                      transpose_out_2.lod(),
                                                      transpose_out_2.offset());
  VLOG(4) << "Builder construction  meta_transpose_out_2";
  paddle::dialect::IrMetaTensor meta_transpose_out_2(&ir_tensor_transpose_out_2);

  VLOG(4) << "Builder construction  dense_qk_out";
  paddle::dialect::IrTensor ir_tensor_qk_out(paddle::dialect::TransToPhiDataType(qk_out.dtype()),
                                                      qk_out.dims(),
                                                      qk_out.data_layout(),
                                                      qk_out.lod(),
                                                      qk_out.offset());
  VLOG(4) << "Builder construction  meta_qk_out";
  paddle::dialect::IrMetaTensor meta_qk_out(&ir_tensor_qk_out);

  VLOG(4) << "Builder construction  dense_qktv_out";
  paddle::dialect::IrTensor ir_tensor_qktv_out(paddle::dialect::TransToPhiDataType(qktv_out.dtype()),
                                                      qktv_out.dims(),
                                                      qktv_out.data_layout(),
                                                      qktv_out.lod(),
                                                      qktv_out.offset());
  VLOG(4) << "Builder construction  meta_qktv_out";
  paddle::dialect::IrMetaTensor meta_qktv_out(&ir_tensor_qktv_out);

  VLOG(4) << "Builder construction  dense_softmax_out";
  paddle::dialect::IrTensor ir_tensor_softmax_out(paddle::dialect::TransToPhiDataType(softmax_out.dtype()),
                                                      softmax_out.dims(),
                                                      softmax_out.data_layout(),
                                                      softmax_out.lod(),
                                                      softmax_out.offset());
  VLOG(4) << "Builder construction  meta_softmax_out";
  paddle::dialect::IrMetaTensor meta_softmax_out(&ir_tensor_softmax_out);

  VLOG(4) << "Builder construction  dense_attn_dropout_mask_out";
  paddle::dialect::IrTensor ir_tensor_attn_dropout_mask_out(paddle::dialect::TransToPhiDataType(attn_dropout_mask_out.dtype()),
                                                      attn_dropout_mask_out.dims(),
                                                      attn_dropout_mask_out.data_layout(),
                                                      attn_dropout_mask_out.lod(),
                                                      attn_dropout_mask_out.offset());
  VLOG(4) << "Builder construction  meta_attn_dropout_mask_out";
  paddle::dialect::IrMetaTensor meta_attn_dropout_mask_out(&ir_tensor_attn_dropout_mask_out);

  VLOG(4) << "Builder construction  dense_attn_dropout_out";
  paddle::dialect::IrTensor ir_tensor_attn_dropout_out(paddle::dialect::TransToPhiDataType(attn_dropout_out.dtype()),
                                                      attn_dropout_out.dims(),
                                                      attn_dropout_out.data_layout(),
                                                      attn_dropout_out.lod(),
                                                      attn_dropout_out.offset());
  VLOG(4) << "Builder construction  meta_attn_dropout_out";
  paddle::dialect::IrMetaTensor meta_attn_dropout_out(&ir_tensor_attn_dropout_out);

  VLOG(4) << "Builder construction  dense_fmha_out";
  paddle::dialect::IrTensor ir_tensor_fmha_out(paddle::dialect::TransToPhiDataType(fmha_out.dtype()),
                                                      fmha_out.dims(),
                                                      fmha_out.data_layout(),
                                                      fmha_out.lod(),
                                                      fmha_out.offset());
  VLOG(4) << "Builder construction  meta_fmha_out";
  paddle::dialect::IrMetaTensor meta_fmha_out(&ir_tensor_fmha_out);

  VLOG(4) << "Builder construction  dense_out_linear_out";
  paddle::dialect::IrTensor ir_tensor_out_linear_out(paddle::dialect::TransToPhiDataType(out_linear_out.dtype()),
                                                      out_linear_out.dims(),
                                                      out_linear_out.data_layout(),
                                                      out_linear_out.lod(),
                                                      out_linear_out.offset());
  VLOG(4) << "Builder construction  meta_out_linear_out";
  paddle::dialect::IrMetaTensor meta_out_linear_out(&ir_tensor_out_linear_out);

  VLOG(4) << "Builder construction  dense_dropout_mask_out";
  paddle::dialect::IrTensor ir_tensor_dropout_mask_out(paddle::dialect::TransToPhiDataType(dropout_mask_out.dtype()),
                                                      dropout_mask_out.dims(),
                                                      dropout_mask_out.data_layout(),
                                                      dropout_mask_out.lod(),
                                                      dropout_mask_out.offset());
  VLOG(4) << "Builder construction  meta_dropout_mask_out";
  paddle::dialect::IrMetaTensor meta_dropout_mask_out(&ir_tensor_dropout_mask_out);
  paddle::dialect::IrTensor dense_qkv_bias_grad;
  paddle::dialect::IrMetaTensor meta_qkv_bias_grad(&dense_qkv_bias_grad);
  paddle::dialect::IrTensor dense_qkv_bias_out_grad;
  paddle::dialect::IrMetaTensor meta_qkv_bias_out_grad(&dense_qkv_bias_out_grad);
  paddle::dialect::IrTensor dense_src_mask_out_grad;
  paddle::dialect::IrMetaTensor meta_src_mask_out_grad(&dense_src_mask_out_grad);
  paddle::dialect::IrTensor dense_out_linear_bias_grad;
  paddle::dialect::IrMetaTensor meta_out_linear_bias_grad(&dense_out_linear_bias_grad);
  paddle::dialect::IrTensor dense_ln_scale_grad;
  paddle::dialect::IrMetaTensor meta_ln_scale_grad(&dense_ln_scale_grad);
  paddle::dialect::IrTensor dense_ln_bias_grad;
  paddle::dialect::IrMetaTensor meta_ln_bias_grad(&dense_ln_bias_grad);
  paddle::dialect::IrTensor dense_ln_scale_2_grad;
  paddle::dialect::IrMetaTensor meta_ln_scale_2_grad(&dense_ln_scale_2_grad);
  paddle::dialect::IrTensor dense_ln_bias_2_grad;
  paddle::dialect::IrMetaTensor meta_ln_bias_2_grad(&dense_ln_bias_2_grad);
  paddle::dialect::IrTensor dense_x_grad;
  paddle::dialect::IrMetaTensor meta_x_grad(&dense_x_grad);
  paddle::dialect::IrTensor dense_qkv_weight_grad;
  paddle::dialect::IrMetaTensor meta_qkv_weight_grad(&dense_qkv_weight_grad);
  paddle::dialect::IrTensor dense_out_linear_weight_grad;
  paddle::dialect::IrMetaTensor meta_out_linear_weight_grad(&dense_out_linear_weight_grad);
  paddle::dialect::IrTensor dense_ln_out_grad;
  paddle::dialect::IrMetaTensor meta_ln_out_grad(&dense_ln_out_grad);
  paddle::dialect::IrTensor dense_bias_dropout_residual_out_grad;
  paddle::dialect::IrMetaTensor meta_bias_dropout_residual_out_grad(&dense_bias_dropout_residual_out_grad);
  paddle::dialect::IrTensor dense_qkv_out_grad;
  paddle::dialect::IrMetaTensor meta_qkv_out_grad(&dense_qkv_out_grad);
  paddle::dialect::IrTensor dense_qktv_out_grad;
  paddle::dialect::IrMetaTensor meta_qktv_out_grad(&dense_qktv_out_grad);
  paddle::dialect::IrTensor dense_transpose_out_2_grad;
  paddle::dialect::IrMetaTensor meta_transpose_out_2_grad(&dense_transpose_out_2_grad);
  paddle::dialect::IrTensor dense_qk_out_grad;
  paddle::dialect::IrMetaTensor meta_qk_out_grad(&dense_qk_out_grad);
  paddle::dialect::IrTensor dense_softmax_out_grad;
  paddle::dialect::IrMetaTensor meta_softmax_out_grad(&dense_softmax_out_grad);
  paddle::dialect::IrTensor dense_attn_dropout_out_grad;
  paddle::dialect::IrMetaTensor meta_attn_dropout_out_grad(&dense_attn_dropout_out_grad);
  paddle::dialect::IrTensor dense_fmha_out_grad;
  paddle::dialect::IrMetaTensor meta_fmha_out_grad(&dense_fmha_out_grad);
  paddle::dialect::IrTensor dense_out_linear_out_grad;
  paddle::dialect::IrMetaTensor meta_out_linear_out_grad(&dense_out_linear_out_grad);

  phi::FusedAttentionGradInferMeta(meta_out_grad, meta_x, meta_qkv_weight, meta_qkv_bias, meta_qkv_bias_out, meta_src_mask, meta_src_mask_out, meta_out_linear_weight, meta_out_linear_bias, meta_ln_scale, meta_ln_bias, meta_ln_scale_2, meta_ln_bias_2, meta_ln_out, meta_ln_mean, meta_ln_var, meta_ln_mean_2, meta_ln_var_2, meta_bias_dropout_residual_out, meta_qkv_out, meta_transpose_out_2, meta_qk_out, meta_qktv_out, meta_softmax_out, meta_attn_dropout_mask_out, meta_attn_dropout_out, meta_fmha_out, meta_out_linear_out, meta_dropout_mask_out, num_heads, transpose_qkv_wb, pre_layer_norm, epsilon, attn_dropout_rate, is_test, attn_dropout_fix_seed, attn_dropout_seed, attn_dropout_implementation, dropout_rate, dropout_fix_seed, dropout_seed, dropout_implementation, ln_epsilon, add_residual, ring_id, &meta_qkv_bias_grad, &meta_qkv_bias_out_grad, &meta_src_mask_out_grad, &meta_out_linear_bias_grad, &meta_ln_scale_grad, &meta_ln_bias_grad, &meta_ln_scale_2_grad, &meta_ln_bias_2_grad, &meta_x_grad, &meta_qkv_weight_grad, &meta_out_linear_weight_grad, &meta_ln_out_grad, &meta_bias_dropout_residual_out_grad, &meta_qkv_out_grad, &meta_qktv_out_grad, &meta_transpose_out_2_grad, &meta_qk_out_grad, &meta_softmax_out_grad, &meta_attn_dropout_out_grad, &meta_fmha_out_grad, &meta_out_linear_out_grad);

  std::vector<pir::Type> argument_outputs;
  pir::Type qkv_bias_grad_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_qkv_bias_grad.dtype()), dense_qkv_bias_grad.dims(), dense_qkv_bias_grad.layout(), dense_qkv_bias_grad.lod(), dense_qkv_bias_grad.offset());
  argument_outputs.push_back(qkv_bias_grad_dense_tensor_type);

  pir::Type qkv_bias_out_grad_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_qkv_bias_out_grad.dtype()), dense_qkv_bias_out_grad.dims(), dense_qkv_bias_out_grad.layout(), dense_qkv_bias_out_grad.lod(), dense_qkv_bias_out_grad.offset());
  argument_outputs.push_back(qkv_bias_out_grad_dense_tensor_type);

  pir::Type src_mask_out_grad_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_src_mask_out_grad.dtype()), dense_src_mask_out_grad.dims(), dense_src_mask_out_grad.layout(), dense_src_mask_out_grad.lod(), dense_src_mask_out_grad.offset());
  argument_outputs.push_back(src_mask_out_grad_dense_tensor_type);

  pir::Type out_linear_bias_grad_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_out_linear_bias_grad.dtype()), dense_out_linear_bias_grad.dims(), dense_out_linear_bias_grad.layout(), dense_out_linear_bias_grad.lod(), dense_out_linear_bias_grad.offset());
  argument_outputs.push_back(out_linear_bias_grad_dense_tensor_type);

  pir::Type ln_scale_grad_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_ln_scale_grad.dtype()), dense_ln_scale_grad.dims(), dense_ln_scale_grad.layout(), dense_ln_scale_grad.lod(), dense_ln_scale_grad.offset());
  argument_outputs.push_back(ln_scale_grad_dense_tensor_type);

  pir::Type ln_bias_grad_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_ln_bias_grad.dtype()), dense_ln_bias_grad.dims(), dense_ln_bias_grad.layout(), dense_ln_bias_grad.lod(), dense_ln_bias_grad.offset());
  argument_outputs.push_back(ln_bias_grad_dense_tensor_type);

  pir::Type ln_scale_2_grad_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_ln_scale_2_grad.dtype()), dense_ln_scale_2_grad.dims(), dense_ln_scale_2_grad.layout(), dense_ln_scale_2_grad.lod(), dense_ln_scale_2_grad.offset());
  argument_outputs.push_back(ln_scale_2_grad_dense_tensor_type);

  pir::Type ln_bias_2_grad_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_ln_bias_2_grad.dtype()), dense_ln_bias_2_grad.dims(), dense_ln_bias_2_grad.layout(), dense_ln_bias_2_grad.lod(), dense_ln_bias_2_grad.offset());
  argument_outputs.push_back(ln_bias_2_grad_dense_tensor_type);

  pir::Type x_grad_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_x_grad.dtype()), dense_x_grad.dims(), dense_x_grad.layout(), dense_x_grad.lod(), dense_x_grad.offset());
  argument_outputs.push_back(x_grad_dense_tensor_type);

  pir::Type qkv_weight_grad_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_qkv_weight_grad.dtype()), dense_qkv_weight_grad.dims(), dense_qkv_weight_grad.layout(), dense_qkv_weight_grad.lod(), dense_qkv_weight_grad.offset());
  argument_outputs.push_back(qkv_weight_grad_dense_tensor_type);

  pir::Type out_linear_weight_grad_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_out_linear_weight_grad.dtype()), dense_out_linear_weight_grad.dims(), dense_out_linear_weight_grad.layout(), dense_out_linear_weight_grad.lod(), dense_out_linear_weight_grad.offset());
  argument_outputs.push_back(out_linear_weight_grad_dense_tensor_type);

  pir::Type ln_out_grad_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_ln_out_grad.dtype()), dense_ln_out_grad.dims(), dense_ln_out_grad.layout(), dense_ln_out_grad.lod(), dense_ln_out_grad.offset());
  argument_outputs.push_back(ln_out_grad_dense_tensor_type);

  pir::Type bias_dropout_residual_out_grad_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_bias_dropout_residual_out_grad.dtype()), dense_bias_dropout_residual_out_grad.dims(), dense_bias_dropout_residual_out_grad.layout(), dense_bias_dropout_residual_out_grad.lod(), dense_bias_dropout_residual_out_grad.offset());
  argument_outputs.push_back(bias_dropout_residual_out_grad_dense_tensor_type);

  pir::Type qkv_out_grad_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_qkv_out_grad.dtype()), dense_qkv_out_grad.dims(), dense_qkv_out_grad.layout(), dense_qkv_out_grad.lod(), dense_qkv_out_grad.offset());
  argument_outputs.push_back(qkv_out_grad_dense_tensor_type);

  pir::Type qktv_out_grad_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_qktv_out_grad.dtype()), dense_qktv_out_grad.dims(), dense_qktv_out_grad.layout(), dense_qktv_out_grad.lod(), dense_qktv_out_grad.offset());
  argument_outputs.push_back(qktv_out_grad_dense_tensor_type);

  pir::Type transpose_out_2_grad_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_transpose_out_2_grad.dtype()), dense_transpose_out_2_grad.dims(), dense_transpose_out_2_grad.layout(), dense_transpose_out_2_grad.lod(), dense_transpose_out_2_grad.offset());
  argument_outputs.push_back(transpose_out_2_grad_dense_tensor_type);

  pir::Type qk_out_grad_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_qk_out_grad.dtype()), dense_qk_out_grad.dims(), dense_qk_out_grad.layout(), dense_qk_out_grad.lod(), dense_qk_out_grad.offset());
  argument_outputs.push_back(qk_out_grad_dense_tensor_type);

  pir::Type softmax_out_grad_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_softmax_out_grad.dtype()), dense_softmax_out_grad.dims(), dense_softmax_out_grad.layout(), dense_softmax_out_grad.lod(), dense_softmax_out_grad.offset());
  argument_outputs.push_back(softmax_out_grad_dense_tensor_type);

  pir::Type attn_dropout_out_grad_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_attn_dropout_out_grad.dtype()), dense_attn_dropout_out_grad.dims(), dense_attn_dropout_out_grad.layout(), dense_attn_dropout_out_grad.lod(), dense_attn_dropout_out_grad.offset());
  argument_outputs.push_back(attn_dropout_out_grad_dense_tensor_type);

  pir::Type fmha_out_grad_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_fmha_out_grad.dtype()), dense_fmha_out_grad.dims(), dense_fmha_out_grad.layout(), dense_fmha_out_grad.lod(), dense_fmha_out_grad.offset());
  argument_outputs.push_back(fmha_out_grad_dense_tensor_type);

  pir::Type out_linear_out_grad_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_out_linear_out_grad.dtype()), dense_out_linear_out_grad.dims(), dense_out_linear_out_grad.layout(), dense_out_linear_out_grad.lod(), dense_out_linear_out_grad.offset());
  argument_outputs.push_back(out_linear_out_grad_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void FusedAttentionGradOp::VerifySig() {}

void FusedAttentionGradOp::InferMeta( phi::InferMetaContext *infer_meta ) {
  auto fn = PD_INFER_META(phi::FusedAttentionGradInferMeta);
  fn(infer_meta);
}

phi::DataType FusedAttentionGradOp::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: FusedAttentionGradOp";
  


  return expected_kernel_dtype;
}

const char *FusedBatchNormActGradOp::attributes_name[3] = { "momentum", "epsilon", "act_type" };

OpInfoTuple FusedBatchNormActGradOp::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("x", "paddle::dialect::DenseTensorType", false, false, false, false), paddle::dialect::OpInputInfo("scale", "paddle::dialect::DenseTensorType", false, false, false, false), paddle::dialect::OpInputInfo("bias", "paddle::dialect::DenseTensorType", false, false, false, false), paddle::dialect::OpInputInfo("out", "paddle::dialect::DenseTensorType", false, false, false, false), paddle::dialect::OpInputInfo("saved_mean", "paddle::dialect::DenseTensorType", false, false, false, false), paddle::dialect::OpInputInfo("saved_variance", "paddle::dialect::DenseTensorType", false, false, false, false), paddle::dialect::OpInputInfo("reserve_space", "paddle::dialect::DenseTensorType", true, false, false, false), paddle::dialect::OpInputInfo("out_grad", "paddle::dialect::DenseTensorType", false, false, false, false) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = { paddle::dialect::OpAttributeInfo("momentum", "pir::FloatAttribute", ""), paddle::dialect::OpAttributeInfo("epsilon", "pir::FloatAttribute", ""), paddle::dialect::OpAttributeInfo("act_type", "pir::StrAttribute", "") };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("x_grad", "paddle::dialect::DenseTensorType", false, false), paddle::dialect::OpOutputInfo("scale_grad", "paddle::dialect::DenseTensorType", false, false), paddle::dialect::OpOutputInfo("bias_grad", "paddle::dialect::DenseTensorType", false, false) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("GeneralTernaryGradInferMeta", {"x", "scale", "bias"}, "fused_batch_norm_act_grad", {"x", "scale", "bias", "out", "saved_mean", "saved_variance", "reserve_space", "out_grad", "momentum", "epsilon", "act_type"}, {"out_grad"}, {}, {}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "fused_batch_norm_act_grad");
}

void FusedBatchNormActGradOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, pir::Value scale_, pir::Value bias_, pir::Value out_, pir::Value saved_mean_, pir::Value saved_variance_, pir::Value reserve_space_, pir::Value out_grad_, float momentum, float epsilon, const std::string& act_type) {
  VLOG(4) << "Start build FusedBatchNormActGradOp";



  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_, scale_, bias_, out_, saved_mean_, saved_variance_, reserve_space_, out_grad_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_momentum = pir::FloatAttribute::get(pir::IrContext::Instance(), momentum);
  argument.AddAttribute("momentum", attr_momentum);
  pir::Attribute attr_epsilon = pir::FloatAttribute::get(pir::IrContext::Instance(), epsilon);
  argument.AddAttribute("epsilon", attr_epsilon);
  pir::Attribute attr_act_type = pir::StrAttribute::get(pir::IrContext::Instance(), act_type);
  argument.AddAttribute("act_type", attr_act_type);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;
  paddle::dialect::DenseTensorType scale = scale_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)scale;
  paddle::dialect::DenseTensorType bias = bias_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)bias;
  paddle::dialect::DenseTensorType out = out_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)out;
  paddle::dialect::DenseTensorType saved_mean = saved_mean_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)saved_mean;
  paddle::dialect::DenseTensorType saved_variance = saved_variance_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)saved_variance;
  paddle::dialect::DenseTensorType out_grad = out_grad_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)out_grad;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);

  VLOG(4) << "Builder construction  dense_scale";
  paddle::dialect::IrTensor ir_tensor_scale(paddle::dialect::TransToPhiDataType(scale.dtype()),
                                                      scale.dims(),
                                                      scale.data_layout(),
                                                      scale.lod(),
                                                      scale.offset());
  VLOG(4) << "Builder construction  meta_scale";
  paddle::dialect::IrMetaTensor meta_scale(&ir_tensor_scale);

  VLOG(4) << "Builder construction  dense_bias";
  paddle::dialect::IrTensor ir_tensor_bias(paddle::dialect::TransToPhiDataType(bias.dtype()),
                                                      bias.dims(),
                                                      bias.data_layout(),
                                                      bias.lod(),
                                                      bias.offset());
  VLOG(4) << "Builder construction  meta_bias";
  paddle::dialect::IrMetaTensor meta_bias(&ir_tensor_bias);
  paddle::dialect::IrTensor dense_x_grad;
  paddle::dialect::IrMetaTensor meta_x_grad(&dense_x_grad);
  paddle::dialect::IrTensor dense_scale_grad;
  paddle::dialect::IrMetaTensor meta_scale_grad(&dense_scale_grad);
  paddle::dialect::IrTensor dense_bias_grad;
  paddle::dialect::IrMetaTensor meta_bias_grad(&dense_bias_grad);

  phi::GeneralTernaryGradInferMeta(meta_x, meta_scale, meta_bias, &meta_x_grad, &meta_scale_grad, &meta_bias_grad);

  std::vector<pir::Type> argument_outputs;
  pir::Type x_grad_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_x_grad.dtype()), dense_x_grad.dims(), dense_x_grad.layout(), dense_x_grad.lod(), dense_x_grad.offset());
  argument_outputs.push_back(x_grad_dense_tensor_type);

  pir::Type scale_grad_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_scale_grad.dtype()), dense_scale_grad.dims(), dense_scale_grad.layout(), dense_scale_grad.lod(), dense_scale_grad.offset());
  argument_outputs.push_back(scale_grad_dense_tensor_type);

  pir::Type bias_grad_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_bias_grad.dtype()), dense_bias_grad.dims(), dense_bias_grad.layout(), dense_bias_grad.lod(), dense_bias_grad.offset());
  argument_outputs.push_back(bias_grad_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void FusedBatchNormActGradOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, pir::Value scale_, pir::Value bias_, pir::Value out_, pir::Value saved_mean_, pir::Value saved_variance_, pir::Value reserve_space_, pir::Value out_grad_, pir::AttributeMap attributes) {
  VLOG(4) << "Start build FusedBatchNormActGradOp";


  IR_ENFORCE(
      attributes.find("momentum") != attributes.end(),
          "'momentum' Attribute is expected for FusedBatchNormActGradOp. ");
  float momentum = attributes.at("momentum").dyn_cast<pir::FloatAttribute>().data();

  IR_ENFORCE(
      attributes.find("epsilon") != attributes.end(),
          "'epsilon' Attribute is expected for FusedBatchNormActGradOp. ");
  float epsilon = attributes.at("epsilon").dyn_cast<pir::FloatAttribute>().data();

  IR_ENFORCE(
      attributes.find("act_type") != attributes.end(),
          "'act_type' Attribute is expected for FusedBatchNormActGradOp. ");
  std::string act_type = attributes.at("act_type").dyn_cast<pir::StrAttribute>().AsString();


  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_, scale_, bias_, out_, saved_mean_, saved_variance_, reserve_space_, out_grad_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_momentum = pir::FloatAttribute::get(pir::IrContext::Instance(), momentum);
  argument.AddAttribute("momentum", attr_momentum);
  pir::Attribute attr_epsilon = pir::FloatAttribute::get(pir::IrContext::Instance(), epsilon);
  argument.AddAttribute("epsilon", attr_epsilon);
  pir::Attribute attr_act_type = pir::StrAttribute::get(pir::IrContext::Instance(), act_type);
  argument.AddAttribute("act_type", attr_act_type);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;
  paddle::dialect::DenseTensorType scale = scale_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)scale;
  paddle::dialect::DenseTensorType bias = bias_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)bias;
  paddle::dialect::DenseTensorType out = out_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)out;
  paddle::dialect::DenseTensorType saved_mean = saved_mean_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)saved_mean;
  paddle::dialect::DenseTensorType saved_variance = saved_variance_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)saved_variance;
  paddle::dialect::DenseTensorType out_grad = out_grad_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)out_grad;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);

  VLOG(4) << "Builder construction  dense_scale";
  paddle::dialect::IrTensor ir_tensor_scale(paddle::dialect::TransToPhiDataType(scale.dtype()),
                                                      scale.dims(),
                                                      scale.data_layout(),
                                                      scale.lod(),
                                                      scale.offset());
  VLOG(4) << "Builder construction  meta_scale";
  paddle::dialect::IrMetaTensor meta_scale(&ir_tensor_scale);

  VLOG(4) << "Builder construction  dense_bias";
  paddle::dialect::IrTensor ir_tensor_bias(paddle::dialect::TransToPhiDataType(bias.dtype()),
                                                      bias.dims(),
                                                      bias.data_layout(),
                                                      bias.lod(),
                                                      bias.offset());
  VLOG(4) << "Builder construction  meta_bias";
  paddle::dialect::IrMetaTensor meta_bias(&ir_tensor_bias);
  paddle::dialect::IrTensor dense_x_grad;
  paddle::dialect::IrMetaTensor meta_x_grad(&dense_x_grad);
  paddle::dialect::IrTensor dense_scale_grad;
  paddle::dialect::IrMetaTensor meta_scale_grad(&dense_scale_grad);
  paddle::dialect::IrTensor dense_bias_grad;
  paddle::dialect::IrMetaTensor meta_bias_grad(&dense_bias_grad);

  phi::GeneralTernaryGradInferMeta(meta_x, meta_scale, meta_bias, &meta_x_grad, &meta_scale_grad, &meta_bias_grad);

  std::vector<pir::Type> argument_outputs;
  pir::Type x_grad_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_x_grad.dtype()), dense_x_grad.dims(), dense_x_grad.layout(), dense_x_grad.lod(), dense_x_grad.offset());
  argument_outputs.push_back(x_grad_dense_tensor_type);

  pir::Type scale_grad_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_scale_grad.dtype()), dense_scale_grad.dims(), dense_scale_grad.layout(), dense_scale_grad.lod(), dense_scale_grad.offset());
  argument_outputs.push_back(scale_grad_dense_tensor_type);

  pir::Type bias_grad_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_bias_grad.dtype()), dense_bias_grad.dims(), dense_bias_grad.layout(), dense_bias_grad.lod(), dense_bias_grad.offset());
  argument_outputs.push_back(bias_grad_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void FusedBatchNormActGradOp::VerifySig() {}

void FusedBatchNormActGradOp::InferMeta( phi::InferMetaContext *infer_meta ) {
  auto fn = PD_INFER_META(phi::GeneralTernaryGradInferMeta);
  fn(infer_meta);
}

phi::DataType FusedBatchNormActGradOp::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: FusedBatchNormActGradOp";
  


  return expected_kernel_dtype;
}

const char *FusedBnAddActivationGradOp::attributes_name[3] = { "momentum", "epsilon", "act_type" };

OpInfoTuple FusedBnAddActivationGradOp::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("x", "paddle::dialect::DenseTensorType", false, false, false, false), paddle::dialect::OpInputInfo("scale", "paddle::dialect::DenseTensorType", false, false, false, false), paddle::dialect::OpInputInfo("bias", "paddle::dialect::DenseTensorType", false, false, false, false), paddle::dialect::OpInputInfo("out", "paddle::dialect::DenseTensorType", false, false, false, false), paddle::dialect::OpInputInfo("saved_mean", "paddle::dialect::DenseTensorType", false, false, false, false), paddle::dialect::OpInputInfo("saved_variance", "paddle::dialect::DenseTensorType", false, false, false, false), paddle::dialect::OpInputInfo("reserve_space", "paddle::dialect::DenseTensorType", true, false, false, false), paddle::dialect::OpInputInfo("out_grad", "paddle::dialect::DenseTensorType", false, false, false, false) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = { paddle::dialect::OpAttributeInfo("momentum", "pir::FloatAttribute", ""), paddle::dialect::OpAttributeInfo("epsilon", "pir::FloatAttribute", ""), paddle::dialect::OpAttributeInfo("act_type", "pir::StrAttribute", "") };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("x_grad", "paddle::dialect::DenseTensorType", false, false), paddle::dialect::OpOutputInfo("z_grad", "paddle::dialect::DenseTensorType", false, false), paddle::dialect::OpOutputInfo("scale_grad", "paddle::dialect::DenseTensorType", false, false), paddle::dialect::OpOutputInfo("bias_grad", "paddle::dialect::DenseTensorType", false, false) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("GeneralQuaternaryGradInferMeta", {"x", "x", "scale", "bias"}, "fused_bn_add_activation_grad", {"x", "scale", "bias", "out", "saved_mean", "saved_variance", "reserve_space", "out_grad", "momentum", "epsilon", "act_type"}, {"out_grad"}, {}, {}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "fused_bn_add_activation_grad");
}

void FusedBnAddActivationGradOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, pir::Value scale_, pir::Value bias_, pir::Value out_, pir::Value saved_mean_, pir::Value saved_variance_, pir::Value reserve_space_, pir::Value out_grad_, float momentum, float epsilon, const std::string& act_type) {
  VLOG(4) << "Start build FusedBnAddActivationGradOp";



  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_, scale_, bias_, out_, saved_mean_, saved_variance_, reserve_space_, out_grad_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_momentum = pir::FloatAttribute::get(pir::IrContext::Instance(), momentum);
  argument.AddAttribute("momentum", attr_momentum);
  pir::Attribute attr_epsilon = pir::FloatAttribute::get(pir::IrContext::Instance(), epsilon);
  argument.AddAttribute("epsilon", attr_epsilon);
  pir::Attribute attr_act_type = pir::StrAttribute::get(pir::IrContext::Instance(), act_type);
  argument.AddAttribute("act_type", attr_act_type);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;
  paddle::dialect::DenseTensorType scale = scale_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)scale;
  paddle::dialect::DenseTensorType bias = bias_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)bias;
  paddle::dialect::DenseTensorType out = out_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)out;
  paddle::dialect::DenseTensorType saved_mean = saved_mean_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)saved_mean;
  paddle::dialect::DenseTensorType saved_variance = saved_variance_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)saved_variance;
  paddle::dialect::DenseTensorType out_grad = out_grad_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)out_grad;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);

  VLOG(4) << "Builder construction  dense_scale";
  paddle::dialect::IrTensor ir_tensor_scale(paddle::dialect::TransToPhiDataType(scale.dtype()),
                                                      scale.dims(),
                                                      scale.data_layout(),
                                                      scale.lod(),
                                                      scale.offset());
  VLOG(4) << "Builder construction  meta_scale";
  paddle::dialect::IrMetaTensor meta_scale(&ir_tensor_scale);

  VLOG(4) << "Builder construction  dense_bias";
  paddle::dialect::IrTensor ir_tensor_bias(paddle::dialect::TransToPhiDataType(bias.dtype()),
                                                      bias.dims(),
                                                      bias.data_layout(),
                                                      bias.lod(),
                                                      bias.offset());
  VLOG(4) << "Builder construction  meta_bias";
  paddle::dialect::IrMetaTensor meta_bias(&ir_tensor_bias);
  paddle::dialect::IrTensor dense_x_grad;
  paddle::dialect::IrMetaTensor meta_x_grad(&dense_x_grad);
  paddle::dialect::IrTensor dense_z_grad;
  paddle::dialect::IrMetaTensor meta_z_grad(&dense_z_grad);
  paddle::dialect::IrTensor dense_scale_grad;
  paddle::dialect::IrMetaTensor meta_scale_grad(&dense_scale_grad);
  paddle::dialect::IrTensor dense_bias_grad;
  paddle::dialect::IrMetaTensor meta_bias_grad(&dense_bias_grad);

  phi::GeneralQuaternaryGradInferMeta(meta_x, meta_x, meta_scale, meta_bias, &meta_x_grad, &meta_z_grad, &meta_scale_grad, &meta_bias_grad);

  std::vector<pir::Type> argument_outputs;
  pir::Type x_grad_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_x_grad.dtype()), dense_x_grad.dims(), dense_x_grad.layout(), dense_x_grad.lod(), dense_x_grad.offset());
  argument_outputs.push_back(x_grad_dense_tensor_type);

  pir::Type z_grad_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_z_grad.dtype()), dense_z_grad.dims(), dense_z_grad.layout(), dense_z_grad.lod(), dense_z_grad.offset());
  argument_outputs.push_back(z_grad_dense_tensor_type);

  pir::Type scale_grad_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_scale_grad.dtype()), dense_scale_grad.dims(), dense_scale_grad.layout(), dense_scale_grad.lod(), dense_scale_grad.offset());
  argument_outputs.push_back(scale_grad_dense_tensor_type);

  pir::Type bias_grad_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_bias_grad.dtype()), dense_bias_grad.dims(), dense_bias_grad.layout(), dense_bias_grad.lod(), dense_bias_grad.offset());
  argument_outputs.push_back(bias_grad_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void FusedBnAddActivationGradOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, pir::Value scale_, pir::Value bias_, pir::Value out_, pir::Value saved_mean_, pir::Value saved_variance_, pir::Value reserve_space_, pir::Value out_grad_, pir::AttributeMap attributes) {
  VLOG(4) << "Start build FusedBnAddActivationGradOp";


  IR_ENFORCE(
      attributes.find("momentum") != attributes.end(),
          "'momentum' Attribute is expected for FusedBnAddActivationGradOp. ");
  float momentum = attributes.at("momentum").dyn_cast<pir::FloatAttribute>().data();

  IR_ENFORCE(
      attributes.find("epsilon") != attributes.end(),
          "'epsilon' Attribute is expected for FusedBnAddActivationGradOp. ");
  float epsilon = attributes.at("epsilon").dyn_cast<pir::FloatAttribute>().data();

  IR_ENFORCE(
      attributes.find("act_type") != attributes.end(),
          "'act_type' Attribute is expected for FusedBnAddActivationGradOp. ");
  std::string act_type = attributes.at("act_type").dyn_cast<pir::StrAttribute>().AsString();


  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_, scale_, bias_, out_, saved_mean_, saved_variance_, reserve_space_, out_grad_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_momentum = pir::FloatAttribute::get(pir::IrContext::Instance(), momentum);
  argument.AddAttribute("momentum", attr_momentum);
  pir::Attribute attr_epsilon = pir::FloatAttribute::get(pir::IrContext::Instance(), epsilon);
  argument.AddAttribute("epsilon", attr_epsilon);
  pir::Attribute attr_act_type = pir::StrAttribute::get(pir::IrContext::Instance(), act_type);
  argument.AddAttribute("act_type", attr_act_type);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;
  paddle::dialect::DenseTensorType scale = scale_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)scale;
  paddle::dialect::DenseTensorType bias = bias_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)bias;
  paddle::dialect::DenseTensorType out = out_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)out;
  paddle::dialect::DenseTensorType saved_mean = saved_mean_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)saved_mean;
  paddle::dialect::DenseTensorType saved_variance = saved_variance_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)saved_variance;
  paddle::dialect::DenseTensorType out_grad = out_grad_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)out_grad;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);

  VLOG(4) << "Builder construction  dense_scale";
  paddle::dialect::IrTensor ir_tensor_scale(paddle::dialect::TransToPhiDataType(scale.dtype()),
                                                      scale.dims(),
                                                      scale.data_layout(),
                                                      scale.lod(),
                                                      scale.offset());
  VLOG(4) << "Builder construction  meta_scale";
  paddle::dialect::IrMetaTensor meta_scale(&ir_tensor_scale);

  VLOG(4) << "Builder construction  dense_bias";
  paddle::dialect::IrTensor ir_tensor_bias(paddle::dialect::TransToPhiDataType(bias.dtype()),
                                                      bias.dims(),
                                                      bias.data_layout(),
                                                      bias.lod(),
                                                      bias.offset());
  VLOG(4) << "Builder construction  meta_bias";
  paddle::dialect::IrMetaTensor meta_bias(&ir_tensor_bias);
  paddle::dialect::IrTensor dense_x_grad;
  paddle::dialect::IrMetaTensor meta_x_grad(&dense_x_grad);
  paddle::dialect::IrTensor dense_z_grad;
  paddle::dialect::IrMetaTensor meta_z_grad(&dense_z_grad);
  paddle::dialect::IrTensor dense_scale_grad;
  paddle::dialect::IrMetaTensor meta_scale_grad(&dense_scale_grad);
  paddle::dialect::IrTensor dense_bias_grad;
  paddle::dialect::IrMetaTensor meta_bias_grad(&dense_bias_grad);

  phi::GeneralQuaternaryGradInferMeta(meta_x, meta_x, meta_scale, meta_bias, &meta_x_grad, &meta_z_grad, &meta_scale_grad, &meta_bias_grad);

  std::vector<pir::Type> argument_outputs;
  pir::Type x_grad_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_x_grad.dtype()), dense_x_grad.dims(), dense_x_grad.layout(), dense_x_grad.lod(), dense_x_grad.offset());
  argument_outputs.push_back(x_grad_dense_tensor_type);

  pir::Type z_grad_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_z_grad.dtype()), dense_z_grad.dims(), dense_z_grad.layout(), dense_z_grad.lod(), dense_z_grad.offset());
  argument_outputs.push_back(z_grad_dense_tensor_type);

  pir::Type scale_grad_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_scale_grad.dtype()), dense_scale_grad.dims(), dense_scale_grad.layout(), dense_scale_grad.lod(), dense_scale_grad.offset());
  argument_outputs.push_back(scale_grad_dense_tensor_type);

  pir::Type bias_grad_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_bias_grad.dtype()), dense_bias_grad.dims(), dense_bias_grad.layout(), dense_bias_grad.lod(), dense_bias_grad.offset());
  argument_outputs.push_back(bias_grad_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void FusedBnAddActivationGradOp::VerifySig() {}

void FusedBnAddActivationGradOp::InferMeta( phi::InferMetaContext *infer_meta ) {
  auto fn = PD_INFER_META(phi::GeneralQuaternaryGradInferMeta);
  fn(infer_meta);
}

phi::DataType FusedBnAddActivationGradOp::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: FusedBnAddActivationGradOp";
  


  return expected_kernel_dtype;
}

const char *FusedFeedforwardGradOp::attributes_name[15] = { "pre_layer_norm", "ln1_epsilon", "ln2_epsilon", "act_method", "dropout1_prob", "dropout2_prob", "dropout1_implementation", "dropout2_implementation", "is_test", "dropout1_fix_seed", "dropout2_fix_seed", "dropout1_seed_val", "dropout2_seed_val", "add_residual", "ring_id" };

OpInfoTuple FusedFeedforwardGradOp::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("out_grad", "paddle::dialect::DenseTensorType", false, false, false, false), paddle::dialect::OpInputInfo("x", "paddle::dialect::DenseTensorType", false, false, false, false), paddle::dialect::OpInputInfo("linear1_weight", "paddle::dialect::DenseTensorType", false, false, false, false), paddle::dialect::OpInputInfo("linear1_bias", "paddle::dialect::DenseTensorType", true, false, false, false), paddle::dialect::OpInputInfo("linear2_weight", "paddle::dialect::DenseTensorType", false, false, false, false), paddle::dialect::OpInputInfo("dropout1_mask", "paddle::dialect::DenseTensorType", false, false, false, false), paddle::dialect::OpInputInfo("dropout2_mask", "paddle::dialect::DenseTensorType", false, false, false, false), paddle::dialect::OpInputInfo("linear1_out", "paddle::dialect::DenseTensorType", false, false, false, false), paddle::dialect::OpInputInfo("dropout1_out", "paddle::dialect::DenseTensorType", false, false, false, false), paddle::dialect::OpInputInfo("dropout2_out", "paddle::dialect::DenseTensorType", true, false, false, false), paddle::dialect::OpInputInfo("ln1_scale", "paddle::dialect::DenseTensorType", true, false, false, false), paddle::dialect::OpInputInfo("ln1_bias", "paddle::dialect::DenseTensorType", true, false, false, false), paddle::dialect::OpInputInfo("ln1_out", "paddle::dialect::DenseTensorType", true, false, false, false), paddle::dialect::OpInputInfo("ln1_mean", "paddle::dialect::DenseTensorType", true, false, false, false), paddle::dialect::OpInputInfo("ln1_variance", "paddle::dialect::DenseTensorType", true, false, false, false), paddle::dialect::OpInputInfo("ln2_scale", "paddle::dialect::DenseTensorType", true, false, false, false), paddle::dialect::OpInputInfo("ln2_bias", "paddle::dialect::DenseTensorType", true, false, false, false), paddle::dialect::OpInputInfo("ln2_mean", "paddle::dialect::DenseTensorType", true, false, false, false), paddle::dialect::OpInputInfo("ln2_variance", "paddle::dialect::DenseTensorType", true, false, false, false), paddle::dialect::OpInputInfo("linear2_bias", "paddle::dialect::DenseTensorType", true, false, false, false) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = { paddle::dialect::OpAttributeInfo("pre_layer_norm", "pir::BoolAttribute", ""), paddle::dialect::OpAttributeInfo("ln1_epsilon", "pir::FloatAttribute", ""), paddle::dialect::OpAttributeInfo("ln2_epsilon", "pir::FloatAttribute", ""), paddle::dialect::OpAttributeInfo("act_method", "pir::StrAttribute", ""), paddle::dialect::OpAttributeInfo("dropout1_prob", "pir::FloatAttribute", ""), paddle::dialect::OpAttributeInfo("dropout2_prob", "pir::FloatAttribute", ""), paddle::dialect::OpAttributeInfo("dropout1_implementation", "pir::StrAttribute", ""), paddle::dialect::OpAttributeInfo("dropout2_implementation", "pir::StrAttribute", ""), paddle::dialect::OpAttributeInfo("is_test", "pir::BoolAttribute", ""), paddle::dialect::OpAttributeInfo("dropout1_fix_seed", "pir::BoolAttribute", ""), paddle::dialect::OpAttributeInfo("dropout2_fix_seed", "pir::BoolAttribute", ""), paddle::dialect::OpAttributeInfo("dropout1_seed_val", "pir::Int32Attribute", ""), paddle::dialect::OpAttributeInfo("dropout2_seed_val", "pir::Int32Attribute", ""), paddle::dialect::OpAttributeInfo("add_residual", "pir::BoolAttribute", ""), paddle::dialect::OpAttributeInfo("ring_id", "pir::Int32Attribute", "") };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("x_grad", "paddle::dialect::DenseTensorType", false, false), paddle::dialect::OpOutputInfo("ln1_scale_grad", "paddle::dialect::DenseTensorType", true, false), paddle::dialect::OpOutputInfo("ln1_bias_grad", "paddle::dialect::DenseTensorType", true, false), paddle::dialect::OpOutputInfo("ln2_scale_grad", "paddle::dialect::DenseTensorType", true, false), paddle::dialect::OpOutputInfo("ln2_bias_grad", "paddle::dialect::DenseTensorType", true, false), paddle::dialect::OpOutputInfo("linear1_weight_grad", "paddle::dialect::DenseTensorType", false, false), paddle::dialect::OpOutputInfo("linear1_bias_grad", "paddle::dialect::DenseTensorType", false, false), paddle::dialect::OpOutputInfo("linear2_weight_grad", "paddle::dialect::DenseTensorType", false, false), paddle::dialect::OpOutputInfo("linear2_bias_grad", "paddle::dialect::DenseTensorType", true, false) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("FusedFeedForwardGradInferMeta", {"out_grad", "x", "linear1_weight", "linear1_bias", "linear2_weight", "dropout1_mask", "dropout2_mask", "linear1_out", "dropout1_out", "dropout2_out", "ln1_scale", "ln1_bias", "ln1_out", "ln1_mean", "ln1_variance", "ln2_scale", "ln2_bias", "ln2_mean", "ln2_variance", "linear2_bias", "pre_layer_norm", "ln1_epsilon", "ln2_epsilon", "act_method", "dropout1_prob", "dropout2_prob", "dropout1_implementation", "dropout2_implementation", "is_test", "dropout1_fix_seed", "dropout2_fix_seed", "dropout1_seed_val", "dropout2_seed_val", "add_residual", "ring_id"}, "fused_feedforward_grad", {"out_grad", "x", "linear1_weight", "linear1_bias", "linear2_weight", "dropout1_mask", "dropout2_mask", "linear1_out", "dropout1_out", "dropout2_out", "ln1_scale", "ln1_bias", "ln1_out", "ln1_mean", "ln1_variance", "ln2_scale", "ln2_bias", "ln2_mean", "ln2_variance", "linear2_bias", "pre_layer_norm", "ln1_epsilon", "ln2_epsilon", "act_method", "dropout1_prob", "dropout2_prob", "dropout1_implementation", "dropout2_implementation", "is_test", "dropout1_fix_seed", "dropout2_fix_seed", "dropout1_seed_val", "dropout2_seed_val", "add_residual", "ring_id"}, {}, {}, {}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "fused_feedforward_grad");
}

void FusedFeedforwardGradOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value out_grad_, pir::Value x_, pir::Value linear1_weight_, pir::Value linear1_bias_, pir::Value linear2_weight_, pir::Value dropout1_mask_, pir::Value dropout2_mask_, pir::Value linear1_out_, pir::Value dropout1_out_, pir::Value dropout2_out_, pir::Value ln1_scale_, pir::Value ln1_bias_, pir::Value ln1_out_, pir::Value ln1_mean_, pir::Value ln1_variance_, pir::Value ln2_scale_, pir::Value ln2_bias_, pir::Value ln2_mean_, pir::Value ln2_variance_, pir::Value linear2_bias_, bool pre_layer_norm, float ln1_epsilon, float ln2_epsilon, const std::string& act_method, float dropout1_prob, float dropout2_prob, const std::string& dropout1_implementation, const std::string& dropout2_implementation, bool is_test, bool dropout1_fix_seed, bool dropout2_fix_seed, int dropout1_seed_val, int dropout2_seed_val, bool add_residual, int ring_id) {
  VLOG(4) << "Start build FusedFeedforwardGradOp";



  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {out_grad_, x_, linear1_weight_, linear1_bias_, linear2_weight_, dropout1_mask_, dropout2_mask_, linear1_out_, dropout1_out_, dropout2_out_, ln1_scale_, ln1_bias_, ln1_out_, ln1_mean_, ln1_variance_, ln2_scale_, ln2_bias_, ln2_mean_, ln2_variance_, linear2_bias_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_pre_layer_norm = pir::BoolAttribute::get(pir::IrContext::Instance(), pre_layer_norm);
  argument.AddAttribute("pre_layer_norm", attr_pre_layer_norm);
  pir::Attribute attr_ln1_epsilon = pir::FloatAttribute::get(pir::IrContext::Instance(), ln1_epsilon);
  argument.AddAttribute("ln1_epsilon", attr_ln1_epsilon);
  pir::Attribute attr_ln2_epsilon = pir::FloatAttribute::get(pir::IrContext::Instance(), ln2_epsilon);
  argument.AddAttribute("ln2_epsilon", attr_ln2_epsilon);
  pir::Attribute attr_act_method = pir::StrAttribute::get(pir::IrContext::Instance(), act_method);
  argument.AddAttribute("act_method", attr_act_method);
  pir::Attribute attr_dropout1_prob = pir::FloatAttribute::get(pir::IrContext::Instance(), dropout1_prob);
  argument.AddAttribute("dropout1_prob", attr_dropout1_prob);
  pir::Attribute attr_dropout2_prob = pir::FloatAttribute::get(pir::IrContext::Instance(), dropout2_prob);
  argument.AddAttribute("dropout2_prob", attr_dropout2_prob);
  pir::Attribute attr_dropout1_implementation = pir::StrAttribute::get(pir::IrContext::Instance(), dropout1_implementation);
  argument.AddAttribute("dropout1_implementation", attr_dropout1_implementation);
  pir::Attribute attr_dropout2_implementation = pir::StrAttribute::get(pir::IrContext::Instance(), dropout2_implementation);
  argument.AddAttribute("dropout2_implementation", attr_dropout2_implementation);
  pir::Attribute attr_is_test = pir::BoolAttribute::get(pir::IrContext::Instance(), is_test);
  argument.AddAttribute("is_test", attr_is_test);
  pir::Attribute attr_dropout1_fix_seed = pir::BoolAttribute::get(pir::IrContext::Instance(), dropout1_fix_seed);
  argument.AddAttribute("dropout1_fix_seed", attr_dropout1_fix_seed);
  pir::Attribute attr_dropout2_fix_seed = pir::BoolAttribute::get(pir::IrContext::Instance(), dropout2_fix_seed);
  argument.AddAttribute("dropout2_fix_seed", attr_dropout2_fix_seed);
  pir::Attribute attr_dropout1_seed_val = pir::Int32Attribute::get(pir::IrContext::Instance(), dropout1_seed_val);
  argument.AddAttribute("dropout1_seed_val", attr_dropout1_seed_val);
  pir::Attribute attr_dropout2_seed_val = pir::Int32Attribute::get(pir::IrContext::Instance(), dropout2_seed_val);
  argument.AddAttribute("dropout2_seed_val", attr_dropout2_seed_val);
  pir::Attribute attr_add_residual = pir::BoolAttribute::get(pir::IrContext::Instance(), add_residual);
  argument.AddAttribute("add_residual", attr_add_residual);
  pir::Attribute attr_ring_id = pir::Int32Attribute::get(pir::IrContext::Instance(), ring_id);
  argument.AddAttribute("ring_id", attr_ring_id);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType out_grad = out_grad_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)out_grad;
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;
  paddle::dialect::DenseTensorType linear1_weight = linear1_weight_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)linear1_weight;
  paddle::dialect::DenseTensorType linear2_weight = linear2_weight_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)linear2_weight;
  paddle::dialect::DenseTensorType dropout1_mask = dropout1_mask_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)dropout1_mask;
  paddle::dialect::DenseTensorType dropout2_mask = dropout2_mask_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)dropout2_mask;
  paddle::dialect::DenseTensorType linear1_out = linear1_out_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)linear1_out;
  paddle::dialect::DenseTensorType dropout1_out = dropout1_out_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)dropout1_out;

  VLOG(4) << "Builder construction  dense_out_grad";
  paddle::dialect::IrTensor ir_tensor_out_grad(paddle::dialect::TransToPhiDataType(out_grad.dtype()),
                                                      out_grad.dims(),
                                                      out_grad.data_layout(),
                                                      out_grad.lod(),
                                                      out_grad.offset());
  VLOG(4) << "Builder construction  meta_out_grad";
  paddle::dialect::IrMetaTensor meta_out_grad(&ir_tensor_out_grad);

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);

  VLOG(4) << "Builder construction  dense_linear1_weight";
  paddle::dialect::IrTensor ir_tensor_linear1_weight(paddle::dialect::TransToPhiDataType(linear1_weight.dtype()),
                                                      linear1_weight.dims(),
                                                      linear1_weight.data_layout(),
                                                      linear1_weight.lod(),
                                                      linear1_weight.offset());
  VLOG(4) << "Builder construction  meta_linear1_weight";
  paddle::dialect::IrMetaTensor meta_linear1_weight(&ir_tensor_linear1_weight);

  paddle::dialect::IrMetaTensor meta_linear1_bias;
  paddle::dialect::IrTensor ir_tensor_linear1_bias;
  if (linear1_bias_.impl() != nullptr) {
    paddle::dialect::DenseTensorType linear1_bias = linear1_bias_.type().dyn_cast<paddle::dialect::DenseTensorType>();
    VLOG(4) << "Builder construction  dense_linear1_bias";
    ir_tensor_linear1_bias = paddle::dialect::IrTensor(paddle::dialect::TransToPhiDataType(linear1_bias.dtype()),
                                                        linear1_bias.dims(),
                                                        linear1_bias.data_layout(),
                                                        linear1_bias.lod(),
                                                        linear1_bias.offset());
    VLOG(4) << "Builder construction  meta_linear1_bias";
    meta_linear1_bias = paddle::dialect::IrMetaTensor(&ir_tensor_linear1_bias);
  }


  VLOG(4) << "Builder construction  dense_linear2_weight";
  paddle::dialect::IrTensor ir_tensor_linear2_weight(paddle::dialect::TransToPhiDataType(linear2_weight.dtype()),
                                                      linear2_weight.dims(),
                                                      linear2_weight.data_layout(),
                                                      linear2_weight.lod(),
                                                      linear2_weight.offset());
  VLOG(4) << "Builder construction  meta_linear2_weight";
  paddle::dialect::IrMetaTensor meta_linear2_weight(&ir_tensor_linear2_weight);

  VLOG(4) << "Builder construction  dense_dropout1_mask";
  paddle::dialect::IrTensor ir_tensor_dropout1_mask(paddle::dialect::TransToPhiDataType(dropout1_mask.dtype()),
                                                      dropout1_mask.dims(),
                                                      dropout1_mask.data_layout(),
                                                      dropout1_mask.lod(),
                                                      dropout1_mask.offset());
  VLOG(4) << "Builder construction  meta_dropout1_mask";
  paddle::dialect::IrMetaTensor meta_dropout1_mask(&ir_tensor_dropout1_mask);

  VLOG(4) << "Builder construction  dense_dropout2_mask";
  paddle::dialect::IrTensor ir_tensor_dropout2_mask(paddle::dialect::TransToPhiDataType(dropout2_mask.dtype()),
                                                      dropout2_mask.dims(),
                                                      dropout2_mask.data_layout(),
                                                      dropout2_mask.lod(),
                                                      dropout2_mask.offset());
  VLOG(4) << "Builder construction  meta_dropout2_mask";
  paddle::dialect::IrMetaTensor meta_dropout2_mask(&ir_tensor_dropout2_mask);

  VLOG(4) << "Builder construction  dense_linear1_out";
  paddle::dialect::IrTensor ir_tensor_linear1_out(paddle::dialect::TransToPhiDataType(linear1_out.dtype()),
                                                      linear1_out.dims(),
                                                      linear1_out.data_layout(),
                                                      linear1_out.lod(),
                                                      linear1_out.offset());
  VLOG(4) << "Builder construction  meta_linear1_out";
  paddle::dialect::IrMetaTensor meta_linear1_out(&ir_tensor_linear1_out);

  VLOG(4) << "Builder construction  dense_dropout1_out";
  paddle::dialect::IrTensor ir_tensor_dropout1_out(paddle::dialect::TransToPhiDataType(dropout1_out.dtype()),
                                                      dropout1_out.dims(),
                                                      dropout1_out.data_layout(),
                                                      dropout1_out.lod(),
                                                      dropout1_out.offset());
  VLOG(4) << "Builder construction  meta_dropout1_out";
  paddle::dialect::IrMetaTensor meta_dropout1_out(&ir_tensor_dropout1_out);

  paddle::dialect::IrMetaTensor meta_dropout2_out;
  paddle::dialect::IrTensor ir_tensor_dropout2_out;
  if (dropout2_out_.impl() != nullptr) {
    paddle::dialect::DenseTensorType dropout2_out = dropout2_out_.type().dyn_cast<paddle::dialect::DenseTensorType>();
    VLOG(4) << "Builder construction  dense_dropout2_out";
    ir_tensor_dropout2_out = paddle::dialect::IrTensor(paddle::dialect::TransToPhiDataType(dropout2_out.dtype()),
                                                        dropout2_out.dims(),
                                                        dropout2_out.data_layout(),
                                                        dropout2_out.lod(),
                                                        dropout2_out.offset());
    VLOG(4) << "Builder construction  meta_dropout2_out";
    meta_dropout2_out = paddle::dialect::IrMetaTensor(&ir_tensor_dropout2_out);
  }


  paddle::dialect::IrMetaTensor meta_ln1_scale;
  paddle::dialect::IrTensor ir_tensor_ln1_scale;
  if (ln1_scale_.impl() != nullptr) {
    paddle::dialect::DenseTensorType ln1_scale = ln1_scale_.type().dyn_cast<paddle::dialect::DenseTensorType>();
    VLOG(4) << "Builder construction  dense_ln1_scale";
    ir_tensor_ln1_scale = paddle::dialect::IrTensor(paddle::dialect::TransToPhiDataType(ln1_scale.dtype()),
                                                        ln1_scale.dims(),
                                                        ln1_scale.data_layout(),
                                                        ln1_scale.lod(),
                                                        ln1_scale.offset());
    VLOG(4) << "Builder construction  meta_ln1_scale";
    meta_ln1_scale = paddle::dialect::IrMetaTensor(&ir_tensor_ln1_scale);
  }


  paddle::dialect::IrMetaTensor meta_ln1_bias;
  paddle::dialect::IrTensor ir_tensor_ln1_bias;
  if (ln1_bias_.impl() != nullptr) {
    paddle::dialect::DenseTensorType ln1_bias = ln1_bias_.type().dyn_cast<paddle::dialect::DenseTensorType>();
    VLOG(4) << "Builder construction  dense_ln1_bias";
    ir_tensor_ln1_bias = paddle::dialect::IrTensor(paddle::dialect::TransToPhiDataType(ln1_bias.dtype()),
                                                        ln1_bias.dims(),
                                                        ln1_bias.data_layout(),
                                                        ln1_bias.lod(),
                                                        ln1_bias.offset());
    VLOG(4) << "Builder construction  meta_ln1_bias";
    meta_ln1_bias = paddle::dialect::IrMetaTensor(&ir_tensor_ln1_bias);
  }


  paddle::dialect::IrMetaTensor meta_ln1_out;
  paddle::dialect::IrTensor ir_tensor_ln1_out;
  if (ln1_out_.impl() != nullptr) {
    paddle::dialect::DenseTensorType ln1_out = ln1_out_.type().dyn_cast<paddle::dialect::DenseTensorType>();
    VLOG(4) << "Builder construction  dense_ln1_out";
    ir_tensor_ln1_out = paddle::dialect::IrTensor(paddle::dialect::TransToPhiDataType(ln1_out.dtype()),
                                                        ln1_out.dims(),
                                                        ln1_out.data_layout(),
                                                        ln1_out.lod(),
                                                        ln1_out.offset());
    VLOG(4) << "Builder construction  meta_ln1_out";
    meta_ln1_out = paddle::dialect::IrMetaTensor(&ir_tensor_ln1_out);
  }


  paddle::dialect::IrMetaTensor meta_ln1_mean;
  paddle::dialect::IrTensor ir_tensor_ln1_mean;
  if (ln1_mean_.impl() != nullptr) {
    paddle::dialect::DenseTensorType ln1_mean = ln1_mean_.type().dyn_cast<paddle::dialect::DenseTensorType>();
    VLOG(4) << "Builder construction  dense_ln1_mean";
    ir_tensor_ln1_mean = paddle::dialect::IrTensor(paddle::dialect::TransToPhiDataType(ln1_mean.dtype()),
                                                        ln1_mean.dims(),
                                                        ln1_mean.data_layout(),
                                                        ln1_mean.lod(),
                                                        ln1_mean.offset());
    VLOG(4) << "Builder construction  meta_ln1_mean";
    meta_ln1_mean = paddle::dialect::IrMetaTensor(&ir_tensor_ln1_mean);
  }


  paddle::dialect::IrMetaTensor meta_ln1_variance;
  paddle::dialect::IrTensor ir_tensor_ln1_variance;
  if (ln1_variance_.impl() != nullptr) {
    paddle::dialect::DenseTensorType ln1_variance = ln1_variance_.type().dyn_cast<paddle::dialect::DenseTensorType>();
    VLOG(4) << "Builder construction  dense_ln1_variance";
    ir_tensor_ln1_variance = paddle::dialect::IrTensor(paddle::dialect::TransToPhiDataType(ln1_variance.dtype()),
                                                        ln1_variance.dims(),
                                                        ln1_variance.data_layout(),
                                                        ln1_variance.lod(),
                                                        ln1_variance.offset());
    VLOG(4) << "Builder construction  meta_ln1_variance";
    meta_ln1_variance = paddle::dialect::IrMetaTensor(&ir_tensor_ln1_variance);
  }


  paddle::dialect::IrMetaTensor meta_ln2_scale;
  paddle::dialect::IrTensor ir_tensor_ln2_scale;
  if (ln2_scale_.impl() != nullptr) {
    paddle::dialect::DenseTensorType ln2_scale = ln2_scale_.type().dyn_cast<paddle::dialect::DenseTensorType>();
    VLOG(4) << "Builder construction  dense_ln2_scale";
    ir_tensor_ln2_scale = paddle::dialect::IrTensor(paddle::dialect::TransToPhiDataType(ln2_scale.dtype()),
                                                        ln2_scale.dims(),
                                                        ln2_scale.data_layout(),
                                                        ln2_scale.lod(),
                                                        ln2_scale.offset());
    VLOG(4) << "Builder construction  meta_ln2_scale";
    meta_ln2_scale = paddle::dialect::IrMetaTensor(&ir_tensor_ln2_scale);
  }


  paddle::dialect::IrMetaTensor meta_ln2_bias;
  paddle::dialect::IrTensor ir_tensor_ln2_bias;
  if (ln2_bias_.impl() != nullptr) {
    paddle::dialect::DenseTensorType ln2_bias = ln2_bias_.type().dyn_cast<paddle::dialect::DenseTensorType>();
    VLOG(4) << "Builder construction  dense_ln2_bias";
    ir_tensor_ln2_bias = paddle::dialect::IrTensor(paddle::dialect::TransToPhiDataType(ln2_bias.dtype()),
                                                        ln2_bias.dims(),
                                                        ln2_bias.data_layout(),
                                                        ln2_bias.lod(),
                                                        ln2_bias.offset());
    VLOG(4) << "Builder construction  meta_ln2_bias";
    meta_ln2_bias = paddle::dialect::IrMetaTensor(&ir_tensor_ln2_bias);
  }


  paddle::dialect::IrMetaTensor meta_ln2_mean;
  paddle::dialect::IrTensor ir_tensor_ln2_mean;
  if (ln2_mean_.impl() != nullptr) {
    paddle::dialect::DenseTensorType ln2_mean = ln2_mean_.type().dyn_cast<paddle::dialect::DenseTensorType>();
    VLOG(4) << "Builder construction  dense_ln2_mean";
    ir_tensor_ln2_mean = paddle::dialect::IrTensor(paddle::dialect::TransToPhiDataType(ln2_mean.dtype()),
                                                        ln2_mean.dims(),
                                                        ln2_mean.data_layout(),
                                                        ln2_mean.lod(),
                                                        ln2_mean.offset());
    VLOG(4) << "Builder construction  meta_ln2_mean";
    meta_ln2_mean = paddle::dialect::IrMetaTensor(&ir_tensor_ln2_mean);
  }


  paddle::dialect::IrMetaTensor meta_ln2_variance;
  paddle::dialect::IrTensor ir_tensor_ln2_variance;
  if (ln2_variance_.impl() != nullptr) {
    paddle::dialect::DenseTensorType ln2_variance = ln2_variance_.type().dyn_cast<paddle::dialect::DenseTensorType>();
    VLOG(4) << "Builder construction  dense_ln2_variance";
    ir_tensor_ln2_variance = paddle::dialect::IrTensor(paddle::dialect::TransToPhiDataType(ln2_variance.dtype()),
                                                        ln2_variance.dims(),
                                                        ln2_variance.data_layout(),
                                                        ln2_variance.lod(),
                                                        ln2_variance.offset());
    VLOG(4) << "Builder construction  meta_ln2_variance";
    meta_ln2_variance = paddle::dialect::IrMetaTensor(&ir_tensor_ln2_variance);
  }


  paddle::dialect::IrMetaTensor meta_linear2_bias;
  paddle::dialect::IrTensor ir_tensor_linear2_bias;
  if (linear2_bias_.impl() != nullptr) {
    paddle::dialect::DenseTensorType linear2_bias = linear2_bias_.type().dyn_cast<paddle::dialect::DenseTensorType>();
    VLOG(4) << "Builder construction  dense_linear2_bias";
    ir_tensor_linear2_bias = paddle::dialect::IrTensor(paddle::dialect::TransToPhiDataType(linear2_bias.dtype()),
                                                        linear2_bias.dims(),
                                                        linear2_bias.data_layout(),
                                                        linear2_bias.lod(),
                                                        linear2_bias.offset());
    VLOG(4) << "Builder construction  meta_linear2_bias";
    meta_linear2_bias = paddle::dialect::IrMetaTensor(&ir_tensor_linear2_bias);
  }

  paddle::dialect::IrTensor dense_x_grad;
  paddle::dialect::IrMetaTensor meta_x_grad(&dense_x_grad);
  paddle::dialect::IrTensor dense_ln1_scale_grad;
  paddle::dialect::IrMetaTensor meta_ln1_scale_grad(&dense_ln1_scale_grad);
  paddle::dialect::IrTensor dense_ln1_bias_grad;
  paddle::dialect::IrMetaTensor meta_ln1_bias_grad(&dense_ln1_bias_grad);
  paddle::dialect::IrTensor dense_ln2_scale_grad;
  paddle::dialect::IrMetaTensor meta_ln2_scale_grad(&dense_ln2_scale_grad);
  paddle::dialect::IrTensor dense_ln2_bias_grad;
  paddle::dialect::IrMetaTensor meta_ln2_bias_grad(&dense_ln2_bias_grad);
  paddle::dialect::IrTensor dense_linear1_weight_grad;
  paddle::dialect::IrMetaTensor meta_linear1_weight_grad(&dense_linear1_weight_grad);
  paddle::dialect::IrTensor dense_linear1_bias_grad;
  paddle::dialect::IrMetaTensor meta_linear1_bias_grad(&dense_linear1_bias_grad);
  paddle::dialect::IrTensor dense_linear2_weight_grad;
  paddle::dialect::IrMetaTensor meta_linear2_weight_grad(&dense_linear2_weight_grad);
  paddle::dialect::IrTensor dense_linear2_bias_grad;
  paddle::dialect::IrMetaTensor meta_linear2_bias_grad(&dense_linear2_bias_grad);

  phi::FusedFeedForwardGradInferMeta(meta_out_grad, meta_x, meta_linear1_weight, meta_linear1_bias, meta_linear2_weight, meta_dropout1_mask, meta_dropout2_mask, meta_linear1_out, meta_dropout1_out, meta_dropout2_out, meta_ln1_scale, meta_ln1_bias, meta_ln1_out, meta_ln1_mean, meta_ln1_variance, meta_ln2_scale, meta_ln2_bias, meta_ln2_mean, meta_ln2_variance, meta_linear2_bias, pre_layer_norm, ln1_epsilon, ln2_epsilon, act_method, dropout1_prob, dropout2_prob, dropout1_implementation, dropout2_implementation, is_test, dropout1_fix_seed, dropout2_fix_seed, dropout1_seed_val, dropout2_seed_val, add_residual, ring_id, &meta_x_grad, &meta_ln1_scale_grad, &meta_ln1_bias_grad, &meta_ln2_scale_grad, &meta_ln2_bias_grad, &meta_linear1_weight_grad, &meta_linear1_bias_grad, &meta_linear2_weight_grad, &meta_linear2_bias_grad);

  std::vector<pir::Type> argument_outputs;
  pir::Type x_grad_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_x_grad.dtype()), dense_x_grad.dims(), dense_x_grad.layout(), dense_x_grad.lod(), dense_x_grad.offset());
  argument_outputs.push_back(x_grad_dense_tensor_type);

  pir::Type ln1_scale_grad_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_ln1_scale_grad.dtype()), dense_ln1_scale_grad.dims(), dense_ln1_scale_grad.layout(), dense_ln1_scale_grad.lod(), dense_ln1_scale_grad.offset());
  argument_outputs.push_back(ln1_scale_grad_dense_tensor_type);

  pir::Type ln1_bias_grad_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_ln1_bias_grad.dtype()), dense_ln1_bias_grad.dims(), dense_ln1_bias_grad.layout(), dense_ln1_bias_grad.lod(), dense_ln1_bias_grad.offset());
  argument_outputs.push_back(ln1_bias_grad_dense_tensor_type);

  pir::Type ln2_scale_grad_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_ln2_scale_grad.dtype()), dense_ln2_scale_grad.dims(), dense_ln2_scale_grad.layout(), dense_ln2_scale_grad.lod(), dense_ln2_scale_grad.offset());
  argument_outputs.push_back(ln2_scale_grad_dense_tensor_type);

  pir::Type ln2_bias_grad_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_ln2_bias_grad.dtype()), dense_ln2_bias_grad.dims(), dense_ln2_bias_grad.layout(), dense_ln2_bias_grad.lod(), dense_ln2_bias_grad.offset());
  argument_outputs.push_back(ln2_bias_grad_dense_tensor_type);

  pir::Type linear1_weight_grad_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_linear1_weight_grad.dtype()), dense_linear1_weight_grad.dims(), dense_linear1_weight_grad.layout(), dense_linear1_weight_grad.lod(), dense_linear1_weight_grad.offset());
  argument_outputs.push_back(linear1_weight_grad_dense_tensor_type);

  pir::Type linear1_bias_grad_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_linear1_bias_grad.dtype()), dense_linear1_bias_grad.dims(), dense_linear1_bias_grad.layout(), dense_linear1_bias_grad.lod(), dense_linear1_bias_grad.offset());
  argument_outputs.push_back(linear1_bias_grad_dense_tensor_type);

  pir::Type linear2_weight_grad_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_linear2_weight_grad.dtype()), dense_linear2_weight_grad.dims(), dense_linear2_weight_grad.layout(), dense_linear2_weight_grad.lod(), dense_linear2_weight_grad.offset());
  argument_outputs.push_back(linear2_weight_grad_dense_tensor_type);

  pir::Type linear2_bias_grad_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_linear2_bias_grad.dtype()), dense_linear2_bias_grad.dims(), dense_linear2_bias_grad.layout(), dense_linear2_bias_grad.lod(), dense_linear2_bias_grad.offset());
  argument_outputs.push_back(linear2_bias_grad_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void FusedFeedforwardGradOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value out_grad_, pir::Value x_, pir::Value linear1_weight_, pir::Value linear1_bias_, pir::Value linear2_weight_, pir::Value dropout1_mask_, pir::Value dropout2_mask_, pir::Value linear1_out_, pir::Value dropout1_out_, pir::Value dropout2_out_, pir::Value ln1_scale_, pir::Value ln1_bias_, pir::Value ln1_out_, pir::Value ln1_mean_, pir::Value ln1_variance_, pir::Value ln2_scale_, pir::Value ln2_bias_, pir::Value ln2_mean_, pir::Value ln2_variance_, pir::Value linear2_bias_, pir::AttributeMap attributes) {
  VLOG(4) << "Start build FusedFeedforwardGradOp";


  IR_ENFORCE(
      attributes.find("pre_layer_norm") != attributes.end(),
          "'pre_layer_norm' Attribute is expected for FusedFeedforwardGradOp. ");
  bool pre_layer_norm = attributes.at("pre_layer_norm").dyn_cast<pir::BoolAttribute>().data();

  IR_ENFORCE(
      attributes.find("ln1_epsilon") != attributes.end(),
          "'ln1_epsilon' Attribute is expected for FusedFeedforwardGradOp. ");
  float ln1_epsilon = attributes.at("ln1_epsilon").dyn_cast<pir::FloatAttribute>().data();

  IR_ENFORCE(
      attributes.find("ln2_epsilon") != attributes.end(),
          "'ln2_epsilon' Attribute is expected for FusedFeedforwardGradOp. ");
  float ln2_epsilon = attributes.at("ln2_epsilon").dyn_cast<pir::FloatAttribute>().data();

  IR_ENFORCE(
      attributes.find("act_method") != attributes.end(),
          "'act_method' Attribute is expected for FusedFeedforwardGradOp. ");
  std::string act_method = attributes.at("act_method").dyn_cast<pir::StrAttribute>().AsString();

  IR_ENFORCE(
      attributes.find("dropout1_prob") != attributes.end(),
          "'dropout1_prob' Attribute is expected for FusedFeedforwardGradOp. ");
  float dropout1_prob = attributes.at("dropout1_prob").dyn_cast<pir::FloatAttribute>().data();

  IR_ENFORCE(
      attributes.find("dropout2_prob") != attributes.end(),
          "'dropout2_prob' Attribute is expected for FusedFeedforwardGradOp. ");
  float dropout2_prob = attributes.at("dropout2_prob").dyn_cast<pir::FloatAttribute>().data();

  IR_ENFORCE(
      attributes.find("dropout1_implementation") != attributes.end(),
          "'dropout1_implementation' Attribute is expected for FusedFeedforwardGradOp. ");
  std::string dropout1_implementation = attributes.at("dropout1_implementation").dyn_cast<pir::StrAttribute>().AsString();

  IR_ENFORCE(
      attributes.find("dropout2_implementation") != attributes.end(),
          "'dropout2_implementation' Attribute is expected for FusedFeedforwardGradOp. ");
  std::string dropout2_implementation = attributes.at("dropout2_implementation").dyn_cast<pir::StrAttribute>().AsString();

  IR_ENFORCE(
      attributes.find("is_test") != attributes.end(),
          "'is_test' Attribute is expected for FusedFeedforwardGradOp. ");
  bool is_test = attributes.at("is_test").dyn_cast<pir::BoolAttribute>().data();

  IR_ENFORCE(
      attributes.find("dropout1_fix_seed") != attributes.end(),
          "'dropout1_fix_seed' Attribute is expected for FusedFeedforwardGradOp. ");
  bool dropout1_fix_seed = attributes.at("dropout1_fix_seed").dyn_cast<pir::BoolAttribute>().data();

  IR_ENFORCE(
      attributes.find("dropout2_fix_seed") != attributes.end(),
          "'dropout2_fix_seed' Attribute is expected for FusedFeedforwardGradOp. ");
  bool dropout2_fix_seed = attributes.at("dropout2_fix_seed").dyn_cast<pir::BoolAttribute>().data();

  IR_ENFORCE(
      attributes.find("dropout1_seed_val") != attributes.end(),
          "'dropout1_seed_val' Attribute is expected for FusedFeedforwardGradOp. ");
  int dropout1_seed_val = attributes.at("dropout1_seed_val").dyn_cast<pir::Int32Attribute>().data();

  IR_ENFORCE(
      attributes.find("dropout2_seed_val") != attributes.end(),
          "'dropout2_seed_val' Attribute is expected for FusedFeedforwardGradOp. ");
  int dropout2_seed_val = attributes.at("dropout2_seed_val").dyn_cast<pir::Int32Attribute>().data();

  IR_ENFORCE(
      attributes.find("add_residual") != attributes.end(),
          "'add_residual' Attribute is expected for FusedFeedforwardGradOp. ");
  bool add_residual = attributes.at("add_residual").dyn_cast<pir::BoolAttribute>().data();

  IR_ENFORCE(
      attributes.find("ring_id") != attributes.end(),
          "'ring_id' Attribute is expected for FusedFeedforwardGradOp. ");
  int ring_id = attributes.at("ring_id").dyn_cast<pir::Int32Attribute>().data();


  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {out_grad_, x_, linear1_weight_, linear1_bias_, linear2_weight_, dropout1_mask_, dropout2_mask_, linear1_out_, dropout1_out_, dropout2_out_, ln1_scale_, ln1_bias_, ln1_out_, ln1_mean_, ln1_variance_, ln2_scale_, ln2_bias_, ln2_mean_, ln2_variance_, linear2_bias_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_pre_layer_norm = pir::BoolAttribute::get(pir::IrContext::Instance(), pre_layer_norm);
  argument.AddAttribute("pre_layer_norm", attr_pre_layer_norm);
  pir::Attribute attr_ln1_epsilon = pir::FloatAttribute::get(pir::IrContext::Instance(), ln1_epsilon);
  argument.AddAttribute("ln1_epsilon", attr_ln1_epsilon);
  pir::Attribute attr_ln2_epsilon = pir::FloatAttribute::get(pir::IrContext::Instance(), ln2_epsilon);
  argument.AddAttribute("ln2_epsilon", attr_ln2_epsilon);
  pir::Attribute attr_act_method = pir::StrAttribute::get(pir::IrContext::Instance(), act_method);
  argument.AddAttribute("act_method", attr_act_method);
  pir::Attribute attr_dropout1_prob = pir::FloatAttribute::get(pir::IrContext::Instance(), dropout1_prob);
  argument.AddAttribute("dropout1_prob", attr_dropout1_prob);
  pir::Attribute attr_dropout2_prob = pir::FloatAttribute::get(pir::IrContext::Instance(), dropout2_prob);
  argument.AddAttribute("dropout2_prob", attr_dropout2_prob);
  pir::Attribute attr_dropout1_implementation = pir::StrAttribute::get(pir::IrContext::Instance(), dropout1_implementation);
  argument.AddAttribute("dropout1_implementation", attr_dropout1_implementation);
  pir::Attribute attr_dropout2_implementation = pir::StrAttribute::get(pir::IrContext::Instance(), dropout2_implementation);
  argument.AddAttribute("dropout2_implementation", attr_dropout2_implementation);
  pir::Attribute attr_is_test = pir::BoolAttribute::get(pir::IrContext::Instance(), is_test);
  argument.AddAttribute("is_test", attr_is_test);
  pir::Attribute attr_dropout1_fix_seed = pir::BoolAttribute::get(pir::IrContext::Instance(), dropout1_fix_seed);
  argument.AddAttribute("dropout1_fix_seed", attr_dropout1_fix_seed);
  pir::Attribute attr_dropout2_fix_seed = pir::BoolAttribute::get(pir::IrContext::Instance(), dropout2_fix_seed);
  argument.AddAttribute("dropout2_fix_seed", attr_dropout2_fix_seed);
  pir::Attribute attr_dropout1_seed_val = pir::Int32Attribute::get(pir::IrContext::Instance(), dropout1_seed_val);
  argument.AddAttribute("dropout1_seed_val", attr_dropout1_seed_val);
  pir::Attribute attr_dropout2_seed_val = pir::Int32Attribute::get(pir::IrContext::Instance(), dropout2_seed_val);
  argument.AddAttribute("dropout2_seed_val", attr_dropout2_seed_val);
  pir::Attribute attr_add_residual = pir::BoolAttribute::get(pir::IrContext::Instance(), add_residual);
  argument.AddAttribute("add_residual", attr_add_residual);
  pir::Attribute attr_ring_id = pir::Int32Attribute::get(pir::IrContext::Instance(), ring_id);
  argument.AddAttribute("ring_id", attr_ring_id);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType out_grad = out_grad_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)out_grad;
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;
  paddle::dialect::DenseTensorType linear1_weight = linear1_weight_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)linear1_weight;
  paddle::dialect::DenseTensorType linear2_weight = linear2_weight_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)linear2_weight;
  paddle::dialect::DenseTensorType dropout1_mask = dropout1_mask_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)dropout1_mask;
  paddle::dialect::DenseTensorType dropout2_mask = dropout2_mask_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)dropout2_mask;
  paddle::dialect::DenseTensorType linear1_out = linear1_out_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)linear1_out;
  paddle::dialect::DenseTensorType dropout1_out = dropout1_out_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)dropout1_out;

  VLOG(4) << "Builder construction  dense_out_grad";
  paddle::dialect::IrTensor ir_tensor_out_grad(paddle::dialect::TransToPhiDataType(out_grad.dtype()),
                                                      out_grad.dims(),
                                                      out_grad.data_layout(),
                                                      out_grad.lod(),
                                                      out_grad.offset());
  VLOG(4) << "Builder construction  meta_out_grad";
  paddle::dialect::IrMetaTensor meta_out_grad(&ir_tensor_out_grad);

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);

  VLOG(4) << "Builder construction  dense_linear1_weight";
  paddle::dialect::IrTensor ir_tensor_linear1_weight(paddle::dialect::TransToPhiDataType(linear1_weight.dtype()),
                                                      linear1_weight.dims(),
                                                      linear1_weight.data_layout(),
                                                      linear1_weight.lod(),
                                                      linear1_weight.offset());
  VLOG(4) << "Builder construction  meta_linear1_weight";
  paddle::dialect::IrMetaTensor meta_linear1_weight(&ir_tensor_linear1_weight);

  paddle::dialect::IrMetaTensor meta_linear1_bias;
  paddle::dialect::IrTensor ir_tensor_linear1_bias;
  if (linear1_bias_.impl() != nullptr) {
    paddle::dialect::DenseTensorType linear1_bias = linear1_bias_.type().dyn_cast<paddle::dialect::DenseTensorType>();
    VLOG(4) << "Builder construction  dense_linear1_bias";
    ir_tensor_linear1_bias = paddle::dialect::IrTensor(paddle::dialect::TransToPhiDataType(linear1_bias.dtype()),
                                                        linear1_bias.dims(),
                                                        linear1_bias.data_layout(),
                                                        linear1_bias.lod(),
                                                        linear1_bias.offset());
    VLOG(4) << "Builder construction  meta_linear1_bias";
    meta_linear1_bias = paddle::dialect::IrMetaTensor(&ir_tensor_linear1_bias);
  }


  VLOG(4) << "Builder construction  dense_linear2_weight";
  paddle::dialect::IrTensor ir_tensor_linear2_weight(paddle::dialect::TransToPhiDataType(linear2_weight.dtype()),
                                                      linear2_weight.dims(),
                                                      linear2_weight.data_layout(),
                                                      linear2_weight.lod(),
                                                      linear2_weight.offset());
  VLOG(4) << "Builder construction  meta_linear2_weight";
  paddle::dialect::IrMetaTensor meta_linear2_weight(&ir_tensor_linear2_weight);

  VLOG(4) << "Builder construction  dense_dropout1_mask";
  paddle::dialect::IrTensor ir_tensor_dropout1_mask(paddle::dialect::TransToPhiDataType(dropout1_mask.dtype()),
                                                      dropout1_mask.dims(),
                                                      dropout1_mask.data_layout(),
                                                      dropout1_mask.lod(),
                                                      dropout1_mask.offset());
  VLOG(4) << "Builder construction  meta_dropout1_mask";
  paddle::dialect::IrMetaTensor meta_dropout1_mask(&ir_tensor_dropout1_mask);

  VLOG(4) << "Builder construction  dense_dropout2_mask";
  paddle::dialect::IrTensor ir_tensor_dropout2_mask(paddle::dialect::TransToPhiDataType(dropout2_mask.dtype()),
                                                      dropout2_mask.dims(),
                                                      dropout2_mask.data_layout(),
                                                      dropout2_mask.lod(),
                                                      dropout2_mask.offset());
  VLOG(4) << "Builder construction  meta_dropout2_mask";
  paddle::dialect::IrMetaTensor meta_dropout2_mask(&ir_tensor_dropout2_mask);

  VLOG(4) << "Builder construction  dense_linear1_out";
  paddle::dialect::IrTensor ir_tensor_linear1_out(paddle::dialect::TransToPhiDataType(linear1_out.dtype()),
                                                      linear1_out.dims(),
                                                      linear1_out.data_layout(),
                                                      linear1_out.lod(),
                                                      linear1_out.offset());
  VLOG(4) << "Builder construction  meta_linear1_out";
  paddle::dialect::IrMetaTensor meta_linear1_out(&ir_tensor_linear1_out);

  VLOG(4) << "Builder construction  dense_dropout1_out";
  paddle::dialect::IrTensor ir_tensor_dropout1_out(paddle::dialect::TransToPhiDataType(dropout1_out.dtype()),
                                                      dropout1_out.dims(),
                                                      dropout1_out.data_layout(),
                                                      dropout1_out.lod(),
                                                      dropout1_out.offset());
  VLOG(4) << "Builder construction  meta_dropout1_out";
  paddle::dialect::IrMetaTensor meta_dropout1_out(&ir_tensor_dropout1_out);

  paddle::dialect::IrMetaTensor meta_dropout2_out;
  paddle::dialect::IrTensor ir_tensor_dropout2_out;
  if (dropout2_out_.impl() != nullptr) {
    paddle::dialect::DenseTensorType dropout2_out = dropout2_out_.type().dyn_cast<paddle::dialect::DenseTensorType>();
    VLOG(4) << "Builder construction  dense_dropout2_out";
    ir_tensor_dropout2_out = paddle::dialect::IrTensor(paddle::dialect::TransToPhiDataType(dropout2_out.dtype()),
                                                        dropout2_out.dims(),
                                                        dropout2_out.data_layout(),
                                                        dropout2_out.lod(),
                                                        dropout2_out.offset());
    VLOG(4) << "Builder construction  meta_dropout2_out";
    meta_dropout2_out = paddle::dialect::IrMetaTensor(&ir_tensor_dropout2_out);
  }


  paddle::dialect::IrMetaTensor meta_ln1_scale;
  paddle::dialect::IrTensor ir_tensor_ln1_scale;
  if (ln1_scale_.impl() != nullptr) {
    paddle::dialect::DenseTensorType ln1_scale = ln1_scale_.type().dyn_cast<paddle::dialect::DenseTensorType>();
    VLOG(4) << "Builder construction  dense_ln1_scale";
    ir_tensor_ln1_scale = paddle::dialect::IrTensor(paddle::dialect::TransToPhiDataType(ln1_scale.dtype()),
                                                        ln1_scale.dims(),
                                                        ln1_scale.data_layout(),
                                                        ln1_scale.lod(),
                                                        ln1_scale.offset());
    VLOG(4) << "Builder construction  meta_ln1_scale";
    meta_ln1_scale = paddle::dialect::IrMetaTensor(&ir_tensor_ln1_scale);
  }


  paddle::dialect::IrMetaTensor meta_ln1_bias;
  paddle::dialect::IrTensor ir_tensor_ln1_bias;
  if (ln1_bias_.impl() != nullptr) {
    paddle::dialect::DenseTensorType ln1_bias = ln1_bias_.type().dyn_cast<paddle::dialect::DenseTensorType>();
    VLOG(4) << "Builder construction  dense_ln1_bias";
    ir_tensor_ln1_bias = paddle::dialect::IrTensor(paddle::dialect::TransToPhiDataType(ln1_bias.dtype()),
                                                        ln1_bias.dims(),
                                                        ln1_bias.data_layout(),
                                                        ln1_bias.lod(),
                                                        ln1_bias.offset());
    VLOG(4) << "Builder construction  meta_ln1_bias";
    meta_ln1_bias = paddle::dialect::IrMetaTensor(&ir_tensor_ln1_bias);
  }


  paddle::dialect::IrMetaTensor meta_ln1_out;
  paddle::dialect::IrTensor ir_tensor_ln1_out;
  if (ln1_out_.impl() != nullptr) {
    paddle::dialect::DenseTensorType ln1_out = ln1_out_.type().dyn_cast<paddle::dialect::DenseTensorType>();
    VLOG(4) << "Builder construction  dense_ln1_out";
    ir_tensor_ln1_out = paddle::dialect::IrTensor(paddle::dialect::TransToPhiDataType(ln1_out.dtype()),
                                                        ln1_out.dims(),
                                                        ln1_out.data_layout(),
                                                        ln1_out.lod(),
                                                        ln1_out.offset());
    VLOG(4) << "Builder construction  meta_ln1_out";
    meta_ln1_out = paddle::dialect::IrMetaTensor(&ir_tensor_ln1_out);
  }


  paddle::dialect::IrMetaTensor meta_ln1_mean;
  paddle::dialect::IrTensor ir_tensor_ln1_mean;
  if (ln1_mean_.impl() != nullptr) {
    paddle::dialect::DenseTensorType ln1_mean = ln1_mean_.type().dyn_cast<paddle::dialect::DenseTensorType>();
    VLOG(4) << "Builder construction  dense_ln1_mean";
    ir_tensor_ln1_mean = paddle::dialect::IrTensor(paddle::dialect::TransToPhiDataType(ln1_mean.dtype()),
                                                        ln1_mean.dims(),
                                                        ln1_mean.data_layout(),
                                                        ln1_mean.lod(),
                                                        ln1_mean.offset());
    VLOG(4) << "Builder construction  meta_ln1_mean";
    meta_ln1_mean = paddle::dialect::IrMetaTensor(&ir_tensor_ln1_mean);
  }


  paddle::dialect::IrMetaTensor meta_ln1_variance;
  paddle::dialect::IrTensor ir_tensor_ln1_variance;
  if (ln1_variance_.impl() != nullptr) {
    paddle::dialect::DenseTensorType ln1_variance = ln1_variance_.type().dyn_cast<paddle::dialect::DenseTensorType>();
    VLOG(4) << "Builder construction  dense_ln1_variance";
    ir_tensor_ln1_variance = paddle::dialect::IrTensor(paddle::dialect::TransToPhiDataType(ln1_variance.dtype()),
                                                        ln1_variance.dims(),
                                                        ln1_variance.data_layout(),
                                                        ln1_variance.lod(),
                                                        ln1_variance.offset());
    VLOG(4) << "Builder construction  meta_ln1_variance";
    meta_ln1_variance = paddle::dialect::IrMetaTensor(&ir_tensor_ln1_variance);
  }


  paddle::dialect::IrMetaTensor meta_ln2_scale;
  paddle::dialect::IrTensor ir_tensor_ln2_scale;
  if (ln2_scale_.impl() != nullptr) {
    paddle::dialect::DenseTensorType ln2_scale = ln2_scale_.type().dyn_cast<paddle::dialect::DenseTensorType>();
    VLOG(4) << "Builder construction  dense_ln2_scale";
    ir_tensor_ln2_scale = paddle::dialect::IrTensor(paddle::dialect::TransToPhiDataType(ln2_scale.dtype()),
                                                        ln2_scale.dims(),
                                                        ln2_scale.data_layout(),
                                                        ln2_scale.lod(),
                                                        ln2_scale.offset());
    VLOG(4) << "Builder construction  meta_ln2_scale";
    meta_ln2_scale = paddle::dialect::IrMetaTensor(&ir_tensor_ln2_scale);
  }


  paddle::dialect::IrMetaTensor meta_ln2_bias;
  paddle::dialect::IrTensor ir_tensor_ln2_bias;
  if (ln2_bias_.impl() != nullptr) {
    paddle::dialect::DenseTensorType ln2_bias = ln2_bias_.type().dyn_cast<paddle::dialect::DenseTensorType>();
    VLOG(4) << "Builder construction  dense_ln2_bias";
    ir_tensor_ln2_bias = paddle::dialect::IrTensor(paddle::dialect::TransToPhiDataType(ln2_bias.dtype()),
                                                        ln2_bias.dims(),
                                                        ln2_bias.data_layout(),
                                                        ln2_bias.lod(),
                                                        ln2_bias.offset());
    VLOG(4) << "Builder construction  meta_ln2_bias";
    meta_ln2_bias = paddle::dialect::IrMetaTensor(&ir_tensor_ln2_bias);
  }


  paddle::dialect::IrMetaTensor meta_ln2_mean;
  paddle::dialect::IrTensor ir_tensor_ln2_mean;
  if (ln2_mean_.impl() != nullptr) {
    paddle::dialect::DenseTensorType ln2_mean = ln2_mean_.type().dyn_cast<paddle::dialect::DenseTensorType>();
    VLOG(4) << "Builder construction  dense_ln2_mean";
    ir_tensor_ln2_mean = paddle::dialect::IrTensor(paddle::dialect::TransToPhiDataType(ln2_mean.dtype()),
                                                        ln2_mean.dims(),
                                                        ln2_mean.data_layout(),
                                                        ln2_mean.lod(),
                                                        ln2_mean.offset());
    VLOG(4) << "Builder construction  meta_ln2_mean";
    meta_ln2_mean = paddle::dialect::IrMetaTensor(&ir_tensor_ln2_mean);
  }


  paddle::dialect::IrMetaTensor meta_ln2_variance;
  paddle::dialect::IrTensor ir_tensor_ln2_variance;
  if (ln2_variance_.impl() != nullptr) {
    paddle::dialect::DenseTensorType ln2_variance = ln2_variance_.type().dyn_cast<paddle::dialect::DenseTensorType>();
    VLOG(4) << "Builder construction  dense_ln2_variance";
    ir_tensor_ln2_variance = paddle::dialect::IrTensor(paddle::dialect::TransToPhiDataType(ln2_variance.dtype()),
                                                        ln2_variance.dims(),
                                                        ln2_variance.data_layout(),
                                                        ln2_variance.lod(),
                                                        ln2_variance.offset());
    VLOG(4) << "Builder construction  meta_ln2_variance";
    meta_ln2_variance = paddle::dialect::IrMetaTensor(&ir_tensor_ln2_variance);
  }


  paddle::dialect::IrMetaTensor meta_linear2_bias;
  paddle::dialect::IrTensor ir_tensor_linear2_bias;
  if (linear2_bias_.impl() != nullptr) {
    paddle::dialect::DenseTensorType linear2_bias = linear2_bias_.type().dyn_cast<paddle::dialect::DenseTensorType>();
    VLOG(4) << "Builder construction  dense_linear2_bias";
    ir_tensor_linear2_bias = paddle::dialect::IrTensor(paddle::dialect::TransToPhiDataType(linear2_bias.dtype()),
                                                        linear2_bias.dims(),
                                                        linear2_bias.data_layout(),
                                                        linear2_bias.lod(),
                                                        linear2_bias.offset());
    VLOG(4) << "Builder construction  meta_linear2_bias";
    meta_linear2_bias = paddle::dialect::IrMetaTensor(&ir_tensor_linear2_bias);
  }

  paddle::dialect::IrTensor dense_x_grad;
  paddle::dialect::IrMetaTensor meta_x_grad(&dense_x_grad);
  paddle::dialect::IrTensor dense_ln1_scale_grad;
  paddle::dialect::IrMetaTensor meta_ln1_scale_grad(&dense_ln1_scale_grad);
  paddle::dialect::IrTensor dense_ln1_bias_grad;
  paddle::dialect::IrMetaTensor meta_ln1_bias_grad(&dense_ln1_bias_grad);
  paddle::dialect::IrTensor dense_ln2_scale_grad;
  paddle::dialect::IrMetaTensor meta_ln2_scale_grad(&dense_ln2_scale_grad);
  paddle::dialect::IrTensor dense_ln2_bias_grad;
  paddle::dialect::IrMetaTensor meta_ln2_bias_grad(&dense_ln2_bias_grad);
  paddle::dialect::IrTensor dense_linear1_weight_grad;
  paddle::dialect::IrMetaTensor meta_linear1_weight_grad(&dense_linear1_weight_grad);
  paddle::dialect::IrTensor dense_linear1_bias_grad;
  paddle::dialect::IrMetaTensor meta_linear1_bias_grad(&dense_linear1_bias_grad);
  paddle::dialect::IrTensor dense_linear2_weight_grad;
  paddle::dialect::IrMetaTensor meta_linear2_weight_grad(&dense_linear2_weight_grad);
  paddle::dialect::IrTensor dense_linear2_bias_grad;
  paddle::dialect::IrMetaTensor meta_linear2_bias_grad(&dense_linear2_bias_grad);

  phi::FusedFeedForwardGradInferMeta(meta_out_grad, meta_x, meta_linear1_weight, meta_linear1_bias, meta_linear2_weight, meta_dropout1_mask, meta_dropout2_mask, meta_linear1_out, meta_dropout1_out, meta_dropout2_out, meta_ln1_scale, meta_ln1_bias, meta_ln1_out, meta_ln1_mean, meta_ln1_variance, meta_ln2_scale, meta_ln2_bias, meta_ln2_mean, meta_ln2_variance, meta_linear2_bias, pre_layer_norm, ln1_epsilon, ln2_epsilon, act_method, dropout1_prob, dropout2_prob, dropout1_implementation, dropout2_implementation, is_test, dropout1_fix_seed, dropout2_fix_seed, dropout1_seed_val, dropout2_seed_val, add_residual, ring_id, &meta_x_grad, &meta_ln1_scale_grad, &meta_ln1_bias_grad, &meta_ln2_scale_grad, &meta_ln2_bias_grad, &meta_linear1_weight_grad, &meta_linear1_bias_grad, &meta_linear2_weight_grad, &meta_linear2_bias_grad);

  std::vector<pir::Type> argument_outputs;
  pir::Type x_grad_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_x_grad.dtype()), dense_x_grad.dims(), dense_x_grad.layout(), dense_x_grad.lod(), dense_x_grad.offset());
  argument_outputs.push_back(x_grad_dense_tensor_type);

  pir::Type ln1_scale_grad_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_ln1_scale_grad.dtype()), dense_ln1_scale_grad.dims(), dense_ln1_scale_grad.layout(), dense_ln1_scale_grad.lod(), dense_ln1_scale_grad.offset());
  argument_outputs.push_back(ln1_scale_grad_dense_tensor_type);

  pir::Type ln1_bias_grad_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_ln1_bias_grad.dtype()), dense_ln1_bias_grad.dims(), dense_ln1_bias_grad.layout(), dense_ln1_bias_grad.lod(), dense_ln1_bias_grad.offset());
  argument_outputs.push_back(ln1_bias_grad_dense_tensor_type);

  pir::Type ln2_scale_grad_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_ln2_scale_grad.dtype()), dense_ln2_scale_grad.dims(), dense_ln2_scale_grad.layout(), dense_ln2_scale_grad.lod(), dense_ln2_scale_grad.offset());
  argument_outputs.push_back(ln2_scale_grad_dense_tensor_type);

  pir::Type ln2_bias_grad_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_ln2_bias_grad.dtype()), dense_ln2_bias_grad.dims(), dense_ln2_bias_grad.layout(), dense_ln2_bias_grad.lod(), dense_ln2_bias_grad.offset());
  argument_outputs.push_back(ln2_bias_grad_dense_tensor_type);

  pir::Type linear1_weight_grad_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_linear1_weight_grad.dtype()), dense_linear1_weight_grad.dims(), dense_linear1_weight_grad.layout(), dense_linear1_weight_grad.lod(), dense_linear1_weight_grad.offset());
  argument_outputs.push_back(linear1_weight_grad_dense_tensor_type);

  pir::Type linear1_bias_grad_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_linear1_bias_grad.dtype()), dense_linear1_bias_grad.dims(), dense_linear1_bias_grad.layout(), dense_linear1_bias_grad.lod(), dense_linear1_bias_grad.offset());
  argument_outputs.push_back(linear1_bias_grad_dense_tensor_type);

  pir::Type linear2_weight_grad_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_linear2_weight_grad.dtype()), dense_linear2_weight_grad.dims(), dense_linear2_weight_grad.layout(), dense_linear2_weight_grad.lod(), dense_linear2_weight_grad.offset());
  argument_outputs.push_back(linear2_weight_grad_dense_tensor_type);

  pir::Type linear2_bias_grad_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_linear2_bias_grad.dtype()), dense_linear2_bias_grad.dims(), dense_linear2_bias_grad.layout(), dense_linear2_bias_grad.lod(), dense_linear2_bias_grad.offset());
  argument_outputs.push_back(linear2_bias_grad_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void FusedFeedforwardGradOp::VerifySig() {}

void FusedFeedforwardGradOp::InferMeta( phi::InferMetaContext *infer_meta ) {
  auto fn = PD_INFER_META(phi::FusedFeedForwardGradInferMeta);
  fn(infer_meta);
}

phi::DataType FusedFeedforwardGradOp::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: FusedFeedforwardGradOp";
  


  return expected_kernel_dtype;
}

OpInfoTuple FusedSoftmaxMaskUpperTriangleGradOp::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("Out", "paddle::dialect::DenseTensorType", false, false, false, false), paddle::dialect::OpInputInfo("Out_grad", "paddle::dialect::DenseTensorType", false, false, false, false) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = {  };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("X_grad", "paddle::dialect::DenseTensorType", false, false) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("UnchangedInferMeta", {"Out_grad"}, "fused_softmax_mask_upper_triangle_grad", {"Out", "Out_grad"}, {}, {}, {}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "fused_softmax_mask_upper_triangle_grad");
}

void FusedSoftmaxMaskUpperTriangleGradOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value Out_, pir::Value Out_grad_) {
  VLOG(4) << "Start build FusedSoftmaxMaskUpperTriangleGradOp";



  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {Out_, Out_grad_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType Out = Out_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)Out;
  paddle::dialect::DenseTensorType Out_grad = Out_grad_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)Out_grad;

  VLOG(4) << "Builder construction  dense_Out_grad";
  paddle::dialect::IrTensor ir_tensor_Out_grad(paddle::dialect::TransToPhiDataType(Out_grad.dtype()),
                                                      Out_grad.dims(),
                                                      Out_grad.data_layout(),
                                                      Out_grad.lod(),
                                                      Out_grad.offset());
  VLOG(4) << "Builder construction  meta_Out_grad";
  paddle::dialect::IrMetaTensor meta_Out_grad(&ir_tensor_Out_grad);
  paddle::dialect::IrTensor dense_X_grad;
  paddle::dialect::IrMetaTensor meta_X_grad(&dense_X_grad);

  phi::UnchangedInferMeta(meta_Out_grad, &meta_X_grad);

  std::vector<pir::Type> argument_outputs;
  pir::Type X_grad_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_X_grad.dtype()), dense_X_grad.dims(), dense_X_grad.layout(), dense_X_grad.lod(), dense_X_grad.offset());
  argument_outputs.push_back(X_grad_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void FusedSoftmaxMaskUpperTriangleGradOp::VerifySig() {}

void FusedSoftmaxMaskUpperTriangleGradOp::InferMeta( phi::InferMetaContext *infer_meta ) {
  auto fn = PD_INFER_META(phi::UnchangedInferMeta);
  fn(infer_meta);
}

phi::DataType FusedSoftmaxMaskUpperTriangleGradOp::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: FusedSoftmaxMaskUpperTriangleGradOp";
  


  return expected_kernel_dtype;
}

OpInfoTuple HardswishGradOp::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("x", "paddle::dialect::DenseTensorType", false, false, false, false), paddle::dialect::OpInputInfo("out_grad", "paddle::dialect::DenseTensorType", false, false, false, false) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = {  };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("x_grad", "paddle::dialect::DenseTensorType", false, false) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("UnchangedInferMeta", {"x"}, "hardswish_grad", {"x", "out_grad"}, {}, {}, {}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "hardswish_grad");
}

void HardswishGradOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, pir::Value out_grad_) {
  VLOG(4) << "Start build HardswishGradOp";



  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_, out_grad_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;
  paddle::dialect::DenseTensorType out_grad = out_grad_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)out_grad;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);
  paddle::dialect::IrTensor dense_x_grad;
  paddle::dialect::IrMetaTensor meta_x_grad(&dense_x_grad);

  phi::UnchangedInferMeta(meta_x, &meta_x_grad);

  std::vector<pir::Type> argument_outputs;
  pir::Type x_grad_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_x_grad.dtype()), dense_x_grad.dims(), dense_x_grad.layout(), dense_x_grad.lod(), dense_x_grad.offset());
  argument_outputs.push_back(x_grad_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void HardswishGradOp::VerifySig() {}

void HardswishGradOp::InferMeta( phi::InferMetaContext *infer_meta ) {
  auto fn = PD_INFER_META(phi::UnchangedInferMeta);
  fn(infer_meta);
}

phi::DataType HardswishGradOp::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: HardswishGradOp";
  


  return expected_kernel_dtype;
}

OpInfoTuple HardswishGrad_Op::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("x", "paddle::dialect::DenseTensorType", false, false, false, false), paddle::dialect::OpInputInfo("out_grad", "paddle::dialect::DenseTensorType", false, false, false, false) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = {  };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("x_grad", "paddle::dialect::DenseTensorType", false, false) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("UnchangedInferMeta", {"x"}, "hardswish_grad", {"x", "out_grad"}, {}, {}, {{"x_grad", "out_grad"}}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "hardswish_grad");
}

void HardswishGrad_Op::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, pir::Value out_grad_) {
  VLOG(4) << "Start build HardswishGrad_Op";



  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_, out_grad_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;
  paddle::dialect::DenseTensorType out_grad = out_grad_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)out_grad;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);
  paddle::dialect::IrTensor dense_x_grad;
  paddle::dialect::IrMetaTensor meta_x_grad(&dense_x_grad);

  phi::UnchangedInferMeta(meta_x, &meta_x_grad);

  std::vector<pir::Type> argument_outputs;
  pir::Type x_grad_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_x_grad.dtype()), dense_x_grad.dims(), dense_x_grad.layout(), dense_x_grad.lod(), dense_x_grad.offset());
  argument_outputs.push_back(x_grad_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void HardswishGrad_Op::VerifySig() {}

void HardswishGrad_Op::InferMeta( phi::InferMetaContext *infer_meta ) {
  auto fn = PD_INFER_META(phi::UnchangedInferMeta);
  fn(infer_meta);
}

phi::DataType HardswishGrad_Op::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: HardswishGrad_Op";
  


  return expected_kernel_dtype;
}

const char *HsigmoidLossGradOp::attributes_name[2] = { "num_classes", "is_sparse" };

OpInfoTuple HsigmoidLossGradOp::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("x", "paddle::dialect::DenseTensorType", false, false, false, false), paddle::dialect::OpInputInfo("w", "paddle::dialect::DenseTensorType", false, false, false, false), paddle::dialect::OpInputInfo("label", "paddle::dialect::DenseTensorType", false, false, false, false), paddle::dialect::OpInputInfo("path", "paddle::dialect::DenseTensorType", true, false, false, false), paddle::dialect::OpInputInfo("code", "paddle::dialect::DenseTensorType", true, false, false, false), paddle::dialect::OpInputInfo("bias", "paddle::dialect::DenseTensorType", true, false, false, false), paddle::dialect::OpInputInfo("pre_out", "paddle::dialect::DenseTensorType", false, false, false, false), paddle::dialect::OpInputInfo("out_grad", "paddle::dialect::DenseTensorType", false, false, false, false) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = { paddle::dialect::OpAttributeInfo("num_classes", "pir::Int32Attribute", ""), paddle::dialect::OpAttributeInfo("is_sparse", "pir::BoolAttribute", "") };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("x_grad", "paddle::dialect::DenseTensorType", false, false), paddle::dialect::OpOutputInfo("w_grad", "paddle::dialect::DenseTensorType", false, false), paddle::dialect::OpOutputInfo("bias_grad", "paddle::dialect::DenseTensorType", false, false) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("GeneralTernaryGradInferMeta", {"x", "w", "bias"}, "hsigmoid_loss_grad", {"x", "w", "label", "path", "code", "bias", "pre_out", "out_grad", "num_classes", "is_sparse"}, {}, {}, {}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "hsigmoid_loss_grad");
}

void HsigmoidLossGradOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, pir::Value w_, pir::Value label_, pir::Value path_, pir::Value code_, pir::Value bias_, pir::Value pre_out_, pir::Value out_grad_, int num_classes, bool is_sparse) {
  VLOG(4) << "Start build HsigmoidLossGradOp";



  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_, w_, label_, path_, code_, bias_, pre_out_, out_grad_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_num_classes = pir::Int32Attribute::get(pir::IrContext::Instance(), num_classes);
  argument.AddAttribute("num_classes", attr_num_classes);
  pir::Attribute attr_is_sparse = pir::BoolAttribute::get(pir::IrContext::Instance(), is_sparse);
  argument.AddAttribute("is_sparse", attr_is_sparse);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;
  paddle::dialect::DenseTensorType w = w_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)w;
  paddle::dialect::DenseTensorType label = label_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)label;
  paddle::dialect::DenseTensorType pre_out = pre_out_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)pre_out;
  paddle::dialect::DenseTensorType out_grad = out_grad_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)out_grad;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);

  VLOG(4) << "Builder construction  dense_w";
  paddle::dialect::IrTensor ir_tensor_w(paddle::dialect::TransToPhiDataType(w.dtype()),
                                                      w.dims(),
                                                      w.data_layout(),
                                                      w.lod(),
                                                      w.offset());
  VLOG(4) << "Builder construction  meta_w";
  paddle::dialect::IrMetaTensor meta_w(&ir_tensor_w);

  paddle::dialect::IrMetaTensor meta_bias;
  paddle::dialect::IrTensor ir_tensor_bias;
  if (bias_.impl() != nullptr) {
    paddle::dialect::DenseTensorType bias = bias_.type().dyn_cast<paddle::dialect::DenseTensorType>();
    VLOG(4) << "Builder construction  dense_bias";
    ir_tensor_bias = paddle::dialect::IrTensor(paddle::dialect::TransToPhiDataType(bias.dtype()),
                                                        bias.dims(),
                                                        bias.data_layout(),
                                                        bias.lod(),
                                                        bias.offset());
    VLOG(4) << "Builder construction  meta_bias";
    meta_bias = paddle::dialect::IrMetaTensor(&ir_tensor_bias);
  }

  paddle::dialect::IrTensor dense_x_grad;
  paddle::dialect::IrMetaTensor meta_x_grad(&dense_x_grad);
  paddle::dialect::IrTensor dense_w_grad;
  paddle::dialect::IrMetaTensor meta_w_grad(&dense_w_grad);
  paddle::dialect::IrTensor dense_bias_grad;
  paddle::dialect::IrMetaTensor meta_bias_grad(&dense_bias_grad);

  phi::GeneralTernaryGradInferMeta(meta_x, meta_w, meta_bias, &meta_x_grad, &meta_w_grad, &meta_bias_grad);

  std::vector<pir::Type> argument_outputs;
  pir::Type x_grad_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_x_grad.dtype()), dense_x_grad.dims(), dense_x_grad.layout(), dense_x_grad.lod(), dense_x_grad.offset());
  argument_outputs.push_back(x_grad_dense_tensor_type);

  pir::Type w_grad_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_w_grad.dtype()), dense_w_grad.dims(), dense_w_grad.layout(), dense_w_grad.lod(), dense_w_grad.offset());
  argument_outputs.push_back(w_grad_dense_tensor_type);

  pir::Type bias_grad_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_bias_grad.dtype()), dense_bias_grad.dims(), dense_bias_grad.layout(), dense_bias_grad.lod(), dense_bias_grad.offset());
  argument_outputs.push_back(bias_grad_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void HsigmoidLossGradOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, pir::Value w_, pir::Value label_, pir::Value path_, pir::Value code_, pir::Value bias_, pir::Value pre_out_, pir::Value out_grad_, pir::AttributeMap attributes) {
  VLOG(4) << "Start build HsigmoidLossGradOp";


  IR_ENFORCE(
      attributes.find("num_classes") != attributes.end(),
          "'num_classes' Attribute is expected for HsigmoidLossGradOp. ");
  int num_classes = attributes.at("num_classes").dyn_cast<pir::Int32Attribute>().data();

  IR_ENFORCE(
      attributes.find("is_sparse") != attributes.end(),
          "'is_sparse' Attribute is expected for HsigmoidLossGradOp. ");
  bool is_sparse = attributes.at("is_sparse").dyn_cast<pir::BoolAttribute>().data();


  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_, w_, label_, path_, code_, bias_, pre_out_, out_grad_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_num_classes = pir::Int32Attribute::get(pir::IrContext::Instance(), num_classes);
  argument.AddAttribute("num_classes", attr_num_classes);
  pir::Attribute attr_is_sparse = pir::BoolAttribute::get(pir::IrContext::Instance(), is_sparse);
  argument.AddAttribute("is_sparse", attr_is_sparse);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;
  paddle::dialect::DenseTensorType w = w_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)w;
  paddle::dialect::DenseTensorType label = label_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)label;
  paddle::dialect::DenseTensorType pre_out = pre_out_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)pre_out;
  paddle::dialect::DenseTensorType out_grad = out_grad_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)out_grad;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);

  VLOG(4) << "Builder construction  dense_w";
  paddle::dialect::IrTensor ir_tensor_w(paddle::dialect::TransToPhiDataType(w.dtype()),
                                                      w.dims(),
                                                      w.data_layout(),
                                                      w.lod(),
                                                      w.offset());
  VLOG(4) << "Builder construction  meta_w";
  paddle::dialect::IrMetaTensor meta_w(&ir_tensor_w);

  paddle::dialect::IrMetaTensor meta_bias;
  paddle::dialect::IrTensor ir_tensor_bias;
  if (bias_.impl() != nullptr) {
    paddle::dialect::DenseTensorType bias = bias_.type().dyn_cast<paddle::dialect::DenseTensorType>();
    VLOG(4) << "Builder construction  dense_bias";
    ir_tensor_bias = paddle::dialect::IrTensor(paddle::dialect::TransToPhiDataType(bias.dtype()),
                                                        bias.dims(),
                                                        bias.data_layout(),
                                                        bias.lod(),
                                                        bias.offset());
    VLOG(4) << "Builder construction  meta_bias";
    meta_bias = paddle::dialect::IrMetaTensor(&ir_tensor_bias);
  }

  paddle::dialect::IrTensor dense_x_grad;
  paddle::dialect::IrMetaTensor meta_x_grad(&dense_x_grad);
  paddle::dialect::IrTensor dense_w_grad;
  paddle::dialect::IrMetaTensor meta_w_grad(&dense_w_grad);
  paddle::dialect::IrTensor dense_bias_grad;
  paddle::dialect::IrMetaTensor meta_bias_grad(&dense_bias_grad);

  phi::GeneralTernaryGradInferMeta(meta_x, meta_w, meta_bias, &meta_x_grad, &meta_w_grad, &meta_bias_grad);

  std::vector<pir::Type> argument_outputs;
  pir::Type x_grad_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_x_grad.dtype()), dense_x_grad.dims(), dense_x_grad.layout(), dense_x_grad.lod(), dense_x_grad.offset());
  argument_outputs.push_back(x_grad_dense_tensor_type);

  pir::Type w_grad_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_w_grad.dtype()), dense_w_grad.dims(), dense_w_grad.layout(), dense_w_grad.lod(), dense_w_grad.offset());
  argument_outputs.push_back(w_grad_dense_tensor_type);

  pir::Type bias_grad_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_bias_grad.dtype()), dense_bias_grad.dims(), dense_bias_grad.layout(), dense_bias_grad.lod(), dense_bias_grad.offset());
  argument_outputs.push_back(bias_grad_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void HsigmoidLossGradOp::VerifySig() {}

void HsigmoidLossGradOp::InferMeta( phi::InferMetaContext *infer_meta ) {
  auto fn = PD_INFER_META(phi::GeneralTernaryGradInferMeta);
  fn(infer_meta);
}

phi::DataType HsigmoidLossGradOp::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: HsigmoidLossGradOp";
  


  return expected_kernel_dtype;
}

const char *LogsumexpGradOp::attributes_name[3] = { "axis", "keepdim", "reduce_all" };

OpInfoTuple LogsumexpGradOp::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("x", "paddle::dialect::DenseTensorType", false, false, false, false), paddle::dialect::OpInputInfo("out", "paddle::dialect::DenseTensorType", false, false, false, false), paddle::dialect::OpInputInfo("out_grad", "paddle::dialect::DenseTensorType", false, false, false, false) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = { paddle::dialect::OpAttributeInfo("axis", "pir::ArrayAttribute<pir::Int64Attribute>", ""), paddle::dialect::OpAttributeInfo("keepdim", "pir::BoolAttribute", ""), paddle::dialect::OpAttributeInfo("reduce_all", "pir::BoolAttribute", "") };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("x_grad", "paddle::dialect::DenseTensorType", false, false) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("UnchangedInferMeta", {"x"}, "logsumexp_grad", {"x", "out", "out_grad", "axis", "keepdim", "reduce_all"}, {}, {}, {}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "logsumexp_grad");
}

void LogsumexpGradOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, pir::Value out_, pir::Value out_grad_, const std::vector<int64_t>& axis, bool keepdim, bool reduce_all) {
  VLOG(4) << "Start build LogsumexpGradOp";



  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_, out_, out_grad_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  std::vector<pir::Attribute> vec_axis;
  for (size_t i = 0; i < static_cast<size_t>(axis.size()); i++) {
      pir::Attribute attr_axis = pir::Int64Attribute::get(pir::IrContext::Instance(), axis[i]);

    vec_axis.push_back(attr_axis);
  }
  pir::Attribute attr_axis = pir::ArrayAttribute::get(pir::IrContext::Instance(), vec_axis);
  argument.AddAttribute("axis", attr_axis);
  pir::Attribute attr_keepdim = pir::BoolAttribute::get(pir::IrContext::Instance(), keepdim);
  argument.AddAttribute("keepdim", attr_keepdim);
  pir::Attribute attr_reduce_all = pir::BoolAttribute::get(pir::IrContext::Instance(), reduce_all);
  argument.AddAttribute("reduce_all", attr_reduce_all);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;
  paddle::dialect::DenseTensorType out = out_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)out;
  paddle::dialect::DenseTensorType out_grad = out_grad_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)out_grad;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);
  paddle::dialect::IrTensor dense_x_grad;
  paddle::dialect::IrMetaTensor meta_x_grad(&dense_x_grad);

  phi::UnchangedInferMeta(meta_x, &meta_x_grad);

  std::vector<pir::Type> argument_outputs;
  pir::Type x_grad_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_x_grad.dtype()), dense_x_grad.dims(), dense_x_grad.layout(), dense_x_grad.lod(), dense_x_grad.offset());
  argument_outputs.push_back(x_grad_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void LogsumexpGradOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, pir::Value out_, pir::Value out_grad_, pir::AttributeMap attributes) {
  VLOG(4) << "Start build LogsumexpGradOp";


  IR_ENFORCE(
      attributes.find("axis") != attributes.end(),
          "'axis' Attribute is expected for LogsumexpGradOp. ");
  std::vector<int64_t> axis;
  for (size_t i = 0; i < attributes.at("axis").dyn_cast<pir::ArrayAttribute>().size(); i++) {
    axis.push_back(attributes.at("axis").dyn_cast<pir::ArrayAttribute>().at(i).dyn_cast<pir::Int64Attribute>().data());
  }

  IR_ENFORCE(
      attributes.find("keepdim") != attributes.end(),
          "'keepdim' Attribute is expected for LogsumexpGradOp. ");
  bool keepdim = attributes.at("keepdim").dyn_cast<pir::BoolAttribute>().data();

  IR_ENFORCE(
      attributes.find("reduce_all") != attributes.end(),
          "'reduce_all' Attribute is expected for LogsumexpGradOp. ");
  bool reduce_all = attributes.at("reduce_all").dyn_cast<pir::BoolAttribute>().data();


  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_, out_, out_grad_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  std::vector<pir::Attribute> vec_axis;
  for (size_t i = 0; i < static_cast<size_t>(axis.size()); i++) {
      pir::Attribute attr_axis = pir::Int64Attribute::get(pir::IrContext::Instance(), axis[i]);

    vec_axis.push_back(attr_axis);
  }
  pir::Attribute attr_axis = pir::ArrayAttribute::get(pir::IrContext::Instance(), vec_axis);
  argument.AddAttribute("axis", attr_axis);
  pir::Attribute attr_keepdim = pir::BoolAttribute::get(pir::IrContext::Instance(), keepdim);
  argument.AddAttribute("keepdim", attr_keepdim);
  pir::Attribute attr_reduce_all = pir::BoolAttribute::get(pir::IrContext::Instance(), reduce_all);
  argument.AddAttribute("reduce_all", attr_reduce_all);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;
  paddle::dialect::DenseTensorType out = out_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)out;
  paddle::dialect::DenseTensorType out_grad = out_grad_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)out_grad;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);
  paddle::dialect::IrTensor dense_x_grad;
  paddle::dialect::IrMetaTensor meta_x_grad(&dense_x_grad);

  phi::UnchangedInferMeta(meta_x, &meta_x_grad);

  std::vector<pir::Type> argument_outputs;
  pir::Type x_grad_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_x_grad.dtype()), dense_x_grad.dims(), dense_x_grad.layout(), dense_x_grad.lod(), dense_x_grad.offset());
  argument_outputs.push_back(x_grad_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void LogsumexpGradOp::VerifySig() {}

void LogsumexpGradOp::InferMeta( phi::InferMetaContext *infer_meta ) {
  auto fn = PD_INFER_META(phi::UnchangedInferMeta);
  fn(infer_meta);
}

phi::DataType LogsumexpGradOp::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: LogsumexpGradOp";
  


  return expected_kernel_dtype;
}

const char *MatmulDoubleGradOp::attributes_name[2] = { "transpose_x", "transpose_y" };

OpInfoTuple MatmulDoubleGradOp::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("x", "paddle::dialect::DenseTensorType", false, false, false, false), paddle::dialect::OpInputInfo("y", "paddle::dialect::DenseTensorType", false, false, false, false), paddle::dialect::OpInputInfo("grad_out", "paddle::dialect::DenseTensorType", false, false, false, false), paddle::dialect::OpInputInfo("grad_x_grad", "paddle::dialect::DenseTensorType", true, false, false, false), paddle::dialect::OpInputInfo("grad_y_grad", "paddle::dialect::DenseTensorType", true, false, false, false) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = { paddle::dialect::OpAttributeInfo("transpose_x", "pir::BoolAttribute", ""), paddle::dialect::OpAttributeInfo("transpose_y", "pir::BoolAttribute", "") };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("x_grad", "paddle::dialect::DenseTensorType", false, false), paddle::dialect::OpOutputInfo("y_grad", "paddle::dialect::DenseTensorType", false, false), paddle::dialect::OpOutputInfo("grad_out_grad", "paddle::dialect::DenseTensorType", false, false) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("GeneralTernaryGradInferMeta", {"x", "y", "grad_out"}, "matmul_double_grad", {"x", "y", "grad_out", "grad_x_grad", "grad_y_grad", "transpose_x", "transpose_y"}, {}, {}, {}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "matmul_double_grad");
}

void MatmulDoubleGradOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, pir::Value y_, pir::Value grad_out_, pir::Value grad_x_grad_, pir::Value grad_y_grad_, bool transpose_x, bool transpose_y) {
  VLOG(4) << "Start build MatmulDoubleGradOp";



  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_, y_, grad_out_, grad_x_grad_, grad_y_grad_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_transpose_x = pir::BoolAttribute::get(pir::IrContext::Instance(), transpose_x);
  argument.AddAttribute("transpose_x", attr_transpose_x);
  pir::Attribute attr_transpose_y = pir::BoolAttribute::get(pir::IrContext::Instance(), transpose_y);
  argument.AddAttribute("transpose_y", attr_transpose_y);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;
  paddle::dialect::DenseTensorType y = y_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)y;
  paddle::dialect::DenseTensorType grad_out = grad_out_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)grad_out;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);

  VLOG(4) << "Builder construction  dense_y";
  paddle::dialect::IrTensor ir_tensor_y(paddle::dialect::TransToPhiDataType(y.dtype()),
                                                      y.dims(),
                                                      y.data_layout(),
                                                      y.lod(),
                                                      y.offset());
  VLOG(4) << "Builder construction  meta_y";
  paddle::dialect::IrMetaTensor meta_y(&ir_tensor_y);

  VLOG(4) << "Builder construction  dense_grad_out";
  paddle::dialect::IrTensor ir_tensor_grad_out(paddle::dialect::TransToPhiDataType(grad_out.dtype()),
                                                      grad_out.dims(),
                                                      grad_out.data_layout(),
                                                      grad_out.lod(),
                                                      grad_out.offset());
  VLOG(4) << "Builder construction  meta_grad_out";
  paddle::dialect::IrMetaTensor meta_grad_out(&ir_tensor_grad_out);
  paddle::dialect::IrTensor dense_x_grad;
  paddle::dialect::IrMetaTensor meta_x_grad(&dense_x_grad);
  paddle::dialect::IrTensor dense_y_grad;
  paddle::dialect::IrMetaTensor meta_y_grad(&dense_y_grad);
  paddle::dialect::IrTensor dense_grad_out_grad;
  paddle::dialect::IrMetaTensor meta_grad_out_grad(&dense_grad_out_grad);

  phi::GeneralTernaryGradInferMeta(meta_x, meta_y, meta_grad_out, &meta_x_grad, &meta_y_grad, &meta_grad_out_grad);

  std::vector<pir::Type> argument_outputs;
  pir::Type x_grad_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_x_grad.dtype()), dense_x_grad.dims(), dense_x_grad.layout(), dense_x_grad.lod(), dense_x_grad.offset());
  argument_outputs.push_back(x_grad_dense_tensor_type);

  pir::Type y_grad_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_y_grad.dtype()), dense_y_grad.dims(), dense_y_grad.layout(), dense_y_grad.lod(), dense_y_grad.offset());
  argument_outputs.push_back(y_grad_dense_tensor_type);

  pir::Type grad_out_grad_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_grad_out_grad.dtype()), dense_grad_out_grad.dims(), dense_grad_out_grad.layout(), dense_grad_out_grad.lod(), dense_grad_out_grad.offset());
  argument_outputs.push_back(grad_out_grad_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void MatmulDoubleGradOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, pir::Value y_, pir::Value grad_out_, pir::Value grad_x_grad_, pir::Value grad_y_grad_, pir::AttributeMap attributes) {
  VLOG(4) << "Start build MatmulDoubleGradOp";


  IR_ENFORCE(
      attributes.find("transpose_x") != attributes.end(),
          "'transpose_x' Attribute is expected for MatmulDoubleGradOp. ");
  bool transpose_x = attributes.at("transpose_x").dyn_cast<pir::BoolAttribute>().data();

  IR_ENFORCE(
      attributes.find("transpose_y") != attributes.end(),
          "'transpose_y' Attribute is expected for MatmulDoubleGradOp. ");
  bool transpose_y = attributes.at("transpose_y").dyn_cast<pir::BoolAttribute>().data();


  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_, y_, grad_out_, grad_x_grad_, grad_y_grad_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_transpose_x = pir::BoolAttribute::get(pir::IrContext::Instance(), transpose_x);
  argument.AddAttribute("transpose_x", attr_transpose_x);
  pir::Attribute attr_transpose_y = pir::BoolAttribute::get(pir::IrContext::Instance(), transpose_y);
  argument.AddAttribute("transpose_y", attr_transpose_y);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;
  paddle::dialect::DenseTensorType y = y_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)y;
  paddle::dialect::DenseTensorType grad_out = grad_out_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)grad_out;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);

  VLOG(4) << "Builder construction  dense_y";
  paddle::dialect::IrTensor ir_tensor_y(paddle::dialect::TransToPhiDataType(y.dtype()),
                                                      y.dims(),
                                                      y.data_layout(),
                                                      y.lod(),
                                                      y.offset());
  VLOG(4) << "Builder construction  meta_y";
  paddle::dialect::IrMetaTensor meta_y(&ir_tensor_y);

  VLOG(4) << "Builder construction  dense_grad_out";
  paddle::dialect::IrTensor ir_tensor_grad_out(paddle::dialect::TransToPhiDataType(grad_out.dtype()),
                                                      grad_out.dims(),
                                                      grad_out.data_layout(),
                                                      grad_out.lod(),
                                                      grad_out.offset());
  VLOG(4) << "Builder construction  meta_grad_out";
  paddle::dialect::IrMetaTensor meta_grad_out(&ir_tensor_grad_out);
  paddle::dialect::IrTensor dense_x_grad;
  paddle::dialect::IrMetaTensor meta_x_grad(&dense_x_grad);
  paddle::dialect::IrTensor dense_y_grad;
  paddle::dialect::IrMetaTensor meta_y_grad(&dense_y_grad);
  paddle::dialect::IrTensor dense_grad_out_grad;
  paddle::dialect::IrMetaTensor meta_grad_out_grad(&dense_grad_out_grad);

  phi::GeneralTernaryGradInferMeta(meta_x, meta_y, meta_grad_out, &meta_x_grad, &meta_y_grad, &meta_grad_out_grad);

  std::vector<pir::Type> argument_outputs;
  pir::Type x_grad_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_x_grad.dtype()), dense_x_grad.dims(), dense_x_grad.layout(), dense_x_grad.lod(), dense_x_grad.offset());
  argument_outputs.push_back(x_grad_dense_tensor_type);

  pir::Type y_grad_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_y_grad.dtype()), dense_y_grad.dims(), dense_y_grad.layout(), dense_y_grad.lod(), dense_y_grad.offset());
  argument_outputs.push_back(y_grad_dense_tensor_type);

  pir::Type grad_out_grad_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_grad_out_grad.dtype()), dense_grad_out_grad.dims(), dense_grad_out_grad.layout(), dense_grad_out_grad.lod(), dense_grad_out_grad.offset());
  argument_outputs.push_back(grad_out_grad_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void MatmulDoubleGradOp::VerifySig() {}

void MatmulDoubleGradOp::InferMeta( phi::InferMetaContext *infer_meta ) {
  auto fn = PD_INFER_META(phi::GeneralTernaryGradInferMeta);
  fn(infer_meta);
}

phi::DataType MatmulDoubleGradOp::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: MatmulDoubleGradOp";
  


  // deal complex_promote
  if (framework::IsComplexType(expected_kernel_dtype)) {{
    // only promote inputsâ€™s types when contains complex input
    return tensor_dtype;
  }}

  return expected_kernel_dtype;
}

const char *MatmulGradOp::attributes_name[2] = { "transpose_x", "transpose_y" };

OpInfoTuple MatmulGradOp::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("x", "paddle::dialect::DenseTensorType", false, false, false, true), paddle::dialect::OpInputInfo("y", "paddle::dialect::DenseTensorType", false, false, false, true), paddle::dialect::OpInputInfo("out_grad", "paddle::dialect::DenseTensorType", false, false, false, true) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = { paddle::dialect::OpAttributeInfo("transpose_x", "pir::BoolAttribute", ""), paddle::dialect::OpAttributeInfo("transpose_y", "pir::BoolAttribute", "") };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("x_grad", "paddle::dialect::DenseTensorType", false, false), paddle::dialect::OpOutputInfo("y_grad", "paddle::dialect::DenseTensorType", false, false) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("GeneralBinaryGradInferMeta", {"x", "y"}, "matmul_grad", {"x", "y", "out_grad", "transpose_x", "transpose_y"}, {}, {}, {}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "matmul_grad");
}

void MatmulGradOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, pir::Value y_, pir::Value out_grad_, bool transpose_x, bool transpose_y) {
  VLOG(4) << "Start build MatmulGradOp";



  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_, y_, out_grad_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_transpose_x = pir::BoolAttribute::get(pir::IrContext::Instance(), transpose_x);
  argument.AddAttribute("transpose_x", attr_transpose_x);
  pir::Attribute attr_transpose_y = pir::BoolAttribute::get(pir::IrContext::Instance(), transpose_y);
  argument.AddAttribute("transpose_y", attr_transpose_y);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;
  paddle::dialect::DenseTensorType y = y_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)y;
  paddle::dialect::DenseTensorType out_grad = out_grad_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)out_grad;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);

  VLOG(4) << "Builder construction  dense_y";
  paddle::dialect::IrTensor ir_tensor_y(paddle::dialect::TransToPhiDataType(y.dtype()),
                                                      y.dims(),
                                                      y.data_layout(),
                                                      y.lod(),
                                                      y.offset());
  VLOG(4) << "Builder construction  meta_y";
  paddle::dialect::IrMetaTensor meta_y(&ir_tensor_y);
  paddle::dialect::IrTensor dense_x_grad;
  paddle::dialect::IrMetaTensor meta_x_grad(&dense_x_grad);
  paddle::dialect::IrTensor dense_y_grad;
  paddle::dialect::IrMetaTensor meta_y_grad(&dense_y_grad);

  phi::GeneralBinaryGradInferMeta(meta_x, meta_y, &meta_x_grad, &meta_y_grad);

  std::vector<pir::Type> argument_outputs;
  pir::Type x_grad_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_x_grad.dtype()), dense_x_grad.dims(), dense_x_grad.layout(), dense_x_grad.lod(), dense_x_grad.offset());
  argument_outputs.push_back(x_grad_dense_tensor_type);

  pir::Type y_grad_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_y_grad.dtype()), dense_y_grad.dims(), dense_y_grad.layout(), dense_y_grad.lod(), dense_y_grad.offset());
  argument_outputs.push_back(y_grad_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void MatmulGradOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, pir::Value y_, pir::Value out_grad_, pir::AttributeMap attributes) {
  VLOG(4) << "Start build MatmulGradOp";


  IR_ENFORCE(
      attributes.find("transpose_x") != attributes.end(),
          "'transpose_x' Attribute is expected for MatmulGradOp. ");
  bool transpose_x = attributes.at("transpose_x").dyn_cast<pir::BoolAttribute>().data();

  IR_ENFORCE(
      attributes.find("transpose_y") != attributes.end(),
          "'transpose_y' Attribute is expected for MatmulGradOp. ");
  bool transpose_y = attributes.at("transpose_y").dyn_cast<pir::BoolAttribute>().data();


  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_, y_, out_grad_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_transpose_x = pir::BoolAttribute::get(pir::IrContext::Instance(), transpose_x);
  argument.AddAttribute("transpose_x", attr_transpose_x);
  pir::Attribute attr_transpose_y = pir::BoolAttribute::get(pir::IrContext::Instance(), transpose_y);
  argument.AddAttribute("transpose_y", attr_transpose_y);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;
  paddle::dialect::DenseTensorType y = y_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)y;
  paddle::dialect::DenseTensorType out_grad = out_grad_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)out_grad;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);

  VLOG(4) << "Builder construction  dense_y";
  paddle::dialect::IrTensor ir_tensor_y(paddle::dialect::TransToPhiDataType(y.dtype()),
                                                      y.dims(),
                                                      y.data_layout(),
                                                      y.lod(),
                                                      y.offset());
  VLOG(4) << "Builder construction  meta_y";
  paddle::dialect::IrMetaTensor meta_y(&ir_tensor_y);
  paddle::dialect::IrTensor dense_x_grad;
  paddle::dialect::IrMetaTensor meta_x_grad(&dense_x_grad);
  paddle::dialect::IrTensor dense_y_grad;
  paddle::dialect::IrMetaTensor meta_y_grad(&dense_y_grad);

  phi::GeneralBinaryGradInferMeta(meta_x, meta_y, &meta_x_grad, &meta_y_grad);

  std::vector<pir::Type> argument_outputs;
  pir::Type x_grad_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_x_grad.dtype()), dense_x_grad.dims(), dense_x_grad.layout(), dense_x_grad.lod(), dense_x_grad.offset());
  argument_outputs.push_back(x_grad_dense_tensor_type);

  pir::Type y_grad_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_y_grad.dtype()), dense_y_grad.dims(), dense_y_grad.layout(), dense_y_grad.lod(), dense_y_grad.offset());
  argument_outputs.push_back(y_grad_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void MatmulGradOp::VerifySig() {}

void MatmulGradOp::InferMeta( phi::InferMetaContext *infer_meta ) {
  auto fn = PD_INFER_META(phi::GeneralBinaryGradInferMeta);
  fn(infer_meta);
}

phi::DataType MatmulGradOp::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: MatmulGradOp";
  


  // deal complex_promote
  if (framework::IsComplexType(expected_kernel_dtype)) {{
    // only promote inputsâ€™s types when contains complex input
    return tensor_dtype;
  }}

  return expected_kernel_dtype;
}

const char *MaxGradOp::attributes_name[2] = { "keepdim", "reduce_all" };

OpInfoTuple MaxGradOp::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("x", "paddle::dialect::DenseTensorType", false, false, false, false), paddle::dialect::OpInputInfo("out", "paddle::dialect::DenseTensorType", false, false, false, false), paddle::dialect::OpInputInfo("out_grad", "paddle::dialect::DenseTensorType", false, false, false, false), paddle::dialect::OpInputInfo("axis", "paddle::dialect::IntArrayAttribute", false, false, true, false) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = { paddle::dialect::OpAttributeInfo("keepdim", "pir::BoolAttribute", ""), paddle::dialect::OpAttributeInfo("reduce_all", "pir::BoolAttribute", "") };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("x_grad", "paddle::dialect::DenseTensorType", false, false) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("UnchangedInferMeta", {"x"}, "max_grad", {"x", "out", "out_grad", "axis", "keepdim", "reduce_all"}, {}, {}, {}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "max_grad");
}

void MaxGradOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, pir::Value out_, pir::Value out_grad_, const std::vector<int64_t>& axis, bool keepdim, bool reduce_all) {
  VLOG(4) << "Start build MaxGradOp";


  // Generate int_array mutable attribute: axis
  paddle::dialect::FullIntArrayOp full_axis_op = builder.Build<paddle::dialect::FullIntArrayOp>(axis, phi::DataType::INT64, phi::CPUPlace());
  pir::OpResult axis_ = full_axis_op->result(0);
    
  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_, out_, out_grad_, axis_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_keepdim = pir::BoolAttribute::get(pir::IrContext::Instance(), keepdim);
  argument.AddAttribute("keepdim", attr_keepdim);
  pir::Attribute attr_reduce_all = pir::BoolAttribute::get(pir::IrContext::Instance(), reduce_all);
  argument.AddAttribute("reduce_all", attr_reduce_all);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;
  paddle::dialect::DenseTensorType out = out_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)out;
  paddle::dialect::DenseTensorType out_grad = out_grad_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)out_grad;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);
  paddle::dialect::IrTensor dense_x_grad;
  paddle::dialect::IrMetaTensor meta_x_grad(&dense_x_grad);

  phi::UnchangedInferMeta(meta_x, &meta_x_grad);

  std::vector<pir::Type> argument_outputs;
  pir::Type x_grad_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_x_grad.dtype()), dense_x_grad.dims(), dense_x_grad.layout(), dense_x_grad.lod(), dense_x_grad.offset());
  argument_outputs.push_back(x_grad_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void MaxGradOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, pir::Value out_, pir::Value out_grad_, pir::AttributeMap attributes) {
  VLOG(4) << "Start build MaxGradOp";


  IR_ENFORCE(
      attributes.find("axis") != attributes.end(),
          "'axis' Attribute is expected for MaxGradOp. ");
  std::vector<int64_t> axis = attributes.at("axis").dyn_cast<paddle::dialect::IntArrayAttribute>().data().GetData();

  IR_ENFORCE(
      attributes.find("keepdim") != attributes.end(),
          "'keepdim' Attribute is expected for MaxGradOp. ");
  bool keepdim = attributes.at("keepdim").dyn_cast<pir::BoolAttribute>().data();

  IR_ENFORCE(
      attributes.find("reduce_all") != attributes.end(),
          "'reduce_all' Attribute is expected for MaxGradOp. ");
  bool reduce_all = attributes.at("reduce_all").dyn_cast<pir::BoolAttribute>().data();

  // Generate int_array mutable attribute: axis
  paddle::dialect::FullIntArrayOp full_axis_op = builder.Build<paddle::dialect::FullIntArrayOp>(axis, phi::DataType::INT64, phi::CPUPlace());
  pir::OpResult axis_ = full_axis_op->result(0);
    
  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_, out_, out_grad_, axis_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_keepdim = pir::BoolAttribute::get(pir::IrContext::Instance(), keepdim);
  argument.AddAttribute("keepdim", attr_keepdim);
  pir::Attribute attr_reduce_all = pir::BoolAttribute::get(pir::IrContext::Instance(), reduce_all);
  argument.AddAttribute("reduce_all", attr_reduce_all);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;
  paddle::dialect::DenseTensorType out = out_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)out;
  paddle::dialect::DenseTensorType out_grad = out_grad_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)out_grad;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);
  paddle::dialect::IrTensor dense_x_grad;
  paddle::dialect::IrMetaTensor meta_x_grad(&dense_x_grad);

  phi::UnchangedInferMeta(meta_x, &meta_x_grad);

  std::vector<pir::Type> argument_outputs;
  pir::Type x_grad_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_x_grad.dtype()), dense_x_grad.dims(), dense_x_grad.layout(), dense_x_grad.lod(), dense_x_grad.offset());
  argument_outputs.push_back(x_grad_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void MaxGradOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, pir::Value out_, pir::Value out_grad_, pir::Value axis_, bool keepdim, bool reduce_all) {
  VLOG(4) << "Start build MaxGradOp";



  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_, out_, out_grad_, axis_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_keepdim = pir::BoolAttribute::get(pir::IrContext::Instance(), keepdim);
  argument.AddAttribute("keepdim", attr_keepdim);
  pir::Attribute attr_reduce_all = pir::BoolAttribute::get(pir::IrContext::Instance(), reduce_all);
  argument.AddAttribute("reduce_all", attr_reduce_all);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;
  paddle::dialect::DenseTensorType out = out_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)out;
  paddle::dialect::DenseTensorType out_grad = out_grad_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)out_grad;
  phi::IntArray axis;
  if (axis_.dyn_cast<pir::OpResult>() && axis_.dyn_cast<pir::OpResult>().owner()->isa<paddle::dialect::FullIntArrayOp>()) {
    axis = std::move(phi::IntArray(paddle::dialect::GetInt64Vector(
                          axis_.dyn_cast<pir::OpResult>().owner()
                          ->dyn_cast<paddle::dialect::FullIntArrayOp>()
                          .attribute("value"))));
  } else if (axis_.type().isa<pir::VectorType>()) {
    size_t axis_size = axis_.type().dyn_cast<pir::VectorType>().size();
    axis = std::move(phi::IntArray(std::vector<int64_t>(axis_size, -1)));
    axis.SetFromTensor(true);
  } else if (axis_.type().isa<paddle::dialect::DenseTensorType>()) {
    common::DDim axis_dim = axis_.type().dyn_cast<paddle::dialect::DenseTensorType>().dims();
    size_t axis_size = common::product(axis_dim);
    if (common::contain_unknown_dim(axis_dim)) {
      axis_size = 1;
    }
    axis = std::move(phi::IntArray(std::vector<int64_t>(axis_size, -1)));
    axis.SetFromTensor(true);
  } else {
    PADDLE_THROW(phi::errors::Unimplemented("Only support VectorType or DenseTensorType"));
  }


  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);
  paddle::dialect::IrTensor dense_x_grad;
  paddle::dialect::IrMetaTensor meta_x_grad(&dense_x_grad);

  phi::UnchangedInferMeta(meta_x, &meta_x_grad);

  std::vector<pir::Type> argument_outputs;
  pir::Type x_grad_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_x_grad.dtype()), dense_x_grad.dims(), dense_x_grad.layout(), dense_x_grad.lod(), dense_x_grad.offset());
  argument_outputs.push_back(x_grad_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void MaxGradOp::VerifySig() {}

void MaxGradOp::InferMeta( phi::InferMetaContext *infer_meta ) {
  auto fn = PD_INFER_META(phi::UnchangedInferMeta);
  fn(infer_meta);
}

phi::DataType MaxGradOp::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: MaxGradOp";
  


  return expected_kernel_dtype;
}

OpInfoTuple MaximumGradOp::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("x", "paddle::dialect::DenseTensorType", false, false, false, false), paddle::dialect::OpInputInfo("y", "paddle::dialect::DenseTensorType", false, false, false, false), paddle::dialect::OpInputInfo("out_grad", "paddle::dialect::DenseTensorType", false, false, false, false) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = {  };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("x_grad", "paddle::dialect::DenseTensorType", false, false), paddle::dialect::OpOutputInfo("y_grad", "paddle::dialect::DenseTensorType", false, false) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("GeneralBinaryGradInferMeta", {"x", "y"}, "maximum_grad", {"x", "y", "out_grad"}, {}, {}, {}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "maximum_grad");
}

void MaximumGradOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, pir::Value y_, pir::Value out_grad_) {
  VLOG(4) << "Start build MaximumGradOp";



  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_, y_, out_grad_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;
  paddle::dialect::DenseTensorType y = y_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)y;
  paddle::dialect::DenseTensorType out_grad = out_grad_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)out_grad;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);

  VLOG(4) << "Builder construction  dense_y";
  paddle::dialect::IrTensor ir_tensor_y(paddle::dialect::TransToPhiDataType(y.dtype()),
                                                      y.dims(),
                                                      y.data_layout(),
                                                      y.lod(),
                                                      y.offset());
  VLOG(4) << "Builder construction  meta_y";
  paddle::dialect::IrMetaTensor meta_y(&ir_tensor_y);
  paddle::dialect::IrTensor dense_x_grad;
  paddle::dialect::IrMetaTensor meta_x_grad(&dense_x_grad);
  paddle::dialect::IrTensor dense_y_grad;
  paddle::dialect::IrMetaTensor meta_y_grad(&dense_y_grad);

  phi::GeneralBinaryGradInferMeta(meta_x, meta_y, &meta_x_grad, &meta_y_grad);

  std::vector<pir::Type> argument_outputs;
  pir::Type x_grad_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_x_grad.dtype()), dense_x_grad.dims(), dense_x_grad.layout(), dense_x_grad.lod(), dense_x_grad.offset());
  argument_outputs.push_back(x_grad_dense_tensor_type);

  pir::Type y_grad_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_y_grad.dtype()), dense_y_grad.dims(), dense_y_grad.layout(), dense_y_grad.lod(), dense_y_grad.offset());
  argument_outputs.push_back(y_grad_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void MaximumGradOp::VerifySig() {}

void MaximumGradOp::InferMeta( phi::InferMetaContext *infer_meta ) {
  auto fn = PD_INFER_META(phi::GeneralBinaryGradInferMeta);
  fn(infer_meta);
}

phi::DataType MaximumGradOp::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: MaximumGradOp";
  


  // deal complex_promote
  if (framework::IsComplexType(expected_kernel_dtype)) {{
    // only promote inputsâ€™s types when contains complex input
    return tensor_dtype;
  }}

  return expected_kernel_dtype;
}

const char *MeanDoubleGradOp::attributes_name[2] = { "axis", "keepdim" };

OpInfoTuple MeanDoubleGradOp::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("grad_x_grad", "paddle::dialect::DenseTensorType", false, false, false, false) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = { paddle::dialect::OpAttributeInfo("axis", "paddle::dialect::IntArrayAttribute", "std::vector<int64_t>"), paddle::dialect::OpAttributeInfo("keepdim", "pir::BoolAttribute", "") };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("grad_out_grad", "paddle::dialect::DenseTensorType", false, false) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("", {""}, "", {""}, {}, {}, {}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "mean_double_grad");
}

void MeanDoubleGradOp::VerifySig() {}

phi::DataType MeanDoubleGradOp::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: MeanDoubleGradOp";
  


  return expected_kernel_dtype;
}

const char *MeanGradOp::attributes_name[3] = { "axis", "keepdim", "reduce_all" };

OpInfoTuple MeanGradOp::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("x", "paddle::dialect::DenseTensorType", false, true, false, false), paddle::dialect::OpInputInfo("out_grad", "paddle::dialect::DenseTensorType", false, false, false, true) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = { paddle::dialect::OpAttributeInfo("axis", "paddle::dialect::IntArrayAttribute", "std::vector<int64_t>"), paddle::dialect::OpAttributeInfo("keepdim", "pir::BoolAttribute", ""), paddle::dialect::OpAttributeInfo("reduce_all", "pir::BoolAttribute", "") };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("x_grad", "paddle::dialect::DenseTensorType", false, false) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("UnchangedInferMeta", {"x"}, "mean_grad", {"x", "out_grad", "axis", "keepdim", "reduce_all"}, {}, {}, {}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "mean_grad");
}

void MeanGradOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, pir::Value out_grad_, const std::vector<int64_t>& axis, bool keepdim, bool reduce_all) {
  VLOG(4) << "Start build MeanGradOp";



  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_, out_grad_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_axis = paddle::dialect::IntArrayAttribute::get(pir::IrContext::Instance(), phi::IntArray(axis));
  argument.AddAttribute("axis", attr_axis);
  pir::Attribute attr_keepdim = pir::BoolAttribute::get(pir::IrContext::Instance(), keepdim);
  argument.AddAttribute("keepdim", attr_keepdim);
  pir::Attribute attr_reduce_all = pir::BoolAttribute::get(pir::IrContext::Instance(), reduce_all);
  argument.AddAttribute("reduce_all", attr_reduce_all);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;
  paddle::dialect::DenseTensorType out_grad = out_grad_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)out_grad;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);
  paddle::dialect::IrTensor dense_x_grad;
  paddle::dialect::IrMetaTensor meta_x_grad(&dense_x_grad);

  phi::UnchangedInferMeta(meta_x, &meta_x_grad);

  std::vector<pir::Type> argument_outputs;
  pir::Type x_grad_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_x_grad.dtype()), dense_x_grad.dims(), dense_x_grad.layout(), dense_x_grad.lod(), dense_x_grad.offset());
  argument_outputs.push_back(x_grad_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void MeanGradOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, pir::Value out_grad_, pir::AttributeMap attributes) {
  VLOG(4) << "Start build MeanGradOp";


  IR_ENFORCE(
      attributes.find("axis") != attributes.end(),
          "'axis' Attribute is expected for MeanGradOp. ");
  std::vector<int64_t> axis = attributes.at("axis").dyn_cast<paddle::dialect::IntArrayAttribute>().data().GetData();

  IR_ENFORCE(
      attributes.find("keepdim") != attributes.end(),
          "'keepdim' Attribute is expected for MeanGradOp. ");
  bool keepdim = attributes.at("keepdim").dyn_cast<pir::BoolAttribute>().data();

  IR_ENFORCE(
      attributes.find("reduce_all") != attributes.end(),
          "'reduce_all' Attribute is expected for MeanGradOp. ");
  bool reduce_all = attributes.at("reduce_all").dyn_cast<pir::BoolAttribute>().data();


  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_, out_grad_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_axis = paddle::dialect::IntArrayAttribute::get(pir::IrContext::Instance(), phi::IntArray(axis));
  argument.AddAttribute("axis", attr_axis);
  pir::Attribute attr_keepdim = pir::BoolAttribute::get(pir::IrContext::Instance(), keepdim);
  argument.AddAttribute("keepdim", attr_keepdim);
  pir::Attribute attr_reduce_all = pir::BoolAttribute::get(pir::IrContext::Instance(), reduce_all);
  argument.AddAttribute("reduce_all", attr_reduce_all);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;
  paddle::dialect::DenseTensorType out_grad = out_grad_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)out_grad;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);
  paddle::dialect::IrTensor dense_x_grad;
  paddle::dialect::IrMetaTensor meta_x_grad(&dense_x_grad);

  phi::UnchangedInferMeta(meta_x, &meta_x_grad);

  std::vector<pir::Type> argument_outputs;
  pir::Type x_grad_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_x_grad.dtype()), dense_x_grad.dims(), dense_x_grad.layout(), dense_x_grad.lod(), dense_x_grad.offset());
  argument_outputs.push_back(x_grad_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void MeanGradOp::VerifySig() {}

void MeanGradOp::InferMeta( phi::InferMetaContext *infer_meta ) {
  auto fn = PD_INFER_META(phi::UnchangedInferMeta);
  fn(infer_meta);
}

phi::DataType MeanGradOp::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: MeanGradOp";
  


  return expected_kernel_dtype;
}

const char *MinGradOp::attributes_name[2] = { "keepdim", "reduce_all" };

OpInfoTuple MinGradOp::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("x", "paddle::dialect::DenseTensorType", false, false, false, false), paddle::dialect::OpInputInfo("out", "paddle::dialect::DenseTensorType", false, false, false, false), paddle::dialect::OpInputInfo("out_grad", "paddle::dialect::DenseTensorType", false, false, false, false), paddle::dialect::OpInputInfo("axis", "paddle::dialect::IntArrayAttribute", false, false, true, false) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = { paddle::dialect::OpAttributeInfo("keepdim", "pir::BoolAttribute", ""), paddle::dialect::OpAttributeInfo("reduce_all", "pir::BoolAttribute", "") };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("x_grad", "paddle::dialect::DenseTensorType", false, false) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("UnchangedInferMeta", {"x"}, "min_grad", {"x", "out", "out_grad", "axis", "keepdim", "reduce_all"}, {}, {}, {}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "min_grad");
}

void MinGradOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, pir::Value out_, pir::Value out_grad_, const std::vector<int64_t>& axis, bool keepdim, bool reduce_all) {
  VLOG(4) << "Start build MinGradOp";


  // Generate int_array mutable attribute: axis
  paddle::dialect::FullIntArrayOp full_axis_op = builder.Build<paddle::dialect::FullIntArrayOp>(axis, phi::DataType::INT64, phi::CPUPlace());
  pir::OpResult axis_ = full_axis_op->result(0);
    
  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_, out_, out_grad_, axis_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_keepdim = pir::BoolAttribute::get(pir::IrContext::Instance(), keepdim);
  argument.AddAttribute("keepdim", attr_keepdim);
  pir::Attribute attr_reduce_all = pir::BoolAttribute::get(pir::IrContext::Instance(), reduce_all);
  argument.AddAttribute("reduce_all", attr_reduce_all);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;
  paddle::dialect::DenseTensorType out = out_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)out;
  paddle::dialect::DenseTensorType out_grad = out_grad_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)out_grad;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);
  paddle::dialect::IrTensor dense_x_grad;
  paddle::dialect::IrMetaTensor meta_x_grad(&dense_x_grad);

  phi::UnchangedInferMeta(meta_x, &meta_x_grad);

  std::vector<pir::Type> argument_outputs;
  pir::Type x_grad_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_x_grad.dtype()), dense_x_grad.dims(), dense_x_grad.layout(), dense_x_grad.lod(), dense_x_grad.offset());
  argument_outputs.push_back(x_grad_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void MinGradOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, pir::Value out_, pir::Value out_grad_, pir::AttributeMap attributes) {
  VLOG(4) << "Start build MinGradOp";


  IR_ENFORCE(
      attributes.find("axis") != attributes.end(),
          "'axis' Attribute is expected for MinGradOp. ");
  std::vector<int64_t> axis = attributes.at("axis").dyn_cast<paddle::dialect::IntArrayAttribute>().data().GetData();

  IR_ENFORCE(
      attributes.find("keepdim") != attributes.end(),
          "'keepdim' Attribute is expected for MinGradOp. ");
  bool keepdim = attributes.at("keepdim").dyn_cast<pir::BoolAttribute>().data();

  IR_ENFORCE(
      attributes.find("reduce_all") != attributes.end(),
          "'reduce_all' Attribute is expected for MinGradOp. ");
  bool reduce_all = attributes.at("reduce_all").dyn_cast<pir::BoolAttribute>().data();

  // Generate int_array mutable attribute: axis
  paddle::dialect::FullIntArrayOp full_axis_op = builder.Build<paddle::dialect::FullIntArrayOp>(axis, phi::DataType::INT64, phi::CPUPlace());
  pir::OpResult axis_ = full_axis_op->result(0);
    
  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_, out_, out_grad_, axis_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_keepdim = pir::BoolAttribute::get(pir::IrContext::Instance(), keepdim);
  argument.AddAttribute("keepdim", attr_keepdim);
  pir::Attribute attr_reduce_all = pir::BoolAttribute::get(pir::IrContext::Instance(), reduce_all);
  argument.AddAttribute("reduce_all", attr_reduce_all);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;
  paddle::dialect::DenseTensorType out = out_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)out;
  paddle::dialect::DenseTensorType out_grad = out_grad_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)out_grad;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);
  paddle::dialect::IrTensor dense_x_grad;
  paddle::dialect::IrMetaTensor meta_x_grad(&dense_x_grad);

  phi::UnchangedInferMeta(meta_x, &meta_x_grad);

  std::vector<pir::Type> argument_outputs;
  pir::Type x_grad_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_x_grad.dtype()), dense_x_grad.dims(), dense_x_grad.layout(), dense_x_grad.lod(), dense_x_grad.offset());
  argument_outputs.push_back(x_grad_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void MinGradOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, pir::Value out_, pir::Value out_grad_, pir::Value axis_, bool keepdim, bool reduce_all) {
  VLOG(4) << "Start build MinGradOp";



  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_, out_, out_grad_, axis_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_keepdim = pir::BoolAttribute::get(pir::IrContext::Instance(), keepdim);
  argument.AddAttribute("keepdim", attr_keepdim);
  pir::Attribute attr_reduce_all = pir::BoolAttribute::get(pir::IrContext::Instance(), reduce_all);
  argument.AddAttribute("reduce_all", attr_reduce_all);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;
  paddle::dialect::DenseTensorType out = out_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)out;
  paddle::dialect::DenseTensorType out_grad = out_grad_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)out_grad;
  phi::IntArray axis;
  if (axis_.dyn_cast<pir::OpResult>() && axis_.dyn_cast<pir::OpResult>().owner()->isa<paddle::dialect::FullIntArrayOp>()) {
    axis = std::move(phi::IntArray(paddle::dialect::GetInt64Vector(
                          axis_.dyn_cast<pir::OpResult>().owner()
                          ->dyn_cast<paddle::dialect::FullIntArrayOp>()
                          .attribute("value"))));
  } else if (axis_.type().isa<pir::VectorType>()) {
    size_t axis_size = axis_.type().dyn_cast<pir::VectorType>().size();
    axis = std::move(phi::IntArray(std::vector<int64_t>(axis_size, -1)));
    axis.SetFromTensor(true);
  } else if (axis_.type().isa<paddle::dialect::DenseTensorType>()) {
    common::DDim axis_dim = axis_.type().dyn_cast<paddle::dialect::DenseTensorType>().dims();
    size_t axis_size = common::product(axis_dim);
    if (common::contain_unknown_dim(axis_dim)) {
      axis_size = 1;
    }
    axis = std::move(phi::IntArray(std::vector<int64_t>(axis_size, -1)));
    axis.SetFromTensor(true);
  } else {
    PADDLE_THROW(phi::errors::Unimplemented("Only support VectorType or DenseTensorType"));
  }


  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);
  paddle::dialect::IrTensor dense_x_grad;
  paddle::dialect::IrMetaTensor meta_x_grad(&dense_x_grad);

  phi::UnchangedInferMeta(meta_x, &meta_x_grad);

  std::vector<pir::Type> argument_outputs;
  pir::Type x_grad_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_x_grad.dtype()), dense_x_grad.dims(), dense_x_grad.layout(), dense_x_grad.lod(), dense_x_grad.offset());
  argument_outputs.push_back(x_grad_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void MinGradOp::VerifySig() {}

void MinGradOp::InferMeta( phi::InferMetaContext *infer_meta ) {
  auto fn = PD_INFER_META(phi::UnchangedInferMeta);
  fn(infer_meta);
}

phi::DataType MinGradOp::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: MinGradOp";
  


  return expected_kernel_dtype;
}

OpInfoTuple MinimumGradOp::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("x", "paddle::dialect::DenseTensorType", false, false, false, false), paddle::dialect::OpInputInfo("y", "paddle::dialect::DenseTensorType", false, false, false, false), paddle::dialect::OpInputInfo("out_grad", "paddle::dialect::DenseTensorType", false, false, false, false) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = {  };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("x_grad", "paddle::dialect::DenseTensorType", false, false), paddle::dialect::OpOutputInfo("y_grad", "paddle::dialect::DenseTensorType", false, false) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("GeneralBinaryGradInferMeta", {"x", "y"}, "minimum_grad", {"x", "y", "out_grad"}, {}, {}, {}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "minimum_grad");
}

void MinimumGradOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, pir::Value y_, pir::Value out_grad_) {
  VLOG(4) << "Start build MinimumGradOp";



  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_, y_, out_grad_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;
  paddle::dialect::DenseTensorType y = y_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)y;
  paddle::dialect::DenseTensorType out_grad = out_grad_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)out_grad;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);

  VLOG(4) << "Builder construction  dense_y";
  paddle::dialect::IrTensor ir_tensor_y(paddle::dialect::TransToPhiDataType(y.dtype()),
                                                      y.dims(),
                                                      y.data_layout(),
                                                      y.lod(),
                                                      y.offset());
  VLOG(4) << "Builder construction  meta_y";
  paddle::dialect::IrMetaTensor meta_y(&ir_tensor_y);
  paddle::dialect::IrTensor dense_x_grad;
  paddle::dialect::IrMetaTensor meta_x_grad(&dense_x_grad);
  paddle::dialect::IrTensor dense_y_grad;
  paddle::dialect::IrMetaTensor meta_y_grad(&dense_y_grad);

  phi::GeneralBinaryGradInferMeta(meta_x, meta_y, &meta_x_grad, &meta_y_grad);

  std::vector<pir::Type> argument_outputs;
  pir::Type x_grad_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_x_grad.dtype()), dense_x_grad.dims(), dense_x_grad.layout(), dense_x_grad.lod(), dense_x_grad.offset());
  argument_outputs.push_back(x_grad_dense_tensor_type);

  pir::Type y_grad_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_y_grad.dtype()), dense_y_grad.dims(), dense_y_grad.layout(), dense_y_grad.lod(), dense_y_grad.offset());
  argument_outputs.push_back(y_grad_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void MinimumGradOp::VerifySig() {}

void MinimumGradOp::InferMeta( phi::InferMetaContext *infer_meta ) {
  auto fn = PD_INFER_META(phi::GeneralBinaryGradInferMeta);
  fn(infer_meta);
}

phi::DataType MinimumGradOp::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: MinimumGradOp";
  


  // deal complex_promote
  if (framework::IsComplexType(expected_kernel_dtype)) {{
    // only promote inputsâ€™s types when contains complex input
    return tensor_dtype;
  }}

  return expected_kernel_dtype;
}

const char *MishGradOp::attributes_name[1] = { "lambda" };

OpInfoTuple MishGradOp::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("x", "paddle::dialect::DenseTensorType", false, false, false, false), paddle::dialect::OpInputInfo("out_grad", "paddle::dialect::DenseTensorType", false, false, false, false) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = { paddle::dialect::OpAttributeInfo("lambda", "pir::FloatAttribute", "") };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("x_grad", "paddle::dialect::DenseTensorType", false, false) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("UnchangedInferMeta", {"x"}, "mish_grad", {"x", "out_grad", "lambda"}, {}, {}, {}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "mish_grad");
}

void MishGradOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, pir::Value out_grad_, float lambda) {
  VLOG(4) << "Start build MishGradOp";



  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_, out_grad_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_lambda = pir::FloatAttribute::get(pir::IrContext::Instance(), lambda);
  argument.AddAttribute("lambda", attr_lambda);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;
  paddle::dialect::DenseTensorType out_grad = out_grad_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)out_grad;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);
  paddle::dialect::IrTensor dense_x_grad;
  paddle::dialect::IrMetaTensor meta_x_grad(&dense_x_grad);

  phi::UnchangedInferMeta(meta_x, &meta_x_grad);

  std::vector<pir::Type> argument_outputs;
  pir::Type x_grad_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_x_grad.dtype()), dense_x_grad.dims(), dense_x_grad.layout(), dense_x_grad.lod(), dense_x_grad.offset());
  argument_outputs.push_back(x_grad_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void MishGradOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, pir::Value out_grad_, pir::AttributeMap attributes) {
  VLOG(4) << "Start build MishGradOp";


  IR_ENFORCE(
      attributes.find("lambda") != attributes.end(),
          "'lambda' Attribute is expected for MishGradOp. ");
  float lambda = attributes.at("lambda").dyn_cast<pir::FloatAttribute>().data();


  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_, out_grad_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_lambda = pir::FloatAttribute::get(pir::IrContext::Instance(), lambda);
  argument.AddAttribute("lambda", attr_lambda);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;
  paddle::dialect::DenseTensorType out_grad = out_grad_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)out_grad;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);
  paddle::dialect::IrTensor dense_x_grad;
  paddle::dialect::IrMetaTensor meta_x_grad(&dense_x_grad);

  phi::UnchangedInferMeta(meta_x, &meta_x_grad);

  std::vector<pir::Type> argument_outputs;
  pir::Type x_grad_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_x_grad.dtype()), dense_x_grad.dims(), dense_x_grad.layout(), dense_x_grad.lod(), dense_x_grad.offset());
  argument_outputs.push_back(x_grad_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void MishGradOp::VerifySig() {}

void MishGradOp::InferMeta( phi::InferMetaContext *infer_meta ) {
  auto fn = PD_INFER_META(phi::UnchangedInferMeta);
  fn(infer_meta);
}

phi::DataType MishGradOp::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: MishGradOp";
  


  return expected_kernel_dtype;
}

const char *MishGrad_Op::attributes_name[1] = { "lambda" };

OpInfoTuple MishGrad_Op::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("x", "paddle::dialect::DenseTensorType", false, false, false, false), paddle::dialect::OpInputInfo("out_grad", "paddle::dialect::DenseTensorType", false, false, false, false) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = { paddle::dialect::OpAttributeInfo("lambda", "pir::FloatAttribute", "") };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("x_grad", "paddle::dialect::DenseTensorType", false, false) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("UnchangedInferMeta", {"x"}, "mish_grad", {"x", "out_grad", "lambda"}, {}, {}, {{"x_grad", "out_grad"}}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "mish_grad");
}

void MishGrad_Op::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, pir::Value out_grad_, float lambda) {
  VLOG(4) << "Start build MishGrad_Op";



  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_, out_grad_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_lambda = pir::FloatAttribute::get(pir::IrContext::Instance(), lambda);
  argument.AddAttribute("lambda", attr_lambda);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;
  paddle::dialect::DenseTensorType out_grad = out_grad_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)out_grad;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);
  paddle::dialect::IrTensor dense_x_grad;
  paddle::dialect::IrMetaTensor meta_x_grad(&dense_x_grad);

  phi::UnchangedInferMeta(meta_x, &meta_x_grad);

  std::vector<pir::Type> argument_outputs;
  pir::Type x_grad_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_x_grad.dtype()), dense_x_grad.dims(), dense_x_grad.layout(), dense_x_grad.lod(), dense_x_grad.offset());
  argument_outputs.push_back(x_grad_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void MishGrad_Op::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, pir::Value out_grad_, pir::AttributeMap attributes) {
  VLOG(4) << "Start build MishGrad_Op";


  IR_ENFORCE(
      attributes.find("lambda") != attributes.end(),
          "'lambda' Attribute is expected for MishGrad_Op. ");
  float lambda = attributes.at("lambda").dyn_cast<pir::FloatAttribute>().data();


  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_, out_grad_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_lambda = pir::FloatAttribute::get(pir::IrContext::Instance(), lambda);
  argument.AddAttribute("lambda", attr_lambda);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;
  paddle::dialect::DenseTensorType out_grad = out_grad_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)out_grad;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);
  paddle::dialect::IrTensor dense_x_grad;
  paddle::dialect::IrMetaTensor meta_x_grad(&dense_x_grad);

  phi::UnchangedInferMeta(meta_x, &meta_x_grad);

  std::vector<pir::Type> argument_outputs;
  pir::Type x_grad_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_x_grad.dtype()), dense_x_grad.dims(), dense_x_grad.layout(), dense_x_grad.lod(), dense_x_grad.offset());
  argument_outputs.push_back(x_grad_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void MishGrad_Op::VerifySig() {}

void MishGrad_Op::InferMeta( phi::InferMetaContext *infer_meta ) {
  auto fn = PD_INFER_META(phi::UnchangedInferMeta);
  fn(infer_meta);
}

phi::DataType MishGrad_Op::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: MishGrad_Op";
  


  return expected_kernel_dtype;
}

const char *MultiplyDoubleGradOp::attributes_name[1] = { "axis" };

OpInfoTuple MultiplyDoubleGradOp::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("x", "paddle::dialect::DenseTensorType", false, false, false, true), paddle::dialect::OpInputInfo("y", "paddle::dialect::DenseTensorType", false, false, false, true), paddle::dialect::OpInputInfo("grad_out", "paddle::dialect::DenseTensorType", false, false, false, true), paddle::dialect::OpInputInfo("grad_x_grad", "paddle::dialect::DenseTensorType", true, false, false, true), paddle::dialect::OpInputInfo("grad_y_grad", "paddle::dialect::DenseTensorType", true, false, false, true) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = { paddle::dialect::OpAttributeInfo("axis", "pir::Int32Attribute", "") };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("x_grad", "paddle::dialect::DenseTensorType", false, false), paddle::dialect::OpOutputInfo("y_grad", "paddle::dialect::DenseTensorType", false, false), paddle::dialect::OpOutputInfo("grad_out_grad", "paddle::dialect::DenseTensorType", false, false) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("GeneralTernaryGradInferMeta", {"x", "y", "grad_out"}, "multiply_double_grad", {"x", "y", "grad_out", "grad_x_grad", "grad_y_grad", "axis"}, {}, {}, {}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "multiply_double_grad");
}

void MultiplyDoubleGradOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, pir::Value y_, pir::Value grad_out_, pir::Value grad_x_grad_, pir::Value grad_y_grad_, int axis) {
  VLOG(4) << "Start build MultiplyDoubleGradOp";



  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_, y_, grad_out_, grad_x_grad_, grad_y_grad_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_axis = pir::Int32Attribute::get(pir::IrContext::Instance(), axis);
  argument.AddAttribute("axis", attr_axis);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;
  paddle::dialect::DenseTensorType y = y_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)y;
  paddle::dialect::DenseTensorType grad_out = grad_out_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)grad_out;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);

  VLOG(4) << "Builder construction  dense_y";
  paddle::dialect::IrTensor ir_tensor_y(paddle::dialect::TransToPhiDataType(y.dtype()),
                                                      y.dims(),
                                                      y.data_layout(),
                                                      y.lod(),
                                                      y.offset());
  VLOG(4) << "Builder construction  meta_y";
  paddle::dialect::IrMetaTensor meta_y(&ir_tensor_y);

  VLOG(4) << "Builder construction  dense_grad_out";
  paddle::dialect::IrTensor ir_tensor_grad_out(paddle::dialect::TransToPhiDataType(grad_out.dtype()),
                                                      grad_out.dims(),
                                                      grad_out.data_layout(),
                                                      grad_out.lod(),
                                                      grad_out.offset());
  VLOG(4) << "Builder construction  meta_grad_out";
  paddle::dialect::IrMetaTensor meta_grad_out(&ir_tensor_grad_out);
  paddle::dialect::IrTensor dense_x_grad;
  paddle::dialect::IrMetaTensor meta_x_grad(&dense_x_grad);
  paddle::dialect::IrTensor dense_y_grad;
  paddle::dialect::IrMetaTensor meta_y_grad(&dense_y_grad);
  paddle::dialect::IrTensor dense_grad_out_grad;
  paddle::dialect::IrMetaTensor meta_grad_out_grad(&dense_grad_out_grad);

  phi::GeneralTernaryGradInferMeta(meta_x, meta_y, meta_grad_out, &meta_x_grad, &meta_y_grad, &meta_grad_out_grad);

  std::vector<pir::Type> argument_outputs;
  pir::Type x_grad_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_x_grad.dtype()), dense_x_grad.dims(), dense_x_grad.layout(), dense_x_grad.lod(), dense_x_grad.offset());
  argument_outputs.push_back(x_grad_dense_tensor_type);

  pir::Type y_grad_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_y_grad.dtype()), dense_y_grad.dims(), dense_y_grad.layout(), dense_y_grad.lod(), dense_y_grad.offset());
  argument_outputs.push_back(y_grad_dense_tensor_type);

  pir::Type grad_out_grad_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_grad_out_grad.dtype()), dense_grad_out_grad.dims(), dense_grad_out_grad.layout(), dense_grad_out_grad.lod(), dense_grad_out_grad.offset());
  argument_outputs.push_back(grad_out_grad_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void MultiplyDoubleGradOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, pir::Value y_, pir::Value grad_out_, pir::Value grad_x_grad_, pir::Value grad_y_grad_, pir::AttributeMap attributes) {
  VLOG(4) << "Start build MultiplyDoubleGradOp";


  IR_ENFORCE(
      attributes.find("axis") != attributes.end(),
          "'axis' Attribute is expected for MultiplyDoubleGradOp. ");
  int axis = attributes.at("axis").dyn_cast<pir::Int32Attribute>().data();


  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_, y_, grad_out_, grad_x_grad_, grad_y_grad_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_axis = pir::Int32Attribute::get(pir::IrContext::Instance(), axis);
  argument.AddAttribute("axis", attr_axis);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;
  paddle::dialect::DenseTensorType y = y_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)y;
  paddle::dialect::DenseTensorType grad_out = grad_out_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)grad_out;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);

  VLOG(4) << "Builder construction  dense_y";
  paddle::dialect::IrTensor ir_tensor_y(paddle::dialect::TransToPhiDataType(y.dtype()),
                                                      y.dims(),
                                                      y.data_layout(),
                                                      y.lod(),
                                                      y.offset());
  VLOG(4) << "Builder construction  meta_y";
  paddle::dialect::IrMetaTensor meta_y(&ir_tensor_y);

  VLOG(4) << "Builder construction  dense_grad_out";
  paddle::dialect::IrTensor ir_tensor_grad_out(paddle::dialect::TransToPhiDataType(grad_out.dtype()),
                                                      grad_out.dims(),
                                                      grad_out.data_layout(),
                                                      grad_out.lod(),
                                                      grad_out.offset());
  VLOG(4) << "Builder construction  meta_grad_out";
  paddle::dialect::IrMetaTensor meta_grad_out(&ir_tensor_grad_out);
  paddle::dialect::IrTensor dense_x_grad;
  paddle::dialect::IrMetaTensor meta_x_grad(&dense_x_grad);
  paddle::dialect::IrTensor dense_y_grad;
  paddle::dialect::IrMetaTensor meta_y_grad(&dense_y_grad);
  paddle::dialect::IrTensor dense_grad_out_grad;
  paddle::dialect::IrMetaTensor meta_grad_out_grad(&dense_grad_out_grad);

  phi::GeneralTernaryGradInferMeta(meta_x, meta_y, meta_grad_out, &meta_x_grad, &meta_y_grad, &meta_grad_out_grad);

  std::vector<pir::Type> argument_outputs;
  pir::Type x_grad_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_x_grad.dtype()), dense_x_grad.dims(), dense_x_grad.layout(), dense_x_grad.lod(), dense_x_grad.offset());
  argument_outputs.push_back(x_grad_dense_tensor_type);

  pir::Type y_grad_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_y_grad.dtype()), dense_y_grad.dims(), dense_y_grad.layout(), dense_y_grad.lod(), dense_y_grad.offset());
  argument_outputs.push_back(y_grad_dense_tensor_type);

  pir::Type grad_out_grad_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_grad_out_grad.dtype()), dense_grad_out_grad.dims(), dense_grad_out_grad.layout(), dense_grad_out_grad.lod(), dense_grad_out_grad.offset());
  argument_outputs.push_back(grad_out_grad_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void MultiplyDoubleGradOp::VerifySig() {}

void MultiplyDoubleGradOp::InferMeta( phi::InferMetaContext *infer_meta ) {
  auto fn = PD_INFER_META(phi::GeneralTernaryGradInferMeta);
  fn(infer_meta);
}

phi::DataType MultiplyDoubleGradOp::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: MultiplyDoubleGradOp";
  


  return expected_kernel_dtype;
}

const char *MultiplyDoubleGrad_Op::attributes_name[1] = { "axis" };

OpInfoTuple MultiplyDoubleGrad_Op::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("x", "paddle::dialect::DenseTensorType", false, false, false, true), paddle::dialect::OpInputInfo("y", "paddle::dialect::DenseTensorType", false, false, false, true), paddle::dialect::OpInputInfo("grad_out", "paddle::dialect::DenseTensorType", false, false, false, true), paddle::dialect::OpInputInfo("grad_x_grad", "paddle::dialect::DenseTensorType", true, false, false, true), paddle::dialect::OpInputInfo("grad_y_grad", "paddle::dialect::DenseTensorType", true, false, false, true) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = { paddle::dialect::OpAttributeInfo("axis", "pir::Int32Attribute", "") };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("x_grad", "paddle::dialect::DenseTensorType", false, false), paddle::dialect::OpOutputInfo("y_grad", "paddle::dialect::DenseTensorType", false, false), paddle::dialect::OpOutputInfo("grad_out_grad", "paddle::dialect::DenseTensorType", false, false) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("GeneralTernaryGradInferMeta", {"x", "y", "grad_out"}, "multiply_double_grad", {"x", "y", "grad_out", "grad_x_grad", "grad_y_grad", "axis"}, {}, {}, {{"grad_out_grad", "grad_x_grad"}}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "multiply_double_grad");
}

void MultiplyDoubleGrad_Op::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, pir::Value y_, pir::Value grad_out_, pir::Value grad_x_grad_, pir::Value grad_y_grad_, int axis) {
  VLOG(4) << "Start build MultiplyDoubleGrad_Op";



  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_, y_, grad_out_, grad_x_grad_, grad_y_grad_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_axis = pir::Int32Attribute::get(pir::IrContext::Instance(), axis);
  argument.AddAttribute("axis", attr_axis);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;
  paddle::dialect::DenseTensorType y = y_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)y;
  paddle::dialect::DenseTensorType grad_out = grad_out_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)grad_out;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);

  VLOG(4) << "Builder construction  dense_y";
  paddle::dialect::IrTensor ir_tensor_y(paddle::dialect::TransToPhiDataType(y.dtype()),
                                                      y.dims(),
                                                      y.data_layout(),
                                                      y.lod(),
                                                      y.offset());
  VLOG(4) << "Builder construction  meta_y";
  paddle::dialect::IrMetaTensor meta_y(&ir_tensor_y);

  VLOG(4) << "Builder construction  dense_grad_out";
  paddle::dialect::IrTensor ir_tensor_grad_out(paddle::dialect::TransToPhiDataType(grad_out.dtype()),
                                                      grad_out.dims(),
                                                      grad_out.data_layout(),
                                                      grad_out.lod(),
                                                      grad_out.offset());
  VLOG(4) << "Builder construction  meta_grad_out";
  paddle::dialect::IrMetaTensor meta_grad_out(&ir_tensor_grad_out);
  paddle::dialect::IrTensor dense_x_grad;
  paddle::dialect::IrMetaTensor meta_x_grad(&dense_x_grad);
  paddle::dialect::IrTensor dense_y_grad;
  paddle::dialect::IrMetaTensor meta_y_grad(&dense_y_grad);
  paddle::dialect::IrTensor dense_grad_out_grad;
  paddle::dialect::IrMetaTensor meta_grad_out_grad(&dense_grad_out_grad);

  phi::GeneralTernaryGradInferMeta(meta_x, meta_y, meta_grad_out, &meta_x_grad, &meta_y_grad, &meta_grad_out_grad);

  std::vector<pir::Type> argument_outputs;
  pir::Type x_grad_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_x_grad.dtype()), dense_x_grad.dims(), dense_x_grad.layout(), dense_x_grad.lod(), dense_x_grad.offset());
  argument_outputs.push_back(x_grad_dense_tensor_type);

  pir::Type y_grad_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_y_grad.dtype()), dense_y_grad.dims(), dense_y_grad.layout(), dense_y_grad.lod(), dense_y_grad.offset());
  argument_outputs.push_back(y_grad_dense_tensor_type);

  pir::Type grad_out_grad_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_grad_out_grad.dtype()), dense_grad_out_grad.dims(), dense_grad_out_grad.layout(), dense_grad_out_grad.lod(), dense_grad_out_grad.offset());
  argument_outputs.push_back(grad_out_grad_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void MultiplyDoubleGrad_Op::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, pir::Value y_, pir::Value grad_out_, pir::Value grad_x_grad_, pir::Value grad_y_grad_, pir::AttributeMap attributes) {
  VLOG(4) << "Start build MultiplyDoubleGrad_Op";


  IR_ENFORCE(
      attributes.find("axis") != attributes.end(),
          "'axis' Attribute is expected for MultiplyDoubleGrad_Op. ");
  int axis = attributes.at("axis").dyn_cast<pir::Int32Attribute>().data();


  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_, y_, grad_out_, grad_x_grad_, grad_y_grad_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_axis = pir::Int32Attribute::get(pir::IrContext::Instance(), axis);
  argument.AddAttribute("axis", attr_axis);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;
  paddle::dialect::DenseTensorType y = y_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)y;
  paddle::dialect::DenseTensorType grad_out = grad_out_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)grad_out;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);

  VLOG(4) << "Builder construction  dense_y";
  paddle::dialect::IrTensor ir_tensor_y(paddle::dialect::TransToPhiDataType(y.dtype()),
                                                      y.dims(),
                                                      y.data_layout(),
                                                      y.lod(),
                                                      y.offset());
  VLOG(4) << "Builder construction  meta_y";
  paddle::dialect::IrMetaTensor meta_y(&ir_tensor_y);

  VLOG(4) << "Builder construction  dense_grad_out";
  paddle::dialect::IrTensor ir_tensor_grad_out(paddle::dialect::TransToPhiDataType(grad_out.dtype()),
                                                      grad_out.dims(),
                                                      grad_out.data_layout(),
                                                      grad_out.lod(),
                                                      grad_out.offset());
  VLOG(4) << "Builder construction  meta_grad_out";
  paddle::dialect::IrMetaTensor meta_grad_out(&ir_tensor_grad_out);
  paddle::dialect::IrTensor dense_x_grad;
  paddle::dialect::IrMetaTensor meta_x_grad(&dense_x_grad);
  paddle::dialect::IrTensor dense_y_grad;
  paddle::dialect::IrMetaTensor meta_y_grad(&dense_y_grad);
  paddle::dialect::IrTensor dense_grad_out_grad;
  paddle::dialect::IrMetaTensor meta_grad_out_grad(&dense_grad_out_grad);

  phi::GeneralTernaryGradInferMeta(meta_x, meta_y, meta_grad_out, &meta_x_grad, &meta_y_grad, &meta_grad_out_grad);

  std::vector<pir::Type> argument_outputs;
  pir::Type x_grad_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_x_grad.dtype()), dense_x_grad.dims(), dense_x_grad.layout(), dense_x_grad.lod(), dense_x_grad.offset());
  argument_outputs.push_back(x_grad_dense_tensor_type);

  pir::Type y_grad_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_y_grad.dtype()), dense_y_grad.dims(), dense_y_grad.layout(), dense_y_grad.lod(), dense_y_grad.offset());
  argument_outputs.push_back(y_grad_dense_tensor_type);

  pir::Type grad_out_grad_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_grad_out_grad.dtype()), dense_grad_out_grad.dims(), dense_grad_out_grad.layout(), dense_grad_out_grad.lod(), dense_grad_out_grad.offset());
  argument_outputs.push_back(grad_out_grad_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void MultiplyDoubleGrad_Op::VerifySig() {}

void MultiplyDoubleGrad_Op::InferMeta( phi::InferMetaContext *infer_meta ) {
  auto fn = PD_INFER_META(phi::GeneralTernaryGradInferMeta);
  fn(infer_meta);
}

phi::DataType MultiplyDoubleGrad_Op::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: MultiplyDoubleGrad_Op";
  


  return expected_kernel_dtype;
}

const char *MultiplyGradOp::attributes_name[1] = { "axis" };

OpInfoTuple MultiplyGradOp::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("x", "paddle::dialect::DenseTensorType", false, false, false, true), paddle::dialect::OpInputInfo("y", "paddle::dialect::DenseTensorType", false, false, false, true), paddle::dialect::OpInputInfo("out_grad", "paddle::dialect::DenseTensorType", false, false, false, true) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = { paddle::dialect::OpAttributeInfo("axis", "pir::Int32Attribute", "") };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("x_grad", "paddle::dialect::DenseTensorType", false, false), paddle::dialect::OpOutputInfo("y_grad", "paddle::dialect::DenseTensorType", false, false) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("GeneralBinaryGradInferMeta", {"x", "y"}, "multiply_grad", {"x", "y", "out_grad", "axis"}, {}, {}, {}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "multiply_grad");
}

void MultiplyGradOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, pir::Value y_, pir::Value out_grad_, int axis) {
  VLOG(4) << "Start build MultiplyGradOp";



  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_, y_, out_grad_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_axis = pir::Int32Attribute::get(pir::IrContext::Instance(), axis);
  argument.AddAttribute("axis", attr_axis);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;
  paddle::dialect::DenseTensorType y = y_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)y;
  paddle::dialect::DenseTensorType out_grad = out_grad_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)out_grad;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);

  VLOG(4) << "Builder construction  dense_y";
  paddle::dialect::IrTensor ir_tensor_y(paddle::dialect::TransToPhiDataType(y.dtype()),
                                                      y.dims(),
                                                      y.data_layout(),
                                                      y.lod(),
                                                      y.offset());
  VLOG(4) << "Builder construction  meta_y";
  paddle::dialect::IrMetaTensor meta_y(&ir_tensor_y);
  paddle::dialect::IrTensor dense_x_grad;
  paddle::dialect::IrMetaTensor meta_x_grad(&dense_x_grad);
  paddle::dialect::IrTensor dense_y_grad;
  paddle::dialect::IrMetaTensor meta_y_grad(&dense_y_grad);

  phi::GeneralBinaryGradInferMeta(meta_x, meta_y, &meta_x_grad, &meta_y_grad);

  std::vector<pir::Type> argument_outputs;
  pir::Type x_grad_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_x_grad.dtype()), dense_x_grad.dims(), dense_x_grad.layout(), dense_x_grad.lod(), dense_x_grad.offset());
  argument_outputs.push_back(x_grad_dense_tensor_type);

  pir::Type y_grad_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_y_grad.dtype()), dense_y_grad.dims(), dense_y_grad.layout(), dense_y_grad.lod(), dense_y_grad.offset());
  argument_outputs.push_back(y_grad_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void MultiplyGradOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, pir::Value y_, pir::Value out_grad_, pir::AttributeMap attributes) {
  VLOG(4) << "Start build MultiplyGradOp";


  IR_ENFORCE(
      attributes.find("axis") != attributes.end(),
          "'axis' Attribute is expected for MultiplyGradOp. ");
  int axis = attributes.at("axis").dyn_cast<pir::Int32Attribute>().data();


  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_, y_, out_grad_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_axis = pir::Int32Attribute::get(pir::IrContext::Instance(), axis);
  argument.AddAttribute("axis", attr_axis);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;
  paddle::dialect::DenseTensorType y = y_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)y;
  paddle::dialect::DenseTensorType out_grad = out_grad_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)out_grad;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);

  VLOG(4) << "Builder construction  dense_y";
  paddle::dialect::IrTensor ir_tensor_y(paddle::dialect::TransToPhiDataType(y.dtype()),
                                                      y.dims(),
                                                      y.data_layout(),
                                                      y.lod(),
                                                      y.offset());
  VLOG(4) << "Builder construction  meta_y";
  paddle::dialect::IrMetaTensor meta_y(&ir_tensor_y);
  paddle::dialect::IrTensor dense_x_grad;
  paddle::dialect::IrMetaTensor meta_x_grad(&dense_x_grad);
  paddle::dialect::IrTensor dense_y_grad;
  paddle::dialect::IrMetaTensor meta_y_grad(&dense_y_grad);

  phi::GeneralBinaryGradInferMeta(meta_x, meta_y, &meta_x_grad, &meta_y_grad);

  std::vector<pir::Type> argument_outputs;
  pir::Type x_grad_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_x_grad.dtype()), dense_x_grad.dims(), dense_x_grad.layout(), dense_x_grad.lod(), dense_x_grad.offset());
  argument_outputs.push_back(x_grad_dense_tensor_type);

  pir::Type y_grad_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_y_grad.dtype()), dense_y_grad.dims(), dense_y_grad.layout(), dense_y_grad.lod(), dense_y_grad.offset());
  argument_outputs.push_back(y_grad_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void MultiplyGradOp::VerifySig() {}

void MultiplyGradOp::InferMeta( phi::InferMetaContext *infer_meta ) {
  auto fn = PD_INFER_META(phi::GeneralBinaryGradInferMeta);
  fn(infer_meta);
}

phi::DataType MultiplyGradOp::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: MultiplyGradOp";
  


  return expected_kernel_dtype;
}

const char *MultiplyTripleGradOp::attributes_name[1] = { "axis" };

OpInfoTuple MultiplyTripleGradOp::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("x", "paddle::dialect::DenseTensorType", false, false, false, false), paddle::dialect::OpInputInfo("y", "paddle::dialect::DenseTensorType", false, false, false, false), paddle::dialect::OpInputInfo("fwd_grad_out", "paddle::dialect::DenseTensorType", false, false, false, false), paddle::dialect::OpInputInfo("fwd_grad_grad_x", "paddle::dialect::DenseTensorType", true, false, false, false), paddle::dialect::OpInputInfo("fwd_grad_grad_y", "paddle::dialect::DenseTensorType", true, false, false, false), paddle::dialect::OpInputInfo("grad_x_grad", "paddle::dialect::DenseTensorType", true, false, false, false), paddle::dialect::OpInputInfo("grad_y_grad", "paddle::dialect::DenseTensorType", true, false, false, false), paddle::dialect::OpInputInfo("grad_grad_out_grad", "paddle::dialect::DenseTensorType", true, false, false, false) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = { paddle::dialect::OpAttributeInfo("axis", "pir::Int32Attribute", "") };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("x_grad", "paddle::dialect::DenseTensorType", false, false), paddle::dialect::OpOutputInfo("y_grad", "paddle::dialect::DenseTensorType", false, false), paddle::dialect::OpOutputInfo("fwd_grad_out_grad", "paddle::dialect::DenseTensorType", false, false), paddle::dialect::OpOutputInfo("fwd_grad_grad_x_grad", "paddle::dialect::DenseTensorType", false, false), paddle::dialect::OpOutputInfo("fwd_grad_grad_y_grad", "paddle::dialect::DenseTensorType", false, false) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("GeneralQuinaryGradInferMeta", {"x", "y", "fwd_grad_out", "fwd_grad_grad_x", "fwd_grad_grad_y"}, "multiply_triple_grad", {"x", "y", "fwd_grad_out", "fwd_grad_grad_x", "fwd_grad_grad_y", "grad_x_grad", "grad_y_grad", "grad_grad_out_grad", "axis"}, {}, {}, {}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "multiply_triple_grad");
}

void MultiplyTripleGradOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, pir::Value y_, pir::Value fwd_grad_out_, pir::Value fwd_grad_grad_x_, pir::Value fwd_grad_grad_y_, pir::Value grad_x_grad_, pir::Value grad_y_grad_, pir::Value grad_grad_out_grad_, int axis) {
  VLOG(4) << "Start build MultiplyTripleGradOp";



  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_, y_, fwd_grad_out_, fwd_grad_grad_x_, fwd_grad_grad_y_, grad_x_grad_, grad_y_grad_, grad_grad_out_grad_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_axis = pir::Int32Attribute::get(pir::IrContext::Instance(), axis);
  argument.AddAttribute("axis", attr_axis);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;
  paddle::dialect::DenseTensorType y = y_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)y;
  paddle::dialect::DenseTensorType fwd_grad_out = fwd_grad_out_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)fwd_grad_out;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);

  VLOG(4) << "Builder construction  dense_y";
  paddle::dialect::IrTensor ir_tensor_y(paddle::dialect::TransToPhiDataType(y.dtype()),
                                                      y.dims(),
                                                      y.data_layout(),
                                                      y.lod(),
                                                      y.offset());
  VLOG(4) << "Builder construction  meta_y";
  paddle::dialect::IrMetaTensor meta_y(&ir_tensor_y);

  VLOG(4) << "Builder construction  dense_fwd_grad_out";
  paddle::dialect::IrTensor ir_tensor_fwd_grad_out(paddle::dialect::TransToPhiDataType(fwd_grad_out.dtype()),
                                                      fwd_grad_out.dims(),
                                                      fwd_grad_out.data_layout(),
                                                      fwd_grad_out.lod(),
                                                      fwd_grad_out.offset());
  VLOG(4) << "Builder construction  meta_fwd_grad_out";
  paddle::dialect::IrMetaTensor meta_fwd_grad_out(&ir_tensor_fwd_grad_out);

  paddle::dialect::IrMetaTensor meta_fwd_grad_grad_x;
  paddle::dialect::IrTensor ir_tensor_fwd_grad_grad_x;
  if (fwd_grad_grad_x_.impl() != nullptr) {
    paddle::dialect::DenseTensorType fwd_grad_grad_x = fwd_grad_grad_x_.type().dyn_cast<paddle::dialect::DenseTensorType>();
    VLOG(4) << "Builder construction  dense_fwd_grad_grad_x";
    ir_tensor_fwd_grad_grad_x = paddle::dialect::IrTensor(paddle::dialect::TransToPhiDataType(fwd_grad_grad_x.dtype()),
                                                        fwd_grad_grad_x.dims(),
                                                        fwd_grad_grad_x.data_layout(),
                                                        fwd_grad_grad_x.lod(),
                                                        fwd_grad_grad_x.offset());
    VLOG(4) << "Builder construction  meta_fwd_grad_grad_x";
    meta_fwd_grad_grad_x = paddle::dialect::IrMetaTensor(&ir_tensor_fwd_grad_grad_x);
  }


  paddle::dialect::IrMetaTensor meta_fwd_grad_grad_y;
  paddle::dialect::IrTensor ir_tensor_fwd_grad_grad_y;
  if (fwd_grad_grad_y_.impl() != nullptr) {
    paddle::dialect::DenseTensorType fwd_grad_grad_y = fwd_grad_grad_y_.type().dyn_cast<paddle::dialect::DenseTensorType>();
    VLOG(4) << "Builder construction  dense_fwd_grad_grad_y";
    ir_tensor_fwd_grad_grad_y = paddle::dialect::IrTensor(paddle::dialect::TransToPhiDataType(fwd_grad_grad_y.dtype()),
                                                        fwd_grad_grad_y.dims(),
                                                        fwd_grad_grad_y.data_layout(),
                                                        fwd_grad_grad_y.lod(),
                                                        fwd_grad_grad_y.offset());
    VLOG(4) << "Builder construction  meta_fwd_grad_grad_y";
    meta_fwd_grad_grad_y = paddle::dialect::IrMetaTensor(&ir_tensor_fwd_grad_grad_y);
  }

  paddle::dialect::IrTensor dense_x_grad;
  paddle::dialect::IrMetaTensor meta_x_grad(&dense_x_grad);
  paddle::dialect::IrTensor dense_y_grad;
  paddle::dialect::IrMetaTensor meta_y_grad(&dense_y_grad);
  paddle::dialect::IrTensor dense_fwd_grad_out_grad;
  paddle::dialect::IrMetaTensor meta_fwd_grad_out_grad(&dense_fwd_grad_out_grad);
  paddle::dialect::IrTensor dense_fwd_grad_grad_x_grad;
  paddle::dialect::IrMetaTensor meta_fwd_grad_grad_x_grad(&dense_fwd_grad_grad_x_grad);
  paddle::dialect::IrTensor dense_fwd_grad_grad_y_grad;
  paddle::dialect::IrMetaTensor meta_fwd_grad_grad_y_grad(&dense_fwd_grad_grad_y_grad);

  phi::GeneralQuinaryGradInferMeta(meta_x, meta_y, meta_fwd_grad_out, meta_fwd_grad_grad_x, meta_fwd_grad_grad_y, &meta_x_grad, &meta_y_grad, &meta_fwd_grad_out_grad, &meta_fwd_grad_grad_x_grad, &meta_fwd_grad_grad_y_grad);

  std::vector<pir::Type> argument_outputs;
  pir::Type x_grad_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_x_grad.dtype()), dense_x_grad.dims(), dense_x_grad.layout(), dense_x_grad.lod(), dense_x_grad.offset());
  argument_outputs.push_back(x_grad_dense_tensor_type);

  pir::Type y_grad_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_y_grad.dtype()), dense_y_grad.dims(), dense_y_grad.layout(), dense_y_grad.lod(), dense_y_grad.offset());
  argument_outputs.push_back(y_grad_dense_tensor_type);

  pir::Type fwd_grad_out_grad_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_fwd_grad_out_grad.dtype()), dense_fwd_grad_out_grad.dims(), dense_fwd_grad_out_grad.layout(), dense_fwd_grad_out_grad.lod(), dense_fwd_grad_out_grad.offset());
  argument_outputs.push_back(fwd_grad_out_grad_dense_tensor_type);

  pir::Type fwd_grad_grad_x_grad_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_fwd_grad_grad_x_grad.dtype()), dense_fwd_grad_grad_x_grad.dims(), dense_fwd_grad_grad_x_grad.layout(), dense_fwd_grad_grad_x_grad.lod(), dense_fwd_grad_grad_x_grad.offset());
  argument_outputs.push_back(fwd_grad_grad_x_grad_dense_tensor_type);

  pir::Type fwd_grad_grad_y_grad_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_fwd_grad_grad_y_grad.dtype()), dense_fwd_grad_grad_y_grad.dims(), dense_fwd_grad_grad_y_grad.layout(), dense_fwd_grad_grad_y_grad.lod(), dense_fwd_grad_grad_y_grad.offset());
  argument_outputs.push_back(fwd_grad_grad_y_grad_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void MultiplyTripleGradOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, pir::Value y_, pir::Value fwd_grad_out_, pir::Value fwd_grad_grad_x_, pir::Value fwd_grad_grad_y_, pir::Value grad_x_grad_, pir::Value grad_y_grad_, pir::Value grad_grad_out_grad_, pir::AttributeMap attributes) {
  VLOG(4) << "Start build MultiplyTripleGradOp";


  IR_ENFORCE(
      attributes.find("axis") != attributes.end(),
          "'axis' Attribute is expected for MultiplyTripleGradOp. ");
  int axis = attributes.at("axis").dyn_cast<pir::Int32Attribute>().data();


  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_, y_, fwd_grad_out_, fwd_grad_grad_x_, fwd_grad_grad_y_, grad_x_grad_, grad_y_grad_, grad_grad_out_grad_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_axis = pir::Int32Attribute::get(pir::IrContext::Instance(), axis);
  argument.AddAttribute("axis", attr_axis);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;
  paddle::dialect::DenseTensorType y = y_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)y;
  paddle::dialect::DenseTensorType fwd_grad_out = fwd_grad_out_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)fwd_grad_out;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);

  VLOG(4) << "Builder construction  dense_y";
  paddle::dialect::IrTensor ir_tensor_y(paddle::dialect::TransToPhiDataType(y.dtype()),
                                                      y.dims(),
                                                      y.data_layout(),
                                                      y.lod(),
                                                      y.offset());
  VLOG(4) << "Builder construction  meta_y";
  paddle::dialect::IrMetaTensor meta_y(&ir_tensor_y);

  VLOG(4) << "Builder construction  dense_fwd_grad_out";
  paddle::dialect::IrTensor ir_tensor_fwd_grad_out(paddle::dialect::TransToPhiDataType(fwd_grad_out.dtype()),
                                                      fwd_grad_out.dims(),
                                                      fwd_grad_out.data_layout(),
                                                      fwd_grad_out.lod(),
                                                      fwd_grad_out.offset());
  VLOG(4) << "Builder construction  meta_fwd_grad_out";
  paddle::dialect::IrMetaTensor meta_fwd_grad_out(&ir_tensor_fwd_grad_out);

  paddle::dialect::IrMetaTensor meta_fwd_grad_grad_x;
  paddle::dialect::IrTensor ir_tensor_fwd_grad_grad_x;
  if (fwd_grad_grad_x_.impl() != nullptr) {
    paddle::dialect::DenseTensorType fwd_grad_grad_x = fwd_grad_grad_x_.type().dyn_cast<paddle::dialect::DenseTensorType>();
    VLOG(4) << "Builder construction  dense_fwd_grad_grad_x";
    ir_tensor_fwd_grad_grad_x = paddle::dialect::IrTensor(paddle::dialect::TransToPhiDataType(fwd_grad_grad_x.dtype()),
                                                        fwd_grad_grad_x.dims(),
                                                        fwd_grad_grad_x.data_layout(),
                                                        fwd_grad_grad_x.lod(),
                                                        fwd_grad_grad_x.offset());
    VLOG(4) << "Builder construction  meta_fwd_grad_grad_x";
    meta_fwd_grad_grad_x = paddle::dialect::IrMetaTensor(&ir_tensor_fwd_grad_grad_x);
  }


  paddle::dialect::IrMetaTensor meta_fwd_grad_grad_y;
  paddle::dialect::IrTensor ir_tensor_fwd_grad_grad_y;
  if (fwd_grad_grad_y_.impl() != nullptr) {
    paddle::dialect::DenseTensorType fwd_grad_grad_y = fwd_grad_grad_y_.type().dyn_cast<paddle::dialect::DenseTensorType>();
    VLOG(4) << "Builder construction  dense_fwd_grad_grad_y";
    ir_tensor_fwd_grad_grad_y = paddle::dialect::IrTensor(paddle::dialect::TransToPhiDataType(fwd_grad_grad_y.dtype()),
                                                        fwd_grad_grad_y.dims(),
                                                        fwd_grad_grad_y.data_layout(),
                                                        fwd_grad_grad_y.lod(),
                                                        fwd_grad_grad_y.offset());
    VLOG(4) << "Builder construction  meta_fwd_grad_grad_y";
    meta_fwd_grad_grad_y = paddle::dialect::IrMetaTensor(&ir_tensor_fwd_grad_grad_y);
  }

  paddle::dialect::IrTensor dense_x_grad;
  paddle::dialect::IrMetaTensor meta_x_grad(&dense_x_grad);
  paddle::dialect::IrTensor dense_y_grad;
  paddle::dialect::IrMetaTensor meta_y_grad(&dense_y_grad);
  paddle::dialect::IrTensor dense_fwd_grad_out_grad;
  paddle::dialect::IrMetaTensor meta_fwd_grad_out_grad(&dense_fwd_grad_out_grad);
  paddle::dialect::IrTensor dense_fwd_grad_grad_x_grad;
  paddle::dialect::IrMetaTensor meta_fwd_grad_grad_x_grad(&dense_fwd_grad_grad_x_grad);
  paddle::dialect::IrTensor dense_fwd_grad_grad_y_grad;
  paddle::dialect::IrMetaTensor meta_fwd_grad_grad_y_grad(&dense_fwd_grad_grad_y_grad);

  phi::GeneralQuinaryGradInferMeta(meta_x, meta_y, meta_fwd_grad_out, meta_fwd_grad_grad_x, meta_fwd_grad_grad_y, &meta_x_grad, &meta_y_grad, &meta_fwd_grad_out_grad, &meta_fwd_grad_grad_x_grad, &meta_fwd_grad_grad_y_grad);

  std::vector<pir::Type> argument_outputs;
  pir::Type x_grad_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_x_grad.dtype()), dense_x_grad.dims(), dense_x_grad.layout(), dense_x_grad.lod(), dense_x_grad.offset());
  argument_outputs.push_back(x_grad_dense_tensor_type);

  pir::Type y_grad_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_y_grad.dtype()), dense_y_grad.dims(), dense_y_grad.layout(), dense_y_grad.lod(), dense_y_grad.offset());
  argument_outputs.push_back(y_grad_dense_tensor_type);

  pir::Type fwd_grad_out_grad_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_fwd_grad_out_grad.dtype()), dense_fwd_grad_out_grad.dims(), dense_fwd_grad_out_grad.layout(), dense_fwd_grad_out_grad.lod(), dense_fwd_grad_out_grad.offset());
  argument_outputs.push_back(fwd_grad_out_grad_dense_tensor_type);

  pir::Type fwd_grad_grad_x_grad_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_fwd_grad_grad_x_grad.dtype()), dense_fwd_grad_grad_x_grad.dims(), dense_fwd_grad_grad_x_grad.layout(), dense_fwd_grad_grad_x_grad.lod(), dense_fwd_grad_grad_x_grad.offset());
  argument_outputs.push_back(fwd_grad_grad_x_grad_dense_tensor_type);

  pir::Type fwd_grad_grad_y_grad_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_fwd_grad_grad_y_grad.dtype()), dense_fwd_grad_grad_y_grad.dims(), dense_fwd_grad_grad_y_grad.layout(), dense_fwd_grad_grad_y_grad.lod(), dense_fwd_grad_grad_y_grad.offset());
  argument_outputs.push_back(fwd_grad_grad_y_grad_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void MultiplyTripleGradOp::VerifySig() {}

void MultiplyTripleGradOp::InferMeta( phi::InferMetaContext *infer_meta ) {
  auto fn = PD_INFER_META(phi::GeneralQuinaryGradInferMeta);
  fn(infer_meta);
}

phi::DataType MultiplyTripleGradOp::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: MultiplyTripleGradOp";
  


  return expected_kernel_dtype;
}

const char *NceGradOp::attributes_name[8] = { "num_total_classes", "custom_neg_classes", "num_neg_samples", "sampler", "seed", "is_sparse", "remote_prefetch", "is_test" };

OpInfoTuple NceGradOp::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("input", "paddle::dialect::DenseTensorType", false, false, false, false), paddle::dialect::OpInputInfo("label", "paddle::dialect::DenseTensorType", false, false, false, false), paddle::dialect::OpInputInfo("bias", "paddle::dialect::DenseTensorType", true, false, false, false), paddle::dialect::OpInputInfo("weight", "paddle::dialect::DenseTensorType", false, false, false, false), paddle::dialect::OpInputInfo("sample_logits", "paddle::dialect::DenseTensorType", false, false, false, false), paddle::dialect::OpInputInfo("sample_labels", "paddle::dialect::DenseTensorType", false, false, false, false), paddle::dialect::OpInputInfo("sample_weight", "paddle::dialect::DenseTensorType", true, false, false, false), paddle::dialect::OpInputInfo("custom_dist_probs", "paddle::dialect::DenseTensorType", true, false, false, false), paddle::dialect::OpInputInfo("custom_dist_alias", "paddle::dialect::DenseTensorType", true, false, false, false), paddle::dialect::OpInputInfo("custom_dist_alias_probs", "paddle::dialect::DenseTensorType", true, false, false, false), paddle::dialect::OpInputInfo("cost_grad", "paddle::dialect::DenseTensorType", false, false, false, false) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = { paddle::dialect::OpAttributeInfo("num_total_classes", "pir::Int32Attribute", ""), paddle::dialect::OpAttributeInfo("custom_neg_classes", "pir::ArrayAttribute<pir::Int32Attribute>", ""), paddle::dialect::OpAttributeInfo("num_neg_samples", "pir::Int32Attribute", ""), paddle::dialect::OpAttributeInfo("sampler", "pir::Int32Attribute", ""), paddle::dialect::OpAttributeInfo("seed", "pir::Int32Attribute", ""), paddle::dialect::OpAttributeInfo("is_sparse", "pir::BoolAttribute", ""), paddle::dialect::OpAttributeInfo("remote_prefetch", "pir::BoolAttribute", ""), paddle::dialect::OpAttributeInfo("is_test", "pir::BoolAttribute", "") };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("input_grad", "paddle::dialect::DenseTensorType", false, false), paddle::dialect::OpOutputInfo("bias_grad", "paddle::dialect::DenseTensorType", false, false), paddle::dialect::OpOutputInfo("weight_grad", "paddle::dialect::DenseTensorType", false, false) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("NceGradInferMeta", {"input", "bias", "weight"}, "nce_grad", {"input", "label", "bias", "weight", "sample_logits", "sample_labels", "sample_weight", "custom_dist_probs", "custom_dist_alias", "custom_dist_alias_probs", "cost_grad", "num_total_classes", "custom_neg_classes", "num_neg_samples", "sampler", "seed", "is_sparse", "remote_prefetch", "is_test"}, {"input"}, {}, {}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "nce_grad");
}

void NceGradOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value input_, pir::Value label_, pir::Value bias_, pir::Value weight_, pir::Value sample_logits_, pir::Value sample_labels_, pir::Value sample_weight_, pir::Value custom_dist_probs_, pir::Value custom_dist_alias_, pir::Value custom_dist_alias_probs_, pir::Value cost_grad_, int num_total_classes, const std::vector<int>& custom_neg_classes, int num_neg_samples, int sampler, int seed, bool is_sparse, bool remote_prefetch, bool is_test) {
  VLOG(4) << "Start build NceGradOp";



  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {input_, label_, bias_, weight_, sample_logits_, sample_labels_, sample_weight_, custom_dist_probs_, custom_dist_alias_, custom_dist_alias_probs_, cost_grad_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_num_total_classes = pir::Int32Attribute::get(pir::IrContext::Instance(), num_total_classes);
  argument.AddAttribute("num_total_classes", attr_num_total_classes);
  std::vector<pir::Attribute> vec_custom_neg_classes;
  for (size_t i = 0; i < static_cast<size_t>(custom_neg_classes.size()); i++) {
      pir::Attribute attr_custom_neg_classes = pir::Int32Attribute::get(pir::IrContext::Instance(), custom_neg_classes[i]);

    vec_custom_neg_classes.push_back(attr_custom_neg_classes);
  }
  pir::Attribute attr_custom_neg_classes = pir::ArrayAttribute::get(pir::IrContext::Instance(), vec_custom_neg_classes);
  argument.AddAttribute("custom_neg_classes", attr_custom_neg_classes);
  pir::Attribute attr_num_neg_samples = pir::Int32Attribute::get(pir::IrContext::Instance(), num_neg_samples);
  argument.AddAttribute("num_neg_samples", attr_num_neg_samples);
  pir::Attribute attr_sampler = pir::Int32Attribute::get(pir::IrContext::Instance(), sampler);
  argument.AddAttribute("sampler", attr_sampler);
  pir::Attribute attr_seed = pir::Int32Attribute::get(pir::IrContext::Instance(), seed);
  argument.AddAttribute("seed", attr_seed);
  pir::Attribute attr_is_sparse = pir::BoolAttribute::get(pir::IrContext::Instance(), is_sparse);
  argument.AddAttribute("is_sparse", attr_is_sparse);
  pir::Attribute attr_remote_prefetch = pir::BoolAttribute::get(pir::IrContext::Instance(), remote_prefetch);
  argument.AddAttribute("remote_prefetch", attr_remote_prefetch);
  pir::Attribute attr_is_test = pir::BoolAttribute::get(pir::IrContext::Instance(), is_test);
  argument.AddAttribute("is_test", attr_is_test);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType input = input_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)input;
  paddle::dialect::DenseTensorType label = label_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)label;
  paddle::dialect::DenseTensorType weight = weight_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)weight;
  paddle::dialect::DenseTensorType sample_logits = sample_logits_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)sample_logits;
  paddle::dialect::DenseTensorType sample_labels = sample_labels_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)sample_labels;
  paddle::dialect::DenseTensorType cost_grad = cost_grad_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)cost_grad;

  VLOG(4) << "Builder construction  dense_input";
  paddle::dialect::IrTensor ir_tensor_input(paddle::dialect::TransToPhiDataType(input.dtype()),
                                                      input.dims(),
                                                      input.data_layout(),
                                                      input.lod(),
                                                      input.offset());
  VLOG(4) << "Builder construction  meta_input";
  paddle::dialect::IrMetaTensor meta_input(&ir_tensor_input);

  paddle::dialect::IrMetaTensor meta_bias;
  paddle::dialect::IrTensor ir_tensor_bias;
  if (bias_.impl() != nullptr) {
    paddle::dialect::DenseTensorType bias = bias_.type().dyn_cast<paddle::dialect::DenseTensorType>();
    VLOG(4) << "Builder construction  dense_bias";
    ir_tensor_bias = paddle::dialect::IrTensor(paddle::dialect::TransToPhiDataType(bias.dtype()),
                                                        bias.dims(),
                                                        bias.data_layout(),
                                                        bias.lod(),
                                                        bias.offset());
    VLOG(4) << "Builder construction  meta_bias";
    meta_bias = paddle::dialect::IrMetaTensor(&ir_tensor_bias);
  }


  VLOG(4) << "Builder construction  dense_weight";
  paddle::dialect::IrTensor ir_tensor_weight(paddle::dialect::TransToPhiDataType(weight.dtype()),
                                                      weight.dims(),
                                                      weight.data_layout(),
                                                      weight.lod(),
                                                      weight.offset());
  VLOG(4) << "Builder construction  meta_weight";
  paddle::dialect::IrMetaTensor meta_weight(&ir_tensor_weight);
  paddle::dialect::IrTensor dense_input_grad;
  paddle::dialect::IrMetaTensor meta_input_grad(&dense_input_grad);
  paddle::dialect::IrTensor dense_bias_grad;
  paddle::dialect::IrMetaTensor meta_bias_grad(&dense_bias_grad);
  paddle::dialect::IrTensor dense_weight_grad;
  paddle::dialect::IrMetaTensor meta_weight_grad(&dense_weight_grad);

  phi::NceGradInferMeta(meta_input, meta_bias, meta_weight, &meta_input_grad, &meta_bias_grad, &meta_weight_grad);

  std::vector<pir::Type> argument_outputs;
  pir::Type input_grad_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_input_grad.dtype()), dense_input_grad.dims(), dense_input_grad.layout(), dense_input_grad.lod(), dense_input_grad.offset());
  argument_outputs.push_back(input_grad_dense_tensor_type);

  pir::Type bias_grad_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_bias_grad.dtype()), dense_bias_grad.dims(), dense_bias_grad.layout(), dense_bias_grad.lod(), dense_bias_grad.offset());
  argument_outputs.push_back(bias_grad_dense_tensor_type);

  pir::Type weight_grad_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_weight_grad.dtype()), dense_weight_grad.dims(), dense_weight_grad.layout(), dense_weight_grad.lod(), dense_weight_grad.offset());
  argument_outputs.push_back(weight_grad_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void NceGradOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value input_, pir::Value label_, pir::Value bias_, pir::Value weight_, pir::Value sample_logits_, pir::Value sample_labels_, pir::Value sample_weight_, pir::Value custom_dist_probs_, pir::Value custom_dist_alias_, pir::Value custom_dist_alias_probs_, pir::Value cost_grad_, pir::AttributeMap attributes) {
  VLOG(4) << "Start build NceGradOp";


  IR_ENFORCE(
      attributes.find("num_total_classes") != attributes.end(),
          "'num_total_classes' Attribute is expected for NceGradOp. ");
  int num_total_classes = attributes.at("num_total_classes").dyn_cast<pir::Int32Attribute>().data();

  IR_ENFORCE(
      attributes.find("custom_neg_classes") != attributes.end(),
          "'custom_neg_classes' Attribute is expected for NceGradOp. ");
  std::vector<int> custom_neg_classes;
  for (size_t i = 0; i < attributes.at("custom_neg_classes").dyn_cast<pir::ArrayAttribute>().size(); i++) {
    custom_neg_classes.push_back(attributes.at("custom_neg_classes").dyn_cast<pir::ArrayAttribute>().at(i).dyn_cast<pir::Int32Attribute>().data());
  }

  IR_ENFORCE(
      attributes.find("num_neg_samples") != attributes.end(),
          "'num_neg_samples' Attribute is expected for NceGradOp. ");
  int num_neg_samples = attributes.at("num_neg_samples").dyn_cast<pir::Int32Attribute>().data();

  IR_ENFORCE(
      attributes.find("sampler") != attributes.end(),
          "'sampler' Attribute is expected for NceGradOp. ");
  int sampler = attributes.at("sampler").dyn_cast<pir::Int32Attribute>().data();

  IR_ENFORCE(
      attributes.find("seed") != attributes.end(),
          "'seed' Attribute is expected for NceGradOp. ");
  int seed = attributes.at("seed").dyn_cast<pir::Int32Attribute>().data();

  IR_ENFORCE(
      attributes.find("is_sparse") != attributes.end(),
          "'is_sparse' Attribute is expected for NceGradOp. ");
  bool is_sparse = attributes.at("is_sparse").dyn_cast<pir::BoolAttribute>().data();

  IR_ENFORCE(
      attributes.find("remote_prefetch") != attributes.end(),
          "'remote_prefetch' Attribute is expected for NceGradOp. ");
  bool remote_prefetch = attributes.at("remote_prefetch").dyn_cast<pir::BoolAttribute>().data();

  IR_ENFORCE(
      attributes.find("is_test") != attributes.end(),
          "'is_test' Attribute is expected for NceGradOp. ");
  bool is_test = attributes.at("is_test").dyn_cast<pir::BoolAttribute>().data();


  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {input_, label_, bias_, weight_, sample_logits_, sample_labels_, sample_weight_, custom_dist_probs_, custom_dist_alias_, custom_dist_alias_probs_, cost_grad_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_num_total_classes = pir::Int32Attribute::get(pir::IrContext::Instance(), num_total_classes);
  argument.AddAttribute("num_total_classes", attr_num_total_classes);
  std::vector<pir::Attribute> vec_custom_neg_classes;
  for (size_t i = 0; i < static_cast<size_t>(custom_neg_classes.size()); i++) {
      pir::Attribute attr_custom_neg_classes = pir::Int32Attribute::get(pir::IrContext::Instance(), custom_neg_classes[i]);

    vec_custom_neg_classes.push_back(attr_custom_neg_classes);
  }
  pir::Attribute attr_custom_neg_classes = pir::ArrayAttribute::get(pir::IrContext::Instance(), vec_custom_neg_classes);
  argument.AddAttribute("custom_neg_classes", attr_custom_neg_classes);
  pir::Attribute attr_num_neg_samples = pir::Int32Attribute::get(pir::IrContext::Instance(), num_neg_samples);
  argument.AddAttribute("num_neg_samples", attr_num_neg_samples);
  pir::Attribute attr_sampler = pir::Int32Attribute::get(pir::IrContext::Instance(), sampler);
  argument.AddAttribute("sampler", attr_sampler);
  pir::Attribute attr_seed = pir::Int32Attribute::get(pir::IrContext::Instance(), seed);
  argument.AddAttribute("seed", attr_seed);
  pir::Attribute attr_is_sparse = pir::BoolAttribute::get(pir::IrContext::Instance(), is_sparse);
  argument.AddAttribute("is_sparse", attr_is_sparse);
  pir::Attribute attr_remote_prefetch = pir::BoolAttribute::get(pir::IrContext::Instance(), remote_prefetch);
  argument.AddAttribute("remote_prefetch", attr_remote_prefetch);
  pir::Attribute attr_is_test = pir::BoolAttribute::get(pir::IrContext::Instance(), is_test);
  argument.AddAttribute("is_test", attr_is_test);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType input = input_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)input;
  paddle::dialect::DenseTensorType label = label_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)label;
  paddle::dialect::DenseTensorType weight = weight_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)weight;
  paddle::dialect::DenseTensorType sample_logits = sample_logits_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)sample_logits;
  paddle::dialect::DenseTensorType sample_labels = sample_labels_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)sample_labels;
  paddle::dialect::DenseTensorType cost_grad = cost_grad_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)cost_grad;

  VLOG(4) << "Builder construction  dense_input";
  paddle::dialect::IrTensor ir_tensor_input(paddle::dialect::TransToPhiDataType(input.dtype()),
                                                      input.dims(),
                                                      input.data_layout(),
                                                      input.lod(),
                                                      input.offset());
  VLOG(4) << "Builder construction  meta_input";
  paddle::dialect::IrMetaTensor meta_input(&ir_tensor_input);

  paddle::dialect::IrMetaTensor meta_bias;
  paddle::dialect::IrTensor ir_tensor_bias;
  if (bias_.impl() != nullptr) {
    paddle::dialect::DenseTensorType bias = bias_.type().dyn_cast<paddle::dialect::DenseTensorType>();
    VLOG(4) << "Builder construction  dense_bias";
    ir_tensor_bias = paddle::dialect::IrTensor(paddle::dialect::TransToPhiDataType(bias.dtype()),
                                                        bias.dims(),
                                                        bias.data_layout(),
                                                        bias.lod(),
                                                        bias.offset());
    VLOG(4) << "Builder construction  meta_bias";
    meta_bias = paddle::dialect::IrMetaTensor(&ir_tensor_bias);
  }


  VLOG(4) << "Builder construction  dense_weight";
  paddle::dialect::IrTensor ir_tensor_weight(paddle::dialect::TransToPhiDataType(weight.dtype()),
                                                      weight.dims(),
                                                      weight.data_layout(),
                                                      weight.lod(),
                                                      weight.offset());
  VLOG(4) << "Builder construction  meta_weight";
  paddle::dialect::IrMetaTensor meta_weight(&ir_tensor_weight);
  paddle::dialect::IrTensor dense_input_grad;
  paddle::dialect::IrMetaTensor meta_input_grad(&dense_input_grad);
  paddle::dialect::IrTensor dense_bias_grad;
  paddle::dialect::IrMetaTensor meta_bias_grad(&dense_bias_grad);
  paddle::dialect::IrTensor dense_weight_grad;
  paddle::dialect::IrMetaTensor meta_weight_grad(&dense_weight_grad);

  phi::NceGradInferMeta(meta_input, meta_bias, meta_weight, &meta_input_grad, &meta_bias_grad, &meta_weight_grad);

  std::vector<pir::Type> argument_outputs;
  pir::Type input_grad_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_input_grad.dtype()), dense_input_grad.dims(), dense_input_grad.layout(), dense_input_grad.lod(), dense_input_grad.offset());
  argument_outputs.push_back(input_grad_dense_tensor_type);

  pir::Type bias_grad_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_bias_grad.dtype()), dense_bias_grad.dims(), dense_bias_grad.layout(), dense_bias_grad.lod(), dense_bias_grad.offset());
  argument_outputs.push_back(bias_grad_dense_tensor_type);

  pir::Type weight_grad_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_weight_grad.dtype()), dense_weight_grad.dims(), dense_weight_grad.layout(), dense_weight_grad.lod(), dense_weight_grad.offset());
  argument_outputs.push_back(weight_grad_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void NceGradOp::VerifySig() {}

void NceGradOp::InferMeta( phi::InferMetaContext *infer_meta ) {
  auto fn = PD_INFER_META(phi::NceGradInferMeta);
  fn(infer_meta);
}

phi::DataType NceGradOp::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: NceGradOp";
  


  return expected_kernel_dtype;
}

const char *NormGradOp::attributes_name[3] = { "axis", "epsilon", "is_test" };

OpInfoTuple NormGradOp::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("x", "paddle::dialect::DenseTensorType", false, false, false, false), paddle::dialect::OpInputInfo("norm", "paddle::dialect::DenseTensorType", false, false, false, false), paddle::dialect::OpInputInfo("out_grad", "paddle::dialect::DenseTensorType", false, false, false, false) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = { paddle::dialect::OpAttributeInfo("axis", "pir::Int32Attribute", ""), paddle::dialect::OpAttributeInfo("epsilon", "pir::FloatAttribute", ""), paddle::dialect::OpAttributeInfo("is_test", "pir::BoolAttribute", "") };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("x_grad", "paddle::dialect::DenseTensorType", false, false) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("UnchangedInferMeta", {"x"}, "norm_grad", {"x", "norm", "out_grad", "axis", "epsilon", "is_test"}, {}, {}, {}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "norm_grad");
}

void NormGradOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, pir::Value norm_, pir::Value out_grad_, int axis, float epsilon, bool is_test) {
  VLOG(4) << "Start build NormGradOp";



  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_, norm_, out_grad_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_axis = pir::Int32Attribute::get(pir::IrContext::Instance(), axis);
  argument.AddAttribute("axis", attr_axis);
  pir::Attribute attr_epsilon = pir::FloatAttribute::get(pir::IrContext::Instance(), epsilon);
  argument.AddAttribute("epsilon", attr_epsilon);
  pir::Attribute attr_is_test = pir::BoolAttribute::get(pir::IrContext::Instance(), is_test);
  argument.AddAttribute("is_test", attr_is_test);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;
  paddle::dialect::DenseTensorType norm = norm_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)norm;
  paddle::dialect::DenseTensorType out_grad = out_grad_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)out_grad;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);
  paddle::dialect::IrTensor dense_x_grad;
  paddle::dialect::IrMetaTensor meta_x_grad(&dense_x_grad);

  phi::UnchangedInferMeta(meta_x, &meta_x_grad);

  std::vector<pir::Type> argument_outputs;
  pir::Type x_grad_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_x_grad.dtype()), dense_x_grad.dims(), dense_x_grad.layout(), dense_x_grad.lod(), dense_x_grad.offset());
  argument_outputs.push_back(x_grad_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void NormGradOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, pir::Value norm_, pir::Value out_grad_, pir::AttributeMap attributes) {
  VLOG(4) << "Start build NormGradOp";


  IR_ENFORCE(
      attributes.find("axis") != attributes.end(),
          "'axis' Attribute is expected for NormGradOp. ");
  int axis = attributes.at("axis").dyn_cast<pir::Int32Attribute>().data();

  IR_ENFORCE(
      attributes.find("epsilon") != attributes.end(),
          "'epsilon' Attribute is expected for NormGradOp. ");
  float epsilon = attributes.at("epsilon").dyn_cast<pir::FloatAttribute>().data();

  IR_ENFORCE(
      attributes.find("is_test") != attributes.end(),
          "'is_test' Attribute is expected for NormGradOp. ");
  bool is_test = attributes.at("is_test").dyn_cast<pir::BoolAttribute>().data();


  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_, norm_, out_grad_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_axis = pir::Int32Attribute::get(pir::IrContext::Instance(), axis);
  argument.AddAttribute("axis", attr_axis);
  pir::Attribute attr_epsilon = pir::FloatAttribute::get(pir::IrContext::Instance(), epsilon);
  argument.AddAttribute("epsilon", attr_epsilon);
  pir::Attribute attr_is_test = pir::BoolAttribute::get(pir::IrContext::Instance(), is_test);
  argument.AddAttribute("is_test", attr_is_test);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;
  paddle::dialect::DenseTensorType norm = norm_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)norm;
  paddle::dialect::DenseTensorType out_grad = out_grad_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)out_grad;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);
  paddle::dialect::IrTensor dense_x_grad;
  paddle::dialect::IrMetaTensor meta_x_grad(&dense_x_grad);

  phi::UnchangedInferMeta(meta_x, &meta_x_grad);

  std::vector<pir::Type> argument_outputs;
  pir::Type x_grad_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_x_grad.dtype()), dense_x_grad.dims(), dense_x_grad.layout(), dense_x_grad.lod(), dense_x_grad.offset());
  argument_outputs.push_back(x_grad_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void NormGradOp::VerifySig() {}

void NormGradOp::InferMeta( phi::InferMetaContext *infer_meta ) {
  auto fn = PD_INFER_META(phi::UnchangedInferMeta);
  fn(infer_meta);
}

phi::DataType NormGradOp::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: NormGradOp";
  


  return expected_kernel_dtype;
}

const char *PadDoubleGradOp::attributes_name[1] = { "paddings" };

OpInfoTuple PadDoubleGradOp::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("grad_x_grad", "paddle::dialect::DenseTensorType", false, false, false, false), paddle::dialect::OpInputInfo("pad_value", "paddle::dialect::ScalarAttribute", false, false, true, false) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = { paddle::dialect::OpAttributeInfo("paddings", "pir::ArrayAttribute<pir::Int32Attribute>", "") };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("grad_out_grad", "paddle::dialect::DenseTensorType", false, false) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("PadInferMeta", {"grad_x_grad", "paddings", "pad_value"}, "pad", {"grad_x_grad", "paddings", "pad_value"}, {}, {}, {}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "pad_double_grad");
}

void PadDoubleGradOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value grad_x_grad_, const std::vector<int>& paddings, float pad_value) {
  VLOG(4) << "Start build PadDoubleGradOp";


  // Generate scalar mutable attribute: pad_value
  paddle::dialect::FullOp full_pad_value_op = builder.Build<paddle::dialect::FullOp>(std::vector<int64_t>{1}, pad_value, phi::DataType::FLOAT32, phi::CPUPlace());
  pir::OpResult pad_value_ = full_pad_value_op->result(0);
    
  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {grad_x_grad_, pad_value_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  std::vector<pir::Attribute> vec_paddings;
  for (size_t i = 0; i < static_cast<size_t>(paddings.size()); i++) {
      pir::Attribute attr_paddings = pir::Int32Attribute::get(pir::IrContext::Instance(), paddings[i]);

    vec_paddings.push_back(attr_paddings);
  }
  pir::Attribute attr_paddings = pir::ArrayAttribute::get(pir::IrContext::Instance(), vec_paddings);
  argument.AddAttribute("paddings", attr_paddings);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType grad_x_grad = grad_x_grad_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)grad_x_grad;

  VLOG(4) << "Builder construction  dense_grad_x_grad";
  paddle::dialect::IrTensor ir_tensor_grad_x_grad(paddle::dialect::TransToPhiDataType(grad_x_grad.dtype()),
                                                      grad_x_grad.dims(),
                                                      grad_x_grad.data_layout(),
                                                      grad_x_grad.lod(),
                                                      grad_x_grad.offset());
  VLOG(4) << "Builder construction  meta_grad_x_grad";
  paddle::dialect::IrMetaTensor meta_grad_x_grad(&ir_tensor_grad_x_grad);
  paddle::dialect::IrTensor dense_grad_out_grad;
  paddle::dialect::IrMetaTensor meta_grad_out_grad(&dense_grad_out_grad);

  phi::PadInferMeta(meta_grad_x_grad, paddings, pad_value, &meta_grad_out_grad);

  std::vector<pir::Type> argument_outputs;
  pir::Type grad_out_grad_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_grad_out_grad.dtype()), dense_grad_out_grad.dims(), dense_grad_out_grad.layout(), dense_grad_out_grad.lod(), dense_grad_out_grad.offset());
  argument_outputs.push_back(grad_out_grad_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void PadDoubleGradOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value grad_x_grad_, pir::AttributeMap attributes) {
  VLOG(4) << "Start build PadDoubleGradOp";


  IR_ENFORCE(
      attributes.find("paddings") != attributes.end(),
          "'paddings' Attribute is expected for PadDoubleGradOp. ");
  std::vector<int> paddings;
  for (size_t i = 0; i < attributes.at("paddings").dyn_cast<pir::ArrayAttribute>().size(); i++) {
    paddings.push_back(attributes.at("paddings").dyn_cast<pir::ArrayAttribute>().at(i).dyn_cast<pir::Int32Attribute>().data());
  }

  IR_ENFORCE(
      attributes.find("pad_value") != attributes.end(),
          "'pad_value' Attribute is expected for PadDoubleGradOp. ");
  float pad_value = attributes.at("pad_value").dyn_cast<paddle::dialect::ScalarAttribute>().data().to<float>();

  // Generate scalar mutable attribute: pad_value
  paddle::dialect::FullOp full_pad_value_op = builder.Build<paddle::dialect::FullOp>(std::vector<int64_t>{1}, pad_value, phi::DataType::FLOAT32, phi::CPUPlace());
  pir::OpResult pad_value_ = full_pad_value_op->result(0);
    
  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {grad_x_grad_, pad_value_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  std::vector<pir::Attribute> vec_paddings;
  for (size_t i = 0; i < static_cast<size_t>(paddings.size()); i++) {
      pir::Attribute attr_paddings = pir::Int32Attribute::get(pir::IrContext::Instance(), paddings[i]);

    vec_paddings.push_back(attr_paddings);
  }
  pir::Attribute attr_paddings = pir::ArrayAttribute::get(pir::IrContext::Instance(), vec_paddings);
  argument.AddAttribute("paddings", attr_paddings);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType grad_x_grad = grad_x_grad_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)grad_x_grad;

  VLOG(4) << "Builder construction  dense_grad_x_grad";
  paddle::dialect::IrTensor ir_tensor_grad_x_grad(paddle::dialect::TransToPhiDataType(grad_x_grad.dtype()),
                                                      grad_x_grad.dims(),
                                                      grad_x_grad.data_layout(),
                                                      grad_x_grad.lod(),
                                                      grad_x_grad.offset());
  VLOG(4) << "Builder construction  meta_grad_x_grad";
  paddle::dialect::IrMetaTensor meta_grad_x_grad(&ir_tensor_grad_x_grad);
  paddle::dialect::IrTensor dense_grad_out_grad;
  paddle::dialect::IrMetaTensor meta_grad_out_grad(&dense_grad_out_grad);

  phi::PadInferMeta(meta_grad_x_grad, paddings, pad_value, &meta_grad_out_grad);

  std::vector<pir::Type> argument_outputs;
  pir::Type grad_out_grad_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_grad_out_grad.dtype()), dense_grad_out_grad.dims(), dense_grad_out_grad.layout(), dense_grad_out_grad.lod(), dense_grad_out_grad.offset());
  argument_outputs.push_back(grad_out_grad_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void PadDoubleGradOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value grad_x_grad_, pir::Value pad_value_, const std::vector<int>& paddings) {
  VLOG(4) << "Start build PadDoubleGradOp";



  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {grad_x_grad_, pad_value_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  std::vector<pir::Attribute> vec_paddings;
  for (size_t i = 0; i < static_cast<size_t>(paddings.size()); i++) {
      pir::Attribute attr_paddings = pir::Int32Attribute::get(pir::IrContext::Instance(), paddings[i]);

    vec_paddings.push_back(attr_paddings);
  }
  pir::Attribute attr_paddings = pir::ArrayAttribute::get(pir::IrContext::Instance(), vec_paddings);
  argument.AddAttribute("paddings", attr_paddings);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType grad_x_grad = grad_x_grad_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)grad_x_grad;
  phi::Scalar pad_value;
  if (pad_value_.dyn_cast<pir::OpResult>() && pad_value_.dyn_cast<pir::OpResult>().owner()->isa<paddle::dialect::FullOp>()) {
    pad_value = std::move(phi::Scalar(pad_value_.dyn_cast<pir::OpResult>().owner()
                                  ->dyn_cast<paddle::dialect::FullOp>()
                                  .attribute("value")
                                  .dyn_cast<paddle::dialect::ScalarAttribute>()
                                  .data()
                                  .to<int>()));
  }
  else {
    pad_value = std::move(phi::Scalar(-1));
    pad_value.SetFromTensor(true);
  }


  VLOG(4) << "Builder construction  dense_grad_x_grad";
  paddle::dialect::IrTensor ir_tensor_grad_x_grad(paddle::dialect::TransToPhiDataType(grad_x_grad.dtype()),
                                                      grad_x_grad.dims(),
                                                      grad_x_grad.data_layout(),
                                                      grad_x_grad.lod(),
                                                      grad_x_grad.offset());
  VLOG(4) << "Builder construction  meta_grad_x_grad";
  paddle::dialect::IrMetaTensor meta_grad_x_grad(&ir_tensor_grad_x_grad);
  paddle::dialect::IrTensor dense_grad_out_grad;
  paddle::dialect::IrMetaTensor meta_grad_out_grad(&dense_grad_out_grad);

  phi::PadInferMeta(meta_grad_x_grad, paddings, pad_value, &meta_grad_out_grad);

  std::vector<pir::Type> argument_outputs;
  pir::Type grad_out_grad_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_grad_out_grad.dtype()), dense_grad_out_grad.dims(), dense_grad_out_grad.layout(), dense_grad_out_grad.lod(), dense_grad_out_grad.offset());
  argument_outputs.push_back(grad_out_grad_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void PadDoubleGradOp::VerifySig() {}

void PadDoubleGradOp::InferMeta( phi::InferMetaContext *infer_meta ) {
  auto fn = PD_INFER_META(phi::PadInferMeta);
  fn(infer_meta);
}

phi::DataType PadDoubleGradOp::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: PadDoubleGradOp";
  


  return expected_kernel_dtype;
}

const char *PadGradOp::attributes_name[1] = { "paddings" };

OpInfoTuple PadGradOp::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("x", "paddle::dialect::DenseTensorType", false, true, false, false), paddle::dialect::OpInputInfo("out_grad", "paddle::dialect::DenseTensorType", false, false, false, true), paddle::dialect::OpInputInfo("pad_value", "paddle::dialect::ScalarAttribute", false, false, true, false) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = { paddle::dialect::OpAttributeInfo("paddings", "pir::ArrayAttribute<pir::Int32Attribute>", "") };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("x_grad", "paddle::dialect::DenseTensorType", false, false) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("UnchangedInferMeta", {"x"}, "pad_grad", {"out_grad", "paddings", "pad_value"}, {}, {}, {}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "pad_grad");
}

void PadGradOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, pir::Value out_grad_, const std::vector<int>& paddings, float pad_value) {
  VLOG(4) << "Start build PadGradOp";


  // Generate scalar mutable attribute: pad_value
  paddle::dialect::FullOp full_pad_value_op = builder.Build<paddle::dialect::FullOp>(std::vector<int64_t>{1}, pad_value, phi::DataType::FLOAT32, phi::CPUPlace());
  pir::OpResult pad_value_ = full_pad_value_op->result(0);
    
  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_, out_grad_, pad_value_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  std::vector<pir::Attribute> vec_paddings;
  for (size_t i = 0; i < static_cast<size_t>(paddings.size()); i++) {
      pir::Attribute attr_paddings = pir::Int32Attribute::get(pir::IrContext::Instance(), paddings[i]);

    vec_paddings.push_back(attr_paddings);
  }
  pir::Attribute attr_paddings = pir::ArrayAttribute::get(pir::IrContext::Instance(), vec_paddings);
  argument.AddAttribute("paddings", attr_paddings);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;
  paddle::dialect::DenseTensorType out_grad = out_grad_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)out_grad;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);
  paddle::dialect::IrTensor dense_x_grad;
  paddle::dialect::IrMetaTensor meta_x_grad(&dense_x_grad);

  phi::UnchangedInferMeta(meta_x, &meta_x_grad);

  std::vector<pir::Type> argument_outputs;
  pir::Type x_grad_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_x_grad.dtype()), dense_x_grad.dims(), dense_x_grad.layout(), dense_x_grad.lod(), dense_x_grad.offset());
  argument_outputs.push_back(x_grad_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void PadGradOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, pir::Value out_grad_, pir::AttributeMap attributes) {
  VLOG(4) << "Start build PadGradOp";


  IR_ENFORCE(
      attributes.find("paddings") != attributes.end(),
          "'paddings' Attribute is expected for PadGradOp. ");
  std::vector<int> paddings;
  for (size_t i = 0; i < attributes.at("paddings").dyn_cast<pir::ArrayAttribute>().size(); i++) {
    paddings.push_back(attributes.at("paddings").dyn_cast<pir::ArrayAttribute>().at(i).dyn_cast<pir::Int32Attribute>().data());
  }

  IR_ENFORCE(
      attributes.find("pad_value") != attributes.end(),
          "'pad_value' Attribute is expected for PadGradOp. ");
  float pad_value = attributes.at("pad_value").dyn_cast<paddle::dialect::ScalarAttribute>().data().to<float>();

  // Generate scalar mutable attribute: pad_value
  paddle::dialect::FullOp full_pad_value_op = builder.Build<paddle::dialect::FullOp>(std::vector<int64_t>{1}, pad_value, phi::DataType::FLOAT32, phi::CPUPlace());
  pir::OpResult pad_value_ = full_pad_value_op->result(0);
    
  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_, out_grad_, pad_value_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  std::vector<pir::Attribute> vec_paddings;
  for (size_t i = 0; i < static_cast<size_t>(paddings.size()); i++) {
      pir::Attribute attr_paddings = pir::Int32Attribute::get(pir::IrContext::Instance(), paddings[i]);

    vec_paddings.push_back(attr_paddings);
  }
  pir::Attribute attr_paddings = pir::ArrayAttribute::get(pir::IrContext::Instance(), vec_paddings);
  argument.AddAttribute("paddings", attr_paddings);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;
  paddle::dialect::DenseTensorType out_grad = out_grad_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)out_grad;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);
  paddle::dialect::IrTensor dense_x_grad;
  paddle::dialect::IrMetaTensor meta_x_grad(&dense_x_grad);

  phi::UnchangedInferMeta(meta_x, &meta_x_grad);

  std::vector<pir::Type> argument_outputs;
  pir::Type x_grad_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_x_grad.dtype()), dense_x_grad.dims(), dense_x_grad.layout(), dense_x_grad.lod(), dense_x_grad.offset());
  argument_outputs.push_back(x_grad_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void PadGradOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, pir::Value out_grad_, pir::Value pad_value_, const std::vector<int>& paddings) {
  VLOG(4) << "Start build PadGradOp";



  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_, out_grad_, pad_value_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  std::vector<pir::Attribute> vec_paddings;
  for (size_t i = 0; i < static_cast<size_t>(paddings.size()); i++) {
      pir::Attribute attr_paddings = pir::Int32Attribute::get(pir::IrContext::Instance(), paddings[i]);

    vec_paddings.push_back(attr_paddings);
  }
  pir::Attribute attr_paddings = pir::ArrayAttribute::get(pir::IrContext::Instance(), vec_paddings);
  argument.AddAttribute("paddings", attr_paddings);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;
  paddle::dialect::DenseTensorType out_grad = out_grad_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)out_grad;
  phi::Scalar pad_value;
  if (pad_value_.dyn_cast<pir::OpResult>() && pad_value_.dyn_cast<pir::OpResult>().owner()->isa<paddle::dialect::FullOp>()) {
    pad_value = std::move(phi::Scalar(pad_value_.dyn_cast<pir::OpResult>().owner()
                                  ->dyn_cast<paddle::dialect::FullOp>()
                                  .attribute("value")
                                  .dyn_cast<paddle::dialect::ScalarAttribute>()
                                  .data()
                                  .to<int>()));
  }
  else {
    pad_value = std::move(phi::Scalar(-1));
    pad_value.SetFromTensor(true);
  }


  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);
  paddle::dialect::IrTensor dense_x_grad;
  paddle::dialect::IrMetaTensor meta_x_grad(&dense_x_grad);

  phi::UnchangedInferMeta(meta_x, &meta_x_grad);

  std::vector<pir::Type> argument_outputs;
  pir::Type x_grad_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_x_grad.dtype()), dense_x_grad.dims(), dense_x_grad.layout(), dense_x_grad.lod(), dense_x_grad.offset());
  argument_outputs.push_back(x_grad_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void PadGradOp::VerifySig() {}

void PadGradOp::InferMeta( phi::InferMetaContext *infer_meta ) {
  auto fn = PD_INFER_META(phi::UnchangedInferMeta);
  fn(infer_meta);
}

phi::DataType PadGradOp::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: PadGradOp";
  


  return expected_kernel_dtype;
}

const char *Pool2dDoubleGradOp::attributes_name[9] = { "strides", "paddings", "ceil_mode", "exclusive", "data_format", "pooling_type", "global_pooling", "adaptive", "padding_algorithm" };

OpInfoTuple Pool2dDoubleGradOp::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("x", "paddle::dialect::DenseTensorType", false, true, false, false), paddle::dialect::OpInputInfo("grad_x_grad", "paddle::dialect::DenseTensorType", false, false, false, false), paddle::dialect::OpInputInfo("kernel_size", "paddle::dialect::IntArrayAttribute", false, false, true, false) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = { paddle::dialect::OpAttributeInfo("strides", "pir::ArrayAttribute<pir::Int32Attribute>", ""), paddle::dialect::OpAttributeInfo("paddings", "pir::ArrayAttribute<pir::Int32Attribute>", ""), paddle::dialect::OpAttributeInfo("ceil_mode", "pir::BoolAttribute", ""), paddle::dialect::OpAttributeInfo("exclusive", "pir::BoolAttribute", ""), paddle::dialect::OpAttributeInfo("data_format", "pir::StrAttribute", ""), paddle::dialect::OpAttributeInfo("pooling_type", "pir::StrAttribute", ""), paddle::dialect::OpAttributeInfo("global_pooling", "pir::BoolAttribute", ""), paddle::dialect::OpAttributeInfo("adaptive", "pir::BoolAttribute", ""), paddle::dialect::OpAttributeInfo("padding_algorithm", "pir::StrAttribute", "") };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("grad_out_grad", "paddle::dialect::DenseTensorType", false, false) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("Pool2DInferMeta", {"grad_x_grad", "kernel_size", "strides", "paddings", "ceil_mode", "exclusive", "data_format", "pooling_type", "global_pooling", "adaptive", "padding_algorithm"}, "pool2d_double_grad", {"grad_x_grad", "kernel_size", "strides", "paddings", "ceil_mode", "exclusive", "data_format", "pooling_type", "global_pooling", "adaptive", "padding_algorithm"}, {}, {}, {}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "pool2d_double_grad");
}

void Pool2dDoubleGradOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, pir::Value grad_x_grad_, const std::vector<int64_t>& kernel_size, const std::vector<int>& strides, const std::vector<int>& paddings, bool ceil_mode, bool exclusive, const std::string& data_format, const std::string& pooling_type, bool global_pooling, bool adaptive, const std::string& padding_algorithm) {
  VLOG(4) << "Start build Pool2dDoubleGradOp";


  // Generate int_array mutable attribute: kernel_size
  paddle::dialect::FullIntArrayOp full_kernel_size_op = builder.Build<paddle::dialect::FullIntArrayOp>(kernel_size, phi::DataType::INT64, phi::CPUPlace());
  pir::OpResult kernel_size_ = full_kernel_size_op->result(0);
    
  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_, grad_x_grad_, kernel_size_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  std::vector<pir::Attribute> vec_strides;
  for (size_t i = 0; i < static_cast<size_t>(strides.size()); i++) {
      pir::Attribute attr_strides = pir::Int32Attribute::get(pir::IrContext::Instance(), strides[i]);

    vec_strides.push_back(attr_strides);
  }
  pir::Attribute attr_strides = pir::ArrayAttribute::get(pir::IrContext::Instance(), vec_strides);
  argument.AddAttribute("strides", attr_strides);
  std::vector<pir::Attribute> vec_paddings;
  for (size_t i = 0; i < static_cast<size_t>(paddings.size()); i++) {
      pir::Attribute attr_paddings = pir::Int32Attribute::get(pir::IrContext::Instance(), paddings[i]);

    vec_paddings.push_back(attr_paddings);
  }
  pir::Attribute attr_paddings = pir::ArrayAttribute::get(pir::IrContext::Instance(), vec_paddings);
  argument.AddAttribute("paddings", attr_paddings);
  pir::Attribute attr_ceil_mode = pir::BoolAttribute::get(pir::IrContext::Instance(), ceil_mode);
  argument.AddAttribute("ceil_mode", attr_ceil_mode);
  pir::Attribute attr_exclusive = pir::BoolAttribute::get(pir::IrContext::Instance(), exclusive);
  argument.AddAttribute("exclusive", attr_exclusive);
  pir::Attribute attr_data_format = pir::StrAttribute::get(pir::IrContext::Instance(), data_format);
  argument.AddAttribute("data_format", attr_data_format);
  pir::Attribute attr_pooling_type = pir::StrAttribute::get(pir::IrContext::Instance(), pooling_type);
  argument.AddAttribute("pooling_type", attr_pooling_type);
  pir::Attribute attr_global_pooling = pir::BoolAttribute::get(pir::IrContext::Instance(), global_pooling);
  argument.AddAttribute("global_pooling", attr_global_pooling);
  pir::Attribute attr_adaptive = pir::BoolAttribute::get(pir::IrContext::Instance(), adaptive);
  argument.AddAttribute("adaptive", attr_adaptive);
  pir::Attribute attr_padding_algorithm = pir::StrAttribute::get(pir::IrContext::Instance(), padding_algorithm);
  argument.AddAttribute("padding_algorithm", attr_padding_algorithm);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;
  paddle::dialect::DenseTensorType grad_x_grad = grad_x_grad_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)grad_x_grad;

  VLOG(4) << "Builder construction  dense_grad_x_grad";
  paddle::dialect::IrTensor ir_tensor_grad_x_grad(paddle::dialect::TransToPhiDataType(grad_x_grad.dtype()),
                                                      grad_x_grad.dims(),
                                                      grad_x_grad.data_layout(),
                                                      grad_x_grad.lod(),
                                                      grad_x_grad.offset());
  VLOG(4) << "Builder construction  meta_grad_x_grad";
  paddle::dialect::IrMetaTensor meta_grad_x_grad(&ir_tensor_grad_x_grad);
  paddle::dialect::IrTensor dense_grad_out_grad;
  paddle::dialect::IrMetaTensor meta_grad_out_grad(&dense_grad_out_grad);

  phi::Pool2DInferMeta(meta_grad_x_grad, kernel_size, strides, paddings, ceil_mode, exclusive, data_format, pooling_type, global_pooling, adaptive, padding_algorithm, &meta_grad_out_grad);

  std::vector<pir::Type> argument_outputs;
  pir::Type grad_out_grad_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_grad_out_grad.dtype()), dense_grad_out_grad.dims(), dense_grad_out_grad.layout(), dense_grad_out_grad.lod(), dense_grad_out_grad.offset());
  argument_outputs.push_back(grad_out_grad_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void Pool2dDoubleGradOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, pir::Value grad_x_grad_, pir::AttributeMap attributes) {
  VLOG(4) << "Start build Pool2dDoubleGradOp";


  IR_ENFORCE(
      attributes.find("kernel_size") != attributes.end(),
          "'kernel_size' Attribute is expected for Pool2dDoubleGradOp. ");
  std::vector<int64_t> kernel_size = attributes.at("kernel_size").dyn_cast<paddle::dialect::IntArrayAttribute>().data().GetData();

  IR_ENFORCE(
      attributes.find("strides") != attributes.end(),
          "'strides' Attribute is expected for Pool2dDoubleGradOp. ");
  std::vector<int> strides;
  for (size_t i = 0; i < attributes.at("strides").dyn_cast<pir::ArrayAttribute>().size(); i++) {
    strides.push_back(attributes.at("strides").dyn_cast<pir::ArrayAttribute>().at(i).dyn_cast<pir::Int32Attribute>().data());
  }

  IR_ENFORCE(
      attributes.find("paddings") != attributes.end(),
          "'paddings' Attribute is expected for Pool2dDoubleGradOp. ");
  std::vector<int> paddings;
  for (size_t i = 0; i < attributes.at("paddings").dyn_cast<pir::ArrayAttribute>().size(); i++) {
    paddings.push_back(attributes.at("paddings").dyn_cast<pir::ArrayAttribute>().at(i).dyn_cast<pir::Int32Attribute>().data());
  }

  IR_ENFORCE(
      attributes.find("ceil_mode") != attributes.end(),
          "'ceil_mode' Attribute is expected for Pool2dDoubleGradOp. ");
  bool ceil_mode = attributes.at("ceil_mode").dyn_cast<pir::BoolAttribute>().data();

  IR_ENFORCE(
      attributes.find("exclusive") != attributes.end(),
          "'exclusive' Attribute is expected for Pool2dDoubleGradOp. ");
  bool exclusive = attributes.at("exclusive").dyn_cast<pir::BoolAttribute>().data();

  IR_ENFORCE(
      attributes.find("data_format") != attributes.end(),
          "'data_format' Attribute is expected for Pool2dDoubleGradOp. ");
  std::string data_format = attributes.at("data_format").dyn_cast<pir::StrAttribute>().AsString();

  IR_ENFORCE(
      attributes.find("pooling_type") != attributes.end(),
          "'pooling_type' Attribute is expected for Pool2dDoubleGradOp. ");
  std::string pooling_type = attributes.at("pooling_type").dyn_cast<pir::StrAttribute>().AsString();

  IR_ENFORCE(
      attributes.find("global_pooling") != attributes.end(),
          "'global_pooling' Attribute is expected for Pool2dDoubleGradOp. ");
  bool global_pooling = attributes.at("global_pooling").dyn_cast<pir::BoolAttribute>().data();

  IR_ENFORCE(
      attributes.find("adaptive") != attributes.end(),
          "'adaptive' Attribute is expected for Pool2dDoubleGradOp. ");
  bool adaptive = attributes.at("adaptive").dyn_cast<pir::BoolAttribute>().data();

  IR_ENFORCE(
      attributes.find("padding_algorithm") != attributes.end(),
          "'padding_algorithm' Attribute is expected for Pool2dDoubleGradOp. ");
  std::string padding_algorithm = attributes.at("padding_algorithm").dyn_cast<pir::StrAttribute>().AsString();

  // Generate int_array mutable attribute: kernel_size
  paddle::dialect::FullIntArrayOp full_kernel_size_op = builder.Build<paddle::dialect::FullIntArrayOp>(kernel_size, phi::DataType::INT64, phi::CPUPlace());
  pir::OpResult kernel_size_ = full_kernel_size_op->result(0);
    
  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_, grad_x_grad_, kernel_size_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  std::vector<pir::Attribute> vec_strides;
  for (size_t i = 0; i < static_cast<size_t>(strides.size()); i++) {
      pir::Attribute attr_strides = pir::Int32Attribute::get(pir::IrContext::Instance(), strides[i]);

    vec_strides.push_back(attr_strides);
  }
  pir::Attribute attr_strides = pir::ArrayAttribute::get(pir::IrContext::Instance(), vec_strides);
  argument.AddAttribute("strides", attr_strides);
  std::vector<pir::Attribute> vec_paddings;
  for (size_t i = 0; i < static_cast<size_t>(paddings.size()); i++) {
      pir::Attribute attr_paddings = pir::Int32Attribute::get(pir::IrContext::Instance(), paddings[i]);

    vec_paddings.push_back(attr_paddings);
  }
  pir::Attribute attr_paddings = pir::ArrayAttribute::get(pir::IrContext::Instance(), vec_paddings);
  argument.AddAttribute("paddings", attr_paddings);
  pir::Attribute attr_ceil_mode = pir::BoolAttribute::get(pir::IrContext::Instance(), ceil_mode);
  argument.AddAttribute("ceil_mode", attr_ceil_mode);
  pir::Attribute attr_exclusive = pir::BoolAttribute::get(pir::IrContext::Instance(), exclusive);
  argument.AddAttribute("exclusive", attr_exclusive);
  pir::Attribute attr_data_format = pir::StrAttribute::get(pir::IrContext::Instance(), data_format);
  argument.AddAttribute("data_format", attr_data_format);
  pir::Attribute attr_pooling_type = pir::StrAttribute::get(pir::IrContext::Instance(), pooling_type);
  argument.AddAttribute("pooling_type", attr_pooling_type);
  pir::Attribute attr_global_pooling = pir::BoolAttribute::get(pir::IrContext::Instance(), global_pooling);
  argument.AddAttribute("global_pooling", attr_global_pooling);
  pir::Attribute attr_adaptive = pir::BoolAttribute::get(pir::IrContext::Instance(), adaptive);
  argument.AddAttribute("adaptive", attr_adaptive);
  pir::Attribute attr_padding_algorithm = pir::StrAttribute::get(pir::IrContext::Instance(), padding_algorithm);
  argument.AddAttribute("padding_algorithm", attr_padding_algorithm);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;
  paddle::dialect::DenseTensorType grad_x_grad = grad_x_grad_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)grad_x_grad;

  VLOG(4) << "Builder construction  dense_grad_x_grad";
  paddle::dialect::IrTensor ir_tensor_grad_x_grad(paddle::dialect::TransToPhiDataType(grad_x_grad.dtype()),
                                                      grad_x_grad.dims(),
                                                      grad_x_grad.data_layout(),
                                                      grad_x_grad.lod(),
                                                      grad_x_grad.offset());
  VLOG(4) << "Builder construction  meta_grad_x_grad";
  paddle::dialect::IrMetaTensor meta_grad_x_grad(&ir_tensor_grad_x_grad);
  paddle::dialect::IrTensor dense_grad_out_grad;
  paddle::dialect::IrMetaTensor meta_grad_out_grad(&dense_grad_out_grad);

  phi::Pool2DInferMeta(meta_grad_x_grad, kernel_size, strides, paddings, ceil_mode, exclusive, data_format, pooling_type, global_pooling, adaptive, padding_algorithm, &meta_grad_out_grad);

  std::vector<pir::Type> argument_outputs;
  pir::Type grad_out_grad_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_grad_out_grad.dtype()), dense_grad_out_grad.dims(), dense_grad_out_grad.layout(), dense_grad_out_grad.lod(), dense_grad_out_grad.offset());
  argument_outputs.push_back(grad_out_grad_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void Pool2dDoubleGradOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, pir::Value grad_x_grad_, pir::Value kernel_size_, const std::vector<int>& strides, const std::vector<int>& paddings, bool ceil_mode, bool exclusive, const std::string& data_format, const std::string& pooling_type, bool global_pooling, bool adaptive, const std::string& padding_algorithm) {
  VLOG(4) << "Start build Pool2dDoubleGradOp";



  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_, grad_x_grad_, kernel_size_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  std::vector<pir::Attribute> vec_strides;
  for (size_t i = 0; i < static_cast<size_t>(strides.size()); i++) {
      pir::Attribute attr_strides = pir::Int32Attribute::get(pir::IrContext::Instance(), strides[i]);

    vec_strides.push_back(attr_strides);
  }
  pir::Attribute attr_strides = pir::ArrayAttribute::get(pir::IrContext::Instance(), vec_strides);
  argument.AddAttribute("strides", attr_strides);
  std::vector<pir::Attribute> vec_paddings;
  for (size_t i = 0; i < static_cast<size_t>(paddings.size()); i++) {
      pir::Attribute attr_paddings = pir::Int32Attribute::get(pir::IrContext::Instance(), paddings[i]);

    vec_paddings.push_back(attr_paddings);
  }
  pir::Attribute attr_paddings = pir::ArrayAttribute::get(pir::IrContext::Instance(), vec_paddings);
  argument.AddAttribute("paddings", attr_paddings);
  pir::Attribute attr_ceil_mode = pir::BoolAttribute::get(pir::IrContext::Instance(), ceil_mode);
  argument.AddAttribute("ceil_mode", attr_ceil_mode);
  pir::Attribute attr_exclusive = pir::BoolAttribute::get(pir::IrContext::Instance(), exclusive);
  argument.AddAttribute("exclusive", attr_exclusive);
  pir::Attribute attr_data_format = pir::StrAttribute::get(pir::IrContext::Instance(), data_format);
  argument.AddAttribute("data_format", attr_data_format);
  pir::Attribute attr_pooling_type = pir::StrAttribute::get(pir::IrContext::Instance(), pooling_type);
  argument.AddAttribute("pooling_type", attr_pooling_type);
  pir::Attribute attr_global_pooling = pir::BoolAttribute::get(pir::IrContext::Instance(), global_pooling);
  argument.AddAttribute("global_pooling", attr_global_pooling);
  pir::Attribute attr_adaptive = pir::BoolAttribute::get(pir::IrContext::Instance(), adaptive);
  argument.AddAttribute("adaptive", attr_adaptive);
  pir::Attribute attr_padding_algorithm = pir::StrAttribute::get(pir::IrContext::Instance(), padding_algorithm);
  argument.AddAttribute("padding_algorithm", attr_padding_algorithm);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;
  paddle::dialect::DenseTensorType grad_x_grad = grad_x_grad_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)grad_x_grad;
  phi::IntArray kernel_size;
  if (kernel_size_.dyn_cast<pir::OpResult>() && kernel_size_.dyn_cast<pir::OpResult>().owner()->isa<paddle::dialect::FullIntArrayOp>()) {
    kernel_size = std::move(phi::IntArray(paddle::dialect::GetInt64Vector(
                          kernel_size_.dyn_cast<pir::OpResult>().owner()
                          ->dyn_cast<paddle::dialect::FullIntArrayOp>()
                          .attribute("value"))));
  } else if (kernel_size_.type().isa<pir::VectorType>()) {
    size_t kernel_size_size = kernel_size_.type().dyn_cast<pir::VectorType>().size();
    kernel_size = std::move(phi::IntArray(std::vector<int64_t>(kernel_size_size, -1)));
    kernel_size.SetFromTensor(true);
  } else if (kernel_size_.type().isa<paddle::dialect::DenseTensorType>()) {
    common::DDim kernel_size_dim = kernel_size_.type().dyn_cast<paddle::dialect::DenseTensorType>().dims();
    size_t kernel_size_size = common::product(kernel_size_dim);
    if (common::contain_unknown_dim(kernel_size_dim)) {
      kernel_size_size = 1;
    }
    kernel_size = std::move(phi::IntArray(std::vector<int64_t>(kernel_size_size, -1)));
    kernel_size.SetFromTensor(true);
  } else {
    PADDLE_THROW(phi::errors::Unimplemented("Only support VectorType or DenseTensorType"));
  }


  VLOG(4) << "Builder construction  dense_grad_x_grad";
  paddle::dialect::IrTensor ir_tensor_grad_x_grad(paddle::dialect::TransToPhiDataType(grad_x_grad.dtype()),
                                                      grad_x_grad.dims(),
                                                      grad_x_grad.data_layout(),
                                                      grad_x_grad.lod(),
                                                      grad_x_grad.offset());
  VLOG(4) << "Builder construction  meta_grad_x_grad";
  paddle::dialect::IrMetaTensor meta_grad_x_grad(&ir_tensor_grad_x_grad);
  paddle::dialect::IrTensor dense_grad_out_grad;
  paddle::dialect::IrMetaTensor meta_grad_out_grad(&dense_grad_out_grad);

  phi::Pool2DInferMeta(meta_grad_x_grad, kernel_size, strides, paddings, ceil_mode, exclusive, data_format, pooling_type, global_pooling, adaptive, padding_algorithm, &meta_grad_out_grad);

  std::vector<pir::Type> argument_outputs;
  pir::Type grad_out_grad_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_grad_out_grad.dtype()), dense_grad_out_grad.dims(), dense_grad_out_grad.layout(), dense_grad_out_grad.lod(), dense_grad_out_grad.offset());
  argument_outputs.push_back(grad_out_grad_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void Pool2dDoubleGradOp::VerifySig() {}

void Pool2dDoubleGradOp::InferMeta( phi::InferMetaContext *infer_meta ) {
  auto fn = PD_INFER_META(phi::Pool2DInferMeta);
  fn(infer_meta);
}

phi::DataType Pool2dDoubleGradOp::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: Pool2dDoubleGradOp";
  


  return expected_kernel_dtype;
}

const char *Pool2dGradOp::attributes_name[9] = { "strides", "paddings", "ceil_mode", "exclusive", "data_format", "pooling_type", "global_pooling", "adaptive", "padding_algorithm" };

OpInfoTuple Pool2dGradOp::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("x", "paddle::dialect::DenseTensorType", false, false, false, false), paddle::dialect::OpInputInfo("out", "paddle::dialect::DenseTensorType", false, false, false, false), paddle::dialect::OpInputInfo("out_grad", "paddle::dialect::DenseTensorType", false, false, false, true), paddle::dialect::OpInputInfo("kernel_size", "paddle::dialect::IntArrayAttribute", false, false, true, false) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = { paddle::dialect::OpAttributeInfo("strides", "pir::ArrayAttribute<pir::Int32Attribute>", ""), paddle::dialect::OpAttributeInfo("paddings", "pir::ArrayAttribute<pir::Int32Attribute>", ""), paddle::dialect::OpAttributeInfo("ceil_mode", "pir::BoolAttribute", ""), paddle::dialect::OpAttributeInfo("exclusive", "pir::BoolAttribute", ""), paddle::dialect::OpAttributeInfo("data_format", "pir::StrAttribute", ""), paddle::dialect::OpAttributeInfo("pooling_type", "pir::StrAttribute", ""), paddle::dialect::OpAttributeInfo("global_pooling", "pir::BoolAttribute", ""), paddle::dialect::OpAttributeInfo("adaptive", "pir::BoolAttribute", ""), paddle::dialect::OpAttributeInfo("padding_algorithm", "pir::StrAttribute", "") };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("x_grad", "paddle::dialect::DenseTensorType", false, false) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("UnchangedInferMeta", {"x"}, "pool2d_grad", {"x", "out", "out_grad", "kernel_size", "strides", "paddings", "ceil_mode", "exclusive", "data_format", "pooling_type", "global_pooling", "adaptive", "padding_algorithm"}, {}, {}, {}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "pool2d_grad");
}

void Pool2dGradOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, pir::Value out_, pir::Value out_grad_, const std::vector<int64_t>& kernel_size, const std::vector<int>& strides, const std::vector<int>& paddings, bool ceil_mode, bool exclusive, const std::string& data_format, const std::string& pooling_type, bool global_pooling, bool adaptive, const std::string& padding_algorithm) {
  VLOG(4) << "Start build Pool2dGradOp";


  // Generate int_array mutable attribute: kernel_size
  paddle::dialect::FullIntArrayOp full_kernel_size_op = builder.Build<paddle::dialect::FullIntArrayOp>(kernel_size, phi::DataType::INT64, phi::CPUPlace());
  pir::OpResult kernel_size_ = full_kernel_size_op->result(0);
    
  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_, out_, out_grad_, kernel_size_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  std::vector<pir::Attribute> vec_strides;
  for (size_t i = 0; i < static_cast<size_t>(strides.size()); i++) {
      pir::Attribute attr_strides = pir::Int32Attribute::get(pir::IrContext::Instance(), strides[i]);

    vec_strides.push_back(attr_strides);
  }
  pir::Attribute attr_strides = pir::ArrayAttribute::get(pir::IrContext::Instance(), vec_strides);
  argument.AddAttribute("strides", attr_strides);
  std::vector<pir::Attribute> vec_paddings;
  for (size_t i = 0; i < static_cast<size_t>(paddings.size()); i++) {
      pir::Attribute attr_paddings = pir::Int32Attribute::get(pir::IrContext::Instance(), paddings[i]);

    vec_paddings.push_back(attr_paddings);
  }
  pir::Attribute attr_paddings = pir::ArrayAttribute::get(pir::IrContext::Instance(), vec_paddings);
  argument.AddAttribute("paddings", attr_paddings);
  pir::Attribute attr_ceil_mode = pir::BoolAttribute::get(pir::IrContext::Instance(), ceil_mode);
  argument.AddAttribute("ceil_mode", attr_ceil_mode);
  pir::Attribute attr_exclusive = pir::BoolAttribute::get(pir::IrContext::Instance(), exclusive);
  argument.AddAttribute("exclusive", attr_exclusive);
  pir::Attribute attr_data_format = pir::StrAttribute::get(pir::IrContext::Instance(), data_format);
  argument.AddAttribute("data_format", attr_data_format);
  pir::Attribute attr_pooling_type = pir::StrAttribute::get(pir::IrContext::Instance(), pooling_type);
  argument.AddAttribute("pooling_type", attr_pooling_type);
  pir::Attribute attr_global_pooling = pir::BoolAttribute::get(pir::IrContext::Instance(), global_pooling);
  argument.AddAttribute("global_pooling", attr_global_pooling);
  pir::Attribute attr_adaptive = pir::BoolAttribute::get(pir::IrContext::Instance(), adaptive);
  argument.AddAttribute("adaptive", attr_adaptive);
  pir::Attribute attr_padding_algorithm = pir::StrAttribute::get(pir::IrContext::Instance(), padding_algorithm);
  argument.AddAttribute("padding_algorithm", attr_padding_algorithm);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;
  paddle::dialect::DenseTensorType out = out_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)out;
  paddle::dialect::DenseTensorType out_grad = out_grad_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)out_grad;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);
  paddle::dialect::IrTensor dense_x_grad;
  paddle::dialect::IrMetaTensor meta_x_grad(&dense_x_grad);

  phi::UnchangedInferMeta(meta_x, &meta_x_grad);

  std::vector<pir::Type> argument_outputs;
  pir::Type x_grad_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_x_grad.dtype()), dense_x_grad.dims(), dense_x_grad.layout(), dense_x_grad.lod(), dense_x_grad.offset());
  argument_outputs.push_back(x_grad_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void Pool2dGradOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, pir::Value out_, pir::Value out_grad_, pir::AttributeMap attributes) {
  VLOG(4) << "Start build Pool2dGradOp";


  IR_ENFORCE(
      attributes.find("kernel_size") != attributes.end(),
          "'kernel_size' Attribute is expected for Pool2dGradOp. ");
  std::vector<int64_t> kernel_size = attributes.at("kernel_size").dyn_cast<paddle::dialect::IntArrayAttribute>().data().GetData();

  IR_ENFORCE(
      attributes.find("strides") != attributes.end(),
          "'strides' Attribute is expected for Pool2dGradOp. ");
  std::vector<int> strides;
  for (size_t i = 0; i < attributes.at("strides").dyn_cast<pir::ArrayAttribute>().size(); i++) {
    strides.push_back(attributes.at("strides").dyn_cast<pir::ArrayAttribute>().at(i).dyn_cast<pir::Int32Attribute>().data());
  }

  IR_ENFORCE(
      attributes.find("paddings") != attributes.end(),
          "'paddings' Attribute is expected for Pool2dGradOp. ");
  std::vector<int> paddings;
  for (size_t i = 0; i < attributes.at("paddings").dyn_cast<pir::ArrayAttribute>().size(); i++) {
    paddings.push_back(attributes.at("paddings").dyn_cast<pir::ArrayAttribute>().at(i).dyn_cast<pir::Int32Attribute>().data());
  }

  IR_ENFORCE(
      attributes.find("ceil_mode") != attributes.end(),
          "'ceil_mode' Attribute is expected for Pool2dGradOp. ");
  bool ceil_mode = attributes.at("ceil_mode").dyn_cast<pir::BoolAttribute>().data();

  IR_ENFORCE(
      attributes.find("exclusive") != attributes.end(),
          "'exclusive' Attribute is expected for Pool2dGradOp. ");
  bool exclusive = attributes.at("exclusive").dyn_cast<pir::BoolAttribute>().data();

  IR_ENFORCE(
      attributes.find("data_format") != attributes.end(),
          "'data_format' Attribute is expected for Pool2dGradOp. ");
  std::string data_format = attributes.at("data_format").dyn_cast<pir::StrAttribute>().AsString();

  IR_ENFORCE(
      attributes.find("pooling_type") != attributes.end(),
          "'pooling_type' Attribute is expected for Pool2dGradOp. ");
  std::string pooling_type = attributes.at("pooling_type").dyn_cast<pir::StrAttribute>().AsString();

  IR_ENFORCE(
      attributes.find("global_pooling") != attributes.end(),
          "'global_pooling' Attribute is expected for Pool2dGradOp. ");
  bool global_pooling = attributes.at("global_pooling").dyn_cast<pir::BoolAttribute>().data();

  IR_ENFORCE(
      attributes.find("adaptive") != attributes.end(),
          "'adaptive' Attribute is expected for Pool2dGradOp. ");
  bool adaptive = attributes.at("adaptive").dyn_cast<pir::BoolAttribute>().data();

  IR_ENFORCE(
      attributes.find("padding_algorithm") != attributes.end(),
          "'padding_algorithm' Attribute is expected for Pool2dGradOp. ");
  std::string padding_algorithm = attributes.at("padding_algorithm").dyn_cast<pir::StrAttribute>().AsString();

  // Generate int_array mutable attribute: kernel_size
  paddle::dialect::FullIntArrayOp full_kernel_size_op = builder.Build<paddle::dialect::FullIntArrayOp>(kernel_size, phi::DataType::INT64, phi::CPUPlace());
  pir::OpResult kernel_size_ = full_kernel_size_op->result(0);
    
  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_, out_, out_grad_, kernel_size_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  std::vector<pir::Attribute> vec_strides;
  for (size_t i = 0; i < static_cast<size_t>(strides.size()); i++) {
      pir::Attribute attr_strides = pir::Int32Attribute::get(pir::IrContext::Instance(), strides[i]);

    vec_strides.push_back(attr_strides);
  }
  pir::Attribute attr_strides = pir::ArrayAttribute::get(pir::IrContext::Instance(), vec_strides);
  argument.AddAttribute("strides", attr_strides);
  std::vector<pir::Attribute> vec_paddings;
  for (size_t i = 0; i < static_cast<size_t>(paddings.size()); i++) {
      pir::Attribute attr_paddings = pir::Int32Attribute::get(pir::IrContext::Instance(), paddings[i]);

    vec_paddings.push_back(attr_paddings);
  }
  pir::Attribute attr_paddings = pir::ArrayAttribute::get(pir::IrContext::Instance(), vec_paddings);
  argument.AddAttribute("paddings", attr_paddings);
  pir::Attribute attr_ceil_mode = pir::BoolAttribute::get(pir::IrContext::Instance(), ceil_mode);
  argument.AddAttribute("ceil_mode", attr_ceil_mode);
  pir::Attribute attr_exclusive = pir::BoolAttribute::get(pir::IrContext::Instance(), exclusive);
  argument.AddAttribute("exclusive", attr_exclusive);
  pir::Attribute attr_data_format = pir::StrAttribute::get(pir::IrContext::Instance(), data_format);
  argument.AddAttribute("data_format", attr_data_format);
  pir::Attribute attr_pooling_type = pir::StrAttribute::get(pir::IrContext::Instance(), pooling_type);
  argument.AddAttribute("pooling_type", attr_pooling_type);
  pir::Attribute attr_global_pooling = pir::BoolAttribute::get(pir::IrContext::Instance(), global_pooling);
  argument.AddAttribute("global_pooling", attr_global_pooling);
  pir::Attribute attr_adaptive = pir::BoolAttribute::get(pir::IrContext::Instance(), adaptive);
  argument.AddAttribute("adaptive", attr_adaptive);
  pir::Attribute attr_padding_algorithm = pir::StrAttribute::get(pir::IrContext::Instance(), padding_algorithm);
  argument.AddAttribute("padding_algorithm", attr_padding_algorithm);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;
  paddle::dialect::DenseTensorType out = out_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)out;
  paddle::dialect::DenseTensorType out_grad = out_grad_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)out_grad;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);
  paddle::dialect::IrTensor dense_x_grad;
  paddle::dialect::IrMetaTensor meta_x_grad(&dense_x_grad);

  phi::UnchangedInferMeta(meta_x, &meta_x_grad);

  std::vector<pir::Type> argument_outputs;
  pir::Type x_grad_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_x_grad.dtype()), dense_x_grad.dims(), dense_x_grad.layout(), dense_x_grad.lod(), dense_x_grad.offset());
  argument_outputs.push_back(x_grad_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void Pool2dGradOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, pir::Value out_, pir::Value out_grad_, pir::Value kernel_size_, const std::vector<int>& strides, const std::vector<int>& paddings, bool ceil_mode, bool exclusive, const std::string& data_format, const std::string& pooling_type, bool global_pooling, bool adaptive, const std::string& padding_algorithm) {
  VLOG(4) << "Start build Pool2dGradOp";



  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_, out_, out_grad_, kernel_size_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  std::vector<pir::Attribute> vec_strides;
  for (size_t i = 0; i < static_cast<size_t>(strides.size()); i++) {
      pir::Attribute attr_strides = pir::Int32Attribute::get(pir::IrContext::Instance(), strides[i]);

    vec_strides.push_back(attr_strides);
  }
  pir::Attribute attr_strides = pir::ArrayAttribute::get(pir::IrContext::Instance(), vec_strides);
  argument.AddAttribute("strides", attr_strides);
  std::vector<pir::Attribute> vec_paddings;
  for (size_t i = 0; i < static_cast<size_t>(paddings.size()); i++) {
      pir::Attribute attr_paddings = pir::Int32Attribute::get(pir::IrContext::Instance(), paddings[i]);

    vec_paddings.push_back(attr_paddings);
  }
  pir::Attribute attr_paddings = pir::ArrayAttribute::get(pir::IrContext::Instance(), vec_paddings);
  argument.AddAttribute("paddings", attr_paddings);
  pir::Attribute attr_ceil_mode = pir::BoolAttribute::get(pir::IrContext::Instance(), ceil_mode);
  argument.AddAttribute("ceil_mode", attr_ceil_mode);
  pir::Attribute attr_exclusive = pir::BoolAttribute::get(pir::IrContext::Instance(), exclusive);
  argument.AddAttribute("exclusive", attr_exclusive);
  pir::Attribute attr_data_format = pir::StrAttribute::get(pir::IrContext::Instance(), data_format);
  argument.AddAttribute("data_format", attr_data_format);
  pir::Attribute attr_pooling_type = pir::StrAttribute::get(pir::IrContext::Instance(), pooling_type);
  argument.AddAttribute("pooling_type", attr_pooling_type);
  pir::Attribute attr_global_pooling = pir::BoolAttribute::get(pir::IrContext::Instance(), global_pooling);
  argument.AddAttribute("global_pooling", attr_global_pooling);
  pir::Attribute attr_adaptive = pir::BoolAttribute::get(pir::IrContext::Instance(), adaptive);
  argument.AddAttribute("adaptive", attr_adaptive);
  pir::Attribute attr_padding_algorithm = pir::StrAttribute::get(pir::IrContext::Instance(), padding_algorithm);
  argument.AddAttribute("padding_algorithm", attr_padding_algorithm);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;
  paddle::dialect::DenseTensorType out = out_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)out;
  paddle::dialect::DenseTensorType out_grad = out_grad_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)out_grad;
  phi::IntArray kernel_size;
  if (kernel_size_.dyn_cast<pir::OpResult>() && kernel_size_.dyn_cast<pir::OpResult>().owner()->isa<paddle::dialect::FullIntArrayOp>()) {
    kernel_size = std::move(phi::IntArray(paddle::dialect::GetInt64Vector(
                          kernel_size_.dyn_cast<pir::OpResult>().owner()
                          ->dyn_cast<paddle::dialect::FullIntArrayOp>()
                          .attribute("value"))));
  } else if (kernel_size_.type().isa<pir::VectorType>()) {
    size_t kernel_size_size = kernel_size_.type().dyn_cast<pir::VectorType>().size();
    kernel_size = std::move(phi::IntArray(std::vector<int64_t>(kernel_size_size, -1)));
    kernel_size.SetFromTensor(true);
  } else if (kernel_size_.type().isa<paddle::dialect::DenseTensorType>()) {
    common::DDim kernel_size_dim = kernel_size_.type().dyn_cast<paddle::dialect::DenseTensorType>().dims();
    size_t kernel_size_size = common::product(kernel_size_dim);
    if (common::contain_unknown_dim(kernel_size_dim)) {
      kernel_size_size = 1;
    }
    kernel_size = std::move(phi::IntArray(std::vector<int64_t>(kernel_size_size, -1)));
    kernel_size.SetFromTensor(true);
  } else {
    PADDLE_THROW(phi::errors::Unimplemented("Only support VectorType or DenseTensorType"));
  }


  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);
  paddle::dialect::IrTensor dense_x_grad;
  paddle::dialect::IrMetaTensor meta_x_grad(&dense_x_grad);

  phi::UnchangedInferMeta(meta_x, &meta_x_grad);

  std::vector<pir::Type> argument_outputs;
  pir::Type x_grad_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_x_grad.dtype()), dense_x_grad.dims(), dense_x_grad.layout(), dense_x_grad.lod(), dense_x_grad.offset());
  argument_outputs.push_back(x_grad_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void Pool2dGradOp::VerifySig() {}

void Pool2dGradOp::InferMeta( phi::InferMetaContext *infer_meta ) {
  auto fn = PD_INFER_META(phi::UnchangedInferMeta);
  fn(infer_meta);
}

phi::DataType Pool2dGradOp::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: Pool2dGradOp";
  


  return expected_kernel_dtype;
}

const char *Pool3dGradOp::attributes_name[10] = { "kernel_size", "strides", "paddings", "ceil_mode", "exclusive", "data_format", "pooling_type", "global_pooling", "adaptive", "padding_algorithm" };

OpInfoTuple Pool3dGradOp::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("x", "paddle::dialect::DenseTensorType", false, false, false, false), paddle::dialect::OpInputInfo("out", "paddle::dialect::DenseTensorType", false, false, false, false), paddle::dialect::OpInputInfo("out_grad", "paddle::dialect::DenseTensorType", false, false, false, false) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = { paddle::dialect::OpAttributeInfo("kernel_size", "pir::ArrayAttribute<pir::Int32Attribute>", ""), paddle::dialect::OpAttributeInfo("strides", "pir::ArrayAttribute<pir::Int32Attribute>", ""), paddle::dialect::OpAttributeInfo("paddings", "pir::ArrayAttribute<pir::Int32Attribute>", ""), paddle::dialect::OpAttributeInfo("ceil_mode", "pir::BoolAttribute", ""), paddle::dialect::OpAttributeInfo("exclusive", "pir::BoolAttribute", ""), paddle::dialect::OpAttributeInfo("data_format", "pir::StrAttribute", ""), paddle::dialect::OpAttributeInfo("pooling_type", "pir::StrAttribute", ""), paddle::dialect::OpAttributeInfo("global_pooling", "pir::BoolAttribute", ""), paddle::dialect::OpAttributeInfo("adaptive", "pir::BoolAttribute", ""), paddle::dialect::OpAttributeInfo("padding_algorithm", "pir::StrAttribute", "") };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("x_grad", "paddle::dialect::DenseTensorType", false, false) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("UnchangedInferMeta", {"x"}, "pool3d_grad", {"x", "out", "out_grad", "kernel_size", "strides", "paddings", "ceil_mode", "exclusive", "data_format", "pooling_type", "global_pooling", "adaptive", "padding_algorithm"}, {}, {}, {}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "pool3d_grad");
}

void Pool3dGradOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, pir::Value out_, pir::Value out_grad_, const std::vector<int>& kernel_size, const std::vector<int>& strides, const std::vector<int>& paddings, bool ceil_mode, bool exclusive, const std::string& data_format, const std::string& pooling_type, bool global_pooling, bool adaptive, const std::string& padding_algorithm) {
  VLOG(4) << "Start build Pool3dGradOp";



  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_, out_, out_grad_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  std::vector<pir::Attribute> vec_kernel_size;
  for (size_t i = 0; i < static_cast<size_t>(kernel_size.size()); i++) {
      pir::Attribute attr_kernel_size = pir::Int32Attribute::get(pir::IrContext::Instance(), kernel_size[i]);

    vec_kernel_size.push_back(attr_kernel_size);
  }
  pir::Attribute attr_kernel_size = pir::ArrayAttribute::get(pir::IrContext::Instance(), vec_kernel_size);
  argument.AddAttribute("kernel_size", attr_kernel_size);
  std::vector<pir::Attribute> vec_strides;
  for (size_t i = 0; i < static_cast<size_t>(strides.size()); i++) {
      pir::Attribute attr_strides = pir::Int32Attribute::get(pir::IrContext::Instance(), strides[i]);

    vec_strides.push_back(attr_strides);
  }
  pir::Attribute attr_strides = pir::ArrayAttribute::get(pir::IrContext::Instance(), vec_strides);
  argument.AddAttribute("strides", attr_strides);
  std::vector<pir::Attribute> vec_paddings;
  for (size_t i = 0; i < static_cast<size_t>(paddings.size()); i++) {
      pir::Attribute attr_paddings = pir::Int32Attribute::get(pir::IrContext::Instance(), paddings[i]);

    vec_paddings.push_back(attr_paddings);
  }
  pir::Attribute attr_paddings = pir::ArrayAttribute::get(pir::IrContext::Instance(), vec_paddings);
  argument.AddAttribute("paddings", attr_paddings);
  pir::Attribute attr_ceil_mode = pir::BoolAttribute::get(pir::IrContext::Instance(), ceil_mode);
  argument.AddAttribute("ceil_mode", attr_ceil_mode);
  pir::Attribute attr_exclusive = pir::BoolAttribute::get(pir::IrContext::Instance(), exclusive);
  argument.AddAttribute("exclusive", attr_exclusive);
  pir::Attribute attr_data_format = pir::StrAttribute::get(pir::IrContext::Instance(), data_format);
  argument.AddAttribute("data_format", attr_data_format);
  pir::Attribute attr_pooling_type = pir::StrAttribute::get(pir::IrContext::Instance(), pooling_type);
  argument.AddAttribute("pooling_type", attr_pooling_type);
  pir::Attribute attr_global_pooling = pir::BoolAttribute::get(pir::IrContext::Instance(), global_pooling);
  argument.AddAttribute("global_pooling", attr_global_pooling);
  pir::Attribute attr_adaptive = pir::BoolAttribute::get(pir::IrContext::Instance(), adaptive);
  argument.AddAttribute("adaptive", attr_adaptive);
  pir::Attribute attr_padding_algorithm = pir::StrAttribute::get(pir::IrContext::Instance(), padding_algorithm);
  argument.AddAttribute("padding_algorithm", attr_padding_algorithm);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;
  paddle::dialect::DenseTensorType out = out_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)out;
  paddle::dialect::DenseTensorType out_grad = out_grad_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)out_grad;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);
  paddle::dialect::IrTensor dense_x_grad;
  paddle::dialect::IrMetaTensor meta_x_grad(&dense_x_grad);

  phi::UnchangedInferMeta(meta_x, &meta_x_grad);

  std::vector<pir::Type> argument_outputs;
  pir::Type x_grad_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_x_grad.dtype()), dense_x_grad.dims(), dense_x_grad.layout(), dense_x_grad.lod(), dense_x_grad.offset());
  argument_outputs.push_back(x_grad_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void Pool3dGradOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, pir::Value out_, pir::Value out_grad_, pir::AttributeMap attributes) {
  VLOG(4) << "Start build Pool3dGradOp";


  IR_ENFORCE(
      attributes.find("kernel_size") != attributes.end(),
          "'kernel_size' Attribute is expected for Pool3dGradOp. ");
  std::vector<int> kernel_size;
  for (size_t i = 0; i < attributes.at("kernel_size").dyn_cast<pir::ArrayAttribute>().size(); i++) {
    kernel_size.push_back(attributes.at("kernel_size").dyn_cast<pir::ArrayAttribute>().at(i).dyn_cast<pir::Int32Attribute>().data());
  }

  IR_ENFORCE(
      attributes.find("strides") != attributes.end(),
          "'strides' Attribute is expected for Pool3dGradOp. ");
  std::vector<int> strides;
  for (size_t i = 0; i < attributes.at("strides").dyn_cast<pir::ArrayAttribute>().size(); i++) {
    strides.push_back(attributes.at("strides").dyn_cast<pir::ArrayAttribute>().at(i).dyn_cast<pir::Int32Attribute>().data());
  }

  IR_ENFORCE(
      attributes.find("paddings") != attributes.end(),
          "'paddings' Attribute is expected for Pool3dGradOp. ");
  std::vector<int> paddings;
  for (size_t i = 0; i < attributes.at("paddings").dyn_cast<pir::ArrayAttribute>().size(); i++) {
    paddings.push_back(attributes.at("paddings").dyn_cast<pir::ArrayAttribute>().at(i).dyn_cast<pir::Int32Attribute>().data());
  }

  IR_ENFORCE(
      attributes.find("ceil_mode") != attributes.end(),
          "'ceil_mode' Attribute is expected for Pool3dGradOp. ");
  bool ceil_mode = attributes.at("ceil_mode").dyn_cast<pir::BoolAttribute>().data();

  IR_ENFORCE(
      attributes.find("exclusive") != attributes.end(),
          "'exclusive' Attribute is expected for Pool3dGradOp. ");
  bool exclusive = attributes.at("exclusive").dyn_cast<pir::BoolAttribute>().data();

  IR_ENFORCE(
      attributes.find("data_format") != attributes.end(),
          "'data_format' Attribute is expected for Pool3dGradOp. ");
  std::string data_format = attributes.at("data_format").dyn_cast<pir::StrAttribute>().AsString();

  IR_ENFORCE(
      attributes.find("pooling_type") != attributes.end(),
          "'pooling_type' Attribute is expected for Pool3dGradOp. ");
  std::string pooling_type = attributes.at("pooling_type").dyn_cast<pir::StrAttribute>().AsString();

  IR_ENFORCE(
      attributes.find("global_pooling") != attributes.end(),
          "'global_pooling' Attribute is expected for Pool3dGradOp. ");
  bool global_pooling = attributes.at("global_pooling").dyn_cast<pir::BoolAttribute>().data();

  IR_ENFORCE(
      attributes.find("adaptive") != attributes.end(),
          "'adaptive' Attribute is expected for Pool3dGradOp. ");
  bool adaptive = attributes.at("adaptive").dyn_cast<pir::BoolAttribute>().data();

  IR_ENFORCE(
      attributes.find("padding_algorithm") != attributes.end(),
          "'padding_algorithm' Attribute is expected for Pool3dGradOp. ");
  std::string padding_algorithm = attributes.at("padding_algorithm").dyn_cast<pir::StrAttribute>().AsString();


  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_, out_, out_grad_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  std::vector<pir::Attribute> vec_kernel_size;
  for (size_t i = 0; i < static_cast<size_t>(kernel_size.size()); i++) {
      pir::Attribute attr_kernel_size = pir::Int32Attribute::get(pir::IrContext::Instance(), kernel_size[i]);

    vec_kernel_size.push_back(attr_kernel_size);
  }
  pir::Attribute attr_kernel_size = pir::ArrayAttribute::get(pir::IrContext::Instance(), vec_kernel_size);
  argument.AddAttribute("kernel_size", attr_kernel_size);
  std::vector<pir::Attribute> vec_strides;
  for (size_t i = 0; i < static_cast<size_t>(strides.size()); i++) {
      pir::Attribute attr_strides = pir::Int32Attribute::get(pir::IrContext::Instance(), strides[i]);

    vec_strides.push_back(attr_strides);
  }
  pir::Attribute attr_strides = pir::ArrayAttribute::get(pir::IrContext::Instance(), vec_strides);
  argument.AddAttribute("strides", attr_strides);
  std::vector<pir::Attribute> vec_paddings;
  for (size_t i = 0; i < static_cast<size_t>(paddings.size()); i++) {
      pir::Attribute attr_paddings = pir::Int32Attribute::get(pir::IrContext::Instance(), paddings[i]);

    vec_paddings.push_back(attr_paddings);
  }
  pir::Attribute attr_paddings = pir::ArrayAttribute::get(pir::IrContext::Instance(), vec_paddings);
  argument.AddAttribute("paddings", attr_paddings);
  pir::Attribute attr_ceil_mode = pir::BoolAttribute::get(pir::IrContext::Instance(), ceil_mode);
  argument.AddAttribute("ceil_mode", attr_ceil_mode);
  pir::Attribute attr_exclusive = pir::BoolAttribute::get(pir::IrContext::Instance(), exclusive);
  argument.AddAttribute("exclusive", attr_exclusive);
  pir::Attribute attr_data_format = pir::StrAttribute::get(pir::IrContext::Instance(), data_format);
  argument.AddAttribute("data_format", attr_data_format);
  pir::Attribute attr_pooling_type = pir::StrAttribute::get(pir::IrContext::Instance(), pooling_type);
  argument.AddAttribute("pooling_type", attr_pooling_type);
  pir::Attribute attr_global_pooling = pir::BoolAttribute::get(pir::IrContext::Instance(), global_pooling);
  argument.AddAttribute("global_pooling", attr_global_pooling);
  pir::Attribute attr_adaptive = pir::BoolAttribute::get(pir::IrContext::Instance(), adaptive);
  argument.AddAttribute("adaptive", attr_adaptive);
  pir::Attribute attr_padding_algorithm = pir::StrAttribute::get(pir::IrContext::Instance(), padding_algorithm);
  argument.AddAttribute("padding_algorithm", attr_padding_algorithm);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;
  paddle::dialect::DenseTensorType out = out_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)out;
  paddle::dialect::DenseTensorType out_grad = out_grad_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)out_grad;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);
  paddle::dialect::IrTensor dense_x_grad;
  paddle::dialect::IrMetaTensor meta_x_grad(&dense_x_grad);

  phi::UnchangedInferMeta(meta_x, &meta_x_grad);

  std::vector<pir::Type> argument_outputs;
  pir::Type x_grad_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_x_grad.dtype()), dense_x_grad.dims(), dense_x_grad.layout(), dense_x_grad.lod(), dense_x_grad.offset());
  argument_outputs.push_back(x_grad_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void Pool3dGradOp::VerifySig() {}

void Pool3dGradOp::InferMeta( phi::InferMetaContext *infer_meta ) {
  auto fn = PD_INFER_META(phi::UnchangedInferMeta);
  fn(infer_meta);
}

phi::DataType Pool3dGradOp::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: Pool3dGradOp";
  


  return expected_kernel_dtype;
}

const char *ProdGradOp::attributes_name[2] = { "keep_dim", "reduce_all" };

OpInfoTuple ProdGradOp::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("x", "paddle::dialect::DenseTensorType", false, false, false, false), paddle::dialect::OpInputInfo("out", "paddle::dialect::DenseTensorType", false, false, false, false), paddle::dialect::OpInputInfo("out_grad", "paddle::dialect::DenseTensorType", false, false, false, false), paddle::dialect::OpInputInfo("dims", "paddle::dialect::IntArrayAttribute", false, false, true, false) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = { paddle::dialect::OpAttributeInfo("keep_dim", "pir::BoolAttribute", ""), paddle::dialect::OpAttributeInfo("reduce_all", "pir::BoolAttribute", "") };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("x_grad", "paddle::dialect::DenseTensorType", false, false) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("UnchangedInferMeta", {"x"}, "prod_grad", {"x", "out", "out_grad", "dims", "keep_dim", "reduce_all"}, {}, {}, {}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "prod_grad");
}

void ProdGradOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, pir::Value out_, pir::Value out_grad_, const std::vector<int64_t>& dims, bool keep_dim, bool reduce_all) {
  VLOG(4) << "Start build ProdGradOp";


  // Generate int_array mutable attribute: dims
  paddle::dialect::FullIntArrayOp full_dims_op = builder.Build<paddle::dialect::FullIntArrayOp>(dims, phi::DataType::INT64, phi::CPUPlace());
  pir::OpResult dims_ = full_dims_op->result(0);
    
  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_, out_, out_grad_, dims_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_keep_dim = pir::BoolAttribute::get(pir::IrContext::Instance(), keep_dim);
  argument.AddAttribute("keep_dim", attr_keep_dim);
  pir::Attribute attr_reduce_all = pir::BoolAttribute::get(pir::IrContext::Instance(), reduce_all);
  argument.AddAttribute("reduce_all", attr_reduce_all);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;
  paddle::dialect::DenseTensorType out = out_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)out;
  paddle::dialect::DenseTensorType out_grad = out_grad_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)out_grad;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);
  paddle::dialect::IrTensor dense_x_grad;
  paddle::dialect::IrMetaTensor meta_x_grad(&dense_x_grad);

  phi::UnchangedInferMeta(meta_x, &meta_x_grad);

  std::vector<pir::Type> argument_outputs;
  pir::Type x_grad_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_x_grad.dtype()), dense_x_grad.dims(), dense_x_grad.layout(), dense_x_grad.lod(), dense_x_grad.offset());
  argument_outputs.push_back(x_grad_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void ProdGradOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, pir::Value out_, pir::Value out_grad_, pir::AttributeMap attributes) {
  VLOG(4) << "Start build ProdGradOp";


  IR_ENFORCE(
      attributes.find("dims") != attributes.end(),
          "'dims' Attribute is expected for ProdGradOp. ");
  std::vector<int64_t> dims = attributes.at("dims").dyn_cast<paddle::dialect::IntArrayAttribute>().data().GetData();

  IR_ENFORCE(
      attributes.find("keep_dim") != attributes.end(),
          "'keep_dim' Attribute is expected for ProdGradOp. ");
  bool keep_dim = attributes.at("keep_dim").dyn_cast<pir::BoolAttribute>().data();

  IR_ENFORCE(
      attributes.find("reduce_all") != attributes.end(),
          "'reduce_all' Attribute is expected for ProdGradOp. ");
  bool reduce_all = attributes.at("reduce_all").dyn_cast<pir::BoolAttribute>().data();

  // Generate int_array mutable attribute: dims
  paddle::dialect::FullIntArrayOp full_dims_op = builder.Build<paddle::dialect::FullIntArrayOp>(dims, phi::DataType::INT64, phi::CPUPlace());
  pir::OpResult dims_ = full_dims_op->result(0);
    
  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_, out_, out_grad_, dims_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_keep_dim = pir::BoolAttribute::get(pir::IrContext::Instance(), keep_dim);
  argument.AddAttribute("keep_dim", attr_keep_dim);
  pir::Attribute attr_reduce_all = pir::BoolAttribute::get(pir::IrContext::Instance(), reduce_all);
  argument.AddAttribute("reduce_all", attr_reduce_all);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;
  paddle::dialect::DenseTensorType out = out_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)out;
  paddle::dialect::DenseTensorType out_grad = out_grad_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)out_grad;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);
  paddle::dialect::IrTensor dense_x_grad;
  paddle::dialect::IrMetaTensor meta_x_grad(&dense_x_grad);

  phi::UnchangedInferMeta(meta_x, &meta_x_grad);

  std::vector<pir::Type> argument_outputs;
  pir::Type x_grad_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_x_grad.dtype()), dense_x_grad.dims(), dense_x_grad.layout(), dense_x_grad.lod(), dense_x_grad.offset());
  argument_outputs.push_back(x_grad_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void ProdGradOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, pir::Value out_, pir::Value out_grad_, pir::Value dims_, bool keep_dim, bool reduce_all) {
  VLOG(4) << "Start build ProdGradOp";



  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_, out_, out_grad_, dims_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_keep_dim = pir::BoolAttribute::get(pir::IrContext::Instance(), keep_dim);
  argument.AddAttribute("keep_dim", attr_keep_dim);
  pir::Attribute attr_reduce_all = pir::BoolAttribute::get(pir::IrContext::Instance(), reduce_all);
  argument.AddAttribute("reduce_all", attr_reduce_all);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;
  paddle::dialect::DenseTensorType out = out_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)out;
  paddle::dialect::DenseTensorType out_grad = out_grad_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)out_grad;
  phi::IntArray dims;
  if (dims_.dyn_cast<pir::OpResult>() && dims_.dyn_cast<pir::OpResult>().owner()->isa<paddle::dialect::FullIntArrayOp>()) {
    dims = std::move(phi::IntArray(paddle::dialect::GetInt64Vector(
                          dims_.dyn_cast<pir::OpResult>().owner()
                          ->dyn_cast<paddle::dialect::FullIntArrayOp>()
                          .attribute("value"))));
  } else if (dims_.type().isa<pir::VectorType>()) {
    size_t dims_size = dims_.type().dyn_cast<pir::VectorType>().size();
    dims = std::move(phi::IntArray(std::vector<int64_t>(dims_size, -1)));
    dims.SetFromTensor(true);
  } else if (dims_.type().isa<paddle::dialect::DenseTensorType>()) {
    common::DDim dims_dim = dims_.type().dyn_cast<paddle::dialect::DenseTensorType>().dims();
    size_t dims_size = common::product(dims_dim);
    if (common::contain_unknown_dim(dims_dim)) {
      dims_size = 1;
    }
    dims = std::move(phi::IntArray(std::vector<int64_t>(dims_size, -1)));
    dims.SetFromTensor(true);
  } else {
    PADDLE_THROW(phi::errors::Unimplemented("Only support VectorType or DenseTensorType"));
  }


  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);
  paddle::dialect::IrTensor dense_x_grad;
  paddle::dialect::IrMetaTensor meta_x_grad(&dense_x_grad);

  phi::UnchangedInferMeta(meta_x, &meta_x_grad);

  std::vector<pir::Type> argument_outputs;
  pir::Type x_grad_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_x_grad.dtype()), dense_x_grad.dims(), dense_x_grad.layout(), dense_x_grad.lod(), dense_x_grad.offset());
  argument_outputs.push_back(x_grad_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void ProdGradOp::VerifySig() {}

void ProdGradOp::InferMeta( phi::InferMetaContext *infer_meta ) {
  auto fn = PD_INFER_META(phi::UnchangedInferMeta);
  fn(infer_meta);
}

phi::DataType ProdGradOp::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: ProdGradOp";
  


  return expected_kernel_dtype;
}

const char *RepeatInterleaveGradOp::attributes_name[2] = { "repeats", "axis" };

OpInfoTuple RepeatInterleaveGradOp::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("x", "paddle::dialect::DenseTensorType", false, false, false, false), paddle::dialect::OpInputInfo("out_grad", "paddle::dialect::DenseTensorType", false, false, false, false) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = { paddle::dialect::OpAttributeInfo("repeats", "pir::Int32Attribute", ""), paddle::dialect::OpAttributeInfo("axis", "pir::Int32Attribute", "") };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("x_grad", "paddle::dialect::DenseTensorType", false, false) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("UnchangedInferMeta", {"x"}, "repeat_interleave_grad", {"x", "out_grad", "repeats", "axis"}, {}, {}, {}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "repeat_interleave_grad");
}

void RepeatInterleaveGradOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, pir::Value out_grad_, int repeats, int axis) {
  VLOG(4) << "Start build RepeatInterleaveGradOp";



  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_, out_grad_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_repeats = pir::Int32Attribute::get(pir::IrContext::Instance(), repeats);
  argument.AddAttribute("repeats", attr_repeats);
  pir::Attribute attr_axis = pir::Int32Attribute::get(pir::IrContext::Instance(), axis);
  argument.AddAttribute("axis", attr_axis);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;
  paddle::dialect::DenseTensorType out_grad = out_grad_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)out_grad;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);
  paddle::dialect::IrTensor dense_x_grad;
  paddle::dialect::IrMetaTensor meta_x_grad(&dense_x_grad);

  phi::UnchangedInferMeta(meta_x, &meta_x_grad);

  std::vector<pir::Type> argument_outputs;
  pir::Type x_grad_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_x_grad.dtype()), dense_x_grad.dims(), dense_x_grad.layout(), dense_x_grad.lod(), dense_x_grad.offset());
  argument_outputs.push_back(x_grad_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void RepeatInterleaveGradOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, pir::Value out_grad_, pir::AttributeMap attributes) {
  VLOG(4) << "Start build RepeatInterleaveGradOp";


  IR_ENFORCE(
      attributes.find("repeats") != attributes.end(),
          "'repeats' Attribute is expected for RepeatInterleaveGradOp. ");
  int repeats = attributes.at("repeats").dyn_cast<pir::Int32Attribute>().data();

  IR_ENFORCE(
      attributes.find("axis") != attributes.end(),
          "'axis' Attribute is expected for RepeatInterleaveGradOp. ");
  int axis = attributes.at("axis").dyn_cast<pir::Int32Attribute>().data();


  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_, out_grad_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_repeats = pir::Int32Attribute::get(pir::IrContext::Instance(), repeats);
  argument.AddAttribute("repeats", attr_repeats);
  pir::Attribute attr_axis = pir::Int32Attribute::get(pir::IrContext::Instance(), axis);
  argument.AddAttribute("axis", attr_axis);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;
  paddle::dialect::DenseTensorType out_grad = out_grad_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)out_grad;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);
  paddle::dialect::IrTensor dense_x_grad;
  paddle::dialect::IrMetaTensor meta_x_grad(&dense_x_grad);

  phi::UnchangedInferMeta(meta_x, &meta_x_grad);

  std::vector<pir::Type> argument_outputs;
  pir::Type x_grad_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_x_grad.dtype()), dense_x_grad.dims(), dense_x_grad.layout(), dense_x_grad.lod(), dense_x_grad.offset());
  argument_outputs.push_back(x_grad_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void RepeatInterleaveGradOp::VerifySig() {}

void RepeatInterleaveGradOp::InferMeta( phi::InferMetaContext *infer_meta ) {
  auto fn = PD_INFER_META(phi::UnchangedInferMeta);
  fn(infer_meta);
}

phi::DataType RepeatInterleaveGradOp::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: RepeatInterleaveGradOp";
  


  return expected_kernel_dtype;
}

const char *RepeatInterleaveWithTensorIndexGradOp::attributes_name[1] = { "axis" };

OpInfoTuple RepeatInterleaveWithTensorIndexGradOp::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("x", "paddle::dialect::DenseTensorType", false, false, false, false), paddle::dialect::OpInputInfo("repeats", "paddle::dialect::DenseTensorType", false, false, false, false), paddle::dialect::OpInputInfo("out_grad", "paddle::dialect::DenseTensorType", false, false, false, false) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = { paddle::dialect::OpAttributeInfo("axis", "pir::Int32Attribute", "") };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("x_grad", "paddle::dialect::DenseTensorType", false, false) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("UnchangedInferMeta", {"x"}, "repeat_interleave_with_tensor_index_grad", {"x", "repeats", "out_grad", "axis"}, {"x"}, {}, {}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "repeat_interleave_with_tensor_index_grad");
}

void RepeatInterleaveWithTensorIndexGradOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, pir::Value repeats_, pir::Value out_grad_, int axis) {
  VLOG(4) << "Start build RepeatInterleaveWithTensorIndexGradOp";



  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_, repeats_, out_grad_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_axis = pir::Int32Attribute::get(pir::IrContext::Instance(), axis);
  argument.AddAttribute("axis", attr_axis);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;
  paddle::dialect::DenseTensorType repeats = repeats_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)repeats;
  paddle::dialect::DenseTensorType out_grad = out_grad_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)out_grad;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);
  paddle::dialect::IrTensor dense_x_grad;
  paddle::dialect::IrMetaTensor meta_x_grad(&dense_x_grad);

  phi::UnchangedInferMeta(meta_x, &meta_x_grad);

  std::vector<pir::Type> argument_outputs;
  pir::Type x_grad_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_x_grad.dtype()), dense_x_grad.dims(), dense_x_grad.layout(), dense_x_grad.lod(), dense_x_grad.offset());
  argument_outputs.push_back(x_grad_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void RepeatInterleaveWithTensorIndexGradOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, pir::Value repeats_, pir::Value out_grad_, pir::AttributeMap attributes) {
  VLOG(4) << "Start build RepeatInterleaveWithTensorIndexGradOp";


  IR_ENFORCE(
      attributes.find("axis") != attributes.end(),
          "'axis' Attribute is expected for RepeatInterleaveWithTensorIndexGradOp. ");
  int axis = attributes.at("axis").dyn_cast<pir::Int32Attribute>().data();


  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_, repeats_, out_grad_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_axis = pir::Int32Attribute::get(pir::IrContext::Instance(), axis);
  argument.AddAttribute("axis", attr_axis);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;
  paddle::dialect::DenseTensorType repeats = repeats_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)repeats;
  paddle::dialect::DenseTensorType out_grad = out_grad_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)out_grad;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);
  paddle::dialect::IrTensor dense_x_grad;
  paddle::dialect::IrMetaTensor meta_x_grad(&dense_x_grad);

  phi::UnchangedInferMeta(meta_x, &meta_x_grad);

  std::vector<pir::Type> argument_outputs;
  pir::Type x_grad_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_x_grad.dtype()), dense_x_grad.dims(), dense_x_grad.layout(), dense_x_grad.lod(), dense_x_grad.offset());
  argument_outputs.push_back(x_grad_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void RepeatInterleaveWithTensorIndexGradOp::VerifySig() {}

void RepeatInterleaveWithTensorIndexGradOp::InferMeta( phi::InferMetaContext *infer_meta ) {
  auto fn = PD_INFER_META(phi::UnchangedInferMeta);
  fn(infer_meta);
}

phi::DataType RepeatInterleaveWithTensorIndexGradOp::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: RepeatInterleaveWithTensorIndexGradOp";
  


  return expected_kernel_dtype;
}

OpInfoTuple ReshapeDoubleGradOp::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("grad_out", "paddle::dialect::DenseTensorType", false, true, false, false), paddle::dialect::OpInputInfo("grad_x_grad", "paddle::dialect::DenseTensorType", false, false, false, false) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = {  };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("grad_out_grad", "paddle::dialect::DenseTensorType", false, false) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("UnchangedInferMeta", {"grad_out"}, "reshape_double_grad", {"grad_out", "grad_x_grad"}, {}, {}, {}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "reshape_double_grad");
}

void ReshapeDoubleGradOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value grad_out_, pir::Value grad_x_grad_) {
  VLOG(4) << "Start build ReshapeDoubleGradOp";



  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {grad_out_, grad_x_grad_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType grad_out = grad_out_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)grad_out;
  paddle::dialect::DenseTensorType grad_x_grad = grad_x_grad_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)grad_x_grad;

  VLOG(4) << "Builder construction  dense_grad_out";
  paddle::dialect::IrTensor ir_tensor_grad_out(paddle::dialect::TransToPhiDataType(grad_out.dtype()),
                                                      grad_out.dims(),
                                                      grad_out.data_layout(),
                                                      grad_out.lod(),
                                                      grad_out.offset());
  VLOG(4) << "Builder construction  meta_grad_out";
  paddle::dialect::IrMetaTensor meta_grad_out(&ir_tensor_grad_out);
  paddle::dialect::IrTensor dense_grad_out_grad;
  paddle::dialect::IrMetaTensor meta_grad_out_grad(&dense_grad_out_grad);

  phi::UnchangedInferMeta(meta_grad_out, &meta_grad_out_grad);

  std::vector<pir::Type> argument_outputs;
  pir::Type grad_out_grad_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_grad_out_grad.dtype()), dense_grad_out_grad.dims(), dense_grad_out_grad.layout(), dense_grad_out_grad.lod(), dense_grad_out_grad.offset());
  argument_outputs.push_back(grad_out_grad_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void ReshapeDoubleGradOp::VerifySig() {}

void ReshapeDoubleGradOp::InferMeta( phi::InferMetaContext *infer_meta ) {
  auto fn = PD_INFER_META(phi::UnchangedInferMeta);
  fn(infer_meta);
}

phi::DataType ReshapeDoubleGradOp::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: ReshapeDoubleGradOp";
  


  return expected_kernel_dtype;
}

OpInfoTuple ReshapeDoubleGrad_Op::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("grad_out", "paddle::dialect::DenseTensorType", false, true, false, false), paddle::dialect::OpInputInfo("grad_x_grad", "paddle::dialect::DenseTensorType", false, false, false, false) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = {  };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("grad_out_grad", "paddle::dialect::DenseTensorType", false, false) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("UnchangedInferMeta", {"grad_out"}, "reshape_double_grad", {"grad_out", "grad_x_grad"}, {}, {}, {{"grad_out_grad", "grad_x_grad"}}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "reshape_double_grad");
}

void ReshapeDoubleGrad_Op::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value grad_out_, pir::Value grad_x_grad_) {
  VLOG(4) << "Start build ReshapeDoubleGrad_Op";



  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {grad_out_, grad_x_grad_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType grad_out = grad_out_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)grad_out;
  paddle::dialect::DenseTensorType grad_x_grad = grad_x_grad_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)grad_x_grad;

  VLOG(4) << "Builder construction  dense_grad_out";
  paddle::dialect::IrTensor ir_tensor_grad_out(paddle::dialect::TransToPhiDataType(grad_out.dtype()),
                                                      grad_out.dims(),
                                                      grad_out.data_layout(),
                                                      grad_out.lod(),
                                                      grad_out.offset());
  VLOG(4) << "Builder construction  meta_grad_out";
  paddle::dialect::IrMetaTensor meta_grad_out(&ir_tensor_grad_out);
  paddle::dialect::IrTensor dense_grad_out_grad;
  paddle::dialect::IrMetaTensor meta_grad_out_grad(&dense_grad_out_grad);

  phi::UnchangedInferMeta(meta_grad_out, &meta_grad_out_grad);

  std::vector<pir::Type> argument_outputs;
  pir::Type grad_out_grad_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_grad_out_grad.dtype()), dense_grad_out_grad.dims(), dense_grad_out_grad.layout(), dense_grad_out_grad.lod(), dense_grad_out_grad.offset());
  argument_outputs.push_back(grad_out_grad_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void ReshapeDoubleGrad_Op::VerifySig() {}

void ReshapeDoubleGrad_Op::InferMeta( phi::InferMetaContext *infer_meta ) {
  auto fn = PD_INFER_META(phi::UnchangedInferMeta);
  fn(infer_meta);
}

phi::DataType ReshapeDoubleGrad_Op::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: ReshapeDoubleGrad_Op";
  


  return expected_kernel_dtype;
}

OpInfoTuple ReshapeGradOp::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("xshape", "paddle::dialect::DenseTensorType", false, false, false, false), paddle::dialect::OpInputInfo("out_grad", "paddle::dialect::DenseTensorType", false, false, false, true) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = {  };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("x_grad", "paddle::dialect::DenseTensorType", false, false) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("KernelWithXShapeInferMeta", {"xshape", "out_grad"}, "reshape_grad", {"out_grad"}, {"out_grad"}, {"out_grad"}, {}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "reshape_grad");
}

void ReshapeGradOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value xshape_, pir::Value out_grad_) {
  VLOG(4) << "Start build ReshapeGradOp";



  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {xshape_, out_grad_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType xshape = xshape_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)xshape;
  paddle::dialect::DenseTensorType out_grad = out_grad_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)out_grad;

  VLOG(4) << "Builder construction  dense_xshape";
  paddle::dialect::IrTensor ir_tensor_xshape(paddle::dialect::TransToPhiDataType(xshape.dtype()),
                                                      xshape.dims(),
                                                      xshape.data_layout(),
                                                      xshape.lod(),
                                                      xshape.offset());
  VLOG(4) << "Builder construction  meta_xshape";
  paddle::dialect::IrMetaTensor meta_xshape(&ir_tensor_xshape);

  VLOG(4) << "Builder construction  dense_out_grad";
  paddle::dialect::IrTensor ir_tensor_out_grad(paddle::dialect::TransToPhiDataType(out_grad.dtype()),
                                                      out_grad.dims(),
                                                      out_grad.data_layout(),
                                                      out_grad.lod(),
                                                      out_grad.offset());
  VLOG(4) << "Builder construction  meta_out_grad";
  paddle::dialect::IrMetaTensor meta_out_grad(&ir_tensor_out_grad);
  paddle::dialect::IrTensor dense_x_grad;
  paddle::dialect::IrMetaTensor meta_x_grad(&dense_x_grad);

  phi::KernelWithXShapeInferMeta(meta_xshape, meta_out_grad, &meta_x_grad);

  std::vector<pir::Type> argument_outputs;
  pir::Type x_grad_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_x_grad.dtype()), dense_x_grad.dims(), dense_x_grad.layout(), dense_x_grad.lod(), dense_x_grad.offset());
  argument_outputs.push_back(x_grad_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void ReshapeGradOp::VerifySig() {}

void ReshapeGradOp::InferMeta( phi::InferMetaContext *infer_meta ) {
  auto fn = PD_INFER_META(phi::KernelWithXShapeInferMeta);
  fn(infer_meta);
}

phi::DataType ReshapeGradOp::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: ReshapeGradOp";
  


  return expected_kernel_dtype;
}

OpInfoTuple ReshapeGrad_Op::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("xshape", "paddle::dialect::DenseTensorType", false, false, false, false), paddle::dialect::OpInputInfo("out_grad", "paddle::dialect::DenseTensorType", false, false, false, true) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = {  };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("x_grad", "paddle::dialect::DenseTensorType", false, false) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("KernelWithXShapeInferMeta", {"xshape", "out_grad"}, "reshape_grad", {"out_grad"}, {"out_grad"}, {"out_grad"}, {{"x_grad", "out_grad"}}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "reshape_grad");
}

void ReshapeGrad_Op::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value xshape_, pir::Value out_grad_) {
  VLOG(4) << "Start build ReshapeGrad_Op";



  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {xshape_, out_grad_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType xshape = xshape_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)xshape;
  paddle::dialect::DenseTensorType out_grad = out_grad_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)out_grad;

  VLOG(4) << "Builder construction  dense_xshape";
  paddle::dialect::IrTensor ir_tensor_xshape(paddle::dialect::TransToPhiDataType(xshape.dtype()),
                                                      xshape.dims(),
                                                      xshape.data_layout(),
                                                      xshape.lod(),
                                                      xshape.offset());
  VLOG(4) << "Builder construction  meta_xshape";
  paddle::dialect::IrMetaTensor meta_xshape(&ir_tensor_xshape);

  VLOG(4) << "Builder construction  dense_out_grad";
  paddle::dialect::IrTensor ir_tensor_out_grad(paddle::dialect::TransToPhiDataType(out_grad.dtype()),
                                                      out_grad.dims(),
                                                      out_grad.data_layout(),
                                                      out_grad.lod(),
                                                      out_grad.offset());
  VLOG(4) << "Builder construction  meta_out_grad";
  paddle::dialect::IrMetaTensor meta_out_grad(&ir_tensor_out_grad);
  paddle::dialect::IrTensor dense_x_grad;
  paddle::dialect::IrMetaTensor meta_x_grad(&dense_x_grad);

  phi::KernelWithXShapeInferMeta(meta_xshape, meta_out_grad, &meta_x_grad);

  std::vector<pir::Type> argument_outputs;
  pir::Type x_grad_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_x_grad.dtype()), dense_x_grad.dims(), dense_x_grad.layout(), dense_x_grad.lod(), dense_x_grad.offset());
  argument_outputs.push_back(x_grad_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void ReshapeGrad_Op::VerifySig() {}

void ReshapeGrad_Op::InferMeta( phi::InferMetaContext *infer_meta ) {
  auto fn = PD_INFER_META(phi::KernelWithXShapeInferMeta);
  fn(infer_meta);
}

phi::DataType ReshapeGrad_Op::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: ReshapeGrad_Op";
  


  return expected_kernel_dtype;
}

const char *RnnGradOp::attributes_name[8] = { "dropout_prob", "is_bidirec", "input_size", "hidden_size", "num_layers", "mode", "seed", "is_test" };

OpInfoTuple RnnGradOp::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("x", "paddle::dialect::DenseTensorType", false, false, false, false), paddle::dialect::OpInputInfo("pre_state", "pir::VectorType<paddle::dialect::DenseTensorType>", false, false, false, false), paddle::dialect::OpInputInfo("weight_list", "pir::VectorType<paddle::dialect::DenseTensorType>", false, false, false, false), paddle::dialect::OpInputInfo("sequence_length", "paddle::dialect::DenseTensorType", true, false, false, false), paddle::dialect::OpInputInfo("out", "paddle::dialect::DenseTensorType", false, false, false, false), paddle::dialect::OpInputInfo("dropout_state_out", "paddle::dialect::DenseTensorType", false, false, false, false), paddle::dialect::OpInputInfo("reserve", "paddle::dialect::DenseTensorType", false, false, false, false), paddle::dialect::OpInputInfo("out_grad", "paddle::dialect::DenseTensorType", false, false, false, false), paddle::dialect::OpInputInfo("state_grad", "pir::VectorType<paddle::dialect::DenseTensorType>", false, false, false, false) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = { paddle::dialect::OpAttributeInfo("dropout_prob", "pir::FloatAttribute", ""), paddle::dialect::OpAttributeInfo("is_bidirec", "pir::BoolAttribute", ""), paddle::dialect::OpAttributeInfo("input_size", "pir::Int32Attribute", ""), paddle::dialect::OpAttributeInfo("hidden_size", "pir::Int32Attribute", ""), paddle::dialect::OpAttributeInfo("num_layers", "pir::Int32Attribute", ""), paddle::dialect::OpAttributeInfo("mode", "pir::StrAttribute", ""), paddle::dialect::OpAttributeInfo("seed", "pir::Int32Attribute", ""), paddle::dialect::OpAttributeInfo("is_test", "pir::BoolAttribute", "") };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("x_grad", "paddle::dialect::DenseTensorType", false, false), paddle::dialect::OpOutputInfo("pre_state_grad", "pir::VectorType<paddle::dialect::DenseTensorType>", false, false), paddle::dialect::OpOutputInfo("weight_list_grad", "pir::VectorType<paddle::dialect::DenseTensorType>", false, false) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("RnnGradInferMeta", {"x", "pre_state", "weight_list"}, "rnn_grad", {"x", "pre_state", "weight_list", "sequence_length", "out", "dropout_state_out", "reserve", "out_grad", "state_grad", "dropout_prob", "is_bidirec", "input_size", "hidden_size", "num_layers", "mode", "seed", "is_test"}, {"out_grad"}, {}, {}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "rnn_grad");
}

void RnnGradOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, pir::Value pre_state_, pir::Value weight_list_, pir::Value sequence_length_, pir::Value out_, pir::Value dropout_state_out_, pir::Value reserve_, pir::Value out_grad_, pir::Value state_grad_, float dropout_prob, bool is_bidirec, int input_size, int hidden_size, int num_layers, const std::string& mode, int seed, bool is_test) {
  VLOG(4) << "Start build RnnGradOp";



  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_, pre_state_, weight_list_, sequence_length_, out_, dropout_state_out_, reserve_, out_grad_, state_grad_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_dropout_prob = pir::FloatAttribute::get(pir::IrContext::Instance(), dropout_prob);
  argument.AddAttribute("dropout_prob", attr_dropout_prob);
  pir::Attribute attr_is_bidirec = pir::BoolAttribute::get(pir::IrContext::Instance(), is_bidirec);
  argument.AddAttribute("is_bidirec", attr_is_bidirec);
  pir::Attribute attr_input_size = pir::Int32Attribute::get(pir::IrContext::Instance(), input_size);
  argument.AddAttribute("input_size", attr_input_size);
  pir::Attribute attr_hidden_size = pir::Int32Attribute::get(pir::IrContext::Instance(), hidden_size);
  argument.AddAttribute("hidden_size", attr_hidden_size);
  pir::Attribute attr_num_layers = pir::Int32Attribute::get(pir::IrContext::Instance(), num_layers);
  argument.AddAttribute("num_layers", attr_num_layers);
  pir::Attribute attr_mode = pir::StrAttribute::get(pir::IrContext::Instance(), mode);
  argument.AddAttribute("mode", attr_mode);
  pir::Attribute attr_seed = pir::Int32Attribute::get(pir::IrContext::Instance(), seed);
  argument.AddAttribute("seed", attr_seed);
  pir::Attribute attr_is_test = pir::BoolAttribute::get(pir::IrContext::Instance(), is_test);
  argument.AddAttribute("is_test", attr_is_test);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;
  pir::VectorType pre_state = pre_state_.type().dyn_cast<pir::VectorType>(); (void)pre_state;
  pir::VectorType weight_list = weight_list_.type().dyn_cast<pir::VectorType>(); (void)weight_list;
  paddle::dialect::DenseTensorType out = out_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)out;
  paddle::dialect::DenseTensorType dropout_state_out = dropout_state_out_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)dropout_state_out;
  paddle::dialect::DenseTensorType reserve = reserve_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)reserve;
  paddle::dialect::DenseTensorType out_grad = out_grad_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)out_grad;
  pir::VectorType state_grad = state_grad_.type().dyn_cast<pir::VectorType>(); (void)state_grad;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);
  std::vector<paddle::dialect::IrTensor> vec_ir_tensor_pre_state;
  for (size_t i=0; i < static_cast<size_t>(pre_state.size()); i++) {
    vec_ir_tensor_pre_state.push_back(paddle::dialect::IrTensor(paddle::dialect::TransToPhiDataType(pre_state[i].dyn_cast<paddle::dialect::DenseTensorType>().dtype()),
                                                                     pre_state[i].dyn_cast<paddle::dialect::DenseTensorType>().dims(),
                                                                     pre_state[i].dyn_cast<paddle::dialect::DenseTensorType>().data_layout(),
                                                                     pre_state[i].dyn_cast<paddle::dialect::DenseTensorType>().lod(),
                                                                     pre_state[i].dyn_cast<paddle::dialect::DenseTensorType>().offset()));
  }
  std::vector<paddle::dialect::IrMetaTensor> vec_meta_pre_state;
  for (size_t i=0; i < vec_ir_tensor_pre_state.size(); i++) {
    vec_meta_pre_state.push_back(paddle::dialect::IrMetaTensor(&vec_ir_tensor_pre_state[i]));
  }

  std::vector<const phi::MetaTensor*> meta_pre_state;
  for (size_t i=0; i < static_cast<size_t>(vec_meta_pre_state.size()); i++) {
    meta_pre_state.push_back(&vec_meta_pre_state[i]);
  }
   std::vector<paddle::dialect::IrTensor> vec_ir_tensor_weight_list;
  for (size_t i=0; i < static_cast<size_t>(weight_list.size()); i++) {
    vec_ir_tensor_weight_list.push_back(paddle::dialect::IrTensor(paddle::dialect::TransToPhiDataType(weight_list[i].dyn_cast<paddle::dialect::DenseTensorType>().dtype()),
                                                                     weight_list[i].dyn_cast<paddle::dialect::DenseTensorType>().dims(),
                                                                     weight_list[i].dyn_cast<paddle::dialect::DenseTensorType>().data_layout(),
                                                                     weight_list[i].dyn_cast<paddle::dialect::DenseTensorType>().lod(),
                                                                     weight_list[i].dyn_cast<paddle::dialect::DenseTensorType>().offset()));
  }
  std::vector<paddle::dialect::IrMetaTensor> vec_meta_weight_list;
  for (size_t i=0; i < vec_ir_tensor_weight_list.size(); i++) {
    vec_meta_weight_list.push_back(paddle::dialect::IrMetaTensor(&vec_ir_tensor_weight_list[i]));
  }

  std::vector<const phi::MetaTensor*> meta_weight_list;
  for (size_t i=0; i < static_cast<size_t>(vec_meta_weight_list.size()); i++) {
    meta_weight_list.push_back(&vec_meta_weight_list[i]);
  }
   paddle::dialect::IrTensor dense_x_grad;
  paddle::dialect::IrMetaTensor meta_x_grad(&dense_x_grad);
  std::vector<paddle::dialect::IrTensor> vec_dense_pre_state_grad((pre_state.size()), paddle::dialect::IrTensor());
  std::vector<paddle::dialect::IrMetaTensor> vec_meta_pre_state_grad;
  for (size_t i=0; i < static_cast<size_t>(pre_state.size()); i++) {
    vec_meta_pre_state_grad.push_back(paddle::dialect::IrMetaTensor(&vec_dense_pre_state_grad[i]));
  }
  std::vector<phi::MetaTensor*> meta_pre_state_grad;
  for (size_t i=0; i < static_cast<size_t>(vec_meta_pre_state_grad.size()); i++) {
    meta_pre_state_grad.push_back(&vec_meta_pre_state_grad[i]);
  }
  std::vector<paddle::dialect::IrTensor> vec_dense_weight_list_grad((weight_list.size()), paddle::dialect::IrTensor());
  std::vector<paddle::dialect::IrMetaTensor> vec_meta_weight_list_grad;
  for (size_t i=0; i < static_cast<size_t>(weight_list.size()); i++) {
    vec_meta_weight_list_grad.push_back(paddle::dialect::IrMetaTensor(&vec_dense_weight_list_grad[i]));
  }
  std::vector<phi::MetaTensor*> meta_weight_list_grad;
  for (size_t i=0; i < static_cast<size_t>(vec_meta_weight_list_grad.size()); i++) {
    meta_weight_list_grad.push_back(&vec_meta_weight_list_grad[i]);
  }

  phi::RnnGradInferMeta(meta_x, meta_pre_state, meta_weight_list, &meta_x_grad, meta_pre_state_grad, meta_weight_list_grad);

  std::vector<pir::Type> argument_outputs;
  pir::Type x_grad_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_x_grad.dtype()), dense_x_grad.dims(), dense_x_grad.layout(), dense_x_grad.lod(), dense_x_grad.offset());
  argument_outputs.push_back(x_grad_dense_tensor_type);

  std::vector<pir::Type> pre_state_grad_types;
  for (size_t i=0; i < static_cast<size_t>(pre_state.size()); i++) {
    pre_state_grad_types.push_back(paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(vec_dense_pre_state_grad[i].dtype()), vec_dense_pre_state_grad[i].dims(), vec_dense_pre_state_grad[i].layout(), vec_dense_pre_state_grad[i].lod(), vec_dense_pre_state_grad[i].offset()));
  }
  pir::Type pre_state_grad_vector_type = pir::VectorType::get(pir::IrContext::Instance(), pre_state_grad_types);
  argument_outputs.push_back(pre_state_grad_vector_type);

  std::vector<pir::Type> weight_list_grad_types;
  for (size_t i=0; i < static_cast<size_t>(weight_list.size()); i++) {
    weight_list_grad_types.push_back(paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(vec_dense_weight_list_grad[i].dtype()), vec_dense_weight_list_grad[i].dims(), vec_dense_weight_list_grad[i].layout(), vec_dense_weight_list_grad[i].lod(), vec_dense_weight_list_grad[i].offset()));
  }
  pir::Type weight_list_grad_vector_type = pir::VectorType::get(pir::IrContext::Instance(), weight_list_grad_types);
  argument_outputs.push_back(weight_list_grad_vector_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void RnnGradOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, pir::Value pre_state_, pir::Value weight_list_, pir::Value sequence_length_, pir::Value out_, pir::Value dropout_state_out_, pir::Value reserve_, pir::Value out_grad_, pir::Value state_grad_, pir::AttributeMap attributes) {
  VLOG(4) << "Start build RnnGradOp";


  IR_ENFORCE(
      attributes.find("dropout_prob") != attributes.end(),
          "'dropout_prob' Attribute is expected for RnnGradOp. ");
  float dropout_prob = attributes.at("dropout_prob").dyn_cast<pir::FloatAttribute>().data();

  IR_ENFORCE(
      attributes.find("is_bidirec") != attributes.end(),
          "'is_bidirec' Attribute is expected for RnnGradOp. ");
  bool is_bidirec = attributes.at("is_bidirec").dyn_cast<pir::BoolAttribute>().data();

  IR_ENFORCE(
      attributes.find("input_size") != attributes.end(),
          "'input_size' Attribute is expected for RnnGradOp. ");
  int input_size = attributes.at("input_size").dyn_cast<pir::Int32Attribute>().data();

  IR_ENFORCE(
      attributes.find("hidden_size") != attributes.end(),
          "'hidden_size' Attribute is expected for RnnGradOp. ");
  int hidden_size = attributes.at("hidden_size").dyn_cast<pir::Int32Attribute>().data();

  IR_ENFORCE(
      attributes.find("num_layers") != attributes.end(),
          "'num_layers' Attribute is expected for RnnGradOp. ");
  int num_layers = attributes.at("num_layers").dyn_cast<pir::Int32Attribute>().data();

  IR_ENFORCE(
      attributes.find("mode") != attributes.end(),
          "'mode' Attribute is expected for RnnGradOp. ");
  std::string mode = attributes.at("mode").dyn_cast<pir::StrAttribute>().AsString();

  IR_ENFORCE(
      attributes.find("seed") != attributes.end(),
          "'seed' Attribute is expected for RnnGradOp. ");
  int seed = attributes.at("seed").dyn_cast<pir::Int32Attribute>().data();

  IR_ENFORCE(
      attributes.find("is_test") != attributes.end(),
          "'is_test' Attribute is expected for RnnGradOp. ");
  bool is_test = attributes.at("is_test").dyn_cast<pir::BoolAttribute>().data();


  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_, pre_state_, weight_list_, sequence_length_, out_, dropout_state_out_, reserve_, out_grad_, state_grad_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_dropout_prob = pir::FloatAttribute::get(pir::IrContext::Instance(), dropout_prob);
  argument.AddAttribute("dropout_prob", attr_dropout_prob);
  pir::Attribute attr_is_bidirec = pir::BoolAttribute::get(pir::IrContext::Instance(), is_bidirec);
  argument.AddAttribute("is_bidirec", attr_is_bidirec);
  pir::Attribute attr_input_size = pir::Int32Attribute::get(pir::IrContext::Instance(), input_size);
  argument.AddAttribute("input_size", attr_input_size);
  pir::Attribute attr_hidden_size = pir::Int32Attribute::get(pir::IrContext::Instance(), hidden_size);
  argument.AddAttribute("hidden_size", attr_hidden_size);
  pir::Attribute attr_num_layers = pir::Int32Attribute::get(pir::IrContext::Instance(), num_layers);
  argument.AddAttribute("num_layers", attr_num_layers);
  pir::Attribute attr_mode = pir::StrAttribute::get(pir::IrContext::Instance(), mode);
  argument.AddAttribute("mode", attr_mode);
  pir::Attribute attr_seed = pir::Int32Attribute::get(pir::IrContext::Instance(), seed);
  argument.AddAttribute("seed", attr_seed);
  pir::Attribute attr_is_test = pir::BoolAttribute::get(pir::IrContext::Instance(), is_test);
  argument.AddAttribute("is_test", attr_is_test);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;
  pir::VectorType pre_state = pre_state_.type().dyn_cast<pir::VectorType>(); (void)pre_state;
  pir::VectorType weight_list = weight_list_.type().dyn_cast<pir::VectorType>(); (void)weight_list;
  paddle::dialect::DenseTensorType out = out_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)out;
  paddle::dialect::DenseTensorType dropout_state_out = dropout_state_out_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)dropout_state_out;
  paddle::dialect::DenseTensorType reserve = reserve_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)reserve;
  paddle::dialect::DenseTensorType out_grad = out_grad_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)out_grad;
  pir::VectorType state_grad = state_grad_.type().dyn_cast<pir::VectorType>(); (void)state_grad;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);
  std::vector<paddle::dialect::IrTensor> vec_ir_tensor_pre_state;
  for (size_t i=0; i < static_cast<size_t>(pre_state.size()); i++) {
    vec_ir_tensor_pre_state.push_back(paddle::dialect::IrTensor(paddle::dialect::TransToPhiDataType(pre_state[i].dyn_cast<paddle::dialect::DenseTensorType>().dtype()),
                                                                     pre_state[i].dyn_cast<paddle::dialect::DenseTensorType>().dims(),
                                                                     pre_state[i].dyn_cast<paddle::dialect::DenseTensorType>().data_layout(),
                                                                     pre_state[i].dyn_cast<paddle::dialect::DenseTensorType>().lod(),
                                                                     pre_state[i].dyn_cast<paddle::dialect::DenseTensorType>().offset()));
  }
  std::vector<paddle::dialect::IrMetaTensor> vec_meta_pre_state;
  for (size_t i=0; i < vec_ir_tensor_pre_state.size(); i++) {
    vec_meta_pre_state.push_back(paddle::dialect::IrMetaTensor(&vec_ir_tensor_pre_state[i]));
  }

  std::vector<const phi::MetaTensor*> meta_pre_state;
  for (size_t i=0; i < static_cast<size_t>(vec_meta_pre_state.size()); i++) {
    meta_pre_state.push_back(&vec_meta_pre_state[i]);
  }
   std::vector<paddle::dialect::IrTensor> vec_ir_tensor_weight_list;
  for (size_t i=0; i < static_cast<size_t>(weight_list.size()); i++) {
    vec_ir_tensor_weight_list.push_back(paddle::dialect::IrTensor(paddle::dialect::TransToPhiDataType(weight_list[i].dyn_cast<paddle::dialect::DenseTensorType>().dtype()),
                                                                     weight_list[i].dyn_cast<paddle::dialect::DenseTensorType>().dims(),
                                                                     weight_list[i].dyn_cast<paddle::dialect::DenseTensorType>().data_layout(),
                                                                     weight_list[i].dyn_cast<paddle::dialect::DenseTensorType>().lod(),
                                                                     weight_list[i].dyn_cast<paddle::dialect::DenseTensorType>().offset()));
  }
  std::vector<paddle::dialect::IrMetaTensor> vec_meta_weight_list;
  for (size_t i=0; i < vec_ir_tensor_weight_list.size(); i++) {
    vec_meta_weight_list.push_back(paddle::dialect::IrMetaTensor(&vec_ir_tensor_weight_list[i]));
  }

  std::vector<const phi::MetaTensor*> meta_weight_list;
  for (size_t i=0; i < static_cast<size_t>(vec_meta_weight_list.size()); i++) {
    meta_weight_list.push_back(&vec_meta_weight_list[i]);
  }
   paddle::dialect::IrTensor dense_x_grad;
  paddle::dialect::IrMetaTensor meta_x_grad(&dense_x_grad);
  std::vector<paddle::dialect::IrTensor> vec_dense_pre_state_grad((pre_state.size()), paddle::dialect::IrTensor());
  std::vector<paddle::dialect::IrMetaTensor> vec_meta_pre_state_grad;
  for (size_t i=0; i < static_cast<size_t>(pre_state.size()); i++) {
    vec_meta_pre_state_grad.push_back(paddle::dialect::IrMetaTensor(&vec_dense_pre_state_grad[i]));
  }
  std::vector<phi::MetaTensor*> meta_pre_state_grad;
  for (size_t i=0; i < static_cast<size_t>(vec_meta_pre_state_grad.size()); i++) {
    meta_pre_state_grad.push_back(&vec_meta_pre_state_grad[i]);
  }
  std::vector<paddle::dialect::IrTensor> vec_dense_weight_list_grad((weight_list.size()), paddle::dialect::IrTensor());
  std::vector<paddle::dialect::IrMetaTensor> vec_meta_weight_list_grad;
  for (size_t i=0; i < static_cast<size_t>(weight_list.size()); i++) {
    vec_meta_weight_list_grad.push_back(paddle::dialect::IrMetaTensor(&vec_dense_weight_list_grad[i]));
  }
  std::vector<phi::MetaTensor*> meta_weight_list_grad;
  for (size_t i=0; i < static_cast<size_t>(vec_meta_weight_list_grad.size()); i++) {
    meta_weight_list_grad.push_back(&vec_meta_weight_list_grad[i]);
  }

  phi::RnnGradInferMeta(meta_x, meta_pre_state, meta_weight_list, &meta_x_grad, meta_pre_state_grad, meta_weight_list_grad);

  std::vector<pir::Type> argument_outputs;
  pir::Type x_grad_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_x_grad.dtype()), dense_x_grad.dims(), dense_x_grad.layout(), dense_x_grad.lod(), dense_x_grad.offset());
  argument_outputs.push_back(x_grad_dense_tensor_type);

  std::vector<pir::Type> pre_state_grad_types;
  for (size_t i=0; i < static_cast<size_t>(pre_state.size()); i++) {
    pre_state_grad_types.push_back(paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(vec_dense_pre_state_grad[i].dtype()), vec_dense_pre_state_grad[i].dims(), vec_dense_pre_state_grad[i].layout(), vec_dense_pre_state_grad[i].lod(), vec_dense_pre_state_grad[i].offset()));
  }
  pir::Type pre_state_grad_vector_type = pir::VectorType::get(pir::IrContext::Instance(), pre_state_grad_types);
  argument_outputs.push_back(pre_state_grad_vector_type);

  std::vector<pir::Type> weight_list_grad_types;
  for (size_t i=0; i < static_cast<size_t>(weight_list.size()); i++) {
    weight_list_grad_types.push_back(paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(vec_dense_weight_list_grad[i].dtype()), vec_dense_weight_list_grad[i].dims(), vec_dense_weight_list_grad[i].layout(), vec_dense_weight_list_grad[i].lod(), vec_dense_weight_list_grad[i].offset()));
  }
  pir::Type weight_list_grad_vector_type = pir::VectorType::get(pir::IrContext::Instance(), weight_list_grad_types);
  argument_outputs.push_back(weight_list_grad_vector_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void RnnGradOp::VerifySig() {}

void RnnGradOp::InferMeta( phi::InferMetaContext *infer_meta ) {
  auto fn = PD_INFER_META(phi::RnnGradInferMeta);
  fn(infer_meta);
}

phi::DataType RnnGradOp::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: RnnGradOp";
  


  return expected_kernel_dtype;
}

OpInfoTuple RowConvGradOp::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("x", "paddle::dialect::DenseTensorType", false, false, false, false), paddle::dialect::OpInputInfo("filter", "paddle::dialect::DenseTensorType", false, false, false, false), paddle::dialect::OpInputInfo("out_grad", "paddle::dialect::DenseTensorType", false, false, false, false) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = {  };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("x_grad", "paddle::dialect::DenseTensorType", false, false), paddle::dialect::OpOutputInfo("filter_grad", "paddle::dialect::DenseTensorType", false, false) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("RowConvGradInferMeta", {"out_grad", "filter"}, "row_conv_grad", {"x", "filter", "out_grad"}, {}, {}, {}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "row_conv_grad");
}

void RowConvGradOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, pir::Value filter_, pir::Value out_grad_) {
  VLOG(4) << "Start build RowConvGradOp";



  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_, filter_, out_grad_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;
  paddle::dialect::DenseTensorType filter = filter_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)filter;
  paddle::dialect::DenseTensorType out_grad = out_grad_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)out_grad;

  VLOG(4) << "Builder construction  dense_out_grad";
  paddle::dialect::IrTensor ir_tensor_out_grad(paddle::dialect::TransToPhiDataType(out_grad.dtype()),
                                                      out_grad.dims(),
                                                      out_grad.data_layout(),
                                                      out_grad.lod(),
                                                      out_grad.offset());
  VLOG(4) << "Builder construction  meta_out_grad";
  paddle::dialect::IrMetaTensor meta_out_grad(&ir_tensor_out_grad);

  VLOG(4) << "Builder construction  dense_filter";
  paddle::dialect::IrTensor ir_tensor_filter(paddle::dialect::TransToPhiDataType(filter.dtype()),
                                                      filter.dims(),
                                                      filter.data_layout(),
                                                      filter.lod(),
                                                      filter.offset());
  VLOG(4) << "Builder construction  meta_filter";
  paddle::dialect::IrMetaTensor meta_filter(&ir_tensor_filter);
  paddle::dialect::IrTensor dense_x_grad;
  paddle::dialect::IrMetaTensor meta_x_grad(&dense_x_grad);
  paddle::dialect::IrTensor dense_filter_grad;
  paddle::dialect::IrMetaTensor meta_filter_grad(&dense_filter_grad);

  phi::RowConvGradInferMeta(meta_out_grad, meta_filter, &meta_x_grad, &meta_filter_grad);

  std::vector<pir::Type> argument_outputs;
  pir::Type x_grad_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_x_grad.dtype()), dense_x_grad.dims(), dense_x_grad.layout(), dense_x_grad.lod(), dense_x_grad.offset());
  argument_outputs.push_back(x_grad_dense_tensor_type);

  pir::Type filter_grad_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_filter_grad.dtype()), dense_filter_grad.dims(), dense_filter_grad.layout(), dense_filter_grad.lod(), dense_filter_grad.offset());
  argument_outputs.push_back(filter_grad_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void RowConvGradOp::VerifySig() {}

void RowConvGradOp::InferMeta( phi::InferMetaContext *infer_meta ) {
  auto fn = PD_INFER_META(phi::RowConvGradInferMeta);
  fn(infer_meta);
}

phi::DataType RowConvGradOp::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: RowConvGradOp";
  


  return expected_kernel_dtype;
}

OpInfoTuple RreluGradOp::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("x", "paddle::dialect::DenseTensorType", false, false, false, false), paddle::dialect::OpInputInfo("noise", "paddle::dialect::DenseTensorType", false, false, false, false), paddle::dialect::OpInputInfo("out_grad", "paddle::dialect::DenseTensorType", false, false, false, false) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = {  };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("x_grad", "paddle::dialect::DenseTensorType", false, false) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("RReluGradInferMeta", {"out_grad", "noise"}, "rrelu_grad", {"x", "noise", "out_grad"}, {"x"}, {}, {}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "rrelu_grad");
}

void RreluGradOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, pir::Value noise_, pir::Value out_grad_) {
  VLOG(4) << "Start build RreluGradOp";



  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_, noise_, out_grad_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;
  paddle::dialect::DenseTensorType noise = noise_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)noise;
  paddle::dialect::DenseTensorType out_grad = out_grad_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)out_grad;

  VLOG(4) << "Builder construction  dense_out_grad";
  paddle::dialect::IrTensor ir_tensor_out_grad(paddle::dialect::TransToPhiDataType(out_grad.dtype()),
                                                      out_grad.dims(),
                                                      out_grad.data_layout(),
                                                      out_grad.lod(),
                                                      out_grad.offset());
  VLOG(4) << "Builder construction  meta_out_grad";
  paddle::dialect::IrMetaTensor meta_out_grad(&ir_tensor_out_grad);

  VLOG(4) << "Builder construction  dense_noise";
  paddle::dialect::IrTensor ir_tensor_noise(paddle::dialect::TransToPhiDataType(noise.dtype()),
                                                      noise.dims(),
                                                      noise.data_layout(),
                                                      noise.lod(),
                                                      noise.offset());
  VLOG(4) << "Builder construction  meta_noise";
  paddle::dialect::IrMetaTensor meta_noise(&ir_tensor_noise);
  paddle::dialect::IrTensor dense_x_grad;
  paddle::dialect::IrMetaTensor meta_x_grad(&dense_x_grad);

  phi::RReluGradInferMeta(meta_out_grad, meta_noise, &meta_x_grad);

  std::vector<pir::Type> argument_outputs;
  pir::Type x_grad_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_x_grad.dtype()), dense_x_grad.dims(), dense_x_grad.layout(), dense_x_grad.lod(), dense_x_grad.offset());
  argument_outputs.push_back(x_grad_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void RreluGradOp::VerifySig() {}

void RreluGradOp::InferMeta( phi::InferMetaContext *infer_meta ) {
  auto fn = PD_INFER_META(phi::RReluGradInferMeta);
  fn(infer_meta);
}

phi::DataType RreluGradOp::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: RreluGradOp";
  


  return expected_kernel_dtype;
}

OpInfoTuple SetValueGradOp::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("out_grad", "paddle::dialect::DenseTensorType", false, false, false, false) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = {  };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("x_grad", "paddle::dialect::DenseTensorType", false, false) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("UnchangedInferMeta", {"out_grad"}, "assign", {"out_grad"}, {}, {}, {}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "set_value_grad");
}

void SetValueGradOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value out_grad_) {
  VLOG(4) << "Start build SetValueGradOp";



  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {out_grad_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType out_grad = out_grad_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)out_grad;

  VLOG(4) << "Builder construction  dense_out_grad";
  paddle::dialect::IrTensor ir_tensor_out_grad(paddle::dialect::TransToPhiDataType(out_grad.dtype()),
                                                      out_grad.dims(),
                                                      out_grad.data_layout(),
                                                      out_grad.lod(),
                                                      out_grad.offset());
  VLOG(4) << "Builder construction  meta_out_grad";
  paddle::dialect::IrMetaTensor meta_out_grad(&ir_tensor_out_grad);
  paddle::dialect::IrTensor dense_x_grad;
  paddle::dialect::IrMetaTensor meta_x_grad(&dense_x_grad);

  phi::UnchangedInferMeta(meta_out_grad, &meta_x_grad);

  std::vector<pir::Type> argument_outputs;
  pir::Type x_grad_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_x_grad.dtype()), dense_x_grad.dims(), dense_x_grad.layout(), dense_x_grad.lod(), dense_x_grad.offset());
  argument_outputs.push_back(x_grad_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void SetValueGradOp::VerifySig() {}

void SetValueGradOp::InferMeta( phi::InferMetaContext *infer_meta ) {
  auto fn = PD_INFER_META(phi::UnchangedInferMeta);
  fn(infer_meta);
}

phi::DataType SetValueGradOp::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: SetValueGradOp";
  


  return expected_kernel_dtype;
}

const char *SetValueWithTensorGradOp::attributes_name[3] = { "axes", "decrease_axes", "none_axes" };

OpInfoTuple SetValueWithTensorGradOp::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("values", "paddle::dialect::DenseTensorType", false, false, false, false), paddle::dialect::OpInputInfo("out_grad", "paddle::dialect::DenseTensorType", false, false, false, false), paddle::dialect::OpInputInfo("starts", "paddle::dialect::IntArrayAttribute", false, false, true, false), paddle::dialect::OpInputInfo("ends", "paddle::dialect::IntArrayAttribute", false, false, true, false), paddle::dialect::OpInputInfo("steps", "paddle::dialect::IntArrayAttribute", false, false, true, false) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = { paddle::dialect::OpAttributeInfo("axes", "pir::ArrayAttribute<pir::Int64Attribute>", ""), paddle::dialect::OpAttributeInfo("decrease_axes", "pir::ArrayAttribute<pir::Int64Attribute>", ""), paddle::dialect::OpAttributeInfo("none_axes", "pir::ArrayAttribute<pir::Int64Attribute>", "") };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("x_grad", "paddle::dialect::DenseTensorType", false, false), paddle::dialect::OpOutputInfo("values_grad", "paddle::dialect::DenseTensorType", false, false) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("SetValueGradInferMeta", {"out_grad", "values"}, "set_value_grad", {"out_grad", "starts", "ends", "steps", "axes", "decrease_axes", "none_axes"}, {}, {}, {}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "set_value_with_tensor_grad");
}

void SetValueWithTensorGradOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value values_, pir::Value out_grad_, const std::vector<int64_t>& starts, const std::vector<int64_t>& ends, const std::vector<int64_t>& steps, const std::vector<int64_t>& axes, const std::vector<int64_t>& decrease_axes, const std::vector<int64_t>& none_axes) {
  VLOG(4) << "Start build SetValueWithTensorGradOp";


  // Generate int_array mutable attribute: starts
  paddle::dialect::FullIntArrayOp full_starts_op = builder.Build<paddle::dialect::FullIntArrayOp>(starts, phi::DataType::INT64, phi::CPUPlace());
  pir::OpResult starts_ = full_starts_op->result(0);
      // Generate int_array mutable attribute: ends
  paddle::dialect::FullIntArrayOp full_ends_op = builder.Build<paddle::dialect::FullIntArrayOp>(ends, phi::DataType::INT64, phi::CPUPlace());
  pir::OpResult ends_ = full_ends_op->result(0);
      // Generate int_array mutable attribute: steps
  paddle::dialect::FullIntArrayOp full_steps_op = builder.Build<paddle::dialect::FullIntArrayOp>(steps, phi::DataType::INT64, phi::CPUPlace());
  pir::OpResult steps_ = full_steps_op->result(0);
    
  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {values_, out_grad_, starts_, ends_, steps_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  std::vector<pir::Attribute> vec_axes;
  for (size_t i = 0; i < static_cast<size_t>(axes.size()); i++) {
      pir::Attribute attr_axes = pir::Int64Attribute::get(pir::IrContext::Instance(), axes[i]);

    vec_axes.push_back(attr_axes);
  }
  pir::Attribute attr_axes = pir::ArrayAttribute::get(pir::IrContext::Instance(), vec_axes);
  argument.AddAttribute("axes", attr_axes);
  std::vector<pir::Attribute> vec_decrease_axes;
  for (size_t i = 0; i < static_cast<size_t>(decrease_axes.size()); i++) {
      pir::Attribute attr_decrease_axes = pir::Int64Attribute::get(pir::IrContext::Instance(), decrease_axes[i]);

    vec_decrease_axes.push_back(attr_decrease_axes);
  }
  pir::Attribute attr_decrease_axes = pir::ArrayAttribute::get(pir::IrContext::Instance(), vec_decrease_axes);
  argument.AddAttribute("decrease_axes", attr_decrease_axes);
  std::vector<pir::Attribute> vec_none_axes;
  for (size_t i = 0; i < static_cast<size_t>(none_axes.size()); i++) {
      pir::Attribute attr_none_axes = pir::Int64Attribute::get(pir::IrContext::Instance(), none_axes[i]);

    vec_none_axes.push_back(attr_none_axes);
  }
  pir::Attribute attr_none_axes = pir::ArrayAttribute::get(pir::IrContext::Instance(), vec_none_axes);
  argument.AddAttribute("none_axes", attr_none_axes);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType values = values_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)values;
  paddle::dialect::DenseTensorType out_grad = out_grad_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)out_grad;

  VLOG(4) << "Builder construction  dense_out_grad";
  paddle::dialect::IrTensor ir_tensor_out_grad(paddle::dialect::TransToPhiDataType(out_grad.dtype()),
                                                      out_grad.dims(),
                                                      out_grad.data_layout(),
                                                      out_grad.lod(),
                                                      out_grad.offset());
  VLOG(4) << "Builder construction  meta_out_grad";
  paddle::dialect::IrMetaTensor meta_out_grad(&ir_tensor_out_grad);

  VLOG(4) << "Builder construction  dense_values";
  paddle::dialect::IrTensor ir_tensor_values(paddle::dialect::TransToPhiDataType(values.dtype()),
                                                      values.dims(),
                                                      values.data_layout(),
                                                      values.lod(),
                                                      values.offset());
  VLOG(4) << "Builder construction  meta_values";
  paddle::dialect::IrMetaTensor meta_values(&ir_tensor_values);
  paddle::dialect::IrTensor dense_x_grad;
  paddle::dialect::IrMetaTensor meta_x_grad(&dense_x_grad);
  paddle::dialect::IrTensor dense_values_grad;
  paddle::dialect::IrMetaTensor meta_values_grad(&dense_values_grad);

  phi::SetValueGradInferMeta(meta_out_grad, meta_values, &meta_x_grad, &meta_values_grad);

  std::vector<pir::Type> argument_outputs;
  pir::Type x_grad_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_x_grad.dtype()), dense_x_grad.dims(), dense_x_grad.layout(), dense_x_grad.lod(), dense_x_grad.offset());
  argument_outputs.push_back(x_grad_dense_tensor_type);

  pir::Type values_grad_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_values_grad.dtype()), dense_values_grad.dims(), dense_values_grad.layout(), dense_values_grad.lod(), dense_values_grad.offset());
  argument_outputs.push_back(values_grad_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void SetValueWithTensorGradOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value values_, pir::Value out_grad_, pir::AttributeMap attributes) {
  VLOG(4) << "Start build SetValueWithTensorGradOp";


  IR_ENFORCE(
      attributes.find("starts") != attributes.end(),
          "'starts' Attribute is expected for SetValueWithTensorGradOp. ");
  std::vector<int64_t> starts = attributes.at("starts").dyn_cast<paddle::dialect::IntArrayAttribute>().data().GetData();

  IR_ENFORCE(
      attributes.find("ends") != attributes.end(),
          "'ends' Attribute is expected for SetValueWithTensorGradOp. ");
  std::vector<int64_t> ends = attributes.at("ends").dyn_cast<paddle::dialect::IntArrayAttribute>().data().GetData();

  IR_ENFORCE(
      attributes.find("steps") != attributes.end(),
          "'steps' Attribute is expected for SetValueWithTensorGradOp. ");
  std::vector<int64_t> steps = attributes.at("steps").dyn_cast<paddle::dialect::IntArrayAttribute>().data().GetData();

  IR_ENFORCE(
      attributes.find("axes") != attributes.end(),
          "'axes' Attribute is expected for SetValueWithTensorGradOp. ");
  std::vector<int64_t> axes;
  for (size_t i = 0; i < attributes.at("axes").dyn_cast<pir::ArrayAttribute>().size(); i++) {
    axes.push_back(attributes.at("axes").dyn_cast<pir::ArrayAttribute>().at(i).dyn_cast<pir::Int64Attribute>().data());
  }

  IR_ENFORCE(
      attributes.find("decrease_axes") != attributes.end(),
          "'decrease_axes' Attribute is expected for SetValueWithTensorGradOp. ");
  std::vector<int64_t> decrease_axes;
  for (size_t i = 0; i < attributes.at("decrease_axes").dyn_cast<pir::ArrayAttribute>().size(); i++) {
    decrease_axes.push_back(attributes.at("decrease_axes").dyn_cast<pir::ArrayAttribute>().at(i).dyn_cast<pir::Int64Attribute>().data());
  }

  IR_ENFORCE(
      attributes.find("none_axes") != attributes.end(),
          "'none_axes' Attribute is expected for SetValueWithTensorGradOp. ");
  std::vector<int64_t> none_axes;
  for (size_t i = 0; i < attributes.at("none_axes").dyn_cast<pir::ArrayAttribute>().size(); i++) {
    none_axes.push_back(attributes.at("none_axes").dyn_cast<pir::ArrayAttribute>().at(i).dyn_cast<pir::Int64Attribute>().data());
  }

  // Generate int_array mutable attribute: starts
  paddle::dialect::FullIntArrayOp full_starts_op = builder.Build<paddle::dialect::FullIntArrayOp>(starts, phi::DataType::INT64, phi::CPUPlace());
  pir::OpResult starts_ = full_starts_op->result(0);
      // Generate int_array mutable attribute: ends
  paddle::dialect::FullIntArrayOp full_ends_op = builder.Build<paddle::dialect::FullIntArrayOp>(ends, phi::DataType::INT64, phi::CPUPlace());
  pir::OpResult ends_ = full_ends_op->result(0);
      // Generate int_array mutable attribute: steps
  paddle::dialect::FullIntArrayOp full_steps_op = builder.Build<paddle::dialect::FullIntArrayOp>(steps, phi::DataType::INT64, phi::CPUPlace());
  pir::OpResult steps_ = full_steps_op->result(0);
    
  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {values_, out_grad_, starts_, ends_, steps_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  std::vector<pir::Attribute> vec_axes;
  for (size_t i = 0; i < static_cast<size_t>(axes.size()); i++) {
      pir::Attribute attr_axes = pir::Int64Attribute::get(pir::IrContext::Instance(), axes[i]);

    vec_axes.push_back(attr_axes);
  }
  pir::Attribute attr_axes = pir::ArrayAttribute::get(pir::IrContext::Instance(), vec_axes);
  argument.AddAttribute("axes", attr_axes);
  std::vector<pir::Attribute> vec_decrease_axes;
  for (size_t i = 0; i < static_cast<size_t>(decrease_axes.size()); i++) {
      pir::Attribute attr_decrease_axes = pir::Int64Attribute::get(pir::IrContext::Instance(), decrease_axes[i]);

    vec_decrease_axes.push_back(attr_decrease_axes);
  }
  pir::Attribute attr_decrease_axes = pir::ArrayAttribute::get(pir::IrContext::Instance(), vec_decrease_axes);
  argument.AddAttribute("decrease_axes", attr_decrease_axes);
  std::vector<pir::Attribute> vec_none_axes;
  for (size_t i = 0; i < static_cast<size_t>(none_axes.size()); i++) {
      pir::Attribute attr_none_axes = pir::Int64Attribute::get(pir::IrContext::Instance(), none_axes[i]);

    vec_none_axes.push_back(attr_none_axes);
  }
  pir::Attribute attr_none_axes = pir::ArrayAttribute::get(pir::IrContext::Instance(), vec_none_axes);
  argument.AddAttribute("none_axes", attr_none_axes);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType values = values_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)values;
  paddle::dialect::DenseTensorType out_grad = out_grad_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)out_grad;

  VLOG(4) << "Builder construction  dense_out_grad";
  paddle::dialect::IrTensor ir_tensor_out_grad(paddle::dialect::TransToPhiDataType(out_grad.dtype()),
                                                      out_grad.dims(),
                                                      out_grad.data_layout(),
                                                      out_grad.lod(),
                                                      out_grad.offset());
  VLOG(4) << "Builder construction  meta_out_grad";
  paddle::dialect::IrMetaTensor meta_out_grad(&ir_tensor_out_grad);

  VLOG(4) << "Builder construction  dense_values";
  paddle::dialect::IrTensor ir_tensor_values(paddle::dialect::TransToPhiDataType(values.dtype()),
                                                      values.dims(),
                                                      values.data_layout(),
                                                      values.lod(),
                                                      values.offset());
  VLOG(4) << "Builder construction  meta_values";
  paddle::dialect::IrMetaTensor meta_values(&ir_tensor_values);
  paddle::dialect::IrTensor dense_x_grad;
  paddle::dialect::IrMetaTensor meta_x_grad(&dense_x_grad);
  paddle::dialect::IrTensor dense_values_grad;
  paddle::dialect::IrMetaTensor meta_values_grad(&dense_values_grad);

  phi::SetValueGradInferMeta(meta_out_grad, meta_values, &meta_x_grad, &meta_values_grad);

  std::vector<pir::Type> argument_outputs;
  pir::Type x_grad_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_x_grad.dtype()), dense_x_grad.dims(), dense_x_grad.layout(), dense_x_grad.lod(), dense_x_grad.offset());
  argument_outputs.push_back(x_grad_dense_tensor_type);

  pir::Type values_grad_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_values_grad.dtype()), dense_values_grad.dims(), dense_values_grad.layout(), dense_values_grad.lod(), dense_values_grad.offset());
  argument_outputs.push_back(values_grad_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void SetValueWithTensorGradOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value values_, pir::Value out_grad_, pir::Value starts_, pir::Value ends_, pir::Value steps_, const std::vector<int64_t>& axes, const std::vector<int64_t>& decrease_axes, const std::vector<int64_t>& none_axes) {
  VLOG(4) << "Start build SetValueWithTensorGradOp";



  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {values_, out_grad_, starts_, ends_, steps_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  std::vector<pir::Attribute> vec_axes;
  for (size_t i = 0; i < static_cast<size_t>(axes.size()); i++) {
      pir::Attribute attr_axes = pir::Int64Attribute::get(pir::IrContext::Instance(), axes[i]);

    vec_axes.push_back(attr_axes);
  }
  pir::Attribute attr_axes = pir::ArrayAttribute::get(pir::IrContext::Instance(), vec_axes);
  argument.AddAttribute("axes", attr_axes);
  std::vector<pir::Attribute> vec_decrease_axes;
  for (size_t i = 0; i < static_cast<size_t>(decrease_axes.size()); i++) {
      pir::Attribute attr_decrease_axes = pir::Int64Attribute::get(pir::IrContext::Instance(), decrease_axes[i]);

    vec_decrease_axes.push_back(attr_decrease_axes);
  }
  pir::Attribute attr_decrease_axes = pir::ArrayAttribute::get(pir::IrContext::Instance(), vec_decrease_axes);
  argument.AddAttribute("decrease_axes", attr_decrease_axes);
  std::vector<pir::Attribute> vec_none_axes;
  for (size_t i = 0; i < static_cast<size_t>(none_axes.size()); i++) {
      pir::Attribute attr_none_axes = pir::Int64Attribute::get(pir::IrContext::Instance(), none_axes[i]);

    vec_none_axes.push_back(attr_none_axes);
  }
  pir::Attribute attr_none_axes = pir::ArrayAttribute::get(pir::IrContext::Instance(), vec_none_axes);
  argument.AddAttribute("none_axes", attr_none_axes);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType values = values_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)values;
  paddle::dialect::DenseTensorType out_grad = out_grad_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)out_grad;
  phi::IntArray starts;
  if (starts_.dyn_cast<pir::OpResult>() && starts_.dyn_cast<pir::OpResult>().owner()->isa<paddle::dialect::FullIntArrayOp>()) {
    starts = std::move(phi::IntArray(paddle::dialect::GetInt64Vector(
                          starts_.dyn_cast<pir::OpResult>().owner()
                          ->dyn_cast<paddle::dialect::FullIntArrayOp>()
                          .attribute("value"))));
  } else if (starts_.type().isa<pir::VectorType>()) {
    size_t starts_size = starts_.type().dyn_cast<pir::VectorType>().size();
    starts = std::move(phi::IntArray(std::vector<int64_t>(starts_size, -1)));
    starts.SetFromTensor(true);
  } else if (starts_.type().isa<paddle::dialect::DenseTensorType>()) {
    common::DDim starts_dim = starts_.type().dyn_cast<paddle::dialect::DenseTensorType>().dims();
    size_t starts_size = common::product(starts_dim);
    if (common::contain_unknown_dim(starts_dim)) {
      starts_size = 1;
    }
    starts = std::move(phi::IntArray(std::vector<int64_t>(starts_size, -1)));
    starts.SetFromTensor(true);
  } else {
    PADDLE_THROW(phi::errors::Unimplemented("Only support VectorType or DenseTensorType"));
  }
  phi::IntArray ends;
  if (ends_.dyn_cast<pir::OpResult>() && ends_.dyn_cast<pir::OpResult>().owner()->isa<paddle::dialect::FullIntArrayOp>()) {
    ends = std::move(phi::IntArray(paddle::dialect::GetInt64Vector(
                          ends_.dyn_cast<pir::OpResult>().owner()
                          ->dyn_cast<paddle::dialect::FullIntArrayOp>()
                          .attribute("value"))));
  } else if (ends_.type().isa<pir::VectorType>()) {
    size_t ends_size = ends_.type().dyn_cast<pir::VectorType>().size();
    ends = std::move(phi::IntArray(std::vector<int64_t>(ends_size, -1)));
    ends.SetFromTensor(true);
  } else if (ends_.type().isa<paddle::dialect::DenseTensorType>()) {
    common::DDim ends_dim = ends_.type().dyn_cast<paddle::dialect::DenseTensorType>().dims();
    size_t ends_size = common::product(ends_dim);
    if (common::contain_unknown_dim(ends_dim)) {
      ends_size = 1;
    }
    ends = std::move(phi::IntArray(std::vector<int64_t>(ends_size, -1)));
    ends.SetFromTensor(true);
  } else {
    PADDLE_THROW(phi::errors::Unimplemented("Only support VectorType or DenseTensorType"));
  }
  phi::IntArray steps;
  if (steps_.dyn_cast<pir::OpResult>() && steps_.dyn_cast<pir::OpResult>().owner()->isa<paddle::dialect::FullIntArrayOp>()) {
    steps = std::move(phi::IntArray(paddle::dialect::GetInt64Vector(
                          steps_.dyn_cast<pir::OpResult>().owner()
                          ->dyn_cast<paddle::dialect::FullIntArrayOp>()
                          .attribute("value"))));
  } else if (steps_.type().isa<pir::VectorType>()) {
    size_t steps_size = steps_.type().dyn_cast<pir::VectorType>().size();
    steps = std::move(phi::IntArray(std::vector<int64_t>(steps_size, -1)));
    steps.SetFromTensor(true);
  } else if (steps_.type().isa<paddle::dialect::DenseTensorType>()) {
    common::DDim steps_dim = steps_.type().dyn_cast<paddle::dialect::DenseTensorType>().dims();
    size_t steps_size = common::product(steps_dim);
    if (common::contain_unknown_dim(steps_dim)) {
      steps_size = 1;
    }
    steps = std::move(phi::IntArray(std::vector<int64_t>(steps_size, -1)));
    steps.SetFromTensor(true);
  } else {
    PADDLE_THROW(phi::errors::Unimplemented("Only support VectorType or DenseTensorType"));
  }


  VLOG(4) << "Builder construction  dense_out_grad";
  paddle::dialect::IrTensor ir_tensor_out_grad(paddle::dialect::TransToPhiDataType(out_grad.dtype()),
                                                      out_grad.dims(),
                                                      out_grad.data_layout(),
                                                      out_grad.lod(),
                                                      out_grad.offset());
  VLOG(4) << "Builder construction  meta_out_grad";
  paddle::dialect::IrMetaTensor meta_out_grad(&ir_tensor_out_grad);

  VLOG(4) << "Builder construction  dense_values";
  paddle::dialect::IrTensor ir_tensor_values(paddle::dialect::TransToPhiDataType(values.dtype()),
                                                      values.dims(),
                                                      values.data_layout(),
                                                      values.lod(),
                                                      values.offset());
  VLOG(4) << "Builder construction  meta_values";
  paddle::dialect::IrMetaTensor meta_values(&ir_tensor_values);
  paddle::dialect::IrTensor dense_x_grad;
  paddle::dialect::IrMetaTensor meta_x_grad(&dense_x_grad);
  paddle::dialect::IrTensor dense_values_grad;
  paddle::dialect::IrMetaTensor meta_values_grad(&dense_values_grad);

  phi::SetValueGradInferMeta(meta_out_grad, meta_values, &meta_x_grad, &meta_values_grad);

  std::vector<pir::Type> argument_outputs;
  pir::Type x_grad_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_x_grad.dtype()), dense_x_grad.dims(), dense_x_grad.layout(), dense_x_grad.lod(), dense_x_grad.offset());
  argument_outputs.push_back(x_grad_dense_tensor_type);

  pir::Type values_grad_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_values_grad.dtype()), dense_values_grad.dims(), dense_values_grad.layout(), dense_values_grad.lod(), dense_values_grad.offset());
  argument_outputs.push_back(values_grad_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void SetValueWithTensorGradOp::VerifySig() {}

void SetValueWithTensorGradOp::InferMeta( phi::InferMetaContext *infer_meta ) {
  auto fn = PD_INFER_META(phi::SetValueGradInferMeta);
  fn(infer_meta);
}

phi::DataType SetValueWithTensorGradOp::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: SetValueWithTensorGradOp";
  


  return expected_kernel_dtype;
}

const char *SliceDoubleGradOp::attributes_name[3] = { "axes", "infer_flags", "decrease_axis" };

OpInfoTuple SliceDoubleGradOp::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("grad_input_grad", "paddle::dialect::DenseTensorType", false, false, false, false), paddle::dialect::OpInputInfo("starts", "paddle::dialect::IntArrayAttribute", false, false, true, false), paddle::dialect::OpInputInfo("ends", "paddle::dialect::IntArrayAttribute", false, false, true, false) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = { paddle::dialect::OpAttributeInfo("axes", "pir::ArrayAttribute<pir::Int64Attribute>", ""), paddle::dialect::OpAttributeInfo("infer_flags", "pir::ArrayAttribute<pir::Int64Attribute>", ""), paddle::dialect::OpAttributeInfo("decrease_axis", "pir::ArrayAttribute<pir::Int64Attribute>", "") };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("grad_out_grad", "paddle::dialect::DenseTensorType", false, false) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("", {""}, "", {""}, {}, {}, {}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "slice_double_grad");
}

void SliceDoubleGradOp::VerifySig() {}

phi::DataType SliceDoubleGradOp::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: SliceDoubleGradOp";
  


  return expected_kernel_dtype;
}

const char *SliceGradOp::attributes_name[3] = { "axes", "infer_flags", "decrease_axis" };

OpInfoTuple SliceGradOp::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("input", "paddle::dialect::DenseTensorType", false, true, false, false), paddle::dialect::OpInputInfo("out_grad", "paddle::dialect::DenseTensorType", false, false, false, true), paddle::dialect::OpInputInfo("starts", "paddle::dialect::IntArrayAttribute", false, false, true, false), paddle::dialect::OpInputInfo("ends", "paddle::dialect::IntArrayAttribute", false, false, true, false) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = { paddle::dialect::OpAttributeInfo("axes", "pir::ArrayAttribute<pir::Int64Attribute>", ""), paddle::dialect::OpAttributeInfo("infer_flags", "pir::ArrayAttribute<pir::Int64Attribute>", ""), paddle::dialect::OpAttributeInfo("decrease_axis", "pir::ArrayAttribute<pir::Int64Attribute>", "") };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("input_grad", "paddle::dialect::DenseTensorType", false, false) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("UnchangedInferMeta", {"input"}, "slice_grad", {"input", "out_grad", "axes", "starts", "ends", "infer_flags", "decrease_axis"}, {}, {}, {}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "slice_grad");
}

void SliceGradOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value input_, pir::Value out_grad_, const std::vector<int64_t>& axes, const std::vector<int64_t>& starts, const std::vector<int64_t>& ends, const std::vector<int64_t>& infer_flags, const std::vector<int64_t>& decrease_axis) {
  VLOG(4) << "Start build SliceGradOp";


  // Generate int_array mutable attribute: starts
  paddle::dialect::FullIntArrayOp full_starts_op = builder.Build<paddle::dialect::FullIntArrayOp>(starts, phi::DataType::INT64, phi::CPUPlace());
  pir::OpResult starts_ = full_starts_op->result(0);
      // Generate int_array mutable attribute: ends
  paddle::dialect::FullIntArrayOp full_ends_op = builder.Build<paddle::dialect::FullIntArrayOp>(ends, phi::DataType::INT64, phi::CPUPlace());
  pir::OpResult ends_ = full_ends_op->result(0);
    
  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {input_, out_grad_, starts_, ends_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  std::vector<pir::Attribute> vec_axes;
  for (size_t i = 0; i < static_cast<size_t>(axes.size()); i++) {
      pir::Attribute attr_axes = pir::Int64Attribute::get(pir::IrContext::Instance(), axes[i]);

    vec_axes.push_back(attr_axes);
  }
  pir::Attribute attr_axes = pir::ArrayAttribute::get(pir::IrContext::Instance(), vec_axes);
  argument.AddAttribute("axes", attr_axes);
  std::vector<pir::Attribute> vec_infer_flags;
  for (size_t i = 0; i < static_cast<size_t>(infer_flags.size()); i++) {
      pir::Attribute attr_infer_flags = pir::Int64Attribute::get(pir::IrContext::Instance(), infer_flags[i]);

    vec_infer_flags.push_back(attr_infer_flags);
  }
  pir::Attribute attr_infer_flags = pir::ArrayAttribute::get(pir::IrContext::Instance(), vec_infer_flags);
  argument.AddAttribute("infer_flags", attr_infer_flags);
  std::vector<pir::Attribute> vec_decrease_axis;
  for (size_t i = 0; i < static_cast<size_t>(decrease_axis.size()); i++) {
      pir::Attribute attr_decrease_axis = pir::Int64Attribute::get(pir::IrContext::Instance(), decrease_axis[i]);

    vec_decrease_axis.push_back(attr_decrease_axis);
  }
  pir::Attribute attr_decrease_axis = pir::ArrayAttribute::get(pir::IrContext::Instance(), vec_decrease_axis);
  argument.AddAttribute("decrease_axis", attr_decrease_axis);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType input = input_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)input;
  paddle::dialect::DenseTensorType out_grad = out_grad_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)out_grad;

  VLOG(4) << "Builder construction  dense_input";
  paddle::dialect::IrTensor ir_tensor_input(paddle::dialect::TransToPhiDataType(input.dtype()),
                                                      input.dims(),
                                                      input.data_layout(),
                                                      input.lod(),
                                                      input.offset());
  VLOG(4) << "Builder construction  meta_input";
  paddle::dialect::IrMetaTensor meta_input(&ir_tensor_input);
  paddle::dialect::IrTensor dense_input_grad;
  paddle::dialect::IrMetaTensor meta_input_grad(&dense_input_grad);

  phi::UnchangedInferMeta(meta_input, &meta_input_grad);

  std::vector<pir::Type> argument_outputs;
  pir::Type input_grad_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_input_grad.dtype()), dense_input_grad.dims(), dense_input_grad.layout(), dense_input_grad.lod(), dense_input_grad.offset());
  argument_outputs.push_back(input_grad_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void SliceGradOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value input_, pir::Value out_grad_, pir::AttributeMap attributes) {
  VLOG(4) << "Start build SliceGradOp";


  IR_ENFORCE(
      attributes.find("axes") != attributes.end(),
          "'axes' Attribute is expected for SliceGradOp. ");
  std::vector<int64_t> axes;
  for (size_t i = 0; i < attributes.at("axes").dyn_cast<pir::ArrayAttribute>().size(); i++) {
    axes.push_back(attributes.at("axes").dyn_cast<pir::ArrayAttribute>().at(i).dyn_cast<pir::Int64Attribute>().data());
  }

  IR_ENFORCE(
      attributes.find("starts") != attributes.end(),
          "'starts' Attribute is expected for SliceGradOp. ");
  std::vector<int64_t> starts = attributes.at("starts").dyn_cast<paddle::dialect::IntArrayAttribute>().data().GetData();

  IR_ENFORCE(
      attributes.find("ends") != attributes.end(),
          "'ends' Attribute is expected for SliceGradOp. ");
  std::vector<int64_t> ends = attributes.at("ends").dyn_cast<paddle::dialect::IntArrayAttribute>().data().GetData();

  IR_ENFORCE(
      attributes.find("infer_flags") != attributes.end(),
          "'infer_flags' Attribute is expected for SliceGradOp. ");
  std::vector<int64_t> infer_flags;
  for (size_t i = 0; i < attributes.at("infer_flags").dyn_cast<pir::ArrayAttribute>().size(); i++) {
    infer_flags.push_back(attributes.at("infer_flags").dyn_cast<pir::ArrayAttribute>().at(i).dyn_cast<pir::Int64Attribute>().data());
  }

  IR_ENFORCE(
      attributes.find("decrease_axis") != attributes.end(),
          "'decrease_axis' Attribute is expected for SliceGradOp. ");
  std::vector<int64_t> decrease_axis;
  for (size_t i = 0; i < attributes.at("decrease_axis").dyn_cast<pir::ArrayAttribute>().size(); i++) {
    decrease_axis.push_back(attributes.at("decrease_axis").dyn_cast<pir::ArrayAttribute>().at(i).dyn_cast<pir::Int64Attribute>().data());
  }

  // Generate int_array mutable attribute: starts
  paddle::dialect::FullIntArrayOp full_starts_op = builder.Build<paddle::dialect::FullIntArrayOp>(starts, phi::DataType::INT64, phi::CPUPlace());
  pir::OpResult starts_ = full_starts_op->result(0);
      // Generate int_array mutable attribute: ends
  paddle::dialect::FullIntArrayOp full_ends_op = builder.Build<paddle::dialect::FullIntArrayOp>(ends, phi::DataType::INT64, phi::CPUPlace());
  pir::OpResult ends_ = full_ends_op->result(0);
    
  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {input_, out_grad_, starts_, ends_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  std::vector<pir::Attribute> vec_axes;
  for (size_t i = 0; i < static_cast<size_t>(axes.size()); i++) {
      pir::Attribute attr_axes = pir::Int64Attribute::get(pir::IrContext::Instance(), axes[i]);

    vec_axes.push_back(attr_axes);
  }
  pir::Attribute attr_axes = pir::ArrayAttribute::get(pir::IrContext::Instance(), vec_axes);
  argument.AddAttribute("axes", attr_axes);
  std::vector<pir::Attribute> vec_infer_flags;
  for (size_t i = 0; i < static_cast<size_t>(infer_flags.size()); i++) {
      pir::Attribute attr_infer_flags = pir::Int64Attribute::get(pir::IrContext::Instance(), infer_flags[i]);

    vec_infer_flags.push_back(attr_infer_flags);
  }
  pir::Attribute attr_infer_flags = pir::ArrayAttribute::get(pir::IrContext::Instance(), vec_infer_flags);
  argument.AddAttribute("infer_flags", attr_infer_flags);
  std::vector<pir::Attribute> vec_decrease_axis;
  for (size_t i = 0; i < static_cast<size_t>(decrease_axis.size()); i++) {
      pir::Attribute attr_decrease_axis = pir::Int64Attribute::get(pir::IrContext::Instance(), decrease_axis[i]);

    vec_decrease_axis.push_back(attr_decrease_axis);
  }
  pir::Attribute attr_decrease_axis = pir::ArrayAttribute::get(pir::IrContext::Instance(), vec_decrease_axis);
  argument.AddAttribute("decrease_axis", attr_decrease_axis);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType input = input_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)input;
  paddle::dialect::DenseTensorType out_grad = out_grad_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)out_grad;

  VLOG(4) << "Builder construction  dense_input";
  paddle::dialect::IrTensor ir_tensor_input(paddle::dialect::TransToPhiDataType(input.dtype()),
                                                      input.dims(),
                                                      input.data_layout(),
                                                      input.lod(),
                                                      input.offset());
  VLOG(4) << "Builder construction  meta_input";
  paddle::dialect::IrMetaTensor meta_input(&ir_tensor_input);
  paddle::dialect::IrTensor dense_input_grad;
  paddle::dialect::IrMetaTensor meta_input_grad(&dense_input_grad);

  phi::UnchangedInferMeta(meta_input, &meta_input_grad);

  std::vector<pir::Type> argument_outputs;
  pir::Type input_grad_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_input_grad.dtype()), dense_input_grad.dims(), dense_input_grad.layout(), dense_input_grad.lod(), dense_input_grad.offset());
  argument_outputs.push_back(input_grad_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void SliceGradOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value input_, pir::Value out_grad_, pir::Value starts_, pir::Value ends_, const std::vector<int64_t>& axes, const std::vector<int64_t>& infer_flags, const std::vector<int64_t>& decrease_axis) {
  VLOG(4) << "Start build SliceGradOp";



  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {input_, out_grad_, starts_, ends_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  std::vector<pir::Attribute> vec_axes;
  for (size_t i = 0; i < static_cast<size_t>(axes.size()); i++) {
      pir::Attribute attr_axes = pir::Int64Attribute::get(pir::IrContext::Instance(), axes[i]);

    vec_axes.push_back(attr_axes);
  }
  pir::Attribute attr_axes = pir::ArrayAttribute::get(pir::IrContext::Instance(), vec_axes);
  argument.AddAttribute("axes", attr_axes);
  std::vector<pir::Attribute> vec_infer_flags;
  for (size_t i = 0; i < static_cast<size_t>(infer_flags.size()); i++) {
      pir::Attribute attr_infer_flags = pir::Int64Attribute::get(pir::IrContext::Instance(), infer_flags[i]);

    vec_infer_flags.push_back(attr_infer_flags);
  }
  pir::Attribute attr_infer_flags = pir::ArrayAttribute::get(pir::IrContext::Instance(), vec_infer_flags);
  argument.AddAttribute("infer_flags", attr_infer_flags);
  std::vector<pir::Attribute> vec_decrease_axis;
  for (size_t i = 0; i < static_cast<size_t>(decrease_axis.size()); i++) {
      pir::Attribute attr_decrease_axis = pir::Int64Attribute::get(pir::IrContext::Instance(), decrease_axis[i]);

    vec_decrease_axis.push_back(attr_decrease_axis);
  }
  pir::Attribute attr_decrease_axis = pir::ArrayAttribute::get(pir::IrContext::Instance(), vec_decrease_axis);
  argument.AddAttribute("decrease_axis", attr_decrease_axis);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType input = input_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)input;
  paddle::dialect::DenseTensorType out_grad = out_grad_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)out_grad;
  phi::IntArray starts;
  if (starts_.dyn_cast<pir::OpResult>() && starts_.dyn_cast<pir::OpResult>().owner()->isa<paddle::dialect::FullIntArrayOp>()) {
    starts = std::move(phi::IntArray(paddle::dialect::GetInt64Vector(
                          starts_.dyn_cast<pir::OpResult>().owner()
                          ->dyn_cast<paddle::dialect::FullIntArrayOp>()
                          .attribute("value"))));
  } else if (starts_.type().isa<pir::VectorType>()) {
    size_t starts_size = starts_.type().dyn_cast<pir::VectorType>().size();
    starts = std::move(phi::IntArray(std::vector<int64_t>(starts_size, -1)));
    starts.SetFromTensor(true);
  } else if (starts_.type().isa<paddle::dialect::DenseTensorType>()) {
    common::DDim starts_dim = starts_.type().dyn_cast<paddle::dialect::DenseTensorType>().dims();
    size_t starts_size = common::product(starts_dim);
    if (common::contain_unknown_dim(starts_dim)) {
      starts_size = 1;
    }
    starts = std::move(phi::IntArray(std::vector<int64_t>(starts_size, -1)));
    starts.SetFromTensor(true);
  } else {
    PADDLE_THROW(phi::errors::Unimplemented("Only support VectorType or DenseTensorType"));
  }
  phi::IntArray ends;
  if (ends_.dyn_cast<pir::OpResult>() && ends_.dyn_cast<pir::OpResult>().owner()->isa<paddle::dialect::FullIntArrayOp>()) {
    ends = std::move(phi::IntArray(paddle::dialect::GetInt64Vector(
                          ends_.dyn_cast<pir::OpResult>().owner()
                          ->dyn_cast<paddle::dialect::FullIntArrayOp>()
                          .attribute("value"))));
  } else if (ends_.type().isa<pir::VectorType>()) {
    size_t ends_size = ends_.type().dyn_cast<pir::VectorType>().size();
    ends = std::move(phi::IntArray(std::vector<int64_t>(ends_size, -1)));
    ends.SetFromTensor(true);
  } else if (ends_.type().isa<paddle::dialect::DenseTensorType>()) {
    common::DDim ends_dim = ends_.type().dyn_cast<paddle::dialect::DenseTensorType>().dims();
    size_t ends_size = common::product(ends_dim);
    if (common::contain_unknown_dim(ends_dim)) {
      ends_size = 1;
    }
    ends = std::move(phi::IntArray(std::vector<int64_t>(ends_size, -1)));
    ends.SetFromTensor(true);
  } else {
    PADDLE_THROW(phi::errors::Unimplemented("Only support VectorType or DenseTensorType"));
  }


  VLOG(4) << "Builder construction  dense_input";
  paddle::dialect::IrTensor ir_tensor_input(paddle::dialect::TransToPhiDataType(input.dtype()),
                                                      input.dims(),
                                                      input.data_layout(),
                                                      input.lod(),
                                                      input.offset());
  VLOG(4) << "Builder construction  meta_input";
  paddle::dialect::IrMetaTensor meta_input(&ir_tensor_input);
  paddle::dialect::IrTensor dense_input_grad;
  paddle::dialect::IrMetaTensor meta_input_grad(&dense_input_grad);

  phi::UnchangedInferMeta(meta_input, &meta_input_grad);

  std::vector<pir::Type> argument_outputs;
  pir::Type input_grad_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_input_grad.dtype()), dense_input_grad.dims(), dense_input_grad.layout(), dense_input_grad.lod(), dense_input_grad.offset());
  argument_outputs.push_back(input_grad_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void SliceGradOp::VerifySig() {}

void SliceGradOp::InferMeta( phi::InferMetaContext *infer_meta ) {
  auto fn = PD_INFER_META(phi::UnchangedInferMeta);
  fn(infer_meta);
}

phi::DataType SliceGradOp::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: SliceGradOp";
  


  return expected_kernel_dtype;
}

const char *SoftReluGradOp::attributes_name[1] = { "threshold" };

OpInfoTuple SoftReluGradOp::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("out", "paddle::dialect::DenseTensorType", false, false, false, false), paddle::dialect::OpInputInfo("out_grad", "paddle::dialect::DenseTensorType", false, false, false, false) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = { paddle::dialect::OpAttributeInfo("threshold", "pir::FloatAttribute", "") };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("x_grad", "paddle::dialect::DenseTensorType", false, false) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("UnchangedInferMeta", {"out"}, "soft_relu_grad", {"out", "out_grad", "threshold"}, {}, {}, {}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "soft_relu_grad");
}

void SoftReluGradOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value out_, pir::Value out_grad_, float threshold) {
  VLOG(4) << "Start build SoftReluGradOp";



  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {out_, out_grad_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_threshold = pir::FloatAttribute::get(pir::IrContext::Instance(), threshold);
  argument.AddAttribute("threshold", attr_threshold);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType out = out_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)out;
  paddle::dialect::DenseTensorType out_grad = out_grad_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)out_grad;

  VLOG(4) << "Builder construction  dense_out";
  paddle::dialect::IrTensor ir_tensor_out(paddle::dialect::TransToPhiDataType(out.dtype()),
                                                      out.dims(),
                                                      out.data_layout(),
                                                      out.lod(),
                                                      out.offset());
  VLOG(4) << "Builder construction  meta_out";
  paddle::dialect::IrMetaTensor meta_out(&ir_tensor_out);
  paddle::dialect::IrTensor dense_x_grad;
  paddle::dialect::IrMetaTensor meta_x_grad(&dense_x_grad);

  phi::UnchangedInferMeta(meta_out, &meta_x_grad);

  std::vector<pir::Type> argument_outputs;
  pir::Type x_grad_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_x_grad.dtype()), dense_x_grad.dims(), dense_x_grad.layout(), dense_x_grad.lod(), dense_x_grad.offset());
  argument_outputs.push_back(x_grad_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void SoftReluGradOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value out_, pir::Value out_grad_, pir::AttributeMap attributes) {
  VLOG(4) << "Start build SoftReluGradOp";


  IR_ENFORCE(
      attributes.find("threshold") != attributes.end(),
          "'threshold' Attribute is expected for SoftReluGradOp. ");
  float threshold = attributes.at("threshold").dyn_cast<pir::FloatAttribute>().data();


  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {out_, out_grad_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_threshold = pir::FloatAttribute::get(pir::IrContext::Instance(), threshold);
  argument.AddAttribute("threshold", attr_threshold);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType out = out_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)out;
  paddle::dialect::DenseTensorType out_grad = out_grad_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)out_grad;

  VLOG(4) << "Builder construction  dense_out";
  paddle::dialect::IrTensor ir_tensor_out(paddle::dialect::TransToPhiDataType(out.dtype()),
                                                      out.dims(),
                                                      out.data_layout(),
                                                      out.lod(),
                                                      out.offset());
  VLOG(4) << "Builder construction  meta_out";
  paddle::dialect::IrMetaTensor meta_out(&ir_tensor_out);
  paddle::dialect::IrTensor dense_x_grad;
  paddle::dialect::IrMetaTensor meta_x_grad(&dense_x_grad);

  phi::UnchangedInferMeta(meta_out, &meta_x_grad);

  std::vector<pir::Type> argument_outputs;
  pir::Type x_grad_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_x_grad.dtype()), dense_x_grad.dims(), dense_x_grad.layout(), dense_x_grad.lod(), dense_x_grad.offset());
  argument_outputs.push_back(x_grad_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void SoftReluGradOp::VerifySig() {}

void SoftReluGradOp::InferMeta( phi::InferMetaContext *infer_meta ) {
  auto fn = PD_INFER_META(phi::UnchangedInferMeta);
  fn(infer_meta);
}

phi::DataType SoftReluGradOp::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: SoftReluGradOp";
  


  return expected_kernel_dtype;
}

const char *SoftmaxGradOp::attributes_name[1] = { "axis" };

OpInfoTuple SoftmaxGradOp::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("out", "paddle::dialect::DenseTensorType", false, false, false, false), paddle::dialect::OpInputInfo("out_grad", "paddle::dialect::DenseTensorType", false, false, false, false) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = { paddle::dialect::OpAttributeInfo("axis", "pir::Int32Attribute", "") };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("x_grad", "paddle::dialect::DenseTensorType", false, false) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("UnchangedInferMeta", {"out"}, "softmax_grad", {"out", "out_grad", "axis"}, {}, {}, {}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "softmax_grad");
}

void SoftmaxGradOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value out_, pir::Value out_grad_, int axis) {
  VLOG(4) << "Start build SoftmaxGradOp";



  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {out_, out_grad_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_axis = pir::Int32Attribute::get(pir::IrContext::Instance(), axis);
  argument.AddAttribute("axis", attr_axis);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType out = out_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)out;
  paddle::dialect::DenseTensorType out_grad = out_grad_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)out_grad;

  VLOG(4) << "Builder construction  dense_out";
  paddle::dialect::IrTensor ir_tensor_out(paddle::dialect::TransToPhiDataType(out.dtype()),
                                                      out.dims(),
                                                      out.data_layout(),
                                                      out.lod(),
                                                      out.offset());
  VLOG(4) << "Builder construction  meta_out";
  paddle::dialect::IrMetaTensor meta_out(&ir_tensor_out);
  paddle::dialect::IrTensor dense_x_grad;
  paddle::dialect::IrMetaTensor meta_x_grad(&dense_x_grad);

  phi::UnchangedInferMeta(meta_out, &meta_x_grad);

  std::vector<pir::Type> argument_outputs;
  pir::Type x_grad_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_x_grad.dtype()), dense_x_grad.dims(), dense_x_grad.layout(), dense_x_grad.lod(), dense_x_grad.offset());
  argument_outputs.push_back(x_grad_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void SoftmaxGradOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value out_, pir::Value out_grad_, pir::AttributeMap attributes) {
  VLOG(4) << "Start build SoftmaxGradOp";


  IR_ENFORCE(
      attributes.find("axis") != attributes.end(),
          "'axis' Attribute is expected for SoftmaxGradOp. ");
  int axis = attributes.at("axis").dyn_cast<pir::Int32Attribute>().data();


  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {out_, out_grad_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_axis = pir::Int32Attribute::get(pir::IrContext::Instance(), axis);
  argument.AddAttribute("axis", attr_axis);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType out = out_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)out;
  paddle::dialect::DenseTensorType out_grad = out_grad_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)out_grad;

  VLOG(4) << "Builder construction  dense_out";
  paddle::dialect::IrTensor ir_tensor_out(paddle::dialect::TransToPhiDataType(out.dtype()),
                                                      out.dims(),
                                                      out.data_layout(),
                                                      out.lod(),
                                                      out.offset());
  VLOG(4) << "Builder construction  meta_out";
  paddle::dialect::IrMetaTensor meta_out(&ir_tensor_out);
  paddle::dialect::IrTensor dense_x_grad;
  paddle::dialect::IrMetaTensor meta_x_grad(&dense_x_grad);

  phi::UnchangedInferMeta(meta_out, &meta_x_grad);

  std::vector<pir::Type> argument_outputs;
  pir::Type x_grad_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_x_grad.dtype()), dense_x_grad.dims(), dense_x_grad.layout(), dense_x_grad.lod(), dense_x_grad.offset());
  argument_outputs.push_back(x_grad_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void SoftmaxGradOp::VerifySig() {}

void SoftmaxGradOp::InferMeta( phi::InferMetaContext *infer_meta ) {
  auto fn = PD_INFER_META(phi::UnchangedInferMeta);
  fn(infer_meta);
}

phi::DataType SoftmaxGradOp::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: SoftmaxGradOp";
  


  return expected_kernel_dtype;
}

OpInfoTuple SplitWithNumGradOp::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("out_grad", "pir::VectorType<paddle::dialect::DenseTensorType>", false, false, false, false), paddle::dialect::OpInputInfo("axis", "paddle::dialect::ScalarAttribute", false, false, true, false) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = {  };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("x_grad", "paddle::dialect::DenseTensorType", false, false) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("", {""}, "", {""}, {}, {}, {}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "split_with_num_grad");
}

void SplitWithNumGradOp::VerifySig() {}

phi::DataType SplitWithNumGradOp::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: SplitWithNumGradOp";
  


  return expected_kernel_dtype;
}

const char *StridedSliceGradOp::attributes_name[1] = { "axes" };

OpInfoTuple StridedSliceGradOp::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("x", "paddle::dialect::DenseTensorType", false, true, false, false), paddle::dialect::OpInputInfo("out_grad", "paddle::dialect::DenseTensorType", false, false, false, false), paddle::dialect::OpInputInfo("starts", "paddle::dialect::IntArrayAttribute", false, false, true, false), paddle::dialect::OpInputInfo("ends", "paddle::dialect::IntArrayAttribute", false, false, true, false), paddle::dialect::OpInputInfo("strides", "paddle::dialect::IntArrayAttribute", false, false, true, false) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = { paddle::dialect::OpAttributeInfo("axes", "pir::ArrayAttribute<pir::Int32Attribute>", "") };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("x_grad", "paddle::dialect::DenseTensorType", false, false) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("GeneralUnaryGradInferMeta", {"x"}, "strided_slice_grad", {"x", "out_grad", "axes", "starts", "ends", "strides"}, {}, {}, {}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "strided_slice_grad");
}

void StridedSliceGradOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, pir::Value out_grad_, const std::vector<int>& axes, const std::vector<int64_t>& starts, const std::vector<int64_t>& ends, const std::vector<int64_t>& strides) {
  VLOG(4) << "Start build StridedSliceGradOp";


  // Generate int_array mutable attribute: starts
  paddle::dialect::FullIntArrayOp full_starts_op = builder.Build<paddle::dialect::FullIntArrayOp>(starts, phi::DataType::INT64, phi::CPUPlace());
  pir::OpResult starts_ = full_starts_op->result(0);
      // Generate int_array mutable attribute: ends
  paddle::dialect::FullIntArrayOp full_ends_op = builder.Build<paddle::dialect::FullIntArrayOp>(ends, phi::DataType::INT64, phi::CPUPlace());
  pir::OpResult ends_ = full_ends_op->result(0);
      // Generate int_array mutable attribute: strides
  paddle::dialect::FullIntArrayOp full_strides_op = builder.Build<paddle::dialect::FullIntArrayOp>(strides, phi::DataType::INT64, phi::CPUPlace());
  pir::OpResult strides_ = full_strides_op->result(0);
    
  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_, out_grad_, starts_, ends_, strides_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  std::vector<pir::Attribute> vec_axes;
  for (size_t i = 0; i < static_cast<size_t>(axes.size()); i++) {
      pir::Attribute attr_axes = pir::Int32Attribute::get(pir::IrContext::Instance(), axes[i]);

    vec_axes.push_back(attr_axes);
  }
  pir::Attribute attr_axes = pir::ArrayAttribute::get(pir::IrContext::Instance(), vec_axes);
  argument.AddAttribute("axes", attr_axes);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;
  paddle::dialect::DenseTensorType out_grad = out_grad_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)out_grad;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);
  paddle::dialect::IrTensor dense_x_grad;
  paddle::dialect::IrMetaTensor meta_x_grad(&dense_x_grad);

  phi::GeneralUnaryGradInferMeta(meta_x, &meta_x_grad);

  std::vector<pir::Type> argument_outputs;
  pir::Type x_grad_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_x_grad.dtype()), dense_x_grad.dims(), dense_x_grad.layout(), dense_x_grad.lod(), dense_x_grad.offset());
  argument_outputs.push_back(x_grad_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void StridedSliceGradOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, pir::Value out_grad_, pir::AttributeMap attributes) {
  VLOG(4) << "Start build StridedSliceGradOp";


  IR_ENFORCE(
      attributes.find("axes") != attributes.end(),
          "'axes' Attribute is expected for StridedSliceGradOp. ");
  std::vector<int> axes;
  for (size_t i = 0; i < attributes.at("axes").dyn_cast<pir::ArrayAttribute>().size(); i++) {
    axes.push_back(attributes.at("axes").dyn_cast<pir::ArrayAttribute>().at(i).dyn_cast<pir::Int32Attribute>().data());
  }

  IR_ENFORCE(
      attributes.find("starts") != attributes.end(),
          "'starts' Attribute is expected for StridedSliceGradOp. ");
  std::vector<int64_t> starts = attributes.at("starts").dyn_cast<paddle::dialect::IntArrayAttribute>().data().GetData();

  IR_ENFORCE(
      attributes.find("ends") != attributes.end(),
          "'ends' Attribute is expected for StridedSliceGradOp. ");
  std::vector<int64_t> ends = attributes.at("ends").dyn_cast<paddle::dialect::IntArrayAttribute>().data().GetData();

  IR_ENFORCE(
      attributes.find("strides") != attributes.end(),
          "'strides' Attribute is expected for StridedSliceGradOp. ");
  std::vector<int64_t> strides = attributes.at("strides").dyn_cast<paddle::dialect::IntArrayAttribute>().data().GetData();

  // Generate int_array mutable attribute: starts
  paddle::dialect::FullIntArrayOp full_starts_op = builder.Build<paddle::dialect::FullIntArrayOp>(starts, phi::DataType::INT64, phi::CPUPlace());
  pir::OpResult starts_ = full_starts_op->result(0);
      // Generate int_array mutable attribute: ends
  paddle::dialect::FullIntArrayOp full_ends_op = builder.Build<paddle::dialect::FullIntArrayOp>(ends, phi::DataType::INT64, phi::CPUPlace());
  pir::OpResult ends_ = full_ends_op->result(0);
      // Generate int_array mutable attribute: strides
  paddle::dialect::FullIntArrayOp full_strides_op = builder.Build<paddle::dialect::FullIntArrayOp>(strides, phi::DataType::INT64, phi::CPUPlace());
  pir::OpResult strides_ = full_strides_op->result(0);
    
  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_, out_grad_, starts_, ends_, strides_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  std::vector<pir::Attribute> vec_axes;
  for (size_t i = 0; i < static_cast<size_t>(axes.size()); i++) {
      pir::Attribute attr_axes = pir::Int32Attribute::get(pir::IrContext::Instance(), axes[i]);

    vec_axes.push_back(attr_axes);
  }
  pir::Attribute attr_axes = pir::ArrayAttribute::get(pir::IrContext::Instance(), vec_axes);
  argument.AddAttribute("axes", attr_axes);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;
  paddle::dialect::DenseTensorType out_grad = out_grad_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)out_grad;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);
  paddle::dialect::IrTensor dense_x_grad;
  paddle::dialect::IrMetaTensor meta_x_grad(&dense_x_grad);

  phi::GeneralUnaryGradInferMeta(meta_x, &meta_x_grad);

  std::vector<pir::Type> argument_outputs;
  pir::Type x_grad_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_x_grad.dtype()), dense_x_grad.dims(), dense_x_grad.layout(), dense_x_grad.lod(), dense_x_grad.offset());
  argument_outputs.push_back(x_grad_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void StridedSliceGradOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, pir::Value out_grad_, pir::Value starts_, pir::Value ends_, pir::Value strides_, const std::vector<int>& axes) {
  VLOG(4) << "Start build StridedSliceGradOp";



  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_, out_grad_, starts_, ends_, strides_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  std::vector<pir::Attribute> vec_axes;
  for (size_t i = 0; i < static_cast<size_t>(axes.size()); i++) {
      pir::Attribute attr_axes = pir::Int32Attribute::get(pir::IrContext::Instance(), axes[i]);

    vec_axes.push_back(attr_axes);
  }
  pir::Attribute attr_axes = pir::ArrayAttribute::get(pir::IrContext::Instance(), vec_axes);
  argument.AddAttribute("axes", attr_axes);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;
  paddle::dialect::DenseTensorType out_grad = out_grad_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)out_grad;
  phi::IntArray starts;
  if (starts_.dyn_cast<pir::OpResult>() && starts_.dyn_cast<pir::OpResult>().owner()->isa<paddle::dialect::FullIntArrayOp>()) {
    starts = std::move(phi::IntArray(paddle::dialect::GetInt64Vector(
                          starts_.dyn_cast<pir::OpResult>().owner()
                          ->dyn_cast<paddle::dialect::FullIntArrayOp>()
                          .attribute("value"))));
  } else if (starts_.type().isa<pir::VectorType>()) {
    size_t starts_size = starts_.type().dyn_cast<pir::VectorType>().size();
    starts = std::move(phi::IntArray(std::vector<int64_t>(starts_size, -1)));
    starts.SetFromTensor(true);
  } else if (starts_.type().isa<paddle::dialect::DenseTensorType>()) {
    common::DDim starts_dim = starts_.type().dyn_cast<paddle::dialect::DenseTensorType>().dims();
    size_t starts_size = common::product(starts_dim);
    if (common::contain_unknown_dim(starts_dim)) {
      starts_size = 1;
    }
    starts = std::move(phi::IntArray(std::vector<int64_t>(starts_size, -1)));
    starts.SetFromTensor(true);
  } else {
    PADDLE_THROW(phi::errors::Unimplemented("Only support VectorType or DenseTensorType"));
  }
  phi::IntArray ends;
  if (ends_.dyn_cast<pir::OpResult>() && ends_.dyn_cast<pir::OpResult>().owner()->isa<paddle::dialect::FullIntArrayOp>()) {
    ends = std::move(phi::IntArray(paddle::dialect::GetInt64Vector(
                          ends_.dyn_cast<pir::OpResult>().owner()
                          ->dyn_cast<paddle::dialect::FullIntArrayOp>()
                          .attribute("value"))));
  } else if (ends_.type().isa<pir::VectorType>()) {
    size_t ends_size = ends_.type().dyn_cast<pir::VectorType>().size();
    ends = std::move(phi::IntArray(std::vector<int64_t>(ends_size, -1)));
    ends.SetFromTensor(true);
  } else if (ends_.type().isa<paddle::dialect::DenseTensorType>()) {
    common::DDim ends_dim = ends_.type().dyn_cast<paddle::dialect::DenseTensorType>().dims();
    size_t ends_size = common::product(ends_dim);
    if (common::contain_unknown_dim(ends_dim)) {
      ends_size = 1;
    }
    ends = std::move(phi::IntArray(std::vector<int64_t>(ends_size, -1)));
    ends.SetFromTensor(true);
  } else {
    PADDLE_THROW(phi::errors::Unimplemented("Only support VectorType or DenseTensorType"));
  }
  phi::IntArray strides;
  if (strides_.dyn_cast<pir::OpResult>() && strides_.dyn_cast<pir::OpResult>().owner()->isa<paddle::dialect::FullIntArrayOp>()) {
    strides = std::move(phi::IntArray(paddle::dialect::GetInt64Vector(
                          strides_.dyn_cast<pir::OpResult>().owner()
                          ->dyn_cast<paddle::dialect::FullIntArrayOp>()
                          .attribute("value"))));
  } else if (strides_.type().isa<pir::VectorType>()) {
    size_t strides_size = strides_.type().dyn_cast<pir::VectorType>().size();
    strides = std::move(phi::IntArray(std::vector<int64_t>(strides_size, -1)));
    strides.SetFromTensor(true);
  } else if (strides_.type().isa<paddle::dialect::DenseTensorType>()) {
    common::DDim strides_dim = strides_.type().dyn_cast<paddle::dialect::DenseTensorType>().dims();
    size_t strides_size = common::product(strides_dim);
    if (common::contain_unknown_dim(strides_dim)) {
      strides_size = 1;
    }
    strides = std::move(phi::IntArray(std::vector<int64_t>(strides_size, -1)));
    strides.SetFromTensor(true);
  } else {
    PADDLE_THROW(phi::errors::Unimplemented("Only support VectorType or DenseTensorType"));
  }


  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);
  paddle::dialect::IrTensor dense_x_grad;
  paddle::dialect::IrMetaTensor meta_x_grad(&dense_x_grad);

  phi::GeneralUnaryGradInferMeta(meta_x, &meta_x_grad);

  std::vector<pir::Type> argument_outputs;
  pir::Type x_grad_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_x_grad.dtype()), dense_x_grad.dims(), dense_x_grad.layout(), dense_x_grad.lod(), dense_x_grad.offset());
  argument_outputs.push_back(x_grad_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void StridedSliceGradOp::VerifySig() {}

void StridedSliceGradOp::InferMeta( phi::InferMetaContext *infer_meta ) {
  auto fn = PD_INFER_META(phi::GeneralUnaryGradInferMeta);
  fn(infer_meta);
}

phi::DataType StridedSliceGradOp::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: StridedSliceGradOp";
  


  return expected_kernel_dtype;
}

const char *SubtractDoubleGradOp::attributes_name[1] = { "axis" };

OpInfoTuple SubtractDoubleGradOp::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("y", "paddle::dialect::DenseTensorType", false, true, false, false), paddle::dialect::OpInputInfo("grad_out", "paddle::dialect::DenseTensorType", false, true, false, false), paddle::dialect::OpInputInfo("grad_x_grad", "paddle::dialect::DenseTensorType", true, false, false, false), paddle::dialect::OpInputInfo("grad_y_grad", "paddle::dialect::DenseTensorType", true, false, false, false) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = { paddle::dialect::OpAttributeInfo("axis", "pir::Int32Attribute", "") };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("grad_out_grad", "paddle::dialect::DenseTensorType", false, false) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("UnchangedInferMeta", {"grad_out"}, "subtract_double_grad", {"y", "grad_out", "grad_x_grad", "grad_y_grad", "axis"}, {}, {}, {}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "subtract_double_grad");
}

void SubtractDoubleGradOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value y_, pir::Value grad_out_, pir::Value grad_x_grad_, pir::Value grad_y_grad_, int axis) {
  VLOG(4) << "Start build SubtractDoubleGradOp";



  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {y_, grad_out_, grad_x_grad_, grad_y_grad_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_axis = pir::Int32Attribute::get(pir::IrContext::Instance(), axis);
  argument.AddAttribute("axis", attr_axis);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType y = y_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)y;
  paddle::dialect::DenseTensorType grad_out = grad_out_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)grad_out;

  VLOG(4) << "Builder construction  dense_grad_out";
  paddle::dialect::IrTensor ir_tensor_grad_out(paddle::dialect::TransToPhiDataType(grad_out.dtype()),
                                                      grad_out.dims(),
                                                      grad_out.data_layout(),
                                                      grad_out.lod(),
                                                      grad_out.offset());
  VLOG(4) << "Builder construction  meta_grad_out";
  paddle::dialect::IrMetaTensor meta_grad_out(&ir_tensor_grad_out);
  paddle::dialect::IrTensor dense_grad_out_grad;
  paddle::dialect::IrMetaTensor meta_grad_out_grad(&dense_grad_out_grad);

  phi::UnchangedInferMeta(meta_grad_out, &meta_grad_out_grad);

  std::vector<pir::Type> argument_outputs;
  pir::Type grad_out_grad_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_grad_out_grad.dtype()), dense_grad_out_grad.dims(), dense_grad_out_grad.layout(), dense_grad_out_grad.lod(), dense_grad_out_grad.offset());
  argument_outputs.push_back(grad_out_grad_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void SubtractDoubleGradOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value y_, pir::Value grad_out_, pir::Value grad_x_grad_, pir::Value grad_y_grad_, pir::AttributeMap attributes) {
  VLOG(4) << "Start build SubtractDoubleGradOp";


  IR_ENFORCE(
      attributes.find("axis") != attributes.end(),
          "'axis' Attribute is expected for SubtractDoubleGradOp. ");
  int axis = attributes.at("axis").dyn_cast<pir::Int32Attribute>().data();


  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {y_, grad_out_, grad_x_grad_, grad_y_grad_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_axis = pir::Int32Attribute::get(pir::IrContext::Instance(), axis);
  argument.AddAttribute("axis", attr_axis);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType y = y_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)y;
  paddle::dialect::DenseTensorType grad_out = grad_out_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)grad_out;

  VLOG(4) << "Builder construction  dense_grad_out";
  paddle::dialect::IrTensor ir_tensor_grad_out(paddle::dialect::TransToPhiDataType(grad_out.dtype()),
                                                      grad_out.dims(),
                                                      grad_out.data_layout(),
                                                      grad_out.lod(),
                                                      grad_out.offset());
  VLOG(4) << "Builder construction  meta_grad_out";
  paddle::dialect::IrMetaTensor meta_grad_out(&ir_tensor_grad_out);
  paddle::dialect::IrTensor dense_grad_out_grad;
  paddle::dialect::IrMetaTensor meta_grad_out_grad(&dense_grad_out_grad);

  phi::UnchangedInferMeta(meta_grad_out, &meta_grad_out_grad);

  std::vector<pir::Type> argument_outputs;
  pir::Type grad_out_grad_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_grad_out_grad.dtype()), dense_grad_out_grad.dims(), dense_grad_out_grad.layout(), dense_grad_out_grad.lod(), dense_grad_out_grad.offset());
  argument_outputs.push_back(grad_out_grad_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void SubtractDoubleGradOp::VerifySig() {}

void SubtractDoubleGradOp::InferMeta( phi::InferMetaContext *infer_meta ) {
  auto fn = PD_INFER_META(phi::UnchangedInferMeta);
  fn(infer_meta);
}

phi::DataType SubtractDoubleGradOp::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: SubtractDoubleGradOp";
  


  // deal complex_promote
  if (framework::IsComplexType(expected_kernel_dtype)) {{
    // only promote inputsâ€™s types when contains complex input
    return tensor_dtype;
  }}

  return expected_kernel_dtype;
}

const char *SubtractDoubleGrad_Op::attributes_name[1] = { "axis" };

OpInfoTuple SubtractDoubleGrad_Op::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("y", "paddle::dialect::DenseTensorType", false, true, false, false), paddle::dialect::OpInputInfo("grad_out", "paddle::dialect::DenseTensorType", false, true, false, false), paddle::dialect::OpInputInfo("grad_x_grad", "paddle::dialect::DenseTensorType", true, false, false, false), paddle::dialect::OpInputInfo("grad_y_grad", "paddle::dialect::DenseTensorType", true, false, false, false) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = { paddle::dialect::OpAttributeInfo("axis", "pir::Int32Attribute", "") };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("grad_out_grad", "paddle::dialect::DenseTensorType", false, false) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("UnchangedInferMeta", {"grad_out"}, "subtract_double_grad", {"y", "grad_out", "grad_x_grad", "grad_y_grad", "axis"}, {}, {}, {{"grad_out_grad", "grad_x_grad"}}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "subtract_double_grad");
}

void SubtractDoubleGrad_Op::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value y_, pir::Value grad_out_, pir::Value grad_x_grad_, pir::Value grad_y_grad_, int axis) {
  VLOG(4) << "Start build SubtractDoubleGrad_Op";



  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {y_, grad_out_, grad_x_grad_, grad_y_grad_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_axis = pir::Int32Attribute::get(pir::IrContext::Instance(), axis);
  argument.AddAttribute("axis", attr_axis);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType y = y_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)y;
  paddle::dialect::DenseTensorType grad_out = grad_out_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)grad_out;

  VLOG(4) << "Builder construction  dense_grad_out";
  paddle::dialect::IrTensor ir_tensor_grad_out(paddle::dialect::TransToPhiDataType(grad_out.dtype()),
                                                      grad_out.dims(),
                                                      grad_out.data_layout(),
                                                      grad_out.lod(),
                                                      grad_out.offset());
  VLOG(4) << "Builder construction  meta_grad_out";
  paddle::dialect::IrMetaTensor meta_grad_out(&ir_tensor_grad_out);
  paddle::dialect::IrTensor dense_grad_out_grad;
  paddle::dialect::IrMetaTensor meta_grad_out_grad(&dense_grad_out_grad);

  phi::UnchangedInferMeta(meta_grad_out, &meta_grad_out_grad);

  std::vector<pir::Type> argument_outputs;
  pir::Type grad_out_grad_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_grad_out_grad.dtype()), dense_grad_out_grad.dims(), dense_grad_out_grad.layout(), dense_grad_out_grad.lod(), dense_grad_out_grad.offset());
  argument_outputs.push_back(grad_out_grad_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void SubtractDoubleGrad_Op::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value y_, pir::Value grad_out_, pir::Value grad_x_grad_, pir::Value grad_y_grad_, pir::AttributeMap attributes) {
  VLOG(4) << "Start build SubtractDoubleGrad_Op";


  IR_ENFORCE(
      attributes.find("axis") != attributes.end(),
          "'axis' Attribute is expected for SubtractDoubleGrad_Op. ");
  int axis = attributes.at("axis").dyn_cast<pir::Int32Attribute>().data();


  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {y_, grad_out_, grad_x_grad_, grad_y_grad_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_axis = pir::Int32Attribute::get(pir::IrContext::Instance(), axis);
  argument.AddAttribute("axis", attr_axis);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType y = y_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)y;
  paddle::dialect::DenseTensorType grad_out = grad_out_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)grad_out;

  VLOG(4) << "Builder construction  dense_grad_out";
  paddle::dialect::IrTensor ir_tensor_grad_out(paddle::dialect::TransToPhiDataType(grad_out.dtype()),
                                                      grad_out.dims(),
                                                      grad_out.data_layout(),
                                                      grad_out.lod(),
                                                      grad_out.offset());
  VLOG(4) << "Builder construction  meta_grad_out";
  paddle::dialect::IrMetaTensor meta_grad_out(&ir_tensor_grad_out);
  paddle::dialect::IrTensor dense_grad_out_grad;
  paddle::dialect::IrMetaTensor meta_grad_out_grad(&dense_grad_out_grad);

  phi::UnchangedInferMeta(meta_grad_out, &meta_grad_out_grad);

  std::vector<pir::Type> argument_outputs;
  pir::Type grad_out_grad_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_grad_out_grad.dtype()), dense_grad_out_grad.dims(), dense_grad_out_grad.layout(), dense_grad_out_grad.lod(), dense_grad_out_grad.offset());
  argument_outputs.push_back(grad_out_grad_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void SubtractDoubleGrad_Op::VerifySig() {}

void SubtractDoubleGrad_Op::InferMeta( phi::InferMetaContext *infer_meta ) {
  auto fn = PD_INFER_META(phi::UnchangedInferMeta);
  fn(infer_meta);
}

phi::DataType SubtractDoubleGrad_Op::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: SubtractDoubleGrad_Op";
  


  // deal complex_promote
  if (framework::IsComplexType(expected_kernel_dtype)) {{
    // only promote inputsâ€™s types when contains complex input
    return tensor_dtype;
  }}

  return expected_kernel_dtype;
}

const char *SubtractGradOp::attributes_name[1] = { "axis" };

OpInfoTuple SubtractGradOp::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("x", "paddle::dialect::DenseTensorType", false, true, false, false), paddle::dialect::OpInputInfo("y", "paddle::dialect::DenseTensorType", false, true, false, false), paddle::dialect::OpInputInfo("out_grad", "paddle::dialect::DenseTensorType", false, false, false, true) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = { paddle::dialect::OpAttributeInfo("axis", "pir::Int32Attribute", "") };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("x_grad", "paddle::dialect::DenseTensorType", false, false), paddle::dialect::OpOutputInfo("y_grad", "paddle::dialect::DenseTensorType", false, false) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("GeneralBinaryGradInferMeta", {"x", "y"}, "subtract_grad", {"x", "y", "out_grad", "axis"}, {}, {}, {}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "subtract_grad");
}

void SubtractGradOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, pir::Value y_, pir::Value out_grad_, int axis) {
  VLOG(4) << "Start build SubtractGradOp";



  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_, y_, out_grad_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_axis = pir::Int32Attribute::get(pir::IrContext::Instance(), axis);
  argument.AddAttribute("axis", attr_axis);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;
  paddle::dialect::DenseTensorType y = y_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)y;
  paddle::dialect::DenseTensorType out_grad = out_grad_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)out_grad;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);

  VLOG(4) << "Builder construction  dense_y";
  paddle::dialect::IrTensor ir_tensor_y(paddle::dialect::TransToPhiDataType(y.dtype()),
                                                      y.dims(),
                                                      y.data_layout(),
                                                      y.lod(),
                                                      y.offset());
  VLOG(4) << "Builder construction  meta_y";
  paddle::dialect::IrMetaTensor meta_y(&ir_tensor_y);
  paddle::dialect::IrTensor dense_x_grad;
  paddle::dialect::IrMetaTensor meta_x_grad(&dense_x_grad);
  paddle::dialect::IrTensor dense_y_grad;
  paddle::dialect::IrMetaTensor meta_y_grad(&dense_y_grad);

  phi::GeneralBinaryGradInferMeta(meta_x, meta_y, &meta_x_grad, &meta_y_grad);

  std::vector<pir::Type> argument_outputs;
  pir::Type x_grad_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_x_grad.dtype()), dense_x_grad.dims(), dense_x_grad.layout(), dense_x_grad.lod(), dense_x_grad.offset());
  argument_outputs.push_back(x_grad_dense_tensor_type);

  pir::Type y_grad_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_y_grad.dtype()), dense_y_grad.dims(), dense_y_grad.layout(), dense_y_grad.lod(), dense_y_grad.offset());
  argument_outputs.push_back(y_grad_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void SubtractGradOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, pir::Value y_, pir::Value out_grad_, pir::AttributeMap attributes) {
  VLOG(4) << "Start build SubtractGradOp";


  IR_ENFORCE(
      attributes.find("axis") != attributes.end(),
          "'axis' Attribute is expected for SubtractGradOp. ");
  int axis = attributes.at("axis").dyn_cast<pir::Int32Attribute>().data();


  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_, y_, out_grad_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_axis = pir::Int32Attribute::get(pir::IrContext::Instance(), axis);
  argument.AddAttribute("axis", attr_axis);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;
  paddle::dialect::DenseTensorType y = y_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)y;
  paddle::dialect::DenseTensorType out_grad = out_grad_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)out_grad;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);

  VLOG(4) << "Builder construction  dense_y";
  paddle::dialect::IrTensor ir_tensor_y(paddle::dialect::TransToPhiDataType(y.dtype()),
                                                      y.dims(),
                                                      y.data_layout(),
                                                      y.lod(),
                                                      y.offset());
  VLOG(4) << "Builder construction  meta_y";
  paddle::dialect::IrMetaTensor meta_y(&ir_tensor_y);
  paddle::dialect::IrTensor dense_x_grad;
  paddle::dialect::IrMetaTensor meta_x_grad(&dense_x_grad);
  paddle::dialect::IrTensor dense_y_grad;
  paddle::dialect::IrMetaTensor meta_y_grad(&dense_y_grad);

  phi::GeneralBinaryGradInferMeta(meta_x, meta_y, &meta_x_grad, &meta_y_grad);

  std::vector<pir::Type> argument_outputs;
  pir::Type x_grad_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_x_grad.dtype()), dense_x_grad.dims(), dense_x_grad.layout(), dense_x_grad.lod(), dense_x_grad.offset());
  argument_outputs.push_back(x_grad_dense_tensor_type);

  pir::Type y_grad_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_y_grad.dtype()), dense_y_grad.dims(), dense_y_grad.layout(), dense_y_grad.lod(), dense_y_grad.offset());
  argument_outputs.push_back(y_grad_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void SubtractGradOp::VerifySig() {}

void SubtractGradOp::InferMeta( phi::InferMetaContext *infer_meta ) {
  auto fn = PD_INFER_META(phi::GeneralBinaryGradInferMeta);
  fn(infer_meta);
}

phi::DataType SubtractGradOp::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: SubtractGradOp";
  


  // deal complex_promote
  if (framework::IsComplexType(expected_kernel_dtype)) {{
    // only promote inputsâ€™s types when contains complex input
    return tensor_dtype;
  }}

  return expected_kernel_dtype;
}

const char *SubtractGrad_Op::attributes_name[1] = { "axis" };

OpInfoTuple SubtractGrad_Op::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("x", "paddle::dialect::DenseTensorType", false, true, false, false), paddle::dialect::OpInputInfo("y", "paddle::dialect::DenseTensorType", false, true, false, false), paddle::dialect::OpInputInfo("out_grad", "paddle::dialect::DenseTensorType", false, false, false, true) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = { paddle::dialect::OpAttributeInfo("axis", "pir::Int32Attribute", "") };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("x_grad", "paddle::dialect::DenseTensorType", false, false), paddle::dialect::OpOutputInfo("y_grad", "paddle::dialect::DenseTensorType", false, false) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("GeneralBinaryGradInferMeta", {"x", "y"}, "subtract_grad", {"x", "y", "out_grad", "axis"}, {}, {}, {{"x_grad", "out_grad"}}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "subtract_grad");
}

void SubtractGrad_Op::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, pir::Value y_, pir::Value out_grad_, int axis) {
  VLOG(4) << "Start build SubtractGrad_Op";



  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_, y_, out_grad_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_axis = pir::Int32Attribute::get(pir::IrContext::Instance(), axis);
  argument.AddAttribute("axis", attr_axis);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;
  paddle::dialect::DenseTensorType y = y_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)y;
  paddle::dialect::DenseTensorType out_grad = out_grad_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)out_grad;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);

  VLOG(4) << "Builder construction  dense_y";
  paddle::dialect::IrTensor ir_tensor_y(paddle::dialect::TransToPhiDataType(y.dtype()),
                                                      y.dims(),
                                                      y.data_layout(),
                                                      y.lod(),
                                                      y.offset());
  VLOG(4) << "Builder construction  meta_y";
  paddle::dialect::IrMetaTensor meta_y(&ir_tensor_y);
  paddle::dialect::IrTensor dense_x_grad;
  paddle::dialect::IrMetaTensor meta_x_grad(&dense_x_grad);
  paddle::dialect::IrTensor dense_y_grad;
  paddle::dialect::IrMetaTensor meta_y_grad(&dense_y_grad);

  phi::GeneralBinaryGradInferMeta(meta_x, meta_y, &meta_x_grad, &meta_y_grad);

  std::vector<pir::Type> argument_outputs;
  pir::Type x_grad_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_x_grad.dtype()), dense_x_grad.dims(), dense_x_grad.layout(), dense_x_grad.lod(), dense_x_grad.offset());
  argument_outputs.push_back(x_grad_dense_tensor_type);

  pir::Type y_grad_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_y_grad.dtype()), dense_y_grad.dims(), dense_y_grad.layout(), dense_y_grad.lod(), dense_y_grad.offset());
  argument_outputs.push_back(y_grad_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void SubtractGrad_Op::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, pir::Value y_, pir::Value out_grad_, pir::AttributeMap attributes) {
  VLOG(4) << "Start build SubtractGrad_Op";


  IR_ENFORCE(
      attributes.find("axis") != attributes.end(),
          "'axis' Attribute is expected for SubtractGrad_Op. ");
  int axis = attributes.at("axis").dyn_cast<pir::Int32Attribute>().data();


  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_, y_, out_grad_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_axis = pir::Int32Attribute::get(pir::IrContext::Instance(), axis);
  argument.AddAttribute("axis", attr_axis);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;
  paddle::dialect::DenseTensorType y = y_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)y;
  paddle::dialect::DenseTensorType out_grad = out_grad_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)out_grad;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);

  VLOG(4) << "Builder construction  dense_y";
  paddle::dialect::IrTensor ir_tensor_y(paddle::dialect::TransToPhiDataType(y.dtype()),
                                                      y.dims(),
                                                      y.data_layout(),
                                                      y.lod(),
                                                      y.offset());
  VLOG(4) << "Builder construction  meta_y";
  paddle::dialect::IrMetaTensor meta_y(&ir_tensor_y);
  paddle::dialect::IrTensor dense_x_grad;
  paddle::dialect::IrMetaTensor meta_x_grad(&dense_x_grad);
  paddle::dialect::IrTensor dense_y_grad;
  paddle::dialect::IrMetaTensor meta_y_grad(&dense_y_grad);

  phi::GeneralBinaryGradInferMeta(meta_x, meta_y, &meta_x_grad, &meta_y_grad);

  std::vector<pir::Type> argument_outputs;
  pir::Type x_grad_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_x_grad.dtype()), dense_x_grad.dims(), dense_x_grad.layout(), dense_x_grad.lod(), dense_x_grad.offset());
  argument_outputs.push_back(x_grad_dense_tensor_type);

  pir::Type y_grad_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_y_grad.dtype()), dense_y_grad.dims(), dense_y_grad.layout(), dense_y_grad.lod(), dense_y_grad.offset());
  argument_outputs.push_back(y_grad_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void SubtractGrad_Op::VerifySig() {}

void SubtractGrad_Op::InferMeta( phi::InferMetaContext *infer_meta ) {
  auto fn = PD_INFER_META(phi::GeneralBinaryGradInferMeta);
  fn(infer_meta);
}

phi::DataType SubtractGrad_Op::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: SubtractGrad_Op";
  


  // deal complex_promote
  if (framework::IsComplexType(expected_kernel_dtype)) {{
    // only promote inputsâ€™s types when contains complex input
    return tensor_dtype;
  }}

  return expected_kernel_dtype;
}

const char *SumDoubleGradOp::attributes_name[1] = { "keepdim" };

OpInfoTuple SumDoubleGradOp::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("grad_x_grad", "paddle::dialect::DenseTensorType", false, false, false, false), paddle::dialect::OpInputInfo("axis", "paddle::dialect::IntArrayAttribute", false, false, true, false) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = { paddle::dialect::OpAttributeInfo("keepdim", "pir::BoolAttribute", "") };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("grad_out_grad", "paddle::dialect::DenseTensorType", false, false) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("", {""}, "", {""}, {}, {}, {}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "sum_double_grad");
}

void SumDoubleGradOp::VerifySig() {}

phi::DataType SumDoubleGradOp::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: SumDoubleGradOp";
  


  return expected_kernel_dtype;
}

const char *SumGradOp::attributes_name[2] = { "keepdim", "reduce_all" };

OpInfoTuple SumGradOp::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("x", "paddle::dialect::DenseTensorType", false, true, false, false), paddle::dialect::OpInputInfo("out_grad", "paddle::dialect::DenseTensorType", false, false, false, true), paddle::dialect::OpInputInfo("axis", "paddle::dialect::IntArrayAttribute", false, false, true, false) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = { paddle::dialect::OpAttributeInfo("keepdim", "pir::BoolAttribute", ""), paddle::dialect::OpAttributeInfo("reduce_all", "pir::BoolAttribute", "") };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("x_grad", "paddle::dialect::DenseTensorType", false, false) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("UnchangedInferMeta", {"x"}, "sum_grad", {"x", "out_grad", "axis", "keepdim", "reduce_all"}, {}, {}, {}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "sum_grad");
}

void SumGradOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, pir::Value out_grad_, const std::vector<int64_t>& axis, bool keepdim, bool reduce_all) {
  VLOG(4) << "Start build SumGradOp";


  // Generate int_array mutable attribute: axis
  paddle::dialect::FullIntArrayOp full_axis_op = builder.Build<paddle::dialect::FullIntArrayOp>(axis, phi::DataType::INT64, phi::CPUPlace());
  pir::OpResult axis_ = full_axis_op->result(0);
    
  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_, out_grad_, axis_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_keepdim = pir::BoolAttribute::get(pir::IrContext::Instance(), keepdim);
  argument.AddAttribute("keepdim", attr_keepdim);
  pir::Attribute attr_reduce_all = pir::BoolAttribute::get(pir::IrContext::Instance(), reduce_all);
  argument.AddAttribute("reduce_all", attr_reduce_all);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;
  paddle::dialect::DenseTensorType out_grad = out_grad_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)out_grad;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);
  paddle::dialect::IrTensor dense_x_grad;
  paddle::dialect::IrMetaTensor meta_x_grad(&dense_x_grad);

  phi::UnchangedInferMeta(meta_x, &meta_x_grad);

  std::vector<pir::Type> argument_outputs;
  pir::Type x_grad_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_x_grad.dtype()), dense_x_grad.dims(), dense_x_grad.layout(), dense_x_grad.lod(), dense_x_grad.offset());
  argument_outputs.push_back(x_grad_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void SumGradOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, pir::Value out_grad_, pir::AttributeMap attributes) {
  VLOG(4) << "Start build SumGradOp";


  IR_ENFORCE(
      attributes.find("axis") != attributes.end(),
          "'axis' Attribute is expected for SumGradOp. ");
  std::vector<int64_t> axis = attributes.at("axis").dyn_cast<paddle::dialect::IntArrayAttribute>().data().GetData();

  IR_ENFORCE(
      attributes.find("keepdim") != attributes.end(),
          "'keepdim' Attribute is expected for SumGradOp. ");
  bool keepdim = attributes.at("keepdim").dyn_cast<pir::BoolAttribute>().data();

  IR_ENFORCE(
      attributes.find("reduce_all") != attributes.end(),
          "'reduce_all' Attribute is expected for SumGradOp. ");
  bool reduce_all = attributes.at("reduce_all").dyn_cast<pir::BoolAttribute>().data();

  // Generate int_array mutable attribute: axis
  paddle::dialect::FullIntArrayOp full_axis_op = builder.Build<paddle::dialect::FullIntArrayOp>(axis, phi::DataType::INT64, phi::CPUPlace());
  pir::OpResult axis_ = full_axis_op->result(0);
    
  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_, out_grad_, axis_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_keepdim = pir::BoolAttribute::get(pir::IrContext::Instance(), keepdim);
  argument.AddAttribute("keepdim", attr_keepdim);
  pir::Attribute attr_reduce_all = pir::BoolAttribute::get(pir::IrContext::Instance(), reduce_all);
  argument.AddAttribute("reduce_all", attr_reduce_all);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;
  paddle::dialect::DenseTensorType out_grad = out_grad_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)out_grad;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);
  paddle::dialect::IrTensor dense_x_grad;
  paddle::dialect::IrMetaTensor meta_x_grad(&dense_x_grad);

  phi::UnchangedInferMeta(meta_x, &meta_x_grad);

  std::vector<pir::Type> argument_outputs;
  pir::Type x_grad_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_x_grad.dtype()), dense_x_grad.dims(), dense_x_grad.layout(), dense_x_grad.lod(), dense_x_grad.offset());
  argument_outputs.push_back(x_grad_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void SumGradOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, pir::Value out_grad_, pir::Value axis_, bool keepdim, bool reduce_all) {
  VLOG(4) << "Start build SumGradOp";



  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_, out_grad_, axis_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_keepdim = pir::BoolAttribute::get(pir::IrContext::Instance(), keepdim);
  argument.AddAttribute("keepdim", attr_keepdim);
  pir::Attribute attr_reduce_all = pir::BoolAttribute::get(pir::IrContext::Instance(), reduce_all);
  argument.AddAttribute("reduce_all", attr_reduce_all);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;
  paddle::dialect::DenseTensorType out_grad = out_grad_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)out_grad;
  phi::IntArray axis;
  if (axis_.dyn_cast<pir::OpResult>() && axis_.dyn_cast<pir::OpResult>().owner()->isa<paddle::dialect::FullIntArrayOp>()) {
    axis = std::move(phi::IntArray(paddle::dialect::GetInt64Vector(
                          axis_.dyn_cast<pir::OpResult>().owner()
                          ->dyn_cast<paddle::dialect::FullIntArrayOp>()
                          .attribute("value"))));
  } else if (axis_.type().isa<pir::VectorType>()) {
    size_t axis_size = axis_.type().dyn_cast<pir::VectorType>().size();
    axis = std::move(phi::IntArray(std::vector<int64_t>(axis_size, -1)));
    axis.SetFromTensor(true);
  } else if (axis_.type().isa<paddle::dialect::DenseTensorType>()) {
    common::DDim axis_dim = axis_.type().dyn_cast<paddle::dialect::DenseTensorType>().dims();
    size_t axis_size = common::product(axis_dim);
    if (common::contain_unknown_dim(axis_dim)) {
      axis_size = 1;
    }
    axis = std::move(phi::IntArray(std::vector<int64_t>(axis_size, -1)));
    axis.SetFromTensor(true);
  } else {
    PADDLE_THROW(phi::errors::Unimplemented("Only support VectorType or DenseTensorType"));
  }


  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);
  paddle::dialect::IrTensor dense_x_grad;
  paddle::dialect::IrMetaTensor meta_x_grad(&dense_x_grad);

  phi::UnchangedInferMeta(meta_x, &meta_x_grad);

  std::vector<pir::Type> argument_outputs;
  pir::Type x_grad_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_x_grad.dtype()), dense_x_grad.dims(), dense_x_grad.layout(), dense_x_grad.lod(), dense_x_grad.offset());
  argument_outputs.push_back(x_grad_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void SumGradOp::VerifySig() {}

void SumGradOp::InferMeta( phi::InferMetaContext *infer_meta ) {
  auto fn = PD_INFER_META(phi::UnchangedInferMeta);
  fn(infer_meta);
}

phi::DataType SumGradOp::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: SumGradOp";
  


  return expected_kernel_dtype;
}

OpInfoTuple SwishGradOp::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("x", "paddle::dialect::DenseTensorType", false, false, false, false), paddle::dialect::OpInputInfo("out_grad", "paddle::dialect::DenseTensorType", false, false, false, false) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = {  };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("x_grad", "paddle::dialect::DenseTensorType", false, false) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("GeneralUnaryGradInferMeta", {"x"}, "swish_grad", {"x", "out_grad"}, {}, {}, {}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "swish_grad");
}

void SwishGradOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, pir::Value out_grad_) {
  VLOG(4) << "Start build SwishGradOp";



  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_, out_grad_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;
  paddle::dialect::DenseTensorType out_grad = out_grad_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)out_grad;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);
  paddle::dialect::IrTensor dense_x_grad;
  paddle::dialect::IrMetaTensor meta_x_grad(&dense_x_grad);

  phi::GeneralUnaryGradInferMeta(meta_x, &meta_x_grad);

  std::vector<pir::Type> argument_outputs;
  pir::Type x_grad_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_x_grad.dtype()), dense_x_grad.dims(), dense_x_grad.layout(), dense_x_grad.lod(), dense_x_grad.offset());
  argument_outputs.push_back(x_grad_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void SwishGradOp::VerifySig() {}

void SwishGradOp::InferMeta( phi::InferMetaContext *infer_meta ) {
  auto fn = PD_INFER_META(phi::GeneralUnaryGradInferMeta);
  fn(infer_meta);
}

phi::DataType SwishGradOp::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: SwishGradOp";
  


  return expected_kernel_dtype;
}

OpInfoTuple SwishGrad_Op::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("x", "paddle::dialect::DenseTensorType", false, false, false, false), paddle::dialect::OpInputInfo("out_grad", "paddle::dialect::DenseTensorType", false, false, false, false) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = {  };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("x_grad", "paddle::dialect::DenseTensorType", false, false) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("GeneralUnaryGradInferMeta", {"x"}, "swish_grad", {"x", "out_grad"}, {}, {}, {{"x_grad", "out_grad"}}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "swish_grad");
}

void SwishGrad_Op::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, pir::Value out_grad_) {
  VLOG(4) << "Start build SwishGrad_Op";



  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_, out_grad_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;
  paddle::dialect::DenseTensorType out_grad = out_grad_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)out_grad;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);
  paddle::dialect::IrTensor dense_x_grad;
  paddle::dialect::IrMetaTensor meta_x_grad(&dense_x_grad);

  phi::GeneralUnaryGradInferMeta(meta_x, &meta_x_grad);

  std::vector<pir::Type> argument_outputs;
  pir::Type x_grad_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_x_grad.dtype()), dense_x_grad.dims(), dense_x_grad.layout(), dense_x_grad.lod(), dense_x_grad.offset());
  argument_outputs.push_back(x_grad_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void SwishGrad_Op::VerifySig() {}

void SwishGrad_Op::InferMeta( phi::InferMetaContext *infer_meta ) {
  auto fn = PD_INFER_META(phi::GeneralUnaryGradInferMeta);
  fn(infer_meta);
}

phi::DataType SwishGrad_Op::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: SwishGrad_Op";
  


  return expected_kernel_dtype;
}

const char *SyncBatchNormGradOp::attributes_name[6] = { "momentum", "epsilon", "data_layout", "is_test", "use_global_stats", "trainable_statistics" };

OpInfoTuple SyncBatchNormGradOp::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("x", "paddle::dialect::DenseTensorType", false, false, false, false), paddle::dialect::OpInputInfo("scale", "paddle::dialect::DenseTensorType", false, false, false, false), paddle::dialect::OpInputInfo("bias", "paddle::dialect::DenseTensorType", false, false, false, false), paddle::dialect::OpInputInfo("saved_mean", "paddle::dialect::DenseTensorType", false, false, false, false), paddle::dialect::OpInputInfo("saved_variance", "paddle::dialect::DenseTensorType", false, false, false, false), paddle::dialect::OpInputInfo("reserve_space", "paddle::dialect::DenseTensorType", true, false, false, false), paddle::dialect::OpInputInfo("out_grad", "paddle::dialect::DenseTensorType", false, false, false, false) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = { paddle::dialect::OpAttributeInfo("momentum", "pir::FloatAttribute", ""), paddle::dialect::OpAttributeInfo("epsilon", "pir::FloatAttribute", ""), paddle::dialect::OpAttributeInfo("data_layout", "pir::StrAttribute", ""), paddle::dialect::OpAttributeInfo("is_test", "pir::BoolAttribute", ""), paddle::dialect::OpAttributeInfo("use_global_stats", "pir::BoolAttribute", ""), paddle::dialect::OpAttributeInfo("trainable_statistics", "pir::BoolAttribute", "") };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("x_grad", "paddle::dialect::DenseTensorType", false, false), paddle::dialect::OpOutputInfo("scale_grad", "paddle::dialect::DenseTensorType", false, false), paddle::dialect::OpOutputInfo("bias_grad", "paddle::dialect::DenseTensorType", false, false) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("GeneralTernaryGradInferMeta", {"x", "scale", "bias"}, "sync_batch_norm_grad", {"x", "scale", "bias", "saved_mean", "saved_variance", "reserve_space", "out_grad", "momentum", "epsilon", "data_layout", "is_test", "use_global_stats", "trainable_statistics"}, {"out_grad"}, {}, {}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "sync_batch_norm_grad");
}

void SyncBatchNormGradOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, pir::Value scale_, pir::Value bias_, pir::Value saved_mean_, pir::Value saved_variance_, pir::Value reserve_space_, pir::Value out_grad_, float momentum, float epsilon, const std::string& data_layout, bool is_test, bool use_global_stats, bool trainable_statistics) {
  VLOG(4) << "Start build SyncBatchNormGradOp";



  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_, scale_, bias_, saved_mean_, saved_variance_, reserve_space_, out_grad_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_momentum = pir::FloatAttribute::get(pir::IrContext::Instance(), momentum);
  argument.AddAttribute("momentum", attr_momentum);
  pir::Attribute attr_epsilon = pir::FloatAttribute::get(pir::IrContext::Instance(), epsilon);
  argument.AddAttribute("epsilon", attr_epsilon);
  pir::Attribute attr_data_layout = pir::StrAttribute::get(pir::IrContext::Instance(), data_layout);
  argument.AddAttribute("data_layout", attr_data_layout);
  pir::Attribute attr_is_test = pir::BoolAttribute::get(pir::IrContext::Instance(), is_test);
  argument.AddAttribute("is_test", attr_is_test);
  pir::Attribute attr_use_global_stats = pir::BoolAttribute::get(pir::IrContext::Instance(), use_global_stats);
  argument.AddAttribute("use_global_stats", attr_use_global_stats);
  pir::Attribute attr_trainable_statistics = pir::BoolAttribute::get(pir::IrContext::Instance(), trainable_statistics);
  argument.AddAttribute("trainable_statistics", attr_trainable_statistics);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;
  paddle::dialect::DenseTensorType scale = scale_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)scale;
  paddle::dialect::DenseTensorType bias = bias_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)bias;
  paddle::dialect::DenseTensorType saved_mean = saved_mean_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)saved_mean;
  paddle::dialect::DenseTensorType saved_variance = saved_variance_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)saved_variance;
  paddle::dialect::DenseTensorType out_grad = out_grad_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)out_grad;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);

  VLOG(4) << "Builder construction  dense_scale";
  paddle::dialect::IrTensor ir_tensor_scale(paddle::dialect::TransToPhiDataType(scale.dtype()),
                                                      scale.dims(),
                                                      scale.data_layout(),
                                                      scale.lod(),
                                                      scale.offset());
  VLOG(4) << "Builder construction  meta_scale";
  paddle::dialect::IrMetaTensor meta_scale(&ir_tensor_scale);

  VLOG(4) << "Builder construction  dense_bias";
  paddle::dialect::IrTensor ir_tensor_bias(paddle::dialect::TransToPhiDataType(bias.dtype()),
                                                      bias.dims(),
                                                      bias.data_layout(),
                                                      bias.lod(),
                                                      bias.offset());
  VLOG(4) << "Builder construction  meta_bias";
  paddle::dialect::IrMetaTensor meta_bias(&ir_tensor_bias);
  paddle::dialect::IrTensor dense_x_grad;
  paddle::dialect::IrMetaTensor meta_x_grad(&dense_x_grad);
  paddle::dialect::IrTensor dense_scale_grad;
  paddle::dialect::IrMetaTensor meta_scale_grad(&dense_scale_grad);
  paddle::dialect::IrTensor dense_bias_grad;
  paddle::dialect::IrMetaTensor meta_bias_grad(&dense_bias_grad);

  phi::GeneralTernaryGradInferMeta(meta_x, meta_scale, meta_bias, &meta_x_grad, &meta_scale_grad, &meta_bias_grad);

  std::vector<pir::Type> argument_outputs;
  pir::Type x_grad_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_x_grad.dtype()), dense_x_grad.dims(), dense_x_grad.layout(), dense_x_grad.lod(), dense_x_grad.offset());
  argument_outputs.push_back(x_grad_dense_tensor_type);

  pir::Type scale_grad_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_scale_grad.dtype()), dense_scale_grad.dims(), dense_scale_grad.layout(), dense_scale_grad.lod(), dense_scale_grad.offset());
  argument_outputs.push_back(scale_grad_dense_tensor_type);

  pir::Type bias_grad_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_bias_grad.dtype()), dense_bias_grad.dims(), dense_bias_grad.layout(), dense_bias_grad.lod(), dense_bias_grad.offset());
  argument_outputs.push_back(bias_grad_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void SyncBatchNormGradOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, pir::Value scale_, pir::Value bias_, pir::Value saved_mean_, pir::Value saved_variance_, pir::Value reserve_space_, pir::Value out_grad_, pir::AttributeMap attributes) {
  VLOG(4) << "Start build SyncBatchNormGradOp";


  IR_ENFORCE(
      attributes.find("momentum") != attributes.end(),
          "'momentum' Attribute is expected for SyncBatchNormGradOp. ");
  float momentum = attributes.at("momentum").dyn_cast<pir::FloatAttribute>().data();

  IR_ENFORCE(
      attributes.find("epsilon") != attributes.end(),
          "'epsilon' Attribute is expected for SyncBatchNormGradOp. ");
  float epsilon = attributes.at("epsilon").dyn_cast<pir::FloatAttribute>().data();

  IR_ENFORCE(
      attributes.find("data_layout") != attributes.end(),
          "'data_layout' Attribute is expected for SyncBatchNormGradOp. ");
  std::string data_layout = attributes.at("data_layout").dyn_cast<pir::StrAttribute>().AsString();

  IR_ENFORCE(
      attributes.find("is_test") != attributes.end(),
          "'is_test' Attribute is expected for SyncBatchNormGradOp. ");
  bool is_test = attributes.at("is_test").dyn_cast<pir::BoolAttribute>().data();

  IR_ENFORCE(
      attributes.find("use_global_stats") != attributes.end(),
          "'use_global_stats' Attribute is expected for SyncBatchNormGradOp. ");
  bool use_global_stats = attributes.at("use_global_stats").dyn_cast<pir::BoolAttribute>().data();

  IR_ENFORCE(
      attributes.find("trainable_statistics") != attributes.end(),
          "'trainable_statistics' Attribute is expected for SyncBatchNormGradOp. ");
  bool trainable_statistics = attributes.at("trainable_statistics").dyn_cast<pir::BoolAttribute>().data();


  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_, scale_, bias_, saved_mean_, saved_variance_, reserve_space_, out_grad_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_momentum = pir::FloatAttribute::get(pir::IrContext::Instance(), momentum);
  argument.AddAttribute("momentum", attr_momentum);
  pir::Attribute attr_epsilon = pir::FloatAttribute::get(pir::IrContext::Instance(), epsilon);
  argument.AddAttribute("epsilon", attr_epsilon);
  pir::Attribute attr_data_layout = pir::StrAttribute::get(pir::IrContext::Instance(), data_layout);
  argument.AddAttribute("data_layout", attr_data_layout);
  pir::Attribute attr_is_test = pir::BoolAttribute::get(pir::IrContext::Instance(), is_test);
  argument.AddAttribute("is_test", attr_is_test);
  pir::Attribute attr_use_global_stats = pir::BoolAttribute::get(pir::IrContext::Instance(), use_global_stats);
  argument.AddAttribute("use_global_stats", attr_use_global_stats);
  pir::Attribute attr_trainable_statistics = pir::BoolAttribute::get(pir::IrContext::Instance(), trainable_statistics);
  argument.AddAttribute("trainable_statistics", attr_trainable_statistics);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;
  paddle::dialect::DenseTensorType scale = scale_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)scale;
  paddle::dialect::DenseTensorType bias = bias_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)bias;
  paddle::dialect::DenseTensorType saved_mean = saved_mean_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)saved_mean;
  paddle::dialect::DenseTensorType saved_variance = saved_variance_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)saved_variance;
  paddle::dialect::DenseTensorType out_grad = out_grad_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)out_grad;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);

  VLOG(4) << "Builder construction  dense_scale";
  paddle::dialect::IrTensor ir_tensor_scale(paddle::dialect::TransToPhiDataType(scale.dtype()),
                                                      scale.dims(),
                                                      scale.data_layout(),
                                                      scale.lod(),
                                                      scale.offset());
  VLOG(4) << "Builder construction  meta_scale";
  paddle::dialect::IrMetaTensor meta_scale(&ir_tensor_scale);

  VLOG(4) << "Builder construction  dense_bias";
  paddle::dialect::IrTensor ir_tensor_bias(paddle::dialect::TransToPhiDataType(bias.dtype()),
                                                      bias.dims(),
                                                      bias.data_layout(),
                                                      bias.lod(),
                                                      bias.offset());
  VLOG(4) << "Builder construction  meta_bias";
  paddle::dialect::IrMetaTensor meta_bias(&ir_tensor_bias);
  paddle::dialect::IrTensor dense_x_grad;
  paddle::dialect::IrMetaTensor meta_x_grad(&dense_x_grad);
  paddle::dialect::IrTensor dense_scale_grad;
  paddle::dialect::IrMetaTensor meta_scale_grad(&dense_scale_grad);
  paddle::dialect::IrTensor dense_bias_grad;
  paddle::dialect::IrMetaTensor meta_bias_grad(&dense_bias_grad);

  phi::GeneralTernaryGradInferMeta(meta_x, meta_scale, meta_bias, &meta_x_grad, &meta_scale_grad, &meta_bias_grad);

  std::vector<pir::Type> argument_outputs;
  pir::Type x_grad_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_x_grad.dtype()), dense_x_grad.dims(), dense_x_grad.layout(), dense_x_grad.lod(), dense_x_grad.offset());
  argument_outputs.push_back(x_grad_dense_tensor_type);

  pir::Type scale_grad_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_scale_grad.dtype()), dense_scale_grad.dims(), dense_scale_grad.layout(), dense_scale_grad.lod(), dense_scale_grad.offset());
  argument_outputs.push_back(scale_grad_dense_tensor_type);

  pir::Type bias_grad_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_bias_grad.dtype()), dense_bias_grad.dims(), dense_bias_grad.layout(), dense_bias_grad.lod(), dense_bias_grad.offset());
  argument_outputs.push_back(bias_grad_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void SyncBatchNormGradOp::VerifySig() {}

void SyncBatchNormGradOp::InferMeta( phi::InferMetaContext *infer_meta ) {
  auto fn = PD_INFER_META(phi::GeneralTernaryGradInferMeta);
  fn(infer_meta);
}

phi::DataType SyncBatchNormGradOp::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: SyncBatchNormGradOp";
  


  return expected_kernel_dtype;
}

OpInfoTuple TileDoubleGradOp::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("grad_x_grad", "paddle::dialect::DenseTensorType", false, false, false, false), paddle::dialect::OpInputInfo("repeat_times", "paddle::dialect::IntArrayAttribute", false, false, true, false) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = {  };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("grad_out_grad", "paddle::dialect::DenseTensorType", false, false) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("", {""}, "", {""}, {}, {}, {}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "tile_double_grad");
}

void TileDoubleGradOp::VerifySig() {}

phi::DataType TileDoubleGradOp::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: TileDoubleGradOp";
  


  return expected_kernel_dtype;
}

OpInfoTuple TileGradOp::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("x", "paddle::dialect::DenseTensorType", false, true, false, false), paddle::dialect::OpInputInfo("out_grad", "paddle::dialect::DenseTensorType", false, false, false, true), paddle::dialect::OpInputInfo("repeat_times", "paddle::dialect::IntArrayAttribute", false, false, true, false) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = {  };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("x_grad", "paddle::dialect::DenseTensorType", false, false) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("UnchangedInferMeta", {"x"}, "tile_grad", {"x", "out_grad", "repeat_times"}, {}, {}, {}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "tile_grad");
}

void TileGradOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, pir::Value out_grad_, const std::vector<int64_t>& repeat_times) {
  VLOG(4) << "Start build TileGradOp";


  // Generate int_array mutable attribute: repeat_times
  paddle::dialect::FullIntArrayOp full_repeat_times_op = builder.Build<paddle::dialect::FullIntArrayOp>(repeat_times, phi::DataType::INT64, phi::CPUPlace());
  pir::OpResult repeat_times_ = full_repeat_times_op->result(0);
    
  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_, out_grad_, repeat_times_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;
  paddle::dialect::DenseTensorType out_grad = out_grad_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)out_grad;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);
  paddle::dialect::IrTensor dense_x_grad;
  paddle::dialect::IrMetaTensor meta_x_grad(&dense_x_grad);

  phi::UnchangedInferMeta(meta_x, &meta_x_grad);

  std::vector<pir::Type> argument_outputs;
  pir::Type x_grad_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_x_grad.dtype()), dense_x_grad.dims(), dense_x_grad.layout(), dense_x_grad.lod(), dense_x_grad.offset());
  argument_outputs.push_back(x_grad_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void TileGradOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, pir::Value out_grad_, pir::AttributeMap attributes) {
  VLOG(4) << "Start build TileGradOp";


  IR_ENFORCE(
      attributes.find("repeat_times") != attributes.end(),
          "'repeat_times' Attribute is expected for TileGradOp. ");
  std::vector<int64_t> repeat_times = attributes.at("repeat_times").dyn_cast<paddle::dialect::IntArrayAttribute>().data().GetData();

  // Generate int_array mutable attribute: repeat_times
  paddle::dialect::FullIntArrayOp full_repeat_times_op = builder.Build<paddle::dialect::FullIntArrayOp>(repeat_times, phi::DataType::INT64, phi::CPUPlace());
  pir::OpResult repeat_times_ = full_repeat_times_op->result(0);
    
  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_, out_grad_, repeat_times_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;
  paddle::dialect::DenseTensorType out_grad = out_grad_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)out_grad;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);
  paddle::dialect::IrTensor dense_x_grad;
  paddle::dialect::IrMetaTensor meta_x_grad(&dense_x_grad);

  phi::UnchangedInferMeta(meta_x, &meta_x_grad);

  std::vector<pir::Type> argument_outputs;
  pir::Type x_grad_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_x_grad.dtype()), dense_x_grad.dims(), dense_x_grad.layout(), dense_x_grad.lod(), dense_x_grad.offset());
  argument_outputs.push_back(x_grad_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void TileGradOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, pir::Value out_grad_, pir::Value repeat_times_) {
  VLOG(4) << "Start build TileGradOp";



  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_, out_grad_, repeat_times_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;
  paddle::dialect::DenseTensorType out_grad = out_grad_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)out_grad;
  phi::IntArray repeat_times;
  if (repeat_times_.dyn_cast<pir::OpResult>() && repeat_times_.dyn_cast<pir::OpResult>().owner()->isa<paddle::dialect::FullIntArrayOp>()) {
    repeat_times = std::move(phi::IntArray(paddle::dialect::GetInt64Vector(
                          repeat_times_.dyn_cast<pir::OpResult>().owner()
                          ->dyn_cast<paddle::dialect::FullIntArrayOp>()
                          .attribute("value"))));
  } else if (repeat_times_.type().isa<pir::VectorType>()) {
    size_t repeat_times_size = repeat_times_.type().dyn_cast<pir::VectorType>().size();
    repeat_times = std::move(phi::IntArray(std::vector<int64_t>(repeat_times_size, -1)));
    repeat_times.SetFromTensor(true);
  } else if (repeat_times_.type().isa<paddle::dialect::DenseTensorType>()) {
    common::DDim repeat_times_dim = repeat_times_.type().dyn_cast<paddle::dialect::DenseTensorType>().dims();
    size_t repeat_times_size = common::product(repeat_times_dim);
    if (common::contain_unknown_dim(repeat_times_dim)) {
      repeat_times_size = 1;
    }
    repeat_times = std::move(phi::IntArray(std::vector<int64_t>(repeat_times_size, -1)));
    repeat_times.SetFromTensor(true);
  } else {
    PADDLE_THROW(phi::errors::Unimplemented("Only support VectorType or DenseTensorType"));
  }


  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);
  paddle::dialect::IrTensor dense_x_grad;
  paddle::dialect::IrMetaTensor meta_x_grad(&dense_x_grad);

  phi::UnchangedInferMeta(meta_x, &meta_x_grad);

  std::vector<pir::Type> argument_outputs;
  pir::Type x_grad_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_x_grad.dtype()), dense_x_grad.dims(), dense_x_grad.layout(), dense_x_grad.lod(), dense_x_grad.offset());
  argument_outputs.push_back(x_grad_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void TileGradOp::VerifySig() {}

void TileGradOp::InferMeta( phi::InferMetaContext *infer_meta ) {
  auto fn = PD_INFER_META(phi::UnchangedInferMeta);
  fn(infer_meta);
}

phi::DataType TileGradOp::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: TileGradOp";
  


  return expected_kernel_dtype;
}

const char *TransLayoutGradOp::attributes_name[1] = { "perm" };

OpInfoTuple TransLayoutGradOp::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("x", "paddle::dialect::DenseTensorType", false, false, false, false), paddle::dialect::OpInputInfo("out_grad", "paddle::dialect::DenseTensorType", false, false, false, false) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = { paddle::dialect::OpAttributeInfo("perm", "pir::ArrayAttribute<pir::Int32Attribute>", "") };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("x_grad", "paddle::dialect::DenseTensorType", false, false) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("TransLayoutGradInferMeta", {"x", "out_grad", "perm"}, "trans_layout_grad", {"x", "out_grad", "perm"}, {}, {}, {}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "trans_layout_grad");
}

void TransLayoutGradOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, pir::Value out_grad_, const std::vector<int>& perm) {
  VLOG(4) << "Start build TransLayoutGradOp";



  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_, out_grad_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  std::vector<pir::Attribute> vec_perm;
  for (size_t i = 0; i < static_cast<size_t>(perm.size()); i++) {
      pir::Attribute attr_perm = pir::Int32Attribute::get(pir::IrContext::Instance(), perm[i]);

    vec_perm.push_back(attr_perm);
  }
  pir::Attribute attr_perm = pir::ArrayAttribute::get(pir::IrContext::Instance(), vec_perm);
  argument.AddAttribute("perm", attr_perm);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;
  paddle::dialect::DenseTensorType out_grad = out_grad_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)out_grad;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);

  VLOG(4) << "Builder construction  dense_out_grad";
  paddle::dialect::IrTensor ir_tensor_out_grad(paddle::dialect::TransToPhiDataType(out_grad.dtype()),
                                                      out_grad.dims(),
                                                      out_grad.data_layout(),
                                                      out_grad.lod(),
                                                      out_grad.offset());
  VLOG(4) << "Builder construction  meta_out_grad";
  paddle::dialect::IrMetaTensor meta_out_grad(&ir_tensor_out_grad);
  paddle::dialect::IrTensor dense_x_grad;
  paddle::dialect::IrMetaTensor meta_x_grad(&dense_x_grad);

  phi::TransLayoutGradInferMeta(meta_x, meta_out_grad, perm, &meta_x_grad);

  std::vector<pir::Type> argument_outputs;
  pir::Type x_grad_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_x_grad.dtype()), dense_x_grad.dims(), dense_x_grad.layout(), dense_x_grad.lod(), dense_x_grad.offset());
  argument_outputs.push_back(x_grad_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void TransLayoutGradOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, pir::Value out_grad_, pir::AttributeMap attributes) {
  VLOG(4) << "Start build TransLayoutGradOp";


  IR_ENFORCE(
      attributes.find("perm") != attributes.end(),
          "'perm' Attribute is expected for TransLayoutGradOp. ");
  std::vector<int> perm;
  for (size_t i = 0; i < attributes.at("perm").dyn_cast<pir::ArrayAttribute>().size(); i++) {
    perm.push_back(attributes.at("perm").dyn_cast<pir::ArrayAttribute>().at(i).dyn_cast<pir::Int32Attribute>().data());
  }


  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_, out_grad_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  std::vector<pir::Attribute> vec_perm;
  for (size_t i = 0; i < static_cast<size_t>(perm.size()); i++) {
      pir::Attribute attr_perm = pir::Int32Attribute::get(pir::IrContext::Instance(), perm[i]);

    vec_perm.push_back(attr_perm);
  }
  pir::Attribute attr_perm = pir::ArrayAttribute::get(pir::IrContext::Instance(), vec_perm);
  argument.AddAttribute("perm", attr_perm);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;
  paddle::dialect::DenseTensorType out_grad = out_grad_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)out_grad;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);

  VLOG(4) << "Builder construction  dense_out_grad";
  paddle::dialect::IrTensor ir_tensor_out_grad(paddle::dialect::TransToPhiDataType(out_grad.dtype()),
                                                      out_grad.dims(),
                                                      out_grad.data_layout(),
                                                      out_grad.lod(),
                                                      out_grad.offset());
  VLOG(4) << "Builder construction  meta_out_grad";
  paddle::dialect::IrMetaTensor meta_out_grad(&ir_tensor_out_grad);
  paddle::dialect::IrTensor dense_x_grad;
  paddle::dialect::IrMetaTensor meta_x_grad(&dense_x_grad);

  phi::TransLayoutGradInferMeta(meta_x, meta_out_grad, perm, &meta_x_grad);

  std::vector<pir::Type> argument_outputs;
  pir::Type x_grad_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_x_grad.dtype()), dense_x_grad.dims(), dense_x_grad.layout(), dense_x_grad.lod(), dense_x_grad.offset());
  argument_outputs.push_back(x_grad_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void TransLayoutGradOp::VerifySig() {}

void TransLayoutGradOp::InferMeta( phi::InferMetaContext *infer_meta ) {
  auto fn = PD_INFER_META(phi::TransLayoutGradInferMeta);
  fn(infer_meta);
}

phi::DataType TransLayoutGradOp::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: TransLayoutGradOp";
  


  return expected_kernel_dtype;
}

const char *TransposeDoubleGradOp::attributes_name[1] = { "perm" };

OpInfoTuple TransposeDoubleGradOp::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("grad_x_grad", "paddle::dialect::DenseTensorType", false, false, false, false) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = { paddle::dialect::OpAttributeInfo("perm", "pir::ArrayAttribute<pir::Int32Attribute>", "") };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("grad_out_grad", "paddle::dialect::DenseTensorType", false, false) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("", {""}, "", {""}, {}, {}, {}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "transpose_double_grad");
}

void TransposeDoubleGradOp::VerifySig() {}

phi::DataType TransposeDoubleGradOp::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: TransposeDoubleGradOp";
  


  return expected_kernel_dtype;
}

const char *TransposeGradOp::attributes_name[1] = { "perm" };

OpInfoTuple TransposeGradOp::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("out_grad", "paddle::dialect::DenseTensorType", false, false, false, true) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = { paddle::dialect::OpAttributeInfo("perm", "pir::ArrayAttribute<pir::Int32Attribute>", "") };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("x_grad", "paddle::dialect::DenseTensorType", false, false) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("TransposeGradInferMeta", {"out_grad", "perm"}, "transpose_grad", {"out_grad", "perm"}, {}, {}, {}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "transpose_grad");
}

void TransposeGradOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value out_grad_, const std::vector<int>& perm) {
  VLOG(4) << "Start build TransposeGradOp";



  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {out_grad_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  std::vector<pir::Attribute> vec_perm;
  for (size_t i = 0; i < static_cast<size_t>(perm.size()); i++) {
      pir::Attribute attr_perm = pir::Int32Attribute::get(pir::IrContext::Instance(), perm[i]);

    vec_perm.push_back(attr_perm);
  }
  pir::Attribute attr_perm = pir::ArrayAttribute::get(pir::IrContext::Instance(), vec_perm);
  argument.AddAttribute("perm", attr_perm);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType out_grad = out_grad_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)out_grad;

  VLOG(4) << "Builder construction  dense_out_grad";
  paddle::dialect::IrTensor ir_tensor_out_grad(paddle::dialect::TransToPhiDataType(out_grad.dtype()),
                                                      out_grad.dims(),
                                                      out_grad.data_layout(),
                                                      out_grad.lod(),
                                                      out_grad.offset());
  VLOG(4) << "Builder construction  meta_out_grad";
  paddle::dialect::IrMetaTensor meta_out_grad(&ir_tensor_out_grad);
  paddle::dialect::IrTensor dense_x_grad;
  paddle::dialect::IrMetaTensor meta_x_grad(&dense_x_grad);

  phi::TransposeGradInferMeta(meta_out_grad, perm, &meta_x_grad);

  std::vector<pir::Type> argument_outputs;
  pir::Type x_grad_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_x_grad.dtype()), dense_x_grad.dims(), dense_x_grad.layout(), dense_x_grad.lod(), dense_x_grad.offset());
  argument_outputs.push_back(x_grad_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void TransposeGradOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value out_grad_, pir::AttributeMap attributes) {
  VLOG(4) << "Start build TransposeGradOp";


  IR_ENFORCE(
      attributes.find("perm") != attributes.end(),
          "'perm' Attribute is expected for TransposeGradOp. ");
  std::vector<int> perm;
  for (size_t i = 0; i < attributes.at("perm").dyn_cast<pir::ArrayAttribute>().size(); i++) {
    perm.push_back(attributes.at("perm").dyn_cast<pir::ArrayAttribute>().at(i).dyn_cast<pir::Int32Attribute>().data());
  }


  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {out_grad_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  std::vector<pir::Attribute> vec_perm;
  for (size_t i = 0; i < static_cast<size_t>(perm.size()); i++) {
      pir::Attribute attr_perm = pir::Int32Attribute::get(pir::IrContext::Instance(), perm[i]);

    vec_perm.push_back(attr_perm);
  }
  pir::Attribute attr_perm = pir::ArrayAttribute::get(pir::IrContext::Instance(), vec_perm);
  argument.AddAttribute("perm", attr_perm);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType out_grad = out_grad_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)out_grad;

  VLOG(4) << "Builder construction  dense_out_grad";
  paddle::dialect::IrTensor ir_tensor_out_grad(paddle::dialect::TransToPhiDataType(out_grad.dtype()),
                                                      out_grad.dims(),
                                                      out_grad.data_layout(),
                                                      out_grad.lod(),
                                                      out_grad.offset());
  VLOG(4) << "Builder construction  meta_out_grad";
  paddle::dialect::IrMetaTensor meta_out_grad(&ir_tensor_out_grad);
  paddle::dialect::IrTensor dense_x_grad;
  paddle::dialect::IrMetaTensor meta_x_grad(&dense_x_grad);

  phi::TransposeGradInferMeta(meta_out_grad, perm, &meta_x_grad);

  std::vector<pir::Type> argument_outputs;
  pir::Type x_grad_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_x_grad.dtype()), dense_x_grad.dims(), dense_x_grad.layout(), dense_x_grad.lod(), dense_x_grad.offset());
  argument_outputs.push_back(x_grad_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void TransposeGradOp::VerifySig() {}

void TransposeGradOp::InferMeta( phi::InferMetaContext *infer_meta ) {
  auto fn = PD_INFER_META(phi::TransposeGradInferMeta);
  fn(infer_meta);
}

phi::DataType TransposeGradOp::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: TransposeGradOp";
  


  return expected_kernel_dtype;
}

const char *TrilGradOp::attributes_name[1] = { "diagonal" };

OpInfoTuple TrilGradOp::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("out_grad", "paddle::dialect::DenseTensorType", false, false, false, false) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = { paddle::dialect::OpAttributeInfo("diagonal", "pir::Int32Attribute", "") };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("x_grad", "paddle::dialect::DenseTensorType", false, false) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("UnchangedInferMeta", {"out_grad"}, "tril_grad", {"out_grad", "diagonal"}, {}, {}, {}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "tril_grad");
}

void TrilGradOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value out_grad_, int diagonal) {
  VLOG(4) << "Start build TrilGradOp";



  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {out_grad_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_diagonal = pir::Int32Attribute::get(pir::IrContext::Instance(), diagonal);
  argument.AddAttribute("diagonal", attr_diagonal);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType out_grad = out_grad_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)out_grad;

  VLOG(4) << "Builder construction  dense_out_grad";
  paddle::dialect::IrTensor ir_tensor_out_grad(paddle::dialect::TransToPhiDataType(out_grad.dtype()),
                                                      out_grad.dims(),
                                                      out_grad.data_layout(),
                                                      out_grad.lod(),
                                                      out_grad.offset());
  VLOG(4) << "Builder construction  meta_out_grad";
  paddle::dialect::IrMetaTensor meta_out_grad(&ir_tensor_out_grad);
  paddle::dialect::IrTensor dense_x_grad;
  paddle::dialect::IrMetaTensor meta_x_grad(&dense_x_grad);

  phi::UnchangedInferMeta(meta_out_grad, &meta_x_grad);

  std::vector<pir::Type> argument_outputs;
  pir::Type x_grad_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_x_grad.dtype()), dense_x_grad.dims(), dense_x_grad.layout(), dense_x_grad.lod(), dense_x_grad.offset());
  argument_outputs.push_back(x_grad_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void TrilGradOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value out_grad_, pir::AttributeMap attributes) {
  VLOG(4) << "Start build TrilGradOp";


  IR_ENFORCE(
      attributes.find("diagonal") != attributes.end(),
          "'diagonal' Attribute is expected for TrilGradOp. ");
  int diagonal = attributes.at("diagonal").dyn_cast<pir::Int32Attribute>().data();


  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {out_grad_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_diagonal = pir::Int32Attribute::get(pir::IrContext::Instance(), diagonal);
  argument.AddAttribute("diagonal", attr_diagonal);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType out_grad = out_grad_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)out_grad;

  VLOG(4) << "Builder construction  dense_out_grad";
  paddle::dialect::IrTensor ir_tensor_out_grad(paddle::dialect::TransToPhiDataType(out_grad.dtype()),
                                                      out_grad.dims(),
                                                      out_grad.data_layout(),
                                                      out_grad.lod(),
                                                      out_grad.offset());
  VLOG(4) << "Builder construction  meta_out_grad";
  paddle::dialect::IrMetaTensor meta_out_grad(&ir_tensor_out_grad);
  paddle::dialect::IrTensor dense_x_grad;
  paddle::dialect::IrMetaTensor meta_x_grad(&dense_x_grad);

  phi::UnchangedInferMeta(meta_out_grad, &meta_x_grad);

  std::vector<pir::Type> argument_outputs;
  pir::Type x_grad_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_x_grad.dtype()), dense_x_grad.dims(), dense_x_grad.layout(), dense_x_grad.lod(), dense_x_grad.offset());
  argument_outputs.push_back(x_grad_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void TrilGradOp::VerifySig() {}

void TrilGradOp::InferMeta( phi::InferMetaContext *infer_meta ) {
  auto fn = PD_INFER_META(phi::UnchangedInferMeta);
  fn(infer_meta);
}

phi::DataType TrilGradOp::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: TrilGradOp";
  


  return expected_kernel_dtype;
}

const char *TriuGradOp::attributes_name[1] = { "diagonal" };

OpInfoTuple TriuGradOp::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("out_grad", "paddle::dialect::DenseTensorType", false, false, false, false) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = { paddle::dialect::OpAttributeInfo("diagonal", "pir::Int32Attribute", "") };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("x_grad", "paddle::dialect::DenseTensorType", false, false) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("UnchangedInferMeta", {"out_grad"}, "triu_grad", {"out_grad", "diagonal"}, {}, {}, {}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "triu_grad");
}

void TriuGradOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value out_grad_, int diagonal) {
  VLOG(4) << "Start build TriuGradOp";



  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {out_grad_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_diagonal = pir::Int32Attribute::get(pir::IrContext::Instance(), diagonal);
  argument.AddAttribute("diagonal", attr_diagonal);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType out_grad = out_grad_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)out_grad;

  VLOG(4) << "Builder construction  dense_out_grad";
  paddle::dialect::IrTensor ir_tensor_out_grad(paddle::dialect::TransToPhiDataType(out_grad.dtype()),
                                                      out_grad.dims(),
                                                      out_grad.data_layout(),
                                                      out_grad.lod(),
                                                      out_grad.offset());
  VLOG(4) << "Builder construction  meta_out_grad";
  paddle::dialect::IrMetaTensor meta_out_grad(&ir_tensor_out_grad);
  paddle::dialect::IrTensor dense_x_grad;
  paddle::dialect::IrMetaTensor meta_x_grad(&dense_x_grad);

  phi::UnchangedInferMeta(meta_out_grad, &meta_x_grad);

  std::vector<pir::Type> argument_outputs;
  pir::Type x_grad_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_x_grad.dtype()), dense_x_grad.dims(), dense_x_grad.layout(), dense_x_grad.lod(), dense_x_grad.offset());
  argument_outputs.push_back(x_grad_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void TriuGradOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value out_grad_, pir::AttributeMap attributes) {
  VLOG(4) << "Start build TriuGradOp";


  IR_ENFORCE(
      attributes.find("diagonal") != attributes.end(),
          "'diagonal' Attribute is expected for TriuGradOp. ");
  int diagonal = attributes.at("diagonal").dyn_cast<pir::Int32Attribute>().data();


  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {out_grad_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_diagonal = pir::Int32Attribute::get(pir::IrContext::Instance(), diagonal);
  argument.AddAttribute("diagonal", attr_diagonal);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType out_grad = out_grad_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)out_grad;

  VLOG(4) << "Builder construction  dense_out_grad";
  paddle::dialect::IrTensor ir_tensor_out_grad(paddle::dialect::TransToPhiDataType(out_grad.dtype()),
                                                      out_grad.dims(),
                                                      out_grad.data_layout(),
                                                      out_grad.lod(),
                                                      out_grad.offset());
  VLOG(4) << "Builder construction  meta_out_grad";
  paddle::dialect::IrMetaTensor meta_out_grad(&ir_tensor_out_grad);
  paddle::dialect::IrTensor dense_x_grad;
  paddle::dialect::IrMetaTensor meta_x_grad(&dense_x_grad);

  phi::UnchangedInferMeta(meta_out_grad, &meta_x_grad);

  std::vector<pir::Type> argument_outputs;
  pir::Type x_grad_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_x_grad.dtype()), dense_x_grad.dims(), dense_x_grad.layout(), dense_x_grad.lod(), dense_x_grad.offset());
  argument_outputs.push_back(x_grad_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void TriuGradOp::VerifySig() {}

void TriuGradOp::InferMeta( phi::InferMetaContext *infer_meta ) {
  auto fn = PD_INFER_META(phi::UnchangedInferMeta);
  fn(infer_meta);
}

phi::DataType TriuGradOp::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: TriuGradOp";
  


  return expected_kernel_dtype;
}

const char *DisableCheckModelNanInfGradOp::attributes_name[1] = { "unsetflag" };

OpInfoTuple DisableCheckModelNanInfGradOp::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("out_grad", "paddle::dialect::DenseTensorType", false, false, false, false) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = { paddle::dialect::OpAttributeInfo("unsetflag", "pir::Int32Attribute", "") };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("x_grad", "paddle::dialect::DenseTensorType", false, false) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("UnchangedInferMeta", {"out_grad"}, "check_model_nan_inf", {"out_grad", "unsetflag"}, {"out_grad"}, {}, {}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "disable_check_model_nan_inf_grad");
}

void DisableCheckModelNanInfGradOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value out_grad_, int unsetflag) {
  VLOG(4) << "Start build DisableCheckModelNanInfGradOp";



  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {out_grad_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_unsetflag = pir::Int32Attribute::get(pir::IrContext::Instance(), unsetflag);
  argument.AddAttribute("unsetflag", attr_unsetflag);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType out_grad = out_grad_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)out_grad;

  VLOG(4) << "Builder construction  dense_out_grad";
  paddle::dialect::IrTensor ir_tensor_out_grad(paddle::dialect::TransToPhiDataType(out_grad.dtype()),
                                                      out_grad.dims(),
                                                      out_grad.data_layout(),
                                                      out_grad.lod(),
                                                      out_grad.offset());
  VLOG(4) << "Builder construction  meta_out_grad";
  paddle::dialect::IrMetaTensor meta_out_grad(&ir_tensor_out_grad);
  paddle::dialect::IrTensor dense_x_grad;
  paddle::dialect::IrMetaTensor meta_x_grad(&dense_x_grad);

  phi::UnchangedInferMeta(meta_out_grad, &meta_x_grad);

  std::vector<pir::Type> argument_outputs;
  pir::Type x_grad_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_x_grad.dtype()), dense_x_grad.dims(), dense_x_grad.layout(), dense_x_grad.lod(), dense_x_grad.offset());
  argument_outputs.push_back(x_grad_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void DisableCheckModelNanInfGradOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value out_grad_, pir::AttributeMap attributes) {
  VLOG(4) << "Start build DisableCheckModelNanInfGradOp";


  IR_ENFORCE(
      attributes.find("unsetflag") != attributes.end(),
          "'unsetflag' Attribute is expected for DisableCheckModelNanInfGradOp. ");
  int unsetflag = attributes.at("unsetflag").dyn_cast<pir::Int32Attribute>().data();


  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {out_grad_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_unsetflag = pir::Int32Attribute::get(pir::IrContext::Instance(), unsetflag);
  argument.AddAttribute("unsetflag", attr_unsetflag);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType out_grad = out_grad_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)out_grad;

  VLOG(4) << "Builder construction  dense_out_grad";
  paddle::dialect::IrTensor ir_tensor_out_grad(paddle::dialect::TransToPhiDataType(out_grad.dtype()),
                                                      out_grad.dims(),
                                                      out_grad.data_layout(),
                                                      out_grad.lod(),
                                                      out_grad.offset());
  VLOG(4) << "Builder construction  meta_out_grad";
  paddle::dialect::IrMetaTensor meta_out_grad(&ir_tensor_out_grad);
  paddle::dialect::IrTensor dense_x_grad;
  paddle::dialect::IrMetaTensor meta_x_grad(&dense_x_grad);

  phi::UnchangedInferMeta(meta_out_grad, &meta_x_grad);

  std::vector<pir::Type> argument_outputs;
  pir::Type x_grad_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_x_grad.dtype()), dense_x_grad.dims(), dense_x_grad.layout(), dense_x_grad.lod(), dense_x_grad.offset());
  argument_outputs.push_back(x_grad_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void DisableCheckModelNanInfGradOp::VerifySig() {}

void DisableCheckModelNanInfGradOp::InferMeta( phi::InferMetaContext *infer_meta ) {
  auto fn = PD_INFER_META(phi::UnchangedInferMeta);
  fn(infer_meta);
}

phi::DataType DisableCheckModelNanInfGradOp::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: DisableCheckModelNanInfGradOp";
  


  return expected_kernel_dtype;
}

const char *EnableCheckModelNanInfGradOp::attributes_name[1] = { "unsetflag" };

OpInfoTuple EnableCheckModelNanInfGradOp::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("out_grad", "paddle::dialect::DenseTensorType", false, false, false, false) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = { paddle::dialect::OpAttributeInfo("unsetflag", "pir::Int32Attribute", "") };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("x_grad", "paddle::dialect::DenseTensorType", false, false) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("UnchangedInferMeta", {"out_grad"}, "check_model_nan_inf", {"out_grad", "unsetflag"}, {"out_grad"}, {}, {}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "enable_check_model_nan_inf_grad");
}

void EnableCheckModelNanInfGradOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value out_grad_, int unsetflag) {
  VLOG(4) << "Start build EnableCheckModelNanInfGradOp";



  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {out_grad_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_unsetflag = pir::Int32Attribute::get(pir::IrContext::Instance(), unsetflag);
  argument.AddAttribute("unsetflag", attr_unsetflag);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType out_grad = out_grad_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)out_grad;

  VLOG(4) << "Builder construction  dense_out_grad";
  paddle::dialect::IrTensor ir_tensor_out_grad(paddle::dialect::TransToPhiDataType(out_grad.dtype()),
                                                      out_grad.dims(),
                                                      out_grad.data_layout(),
                                                      out_grad.lod(),
                                                      out_grad.offset());
  VLOG(4) << "Builder construction  meta_out_grad";
  paddle::dialect::IrMetaTensor meta_out_grad(&ir_tensor_out_grad);
  paddle::dialect::IrTensor dense_x_grad;
  paddle::dialect::IrMetaTensor meta_x_grad(&dense_x_grad);

  phi::UnchangedInferMeta(meta_out_grad, &meta_x_grad);

  std::vector<pir::Type> argument_outputs;
  pir::Type x_grad_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_x_grad.dtype()), dense_x_grad.dims(), dense_x_grad.layout(), dense_x_grad.lod(), dense_x_grad.offset());
  argument_outputs.push_back(x_grad_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void EnableCheckModelNanInfGradOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value out_grad_, pir::AttributeMap attributes) {
  VLOG(4) << "Start build EnableCheckModelNanInfGradOp";


  IR_ENFORCE(
      attributes.find("unsetflag") != attributes.end(),
          "'unsetflag' Attribute is expected for EnableCheckModelNanInfGradOp. ");
  int unsetflag = attributes.at("unsetflag").dyn_cast<pir::Int32Attribute>().data();


  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {out_grad_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_unsetflag = pir::Int32Attribute::get(pir::IrContext::Instance(), unsetflag);
  argument.AddAttribute("unsetflag", attr_unsetflag);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType out_grad = out_grad_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)out_grad;

  VLOG(4) << "Builder construction  dense_out_grad";
  paddle::dialect::IrTensor ir_tensor_out_grad(paddle::dialect::TransToPhiDataType(out_grad.dtype()),
                                                      out_grad.dims(),
                                                      out_grad.data_layout(),
                                                      out_grad.lod(),
                                                      out_grad.offset());
  VLOG(4) << "Builder construction  meta_out_grad";
  paddle::dialect::IrMetaTensor meta_out_grad(&ir_tensor_out_grad);
  paddle::dialect::IrTensor dense_x_grad;
  paddle::dialect::IrMetaTensor meta_x_grad(&dense_x_grad);

  phi::UnchangedInferMeta(meta_out_grad, &meta_x_grad);

  std::vector<pir::Type> argument_outputs;
  pir::Type x_grad_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_x_grad.dtype()), dense_x_grad.dims(), dense_x_grad.layout(), dense_x_grad.lod(), dense_x_grad.offset());
  argument_outputs.push_back(x_grad_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void EnableCheckModelNanInfGradOp::VerifySig() {}

void EnableCheckModelNanInfGradOp::InferMeta( phi::InferMetaContext *infer_meta ) {
  auto fn = PD_INFER_META(phi::UnchangedInferMeta);
  fn(infer_meta);
}

phi::DataType EnableCheckModelNanInfGradOp::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: EnableCheckModelNanInfGradOp";
  


  return expected_kernel_dtype;
}

const char *FusedElemwiseAddActivationGradOp::attributes_name[4] = { "functor_list", "scale", "axis", "save_intermediate_out" };

OpInfoTuple FusedElemwiseAddActivationGradOp::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("x", "paddle::dialect::DenseTensorType", true, false, false, false), paddle::dialect::OpInputInfo("y", "paddle::dialect::DenseTensorType", false, false, false, false), paddle::dialect::OpInputInfo("out", "paddle::dialect::DenseTensorType", false, false, false, false), paddle::dialect::OpInputInfo("intermediate_out", "paddle::dialect::DenseTensorType", true, false, false, false), paddle::dialect::OpInputInfo("out_grad", "paddle::dialect::DenseTensorType", false, false, false, false) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = { paddle::dialect::OpAttributeInfo("functor_list", "pir::ArrayAttribute<pir::StrAttribute>", ""), paddle::dialect::OpAttributeInfo("scale", "pir::FloatAttribute", ""), paddle::dialect::OpAttributeInfo("axis", "pir::Int32Attribute", ""), paddle::dialect::OpAttributeInfo("save_intermediate_out", "pir::BoolAttribute", "") };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("x_grad", "paddle::dialect::DenseTensorType", false, false), paddle::dialect::OpOutputInfo("y_grad", "paddle::dialect::DenseTensorType", false, false) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("FusedElemwiseAddActivationGradInferMeta", {"x", "y", "out", "intermediate_out", "out_grad", "functor_list", "scale", "axis", "save_intermediate_out"}, "fused_elemwise_add_activation_grad", {"x", "y", "out", "intermediate_out", "out_grad", "functor_list", "scale", "axis", "save_intermediate_out"}, {}, {}, {}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "fused_elemwise_add_activation_grad");
}

void FusedElemwiseAddActivationGradOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, pir::Value y_, pir::Value out_, pir::Value intermediate_out_, pir::Value out_grad_, const std::vector<std::string>& functor_list, float scale, int axis, bool save_intermediate_out) {
  VLOG(4) << "Start build FusedElemwiseAddActivationGradOp";



  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_, y_, out_, intermediate_out_, out_grad_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  std::vector<pir::Attribute> vec_functor_list;
  for (size_t i = 0; i < static_cast<size_t>(functor_list.size()); i++) {
      pir::Attribute attr_functor_list = pir::StrAttribute::get(pir::IrContext::Instance(), functor_list[i]);

    vec_functor_list.push_back(attr_functor_list);
  }
  pir::Attribute attr_functor_list = pir::ArrayAttribute::get(pir::IrContext::Instance(), vec_functor_list);
  argument.AddAttribute("functor_list", attr_functor_list);
  pir::Attribute attr_scale = pir::FloatAttribute::get(pir::IrContext::Instance(), scale);
  argument.AddAttribute("scale", attr_scale);
  pir::Attribute attr_axis = pir::Int32Attribute::get(pir::IrContext::Instance(), axis);
  argument.AddAttribute("axis", attr_axis);
  pir::Attribute attr_save_intermediate_out = pir::BoolAttribute::get(pir::IrContext::Instance(), save_intermediate_out);
  argument.AddAttribute("save_intermediate_out", attr_save_intermediate_out);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType y = y_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)y;
  paddle::dialect::DenseTensorType out = out_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)out;
  paddle::dialect::DenseTensorType out_grad = out_grad_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)out_grad;

  paddle::dialect::IrMetaTensor meta_x;
  paddle::dialect::IrTensor ir_tensor_x;
  if (x_.impl() != nullptr) {
    paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>();
    VLOG(4) << "Builder construction  dense_x";
    ir_tensor_x = paddle::dialect::IrTensor(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                        x.dims(),
                                                        x.data_layout(),
                                                        x.lod(),
                                                        x.offset());
    VLOG(4) << "Builder construction  meta_x";
    meta_x = paddle::dialect::IrMetaTensor(&ir_tensor_x);
  }


  VLOG(4) << "Builder construction  dense_y";
  paddle::dialect::IrTensor ir_tensor_y(paddle::dialect::TransToPhiDataType(y.dtype()),
                                                      y.dims(),
                                                      y.data_layout(),
                                                      y.lod(),
                                                      y.offset());
  VLOG(4) << "Builder construction  meta_y";
  paddle::dialect::IrMetaTensor meta_y(&ir_tensor_y);

  VLOG(4) << "Builder construction  dense_out";
  paddle::dialect::IrTensor ir_tensor_out(paddle::dialect::TransToPhiDataType(out.dtype()),
                                                      out.dims(),
                                                      out.data_layout(),
                                                      out.lod(),
                                                      out.offset());
  VLOG(4) << "Builder construction  meta_out";
  paddle::dialect::IrMetaTensor meta_out(&ir_tensor_out);

  paddle::dialect::IrMetaTensor meta_intermediate_out;
  paddle::dialect::IrTensor ir_tensor_intermediate_out;
  if (intermediate_out_.impl() != nullptr) {
    paddle::dialect::DenseTensorType intermediate_out = intermediate_out_.type().dyn_cast<paddle::dialect::DenseTensorType>();
    VLOG(4) << "Builder construction  dense_intermediate_out";
    ir_tensor_intermediate_out = paddle::dialect::IrTensor(paddle::dialect::TransToPhiDataType(intermediate_out.dtype()),
                                                        intermediate_out.dims(),
                                                        intermediate_out.data_layout(),
                                                        intermediate_out.lod(),
                                                        intermediate_out.offset());
    VLOG(4) << "Builder construction  meta_intermediate_out";
    meta_intermediate_out = paddle::dialect::IrMetaTensor(&ir_tensor_intermediate_out);
  }


  VLOG(4) << "Builder construction  dense_out_grad";
  paddle::dialect::IrTensor ir_tensor_out_grad(paddle::dialect::TransToPhiDataType(out_grad.dtype()),
                                                      out_grad.dims(),
                                                      out_grad.data_layout(),
                                                      out_grad.lod(),
                                                      out_grad.offset());
  VLOG(4) << "Builder construction  meta_out_grad";
  paddle::dialect::IrMetaTensor meta_out_grad(&ir_tensor_out_grad);
  paddle::dialect::IrTensor dense_x_grad;
  paddle::dialect::IrMetaTensor meta_x_grad(&dense_x_grad);
  paddle::dialect::IrTensor dense_y_grad;
  paddle::dialect::IrMetaTensor meta_y_grad(&dense_y_grad);

  phi::FusedElemwiseAddActivationGradInferMeta(meta_x, meta_y, meta_out, meta_intermediate_out, meta_out_grad, functor_list, scale, axis, save_intermediate_out, &meta_x_grad, &meta_y_grad);

  std::vector<pir::Type> argument_outputs;
  pir::Type x_grad_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_x_grad.dtype()), dense_x_grad.dims(), dense_x_grad.layout(), dense_x_grad.lod(), dense_x_grad.offset());
  argument_outputs.push_back(x_grad_dense_tensor_type);

  pir::Type y_grad_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_y_grad.dtype()), dense_y_grad.dims(), dense_y_grad.layout(), dense_y_grad.lod(), dense_y_grad.offset());
  argument_outputs.push_back(y_grad_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void FusedElemwiseAddActivationGradOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, pir::Value y_, pir::Value out_, pir::Value intermediate_out_, pir::Value out_grad_, pir::AttributeMap attributes) {
  VLOG(4) << "Start build FusedElemwiseAddActivationGradOp";


  IR_ENFORCE(
      attributes.find("functor_list") != attributes.end(),
          "'functor_list' Attribute is expected for FusedElemwiseAddActivationGradOp. ");
  std::vector<std::string> functor_list;
  for (size_t i = 0; i < attributes.at("functor_list").dyn_cast<pir::ArrayAttribute>().size(); i++) {
    functor_list.push_back(attributes.at("functor_list").dyn_cast<pir::ArrayAttribute>().at(i).dyn_cast<pir::StrAttribute>().AsString());
  }

  IR_ENFORCE(
      attributes.find("scale") != attributes.end(),
          "'scale' Attribute is expected for FusedElemwiseAddActivationGradOp. ");
  float scale = attributes.at("scale").dyn_cast<pir::FloatAttribute>().data();

  IR_ENFORCE(
      attributes.find("axis") != attributes.end(),
          "'axis' Attribute is expected for FusedElemwiseAddActivationGradOp. ");
  int axis = attributes.at("axis").dyn_cast<pir::Int32Attribute>().data();

  IR_ENFORCE(
      attributes.find("save_intermediate_out") != attributes.end(),
          "'save_intermediate_out' Attribute is expected for FusedElemwiseAddActivationGradOp. ");
  bool save_intermediate_out = attributes.at("save_intermediate_out").dyn_cast<pir::BoolAttribute>().data();


  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_, y_, out_, intermediate_out_, out_grad_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  std::vector<pir::Attribute> vec_functor_list;
  for (size_t i = 0; i < static_cast<size_t>(functor_list.size()); i++) {
      pir::Attribute attr_functor_list = pir::StrAttribute::get(pir::IrContext::Instance(), functor_list[i]);

    vec_functor_list.push_back(attr_functor_list);
  }
  pir::Attribute attr_functor_list = pir::ArrayAttribute::get(pir::IrContext::Instance(), vec_functor_list);
  argument.AddAttribute("functor_list", attr_functor_list);
  pir::Attribute attr_scale = pir::FloatAttribute::get(pir::IrContext::Instance(), scale);
  argument.AddAttribute("scale", attr_scale);
  pir::Attribute attr_axis = pir::Int32Attribute::get(pir::IrContext::Instance(), axis);
  argument.AddAttribute("axis", attr_axis);
  pir::Attribute attr_save_intermediate_out = pir::BoolAttribute::get(pir::IrContext::Instance(), save_intermediate_out);
  argument.AddAttribute("save_intermediate_out", attr_save_intermediate_out);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType y = y_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)y;
  paddle::dialect::DenseTensorType out = out_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)out;
  paddle::dialect::DenseTensorType out_grad = out_grad_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)out_grad;

  paddle::dialect::IrMetaTensor meta_x;
  paddle::dialect::IrTensor ir_tensor_x;
  if (x_.impl() != nullptr) {
    paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>();
    VLOG(4) << "Builder construction  dense_x";
    ir_tensor_x = paddle::dialect::IrTensor(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                        x.dims(),
                                                        x.data_layout(),
                                                        x.lod(),
                                                        x.offset());
    VLOG(4) << "Builder construction  meta_x";
    meta_x = paddle::dialect::IrMetaTensor(&ir_tensor_x);
  }


  VLOG(4) << "Builder construction  dense_y";
  paddle::dialect::IrTensor ir_tensor_y(paddle::dialect::TransToPhiDataType(y.dtype()),
                                                      y.dims(),
                                                      y.data_layout(),
                                                      y.lod(),
                                                      y.offset());
  VLOG(4) << "Builder construction  meta_y";
  paddle::dialect::IrMetaTensor meta_y(&ir_tensor_y);

  VLOG(4) << "Builder construction  dense_out";
  paddle::dialect::IrTensor ir_tensor_out(paddle::dialect::TransToPhiDataType(out.dtype()),
                                                      out.dims(),
                                                      out.data_layout(),
                                                      out.lod(),
                                                      out.offset());
  VLOG(4) << "Builder construction  meta_out";
  paddle::dialect::IrMetaTensor meta_out(&ir_tensor_out);

  paddle::dialect::IrMetaTensor meta_intermediate_out;
  paddle::dialect::IrTensor ir_tensor_intermediate_out;
  if (intermediate_out_.impl() != nullptr) {
    paddle::dialect::DenseTensorType intermediate_out = intermediate_out_.type().dyn_cast<paddle::dialect::DenseTensorType>();
    VLOG(4) << "Builder construction  dense_intermediate_out";
    ir_tensor_intermediate_out = paddle::dialect::IrTensor(paddle::dialect::TransToPhiDataType(intermediate_out.dtype()),
                                                        intermediate_out.dims(),
                                                        intermediate_out.data_layout(),
                                                        intermediate_out.lod(),
                                                        intermediate_out.offset());
    VLOG(4) << "Builder construction  meta_intermediate_out";
    meta_intermediate_out = paddle::dialect::IrMetaTensor(&ir_tensor_intermediate_out);
  }


  VLOG(4) << "Builder construction  dense_out_grad";
  paddle::dialect::IrTensor ir_tensor_out_grad(paddle::dialect::TransToPhiDataType(out_grad.dtype()),
                                                      out_grad.dims(),
                                                      out_grad.data_layout(),
                                                      out_grad.lod(),
                                                      out_grad.offset());
  VLOG(4) << "Builder construction  meta_out_grad";
  paddle::dialect::IrMetaTensor meta_out_grad(&ir_tensor_out_grad);
  paddle::dialect::IrTensor dense_x_grad;
  paddle::dialect::IrMetaTensor meta_x_grad(&dense_x_grad);
  paddle::dialect::IrTensor dense_y_grad;
  paddle::dialect::IrMetaTensor meta_y_grad(&dense_y_grad);

  phi::FusedElemwiseAddActivationGradInferMeta(meta_x, meta_y, meta_out, meta_intermediate_out, meta_out_grad, functor_list, scale, axis, save_intermediate_out, &meta_x_grad, &meta_y_grad);

  std::vector<pir::Type> argument_outputs;
  pir::Type x_grad_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_x_grad.dtype()), dense_x_grad.dims(), dense_x_grad.layout(), dense_x_grad.lod(), dense_x_grad.offset());
  argument_outputs.push_back(x_grad_dense_tensor_type);

  pir::Type y_grad_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_y_grad.dtype()), dense_y_grad.dims(), dense_y_grad.layout(), dense_y_grad.lod(), dense_y_grad.offset());
  argument_outputs.push_back(y_grad_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void FusedElemwiseAddActivationGradOp::VerifySig() {}

void FusedElemwiseAddActivationGradOp::InferMeta( phi::InferMetaContext *infer_meta ) {
  auto fn = PD_INFER_META(phi::FusedElemwiseAddActivationGradInferMeta);
  fn(infer_meta);
}

phi::DataType FusedElemwiseAddActivationGradOp::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: FusedElemwiseAddActivationGradOp";
  


  return expected_kernel_dtype;
}

const char *ShuffleBatchGradOp::attributes_name[1] = { "startup_seed" };

OpInfoTuple ShuffleBatchGradOp::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("shuffle_idx", "paddle::dialect::DenseTensorType", false, false, false, false), paddle::dialect::OpInputInfo("out_grad", "paddle::dialect::DenseTensorType", false, false, false, false) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = { paddle::dialect::OpAttributeInfo("startup_seed", "pir::Int32Attribute", "") };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("x_grad", "paddle::dialect::DenseTensorType", false, false) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("ShuffleBatchGradInferMeta", {"shuffle_idx", "out_grad", "startup_seed"}, "shuffle_batch_grad", {"shuffle_idx", "out_grad", "startup_seed"}, {"out_grad"}, {}, {}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "shuffle_batch_grad");
}

void ShuffleBatchGradOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value shuffle_idx_, pir::Value out_grad_, int startup_seed) {
  VLOG(4) << "Start build ShuffleBatchGradOp";



  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {shuffle_idx_, out_grad_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_startup_seed = pir::Int32Attribute::get(pir::IrContext::Instance(), startup_seed);
  argument.AddAttribute("startup_seed", attr_startup_seed);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType shuffle_idx = shuffle_idx_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)shuffle_idx;
  paddle::dialect::DenseTensorType out_grad = out_grad_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)out_grad;

  VLOG(4) << "Builder construction  dense_shuffle_idx";
  paddle::dialect::IrTensor ir_tensor_shuffle_idx(paddle::dialect::TransToPhiDataType(shuffle_idx.dtype()),
                                                      shuffle_idx.dims(),
                                                      shuffle_idx.data_layout(),
                                                      shuffle_idx.lod(),
                                                      shuffle_idx.offset());
  VLOG(4) << "Builder construction  meta_shuffle_idx";
  paddle::dialect::IrMetaTensor meta_shuffle_idx(&ir_tensor_shuffle_idx);

  VLOG(4) << "Builder construction  dense_out_grad";
  paddle::dialect::IrTensor ir_tensor_out_grad(paddle::dialect::TransToPhiDataType(out_grad.dtype()),
                                                      out_grad.dims(),
                                                      out_grad.data_layout(),
                                                      out_grad.lod(),
                                                      out_grad.offset());
  VLOG(4) << "Builder construction  meta_out_grad";
  paddle::dialect::IrMetaTensor meta_out_grad(&ir_tensor_out_grad);
  paddle::dialect::IrTensor dense_x_grad;
  paddle::dialect::IrMetaTensor meta_x_grad(&dense_x_grad);

  phi::ShuffleBatchGradInferMeta(meta_shuffle_idx, meta_out_grad, startup_seed, &meta_x_grad);

  std::vector<pir::Type> argument_outputs;
  pir::Type x_grad_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_x_grad.dtype()), dense_x_grad.dims(), dense_x_grad.layout(), dense_x_grad.lod(), dense_x_grad.offset());
  argument_outputs.push_back(x_grad_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void ShuffleBatchGradOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value shuffle_idx_, pir::Value out_grad_, pir::AttributeMap attributes) {
  VLOG(4) << "Start build ShuffleBatchGradOp";


  IR_ENFORCE(
      attributes.find("startup_seed") != attributes.end(),
          "'startup_seed' Attribute is expected for ShuffleBatchGradOp. ");
  int startup_seed = attributes.at("startup_seed").dyn_cast<pir::Int32Attribute>().data();


  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {shuffle_idx_, out_grad_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_startup_seed = pir::Int32Attribute::get(pir::IrContext::Instance(), startup_seed);
  argument.AddAttribute("startup_seed", attr_startup_seed);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType shuffle_idx = shuffle_idx_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)shuffle_idx;
  paddle::dialect::DenseTensorType out_grad = out_grad_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)out_grad;

  VLOG(4) << "Builder construction  dense_shuffle_idx";
  paddle::dialect::IrTensor ir_tensor_shuffle_idx(paddle::dialect::TransToPhiDataType(shuffle_idx.dtype()),
                                                      shuffle_idx.dims(),
                                                      shuffle_idx.data_layout(),
                                                      shuffle_idx.lod(),
                                                      shuffle_idx.offset());
  VLOG(4) << "Builder construction  meta_shuffle_idx";
  paddle::dialect::IrMetaTensor meta_shuffle_idx(&ir_tensor_shuffle_idx);

  VLOG(4) << "Builder construction  dense_out_grad";
  paddle::dialect::IrTensor ir_tensor_out_grad(paddle::dialect::TransToPhiDataType(out_grad.dtype()),
                                                      out_grad.dims(),
                                                      out_grad.data_layout(),
                                                      out_grad.lod(),
                                                      out_grad.offset());
  VLOG(4) << "Builder construction  meta_out_grad";
  paddle::dialect::IrMetaTensor meta_out_grad(&ir_tensor_out_grad);
  paddle::dialect::IrTensor dense_x_grad;
  paddle::dialect::IrMetaTensor meta_x_grad(&dense_x_grad);

  phi::ShuffleBatchGradInferMeta(meta_shuffle_idx, meta_out_grad, startup_seed, &meta_x_grad);

  std::vector<pir::Type> argument_outputs;
  pir::Type x_grad_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_x_grad.dtype()), dense_x_grad.dims(), dense_x_grad.layout(), dense_x_grad.lod(), dense_x_grad.offset());
  argument_outputs.push_back(x_grad_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void ShuffleBatchGradOp::VerifySig() {}

void ShuffleBatchGradOp::InferMeta( phi::InferMetaContext *infer_meta ) {
  auto fn = PD_INFER_META(phi::ShuffleBatchGradInferMeta);
  fn(infer_meta);
}

phi::DataType ShuffleBatchGradOp::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: ShuffleBatchGradOp";
  


  return expected_kernel_dtype;
}

const char *UnpoolGradOp::attributes_name[4] = { "ksize", "strides", "padding", "data_format" };

OpInfoTuple UnpoolGradOp::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("x", "paddle::dialect::DenseTensorType", false, false, false, false), paddle::dialect::OpInputInfo("indices", "paddle::dialect::DenseTensorType", false, false, false, false), paddle::dialect::OpInputInfo("out", "paddle::dialect::DenseTensorType", false, false, false, false), paddle::dialect::OpInputInfo("out_grad", "paddle::dialect::DenseTensorType", false, false, false, false), paddle::dialect::OpInputInfo("output_size", "paddle::dialect::IntArrayAttribute", false, false, true, false) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = { paddle::dialect::OpAttributeInfo("ksize", "pir::ArrayAttribute<pir::Int32Attribute>", ""), paddle::dialect::OpAttributeInfo("strides", "pir::ArrayAttribute<pir::Int32Attribute>", ""), paddle::dialect::OpAttributeInfo("padding", "pir::ArrayAttribute<pir::Int32Attribute>", ""), paddle::dialect::OpAttributeInfo("data_format", "pir::StrAttribute", "") };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("x_grad", "paddle::dialect::DenseTensorType", false, false) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("UnchangedInferMeta", {"x"}, "unpool_grad", {"x", "indices", "out", "out_grad", "ksize", "strides", "padding", "output_size", "data_format"}, {"x"}, {}, {}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "unpool_grad");
}

void UnpoolGradOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, pir::Value indices_, pir::Value out_, pir::Value out_grad_, const std::vector<int>& ksize, const std::vector<int>& strides, const std::vector<int>& padding, const std::vector<int64_t>& output_size, const std::string& data_format) {
  VLOG(4) << "Start build UnpoolGradOp";


  // Generate int_array mutable attribute: output_size
  paddle::dialect::FullIntArrayOp full_output_size_op = builder.Build<paddle::dialect::FullIntArrayOp>(output_size, phi::DataType::INT64, phi::CPUPlace());
  pir::OpResult output_size_ = full_output_size_op->result(0);
    
  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_, indices_, out_, out_grad_, output_size_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  std::vector<pir::Attribute> vec_ksize;
  for (size_t i = 0; i < static_cast<size_t>(ksize.size()); i++) {
      pir::Attribute attr_ksize = pir::Int32Attribute::get(pir::IrContext::Instance(), ksize[i]);

    vec_ksize.push_back(attr_ksize);
  }
  pir::Attribute attr_ksize = pir::ArrayAttribute::get(pir::IrContext::Instance(), vec_ksize);
  argument.AddAttribute("ksize", attr_ksize);
  std::vector<pir::Attribute> vec_strides;
  for (size_t i = 0; i < static_cast<size_t>(strides.size()); i++) {
      pir::Attribute attr_strides = pir::Int32Attribute::get(pir::IrContext::Instance(), strides[i]);

    vec_strides.push_back(attr_strides);
  }
  pir::Attribute attr_strides = pir::ArrayAttribute::get(pir::IrContext::Instance(), vec_strides);
  argument.AddAttribute("strides", attr_strides);
  std::vector<pir::Attribute> vec_padding;
  for (size_t i = 0; i < static_cast<size_t>(padding.size()); i++) {
      pir::Attribute attr_padding = pir::Int32Attribute::get(pir::IrContext::Instance(), padding[i]);

    vec_padding.push_back(attr_padding);
  }
  pir::Attribute attr_padding = pir::ArrayAttribute::get(pir::IrContext::Instance(), vec_padding);
  argument.AddAttribute("padding", attr_padding);
  pir::Attribute attr_data_format = pir::StrAttribute::get(pir::IrContext::Instance(), data_format);
  argument.AddAttribute("data_format", attr_data_format);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;
  paddle::dialect::DenseTensorType indices = indices_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)indices;
  paddle::dialect::DenseTensorType out = out_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)out;
  paddle::dialect::DenseTensorType out_grad = out_grad_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)out_grad;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);
  paddle::dialect::IrTensor dense_x_grad;
  paddle::dialect::IrMetaTensor meta_x_grad(&dense_x_grad);

  phi::UnchangedInferMeta(meta_x, &meta_x_grad);

  std::vector<pir::Type> argument_outputs;
  pir::Type x_grad_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_x_grad.dtype()), dense_x_grad.dims(), dense_x_grad.layout(), dense_x_grad.lod(), dense_x_grad.offset());
  argument_outputs.push_back(x_grad_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void UnpoolGradOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, pir::Value indices_, pir::Value out_, pir::Value out_grad_, pir::AttributeMap attributes) {
  VLOG(4) << "Start build UnpoolGradOp";


  IR_ENFORCE(
      attributes.find("ksize") != attributes.end(),
          "'ksize' Attribute is expected for UnpoolGradOp. ");
  std::vector<int> ksize;
  for (size_t i = 0; i < attributes.at("ksize").dyn_cast<pir::ArrayAttribute>().size(); i++) {
    ksize.push_back(attributes.at("ksize").dyn_cast<pir::ArrayAttribute>().at(i).dyn_cast<pir::Int32Attribute>().data());
  }

  IR_ENFORCE(
      attributes.find("strides") != attributes.end(),
          "'strides' Attribute is expected for UnpoolGradOp. ");
  std::vector<int> strides;
  for (size_t i = 0; i < attributes.at("strides").dyn_cast<pir::ArrayAttribute>().size(); i++) {
    strides.push_back(attributes.at("strides").dyn_cast<pir::ArrayAttribute>().at(i).dyn_cast<pir::Int32Attribute>().data());
  }

  IR_ENFORCE(
      attributes.find("padding") != attributes.end(),
          "'padding' Attribute is expected for UnpoolGradOp. ");
  std::vector<int> padding;
  for (size_t i = 0; i < attributes.at("padding").dyn_cast<pir::ArrayAttribute>().size(); i++) {
    padding.push_back(attributes.at("padding").dyn_cast<pir::ArrayAttribute>().at(i).dyn_cast<pir::Int32Attribute>().data());
  }

  IR_ENFORCE(
      attributes.find("output_size") != attributes.end(),
          "'output_size' Attribute is expected for UnpoolGradOp. ");
  std::vector<int64_t> output_size = attributes.at("output_size").dyn_cast<paddle::dialect::IntArrayAttribute>().data().GetData();

  IR_ENFORCE(
      attributes.find("data_format") != attributes.end(),
          "'data_format' Attribute is expected for UnpoolGradOp. ");
  std::string data_format = attributes.at("data_format").dyn_cast<pir::StrAttribute>().AsString();

  // Generate int_array mutable attribute: output_size
  paddle::dialect::FullIntArrayOp full_output_size_op = builder.Build<paddle::dialect::FullIntArrayOp>(output_size, phi::DataType::INT64, phi::CPUPlace());
  pir::OpResult output_size_ = full_output_size_op->result(0);
    
  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_, indices_, out_, out_grad_, output_size_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  std::vector<pir::Attribute> vec_ksize;
  for (size_t i = 0; i < static_cast<size_t>(ksize.size()); i++) {
      pir::Attribute attr_ksize = pir::Int32Attribute::get(pir::IrContext::Instance(), ksize[i]);

    vec_ksize.push_back(attr_ksize);
  }
  pir::Attribute attr_ksize = pir::ArrayAttribute::get(pir::IrContext::Instance(), vec_ksize);
  argument.AddAttribute("ksize", attr_ksize);
  std::vector<pir::Attribute> vec_strides;
  for (size_t i = 0; i < static_cast<size_t>(strides.size()); i++) {
      pir::Attribute attr_strides = pir::Int32Attribute::get(pir::IrContext::Instance(), strides[i]);

    vec_strides.push_back(attr_strides);
  }
  pir::Attribute attr_strides = pir::ArrayAttribute::get(pir::IrContext::Instance(), vec_strides);
  argument.AddAttribute("strides", attr_strides);
  std::vector<pir::Attribute> vec_padding;
  for (size_t i = 0; i < static_cast<size_t>(padding.size()); i++) {
      pir::Attribute attr_padding = pir::Int32Attribute::get(pir::IrContext::Instance(), padding[i]);

    vec_padding.push_back(attr_padding);
  }
  pir::Attribute attr_padding = pir::ArrayAttribute::get(pir::IrContext::Instance(), vec_padding);
  argument.AddAttribute("padding", attr_padding);
  pir::Attribute attr_data_format = pir::StrAttribute::get(pir::IrContext::Instance(), data_format);
  argument.AddAttribute("data_format", attr_data_format);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;
  paddle::dialect::DenseTensorType indices = indices_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)indices;
  paddle::dialect::DenseTensorType out = out_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)out;
  paddle::dialect::DenseTensorType out_grad = out_grad_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)out_grad;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);
  paddle::dialect::IrTensor dense_x_grad;
  paddle::dialect::IrMetaTensor meta_x_grad(&dense_x_grad);

  phi::UnchangedInferMeta(meta_x, &meta_x_grad);

  std::vector<pir::Type> argument_outputs;
  pir::Type x_grad_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_x_grad.dtype()), dense_x_grad.dims(), dense_x_grad.layout(), dense_x_grad.lod(), dense_x_grad.offset());
  argument_outputs.push_back(x_grad_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void UnpoolGradOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, pir::Value indices_, pir::Value out_, pir::Value out_grad_, pir::Value output_size_, const std::vector<int>& ksize, const std::vector<int>& strides, const std::vector<int>& padding, const std::string& data_format) {
  VLOG(4) << "Start build UnpoolGradOp";



  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_, indices_, out_, out_grad_, output_size_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  std::vector<pir::Attribute> vec_ksize;
  for (size_t i = 0; i < static_cast<size_t>(ksize.size()); i++) {
      pir::Attribute attr_ksize = pir::Int32Attribute::get(pir::IrContext::Instance(), ksize[i]);

    vec_ksize.push_back(attr_ksize);
  }
  pir::Attribute attr_ksize = pir::ArrayAttribute::get(pir::IrContext::Instance(), vec_ksize);
  argument.AddAttribute("ksize", attr_ksize);
  std::vector<pir::Attribute> vec_strides;
  for (size_t i = 0; i < static_cast<size_t>(strides.size()); i++) {
      pir::Attribute attr_strides = pir::Int32Attribute::get(pir::IrContext::Instance(), strides[i]);

    vec_strides.push_back(attr_strides);
  }
  pir::Attribute attr_strides = pir::ArrayAttribute::get(pir::IrContext::Instance(), vec_strides);
  argument.AddAttribute("strides", attr_strides);
  std::vector<pir::Attribute> vec_padding;
  for (size_t i = 0; i < static_cast<size_t>(padding.size()); i++) {
      pir::Attribute attr_padding = pir::Int32Attribute::get(pir::IrContext::Instance(), padding[i]);

    vec_padding.push_back(attr_padding);
  }
  pir::Attribute attr_padding = pir::ArrayAttribute::get(pir::IrContext::Instance(), vec_padding);
  argument.AddAttribute("padding", attr_padding);
  pir::Attribute attr_data_format = pir::StrAttribute::get(pir::IrContext::Instance(), data_format);
  argument.AddAttribute("data_format", attr_data_format);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;
  paddle::dialect::DenseTensorType indices = indices_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)indices;
  paddle::dialect::DenseTensorType out = out_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)out;
  paddle::dialect::DenseTensorType out_grad = out_grad_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)out_grad;
  phi::IntArray output_size;
  if (output_size_.dyn_cast<pir::OpResult>() && output_size_.dyn_cast<pir::OpResult>().owner()->isa<paddle::dialect::FullIntArrayOp>()) {
    output_size = std::move(phi::IntArray(paddle::dialect::GetInt64Vector(
                          output_size_.dyn_cast<pir::OpResult>().owner()
                          ->dyn_cast<paddle::dialect::FullIntArrayOp>()
                          .attribute("value"))));
  } else if (output_size_.type().isa<pir::VectorType>()) {
    size_t output_size_size = output_size_.type().dyn_cast<pir::VectorType>().size();
    output_size = std::move(phi::IntArray(std::vector<int64_t>(output_size_size, -1)));
    output_size.SetFromTensor(true);
  } else if (output_size_.type().isa<paddle::dialect::DenseTensorType>()) {
    common::DDim output_size_dim = output_size_.type().dyn_cast<paddle::dialect::DenseTensorType>().dims();
    size_t output_size_size = common::product(output_size_dim);
    if (common::contain_unknown_dim(output_size_dim)) {
      output_size_size = 1;
    }
    output_size = std::move(phi::IntArray(std::vector<int64_t>(output_size_size, -1)));
    output_size.SetFromTensor(true);
  } else {
    PADDLE_THROW(phi::errors::Unimplemented("Only support VectorType or DenseTensorType"));
  }


  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);
  paddle::dialect::IrTensor dense_x_grad;
  paddle::dialect::IrMetaTensor meta_x_grad(&dense_x_grad);

  phi::UnchangedInferMeta(meta_x, &meta_x_grad);

  std::vector<pir::Type> argument_outputs;
  pir::Type x_grad_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_x_grad.dtype()), dense_x_grad.dims(), dense_x_grad.layout(), dense_x_grad.lod(), dense_x_grad.offset());
  argument_outputs.push_back(x_grad_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void UnpoolGradOp::VerifySig() {}

void UnpoolGradOp::InferMeta( phi::InferMetaContext *infer_meta ) {
  auto fn = PD_INFER_META(phi::UnchangedInferMeta);
  fn(infer_meta);
}

phi::DataType UnpoolGradOp::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: UnpoolGradOp";
  


  return expected_kernel_dtype;
}

const char *MatchMatrixTensorGradOp::attributes_name[1] = { "dim_t" };

OpInfoTuple MatchMatrixTensorGradOp::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("x", "paddle::dialect::DenseTensorType", false, false, false, false), paddle::dialect::OpInputInfo("y", "paddle::dialect::DenseTensorType", false, false, false, false), paddle::dialect::OpInputInfo("w", "paddle::dialect::DenseTensorType", false, false, false, false), paddle::dialect::OpInputInfo("tmp", "paddle::dialect::DenseTensorType", false, false, false, false), paddle::dialect::OpInputInfo("out_grad", "paddle::dialect::DenseTensorType", false, false, false, false) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = { paddle::dialect::OpAttributeInfo("dim_t", "pir::Int32Attribute", "") };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("x_grad", "paddle::dialect::DenseTensorType", false, false), paddle::dialect::OpOutputInfo("y_grad", "paddle::dialect::DenseTensorType", false, false), paddle::dialect::OpOutputInfo("w_grad", "paddle::dialect::DenseTensorType", false, false) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("MatchMatrixTensorGradInferMeta", {"x", "y", "w", "tmp", "out_grad", "dim_t"}, "match_matrix_tensor_grad", {"x", "y", "w", "tmp", "out_grad", "dim_t"}, {}, {}, {}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "match_matrix_tensor_grad");
}

void MatchMatrixTensorGradOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, pir::Value y_, pir::Value w_, pir::Value tmp_, pir::Value out_grad_, int dim_t) {
  VLOG(4) << "Start build MatchMatrixTensorGradOp";



  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_, y_, w_, tmp_, out_grad_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_dim_t = pir::Int32Attribute::get(pir::IrContext::Instance(), dim_t);
  argument.AddAttribute("dim_t", attr_dim_t);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;
  paddle::dialect::DenseTensorType y = y_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)y;
  paddle::dialect::DenseTensorType w = w_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)w;
  paddle::dialect::DenseTensorType tmp = tmp_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)tmp;
  paddle::dialect::DenseTensorType out_grad = out_grad_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)out_grad;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);

  VLOG(4) << "Builder construction  dense_y";
  paddle::dialect::IrTensor ir_tensor_y(paddle::dialect::TransToPhiDataType(y.dtype()),
                                                      y.dims(),
                                                      y.data_layout(),
                                                      y.lod(),
                                                      y.offset());
  VLOG(4) << "Builder construction  meta_y";
  paddle::dialect::IrMetaTensor meta_y(&ir_tensor_y);

  VLOG(4) << "Builder construction  dense_w";
  paddle::dialect::IrTensor ir_tensor_w(paddle::dialect::TransToPhiDataType(w.dtype()),
                                                      w.dims(),
                                                      w.data_layout(),
                                                      w.lod(),
                                                      w.offset());
  VLOG(4) << "Builder construction  meta_w";
  paddle::dialect::IrMetaTensor meta_w(&ir_tensor_w);

  VLOG(4) << "Builder construction  dense_tmp";
  paddle::dialect::IrTensor ir_tensor_tmp(paddle::dialect::TransToPhiDataType(tmp.dtype()),
                                                      tmp.dims(),
                                                      tmp.data_layout(),
                                                      tmp.lod(),
                                                      tmp.offset());
  VLOG(4) << "Builder construction  meta_tmp";
  paddle::dialect::IrMetaTensor meta_tmp(&ir_tensor_tmp);

  VLOG(4) << "Builder construction  dense_out_grad";
  paddle::dialect::IrTensor ir_tensor_out_grad(paddle::dialect::TransToPhiDataType(out_grad.dtype()),
                                                      out_grad.dims(),
                                                      out_grad.data_layout(),
                                                      out_grad.lod(),
                                                      out_grad.offset());
  VLOG(4) << "Builder construction  meta_out_grad";
  paddle::dialect::IrMetaTensor meta_out_grad(&ir_tensor_out_grad);
  paddle::dialect::IrTensor dense_x_grad;
  paddle::dialect::IrMetaTensor meta_x_grad(&dense_x_grad);
  paddle::dialect::IrTensor dense_y_grad;
  paddle::dialect::IrMetaTensor meta_y_grad(&dense_y_grad);
  paddle::dialect::IrTensor dense_w_grad;
  paddle::dialect::IrMetaTensor meta_w_grad(&dense_w_grad);

  phi::MatchMatrixTensorGradInferMeta(meta_x, meta_y, meta_w, meta_tmp, meta_out_grad, dim_t, &meta_x_grad, &meta_y_grad, &meta_w_grad);

  std::vector<pir::Type> argument_outputs;
  pir::Type x_grad_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_x_grad.dtype()), dense_x_grad.dims(), dense_x_grad.layout(), dense_x_grad.lod(), dense_x_grad.offset());
  argument_outputs.push_back(x_grad_dense_tensor_type);

  pir::Type y_grad_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_y_grad.dtype()), dense_y_grad.dims(), dense_y_grad.layout(), dense_y_grad.lod(), dense_y_grad.offset());
  argument_outputs.push_back(y_grad_dense_tensor_type);

  pir::Type w_grad_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_w_grad.dtype()), dense_w_grad.dims(), dense_w_grad.layout(), dense_w_grad.lod(), dense_w_grad.offset());
  argument_outputs.push_back(w_grad_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void MatchMatrixTensorGradOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, pir::Value y_, pir::Value w_, pir::Value tmp_, pir::Value out_grad_, pir::AttributeMap attributes) {
  VLOG(4) << "Start build MatchMatrixTensorGradOp";


  IR_ENFORCE(
      attributes.find("dim_t") != attributes.end(),
          "'dim_t' Attribute is expected for MatchMatrixTensorGradOp. ");
  int dim_t = attributes.at("dim_t").dyn_cast<pir::Int32Attribute>().data();


  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_, y_, w_, tmp_, out_grad_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_dim_t = pir::Int32Attribute::get(pir::IrContext::Instance(), dim_t);
  argument.AddAttribute("dim_t", attr_dim_t);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;
  paddle::dialect::DenseTensorType y = y_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)y;
  paddle::dialect::DenseTensorType w = w_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)w;
  paddle::dialect::DenseTensorType tmp = tmp_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)tmp;
  paddle::dialect::DenseTensorType out_grad = out_grad_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)out_grad;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);

  VLOG(4) << "Builder construction  dense_y";
  paddle::dialect::IrTensor ir_tensor_y(paddle::dialect::TransToPhiDataType(y.dtype()),
                                                      y.dims(),
                                                      y.data_layout(),
                                                      y.lod(),
                                                      y.offset());
  VLOG(4) << "Builder construction  meta_y";
  paddle::dialect::IrMetaTensor meta_y(&ir_tensor_y);

  VLOG(4) << "Builder construction  dense_w";
  paddle::dialect::IrTensor ir_tensor_w(paddle::dialect::TransToPhiDataType(w.dtype()),
                                                      w.dims(),
                                                      w.data_layout(),
                                                      w.lod(),
                                                      w.offset());
  VLOG(4) << "Builder construction  meta_w";
  paddle::dialect::IrMetaTensor meta_w(&ir_tensor_w);

  VLOG(4) << "Builder construction  dense_tmp";
  paddle::dialect::IrTensor ir_tensor_tmp(paddle::dialect::TransToPhiDataType(tmp.dtype()),
                                                      tmp.dims(),
                                                      tmp.data_layout(),
                                                      tmp.lod(),
                                                      tmp.offset());
  VLOG(4) << "Builder construction  meta_tmp";
  paddle::dialect::IrMetaTensor meta_tmp(&ir_tensor_tmp);

  VLOG(4) << "Builder construction  dense_out_grad";
  paddle::dialect::IrTensor ir_tensor_out_grad(paddle::dialect::TransToPhiDataType(out_grad.dtype()),
                                                      out_grad.dims(),
                                                      out_grad.data_layout(),
                                                      out_grad.lod(),
                                                      out_grad.offset());
  VLOG(4) << "Builder construction  meta_out_grad";
  paddle::dialect::IrMetaTensor meta_out_grad(&ir_tensor_out_grad);
  paddle::dialect::IrTensor dense_x_grad;
  paddle::dialect::IrMetaTensor meta_x_grad(&dense_x_grad);
  paddle::dialect::IrTensor dense_y_grad;
  paddle::dialect::IrMetaTensor meta_y_grad(&dense_y_grad);
  paddle::dialect::IrTensor dense_w_grad;
  paddle::dialect::IrMetaTensor meta_w_grad(&dense_w_grad);

  phi::MatchMatrixTensorGradInferMeta(meta_x, meta_y, meta_w, meta_tmp, meta_out_grad, dim_t, &meta_x_grad, &meta_y_grad, &meta_w_grad);

  std::vector<pir::Type> argument_outputs;
  pir::Type x_grad_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_x_grad.dtype()), dense_x_grad.dims(), dense_x_grad.layout(), dense_x_grad.lod(), dense_x_grad.offset());
  argument_outputs.push_back(x_grad_dense_tensor_type);

  pir::Type y_grad_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_y_grad.dtype()), dense_y_grad.dims(), dense_y_grad.layout(), dense_y_grad.lod(), dense_y_grad.offset());
  argument_outputs.push_back(y_grad_dense_tensor_type);

  pir::Type w_grad_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_w_grad.dtype()), dense_w_grad.dims(), dense_w_grad.layout(), dense_w_grad.lod(), dense_w_grad.offset());
  argument_outputs.push_back(w_grad_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void MatchMatrixTensorGradOp::VerifySig() {}

void MatchMatrixTensorGradOp::InferMeta( phi::InferMetaContext *infer_meta ) {
  auto fn = PD_INFER_META(phi::MatchMatrixTensorGradInferMeta);
  fn(infer_meta);
}

phi::DataType MatchMatrixTensorGradOp::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: MatchMatrixTensorGradOp";
  


  return expected_kernel_dtype;
}

} // namespace dialect
} // namespace paddle


IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::AddDoubleGradOp)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::AddDoubleGrad_Op)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::AddGradOp)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::AddGrad_Op)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::AddTripleGradOp)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::AddTripleGrad_Op)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::AmaxGradOp)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::AminGradOp)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::AssignGradOp)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::AssignOutGradOp)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::AssignOutGrad_Op)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::BatchNormDoubleGradOp)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::BatchNormDoubleGrad_Op)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::BatchNormGradOp)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::CEmbeddingGradOp)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::CSoftmaxWithCrossEntropyGradOp)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::CastGradOp)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::ChannelShuffleGradOp)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::Conv2dTransposeDoubleGradOp)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::Conv2dTransposeGradOp)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::DeformableConvGradOp)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::DepthwiseConv2dTransposeGradOp)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::DivideDoubleGradOp)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::DivideDoubleGrad_Op)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::DivideGradOp)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::DropoutGradOp)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::EinsumGradOp)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::ElementwisePowGradOp)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::EmbeddingGradOp)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::EmbeddingSparseGradOp)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::SparseWeightEmbeddingGradOp)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::SparseWeightEmbeddingSparseGradOp)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::ExponentialGradOp)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::FrobeniusNormGradOp)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::FusedAttentionGradOp)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::FusedBatchNormActGradOp)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::FusedBnAddActivationGradOp)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::FusedFeedforwardGradOp)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::FusedSoftmaxMaskUpperTriangleGradOp)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::HardswishGradOp)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::HardswishGrad_Op)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::HsigmoidLossGradOp)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::LogsumexpGradOp)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::MatmulDoubleGradOp)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::MatmulGradOp)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::MaxGradOp)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::MaximumGradOp)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::MeanDoubleGradOp)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::MeanGradOp)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::MinGradOp)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::MinimumGradOp)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::MishGradOp)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::MishGrad_Op)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::MultiplyDoubleGradOp)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::MultiplyDoubleGrad_Op)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::MultiplyGradOp)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::MultiplyTripleGradOp)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::NceGradOp)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::NormGradOp)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::PadDoubleGradOp)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::PadGradOp)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::Pool2dDoubleGradOp)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::Pool2dGradOp)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::Pool3dGradOp)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::ProdGradOp)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::RepeatInterleaveGradOp)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::RepeatInterleaveWithTensorIndexGradOp)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::ReshapeDoubleGradOp)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::ReshapeDoubleGrad_Op)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::ReshapeGradOp)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::ReshapeGrad_Op)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::RnnGradOp)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::RowConvGradOp)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::RreluGradOp)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::SetValueGradOp)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::SetValueWithTensorGradOp)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::SliceDoubleGradOp)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::SliceGradOp)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::SoftReluGradOp)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::SoftmaxGradOp)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::SplitWithNumGradOp)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::StridedSliceGradOp)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::SubtractDoubleGradOp)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::SubtractDoubleGrad_Op)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::SubtractGradOp)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::SubtractGrad_Op)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::SumDoubleGradOp)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::SumGradOp)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::SwishGradOp)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::SwishGrad_Op)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::SyncBatchNormGradOp)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::TileDoubleGradOp)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::TileGradOp)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::TransLayoutGradOp)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::TransposeDoubleGradOp)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::TransposeGradOp)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::TrilGradOp)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::TriuGradOp)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::DisableCheckModelNanInfGradOp)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::EnableCheckModelNanInfGradOp)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::FusedElemwiseAddActivationGradOp)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::ShuffleBatchGradOp)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::UnpoolGradOp)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::MatchMatrixTensorGradOp)

