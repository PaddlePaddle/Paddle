// This file is generated by "paddle/fluid/pir/dialect/op_generator/op_gen.py"
#include "paddle/fluid/pir/dialect/operator/ir/pd_op.h"
#include "/home/aistudio/fix_op/Paddle/paddle/fluid/pir/dialect/operator/ir/pd_onednn_op.h"
#include "paddle/fluid/pir/dialect/operator/ir/op_type.h"
#include "paddle/fluid/pir/dialect/operator/ir/op_attribute.h"
#include "paddle/fluid/pir/dialect/operator/ir/ir_tensor.h"
#include "paddle/fluid/pir/dialect/operator/ir/ir_selected_rows.h"
#include "paddle/fluid/pir/dialect/operator/ir/ir_meta_tensor.h"
#include "paddle/pir/core/builtin_attribute.h"
#include "paddle/pir/core/builtin_type.h"
#include "paddle/pir/core/builtin_op.h"
#include "paddle/pir/core/ir_context.h"
#include "paddle/phi/core/enforce.h"
#include "paddle/phi/core/dense_tensor.h"
#include "paddle/phi/infermeta/binary.h"
#include "paddle/phi/infermeta/multiary.h"
#include "paddle/phi/infermeta/nullary.h"
#include "paddle/phi/infermeta/unary.h"
#include "paddle/phi/infermeta/ternary.h"
#include "paddle/phi/infermeta/backward.h"
#include "paddle/phi/infermeta/fusion.h"
#include "paddle/phi/api/lib/utils/allocator.h"
#include "paddle/fluid/primitive/rule/vjp/vjp.h"
#include "paddle/pir/core/op_base.h"

namespace paddle {
namespace onednn {
namespace dialect {

const char *QuantizeOp::attributes_name[5] = { "is_negative_input", "scale", "shift", "output_format", "bfloat16" };

OpInfoTuple QuantizeOp::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("input", "paddle::dialect::DenseTensorType", false, false, false, false) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = { paddle::dialect::OpAttributeInfo("is_negative_input", "pir::BoolAttribute", ""), paddle::dialect::OpAttributeInfo("scale", "pir::FloatAttribute", ""), paddle::dialect::OpAttributeInfo("shift", "pir::FloatAttribute", ""), paddle::dialect::OpAttributeInfo("output_format", "pir::StrAttribute", ""), paddle::dialect::OpAttributeInfo("bfloat16", "pir::BoolAttribute", "") };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("output", "paddle::dialect::DenseTensorType", false, false) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("UnchangedInferMeta", {"input"}, "quantize", {"input", "is_negative_input", "scale", "shift", "output_format", "bfloat16"}, {"input"}, {}, {}, {}, {}, "", {}, false, false);
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "quantize");
}

void QuantizeOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value input_, bool is_negative_input, float scale, float shift, const std::string& output_format, bool bfloat16) {
  VLOG(4) << "Start build QuantizeOp";



  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {input_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_is_negative_input = pir::BoolAttribute::get(pir::IrContext::Instance(), is_negative_input);
  argument.AddAttribute("is_negative_input", attr_is_negative_input);
  pir::Attribute attr_scale = pir::FloatAttribute::get(pir::IrContext::Instance(), scale);
  argument.AddAttribute("scale", attr_scale);
  pir::Attribute attr_shift = pir::FloatAttribute::get(pir::IrContext::Instance(), shift);
  argument.AddAttribute("shift", attr_shift);
  pir::Attribute attr_output_format = pir::StrAttribute::get(pir::IrContext::Instance(), output_format);
  argument.AddAttribute("output_format", attr_output_format);
  pir::Attribute attr_bfloat16 = pir::BoolAttribute::get(pir::IrContext::Instance(), bfloat16);
  argument.AddAttribute("bfloat16", attr_bfloat16);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType input = input_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)input;

  VLOG(4) << "Builder construction  dense_input";
  paddle::dialect::IrTensor ir_tensor_input(paddle::dialect::TransToPhiDataType(input.dtype()),
                                                      input.dims(),
                                                      input.data_layout(),
                                                      input.lod(),
                                                      input.offset());
  VLOG(4) << "Builder construction  meta_input";
  paddle::dialect::IrMetaTensor meta_input(&ir_tensor_input);
  paddle::dialect::IrTensor dense_output;
  paddle::dialect::IrMetaTensor meta_output(&dense_output);

  phi::UnchangedInferMeta(meta_input, &meta_output);

  std::vector<pir::Type> argument_outputs;
  pir::Type output_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_output.dtype()), dense_output.dims(), dense_output.layout(), dense_output.lod(), dense_output.offset());
  argument_outputs.push_back(output_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void QuantizeOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value input_, pir::AttributeMap attributes) {
  VLOG(4) << "Start build QuantizeOp";


  IR_ENFORCE(
      attributes.find("is_negative_input") != attributes.end(),
          "'is_negative_input' Attribute is expected for QuantizeOp. ");
  bool is_negative_input = attributes.at("is_negative_input").dyn_cast<pir::BoolAttribute>().data();

  IR_ENFORCE(
      attributes.find("scale") != attributes.end(),
          "'scale' Attribute is expected for QuantizeOp. ");
  float scale = attributes.at("scale").dyn_cast<pir::FloatAttribute>().data();

  IR_ENFORCE(
      attributes.find("shift") != attributes.end(),
          "'shift' Attribute is expected for QuantizeOp. ");
  float shift = attributes.at("shift").dyn_cast<pir::FloatAttribute>().data();

  IR_ENFORCE(
      attributes.find("output_format") != attributes.end(),
          "'output_format' Attribute is expected for QuantizeOp. ");
  std::string output_format = attributes.at("output_format").dyn_cast<pir::StrAttribute>().AsString();

  IR_ENFORCE(
      attributes.find("bfloat16") != attributes.end(),
          "'bfloat16' Attribute is expected for QuantizeOp. ");
  bool bfloat16 = attributes.at("bfloat16").dyn_cast<pir::BoolAttribute>().data();


  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {input_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_is_negative_input = pir::BoolAttribute::get(pir::IrContext::Instance(), is_negative_input);
  argument.AddAttribute("is_negative_input", attr_is_negative_input);
  pir::Attribute attr_scale = pir::FloatAttribute::get(pir::IrContext::Instance(), scale);
  argument.AddAttribute("scale", attr_scale);
  pir::Attribute attr_shift = pir::FloatAttribute::get(pir::IrContext::Instance(), shift);
  argument.AddAttribute("shift", attr_shift);
  pir::Attribute attr_output_format = pir::StrAttribute::get(pir::IrContext::Instance(), output_format);
  argument.AddAttribute("output_format", attr_output_format);
  pir::Attribute attr_bfloat16 = pir::BoolAttribute::get(pir::IrContext::Instance(), bfloat16);
  argument.AddAttribute("bfloat16", attr_bfloat16);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType input = input_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)input;

  VLOG(4) << "Builder construction  dense_input";
  paddle::dialect::IrTensor ir_tensor_input(paddle::dialect::TransToPhiDataType(input.dtype()),
                                                      input.dims(),
                                                      input.data_layout(),
                                                      input.lod(),
                                                      input.offset());
  VLOG(4) << "Builder construction  meta_input";
  paddle::dialect::IrMetaTensor meta_input(&ir_tensor_input);
  paddle::dialect::IrTensor dense_output;
  paddle::dialect::IrMetaTensor meta_output(&dense_output);

  phi::UnchangedInferMeta(meta_input, &meta_output);

  std::vector<pir::Type> argument_outputs;
  pir::Type output_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_output.dtype()), dense_output.dims(), dense_output.layout(), dense_output.lod(), dense_output.offset());
  argument_outputs.push_back(output_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void QuantizeOp::VerifySig() {
  VLOG(4) << "Start Verifying inputs, outputs and attributes for: QuantizeOp.";
  VLOG(4) << "Verifying inputs:";
  {
  auto input_size = num_operands();
  IR_ENFORCE(input_size == 1u,
                    "The size %d of inputs must be equal to 1.", input_size);
  IR_ENFORCE((*this)->operand_source(0).type().isa<paddle::dialect::DenseTensorType>(),
                  "Type validation failed for the 0th input, got %s.", (*this)->operand_source(0).type());
  }
  VLOG(4) << "Verifying attributes:";
  {
  auto& attributes = this->attributes();
  IR_ENFORCE(attributes.count("is_negative_input")>0,
                 "is_negative_input does not exist.");
  IR_ENFORCE(attributes.at("is_negative_input").isa<pir::BoolAttribute>(),
                 "Type of attribute: is_negative_input is not pir::BoolAttribute.");

  IR_ENFORCE(attributes.count("scale")>0,
                 "scale does not exist.");
  IR_ENFORCE(attributes.at("scale").isa<pir::FloatAttribute>(),
                 "Type of attribute: scale is not pir::FloatAttribute.");

  IR_ENFORCE(attributes.count("shift")>0,
                 "shift does not exist.");
  IR_ENFORCE(attributes.at("shift").isa<pir::FloatAttribute>(),
                 "Type of attribute: shift is not pir::FloatAttribute.");

  IR_ENFORCE(attributes.count("output_format")>0,
                 "output_format does not exist.");
  IR_ENFORCE(attributes.at("output_format").isa<pir::StrAttribute>(),
                 "Type of attribute: output_format is not pir::StrAttribute.");

  IR_ENFORCE(attributes.count("bfloat16")>0,
                 "bfloat16 does not exist.");
  IR_ENFORCE(attributes.at("bfloat16").isa<pir::BoolAttribute>(),
                 "Type of attribute: bfloat16 is not pir::BoolAttribute.");

  }
  VLOG(4) << "Verifying outputs:";
  {
  auto output_size = num_results();
  IR_ENFORCE(output_size == 1u,
                    "The size %d of outputs must be equal to 1.", output_size);
  IR_ENFORCE((*this)->result(0).type().isa<paddle::dialect::DenseTensorType>(),
                 "Type validation failed for the 0th output.");
  }
  VLOG(4) << "End Verifying for: QuantizeOp.";
}

void QuantizeOp::InferMeta( phi::InferMetaContext *infer_meta ) {
  auto fn = PD_INFER_META(phi::UnchangedInferMeta);
  fn(infer_meta);
}

phi::DataType QuantizeOp::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: QuantizeOp";
  


  return expected_kernel_dtype;
}

const char *Conv2dOp::attributes_name[7] = { "strides", "paddings", "padding_algorithm", "dilations", "groups", "data_format", "is_test" };

OpInfoTuple Conv2dOp::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("input", "paddle::dialect::DenseTensorType", false, false, false, true), paddle::dialect::OpInputInfo("filter", "paddle::dialect::DenseTensorType", false, false, false, true) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = { paddle::dialect::OpAttributeInfo("strides", "pir::ArrayAttribute<pir::Int32Attribute>", ""), paddle::dialect::OpAttributeInfo("paddings", "pir::ArrayAttribute<pir::Int32Attribute>", ""), paddle::dialect::OpAttributeInfo("padding_algorithm", "pir::StrAttribute", ""), paddle::dialect::OpAttributeInfo("dilations", "pir::ArrayAttribute<pir::Int32Attribute>", ""), paddle::dialect::OpAttributeInfo("groups", "pir::Int32Attribute", ""), paddle::dialect::OpAttributeInfo("data_format", "pir::StrAttribute", ""), paddle::dialect::OpAttributeInfo("is_test", "pir::BoolAttribute", "") };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("out", "paddle::dialect::DenseTensorType", false, false) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("ConvInferMeta", {"input", "filter", "strides", "paddings", "padding_algorithm", "dilations", "groups", "data_format"}, "conv2d", {"input", "filter", "strides", "paddings", "padding_algorithm", "dilations", "groups", "data_format"}, {}, {}, {}, {}, {"is_test"}, "data_format", {"input"}, false, false);
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "conv2d");
}

void Conv2dOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value input_, pir::Value filter_, const std::vector<int>& strides, const std::vector<int>& paddings, const std::string& padding_algorithm, const std::vector<int>& dilations, int groups, const std::string& data_format, bool is_test) {
  VLOG(4) << "Start build Conv2dOp";



  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {input_, filter_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  std::vector<pir::Attribute> vec_strides;
  for (size_t i = 0; i < static_cast<size_t>(strides.size()); i++) {
      pir::Attribute attr_strides = pir::Int32Attribute::get(pir::IrContext::Instance(), strides[i]);

    vec_strides.push_back(attr_strides);
  }
  pir::Attribute attr_strides = pir::ArrayAttribute::get(pir::IrContext::Instance(), vec_strides);
  argument.AddAttribute("strides", attr_strides);
  std::vector<pir::Attribute> vec_paddings;
  for (size_t i = 0; i < static_cast<size_t>(paddings.size()); i++) {
      pir::Attribute attr_paddings = pir::Int32Attribute::get(pir::IrContext::Instance(), paddings[i]);

    vec_paddings.push_back(attr_paddings);
  }
  pir::Attribute attr_paddings = pir::ArrayAttribute::get(pir::IrContext::Instance(), vec_paddings);
  argument.AddAttribute("paddings", attr_paddings);
  pir::Attribute attr_padding_algorithm = pir::StrAttribute::get(pir::IrContext::Instance(), padding_algorithm);
  argument.AddAttribute("padding_algorithm", attr_padding_algorithm);
  std::vector<pir::Attribute> vec_dilations;
  for (size_t i = 0; i < static_cast<size_t>(dilations.size()); i++) {
      pir::Attribute attr_dilations = pir::Int32Attribute::get(pir::IrContext::Instance(), dilations[i]);

    vec_dilations.push_back(attr_dilations);
  }
  pir::Attribute attr_dilations = pir::ArrayAttribute::get(pir::IrContext::Instance(), vec_dilations);
  argument.AddAttribute("dilations", attr_dilations);
  pir::Attribute attr_groups = pir::Int32Attribute::get(pir::IrContext::Instance(), groups);
  argument.AddAttribute("groups", attr_groups);
  pir::Attribute attr_data_format = pir::StrAttribute::get(pir::IrContext::Instance(), data_format);
  argument.AddAttribute("data_format", attr_data_format);
  pir::Attribute attr_is_test = pir::BoolAttribute::get(pir::IrContext::Instance(), is_test);
  argument.AddAttribute("is_test", attr_is_test);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType input = input_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)input;
  paddle::dialect::DenseTensorType filter = filter_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)filter;

  VLOG(4) << "Builder construction  dense_input";
  paddle::dialect::IrTensor ir_tensor_input(paddle::dialect::TransToPhiDataType(input.dtype()),
                                                      input.dims(),
                                                      input.data_layout(),
                                                      input.lod(),
                                                      input.offset());
  VLOG(4) << "Builder construction  meta_input";
  paddle::dialect::IrMetaTensor meta_input(&ir_tensor_input);

  VLOG(4) << "Builder construction  dense_filter";
  paddle::dialect::IrTensor ir_tensor_filter(paddle::dialect::TransToPhiDataType(filter.dtype()),
                                                      filter.dims(),
                                                      filter.data_layout(),
                                                      filter.lod(),
                                                      filter.offset());
  VLOG(4) << "Builder construction  meta_filter";
  paddle::dialect::IrMetaTensor meta_filter(&ir_tensor_filter);
  paddle::dialect::IrTensor dense_out;
  paddle::dialect::IrMetaTensor meta_out(&dense_out);

  phi::ConvInferMeta(meta_input, meta_filter, strides, paddings, padding_algorithm, dilations, groups, data_format, &meta_out);

  std::vector<pir::Type> argument_outputs;
  pir::Type out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_out.dtype()), dense_out.dims(), dense_out.layout(), dense_out.lod(), dense_out.offset());
  argument_outputs.push_back(out_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void Conv2dOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value input_, pir::Value filter_, pir::AttributeMap attributes) {
  VLOG(4) << "Start build Conv2dOp";


  IR_ENFORCE(
      attributes.find("strides") != attributes.end(),
          "'strides' Attribute is expected for Conv2dOp. ");
  std::vector<int> strides;
  for (size_t i = 0; i < attributes.at("strides").dyn_cast<pir::ArrayAttribute>().size(); i++) {
    strides.push_back(attributes.at("strides").dyn_cast<pir::ArrayAttribute>().at(i).dyn_cast<pir::Int32Attribute>().data());
  }

  IR_ENFORCE(
      attributes.find("paddings") != attributes.end(),
          "'paddings' Attribute is expected for Conv2dOp. ");
  std::vector<int> paddings;
  for (size_t i = 0; i < attributes.at("paddings").dyn_cast<pir::ArrayAttribute>().size(); i++) {
    paddings.push_back(attributes.at("paddings").dyn_cast<pir::ArrayAttribute>().at(i).dyn_cast<pir::Int32Attribute>().data());
  }

  IR_ENFORCE(
      attributes.find("padding_algorithm") != attributes.end(),
          "'padding_algorithm' Attribute is expected for Conv2dOp. ");
  std::string padding_algorithm = attributes.at("padding_algorithm").dyn_cast<pir::StrAttribute>().AsString();

  IR_ENFORCE(
      attributes.find("dilations") != attributes.end(),
          "'dilations' Attribute is expected for Conv2dOp. ");
  std::vector<int> dilations;
  for (size_t i = 0; i < attributes.at("dilations").dyn_cast<pir::ArrayAttribute>().size(); i++) {
    dilations.push_back(attributes.at("dilations").dyn_cast<pir::ArrayAttribute>().at(i).dyn_cast<pir::Int32Attribute>().data());
  }

  IR_ENFORCE(
      attributes.find("groups") != attributes.end(),
          "'groups' Attribute is expected for Conv2dOp. ");
  int groups = attributes.at("groups").dyn_cast<pir::Int32Attribute>().data();

  IR_ENFORCE(
      attributes.find("data_format") != attributes.end(),
          "'data_format' Attribute is expected for Conv2dOp. ");
  std::string data_format = attributes.at("data_format").dyn_cast<pir::StrAttribute>().AsString();

  IR_ENFORCE(
      attributes.find("is_test") != attributes.end(),
          "'is_test' Attribute is expected for Conv2dOp. ");
  bool is_test = attributes.at("is_test").dyn_cast<pir::BoolAttribute>().data();


  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {input_, filter_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  std::vector<pir::Attribute> vec_strides;
  for (size_t i = 0; i < static_cast<size_t>(strides.size()); i++) {
      pir::Attribute attr_strides = pir::Int32Attribute::get(pir::IrContext::Instance(), strides[i]);

    vec_strides.push_back(attr_strides);
  }
  pir::Attribute attr_strides = pir::ArrayAttribute::get(pir::IrContext::Instance(), vec_strides);
  argument.AddAttribute("strides", attr_strides);
  std::vector<pir::Attribute> vec_paddings;
  for (size_t i = 0; i < static_cast<size_t>(paddings.size()); i++) {
      pir::Attribute attr_paddings = pir::Int32Attribute::get(pir::IrContext::Instance(), paddings[i]);

    vec_paddings.push_back(attr_paddings);
  }
  pir::Attribute attr_paddings = pir::ArrayAttribute::get(pir::IrContext::Instance(), vec_paddings);
  argument.AddAttribute("paddings", attr_paddings);
  pir::Attribute attr_padding_algorithm = pir::StrAttribute::get(pir::IrContext::Instance(), padding_algorithm);
  argument.AddAttribute("padding_algorithm", attr_padding_algorithm);
  std::vector<pir::Attribute> vec_dilations;
  for (size_t i = 0; i < static_cast<size_t>(dilations.size()); i++) {
      pir::Attribute attr_dilations = pir::Int32Attribute::get(pir::IrContext::Instance(), dilations[i]);

    vec_dilations.push_back(attr_dilations);
  }
  pir::Attribute attr_dilations = pir::ArrayAttribute::get(pir::IrContext::Instance(), vec_dilations);
  argument.AddAttribute("dilations", attr_dilations);
  pir::Attribute attr_groups = pir::Int32Attribute::get(pir::IrContext::Instance(), groups);
  argument.AddAttribute("groups", attr_groups);
  pir::Attribute attr_data_format = pir::StrAttribute::get(pir::IrContext::Instance(), data_format);
  argument.AddAttribute("data_format", attr_data_format);
  pir::Attribute attr_is_test = pir::BoolAttribute::get(pir::IrContext::Instance(), is_test);
  argument.AddAttribute("is_test", attr_is_test);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType input = input_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)input;
  paddle::dialect::DenseTensorType filter = filter_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)filter;

  VLOG(4) << "Builder construction  dense_input";
  paddle::dialect::IrTensor ir_tensor_input(paddle::dialect::TransToPhiDataType(input.dtype()),
                                                      input.dims(),
                                                      input.data_layout(),
                                                      input.lod(),
                                                      input.offset());
  VLOG(4) << "Builder construction  meta_input";
  paddle::dialect::IrMetaTensor meta_input(&ir_tensor_input);

  VLOG(4) << "Builder construction  dense_filter";
  paddle::dialect::IrTensor ir_tensor_filter(paddle::dialect::TransToPhiDataType(filter.dtype()),
                                                      filter.dims(),
                                                      filter.data_layout(),
                                                      filter.lod(),
                                                      filter.offset());
  VLOG(4) << "Builder construction  meta_filter";
  paddle::dialect::IrMetaTensor meta_filter(&ir_tensor_filter);
  paddle::dialect::IrTensor dense_out;
  paddle::dialect::IrMetaTensor meta_out(&dense_out);

  phi::ConvInferMeta(meta_input, meta_filter, strides, paddings, padding_algorithm, dilations, groups, data_format, &meta_out);

  std::vector<pir::Type> argument_outputs;
  pir::Type out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_out.dtype()), dense_out.dims(), dense_out.layout(), dense_out.lod(), dense_out.offset());
  argument_outputs.push_back(out_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void Conv2dOp::VerifySig() {
  VLOG(4) << "Start Verifying inputs, outputs and attributes for: Conv2dOp.";
  VLOG(4) << "Verifying inputs:";
  {
  auto input_size = num_operands();
  IR_ENFORCE(input_size == 2u,
                    "The size %d of inputs must be equal to 2.", input_size);
  IR_ENFORCE((*this)->operand_source(0).type().isa<paddle::dialect::DenseTensorType>(),
                  "Type validation failed for the 0th input, got %s.", (*this)->operand_source(0).type());
  IR_ENFORCE((*this)->operand_source(1).type().isa<paddle::dialect::DenseTensorType>(),
                  "Type validation failed for the 1th input, got %s.", (*this)->operand_source(1).type());
  }
  VLOG(4) << "Verifying attributes:";
  {
  auto& attributes = this->attributes();
  IR_ENFORCE(attributes.count("strides")>0,
                 "strides does not exist.");
  IR_ENFORCE(attributes.at("strides").isa<pir::ArrayAttribute>(),
                 "Type of attribute: strides is not pir::ArrayAttribute.");
  for (size_t i = 0; i < attributes.at("strides").dyn_cast<pir::ArrayAttribute>().size(); i++) {
    IR_ENFORCE(attributes.at("strides").dyn_cast<pir::ArrayAttribute>().at(i).isa<pir::Int32Attribute>(),
                   "Type of attribute: strides is not right.");
  }
  IR_ENFORCE(attributes.count("paddings")>0,
                 "paddings does not exist.");
  IR_ENFORCE(attributes.at("paddings").isa<pir::ArrayAttribute>(),
                 "Type of attribute: paddings is not pir::ArrayAttribute.");
  for (size_t i = 0; i < attributes.at("paddings").dyn_cast<pir::ArrayAttribute>().size(); i++) {
    IR_ENFORCE(attributes.at("paddings").dyn_cast<pir::ArrayAttribute>().at(i).isa<pir::Int32Attribute>(),
                   "Type of attribute: paddings is not right.");
  }
  IR_ENFORCE(attributes.count("padding_algorithm")>0,
                 "padding_algorithm does not exist.");
  IR_ENFORCE(attributes.at("padding_algorithm").isa<pir::StrAttribute>(),
                 "Type of attribute: padding_algorithm is not pir::StrAttribute.");

  IR_ENFORCE(attributes.count("dilations")>0,
                 "dilations does not exist.");
  IR_ENFORCE(attributes.at("dilations").isa<pir::ArrayAttribute>(),
                 "Type of attribute: dilations is not pir::ArrayAttribute.");
  for (size_t i = 0; i < attributes.at("dilations").dyn_cast<pir::ArrayAttribute>().size(); i++) {
    IR_ENFORCE(attributes.at("dilations").dyn_cast<pir::ArrayAttribute>().at(i).isa<pir::Int32Attribute>(),
                   "Type of attribute: dilations is not right.");
  }
  IR_ENFORCE(attributes.count("groups")>0,
                 "groups does not exist.");
  IR_ENFORCE(attributes.at("groups").isa<pir::Int32Attribute>(),
                 "Type of attribute: groups is not pir::Int32Attribute.");

  IR_ENFORCE(attributes.count("data_format")>0,
                 "data_format does not exist.");
  IR_ENFORCE(attributes.at("data_format").isa<pir::StrAttribute>(),
                 "Type of attribute: data_format is not pir::StrAttribute.");

  IR_ENFORCE(attributes.count("is_test")>0,
                 "is_test does not exist.");
  IR_ENFORCE(attributes.at("is_test").isa<pir::BoolAttribute>(),
                 "Type of attribute: is_test is not pir::BoolAttribute.");

  }
  VLOG(4) << "Verifying outputs:";
  {
  auto output_size = num_results();
  IR_ENFORCE(output_size == 1u,
                    "The size %d of outputs must be equal to 1.", output_size);
  IR_ENFORCE((*this)->result(0).type().isa<paddle::dialect::DenseTensorType>(),
                 "Type validation failed for the 0th output.");
  }
  VLOG(4) << "End Verifying for: Conv2dOp.";
}

void Conv2dOp::InferMeta( phi::InferMetaContext *infer_meta ) {
  auto fn = PD_INFER_META(phi::ConvInferMeta);
  fn(infer_meta);
}

phi::DataType Conv2dOp::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: Conv2dOp";
  


  return expected_kernel_dtype;
}

const char *Conv2dGradOp::attributes_name[7] = { "strides", "paddings", "padding_algorithm", "dilations", "groups", "data_format", "is_test" };

OpInfoTuple Conv2dGradOp::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("input", "paddle::dialect::DenseTensorType", false, false, false, false), paddle::dialect::OpInputInfo("filter", "paddle::dialect::DenseTensorType", false, false, false, false), paddle::dialect::OpInputInfo("out_grad", "paddle::dialect::DenseTensorType", false, false, false, false) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = { paddle::dialect::OpAttributeInfo("strides", "pir::ArrayAttribute<pir::Int32Attribute>", ""), paddle::dialect::OpAttributeInfo("paddings", "pir::ArrayAttribute<pir::Int32Attribute>", ""), paddle::dialect::OpAttributeInfo("padding_algorithm", "pir::StrAttribute", ""), paddle::dialect::OpAttributeInfo("dilations", "pir::ArrayAttribute<pir::Int32Attribute>", ""), paddle::dialect::OpAttributeInfo("groups", "pir::Int32Attribute", ""), paddle::dialect::OpAttributeInfo("data_format", "pir::StrAttribute", ""), paddle::dialect::OpAttributeInfo("is_test", "pir::BoolAttribute", "") };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("input_grad", "paddle::dialect::DenseTensorType", false, false), paddle::dialect::OpOutputInfo("filter_grad", "paddle::dialect::DenseTensorType", false, false) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("GeneralBinaryGradInferMeta", {"input", "filter"}, "conv2d_grad", {"input", "filter", "out_grad", "strides", "paddings", "padding_algorithm", "dilations", "groups", "data_format"}, {"input"}, {}, {}, {}, {"is_test"}, "data_format", {"input", "out_grad"}, false, false);
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "conv2d_grad");
}

void Conv2dGradOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value input_, pir::Value filter_, pir::Value out_grad_, const std::vector<int>& strides, const std::vector<int>& paddings, const std::string& padding_algorithm, const std::vector<int>& dilations, int groups, const std::string& data_format, bool is_test) {
  VLOG(4) << "Start build Conv2dGradOp";



  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {input_, filter_, out_grad_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  std::vector<pir::Attribute> vec_strides;
  for (size_t i = 0; i < static_cast<size_t>(strides.size()); i++) {
      pir::Attribute attr_strides = pir::Int32Attribute::get(pir::IrContext::Instance(), strides[i]);

    vec_strides.push_back(attr_strides);
  }
  pir::Attribute attr_strides = pir::ArrayAttribute::get(pir::IrContext::Instance(), vec_strides);
  argument.AddAttribute("strides", attr_strides);
  std::vector<pir::Attribute> vec_paddings;
  for (size_t i = 0; i < static_cast<size_t>(paddings.size()); i++) {
      pir::Attribute attr_paddings = pir::Int32Attribute::get(pir::IrContext::Instance(), paddings[i]);

    vec_paddings.push_back(attr_paddings);
  }
  pir::Attribute attr_paddings = pir::ArrayAttribute::get(pir::IrContext::Instance(), vec_paddings);
  argument.AddAttribute("paddings", attr_paddings);
  pir::Attribute attr_padding_algorithm = pir::StrAttribute::get(pir::IrContext::Instance(), padding_algorithm);
  argument.AddAttribute("padding_algorithm", attr_padding_algorithm);
  std::vector<pir::Attribute> vec_dilations;
  for (size_t i = 0; i < static_cast<size_t>(dilations.size()); i++) {
      pir::Attribute attr_dilations = pir::Int32Attribute::get(pir::IrContext::Instance(), dilations[i]);

    vec_dilations.push_back(attr_dilations);
  }
  pir::Attribute attr_dilations = pir::ArrayAttribute::get(pir::IrContext::Instance(), vec_dilations);
  argument.AddAttribute("dilations", attr_dilations);
  pir::Attribute attr_groups = pir::Int32Attribute::get(pir::IrContext::Instance(), groups);
  argument.AddAttribute("groups", attr_groups);
  pir::Attribute attr_data_format = pir::StrAttribute::get(pir::IrContext::Instance(), data_format);
  argument.AddAttribute("data_format", attr_data_format);
  pir::Attribute attr_is_test = pir::BoolAttribute::get(pir::IrContext::Instance(), is_test);
  argument.AddAttribute("is_test", attr_is_test);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType input = input_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)input;
  paddle::dialect::DenseTensorType filter = filter_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)filter;
  paddle::dialect::DenseTensorType out_grad = out_grad_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)out_grad;

  VLOG(4) << "Builder construction  dense_input";
  paddle::dialect::IrTensor ir_tensor_input(paddle::dialect::TransToPhiDataType(input.dtype()),
                                                      input.dims(),
                                                      input.data_layout(),
                                                      input.lod(),
                                                      input.offset());
  VLOG(4) << "Builder construction  meta_input";
  paddle::dialect::IrMetaTensor meta_input(&ir_tensor_input);

  VLOG(4) << "Builder construction  dense_filter";
  paddle::dialect::IrTensor ir_tensor_filter(paddle::dialect::TransToPhiDataType(filter.dtype()),
                                                      filter.dims(),
                                                      filter.data_layout(),
                                                      filter.lod(),
                                                      filter.offset());
  VLOG(4) << "Builder construction  meta_filter";
  paddle::dialect::IrMetaTensor meta_filter(&ir_tensor_filter);
  paddle::dialect::IrTensor dense_input_grad;
  paddle::dialect::IrMetaTensor meta_input_grad(&dense_input_grad);
  paddle::dialect::IrTensor dense_filter_grad;
  paddle::dialect::IrMetaTensor meta_filter_grad(&dense_filter_grad);

  phi::GeneralBinaryGradInferMeta(meta_input, meta_filter, &meta_input_grad, &meta_filter_grad);

  std::vector<pir::Type> argument_outputs;
  pir::Type input_grad_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_input_grad.dtype()), dense_input_grad.dims(), dense_input_grad.layout(), dense_input_grad.lod(), dense_input_grad.offset());
  argument_outputs.push_back(input_grad_dense_tensor_type);

  pir::Type filter_grad_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_filter_grad.dtype()), dense_filter_grad.dims(), dense_filter_grad.layout(), dense_filter_grad.lod(), dense_filter_grad.offset());
  argument_outputs.push_back(filter_grad_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void Conv2dGradOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value input_, pir::Value filter_, pir::Value out_grad_, pir::AttributeMap attributes) {
  VLOG(4) << "Start build Conv2dGradOp";


  IR_ENFORCE(
      attributes.find("strides") != attributes.end(),
          "'strides' Attribute is expected for Conv2dGradOp. ");
  std::vector<int> strides;
  for (size_t i = 0; i < attributes.at("strides").dyn_cast<pir::ArrayAttribute>().size(); i++) {
    strides.push_back(attributes.at("strides").dyn_cast<pir::ArrayAttribute>().at(i).dyn_cast<pir::Int32Attribute>().data());
  }

  IR_ENFORCE(
      attributes.find("paddings") != attributes.end(),
          "'paddings' Attribute is expected for Conv2dGradOp. ");
  std::vector<int> paddings;
  for (size_t i = 0; i < attributes.at("paddings").dyn_cast<pir::ArrayAttribute>().size(); i++) {
    paddings.push_back(attributes.at("paddings").dyn_cast<pir::ArrayAttribute>().at(i).dyn_cast<pir::Int32Attribute>().data());
  }

  IR_ENFORCE(
      attributes.find("padding_algorithm") != attributes.end(),
          "'padding_algorithm' Attribute is expected for Conv2dGradOp. ");
  std::string padding_algorithm = attributes.at("padding_algorithm").dyn_cast<pir::StrAttribute>().AsString();

  IR_ENFORCE(
      attributes.find("dilations") != attributes.end(),
          "'dilations' Attribute is expected for Conv2dGradOp. ");
  std::vector<int> dilations;
  for (size_t i = 0; i < attributes.at("dilations").dyn_cast<pir::ArrayAttribute>().size(); i++) {
    dilations.push_back(attributes.at("dilations").dyn_cast<pir::ArrayAttribute>().at(i).dyn_cast<pir::Int32Attribute>().data());
  }

  IR_ENFORCE(
      attributes.find("groups") != attributes.end(),
          "'groups' Attribute is expected for Conv2dGradOp. ");
  int groups = attributes.at("groups").dyn_cast<pir::Int32Attribute>().data();

  IR_ENFORCE(
      attributes.find("data_format") != attributes.end(),
          "'data_format' Attribute is expected for Conv2dGradOp. ");
  std::string data_format = attributes.at("data_format").dyn_cast<pir::StrAttribute>().AsString();

  IR_ENFORCE(
      attributes.find("is_test") != attributes.end(),
          "'is_test' Attribute is expected for Conv2dGradOp. ");
  bool is_test = attributes.at("is_test").dyn_cast<pir::BoolAttribute>().data();


  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {input_, filter_, out_grad_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  std::vector<pir::Attribute> vec_strides;
  for (size_t i = 0; i < static_cast<size_t>(strides.size()); i++) {
      pir::Attribute attr_strides = pir::Int32Attribute::get(pir::IrContext::Instance(), strides[i]);

    vec_strides.push_back(attr_strides);
  }
  pir::Attribute attr_strides = pir::ArrayAttribute::get(pir::IrContext::Instance(), vec_strides);
  argument.AddAttribute("strides", attr_strides);
  std::vector<pir::Attribute> vec_paddings;
  for (size_t i = 0; i < static_cast<size_t>(paddings.size()); i++) {
      pir::Attribute attr_paddings = pir::Int32Attribute::get(pir::IrContext::Instance(), paddings[i]);

    vec_paddings.push_back(attr_paddings);
  }
  pir::Attribute attr_paddings = pir::ArrayAttribute::get(pir::IrContext::Instance(), vec_paddings);
  argument.AddAttribute("paddings", attr_paddings);
  pir::Attribute attr_padding_algorithm = pir::StrAttribute::get(pir::IrContext::Instance(), padding_algorithm);
  argument.AddAttribute("padding_algorithm", attr_padding_algorithm);
  std::vector<pir::Attribute> vec_dilations;
  for (size_t i = 0; i < static_cast<size_t>(dilations.size()); i++) {
      pir::Attribute attr_dilations = pir::Int32Attribute::get(pir::IrContext::Instance(), dilations[i]);

    vec_dilations.push_back(attr_dilations);
  }
  pir::Attribute attr_dilations = pir::ArrayAttribute::get(pir::IrContext::Instance(), vec_dilations);
  argument.AddAttribute("dilations", attr_dilations);
  pir::Attribute attr_groups = pir::Int32Attribute::get(pir::IrContext::Instance(), groups);
  argument.AddAttribute("groups", attr_groups);
  pir::Attribute attr_data_format = pir::StrAttribute::get(pir::IrContext::Instance(), data_format);
  argument.AddAttribute("data_format", attr_data_format);
  pir::Attribute attr_is_test = pir::BoolAttribute::get(pir::IrContext::Instance(), is_test);
  argument.AddAttribute("is_test", attr_is_test);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType input = input_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)input;
  paddle::dialect::DenseTensorType filter = filter_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)filter;
  paddle::dialect::DenseTensorType out_grad = out_grad_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)out_grad;

  VLOG(4) << "Builder construction  dense_input";
  paddle::dialect::IrTensor ir_tensor_input(paddle::dialect::TransToPhiDataType(input.dtype()),
                                                      input.dims(),
                                                      input.data_layout(),
                                                      input.lod(),
                                                      input.offset());
  VLOG(4) << "Builder construction  meta_input";
  paddle::dialect::IrMetaTensor meta_input(&ir_tensor_input);

  VLOG(4) << "Builder construction  dense_filter";
  paddle::dialect::IrTensor ir_tensor_filter(paddle::dialect::TransToPhiDataType(filter.dtype()),
                                                      filter.dims(),
                                                      filter.data_layout(),
                                                      filter.lod(),
                                                      filter.offset());
  VLOG(4) << "Builder construction  meta_filter";
  paddle::dialect::IrMetaTensor meta_filter(&ir_tensor_filter);
  paddle::dialect::IrTensor dense_input_grad;
  paddle::dialect::IrMetaTensor meta_input_grad(&dense_input_grad);
  paddle::dialect::IrTensor dense_filter_grad;
  paddle::dialect::IrMetaTensor meta_filter_grad(&dense_filter_grad);

  phi::GeneralBinaryGradInferMeta(meta_input, meta_filter, &meta_input_grad, &meta_filter_grad);

  std::vector<pir::Type> argument_outputs;
  pir::Type input_grad_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_input_grad.dtype()), dense_input_grad.dims(), dense_input_grad.layout(), dense_input_grad.lod(), dense_input_grad.offset());
  argument_outputs.push_back(input_grad_dense_tensor_type);

  pir::Type filter_grad_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_filter_grad.dtype()), dense_filter_grad.dims(), dense_filter_grad.layout(), dense_filter_grad.lod(), dense_filter_grad.offset());
  argument_outputs.push_back(filter_grad_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void Conv2dGradOp::VerifySig() {}

void Conv2dGradOp::InferMeta( phi::InferMetaContext *infer_meta ) {
  auto fn = PD_INFER_META(phi::GeneralBinaryGradInferMeta);
  fn(infer_meta);
}

phi::DataType Conv2dGradOp::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: Conv2dGradOp";
  


  return expected_kernel_dtype;
}

} // namespace dialect
} // namespace onednn
} // namespace paddle


IR_DEFINE_EXPLICIT_TYPE_ID(paddle::onednn::dialect::QuantizeOp)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::onednn::dialect::Conv2dOp)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::onednn::dialect::Conv2dGradOp)

