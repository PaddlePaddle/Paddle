// This file is generated by "paddle/fluid/pir/dialect/op_generator/op_gen.py"
#include "/home/aistudio/fix_op/Paddle/paddle/fluid/pir/dialect/operator/ir/pd_op.h"
#include "paddle/fluid/pir/dialect/operator/ir/op_type.h"
#include "paddle/fluid/pir/dialect/operator/ir/op_attribute.h"
#include "paddle/fluid/pir/dialect/operator/ir/ir_tensor.h"
#include "paddle/fluid/pir/dialect/operator/ir/ir_selected_rows.h"
#include "paddle/fluid/pir/dialect/operator/ir/ir_meta_tensor.h"
#include "paddle/pir/core/builtin_attribute.h"
#include "paddle/pir/core/builtin_type.h"
#include "paddle/pir/core/builtin_op.h"
#include "paddle/pir/core/ir_context.h"
#include "paddle/phi/core/enforce.h"
#include "paddle/phi/core/dense_tensor.h"
#include "paddle/phi/infermeta/binary.h"
#include "paddle/phi/infermeta/multiary.h"
#include "paddle/phi/infermeta/nullary.h"
#include "paddle/phi/infermeta/unary.h"
#include "paddle/phi/infermeta/ternary.h"
#include "paddle/phi/infermeta/backward.h"
#include "paddle/phi/infermeta/fusion.h"
#include "paddle/phi/api/lib/utils/allocator.h"
#include "paddle/fluid/primitive/rule/vjp/vjp.h"
#include "paddle/pir/core/op_base.h"

namespace paddle {
namespace dialect {

const char *AddActXpuOp::attributes_name[1] = { "act_type" };

OpInfoTuple AddActXpuOp::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("x", "paddle::dialect::DenseTensorType", false, false, false, false), paddle::dialect::OpInputInfo("x_max", "paddle::dialect::DenseTensorType", true, false, false, false), paddle::dialect::OpInputInfo("y", "paddle::dialect::DenseTensorType", false, false, false, false), paddle::dialect::OpInputInfo("y_max", "paddle::dialect::DenseTensorType", true, false, false, false) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = { paddle::dialect::OpAttributeInfo("act_type", "pir::Int32Attribute", "") };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("out", "paddle::dialect::DenseTensorType", false, false), paddle::dialect::OpOutputInfo("out_max", "paddle::dialect::DenseTensorType", false, false) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("AddActXPUInferMeta", {"x", "x_max", "y", "y_max", "act_type"}, "add_act_xpu", {"x", "x_max", "y", "y_max", "act_type"}, {"x"}, {}, {}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "add_act_xpu");
}

void AddActXpuOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, pir::Value x_max_, pir::Value y_, pir::Value y_max_, int act_type) {
  VLOG(4) << "Start build AddActXpuOp";



  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_, x_max_, y_, y_max_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_act_type = pir::Int32Attribute::get(pir::IrContext::Instance(), act_type);
  argument.AddAttribute("act_type", attr_act_type);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;
  paddle::dialect::DenseTensorType y = y_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)y;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);

  paddle::dialect::IrMetaTensor meta_x_max;
  paddle::dialect::IrTensor ir_tensor_x_max;
  if (x_max_.impl() != nullptr) {
    paddle::dialect::DenseTensorType x_max = x_max_.type().dyn_cast<paddle::dialect::DenseTensorType>();
    VLOG(4) << "Builder construction  dense_x_max";
    ir_tensor_x_max = paddle::dialect::IrTensor(paddle::dialect::TransToPhiDataType(x_max.dtype()),
                                                        x_max.dims(),
                                                        x_max.data_layout(),
                                                        x_max.lod(),
                                                        x_max.offset());
    VLOG(4) << "Builder construction  meta_x_max";
    meta_x_max = paddle::dialect::IrMetaTensor(&ir_tensor_x_max);
  }


  VLOG(4) << "Builder construction  dense_y";
  paddle::dialect::IrTensor ir_tensor_y(paddle::dialect::TransToPhiDataType(y.dtype()),
                                                      y.dims(),
                                                      y.data_layout(),
                                                      y.lod(),
                                                      y.offset());
  VLOG(4) << "Builder construction  meta_y";
  paddle::dialect::IrMetaTensor meta_y(&ir_tensor_y);

  paddle::dialect::IrMetaTensor meta_y_max;
  paddle::dialect::IrTensor ir_tensor_y_max;
  if (y_max_.impl() != nullptr) {
    paddle::dialect::DenseTensorType y_max = y_max_.type().dyn_cast<paddle::dialect::DenseTensorType>();
    VLOG(4) << "Builder construction  dense_y_max";
    ir_tensor_y_max = paddle::dialect::IrTensor(paddle::dialect::TransToPhiDataType(y_max.dtype()),
                                                        y_max.dims(),
                                                        y_max.data_layout(),
                                                        y_max.lod(),
                                                        y_max.offset());
    VLOG(4) << "Builder construction  meta_y_max";
    meta_y_max = paddle::dialect::IrMetaTensor(&ir_tensor_y_max);
  }

  paddle::dialect::IrTensor dense_out;
  paddle::dialect::IrMetaTensor meta_out(&dense_out);
  paddle::dialect::IrTensor dense_out_max;
  paddle::dialect::IrMetaTensor meta_out_max(&dense_out_max);

  phi::AddActXPUInferMeta(meta_x, meta_x_max, meta_y, meta_y_max, act_type, &meta_out, &meta_out_max);

  std::vector<pir::Type> argument_outputs;
  pir::Type out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_out.dtype()), dense_out.dims(), dense_out.layout(), dense_out.lod(), dense_out.offset());
  argument_outputs.push_back(out_dense_tensor_type);

  pir::Type out_max_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_out_max.dtype()), dense_out_max.dims(), dense_out_max.layout(), dense_out_max.lod(), dense_out_max.offset());
  argument_outputs.push_back(out_max_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void AddActXpuOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, pir::Value x_max_, pir::Value y_, pir::Value y_max_, pir::AttributeMap attributes) {
  VLOG(4) << "Start build AddActXpuOp";


  IR_ENFORCE(
      attributes.find("act_type") != attributes.end(),
          "'act_type' Attribute is expected for AddActXpuOp. ");
  int act_type = attributes.at("act_type").dyn_cast<pir::Int32Attribute>().data();


  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_, x_max_, y_, y_max_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_act_type = pir::Int32Attribute::get(pir::IrContext::Instance(), act_type);
  argument.AddAttribute("act_type", attr_act_type);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;
  paddle::dialect::DenseTensorType y = y_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)y;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);

  paddle::dialect::IrMetaTensor meta_x_max;
  paddle::dialect::IrTensor ir_tensor_x_max;
  if (x_max_.impl() != nullptr) {
    paddle::dialect::DenseTensorType x_max = x_max_.type().dyn_cast<paddle::dialect::DenseTensorType>();
    VLOG(4) << "Builder construction  dense_x_max";
    ir_tensor_x_max = paddle::dialect::IrTensor(paddle::dialect::TransToPhiDataType(x_max.dtype()),
                                                        x_max.dims(),
                                                        x_max.data_layout(),
                                                        x_max.lod(),
                                                        x_max.offset());
    VLOG(4) << "Builder construction  meta_x_max";
    meta_x_max = paddle::dialect::IrMetaTensor(&ir_tensor_x_max);
  }


  VLOG(4) << "Builder construction  dense_y";
  paddle::dialect::IrTensor ir_tensor_y(paddle::dialect::TransToPhiDataType(y.dtype()),
                                                      y.dims(),
                                                      y.data_layout(),
                                                      y.lod(),
                                                      y.offset());
  VLOG(4) << "Builder construction  meta_y";
  paddle::dialect::IrMetaTensor meta_y(&ir_tensor_y);

  paddle::dialect::IrMetaTensor meta_y_max;
  paddle::dialect::IrTensor ir_tensor_y_max;
  if (y_max_.impl() != nullptr) {
    paddle::dialect::DenseTensorType y_max = y_max_.type().dyn_cast<paddle::dialect::DenseTensorType>();
    VLOG(4) << "Builder construction  dense_y_max";
    ir_tensor_y_max = paddle::dialect::IrTensor(paddle::dialect::TransToPhiDataType(y_max.dtype()),
                                                        y_max.dims(),
                                                        y_max.data_layout(),
                                                        y_max.lod(),
                                                        y_max.offset());
    VLOG(4) << "Builder construction  meta_y_max";
    meta_y_max = paddle::dialect::IrMetaTensor(&ir_tensor_y_max);
  }

  paddle::dialect::IrTensor dense_out;
  paddle::dialect::IrMetaTensor meta_out(&dense_out);
  paddle::dialect::IrTensor dense_out_max;
  paddle::dialect::IrMetaTensor meta_out_max(&dense_out_max);

  phi::AddActXPUInferMeta(meta_x, meta_x_max, meta_y, meta_y_max, act_type, &meta_out, &meta_out_max);

  std::vector<pir::Type> argument_outputs;
  pir::Type out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_out.dtype()), dense_out.dims(), dense_out.layout(), dense_out.lod(), dense_out.offset());
  argument_outputs.push_back(out_dense_tensor_type);

  pir::Type out_max_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_out_max.dtype()), dense_out_max.dims(), dense_out_max.layout(), dense_out_max.lod(), dense_out_max.offset());
  argument_outputs.push_back(out_max_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void AddActXpuOp::VerifySig() {
  VLOG(4) << "Start Verifying inputs, outputs and attributes for: AddActXpuOp.";
  VLOG(4) << "Verifying inputs:";
  {
  auto input_size = num_operands();
  IR_ENFORCE(input_size == 4u,
                    "The size %d of inputs must be equal to 4.", input_size);
  IR_ENFORCE((*this)->operand_source(0).type().isa<paddle::dialect::DenseTensorType>(),
                  "Type validation failed for the 0th input, got %s.", (*this)->operand_source(0).type());
  if (auto val = (*this)->operand(1)) {
    IR_ENFORCE(val.type().isa<paddle::dialect::DenseTensorType>(),
                   "Type validation failed for the 1th input, got %s.", (*this)->operand_source(1).type());
  }
  IR_ENFORCE((*this)->operand_source(2).type().isa<paddle::dialect::DenseTensorType>(),
                  "Type validation failed for the 2th input, got %s.", (*this)->operand_source(2).type());
  if (auto val = (*this)->operand(3)) {
    IR_ENFORCE(val.type().isa<paddle::dialect::DenseTensorType>(),
                   "Type validation failed for the 3th input, got %s.", (*this)->operand_source(3).type());
  }
  }
  VLOG(4) << "Verifying attributes:";
  {
  auto& attributes = this->attributes();
  IR_ENFORCE(attributes.count("act_type")>0,
                 "act_type does not exist.");
  IR_ENFORCE(attributes.at("act_type").isa<pir::Int32Attribute>(),
                 "Type of attribute: act_type is not pir::Int32Attribute.");

  }
  VLOG(4) << "Verifying outputs:";
  {
  auto output_size = num_results();
  IR_ENFORCE(output_size == 2u,
                    "The size %d of outputs must be equal to 2.", output_size);
  IR_ENFORCE((*this)->result(0).type().isa<paddle::dialect::DenseTensorType>(),
                 "Type validation failed for the 0th output.");
  IR_ENFORCE((*this)->result(1).type().isa<paddle::dialect::DenseTensorType>(),
                 "Type validation failed for the 1th output.");
  }
  VLOG(4) << "End Verifying for: AddActXpuOp.";
}

void AddActXpuOp::InferMeta( phi::InferMetaContext *infer_meta ) {
  auto fn = PD_INFER_META(phi::AddActXPUInferMeta);
  fn(infer_meta);
}

phi::DataType AddActXpuOp::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: AddActXpuOp";
  


  return expected_kernel_dtype;
}

const char *AddLayernormXpuOp::attributes_name[2] = { "begin_norm_axis", "epsilon" };

OpInfoTuple AddLayernormXpuOp::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("x", "paddle::dialect::DenseTensorType", false, false, false, false), paddle::dialect::OpInputInfo("y", "paddle::dialect::DenseTensorType", false, false, false, false), paddle::dialect::OpInputInfo("scale", "paddle::dialect::DenseTensorType", false, false, false, false), paddle::dialect::OpInputInfo("bias", "paddle::dialect::DenseTensorType", false, false, false, false) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = { paddle::dialect::OpAttributeInfo("begin_norm_axis", "pir::Int32Attribute", ""), paddle::dialect::OpAttributeInfo("epsilon", "pir::FloatAttribute", "") };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("out", "paddle::dialect::DenseTensorType", false, false) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("AddLayernormXPUInferMeta", {"x", "y", "scale", "bias", "begin_norm_axis", "epsilon"}, "add_layernorm_xpu", {"x", "y", "scale", "bias", "begin_norm_axis", "epsilon"}, {"x"}, {}, {}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "add_layernorm_xpu");
}

void AddLayernormXpuOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, pir::Value y_, pir::Value scale_, pir::Value bias_, int begin_norm_axis, float epsilon) {
  VLOG(4) << "Start build AddLayernormXpuOp";



  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_, y_, scale_, bias_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_begin_norm_axis = pir::Int32Attribute::get(pir::IrContext::Instance(), begin_norm_axis);
  argument.AddAttribute("begin_norm_axis", attr_begin_norm_axis);
  pir::Attribute attr_epsilon = pir::FloatAttribute::get(pir::IrContext::Instance(), epsilon);
  argument.AddAttribute("epsilon", attr_epsilon);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;
  paddle::dialect::DenseTensorType y = y_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)y;
  paddle::dialect::DenseTensorType scale = scale_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)scale;
  paddle::dialect::DenseTensorType bias = bias_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)bias;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);

  VLOG(4) << "Builder construction  dense_y";
  paddle::dialect::IrTensor ir_tensor_y(paddle::dialect::TransToPhiDataType(y.dtype()),
                                                      y.dims(),
                                                      y.data_layout(),
                                                      y.lod(),
                                                      y.offset());
  VLOG(4) << "Builder construction  meta_y";
  paddle::dialect::IrMetaTensor meta_y(&ir_tensor_y);

  VLOG(4) << "Builder construction  dense_scale";
  paddle::dialect::IrTensor ir_tensor_scale(paddle::dialect::TransToPhiDataType(scale.dtype()),
                                                      scale.dims(),
                                                      scale.data_layout(),
                                                      scale.lod(),
                                                      scale.offset());
  VLOG(4) << "Builder construction  meta_scale";
  paddle::dialect::IrMetaTensor meta_scale(&ir_tensor_scale);

  VLOG(4) << "Builder construction  dense_bias";
  paddle::dialect::IrTensor ir_tensor_bias(paddle::dialect::TransToPhiDataType(bias.dtype()),
                                                      bias.dims(),
                                                      bias.data_layout(),
                                                      bias.lod(),
                                                      bias.offset());
  VLOG(4) << "Builder construction  meta_bias";
  paddle::dialect::IrMetaTensor meta_bias(&ir_tensor_bias);
  paddle::dialect::IrTensor dense_out;
  paddle::dialect::IrMetaTensor meta_out(&dense_out);

  phi::AddLayernormXPUInferMeta(meta_x, meta_y, meta_scale, meta_bias, begin_norm_axis, epsilon, &meta_out);

  std::vector<pir::Type> argument_outputs;
  pir::Type out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_out.dtype()), dense_out.dims(), dense_out.layout(), dense_out.lod(), dense_out.offset());
  argument_outputs.push_back(out_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void AddLayernormXpuOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, pir::Value y_, pir::Value scale_, pir::Value bias_, pir::AttributeMap attributes) {
  VLOG(4) << "Start build AddLayernormXpuOp";


  IR_ENFORCE(
      attributes.find("begin_norm_axis") != attributes.end(),
          "'begin_norm_axis' Attribute is expected for AddLayernormXpuOp. ");
  int begin_norm_axis = attributes.at("begin_norm_axis").dyn_cast<pir::Int32Attribute>().data();

  IR_ENFORCE(
      attributes.find("epsilon") != attributes.end(),
          "'epsilon' Attribute is expected for AddLayernormXpuOp. ");
  float epsilon = attributes.at("epsilon").dyn_cast<pir::FloatAttribute>().data();


  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_, y_, scale_, bias_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_begin_norm_axis = pir::Int32Attribute::get(pir::IrContext::Instance(), begin_norm_axis);
  argument.AddAttribute("begin_norm_axis", attr_begin_norm_axis);
  pir::Attribute attr_epsilon = pir::FloatAttribute::get(pir::IrContext::Instance(), epsilon);
  argument.AddAttribute("epsilon", attr_epsilon);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;
  paddle::dialect::DenseTensorType y = y_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)y;
  paddle::dialect::DenseTensorType scale = scale_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)scale;
  paddle::dialect::DenseTensorType bias = bias_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)bias;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);

  VLOG(4) << "Builder construction  dense_y";
  paddle::dialect::IrTensor ir_tensor_y(paddle::dialect::TransToPhiDataType(y.dtype()),
                                                      y.dims(),
                                                      y.data_layout(),
                                                      y.lod(),
                                                      y.offset());
  VLOG(4) << "Builder construction  meta_y";
  paddle::dialect::IrMetaTensor meta_y(&ir_tensor_y);

  VLOG(4) << "Builder construction  dense_scale";
  paddle::dialect::IrTensor ir_tensor_scale(paddle::dialect::TransToPhiDataType(scale.dtype()),
                                                      scale.dims(),
                                                      scale.data_layout(),
                                                      scale.lod(),
                                                      scale.offset());
  VLOG(4) << "Builder construction  meta_scale";
  paddle::dialect::IrMetaTensor meta_scale(&ir_tensor_scale);

  VLOG(4) << "Builder construction  dense_bias";
  paddle::dialect::IrTensor ir_tensor_bias(paddle::dialect::TransToPhiDataType(bias.dtype()),
                                                      bias.dims(),
                                                      bias.data_layout(),
                                                      bias.lod(),
                                                      bias.offset());
  VLOG(4) << "Builder construction  meta_bias";
  paddle::dialect::IrMetaTensor meta_bias(&ir_tensor_bias);
  paddle::dialect::IrTensor dense_out;
  paddle::dialect::IrMetaTensor meta_out(&dense_out);

  phi::AddLayernormXPUInferMeta(meta_x, meta_y, meta_scale, meta_bias, begin_norm_axis, epsilon, &meta_out);

  std::vector<pir::Type> argument_outputs;
  pir::Type out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_out.dtype()), dense_out.dims(), dense_out.layout(), dense_out.lod(), dense_out.offset());
  argument_outputs.push_back(out_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void AddLayernormXpuOp::VerifySig() {
  VLOG(4) << "Start Verifying inputs, outputs and attributes for: AddLayernormXpuOp.";
  VLOG(4) << "Verifying inputs:";
  {
  auto input_size = num_operands();
  IR_ENFORCE(input_size == 4u,
                    "The size %d of inputs must be equal to 4.", input_size);
  IR_ENFORCE((*this)->operand_source(0).type().isa<paddle::dialect::DenseTensorType>(),
                  "Type validation failed for the 0th input, got %s.", (*this)->operand_source(0).type());
  IR_ENFORCE((*this)->operand_source(1).type().isa<paddle::dialect::DenseTensorType>(),
                  "Type validation failed for the 1th input, got %s.", (*this)->operand_source(1).type());
  IR_ENFORCE((*this)->operand_source(2).type().isa<paddle::dialect::DenseTensorType>(),
                  "Type validation failed for the 2th input, got %s.", (*this)->operand_source(2).type());
  IR_ENFORCE((*this)->operand_source(3).type().isa<paddle::dialect::DenseTensorType>(),
                  "Type validation failed for the 3th input, got %s.", (*this)->operand_source(3).type());
  }
  VLOG(4) << "Verifying attributes:";
  {
  auto& attributes = this->attributes();
  IR_ENFORCE(attributes.count("begin_norm_axis")>0,
                 "begin_norm_axis does not exist.");
  IR_ENFORCE(attributes.at("begin_norm_axis").isa<pir::Int32Attribute>(),
                 "Type of attribute: begin_norm_axis is not pir::Int32Attribute.");

  IR_ENFORCE(attributes.count("epsilon")>0,
                 "epsilon does not exist.");
  IR_ENFORCE(attributes.at("epsilon").isa<pir::FloatAttribute>(),
                 "Type of attribute: epsilon is not pir::FloatAttribute.");

  }
  VLOG(4) << "Verifying outputs:";
  {
  auto output_size = num_results();
  IR_ENFORCE(output_size == 1u,
                    "The size %d of outputs must be equal to 1.", output_size);
  IR_ENFORCE((*this)->result(0).type().isa<paddle::dialect::DenseTensorType>(),
                 "Type validation failed for the 0th output.");
  }
  VLOG(4) << "End Verifying for: AddLayernormXpuOp.";
}

void AddLayernormXpuOp::InferMeta( phi::InferMetaContext *infer_meta ) {
  auto fn = PD_INFER_META(phi::AddLayernormXPUInferMeta);
  fn(infer_meta);
}

phi::DataType AddLayernormXpuOp::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: AddLayernormXpuOp";
  


  return expected_kernel_dtype;
}

OpInfoTuple AddcmulXpuOp::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("x", "paddle::dialect::DenseTensorType", false, false, false, false), paddle::dialect::OpInputInfo("y", "paddle::dialect::DenseTensorType", false, false, false, false), paddle::dialect::OpInputInfo("w", "paddle::dialect::DenseTensorType", false, false, false, false) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = {  };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("out", "paddle::dialect::DenseTensorType", false, false) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("AddCMulXPUInferMeta", {"x", "y", "w"}, "addcmul_xpu", {"x", "y", "w"}, {"x"}, {}, {}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "addcmul_xpu");
}

void AddcmulXpuOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, pir::Value y_, pir::Value w_) {
  VLOG(4) << "Start build AddcmulXpuOp";



  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_, y_, w_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;
  paddle::dialect::DenseTensorType y = y_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)y;
  paddle::dialect::DenseTensorType w = w_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)w;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);

  VLOG(4) << "Builder construction  dense_y";
  paddle::dialect::IrTensor ir_tensor_y(paddle::dialect::TransToPhiDataType(y.dtype()),
                                                      y.dims(),
                                                      y.data_layout(),
                                                      y.lod(),
                                                      y.offset());
  VLOG(4) << "Builder construction  meta_y";
  paddle::dialect::IrMetaTensor meta_y(&ir_tensor_y);

  VLOG(4) << "Builder construction  dense_w";
  paddle::dialect::IrTensor ir_tensor_w(paddle::dialect::TransToPhiDataType(w.dtype()),
                                                      w.dims(),
                                                      w.data_layout(),
                                                      w.lod(),
                                                      w.offset());
  VLOG(4) << "Builder construction  meta_w";
  paddle::dialect::IrMetaTensor meta_w(&ir_tensor_w);
  paddle::dialect::IrTensor dense_out;
  paddle::dialect::IrMetaTensor meta_out(&dense_out);

  phi::AddCMulXPUInferMeta(meta_x, meta_y, meta_w, &meta_out);

  std::vector<pir::Type> argument_outputs;
  pir::Type out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_out.dtype()), dense_out.dims(), dense_out.layout(), dense_out.lod(), dense_out.offset());
  argument_outputs.push_back(out_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void AddcmulXpuOp::VerifySig() {
  VLOG(4) << "Start Verifying inputs, outputs and attributes for: AddcmulXpuOp.";
  VLOG(4) << "Verifying inputs:";
  {
  auto input_size = num_operands();
  IR_ENFORCE(input_size == 3u,
                    "The size %d of inputs must be equal to 3.", input_size);
  IR_ENFORCE((*this)->operand_source(0).type().isa<paddle::dialect::DenseTensorType>(),
                  "Type validation failed for the 0th input, got %s.", (*this)->operand_source(0).type());
  IR_ENFORCE((*this)->operand_source(1).type().isa<paddle::dialect::DenseTensorType>(),
                  "Type validation failed for the 1th input, got %s.", (*this)->operand_source(1).type());
  IR_ENFORCE((*this)->operand_source(2).type().isa<paddle::dialect::DenseTensorType>(),
                  "Type validation failed for the 2th input, got %s.", (*this)->operand_source(2).type());
  }
  VLOG(4) << "Verifying attributes:";
  {
  // Attributes num is 0, not need to check attributes type.
  }
  VLOG(4) << "Verifying outputs:";
  {
  auto output_size = num_results();
  IR_ENFORCE(output_size == 1u,
                    "The size %d of outputs must be equal to 1.", output_size);
  IR_ENFORCE((*this)->result(0).type().isa<paddle::dialect::DenseTensorType>(),
                 "Type validation failed for the 0th output.");
  }
  VLOG(4) << "End Verifying for: AddcmulXpuOp.";
}

void AddcmulXpuOp::InferMeta( phi::InferMetaContext *infer_meta ) {
  auto fn = PD_INFER_META(phi::AddCMulXPUInferMeta);
  fn(infer_meta);
}

phi::DataType AddcmulXpuOp::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: AddcmulXpuOp";
  


  return expected_kernel_dtype;
}

const char *BlockMultiheadAttention_Op::attributes_name[9] = { "max_seq_len", "block_size", "use_neox_style", "dynamic_cachekv_quant", "quant_round_type", "quant_max_bound", "quant_min_bound", "out_scale", "compute_dtype" };

OpInfoTuple BlockMultiheadAttention_Op::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("qkv", "paddle::dialect::DenseTensorType", false, false, false, false), paddle::dialect::OpInputInfo("key_cache", "paddle::dialect::DenseTensorType", false, false, false, false), paddle::dialect::OpInputInfo("value_cache", "paddle::dialect::DenseTensorType", false, false, false, false), paddle::dialect::OpInputInfo("seq_lens_encoder", "paddle::dialect::DenseTensorType", false, false, false, false), paddle::dialect::OpInputInfo("seq_lens_decoder", "paddle::dialect::DenseTensorType", false, false, false, false), paddle::dialect::OpInputInfo("seq_lens_this_time", "paddle::dialect::DenseTensorType", false, false, false, false), paddle::dialect::OpInputInfo("padding_offsets", "paddle::dialect::DenseTensorType", false, false, false, false), paddle::dialect::OpInputInfo("cum_offsets", "paddle::dialect::DenseTensorType", false, false, false, false), paddle::dialect::OpInputInfo("cu_seqlens_q", "paddle::dialect::DenseTensorType", false, false, false, false), paddle::dialect::OpInputInfo("cu_seqlens_k", "paddle::dialect::DenseTensorType", false, false, false, false), paddle::dialect::OpInputInfo("block_tables", "paddle::dialect::DenseTensorType", false, false, false, false), paddle::dialect::OpInputInfo("pre_key_cache", "paddle::dialect::DenseTensorType", true, false, false, false), paddle::dialect::OpInputInfo("pre_value_cache", "paddle::dialect::DenseTensorType", true, false, false, false), paddle::dialect::OpInputInfo("rope_emb", "paddle::dialect::DenseTensorType", true, false, false, false), paddle::dialect::OpInputInfo("mask", "paddle::dialect::DenseTensorType", true, false, false, false), paddle::dialect::OpInputInfo("tgt_mask", "paddle::dialect::DenseTensorType", true, false, false, false), paddle::dialect::OpInputInfo("cache_k_quant_scales", "paddle::dialect::DenseTensorType", true, false, false, false), paddle::dialect::OpInputInfo("cache_v_quant_scales", "paddle::dialect::DenseTensorType", true, false, false, false), paddle::dialect::OpInputInfo("cache_k_dequant_scales", "paddle::dialect::DenseTensorType", true, false, false, false), paddle::dialect::OpInputInfo("cache_v_dequant_scales", "paddle::dialect::DenseTensorType", true, false, false, false), paddle::dialect::OpInputInfo("qkv_out_scale", "paddle::dialect::DenseTensorType", true, false, false, false), paddle::dialect::OpInputInfo("qkv_bias", "paddle::dialect::DenseTensorType", true, false, false, false), paddle::dialect::OpInputInfo("out_shift", "paddle::dialect::DenseTensorType", true, false, false, false), paddle::dialect::OpInputInfo("out_smooth", "paddle::dialect::DenseTensorType", true, false, false, false) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = { paddle::dialect::OpAttributeInfo("max_seq_len", "pir::Int32Attribute", ""), paddle::dialect::OpAttributeInfo("block_size", "pir::Int32Attribute", ""), paddle::dialect::OpAttributeInfo("use_neox_style", "pir::BoolAttribute", ""), paddle::dialect::OpAttributeInfo("dynamic_cachekv_quant", "pir::BoolAttribute", ""), paddle::dialect::OpAttributeInfo("quant_round_type", "pir::Int32Attribute", ""), paddle::dialect::OpAttributeInfo("quant_max_bound", "pir::FloatAttribute", ""), paddle::dialect::OpAttributeInfo("quant_min_bound", "pir::FloatAttribute", ""), paddle::dialect::OpAttributeInfo("out_scale", "pir::FloatAttribute", ""), paddle::dialect::OpAttributeInfo("compute_dtype", "pir::StrAttribute", "") };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("fmha_out", "paddle::dialect::DenseTensorType", false, false), paddle::dialect::OpOutputInfo("qkv_out", "paddle::dialect::DenseTensorType", false, false), paddle::dialect::OpOutputInfo("key_cache_out", "paddle::dialect::DenseTensorType", false, false), paddle::dialect::OpOutputInfo("value_cache_out", "paddle::dialect::DenseTensorType", false, false) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("BlockMultiheadAttentionInferMeta", {"qkv", "key_cache", "value_cache", "seq_lens_encoder", "seq_lens_decoder", "seq_lens_this_time", "padding_offsets", "cum_offsets", "cu_seqlens_q", "cu_seqlens_k", "block_tables", "pre_key_cache", "pre_value_cache", "rope_emb", "mask", "tgt_mask", "cache_k_quant_scales", "cache_v_quant_scales", "cache_k_dequant_scales", "cache_v_dequant_scales", "qkv_out_scale", "qkv_bias", "out_shift", "out_smooth", "max_seq_len", "block_size", "use_neox_style", "dynamic_cachekv_quant", "quant_round_type", "quant_max_bound", "quant_min_bound", "out_scale", "compute_dtype"}, "block_multihead_attention", {"qkv", "key_cache", "value_cache", "seq_lens_encoder", "seq_lens_decoder", "seq_lens_this_time", "padding_offsets", "cum_offsets", "cu_seqlens_q", "cu_seqlens_k", "block_tables", "pre_key_cache", "pre_value_cache", "rope_emb", "mask", "tgt_mask", "cache_k_quant_scales", "cache_v_quant_scales", "cache_k_dequant_scales", "cache_v_dequant_scales", "qkv_out_scale", "qkv_bias", "out_shift", "out_smooth", "max_seq_len", "block_size", "use_neox_style", "dynamic_cachekv_quant", "quant_round_type", "quant_max_bound", "quant_min_bound", "out_scale", "compute_dtype"}, {"qkv"}, {}, {{"qkv_out", "qkv"},{"key_cache_out", "key_cache"},{"value_cache_out", "value_cache"}}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "block_multihead_attention_");
}

void BlockMultiheadAttention_Op::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value qkv_, pir::Value key_cache_, pir::Value value_cache_, pir::Value seq_lens_encoder_, pir::Value seq_lens_decoder_, pir::Value seq_lens_this_time_, pir::Value padding_offsets_, pir::Value cum_offsets_, pir::Value cu_seqlens_q_, pir::Value cu_seqlens_k_, pir::Value block_tables_, pir::Value pre_key_cache_, pir::Value pre_value_cache_, pir::Value rope_emb_, pir::Value mask_, pir::Value tgt_mask_, pir::Value cache_k_quant_scales_, pir::Value cache_v_quant_scales_, pir::Value cache_k_dequant_scales_, pir::Value cache_v_dequant_scales_, pir::Value qkv_out_scale_, pir::Value qkv_bias_, pir::Value out_shift_, pir::Value out_smooth_, int max_seq_len, int block_size, bool use_neox_style, bool dynamic_cachekv_quant, int quant_round_type, float quant_max_bound, float quant_min_bound, float out_scale, const std::string& compute_dtype) {
  VLOG(4) << "Start build BlockMultiheadAttention_Op";



  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {qkv_, key_cache_, value_cache_, seq_lens_encoder_, seq_lens_decoder_, seq_lens_this_time_, padding_offsets_, cum_offsets_, cu_seqlens_q_, cu_seqlens_k_, block_tables_, pre_key_cache_, pre_value_cache_, rope_emb_, mask_, tgt_mask_, cache_k_quant_scales_, cache_v_quant_scales_, cache_k_dequant_scales_, cache_v_dequant_scales_, qkv_out_scale_, qkv_bias_, out_shift_, out_smooth_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_max_seq_len = pir::Int32Attribute::get(pir::IrContext::Instance(), max_seq_len);
  argument.AddAttribute("max_seq_len", attr_max_seq_len);
  pir::Attribute attr_block_size = pir::Int32Attribute::get(pir::IrContext::Instance(), block_size);
  argument.AddAttribute("block_size", attr_block_size);
  pir::Attribute attr_use_neox_style = pir::BoolAttribute::get(pir::IrContext::Instance(), use_neox_style);
  argument.AddAttribute("use_neox_style", attr_use_neox_style);
  pir::Attribute attr_dynamic_cachekv_quant = pir::BoolAttribute::get(pir::IrContext::Instance(), dynamic_cachekv_quant);
  argument.AddAttribute("dynamic_cachekv_quant", attr_dynamic_cachekv_quant);
  pir::Attribute attr_quant_round_type = pir::Int32Attribute::get(pir::IrContext::Instance(), quant_round_type);
  argument.AddAttribute("quant_round_type", attr_quant_round_type);
  pir::Attribute attr_quant_max_bound = pir::FloatAttribute::get(pir::IrContext::Instance(), quant_max_bound);
  argument.AddAttribute("quant_max_bound", attr_quant_max_bound);
  pir::Attribute attr_quant_min_bound = pir::FloatAttribute::get(pir::IrContext::Instance(), quant_min_bound);
  argument.AddAttribute("quant_min_bound", attr_quant_min_bound);
  pir::Attribute attr_out_scale = pir::FloatAttribute::get(pir::IrContext::Instance(), out_scale);
  argument.AddAttribute("out_scale", attr_out_scale);
  pir::Attribute attr_compute_dtype = pir::StrAttribute::get(pir::IrContext::Instance(), compute_dtype);
  argument.AddAttribute("compute_dtype", attr_compute_dtype);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType qkv = qkv_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)qkv;
  paddle::dialect::DenseTensorType key_cache = key_cache_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)key_cache;
  paddle::dialect::DenseTensorType value_cache = value_cache_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)value_cache;
  paddle::dialect::DenseTensorType seq_lens_encoder = seq_lens_encoder_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)seq_lens_encoder;
  paddle::dialect::DenseTensorType seq_lens_decoder = seq_lens_decoder_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)seq_lens_decoder;
  paddle::dialect::DenseTensorType seq_lens_this_time = seq_lens_this_time_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)seq_lens_this_time;
  paddle::dialect::DenseTensorType padding_offsets = padding_offsets_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)padding_offsets;
  paddle::dialect::DenseTensorType cum_offsets = cum_offsets_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)cum_offsets;
  paddle::dialect::DenseTensorType cu_seqlens_q = cu_seqlens_q_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)cu_seqlens_q;
  paddle::dialect::DenseTensorType cu_seqlens_k = cu_seqlens_k_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)cu_seqlens_k;
  paddle::dialect::DenseTensorType block_tables = block_tables_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)block_tables;

  VLOG(4) << "Builder construction  dense_qkv";
  paddle::dialect::IrTensor ir_tensor_qkv(paddle::dialect::TransToPhiDataType(qkv.dtype()),
                                                      qkv.dims(),
                                                      qkv.data_layout(),
                                                      qkv.lod(),
                                                      qkv.offset());
  VLOG(4) << "Builder construction  meta_qkv";
  paddle::dialect::IrMetaTensor meta_qkv(&ir_tensor_qkv);

  VLOG(4) << "Builder construction  dense_key_cache";
  paddle::dialect::IrTensor ir_tensor_key_cache(paddle::dialect::TransToPhiDataType(key_cache.dtype()),
                                                      key_cache.dims(),
                                                      key_cache.data_layout(),
                                                      key_cache.lod(),
                                                      key_cache.offset());
  VLOG(4) << "Builder construction  meta_key_cache";
  paddle::dialect::IrMetaTensor meta_key_cache(&ir_tensor_key_cache);

  VLOG(4) << "Builder construction  dense_value_cache";
  paddle::dialect::IrTensor ir_tensor_value_cache(paddle::dialect::TransToPhiDataType(value_cache.dtype()),
                                                      value_cache.dims(),
                                                      value_cache.data_layout(),
                                                      value_cache.lod(),
                                                      value_cache.offset());
  VLOG(4) << "Builder construction  meta_value_cache";
  paddle::dialect::IrMetaTensor meta_value_cache(&ir_tensor_value_cache);

  VLOG(4) << "Builder construction  dense_seq_lens_encoder";
  paddle::dialect::IrTensor ir_tensor_seq_lens_encoder(paddle::dialect::TransToPhiDataType(seq_lens_encoder.dtype()),
                                                      seq_lens_encoder.dims(),
                                                      seq_lens_encoder.data_layout(),
                                                      seq_lens_encoder.lod(),
                                                      seq_lens_encoder.offset());
  VLOG(4) << "Builder construction  meta_seq_lens_encoder";
  paddle::dialect::IrMetaTensor meta_seq_lens_encoder(&ir_tensor_seq_lens_encoder);

  VLOG(4) << "Builder construction  dense_seq_lens_decoder";
  paddle::dialect::IrTensor ir_tensor_seq_lens_decoder(paddle::dialect::TransToPhiDataType(seq_lens_decoder.dtype()),
                                                      seq_lens_decoder.dims(),
                                                      seq_lens_decoder.data_layout(),
                                                      seq_lens_decoder.lod(),
                                                      seq_lens_decoder.offset());
  VLOG(4) << "Builder construction  meta_seq_lens_decoder";
  paddle::dialect::IrMetaTensor meta_seq_lens_decoder(&ir_tensor_seq_lens_decoder);

  VLOG(4) << "Builder construction  dense_seq_lens_this_time";
  paddle::dialect::IrTensor ir_tensor_seq_lens_this_time(paddle::dialect::TransToPhiDataType(seq_lens_this_time.dtype()),
                                                      seq_lens_this_time.dims(),
                                                      seq_lens_this_time.data_layout(),
                                                      seq_lens_this_time.lod(),
                                                      seq_lens_this_time.offset());
  VLOG(4) << "Builder construction  meta_seq_lens_this_time";
  paddle::dialect::IrMetaTensor meta_seq_lens_this_time(&ir_tensor_seq_lens_this_time);

  VLOG(4) << "Builder construction  dense_padding_offsets";
  paddle::dialect::IrTensor ir_tensor_padding_offsets(paddle::dialect::TransToPhiDataType(padding_offsets.dtype()),
                                                      padding_offsets.dims(),
                                                      padding_offsets.data_layout(),
                                                      padding_offsets.lod(),
                                                      padding_offsets.offset());
  VLOG(4) << "Builder construction  meta_padding_offsets";
  paddle::dialect::IrMetaTensor meta_padding_offsets(&ir_tensor_padding_offsets);

  VLOG(4) << "Builder construction  dense_cum_offsets";
  paddle::dialect::IrTensor ir_tensor_cum_offsets(paddle::dialect::TransToPhiDataType(cum_offsets.dtype()),
                                                      cum_offsets.dims(),
                                                      cum_offsets.data_layout(),
                                                      cum_offsets.lod(),
                                                      cum_offsets.offset());
  VLOG(4) << "Builder construction  meta_cum_offsets";
  paddle::dialect::IrMetaTensor meta_cum_offsets(&ir_tensor_cum_offsets);

  VLOG(4) << "Builder construction  dense_cu_seqlens_q";
  paddle::dialect::IrTensor ir_tensor_cu_seqlens_q(paddle::dialect::TransToPhiDataType(cu_seqlens_q.dtype()),
                                                      cu_seqlens_q.dims(),
                                                      cu_seqlens_q.data_layout(),
                                                      cu_seqlens_q.lod(),
                                                      cu_seqlens_q.offset());
  VLOG(4) << "Builder construction  meta_cu_seqlens_q";
  paddle::dialect::IrMetaTensor meta_cu_seqlens_q(&ir_tensor_cu_seqlens_q);

  VLOG(4) << "Builder construction  dense_cu_seqlens_k";
  paddle::dialect::IrTensor ir_tensor_cu_seqlens_k(paddle::dialect::TransToPhiDataType(cu_seqlens_k.dtype()),
                                                      cu_seqlens_k.dims(),
                                                      cu_seqlens_k.data_layout(),
                                                      cu_seqlens_k.lod(),
                                                      cu_seqlens_k.offset());
  VLOG(4) << "Builder construction  meta_cu_seqlens_k";
  paddle::dialect::IrMetaTensor meta_cu_seqlens_k(&ir_tensor_cu_seqlens_k);

  VLOG(4) << "Builder construction  dense_block_tables";
  paddle::dialect::IrTensor ir_tensor_block_tables(paddle::dialect::TransToPhiDataType(block_tables.dtype()),
                                                      block_tables.dims(),
                                                      block_tables.data_layout(),
                                                      block_tables.lod(),
                                                      block_tables.offset());
  VLOG(4) << "Builder construction  meta_block_tables";
  paddle::dialect::IrMetaTensor meta_block_tables(&ir_tensor_block_tables);

  paddle::dialect::IrMetaTensor meta_pre_key_cache;
  paddle::dialect::IrTensor ir_tensor_pre_key_cache;
  if (pre_key_cache_.impl() != nullptr) {
    paddle::dialect::DenseTensorType pre_key_cache = pre_key_cache_.type().dyn_cast<paddle::dialect::DenseTensorType>();
    VLOG(4) << "Builder construction  dense_pre_key_cache";
    ir_tensor_pre_key_cache = paddle::dialect::IrTensor(paddle::dialect::TransToPhiDataType(pre_key_cache.dtype()),
                                                        pre_key_cache.dims(),
                                                        pre_key_cache.data_layout(),
                                                        pre_key_cache.lod(),
                                                        pre_key_cache.offset());
    VLOG(4) << "Builder construction  meta_pre_key_cache";
    meta_pre_key_cache = paddle::dialect::IrMetaTensor(&ir_tensor_pre_key_cache);
  }


  paddle::dialect::IrMetaTensor meta_pre_value_cache;
  paddle::dialect::IrTensor ir_tensor_pre_value_cache;
  if (pre_value_cache_.impl() != nullptr) {
    paddle::dialect::DenseTensorType pre_value_cache = pre_value_cache_.type().dyn_cast<paddle::dialect::DenseTensorType>();
    VLOG(4) << "Builder construction  dense_pre_value_cache";
    ir_tensor_pre_value_cache = paddle::dialect::IrTensor(paddle::dialect::TransToPhiDataType(pre_value_cache.dtype()),
                                                        pre_value_cache.dims(),
                                                        pre_value_cache.data_layout(),
                                                        pre_value_cache.lod(),
                                                        pre_value_cache.offset());
    VLOG(4) << "Builder construction  meta_pre_value_cache";
    meta_pre_value_cache = paddle::dialect::IrMetaTensor(&ir_tensor_pre_value_cache);
  }


  paddle::dialect::IrMetaTensor meta_rope_emb;
  paddle::dialect::IrTensor ir_tensor_rope_emb;
  if (rope_emb_.impl() != nullptr) {
    paddle::dialect::DenseTensorType rope_emb = rope_emb_.type().dyn_cast<paddle::dialect::DenseTensorType>();
    VLOG(4) << "Builder construction  dense_rope_emb";
    ir_tensor_rope_emb = paddle::dialect::IrTensor(paddle::dialect::TransToPhiDataType(rope_emb.dtype()),
                                                        rope_emb.dims(),
                                                        rope_emb.data_layout(),
                                                        rope_emb.lod(),
                                                        rope_emb.offset());
    VLOG(4) << "Builder construction  meta_rope_emb";
    meta_rope_emb = paddle::dialect::IrMetaTensor(&ir_tensor_rope_emb);
  }


  paddle::dialect::IrMetaTensor meta_mask;
  paddle::dialect::IrTensor ir_tensor_mask;
  if (mask_.impl() != nullptr) {
    paddle::dialect::DenseTensorType mask = mask_.type().dyn_cast<paddle::dialect::DenseTensorType>();
    VLOG(4) << "Builder construction  dense_mask";
    ir_tensor_mask = paddle::dialect::IrTensor(paddle::dialect::TransToPhiDataType(mask.dtype()),
                                                        mask.dims(),
                                                        mask.data_layout(),
                                                        mask.lod(),
                                                        mask.offset());
    VLOG(4) << "Builder construction  meta_mask";
    meta_mask = paddle::dialect::IrMetaTensor(&ir_tensor_mask);
  }


  paddle::dialect::IrMetaTensor meta_tgt_mask;
  paddle::dialect::IrTensor ir_tensor_tgt_mask;
  if (tgt_mask_.impl() != nullptr) {
    paddle::dialect::DenseTensorType tgt_mask = tgt_mask_.type().dyn_cast<paddle::dialect::DenseTensorType>();
    VLOG(4) << "Builder construction  dense_tgt_mask";
    ir_tensor_tgt_mask = paddle::dialect::IrTensor(paddle::dialect::TransToPhiDataType(tgt_mask.dtype()),
                                                        tgt_mask.dims(),
                                                        tgt_mask.data_layout(),
                                                        tgt_mask.lod(),
                                                        tgt_mask.offset());
    VLOG(4) << "Builder construction  meta_tgt_mask";
    meta_tgt_mask = paddle::dialect::IrMetaTensor(&ir_tensor_tgt_mask);
  }


  paddle::dialect::IrMetaTensor meta_cache_k_quant_scales;
  paddle::dialect::IrTensor ir_tensor_cache_k_quant_scales;
  if (cache_k_quant_scales_.impl() != nullptr) {
    paddle::dialect::DenseTensorType cache_k_quant_scales = cache_k_quant_scales_.type().dyn_cast<paddle::dialect::DenseTensorType>();
    VLOG(4) << "Builder construction  dense_cache_k_quant_scales";
    ir_tensor_cache_k_quant_scales = paddle::dialect::IrTensor(paddle::dialect::TransToPhiDataType(cache_k_quant_scales.dtype()),
                                                        cache_k_quant_scales.dims(),
                                                        cache_k_quant_scales.data_layout(),
                                                        cache_k_quant_scales.lod(),
                                                        cache_k_quant_scales.offset());
    VLOG(4) << "Builder construction  meta_cache_k_quant_scales";
    meta_cache_k_quant_scales = paddle::dialect::IrMetaTensor(&ir_tensor_cache_k_quant_scales);
  }


  paddle::dialect::IrMetaTensor meta_cache_v_quant_scales;
  paddle::dialect::IrTensor ir_tensor_cache_v_quant_scales;
  if (cache_v_quant_scales_.impl() != nullptr) {
    paddle::dialect::DenseTensorType cache_v_quant_scales = cache_v_quant_scales_.type().dyn_cast<paddle::dialect::DenseTensorType>();
    VLOG(4) << "Builder construction  dense_cache_v_quant_scales";
    ir_tensor_cache_v_quant_scales = paddle::dialect::IrTensor(paddle::dialect::TransToPhiDataType(cache_v_quant_scales.dtype()),
                                                        cache_v_quant_scales.dims(),
                                                        cache_v_quant_scales.data_layout(),
                                                        cache_v_quant_scales.lod(),
                                                        cache_v_quant_scales.offset());
    VLOG(4) << "Builder construction  meta_cache_v_quant_scales";
    meta_cache_v_quant_scales = paddle::dialect::IrMetaTensor(&ir_tensor_cache_v_quant_scales);
  }


  paddle::dialect::IrMetaTensor meta_cache_k_dequant_scales;
  paddle::dialect::IrTensor ir_tensor_cache_k_dequant_scales;
  if (cache_k_dequant_scales_.impl() != nullptr) {
    paddle::dialect::DenseTensorType cache_k_dequant_scales = cache_k_dequant_scales_.type().dyn_cast<paddle::dialect::DenseTensorType>();
    VLOG(4) << "Builder construction  dense_cache_k_dequant_scales";
    ir_tensor_cache_k_dequant_scales = paddle::dialect::IrTensor(paddle::dialect::TransToPhiDataType(cache_k_dequant_scales.dtype()),
                                                        cache_k_dequant_scales.dims(),
                                                        cache_k_dequant_scales.data_layout(),
                                                        cache_k_dequant_scales.lod(),
                                                        cache_k_dequant_scales.offset());
    VLOG(4) << "Builder construction  meta_cache_k_dequant_scales";
    meta_cache_k_dequant_scales = paddle::dialect::IrMetaTensor(&ir_tensor_cache_k_dequant_scales);
  }


  paddle::dialect::IrMetaTensor meta_cache_v_dequant_scales;
  paddle::dialect::IrTensor ir_tensor_cache_v_dequant_scales;
  if (cache_v_dequant_scales_.impl() != nullptr) {
    paddle::dialect::DenseTensorType cache_v_dequant_scales = cache_v_dequant_scales_.type().dyn_cast<paddle::dialect::DenseTensorType>();
    VLOG(4) << "Builder construction  dense_cache_v_dequant_scales";
    ir_tensor_cache_v_dequant_scales = paddle::dialect::IrTensor(paddle::dialect::TransToPhiDataType(cache_v_dequant_scales.dtype()),
                                                        cache_v_dequant_scales.dims(),
                                                        cache_v_dequant_scales.data_layout(),
                                                        cache_v_dequant_scales.lod(),
                                                        cache_v_dequant_scales.offset());
    VLOG(4) << "Builder construction  meta_cache_v_dequant_scales";
    meta_cache_v_dequant_scales = paddle::dialect::IrMetaTensor(&ir_tensor_cache_v_dequant_scales);
  }


  paddle::dialect::IrMetaTensor meta_qkv_out_scale;
  paddle::dialect::IrTensor ir_tensor_qkv_out_scale;
  if (qkv_out_scale_.impl() != nullptr) {
    paddle::dialect::DenseTensorType qkv_out_scale = qkv_out_scale_.type().dyn_cast<paddle::dialect::DenseTensorType>();
    VLOG(4) << "Builder construction  dense_qkv_out_scale";
    ir_tensor_qkv_out_scale = paddle::dialect::IrTensor(paddle::dialect::TransToPhiDataType(qkv_out_scale.dtype()),
                                                        qkv_out_scale.dims(),
                                                        qkv_out_scale.data_layout(),
                                                        qkv_out_scale.lod(),
                                                        qkv_out_scale.offset());
    VLOG(4) << "Builder construction  meta_qkv_out_scale";
    meta_qkv_out_scale = paddle::dialect::IrMetaTensor(&ir_tensor_qkv_out_scale);
  }


  paddle::dialect::IrMetaTensor meta_qkv_bias;
  paddle::dialect::IrTensor ir_tensor_qkv_bias;
  if (qkv_bias_.impl() != nullptr) {
    paddle::dialect::DenseTensorType qkv_bias = qkv_bias_.type().dyn_cast<paddle::dialect::DenseTensorType>();
    VLOG(4) << "Builder construction  dense_qkv_bias";
    ir_tensor_qkv_bias = paddle::dialect::IrTensor(paddle::dialect::TransToPhiDataType(qkv_bias.dtype()),
                                                        qkv_bias.dims(),
                                                        qkv_bias.data_layout(),
                                                        qkv_bias.lod(),
                                                        qkv_bias.offset());
    VLOG(4) << "Builder construction  meta_qkv_bias";
    meta_qkv_bias = paddle::dialect::IrMetaTensor(&ir_tensor_qkv_bias);
  }


  paddle::dialect::IrMetaTensor meta_out_shift;
  paddle::dialect::IrTensor ir_tensor_out_shift;
  if (out_shift_.impl() != nullptr) {
    paddle::dialect::DenseTensorType out_shift = out_shift_.type().dyn_cast<paddle::dialect::DenseTensorType>();
    VLOG(4) << "Builder construction  dense_out_shift";
    ir_tensor_out_shift = paddle::dialect::IrTensor(paddle::dialect::TransToPhiDataType(out_shift.dtype()),
                                                        out_shift.dims(),
                                                        out_shift.data_layout(),
                                                        out_shift.lod(),
                                                        out_shift.offset());
    VLOG(4) << "Builder construction  meta_out_shift";
    meta_out_shift = paddle::dialect::IrMetaTensor(&ir_tensor_out_shift);
  }


  paddle::dialect::IrMetaTensor meta_out_smooth;
  paddle::dialect::IrTensor ir_tensor_out_smooth;
  if (out_smooth_.impl() != nullptr) {
    paddle::dialect::DenseTensorType out_smooth = out_smooth_.type().dyn_cast<paddle::dialect::DenseTensorType>();
    VLOG(4) << "Builder construction  dense_out_smooth";
    ir_tensor_out_smooth = paddle::dialect::IrTensor(paddle::dialect::TransToPhiDataType(out_smooth.dtype()),
                                                        out_smooth.dims(),
                                                        out_smooth.data_layout(),
                                                        out_smooth.lod(),
                                                        out_smooth.offset());
    VLOG(4) << "Builder construction  meta_out_smooth";
    meta_out_smooth = paddle::dialect::IrMetaTensor(&ir_tensor_out_smooth);
  }

  paddle::dialect::IrTensor dense_fmha_out;
  paddle::dialect::IrMetaTensor meta_fmha_out(&dense_fmha_out);
  paddle::dialect::IrTensor dense_qkv_out;
  paddle::dialect::IrMetaTensor meta_qkv_out(&dense_qkv_out);
  paddle::dialect::IrTensor dense_key_cache_out;
  paddle::dialect::IrMetaTensor meta_key_cache_out(&dense_key_cache_out);
  paddle::dialect::IrTensor dense_value_cache_out;
  paddle::dialect::IrMetaTensor meta_value_cache_out(&dense_value_cache_out);

  phi::BlockMultiheadAttentionInferMeta(meta_qkv, meta_key_cache, meta_value_cache, meta_seq_lens_encoder, meta_seq_lens_decoder, meta_seq_lens_this_time, meta_padding_offsets, meta_cum_offsets, meta_cu_seqlens_q, meta_cu_seqlens_k, meta_block_tables, meta_pre_key_cache, meta_pre_value_cache, meta_rope_emb, meta_mask, meta_tgt_mask, meta_cache_k_quant_scales, meta_cache_v_quant_scales, meta_cache_k_dequant_scales, meta_cache_v_dequant_scales, meta_qkv_out_scale, meta_qkv_bias, meta_out_shift, meta_out_smooth, max_seq_len, block_size, use_neox_style, dynamic_cachekv_quant, quant_round_type, quant_max_bound, quant_min_bound, out_scale, compute_dtype, &meta_fmha_out, &meta_qkv_out, &meta_key_cache_out, &meta_value_cache_out);

  std::vector<pir::Type> argument_outputs;
  pir::Type fmha_out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_fmha_out.dtype()), dense_fmha_out.dims(), dense_fmha_out.layout(), dense_fmha_out.lod(), dense_fmha_out.offset());
  argument_outputs.push_back(fmha_out_dense_tensor_type);

  pir::Type qkv_out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_qkv_out.dtype()), dense_qkv_out.dims(), dense_qkv_out.layout(), dense_qkv_out.lod(), dense_qkv_out.offset());
  argument_outputs.push_back(qkv_out_dense_tensor_type);

  pir::Type key_cache_out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_key_cache_out.dtype()), dense_key_cache_out.dims(), dense_key_cache_out.layout(), dense_key_cache_out.lod(), dense_key_cache_out.offset());
  argument_outputs.push_back(key_cache_out_dense_tensor_type);

  pir::Type value_cache_out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_value_cache_out.dtype()), dense_value_cache_out.dims(), dense_value_cache_out.layout(), dense_value_cache_out.lod(), dense_value_cache_out.offset());
  argument_outputs.push_back(value_cache_out_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void BlockMultiheadAttention_Op::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value qkv_, pir::Value key_cache_, pir::Value value_cache_, pir::Value seq_lens_encoder_, pir::Value seq_lens_decoder_, pir::Value seq_lens_this_time_, pir::Value padding_offsets_, pir::Value cum_offsets_, pir::Value cu_seqlens_q_, pir::Value cu_seqlens_k_, pir::Value block_tables_, pir::Value pre_key_cache_, pir::Value pre_value_cache_, pir::Value rope_emb_, pir::Value mask_, pir::Value tgt_mask_, pir::Value cache_k_quant_scales_, pir::Value cache_v_quant_scales_, pir::Value cache_k_dequant_scales_, pir::Value cache_v_dequant_scales_, pir::Value qkv_out_scale_, pir::Value qkv_bias_, pir::Value out_shift_, pir::Value out_smooth_, pir::AttributeMap attributes) {
  VLOG(4) << "Start build BlockMultiheadAttention_Op";


  IR_ENFORCE(
      attributes.find("max_seq_len") != attributes.end(),
          "'max_seq_len' Attribute is expected for BlockMultiheadAttention_Op. ");
  int max_seq_len = attributes.at("max_seq_len").dyn_cast<pir::Int32Attribute>().data();

  IR_ENFORCE(
      attributes.find("block_size") != attributes.end(),
          "'block_size' Attribute is expected for BlockMultiheadAttention_Op. ");
  int block_size = attributes.at("block_size").dyn_cast<pir::Int32Attribute>().data();

  IR_ENFORCE(
      attributes.find("use_neox_style") != attributes.end(),
          "'use_neox_style' Attribute is expected for BlockMultiheadAttention_Op. ");
  bool use_neox_style = attributes.at("use_neox_style").dyn_cast<pir::BoolAttribute>().data();

  IR_ENFORCE(
      attributes.find("dynamic_cachekv_quant") != attributes.end(),
          "'dynamic_cachekv_quant' Attribute is expected for BlockMultiheadAttention_Op. ");
  bool dynamic_cachekv_quant = attributes.at("dynamic_cachekv_quant").dyn_cast<pir::BoolAttribute>().data();

  IR_ENFORCE(
      attributes.find("quant_round_type") != attributes.end(),
          "'quant_round_type' Attribute is expected for BlockMultiheadAttention_Op. ");
  int quant_round_type = attributes.at("quant_round_type").dyn_cast<pir::Int32Attribute>().data();

  IR_ENFORCE(
      attributes.find("quant_max_bound") != attributes.end(),
          "'quant_max_bound' Attribute is expected for BlockMultiheadAttention_Op. ");
  float quant_max_bound = attributes.at("quant_max_bound").dyn_cast<pir::FloatAttribute>().data();

  IR_ENFORCE(
      attributes.find("quant_min_bound") != attributes.end(),
          "'quant_min_bound' Attribute is expected for BlockMultiheadAttention_Op. ");
  float quant_min_bound = attributes.at("quant_min_bound").dyn_cast<pir::FloatAttribute>().data();

  IR_ENFORCE(
      attributes.find("out_scale") != attributes.end(),
          "'out_scale' Attribute is expected for BlockMultiheadAttention_Op. ");
  float out_scale = attributes.at("out_scale").dyn_cast<pir::FloatAttribute>().data();

  IR_ENFORCE(
      attributes.find("compute_dtype") != attributes.end(),
          "'compute_dtype' Attribute is expected for BlockMultiheadAttention_Op. ");
  std::string compute_dtype = attributes.at("compute_dtype").dyn_cast<pir::StrAttribute>().AsString();


  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {qkv_, key_cache_, value_cache_, seq_lens_encoder_, seq_lens_decoder_, seq_lens_this_time_, padding_offsets_, cum_offsets_, cu_seqlens_q_, cu_seqlens_k_, block_tables_, pre_key_cache_, pre_value_cache_, rope_emb_, mask_, tgt_mask_, cache_k_quant_scales_, cache_v_quant_scales_, cache_k_dequant_scales_, cache_v_dequant_scales_, qkv_out_scale_, qkv_bias_, out_shift_, out_smooth_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_max_seq_len = pir::Int32Attribute::get(pir::IrContext::Instance(), max_seq_len);
  argument.AddAttribute("max_seq_len", attr_max_seq_len);
  pir::Attribute attr_block_size = pir::Int32Attribute::get(pir::IrContext::Instance(), block_size);
  argument.AddAttribute("block_size", attr_block_size);
  pir::Attribute attr_use_neox_style = pir::BoolAttribute::get(pir::IrContext::Instance(), use_neox_style);
  argument.AddAttribute("use_neox_style", attr_use_neox_style);
  pir::Attribute attr_dynamic_cachekv_quant = pir::BoolAttribute::get(pir::IrContext::Instance(), dynamic_cachekv_quant);
  argument.AddAttribute("dynamic_cachekv_quant", attr_dynamic_cachekv_quant);
  pir::Attribute attr_quant_round_type = pir::Int32Attribute::get(pir::IrContext::Instance(), quant_round_type);
  argument.AddAttribute("quant_round_type", attr_quant_round_type);
  pir::Attribute attr_quant_max_bound = pir::FloatAttribute::get(pir::IrContext::Instance(), quant_max_bound);
  argument.AddAttribute("quant_max_bound", attr_quant_max_bound);
  pir::Attribute attr_quant_min_bound = pir::FloatAttribute::get(pir::IrContext::Instance(), quant_min_bound);
  argument.AddAttribute("quant_min_bound", attr_quant_min_bound);
  pir::Attribute attr_out_scale = pir::FloatAttribute::get(pir::IrContext::Instance(), out_scale);
  argument.AddAttribute("out_scale", attr_out_scale);
  pir::Attribute attr_compute_dtype = pir::StrAttribute::get(pir::IrContext::Instance(), compute_dtype);
  argument.AddAttribute("compute_dtype", attr_compute_dtype);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType qkv = qkv_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)qkv;
  paddle::dialect::DenseTensorType key_cache = key_cache_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)key_cache;
  paddle::dialect::DenseTensorType value_cache = value_cache_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)value_cache;
  paddle::dialect::DenseTensorType seq_lens_encoder = seq_lens_encoder_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)seq_lens_encoder;
  paddle::dialect::DenseTensorType seq_lens_decoder = seq_lens_decoder_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)seq_lens_decoder;
  paddle::dialect::DenseTensorType seq_lens_this_time = seq_lens_this_time_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)seq_lens_this_time;
  paddle::dialect::DenseTensorType padding_offsets = padding_offsets_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)padding_offsets;
  paddle::dialect::DenseTensorType cum_offsets = cum_offsets_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)cum_offsets;
  paddle::dialect::DenseTensorType cu_seqlens_q = cu_seqlens_q_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)cu_seqlens_q;
  paddle::dialect::DenseTensorType cu_seqlens_k = cu_seqlens_k_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)cu_seqlens_k;
  paddle::dialect::DenseTensorType block_tables = block_tables_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)block_tables;

  VLOG(4) << "Builder construction  dense_qkv";
  paddle::dialect::IrTensor ir_tensor_qkv(paddle::dialect::TransToPhiDataType(qkv.dtype()),
                                                      qkv.dims(),
                                                      qkv.data_layout(),
                                                      qkv.lod(),
                                                      qkv.offset());
  VLOG(4) << "Builder construction  meta_qkv";
  paddle::dialect::IrMetaTensor meta_qkv(&ir_tensor_qkv);

  VLOG(4) << "Builder construction  dense_key_cache";
  paddle::dialect::IrTensor ir_tensor_key_cache(paddle::dialect::TransToPhiDataType(key_cache.dtype()),
                                                      key_cache.dims(),
                                                      key_cache.data_layout(),
                                                      key_cache.lod(),
                                                      key_cache.offset());
  VLOG(4) << "Builder construction  meta_key_cache";
  paddle::dialect::IrMetaTensor meta_key_cache(&ir_tensor_key_cache);

  VLOG(4) << "Builder construction  dense_value_cache";
  paddle::dialect::IrTensor ir_tensor_value_cache(paddle::dialect::TransToPhiDataType(value_cache.dtype()),
                                                      value_cache.dims(),
                                                      value_cache.data_layout(),
                                                      value_cache.lod(),
                                                      value_cache.offset());
  VLOG(4) << "Builder construction  meta_value_cache";
  paddle::dialect::IrMetaTensor meta_value_cache(&ir_tensor_value_cache);

  VLOG(4) << "Builder construction  dense_seq_lens_encoder";
  paddle::dialect::IrTensor ir_tensor_seq_lens_encoder(paddle::dialect::TransToPhiDataType(seq_lens_encoder.dtype()),
                                                      seq_lens_encoder.dims(),
                                                      seq_lens_encoder.data_layout(),
                                                      seq_lens_encoder.lod(),
                                                      seq_lens_encoder.offset());
  VLOG(4) << "Builder construction  meta_seq_lens_encoder";
  paddle::dialect::IrMetaTensor meta_seq_lens_encoder(&ir_tensor_seq_lens_encoder);

  VLOG(4) << "Builder construction  dense_seq_lens_decoder";
  paddle::dialect::IrTensor ir_tensor_seq_lens_decoder(paddle::dialect::TransToPhiDataType(seq_lens_decoder.dtype()),
                                                      seq_lens_decoder.dims(),
                                                      seq_lens_decoder.data_layout(),
                                                      seq_lens_decoder.lod(),
                                                      seq_lens_decoder.offset());
  VLOG(4) << "Builder construction  meta_seq_lens_decoder";
  paddle::dialect::IrMetaTensor meta_seq_lens_decoder(&ir_tensor_seq_lens_decoder);

  VLOG(4) << "Builder construction  dense_seq_lens_this_time";
  paddle::dialect::IrTensor ir_tensor_seq_lens_this_time(paddle::dialect::TransToPhiDataType(seq_lens_this_time.dtype()),
                                                      seq_lens_this_time.dims(),
                                                      seq_lens_this_time.data_layout(),
                                                      seq_lens_this_time.lod(),
                                                      seq_lens_this_time.offset());
  VLOG(4) << "Builder construction  meta_seq_lens_this_time";
  paddle::dialect::IrMetaTensor meta_seq_lens_this_time(&ir_tensor_seq_lens_this_time);

  VLOG(4) << "Builder construction  dense_padding_offsets";
  paddle::dialect::IrTensor ir_tensor_padding_offsets(paddle::dialect::TransToPhiDataType(padding_offsets.dtype()),
                                                      padding_offsets.dims(),
                                                      padding_offsets.data_layout(),
                                                      padding_offsets.lod(),
                                                      padding_offsets.offset());
  VLOG(4) << "Builder construction  meta_padding_offsets";
  paddle::dialect::IrMetaTensor meta_padding_offsets(&ir_tensor_padding_offsets);

  VLOG(4) << "Builder construction  dense_cum_offsets";
  paddle::dialect::IrTensor ir_tensor_cum_offsets(paddle::dialect::TransToPhiDataType(cum_offsets.dtype()),
                                                      cum_offsets.dims(),
                                                      cum_offsets.data_layout(),
                                                      cum_offsets.lod(),
                                                      cum_offsets.offset());
  VLOG(4) << "Builder construction  meta_cum_offsets";
  paddle::dialect::IrMetaTensor meta_cum_offsets(&ir_tensor_cum_offsets);

  VLOG(4) << "Builder construction  dense_cu_seqlens_q";
  paddle::dialect::IrTensor ir_tensor_cu_seqlens_q(paddle::dialect::TransToPhiDataType(cu_seqlens_q.dtype()),
                                                      cu_seqlens_q.dims(),
                                                      cu_seqlens_q.data_layout(),
                                                      cu_seqlens_q.lod(),
                                                      cu_seqlens_q.offset());
  VLOG(4) << "Builder construction  meta_cu_seqlens_q";
  paddle::dialect::IrMetaTensor meta_cu_seqlens_q(&ir_tensor_cu_seqlens_q);

  VLOG(4) << "Builder construction  dense_cu_seqlens_k";
  paddle::dialect::IrTensor ir_tensor_cu_seqlens_k(paddle::dialect::TransToPhiDataType(cu_seqlens_k.dtype()),
                                                      cu_seqlens_k.dims(),
                                                      cu_seqlens_k.data_layout(),
                                                      cu_seqlens_k.lod(),
                                                      cu_seqlens_k.offset());
  VLOG(4) << "Builder construction  meta_cu_seqlens_k";
  paddle::dialect::IrMetaTensor meta_cu_seqlens_k(&ir_tensor_cu_seqlens_k);

  VLOG(4) << "Builder construction  dense_block_tables";
  paddle::dialect::IrTensor ir_tensor_block_tables(paddle::dialect::TransToPhiDataType(block_tables.dtype()),
                                                      block_tables.dims(),
                                                      block_tables.data_layout(),
                                                      block_tables.lod(),
                                                      block_tables.offset());
  VLOG(4) << "Builder construction  meta_block_tables";
  paddle::dialect::IrMetaTensor meta_block_tables(&ir_tensor_block_tables);

  paddle::dialect::IrMetaTensor meta_pre_key_cache;
  paddle::dialect::IrTensor ir_tensor_pre_key_cache;
  if (pre_key_cache_.impl() != nullptr) {
    paddle::dialect::DenseTensorType pre_key_cache = pre_key_cache_.type().dyn_cast<paddle::dialect::DenseTensorType>();
    VLOG(4) << "Builder construction  dense_pre_key_cache";
    ir_tensor_pre_key_cache = paddle::dialect::IrTensor(paddle::dialect::TransToPhiDataType(pre_key_cache.dtype()),
                                                        pre_key_cache.dims(),
                                                        pre_key_cache.data_layout(),
                                                        pre_key_cache.lod(),
                                                        pre_key_cache.offset());
    VLOG(4) << "Builder construction  meta_pre_key_cache";
    meta_pre_key_cache = paddle::dialect::IrMetaTensor(&ir_tensor_pre_key_cache);
  }


  paddle::dialect::IrMetaTensor meta_pre_value_cache;
  paddle::dialect::IrTensor ir_tensor_pre_value_cache;
  if (pre_value_cache_.impl() != nullptr) {
    paddle::dialect::DenseTensorType pre_value_cache = pre_value_cache_.type().dyn_cast<paddle::dialect::DenseTensorType>();
    VLOG(4) << "Builder construction  dense_pre_value_cache";
    ir_tensor_pre_value_cache = paddle::dialect::IrTensor(paddle::dialect::TransToPhiDataType(pre_value_cache.dtype()),
                                                        pre_value_cache.dims(),
                                                        pre_value_cache.data_layout(),
                                                        pre_value_cache.lod(),
                                                        pre_value_cache.offset());
    VLOG(4) << "Builder construction  meta_pre_value_cache";
    meta_pre_value_cache = paddle::dialect::IrMetaTensor(&ir_tensor_pre_value_cache);
  }


  paddle::dialect::IrMetaTensor meta_rope_emb;
  paddle::dialect::IrTensor ir_tensor_rope_emb;
  if (rope_emb_.impl() != nullptr) {
    paddle::dialect::DenseTensorType rope_emb = rope_emb_.type().dyn_cast<paddle::dialect::DenseTensorType>();
    VLOG(4) << "Builder construction  dense_rope_emb";
    ir_tensor_rope_emb = paddle::dialect::IrTensor(paddle::dialect::TransToPhiDataType(rope_emb.dtype()),
                                                        rope_emb.dims(),
                                                        rope_emb.data_layout(),
                                                        rope_emb.lod(),
                                                        rope_emb.offset());
    VLOG(4) << "Builder construction  meta_rope_emb";
    meta_rope_emb = paddle::dialect::IrMetaTensor(&ir_tensor_rope_emb);
  }


  paddle::dialect::IrMetaTensor meta_mask;
  paddle::dialect::IrTensor ir_tensor_mask;
  if (mask_.impl() != nullptr) {
    paddle::dialect::DenseTensorType mask = mask_.type().dyn_cast<paddle::dialect::DenseTensorType>();
    VLOG(4) << "Builder construction  dense_mask";
    ir_tensor_mask = paddle::dialect::IrTensor(paddle::dialect::TransToPhiDataType(mask.dtype()),
                                                        mask.dims(),
                                                        mask.data_layout(),
                                                        mask.lod(),
                                                        mask.offset());
    VLOG(4) << "Builder construction  meta_mask";
    meta_mask = paddle::dialect::IrMetaTensor(&ir_tensor_mask);
  }


  paddle::dialect::IrMetaTensor meta_tgt_mask;
  paddle::dialect::IrTensor ir_tensor_tgt_mask;
  if (tgt_mask_.impl() != nullptr) {
    paddle::dialect::DenseTensorType tgt_mask = tgt_mask_.type().dyn_cast<paddle::dialect::DenseTensorType>();
    VLOG(4) << "Builder construction  dense_tgt_mask";
    ir_tensor_tgt_mask = paddle::dialect::IrTensor(paddle::dialect::TransToPhiDataType(tgt_mask.dtype()),
                                                        tgt_mask.dims(),
                                                        tgt_mask.data_layout(),
                                                        tgt_mask.lod(),
                                                        tgt_mask.offset());
    VLOG(4) << "Builder construction  meta_tgt_mask";
    meta_tgt_mask = paddle::dialect::IrMetaTensor(&ir_tensor_tgt_mask);
  }


  paddle::dialect::IrMetaTensor meta_cache_k_quant_scales;
  paddle::dialect::IrTensor ir_tensor_cache_k_quant_scales;
  if (cache_k_quant_scales_.impl() != nullptr) {
    paddle::dialect::DenseTensorType cache_k_quant_scales = cache_k_quant_scales_.type().dyn_cast<paddle::dialect::DenseTensorType>();
    VLOG(4) << "Builder construction  dense_cache_k_quant_scales";
    ir_tensor_cache_k_quant_scales = paddle::dialect::IrTensor(paddle::dialect::TransToPhiDataType(cache_k_quant_scales.dtype()),
                                                        cache_k_quant_scales.dims(),
                                                        cache_k_quant_scales.data_layout(),
                                                        cache_k_quant_scales.lod(),
                                                        cache_k_quant_scales.offset());
    VLOG(4) << "Builder construction  meta_cache_k_quant_scales";
    meta_cache_k_quant_scales = paddle::dialect::IrMetaTensor(&ir_tensor_cache_k_quant_scales);
  }


  paddle::dialect::IrMetaTensor meta_cache_v_quant_scales;
  paddle::dialect::IrTensor ir_tensor_cache_v_quant_scales;
  if (cache_v_quant_scales_.impl() != nullptr) {
    paddle::dialect::DenseTensorType cache_v_quant_scales = cache_v_quant_scales_.type().dyn_cast<paddle::dialect::DenseTensorType>();
    VLOG(4) << "Builder construction  dense_cache_v_quant_scales";
    ir_tensor_cache_v_quant_scales = paddle::dialect::IrTensor(paddle::dialect::TransToPhiDataType(cache_v_quant_scales.dtype()),
                                                        cache_v_quant_scales.dims(),
                                                        cache_v_quant_scales.data_layout(),
                                                        cache_v_quant_scales.lod(),
                                                        cache_v_quant_scales.offset());
    VLOG(4) << "Builder construction  meta_cache_v_quant_scales";
    meta_cache_v_quant_scales = paddle::dialect::IrMetaTensor(&ir_tensor_cache_v_quant_scales);
  }


  paddle::dialect::IrMetaTensor meta_cache_k_dequant_scales;
  paddle::dialect::IrTensor ir_tensor_cache_k_dequant_scales;
  if (cache_k_dequant_scales_.impl() != nullptr) {
    paddle::dialect::DenseTensorType cache_k_dequant_scales = cache_k_dequant_scales_.type().dyn_cast<paddle::dialect::DenseTensorType>();
    VLOG(4) << "Builder construction  dense_cache_k_dequant_scales";
    ir_tensor_cache_k_dequant_scales = paddle::dialect::IrTensor(paddle::dialect::TransToPhiDataType(cache_k_dequant_scales.dtype()),
                                                        cache_k_dequant_scales.dims(),
                                                        cache_k_dequant_scales.data_layout(),
                                                        cache_k_dequant_scales.lod(),
                                                        cache_k_dequant_scales.offset());
    VLOG(4) << "Builder construction  meta_cache_k_dequant_scales";
    meta_cache_k_dequant_scales = paddle::dialect::IrMetaTensor(&ir_tensor_cache_k_dequant_scales);
  }


  paddle::dialect::IrMetaTensor meta_cache_v_dequant_scales;
  paddle::dialect::IrTensor ir_tensor_cache_v_dequant_scales;
  if (cache_v_dequant_scales_.impl() != nullptr) {
    paddle::dialect::DenseTensorType cache_v_dequant_scales = cache_v_dequant_scales_.type().dyn_cast<paddle::dialect::DenseTensorType>();
    VLOG(4) << "Builder construction  dense_cache_v_dequant_scales";
    ir_tensor_cache_v_dequant_scales = paddle::dialect::IrTensor(paddle::dialect::TransToPhiDataType(cache_v_dequant_scales.dtype()),
                                                        cache_v_dequant_scales.dims(),
                                                        cache_v_dequant_scales.data_layout(),
                                                        cache_v_dequant_scales.lod(),
                                                        cache_v_dequant_scales.offset());
    VLOG(4) << "Builder construction  meta_cache_v_dequant_scales";
    meta_cache_v_dequant_scales = paddle::dialect::IrMetaTensor(&ir_tensor_cache_v_dequant_scales);
  }


  paddle::dialect::IrMetaTensor meta_qkv_out_scale;
  paddle::dialect::IrTensor ir_tensor_qkv_out_scale;
  if (qkv_out_scale_.impl() != nullptr) {
    paddle::dialect::DenseTensorType qkv_out_scale = qkv_out_scale_.type().dyn_cast<paddle::dialect::DenseTensorType>();
    VLOG(4) << "Builder construction  dense_qkv_out_scale";
    ir_tensor_qkv_out_scale = paddle::dialect::IrTensor(paddle::dialect::TransToPhiDataType(qkv_out_scale.dtype()),
                                                        qkv_out_scale.dims(),
                                                        qkv_out_scale.data_layout(),
                                                        qkv_out_scale.lod(),
                                                        qkv_out_scale.offset());
    VLOG(4) << "Builder construction  meta_qkv_out_scale";
    meta_qkv_out_scale = paddle::dialect::IrMetaTensor(&ir_tensor_qkv_out_scale);
  }


  paddle::dialect::IrMetaTensor meta_qkv_bias;
  paddle::dialect::IrTensor ir_tensor_qkv_bias;
  if (qkv_bias_.impl() != nullptr) {
    paddle::dialect::DenseTensorType qkv_bias = qkv_bias_.type().dyn_cast<paddle::dialect::DenseTensorType>();
    VLOG(4) << "Builder construction  dense_qkv_bias";
    ir_tensor_qkv_bias = paddle::dialect::IrTensor(paddle::dialect::TransToPhiDataType(qkv_bias.dtype()),
                                                        qkv_bias.dims(),
                                                        qkv_bias.data_layout(),
                                                        qkv_bias.lod(),
                                                        qkv_bias.offset());
    VLOG(4) << "Builder construction  meta_qkv_bias";
    meta_qkv_bias = paddle::dialect::IrMetaTensor(&ir_tensor_qkv_bias);
  }


  paddle::dialect::IrMetaTensor meta_out_shift;
  paddle::dialect::IrTensor ir_tensor_out_shift;
  if (out_shift_.impl() != nullptr) {
    paddle::dialect::DenseTensorType out_shift = out_shift_.type().dyn_cast<paddle::dialect::DenseTensorType>();
    VLOG(4) << "Builder construction  dense_out_shift";
    ir_tensor_out_shift = paddle::dialect::IrTensor(paddle::dialect::TransToPhiDataType(out_shift.dtype()),
                                                        out_shift.dims(),
                                                        out_shift.data_layout(),
                                                        out_shift.lod(),
                                                        out_shift.offset());
    VLOG(4) << "Builder construction  meta_out_shift";
    meta_out_shift = paddle::dialect::IrMetaTensor(&ir_tensor_out_shift);
  }


  paddle::dialect::IrMetaTensor meta_out_smooth;
  paddle::dialect::IrTensor ir_tensor_out_smooth;
  if (out_smooth_.impl() != nullptr) {
    paddle::dialect::DenseTensorType out_smooth = out_smooth_.type().dyn_cast<paddle::dialect::DenseTensorType>();
    VLOG(4) << "Builder construction  dense_out_smooth";
    ir_tensor_out_smooth = paddle::dialect::IrTensor(paddle::dialect::TransToPhiDataType(out_smooth.dtype()),
                                                        out_smooth.dims(),
                                                        out_smooth.data_layout(),
                                                        out_smooth.lod(),
                                                        out_smooth.offset());
    VLOG(4) << "Builder construction  meta_out_smooth";
    meta_out_smooth = paddle::dialect::IrMetaTensor(&ir_tensor_out_smooth);
  }

  paddle::dialect::IrTensor dense_fmha_out;
  paddle::dialect::IrMetaTensor meta_fmha_out(&dense_fmha_out);
  paddle::dialect::IrTensor dense_qkv_out;
  paddle::dialect::IrMetaTensor meta_qkv_out(&dense_qkv_out);
  paddle::dialect::IrTensor dense_key_cache_out;
  paddle::dialect::IrMetaTensor meta_key_cache_out(&dense_key_cache_out);
  paddle::dialect::IrTensor dense_value_cache_out;
  paddle::dialect::IrMetaTensor meta_value_cache_out(&dense_value_cache_out);

  phi::BlockMultiheadAttentionInferMeta(meta_qkv, meta_key_cache, meta_value_cache, meta_seq_lens_encoder, meta_seq_lens_decoder, meta_seq_lens_this_time, meta_padding_offsets, meta_cum_offsets, meta_cu_seqlens_q, meta_cu_seqlens_k, meta_block_tables, meta_pre_key_cache, meta_pre_value_cache, meta_rope_emb, meta_mask, meta_tgt_mask, meta_cache_k_quant_scales, meta_cache_v_quant_scales, meta_cache_k_dequant_scales, meta_cache_v_dequant_scales, meta_qkv_out_scale, meta_qkv_bias, meta_out_shift, meta_out_smooth, max_seq_len, block_size, use_neox_style, dynamic_cachekv_quant, quant_round_type, quant_max_bound, quant_min_bound, out_scale, compute_dtype, &meta_fmha_out, &meta_qkv_out, &meta_key_cache_out, &meta_value_cache_out);

  std::vector<pir::Type> argument_outputs;
  pir::Type fmha_out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_fmha_out.dtype()), dense_fmha_out.dims(), dense_fmha_out.layout(), dense_fmha_out.lod(), dense_fmha_out.offset());
  argument_outputs.push_back(fmha_out_dense_tensor_type);

  pir::Type qkv_out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_qkv_out.dtype()), dense_qkv_out.dims(), dense_qkv_out.layout(), dense_qkv_out.lod(), dense_qkv_out.offset());
  argument_outputs.push_back(qkv_out_dense_tensor_type);

  pir::Type key_cache_out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_key_cache_out.dtype()), dense_key_cache_out.dims(), dense_key_cache_out.layout(), dense_key_cache_out.lod(), dense_key_cache_out.offset());
  argument_outputs.push_back(key_cache_out_dense_tensor_type);

  pir::Type value_cache_out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_value_cache_out.dtype()), dense_value_cache_out.dims(), dense_value_cache_out.layout(), dense_value_cache_out.lod(), dense_value_cache_out.offset());
  argument_outputs.push_back(value_cache_out_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void BlockMultiheadAttention_Op::VerifySig() {
  VLOG(4) << "Start Verifying inputs, outputs and attributes for: BlockMultiheadAttention_Op.";
  VLOG(4) << "Verifying inputs:";
  {
  auto input_size = num_operands();
  IR_ENFORCE(input_size == 24u,
                    "The size %d of inputs must be equal to 24.", input_size);
  IR_ENFORCE((*this)->operand_source(0).type().isa<paddle::dialect::DenseTensorType>(),
                  "Type validation failed for the 0th input, got %s.", (*this)->operand_source(0).type());
  IR_ENFORCE((*this)->operand_source(1).type().isa<paddle::dialect::DenseTensorType>(),
                  "Type validation failed for the 1th input, got %s.", (*this)->operand_source(1).type());
  IR_ENFORCE((*this)->operand_source(2).type().isa<paddle::dialect::DenseTensorType>(),
                  "Type validation failed for the 2th input, got %s.", (*this)->operand_source(2).type());
  IR_ENFORCE((*this)->operand_source(3).type().isa<paddle::dialect::DenseTensorType>(),
                  "Type validation failed for the 3th input, got %s.", (*this)->operand_source(3).type());
  IR_ENFORCE((*this)->operand_source(4).type().isa<paddle::dialect::DenseTensorType>(),
                  "Type validation failed for the 4th input, got %s.", (*this)->operand_source(4).type());
  IR_ENFORCE((*this)->operand_source(5).type().isa<paddle::dialect::DenseTensorType>(),
                  "Type validation failed for the 5th input, got %s.", (*this)->operand_source(5).type());
  IR_ENFORCE((*this)->operand_source(6).type().isa<paddle::dialect::DenseTensorType>(),
                  "Type validation failed for the 6th input, got %s.", (*this)->operand_source(6).type());
  IR_ENFORCE((*this)->operand_source(7).type().isa<paddle::dialect::DenseTensorType>(),
                  "Type validation failed for the 7th input, got %s.", (*this)->operand_source(7).type());
  IR_ENFORCE((*this)->operand_source(8).type().isa<paddle::dialect::DenseTensorType>(),
                  "Type validation failed for the 8th input, got %s.", (*this)->operand_source(8).type());
  IR_ENFORCE((*this)->operand_source(9).type().isa<paddle::dialect::DenseTensorType>(),
                  "Type validation failed for the 9th input, got %s.", (*this)->operand_source(9).type());
  IR_ENFORCE((*this)->operand_source(10).type().isa<paddle::dialect::DenseTensorType>(),
                  "Type validation failed for the 10th input, got %s.", (*this)->operand_source(10).type());
  if (auto val = (*this)->operand(11)) {
    IR_ENFORCE(val.type().isa<paddle::dialect::DenseTensorType>(),
                   "Type validation failed for the 11th input, got %s.", (*this)->operand_source(11).type());
  }
  if (auto val = (*this)->operand(12)) {
    IR_ENFORCE(val.type().isa<paddle::dialect::DenseTensorType>(),
                   "Type validation failed for the 12th input, got %s.", (*this)->operand_source(12).type());
  }
  if (auto val = (*this)->operand(13)) {
    IR_ENFORCE(val.type().isa<paddle::dialect::DenseTensorType>(),
                   "Type validation failed for the 13th input, got %s.", (*this)->operand_source(13).type());
  }
  if (auto val = (*this)->operand(14)) {
    IR_ENFORCE(val.type().isa<paddle::dialect::DenseTensorType>(),
                   "Type validation failed for the 14th input, got %s.", (*this)->operand_source(14).type());
  }
  if (auto val = (*this)->operand(15)) {
    IR_ENFORCE(val.type().isa<paddle::dialect::DenseTensorType>(),
                   "Type validation failed for the 15th input, got %s.", (*this)->operand_source(15).type());
  }
  if (auto val = (*this)->operand(16)) {
    IR_ENFORCE(val.type().isa<paddle::dialect::DenseTensorType>(),
                   "Type validation failed for the 16th input, got %s.", (*this)->operand_source(16).type());
  }
  if (auto val = (*this)->operand(17)) {
    IR_ENFORCE(val.type().isa<paddle::dialect::DenseTensorType>(),
                   "Type validation failed for the 17th input, got %s.", (*this)->operand_source(17).type());
  }
  if (auto val = (*this)->operand(18)) {
    IR_ENFORCE(val.type().isa<paddle::dialect::DenseTensorType>(),
                   "Type validation failed for the 18th input, got %s.", (*this)->operand_source(18).type());
  }
  if (auto val = (*this)->operand(19)) {
    IR_ENFORCE(val.type().isa<paddle::dialect::DenseTensorType>(),
                   "Type validation failed for the 19th input, got %s.", (*this)->operand_source(19).type());
  }
  if (auto val = (*this)->operand(20)) {
    IR_ENFORCE(val.type().isa<paddle::dialect::DenseTensorType>(),
                   "Type validation failed for the 20th input, got %s.", (*this)->operand_source(20).type());
  }
  if (auto val = (*this)->operand(21)) {
    IR_ENFORCE(val.type().isa<paddle::dialect::DenseTensorType>(),
                   "Type validation failed for the 21th input, got %s.", (*this)->operand_source(21).type());
  }
  if (auto val = (*this)->operand(22)) {
    IR_ENFORCE(val.type().isa<paddle::dialect::DenseTensorType>(),
                   "Type validation failed for the 22th input, got %s.", (*this)->operand_source(22).type());
  }
  if (auto val = (*this)->operand(23)) {
    IR_ENFORCE(val.type().isa<paddle::dialect::DenseTensorType>(),
                   "Type validation failed for the 23th input, got %s.", (*this)->operand_source(23).type());
  }
  }
  VLOG(4) << "Verifying attributes:";
  {
  auto& attributes = this->attributes();
  IR_ENFORCE(attributes.count("max_seq_len")>0,
                 "max_seq_len does not exist.");
  IR_ENFORCE(attributes.at("max_seq_len").isa<pir::Int32Attribute>(),
                 "Type of attribute: max_seq_len is not pir::Int32Attribute.");

  IR_ENFORCE(attributes.count("block_size")>0,
                 "block_size does not exist.");
  IR_ENFORCE(attributes.at("block_size").isa<pir::Int32Attribute>(),
                 "Type of attribute: block_size is not pir::Int32Attribute.");

  IR_ENFORCE(attributes.count("use_neox_style")>0,
                 "use_neox_style does not exist.");
  IR_ENFORCE(attributes.at("use_neox_style").isa<pir::BoolAttribute>(),
                 "Type of attribute: use_neox_style is not pir::BoolAttribute.");

  IR_ENFORCE(attributes.count("dynamic_cachekv_quant")>0,
                 "dynamic_cachekv_quant does not exist.");
  IR_ENFORCE(attributes.at("dynamic_cachekv_quant").isa<pir::BoolAttribute>(),
                 "Type of attribute: dynamic_cachekv_quant is not pir::BoolAttribute.");

  IR_ENFORCE(attributes.count("quant_round_type")>0,
                 "quant_round_type does not exist.");
  IR_ENFORCE(attributes.at("quant_round_type").isa<pir::Int32Attribute>(),
                 "Type of attribute: quant_round_type is not pir::Int32Attribute.");

  IR_ENFORCE(attributes.count("quant_max_bound")>0,
                 "quant_max_bound does not exist.");
  IR_ENFORCE(attributes.at("quant_max_bound").isa<pir::FloatAttribute>(),
                 "Type of attribute: quant_max_bound is not pir::FloatAttribute.");

  IR_ENFORCE(attributes.count("quant_min_bound")>0,
                 "quant_min_bound does not exist.");
  IR_ENFORCE(attributes.at("quant_min_bound").isa<pir::FloatAttribute>(),
                 "Type of attribute: quant_min_bound is not pir::FloatAttribute.");

  IR_ENFORCE(attributes.count("out_scale")>0,
                 "out_scale does not exist.");
  IR_ENFORCE(attributes.at("out_scale").isa<pir::FloatAttribute>(),
                 "Type of attribute: out_scale is not pir::FloatAttribute.");

  IR_ENFORCE(attributes.count("compute_dtype")>0,
                 "compute_dtype does not exist.");
  IR_ENFORCE(attributes.at("compute_dtype").isa<pir::StrAttribute>(),
                 "Type of attribute: compute_dtype is not pir::StrAttribute.");

  }
  VLOG(4) << "Verifying outputs:";
  {
  auto output_size = num_results();
  IR_ENFORCE(output_size == 4u,
                    "The size %d of outputs must be equal to 4.", output_size);
  IR_ENFORCE((*this)->result(0).type().isa<paddle::dialect::DenseTensorType>(),
                 "Type validation failed for the 0th output.");
  IR_ENFORCE((*this)->result(1).type().isa<paddle::dialect::DenseTensorType>(),
                 "Type validation failed for the 1th output.");
  IR_ENFORCE((*this)->result(2).type().isa<paddle::dialect::DenseTensorType>(),
                 "Type validation failed for the 2th output.");
  IR_ENFORCE((*this)->result(3).type().isa<paddle::dialect::DenseTensorType>(),
                 "Type validation failed for the 3th output.");
  }
  VLOG(4) << "End Verifying for: BlockMultiheadAttention_Op.";
}

void BlockMultiheadAttention_Op::InferMeta( phi::InferMetaContext *infer_meta ) {
  auto fn = PD_INFER_META(phi::BlockMultiheadAttentionInferMeta);
  fn(infer_meta);
}

phi::DataType BlockMultiheadAttention_Op::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: BlockMultiheadAttention_Op";
  


  return expected_kernel_dtype;
}

const char *BnActXpuOp::attributes_name[4] = { "momentum", "epsilon", "data_layout", "act_type" };

OpInfoTuple BnActXpuOp::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("x", "paddle::dialect::DenseTensorType", false, false, false, false), paddle::dialect::OpInputInfo("mean", "paddle::dialect::DenseTensorType", false, false, false, false), paddle::dialect::OpInputInfo("variance", "paddle::dialect::DenseTensorType", false, false, false, false), paddle::dialect::OpInputInfo("scale", "paddle::dialect::DenseTensorType", false, false, false, false), paddle::dialect::OpInputInfo("bias", "paddle::dialect::DenseTensorType", false, false, false, false) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = { paddle::dialect::OpAttributeInfo("momentum", "pir::FloatAttribute", ""), paddle::dialect::OpAttributeInfo("epsilon", "pir::FloatAttribute", ""), paddle::dialect::OpAttributeInfo("data_layout", "pir::StrAttribute", ""), paddle::dialect::OpAttributeInfo("act_type", "pir::Int32Attribute", "") };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("out", "paddle::dialect::DenseTensorType", false, false) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("BNActXPUInferMeta", {"x", "mean", "variance", "scale", "bias", "momentum", "epsilon", "data_layout", "act_type"}, "bn_act_xpu", {"x", "mean", "variance", "scale", "bias", "momentum", "epsilon", "data_layout", "act_type"}, {"x"}, {}, {}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "bn_act_xpu");
}

void BnActXpuOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, pir::Value mean_, pir::Value variance_, pir::Value scale_, pir::Value bias_, float momentum, float epsilon, const std::string& data_layout, int act_type) {
  VLOG(4) << "Start build BnActXpuOp";



  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_, mean_, variance_, scale_, bias_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_momentum = pir::FloatAttribute::get(pir::IrContext::Instance(), momentum);
  argument.AddAttribute("momentum", attr_momentum);
  pir::Attribute attr_epsilon = pir::FloatAttribute::get(pir::IrContext::Instance(), epsilon);
  argument.AddAttribute("epsilon", attr_epsilon);
  pir::Attribute attr_data_layout = pir::StrAttribute::get(pir::IrContext::Instance(), data_layout);
  argument.AddAttribute("data_layout", attr_data_layout);
  pir::Attribute attr_act_type = pir::Int32Attribute::get(pir::IrContext::Instance(), act_type);
  argument.AddAttribute("act_type", attr_act_type);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;
  paddle::dialect::DenseTensorType mean = mean_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)mean;
  paddle::dialect::DenseTensorType variance = variance_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)variance;
  paddle::dialect::DenseTensorType scale = scale_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)scale;
  paddle::dialect::DenseTensorType bias = bias_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)bias;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);

  VLOG(4) << "Builder construction  dense_mean";
  paddle::dialect::IrTensor ir_tensor_mean(paddle::dialect::TransToPhiDataType(mean.dtype()),
                                                      mean.dims(),
                                                      mean.data_layout(),
                                                      mean.lod(),
                                                      mean.offset());
  VLOG(4) << "Builder construction  meta_mean";
  paddle::dialect::IrMetaTensor meta_mean(&ir_tensor_mean);

  VLOG(4) << "Builder construction  dense_variance";
  paddle::dialect::IrTensor ir_tensor_variance(paddle::dialect::TransToPhiDataType(variance.dtype()),
                                                      variance.dims(),
                                                      variance.data_layout(),
                                                      variance.lod(),
                                                      variance.offset());
  VLOG(4) << "Builder construction  meta_variance";
  paddle::dialect::IrMetaTensor meta_variance(&ir_tensor_variance);

  VLOG(4) << "Builder construction  dense_scale";
  paddle::dialect::IrTensor ir_tensor_scale(paddle::dialect::TransToPhiDataType(scale.dtype()),
                                                      scale.dims(),
                                                      scale.data_layout(),
                                                      scale.lod(),
                                                      scale.offset());
  VLOG(4) << "Builder construction  meta_scale";
  paddle::dialect::IrMetaTensor meta_scale(&ir_tensor_scale);

  VLOG(4) << "Builder construction  dense_bias";
  paddle::dialect::IrTensor ir_tensor_bias(paddle::dialect::TransToPhiDataType(bias.dtype()),
                                                      bias.dims(),
                                                      bias.data_layout(),
                                                      bias.lod(),
                                                      bias.offset());
  VLOG(4) << "Builder construction  meta_bias";
  paddle::dialect::IrMetaTensor meta_bias(&ir_tensor_bias);
  paddle::dialect::IrTensor dense_out;
  paddle::dialect::IrMetaTensor meta_out(&dense_out);

  phi::BNActXPUInferMeta(meta_x, meta_mean, meta_variance, meta_scale, meta_bias, momentum, epsilon, data_layout, act_type, &meta_out);

  std::vector<pir::Type> argument_outputs;
  pir::Type out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_out.dtype()), dense_out.dims(), dense_out.layout(), dense_out.lod(), dense_out.offset());
  argument_outputs.push_back(out_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void BnActXpuOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, pir::Value mean_, pir::Value variance_, pir::Value scale_, pir::Value bias_, pir::AttributeMap attributes) {
  VLOG(4) << "Start build BnActXpuOp";


  IR_ENFORCE(
      attributes.find("momentum") != attributes.end(),
          "'momentum' Attribute is expected for BnActXpuOp. ");
  float momentum = attributes.at("momentum").dyn_cast<pir::FloatAttribute>().data();

  IR_ENFORCE(
      attributes.find("epsilon") != attributes.end(),
          "'epsilon' Attribute is expected for BnActXpuOp. ");
  float epsilon = attributes.at("epsilon").dyn_cast<pir::FloatAttribute>().data();

  IR_ENFORCE(
      attributes.find("data_layout") != attributes.end(),
          "'data_layout' Attribute is expected for BnActXpuOp. ");
  std::string data_layout = attributes.at("data_layout").dyn_cast<pir::StrAttribute>().AsString();

  IR_ENFORCE(
      attributes.find("act_type") != attributes.end(),
          "'act_type' Attribute is expected for BnActXpuOp. ");
  int act_type = attributes.at("act_type").dyn_cast<pir::Int32Attribute>().data();


  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_, mean_, variance_, scale_, bias_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_momentum = pir::FloatAttribute::get(pir::IrContext::Instance(), momentum);
  argument.AddAttribute("momentum", attr_momentum);
  pir::Attribute attr_epsilon = pir::FloatAttribute::get(pir::IrContext::Instance(), epsilon);
  argument.AddAttribute("epsilon", attr_epsilon);
  pir::Attribute attr_data_layout = pir::StrAttribute::get(pir::IrContext::Instance(), data_layout);
  argument.AddAttribute("data_layout", attr_data_layout);
  pir::Attribute attr_act_type = pir::Int32Attribute::get(pir::IrContext::Instance(), act_type);
  argument.AddAttribute("act_type", attr_act_type);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;
  paddle::dialect::DenseTensorType mean = mean_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)mean;
  paddle::dialect::DenseTensorType variance = variance_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)variance;
  paddle::dialect::DenseTensorType scale = scale_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)scale;
  paddle::dialect::DenseTensorType bias = bias_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)bias;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);

  VLOG(4) << "Builder construction  dense_mean";
  paddle::dialect::IrTensor ir_tensor_mean(paddle::dialect::TransToPhiDataType(mean.dtype()),
                                                      mean.dims(),
                                                      mean.data_layout(),
                                                      mean.lod(),
                                                      mean.offset());
  VLOG(4) << "Builder construction  meta_mean";
  paddle::dialect::IrMetaTensor meta_mean(&ir_tensor_mean);

  VLOG(4) << "Builder construction  dense_variance";
  paddle::dialect::IrTensor ir_tensor_variance(paddle::dialect::TransToPhiDataType(variance.dtype()),
                                                      variance.dims(),
                                                      variance.data_layout(),
                                                      variance.lod(),
                                                      variance.offset());
  VLOG(4) << "Builder construction  meta_variance";
  paddle::dialect::IrMetaTensor meta_variance(&ir_tensor_variance);

  VLOG(4) << "Builder construction  dense_scale";
  paddle::dialect::IrTensor ir_tensor_scale(paddle::dialect::TransToPhiDataType(scale.dtype()),
                                                      scale.dims(),
                                                      scale.data_layout(),
                                                      scale.lod(),
                                                      scale.offset());
  VLOG(4) << "Builder construction  meta_scale";
  paddle::dialect::IrMetaTensor meta_scale(&ir_tensor_scale);

  VLOG(4) << "Builder construction  dense_bias";
  paddle::dialect::IrTensor ir_tensor_bias(paddle::dialect::TransToPhiDataType(bias.dtype()),
                                                      bias.dims(),
                                                      bias.data_layout(),
                                                      bias.lod(),
                                                      bias.offset());
  VLOG(4) << "Builder construction  meta_bias";
  paddle::dialect::IrMetaTensor meta_bias(&ir_tensor_bias);
  paddle::dialect::IrTensor dense_out;
  paddle::dialect::IrMetaTensor meta_out(&dense_out);

  phi::BNActXPUInferMeta(meta_x, meta_mean, meta_variance, meta_scale, meta_bias, momentum, epsilon, data_layout, act_type, &meta_out);

  std::vector<pir::Type> argument_outputs;
  pir::Type out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_out.dtype()), dense_out.dims(), dense_out.layout(), dense_out.lod(), dense_out.offset());
  argument_outputs.push_back(out_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void BnActXpuOp::VerifySig() {
  VLOG(4) << "Start Verifying inputs, outputs and attributes for: BnActXpuOp.";
  VLOG(4) << "Verifying inputs:";
  {
  auto input_size = num_operands();
  IR_ENFORCE(input_size == 5u,
                    "The size %d of inputs must be equal to 5.", input_size);
  IR_ENFORCE((*this)->operand_source(0).type().isa<paddle::dialect::DenseTensorType>(),
                  "Type validation failed for the 0th input, got %s.", (*this)->operand_source(0).type());
  IR_ENFORCE((*this)->operand_source(1).type().isa<paddle::dialect::DenseTensorType>(),
                  "Type validation failed for the 1th input, got %s.", (*this)->operand_source(1).type());
  IR_ENFORCE((*this)->operand_source(2).type().isa<paddle::dialect::DenseTensorType>(),
                  "Type validation failed for the 2th input, got %s.", (*this)->operand_source(2).type());
  IR_ENFORCE((*this)->operand_source(3).type().isa<paddle::dialect::DenseTensorType>(),
                  "Type validation failed for the 3th input, got %s.", (*this)->operand_source(3).type());
  IR_ENFORCE((*this)->operand_source(4).type().isa<paddle::dialect::DenseTensorType>(),
                  "Type validation failed for the 4th input, got %s.", (*this)->operand_source(4).type());
  }
  VLOG(4) << "Verifying attributes:";
  {
  auto& attributes = this->attributes();
  IR_ENFORCE(attributes.count("momentum")>0,
                 "momentum does not exist.");
  IR_ENFORCE(attributes.at("momentum").isa<pir::FloatAttribute>(),
                 "Type of attribute: momentum is not pir::FloatAttribute.");

  IR_ENFORCE(attributes.count("epsilon")>0,
                 "epsilon does not exist.");
  IR_ENFORCE(attributes.at("epsilon").isa<pir::FloatAttribute>(),
                 "Type of attribute: epsilon is not pir::FloatAttribute.");

  IR_ENFORCE(attributes.count("data_layout")>0,
                 "data_layout does not exist.");
  IR_ENFORCE(attributes.at("data_layout").isa<pir::StrAttribute>(),
                 "Type of attribute: data_layout is not pir::StrAttribute.");

  IR_ENFORCE(attributes.count("act_type")>0,
                 "act_type does not exist.");
  IR_ENFORCE(attributes.at("act_type").isa<pir::Int32Attribute>(),
                 "Type of attribute: act_type is not pir::Int32Attribute.");

  }
  VLOG(4) << "Verifying outputs:";
  {
  auto output_size = num_results();
  IR_ENFORCE(output_size == 1u,
                    "The size %d of outputs must be equal to 1.", output_size);
  IR_ENFORCE((*this)->result(0).type().isa<paddle::dialect::DenseTensorType>(),
                 "Type validation failed for the 0th output.");
  }
  VLOG(4) << "End Verifying for: BnActXpuOp.";
}

void BnActXpuOp::InferMeta( phi::InferMetaContext *infer_meta ) {
  auto fn = PD_INFER_META(phi::BNActXPUInferMeta);
  fn(infer_meta);
}

phi::DataType BnActXpuOp::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: BnActXpuOp";
  


  return expected_kernel_dtype;
}

const char *Conv1dXpuOp::attributes_name[7] = { "paddings", "padding_algorithm", "dilations", "strides", "groups", "act_type", "act_param" };

OpInfoTuple Conv1dXpuOp::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("x", "paddle::dialect::DenseTensorType", false, false, false, false), paddle::dialect::OpInputInfo("x_max", "paddle::dialect::DenseTensorType", true, false, false, false), paddle::dialect::OpInputInfo("filter", "paddle::dialect::DenseTensorType", false, false, false, false), paddle::dialect::OpInputInfo("filter_max", "paddle::dialect::DenseTensorType", false, false, false, false), paddle::dialect::OpInputInfo("bias", "paddle::dialect::DenseTensorType", true, false, false, false), paddle::dialect::OpInputInfo("branch", "paddle::dialect::DenseTensorType", true, false, false, false), paddle::dialect::OpInputInfo("branch_max", "paddle::dialect::DenseTensorType", true, false, false, false) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = { paddle::dialect::OpAttributeInfo("paddings", "pir::ArrayAttribute<pir::Int32Attribute>", ""), paddle::dialect::OpAttributeInfo("padding_algorithm", "pir::StrAttribute", ""), paddle::dialect::OpAttributeInfo("dilations", "pir::Int32Attribute", ""), paddle::dialect::OpAttributeInfo("strides", "pir::Int32Attribute", ""), paddle::dialect::OpAttributeInfo("groups", "pir::Int32Attribute", ""), paddle::dialect::OpAttributeInfo("act_type", "pir::Int32Attribute", ""), paddle::dialect::OpAttributeInfo("act_param", "pir::FloatAttribute", "") };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("out", "paddle::dialect::DenseTensorType", false, false), paddle::dialect::OpOutputInfo("out_max", "paddle::dialect::DenseTensorType", false, false) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("Conv1dXPUInferMeta", {"x", "x_max", "filter", "filter_max", "bias", "branch", "branch_max", "paddings", "padding_algorithm", "dilations", "strides", "groups", "act_type", "act_param"}, "conv1d_xpu", {"x", "x_max", "filter", "filter_max", "bias", "branch", "branch_max", "paddings", "padding_algorithm", "dilations", "strides", "groups", "act_type", "act_param"}, {"x"}, {}, {}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "conv1d_xpu");
}

void Conv1dXpuOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, pir::Value x_max_, pir::Value filter_, pir::Value filter_max_, pir::Value bias_, pir::Value branch_, pir::Value branch_max_, const std::vector<int>& paddings, const std::string& padding_algorithm, int dilations, int strides, int groups, int act_type, float act_param) {
  VLOG(4) << "Start build Conv1dXpuOp";



  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_, x_max_, filter_, filter_max_, bias_, branch_, branch_max_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  std::vector<pir::Attribute> vec_paddings;
  for (size_t i = 0; i < static_cast<size_t>(paddings.size()); i++) {
      pir::Attribute attr_paddings = pir::Int32Attribute::get(pir::IrContext::Instance(), paddings[i]);

    vec_paddings.push_back(attr_paddings);
  }
  pir::Attribute attr_paddings = pir::ArrayAttribute::get(pir::IrContext::Instance(), vec_paddings);
  argument.AddAttribute("paddings", attr_paddings);
  pir::Attribute attr_padding_algorithm = pir::StrAttribute::get(pir::IrContext::Instance(), padding_algorithm);
  argument.AddAttribute("padding_algorithm", attr_padding_algorithm);
  pir::Attribute attr_dilations = pir::Int32Attribute::get(pir::IrContext::Instance(), dilations);
  argument.AddAttribute("dilations", attr_dilations);
  pir::Attribute attr_strides = pir::Int32Attribute::get(pir::IrContext::Instance(), strides);
  argument.AddAttribute("strides", attr_strides);
  pir::Attribute attr_groups = pir::Int32Attribute::get(pir::IrContext::Instance(), groups);
  argument.AddAttribute("groups", attr_groups);
  pir::Attribute attr_act_type = pir::Int32Attribute::get(pir::IrContext::Instance(), act_type);
  argument.AddAttribute("act_type", attr_act_type);
  pir::Attribute attr_act_param = pir::FloatAttribute::get(pir::IrContext::Instance(), act_param);
  argument.AddAttribute("act_param", attr_act_param);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;
  paddle::dialect::DenseTensorType filter = filter_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)filter;
  paddle::dialect::DenseTensorType filter_max = filter_max_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)filter_max;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);

  paddle::dialect::IrMetaTensor meta_x_max;
  paddle::dialect::IrTensor ir_tensor_x_max;
  if (x_max_.impl() != nullptr) {
    paddle::dialect::DenseTensorType x_max = x_max_.type().dyn_cast<paddle::dialect::DenseTensorType>();
    VLOG(4) << "Builder construction  dense_x_max";
    ir_tensor_x_max = paddle::dialect::IrTensor(paddle::dialect::TransToPhiDataType(x_max.dtype()),
                                                        x_max.dims(),
                                                        x_max.data_layout(),
                                                        x_max.lod(),
                                                        x_max.offset());
    VLOG(4) << "Builder construction  meta_x_max";
    meta_x_max = paddle::dialect::IrMetaTensor(&ir_tensor_x_max);
  }


  VLOG(4) << "Builder construction  dense_filter";
  paddle::dialect::IrTensor ir_tensor_filter(paddle::dialect::TransToPhiDataType(filter.dtype()),
                                                      filter.dims(),
                                                      filter.data_layout(),
                                                      filter.lod(),
                                                      filter.offset());
  VLOG(4) << "Builder construction  meta_filter";
  paddle::dialect::IrMetaTensor meta_filter(&ir_tensor_filter);

  VLOG(4) << "Builder construction  dense_filter_max";
  paddle::dialect::IrTensor ir_tensor_filter_max(paddle::dialect::TransToPhiDataType(filter_max.dtype()),
                                                      filter_max.dims(),
                                                      filter_max.data_layout(),
                                                      filter_max.lod(),
                                                      filter_max.offset());
  VLOG(4) << "Builder construction  meta_filter_max";
  paddle::dialect::IrMetaTensor meta_filter_max(&ir_tensor_filter_max);

  paddle::dialect::IrMetaTensor meta_bias;
  paddle::dialect::IrTensor ir_tensor_bias;
  if (bias_.impl() != nullptr) {
    paddle::dialect::DenseTensorType bias = bias_.type().dyn_cast<paddle::dialect::DenseTensorType>();
    VLOG(4) << "Builder construction  dense_bias";
    ir_tensor_bias = paddle::dialect::IrTensor(paddle::dialect::TransToPhiDataType(bias.dtype()),
                                                        bias.dims(),
                                                        bias.data_layout(),
                                                        bias.lod(),
                                                        bias.offset());
    VLOG(4) << "Builder construction  meta_bias";
    meta_bias = paddle::dialect::IrMetaTensor(&ir_tensor_bias);
  }


  paddle::dialect::IrMetaTensor meta_branch;
  paddle::dialect::IrTensor ir_tensor_branch;
  if (branch_.impl() != nullptr) {
    paddle::dialect::DenseTensorType branch = branch_.type().dyn_cast<paddle::dialect::DenseTensorType>();
    VLOG(4) << "Builder construction  dense_branch";
    ir_tensor_branch = paddle::dialect::IrTensor(paddle::dialect::TransToPhiDataType(branch.dtype()),
                                                        branch.dims(),
                                                        branch.data_layout(),
                                                        branch.lod(),
                                                        branch.offset());
    VLOG(4) << "Builder construction  meta_branch";
    meta_branch = paddle::dialect::IrMetaTensor(&ir_tensor_branch);
  }


  paddle::dialect::IrMetaTensor meta_branch_max;
  paddle::dialect::IrTensor ir_tensor_branch_max;
  if (branch_max_.impl() != nullptr) {
    paddle::dialect::DenseTensorType branch_max = branch_max_.type().dyn_cast<paddle::dialect::DenseTensorType>();
    VLOG(4) << "Builder construction  dense_branch_max";
    ir_tensor_branch_max = paddle::dialect::IrTensor(paddle::dialect::TransToPhiDataType(branch_max.dtype()),
                                                        branch_max.dims(),
                                                        branch_max.data_layout(),
                                                        branch_max.lod(),
                                                        branch_max.offset());
    VLOG(4) << "Builder construction  meta_branch_max";
    meta_branch_max = paddle::dialect::IrMetaTensor(&ir_tensor_branch_max);
  }

  paddle::dialect::IrTensor dense_out;
  paddle::dialect::IrMetaTensor meta_out(&dense_out);
  paddle::dialect::IrTensor dense_out_max;
  paddle::dialect::IrMetaTensor meta_out_max(&dense_out_max);

  phi::Conv1dXPUInferMeta(meta_x, meta_x_max, meta_filter, meta_filter_max, meta_bias, meta_branch, meta_branch_max, paddings, padding_algorithm, dilations, strides, groups, act_type, act_param, &meta_out, &meta_out_max);

  std::vector<pir::Type> argument_outputs;
  pir::Type out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_out.dtype()), dense_out.dims(), dense_out.layout(), dense_out.lod(), dense_out.offset());
  argument_outputs.push_back(out_dense_tensor_type);

  pir::Type out_max_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_out_max.dtype()), dense_out_max.dims(), dense_out_max.layout(), dense_out_max.lod(), dense_out_max.offset());
  argument_outputs.push_back(out_max_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void Conv1dXpuOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, pir::Value x_max_, pir::Value filter_, pir::Value filter_max_, pir::Value bias_, pir::Value branch_, pir::Value branch_max_, pir::AttributeMap attributes) {
  VLOG(4) << "Start build Conv1dXpuOp";


  IR_ENFORCE(
      attributes.find("paddings") != attributes.end(),
          "'paddings' Attribute is expected for Conv1dXpuOp. ");
  std::vector<int> paddings;
  for (size_t i = 0; i < attributes.at("paddings").dyn_cast<pir::ArrayAttribute>().size(); i++) {
    paddings.push_back(attributes.at("paddings").dyn_cast<pir::ArrayAttribute>().at(i).dyn_cast<pir::Int32Attribute>().data());
  }

  IR_ENFORCE(
      attributes.find("padding_algorithm") != attributes.end(),
          "'padding_algorithm' Attribute is expected for Conv1dXpuOp. ");
  std::string padding_algorithm = attributes.at("padding_algorithm").dyn_cast<pir::StrAttribute>().AsString();

  IR_ENFORCE(
      attributes.find("dilations") != attributes.end(),
          "'dilations' Attribute is expected for Conv1dXpuOp. ");
  int dilations = attributes.at("dilations").dyn_cast<pir::Int32Attribute>().data();

  IR_ENFORCE(
      attributes.find("strides") != attributes.end(),
          "'strides' Attribute is expected for Conv1dXpuOp. ");
  int strides = attributes.at("strides").dyn_cast<pir::Int32Attribute>().data();

  IR_ENFORCE(
      attributes.find("groups") != attributes.end(),
          "'groups' Attribute is expected for Conv1dXpuOp. ");
  int groups = attributes.at("groups").dyn_cast<pir::Int32Attribute>().data();

  IR_ENFORCE(
      attributes.find("act_type") != attributes.end(),
          "'act_type' Attribute is expected for Conv1dXpuOp. ");
  int act_type = attributes.at("act_type").dyn_cast<pir::Int32Attribute>().data();

  IR_ENFORCE(
      attributes.find("act_param") != attributes.end(),
          "'act_param' Attribute is expected for Conv1dXpuOp. ");
  float act_param = attributes.at("act_param").dyn_cast<pir::FloatAttribute>().data();


  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_, x_max_, filter_, filter_max_, bias_, branch_, branch_max_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  std::vector<pir::Attribute> vec_paddings;
  for (size_t i = 0; i < static_cast<size_t>(paddings.size()); i++) {
      pir::Attribute attr_paddings = pir::Int32Attribute::get(pir::IrContext::Instance(), paddings[i]);

    vec_paddings.push_back(attr_paddings);
  }
  pir::Attribute attr_paddings = pir::ArrayAttribute::get(pir::IrContext::Instance(), vec_paddings);
  argument.AddAttribute("paddings", attr_paddings);
  pir::Attribute attr_padding_algorithm = pir::StrAttribute::get(pir::IrContext::Instance(), padding_algorithm);
  argument.AddAttribute("padding_algorithm", attr_padding_algorithm);
  pir::Attribute attr_dilations = pir::Int32Attribute::get(pir::IrContext::Instance(), dilations);
  argument.AddAttribute("dilations", attr_dilations);
  pir::Attribute attr_strides = pir::Int32Attribute::get(pir::IrContext::Instance(), strides);
  argument.AddAttribute("strides", attr_strides);
  pir::Attribute attr_groups = pir::Int32Attribute::get(pir::IrContext::Instance(), groups);
  argument.AddAttribute("groups", attr_groups);
  pir::Attribute attr_act_type = pir::Int32Attribute::get(pir::IrContext::Instance(), act_type);
  argument.AddAttribute("act_type", attr_act_type);
  pir::Attribute attr_act_param = pir::FloatAttribute::get(pir::IrContext::Instance(), act_param);
  argument.AddAttribute("act_param", attr_act_param);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;
  paddle::dialect::DenseTensorType filter = filter_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)filter;
  paddle::dialect::DenseTensorType filter_max = filter_max_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)filter_max;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);

  paddle::dialect::IrMetaTensor meta_x_max;
  paddle::dialect::IrTensor ir_tensor_x_max;
  if (x_max_.impl() != nullptr) {
    paddle::dialect::DenseTensorType x_max = x_max_.type().dyn_cast<paddle::dialect::DenseTensorType>();
    VLOG(4) << "Builder construction  dense_x_max";
    ir_tensor_x_max = paddle::dialect::IrTensor(paddle::dialect::TransToPhiDataType(x_max.dtype()),
                                                        x_max.dims(),
                                                        x_max.data_layout(),
                                                        x_max.lod(),
                                                        x_max.offset());
    VLOG(4) << "Builder construction  meta_x_max";
    meta_x_max = paddle::dialect::IrMetaTensor(&ir_tensor_x_max);
  }


  VLOG(4) << "Builder construction  dense_filter";
  paddle::dialect::IrTensor ir_tensor_filter(paddle::dialect::TransToPhiDataType(filter.dtype()),
                                                      filter.dims(),
                                                      filter.data_layout(),
                                                      filter.lod(),
                                                      filter.offset());
  VLOG(4) << "Builder construction  meta_filter";
  paddle::dialect::IrMetaTensor meta_filter(&ir_tensor_filter);

  VLOG(4) << "Builder construction  dense_filter_max";
  paddle::dialect::IrTensor ir_tensor_filter_max(paddle::dialect::TransToPhiDataType(filter_max.dtype()),
                                                      filter_max.dims(),
                                                      filter_max.data_layout(),
                                                      filter_max.lod(),
                                                      filter_max.offset());
  VLOG(4) << "Builder construction  meta_filter_max";
  paddle::dialect::IrMetaTensor meta_filter_max(&ir_tensor_filter_max);

  paddle::dialect::IrMetaTensor meta_bias;
  paddle::dialect::IrTensor ir_tensor_bias;
  if (bias_.impl() != nullptr) {
    paddle::dialect::DenseTensorType bias = bias_.type().dyn_cast<paddle::dialect::DenseTensorType>();
    VLOG(4) << "Builder construction  dense_bias";
    ir_tensor_bias = paddle::dialect::IrTensor(paddle::dialect::TransToPhiDataType(bias.dtype()),
                                                        bias.dims(),
                                                        bias.data_layout(),
                                                        bias.lod(),
                                                        bias.offset());
    VLOG(4) << "Builder construction  meta_bias";
    meta_bias = paddle::dialect::IrMetaTensor(&ir_tensor_bias);
  }


  paddle::dialect::IrMetaTensor meta_branch;
  paddle::dialect::IrTensor ir_tensor_branch;
  if (branch_.impl() != nullptr) {
    paddle::dialect::DenseTensorType branch = branch_.type().dyn_cast<paddle::dialect::DenseTensorType>();
    VLOG(4) << "Builder construction  dense_branch";
    ir_tensor_branch = paddle::dialect::IrTensor(paddle::dialect::TransToPhiDataType(branch.dtype()),
                                                        branch.dims(),
                                                        branch.data_layout(),
                                                        branch.lod(),
                                                        branch.offset());
    VLOG(4) << "Builder construction  meta_branch";
    meta_branch = paddle::dialect::IrMetaTensor(&ir_tensor_branch);
  }


  paddle::dialect::IrMetaTensor meta_branch_max;
  paddle::dialect::IrTensor ir_tensor_branch_max;
  if (branch_max_.impl() != nullptr) {
    paddle::dialect::DenseTensorType branch_max = branch_max_.type().dyn_cast<paddle::dialect::DenseTensorType>();
    VLOG(4) << "Builder construction  dense_branch_max";
    ir_tensor_branch_max = paddle::dialect::IrTensor(paddle::dialect::TransToPhiDataType(branch_max.dtype()),
                                                        branch_max.dims(),
                                                        branch_max.data_layout(),
                                                        branch_max.lod(),
                                                        branch_max.offset());
    VLOG(4) << "Builder construction  meta_branch_max";
    meta_branch_max = paddle::dialect::IrMetaTensor(&ir_tensor_branch_max);
  }

  paddle::dialect::IrTensor dense_out;
  paddle::dialect::IrMetaTensor meta_out(&dense_out);
  paddle::dialect::IrTensor dense_out_max;
  paddle::dialect::IrMetaTensor meta_out_max(&dense_out_max);

  phi::Conv1dXPUInferMeta(meta_x, meta_x_max, meta_filter, meta_filter_max, meta_bias, meta_branch, meta_branch_max, paddings, padding_algorithm, dilations, strides, groups, act_type, act_param, &meta_out, &meta_out_max);

  std::vector<pir::Type> argument_outputs;
  pir::Type out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_out.dtype()), dense_out.dims(), dense_out.layout(), dense_out.lod(), dense_out.offset());
  argument_outputs.push_back(out_dense_tensor_type);

  pir::Type out_max_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_out_max.dtype()), dense_out_max.dims(), dense_out_max.layout(), dense_out_max.lod(), dense_out_max.offset());
  argument_outputs.push_back(out_max_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void Conv1dXpuOp::VerifySig() {
  VLOG(4) << "Start Verifying inputs, outputs and attributes for: Conv1dXpuOp.";
  VLOG(4) << "Verifying inputs:";
  {
  auto input_size = num_operands();
  IR_ENFORCE(input_size == 7u,
                    "The size %d of inputs must be equal to 7.", input_size);
  IR_ENFORCE((*this)->operand_source(0).type().isa<paddle::dialect::DenseTensorType>(),
                  "Type validation failed for the 0th input, got %s.", (*this)->operand_source(0).type());
  if (auto val = (*this)->operand(1)) {
    IR_ENFORCE(val.type().isa<paddle::dialect::DenseTensorType>(),
                   "Type validation failed for the 1th input, got %s.", (*this)->operand_source(1).type());
  }
  IR_ENFORCE((*this)->operand_source(2).type().isa<paddle::dialect::DenseTensorType>(),
                  "Type validation failed for the 2th input, got %s.", (*this)->operand_source(2).type());
  IR_ENFORCE((*this)->operand_source(3).type().isa<paddle::dialect::DenseTensorType>(),
                  "Type validation failed for the 3th input, got %s.", (*this)->operand_source(3).type());
  if (auto val = (*this)->operand(4)) {
    IR_ENFORCE(val.type().isa<paddle::dialect::DenseTensorType>(),
                   "Type validation failed for the 4th input, got %s.", (*this)->operand_source(4).type());
  }
  if (auto val = (*this)->operand(5)) {
    IR_ENFORCE(val.type().isa<paddle::dialect::DenseTensorType>(),
                   "Type validation failed for the 5th input, got %s.", (*this)->operand_source(5).type());
  }
  if (auto val = (*this)->operand(6)) {
    IR_ENFORCE(val.type().isa<paddle::dialect::DenseTensorType>(),
                   "Type validation failed for the 6th input, got %s.", (*this)->operand_source(6).type());
  }
  }
  VLOG(4) << "Verifying attributes:";
  {
  auto& attributes = this->attributes();
  IR_ENFORCE(attributes.count("paddings")>0,
                 "paddings does not exist.");
  IR_ENFORCE(attributes.at("paddings").isa<pir::ArrayAttribute>(),
                 "Type of attribute: paddings is not pir::ArrayAttribute.");
  for (size_t i = 0; i < attributes.at("paddings").dyn_cast<pir::ArrayAttribute>().size(); i++) {
    IR_ENFORCE(attributes.at("paddings").dyn_cast<pir::ArrayAttribute>().at(i).isa<pir::Int32Attribute>(),
                   "Type of attribute: paddings is not right.");
  }
  IR_ENFORCE(attributes.count("padding_algorithm")>0,
                 "padding_algorithm does not exist.");
  IR_ENFORCE(attributes.at("padding_algorithm").isa<pir::StrAttribute>(),
                 "Type of attribute: padding_algorithm is not pir::StrAttribute.");

  IR_ENFORCE(attributes.count("dilations")>0,
                 "dilations does not exist.");
  IR_ENFORCE(attributes.at("dilations").isa<pir::Int32Attribute>(),
                 "Type of attribute: dilations is not pir::Int32Attribute.");

  IR_ENFORCE(attributes.count("strides")>0,
                 "strides does not exist.");
  IR_ENFORCE(attributes.at("strides").isa<pir::Int32Attribute>(),
                 "Type of attribute: strides is not pir::Int32Attribute.");

  IR_ENFORCE(attributes.count("groups")>0,
                 "groups does not exist.");
  IR_ENFORCE(attributes.at("groups").isa<pir::Int32Attribute>(),
                 "Type of attribute: groups is not pir::Int32Attribute.");

  IR_ENFORCE(attributes.count("act_type")>0,
                 "act_type does not exist.");
  IR_ENFORCE(attributes.at("act_type").isa<pir::Int32Attribute>(),
                 "Type of attribute: act_type is not pir::Int32Attribute.");

  IR_ENFORCE(attributes.count("act_param")>0,
                 "act_param does not exist.");
  IR_ENFORCE(attributes.at("act_param").isa<pir::FloatAttribute>(),
                 "Type of attribute: act_param is not pir::FloatAttribute.");

  }
  VLOG(4) << "Verifying outputs:";
  {
  auto output_size = num_results();
  IR_ENFORCE(output_size == 2u,
                    "The size %d of outputs must be equal to 2.", output_size);
  IR_ENFORCE((*this)->result(0).type().isa<paddle::dialect::DenseTensorType>(),
                 "Type validation failed for the 0th output.");
  IR_ENFORCE((*this)->result(1).type().isa<paddle::dialect::DenseTensorType>(),
                 "Type validation failed for the 1th output.");
  }
  VLOG(4) << "End Verifying for: Conv1dXpuOp.";
}

void Conv1dXpuOp::InferMeta( phi::InferMetaContext *infer_meta ) {
  auto fn = PD_INFER_META(phi::Conv1dXPUInferMeta);
  fn(infer_meta);
}

phi::DataType Conv1dXpuOp::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: Conv1dXpuOp";
  


  return expected_kernel_dtype;
}

const char *Conv2dTransposeXpuOp::attributes_name[11] = { "strides", "paddings", "output_padding", "output_size", "padding_algorithm", "groups", "dilations", "data_format", "has_bias", "with_act", "act_type" };

OpInfoTuple Conv2dTransposeXpuOp::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("x", "paddle::dialect::DenseTensorType", false, false, false, false), paddle::dialect::OpInputInfo("x_max", "paddle::dialect::DenseTensorType", true, false, false, false), paddle::dialect::OpInputInfo("filter", "paddle::dialect::DenseTensorType", false, false, false, false), paddle::dialect::OpInputInfo("filter_max", "paddle::dialect::DenseTensorType", false, false, false, false), paddle::dialect::OpInputInfo("bias", "paddle::dialect::DenseTensorType", true, false, false, false) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = { paddle::dialect::OpAttributeInfo("strides", "pir::ArrayAttribute<pir::Int32Attribute>", ""), paddle::dialect::OpAttributeInfo("paddings", "pir::ArrayAttribute<pir::Int32Attribute>", ""), paddle::dialect::OpAttributeInfo("output_padding", "pir::ArrayAttribute<pir::Int32Attribute>", ""), paddle::dialect::OpAttributeInfo("output_size", "paddle::dialect::IntArrayAttribute", "std::vector<int64_t>"), paddle::dialect::OpAttributeInfo("padding_algorithm", "pir::StrAttribute", ""), paddle::dialect::OpAttributeInfo("groups", "pir::Int32Attribute", ""), paddle::dialect::OpAttributeInfo("dilations", "pir::ArrayAttribute<pir::Int32Attribute>", ""), paddle::dialect::OpAttributeInfo("data_format", "pir::StrAttribute", ""), paddle::dialect::OpAttributeInfo("has_bias", "pir::BoolAttribute", ""), paddle::dialect::OpAttributeInfo("with_act", "pir::BoolAttribute", ""), paddle::dialect::OpAttributeInfo("act_type", "pir::StrAttribute", "") };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("out", "paddle::dialect::DenseTensorType", false, false), paddle::dialect::OpOutputInfo("out_max", "paddle::dialect::DenseTensorType", false, false) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("Conv2dTransposeXPUInferMeta", {"x", "x_max", "filter", "filter_max", "bias", "strides", "paddings", "output_padding", "output_size", "padding_algorithm", "groups", "dilations", "data_format", "has_bias", "with_act", "act_type"}, "conv2d_transpose_xpu", {"x", "x_max", "filter", "filter_max", "bias", "strides", "paddings", "output_padding", "output_size", "padding_algorithm", "groups", "dilations", "data_format", "has_bias", "with_act", "act_type"}, {"x"}, {}, {}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "conv2d_transpose_xpu");
}

void Conv2dTransposeXpuOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, pir::Value x_max_, pir::Value filter_, pir::Value filter_max_, pir::Value bias_, const std::vector<int>& strides, const std::vector<int>& paddings, const std::vector<int>& output_padding, const std::vector<int64_t>& output_size, const std::string& padding_algorithm, int groups, const std::vector<int>& dilations, const std::string& data_format, bool has_bias, bool with_act, const std::string& act_type) {
  VLOG(4) << "Start build Conv2dTransposeXpuOp";



  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_, x_max_, filter_, filter_max_, bias_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  std::vector<pir::Attribute> vec_strides;
  for (size_t i = 0; i < static_cast<size_t>(strides.size()); i++) {
      pir::Attribute attr_strides = pir::Int32Attribute::get(pir::IrContext::Instance(), strides[i]);

    vec_strides.push_back(attr_strides);
  }
  pir::Attribute attr_strides = pir::ArrayAttribute::get(pir::IrContext::Instance(), vec_strides);
  argument.AddAttribute("strides", attr_strides);
  std::vector<pir::Attribute> vec_paddings;
  for (size_t i = 0; i < static_cast<size_t>(paddings.size()); i++) {
      pir::Attribute attr_paddings = pir::Int32Attribute::get(pir::IrContext::Instance(), paddings[i]);

    vec_paddings.push_back(attr_paddings);
  }
  pir::Attribute attr_paddings = pir::ArrayAttribute::get(pir::IrContext::Instance(), vec_paddings);
  argument.AddAttribute("paddings", attr_paddings);
  std::vector<pir::Attribute> vec_output_padding;
  for (size_t i = 0; i < static_cast<size_t>(output_padding.size()); i++) {
      pir::Attribute attr_output_padding = pir::Int32Attribute::get(pir::IrContext::Instance(), output_padding[i]);

    vec_output_padding.push_back(attr_output_padding);
  }
  pir::Attribute attr_output_padding = pir::ArrayAttribute::get(pir::IrContext::Instance(), vec_output_padding);
  argument.AddAttribute("output_padding", attr_output_padding);
  pir::Attribute attr_output_size = paddle::dialect::IntArrayAttribute::get(pir::IrContext::Instance(), phi::IntArray(output_size));
  argument.AddAttribute("output_size", attr_output_size);
  pir::Attribute attr_padding_algorithm = pir::StrAttribute::get(pir::IrContext::Instance(), padding_algorithm);
  argument.AddAttribute("padding_algorithm", attr_padding_algorithm);
  pir::Attribute attr_groups = pir::Int32Attribute::get(pir::IrContext::Instance(), groups);
  argument.AddAttribute("groups", attr_groups);
  std::vector<pir::Attribute> vec_dilations;
  for (size_t i = 0; i < static_cast<size_t>(dilations.size()); i++) {
      pir::Attribute attr_dilations = pir::Int32Attribute::get(pir::IrContext::Instance(), dilations[i]);

    vec_dilations.push_back(attr_dilations);
  }
  pir::Attribute attr_dilations = pir::ArrayAttribute::get(pir::IrContext::Instance(), vec_dilations);
  argument.AddAttribute("dilations", attr_dilations);
  pir::Attribute attr_data_format = pir::StrAttribute::get(pir::IrContext::Instance(), data_format);
  argument.AddAttribute("data_format", attr_data_format);
  pir::Attribute attr_has_bias = pir::BoolAttribute::get(pir::IrContext::Instance(), has_bias);
  argument.AddAttribute("has_bias", attr_has_bias);
  pir::Attribute attr_with_act = pir::BoolAttribute::get(pir::IrContext::Instance(), with_act);
  argument.AddAttribute("with_act", attr_with_act);
  pir::Attribute attr_act_type = pir::StrAttribute::get(pir::IrContext::Instance(), act_type);
  argument.AddAttribute("act_type", attr_act_type);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;
  paddle::dialect::DenseTensorType filter = filter_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)filter;
  paddle::dialect::DenseTensorType filter_max = filter_max_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)filter_max;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);

  paddle::dialect::IrMetaTensor meta_x_max;
  paddle::dialect::IrTensor ir_tensor_x_max;
  if (x_max_.impl() != nullptr) {
    paddle::dialect::DenseTensorType x_max = x_max_.type().dyn_cast<paddle::dialect::DenseTensorType>();
    VLOG(4) << "Builder construction  dense_x_max";
    ir_tensor_x_max = paddle::dialect::IrTensor(paddle::dialect::TransToPhiDataType(x_max.dtype()),
                                                        x_max.dims(),
                                                        x_max.data_layout(),
                                                        x_max.lod(),
                                                        x_max.offset());
    VLOG(4) << "Builder construction  meta_x_max";
    meta_x_max = paddle::dialect::IrMetaTensor(&ir_tensor_x_max);
  }


  VLOG(4) << "Builder construction  dense_filter";
  paddle::dialect::IrTensor ir_tensor_filter(paddle::dialect::TransToPhiDataType(filter.dtype()),
                                                      filter.dims(),
                                                      filter.data_layout(),
                                                      filter.lod(),
                                                      filter.offset());
  VLOG(4) << "Builder construction  meta_filter";
  paddle::dialect::IrMetaTensor meta_filter(&ir_tensor_filter);

  VLOG(4) << "Builder construction  dense_filter_max";
  paddle::dialect::IrTensor ir_tensor_filter_max(paddle::dialect::TransToPhiDataType(filter_max.dtype()),
                                                      filter_max.dims(),
                                                      filter_max.data_layout(),
                                                      filter_max.lod(),
                                                      filter_max.offset());
  VLOG(4) << "Builder construction  meta_filter_max";
  paddle::dialect::IrMetaTensor meta_filter_max(&ir_tensor_filter_max);

  paddle::dialect::IrMetaTensor meta_bias;
  paddle::dialect::IrTensor ir_tensor_bias;
  if (bias_.impl() != nullptr) {
    paddle::dialect::DenseTensorType bias = bias_.type().dyn_cast<paddle::dialect::DenseTensorType>();
    VLOG(4) << "Builder construction  dense_bias";
    ir_tensor_bias = paddle::dialect::IrTensor(paddle::dialect::TransToPhiDataType(bias.dtype()),
                                                        bias.dims(),
                                                        bias.data_layout(),
                                                        bias.lod(),
                                                        bias.offset());
    VLOG(4) << "Builder construction  meta_bias";
    meta_bias = paddle::dialect::IrMetaTensor(&ir_tensor_bias);
  }

  paddle::dialect::IrTensor dense_out;
  paddle::dialect::IrMetaTensor meta_out(&dense_out);
  paddle::dialect::IrTensor dense_out_max;
  paddle::dialect::IrMetaTensor meta_out_max(&dense_out_max);

  phi::Conv2dTransposeXPUInferMeta(meta_x, meta_x_max, meta_filter, meta_filter_max, meta_bias, strides, paddings, output_padding, output_size, padding_algorithm, groups, dilations, data_format, has_bias, with_act, act_type, &meta_out, &meta_out_max);

  std::vector<pir::Type> argument_outputs;
  pir::Type out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_out.dtype()), dense_out.dims(), dense_out.layout(), dense_out.lod(), dense_out.offset());
  argument_outputs.push_back(out_dense_tensor_type);

  pir::Type out_max_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_out_max.dtype()), dense_out_max.dims(), dense_out_max.layout(), dense_out_max.lod(), dense_out_max.offset());
  argument_outputs.push_back(out_max_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void Conv2dTransposeXpuOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, pir::Value x_max_, pir::Value filter_, pir::Value filter_max_, pir::Value bias_, pir::AttributeMap attributes) {
  VLOG(4) << "Start build Conv2dTransposeXpuOp";


  IR_ENFORCE(
      attributes.find("strides") != attributes.end(),
          "'strides' Attribute is expected for Conv2dTransposeXpuOp. ");
  std::vector<int> strides;
  for (size_t i = 0; i < attributes.at("strides").dyn_cast<pir::ArrayAttribute>().size(); i++) {
    strides.push_back(attributes.at("strides").dyn_cast<pir::ArrayAttribute>().at(i).dyn_cast<pir::Int32Attribute>().data());
  }

  IR_ENFORCE(
      attributes.find("paddings") != attributes.end(),
          "'paddings' Attribute is expected for Conv2dTransposeXpuOp. ");
  std::vector<int> paddings;
  for (size_t i = 0; i < attributes.at("paddings").dyn_cast<pir::ArrayAttribute>().size(); i++) {
    paddings.push_back(attributes.at("paddings").dyn_cast<pir::ArrayAttribute>().at(i).dyn_cast<pir::Int32Attribute>().data());
  }

  IR_ENFORCE(
      attributes.find("output_padding") != attributes.end(),
          "'output_padding' Attribute is expected for Conv2dTransposeXpuOp. ");
  std::vector<int> output_padding;
  for (size_t i = 0; i < attributes.at("output_padding").dyn_cast<pir::ArrayAttribute>().size(); i++) {
    output_padding.push_back(attributes.at("output_padding").dyn_cast<pir::ArrayAttribute>().at(i).dyn_cast<pir::Int32Attribute>().data());
  }

  IR_ENFORCE(
      attributes.find("output_size") != attributes.end(),
          "'output_size' Attribute is expected for Conv2dTransposeXpuOp. ");
  std::vector<int64_t> output_size = attributes.at("output_size").dyn_cast<paddle::dialect::IntArrayAttribute>().data().GetData();

  IR_ENFORCE(
      attributes.find("padding_algorithm") != attributes.end(),
          "'padding_algorithm' Attribute is expected for Conv2dTransposeXpuOp. ");
  std::string padding_algorithm = attributes.at("padding_algorithm").dyn_cast<pir::StrAttribute>().AsString();

  IR_ENFORCE(
      attributes.find("groups") != attributes.end(),
          "'groups' Attribute is expected for Conv2dTransposeXpuOp. ");
  int groups = attributes.at("groups").dyn_cast<pir::Int32Attribute>().data();

  IR_ENFORCE(
      attributes.find("dilations") != attributes.end(),
          "'dilations' Attribute is expected for Conv2dTransposeXpuOp. ");
  std::vector<int> dilations;
  for (size_t i = 0; i < attributes.at("dilations").dyn_cast<pir::ArrayAttribute>().size(); i++) {
    dilations.push_back(attributes.at("dilations").dyn_cast<pir::ArrayAttribute>().at(i).dyn_cast<pir::Int32Attribute>().data());
  }

  IR_ENFORCE(
      attributes.find("data_format") != attributes.end(),
          "'data_format' Attribute is expected for Conv2dTransposeXpuOp. ");
  std::string data_format = attributes.at("data_format").dyn_cast<pir::StrAttribute>().AsString();

  IR_ENFORCE(
      attributes.find("has_bias") != attributes.end(),
          "'has_bias' Attribute is expected for Conv2dTransposeXpuOp. ");
  bool has_bias = attributes.at("has_bias").dyn_cast<pir::BoolAttribute>().data();

  IR_ENFORCE(
      attributes.find("with_act") != attributes.end(),
          "'with_act' Attribute is expected for Conv2dTransposeXpuOp. ");
  bool with_act = attributes.at("with_act").dyn_cast<pir::BoolAttribute>().data();

  IR_ENFORCE(
      attributes.find("act_type") != attributes.end(),
          "'act_type' Attribute is expected for Conv2dTransposeXpuOp. ");
  std::string act_type = attributes.at("act_type").dyn_cast<pir::StrAttribute>().AsString();


  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_, x_max_, filter_, filter_max_, bias_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  std::vector<pir::Attribute> vec_strides;
  for (size_t i = 0; i < static_cast<size_t>(strides.size()); i++) {
      pir::Attribute attr_strides = pir::Int32Attribute::get(pir::IrContext::Instance(), strides[i]);

    vec_strides.push_back(attr_strides);
  }
  pir::Attribute attr_strides = pir::ArrayAttribute::get(pir::IrContext::Instance(), vec_strides);
  argument.AddAttribute("strides", attr_strides);
  std::vector<pir::Attribute> vec_paddings;
  for (size_t i = 0; i < static_cast<size_t>(paddings.size()); i++) {
      pir::Attribute attr_paddings = pir::Int32Attribute::get(pir::IrContext::Instance(), paddings[i]);

    vec_paddings.push_back(attr_paddings);
  }
  pir::Attribute attr_paddings = pir::ArrayAttribute::get(pir::IrContext::Instance(), vec_paddings);
  argument.AddAttribute("paddings", attr_paddings);
  std::vector<pir::Attribute> vec_output_padding;
  for (size_t i = 0; i < static_cast<size_t>(output_padding.size()); i++) {
      pir::Attribute attr_output_padding = pir::Int32Attribute::get(pir::IrContext::Instance(), output_padding[i]);

    vec_output_padding.push_back(attr_output_padding);
  }
  pir::Attribute attr_output_padding = pir::ArrayAttribute::get(pir::IrContext::Instance(), vec_output_padding);
  argument.AddAttribute("output_padding", attr_output_padding);
  pir::Attribute attr_output_size = paddle::dialect::IntArrayAttribute::get(pir::IrContext::Instance(), phi::IntArray(output_size));
  argument.AddAttribute("output_size", attr_output_size);
  pir::Attribute attr_padding_algorithm = pir::StrAttribute::get(pir::IrContext::Instance(), padding_algorithm);
  argument.AddAttribute("padding_algorithm", attr_padding_algorithm);
  pir::Attribute attr_groups = pir::Int32Attribute::get(pir::IrContext::Instance(), groups);
  argument.AddAttribute("groups", attr_groups);
  std::vector<pir::Attribute> vec_dilations;
  for (size_t i = 0; i < static_cast<size_t>(dilations.size()); i++) {
      pir::Attribute attr_dilations = pir::Int32Attribute::get(pir::IrContext::Instance(), dilations[i]);

    vec_dilations.push_back(attr_dilations);
  }
  pir::Attribute attr_dilations = pir::ArrayAttribute::get(pir::IrContext::Instance(), vec_dilations);
  argument.AddAttribute("dilations", attr_dilations);
  pir::Attribute attr_data_format = pir::StrAttribute::get(pir::IrContext::Instance(), data_format);
  argument.AddAttribute("data_format", attr_data_format);
  pir::Attribute attr_has_bias = pir::BoolAttribute::get(pir::IrContext::Instance(), has_bias);
  argument.AddAttribute("has_bias", attr_has_bias);
  pir::Attribute attr_with_act = pir::BoolAttribute::get(pir::IrContext::Instance(), with_act);
  argument.AddAttribute("with_act", attr_with_act);
  pir::Attribute attr_act_type = pir::StrAttribute::get(pir::IrContext::Instance(), act_type);
  argument.AddAttribute("act_type", attr_act_type);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;
  paddle::dialect::DenseTensorType filter = filter_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)filter;
  paddle::dialect::DenseTensorType filter_max = filter_max_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)filter_max;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);

  paddle::dialect::IrMetaTensor meta_x_max;
  paddle::dialect::IrTensor ir_tensor_x_max;
  if (x_max_.impl() != nullptr) {
    paddle::dialect::DenseTensorType x_max = x_max_.type().dyn_cast<paddle::dialect::DenseTensorType>();
    VLOG(4) << "Builder construction  dense_x_max";
    ir_tensor_x_max = paddle::dialect::IrTensor(paddle::dialect::TransToPhiDataType(x_max.dtype()),
                                                        x_max.dims(),
                                                        x_max.data_layout(),
                                                        x_max.lod(),
                                                        x_max.offset());
    VLOG(4) << "Builder construction  meta_x_max";
    meta_x_max = paddle::dialect::IrMetaTensor(&ir_tensor_x_max);
  }


  VLOG(4) << "Builder construction  dense_filter";
  paddle::dialect::IrTensor ir_tensor_filter(paddle::dialect::TransToPhiDataType(filter.dtype()),
                                                      filter.dims(),
                                                      filter.data_layout(),
                                                      filter.lod(),
                                                      filter.offset());
  VLOG(4) << "Builder construction  meta_filter";
  paddle::dialect::IrMetaTensor meta_filter(&ir_tensor_filter);

  VLOG(4) << "Builder construction  dense_filter_max";
  paddle::dialect::IrTensor ir_tensor_filter_max(paddle::dialect::TransToPhiDataType(filter_max.dtype()),
                                                      filter_max.dims(),
                                                      filter_max.data_layout(),
                                                      filter_max.lod(),
                                                      filter_max.offset());
  VLOG(4) << "Builder construction  meta_filter_max";
  paddle::dialect::IrMetaTensor meta_filter_max(&ir_tensor_filter_max);

  paddle::dialect::IrMetaTensor meta_bias;
  paddle::dialect::IrTensor ir_tensor_bias;
  if (bias_.impl() != nullptr) {
    paddle::dialect::DenseTensorType bias = bias_.type().dyn_cast<paddle::dialect::DenseTensorType>();
    VLOG(4) << "Builder construction  dense_bias";
    ir_tensor_bias = paddle::dialect::IrTensor(paddle::dialect::TransToPhiDataType(bias.dtype()),
                                                        bias.dims(),
                                                        bias.data_layout(),
                                                        bias.lod(),
                                                        bias.offset());
    VLOG(4) << "Builder construction  meta_bias";
    meta_bias = paddle::dialect::IrMetaTensor(&ir_tensor_bias);
  }

  paddle::dialect::IrTensor dense_out;
  paddle::dialect::IrMetaTensor meta_out(&dense_out);
  paddle::dialect::IrTensor dense_out_max;
  paddle::dialect::IrMetaTensor meta_out_max(&dense_out_max);

  phi::Conv2dTransposeXPUInferMeta(meta_x, meta_x_max, meta_filter, meta_filter_max, meta_bias, strides, paddings, output_padding, output_size, padding_algorithm, groups, dilations, data_format, has_bias, with_act, act_type, &meta_out, &meta_out_max);

  std::vector<pir::Type> argument_outputs;
  pir::Type out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_out.dtype()), dense_out.dims(), dense_out.layout(), dense_out.lod(), dense_out.offset());
  argument_outputs.push_back(out_dense_tensor_type);

  pir::Type out_max_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_out_max.dtype()), dense_out_max.dims(), dense_out_max.layout(), dense_out_max.lod(), dense_out_max.offset());
  argument_outputs.push_back(out_max_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void Conv2dTransposeXpuOp::VerifySig() {
  VLOG(4) << "Start Verifying inputs, outputs and attributes for: Conv2dTransposeXpuOp.";
  VLOG(4) << "Verifying inputs:";
  {
  auto input_size = num_operands();
  IR_ENFORCE(input_size == 5u,
                    "The size %d of inputs must be equal to 5.", input_size);
  IR_ENFORCE((*this)->operand_source(0).type().isa<paddle::dialect::DenseTensorType>(),
                  "Type validation failed for the 0th input, got %s.", (*this)->operand_source(0).type());
  if (auto val = (*this)->operand(1)) {
    IR_ENFORCE(val.type().isa<paddle::dialect::DenseTensorType>(),
                   "Type validation failed for the 1th input, got %s.", (*this)->operand_source(1).type());
  }
  IR_ENFORCE((*this)->operand_source(2).type().isa<paddle::dialect::DenseTensorType>(),
                  "Type validation failed for the 2th input, got %s.", (*this)->operand_source(2).type());
  IR_ENFORCE((*this)->operand_source(3).type().isa<paddle::dialect::DenseTensorType>(),
                  "Type validation failed for the 3th input, got %s.", (*this)->operand_source(3).type());
  if (auto val = (*this)->operand(4)) {
    IR_ENFORCE(val.type().isa<paddle::dialect::DenseTensorType>(),
                   "Type validation failed for the 4th input, got %s.", (*this)->operand_source(4).type());
  }
  }
  VLOG(4) << "Verifying attributes:";
  {
  auto& attributes = this->attributes();
  IR_ENFORCE(attributes.count("strides")>0,
                 "strides does not exist.");
  IR_ENFORCE(attributes.at("strides").isa<pir::ArrayAttribute>(),
                 "Type of attribute: strides is not pir::ArrayAttribute.");
  for (size_t i = 0; i < attributes.at("strides").dyn_cast<pir::ArrayAttribute>().size(); i++) {
    IR_ENFORCE(attributes.at("strides").dyn_cast<pir::ArrayAttribute>().at(i).isa<pir::Int32Attribute>(),
                   "Type of attribute: strides is not right.");
  }
  IR_ENFORCE(attributes.count("paddings")>0,
                 "paddings does not exist.");
  IR_ENFORCE(attributes.at("paddings").isa<pir::ArrayAttribute>(),
                 "Type of attribute: paddings is not pir::ArrayAttribute.");
  for (size_t i = 0; i < attributes.at("paddings").dyn_cast<pir::ArrayAttribute>().size(); i++) {
    IR_ENFORCE(attributes.at("paddings").dyn_cast<pir::ArrayAttribute>().at(i).isa<pir::Int32Attribute>(),
                   "Type of attribute: paddings is not right.");
  }
  IR_ENFORCE(attributes.count("output_padding")>0,
                 "output_padding does not exist.");
  IR_ENFORCE(attributes.at("output_padding").isa<pir::ArrayAttribute>(),
                 "Type of attribute: output_padding is not pir::ArrayAttribute.");
  for (size_t i = 0; i < attributes.at("output_padding").dyn_cast<pir::ArrayAttribute>().size(); i++) {
    IR_ENFORCE(attributes.at("output_padding").dyn_cast<pir::ArrayAttribute>().at(i).isa<pir::Int32Attribute>(),
                   "Type of attribute: output_padding is not right.");
  }
  IR_ENFORCE(attributes.count("output_size")>0,
                 "output_size does not exist.");
  IR_ENFORCE(attributes.at("output_size").isa<paddle::dialect::IntArrayAttribute>(),
                 "Type of attribute: output_size is not paddle::dialect::IntArrayAttribute.");

  IR_ENFORCE(attributes.count("padding_algorithm")>0,
                 "padding_algorithm does not exist.");
  IR_ENFORCE(attributes.at("padding_algorithm").isa<pir::StrAttribute>(),
                 "Type of attribute: padding_algorithm is not pir::StrAttribute.");

  IR_ENFORCE(attributes.count("groups")>0,
                 "groups does not exist.");
  IR_ENFORCE(attributes.at("groups").isa<pir::Int32Attribute>(),
                 "Type of attribute: groups is not pir::Int32Attribute.");

  IR_ENFORCE(attributes.count("dilations")>0,
                 "dilations does not exist.");
  IR_ENFORCE(attributes.at("dilations").isa<pir::ArrayAttribute>(),
                 "Type of attribute: dilations is not pir::ArrayAttribute.");
  for (size_t i = 0; i < attributes.at("dilations").dyn_cast<pir::ArrayAttribute>().size(); i++) {
    IR_ENFORCE(attributes.at("dilations").dyn_cast<pir::ArrayAttribute>().at(i).isa<pir::Int32Attribute>(),
                   "Type of attribute: dilations is not right.");
  }
  IR_ENFORCE(attributes.count("data_format")>0,
                 "data_format does not exist.");
  IR_ENFORCE(attributes.at("data_format").isa<pir::StrAttribute>(),
                 "Type of attribute: data_format is not pir::StrAttribute.");

  IR_ENFORCE(attributes.count("has_bias")>0,
                 "has_bias does not exist.");
  IR_ENFORCE(attributes.at("has_bias").isa<pir::BoolAttribute>(),
                 "Type of attribute: has_bias is not pir::BoolAttribute.");

  IR_ENFORCE(attributes.count("with_act")>0,
                 "with_act does not exist.");
  IR_ENFORCE(attributes.at("with_act").isa<pir::BoolAttribute>(),
                 "Type of attribute: with_act is not pir::BoolAttribute.");

  IR_ENFORCE(attributes.count("act_type")>0,
                 "act_type does not exist.");
  IR_ENFORCE(attributes.at("act_type").isa<pir::StrAttribute>(),
                 "Type of attribute: act_type is not pir::StrAttribute.");

  }
  VLOG(4) << "Verifying outputs:";
  {
  auto output_size = num_results();
  IR_ENFORCE(output_size == 2u,
                    "The size %d of outputs must be equal to 2.", output_size);
  IR_ENFORCE((*this)->result(0).type().isa<paddle::dialect::DenseTensorType>(),
                 "Type validation failed for the 0th output.");
  IR_ENFORCE((*this)->result(1).type().isa<paddle::dialect::DenseTensorType>(),
                 "Type validation failed for the 1th output.");
  }
  VLOG(4) << "End Verifying for: Conv2dTransposeXpuOp.";
}

void Conv2dTransposeXpuOp::InferMeta( phi::InferMetaContext *infer_meta ) {
  auto fn = PD_INFER_META(phi::Conv2dTransposeXPUInferMeta);
  fn(infer_meta);
}

phi::DataType Conv2dTransposeXpuOp::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: Conv2dTransposeXpuOp";
  


  return expected_kernel_dtype;
}

const char *Conv2dXpuOp::attributes_name[8] = { "paddings", "dilations", "strides", "padding_algorithm", "groups", "act_type", "act_param", "out_dtype" };

OpInfoTuple Conv2dXpuOp::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("x", "paddle::dialect::DenseTensorType", false, false, false, false), paddle::dialect::OpInputInfo("x_max", "paddle::dialect::DenseTensorType", true, false, false, false), paddle::dialect::OpInputInfo("filter", "paddle::dialect::DenseTensorType", false, false, false, false), paddle::dialect::OpInputInfo("filter_max", "paddle::dialect::DenseTensorType", false, false, false, false), paddle::dialect::OpInputInfo("bias", "paddle::dialect::DenseTensorType", true, false, false, false), paddle::dialect::OpInputInfo("branch", "paddle::dialect::DenseTensorType", true, false, false, false), paddle::dialect::OpInputInfo("branch_max", "paddle::dialect::DenseTensorType", true, false, false, false), paddle::dialect::OpInputInfo("scale_max", "paddle::dialect::DenseTensorType", true, false, false, false), paddle::dialect::OpInputInfo("out_max_in", "paddle::dialect::DenseTensorType", true, false, false, false) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = { paddle::dialect::OpAttributeInfo("paddings", "pir::ArrayAttribute<pir::Int32Attribute>", ""), paddle::dialect::OpAttributeInfo("dilations", "pir::ArrayAttribute<pir::Int32Attribute>", ""), paddle::dialect::OpAttributeInfo("strides", "pir::ArrayAttribute<pir::Int32Attribute>", ""), paddle::dialect::OpAttributeInfo("padding_algorithm", "pir::StrAttribute", ""), paddle::dialect::OpAttributeInfo("groups", "pir::Int32Attribute", ""), paddle::dialect::OpAttributeInfo("act_type", "pir::Int32Attribute", ""), paddle::dialect::OpAttributeInfo("act_param", "pir::FloatAttribute", ""), paddle::dialect::OpAttributeInfo("out_dtype", "paddle::dialect::DataTypeAttribute", "") };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("out", "paddle::dialect::DenseTensorType", false, false), paddle::dialect::OpOutputInfo("out_max", "paddle::dialect::DenseTensorType", false, false) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("Conv2dXPUInferMeta", {"x", "x_max", "filter", "filter_max", "bias", "branch", "branch_max", "scale_max", "out_max_in", "paddings", "dilations", "strides", "padding_algorithm", "groups", "act_type", "act_param", "out_dtype"}, "conv2d_xpu", {"x", "x_max", "filter", "filter_max", "bias", "branch", "branch_max", "scale_max", "out_max_in", "paddings", "dilations", "strides", "padding_algorithm", "groups", "act_type", "act_param", "out_dtype"}, {"x"}, {}, {}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "conv2d_xpu");
}

void Conv2dXpuOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, pir::Value x_max_, pir::Value filter_, pir::Value filter_max_, pir::Value bias_, pir::Value branch_, pir::Value branch_max_, pir::Value scale_max_, pir::Value out_max_in_, const std::vector<int>& paddings, const std::vector<int>& dilations, const std::vector<int>& strides, const std::string& padding_algorithm, int groups, int act_type, float act_param, phi::DataType out_dtype) {
  VLOG(4) << "Start build Conv2dXpuOp";



  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_, x_max_, filter_, filter_max_, bias_, branch_, branch_max_, scale_max_, out_max_in_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  std::vector<pir::Attribute> vec_paddings;
  for (size_t i = 0; i < static_cast<size_t>(paddings.size()); i++) {
      pir::Attribute attr_paddings = pir::Int32Attribute::get(pir::IrContext::Instance(), paddings[i]);

    vec_paddings.push_back(attr_paddings);
  }
  pir::Attribute attr_paddings = pir::ArrayAttribute::get(pir::IrContext::Instance(), vec_paddings);
  argument.AddAttribute("paddings", attr_paddings);
  std::vector<pir::Attribute> vec_dilations;
  for (size_t i = 0; i < static_cast<size_t>(dilations.size()); i++) {
      pir::Attribute attr_dilations = pir::Int32Attribute::get(pir::IrContext::Instance(), dilations[i]);

    vec_dilations.push_back(attr_dilations);
  }
  pir::Attribute attr_dilations = pir::ArrayAttribute::get(pir::IrContext::Instance(), vec_dilations);
  argument.AddAttribute("dilations", attr_dilations);
  std::vector<pir::Attribute> vec_strides;
  for (size_t i = 0; i < static_cast<size_t>(strides.size()); i++) {
      pir::Attribute attr_strides = pir::Int32Attribute::get(pir::IrContext::Instance(), strides[i]);

    vec_strides.push_back(attr_strides);
  }
  pir::Attribute attr_strides = pir::ArrayAttribute::get(pir::IrContext::Instance(), vec_strides);
  argument.AddAttribute("strides", attr_strides);
  pir::Attribute attr_padding_algorithm = pir::StrAttribute::get(pir::IrContext::Instance(), padding_algorithm);
  argument.AddAttribute("padding_algorithm", attr_padding_algorithm);
  pir::Attribute attr_groups = pir::Int32Attribute::get(pir::IrContext::Instance(), groups);
  argument.AddAttribute("groups", attr_groups);
  pir::Attribute attr_act_type = pir::Int32Attribute::get(pir::IrContext::Instance(), act_type);
  argument.AddAttribute("act_type", attr_act_type);
  pir::Attribute attr_act_param = pir::FloatAttribute::get(pir::IrContext::Instance(), act_param);
  argument.AddAttribute("act_param", attr_act_param);
  pir::Attribute attr_out_dtype = paddle::dialect::DataTypeAttribute::get(pir::IrContext::Instance(), out_dtype);
  argument.AddAttribute("out_dtype", attr_out_dtype);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;
  paddle::dialect::DenseTensorType filter = filter_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)filter;
  paddle::dialect::DenseTensorType filter_max = filter_max_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)filter_max;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);

  paddle::dialect::IrMetaTensor meta_x_max;
  paddle::dialect::IrTensor ir_tensor_x_max;
  if (x_max_.impl() != nullptr) {
    paddle::dialect::DenseTensorType x_max = x_max_.type().dyn_cast<paddle::dialect::DenseTensorType>();
    VLOG(4) << "Builder construction  dense_x_max";
    ir_tensor_x_max = paddle::dialect::IrTensor(paddle::dialect::TransToPhiDataType(x_max.dtype()),
                                                        x_max.dims(),
                                                        x_max.data_layout(),
                                                        x_max.lod(),
                                                        x_max.offset());
    VLOG(4) << "Builder construction  meta_x_max";
    meta_x_max = paddle::dialect::IrMetaTensor(&ir_tensor_x_max);
  }


  VLOG(4) << "Builder construction  dense_filter";
  paddle::dialect::IrTensor ir_tensor_filter(paddle::dialect::TransToPhiDataType(filter.dtype()),
                                                      filter.dims(),
                                                      filter.data_layout(),
                                                      filter.lod(),
                                                      filter.offset());
  VLOG(4) << "Builder construction  meta_filter";
  paddle::dialect::IrMetaTensor meta_filter(&ir_tensor_filter);

  VLOG(4) << "Builder construction  dense_filter_max";
  paddle::dialect::IrTensor ir_tensor_filter_max(paddle::dialect::TransToPhiDataType(filter_max.dtype()),
                                                      filter_max.dims(),
                                                      filter_max.data_layout(),
                                                      filter_max.lod(),
                                                      filter_max.offset());
  VLOG(4) << "Builder construction  meta_filter_max";
  paddle::dialect::IrMetaTensor meta_filter_max(&ir_tensor_filter_max);

  paddle::dialect::IrMetaTensor meta_bias;
  paddle::dialect::IrTensor ir_tensor_bias;
  if (bias_.impl() != nullptr) {
    paddle::dialect::DenseTensorType bias = bias_.type().dyn_cast<paddle::dialect::DenseTensorType>();
    VLOG(4) << "Builder construction  dense_bias";
    ir_tensor_bias = paddle::dialect::IrTensor(paddle::dialect::TransToPhiDataType(bias.dtype()),
                                                        bias.dims(),
                                                        bias.data_layout(),
                                                        bias.lod(),
                                                        bias.offset());
    VLOG(4) << "Builder construction  meta_bias";
    meta_bias = paddle::dialect::IrMetaTensor(&ir_tensor_bias);
  }


  paddle::dialect::IrMetaTensor meta_branch;
  paddle::dialect::IrTensor ir_tensor_branch;
  if (branch_.impl() != nullptr) {
    paddle::dialect::DenseTensorType branch = branch_.type().dyn_cast<paddle::dialect::DenseTensorType>();
    VLOG(4) << "Builder construction  dense_branch";
    ir_tensor_branch = paddle::dialect::IrTensor(paddle::dialect::TransToPhiDataType(branch.dtype()),
                                                        branch.dims(),
                                                        branch.data_layout(),
                                                        branch.lod(),
                                                        branch.offset());
    VLOG(4) << "Builder construction  meta_branch";
    meta_branch = paddle::dialect::IrMetaTensor(&ir_tensor_branch);
  }


  paddle::dialect::IrMetaTensor meta_branch_max;
  paddle::dialect::IrTensor ir_tensor_branch_max;
  if (branch_max_.impl() != nullptr) {
    paddle::dialect::DenseTensorType branch_max = branch_max_.type().dyn_cast<paddle::dialect::DenseTensorType>();
    VLOG(4) << "Builder construction  dense_branch_max";
    ir_tensor_branch_max = paddle::dialect::IrTensor(paddle::dialect::TransToPhiDataType(branch_max.dtype()),
                                                        branch_max.dims(),
                                                        branch_max.data_layout(),
                                                        branch_max.lod(),
                                                        branch_max.offset());
    VLOG(4) << "Builder construction  meta_branch_max";
    meta_branch_max = paddle::dialect::IrMetaTensor(&ir_tensor_branch_max);
  }


  paddle::dialect::IrMetaTensor meta_scale_max;
  paddle::dialect::IrTensor ir_tensor_scale_max;
  if (scale_max_.impl() != nullptr) {
    paddle::dialect::DenseTensorType scale_max = scale_max_.type().dyn_cast<paddle::dialect::DenseTensorType>();
    VLOG(4) << "Builder construction  dense_scale_max";
    ir_tensor_scale_max = paddle::dialect::IrTensor(paddle::dialect::TransToPhiDataType(scale_max.dtype()),
                                                        scale_max.dims(),
                                                        scale_max.data_layout(),
                                                        scale_max.lod(),
                                                        scale_max.offset());
    VLOG(4) << "Builder construction  meta_scale_max";
    meta_scale_max = paddle::dialect::IrMetaTensor(&ir_tensor_scale_max);
  }


  paddle::dialect::IrMetaTensor meta_out_max_in;
  paddle::dialect::IrTensor ir_tensor_out_max_in;
  if (out_max_in_.impl() != nullptr) {
    paddle::dialect::DenseTensorType out_max_in = out_max_in_.type().dyn_cast<paddle::dialect::DenseTensorType>();
    VLOG(4) << "Builder construction  dense_out_max_in";
    ir_tensor_out_max_in = paddle::dialect::IrTensor(paddle::dialect::TransToPhiDataType(out_max_in.dtype()),
                                                        out_max_in.dims(),
                                                        out_max_in.data_layout(),
                                                        out_max_in.lod(),
                                                        out_max_in.offset());
    VLOG(4) << "Builder construction  meta_out_max_in";
    meta_out_max_in = paddle::dialect::IrMetaTensor(&ir_tensor_out_max_in);
  }

  paddle::dialect::IrTensor dense_out;
  paddle::dialect::IrMetaTensor meta_out(&dense_out);
  paddle::dialect::IrTensor dense_out_max;
  paddle::dialect::IrMetaTensor meta_out_max(&dense_out_max);

  phi::Conv2dXPUInferMeta(meta_x, meta_x_max, meta_filter, meta_filter_max, meta_bias, meta_branch, meta_branch_max, meta_scale_max, meta_out_max_in, paddings, dilations, strides, padding_algorithm, groups, act_type, act_param, out_dtype, &meta_out, &meta_out_max);

  std::vector<pir::Type> argument_outputs;
  pir::Type out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_out.dtype()), dense_out.dims(), dense_out.layout(), dense_out.lod(), dense_out.offset());
  argument_outputs.push_back(out_dense_tensor_type);

  pir::Type out_max_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_out_max.dtype()), dense_out_max.dims(), dense_out_max.layout(), dense_out_max.lod(), dense_out_max.offset());
  argument_outputs.push_back(out_max_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void Conv2dXpuOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, pir::Value x_max_, pir::Value filter_, pir::Value filter_max_, pir::Value bias_, pir::Value branch_, pir::Value branch_max_, pir::Value scale_max_, pir::Value out_max_in_, pir::AttributeMap attributes) {
  VLOG(4) << "Start build Conv2dXpuOp";


  IR_ENFORCE(
      attributes.find("paddings") != attributes.end(),
          "'paddings' Attribute is expected for Conv2dXpuOp. ");
  std::vector<int> paddings;
  for (size_t i = 0; i < attributes.at("paddings").dyn_cast<pir::ArrayAttribute>().size(); i++) {
    paddings.push_back(attributes.at("paddings").dyn_cast<pir::ArrayAttribute>().at(i).dyn_cast<pir::Int32Attribute>().data());
  }

  IR_ENFORCE(
      attributes.find("dilations") != attributes.end(),
          "'dilations' Attribute is expected for Conv2dXpuOp. ");
  std::vector<int> dilations;
  for (size_t i = 0; i < attributes.at("dilations").dyn_cast<pir::ArrayAttribute>().size(); i++) {
    dilations.push_back(attributes.at("dilations").dyn_cast<pir::ArrayAttribute>().at(i).dyn_cast<pir::Int32Attribute>().data());
  }

  IR_ENFORCE(
      attributes.find("strides") != attributes.end(),
          "'strides' Attribute is expected for Conv2dXpuOp. ");
  std::vector<int> strides;
  for (size_t i = 0; i < attributes.at("strides").dyn_cast<pir::ArrayAttribute>().size(); i++) {
    strides.push_back(attributes.at("strides").dyn_cast<pir::ArrayAttribute>().at(i).dyn_cast<pir::Int32Attribute>().data());
  }

  IR_ENFORCE(
      attributes.find("padding_algorithm") != attributes.end(),
          "'padding_algorithm' Attribute is expected for Conv2dXpuOp. ");
  std::string padding_algorithm = attributes.at("padding_algorithm").dyn_cast<pir::StrAttribute>().AsString();

  IR_ENFORCE(
      attributes.find("groups") != attributes.end(),
          "'groups' Attribute is expected for Conv2dXpuOp. ");
  int groups = attributes.at("groups").dyn_cast<pir::Int32Attribute>().data();

  IR_ENFORCE(
      attributes.find("act_type") != attributes.end(),
          "'act_type' Attribute is expected for Conv2dXpuOp. ");
  int act_type = attributes.at("act_type").dyn_cast<pir::Int32Attribute>().data();

  IR_ENFORCE(
      attributes.find("act_param") != attributes.end(),
          "'act_param' Attribute is expected for Conv2dXpuOp. ");
  float act_param = attributes.at("act_param").dyn_cast<pir::FloatAttribute>().data();

  IR_ENFORCE(
      attributes.find("out_dtype") != attributes.end(),
          "'out_dtype' Attribute is expected for Conv2dXpuOp. ");
  phi::DataType out_dtype = attributes.at("out_dtype").dyn_cast<paddle::dialect::DataTypeAttribute>().data();


  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_, x_max_, filter_, filter_max_, bias_, branch_, branch_max_, scale_max_, out_max_in_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  std::vector<pir::Attribute> vec_paddings;
  for (size_t i = 0; i < static_cast<size_t>(paddings.size()); i++) {
      pir::Attribute attr_paddings = pir::Int32Attribute::get(pir::IrContext::Instance(), paddings[i]);

    vec_paddings.push_back(attr_paddings);
  }
  pir::Attribute attr_paddings = pir::ArrayAttribute::get(pir::IrContext::Instance(), vec_paddings);
  argument.AddAttribute("paddings", attr_paddings);
  std::vector<pir::Attribute> vec_dilations;
  for (size_t i = 0; i < static_cast<size_t>(dilations.size()); i++) {
      pir::Attribute attr_dilations = pir::Int32Attribute::get(pir::IrContext::Instance(), dilations[i]);

    vec_dilations.push_back(attr_dilations);
  }
  pir::Attribute attr_dilations = pir::ArrayAttribute::get(pir::IrContext::Instance(), vec_dilations);
  argument.AddAttribute("dilations", attr_dilations);
  std::vector<pir::Attribute> vec_strides;
  for (size_t i = 0; i < static_cast<size_t>(strides.size()); i++) {
      pir::Attribute attr_strides = pir::Int32Attribute::get(pir::IrContext::Instance(), strides[i]);

    vec_strides.push_back(attr_strides);
  }
  pir::Attribute attr_strides = pir::ArrayAttribute::get(pir::IrContext::Instance(), vec_strides);
  argument.AddAttribute("strides", attr_strides);
  pir::Attribute attr_padding_algorithm = pir::StrAttribute::get(pir::IrContext::Instance(), padding_algorithm);
  argument.AddAttribute("padding_algorithm", attr_padding_algorithm);
  pir::Attribute attr_groups = pir::Int32Attribute::get(pir::IrContext::Instance(), groups);
  argument.AddAttribute("groups", attr_groups);
  pir::Attribute attr_act_type = pir::Int32Attribute::get(pir::IrContext::Instance(), act_type);
  argument.AddAttribute("act_type", attr_act_type);
  pir::Attribute attr_act_param = pir::FloatAttribute::get(pir::IrContext::Instance(), act_param);
  argument.AddAttribute("act_param", attr_act_param);
  pir::Attribute attr_out_dtype = paddle::dialect::DataTypeAttribute::get(pir::IrContext::Instance(), out_dtype);
  argument.AddAttribute("out_dtype", attr_out_dtype);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;
  paddle::dialect::DenseTensorType filter = filter_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)filter;
  paddle::dialect::DenseTensorType filter_max = filter_max_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)filter_max;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);

  paddle::dialect::IrMetaTensor meta_x_max;
  paddle::dialect::IrTensor ir_tensor_x_max;
  if (x_max_.impl() != nullptr) {
    paddle::dialect::DenseTensorType x_max = x_max_.type().dyn_cast<paddle::dialect::DenseTensorType>();
    VLOG(4) << "Builder construction  dense_x_max";
    ir_tensor_x_max = paddle::dialect::IrTensor(paddle::dialect::TransToPhiDataType(x_max.dtype()),
                                                        x_max.dims(),
                                                        x_max.data_layout(),
                                                        x_max.lod(),
                                                        x_max.offset());
    VLOG(4) << "Builder construction  meta_x_max";
    meta_x_max = paddle::dialect::IrMetaTensor(&ir_tensor_x_max);
  }


  VLOG(4) << "Builder construction  dense_filter";
  paddle::dialect::IrTensor ir_tensor_filter(paddle::dialect::TransToPhiDataType(filter.dtype()),
                                                      filter.dims(),
                                                      filter.data_layout(),
                                                      filter.lod(),
                                                      filter.offset());
  VLOG(4) << "Builder construction  meta_filter";
  paddle::dialect::IrMetaTensor meta_filter(&ir_tensor_filter);

  VLOG(4) << "Builder construction  dense_filter_max";
  paddle::dialect::IrTensor ir_tensor_filter_max(paddle::dialect::TransToPhiDataType(filter_max.dtype()),
                                                      filter_max.dims(),
                                                      filter_max.data_layout(),
                                                      filter_max.lod(),
                                                      filter_max.offset());
  VLOG(4) << "Builder construction  meta_filter_max";
  paddle::dialect::IrMetaTensor meta_filter_max(&ir_tensor_filter_max);

  paddle::dialect::IrMetaTensor meta_bias;
  paddle::dialect::IrTensor ir_tensor_bias;
  if (bias_.impl() != nullptr) {
    paddle::dialect::DenseTensorType bias = bias_.type().dyn_cast<paddle::dialect::DenseTensorType>();
    VLOG(4) << "Builder construction  dense_bias";
    ir_tensor_bias = paddle::dialect::IrTensor(paddle::dialect::TransToPhiDataType(bias.dtype()),
                                                        bias.dims(),
                                                        bias.data_layout(),
                                                        bias.lod(),
                                                        bias.offset());
    VLOG(4) << "Builder construction  meta_bias";
    meta_bias = paddle::dialect::IrMetaTensor(&ir_tensor_bias);
  }


  paddle::dialect::IrMetaTensor meta_branch;
  paddle::dialect::IrTensor ir_tensor_branch;
  if (branch_.impl() != nullptr) {
    paddle::dialect::DenseTensorType branch = branch_.type().dyn_cast<paddle::dialect::DenseTensorType>();
    VLOG(4) << "Builder construction  dense_branch";
    ir_tensor_branch = paddle::dialect::IrTensor(paddle::dialect::TransToPhiDataType(branch.dtype()),
                                                        branch.dims(),
                                                        branch.data_layout(),
                                                        branch.lod(),
                                                        branch.offset());
    VLOG(4) << "Builder construction  meta_branch";
    meta_branch = paddle::dialect::IrMetaTensor(&ir_tensor_branch);
  }


  paddle::dialect::IrMetaTensor meta_branch_max;
  paddle::dialect::IrTensor ir_tensor_branch_max;
  if (branch_max_.impl() != nullptr) {
    paddle::dialect::DenseTensorType branch_max = branch_max_.type().dyn_cast<paddle::dialect::DenseTensorType>();
    VLOG(4) << "Builder construction  dense_branch_max";
    ir_tensor_branch_max = paddle::dialect::IrTensor(paddle::dialect::TransToPhiDataType(branch_max.dtype()),
                                                        branch_max.dims(),
                                                        branch_max.data_layout(),
                                                        branch_max.lod(),
                                                        branch_max.offset());
    VLOG(4) << "Builder construction  meta_branch_max";
    meta_branch_max = paddle::dialect::IrMetaTensor(&ir_tensor_branch_max);
  }


  paddle::dialect::IrMetaTensor meta_scale_max;
  paddle::dialect::IrTensor ir_tensor_scale_max;
  if (scale_max_.impl() != nullptr) {
    paddle::dialect::DenseTensorType scale_max = scale_max_.type().dyn_cast<paddle::dialect::DenseTensorType>();
    VLOG(4) << "Builder construction  dense_scale_max";
    ir_tensor_scale_max = paddle::dialect::IrTensor(paddle::dialect::TransToPhiDataType(scale_max.dtype()),
                                                        scale_max.dims(),
                                                        scale_max.data_layout(),
                                                        scale_max.lod(),
                                                        scale_max.offset());
    VLOG(4) << "Builder construction  meta_scale_max";
    meta_scale_max = paddle::dialect::IrMetaTensor(&ir_tensor_scale_max);
  }


  paddle::dialect::IrMetaTensor meta_out_max_in;
  paddle::dialect::IrTensor ir_tensor_out_max_in;
  if (out_max_in_.impl() != nullptr) {
    paddle::dialect::DenseTensorType out_max_in = out_max_in_.type().dyn_cast<paddle::dialect::DenseTensorType>();
    VLOG(4) << "Builder construction  dense_out_max_in";
    ir_tensor_out_max_in = paddle::dialect::IrTensor(paddle::dialect::TransToPhiDataType(out_max_in.dtype()),
                                                        out_max_in.dims(),
                                                        out_max_in.data_layout(),
                                                        out_max_in.lod(),
                                                        out_max_in.offset());
    VLOG(4) << "Builder construction  meta_out_max_in";
    meta_out_max_in = paddle::dialect::IrMetaTensor(&ir_tensor_out_max_in);
  }

  paddle::dialect::IrTensor dense_out;
  paddle::dialect::IrMetaTensor meta_out(&dense_out);
  paddle::dialect::IrTensor dense_out_max;
  paddle::dialect::IrMetaTensor meta_out_max(&dense_out_max);

  phi::Conv2dXPUInferMeta(meta_x, meta_x_max, meta_filter, meta_filter_max, meta_bias, meta_branch, meta_branch_max, meta_scale_max, meta_out_max_in, paddings, dilations, strides, padding_algorithm, groups, act_type, act_param, out_dtype, &meta_out, &meta_out_max);

  std::vector<pir::Type> argument_outputs;
  pir::Type out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_out.dtype()), dense_out.dims(), dense_out.layout(), dense_out.lod(), dense_out.offset());
  argument_outputs.push_back(out_dense_tensor_type);

  pir::Type out_max_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_out_max.dtype()), dense_out_max.dims(), dense_out_max.layout(), dense_out_max.lod(), dense_out_max.offset());
  argument_outputs.push_back(out_max_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void Conv2dXpuOp::VerifySig() {
  VLOG(4) << "Start Verifying inputs, outputs and attributes for: Conv2dXpuOp.";
  VLOG(4) << "Verifying inputs:";
  {
  auto input_size = num_operands();
  IR_ENFORCE(input_size == 9u,
                    "The size %d of inputs must be equal to 9.", input_size);
  IR_ENFORCE((*this)->operand_source(0).type().isa<paddle::dialect::DenseTensorType>(),
                  "Type validation failed for the 0th input, got %s.", (*this)->operand_source(0).type());
  if (auto val = (*this)->operand(1)) {
    IR_ENFORCE(val.type().isa<paddle::dialect::DenseTensorType>(),
                   "Type validation failed for the 1th input, got %s.", (*this)->operand_source(1).type());
  }
  IR_ENFORCE((*this)->operand_source(2).type().isa<paddle::dialect::DenseTensorType>(),
                  "Type validation failed for the 2th input, got %s.", (*this)->operand_source(2).type());
  IR_ENFORCE((*this)->operand_source(3).type().isa<paddle::dialect::DenseTensorType>(),
                  "Type validation failed for the 3th input, got %s.", (*this)->operand_source(3).type());
  if (auto val = (*this)->operand(4)) {
    IR_ENFORCE(val.type().isa<paddle::dialect::DenseTensorType>(),
                   "Type validation failed for the 4th input, got %s.", (*this)->operand_source(4).type());
  }
  if (auto val = (*this)->operand(5)) {
    IR_ENFORCE(val.type().isa<paddle::dialect::DenseTensorType>(),
                   "Type validation failed for the 5th input, got %s.", (*this)->operand_source(5).type());
  }
  if (auto val = (*this)->operand(6)) {
    IR_ENFORCE(val.type().isa<paddle::dialect::DenseTensorType>(),
                   "Type validation failed for the 6th input, got %s.", (*this)->operand_source(6).type());
  }
  if (auto val = (*this)->operand(7)) {
    IR_ENFORCE(val.type().isa<paddle::dialect::DenseTensorType>(),
                   "Type validation failed for the 7th input, got %s.", (*this)->operand_source(7).type());
  }
  if (auto val = (*this)->operand(8)) {
    IR_ENFORCE(val.type().isa<paddle::dialect::DenseTensorType>(),
                   "Type validation failed for the 8th input, got %s.", (*this)->operand_source(8).type());
  }
  }
  VLOG(4) << "Verifying attributes:";
  {
  auto& attributes = this->attributes();
  IR_ENFORCE(attributes.count("paddings")>0,
                 "paddings does not exist.");
  IR_ENFORCE(attributes.at("paddings").isa<pir::ArrayAttribute>(),
                 "Type of attribute: paddings is not pir::ArrayAttribute.");
  for (size_t i = 0; i < attributes.at("paddings").dyn_cast<pir::ArrayAttribute>().size(); i++) {
    IR_ENFORCE(attributes.at("paddings").dyn_cast<pir::ArrayAttribute>().at(i).isa<pir::Int32Attribute>(),
                   "Type of attribute: paddings is not right.");
  }
  IR_ENFORCE(attributes.count("dilations")>0,
                 "dilations does not exist.");
  IR_ENFORCE(attributes.at("dilations").isa<pir::ArrayAttribute>(),
                 "Type of attribute: dilations is not pir::ArrayAttribute.");
  for (size_t i = 0; i < attributes.at("dilations").dyn_cast<pir::ArrayAttribute>().size(); i++) {
    IR_ENFORCE(attributes.at("dilations").dyn_cast<pir::ArrayAttribute>().at(i).isa<pir::Int32Attribute>(),
                   "Type of attribute: dilations is not right.");
  }
  IR_ENFORCE(attributes.count("strides")>0,
                 "strides does not exist.");
  IR_ENFORCE(attributes.at("strides").isa<pir::ArrayAttribute>(),
                 "Type of attribute: strides is not pir::ArrayAttribute.");
  for (size_t i = 0; i < attributes.at("strides").dyn_cast<pir::ArrayAttribute>().size(); i++) {
    IR_ENFORCE(attributes.at("strides").dyn_cast<pir::ArrayAttribute>().at(i).isa<pir::Int32Attribute>(),
                   "Type of attribute: strides is not right.");
  }
  IR_ENFORCE(attributes.count("padding_algorithm")>0,
                 "padding_algorithm does not exist.");
  IR_ENFORCE(attributes.at("padding_algorithm").isa<pir::StrAttribute>(),
                 "Type of attribute: padding_algorithm is not pir::StrAttribute.");

  IR_ENFORCE(attributes.count("groups")>0,
                 "groups does not exist.");
  IR_ENFORCE(attributes.at("groups").isa<pir::Int32Attribute>(),
                 "Type of attribute: groups is not pir::Int32Attribute.");

  IR_ENFORCE(attributes.count("act_type")>0,
                 "act_type does not exist.");
  IR_ENFORCE(attributes.at("act_type").isa<pir::Int32Attribute>(),
                 "Type of attribute: act_type is not pir::Int32Attribute.");

  IR_ENFORCE(attributes.count("act_param")>0,
                 "act_param does not exist.");
  IR_ENFORCE(attributes.at("act_param").isa<pir::FloatAttribute>(),
                 "Type of attribute: act_param is not pir::FloatAttribute.");

  IR_ENFORCE(attributes.count("out_dtype")>0,
                 "out_dtype does not exist.");
  IR_ENFORCE(attributes.at("out_dtype").isa<paddle::dialect::DataTypeAttribute>(),
                 "Type of attribute: out_dtype is not paddle::dialect::DataTypeAttribute.");

  }
  VLOG(4) << "Verifying outputs:";
  {
  auto output_size = num_results();
  IR_ENFORCE(output_size == 2u,
                    "The size %d of outputs must be equal to 2.", output_size);
  IR_ENFORCE((*this)->result(0).type().isa<paddle::dialect::DenseTensorType>(),
                 "Type validation failed for the 0th output.");
  IR_ENFORCE((*this)->result(1).type().isa<paddle::dialect::DenseTensorType>(),
                 "Type validation failed for the 1th output.");
  }
  VLOG(4) << "End Verifying for: Conv2dXpuOp.";
}

void Conv2dXpuOp::InferMeta( phi::InferMetaContext *infer_meta ) {
  auto fn = PD_INFER_META(phi::Conv2dXPUInferMeta);
  fn(infer_meta);
}

phi::DataType Conv2dXpuOp::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: Conv2dXpuOp";
  


  return expected_kernel_dtype;
}

const char *DequantizeXpuOp::attributes_name[2] = { "out_dtype", "scale" };

OpInfoTuple DequantizeXpuOp::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("x", "paddle::dialect::DenseTensorType", false, false, false, false) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = { paddle::dialect::OpAttributeInfo("out_dtype", "paddle::dialect::DataTypeAttribute", ""), paddle::dialect::OpAttributeInfo("scale", "pir::FloatAttribute", "") };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("y", "paddle::dialect::DenseTensorType", false, false) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("DeQuantizeXPUInferMeta", {"x", "out_dtype", "scale"}, "dequantize_xpu", {"x", "out_dtype", "scale"}, {"x"}, {}, {}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "dequantize_xpu");
}

void DequantizeXpuOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, phi::DataType out_dtype, float scale) {
  VLOG(4) << "Start build DequantizeXpuOp";



  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_out_dtype = paddle::dialect::DataTypeAttribute::get(pir::IrContext::Instance(), out_dtype);
  argument.AddAttribute("out_dtype", attr_out_dtype);
  pir::Attribute attr_scale = pir::FloatAttribute::get(pir::IrContext::Instance(), scale);
  argument.AddAttribute("scale", attr_scale);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);
  paddle::dialect::IrTensor dense_y;
  paddle::dialect::IrMetaTensor meta_y(&dense_y);

  phi::DeQuantizeXPUInferMeta(meta_x, out_dtype, scale, &meta_y);

  std::vector<pir::Type> argument_outputs;
  pir::Type y_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_y.dtype()), dense_y.dims(), dense_y.layout(), dense_y.lod(), dense_y.offset());
  argument_outputs.push_back(y_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void DequantizeXpuOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, pir::AttributeMap attributes) {
  VLOG(4) << "Start build DequantizeXpuOp";


  IR_ENFORCE(
      attributes.find("out_dtype") != attributes.end(),
          "'out_dtype' Attribute is expected for DequantizeXpuOp. ");
  phi::DataType out_dtype = attributes.at("out_dtype").dyn_cast<paddle::dialect::DataTypeAttribute>().data();

  IR_ENFORCE(
      attributes.find("scale") != attributes.end(),
          "'scale' Attribute is expected for DequantizeXpuOp. ");
  float scale = attributes.at("scale").dyn_cast<pir::FloatAttribute>().data();


  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_out_dtype = paddle::dialect::DataTypeAttribute::get(pir::IrContext::Instance(), out_dtype);
  argument.AddAttribute("out_dtype", attr_out_dtype);
  pir::Attribute attr_scale = pir::FloatAttribute::get(pir::IrContext::Instance(), scale);
  argument.AddAttribute("scale", attr_scale);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);
  paddle::dialect::IrTensor dense_y;
  paddle::dialect::IrMetaTensor meta_y(&dense_y);

  phi::DeQuantizeXPUInferMeta(meta_x, out_dtype, scale, &meta_y);

  std::vector<pir::Type> argument_outputs;
  pir::Type y_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_y.dtype()), dense_y.dims(), dense_y.layout(), dense_y.lod(), dense_y.offset());
  argument_outputs.push_back(y_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void DequantizeXpuOp::VerifySig() {
  VLOG(4) << "Start Verifying inputs, outputs and attributes for: DequantizeXpuOp.";
  VLOG(4) << "Verifying inputs:";
  {
  auto input_size = num_operands();
  IR_ENFORCE(input_size == 1u,
                    "The size %d of inputs must be equal to 1.", input_size);
  IR_ENFORCE((*this)->operand_source(0).type().isa<paddle::dialect::DenseTensorType>(),
                  "Type validation failed for the 0th input, got %s.", (*this)->operand_source(0).type());
  }
  VLOG(4) << "Verifying attributes:";
  {
  auto& attributes = this->attributes();
  IR_ENFORCE(attributes.count("out_dtype")>0,
                 "out_dtype does not exist.");
  IR_ENFORCE(attributes.at("out_dtype").isa<paddle::dialect::DataTypeAttribute>(),
                 "Type of attribute: out_dtype is not paddle::dialect::DataTypeAttribute.");

  IR_ENFORCE(attributes.count("scale")>0,
                 "scale does not exist.");
  IR_ENFORCE(attributes.at("scale").isa<pir::FloatAttribute>(),
                 "Type of attribute: scale is not pir::FloatAttribute.");

  }
  VLOG(4) << "Verifying outputs:";
  {
  auto output_size = num_results();
  IR_ENFORCE(output_size == 1u,
                    "The size %d of outputs must be equal to 1.", output_size);
  IR_ENFORCE((*this)->result(0).type().isa<paddle::dialect::DenseTensorType>(),
                 "Type validation failed for the 0th output.");
  }
  VLOG(4) << "End Verifying for: DequantizeXpuOp.";
}

void DequantizeXpuOp::InferMeta( phi::InferMetaContext *infer_meta ) {
  auto fn = PD_INFER_META(phi::DeQuantizeXPUInferMeta);
  fn(infer_meta);
}

phi::DataType DequantizeXpuOp::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: DequantizeXpuOp";
  


  return expected_kernel_dtype;
}

const char *EmbeddingWithEltwiseAddXpuOp::attributes_name[1] = { "padding_idx" };

OpInfoTuple EmbeddingWithEltwiseAddXpuOp::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("ids", "pir::VectorType<paddle::dialect::DenseTensorType>", false, false, false, false), paddle::dialect::OpInputInfo("tables", "pir::VectorType<paddle::dialect::DenseTensorType>", false, false, false, false), paddle::dialect::OpInputInfo("mask", "paddle::dialect::DenseTensorType", true, false, false, false) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = { paddle::dialect::OpAttributeInfo("padding_idx", "pir::Int64Attribute", "") };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("out", "paddle::dialect::DenseTensorType", false, false), paddle::dialect::OpOutputInfo("seq_lod", "paddle::dialect::DenseTensorType", true, false), paddle::dialect::OpOutputInfo("max_seq_len", "paddle::dialect::DenseTensorType", true, false) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("EmbeddingWithEltwiseAddXPUInferMeta", {"ids", "tables", "mask"}, "embedding_with_eltwise_add_xpu", {"ids", "tables", "mask", "padding_idx"}, {"tables"}, {}, {}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "embedding_with_eltwise_add_xpu");
}

void EmbeddingWithEltwiseAddXpuOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value ids_, pir::Value tables_, pir::Value mask_, int64_t padding_idx) {
  VLOG(4) << "Start build EmbeddingWithEltwiseAddXpuOp";



  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {ids_, tables_, mask_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_padding_idx = pir::Int64Attribute::get(pir::IrContext::Instance(), padding_idx);
  argument.AddAttribute("padding_idx", attr_padding_idx);

  VLOG(4) << "Builder construction outputs";
  pir::VectorType ids = ids_.type().dyn_cast<pir::VectorType>(); (void)ids;
  pir::VectorType tables = tables_.type().dyn_cast<pir::VectorType>(); (void)tables;
  std::vector<paddle::dialect::IrTensor> vec_ir_tensor_ids;
  for (size_t i=0; i < static_cast<size_t>(ids.size()); i++) {
    vec_ir_tensor_ids.push_back(paddle::dialect::IrTensor(paddle::dialect::TransToPhiDataType(ids[i].dyn_cast<paddle::dialect::DenseTensorType>().dtype()),
                                                                     ids[i].dyn_cast<paddle::dialect::DenseTensorType>().dims(),
                                                                     ids[i].dyn_cast<paddle::dialect::DenseTensorType>().data_layout(),
                                                                     ids[i].dyn_cast<paddle::dialect::DenseTensorType>().lod(),
                                                                     ids[i].dyn_cast<paddle::dialect::DenseTensorType>().offset()));
  }
  std::vector<paddle::dialect::IrMetaTensor> vec_meta_ids;
  for (size_t i=0; i < vec_ir_tensor_ids.size(); i++) {
    vec_meta_ids.push_back(paddle::dialect::IrMetaTensor(&vec_ir_tensor_ids[i]));
  }

  std::vector<const phi::MetaTensor*> meta_ids;
  for (size_t i=0; i < static_cast<size_t>(vec_meta_ids.size()); i++) {
    meta_ids.push_back(&vec_meta_ids[i]);
  }
   std::vector<paddle::dialect::IrTensor> vec_ir_tensor_tables;
  for (size_t i=0; i < static_cast<size_t>(tables.size()); i++) {
    vec_ir_tensor_tables.push_back(paddle::dialect::IrTensor(paddle::dialect::TransToPhiDataType(tables[i].dyn_cast<paddle::dialect::DenseTensorType>().dtype()),
                                                                     tables[i].dyn_cast<paddle::dialect::DenseTensorType>().dims(),
                                                                     tables[i].dyn_cast<paddle::dialect::DenseTensorType>().data_layout(),
                                                                     tables[i].dyn_cast<paddle::dialect::DenseTensorType>().lod(),
                                                                     tables[i].dyn_cast<paddle::dialect::DenseTensorType>().offset()));
  }
  std::vector<paddle::dialect::IrMetaTensor> vec_meta_tables;
  for (size_t i=0; i < vec_ir_tensor_tables.size(); i++) {
    vec_meta_tables.push_back(paddle::dialect::IrMetaTensor(&vec_ir_tensor_tables[i]));
  }

  std::vector<const phi::MetaTensor*> meta_tables;
  for (size_t i=0; i < static_cast<size_t>(vec_meta_tables.size()); i++) {
    meta_tables.push_back(&vec_meta_tables[i]);
  }
 
  paddle::dialect::IrMetaTensor meta_mask;
  paddle::dialect::IrTensor ir_tensor_mask;
  if (mask_.impl() != nullptr) {
    paddle::dialect::DenseTensorType mask = mask_.type().dyn_cast<paddle::dialect::DenseTensorType>();
    VLOG(4) << "Builder construction  dense_mask";
    ir_tensor_mask = paddle::dialect::IrTensor(paddle::dialect::TransToPhiDataType(mask.dtype()),
                                                        mask.dims(),
                                                        mask.data_layout(),
                                                        mask.lod(),
                                                        mask.offset());
    VLOG(4) << "Builder construction  meta_mask";
    meta_mask = paddle::dialect::IrMetaTensor(&ir_tensor_mask);
  }

  paddle::dialect::IrTensor dense_out;
  paddle::dialect::IrMetaTensor meta_out(&dense_out);
  paddle::dialect::IrTensor dense_seq_lod;
  paddle::dialect::IrMetaTensor meta_seq_lod(&dense_seq_lod);
  paddle::dialect::IrTensor dense_max_seq_len;
  paddle::dialect::IrMetaTensor meta_max_seq_len(&dense_max_seq_len);

  phi::EmbeddingWithEltwiseAddXPUInferMeta(meta_ids, meta_tables, meta_mask, &meta_out, &meta_seq_lod, &meta_max_seq_len);

  std::vector<pir::Type> argument_outputs;
  pir::Type out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_out.dtype()), dense_out.dims(), dense_out.layout(), dense_out.lod(), dense_out.offset());
  argument_outputs.push_back(out_dense_tensor_type);

  pir::Type seq_lod_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_seq_lod.dtype()), dense_seq_lod.dims(), dense_seq_lod.layout(), dense_seq_lod.lod(), dense_seq_lod.offset());
  argument_outputs.push_back(seq_lod_dense_tensor_type);

  pir::Type max_seq_len_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_max_seq_len.dtype()), dense_max_seq_len.dims(), dense_max_seq_len.layout(), dense_max_seq_len.lod(), dense_max_seq_len.offset());
  argument_outputs.push_back(max_seq_len_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void EmbeddingWithEltwiseAddXpuOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value ids_, pir::Value tables_, pir::Value mask_, pir::AttributeMap attributes) {
  VLOG(4) << "Start build EmbeddingWithEltwiseAddXpuOp";


  IR_ENFORCE(
      attributes.find("padding_idx") != attributes.end(),
          "'padding_idx' Attribute is expected for EmbeddingWithEltwiseAddXpuOp. ");
  int64_t padding_idx = attributes.at("padding_idx").dyn_cast<pir::Int64Attribute>().data();


  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {ids_, tables_, mask_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_padding_idx = pir::Int64Attribute::get(pir::IrContext::Instance(), padding_idx);
  argument.AddAttribute("padding_idx", attr_padding_idx);

  VLOG(4) << "Builder construction outputs";
  pir::VectorType ids = ids_.type().dyn_cast<pir::VectorType>(); (void)ids;
  pir::VectorType tables = tables_.type().dyn_cast<pir::VectorType>(); (void)tables;
  std::vector<paddle::dialect::IrTensor> vec_ir_tensor_ids;
  for (size_t i=0; i < static_cast<size_t>(ids.size()); i++) {
    vec_ir_tensor_ids.push_back(paddle::dialect::IrTensor(paddle::dialect::TransToPhiDataType(ids[i].dyn_cast<paddle::dialect::DenseTensorType>().dtype()),
                                                                     ids[i].dyn_cast<paddle::dialect::DenseTensorType>().dims(),
                                                                     ids[i].dyn_cast<paddle::dialect::DenseTensorType>().data_layout(),
                                                                     ids[i].dyn_cast<paddle::dialect::DenseTensorType>().lod(),
                                                                     ids[i].dyn_cast<paddle::dialect::DenseTensorType>().offset()));
  }
  std::vector<paddle::dialect::IrMetaTensor> vec_meta_ids;
  for (size_t i=0; i < vec_ir_tensor_ids.size(); i++) {
    vec_meta_ids.push_back(paddle::dialect::IrMetaTensor(&vec_ir_tensor_ids[i]));
  }

  std::vector<const phi::MetaTensor*> meta_ids;
  for (size_t i=0; i < static_cast<size_t>(vec_meta_ids.size()); i++) {
    meta_ids.push_back(&vec_meta_ids[i]);
  }
   std::vector<paddle::dialect::IrTensor> vec_ir_tensor_tables;
  for (size_t i=0; i < static_cast<size_t>(tables.size()); i++) {
    vec_ir_tensor_tables.push_back(paddle::dialect::IrTensor(paddle::dialect::TransToPhiDataType(tables[i].dyn_cast<paddle::dialect::DenseTensorType>().dtype()),
                                                                     tables[i].dyn_cast<paddle::dialect::DenseTensorType>().dims(),
                                                                     tables[i].dyn_cast<paddle::dialect::DenseTensorType>().data_layout(),
                                                                     tables[i].dyn_cast<paddle::dialect::DenseTensorType>().lod(),
                                                                     tables[i].dyn_cast<paddle::dialect::DenseTensorType>().offset()));
  }
  std::vector<paddle::dialect::IrMetaTensor> vec_meta_tables;
  for (size_t i=0; i < vec_ir_tensor_tables.size(); i++) {
    vec_meta_tables.push_back(paddle::dialect::IrMetaTensor(&vec_ir_tensor_tables[i]));
  }

  std::vector<const phi::MetaTensor*> meta_tables;
  for (size_t i=0; i < static_cast<size_t>(vec_meta_tables.size()); i++) {
    meta_tables.push_back(&vec_meta_tables[i]);
  }
 
  paddle::dialect::IrMetaTensor meta_mask;
  paddle::dialect::IrTensor ir_tensor_mask;
  if (mask_.impl() != nullptr) {
    paddle::dialect::DenseTensorType mask = mask_.type().dyn_cast<paddle::dialect::DenseTensorType>();
    VLOG(4) << "Builder construction  dense_mask";
    ir_tensor_mask = paddle::dialect::IrTensor(paddle::dialect::TransToPhiDataType(mask.dtype()),
                                                        mask.dims(),
                                                        mask.data_layout(),
                                                        mask.lod(),
                                                        mask.offset());
    VLOG(4) << "Builder construction  meta_mask";
    meta_mask = paddle::dialect::IrMetaTensor(&ir_tensor_mask);
  }

  paddle::dialect::IrTensor dense_out;
  paddle::dialect::IrMetaTensor meta_out(&dense_out);
  paddle::dialect::IrTensor dense_seq_lod;
  paddle::dialect::IrMetaTensor meta_seq_lod(&dense_seq_lod);
  paddle::dialect::IrTensor dense_max_seq_len;
  paddle::dialect::IrMetaTensor meta_max_seq_len(&dense_max_seq_len);

  phi::EmbeddingWithEltwiseAddXPUInferMeta(meta_ids, meta_tables, meta_mask, &meta_out, &meta_seq_lod, &meta_max_seq_len);

  std::vector<pir::Type> argument_outputs;
  pir::Type out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_out.dtype()), dense_out.dims(), dense_out.layout(), dense_out.lod(), dense_out.offset());
  argument_outputs.push_back(out_dense_tensor_type);

  pir::Type seq_lod_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_seq_lod.dtype()), dense_seq_lod.dims(), dense_seq_lod.layout(), dense_seq_lod.lod(), dense_seq_lod.offset());
  argument_outputs.push_back(seq_lod_dense_tensor_type);

  pir::Type max_seq_len_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_max_seq_len.dtype()), dense_max_seq_len.dims(), dense_max_seq_len.layout(), dense_max_seq_len.lod(), dense_max_seq_len.offset());
  argument_outputs.push_back(max_seq_len_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void EmbeddingWithEltwiseAddXpuOp::VerifySig() {
  VLOG(4) << "Start Verifying inputs, outputs and attributes for: EmbeddingWithEltwiseAddXpuOp.";
  VLOG(4) << "Verifying inputs:";
  {
  auto input_size = num_operands();
  IR_ENFORCE(input_size == 3u,
                    "The size %d of inputs must be equal to 3.", input_size);
  if (auto vec_type = (*this)->operand_source(0).type().dyn_cast<pir::VectorType>()) {
      for (size_t i = 0; i < vec_type.size(); ++i) {
        IR_ENFORCE(vec_type[i].isa<paddle::dialect::DenseTensorType>(),
                       "Type validation failed for the 0th input, got %s.", (*this)->operand_source(0).type());
      }
  }
  else {
    IR_ENFORCE((*this)->operand_source(0).type().isa<paddle::dialect::DenseTensorType>(),
                   "Type validation failed for the 0th input, got %s.", (*this)->operand_source(0).type());
  }
  if (auto vec_type = (*this)->operand_source(1).type().dyn_cast<pir::VectorType>()) {
      for (size_t i = 0; i < vec_type.size(); ++i) {
        IR_ENFORCE(vec_type[i].isa<paddle::dialect::DenseTensorType>(),
                       "Type validation failed for the 1th input, got %s.", (*this)->operand_source(1).type());
      }
  }
  else {
    IR_ENFORCE((*this)->operand_source(1).type().isa<paddle::dialect::DenseTensorType>(),
                   "Type validation failed for the 1th input, got %s.", (*this)->operand_source(1).type());
  }
  if (auto val = (*this)->operand(2)) {
    IR_ENFORCE(val.type().isa<paddle::dialect::DenseTensorType>(),
                   "Type validation failed for the 2th input, got %s.", (*this)->operand_source(2).type());
  }
  }
  VLOG(4) << "Verifying attributes:";
  {
  auto& attributes = this->attributes();
  IR_ENFORCE(attributes.count("padding_idx")>0,
                 "padding_idx does not exist.");
  IR_ENFORCE(attributes.at("padding_idx").isa<pir::Int64Attribute>(),
                 "Type of attribute: padding_idx is not pir::Int64Attribute.");

  }
  VLOG(4) << "Verifying outputs:";
  {
  auto output_size = num_results();
  IR_ENFORCE(output_size == 3u,
                    "The size %d of outputs must be equal to 3.", output_size);
  IR_ENFORCE((*this)->result(0).type().isa<paddle::dialect::DenseTensorType>(),
                 "Type validation failed for the 0th output.");
  if (auto output_1_type = (*this)->result(1).type()) {
    IR_ENFORCE(output_1_type.isa<paddle::dialect::DenseTensorType>(),
                   "Type validation failed for the 1th output.");
  }
  if (auto output_2_type = (*this)->result(2).type()) {
    IR_ENFORCE(output_2_type.isa<paddle::dialect::DenseTensorType>(),
                   "Type validation failed for the 2th output.");
  }
  }
  VLOG(4) << "End Verifying for: EmbeddingWithEltwiseAddXpuOp.";
}

void EmbeddingWithEltwiseAddXpuOp::InferMeta( phi::InferMetaContext *infer_meta ) {
  auto fn = PD_INFER_META(phi::EmbeddingWithEltwiseAddXPUInferMeta);
  fn(infer_meta);
}

phi::DataType EmbeddingWithEltwiseAddXpuOp::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: EmbeddingWithEltwiseAddXpuOp";
  


  return expected_kernel_dtype;
}

const char *FastLayernormXpuOp::attributes_name[2] = { "begin_norm_axis", "epsilon" };

OpInfoTuple FastLayernormXpuOp::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("x", "paddle::dialect::DenseTensorType", false, false, false, false), paddle::dialect::OpInputInfo("scale", "paddle::dialect::DenseTensorType", false, false, false, false), paddle::dialect::OpInputInfo("bias", "paddle::dialect::DenseTensorType", false, false, false, false) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = { paddle::dialect::OpAttributeInfo("begin_norm_axis", "pir::Int32Attribute", ""), paddle::dialect::OpAttributeInfo("epsilon", "pir::FloatAttribute", "") };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("out", "paddle::dialect::DenseTensorType", false, false) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("FastLayernormXPUInferMeta", {"x", "scale", "bias", "begin_norm_axis", "epsilon"}, "fast_layernorm_xpu", {"x", "scale", "bias", "begin_norm_axis", "epsilon"}, {"x"}, {}, {}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "fast_layernorm_xpu");
}

void FastLayernormXpuOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, pir::Value scale_, pir::Value bias_, int begin_norm_axis, float epsilon) {
  VLOG(4) << "Start build FastLayernormXpuOp";



  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_, scale_, bias_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_begin_norm_axis = pir::Int32Attribute::get(pir::IrContext::Instance(), begin_norm_axis);
  argument.AddAttribute("begin_norm_axis", attr_begin_norm_axis);
  pir::Attribute attr_epsilon = pir::FloatAttribute::get(pir::IrContext::Instance(), epsilon);
  argument.AddAttribute("epsilon", attr_epsilon);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;
  paddle::dialect::DenseTensorType scale = scale_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)scale;
  paddle::dialect::DenseTensorType bias = bias_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)bias;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);

  VLOG(4) << "Builder construction  dense_scale";
  paddle::dialect::IrTensor ir_tensor_scale(paddle::dialect::TransToPhiDataType(scale.dtype()),
                                                      scale.dims(),
                                                      scale.data_layout(),
                                                      scale.lod(),
                                                      scale.offset());
  VLOG(4) << "Builder construction  meta_scale";
  paddle::dialect::IrMetaTensor meta_scale(&ir_tensor_scale);

  VLOG(4) << "Builder construction  dense_bias";
  paddle::dialect::IrTensor ir_tensor_bias(paddle::dialect::TransToPhiDataType(bias.dtype()),
                                                      bias.dims(),
                                                      bias.data_layout(),
                                                      bias.lod(),
                                                      bias.offset());
  VLOG(4) << "Builder construction  meta_bias";
  paddle::dialect::IrMetaTensor meta_bias(&ir_tensor_bias);
  paddle::dialect::IrTensor dense_out;
  paddle::dialect::IrMetaTensor meta_out(&dense_out);

  phi::FastLayernormXPUInferMeta(meta_x, meta_scale, meta_bias, begin_norm_axis, epsilon, &meta_out);

  std::vector<pir::Type> argument_outputs;
  pir::Type out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_out.dtype()), dense_out.dims(), dense_out.layout(), dense_out.lod(), dense_out.offset());
  argument_outputs.push_back(out_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void FastLayernormXpuOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, pir::Value scale_, pir::Value bias_, pir::AttributeMap attributes) {
  VLOG(4) << "Start build FastLayernormXpuOp";


  IR_ENFORCE(
      attributes.find("begin_norm_axis") != attributes.end(),
          "'begin_norm_axis' Attribute is expected for FastLayernormXpuOp. ");
  int begin_norm_axis = attributes.at("begin_norm_axis").dyn_cast<pir::Int32Attribute>().data();

  IR_ENFORCE(
      attributes.find("epsilon") != attributes.end(),
          "'epsilon' Attribute is expected for FastLayernormXpuOp. ");
  float epsilon = attributes.at("epsilon").dyn_cast<pir::FloatAttribute>().data();


  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_, scale_, bias_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_begin_norm_axis = pir::Int32Attribute::get(pir::IrContext::Instance(), begin_norm_axis);
  argument.AddAttribute("begin_norm_axis", attr_begin_norm_axis);
  pir::Attribute attr_epsilon = pir::FloatAttribute::get(pir::IrContext::Instance(), epsilon);
  argument.AddAttribute("epsilon", attr_epsilon);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;
  paddle::dialect::DenseTensorType scale = scale_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)scale;
  paddle::dialect::DenseTensorType bias = bias_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)bias;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);

  VLOG(4) << "Builder construction  dense_scale";
  paddle::dialect::IrTensor ir_tensor_scale(paddle::dialect::TransToPhiDataType(scale.dtype()),
                                                      scale.dims(),
                                                      scale.data_layout(),
                                                      scale.lod(),
                                                      scale.offset());
  VLOG(4) << "Builder construction  meta_scale";
  paddle::dialect::IrMetaTensor meta_scale(&ir_tensor_scale);

  VLOG(4) << "Builder construction  dense_bias";
  paddle::dialect::IrTensor ir_tensor_bias(paddle::dialect::TransToPhiDataType(bias.dtype()),
                                                      bias.dims(),
                                                      bias.data_layout(),
                                                      bias.lod(),
                                                      bias.offset());
  VLOG(4) << "Builder construction  meta_bias";
  paddle::dialect::IrMetaTensor meta_bias(&ir_tensor_bias);
  paddle::dialect::IrTensor dense_out;
  paddle::dialect::IrMetaTensor meta_out(&dense_out);

  phi::FastLayernormXPUInferMeta(meta_x, meta_scale, meta_bias, begin_norm_axis, epsilon, &meta_out);

  std::vector<pir::Type> argument_outputs;
  pir::Type out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_out.dtype()), dense_out.dims(), dense_out.layout(), dense_out.lod(), dense_out.offset());
  argument_outputs.push_back(out_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void FastLayernormXpuOp::VerifySig() {
  VLOG(4) << "Start Verifying inputs, outputs and attributes for: FastLayernormXpuOp.";
  VLOG(4) << "Verifying inputs:";
  {
  auto input_size = num_operands();
  IR_ENFORCE(input_size == 3u,
                    "The size %d of inputs must be equal to 3.", input_size);
  IR_ENFORCE((*this)->operand_source(0).type().isa<paddle::dialect::DenseTensorType>(),
                  "Type validation failed for the 0th input, got %s.", (*this)->operand_source(0).type());
  IR_ENFORCE((*this)->operand_source(1).type().isa<paddle::dialect::DenseTensorType>(),
                  "Type validation failed for the 1th input, got %s.", (*this)->operand_source(1).type());
  IR_ENFORCE((*this)->operand_source(2).type().isa<paddle::dialect::DenseTensorType>(),
                  "Type validation failed for the 2th input, got %s.", (*this)->operand_source(2).type());
  }
  VLOG(4) << "Verifying attributes:";
  {
  auto& attributes = this->attributes();
  IR_ENFORCE(attributes.count("begin_norm_axis")>0,
                 "begin_norm_axis does not exist.");
  IR_ENFORCE(attributes.at("begin_norm_axis").isa<pir::Int32Attribute>(),
                 "Type of attribute: begin_norm_axis is not pir::Int32Attribute.");

  IR_ENFORCE(attributes.count("epsilon")>0,
                 "epsilon does not exist.");
  IR_ENFORCE(attributes.at("epsilon").isa<pir::FloatAttribute>(),
                 "Type of attribute: epsilon is not pir::FloatAttribute.");

  }
  VLOG(4) << "Verifying outputs:";
  {
  auto output_size = num_results();
  IR_ENFORCE(output_size == 1u,
                    "The size %d of outputs must be equal to 1.", output_size);
  IR_ENFORCE((*this)->result(0).type().isa<paddle::dialect::DenseTensorType>(),
                 "Type validation failed for the 0th output.");
  }
  VLOG(4) << "End Verifying for: FastLayernormXpuOp.";
}

void FastLayernormXpuOp::InferMeta( phi::InferMetaContext *infer_meta ) {
  auto fn = PD_INFER_META(phi::FastLayernormXPUInferMeta);
  fn(infer_meta);
}

phi::DataType FastLayernormXpuOp::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: FastLayernormXpuOp";
  


  return expected_kernel_dtype;
}

OpInfoTuple FastWhereXpuOp::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("condition", "paddle::dialect::DenseTensorType", false, false, false, false), paddle::dialect::OpInputInfo("x", "paddle::dialect::DenseTensorType", false, false, false, false), paddle::dialect::OpInputInfo("y", "paddle::dialect::DenseTensorType", false, false, false, false) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = {  };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("out", "paddle::dialect::DenseTensorType", false, false) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("FastWhereXPUInferMeta", {"condition", "x", "y"}, "fast_where_xpu", {"condition", "x", "y"}, {"x"}, {}, {}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "fast_where_xpu");
}

void FastWhereXpuOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value condition_, pir::Value x_, pir::Value y_) {
  VLOG(4) << "Start build FastWhereXpuOp";



  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {condition_, x_, y_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType condition = condition_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)condition;
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;
  paddle::dialect::DenseTensorType y = y_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)y;

  VLOG(4) << "Builder construction  dense_condition";
  paddle::dialect::IrTensor ir_tensor_condition(paddle::dialect::TransToPhiDataType(condition.dtype()),
                                                      condition.dims(),
                                                      condition.data_layout(),
                                                      condition.lod(),
                                                      condition.offset());
  VLOG(4) << "Builder construction  meta_condition";
  paddle::dialect::IrMetaTensor meta_condition(&ir_tensor_condition);

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);

  VLOG(4) << "Builder construction  dense_y";
  paddle::dialect::IrTensor ir_tensor_y(paddle::dialect::TransToPhiDataType(y.dtype()),
                                                      y.dims(),
                                                      y.data_layout(),
                                                      y.lod(),
                                                      y.offset());
  VLOG(4) << "Builder construction  meta_y";
  paddle::dialect::IrMetaTensor meta_y(&ir_tensor_y);
  paddle::dialect::IrTensor dense_out;
  paddle::dialect::IrMetaTensor meta_out(&dense_out);

  phi::FastWhereXPUInferMeta(meta_condition, meta_x, meta_y, &meta_out);

  std::vector<pir::Type> argument_outputs;
  pir::Type out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_out.dtype()), dense_out.dims(), dense_out.layout(), dense_out.lod(), dense_out.offset());
  argument_outputs.push_back(out_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void FastWhereXpuOp::VerifySig() {
  VLOG(4) << "Start Verifying inputs, outputs and attributes for: FastWhereXpuOp.";
  VLOG(4) << "Verifying inputs:";
  {
  auto input_size = num_operands();
  IR_ENFORCE(input_size == 3u,
                    "The size %d of inputs must be equal to 3.", input_size);
  IR_ENFORCE((*this)->operand_source(0).type().isa<paddle::dialect::DenseTensorType>(),
                  "Type validation failed for the 0th input, got %s.", (*this)->operand_source(0).type());
  IR_ENFORCE((*this)->operand_source(1).type().isa<paddle::dialect::DenseTensorType>(),
                  "Type validation failed for the 1th input, got %s.", (*this)->operand_source(1).type());
  IR_ENFORCE((*this)->operand_source(2).type().isa<paddle::dialect::DenseTensorType>(),
                  "Type validation failed for the 2th input, got %s.", (*this)->operand_source(2).type());
  }
  VLOG(4) << "Verifying attributes:";
  {
  // Attributes num is 0, not need to check attributes type.
  }
  VLOG(4) << "Verifying outputs:";
  {
  auto output_size = num_results();
  IR_ENFORCE(output_size == 1u,
                    "The size %d of outputs must be equal to 1.", output_size);
  IR_ENFORCE((*this)->result(0).type().isa<paddle::dialect::DenseTensorType>(),
                 "Type validation failed for the 0th output.");
  }
  VLOG(4) << "End Verifying for: FastWhereXpuOp.";
}

void FastWhereXpuOp::InferMeta( phi::InferMetaContext *infer_meta ) {
  auto fn = PD_INFER_META(phi::FastWhereXPUInferMeta);
  fn(infer_meta);
}

phi::DataType FastWhereXpuOp::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: FastWhereXpuOp";
  


  return expected_kernel_dtype;
}

const char *FcOp::attributes_name[3] = { "in_num_col_dims", "activation_type", "padding_weights" };

OpInfoTuple FcOp::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("input", "paddle::dialect::DenseTensorType", false, false, false, false), paddle::dialect::OpInputInfo("w", "paddle::dialect::DenseTensorType", false, false, false, false), paddle::dialect::OpInputInfo("bias", "paddle::dialect::DenseTensorType", true, false, false, false) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = { paddle::dialect::OpAttributeInfo("in_num_col_dims", "pir::Int32Attribute", ""), paddle::dialect::OpAttributeInfo("activation_type", "pir::StrAttribute", ""), paddle::dialect::OpAttributeInfo("padding_weights", "pir::BoolAttribute", "") };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("out", "paddle::dialect::DenseTensorType", false, false) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("FCInferMeta", {"input", "w", "bias", "in_num_col_dims", "activation_type", "padding_weights"}, "fc", {"input", "w", "bias", "in_num_col_dims", "activation_type", "padding_weights"}, {"input"}, {}, {}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "fc");
}

void FcOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value input_, pir::Value w_, pir::Value bias_, int in_num_col_dims, const std::string& activation_type, bool padding_weights) {
  VLOG(4) << "Start build FcOp";



  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {input_, w_, bias_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_in_num_col_dims = pir::Int32Attribute::get(pir::IrContext::Instance(), in_num_col_dims);
  argument.AddAttribute("in_num_col_dims", attr_in_num_col_dims);
  pir::Attribute attr_activation_type = pir::StrAttribute::get(pir::IrContext::Instance(), activation_type);
  argument.AddAttribute("activation_type", attr_activation_type);
  pir::Attribute attr_padding_weights = pir::BoolAttribute::get(pir::IrContext::Instance(), padding_weights);
  argument.AddAttribute("padding_weights", attr_padding_weights);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType input = input_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)input;
  paddle::dialect::DenseTensorType w = w_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)w;

  VLOG(4) << "Builder construction  dense_input";
  paddle::dialect::IrTensor ir_tensor_input(paddle::dialect::TransToPhiDataType(input.dtype()),
                                                      input.dims(),
                                                      input.data_layout(),
                                                      input.lod(),
                                                      input.offset());
  VLOG(4) << "Builder construction  meta_input";
  paddle::dialect::IrMetaTensor meta_input(&ir_tensor_input);

  VLOG(4) << "Builder construction  dense_w";
  paddle::dialect::IrTensor ir_tensor_w(paddle::dialect::TransToPhiDataType(w.dtype()),
                                                      w.dims(),
                                                      w.data_layout(),
                                                      w.lod(),
                                                      w.offset());
  VLOG(4) << "Builder construction  meta_w";
  paddle::dialect::IrMetaTensor meta_w(&ir_tensor_w);

  paddle::dialect::IrMetaTensor meta_bias;
  paddle::dialect::IrTensor ir_tensor_bias;
  if (bias_.impl() != nullptr) {
    paddle::dialect::DenseTensorType bias = bias_.type().dyn_cast<paddle::dialect::DenseTensorType>();
    VLOG(4) << "Builder construction  dense_bias";
    ir_tensor_bias = paddle::dialect::IrTensor(paddle::dialect::TransToPhiDataType(bias.dtype()),
                                                        bias.dims(),
                                                        bias.data_layout(),
                                                        bias.lod(),
                                                        bias.offset());
    VLOG(4) << "Builder construction  meta_bias";
    meta_bias = paddle::dialect::IrMetaTensor(&ir_tensor_bias);
  }

  paddle::dialect::IrTensor dense_out;
  paddle::dialect::IrMetaTensor meta_out(&dense_out);

  phi::FCInferMeta(meta_input, meta_w, meta_bias, in_num_col_dims, activation_type, padding_weights, &meta_out);

  std::vector<pir::Type> argument_outputs;
  pir::Type out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_out.dtype()), dense_out.dims(), dense_out.layout(), dense_out.lod(), dense_out.offset());
  argument_outputs.push_back(out_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void FcOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value input_, pir::Value w_, pir::Value bias_, pir::AttributeMap attributes) {
  VLOG(4) << "Start build FcOp";


  IR_ENFORCE(
      attributes.find("in_num_col_dims") != attributes.end(),
          "'in_num_col_dims' Attribute is expected for FcOp. ");
  int in_num_col_dims = attributes.at("in_num_col_dims").dyn_cast<pir::Int32Attribute>().data();

  IR_ENFORCE(
      attributes.find("activation_type") != attributes.end(),
          "'activation_type' Attribute is expected for FcOp. ");
  std::string activation_type = attributes.at("activation_type").dyn_cast<pir::StrAttribute>().AsString();

  IR_ENFORCE(
      attributes.find("padding_weights") != attributes.end(),
          "'padding_weights' Attribute is expected for FcOp. ");
  bool padding_weights = attributes.at("padding_weights").dyn_cast<pir::BoolAttribute>().data();


  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {input_, w_, bias_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_in_num_col_dims = pir::Int32Attribute::get(pir::IrContext::Instance(), in_num_col_dims);
  argument.AddAttribute("in_num_col_dims", attr_in_num_col_dims);
  pir::Attribute attr_activation_type = pir::StrAttribute::get(pir::IrContext::Instance(), activation_type);
  argument.AddAttribute("activation_type", attr_activation_type);
  pir::Attribute attr_padding_weights = pir::BoolAttribute::get(pir::IrContext::Instance(), padding_weights);
  argument.AddAttribute("padding_weights", attr_padding_weights);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType input = input_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)input;
  paddle::dialect::DenseTensorType w = w_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)w;

  VLOG(4) << "Builder construction  dense_input";
  paddle::dialect::IrTensor ir_tensor_input(paddle::dialect::TransToPhiDataType(input.dtype()),
                                                      input.dims(),
                                                      input.data_layout(),
                                                      input.lod(),
                                                      input.offset());
  VLOG(4) << "Builder construction  meta_input";
  paddle::dialect::IrMetaTensor meta_input(&ir_tensor_input);

  VLOG(4) << "Builder construction  dense_w";
  paddle::dialect::IrTensor ir_tensor_w(paddle::dialect::TransToPhiDataType(w.dtype()),
                                                      w.dims(),
                                                      w.data_layout(),
                                                      w.lod(),
                                                      w.offset());
  VLOG(4) << "Builder construction  meta_w";
  paddle::dialect::IrMetaTensor meta_w(&ir_tensor_w);

  paddle::dialect::IrMetaTensor meta_bias;
  paddle::dialect::IrTensor ir_tensor_bias;
  if (bias_.impl() != nullptr) {
    paddle::dialect::DenseTensorType bias = bias_.type().dyn_cast<paddle::dialect::DenseTensorType>();
    VLOG(4) << "Builder construction  dense_bias";
    ir_tensor_bias = paddle::dialect::IrTensor(paddle::dialect::TransToPhiDataType(bias.dtype()),
                                                        bias.dims(),
                                                        bias.data_layout(),
                                                        bias.lod(),
                                                        bias.offset());
    VLOG(4) << "Builder construction  meta_bias";
    meta_bias = paddle::dialect::IrMetaTensor(&ir_tensor_bias);
  }

  paddle::dialect::IrTensor dense_out;
  paddle::dialect::IrMetaTensor meta_out(&dense_out);

  phi::FCInferMeta(meta_input, meta_w, meta_bias, in_num_col_dims, activation_type, padding_weights, &meta_out);

  std::vector<pir::Type> argument_outputs;
  pir::Type out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_out.dtype()), dense_out.dims(), dense_out.layout(), dense_out.lod(), dense_out.offset());
  argument_outputs.push_back(out_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void FcOp::VerifySig() {
  VLOG(4) << "Start Verifying inputs, outputs and attributes for: FcOp.";
  VLOG(4) << "Verifying inputs:";
  {
  auto input_size = num_operands();
  IR_ENFORCE(input_size == 3u,
                    "The size %d of inputs must be equal to 3.", input_size);
  IR_ENFORCE((*this)->operand_source(0).type().isa<paddle::dialect::DenseTensorType>(),
                  "Type validation failed for the 0th input, got %s.", (*this)->operand_source(0).type());
  IR_ENFORCE((*this)->operand_source(1).type().isa<paddle::dialect::DenseTensorType>(),
                  "Type validation failed for the 1th input, got %s.", (*this)->operand_source(1).type());
  if (auto val = (*this)->operand(2)) {
    IR_ENFORCE(val.type().isa<paddle::dialect::DenseTensorType>(),
                   "Type validation failed for the 2th input, got %s.", (*this)->operand_source(2).type());
  }
  }
  VLOG(4) << "Verifying attributes:";
  {
  auto& attributes = this->attributes();
  IR_ENFORCE(attributes.count("in_num_col_dims")>0,
                 "in_num_col_dims does not exist.");
  IR_ENFORCE(attributes.at("in_num_col_dims").isa<pir::Int32Attribute>(),
                 "Type of attribute: in_num_col_dims is not pir::Int32Attribute.");

  IR_ENFORCE(attributes.count("activation_type")>0,
                 "activation_type does not exist.");
  IR_ENFORCE(attributes.at("activation_type").isa<pir::StrAttribute>(),
                 "Type of attribute: activation_type is not pir::StrAttribute.");

  IR_ENFORCE(attributes.count("padding_weights")>0,
                 "padding_weights does not exist.");
  IR_ENFORCE(attributes.at("padding_weights").isa<pir::BoolAttribute>(),
                 "Type of attribute: padding_weights is not pir::BoolAttribute.");

  }
  VLOG(4) << "Verifying outputs:";
  {
  auto output_size = num_results();
  IR_ENFORCE(output_size == 1u,
                    "The size %d of outputs must be equal to 1.", output_size);
  IR_ENFORCE((*this)->result(0).type().isa<paddle::dialect::DenseTensorType>(),
                 "Type validation failed for the 0th output.");
  }
  VLOG(4) << "End Verifying for: FcOp.";
}

void FcOp::InferMeta( phi::InferMetaContext *infer_meta ) {
  auto fn = PD_INFER_META(phi::FCInferMeta);
  fn(infer_meta);
}

phi::DataType FcOp::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: FcOp";
  


  return expected_kernel_dtype;
}

const char *FcXpuOp::attributes_name[7] = { "in_num_col_dims", "transpose_x", "alpha", "beta", "act_type", "act_alpha", "out_dtype" };

OpInfoTuple FcXpuOp::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("x", "paddle::dialect::DenseTensorType", false, false, false, false), paddle::dialect::OpInputInfo("x_max", "paddle::dialect::DenseTensorType", true, false, false, false), paddle::dialect::OpInputInfo("w", "paddle::dialect::DenseTensorType", false, false, false, false), paddle::dialect::OpInputInfo("w_max", "paddle::dialect::DenseTensorType", false, false, false, false), paddle::dialect::OpInputInfo("bias", "paddle::dialect::DenseTensorType", true, false, false, false), paddle::dialect::OpInputInfo("scale_max", "paddle::dialect::DenseTensorType", true, false, false, false), paddle::dialect::OpInputInfo("out_max_in", "paddle::dialect::DenseTensorType", true, false, false, false) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = { paddle::dialect::OpAttributeInfo("in_num_col_dims", "pir::Int32Attribute", ""), paddle::dialect::OpAttributeInfo("transpose_x", "pir::BoolAttribute", ""), paddle::dialect::OpAttributeInfo("alpha", "pir::FloatAttribute", ""), paddle::dialect::OpAttributeInfo("beta", "pir::FloatAttribute", ""), paddle::dialect::OpAttributeInfo("act_type", "pir::Int32Attribute", ""), paddle::dialect::OpAttributeInfo("act_alpha", "pir::FloatAttribute", ""), paddle::dialect::OpAttributeInfo("out_dtype", "paddle::dialect::DataTypeAttribute", "") };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("out", "paddle::dialect::DenseTensorType", false, false), paddle::dialect::OpOutputInfo("out_max", "paddle::dialect::DenseTensorType", false, false) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("FcXPUInferMeta", {"x", "x_max", "w", "w_max", "bias", "scale_max", "out_max_in", "in_num_col_dims", "transpose_x", "alpha", "beta", "act_type", "act_alpha", "out_dtype"}, "fc_xpu", {"x", "x_max", "w", "w_max", "bias", "scale_max", "out_max_in", "in_num_col_dims", "transpose_x", "alpha", "beta", "act_type", "act_alpha", "out_dtype"}, {"x"}, {}, {}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "fc_xpu");
}

void FcXpuOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, pir::Value x_max_, pir::Value w_, pir::Value w_max_, pir::Value bias_, pir::Value scale_max_, pir::Value out_max_in_, int in_num_col_dims, bool transpose_x, float alpha, float beta, int act_type, float act_alpha, phi::DataType out_dtype) {
  VLOG(4) << "Start build FcXpuOp";



  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_, x_max_, w_, w_max_, bias_, scale_max_, out_max_in_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_in_num_col_dims = pir::Int32Attribute::get(pir::IrContext::Instance(), in_num_col_dims);
  argument.AddAttribute("in_num_col_dims", attr_in_num_col_dims);
  pir::Attribute attr_transpose_x = pir::BoolAttribute::get(pir::IrContext::Instance(), transpose_x);
  argument.AddAttribute("transpose_x", attr_transpose_x);
  pir::Attribute attr_alpha = pir::FloatAttribute::get(pir::IrContext::Instance(), alpha);
  argument.AddAttribute("alpha", attr_alpha);
  pir::Attribute attr_beta = pir::FloatAttribute::get(pir::IrContext::Instance(), beta);
  argument.AddAttribute("beta", attr_beta);
  pir::Attribute attr_act_type = pir::Int32Attribute::get(pir::IrContext::Instance(), act_type);
  argument.AddAttribute("act_type", attr_act_type);
  pir::Attribute attr_act_alpha = pir::FloatAttribute::get(pir::IrContext::Instance(), act_alpha);
  argument.AddAttribute("act_alpha", attr_act_alpha);
  pir::Attribute attr_out_dtype = paddle::dialect::DataTypeAttribute::get(pir::IrContext::Instance(), out_dtype);
  argument.AddAttribute("out_dtype", attr_out_dtype);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;
  paddle::dialect::DenseTensorType w = w_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)w;
  paddle::dialect::DenseTensorType w_max = w_max_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)w_max;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);

  paddle::dialect::IrMetaTensor meta_x_max;
  paddle::dialect::IrTensor ir_tensor_x_max;
  if (x_max_.impl() != nullptr) {
    paddle::dialect::DenseTensorType x_max = x_max_.type().dyn_cast<paddle::dialect::DenseTensorType>();
    VLOG(4) << "Builder construction  dense_x_max";
    ir_tensor_x_max = paddle::dialect::IrTensor(paddle::dialect::TransToPhiDataType(x_max.dtype()),
                                                        x_max.dims(),
                                                        x_max.data_layout(),
                                                        x_max.lod(),
                                                        x_max.offset());
    VLOG(4) << "Builder construction  meta_x_max";
    meta_x_max = paddle::dialect::IrMetaTensor(&ir_tensor_x_max);
  }


  VLOG(4) << "Builder construction  dense_w";
  paddle::dialect::IrTensor ir_tensor_w(paddle::dialect::TransToPhiDataType(w.dtype()),
                                                      w.dims(),
                                                      w.data_layout(),
                                                      w.lod(),
                                                      w.offset());
  VLOG(4) << "Builder construction  meta_w";
  paddle::dialect::IrMetaTensor meta_w(&ir_tensor_w);

  VLOG(4) << "Builder construction  dense_w_max";
  paddle::dialect::IrTensor ir_tensor_w_max(paddle::dialect::TransToPhiDataType(w_max.dtype()),
                                                      w_max.dims(),
                                                      w_max.data_layout(),
                                                      w_max.lod(),
                                                      w_max.offset());
  VLOG(4) << "Builder construction  meta_w_max";
  paddle::dialect::IrMetaTensor meta_w_max(&ir_tensor_w_max);

  paddle::dialect::IrMetaTensor meta_bias;
  paddle::dialect::IrTensor ir_tensor_bias;
  if (bias_.impl() != nullptr) {
    paddle::dialect::DenseTensorType bias = bias_.type().dyn_cast<paddle::dialect::DenseTensorType>();
    VLOG(4) << "Builder construction  dense_bias";
    ir_tensor_bias = paddle::dialect::IrTensor(paddle::dialect::TransToPhiDataType(bias.dtype()),
                                                        bias.dims(),
                                                        bias.data_layout(),
                                                        bias.lod(),
                                                        bias.offset());
    VLOG(4) << "Builder construction  meta_bias";
    meta_bias = paddle::dialect::IrMetaTensor(&ir_tensor_bias);
  }


  paddle::dialect::IrMetaTensor meta_scale_max;
  paddle::dialect::IrTensor ir_tensor_scale_max;
  if (scale_max_.impl() != nullptr) {
    paddle::dialect::DenseTensorType scale_max = scale_max_.type().dyn_cast<paddle::dialect::DenseTensorType>();
    VLOG(4) << "Builder construction  dense_scale_max";
    ir_tensor_scale_max = paddle::dialect::IrTensor(paddle::dialect::TransToPhiDataType(scale_max.dtype()),
                                                        scale_max.dims(),
                                                        scale_max.data_layout(),
                                                        scale_max.lod(),
                                                        scale_max.offset());
    VLOG(4) << "Builder construction  meta_scale_max";
    meta_scale_max = paddle::dialect::IrMetaTensor(&ir_tensor_scale_max);
  }


  paddle::dialect::IrMetaTensor meta_out_max_in;
  paddle::dialect::IrTensor ir_tensor_out_max_in;
  if (out_max_in_.impl() != nullptr) {
    paddle::dialect::DenseTensorType out_max_in = out_max_in_.type().dyn_cast<paddle::dialect::DenseTensorType>();
    VLOG(4) << "Builder construction  dense_out_max_in";
    ir_tensor_out_max_in = paddle::dialect::IrTensor(paddle::dialect::TransToPhiDataType(out_max_in.dtype()),
                                                        out_max_in.dims(),
                                                        out_max_in.data_layout(),
                                                        out_max_in.lod(),
                                                        out_max_in.offset());
    VLOG(4) << "Builder construction  meta_out_max_in";
    meta_out_max_in = paddle::dialect::IrMetaTensor(&ir_tensor_out_max_in);
  }

  paddle::dialect::IrTensor dense_out;
  paddle::dialect::IrMetaTensor meta_out(&dense_out);
  paddle::dialect::IrTensor dense_out_max;
  paddle::dialect::IrMetaTensor meta_out_max(&dense_out_max);

  phi::FcXPUInferMeta(meta_x, meta_x_max, meta_w, meta_w_max, meta_bias, meta_scale_max, meta_out_max_in, in_num_col_dims, transpose_x, alpha, beta, act_type, act_alpha, out_dtype, &meta_out, &meta_out_max);

  std::vector<pir::Type> argument_outputs;
  pir::Type out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_out.dtype()), dense_out.dims(), dense_out.layout(), dense_out.lod(), dense_out.offset());
  argument_outputs.push_back(out_dense_tensor_type);

  pir::Type out_max_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_out_max.dtype()), dense_out_max.dims(), dense_out_max.layout(), dense_out_max.lod(), dense_out_max.offset());
  argument_outputs.push_back(out_max_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void FcXpuOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, pir::Value x_max_, pir::Value w_, pir::Value w_max_, pir::Value bias_, pir::Value scale_max_, pir::Value out_max_in_, pir::AttributeMap attributes) {
  VLOG(4) << "Start build FcXpuOp";


  IR_ENFORCE(
      attributes.find("in_num_col_dims") != attributes.end(),
          "'in_num_col_dims' Attribute is expected for FcXpuOp. ");
  int in_num_col_dims = attributes.at("in_num_col_dims").dyn_cast<pir::Int32Attribute>().data();

  IR_ENFORCE(
      attributes.find("transpose_x") != attributes.end(),
          "'transpose_x' Attribute is expected for FcXpuOp. ");
  bool transpose_x = attributes.at("transpose_x").dyn_cast<pir::BoolAttribute>().data();

  IR_ENFORCE(
      attributes.find("alpha") != attributes.end(),
          "'alpha' Attribute is expected for FcXpuOp. ");
  float alpha = attributes.at("alpha").dyn_cast<pir::FloatAttribute>().data();

  IR_ENFORCE(
      attributes.find("beta") != attributes.end(),
          "'beta' Attribute is expected for FcXpuOp. ");
  float beta = attributes.at("beta").dyn_cast<pir::FloatAttribute>().data();

  IR_ENFORCE(
      attributes.find("act_type") != attributes.end(),
          "'act_type' Attribute is expected for FcXpuOp. ");
  int act_type = attributes.at("act_type").dyn_cast<pir::Int32Attribute>().data();

  IR_ENFORCE(
      attributes.find("act_alpha") != attributes.end(),
          "'act_alpha' Attribute is expected for FcXpuOp. ");
  float act_alpha = attributes.at("act_alpha").dyn_cast<pir::FloatAttribute>().data();

  IR_ENFORCE(
      attributes.find("out_dtype") != attributes.end(),
          "'out_dtype' Attribute is expected for FcXpuOp. ");
  phi::DataType out_dtype = attributes.at("out_dtype").dyn_cast<paddle::dialect::DataTypeAttribute>().data();


  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_, x_max_, w_, w_max_, bias_, scale_max_, out_max_in_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_in_num_col_dims = pir::Int32Attribute::get(pir::IrContext::Instance(), in_num_col_dims);
  argument.AddAttribute("in_num_col_dims", attr_in_num_col_dims);
  pir::Attribute attr_transpose_x = pir::BoolAttribute::get(pir::IrContext::Instance(), transpose_x);
  argument.AddAttribute("transpose_x", attr_transpose_x);
  pir::Attribute attr_alpha = pir::FloatAttribute::get(pir::IrContext::Instance(), alpha);
  argument.AddAttribute("alpha", attr_alpha);
  pir::Attribute attr_beta = pir::FloatAttribute::get(pir::IrContext::Instance(), beta);
  argument.AddAttribute("beta", attr_beta);
  pir::Attribute attr_act_type = pir::Int32Attribute::get(pir::IrContext::Instance(), act_type);
  argument.AddAttribute("act_type", attr_act_type);
  pir::Attribute attr_act_alpha = pir::FloatAttribute::get(pir::IrContext::Instance(), act_alpha);
  argument.AddAttribute("act_alpha", attr_act_alpha);
  pir::Attribute attr_out_dtype = paddle::dialect::DataTypeAttribute::get(pir::IrContext::Instance(), out_dtype);
  argument.AddAttribute("out_dtype", attr_out_dtype);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;
  paddle::dialect::DenseTensorType w = w_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)w;
  paddle::dialect::DenseTensorType w_max = w_max_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)w_max;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);

  paddle::dialect::IrMetaTensor meta_x_max;
  paddle::dialect::IrTensor ir_tensor_x_max;
  if (x_max_.impl() != nullptr) {
    paddle::dialect::DenseTensorType x_max = x_max_.type().dyn_cast<paddle::dialect::DenseTensorType>();
    VLOG(4) << "Builder construction  dense_x_max";
    ir_tensor_x_max = paddle::dialect::IrTensor(paddle::dialect::TransToPhiDataType(x_max.dtype()),
                                                        x_max.dims(),
                                                        x_max.data_layout(),
                                                        x_max.lod(),
                                                        x_max.offset());
    VLOG(4) << "Builder construction  meta_x_max";
    meta_x_max = paddle::dialect::IrMetaTensor(&ir_tensor_x_max);
  }


  VLOG(4) << "Builder construction  dense_w";
  paddle::dialect::IrTensor ir_tensor_w(paddle::dialect::TransToPhiDataType(w.dtype()),
                                                      w.dims(),
                                                      w.data_layout(),
                                                      w.lod(),
                                                      w.offset());
  VLOG(4) << "Builder construction  meta_w";
  paddle::dialect::IrMetaTensor meta_w(&ir_tensor_w);

  VLOG(4) << "Builder construction  dense_w_max";
  paddle::dialect::IrTensor ir_tensor_w_max(paddle::dialect::TransToPhiDataType(w_max.dtype()),
                                                      w_max.dims(),
                                                      w_max.data_layout(),
                                                      w_max.lod(),
                                                      w_max.offset());
  VLOG(4) << "Builder construction  meta_w_max";
  paddle::dialect::IrMetaTensor meta_w_max(&ir_tensor_w_max);

  paddle::dialect::IrMetaTensor meta_bias;
  paddle::dialect::IrTensor ir_tensor_bias;
  if (bias_.impl() != nullptr) {
    paddle::dialect::DenseTensorType bias = bias_.type().dyn_cast<paddle::dialect::DenseTensorType>();
    VLOG(4) << "Builder construction  dense_bias";
    ir_tensor_bias = paddle::dialect::IrTensor(paddle::dialect::TransToPhiDataType(bias.dtype()),
                                                        bias.dims(),
                                                        bias.data_layout(),
                                                        bias.lod(),
                                                        bias.offset());
    VLOG(4) << "Builder construction  meta_bias";
    meta_bias = paddle::dialect::IrMetaTensor(&ir_tensor_bias);
  }


  paddle::dialect::IrMetaTensor meta_scale_max;
  paddle::dialect::IrTensor ir_tensor_scale_max;
  if (scale_max_.impl() != nullptr) {
    paddle::dialect::DenseTensorType scale_max = scale_max_.type().dyn_cast<paddle::dialect::DenseTensorType>();
    VLOG(4) << "Builder construction  dense_scale_max";
    ir_tensor_scale_max = paddle::dialect::IrTensor(paddle::dialect::TransToPhiDataType(scale_max.dtype()),
                                                        scale_max.dims(),
                                                        scale_max.data_layout(),
                                                        scale_max.lod(),
                                                        scale_max.offset());
    VLOG(4) << "Builder construction  meta_scale_max";
    meta_scale_max = paddle::dialect::IrMetaTensor(&ir_tensor_scale_max);
  }


  paddle::dialect::IrMetaTensor meta_out_max_in;
  paddle::dialect::IrTensor ir_tensor_out_max_in;
  if (out_max_in_.impl() != nullptr) {
    paddle::dialect::DenseTensorType out_max_in = out_max_in_.type().dyn_cast<paddle::dialect::DenseTensorType>();
    VLOG(4) << "Builder construction  dense_out_max_in";
    ir_tensor_out_max_in = paddle::dialect::IrTensor(paddle::dialect::TransToPhiDataType(out_max_in.dtype()),
                                                        out_max_in.dims(),
                                                        out_max_in.data_layout(),
                                                        out_max_in.lod(),
                                                        out_max_in.offset());
    VLOG(4) << "Builder construction  meta_out_max_in";
    meta_out_max_in = paddle::dialect::IrMetaTensor(&ir_tensor_out_max_in);
  }

  paddle::dialect::IrTensor dense_out;
  paddle::dialect::IrMetaTensor meta_out(&dense_out);
  paddle::dialect::IrTensor dense_out_max;
  paddle::dialect::IrMetaTensor meta_out_max(&dense_out_max);

  phi::FcXPUInferMeta(meta_x, meta_x_max, meta_w, meta_w_max, meta_bias, meta_scale_max, meta_out_max_in, in_num_col_dims, transpose_x, alpha, beta, act_type, act_alpha, out_dtype, &meta_out, &meta_out_max);

  std::vector<pir::Type> argument_outputs;
  pir::Type out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_out.dtype()), dense_out.dims(), dense_out.layout(), dense_out.lod(), dense_out.offset());
  argument_outputs.push_back(out_dense_tensor_type);

  pir::Type out_max_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_out_max.dtype()), dense_out_max.dims(), dense_out_max.layout(), dense_out_max.lod(), dense_out_max.offset());
  argument_outputs.push_back(out_max_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void FcXpuOp::VerifySig() {
  VLOG(4) << "Start Verifying inputs, outputs and attributes for: FcXpuOp.";
  VLOG(4) << "Verifying inputs:";
  {
  auto input_size = num_operands();
  IR_ENFORCE(input_size == 7u,
                    "The size %d of inputs must be equal to 7.", input_size);
  IR_ENFORCE((*this)->operand_source(0).type().isa<paddle::dialect::DenseTensorType>(),
                  "Type validation failed for the 0th input, got %s.", (*this)->operand_source(0).type());
  if (auto val = (*this)->operand(1)) {
    IR_ENFORCE(val.type().isa<paddle::dialect::DenseTensorType>(),
                   "Type validation failed for the 1th input, got %s.", (*this)->operand_source(1).type());
  }
  IR_ENFORCE((*this)->operand_source(2).type().isa<paddle::dialect::DenseTensorType>(),
                  "Type validation failed for the 2th input, got %s.", (*this)->operand_source(2).type());
  IR_ENFORCE((*this)->operand_source(3).type().isa<paddle::dialect::DenseTensorType>(),
                  "Type validation failed for the 3th input, got %s.", (*this)->operand_source(3).type());
  if (auto val = (*this)->operand(4)) {
    IR_ENFORCE(val.type().isa<paddle::dialect::DenseTensorType>(),
                   "Type validation failed for the 4th input, got %s.", (*this)->operand_source(4).type());
  }
  if (auto val = (*this)->operand(5)) {
    IR_ENFORCE(val.type().isa<paddle::dialect::DenseTensorType>(),
                   "Type validation failed for the 5th input, got %s.", (*this)->operand_source(5).type());
  }
  if (auto val = (*this)->operand(6)) {
    IR_ENFORCE(val.type().isa<paddle::dialect::DenseTensorType>(),
                   "Type validation failed for the 6th input, got %s.", (*this)->operand_source(6).type());
  }
  }
  VLOG(4) << "Verifying attributes:";
  {
  auto& attributes = this->attributes();
  IR_ENFORCE(attributes.count("in_num_col_dims")>0,
                 "in_num_col_dims does not exist.");
  IR_ENFORCE(attributes.at("in_num_col_dims").isa<pir::Int32Attribute>(),
                 "Type of attribute: in_num_col_dims is not pir::Int32Attribute.");

  IR_ENFORCE(attributes.count("transpose_x")>0,
                 "transpose_x does not exist.");
  IR_ENFORCE(attributes.at("transpose_x").isa<pir::BoolAttribute>(),
                 "Type of attribute: transpose_x is not pir::BoolAttribute.");

  IR_ENFORCE(attributes.count("alpha")>0,
                 "alpha does not exist.");
  IR_ENFORCE(attributes.at("alpha").isa<pir::FloatAttribute>(),
                 "Type of attribute: alpha is not pir::FloatAttribute.");

  IR_ENFORCE(attributes.count("beta")>0,
                 "beta does not exist.");
  IR_ENFORCE(attributes.at("beta").isa<pir::FloatAttribute>(),
                 "Type of attribute: beta is not pir::FloatAttribute.");

  IR_ENFORCE(attributes.count("act_type")>0,
                 "act_type does not exist.");
  IR_ENFORCE(attributes.at("act_type").isa<pir::Int32Attribute>(),
                 "Type of attribute: act_type is not pir::Int32Attribute.");

  IR_ENFORCE(attributes.count("act_alpha")>0,
                 "act_alpha does not exist.");
  IR_ENFORCE(attributes.at("act_alpha").isa<pir::FloatAttribute>(),
                 "Type of attribute: act_alpha is not pir::FloatAttribute.");

  IR_ENFORCE(attributes.count("out_dtype")>0,
                 "out_dtype does not exist.");
  IR_ENFORCE(attributes.at("out_dtype").isa<paddle::dialect::DataTypeAttribute>(),
                 "Type of attribute: out_dtype is not paddle::dialect::DataTypeAttribute.");

  }
  VLOG(4) << "Verifying outputs:";
  {
  auto output_size = num_results();
  IR_ENFORCE(output_size == 2u,
                    "The size %d of outputs must be equal to 2.", output_size);
  IR_ENFORCE((*this)->result(0).type().isa<paddle::dialect::DenseTensorType>(),
                 "Type validation failed for the 0th output.");
  IR_ENFORCE((*this)->result(1).type().isa<paddle::dialect::DenseTensorType>(),
                 "Type validation failed for the 1th output.");
  }
  VLOG(4) << "End Verifying for: FcXpuOp.";
}

void FcXpuOp::InferMeta( phi::InferMetaContext *infer_meta ) {
  auto fn = PD_INFER_META(phi::FcXPUInferMeta);
  fn(infer_meta);
}

phi::DataType FcXpuOp::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: FcXpuOp";
  


  return expected_kernel_dtype;
}

const char *FusedBiasActOp::attributes_name[6] = { "act_method", "compute_dtype", "quant_scale", "quant_round_type", "quant_max_bound", "quant_min_bound" };

OpInfoTuple FusedBiasActOp::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("x", "paddle::dialect::DenseTensorType", false, false, false, false), paddle::dialect::OpInputInfo("bias", "paddle::dialect::DenseTensorType", true, false, false, false), paddle::dialect::OpInputInfo("dequant_scales", "paddle::dialect::DenseTensorType", true, false, false, false), paddle::dialect::OpInputInfo("shift", "paddle::dialect::DenseTensorType", true, false, false, false), paddle::dialect::OpInputInfo("smooth", "paddle::dialect::DenseTensorType", true, false, false, false) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = { paddle::dialect::OpAttributeInfo("act_method", "pir::StrAttribute", ""), paddle::dialect::OpAttributeInfo("compute_dtype", "pir::StrAttribute", ""), paddle::dialect::OpAttributeInfo("quant_scale", "pir::FloatAttribute", ""), paddle::dialect::OpAttributeInfo("quant_round_type", "pir::Int32Attribute", ""), paddle::dialect::OpAttributeInfo("quant_max_bound", "pir::FloatAttribute", ""), paddle::dialect::OpAttributeInfo("quant_min_bound", "pir::FloatAttribute", "") };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("out", "paddle::dialect::DenseTensorType", false, false) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("FusedBiasActInferMeta", {"x", "bias", "dequant_scales", "shift", "smooth", "act_method", "compute_dtype", "quant_scale", "quant_round_type", "quant_max_bound", "quant_min_bound"}, "fused_bias_act", {"x", "bias", "dequant_scales", "shift", "smooth", "act_method", "compute_dtype", "quant_scale", "quant_round_type", "quant_max_bound", "quant_min_bound"}, {"x"}, {}, {}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "fused_bias_act");
}

void FusedBiasActOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, pir::Value bias_, pir::Value dequant_scales_, pir::Value shift_, pir::Value smooth_, const std::string& act_method, const std::string& compute_dtype, float quant_scale, int quant_round_type, float quant_max_bound, float quant_min_bound) {
  VLOG(4) << "Start build FusedBiasActOp";



  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_, bias_, dequant_scales_, shift_, smooth_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_act_method = pir::StrAttribute::get(pir::IrContext::Instance(), act_method);
  argument.AddAttribute("act_method", attr_act_method);
  pir::Attribute attr_compute_dtype = pir::StrAttribute::get(pir::IrContext::Instance(), compute_dtype);
  argument.AddAttribute("compute_dtype", attr_compute_dtype);
  pir::Attribute attr_quant_scale = pir::FloatAttribute::get(pir::IrContext::Instance(), quant_scale);
  argument.AddAttribute("quant_scale", attr_quant_scale);
  pir::Attribute attr_quant_round_type = pir::Int32Attribute::get(pir::IrContext::Instance(), quant_round_type);
  argument.AddAttribute("quant_round_type", attr_quant_round_type);
  pir::Attribute attr_quant_max_bound = pir::FloatAttribute::get(pir::IrContext::Instance(), quant_max_bound);
  argument.AddAttribute("quant_max_bound", attr_quant_max_bound);
  pir::Attribute attr_quant_min_bound = pir::FloatAttribute::get(pir::IrContext::Instance(), quant_min_bound);
  argument.AddAttribute("quant_min_bound", attr_quant_min_bound);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);

  paddle::dialect::IrMetaTensor meta_bias;
  paddle::dialect::IrTensor ir_tensor_bias;
  if (bias_.impl() != nullptr) {
    paddle::dialect::DenseTensorType bias = bias_.type().dyn_cast<paddle::dialect::DenseTensorType>();
    VLOG(4) << "Builder construction  dense_bias";
    ir_tensor_bias = paddle::dialect::IrTensor(paddle::dialect::TransToPhiDataType(bias.dtype()),
                                                        bias.dims(),
                                                        bias.data_layout(),
                                                        bias.lod(),
                                                        bias.offset());
    VLOG(4) << "Builder construction  meta_bias";
    meta_bias = paddle::dialect::IrMetaTensor(&ir_tensor_bias);
  }


  paddle::dialect::IrMetaTensor meta_dequant_scales;
  paddle::dialect::IrTensor ir_tensor_dequant_scales;
  if (dequant_scales_.impl() != nullptr) {
    paddle::dialect::DenseTensorType dequant_scales = dequant_scales_.type().dyn_cast<paddle::dialect::DenseTensorType>();
    VLOG(4) << "Builder construction  dense_dequant_scales";
    ir_tensor_dequant_scales = paddle::dialect::IrTensor(paddle::dialect::TransToPhiDataType(dequant_scales.dtype()),
                                                        dequant_scales.dims(),
                                                        dequant_scales.data_layout(),
                                                        dequant_scales.lod(),
                                                        dequant_scales.offset());
    VLOG(4) << "Builder construction  meta_dequant_scales";
    meta_dequant_scales = paddle::dialect::IrMetaTensor(&ir_tensor_dequant_scales);
  }


  paddle::dialect::IrMetaTensor meta_shift;
  paddle::dialect::IrTensor ir_tensor_shift;
  if (shift_.impl() != nullptr) {
    paddle::dialect::DenseTensorType shift = shift_.type().dyn_cast<paddle::dialect::DenseTensorType>();
    VLOG(4) << "Builder construction  dense_shift";
    ir_tensor_shift = paddle::dialect::IrTensor(paddle::dialect::TransToPhiDataType(shift.dtype()),
                                                        shift.dims(),
                                                        shift.data_layout(),
                                                        shift.lod(),
                                                        shift.offset());
    VLOG(4) << "Builder construction  meta_shift";
    meta_shift = paddle::dialect::IrMetaTensor(&ir_tensor_shift);
  }


  paddle::dialect::IrMetaTensor meta_smooth;
  paddle::dialect::IrTensor ir_tensor_smooth;
  if (smooth_.impl() != nullptr) {
    paddle::dialect::DenseTensorType smooth = smooth_.type().dyn_cast<paddle::dialect::DenseTensorType>();
    VLOG(4) << "Builder construction  dense_smooth";
    ir_tensor_smooth = paddle::dialect::IrTensor(paddle::dialect::TransToPhiDataType(smooth.dtype()),
                                                        smooth.dims(),
                                                        smooth.data_layout(),
                                                        smooth.lod(),
                                                        smooth.offset());
    VLOG(4) << "Builder construction  meta_smooth";
    meta_smooth = paddle::dialect::IrMetaTensor(&ir_tensor_smooth);
  }

  paddle::dialect::IrTensor dense_out;
  paddle::dialect::IrMetaTensor meta_out(&dense_out);

  phi::FusedBiasActInferMeta(meta_x, meta_bias, meta_dequant_scales, meta_shift, meta_smooth, act_method, compute_dtype, quant_scale, quant_round_type, quant_max_bound, quant_min_bound, &meta_out);

  std::vector<pir::Type> argument_outputs;
  pir::Type out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_out.dtype()), dense_out.dims(), dense_out.layout(), dense_out.lod(), dense_out.offset());
  argument_outputs.push_back(out_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void FusedBiasActOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, pir::Value bias_, pir::Value dequant_scales_, pir::Value shift_, pir::Value smooth_, pir::AttributeMap attributes) {
  VLOG(4) << "Start build FusedBiasActOp";


  IR_ENFORCE(
      attributes.find("act_method") != attributes.end(),
          "'act_method' Attribute is expected for FusedBiasActOp. ");
  std::string act_method = attributes.at("act_method").dyn_cast<pir::StrAttribute>().AsString();

  IR_ENFORCE(
      attributes.find("compute_dtype") != attributes.end(),
          "'compute_dtype' Attribute is expected for FusedBiasActOp. ");
  std::string compute_dtype = attributes.at("compute_dtype").dyn_cast<pir::StrAttribute>().AsString();

  IR_ENFORCE(
      attributes.find("quant_scale") != attributes.end(),
          "'quant_scale' Attribute is expected for FusedBiasActOp. ");
  float quant_scale = attributes.at("quant_scale").dyn_cast<pir::FloatAttribute>().data();

  IR_ENFORCE(
      attributes.find("quant_round_type") != attributes.end(),
          "'quant_round_type' Attribute is expected for FusedBiasActOp. ");
  int quant_round_type = attributes.at("quant_round_type").dyn_cast<pir::Int32Attribute>().data();

  IR_ENFORCE(
      attributes.find("quant_max_bound") != attributes.end(),
          "'quant_max_bound' Attribute is expected for FusedBiasActOp. ");
  float quant_max_bound = attributes.at("quant_max_bound").dyn_cast<pir::FloatAttribute>().data();

  IR_ENFORCE(
      attributes.find("quant_min_bound") != attributes.end(),
          "'quant_min_bound' Attribute is expected for FusedBiasActOp. ");
  float quant_min_bound = attributes.at("quant_min_bound").dyn_cast<pir::FloatAttribute>().data();


  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_, bias_, dequant_scales_, shift_, smooth_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_act_method = pir::StrAttribute::get(pir::IrContext::Instance(), act_method);
  argument.AddAttribute("act_method", attr_act_method);
  pir::Attribute attr_compute_dtype = pir::StrAttribute::get(pir::IrContext::Instance(), compute_dtype);
  argument.AddAttribute("compute_dtype", attr_compute_dtype);
  pir::Attribute attr_quant_scale = pir::FloatAttribute::get(pir::IrContext::Instance(), quant_scale);
  argument.AddAttribute("quant_scale", attr_quant_scale);
  pir::Attribute attr_quant_round_type = pir::Int32Attribute::get(pir::IrContext::Instance(), quant_round_type);
  argument.AddAttribute("quant_round_type", attr_quant_round_type);
  pir::Attribute attr_quant_max_bound = pir::FloatAttribute::get(pir::IrContext::Instance(), quant_max_bound);
  argument.AddAttribute("quant_max_bound", attr_quant_max_bound);
  pir::Attribute attr_quant_min_bound = pir::FloatAttribute::get(pir::IrContext::Instance(), quant_min_bound);
  argument.AddAttribute("quant_min_bound", attr_quant_min_bound);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);

  paddle::dialect::IrMetaTensor meta_bias;
  paddle::dialect::IrTensor ir_tensor_bias;
  if (bias_.impl() != nullptr) {
    paddle::dialect::DenseTensorType bias = bias_.type().dyn_cast<paddle::dialect::DenseTensorType>();
    VLOG(4) << "Builder construction  dense_bias";
    ir_tensor_bias = paddle::dialect::IrTensor(paddle::dialect::TransToPhiDataType(bias.dtype()),
                                                        bias.dims(),
                                                        bias.data_layout(),
                                                        bias.lod(),
                                                        bias.offset());
    VLOG(4) << "Builder construction  meta_bias";
    meta_bias = paddle::dialect::IrMetaTensor(&ir_tensor_bias);
  }


  paddle::dialect::IrMetaTensor meta_dequant_scales;
  paddle::dialect::IrTensor ir_tensor_dequant_scales;
  if (dequant_scales_.impl() != nullptr) {
    paddle::dialect::DenseTensorType dequant_scales = dequant_scales_.type().dyn_cast<paddle::dialect::DenseTensorType>();
    VLOG(4) << "Builder construction  dense_dequant_scales";
    ir_tensor_dequant_scales = paddle::dialect::IrTensor(paddle::dialect::TransToPhiDataType(dequant_scales.dtype()),
                                                        dequant_scales.dims(),
                                                        dequant_scales.data_layout(),
                                                        dequant_scales.lod(),
                                                        dequant_scales.offset());
    VLOG(4) << "Builder construction  meta_dequant_scales";
    meta_dequant_scales = paddle::dialect::IrMetaTensor(&ir_tensor_dequant_scales);
  }


  paddle::dialect::IrMetaTensor meta_shift;
  paddle::dialect::IrTensor ir_tensor_shift;
  if (shift_.impl() != nullptr) {
    paddle::dialect::DenseTensorType shift = shift_.type().dyn_cast<paddle::dialect::DenseTensorType>();
    VLOG(4) << "Builder construction  dense_shift";
    ir_tensor_shift = paddle::dialect::IrTensor(paddle::dialect::TransToPhiDataType(shift.dtype()),
                                                        shift.dims(),
                                                        shift.data_layout(),
                                                        shift.lod(),
                                                        shift.offset());
    VLOG(4) << "Builder construction  meta_shift";
    meta_shift = paddle::dialect::IrMetaTensor(&ir_tensor_shift);
  }


  paddle::dialect::IrMetaTensor meta_smooth;
  paddle::dialect::IrTensor ir_tensor_smooth;
  if (smooth_.impl() != nullptr) {
    paddle::dialect::DenseTensorType smooth = smooth_.type().dyn_cast<paddle::dialect::DenseTensorType>();
    VLOG(4) << "Builder construction  dense_smooth";
    ir_tensor_smooth = paddle::dialect::IrTensor(paddle::dialect::TransToPhiDataType(smooth.dtype()),
                                                        smooth.dims(),
                                                        smooth.data_layout(),
                                                        smooth.lod(),
                                                        smooth.offset());
    VLOG(4) << "Builder construction  meta_smooth";
    meta_smooth = paddle::dialect::IrMetaTensor(&ir_tensor_smooth);
  }

  paddle::dialect::IrTensor dense_out;
  paddle::dialect::IrMetaTensor meta_out(&dense_out);

  phi::FusedBiasActInferMeta(meta_x, meta_bias, meta_dequant_scales, meta_shift, meta_smooth, act_method, compute_dtype, quant_scale, quant_round_type, quant_max_bound, quant_min_bound, &meta_out);

  std::vector<pir::Type> argument_outputs;
  pir::Type out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_out.dtype()), dense_out.dims(), dense_out.layout(), dense_out.lod(), dense_out.offset());
  argument_outputs.push_back(out_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void FusedBiasActOp::VerifySig() {
  VLOG(4) << "Start Verifying inputs, outputs and attributes for: FusedBiasActOp.";
  VLOG(4) << "Verifying inputs:";
  {
  auto input_size = num_operands();
  IR_ENFORCE(input_size == 5u,
                    "The size %d of inputs must be equal to 5.", input_size);
  IR_ENFORCE((*this)->operand_source(0).type().isa<paddle::dialect::DenseTensorType>(),
                  "Type validation failed for the 0th input, got %s.", (*this)->operand_source(0).type());
  if (auto val = (*this)->operand(1)) {
    IR_ENFORCE(val.type().isa<paddle::dialect::DenseTensorType>(),
                   "Type validation failed for the 1th input, got %s.", (*this)->operand_source(1).type());
  }
  if (auto val = (*this)->operand(2)) {
    IR_ENFORCE(val.type().isa<paddle::dialect::DenseTensorType>(),
                   "Type validation failed for the 2th input, got %s.", (*this)->operand_source(2).type());
  }
  if (auto val = (*this)->operand(3)) {
    IR_ENFORCE(val.type().isa<paddle::dialect::DenseTensorType>(),
                   "Type validation failed for the 3th input, got %s.", (*this)->operand_source(3).type());
  }
  if (auto val = (*this)->operand(4)) {
    IR_ENFORCE(val.type().isa<paddle::dialect::DenseTensorType>(),
                   "Type validation failed for the 4th input, got %s.", (*this)->operand_source(4).type());
  }
  }
  VLOG(4) << "Verifying attributes:";
  {
  auto& attributes = this->attributes();
  IR_ENFORCE(attributes.count("act_method")>0,
                 "act_method does not exist.");
  IR_ENFORCE(attributes.at("act_method").isa<pir::StrAttribute>(),
                 "Type of attribute: act_method is not pir::StrAttribute.");

  IR_ENFORCE(attributes.count("compute_dtype")>0,
                 "compute_dtype does not exist.");
  IR_ENFORCE(attributes.at("compute_dtype").isa<pir::StrAttribute>(),
                 "Type of attribute: compute_dtype is not pir::StrAttribute.");

  IR_ENFORCE(attributes.count("quant_scale")>0,
                 "quant_scale does not exist.");
  IR_ENFORCE(attributes.at("quant_scale").isa<pir::FloatAttribute>(),
                 "Type of attribute: quant_scale is not pir::FloatAttribute.");

  IR_ENFORCE(attributes.count("quant_round_type")>0,
                 "quant_round_type does not exist.");
  IR_ENFORCE(attributes.at("quant_round_type").isa<pir::Int32Attribute>(),
                 "Type of attribute: quant_round_type is not pir::Int32Attribute.");

  IR_ENFORCE(attributes.count("quant_max_bound")>0,
                 "quant_max_bound does not exist.");
  IR_ENFORCE(attributes.at("quant_max_bound").isa<pir::FloatAttribute>(),
                 "Type of attribute: quant_max_bound is not pir::FloatAttribute.");

  IR_ENFORCE(attributes.count("quant_min_bound")>0,
                 "quant_min_bound does not exist.");
  IR_ENFORCE(attributes.at("quant_min_bound").isa<pir::FloatAttribute>(),
                 "Type of attribute: quant_min_bound is not pir::FloatAttribute.");

  }
  VLOG(4) << "Verifying outputs:";
  {
  auto output_size = num_results();
  IR_ENFORCE(output_size == 1u,
                    "The size %d of outputs must be equal to 1.", output_size);
  IR_ENFORCE((*this)->result(0).type().isa<paddle::dialect::DenseTensorType>(),
                 "Type validation failed for the 0th output.");
  }
  VLOG(4) << "End Verifying for: FusedBiasActOp.";
}

void FusedBiasActOp::InferMeta( phi::InferMetaContext *infer_meta ) {
  auto fn = PD_INFER_META(phi::FusedBiasActInferMeta);
  fn(infer_meta);
}

phi::DataType FusedBiasActOp::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: FusedBiasActOp";
  


  return expected_kernel_dtype;
}

const char *FusedBiasDropoutResidualLayerNormOp::attributes_name[6] = { "dropout_rate", "is_test", "dropout_fix_seed", "dropout_seed", "dropout_implementation", "ln_epsilon" };

OpInfoTuple FusedBiasDropoutResidualLayerNormOp::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("x", "paddle::dialect::DenseTensorType", false, false, false, true), paddle::dialect::OpInputInfo("residual", "paddle::dialect::DenseTensorType", false, false, false, true), paddle::dialect::OpInputInfo("bias", "paddle::dialect::DenseTensorType", true, false, false, true), paddle::dialect::OpInputInfo("ln_scale", "paddle::dialect::DenseTensorType", true, false, false, true), paddle::dialect::OpInputInfo("ln_bias", "paddle::dialect::DenseTensorType", true, false, false, true) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = { paddle::dialect::OpAttributeInfo("dropout_rate", "pir::FloatAttribute", ""), paddle::dialect::OpAttributeInfo("is_test", "pir::BoolAttribute", ""), paddle::dialect::OpAttributeInfo("dropout_fix_seed", "pir::BoolAttribute", ""), paddle::dialect::OpAttributeInfo("dropout_seed", "pir::Int32Attribute", ""), paddle::dialect::OpAttributeInfo("dropout_implementation", "pir::StrAttribute", ""), paddle::dialect::OpAttributeInfo("ln_epsilon", "pir::FloatAttribute", "") };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("y", "paddle::dialect::DenseTensorType", false, false), paddle::dialect::OpOutputInfo("bias_dropout_residual_out", "paddle::dialect::DenseTensorType", true, true), paddle::dialect::OpOutputInfo("dropout_mask_out", "paddle::dialect::DenseTensorType", true, true), paddle::dialect::OpOutputInfo("ln_mean", "paddle::dialect::DenseTensorType", true, true), paddle::dialect::OpOutputInfo("ln_variance", "paddle::dialect::DenseTensorType", true, true) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("FusedBiasDropoutResidualLnInferMeta", {"x", "residual", "bias", "ln_scale", "ln_bias", "dropout_rate", "is_test", "dropout_fix_seed", "dropout_seed", "dropout_implementation", "ln_epsilon"}, "fused_bias_dropout_residual_layer_norm", {"x", "residual", "bias", "ln_scale", "ln_bias", "dropout_rate", "is_test", "dropout_fix_seed", "dropout_seed", "dropout_implementation", "ln_epsilon"}, {"x"}, {}, {}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "fused_bias_dropout_residual_layer_norm");
}

void FusedBiasDropoutResidualLayerNormOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, pir::Value residual_, pir::Value bias_, pir::Value ln_scale_, pir::Value ln_bias_, float dropout_rate, bool is_test, bool dropout_fix_seed, int dropout_seed, const std::string& dropout_implementation, float ln_epsilon) {
  VLOG(4) << "Start build FusedBiasDropoutResidualLayerNormOp";



  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_, residual_, bias_, ln_scale_, ln_bias_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_dropout_rate = pir::FloatAttribute::get(pir::IrContext::Instance(), dropout_rate);
  argument.AddAttribute("dropout_rate", attr_dropout_rate);
  pir::Attribute attr_is_test = pir::BoolAttribute::get(pir::IrContext::Instance(), is_test);
  argument.AddAttribute("is_test", attr_is_test);
  pir::Attribute attr_dropout_fix_seed = pir::BoolAttribute::get(pir::IrContext::Instance(), dropout_fix_seed);
  argument.AddAttribute("dropout_fix_seed", attr_dropout_fix_seed);
  pir::Attribute attr_dropout_seed = pir::Int32Attribute::get(pir::IrContext::Instance(), dropout_seed);
  argument.AddAttribute("dropout_seed", attr_dropout_seed);
  pir::Attribute attr_dropout_implementation = pir::StrAttribute::get(pir::IrContext::Instance(), dropout_implementation);
  argument.AddAttribute("dropout_implementation", attr_dropout_implementation);
  pir::Attribute attr_ln_epsilon = pir::FloatAttribute::get(pir::IrContext::Instance(), ln_epsilon);
  argument.AddAttribute("ln_epsilon", attr_ln_epsilon);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;
  paddle::dialect::DenseTensorType residual = residual_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)residual;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);

  VLOG(4) << "Builder construction  dense_residual";
  paddle::dialect::IrTensor ir_tensor_residual(paddle::dialect::TransToPhiDataType(residual.dtype()),
                                                      residual.dims(),
                                                      residual.data_layout(),
                                                      residual.lod(),
                                                      residual.offset());
  VLOG(4) << "Builder construction  meta_residual";
  paddle::dialect::IrMetaTensor meta_residual(&ir_tensor_residual);

  paddle::dialect::IrMetaTensor meta_bias;
  paddle::dialect::IrTensor ir_tensor_bias;
  if (bias_.impl() != nullptr) {
    paddle::dialect::DenseTensorType bias = bias_.type().dyn_cast<paddle::dialect::DenseTensorType>();
    VLOG(4) << "Builder construction  dense_bias";
    ir_tensor_bias = paddle::dialect::IrTensor(paddle::dialect::TransToPhiDataType(bias.dtype()),
                                                        bias.dims(),
                                                        bias.data_layout(),
                                                        bias.lod(),
                                                        bias.offset());
    VLOG(4) << "Builder construction  meta_bias";
    meta_bias = paddle::dialect::IrMetaTensor(&ir_tensor_bias);
  }


  paddle::dialect::IrMetaTensor meta_ln_scale;
  paddle::dialect::IrTensor ir_tensor_ln_scale;
  if (ln_scale_.impl() != nullptr) {
    paddle::dialect::DenseTensorType ln_scale = ln_scale_.type().dyn_cast<paddle::dialect::DenseTensorType>();
    VLOG(4) << "Builder construction  dense_ln_scale";
    ir_tensor_ln_scale = paddle::dialect::IrTensor(paddle::dialect::TransToPhiDataType(ln_scale.dtype()),
                                                        ln_scale.dims(),
                                                        ln_scale.data_layout(),
                                                        ln_scale.lod(),
                                                        ln_scale.offset());
    VLOG(4) << "Builder construction  meta_ln_scale";
    meta_ln_scale = paddle::dialect::IrMetaTensor(&ir_tensor_ln_scale);
  }


  paddle::dialect::IrMetaTensor meta_ln_bias;
  paddle::dialect::IrTensor ir_tensor_ln_bias;
  if (ln_bias_.impl() != nullptr) {
    paddle::dialect::DenseTensorType ln_bias = ln_bias_.type().dyn_cast<paddle::dialect::DenseTensorType>();
    VLOG(4) << "Builder construction  dense_ln_bias";
    ir_tensor_ln_bias = paddle::dialect::IrTensor(paddle::dialect::TransToPhiDataType(ln_bias.dtype()),
                                                        ln_bias.dims(),
                                                        ln_bias.data_layout(),
                                                        ln_bias.lod(),
                                                        ln_bias.offset());
    VLOG(4) << "Builder construction  meta_ln_bias";
    meta_ln_bias = paddle::dialect::IrMetaTensor(&ir_tensor_ln_bias);
  }

  paddle::dialect::IrTensor dense_y;
  paddle::dialect::IrMetaTensor meta_y(&dense_y);
  paddle::dialect::IrTensor dense_bias_dropout_residual_out;
  paddle::dialect::IrMetaTensor meta_bias_dropout_residual_out(&dense_bias_dropout_residual_out);
  paddle::dialect::IrTensor dense_dropout_mask_out;
  paddle::dialect::IrMetaTensor meta_dropout_mask_out(&dense_dropout_mask_out);
  paddle::dialect::IrTensor dense_ln_mean;
  paddle::dialect::IrMetaTensor meta_ln_mean(&dense_ln_mean);
  paddle::dialect::IrTensor dense_ln_variance;
  paddle::dialect::IrMetaTensor meta_ln_variance(&dense_ln_variance);

  phi::FusedBiasDropoutResidualLnInferMeta(meta_x, meta_residual, meta_bias, meta_ln_scale, meta_ln_bias, dropout_rate, is_test, dropout_fix_seed, dropout_seed, dropout_implementation, ln_epsilon, &meta_y, &meta_bias_dropout_residual_out, &meta_dropout_mask_out, &meta_ln_mean, &meta_ln_variance);

  std::vector<pir::Type> argument_outputs;
  pir::Type y_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_y.dtype()), dense_y.dims(), dense_y.layout(), dense_y.lod(), dense_y.offset());
  argument_outputs.push_back(y_dense_tensor_type);

  pir::Type bias_dropout_residual_out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_bias_dropout_residual_out.dtype()), dense_bias_dropout_residual_out.dims(), dense_bias_dropout_residual_out.layout(), dense_bias_dropout_residual_out.lod(), dense_bias_dropout_residual_out.offset());
  argument_outputs.push_back(bias_dropout_residual_out_dense_tensor_type);

  pir::Type dropout_mask_out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_dropout_mask_out.dtype()), dense_dropout_mask_out.dims(), dense_dropout_mask_out.layout(), dense_dropout_mask_out.lod(), dense_dropout_mask_out.offset());
  argument_outputs.push_back(dropout_mask_out_dense_tensor_type);

  pir::Type ln_mean_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_ln_mean.dtype()), dense_ln_mean.dims(), dense_ln_mean.layout(), dense_ln_mean.lod(), dense_ln_mean.offset());
  argument_outputs.push_back(ln_mean_dense_tensor_type);

  pir::Type ln_variance_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_ln_variance.dtype()), dense_ln_variance.dims(), dense_ln_variance.layout(), dense_ln_variance.lod(), dense_ln_variance.offset());
  argument_outputs.push_back(ln_variance_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void FusedBiasDropoutResidualLayerNormOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, pir::Value residual_, pir::Value bias_, pir::Value ln_scale_, pir::Value ln_bias_, pir::AttributeMap attributes) {
  VLOG(4) << "Start build FusedBiasDropoutResidualLayerNormOp";


  IR_ENFORCE(
      attributes.find("dropout_rate") != attributes.end(),
          "'dropout_rate' Attribute is expected for FusedBiasDropoutResidualLayerNormOp. ");
  float dropout_rate = attributes.at("dropout_rate").dyn_cast<pir::FloatAttribute>().data();

  IR_ENFORCE(
      attributes.find("is_test") != attributes.end(),
          "'is_test' Attribute is expected for FusedBiasDropoutResidualLayerNormOp. ");
  bool is_test = attributes.at("is_test").dyn_cast<pir::BoolAttribute>().data();

  IR_ENFORCE(
      attributes.find("dropout_fix_seed") != attributes.end(),
          "'dropout_fix_seed' Attribute is expected for FusedBiasDropoutResidualLayerNormOp. ");
  bool dropout_fix_seed = attributes.at("dropout_fix_seed").dyn_cast<pir::BoolAttribute>().data();

  IR_ENFORCE(
      attributes.find("dropout_seed") != attributes.end(),
          "'dropout_seed' Attribute is expected for FusedBiasDropoutResidualLayerNormOp. ");
  int dropout_seed = attributes.at("dropout_seed").dyn_cast<pir::Int32Attribute>().data();

  IR_ENFORCE(
      attributes.find("dropout_implementation") != attributes.end(),
          "'dropout_implementation' Attribute is expected for FusedBiasDropoutResidualLayerNormOp. ");
  std::string dropout_implementation = attributes.at("dropout_implementation").dyn_cast<pir::StrAttribute>().AsString();

  IR_ENFORCE(
      attributes.find("ln_epsilon") != attributes.end(),
          "'ln_epsilon' Attribute is expected for FusedBiasDropoutResidualLayerNormOp. ");
  float ln_epsilon = attributes.at("ln_epsilon").dyn_cast<pir::FloatAttribute>().data();


  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_, residual_, bias_, ln_scale_, ln_bias_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_dropout_rate = pir::FloatAttribute::get(pir::IrContext::Instance(), dropout_rate);
  argument.AddAttribute("dropout_rate", attr_dropout_rate);
  pir::Attribute attr_is_test = pir::BoolAttribute::get(pir::IrContext::Instance(), is_test);
  argument.AddAttribute("is_test", attr_is_test);
  pir::Attribute attr_dropout_fix_seed = pir::BoolAttribute::get(pir::IrContext::Instance(), dropout_fix_seed);
  argument.AddAttribute("dropout_fix_seed", attr_dropout_fix_seed);
  pir::Attribute attr_dropout_seed = pir::Int32Attribute::get(pir::IrContext::Instance(), dropout_seed);
  argument.AddAttribute("dropout_seed", attr_dropout_seed);
  pir::Attribute attr_dropout_implementation = pir::StrAttribute::get(pir::IrContext::Instance(), dropout_implementation);
  argument.AddAttribute("dropout_implementation", attr_dropout_implementation);
  pir::Attribute attr_ln_epsilon = pir::FloatAttribute::get(pir::IrContext::Instance(), ln_epsilon);
  argument.AddAttribute("ln_epsilon", attr_ln_epsilon);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;
  paddle::dialect::DenseTensorType residual = residual_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)residual;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);

  VLOG(4) << "Builder construction  dense_residual";
  paddle::dialect::IrTensor ir_tensor_residual(paddle::dialect::TransToPhiDataType(residual.dtype()),
                                                      residual.dims(),
                                                      residual.data_layout(),
                                                      residual.lod(),
                                                      residual.offset());
  VLOG(4) << "Builder construction  meta_residual";
  paddle::dialect::IrMetaTensor meta_residual(&ir_tensor_residual);

  paddle::dialect::IrMetaTensor meta_bias;
  paddle::dialect::IrTensor ir_tensor_bias;
  if (bias_.impl() != nullptr) {
    paddle::dialect::DenseTensorType bias = bias_.type().dyn_cast<paddle::dialect::DenseTensorType>();
    VLOG(4) << "Builder construction  dense_bias";
    ir_tensor_bias = paddle::dialect::IrTensor(paddle::dialect::TransToPhiDataType(bias.dtype()),
                                                        bias.dims(),
                                                        bias.data_layout(),
                                                        bias.lod(),
                                                        bias.offset());
    VLOG(4) << "Builder construction  meta_bias";
    meta_bias = paddle::dialect::IrMetaTensor(&ir_tensor_bias);
  }


  paddle::dialect::IrMetaTensor meta_ln_scale;
  paddle::dialect::IrTensor ir_tensor_ln_scale;
  if (ln_scale_.impl() != nullptr) {
    paddle::dialect::DenseTensorType ln_scale = ln_scale_.type().dyn_cast<paddle::dialect::DenseTensorType>();
    VLOG(4) << "Builder construction  dense_ln_scale";
    ir_tensor_ln_scale = paddle::dialect::IrTensor(paddle::dialect::TransToPhiDataType(ln_scale.dtype()),
                                                        ln_scale.dims(),
                                                        ln_scale.data_layout(),
                                                        ln_scale.lod(),
                                                        ln_scale.offset());
    VLOG(4) << "Builder construction  meta_ln_scale";
    meta_ln_scale = paddle::dialect::IrMetaTensor(&ir_tensor_ln_scale);
  }


  paddle::dialect::IrMetaTensor meta_ln_bias;
  paddle::dialect::IrTensor ir_tensor_ln_bias;
  if (ln_bias_.impl() != nullptr) {
    paddle::dialect::DenseTensorType ln_bias = ln_bias_.type().dyn_cast<paddle::dialect::DenseTensorType>();
    VLOG(4) << "Builder construction  dense_ln_bias";
    ir_tensor_ln_bias = paddle::dialect::IrTensor(paddle::dialect::TransToPhiDataType(ln_bias.dtype()),
                                                        ln_bias.dims(),
                                                        ln_bias.data_layout(),
                                                        ln_bias.lod(),
                                                        ln_bias.offset());
    VLOG(4) << "Builder construction  meta_ln_bias";
    meta_ln_bias = paddle::dialect::IrMetaTensor(&ir_tensor_ln_bias);
  }

  paddle::dialect::IrTensor dense_y;
  paddle::dialect::IrMetaTensor meta_y(&dense_y);
  paddle::dialect::IrTensor dense_bias_dropout_residual_out;
  paddle::dialect::IrMetaTensor meta_bias_dropout_residual_out(&dense_bias_dropout_residual_out);
  paddle::dialect::IrTensor dense_dropout_mask_out;
  paddle::dialect::IrMetaTensor meta_dropout_mask_out(&dense_dropout_mask_out);
  paddle::dialect::IrTensor dense_ln_mean;
  paddle::dialect::IrMetaTensor meta_ln_mean(&dense_ln_mean);
  paddle::dialect::IrTensor dense_ln_variance;
  paddle::dialect::IrMetaTensor meta_ln_variance(&dense_ln_variance);

  phi::FusedBiasDropoutResidualLnInferMeta(meta_x, meta_residual, meta_bias, meta_ln_scale, meta_ln_bias, dropout_rate, is_test, dropout_fix_seed, dropout_seed, dropout_implementation, ln_epsilon, &meta_y, &meta_bias_dropout_residual_out, &meta_dropout_mask_out, &meta_ln_mean, &meta_ln_variance);

  std::vector<pir::Type> argument_outputs;
  pir::Type y_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_y.dtype()), dense_y.dims(), dense_y.layout(), dense_y.lod(), dense_y.offset());
  argument_outputs.push_back(y_dense_tensor_type);

  pir::Type bias_dropout_residual_out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_bias_dropout_residual_out.dtype()), dense_bias_dropout_residual_out.dims(), dense_bias_dropout_residual_out.layout(), dense_bias_dropout_residual_out.lod(), dense_bias_dropout_residual_out.offset());
  argument_outputs.push_back(bias_dropout_residual_out_dense_tensor_type);

  pir::Type dropout_mask_out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_dropout_mask_out.dtype()), dense_dropout_mask_out.dims(), dense_dropout_mask_out.layout(), dense_dropout_mask_out.lod(), dense_dropout_mask_out.offset());
  argument_outputs.push_back(dropout_mask_out_dense_tensor_type);

  pir::Type ln_mean_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_ln_mean.dtype()), dense_ln_mean.dims(), dense_ln_mean.layout(), dense_ln_mean.lod(), dense_ln_mean.offset());
  argument_outputs.push_back(ln_mean_dense_tensor_type);

  pir::Type ln_variance_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_ln_variance.dtype()), dense_ln_variance.dims(), dense_ln_variance.layout(), dense_ln_variance.lod(), dense_ln_variance.offset());
  argument_outputs.push_back(ln_variance_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void FusedBiasDropoutResidualLayerNormOp::VerifySig() {
  VLOG(4) << "Start Verifying inputs, outputs and attributes for: FusedBiasDropoutResidualLayerNormOp.";
  VLOG(4) << "Verifying inputs:";
  {
  auto input_size = num_operands();
  IR_ENFORCE(input_size == 5u,
                    "The size %d of inputs must be equal to 5.", input_size);
  IR_ENFORCE((*this)->operand_source(0).type().isa<paddle::dialect::DenseTensorType>(),
                  "Type validation failed for the 0th input, got %s.", (*this)->operand_source(0).type());
  IR_ENFORCE((*this)->operand_source(1).type().isa<paddle::dialect::DenseTensorType>(),
                  "Type validation failed for the 1th input, got %s.", (*this)->operand_source(1).type());
  if (auto val = (*this)->operand(2)) {
    IR_ENFORCE(val.type().isa<paddle::dialect::DenseTensorType>(),
                   "Type validation failed for the 2th input, got %s.", (*this)->operand_source(2).type());
  }
  if (auto val = (*this)->operand(3)) {
    IR_ENFORCE(val.type().isa<paddle::dialect::DenseTensorType>(),
                   "Type validation failed for the 3th input, got %s.", (*this)->operand_source(3).type());
  }
  if (auto val = (*this)->operand(4)) {
    IR_ENFORCE(val.type().isa<paddle::dialect::DenseTensorType>(),
                   "Type validation failed for the 4th input, got %s.", (*this)->operand_source(4).type());
  }
  }
  VLOG(4) << "Verifying attributes:";
  {
  auto& attributes = this->attributes();
  IR_ENFORCE(attributes.count("dropout_rate")>0,
                 "dropout_rate does not exist.");
  IR_ENFORCE(attributes.at("dropout_rate").isa<pir::FloatAttribute>(),
                 "Type of attribute: dropout_rate is not pir::FloatAttribute.");

  IR_ENFORCE(attributes.count("is_test")>0,
                 "is_test does not exist.");
  IR_ENFORCE(attributes.at("is_test").isa<pir::BoolAttribute>(),
                 "Type of attribute: is_test is not pir::BoolAttribute.");

  IR_ENFORCE(attributes.count("dropout_fix_seed")>0,
                 "dropout_fix_seed does not exist.");
  IR_ENFORCE(attributes.at("dropout_fix_seed").isa<pir::BoolAttribute>(),
                 "Type of attribute: dropout_fix_seed is not pir::BoolAttribute.");

  IR_ENFORCE(attributes.count("dropout_seed")>0,
                 "dropout_seed does not exist.");
  IR_ENFORCE(attributes.at("dropout_seed").isa<pir::Int32Attribute>(),
                 "Type of attribute: dropout_seed is not pir::Int32Attribute.");

  IR_ENFORCE(attributes.count("dropout_implementation")>0,
                 "dropout_implementation does not exist.");
  IR_ENFORCE(attributes.at("dropout_implementation").isa<pir::StrAttribute>(),
                 "Type of attribute: dropout_implementation is not pir::StrAttribute.");

  IR_ENFORCE(attributes.count("ln_epsilon")>0,
                 "ln_epsilon does not exist.");
  IR_ENFORCE(attributes.at("ln_epsilon").isa<pir::FloatAttribute>(),
                 "Type of attribute: ln_epsilon is not pir::FloatAttribute.");

  }
  VLOG(4) << "Verifying outputs:";
  {
  auto output_size = num_results();
  IR_ENFORCE(output_size == 5u,
                    "The size %d of outputs must be equal to 5.", output_size);
  IR_ENFORCE((*this)->result(0).type().isa<paddle::dialect::DenseTensorType>(),
                 "Type validation failed for the 0th output.");
  if (auto output_1_type = (*this)->result(1).type()) {
    IR_ENFORCE(output_1_type.isa<paddle::dialect::DenseTensorType>(),
                   "Type validation failed for the 1th output.");
  }
  if (auto output_2_type = (*this)->result(2).type()) {
    IR_ENFORCE(output_2_type.isa<paddle::dialect::DenseTensorType>(),
                   "Type validation failed for the 2th output.");
  }
  if (auto output_3_type = (*this)->result(3).type()) {
    IR_ENFORCE(output_3_type.isa<paddle::dialect::DenseTensorType>(),
                   "Type validation failed for the 3th output.");
  }
  if (auto output_4_type = (*this)->result(4).type()) {
    IR_ENFORCE(output_4_type.isa<paddle::dialect::DenseTensorType>(),
                   "Type validation failed for the 4th output.");
  }
  }
  VLOG(4) << "End Verifying for: FusedBiasDropoutResidualLayerNormOp.";
}

void FusedBiasDropoutResidualLayerNormOp::InferMeta( phi::InferMetaContext *infer_meta ) {
  auto fn = PD_INFER_META(phi::FusedBiasDropoutResidualLnInferMeta);
  fn(infer_meta);
}

phi::DataType FusedBiasDropoutResidualLayerNormOp::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: FusedBiasDropoutResidualLayerNormOp";
  


  return expected_kernel_dtype;
}

const char *FusedBiasResidualLayernormOp::attributes_name[7] = { "epsilon", "residual_alpha", "begin_norm_axis", "quant_scale", "quant_round_type", "quant_max_bound", "quant_min_bound" };

OpInfoTuple FusedBiasResidualLayernormOp::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("x", "paddle::dialect::DenseTensorType", false, false, false, false), paddle::dialect::OpInputInfo("bias", "paddle::dialect::DenseTensorType", true, false, false, false), paddle::dialect::OpInputInfo("residual", "paddle::dialect::DenseTensorType", true, false, false, false), paddle::dialect::OpInputInfo("norm_weight", "paddle::dialect::DenseTensorType", true, false, false, false), paddle::dialect::OpInputInfo("norm_bias", "paddle::dialect::DenseTensorType", true, false, false, false) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = { paddle::dialect::OpAttributeInfo("epsilon", "pir::FloatAttribute", ""), paddle::dialect::OpAttributeInfo("residual_alpha", "pir::FloatAttribute", ""), paddle::dialect::OpAttributeInfo("begin_norm_axis", "pir::Int32Attribute", ""), paddle::dialect::OpAttributeInfo("quant_scale", "pir::FloatAttribute", ""), paddle::dialect::OpAttributeInfo("quant_round_type", "pir::Int32Attribute", ""), paddle::dialect::OpAttributeInfo("quant_max_bound", "pir::FloatAttribute", ""), paddle::dialect::OpAttributeInfo("quant_min_bound", "pir::FloatAttribute", "") };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("out", "paddle::dialect::DenseTensorType", false, false), paddle::dialect::OpOutputInfo("residual_out", "paddle::dialect::DenseTensorType", true, false), paddle::dialect::OpOutputInfo("mean", "paddle::dialect::DenseTensorType", false, false), paddle::dialect::OpOutputInfo("variance", "paddle::dialect::DenseTensorType", false, false) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("FusedLayerNormInferMeta", {"x", "bias", "residual", "norm_weight", "norm_bias", "epsilon", "residual_alpha", "begin_norm_axis", "quant_scale", "quant_round_type", "quant_max_bound", "quant_min_bound"}, "fused_bias_residual_layernorm", {"x", "bias", "residual", "norm_weight", "norm_bias", "epsilon", "residual_alpha", "begin_norm_axis", "quant_scale", "quant_round_type", "quant_max_bound", "quant_min_bound"}, {"x"}, {}, {}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "fused_bias_residual_layernorm");
}

void FusedBiasResidualLayernormOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, pir::Value bias_, pir::Value residual_, pir::Value norm_weight_, pir::Value norm_bias_, float epsilon, float residual_alpha, int begin_norm_axis, float quant_scale, int quant_round_type, float quant_max_bound, float quant_min_bound) {
  VLOG(4) << "Start build FusedBiasResidualLayernormOp";



  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_, bias_, residual_, norm_weight_, norm_bias_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_epsilon = pir::FloatAttribute::get(pir::IrContext::Instance(), epsilon);
  argument.AddAttribute("epsilon", attr_epsilon);
  pir::Attribute attr_residual_alpha = pir::FloatAttribute::get(pir::IrContext::Instance(), residual_alpha);
  argument.AddAttribute("residual_alpha", attr_residual_alpha);
  pir::Attribute attr_begin_norm_axis = pir::Int32Attribute::get(pir::IrContext::Instance(), begin_norm_axis);
  argument.AddAttribute("begin_norm_axis", attr_begin_norm_axis);
  pir::Attribute attr_quant_scale = pir::FloatAttribute::get(pir::IrContext::Instance(), quant_scale);
  argument.AddAttribute("quant_scale", attr_quant_scale);
  pir::Attribute attr_quant_round_type = pir::Int32Attribute::get(pir::IrContext::Instance(), quant_round_type);
  argument.AddAttribute("quant_round_type", attr_quant_round_type);
  pir::Attribute attr_quant_max_bound = pir::FloatAttribute::get(pir::IrContext::Instance(), quant_max_bound);
  argument.AddAttribute("quant_max_bound", attr_quant_max_bound);
  pir::Attribute attr_quant_min_bound = pir::FloatAttribute::get(pir::IrContext::Instance(), quant_min_bound);
  argument.AddAttribute("quant_min_bound", attr_quant_min_bound);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);

  paddle::dialect::IrMetaTensor meta_bias;
  paddle::dialect::IrTensor ir_tensor_bias;
  if (bias_.impl() != nullptr) {
    paddle::dialect::DenseTensorType bias = bias_.type().dyn_cast<paddle::dialect::DenseTensorType>();
    VLOG(4) << "Builder construction  dense_bias";
    ir_tensor_bias = paddle::dialect::IrTensor(paddle::dialect::TransToPhiDataType(bias.dtype()),
                                                        bias.dims(),
                                                        bias.data_layout(),
                                                        bias.lod(),
                                                        bias.offset());
    VLOG(4) << "Builder construction  meta_bias";
    meta_bias = paddle::dialect::IrMetaTensor(&ir_tensor_bias);
  }


  paddle::dialect::IrMetaTensor meta_residual;
  paddle::dialect::IrTensor ir_tensor_residual;
  if (residual_.impl() != nullptr) {
    paddle::dialect::DenseTensorType residual = residual_.type().dyn_cast<paddle::dialect::DenseTensorType>();
    VLOG(4) << "Builder construction  dense_residual";
    ir_tensor_residual = paddle::dialect::IrTensor(paddle::dialect::TransToPhiDataType(residual.dtype()),
                                                        residual.dims(),
                                                        residual.data_layout(),
                                                        residual.lod(),
                                                        residual.offset());
    VLOG(4) << "Builder construction  meta_residual";
    meta_residual = paddle::dialect::IrMetaTensor(&ir_tensor_residual);
  }


  paddle::dialect::IrMetaTensor meta_norm_weight;
  paddle::dialect::IrTensor ir_tensor_norm_weight;
  if (norm_weight_.impl() != nullptr) {
    paddle::dialect::DenseTensorType norm_weight = norm_weight_.type().dyn_cast<paddle::dialect::DenseTensorType>();
    VLOG(4) << "Builder construction  dense_norm_weight";
    ir_tensor_norm_weight = paddle::dialect::IrTensor(paddle::dialect::TransToPhiDataType(norm_weight.dtype()),
                                                        norm_weight.dims(),
                                                        norm_weight.data_layout(),
                                                        norm_weight.lod(),
                                                        norm_weight.offset());
    VLOG(4) << "Builder construction  meta_norm_weight";
    meta_norm_weight = paddle::dialect::IrMetaTensor(&ir_tensor_norm_weight);
  }


  paddle::dialect::IrMetaTensor meta_norm_bias;
  paddle::dialect::IrTensor ir_tensor_norm_bias;
  if (norm_bias_.impl() != nullptr) {
    paddle::dialect::DenseTensorType norm_bias = norm_bias_.type().dyn_cast<paddle::dialect::DenseTensorType>();
    VLOG(4) << "Builder construction  dense_norm_bias";
    ir_tensor_norm_bias = paddle::dialect::IrTensor(paddle::dialect::TransToPhiDataType(norm_bias.dtype()),
                                                        norm_bias.dims(),
                                                        norm_bias.data_layout(),
                                                        norm_bias.lod(),
                                                        norm_bias.offset());
    VLOG(4) << "Builder construction  meta_norm_bias";
    meta_norm_bias = paddle::dialect::IrMetaTensor(&ir_tensor_norm_bias);
  }

  paddle::dialect::IrTensor dense_out;
  paddle::dialect::IrMetaTensor meta_out(&dense_out);
  paddle::dialect::IrTensor dense_residual_out;
  paddle::dialect::IrMetaTensor meta_residual_out(&dense_residual_out);
  paddle::dialect::IrTensor dense_mean;
  paddle::dialect::IrMetaTensor meta_mean(&dense_mean);
  paddle::dialect::IrTensor dense_variance;
  paddle::dialect::IrMetaTensor meta_variance(&dense_variance);

  phi::FusedLayerNormInferMeta(meta_x, meta_bias, meta_residual, meta_norm_weight, meta_norm_bias, epsilon, residual_alpha, begin_norm_axis, quant_scale, quant_round_type, quant_max_bound, quant_min_bound, &meta_out, &meta_residual_out, &meta_mean, &meta_variance);

  std::vector<pir::Type> argument_outputs;
  pir::Type out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_out.dtype()), dense_out.dims(), dense_out.layout(), dense_out.lod(), dense_out.offset());
  argument_outputs.push_back(out_dense_tensor_type);

  pir::Type residual_out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_residual_out.dtype()), dense_residual_out.dims(), dense_residual_out.layout(), dense_residual_out.lod(), dense_residual_out.offset());
  argument_outputs.push_back(residual_out_dense_tensor_type);

  pir::Type mean_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_mean.dtype()), dense_mean.dims(), dense_mean.layout(), dense_mean.lod(), dense_mean.offset());
  argument_outputs.push_back(mean_dense_tensor_type);

  pir::Type variance_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_variance.dtype()), dense_variance.dims(), dense_variance.layout(), dense_variance.lod(), dense_variance.offset());
  argument_outputs.push_back(variance_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void FusedBiasResidualLayernormOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, pir::Value bias_, pir::Value residual_, pir::Value norm_weight_, pir::Value norm_bias_, pir::AttributeMap attributes) {
  VLOG(4) << "Start build FusedBiasResidualLayernormOp";


  IR_ENFORCE(
      attributes.find("epsilon") != attributes.end(),
          "'epsilon' Attribute is expected for FusedBiasResidualLayernormOp. ");
  float epsilon = attributes.at("epsilon").dyn_cast<pir::FloatAttribute>().data();

  IR_ENFORCE(
      attributes.find("residual_alpha") != attributes.end(),
          "'residual_alpha' Attribute is expected for FusedBiasResidualLayernormOp. ");
  float residual_alpha = attributes.at("residual_alpha").dyn_cast<pir::FloatAttribute>().data();

  IR_ENFORCE(
      attributes.find("begin_norm_axis") != attributes.end(),
          "'begin_norm_axis' Attribute is expected for FusedBiasResidualLayernormOp. ");
  int begin_norm_axis = attributes.at("begin_norm_axis").dyn_cast<pir::Int32Attribute>().data();

  IR_ENFORCE(
      attributes.find("quant_scale") != attributes.end(),
          "'quant_scale' Attribute is expected for FusedBiasResidualLayernormOp. ");
  float quant_scale = attributes.at("quant_scale").dyn_cast<pir::FloatAttribute>().data();

  IR_ENFORCE(
      attributes.find("quant_round_type") != attributes.end(),
          "'quant_round_type' Attribute is expected for FusedBiasResidualLayernormOp. ");
  int quant_round_type = attributes.at("quant_round_type").dyn_cast<pir::Int32Attribute>().data();

  IR_ENFORCE(
      attributes.find("quant_max_bound") != attributes.end(),
          "'quant_max_bound' Attribute is expected for FusedBiasResidualLayernormOp. ");
  float quant_max_bound = attributes.at("quant_max_bound").dyn_cast<pir::FloatAttribute>().data();

  IR_ENFORCE(
      attributes.find("quant_min_bound") != attributes.end(),
          "'quant_min_bound' Attribute is expected for FusedBiasResidualLayernormOp. ");
  float quant_min_bound = attributes.at("quant_min_bound").dyn_cast<pir::FloatAttribute>().data();


  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_, bias_, residual_, norm_weight_, norm_bias_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_epsilon = pir::FloatAttribute::get(pir::IrContext::Instance(), epsilon);
  argument.AddAttribute("epsilon", attr_epsilon);
  pir::Attribute attr_residual_alpha = pir::FloatAttribute::get(pir::IrContext::Instance(), residual_alpha);
  argument.AddAttribute("residual_alpha", attr_residual_alpha);
  pir::Attribute attr_begin_norm_axis = pir::Int32Attribute::get(pir::IrContext::Instance(), begin_norm_axis);
  argument.AddAttribute("begin_norm_axis", attr_begin_norm_axis);
  pir::Attribute attr_quant_scale = pir::FloatAttribute::get(pir::IrContext::Instance(), quant_scale);
  argument.AddAttribute("quant_scale", attr_quant_scale);
  pir::Attribute attr_quant_round_type = pir::Int32Attribute::get(pir::IrContext::Instance(), quant_round_type);
  argument.AddAttribute("quant_round_type", attr_quant_round_type);
  pir::Attribute attr_quant_max_bound = pir::FloatAttribute::get(pir::IrContext::Instance(), quant_max_bound);
  argument.AddAttribute("quant_max_bound", attr_quant_max_bound);
  pir::Attribute attr_quant_min_bound = pir::FloatAttribute::get(pir::IrContext::Instance(), quant_min_bound);
  argument.AddAttribute("quant_min_bound", attr_quant_min_bound);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);

  paddle::dialect::IrMetaTensor meta_bias;
  paddle::dialect::IrTensor ir_tensor_bias;
  if (bias_.impl() != nullptr) {
    paddle::dialect::DenseTensorType bias = bias_.type().dyn_cast<paddle::dialect::DenseTensorType>();
    VLOG(4) << "Builder construction  dense_bias";
    ir_tensor_bias = paddle::dialect::IrTensor(paddle::dialect::TransToPhiDataType(bias.dtype()),
                                                        bias.dims(),
                                                        bias.data_layout(),
                                                        bias.lod(),
                                                        bias.offset());
    VLOG(4) << "Builder construction  meta_bias";
    meta_bias = paddle::dialect::IrMetaTensor(&ir_tensor_bias);
  }


  paddle::dialect::IrMetaTensor meta_residual;
  paddle::dialect::IrTensor ir_tensor_residual;
  if (residual_.impl() != nullptr) {
    paddle::dialect::DenseTensorType residual = residual_.type().dyn_cast<paddle::dialect::DenseTensorType>();
    VLOG(4) << "Builder construction  dense_residual";
    ir_tensor_residual = paddle::dialect::IrTensor(paddle::dialect::TransToPhiDataType(residual.dtype()),
                                                        residual.dims(),
                                                        residual.data_layout(),
                                                        residual.lod(),
                                                        residual.offset());
    VLOG(4) << "Builder construction  meta_residual";
    meta_residual = paddle::dialect::IrMetaTensor(&ir_tensor_residual);
  }


  paddle::dialect::IrMetaTensor meta_norm_weight;
  paddle::dialect::IrTensor ir_tensor_norm_weight;
  if (norm_weight_.impl() != nullptr) {
    paddle::dialect::DenseTensorType norm_weight = norm_weight_.type().dyn_cast<paddle::dialect::DenseTensorType>();
    VLOG(4) << "Builder construction  dense_norm_weight";
    ir_tensor_norm_weight = paddle::dialect::IrTensor(paddle::dialect::TransToPhiDataType(norm_weight.dtype()),
                                                        norm_weight.dims(),
                                                        norm_weight.data_layout(),
                                                        norm_weight.lod(),
                                                        norm_weight.offset());
    VLOG(4) << "Builder construction  meta_norm_weight";
    meta_norm_weight = paddle::dialect::IrMetaTensor(&ir_tensor_norm_weight);
  }


  paddle::dialect::IrMetaTensor meta_norm_bias;
  paddle::dialect::IrTensor ir_tensor_norm_bias;
  if (norm_bias_.impl() != nullptr) {
    paddle::dialect::DenseTensorType norm_bias = norm_bias_.type().dyn_cast<paddle::dialect::DenseTensorType>();
    VLOG(4) << "Builder construction  dense_norm_bias";
    ir_tensor_norm_bias = paddle::dialect::IrTensor(paddle::dialect::TransToPhiDataType(norm_bias.dtype()),
                                                        norm_bias.dims(),
                                                        norm_bias.data_layout(),
                                                        norm_bias.lod(),
                                                        norm_bias.offset());
    VLOG(4) << "Builder construction  meta_norm_bias";
    meta_norm_bias = paddle::dialect::IrMetaTensor(&ir_tensor_norm_bias);
  }

  paddle::dialect::IrTensor dense_out;
  paddle::dialect::IrMetaTensor meta_out(&dense_out);
  paddle::dialect::IrTensor dense_residual_out;
  paddle::dialect::IrMetaTensor meta_residual_out(&dense_residual_out);
  paddle::dialect::IrTensor dense_mean;
  paddle::dialect::IrMetaTensor meta_mean(&dense_mean);
  paddle::dialect::IrTensor dense_variance;
  paddle::dialect::IrMetaTensor meta_variance(&dense_variance);

  phi::FusedLayerNormInferMeta(meta_x, meta_bias, meta_residual, meta_norm_weight, meta_norm_bias, epsilon, residual_alpha, begin_norm_axis, quant_scale, quant_round_type, quant_max_bound, quant_min_bound, &meta_out, &meta_residual_out, &meta_mean, &meta_variance);

  std::vector<pir::Type> argument_outputs;
  pir::Type out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_out.dtype()), dense_out.dims(), dense_out.layout(), dense_out.lod(), dense_out.offset());
  argument_outputs.push_back(out_dense_tensor_type);

  pir::Type residual_out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_residual_out.dtype()), dense_residual_out.dims(), dense_residual_out.layout(), dense_residual_out.lod(), dense_residual_out.offset());
  argument_outputs.push_back(residual_out_dense_tensor_type);

  pir::Type mean_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_mean.dtype()), dense_mean.dims(), dense_mean.layout(), dense_mean.lod(), dense_mean.offset());
  argument_outputs.push_back(mean_dense_tensor_type);

  pir::Type variance_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_variance.dtype()), dense_variance.dims(), dense_variance.layout(), dense_variance.lod(), dense_variance.offset());
  argument_outputs.push_back(variance_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void FusedBiasResidualLayernormOp::VerifySig() {
  VLOG(4) << "Start Verifying inputs, outputs and attributes for: FusedBiasResidualLayernormOp.";
  VLOG(4) << "Verifying inputs:";
  {
  auto input_size = num_operands();
  IR_ENFORCE(input_size == 5u,
                    "The size %d of inputs must be equal to 5.", input_size);
  IR_ENFORCE((*this)->operand_source(0).type().isa<paddle::dialect::DenseTensorType>(),
                  "Type validation failed for the 0th input, got %s.", (*this)->operand_source(0).type());
  if (auto val = (*this)->operand(1)) {
    IR_ENFORCE(val.type().isa<paddle::dialect::DenseTensorType>(),
                   "Type validation failed for the 1th input, got %s.", (*this)->operand_source(1).type());
  }
  if (auto val = (*this)->operand(2)) {
    IR_ENFORCE(val.type().isa<paddle::dialect::DenseTensorType>(),
                   "Type validation failed for the 2th input, got %s.", (*this)->operand_source(2).type());
  }
  if (auto val = (*this)->operand(3)) {
    IR_ENFORCE(val.type().isa<paddle::dialect::DenseTensorType>(),
                   "Type validation failed for the 3th input, got %s.", (*this)->operand_source(3).type());
  }
  if (auto val = (*this)->operand(4)) {
    IR_ENFORCE(val.type().isa<paddle::dialect::DenseTensorType>(),
                   "Type validation failed for the 4th input, got %s.", (*this)->operand_source(4).type());
  }
  }
  VLOG(4) << "Verifying attributes:";
  {
  auto& attributes = this->attributes();
  IR_ENFORCE(attributes.count("epsilon")>0,
                 "epsilon does not exist.");
  IR_ENFORCE(attributes.at("epsilon").isa<pir::FloatAttribute>(),
                 "Type of attribute: epsilon is not pir::FloatAttribute.");

  IR_ENFORCE(attributes.count("residual_alpha")>0,
                 "residual_alpha does not exist.");
  IR_ENFORCE(attributes.at("residual_alpha").isa<pir::FloatAttribute>(),
                 "Type of attribute: residual_alpha is not pir::FloatAttribute.");

  IR_ENFORCE(attributes.count("begin_norm_axis")>0,
                 "begin_norm_axis does not exist.");
  IR_ENFORCE(attributes.at("begin_norm_axis").isa<pir::Int32Attribute>(),
                 "Type of attribute: begin_norm_axis is not pir::Int32Attribute.");

  IR_ENFORCE(attributes.count("quant_scale")>0,
                 "quant_scale does not exist.");
  IR_ENFORCE(attributes.at("quant_scale").isa<pir::FloatAttribute>(),
                 "Type of attribute: quant_scale is not pir::FloatAttribute.");

  IR_ENFORCE(attributes.count("quant_round_type")>0,
                 "quant_round_type does not exist.");
  IR_ENFORCE(attributes.at("quant_round_type").isa<pir::Int32Attribute>(),
                 "Type of attribute: quant_round_type is not pir::Int32Attribute.");

  IR_ENFORCE(attributes.count("quant_max_bound")>0,
                 "quant_max_bound does not exist.");
  IR_ENFORCE(attributes.at("quant_max_bound").isa<pir::FloatAttribute>(),
                 "Type of attribute: quant_max_bound is not pir::FloatAttribute.");

  IR_ENFORCE(attributes.count("quant_min_bound")>0,
                 "quant_min_bound does not exist.");
  IR_ENFORCE(attributes.at("quant_min_bound").isa<pir::FloatAttribute>(),
                 "Type of attribute: quant_min_bound is not pir::FloatAttribute.");

  }
  VLOG(4) << "Verifying outputs:";
  {
  auto output_size = num_results();
  IR_ENFORCE(output_size == 4u,
                    "The size %d of outputs must be equal to 4.", output_size);
  IR_ENFORCE((*this)->result(0).type().isa<paddle::dialect::DenseTensorType>(),
                 "Type validation failed for the 0th output.");
  if (auto output_1_type = (*this)->result(1).type()) {
    IR_ENFORCE(output_1_type.isa<paddle::dialect::DenseTensorType>(),
                   "Type validation failed for the 1th output.");
  }
  IR_ENFORCE((*this)->result(2).type().isa<paddle::dialect::DenseTensorType>(),
                 "Type validation failed for the 2th output.");
  IR_ENFORCE((*this)->result(3).type().isa<paddle::dialect::DenseTensorType>(),
                 "Type validation failed for the 3th output.");
  }
  VLOG(4) << "End Verifying for: FusedBiasResidualLayernormOp.";
}

void FusedBiasResidualLayernormOp::InferMeta( phi::InferMetaContext *infer_meta ) {
  auto fn = PD_INFER_META(phi::FusedLayerNormInferMeta);
  fn(infer_meta);
}

phi::DataType FusedBiasResidualLayernormOp::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: FusedBiasResidualLayernormOp";
  


  return expected_kernel_dtype;
}

const char *FusedConv2dAddActOp::attributes_name[11] = { "strides", "paddings", "padding_algorithm", "dilations", "groups", "data_format", "activation", "split_channels", "exhaustive_search", "workspace_size_MB", "fuse_alpha" };

OpInfoTuple FusedConv2dAddActOp::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("input", "paddle::dialect::DenseTensorType", false, false, false, false), paddle::dialect::OpInputInfo("filter", "paddle::dialect::DenseTensorType", false, false, false, false), paddle::dialect::OpInputInfo("bias", "paddle::dialect::DenseTensorType", true, false, false, false), paddle::dialect::OpInputInfo("residual_data", "paddle::dialect::DenseTensorType", true, false, false, false) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = { paddle::dialect::OpAttributeInfo("strides", "pir::ArrayAttribute<pir::Int32Attribute>", ""), paddle::dialect::OpAttributeInfo("paddings", "pir::ArrayAttribute<pir::Int32Attribute>", ""), paddle::dialect::OpAttributeInfo("padding_algorithm", "pir::StrAttribute", ""), paddle::dialect::OpAttributeInfo("dilations", "pir::ArrayAttribute<pir::Int32Attribute>", ""), paddle::dialect::OpAttributeInfo("groups", "pir::Int32Attribute", ""), paddle::dialect::OpAttributeInfo("data_format", "pir::StrAttribute", ""), paddle::dialect::OpAttributeInfo("activation", "pir::StrAttribute", ""), paddle::dialect::OpAttributeInfo("split_channels", "pir::ArrayAttribute<pir::Int32Attribute>", ""), paddle::dialect::OpAttributeInfo("exhaustive_search", "pir::BoolAttribute", ""), paddle::dialect::OpAttributeInfo("workspace_size_MB", "pir::Int32Attribute", ""), paddle::dialect::OpAttributeInfo("fuse_alpha", "pir::FloatAttribute", "") };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("output", "paddle::dialect::DenseTensorType", false, false), paddle::dialect::OpOutputInfo("outputs", "pir::VectorType<paddle::dialect::DenseTensorType>", true, false) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("FusedConv2dAddActInferMeta", {"input", "filter", "bias", "residual_data", "strides", "paddings", "padding_algorithm", "dilations", "groups", "data_format", "activation", "split_channels"}, "fused_conv2d_add_act", {"input", "filter", "bias", "residual_data", "strides", "paddings", "padding_algorithm", "dilations", "groups", "data_format", "activation", "split_channels", "exhaustive_search", "workspace_size_MB", "fuse_alpha"}, {"input"}, {}, {}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "fused_conv2d_add_act");
}

void FusedConv2dAddActOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value input_, pir::Value filter_, pir::Value bias_, pir::Value residual_data_, const std::vector<int>& strides, const std::vector<int>& paddings, const std::string& padding_algorithm, const std::vector<int>& dilations, int groups, const std::string& data_format, const std::string& activation, const std::vector<int>& split_channels, bool exhaustive_search, int workspace_size_MB, float fuse_alpha) {
  VLOG(4) << "Start build FusedConv2dAddActOp";



  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {input_, filter_, bias_, residual_data_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  std::vector<pir::Attribute> vec_strides;
  for (size_t i = 0; i < static_cast<size_t>(strides.size()); i++) {
      pir::Attribute attr_strides = pir::Int32Attribute::get(pir::IrContext::Instance(), strides[i]);

    vec_strides.push_back(attr_strides);
  }
  pir::Attribute attr_strides = pir::ArrayAttribute::get(pir::IrContext::Instance(), vec_strides);
  argument.AddAttribute("strides", attr_strides);
  std::vector<pir::Attribute> vec_paddings;
  for (size_t i = 0; i < static_cast<size_t>(paddings.size()); i++) {
      pir::Attribute attr_paddings = pir::Int32Attribute::get(pir::IrContext::Instance(), paddings[i]);

    vec_paddings.push_back(attr_paddings);
  }
  pir::Attribute attr_paddings = pir::ArrayAttribute::get(pir::IrContext::Instance(), vec_paddings);
  argument.AddAttribute("paddings", attr_paddings);
  pir::Attribute attr_padding_algorithm = pir::StrAttribute::get(pir::IrContext::Instance(), padding_algorithm);
  argument.AddAttribute("padding_algorithm", attr_padding_algorithm);
  std::vector<pir::Attribute> vec_dilations;
  for (size_t i = 0; i < static_cast<size_t>(dilations.size()); i++) {
      pir::Attribute attr_dilations = pir::Int32Attribute::get(pir::IrContext::Instance(), dilations[i]);

    vec_dilations.push_back(attr_dilations);
  }
  pir::Attribute attr_dilations = pir::ArrayAttribute::get(pir::IrContext::Instance(), vec_dilations);
  argument.AddAttribute("dilations", attr_dilations);
  pir::Attribute attr_groups = pir::Int32Attribute::get(pir::IrContext::Instance(), groups);
  argument.AddAttribute("groups", attr_groups);
  pir::Attribute attr_data_format = pir::StrAttribute::get(pir::IrContext::Instance(), data_format);
  argument.AddAttribute("data_format", attr_data_format);
  pir::Attribute attr_activation = pir::StrAttribute::get(pir::IrContext::Instance(), activation);
  argument.AddAttribute("activation", attr_activation);
  std::vector<pir::Attribute> vec_split_channels;
  for (size_t i = 0; i < static_cast<size_t>(split_channels.size()); i++) {
      pir::Attribute attr_split_channels = pir::Int32Attribute::get(pir::IrContext::Instance(), split_channels[i]);

    vec_split_channels.push_back(attr_split_channels);
  }
  pir::Attribute attr_split_channels = pir::ArrayAttribute::get(pir::IrContext::Instance(), vec_split_channels);
  argument.AddAttribute("split_channels", attr_split_channels);
  pir::Attribute attr_exhaustive_search = pir::BoolAttribute::get(pir::IrContext::Instance(), exhaustive_search);
  argument.AddAttribute("exhaustive_search", attr_exhaustive_search);
  pir::Attribute attr_workspace_size_MB = pir::Int32Attribute::get(pir::IrContext::Instance(), workspace_size_MB);
  argument.AddAttribute("workspace_size_MB", attr_workspace_size_MB);
  pir::Attribute attr_fuse_alpha = pir::FloatAttribute::get(pir::IrContext::Instance(), fuse_alpha);
  argument.AddAttribute("fuse_alpha", attr_fuse_alpha);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType input = input_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)input;
  paddle::dialect::DenseTensorType filter = filter_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)filter;

  VLOG(4) << "Builder construction  dense_input";
  paddle::dialect::IrTensor ir_tensor_input(paddle::dialect::TransToPhiDataType(input.dtype()),
                                                      input.dims(),
                                                      input.data_layout(),
                                                      input.lod(),
                                                      input.offset());
  VLOG(4) << "Builder construction  meta_input";
  paddle::dialect::IrMetaTensor meta_input(&ir_tensor_input);

  VLOG(4) << "Builder construction  dense_filter";
  paddle::dialect::IrTensor ir_tensor_filter(paddle::dialect::TransToPhiDataType(filter.dtype()),
                                                      filter.dims(),
                                                      filter.data_layout(),
                                                      filter.lod(),
                                                      filter.offset());
  VLOG(4) << "Builder construction  meta_filter";
  paddle::dialect::IrMetaTensor meta_filter(&ir_tensor_filter);

  paddle::dialect::IrMetaTensor meta_bias;
  paddle::dialect::IrTensor ir_tensor_bias;
  if (bias_.impl() != nullptr) {
    paddle::dialect::DenseTensorType bias = bias_.type().dyn_cast<paddle::dialect::DenseTensorType>();
    VLOG(4) << "Builder construction  dense_bias";
    ir_tensor_bias = paddle::dialect::IrTensor(paddle::dialect::TransToPhiDataType(bias.dtype()),
                                                        bias.dims(),
                                                        bias.data_layout(),
                                                        bias.lod(),
                                                        bias.offset());
    VLOG(4) << "Builder construction  meta_bias";
    meta_bias = paddle::dialect::IrMetaTensor(&ir_tensor_bias);
  }


  paddle::dialect::IrMetaTensor meta_residual_data;
  paddle::dialect::IrTensor ir_tensor_residual_data;
  if (residual_data_.impl() != nullptr) {
    paddle::dialect::DenseTensorType residual_data = residual_data_.type().dyn_cast<paddle::dialect::DenseTensorType>();
    VLOG(4) << "Builder construction  dense_residual_data";
    ir_tensor_residual_data = paddle::dialect::IrTensor(paddle::dialect::TransToPhiDataType(residual_data.dtype()),
                                                        residual_data.dims(),
                                                        residual_data.data_layout(),
                                                        residual_data.lod(),
                                                        residual_data.offset());
    VLOG(4) << "Builder construction  meta_residual_data";
    meta_residual_data = paddle::dialect::IrMetaTensor(&ir_tensor_residual_data);
  }

  paddle::dialect::IrTensor dense_output;
  paddle::dialect::IrMetaTensor meta_output(&dense_output);
  std::vector<paddle::dialect::IrTensor> vec_dense_outputs((split_channels.size()), paddle::dialect::IrTensor());
  std::vector<paddle::dialect::IrMetaTensor> vec_meta_outputs;
  for (size_t i=0; i < static_cast<size_t>(split_channels.size()); i++) {
    vec_meta_outputs.push_back(paddle::dialect::IrMetaTensor(&vec_dense_outputs[i]));
  }
  std::vector<phi::MetaTensor*> meta_outputs;
  for (size_t i=0; i < static_cast<size_t>(vec_meta_outputs.size()); i++) {
    meta_outputs.push_back(&vec_meta_outputs[i]);
  }

  phi::FusedConv2dAddActInferMeta(meta_input, meta_filter, meta_bias, meta_residual_data, strides, paddings, padding_algorithm, dilations, groups, data_format, activation, split_channels, &meta_output, meta_outputs, phi::MetaConfig(false, false));

  std::vector<pir::Type> argument_outputs;
  pir::Type output_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_output.dtype()), dense_output.dims(), dense_output.layout(), dense_output.lod(), dense_output.offset());
  argument_outputs.push_back(output_dense_tensor_type);

  std::vector<pir::Type> outputs_types;
  for (size_t i=0; i < static_cast<size_t>(split_channels.size()); i++) {
    outputs_types.push_back(paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(vec_dense_outputs[i].dtype()), vec_dense_outputs[i].dims(), vec_dense_outputs[i].layout(), vec_dense_outputs[i].lod(), vec_dense_outputs[i].offset()));
  }
  pir::Type outputs_vector_type = pir::VectorType::get(pir::IrContext::Instance(), outputs_types);
  argument_outputs.push_back(outputs_vector_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void FusedConv2dAddActOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value input_, pir::Value filter_, pir::Value bias_, pir::Value residual_data_, pir::AttributeMap attributes) {
  VLOG(4) << "Start build FusedConv2dAddActOp";


  IR_ENFORCE(
      attributes.find("strides") != attributes.end(),
          "'strides' Attribute is expected for FusedConv2dAddActOp. ");
  std::vector<int> strides;
  for (size_t i = 0; i < attributes.at("strides").dyn_cast<pir::ArrayAttribute>().size(); i++) {
    strides.push_back(attributes.at("strides").dyn_cast<pir::ArrayAttribute>().at(i).dyn_cast<pir::Int32Attribute>().data());
  }

  IR_ENFORCE(
      attributes.find("paddings") != attributes.end(),
          "'paddings' Attribute is expected for FusedConv2dAddActOp. ");
  std::vector<int> paddings;
  for (size_t i = 0; i < attributes.at("paddings").dyn_cast<pir::ArrayAttribute>().size(); i++) {
    paddings.push_back(attributes.at("paddings").dyn_cast<pir::ArrayAttribute>().at(i).dyn_cast<pir::Int32Attribute>().data());
  }

  IR_ENFORCE(
      attributes.find("padding_algorithm") != attributes.end(),
          "'padding_algorithm' Attribute is expected for FusedConv2dAddActOp. ");
  std::string padding_algorithm = attributes.at("padding_algorithm").dyn_cast<pir::StrAttribute>().AsString();

  IR_ENFORCE(
      attributes.find("dilations") != attributes.end(),
          "'dilations' Attribute is expected for FusedConv2dAddActOp. ");
  std::vector<int> dilations;
  for (size_t i = 0; i < attributes.at("dilations").dyn_cast<pir::ArrayAttribute>().size(); i++) {
    dilations.push_back(attributes.at("dilations").dyn_cast<pir::ArrayAttribute>().at(i).dyn_cast<pir::Int32Attribute>().data());
  }

  IR_ENFORCE(
      attributes.find("groups") != attributes.end(),
          "'groups' Attribute is expected for FusedConv2dAddActOp. ");
  int groups = attributes.at("groups").dyn_cast<pir::Int32Attribute>().data();

  IR_ENFORCE(
      attributes.find("data_format") != attributes.end(),
          "'data_format' Attribute is expected for FusedConv2dAddActOp. ");
  std::string data_format = attributes.at("data_format").dyn_cast<pir::StrAttribute>().AsString();

  IR_ENFORCE(
      attributes.find("activation") != attributes.end(),
          "'activation' Attribute is expected for FusedConv2dAddActOp. ");
  std::string activation = attributes.at("activation").dyn_cast<pir::StrAttribute>().AsString();

  IR_ENFORCE(
      attributes.find("split_channels") != attributes.end(),
          "'split_channels' Attribute is expected for FusedConv2dAddActOp. ");
  std::vector<int> split_channels;
  for (size_t i = 0; i < attributes.at("split_channels").dyn_cast<pir::ArrayAttribute>().size(); i++) {
    split_channels.push_back(attributes.at("split_channels").dyn_cast<pir::ArrayAttribute>().at(i).dyn_cast<pir::Int32Attribute>().data());
  }

  IR_ENFORCE(
      attributes.find("exhaustive_search") != attributes.end(),
          "'exhaustive_search' Attribute is expected for FusedConv2dAddActOp. ");
  bool exhaustive_search = attributes.at("exhaustive_search").dyn_cast<pir::BoolAttribute>().data();

  IR_ENFORCE(
      attributes.find("workspace_size_MB") != attributes.end(),
          "'workspace_size_MB' Attribute is expected for FusedConv2dAddActOp. ");
  int workspace_size_MB = attributes.at("workspace_size_MB").dyn_cast<pir::Int32Attribute>().data();

  IR_ENFORCE(
      attributes.find("fuse_alpha") != attributes.end(),
          "'fuse_alpha' Attribute is expected for FusedConv2dAddActOp. ");
  float fuse_alpha = attributes.at("fuse_alpha").dyn_cast<pir::FloatAttribute>().data();


  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {input_, filter_, bias_, residual_data_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  std::vector<pir::Attribute> vec_strides;
  for (size_t i = 0; i < static_cast<size_t>(strides.size()); i++) {
      pir::Attribute attr_strides = pir::Int32Attribute::get(pir::IrContext::Instance(), strides[i]);

    vec_strides.push_back(attr_strides);
  }
  pir::Attribute attr_strides = pir::ArrayAttribute::get(pir::IrContext::Instance(), vec_strides);
  argument.AddAttribute("strides", attr_strides);
  std::vector<pir::Attribute> vec_paddings;
  for (size_t i = 0; i < static_cast<size_t>(paddings.size()); i++) {
      pir::Attribute attr_paddings = pir::Int32Attribute::get(pir::IrContext::Instance(), paddings[i]);

    vec_paddings.push_back(attr_paddings);
  }
  pir::Attribute attr_paddings = pir::ArrayAttribute::get(pir::IrContext::Instance(), vec_paddings);
  argument.AddAttribute("paddings", attr_paddings);
  pir::Attribute attr_padding_algorithm = pir::StrAttribute::get(pir::IrContext::Instance(), padding_algorithm);
  argument.AddAttribute("padding_algorithm", attr_padding_algorithm);
  std::vector<pir::Attribute> vec_dilations;
  for (size_t i = 0; i < static_cast<size_t>(dilations.size()); i++) {
      pir::Attribute attr_dilations = pir::Int32Attribute::get(pir::IrContext::Instance(), dilations[i]);

    vec_dilations.push_back(attr_dilations);
  }
  pir::Attribute attr_dilations = pir::ArrayAttribute::get(pir::IrContext::Instance(), vec_dilations);
  argument.AddAttribute("dilations", attr_dilations);
  pir::Attribute attr_groups = pir::Int32Attribute::get(pir::IrContext::Instance(), groups);
  argument.AddAttribute("groups", attr_groups);
  pir::Attribute attr_data_format = pir::StrAttribute::get(pir::IrContext::Instance(), data_format);
  argument.AddAttribute("data_format", attr_data_format);
  pir::Attribute attr_activation = pir::StrAttribute::get(pir::IrContext::Instance(), activation);
  argument.AddAttribute("activation", attr_activation);
  std::vector<pir::Attribute> vec_split_channels;
  for (size_t i = 0; i < static_cast<size_t>(split_channels.size()); i++) {
      pir::Attribute attr_split_channels = pir::Int32Attribute::get(pir::IrContext::Instance(), split_channels[i]);

    vec_split_channels.push_back(attr_split_channels);
  }
  pir::Attribute attr_split_channels = pir::ArrayAttribute::get(pir::IrContext::Instance(), vec_split_channels);
  argument.AddAttribute("split_channels", attr_split_channels);
  pir::Attribute attr_exhaustive_search = pir::BoolAttribute::get(pir::IrContext::Instance(), exhaustive_search);
  argument.AddAttribute("exhaustive_search", attr_exhaustive_search);
  pir::Attribute attr_workspace_size_MB = pir::Int32Attribute::get(pir::IrContext::Instance(), workspace_size_MB);
  argument.AddAttribute("workspace_size_MB", attr_workspace_size_MB);
  pir::Attribute attr_fuse_alpha = pir::FloatAttribute::get(pir::IrContext::Instance(), fuse_alpha);
  argument.AddAttribute("fuse_alpha", attr_fuse_alpha);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType input = input_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)input;
  paddle::dialect::DenseTensorType filter = filter_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)filter;

  VLOG(4) << "Builder construction  dense_input";
  paddle::dialect::IrTensor ir_tensor_input(paddle::dialect::TransToPhiDataType(input.dtype()),
                                                      input.dims(),
                                                      input.data_layout(),
                                                      input.lod(),
                                                      input.offset());
  VLOG(4) << "Builder construction  meta_input";
  paddle::dialect::IrMetaTensor meta_input(&ir_tensor_input);

  VLOG(4) << "Builder construction  dense_filter";
  paddle::dialect::IrTensor ir_tensor_filter(paddle::dialect::TransToPhiDataType(filter.dtype()),
                                                      filter.dims(),
                                                      filter.data_layout(),
                                                      filter.lod(),
                                                      filter.offset());
  VLOG(4) << "Builder construction  meta_filter";
  paddle::dialect::IrMetaTensor meta_filter(&ir_tensor_filter);

  paddle::dialect::IrMetaTensor meta_bias;
  paddle::dialect::IrTensor ir_tensor_bias;
  if (bias_.impl() != nullptr) {
    paddle::dialect::DenseTensorType bias = bias_.type().dyn_cast<paddle::dialect::DenseTensorType>();
    VLOG(4) << "Builder construction  dense_bias";
    ir_tensor_bias = paddle::dialect::IrTensor(paddle::dialect::TransToPhiDataType(bias.dtype()),
                                                        bias.dims(),
                                                        bias.data_layout(),
                                                        bias.lod(),
                                                        bias.offset());
    VLOG(4) << "Builder construction  meta_bias";
    meta_bias = paddle::dialect::IrMetaTensor(&ir_tensor_bias);
  }


  paddle::dialect::IrMetaTensor meta_residual_data;
  paddle::dialect::IrTensor ir_tensor_residual_data;
  if (residual_data_.impl() != nullptr) {
    paddle::dialect::DenseTensorType residual_data = residual_data_.type().dyn_cast<paddle::dialect::DenseTensorType>();
    VLOG(4) << "Builder construction  dense_residual_data";
    ir_tensor_residual_data = paddle::dialect::IrTensor(paddle::dialect::TransToPhiDataType(residual_data.dtype()),
                                                        residual_data.dims(),
                                                        residual_data.data_layout(),
                                                        residual_data.lod(),
                                                        residual_data.offset());
    VLOG(4) << "Builder construction  meta_residual_data";
    meta_residual_data = paddle::dialect::IrMetaTensor(&ir_tensor_residual_data);
  }

  paddle::dialect::IrTensor dense_output;
  paddle::dialect::IrMetaTensor meta_output(&dense_output);
  std::vector<paddle::dialect::IrTensor> vec_dense_outputs((split_channels.size()), paddle::dialect::IrTensor());
  std::vector<paddle::dialect::IrMetaTensor> vec_meta_outputs;
  for (size_t i=0; i < static_cast<size_t>(split_channels.size()); i++) {
    vec_meta_outputs.push_back(paddle::dialect::IrMetaTensor(&vec_dense_outputs[i]));
  }
  std::vector<phi::MetaTensor*> meta_outputs;
  for (size_t i=0; i < static_cast<size_t>(vec_meta_outputs.size()); i++) {
    meta_outputs.push_back(&vec_meta_outputs[i]);
  }

  phi::FusedConv2dAddActInferMeta(meta_input, meta_filter, meta_bias, meta_residual_data, strides, paddings, padding_algorithm, dilations, groups, data_format, activation, split_channels, &meta_output, meta_outputs, phi::MetaConfig(false, false));

  std::vector<pir::Type> argument_outputs;
  pir::Type output_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_output.dtype()), dense_output.dims(), dense_output.layout(), dense_output.lod(), dense_output.offset());
  argument_outputs.push_back(output_dense_tensor_type);

  std::vector<pir::Type> outputs_types;
  for (size_t i=0; i < static_cast<size_t>(split_channels.size()); i++) {
    outputs_types.push_back(paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(vec_dense_outputs[i].dtype()), vec_dense_outputs[i].dims(), vec_dense_outputs[i].layout(), vec_dense_outputs[i].lod(), vec_dense_outputs[i].offset()));
  }
  pir::Type outputs_vector_type = pir::VectorType::get(pir::IrContext::Instance(), outputs_types);
  argument_outputs.push_back(outputs_vector_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void FusedConv2dAddActOp::VerifySig() {
  VLOG(4) << "Start Verifying inputs, outputs and attributes for: FusedConv2dAddActOp.";
  VLOG(4) << "Verifying inputs:";
  {
  auto input_size = num_operands();
  IR_ENFORCE(input_size == 4u,
                    "The size %d of inputs must be equal to 4.", input_size);
  IR_ENFORCE((*this)->operand_source(0).type().isa<paddle::dialect::DenseTensorType>(),
                  "Type validation failed for the 0th input, got %s.", (*this)->operand_source(0).type());
  IR_ENFORCE((*this)->operand_source(1).type().isa<paddle::dialect::DenseTensorType>(),
                  "Type validation failed for the 1th input, got %s.", (*this)->operand_source(1).type());
  if (auto val = (*this)->operand(2)) {
    IR_ENFORCE(val.type().isa<paddle::dialect::DenseTensorType>(),
                   "Type validation failed for the 2th input, got %s.", (*this)->operand_source(2).type());
  }
  if (auto val = (*this)->operand(3)) {
    IR_ENFORCE(val.type().isa<paddle::dialect::DenseTensorType>(),
                   "Type validation failed for the 3th input, got %s.", (*this)->operand_source(3).type());
  }
  }
  VLOG(4) << "Verifying attributes:";
  {
  auto& attributes = this->attributes();
  IR_ENFORCE(attributes.count("strides")>0,
                 "strides does not exist.");
  IR_ENFORCE(attributes.at("strides").isa<pir::ArrayAttribute>(),
                 "Type of attribute: strides is not pir::ArrayAttribute.");
  for (size_t i = 0; i < attributes.at("strides").dyn_cast<pir::ArrayAttribute>().size(); i++) {
    IR_ENFORCE(attributes.at("strides").dyn_cast<pir::ArrayAttribute>().at(i).isa<pir::Int32Attribute>(),
                   "Type of attribute: strides is not right.");
  }
  IR_ENFORCE(attributes.count("paddings")>0,
                 "paddings does not exist.");
  IR_ENFORCE(attributes.at("paddings").isa<pir::ArrayAttribute>(),
                 "Type of attribute: paddings is not pir::ArrayAttribute.");
  for (size_t i = 0; i < attributes.at("paddings").dyn_cast<pir::ArrayAttribute>().size(); i++) {
    IR_ENFORCE(attributes.at("paddings").dyn_cast<pir::ArrayAttribute>().at(i).isa<pir::Int32Attribute>(),
                   "Type of attribute: paddings is not right.");
  }
  IR_ENFORCE(attributes.count("padding_algorithm")>0,
                 "padding_algorithm does not exist.");
  IR_ENFORCE(attributes.at("padding_algorithm").isa<pir::StrAttribute>(),
                 "Type of attribute: padding_algorithm is not pir::StrAttribute.");

  IR_ENFORCE(attributes.count("dilations")>0,
                 "dilations does not exist.");
  IR_ENFORCE(attributes.at("dilations").isa<pir::ArrayAttribute>(),
                 "Type of attribute: dilations is not pir::ArrayAttribute.");
  for (size_t i = 0; i < attributes.at("dilations").dyn_cast<pir::ArrayAttribute>().size(); i++) {
    IR_ENFORCE(attributes.at("dilations").dyn_cast<pir::ArrayAttribute>().at(i).isa<pir::Int32Attribute>(),
                   "Type of attribute: dilations is not right.");
  }
  IR_ENFORCE(attributes.count("groups")>0,
                 "groups does not exist.");
  IR_ENFORCE(attributes.at("groups").isa<pir::Int32Attribute>(),
                 "Type of attribute: groups is not pir::Int32Attribute.");

  IR_ENFORCE(attributes.count("data_format")>0,
                 "data_format does not exist.");
  IR_ENFORCE(attributes.at("data_format").isa<pir::StrAttribute>(),
                 "Type of attribute: data_format is not pir::StrAttribute.");

  IR_ENFORCE(attributes.count("activation")>0,
                 "activation does not exist.");
  IR_ENFORCE(attributes.at("activation").isa<pir::StrAttribute>(),
                 "Type of attribute: activation is not pir::StrAttribute.");

  IR_ENFORCE(attributes.count("split_channels")>0,
                 "split_channels does not exist.");
  IR_ENFORCE(attributes.at("split_channels").isa<pir::ArrayAttribute>(),
                 "Type of attribute: split_channels is not pir::ArrayAttribute.");
  for (size_t i = 0; i < attributes.at("split_channels").dyn_cast<pir::ArrayAttribute>().size(); i++) {
    IR_ENFORCE(attributes.at("split_channels").dyn_cast<pir::ArrayAttribute>().at(i).isa<pir::Int32Attribute>(),
                   "Type of attribute: split_channels is not right.");
  }
  IR_ENFORCE(attributes.count("exhaustive_search")>0,
                 "exhaustive_search does not exist.");
  IR_ENFORCE(attributes.at("exhaustive_search").isa<pir::BoolAttribute>(),
                 "Type of attribute: exhaustive_search is not pir::BoolAttribute.");

  IR_ENFORCE(attributes.count("workspace_size_MB")>0,
                 "workspace_size_MB does not exist.");
  IR_ENFORCE(attributes.at("workspace_size_MB").isa<pir::Int32Attribute>(),
                 "Type of attribute: workspace_size_MB is not pir::Int32Attribute.");

  IR_ENFORCE(attributes.count("fuse_alpha")>0,
                 "fuse_alpha does not exist.");
  IR_ENFORCE(attributes.at("fuse_alpha").isa<pir::FloatAttribute>(),
                 "Type of attribute: fuse_alpha is not pir::FloatAttribute.");

  }
  VLOG(4) << "Verifying outputs:";
  {
  auto output_size = num_results();
  IR_ENFORCE(output_size == 2u,
                    "The size %d of outputs must be equal to 2.", output_size);
  IR_ENFORCE((*this)->result(0).type().isa<paddle::dialect::DenseTensorType>(),
                 "Type validation failed for the 0th output.");
  if (auto output_1_type = (*this)->result(1).type()) {
    if (auto vec_type = output_1_type.dyn_cast<pir::VectorType>()) {
      for (size_t i = 0; i < vec_type.size(); ++i) {
        IR_ENFORCE(vec_type[i].isa<paddle::dialect::DenseTensorType>(),
                       "Type validation failed for the 1th output.");
      }
    }
    else {
      IR_ENFORCE(output_1_type.isa<paddle::dialect::DenseTensorType>(),
                     "Type validation failed for the 1th output.");
    }
  }
  }
  VLOG(4) << "End Verifying for: FusedConv2dAddActOp.";
}

void FusedConv2dAddActOp::InferMeta( phi::InferMetaContext *infer_meta ) {
  auto fn = PD_INFER_META(phi::FusedConv2dAddActInferMeta);
  fn(infer_meta);
}

phi::DataType FusedConv2dAddActOp::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: FusedConv2dAddActOp";
  


  return expected_kernel_dtype;
}

const char *FusedDconvDreluDbnOp::attributes_name[10] = { "paddings", "dilations", "strides", "padding_algorithm", "groups", "data_format", "fuse_shortcut", "fuse_dual", "fuse_add", "exhaustive_search" };

OpInfoTuple FusedDconvDreluDbnOp::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("grad_output", "paddle::dialect::DenseTensorType", false, false, false, false), paddle::dialect::OpInputInfo("weight", "paddle::dialect::DenseTensorType", false, false, false, false), paddle::dialect::OpInputInfo("grad_output_add", "paddle::dialect::DenseTensorType", true, false, false, false), paddle::dialect::OpInputInfo("residual_input", "paddle::dialect::DenseTensorType", true, false, false, false), paddle::dialect::OpInputInfo("bn1_eqscale", "paddle::dialect::DenseTensorType", true, false, false, false), paddle::dialect::OpInputInfo("bn1_eqbias", "paddle::dialect::DenseTensorType", true, false, false, false), paddle::dialect::OpInputInfo("conv_input", "paddle::dialect::DenseTensorType", true, false, false, false), paddle::dialect::OpInputInfo("bn1_mean", "paddle::dialect::DenseTensorType", false, false, false, false), paddle::dialect::OpInputInfo("bn1_inv_std", "paddle::dialect::DenseTensorType", false, false, false, false), paddle::dialect::OpInputInfo("bn1_gamma", "paddle::dialect::DenseTensorType", false, false, false, false), paddle::dialect::OpInputInfo("bn1_beta", "paddle::dialect::DenseTensorType", false, false, false, false), paddle::dialect::OpInputInfo("bn1_input", "paddle::dialect::DenseTensorType", false, false, false, false), paddle::dialect::OpInputInfo("bn2_mean", "paddle::dialect::DenseTensorType", true, false, false, false), paddle::dialect::OpInputInfo("bn2_inv_std", "paddle::dialect::DenseTensorType", true, false, false, false), paddle::dialect::OpInputInfo("bn2_gamma", "paddle::dialect::DenseTensorType", true, false, false, false), paddle::dialect::OpInputInfo("bn2_beta", "paddle::dialect::DenseTensorType", true, false, false, false), paddle::dialect::OpInputInfo("bn2_input", "paddle::dialect::DenseTensorType", true, false, false, false) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = { paddle::dialect::OpAttributeInfo("paddings", "pir::ArrayAttribute<pir::Int32Attribute>", ""), paddle::dialect::OpAttributeInfo("dilations", "pir::ArrayAttribute<pir::Int32Attribute>", ""), paddle::dialect::OpAttributeInfo("strides", "pir::ArrayAttribute<pir::Int32Attribute>", ""), paddle::dialect::OpAttributeInfo("padding_algorithm", "pir::StrAttribute", ""), paddle::dialect::OpAttributeInfo("groups", "pir::Int32Attribute", ""), paddle::dialect::OpAttributeInfo("data_format", "pir::StrAttribute", ""), paddle::dialect::OpAttributeInfo("fuse_shortcut", "pir::BoolAttribute", ""), paddle::dialect::OpAttributeInfo("fuse_dual", "pir::BoolAttribute", ""), paddle::dialect::OpAttributeInfo("fuse_add", "pir::BoolAttribute", ""), paddle::dialect::OpAttributeInfo("exhaustive_search", "pir::BoolAttribute", "") };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("grad_weight", "paddle::dialect::DenseTensorType", false, false), paddle::dialect::OpOutputInfo("grad_bn1_input", "paddle::dialect::DenseTensorType", false, false), paddle::dialect::OpOutputInfo("grad_bn1_gamma", "paddle::dialect::DenseTensorType", false, false), paddle::dialect::OpOutputInfo("grad_bn1_beta", "paddle::dialect::DenseTensorType", false, false), paddle::dialect::OpOutputInfo("grad_bn2_input", "paddle::dialect::DenseTensorType", true, false), paddle::dialect::OpOutputInfo("grad_bn2_gamma", "paddle::dialect::DenseTensorType", true, false), paddle::dialect::OpOutputInfo("grad_bn2_beta", "paddle::dialect::DenseTensorType", true, false) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("FusedDconvDreluDbnInferMeta", {"grad_output", "weight", "grad_output_add", "residual_input", "bn1_eqscale", "bn1_eqbias", "conv_input", "bn1_mean", "bn1_inv_std", "bn1_gamma", "bn1_beta", "bn1_input", "bn2_mean", "bn2_inv_std", "bn2_gamma", "bn2_beta", "bn2_input", "paddings", "dilations", "strides", "padding_algorithm", "groups", "data_format", "fuse_shortcut", "fuse_dual", "fuse_add", "exhaustive_search"}, "fused_dconv_drelu_dbn", {"grad_output", "weight", "grad_output_add", "residual_input", "bn1_eqscale", "bn1_eqbias", "conv_input", "bn1_mean", "bn1_inv_std", "bn1_gamma", "bn1_beta", "bn1_input", "bn2_mean", "bn2_inv_std", "bn2_gamma", "bn2_beta", "bn2_input", "paddings", "dilations", "strides", "padding_algorithm", "groups", "data_format", "fuse_shortcut", "fuse_dual", "fuse_add", "exhaustive_search"}, {"grad_output"}, {}, {}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "fused_dconv_drelu_dbn");
}

void FusedDconvDreluDbnOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value grad_output_, pir::Value weight_, pir::Value grad_output_add_, pir::Value residual_input_, pir::Value bn1_eqscale_, pir::Value bn1_eqbias_, pir::Value conv_input_, pir::Value bn1_mean_, pir::Value bn1_inv_std_, pir::Value bn1_gamma_, pir::Value bn1_beta_, pir::Value bn1_input_, pir::Value bn2_mean_, pir::Value bn2_inv_std_, pir::Value bn2_gamma_, pir::Value bn2_beta_, pir::Value bn2_input_, const std::vector<int>& paddings, const std::vector<int>& dilations, const std::vector<int>& strides, const std::string& padding_algorithm, int groups, const std::string& data_format, bool fuse_shortcut, bool fuse_dual, bool fuse_add, bool exhaustive_search) {
  VLOG(4) << "Start build FusedDconvDreluDbnOp";



  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {grad_output_, weight_, grad_output_add_, residual_input_, bn1_eqscale_, bn1_eqbias_, conv_input_, bn1_mean_, bn1_inv_std_, bn1_gamma_, bn1_beta_, bn1_input_, bn2_mean_, bn2_inv_std_, bn2_gamma_, bn2_beta_, bn2_input_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  std::vector<pir::Attribute> vec_paddings;
  for (size_t i = 0; i < static_cast<size_t>(paddings.size()); i++) {
      pir::Attribute attr_paddings = pir::Int32Attribute::get(pir::IrContext::Instance(), paddings[i]);

    vec_paddings.push_back(attr_paddings);
  }
  pir::Attribute attr_paddings = pir::ArrayAttribute::get(pir::IrContext::Instance(), vec_paddings);
  argument.AddAttribute("paddings", attr_paddings);
  std::vector<pir::Attribute> vec_dilations;
  for (size_t i = 0; i < static_cast<size_t>(dilations.size()); i++) {
      pir::Attribute attr_dilations = pir::Int32Attribute::get(pir::IrContext::Instance(), dilations[i]);

    vec_dilations.push_back(attr_dilations);
  }
  pir::Attribute attr_dilations = pir::ArrayAttribute::get(pir::IrContext::Instance(), vec_dilations);
  argument.AddAttribute("dilations", attr_dilations);
  std::vector<pir::Attribute> vec_strides;
  for (size_t i = 0; i < static_cast<size_t>(strides.size()); i++) {
      pir::Attribute attr_strides = pir::Int32Attribute::get(pir::IrContext::Instance(), strides[i]);

    vec_strides.push_back(attr_strides);
  }
  pir::Attribute attr_strides = pir::ArrayAttribute::get(pir::IrContext::Instance(), vec_strides);
  argument.AddAttribute("strides", attr_strides);
  pir::Attribute attr_padding_algorithm = pir::StrAttribute::get(pir::IrContext::Instance(), padding_algorithm);
  argument.AddAttribute("padding_algorithm", attr_padding_algorithm);
  pir::Attribute attr_groups = pir::Int32Attribute::get(pir::IrContext::Instance(), groups);
  argument.AddAttribute("groups", attr_groups);
  pir::Attribute attr_data_format = pir::StrAttribute::get(pir::IrContext::Instance(), data_format);
  argument.AddAttribute("data_format", attr_data_format);
  pir::Attribute attr_fuse_shortcut = pir::BoolAttribute::get(pir::IrContext::Instance(), fuse_shortcut);
  argument.AddAttribute("fuse_shortcut", attr_fuse_shortcut);
  pir::Attribute attr_fuse_dual = pir::BoolAttribute::get(pir::IrContext::Instance(), fuse_dual);
  argument.AddAttribute("fuse_dual", attr_fuse_dual);
  pir::Attribute attr_fuse_add = pir::BoolAttribute::get(pir::IrContext::Instance(), fuse_add);
  argument.AddAttribute("fuse_add", attr_fuse_add);
  pir::Attribute attr_exhaustive_search = pir::BoolAttribute::get(pir::IrContext::Instance(), exhaustive_search);
  argument.AddAttribute("exhaustive_search", attr_exhaustive_search);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType grad_output = grad_output_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)grad_output;
  paddle::dialect::DenseTensorType weight = weight_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)weight;
  paddle::dialect::DenseTensorType bn1_mean = bn1_mean_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)bn1_mean;
  paddle::dialect::DenseTensorType bn1_inv_std = bn1_inv_std_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)bn1_inv_std;
  paddle::dialect::DenseTensorType bn1_gamma = bn1_gamma_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)bn1_gamma;
  paddle::dialect::DenseTensorType bn1_beta = bn1_beta_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)bn1_beta;
  paddle::dialect::DenseTensorType bn1_input = bn1_input_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)bn1_input;

  VLOG(4) << "Builder construction  dense_grad_output";
  paddle::dialect::IrTensor ir_tensor_grad_output(paddle::dialect::TransToPhiDataType(grad_output.dtype()),
                                                      grad_output.dims(),
                                                      grad_output.data_layout(),
                                                      grad_output.lod(),
                                                      grad_output.offset());
  VLOG(4) << "Builder construction  meta_grad_output";
  paddle::dialect::IrMetaTensor meta_grad_output(&ir_tensor_grad_output);

  VLOG(4) << "Builder construction  dense_weight";
  paddle::dialect::IrTensor ir_tensor_weight(paddle::dialect::TransToPhiDataType(weight.dtype()),
                                                      weight.dims(),
                                                      weight.data_layout(),
                                                      weight.lod(),
                                                      weight.offset());
  VLOG(4) << "Builder construction  meta_weight";
  paddle::dialect::IrMetaTensor meta_weight(&ir_tensor_weight);

  paddle::dialect::IrMetaTensor meta_grad_output_add;
  paddle::dialect::IrTensor ir_tensor_grad_output_add;
  if (grad_output_add_.impl() != nullptr) {
    paddle::dialect::DenseTensorType grad_output_add = grad_output_add_.type().dyn_cast<paddle::dialect::DenseTensorType>();
    VLOG(4) << "Builder construction  dense_grad_output_add";
    ir_tensor_grad_output_add = paddle::dialect::IrTensor(paddle::dialect::TransToPhiDataType(grad_output_add.dtype()),
                                                        grad_output_add.dims(),
                                                        grad_output_add.data_layout(),
                                                        grad_output_add.lod(),
                                                        grad_output_add.offset());
    VLOG(4) << "Builder construction  meta_grad_output_add";
    meta_grad_output_add = paddle::dialect::IrMetaTensor(&ir_tensor_grad_output_add);
  }


  paddle::dialect::IrMetaTensor meta_residual_input;
  paddle::dialect::IrTensor ir_tensor_residual_input;
  if (residual_input_.impl() != nullptr) {
    paddle::dialect::DenseTensorType residual_input = residual_input_.type().dyn_cast<paddle::dialect::DenseTensorType>();
    VLOG(4) << "Builder construction  dense_residual_input";
    ir_tensor_residual_input = paddle::dialect::IrTensor(paddle::dialect::TransToPhiDataType(residual_input.dtype()),
                                                        residual_input.dims(),
                                                        residual_input.data_layout(),
                                                        residual_input.lod(),
                                                        residual_input.offset());
    VLOG(4) << "Builder construction  meta_residual_input";
    meta_residual_input = paddle::dialect::IrMetaTensor(&ir_tensor_residual_input);
  }


  paddle::dialect::IrMetaTensor meta_bn1_eqscale;
  paddle::dialect::IrTensor ir_tensor_bn1_eqscale;
  if (bn1_eqscale_.impl() != nullptr) {
    paddle::dialect::DenseTensorType bn1_eqscale = bn1_eqscale_.type().dyn_cast<paddle::dialect::DenseTensorType>();
    VLOG(4) << "Builder construction  dense_bn1_eqscale";
    ir_tensor_bn1_eqscale = paddle::dialect::IrTensor(paddle::dialect::TransToPhiDataType(bn1_eqscale.dtype()),
                                                        bn1_eqscale.dims(),
                                                        bn1_eqscale.data_layout(),
                                                        bn1_eqscale.lod(),
                                                        bn1_eqscale.offset());
    VLOG(4) << "Builder construction  meta_bn1_eqscale";
    meta_bn1_eqscale = paddle::dialect::IrMetaTensor(&ir_tensor_bn1_eqscale);
  }


  paddle::dialect::IrMetaTensor meta_bn1_eqbias;
  paddle::dialect::IrTensor ir_tensor_bn1_eqbias;
  if (bn1_eqbias_.impl() != nullptr) {
    paddle::dialect::DenseTensorType bn1_eqbias = bn1_eqbias_.type().dyn_cast<paddle::dialect::DenseTensorType>();
    VLOG(4) << "Builder construction  dense_bn1_eqbias";
    ir_tensor_bn1_eqbias = paddle::dialect::IrTensor(paddle::dialect::TransToPhiDataType(bn1_eqbias.dtype()),
                                                        bn1_eqbias.dims(),
                                                        bn1_eqbias.data_layout(),
                                                        bn1_eqbias.lod(),
                                                        bn1_eqbias.offset());
    VLOG(4) << "Builder construction  meta_bn1_eqbias";
    meta_bn1_eqbias = paddle::dialect::IrMetaTensor(&ir_tensor_bn1_eqbias);
  }


  paddle::dialect::IrMetaTensor meta_conv_input;
  paddle::dialect::IrTensor ir_tensor_conv_input;
  if (conv_input_.impl() != nullptr) {
    paddle::dialect::DenseTensorType conv_input = conv_input_.type().dyn_cast<paddle::dialect::DenseTensorType>();
    VLOG(4) << "Builder construction  dense_conv_input";
    ir_tensor_conv_input = paddle::dialect::IrTensor(paddle::dialect::TransToPhiDataType(conv_input.dtype()),
                                                        conv_input.dims(),
                                                        conv_input.data_layout(),
                                                        conv_input.lod(),
                                                        conv_input.offset());
    VLOG(4) << "Builder construction  meta_conv_input";
    meta_conv_input = paddle::dialect::IrMetaTensor(&ir_tensor_conv_input);
  }


  VLOG(4) << "Builder construction  dense_bn1_mean";
  paddle::dialect::IrTensor ir_tensor_bn1_mean(paddle::dialect::TransToPhiDataType(bn1_mean.dtype()),
                                                      bn1_mean.dims(),
                                                      bn1_mean.data_layout(),
                                                      bn1_mean.lod(),
                                                      bn1_mean.offset());
  VLOG(4) << "Builder construction  meta_bn1_mean";
  paddle::dialect::IrMetaTensor meta_bn1_mean(&ir_tensor_bn1_mean);

  VLOG(4) << "Builder construction  dense_bn1_inv_std";
  paddle::dialect::IrTensor ir_tensor_bn1_inv_std(paddle::dialect::TransToPhiDataType(bn1_inv_std.dtype()),
                                                      bn1_inv_std.dims(),
                                                      bn1_inv_std.data_layout(),
                                                      bn1_inv_std.lod(),
                                                      bn1_inv_std.offset());
  VLOG(4) << "Builder construction  meta_bn1_inv_std";
  paddle::dialect::IrMetaTensor meta_bn1_inv_std(&ir_tensor_bn1_inv_std);

  VLOG(4) << "Builder construction  dense_bn1_gamma";
  paddle::dialect::IrTensor ir_tensor_bn1_gamma(paddle::dialect::TransToPhiDataType(bn1_gamma.dtype()),
                                                      bn1_gamma.dims(),
                                                      bn1_gamma.data_layout(),
                                                      bn1_gamma.lod(),
                                                      bn1_gamma.offset());
  VLOG(4) << "Builder construction  meta_bn1_gamma";
  paddle::dialect::IrMetaTensor meta_bn1_gamma(&ir_tensor_bn1_gamma);

  VLOG(4) << "Builder construction  dense_bn1_beta";
  paddle::dialect::IrTensor ir_tensor_bn1_beta(paddle::dialect::TransToPhiDataType(bn1_beta.dtype()),
                                                      bn1_beta.dims(),
                                                      bn1_beta.data_layout(),
                                                      bn1_beta.lod(),
                                                      bn1_beta.offset());
  VLOG(4) << "Builder construction  meta_bn1_beta";
  paddle::dialect::IrMetaTensor meta_bn1_beta(&ir_tensor_bn1_beta);

  VLOG(4) << "Builder construction  dense_bn1_input";
  paddle::dialect::IrTensor ir_tensor_bn1_input(paddle::dialect::TransToPhiDataType(bn1_input.dtype()),
                                                      bn1_input.dims(),
                                                      bn1_input.data_layout(),
                                                      bn1_input.lod(),
                                                      bn1_input.offset());
  VLOG(4) << "Builder construction  meta_bn1_input";
  paddle::dialect::IrMetaTensor meta_bn1_input(&ir_tensor_bn1_input);

  paddle::dialect::IrMetaTensor meta_bn2_mean;
  paddle::dialect::IrTensor ir_tensor_bn2_mean;
  if (bn2_mean_.impl() != nullptr) {
    paddle::dialect::DenseTensorType bn2_mean = bn2_mean_.type().dyn_cast<paddle::dialect::DenseTensorType>();
    VLOG(4) << "Builder construction  dense_bn2_mean";
    ir_tensor_bn2_mean = paddle::dialect::IrTensor(paddle::dialect::TransToPhiDataType(bn2_mean.dtype()),
                                                        bn2_mean.dims(),
                                                        bn2_mean.data_layout(),
                                                        bn2_mean.lod(),
                                                        bn2_mean.offset());
    VLOG(4) << "Builder construction  meta_bn2_mean";
    meta_bn2_mean = paddle::dialect::IrMetaTensor(&ir_tensor_bn2_mean);
  }


  paddle::dialect::IrMetaTensor meta_bn2_inv_std;
  paddle::dialect::IrTensor ir_tensor_bn2_inv_std;
  if (bn2_inv_std_.impl() != nullptr) {
    paddle::dialect::DenseTensorType bn2_inv_std = bn2_inv_std_.type().dyn_cast<paddle::dialect::DenseTensorType>();
    VLOG(4) << "Builder construction  dense_bn2_inv_std";
    ir_tensor_bn2_inv_std = paddle::dialect::IrTensor(paddle::dialect::TransToPhiDataType(bn2_inv_std.dtype()),
                                                        bn2_inv_std.dims(),
                                                        bn2_inv_std.data_layout(),
                                                        bn2_inv_std.lod(),
                                                        bn2_inv_std.offset());
    VLOG(4) << "Builder construction  meta_bn2_inv_std";
    meta_bn2_inv_std = paddle::dialect::IrMetaTensor(&ir_tensor_bn2_inv_std);
  }


  paddle::dialect::IrMetaTensor meta_bn2_gamma;
  paddle::dialect::IrTensor ir_tensor_bn2_gamma;
  if (bn2_gamma_.impl() != nullptr) {
    paddle::dialect::DenseTensorType bn2_gamma = bn2_gamma_.type().dyn_cast<paddle::dialect::DenseTensorType>();
    VLOG(4) << "Builder construction  dense_bn2_gamma";
    ir_tensor_bn2_gamma = paddle::dialect::IrTensor(paddle::dialect::TransToPhiDataType(bn2_gamma.dtype()),
                                                        bn2_gamma.dims(),
                                                        bn2_gamma.data_layout(),
                                                        bn2_gamma.lod(),
                                                        bn2_gamma.offset());
    VLOG(4) << "Builder construction  meta_bn2_gamma";
    meta_bn2_gamma = paddle::dialect::IrMetaTensor(&ir_tensor_bn2_gamma);
  }


  paddle::dialect::IrMetaTensor meta_bn2_beta;
  paddle::dialect::IrTensor ir_tensor_bn2_beta;
  if (bn2_beta_.impl() != nullptr) {
    paddle::dialect::DenseTensorType bn2_beta = bn2_beta_.type().dyn_cast<paddle::dialect::DenseTensorType>();
    VLOG(4) << "Builder construction  dense_bn2_beta";
    ir_tensor_bn2_beta = paddle::dialect::IrTensor(paddle::dialect::TransToPhiDataType(bn2_beta.dtype()),
                                                        bn2_beta.dims(),
                                                        bn2_beta.data_layout(),
                                                        bn2_beta.lod(),
                                                        bn2_beta.offset());
    VLOG(4) << "Builder construction  meta_bn2_beta";
    meta_bn2_beta = paddle::dialect::IrMetaTensor(&ir_tensor_bn2_beta);
  }


  paddle::dialect::IrMetaTensor meta_bn2_input;
  paddle::dialect::IrTensor ir_tensor_bn2_input;
  if (bn2_input_.impl() != nullptr) {
    paddle::dialect::DenseTensorType bn2_input = bn2_input_.type().dyn_cast<paddle::dialect::DenseTensorType>();
    VLOG(4) << "Builder construction  dense_bn2_input";
    ir_tensor_bn2_input = paddle::dialect::IrTensor(paddle::dialect::TransToPhiDataType(bn2_input.dtype()),
                                                        bn2_input.dims(),
                                                        bn2_input.data_layout(),
                                                        bn2_input.lod(),
                                                        bn2_input.offset());
    VLOG(4) << "Builder construction  meta_bn2_input";
    meta_bn2_input = paddle::dialect::IrMetaTensor(&ir_tensor_bn2_input);
  }

  paddle::dialect::IrTensor dense_grad_weight;
  paddle::dialect::IrMetaTensor meta_grad_weight(&dense_grad_weight);
  paddle::dialect::IrTensor dense_grad_bn1_input;
  paddle::dialect::IrMetaTensor meta_grad_bn1_input(&dense_grad_bn1_input);
  paddle::dialect::IrTensor dense_grad_bn1_gamma;
  paddle::dialect::IrMetaTensor meta_grad_bn1_gamma(&dense_grad_bn1_gamma);
  paddle::dialect::IrTensor dense_grad_bn1_beta;
  paddle::dialect::IrMetaTensor meta_grad_bn1_beta(&dense_grad_bn1_beta);
  paddle::dialect::IrTensor dense_grad_bn2_input;
  paddle::dialect::IrMetaTensor meta_grad_bn2_input(&dense_grad_bn2_input);
  paddle::dialect::IrTensor dense_grad_bn2_gamma;
  paddle::dialect::IrMetaTensor meta_grad_bn2_gamma(&dense_grad_bn2_gamma);
  paddle::dialect::IrTensor dense_grad_bn2_beta;
  paddle::dialect::IrMetaTensor meta_grad_bn2_beta(&dense_grad_bn2_beta);

  phi::FusedDconvDreluDbnInferMeta(meta_grad_output, meta_weight, meta_grad_output_add, meta_residual_input, meta_bn1_eqscale, meta_bn1_eqbias, meta_conv_input, meta_bn1_mean, meta_bn1_inv_std, meta_bn1_gamma, meta_bn1_beta, meta_bn1_input, meta_bn2_mean, meta_bn2_inv_std, meta_bn2_gamma, meta_bn2_beta, meta_bn2_input, paddings, dilations, strides, padding_algorithm, groups, data_format, fuse_shortcut, fuse_dual, fuse_add, exhaustive_search, &meta_grad_weight, &meta_grad_bn1_input, &meta_grad_bn1_gamma, &meta_grad_bn1_beta, &meta_grad_bn2_input, &meta_grad_bn2_gamma, &meta_grad_bn2_beta);

  std::vector<pir::Type> argument_outputs;
  pir::Type grad_weight_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_grad_weight.dtype()), dense_grad_weight.dims(), dense_grad_weight.layout(), dense_grad_weight.lod(), dense_grad_weight.offset());
  argument_outputs.push_back(grad_weight_dense_tensor_type);

  pir::Type grad_bn1_input_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_grad_bn1_input.dtype()), dense_grad_bn1_input.dims(), dense_grad_bn1_input.layout(), dense_grad_bn1_input.lod(), dense_grad_bn1_input.offset());
  argument_outputs.push_back(grad_bn1_input_dense_tensor_type);

  pir::Type grad_bn1_gamma_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_grad_bn1_gamma.dtype()), dense_grad_bn1_gamma.dims(), dense_grad_bn1_gamma.layout(), dense_grad_bn1_gamma.lod(), dense_grad_bn1_gamma.offset());
  argument_outputs.push_back(grad_bn1_gamma_dense_tensor_type);

  pir::Type grad_bn1_beta_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_grad_bn1_beta.dtype()), dense_grad_bn1_beta.dims(), dense_grad_bn1_beta.layout(), dense_grad_bn1_beta.lod(), dense_grad_bn1_beta.offset());
  argument_outputs.push_back(grad_bn1_beta_dense_tensor_type);

  pir::Type grad_bn2_input_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_grad_bn2_input.dtype()), dense_grad_bn2_input.dims(), dense_grad_bn2_input.layout(), dense_grad_bn2_input.lod(), dense_grad_bn2_input.offset());
  argument_outputs.push_back(grad_bn2_input_dense_tensor_type);

  pir::Type grad_bn2_gamma_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_grad_bn2_gamma.dtype()), dense_grad_bn2_gamma.dims(), dense_grad_bn2_gamma.layout(), dense_grad_bn2_gamma.lod(), dense_grad_bn2_gamma.offset());
  argument_outputs.push_back(grad_bn2_gamma_dense_tensor_type);

  pir::Type grad_bn2_beta_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_grad_bn2_beta.dtype()), dense_grad_bn2_beta.dims(), dense_grad_bn2_beta.layout(), dense_grad_bn2_beta.lod(), dense_grad_bn2_beta.offset());
  argument_outputs.push_back(grad_bn2_beta_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void FusedDconvDreluDbnOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value grad_output_, pir::Value weight_, pir::Value grad_output_add_, pir::Value residual_input_, pir::Value bn1_eqscale_, pir::Value bn1_eqbias_, pir::Value conv_input_, pir::Value bn1_mean_, pir::Value bn1_inv_std_, pir::Value bn1_gamma_, pir::Value bn1_beta_, pir::Value bn1_input_, pir::Value bn2_mean_, pir::Value bn2_inv_std_, pir::Value bn2_gamma_, pir::Value bn2_beta_, pir::Value bn2_input_, pir::AttributeMap attributes) {
  VLOG(4) << "Start build FusedDconvDreluDbnOp";


  IR_ENFORCE(
      attributes.find("paddings") != attributes.end(),
          "'paddings' Attribute is expected for FusedDconvDreluDbnOp. ");
  std::vector<int> paddings;
  for (size_t i = 0; i < attributes.at("paddings").dyn_cast<pir::ArrayAttribute>().size(); i++) {
    paddings.push_back(attributes.at("paddings").dyn_cast<pir::ArrayAttribute>().at(i).dyn_cast<pir::Int32Attribute>().data());
  }

  IR_ENFORCE(
      attributes.find("dilations") != attributes.end(),
          "'dilations' Attribute is expected for FusedDconvDreluDbnOp. ");
  std::vector<int> dilations;
  for (size_t i = 0; i < attributes.at("dilations").dyn_cast<pir::ArrayAttribute>().size(); i++) {
    dilations.push_back(attributes.at("dilations").dyn_cast<pir::ArrayAttribute>().at(i).dyn_cast<pir::Int32Attribute>().data());
  }

  IR_ENFORCE(
      attributes.find("strides") != attributes.end(),
          "'strides' Attribute is expected for FusedDconvDreluDbnOp. ");
  std::vector<int> strides;
  for (size_t i = 0; i < attributes.at("strides").dyn_cast<pir::ArrayAttribute>().size(); i++) {
    strides.push_back(attributes.at("strides").dyn_cast<pir::ArrayAttribute>().at(i).dyn_cast<pir::Int32Attribute>().data());
  }

  IR_ENFORCE(
      attributes.find("padding_algorithm") != attributes.end(),
          "'padding_algorithm' Attribute is expected for FusedDconvDreluDbnOp. ");
  std::string padding_algorithm = attributes.at("padding_algorithm").dyn_cast<pir::StrAttribute>().AsString();

  IR_ENFORCE(
      attributes.find("groups") != attributes.end(),
          "'groups' Attribute is expected for FusedDconvDreluDbnOp. ");
  int groups = attributes.at("groups").dyn_cast<pir::Int32Attribute>().data();

  IR_ENFORCE(
      attributes.find("data_format") != attributes.end(),
          "'data_format' Attribute is expected for FusedDconvDreluDbnOp. ");
  std::string data_format = attributes.at("data_format").dyn_cast<pir::StrAttribute>().AsString();

  IR_ENFORCE(
      attributes.find("fuse_shortcut") != attributes.end(),
          "'fuse_shortcut' Attribute is expected for FusedDconvDreluDbnOp. ");
  bool fuse_shortcut = attributes.at("fuse_shortcut").dyn_cast<pir::BoolAttribute>().data();

  IR_ENFORCE(
      attributes.find("fuse_dual") != attributes.end(),
          "'fuse_dual' Attribute is expected for FusedDconvDreluDbnOp. ");
  bool fuse_dual = attributes.at("fuse_dual").dyn_cast<pir::BoolAttribute>().data();

  IR_ENFORCE(
      attributes.find("fuse_add") != attributes.end(),
          "'fuse_add' Attribute is expected for FusedDconvDreluDbnOp. ");
  bool fuse_add = attributes.at("fuse_add").dyn_cast<pir::BoolAttribute>().data();

  IR_ENFORCE(
      attributes.find("exhaustive_search") != attributes.end(),
          "'exhaustive_search' Attribute is expected for FusedDconvDreluDbnOp. ");
  bool exhaustive_search = attributes.at("exhaustive_search").dyn_cast<pir::BoolAttribute>().data();


  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {grad_output_, weight_, grad_output_add_, residual_input_, bn1_eqscale_, bn1_eqbias_, conv_input_, bn1_mean_, bn1_inv_std_, bn1_gamma_, bn1_beta_, bn1_input_, bn2_mean_, bn2_inv_std_, bn2_gamma_, bn2_beta_, bn2_input_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  std::vector<pir::Attribute> vec_paddings;
  for (size_t i = 0; i < static_cast<size_t>(paddings.size()); i++) {
      pir::Attribute attr_paddings = pir::Int32Attribute::get(pir::IrContext::Instance(), paddings[i]);

    vec_paddings.push_back(attr_paddings);
  }
  pir::Attribute attr_paddings = pir::ArrayAttribute::get(pir::IrContext::Instance(), vec_paddings);
  argument.AddAttribute("paddings", attr_paddings);
  std::vector<pir::Attribute> vec_dilations;
  for (size_t i = 0; i < static_cast<size_t>(dilations.size()); i++) {
      pir::Attribute attr_dilations = pir::Int32Attribute::get(pir::IrContext::Instance(), dilations[i]);

    vec_dilations.push_back(attr_dilations);
  }
  pir::Attribute attr_dilations = pir::ArrayAttribute::get(pir::IrContext::Instance(), vec_dilations);
  argument.AddAttribute("dilations", attr_dilations);
  std::vector<pir::Attribute> vec_strides;
  for (size_t i = 0; i < static_cast<size_t>(strides.size()); i++) {
      pir::Attribute attr_strides = pir::Int32Attribute::get(pir::IrContext::Instance(), strides[i]);

    vec_strides.push_back(attr_strides);
  }
  pir::Attribute attr_strides = pir::ArrayAttribute::get(pir::IrContext::Instance(), vec_strides);
  argument.AddAttribute("strides", attr_strides);
  pir::Attribute attr_padding_algorithm = pir::StrAttribute::get(pir::IrContext::Instance(), padding_algorithm);
  argument.AddAttribute("padding_algorithm", attr_padding_algorithm);
  pir::Attribute attr_groups = pir::Int32Attribute::get(pir::IrContext::Instance(), groups);
  argument.AddAttribute("groups", attr_groups);
  pir::Attribute attr_data_format = pir::StrAttribute::get(pir::IrContext::Instance(), data_format);
  argument.AddAttribute("data_format", attr_data_format);
  pir::Attribute attr_fuse_shortcut = pir::BoolAttribute::get(pir::IrContext::Instance(), fuse_shortcut);
  argument.AddAttribute("fuse_shortcut", attr_fuse_shortcut);
  pir::Attribute attr_fuse_dual = pir::BoolAttribute::get(pir::IrContext::Instance(), fuse_dual);
  argument.AddAttribute("fuse_dual", attr_fuse_dual);
  pir::Attribute attr_fuse_add = pir::BoolAttribute::get(pir::IrContext::Instance(), fuse_add);
  argument.AddAttribute("fuse_add", attr_fuse_add);
  pir::Attribute attr_exhaustive_search = pir::BoolAttribute::get(pir::IrContext::Instance(), exhaustive_search);
  argument.AddAttribute("exhaustive_search", attr_exhaustive_search);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType grad_output = grad_output_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)grad_output;
  paddle::dialect::DenseTensorType weight = weight_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)weight;
  paddle::dialect::DenseTensorType bn1_mean = bn1_mean_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)bn1_mean;
  paddle::dialect::DenseTensorType bn1_inv_std = bn1_inv_std_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)bn1_inv_std;
  paddle::dialect::DenseTensorType bn1_gamma = bn1_gamma_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)bn1_gamma;
  paddle::dialect::DenseTensorType bn1_beta = bn1_beta_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)bn1_beta;
  paddle::dialect::DenseTensorType bn1_input = bn1_input_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)bn1_input;

  VLOG(4) << "Builder construction  dense_grad_output";
  paddle::dialect::IrTensor ir_tensor_grad_output(paddle::dialect::TransToPhiDataType(grad_output.dtype()),
                                                      grad_output.dims(),
                                                      grad_output.data_layout(),
                                                      grad_output.lod(),
                                                      grad_output.offset());
  VLOG(4) << "Builder construction  meta_grad_output";
  paddle::dialect::IrMetaTensor meta_grad_output(&ir_tensor_grad_output);

  VLOG(4) << "Builder construction  dense_weight";
  paddle::dialect::IrTensor ir_tensor_weight(paddle::dialect::TransToPhiDataType(weight.dtype()),
                                                      weight.dims(),
                                                      weight.data_layout(),
                                                      weight.lod(),
                                                      weight.offset());
  VLOG(4) << "Builder construction  meta_weight";
  paddle::dialect::IrMetaTensor meta_weight(&ir_tensor_weight);

  paddle::dialect::IrMetaTensor meta_grad_output_add;
  paddle::dialect::IrTensor ir_tensor_grad_output_add;
  if (grad_output_add_.impl() != nullptr) {
    paddle::dialect::DenseTensorType grad_output_add = grad_output_add_.type().dyn_cast<paddle::dialect::DenseTensorType>();
    VLOG(4) << "Builder construction  dense_grad_output_add";
    ir_tensor_grad_output_add = paddle::dialect::IrTensor(paddle::dialect::TransToPhiDataType(grad_output_add.dtype()),
                                                        grad_output_add.dims(),
                                                        grad_output_add.data_layout(),
                                                        grad_output_add.lod(),
                                                        grad_output_add.offset());
    VLOG(4) << "Builder construction  meta_grad_output_add";
    meta_grad_output_add = paddle::dialect::IrMetaTensor(&ir_tensor_grad_output_add);
  }


  paddle::dialect::IrMetaTensor meta_residual_input;
  paddle::dialect::IrTensor ir_tensor_residual_input;
  if (residual_input_.impl() != nullptr) {
    paddle::dialect::DenseTensorType residual_input = residual_input_.type().dyn_cast<paddle::dialect::DenseTensorType>();
    VLOG(4) << "Builder construction  dense_residual_input";
    ir_tensor_residual_input = paddle::dialect::IrTensor(paddle::dialect::TransToPhiDataType(residual_input.dtype()),
                                                        residual_input.dims(),
                                                        residual_input.data_layout(),
                                                        residual_input.lod(),
                                                        residual_input.offset());
    VLOG(4) << "Builder construction  meta_residual_input";
    meta_residual_input = paddle::dialect::IrMetaTensor(&ir_tensor_residual_input);
  }


  paddle::dialect::IrMetaTensor meta_bn1_eqscale;
  paddle::dialect::IrTensor ir_tensor_bn1_eqscale;
  if (bn1_eqscale_.impl() != nullptr) {
    paddle::dialect::DenseTensorType bn1_eqscale = bn1_eqscale_.type().dyn_cast<paddle::dialect::DenseTensorType>();
    VLOG(4) << "Builder construction  dense_bn1_eqscale";
    ir_tensor_bn1_eqscale = paddle::dialect::IrTensor(paddle::dialect::TransToPhiDataType(bn1_eqscale.dtype()),
                                                        bn1_eqscale.dims(),
                                                        bn1_eqscale.data_layout(),
                                                        bn1_eqscale.lod(),
                                                        bn1_eqscale.offset());
    VLOG(4) << "Builder construction  meta_bn1_eqscale";
    meta_bn1_eqscale = paddle::dialect::IrMetaTensor(&ir_tensor_bn1_eqscale);
  }


  paddle::dialect::IrMetaTensor meta_bn1_eqbias;
  paddle::dialect::IrTensor ir_tensor_bn1_eqbias;
  if (bn1_eqbias_.impl() != nullptr) {
    paddle::dialect::DenseTensorType bn1_eqbias = bn1_eqbias_.type().dyn_cast<paddle::dialect::DenseTensorType>();
    VLOG(4) << "Builder construction  dense_bn1_eqbias";
    ir_tensor_bn1_eqbias = paddle::dialect::IrTensor(paddle::dialect::TransToPhiDataType(bn1_eqbias.dtype()),
                                                        bn1_eqbias.dims(),
                                                        bn1_eqbias.data_layout(),
                                                        bn1_eqbias.lod(),
                                                        bn1_eqbias.offset());
    VLOG(4) << "Builder construction  meta_bn1_eqbias";
    meta_bn1_eqbias = paddle::dialect::IrMetaTensor(&ir_tensor_bn1_eqbias);
  }


  paddle::dialect::IrMetaTensor meta_conv_input;
  paddle::dialect::IrTensor ir_tensor_conv_input;
  if (conv_input_.impl() != nullptr) {
    paddle::dialect::DenseTensorType conv_input = conv_input_.type().dyn_cast<paddle::dialect::DenseTensorType>();
    VLOG(4) << "Builder construction  dense_conv_input";
    ir_tensor_conv_input = paddle::dialect::IrTensor(paddle::dialect::TransToPhiDataType(conv_input.dtype()),
                                                        conv_input.dims(),
                                                        conv_input.data_layout(),
                                                        conv_input.lod(),
                                                        conv_input.offset());
    VLOG(4) << "Builder construction  meta_conv_input";
    meta_conv_input = paddle::dialect::IrMetaTensor(&ir_tensor_conv_input);
  }


  VLOG(4) << "Builder construction  dense_bn1_mean";
  paddle::dialect::IrTensor ir_tensor_bn1_mean(paddle::dialect::TransToPhiDataType(bn1_mean.dtype()),
                                                      bn1_mean.dims(),
                                                      bn1_mean.data_layout(),
                                                      bn1_mean.lod(),
                                                      bn1_mean.offset());
  VLOG(4) << "Builder construction  meta_bn1_mean";
  paddle::dialect::IrMetaTensor meta_bn1_mean(&ir_tensor_bn1_mean);

  VLOG(4) << "Builder construction  dense_bn1_inv_std";
  paddle::dialect::IrTensor ir_tensor_bn1_inv_std(paddle::dialect::TransToPhiDataType(bn1_inv_std.dtype()),
                                                      bn1_inv_std.dims(),
                                                      bn1_inv_std.data_layout(),
                                                      bn1_inv_std.lod(),
                                                      bn1_inv_std.offset());
  VLOG(4) << "Builder construction  meta_bn1_inv_std";
  paddle::dialect::IrMetaTensor meta_bn1_inv_std(&ir_tensor_bn1_inv_std);

  VLOG(4) << "Builder construction  dense_bn1_gamma";
  paddle::dialect::IrTensor ir_tensor_bn1_gamma(paddle::dialect::TransToPhiDataType(bn1_gamma.dtype()),
                                                      bn1_gamma.dims(),
                                                      bn1_gamma.data_layout(),
                                                      bn1_gamma.lod(),
                                                      bn1_gamma.offset());
  VLOG(4) << "Builder construction  meta_bn1_gamma";
  paddle::dialect::IrMetaTensor meta_bn1_gamma(&ir_tensor_bn1_gamma);

  VLOG(4) << "Builder construction  dense_bn1_beta";
  paddle::dialect::IrTensor ir_tensor_bn1_beta(paddle::dialect::TransToPhiDataType(bn1_beta.dtype()),
                                                      bn1_beta.dims(),
                                                      bn1_beta.data_layout(),
                                                      bn1_beta.lod(),
                                                      bn1_beta.offset());
  VLOG(4) << "Builder construction  meta_bn1_beta";
  paddle::dialect::IrMetaTensor meta_bn1_beta(&ir_tensor_bn1_beta);

  VLOG(4) << "Builder construction  dense_bn1_input";
  paddle::dialect::IrTensor ir_tensor_bn1_input(paddle::dialect::TransToPhiDataType(bn1_input.dtype()),
                                                      bn1_input.dims(),
                                                      bn1_input.data_layout(),
                                                      bn1_input.lod(),
                                                      bn1_input.offset());
  VLOG(4) << "Builder construction  meta_bn1_input";
  paddle::dialect::IrMetaTensor meta_bn1_input(&ir_tensor_bn1_input);

  paddle::dialect::IrMetaTensor meta_bn2_mean;
  paddle::dialect::IrTensor ir_tensor_bn2_mean;
  if (bn2_mean_.impl() != nullptr) {
    paddle::dialect::DenseTensorType bn2_mean = bn2_mean_.type().dyn_cast<paddle::dialect::DenseTensorType>();
    VLOG(4) << "Builder construction  dense_bn2_mean";
    ir_tensor_bn2_mean = paddle::dialect::IrTensor(paddle::dialect::TransToPhiDataType(bn2_mean.dtype()),
                                                        bn2_mean.dims(),
                                                        bn2_mean.data_layout(),
                                                        bn2_mean.lod(),
                                                        bn2_mean.offset());
    VLOG(4) << "Builder construction  meta_bn2_mean";
    meta_bn2_mean = paddle::dialect::IrMetaTensor(&ir_tensor_bn2_mean);
  }


  paddle::dialect::IrMetaTensor meta_bn2_inv_std;
  paddle::dialect::IrTensor ir_tensor_bn2_inv_std;
  if (bn2_inv_std_.impl() != nullptr) {
    paddle::dialect::DenseTensorType bn2_inv_std = bn2_inv_std_.type().dyn_cast<paddle::dialect::DenseTensorType>();
    VLOG(4) << "Builder construction  dense_bn2_inv_std";
    ir_tensor_bn2_inv_std = paddle::dialect::IrTensor(paddle::dialect::TransToPhiDataType(bn2_inv_std.dtype()),
                                                        bn2_inv_std.dims(),
                                                        bn2_inv_std.data_layout(),
                                                        bn2_inv_std.lod(),
                                                        bn2_inv_std.offset());
    VLOG(4) << "Builder construction  meta_bn2_inv_std";
    meta_bn2_inv_std = paddle::dialect::IrMetaTensor(&ir_tensor_bn2_inv_std);
  }


  paddle::dialect::IrMetaTensor meta_bn2_gamma;
  paddle::dialect::IrTensor ir_tensor_bn2_gamma;
  if (bn2_gamma_.impl() != nullptr) {
    paddle::dialect::DenseTensorType bn2_gamma = bn2_gamma_.type().dyn_cast<paddle::dialect::DenseTensorType>();
    VLOG(4) << "Builder construction  dense_bn2_gamma";
    ir_tensor_bn2_gamma = paddle::dialect::IrTensor(paddle::dialect::TransToPhiDataType(bn2_gamma.dtype()),
                                                        bn2_gamma.dims(),
                                                        bn2_gamma.data_layout(),
                                                        bn2_gamma.lod(),
                                                        bn2_gamma.offset());
    VLOG(4) << "Builder construction  meta_bn2_gamma";
    meta_bn2_gamma = paddle::dialect::IrMetaTensor(&ir_tensor_bn2_gamma);
  }


  paddle::dialect::IrMetaTensor meta_bn2_beta;
  paddle::dialect::IrTensor ir_tensor_bn2_beta;
  if (bn2_beta_.impl() != nullptr) {
    paddle::dialect::DenseTensorType bn2_beta = bn2_beta_.type().dyn_cast<paddle::dialect::DenseTensorType>();
    VLOG(4) << "Builder construction  dense_bn2_beta";
    ir_tensor_bn2_beta = paddle::dialect::IrTensor(paddle::dialect::TransToPhiDataType(bn2_beta.dtype()),
                                                        bn2_beta.dims(),
                                                        bn2_beta.data_layout(),
                                                        bn2_beta.lod(),
                                                        bn2_beta.offset());
    VLOG(4) << "Builder construction  meta_bn2_beta";
    meta_bn2_beta = paddle::dialect::IrMetaTensor(&ir_tensor_bn2_beta);
  }


  paddle::dialect::IrMetaTensor meta_bn2_input;
  paddle::dialect::IrTensor ir_tensor_bn2_input;
  if (bn2_input_.impl() != nullptr) {
    paddle::dialect::DenseTensorType bn2_input = bn2_input_.type().dyn_cast<paddle::dialect::DenseTensorType>();
    VLOG(4) << "Builder construction  dense_bn2_input";
    ir_tensor_bn2_input = paddle::dialect::IrTensor(paddle::dialect::TransToPhiDataType(bn2_input.dtype()),
                                                        bn2_input.dims(),
                                                        bn2_input.data_layout(),
                                                        bn2_input.lod(),
                                                        bn2_input.offset());
    VLOG(4) << "Builder construction  meta_bn2_input";
    meta_bn2_input = paddle::dialect::IrMetaTensor(&ir_tensor_bn2_input);
  }

  paddle::dialect::IrTensor dense_grad_weight;
  paddle::dialect::IrMetaTensor meta_grad_weight(&dense_grad_weight);
  paddle::dialect::IrTensor dense_grad_bn1_input;
  paddle::dialect::IrMetaTensor meta_grad_bn1_input(&dense_grad_bn1_input);
  paddle::dialect::IrTensor dense_grad_bn1_gamma;
  paddle::dialect::IrMetaTensor meta_grad_bn1_gamma(&dense_grad_bn1_gamma);
  paddle::dialect::IrTensor dense_grad_bn1_beta;
  paddle::dialect::IrMetaTensor meta_grad_bn1_beta(&dense_grad_bn1_beta);
  paddle::dialect::IrTensor dense_grad_bn2_input;
  paddle::dialect::IrMetaTensor meta_grad_bn2_input(&dense_grad_bn2_input);
  paddle::dialect::IrTensor dense_grad_bn2_gamma;
  paddle::dialect::IrMetaTensor meta_grad_bn2_gamma(&dense_grad_bn2_gamma);
  paddle::dialect::IrTensor dense_grad_bn2_beta;
  paddle::dialect::IrMetaTensor meta_grad_bn2_beta(&dense_grad_bn2_beta);

  phi::FusedDconvDreluDbnInferMeta(meta_grad_output, meta_weight, meta_grad_output_add, meta_residual_input, meta_bn1_eqscale, meta_bn1_eqbias, meta_conv_input, meta_bn1_mean, meta_bn1_inv_std, meta_bn1_gamma, meta_bn1_beta, meta_bn1_input, meta_bn2_mean, meta_bn2_inv_std, meta_bn2_gamma, meta_bn2_beta, meta_bn2_input, paddings, dilations, strides, padding_algorithm, groups, data_format, fuse_shortcut, fuse_dual, fuse_add, exhaustive_search, &meta_grad_weight, &meta_grad_bn1_input, &meta_grad_bn1_gamma, &meta_grad_bn1_beta, &meta_grad_bn2_input, &meta_grad_bn2_gamma, &meta_grad_bn2_beta);

  std::vector<pir::Type> argument_outputs;
  pir::Type grad_weight_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_grad_weight.dtype()), dense_grad_weight.dims(), dense_grad_weight.layout(), dense_grad_weight.lod(), dense_grad_weight.offset());
  argument_outputs.push_back(grad_weight_dense_tensor_type);

  pir::Type grad_bn1_input_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_grad_bn1_input.dtype()), dense_grad_bn1_input.dims(), dense_grad_bn1_input.layout(), dense_grad_bn1_input.lod(), dense_grad_bn1_input.offset());
  argument_outputs.push_back(grad_bn1_input_dense_tensor_type);

  pir::Type grad_bn1_gamma_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_grad_bn1_gamma.dtype()), dense_grad_bn1_gamma.dims(), dense_grad_bn1_gamma.layout(), dense_grad_bn1_gamma.lod(), dense_grad_bn1_gamma.offset());
  argument_outputs.push_back(grad_bn1_gamma_dense_tensor_type);

  pir::Type grad_bn1_beta_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_grad_bn1_beta.dtype()), dense_grad_bn1_beta.dims(), dense_grad_bn1_beta.layout(), dense_grad_bn1_beta.lod(), dense_grad_bn1_beta.offset());
  argument_outputs.push_back(grad_bn1_beta_dense_tensor_type);

  pir::Type grad_bn2_input_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_grad_bn2_input.dtype()), dense_grad_bn2_input.dims(), dense_grad_bn2_input.layout(), dense_grad_bn2_input.lod(), dense_grad_bn2_input.offset());
  argument_outputs.push_back(grad_bn2_input_dense_tensor_type);

  pir::Type grad_bn2_gamma_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_grad_bn2_gamma.dtype()), dense_grad_bn2_gamma.dims(), dense_grad_bn2_gamma.layout(), dense_grad_bn2_gamma.lod(), dense_grad_bn2_gamma.offset());
  argument_outputs.push_back(grad_bn2_gamma_dense_tensor_type);

  pir::Type grad_bn2_beta_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_grad_bn2_beta.dtype()), dense_grad_bn2_beta.dims(), dense_grad_bn2_beta.layout(), dense_grad_bn2_beta.lod(), dense_grad_bn2_beta.offset());
  argument_outputs.push_back(grad_bn2_beta_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void FusedDconvDreluDbnOp::VerifySig() {
  VLOG(4) << "Start Verifying inputs, outputs and attributes for: FusedDconvDreluDbnOp.";
  VLOG(4) << "Verifying inputs:";
  {
  auto input_size = num_operands();
  IR_ENFORCE(input_size == 17u,
                    "The size %d of inputs must be equal to 17.", input_size);
  IR_ENFORCE((*this)->operand_source(0).type().isa<paddle::dialect::DenseTensorType>(),
                  "Type validation failed for the 0th input, got %s.", (*this)->operand_source(0).type());
  IR_ENFORCE((*this)->operand_source(1).type().isa<paddle::dialect::DenseTensorType>(),
                  "Type validation failed for the 1th input, got %s.", (*this)->operand_source(1).type());
  if (auto val = (*this)->operand(2)) {
    IR_ENFORCE(val.type().isa<paddle::dialect::DenseTensorType>(),
                   "Type validation failed for the 2th input, got %s.", (*this)->operand_source(2).type());
  }
  if (auto val = (*this)->operand(3)) {
    IR_ENFORCE(val.type().isa<paddle::dialect::DenseTensorType>(),
                   "Type validation failed for the 3th input, got %s.", (*this)->operand_source(3).type());
  }
  if (auto val = (*this)->operand(4)) {
    IR_ENFORCE(val.type().isa<paddle::dialect::DenseTensorType>(),
                   "Type validation failed for the 4th input, got %s.", (*this)->operand_source(4).type());
  }
  if (auto val = (*this)->operand(5)) {
    IR_ENFORCE(val.type().isa<paddle::dialect::DenseTensorType>(),
                   "Type validation failed for the 5th input, got %s.", (*this)->operand_source(5).type());
  }
  if (auto val = (*this)->operand(6)) {
    IR_ENFORCE(val.type().isa<paddle::dialect::DenseTensorType>(),
                   "Type validation failed for the 6th input, got %s.", (*this)->operand_source(6).type());
  }
  IR_ENFORCE((*this)->operand_source(7).type().isa<paddle::dialect::DenseTensorType>(),
                  "Type validation failed for the 7th input, got %s.", (*this)->operand_source(7).type());
  IR_ENFORCE((*this)->operand_source(8).type().isa<paddle::dialect::DenseTensorType>(),
                  "Type validation failed for the 8th input, got %s.", (*this)->operand_source(8).type());
  IR_ENFORCE((*this)->operand_source(9).type().isa<paddle::dialect::DenseTensorType>(),
                  "Type validation failed for the 9th input, got %s.", (*this)->operand_source(9).type());
  IR_ENFORCE((*this)->operand_source(10).type().isa<paddle::dialect::DenseTensorType>(),
                  "Type validation failed for the 10th input, got %s.", (*this)->operand_source(10).type());
  IR_ENFORCE((*this)->operand_source(11).type().isa<paddle::dialect::DenseTensorType>(),
                  "Type validation failed for the 11th input, got %s.", (*this)->operand_source(11).type());
  if (auto val = (*this)->operand(12)) {
    IR_ENFORCE(val.type().isa<paddle::dialect::DenseTensorType>(),
                   "Type validation failed for the 12th input, got %s.", (*this)->operand_source(12).type());
  }
  if (auto val = (*this)->operand(13)) {
    IR_ENFORCE(val.type().isa<paddle::dialect::DenseTensorType>(),
                   "Type validation failed for the 13th input, got %s.", (*this)->operand_source(13).type());
  }
  if (auto val = (*this)->operand(14)) {
    IR_ENFORCE(val.type().isa<paddle::dialect::DenseTensorType>(),
                   "Type validation failed for the 14th input, got %s.", (*this)->operand_source(14).type());
  }
  if (auto val = (*this)->operand(15)) {
    IR_ENFORCE(val.type().isa<paddle::dialect::DenseTensorType>(),
                   "Type validation failed for the 15th input, got %s.", (*this)->operand_source(15).type());
  }
  if (auto val = (*this)->operand(16)) {
    IR_ENFORCE(val.type().isa<paddle::dialect::DenseTensorType>(),
                   "Type validation failed for the 16th input, got %s.", (*this)->operand_source(16).type());
  }
  }
  VLOG(4) << "Verifying attributes:";
  {
  auto& attributes = this->attributes();
  IR_ENFORCE(attributes.count("paddings")>0,
                 "paddings does not exist.");
  IR_ENFORCE(attributes.at("paddings").isa<pir::ArrayAttribute>(),
                 "Type of attribute: paddings is not pir::ArrayAttribute.");
  for (size_t i = 0; i < attributes.at("paddings").dyn_cast<pir::ArrayAttribute>().size(); i++) {
    IR_ENFORCE(attributes.at("paddings").dyn_cast<pir::ArrayAttribute>().at(i).isa<pir::Int32Attribute>(),
                   "Type of attribute: paddings is not right.");
  }
  IR_ENFORCE(attributes.count("dilations")>0,
                 "dilations does not exist.");
  IR_ENFORCE(attributes.at("dilations").isa<pir::ArrayAttribute>(),
                 "Type of attribute: dilations is not pir::ArrayAttribute.");
  for (size_t i = 0; i < attributes.at("dilations").dyn_cast<pir::ArrayAttribute>().size(); i++) {
    IR_ENFORCE(attributes.at("dilations").dyn_cast<pir::ArrayAttribute>().at(i).isa<pir::Int32Attribute>(),
                   "Type of attribute: dilations is not right.");
  }
  IR_ENFORCE(attributes.count("strides")>0,
                 "strides does not exist.");
  IR_ENFORCE(attributes.at("strides").isa<pir::ArrayAttribute>(),
                 "Type of attribute: strides is not pir::ArrayAttribute.");
  for (size_t i = 0; i < attributes.at("strides").dyn_cast<pir::ArrayAttribute>().size(); i++) {
    IR_ENFORCE(attributes.at("strides").dyn_cast<pir::ArrayAttribute>().at(i).isa<pir::Int32Attribute>(),
                   "Type of attribute: strides is not right.");
  }
  IR_ENFORCE(attributes.count("padding_algorithm")>0,
                 "padding_algorithm does not exist.");
  IR_ENFORCE(attributes.at("padding_algorithm").isa<pir::StrAttribute>(),
                 "Type of attribute: padding_algorithm is not pir::StrAttribute.");

  IR_ENFORCE(attributes.count("groups")>0,
                 "groups does not exist.");
  IR_ENFORCE(attributes.at("groups").isa<pir::Int32Attribute>(),
                 "Type of attribute: groups is not pir::Int32Attribute.");

  IR_ENFORCE(attributes.count("data_format")>0,
                 "data_format does not exist.");
  IR_ENFORCE(attributes.at("data_format").isa<pir::StrAttribute>(),
                 "Type of attribute: data_format is not pir::StrAttribute.");

  IR_ENFORCE(attributes.count("fuse_shortcut")>0,
                 "fuse_shortcut does not exist.");
  IR_ENFORCE(attributes.at("fuse_shortcut").isa<pir::BoolAttribute>(),
                 "Type of attribute: fuse_shortcut is not pir::BoolAttribute.");

  IR_ENFORCE(attributes.count("fuse_dual")>0,
                 "fuse_dual does not exist.");
  IR_ENFORCE(attributes.at("fuse_dual").isa<pir::BoolAttribute>(),
                 "Type of attribute: fuse_dual is not pir::BoolAttribute.");

  IR_ENFORCE(attributes.count("fuse_add")>0,
                 "fuse_add does not exist.");
  IR_ENFORCE(attributes.at("fuse_add").isa<pir::BoolAttribute>(),
                 "Type of attribute: fuse_add is not pir::BoolAttribute.");

  IR_ENFORCE(attributes.count("exhaustive_search")>0,
                 "exhaustive_search does not exist.");
  IR_ENFORCE(attributes.at("exhaustive_search").isa<pir::BoolAttribute>(),
                 "Type of attribute: exhaustive_search is not pir::BoolAttribute.");

  }
  VLOG(4) << "Verifying outputs:";
  {
  auto output_size = num_results();
  IR_ENFORCE(output_size == 7u,
                    "The size %d of outputs must be equal to 7.", output_size);
  IR_ENFORCE((*this)->result(0).type().isa<paddle::dialect::DenseTensorType>(),
                 "Type validation failed for the 0th output.");
  IR_ENFORCE((*this)->result(1).type().isa<paddle::dialect::DenseTensorType>(),
                 "Type validation failed for the 1th output.");
  IR_ENFORCE((*this)->result(2).type().isa<paddle::dialect::DenseTensorType>(),
                 "Type validation failed for the 2th output.");
  IR_ENFORCE((*this)->result(3).type().isa<paddle::dialect::DenseTensorType>(),
                 "Type validation failed for the 3th output.");
  if (auto output_4_type = (*this)->result(4).type()) {
    IR_ENFORCE(output_4_type.isa<paddle::dialect::DenseTensorType>(),
                   "Type validation failed for the 4th output.");
  }
  if (auto output_5_type = (*this)->result(5).type()) {
    IR_ENFORCE(output_5_type.isa<paddle::dialect::DenseTensorType>(),
                   "Type validation failed for the 5th output.");
  }
  if (auto output_6_type = (*this)->result(6).type()) {
    IR_ENFORCE(output_6_type.isa<paddle::dialect::DenseTensorType>(),
                   "Type validation failed for the 6th output.");
  }
  }
  VLOG(4) << "End Verifying for: FusedDconvDreluDbnOp.";
}

void FusedDconvDreluDbnOp::InferMeta( phi::InferMetaContext *infer_meta ) {
  auto fn = PD_INFER_META(phi::FusedDconvDreluDbnInferMeta);
  fn(infer_meta);
}

phi::DataType FusedDconvDreluDbnOp::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: FusedDconvDreluDbnOp";
  


  return expected_kernel_dtype;
}

const char *FusedDotProductAttentionOp::attributes_name[4] = { "scaling_factor", "dropout_probability", "is_training", "is_causal_masking" };

OpInfoTuple FusedDotProductAttentionOp::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("q", "paddle::dialect::DenseTensorType", false, false, false, true), paddle::dialect::OpInputInfo("k", "paddle::dialect::DenseTensorType", false, false, false, true), paddle::dialect::OpInputInfo("v", "paddle::dialect::DenseTensorType", false, false, false, true), paddle::dialect::OpInputInfo("mask", "paddle::dialect::DenseTensorType", false, false, false, false) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = { paddle::dialect::OpAttributeInfo("scaling_factor", "pir::FloatAttribute", ""), paddle::dialect::OpAttributeInfo("dropout_probability", "pir::FloatAttribute", ""), paddle::dialect::OpAttributeInfo("is_training", "pir::BoolAttribute", ""), paddle::dialect::OpAttributeInfo("is_causal_masking", "pir::BoolAttribute", "") };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("out", "paddle::dialect::DenseTensorType", false, false), paddle::dialect::OpOutputInfo("softmax_out", "paddle::dialect::DenseTensorType", false, false), paddle::dialect::OpOutputInfo("rng_state", "paddle::dialect::DenseTensorType", false, false) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("FusedDotProductAttentionInferMeta", {"q", "k", "v"}, "fused_dot_product_attention", {"q", "k", "v", "mask", "scaling_factor", "dropout_probability", "is_training", "is_causal_masking"}, {"q"}, {}, {}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "fused_dot_product_attention");
}

void FusedDotProductAttentionOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value q_, pir::Value k_, pir::Value v_, pir::Value mask_, float scaling_factor, float dropout_probability, bool is_training, bool is_causal_masking) {
  VLOG(4) << "Start build FusedDotProductAttentionOp";



  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {q_, k_, v_, mask_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_scaling_factor = pir::FloatAttribute::get(pir::IrContext::Instance(), scaling_factor);
  argument.AddAttribute("scaling_factor", attr_scaling_factor);
  pir::Attribute attr_dropout_probability = pir::FloatAttribute::get(pir::IrContext::Instance(), dropout_probability);
  argument.AddAttribute("dropout_probability", attr_dropout_probability);
  pir::Attribute attr_is_training = pir::BoolAttribute::get(pir::IrContext::Instance(), is_training);
  argument.AddAttribute("is_training", attr_is_training);
  pir::Attribute attr_is_causal_masking = pir::BoolAttribute::get(pir::IrContext::Instance(), is_causal_masking);
  argument.AddAttribute("is_causal_masking", attr_is_causal_masking);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType q = q_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)q;
  paddle::dialect::DenseTensorType k = k_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)k;
  paddle::dialect::DenseTensorType v = v_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)v;
  paddle::dialect::DenseTensorType mask = mask_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)mask;

  VLOG(4) << "Builder construction  dense_q";
  paddle::dialect::IrTensor ir_tensor_q(paddle::dialect::TransToPhiDataType(q.dtype()),
                                                      q.dims(),
                                                      q.data_layout(),
                                                      q.lod(),
                                                      q.offset());
  VLOG(4) << "Builder construction  meta_q";
  paddle::dialect::IrMetaTensor meta_q(&ir_tensor_q);

  VLOG(4) << "Builder construction  dense_k";
  paddle::dialect::IrTensor ir_tensor_k(paddle::dialect::TransToPhiDataType(k.dtype()),
                                                      k.dims(),
                                                      k.data_layout(),
                                                      k.lod(),
                                                      k.offset());
  VLOG(4) << "Builder construction  meta_k";
  paddle::dialect::IrMetaTensor meta_k(&ir_tensor_k);

  VLOG(4) << "Builder construction  dense_v";
  paddle::dialect::IrTensor ir_tensor_v(paddle::dialect::TransToPhiDataType(v.dtype()),
                                                      v.dims(),
                                                      v.data_layout(),
                                                      v.lod(),
                                                      v.offset());
  VLOG(4) << "Builder construction  meta_v";
  paddle::dialect::IrMetaTensor meta_v(&ir_tensor_v);
  paddle::dialect::IrTensor dense_out;
  paddle::dialect::IrMetaTensor meta_out(&dense_out);
  paddle::dialect::IrTensor dense_softmax_out;
  paddle::dialect::IrMetaTensor meta_softmax_out(&dense_softmax_out);
  paddle::dialect::IrTensor dense_rng_state;
  paddle::dialect::IrMetaTensor meta_rng_state(&dense_rng_state);

  phi::FusedDotProductAttentionInferMeta(meta_q, meta_k, meta_v, &meta_out, &meta_softmax_out, &meta_rng_state);

  std::vector<pir::Type> argument_outputs;
  pir::Type out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_out.dtype()), dense_out.dims(), dense_out.layout(), dense_out.lod(), dense_out.offset());
  argument_outputs.push_back(out_dense_tensor_type);

  pir::Type softmax_out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_softmax_out.dtype()), dense_softmax_out.dims(), dense_softmax_out.layout(), dense_softmax_out.lod(), dense_softmax_out.offset());
  argument_outputs.push_back(softmax_out_dense_tensor_type);

  pir::Type rng_state_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_rng_state.dtype()), dense_rng_state.dims(), dense_rng_state.layout(), dense_rng_state.lod(), dense_rng_state.offset());
  argument_outputs.push_back(rng_state_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void FusedDotProductAttentionOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value q_, pir::Value k_, pir::Value v_, pir::Value mask_, pir::AttributeMap attributes) {
  VLOG(4) << "Start build FusedDotProductAttentionOp";


  IR_ENFORCE(
      attributes.find("scaling_factor") != attributes.end(),
          "'scaling_factor' Attribute is expected for FusedDotProductAttentionOp. ");
  float scaling_factor = attributes.at("scaling_factor").dyn_cast<pir::FloatAttribute>().data();

  IR_ENFORCE(
      attributes.find("dropout_probability") != attributes.end(),
          "'dropout_probability' Attribute is expected for FusedDotProductAttentionOp. ");
  float dropout_probability = attributes.at("dropout_probability").dyn_cast<pir::FloatAttribute>().data();

  IR_ENFORCE(
      attributes.find("is_training") != attributes.end(),
          "'is_training' Attribute is expected for FusedDotProductAttentionOp. ");
  bool is_training = attributes.at("is_training").dyn_cast<pir::BoolAttribute>().data();

  IR_ENFORCE(
      attributes.find("is_causal_masking") != attributes.end(),
          "'is_causal_masking' Attribute is expected for FusedDotProductAttentionOp. ");
  bool is_causal_masking = attributes.at("is_causal_masking").dyn_cast<pir::BoolAttribute>().data();


  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {q_, k_, v_, mask_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_scaling_factor = pir::FloatAttribute::get(pir::IrContext::Instance(), scaling_factor);
  argument.AddAttribute("scaling_factor", attr_scaling_factor);
  pir::Attribute attr_dropout_probability = pir::FloatAttribute::get(pir::IrContext::Instance(), dropout_probability);
  argument.AddAttribute("dropout_probability", attr_dropout_probability);
  pir::Attribute attr_is_training = pir::BoolAttribute::get(pir::IrContext::Instance(), is_training);
  argument.AddAttribute("is_training", attr_is_training);
  pir::Attribute attr_is_causal_masking = pir::BoolAttribute::get(pir::IrContext::Instance(), is_causal_masking);
  argument.AddAttribute("is_causal_masking", attr_is_causal_masking);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType q = q_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)q;
  paddle::dialect::DenseTensorType k = k_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)k;
  paddle::dialect::DenseTensorType v = v_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)v;
  paddle::dialect::DenseTensorType mask = mask_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)mask;

  VLOG(4) << "Builder construction  dense_q";
  paddle::dialect::IrTensor ir_tensor_q(paddle::dialect::TransToPhiDataType(q.dtype()),
                                                      q.dims(),
                                                      q.data_layout(),
                                                      q.lod(),
                                                      q.offset());
  VLOG(4) << "Builder construction  meta_q";
  paddle::dialect::IrMetaTensor meta_q(&ir_tensor_q);

  VLOG(4) << "Builder construction  dense_k";
  paddle::dialect::IrTensor ir_tensor_k(paddle::dialect::TransToPhiDataType(k.dtype()),
                                                      k.dims(),
                                                      k.data_layout(),
                                                      k.lod(),
                                                      k.offset());
  VLOG(4) << "Builder construction  meta_k";
  paddle::dialect::IrMetaTensor meta_k(&ir_tensor_k);

  VLOG(4) << "Builder construction  dense_v";
  paddle::dialect::IrTensor ir_tensor_v(paddle::dialect::TransToPhiDataType(v.dtype()),
                                                      v.dims(),
                                                      v.data_layout(),
                                                      v.lod(),
                                                      v.offset());
  VLOG(4) << "Builder construction  meta_v";
  paddle::dialect::IrMetaTensor meta_v(&ir_tensor_v);
  paddle::dialect::IrTensor dense_out;
  paddle::dialect::IrMetaTensor meta_out(&dense_out);
  paddle::dialect::IrTensor dense_softmax_out;
  paddle::dialect::IrMetaTensor meta_softmax_out(&dense_softmax_out);
  paddle::dialect::IrTensor dense_rng_state;
  paddle::dialect::IrMetaTensor meta_rng_state(&dense_rng_state);

  phi::FusedDotProductAttentionInferMeta(meta_q, meta_k, meta_v, &meta_out, &meta_softmax_out, &meta_rng_state);

  std::vector<pir::Type> argument_outputs;
  pir::Type out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_out.dtype()), dense_out.dims(), dense_out.layout(), dense_out.lod(), dense_out.offset());
  argument_outputs.push_back(out_dense_tensor_type);

  pir::Type softmax_out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_softmax_out.dtype()), dense_softmax_out.dims(), dense_softmax_out.layout(), dense_softmax_out.lod(), dense_softmax_out.offset());
  argument_outputs.push_back(softmax_out_dense_tensor_type);

  pir::Type rng_state_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_rng_state.dtype()), dense_rng_state.dims(), dense_rng_state.layout(), dense_rng_state.lod(), dense_rng_state.offset());
  argument_outputs.push_back(rng_state_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void FusedDotProductAttentionOp::VerifySig() {
  VLOG(4) << "Start Verifying inputs, outputs and attributes for: FusedDotProductAttentionOp.";
  VLOG(4) << "Verifying inputs:";
  {
  auto input_size = num_operands();
  IR_ENFORCE(input_size == 4u,
                    "The size %d of inputs must be equal to 4.", input_size);
  IR_ENFORCE((*this)->operand_source(0).type().isa<paddle::dialect::DenseTensorType>(),
                  "Type validation failed for the 0th input, got %s.", (*this)->operand_source(0).type());
  IR_ENFORCE((*this)->operand_source(1).type().isa<paddle::dialect::DenseTensorType>(),
                  "Type validation failed for the 1th input, got %s.", (*this)->operand_source(1).type());
  IR_ENFORCE((*this)->operand_source(2).type().isa<paddle::dialect::DenseTensorType>(),
                  "Type validation failed for the 2th input, got %s.", (*this)->operand_source(2).type());
  IR_ENFORCE((*this)->operand_source(3).type().isa<paddle::dialect::DenseTensorType>(),
                  "Type validation failed for the 3th input, got %s.", (*this)->operand_source(3).type());
  }
  VLOG(4) << "Verifying attributes:";
  {
  auto& attributes = this->attributes();
  IR_ENFORCE(attributes.count("scaling_factor")>0,
                 "scaling_factor does not exist.");
  IR_ENFORCE(attributes.at("scaling_factor").isa<pir::FloatAttribute>(),
                 "Type of attribute: scaling_factor is not pir::FloatAttribute.");

  IR_ENFORCE(attributes.count("dropout_probability")>0,
                 "dropout_probability does not exist.");
  IR_ENFORCE(attributes.at("dropout_probability").isa<pir::FloatAttribute>(),
                 "Type of attribute: dropout_probability is not pir::FloatAttribute.");

  IR_ENFORCE(attributes.count("is_training")>0,
                 "is_training does not exist.");
  IR_ENFORCE(attributes.at("is_training").isa<pir::BoolAttribute>(),
                 "Type of attribute: is_training is not pir::BoolAttribute.");

  IR_ENFORCE(attributes.count("is_causal_masking")>0,
                 "is_causal_masking does not exist.");
  IR_ENFORCE(attributes.at("is_causal_masking").isa<pir::BoolAttribute>(),
                 "Type of attribute: is_causal_masking is not pir::BoolAttribute.");

  }
  VLOG(4) << "Verifying outputs:";
  {
  auto output_size = num_results();
  IR_ENFORCE(output_size == 3u,
                    "The size %d of outputs must be equal to 3.", output_size);
  IR_ENFORCE((*this)->result(0).type().isa<paddle::dialect::DenseTensorType>(),
                 "Type validation failed for the 0th output.");
  IR_ENFORCE((*this)->result(1).type().isa<paddle::dialect::DenseTensorType>(),
                 "Type validation failed for the 1th output.");
  IR_ENFORCE((*this)->result(2).type().isa<paddle::dialect::DenseTensorType>(),
                 "Type validation failed for the 2th output.");
  }
  VLOG(4) << "End Verifying for: FusedDotProductAttentionOp.";
}

void FusedDotProductAttentionOp::InferMeta( phi::InferMetaContext *infer_meta ) {
  auto fn = PD_INFER_META(phi::FusedDotProductAttentionInferMeta);
  fn(infer_meta);
}

phi::DataType FusedDotProductAttentionOp::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: FusedDotProductAttentionOp";
  


  return expected_kernel_dtype;
}

const char *FusedDropoutAddOp::attributes_name[5] = { "p", "is_test", "mode", "seed", "fix_seed" };

OpInfoTuple FusedDropoutAddOp::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("x", "paddle::dialect::DenseTensorType", false, false, false, true), paddle::dialect::OpInputInfo("y", "paddle::dialect::DenseTensorType", false, false, false, true), paddle::dialect::OpInputInfo("seed_tensor", "paddle::dialect::DenseTensorType", true, false, false, false) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = { paddle::dialect::OpAttributeInfo("p", "paddle::dialect::ScalarAttribute", "float"), paddle::dialect::OpAttributeInfo("is_test", "pir::BoolAttribute", ""), paddle::dialect::OpAttributeInfo("mode", "pir::StrAttribute", ""), paddle::dialect::OpAttributeInfo("seed", "pir::Int32Attribute", ""), paddle::dialect::OpAttributeInfo("fix_seed", "pir::BoolAttribute", "") };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("out", "paddle::dialect::DenseTensorType", false, false), paddle::dialect::OpOutputInfo("seed_offset", "paddle::dialect::DenseTensorType", false, false) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("FusedDropoutAddInferMeta", {"x", "y"}, "fused_dropout_add", {"x", "y", "seed_tensor", "p", "is_test", "mode", "seed", "fix_seed"}, {"x"}, {}, {}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "fused_dropout_add");
}

void FusedDropoutAddOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, pir::Value y_, pir::Value seed_tensor_, float p, bool is_test, const std::string& mode, int seed, bool fix_seed) {
  VLOG(4) << "Start build FusedDropoutAddOp";



  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_, y_, seed_tensor_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_p = paddle::dialect::TransToIrAttribute(p, pir::IrContext::Instance());
  argument.AddAttribute("p", attr_p);
  pir::Attribute attr_is_test = pir::BoolAttribute::get(pir::IrContext::Instance(), is_test);
  argument.AddAttribute("is_test", attr_is_test);
  pir::Attribute attr_mode = pir::StrAttribute::get(pir::IrContext::Instance(), mode);
  argument.AddAttribute("mode", attr_mode);
  pir::Attribute attr_seed = pir::Int32Attribute::get(pir::IrContext::Instance(), seed);
  argument.AddAttribute("seed", attr_seed);
  pir::Attribute attr_fix_seed = pir::BoolAttribute::get(pir::IrContext::Instance(), fix_seed);
  argument.AddAttribute("fix_seed", attr_fix_seed);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;
  paddle::dialect::DenseTensorType y = y_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)y;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);

  VLOG(4) << "Builder construction  dense_y";
  paddle::dialect::IrTensor ir_tensor_y(paddle::dialect::TransToPhiDataType(y.dtype()),
                                                      y.dims(),
                                                      y.data_layout(),
                                                      y.lod(),
                                                      y.offset());
  VLOG(4) << "Builder construction  meta_y";
  paddle::dialect::IrMetaTensor meta_y(&ir_tensor_y);
  paddle::dialect::IrTensor dense_out;
  paddle::dialect::IrMetaTensor meta_out(&dense_out);
  paddle::dialect::IrTensor dense_seed_offset;
  paddle::dialect::IrMetaTensor meta_seed_offset(&dense_seed_offset);

  phi::FusedDropoutAddInferMeta(meta_x, meta_y, &meta_out, &meta_seed_offset);

  std::vector<pir::Type> argument_outputs;
  pir::Type out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_out.dtype()), dense_out.dims(), dense_out.layout(), dense_out.lod(), dense_out.offset());
  argument_outputs.push_back(out_dense_tensor_type);

  pir::Type seed_offset_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_seed_offset.dtype()), dense_seed_offset.dims(), dense_seed_offset.layout(), dense_seed_offset.lod(), dense_seed_offset.offset());
  argument_outputs.push_back(seed_offset_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void FusedDropoutAddOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, pir::Value y_, pir::Value seed_tensor_, pir::AttributeMap attributes) {
  VLOG(4) << "Start build FusedDropoutAddOp";


  IR_ENFORCE(
      attributes.find("p") != attributes.end(),
          "'p' Attribute is expected for FusedDropoutAddOp. ");
  float p = attributes.at("p").dyn_cast<paddle::dialect::ScalarAttribute>().data().to<float>();

  IR_ENFORCE(
      attributes.find("is_test") != attributes.end(),
          "'is_test' Attribute is expected for FusedDropoutAddOp. ");
  bool is_test = attributes.at("is_test").dyn_cast<pir::BoolAttribute>().data();

  IR_ENFORCE(
      attributes.find("mode") != attributes.end(),
          "'mode' Attribute is expected for FusedDropoutAddOp. ");
  std::string mode = attributes.at("mode").dyn_cast<pir::StrAttribute>().AsString();

  IR_ENFORCE(
      attributes.find("seed") != attributes.end(),
          "'seed' Attribute is expected for FusedDropoutAddOp. ");
  int seed = attributes.at("seed").dyn_cast<pir::Int32Attribute>().data();

  IR_ENFORCE(
      attributes.find("fix_seed") != attributes.end(),
          "'fix_seed' Attribute is expected for FusedDropoutAddOp. ");
  bool fix_seed = attributes.at("fix_seed").dyn_cast<pir::BoolAttribute>().data();


  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_, y_, seed_tensor_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_p = paddle::dialect::TransToIrAttribute(p, pir::IrContext::Instance());
  argument.AddAttribute("p", attr_p);
  pir::Attribute attr_is_test = pir::BoolAttribute::get(pir::IrContext::Instance(), is_test);
  argument.AddAttribute("is_test", attr_is_test);
  pir::Attribute attr_mode = pir::StrAttribute::get(pir::IrContext::Instance(), mode);
  argument.AddAttribute("mode", attr_mode);
  pir::Attribute attr_seed = pir::Int32Attribute::get(pir::IrContext::Instance(), seed);
  argument.AddAttribute("seed", attr_seed);
  pir::Attribute attr_fix_seed = pir::BoolAttribute::get(pir::IrContext::Instance(), fix_seed);
  argument.AddAttribute("fix_seed", attr_fix_seed);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;
  paddle::dialect::DenseTensorType y = y_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)y;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);

  VLOG(4) << "Builder construction  dense_y";
  paddle::dialect::IrTensor ir_tensor_y(paddle::dialect::TransToPhiDataType(y.dtype()),
                                                      y.dims(),
                                                      y.data_layout(),
                                                      y.lod(),
                                                      y.offset());
  VLOG(4) << "Builder construction  meta_y";
  paddle::dialect::IrMetaTensor meta_y(&ir_tensor_y);
  paddle::dialect::IrTensor dense_out;
  paddle::dialect::IrMetaTensor meta_out(&dense_out);
  paddle::dialect::IrTensor dense_seed_offset;
  paddle::dialect::IrMetaTensor meta_seed_offset(&dense_seed_offset);

  phi::FusedDropoutAddInferMeta(meta_x, meta_y, &meta_out, &meta_seed_offset);

  std::vector<pir::Type> argument_outputs;
  pir::Type out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_out.dtype()), dense_out.dims(), dense_out.layout(), dense_out.lod(), dense_out.offset());
  argument_outputs.push_back(out_dense_tensor_type);

  pir::Type seed_offset_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_seed_offset.dtype()), dense_seed_offset.dims(), dense_seed_offset.layout(), dense_seed_offset.lod(), dense_seed_offset.offset());
  argument_outputs.push_back(seed_offset_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void FusedDropoutAddOp::VerifySig() {
  VLOG(4) << "Start Verifying inputs, outputs and attributes for: FusedDropoutAddOp.";
  VLOG(4) << "Verifying inputs:";
  {
  auto input_size = num_operands();
  IR_ENFORCE(input_size == 3u,
                    "The size %d of inputs must be equal to 3.", input_size);
  IR_ENFORCE((*this)->operand_source(0).type().isa<paddle::dialect::DenseTensorType>(),
                  "Type validation failed for the 0th input, got %s.", (*this)->operand_source(0).type());
  IR_ENFORCE((*this)->operand_source(1).type().isa<paddle::dialect::DenseTensorType>(),
                  "Type validation failed for the 1th input, got %s.", (*this)->operand_source(1).type());
  if (auto val = (*this)->operand(2)) {
    IR_ENFORCE(val.type().isa<paddle::dialect::DenseTensorType>(),
                   "Type validation failed for the 2th input, got %s.", (*this)->operand_source(2).type());
  }
  }
  VLOG(4) << "Verifying attributes:";
  {
  auto& attributes = this->attributes();
  IR_ENFORCE(attributes.count("p")>0,
                 "p does not exist.");
  IR_ENFORCE(attributes.at("p").isa<paddle::dialect::ScalarAttribute>(),
                 "Type of attribute: p is not paddle::dialect::ScalarAttribute.");

  IR_ENFORCE(attributes.count("is_test")>0,
                 "is_test does not exist.");
  IR_ENFORCE(attributes.at("is_test").isa<pir::BoolAttribute>(),
                 "Type of attribute: is_test is not pir::BoolAttribute.");

  IR_ENFORCE(attributes.count("mode")>0,
                 "mode does not exist.");
  IR_ENFORCE(attributes.at("mode").isa<pir::StrAttribute>(),
                 "Type of attribute: mode is not pir::StrAttribute.");

  IR_ENFORCE(attributes.count("seed")>0,
                 "seed does not exist.");
  IR_ENFORCE(attributes.at("seed").isa<pir::Int32Attribute>(),
                 "Type of attribute: seed is not pir::Int32Attribute.");

  IR_ENFORCE(attributes.count("fix_seed")>0,
                 "fix_seed does not exist.");
  IR_ENFORCE(attributes.at("fix_seed").isa<pir::BoolAttribute>(),
                 "Type of attribute: fix_seed is not pir::BoolAttribute.");

  }
  VLOG(4) << "Verifying outputs:";
  {
  auto output_size = num_results();
  IR_ENFORCE(output_size == 2u,
                    "The size %d of outputs must be equal to 2.", output_size);
  IR_ENFORCE((*this)->result(0).type().isa<paddle::dialect::DenseTensorType>(),
                 "Type validation failed for the 0th output.");
  IR_ENFORCE((*this)->result(1).type().isa<paddle::dialect::DenseTensorType>(),
                 "Type validation failed for the 1th output.");
  }
  VLOG(4) << "End Verifying for: FusedDropoutAddOp.";
}

void FusedDropoutAddOp::InferMeta( phi::InferMetaContext *infer_meta ) {
  auto fn = PD_INFER_META(phi::FusedDropoutAddInferMeta);
  fn(infer_meta);
}

phi::DataType FusedDropoutAddOp::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: FusedDropoutAddOp";
  


  return expected_kernel_dtype;
}

const char *FusedEmbeddingEltwiseLayernormOp::attributes_name[1] = { "epsilon" };

OpInfoTuple FusedEmbeddingEltwiseLayernormOp::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("ids", "pir::VectorType<paddle::dialect::DenseTensorType>", false, false, false, false), paddle::dialect::OpInputInfo("embs", "pir::VectorType<paddle::dialect::DenseTensorType>", false, false, false, false), paddle::dialect::OpInputInfo("bias", "paddle::dialect::DenseTensorType", false, false, false, false), paddle::dialect::OpInputInfo("scale", "paddle::dialect::DenseTensorType", false, false, false, false) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = { paddle::dialect::OpAttributeInfo("epsilon", "pir::FloatAttribute", "") };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("out", "paddle::dialect::DenseTensorType", false, false) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("FusedEmbeddingEltWiseLayerNormInferMeta", {"ids", "embs", "bias", "scale", "epsilon"}, "fused_embedding_eltwise_layernorm", {"ids", "embs", "bias", "scale", "epsilon"}, {"embs"}, {}, {}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "fused_embedding_eltwise_layernorm");
}

void FusedEmbeddingEltwiseLayernormOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value ids_, pir::Value embs_, pir::Value bias_, pir::Value scale_, float epsilon) {
  VLOG(4) << "Start build FusedEmbeddingEltwiseLayernormOp";



  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {ids_, embs_, bias_, scale_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_epsilon = pir::FloatAttribute::get(pir::IrContext::Instance(), epsilon);
  argument.AddAttribute("epsilon", attr_epsilon);

  VLOG(4) << "Builder construction outputs";
  pir::VectorType ids = ids_.type().dyn_cast<pir::VectorType>(); (void)ids;
  pir::VectorType embs = embs_.type().dyn_cast<pir::VectorType>(); (void)embs;
  paddle::dialect::DenseTensorType bias = bias_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)bias;
  paddle::dialect::DenseTensorType scale = scale_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)scale;
  std::vector<paddle::dialect::IrTensor> vec_ir_tensor_ids;
  for (size_t i=0; i < static_cast<size_t>(ids.size()); i++) {
    vec_ir_tensor_ids.push_back(paddle::dialect::IrTensor(paddle::dialect::TransToPhiDataType(ids[i].dyn_cast<paddle::dialect::DenseTensorType>().dtype()),
                                                                     ids[i].dyn_cast<paddle::dialect::DenseTensorType>().dims(),
                                                                     ids[i].dyn_cast<paddle::dialect::DenseTensorType>().data_layout(),
                                                                     ids[i].dyn_cast<paddle::dialect::DenseTensorType>().lod(),
                                                                     ids[i].dyn_cast<paddle::dialect::DenseTensorType>().offset()));
  }
  std::vector<paddle::dialect::IrMetaTensor> vec_meta_ids;
  for (size_t i=0; i < vec_ir_tensor_ids.size(); i++) {
    vec_meta_ids.push_back(paddle::dialect::IrMetaTensor(&vec_ir_tensor_ids[i]));
  }

  std::vector<const phi::MetaTensor*> meta_ids;
  for (size_t i=0; i < static_cast<size_t>(vec_meta_ids.size()); i++) {
    meta_ids.push_back(&vec_meta_ids[i]);
  }
   std::vector<paddle::dialect::IrTensor> vec_ir_tensor_embs;
  for (size_t i=0; i < static_cast<size_t>(embs.size()); i++) {
    vec_ir_tensor_embs.push_back(paddle::dialect::IrTensor(paddle::dialect::TransToPhiDataType(embs[i].dyn_cast<paddle::dialect::DenseTensorType>().dtype()),
                                                                     embs[i].dyn_cast<paddle::dialect::DenseTensorType>().dims(),
                                                                     embs[i].dyn_cast<paddle::dialect::DenseTensorType>().data_layout(),
                                                                     embs[i].dyn_cast<paddle::dialect::DenseTensorType>().lod(),
                                                                     embs[i].dyn_cast<paddle::dialect::DenseTensorType>().offset()));
  }
  std::vector<paddle::dialect::IrMetaTensor> vec_meta_embs;
  for (size_t i=0; i < vec_ir_tensor_embs.size(); i++) {
    vec_meta_embs.push_back(paddle::dialect::IrMetaTensor(&vec_ir_tensor_embs[i]));
  }

  std::vector<const phi::MetaTensor*> meta_embs;
  for (size_t i=0; i < static_cast<size_t>(vec_meta_embs.size()); i++) {
    meta_embs.push_back(&vec_meta_embs[i]);
  }
 
  VLOG(4) << "Builder construction  dense_bias";
  paddle::dialect::IrTensor ir_tensor_bias(paddle::dialect::TransToPhiDataType(bias.dtype()),
                                                      bias.dims(),
                                                      bias.data_layout(),
                                                      bias.lod(),
                                                      bias.offset());
  VLOG(4) << "Builder construction  meta_bias";
  paddle::dialect::IrMetaTensor meta_bias(&ir_tensor_bias);

  VLOG(4) << "Builder construction  dense_scale";
  paddle::dialect::IrTensor ir_tensor_scale(paddle::dialect::TransToPhiDataType(scale.dtype()),
                                                      scale.dims(),
                                                      scale.data_layout(),
                                                      scale.lod(),
                                                      scale.offset());
  VLOG(4) << "Builder construction  meta_scale";
  paddle::dialect::IrMetaTensor meta_scale(&ir_tensor_scale);
  paddle::dialect::IrTensor dense_out;
  paddle::dialect::IrMetaTensor meta_out(&dense_out);

  phi::FusedEmbeddingEltWiseLayerNormInferMeta(meta_ids, meta_embs, meta_bias, meta_scale, epsilon, &meta_out);

  std::vector<pir::Type> argument_outputs;
  pir::Type out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_out.dtype()), dense_out.dims(), dense_out.layout(), dense_out.lod(), dense_out.offset());
  argument_outputs.push_back(out_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void FusedEmbeddingEltwiseLayernormOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value ids_, pir::Value embs_, pir::Value bias_, pir::Value scale_, pir::AttributeMap attributes) {
  VLOG(4) << "Start build FusedEmbeddingEltwiseLayernormOp";


  IR_ENFORCE(
      attributes.find("epsilon") != attributes.end(),
          "'epsilon' Attribute is expected for FusedEmbeddingEltwiseLayernormOp. ");
  float epsilon = attributes.at("epsilon").dyn_cast<pir::FloatAttribute>().data();


  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {ids_, embs_, bias_, scale_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_epsilon = pir::FloatAttribute::get(pir::IrContext::Instance(), epsilon);
  argument.AddAttribute("epsilon", attr_epsilon);

  VLOG(4) << "Builder construction outputs";
  pir::VectorType ids = ids_.type().dyn_cast<pir::VectorType>(); (void)ids;
  pir::VectorType embs = embs_.type().dyn_cast<pir::VectorType>(); (void)embs;
  paddle::dialect::DenseTensorType bias = bias_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)bias;
  paddle::dialect::DenseTensorType scale = scale_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)scale;
  std::vector<paddle::dialect::IrTensor> vec_ir_tensor_ids;
  for (size_t i=0; i < static_cast<size_t>(ids.size()); i++) {
    vec_ir_tensor_ids.push_back(paddle::dialect::IrTensor(paddle::dialect::TransToPhiDataType(ids[i].dyn_cast<paddle::dialect::DenseTensorType>().dtype()),
                                                                     ids[i].dyn_cast<paddle::dialect::DenseTensorType>().dims(),
                                                                     ids[i].dyn_cast<paddle::dialect::DenseTensorType>().data_layout(),
                                                                     ids[i].dyn_cast<paddle::dialect::DenseTensorType>().lod(),
                                                                     ids[i].dyn_cast<paddle::dialect::DenseTensorType>().offset()));
  }
  std::vector<paddle::dialect::IrMetaTensor> vec_meta_ids;
  for (size_t i=0; i < vec_ir_tensor_ids.size(); i++) {
    vec_meta_ids.push_back(paddle::dialect::IrMetaTensor(&vec_ir_tensor_ids[i]));
  }

  std::vector<const phi::MetaTensor*> meta_ids;
  for (size_t i=0; i < static_cast<size_t>(vec_meta_ids.size()); i++) {
    meta_ids.push_back(&vec_meta_ids[i]);
  }
   std::vector<paddle::dialect::IrTensor> vec_ir_tensor_embs;
  for (size_t i=0; i < static_cast<size_t>(embs.size()); i++) {
    vec_ir_tensor_embs.push_back(paddle::dialect::IrTensor(paddle::dialect::TransToPhiDataType(embs[i].dyn_cast<paddle::dialect::DenseTensorType>().dtype()),
                                                                     embs[i].dyn_cast<paddle::dialect::DenseTensorType>().dims(),
                                                                     embs[i].dyn_cast<paddle::dialect::DenseTensorType>().data_layout(),
                                                                     embs[i].dyn_cast<paddle::dialect::DenseTensorType>().lod(),
                                                                     embs[i].dyn_cast<paddle::dialect::DenseTensorType>().offset()));
  }
  std::vector<paddle::dialect::IrMetaTensor> vec_meta_embs;
  for (size_t i=0; i < vec_ir_tensor_embs.size(); i++) {
    vec_meta_embs.push_back(paddle::dialect::IrMetaTensor(&vec_ir_tensor_embs[i]));
  }

  std::vector<const phi::MetaTensor*> meta_embs;
  for (size_t i=0; i < static_cast<size_t>(vec_meta_embs.size()); i++) {
    meta_embs.push_back(&vec_meta_embs[i]);
  }
 
  VLOG(4) << "Builder construction  dense_bias";
  paddle::dialect::IrTensor ir_tensor_bias(paddle::dialect::TransToPhiDataType(bias.dtype()),
                                                      bias.dims(),
                                                      bias.data_layout(),
                                                      bias.lod(),
                                                      bias.offset());
  VLOG(4) << "Builder construction  meta_bias";
  paddle::dialect::IrMetaTensor meta_bias(&ir_tensor_bias);

  VLOG(4) << "Builder construction  dense_scale";
  paddle::dialect::IrTensor ir_tensor_scale(paddle::dialect::TransToPhiDataType(scale.dtype()),
                                                      scale.dims(),
                                                      scale.data_layout(),
                                                      scale.lod(),
                                                      scale.offset());
  VLOG(4) << "Builder construction  meta_scale";
  paddle::dialect::IrMetaTensor meta_scale(&ir_tensor_scale);
  paddle::dialect::IrTensor dense_out;
  paddle::dialect::IrMetaTensor meta_out(&dense_out);

  phi::FusedEmbeddingEltWiseLayerNormInferMeta(meta_ids, meta_embs, meta_bias, meta_scale, epsilon, &meta_out);

  std::vector<pir::Type> argument_outputs;
  pir::Type out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_out.dtype()), dense_out.dims(), dense_out.layout(), dense_out.lod(), dense_out.offset());
  argument_outputs.push_back(out_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void FusedEmbeddingEltwiseLayernormOp::VerifySig() {
  VLOG(4) << "Start Verifying inputs, outputs and attributes for: FusedEmbeddingEltwiseLayernormOp.";
  VLOG(4) << "Verifying inputs:";
  {
  auto input_size = num_operands();
  IR_ENFORCE(input_size == 4u,
                    "The size %d of inputs must be equal to 4.", input_size);
  if (auto vec_type = (*this)->operand_source(0).type().dyn_cast<pir::VectorType>()) {
      for (size_t i = 0; i < vec_type.size(); ++i) {
        IR_ENFORCE(vec_type[i].isa<paddle::dialect::DenseTensorType>(),
                       "Type validation failed for the 0th input, got %s.", (*this)->operand_source(0).type());
      }
  }
  else {
    IR_ENFORCE((*this)->operand_source(0).type().isa<paddle::dialect::DenseTensorType>(),
                   "Type validation failed for the 0th input, got %s.", (*this)->operand_source(0).type());
  }
  if (auto vec_type = (*this)->operand_source(1).type().dyn_cast<pir::VectorType>()) {
      for (size_t i = 0; i < vec_type.size(); ++i) {
        IR_ENFORCE(vec_type[i].isa<paddle::dialect::DenseTensorType>(),
                       "Type validation failed for the 1th input, got %s.", (*this)->operand_source(1).type());
      }
  }
  else {
    IR_ENFORCE((*this)->operand_source(1).type().isa<paddle::dialect::DenseTensorType>(),
                   "Type validation failed for the 1th input, got %s.", (*this)->operand_source(1).type());
  }
  IR_ENFORCE((*this)->operand_source(2).type().isa<paddle::dialect::DenseTensorType>(),
                  "Type validation failed for the 2th input, got %s.", (*this)->operand_source(2).type());
  IR_ENFORCE((*this)->operand_source(3).type().isa<paddle::dialect::DenseTensorType>(),
                  "Type validation failed for the 3th input, got %s.", (*this)->operand_source(3).type());
  }
  VLOG(4) << "Verifying attributes:";
  {
  auto& attributes = this->attributes();
  IR_ENFORCE(attributes.count("epsilon")>0,
                 "epsilon does not exist.");
  IR_ENFORCE(attributes.at("epsilon").isa<pir::FloatAttribute>(),
                 "Type of attribute: epsilon is not pir::FloatAttribute.");

  }
  VLOG(4) << "Verifying outputs:";
  {
  auto output_size = num_results();
  IR_ENFORCE(output_size == 1u,
                    "The size %d of outputs must be equal to 1.", output_size);
  IR_ENFORCE((*this)->result(0).type().isa<paddle::dialect::DenseTensorType>(),
                 "Type validation failed for the 0th output.");
  }
  VLOG(4) << "End Verifying for: FusedEmbeddingEltwiseLayernormOp.";
}

void FusedEmbeddingEltwiseLayernormOp::InferMeta( phi::InferMetaContext *infer_meta ) {
  auto fn = PD_INFER_META(phi::FusedEmbeddingEltWiseLayerNormInferMeta);
  fn(infer_meta);
}

phi::DataType FusedEmbeddingEltwiseLayernormOp::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: FusedEmbeddingEltwiseLayernormOp";
  


  return expected_kernel_dtype;
}

const char *FusedFcElementwiseLayernormOp::attributes_name[4] = { "x_num_col_dims", "activation_type", "epsilon", "begin_norm_axis" };

OpInfoTuple FusedFcElementwiseLayernormOp::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("x", "paddle::dialect::DenseTensorType", false, false, false, false), paddle::dialect::OpInputInfo("w", "paddle::dialect::DenseTensorType", false, false, false, false), paddle::dialect::OpInputInfo("y", "paddle::dialect::DenseTensorType", false, false, false, false), paddle::dialect::OpInputInfo("bias0", "paddle::dialect::DenseTensorType", true, false, false, false), paddle::dialect::OpInputInfo("scale", "paddle::dialect::DenseTensorType", true, false, false, false), paddle::dialect::OpInputInfo("bias1", "paddle::dialect::DenseTensorType", true, false, false, false) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = { paddle::dialect::OpAttributeInfo("x_num_col_dims", "pir::Int32Attribute", ""), paddle::dialect::OpAttributeInfo("activation_type", "pir::StrAttribute", ""), paddle::dialect::OpAttributeInfo("epsilon", "pir::FloatAttribute", ""), paddle::dialect::OpAttributeInfo("begin_norm_axis", "pir::Int32Attribute", "") };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("out", "paddle::dialect::DenseTensorType", false, false), paddle::dialect::OpOutputInfo("mean", "paddle::dialect::DenseTensorType", true, false), paddle::dialect::OpOutputInfo("variance", "paddle::dialect::DenseTensorType", true, false) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("FusedFCElementwiseLayerNormInferMeta", {"x", "w", "y", "bias0", "scale", "bias1", "x_num_col_dims", "activation_type", "epsilon", "begin_norm_axis"}, "fused_fc_elementwise_layernorm", {"x", "w", "y", "bias0", "scale", "bias1", "x_num_col_dims", "activation_type", "epsilon", "begin_norm_axis"}, {"x"}, {}, {}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "fused_fc_elementwise_layernorm");
}

void FusedFcElementwiseLayernormOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, pir::Value w_, pir::Value y_, pir::Value bias0_, pir::Value scale_, pir::Value bias1_, int x_num_col_dims, const std::string& activation_type, float epsilon, int begin_norm_axis) {
  VLOG(4) << "Start build FusedFcElementwiseLayernormOp";



  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_, w_, y_, bias0_, scale_, bias1_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_x_num_col_dims = pir::Int32Attribute::get(pir::IrContext::Instance(), x_num_col_dims);
  argument.AddAttribute("x_num_col_dims", attr_x_num_col_dims);
  pir::Attribute attr_activation_type = pir::StrAttribute::get(pir::IrContext::Instance(), activation_type);
  argument.AddAttribute("activation_type", attr_activation_type);
  pir::Attribute attr_epsilon = pir::FloatAttribute::get(pir::IrContext::Instance(), epsilon);
  argument.AddAttribute("epsilon", attr_epsilon);
  pir::Attribute attr_begin_norm_axis = pir::Int32Attribute::get(pir::IrContext::Instance(), begin_norm_axis);
  argument.AddAttribute("begin_norm_axis", attr_begin_norm_axis);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;
  paddle::dialect::DenseTensorType w = w_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)w;
  paddle::dialect::DenseTensorType y = y_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)y;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);

  VLOG(4) << "Builder construction  dense_w";
  paddle::dialect::IrTensor ir_tensor_w(paddle::dialect::TransToPhiDataType(w.dtype()),
                                                      w.dims(),
                                                      w.data_layout(),
                                                      w.lod(),
                                                      w.offset());
  VLOG(4) << "Builder construction  meta_w";
  paddle::dialect::IrMetaTensor meta_w(&ir_tensor_w);

  VLOG(4) << "Builder construction  dense_y";
  paddle::dialect::IrTensor ir_tensor_y(paddle::dialect::TransToPhiDataType(y.dtype()),
                                                      y.dims(),
                                                      y.data_layout(),
                                                      y.lod(),
                                                      y.offset());
  VLOG(4) << "Builder construction  meta_y";
  paddle::dialect::IrMetaTensor meta_y(&ir_tensor_y);

  paddle::dialect::IrMetaTensor meta_bias0;
  paddle::dialect::IrTensor ir_tensor_bias0;
  if (bias0_.impl() != nullptr) {
    paddle::dialect::DenseTensorType bias0 = bias0_.type().dyn_cast<paddle::dialect::DenseTensorType>();
    VLOG(4) << "Builder construction  dense_bias0";
    ir_tensor_bias0 = paddle::dialect::IrTensor(paddle::dialect::TransToPhiDataType(bias0.dtype()),
                                                        bias0.dims(),
                                                        bias0.data_layout(),
                                                        bias0.lod(),
                                                        bias0.offset());
    VLOG(4) << "Builder construction  meta_bias0";
    meta_bias0 = paddle::dialect::IrMetaTensor(&ir_tensor_bias0);
  }


  paddle::dialect::IrMetaTensor meta_scale;
  paddle::dialect::IrTensor ir_tensor_scale;
  if (scale_.impl() != nullptr) {
    paddle::dialect::DenseTensorType scale = scale_.type().dyn_cast<paddle::dialect::DenseTensorType>();
    VLOG(4) << "Builder construction  dense_scale";
    ir_tensor_scale = paddle::dialect::IrTensor(paddle::dialect::TransToPhiDataType(scale.dtype()),
                                                        scale.dims(),
                                                        scale.data_layout(),
                                                        scale.lod(),
                                                        scale.offset());
    VLOG(4) << "Builder construction  meta_scale";
    meta_scale = paddle::dialect::IrMetaTensor(&ir_tensor_scale);
  }


  paddle::dialect::IrMetaTensor meta_bias1;
  paddle::dialect::IrTensor ir_tensor_bias1;
  if (bias1_.impl() != nullptr) {
    paddle::dialect::DenseTensorType bias1 = bias1_.type().dyn_cast<paddle::dialect::DenseTensorType>();
    VLOG(4) << "Builder construction  dense_bias1";
    ir_tensor_bias1 = paddle::dialect::IrTensor(paddle::dialect::TransToPhiDataType(bias1.dtype()),
                                                        bias1.dims(),
                                                        bias1.data_layout(),
                                                        bias1.lod(),
                                                        bias1.offset());
    VLOG(4) << "Builder construction  meta_bias1";
    meta_bias1 = paddle::dialect::IrMetaTensor(&ir_tensor_bias1);
  }

  paddle::dialect::IrTensor dense_out;
  paddle::dialect::IrMetaTensor meta_out(&dense_out);
  paddle::dialect::IrTensor dense_mean;
  paddle::dialect::IrMetaTensor meta_mean(&dense_mean);
  paddle::dialect::IrTensor dense_variance;
  paddle::dialect::IrMetaTensor meta_variance(&dense_variance);

  phi::FusedFCElementwiseLayerNormInferMeta(meta_x, meta_w, meta_y, meta_bias0, meta_scale, meta_bias1, x_num_col_dims, activation_type, epsilon, begin_norm_axis, &meta_out, &meta_mean, &meta_variance);

  std::vector<pir::Type> argument_outputs;
  pir::Type out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_out.dtype()), dense_out.dims(), dense_out.layout(), dense_out.lod(), dense_out.offset());
  argument_outputs.push_back(out_dense_tensor_type);

  pir::Type mean_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_mean.dtype()), dense_mean.dims(), dense_mean.layout(), dense_mean.lod(), dense_mean.offset());
  argument_outputs.push_back(mean_dense_tensor_type);

  pir::Type variance_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_variance.dtype()), dense_variance.dims(), dense_variance.layout(), dense_variance.lod(), dense_variance.offset());
  argument_outputs.push_back(variance_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void FusedFcElementwiseLayernormOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, pir::Value w_, pir::Value y_, pir::Value bias0_, pir::Value scale_, pir::Value bias1_, pir::AttributeMap attributes) {
  VLOG(4) << "Start build FusedFcElementwiseLayernormOp";


  IR_ENFORCE(
      attributes.find("x_num_col_dims") != attributes.end(),
          "'x_num_col_dims' Attribute is expected for FusedFcElementwiseLayernormOp. ");
  int x_num_col_dims = attributes.at("x_num_col_dims").dyn_cast<pir::Int32Attribute>().data();

  IR_ENFORCE(
      attributes.find("activation_type") != attributes.end(),
          "'activation_type' Attribute is expected for FusedFcElementwiseLayernormOp. ");
  std::string activation_type = attributes.at("activation_type").dyn_cast<pir::StrAttribute>().AsString();

  IR_ENFORCE(
      attributes.find("epsilon") != attributes.end(),
          "'epsilon' Attribute is expected for FusedFcElementwiseLayernormOp. ");
  float epsilon = attributes.at("epsilon").dyn_cast<pir::FloatAttribute>().data();

  IR_ENFORCE(
      attributes.find("begin_norm_axis") != attributes.end(),
          "'begin_norm_axis' Attribute is expected for FusedFcElementwiseLayernormOp. ");
  int begin_norm_axis = attributes.at("begin_norm_axis").dyn_cast<pir::Int32Attribute>().data();


  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_, w_, y_, bias0_, scale_, bias1_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_x_num_col_dims = pir::Int32Attribute::get(pir::IrContext::Instance(), x_num_col_dims);
  argument.AddAttribute("x_num_col_dims", attr_x_num_col_dims);
  pir::Attribute attr_activation_type = pir::StrAttribute::get(pir::IrContext::Instance(), activation_type);
  argument.AddAttribute("activation_type", attr_activation_type);
  pir::Attribute attr_epsilon = pir::FloatAttribute::get(pir::IrContext::Instance(), epsilon);
  argument.AddAttribute("epsilon", attr_epsilon);
  pir::Attribute attr_begin_norm_axis = pir::Int32Attribute::get(pir::IrContext::Instance(), begin_norm_axis);
  argument.AddAttribute("begin_norm_axis", attr_begin_norm_axis);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;
  paddle::dialect::DenseTensorType w = w_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)w;
  paddle::dialect::DenseTensorType y = y_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)y;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);

  VLOG(4) << "Builder construction  dense_w";
  paddle::dialect::IrTensor ir_tensor_w(paddle::dialect::TransToPhiDataType(w.dtype()),
                                                      w.dims(),
                                                      w.data_layout(),
                                                      w.lod(),
                                                      w.offset());
  VLOG(4) << "Builder construction  meta_w";
  paddle::dialect::IrMetaTensor meta_w(&ir_tensor_w);

  VLOG(4) << "Builder construction  dense_y";
  paddle::dialect::IrTensor ir_tensor_y(paddle::dialect::TransToPhiDataType(y.dtype()),
                                                      y.dims(),
                                                      y.data_layout(),
                                                      y.lod(),
                                                      y.offset());
  VLOG(4) << "Builder construction  meta_y";
  paddle::dialect::IrMetaTensor meta_y(&ir_tensor_y);

  paddle::dialect::IrMetaTensor meta_bias0;
  paddle::dialect::IrTensor ir_tensor_bias0;
  if (bias0_.impl() != nullptr) {
    paddle::dialect::DenseTensorType bias0 = bias0_.type().dyn_cast<paddle::dialect::DenseTensorType>();
    VLOG(4) << "Builder construction  dense_bias0";
    ir_tensor_bias0 = paddle::dialect::IrTensor(paddle::dialect::TransToPhiDataType(bias0.dtype()),
                                                        bias0.dims(),
                                                        bias0.data_layout(),
                                                        bias0.lod(),
                                                        bias0.offset());
    VLOG(4) << "Builder construction  meta_bias0";
    meta_bias0 = paddle::dialect::IrMetaTensor(&ir_tensor_bias0);
  }


  paddle::dialect::IrMetaTensor meta_scale;
  paddle::dialect::IrTensor ir_tensor_scale;
  if (scale_.impl() != nullptr) {
    paddle::dialect::DenseTensorType scale = scale_.type().dyn_cast<paddle::dialect::DenseTensorType>();
    VLOG(4) << "Builder construction  dense_scale";
    ir_tensor_scale = paddle::dialect::IrTensor(paddle::dialect::TransToPhiDataType(scale.dtype()),
                                                        scale.dims(),
                                                        scale.data_layout(),
                                                        scale.lod(),
                                                        scale.offset());
    VLOG(4) << "Builder construction  meta_scale";
    meta_scale = paddle::dialect::IrMetaTensor(&ir_tensor_scale);
  }


  paddle::dialect::IrMetaTensor meta_bias1;
  paddle::dialect::IrTensor ir_tensor_bias1;
  if (bias1_.impl() != nullptr) {
    paddle::dialect::DenseTensorType bias1 = bias1_.type().dyn_cast<paddle::dialect::DenseTensorType>();
    VLOG(4) << "Builder construction  dense_bias1";
    ir_tensor_bias1 = paddle::dialect::IrTensor(paddle::dialect::TransToPhiDataType(bias1.dtype()),
                                                        bias1.dims(),
                                                        bias1.data_layout(),
                                                        bias1.lod(),
                                                        bias1.offset());
    VLOG(4) << "Builder construction  meta_bias1";
    meta_bias1 = paddle::dialect::IrMetaTensor(&ir_tensor_bias1);
  }

  paddle::dialect::IrTensor dense_out;
  paddle::dialect::IrMetaTensor meta_out(&dense_out);
  paddle::dialect::IrTensor dense_mean;
  paddle::dialect::IrMetaTensor meta_mean(&dense_mean);
  paddle::dialect::IrTensor dense_variance;
  paddle::dialect::IrMetaTensor meta_variance(&dense_variance);

  phi::FusedFCElementwiseLayerNormInferMeta(meta_x, meta_w, meta_y, meta_bias0, meta_scale, meta_bias1, x_num_col_dims, activation_type, epsilon, begin_norm_axis, &meta_out, &meta_mean, &meta_variance);

  std::vector<pir::Type> argument_outputs;
  pir::Type out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_out.dtype()), dense_out.dims(), dense_out.layout(), dense_out.lod(), dense_out.offset());
  argument_outputs.push_back(out_dense_tensor_type);

  pir::Type mean_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_mean.dtype()), dense_mean.dims(), dense_mean.layout(), dense_mean.lod(), dense_mean.offset());
  argument_outputs.push_back(mean_dense_tensor_type);

  pir::Type variance_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_variance.dtype()), dense_variance.dims(), dense_variance.layout(), dense_variance.lod(), dense_variance.offset());
  argument_outputs.push_back(variance_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void FusedFcElementwiseLayernormOp::VerifySig() {
  VLOG(4) << "Start Verifying inputs, outputs and attributes for: FusedFcElementwiseLayernormOp.";
  VLOG(4) << "Verifying inputs:";
  {
  auto input_size = num_operands();
  IR_ENFORCE(input_size == 6u,
                    "The size %d of inputs must be equal to 6.", input_size);
  IR_ENFORCE((*this)->operand_source(0).type().isa<paddle::dialect::DenseTensorType>(),
                  "Type validation failed for the 0th input, got %s.", (*this)->operand_source(0).type());
  IR_ENFORCE((*this)->operand_source(1).type().isa<paddle::dialect::DenseTensorType>(),
                  "Type validation failed for the 1th input, got %s.", (*this)->operand_source(1).type());
  IR_ENFORCE((*this)->operand_source(2).type().isa<paddle::dialect::DenseTensorType>(),
                  "Type validation failed for the 2th input, got %s.", (*this)->operand_source(2).type());
  if (auto val = (*this)->operand(3)) {
    IR_ENFORCE(val.type().isa<paddle::dialect::DenseTensorType>(),
                   "Type validation failed for the 3th input, got %s.", (*this)->operand_source(3).type());
  }
  if (auto val = (*this)->operand(4)) {
    IR_ENFORCE(val.type().isa<paddle::dialect::DenseTensorType>(),
                   "Type validation failed for the 4th input, got %s.", (*this)->operand_source(4).type());
  }
  if (auto val = (*this)->operand(5)) {
    IR_ENFORCE(val.type().isa<paddle::dialect::DenseTensorType>(),
                   "Type validation failed for the 5th input, got %s.", (*this)->operand_source(5).type());
  }
  }
  VLOG(4) << "Verifying attributes:";
  {
  auto& attributes = this->attributes();
  IR_ENFORCE(attributes.count("x_num_col_dims")>0,
                 "x_num_col_dims does not exist.");
  IR_ENFORCE(attributes.at("x_num_col_dims").isa<pir::Int32Attribute>(),
                 "Type of attribute: x_num_col_dims is not pir::Int32Attribute.");

  IR_ENFORCE(attributes.count("activation_type")>0,
                 "activation_type does not exist.");
  IR_ENFORCE(attributes.at("activation_type").isa<pir::StrAttribute>(),
                 "Type of attribute: activation_type is not pir::StrAttribute.");

  IR_ENFORCE(attributes.count("epsilon")>0,
                 "epsilon does not exist.");
  IR_ENFORCE(attributes.at("epsilon").isa<pir::FloatAttribute>(),
                 "Type of attribute: epsilon is not pir::FloatAttribute.");

  IR_ENFORCE(attributes.count("begin_norm_axis")>0,
                 "begin_norm_axis does not exist.");
  IR_ENFORCE(attributes.at("begin_norm_axis").isa<pir::Int32Attribute>(),
                 "Type of attribute: begin_norm_axis is not pir::Int32Attribute.");

  }
  VLOG(4) << "Verifying outputs:";
  {
  auto output_size = num_results();
  IR_ENFORCE(output_size == 3u,
                    "The size %d of outputs must be equal to 3.", output_size);
  IR_ENFORCE((*this)->result(0).type().isa<paddle::dialect::DenseTensorType>(),
                 "Type validation failed for the 0th output.");
  if (auto output_1_type = (*this)->result(1).type()) {
    IR_ENFORCE(output_1_type.isa<paddle::dialect::DenseTensorType>(),
                   "Type validation failed for the 1th output.");
  }
  if (auto output_2_type = (*this)->result(2).type()) {
    IR_ENFORCE(output_2_type.isa<paddle::dialect::DenseTensorType>(),
                   "Type validation failed for the 2th output.");
  }
  }
  VLOG(4) << "End Verifying for: FusedFcElementwiseLayernormOp.";
}

void FusedFcElementwiseLayernormOp::InferMeta( phi::InferMetaContext *infer_meta ) {
  auto fn = PD_INFER_META(phi::FusedFCElementwiseLayerNormInferMeta);
  fn(infer_meta);
}

phi::DataType FusedFcElementwiseLayernormOp::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: FusedFcElementwiseLayernormOp";
  


  return expected_kernel_dtype;
}

const char *FusedLinearParamGradAddOp::attributes_name[2] = { "multi_precision", "has_bias" };

OpInfoTuple FusedLinearParamGradAddOp::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("x", "paddle::dialect::DenseTensorType", false, false, false, false), paddle::dialect::OpInputInfo("dout", "paddle::dialect::DenseTensorType", false, false, false, false), paddle::dialect::OpInputInfo("dweight", "paddle::dialect::DenseTensorType", true, false, false, false), paddle::dialect::OpInputInfo("dbias", "paddle::dialect::DenseTensorType", true, false, false, false) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = { paddle::dialect::OpAttributeInfo("multi_precision", "pir::BoolAttribute", ""), paddle::dialect::OpAttributeInfo("has_bias", "pir::BoolAttribute", "") };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("dweight_out", "paddle::dialect::DenseTensorType", false, false), paddle::dialect::OpOutputInfo("dbias_out", "paddle::dialect::DenseTensorType", false, false) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("FusedLinearParamGradAddInferMeta", {"x", "dout", "dweight", "dbias", "multi_precision", "has_bias"}, "fused_linear_param_grad_add", {"x", "dout", "dweight", "dbias", "multi_precision", "has_bias"}, {"dout"}, {}, {}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "fused_linear_param_grad_add");
}

void FusedLinearParamGradAddOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, pir::Value dout_, pir::Value dweight_, pir::Value dbias_, bool multi_precision, bool has_bias) {
  VLOG(4) << "Start build FusedLinearParamGradAddOp";



  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_, dout_, dweight_, dbias_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_multi_precision = pir::BoolAttribute::get(pir::IrContext::Instance(), multi_precision);
  argument.AddAttribute("multi_precision", attr_multi_precision);
  pir::Attribute attr_has_bias = pir::BoolAttribute::get(pir::IrContext::Instance(), has_bias);
  argument.AddAttribute("has_bias", attr_has_bias);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;
  paddle::dialect::DenseTensorType dout = dout_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)dout;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);

  VLOG(4) << "Builder construction  dense_dout";
  paddle::dialect::IrTensor ir_tensor_dout(paddle::dialect::TransToPhiDataType(dout.dtype()),
                                                      dout.dims(),
                                                      dout.data_layout(),
                                                      dout.lod(),
                                                      dout.offset());
  VLOG(4) << "Builder construction  meta_dout";
  paddle::dialect::IrMetaTensor meta_dout(&ir_tensor_dout);

  paddle::dialect::IrMetaTensor meta_dweight;
  paddle::dialect::IrTensor ir_tensor_dweight;
  if (dweight_.impl() != nullptr) {
    paddle::dialect::DenseTensorType dweight = dweight_.type().dyn_cast<paddle::dialect::DenseTensorType>();
    VLOG(4) << "Builder construction  dense_dweight";
    ir_tensor_dweight = paddle::dialect::IrTensor(paddle::dialect::TransToPhiDataType(dweight.dtype()),
                                                        dweight.dims(),
                                                        dweight.data_layout(),
                                                        dweight.lod(),
                                                        dweight.offset());
    VLOG(4) << "Builder construction  meta_dweight";
    meta_dweight = paddle::dialect::IrMetaTensor(&ir_tensor_dweight);
  }


  paddle::dialect::IrMetaTensor meta_dbias;
  paddle::dialect::IrTensor ir_tensor_dbias;
  if (dbias_.impl() != nullptr) {
    paddle::dialect::DenseTensorType dbias = dbias_.type().dyn_cast<paddle::dialect::DenseTensorType>();
    VLOG(4) << "Builder construction  dense_dbias";
    ir_tensor_dbias = paddle::dialect::IrTensor(paddle::dialect::TransToPhiDataType(dbias.dtype()),
                                                        dbias.dims(),
                                                        dbias.data_layout(),
                                                        dbias.lod(),
                                                        dbias.offset());
    VLOG(4) << "Builder construction  meta_dbias";
    meta_dbias = paddle::dialect::IrMetaTensor(&ir_tensor_dbias);
  }

  paddle::dialect::IrTensor dense_dweight_out;
  paddle::dialect::IrMetaTensor meta_dweight_out(&dense_dweight_out);
  paddle::dialect::IrTensor dense_dbias_out;
  paddle::dialect::IrMetaTensor meta_dbias_out(&dense_dbias_out);

  phi::FusedLinearParamGradAddInferMeta(meta_x, meta_dout, meta_dweight, meta_dbias, multi_precision, has_bias, &meta_dweight_out, &meta_dbias_out);

  std::vector<pir::Type> argument_outputs;
  pir::Type dweight_out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_dweight_out.dtype()), dense_dweight_out.dims(), dense_dweight_out.layout(), dense_dweight_out.lod(), dense_dweight_out.offset());
  argument_outputs.push_back(dweight_out_dense_tensor_type);

  pir::Type dbias_out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_dbias_out.dtype()), dense_dbias_out.dims(), dense_dbias_out.layout(), dense_dbias_out.lod(), dense_dbias_out.offset());
  argument_outputs.push_back(dbias_out_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void FusedLinearParamGradAddOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, pir::Value dout_, pir::Value dweight_, pir::Value dbias_, pir::AttributeMap attributes) {
  VLOG(4) << "Start build FusedLinearParamGradAddOp";


  IR_ENFORCE(
      attributes.find("multi_precision") != attributes.end(),
          "'multi_precision' Attribute is expected for FusedLinearParamGradAddOp. ");
  bool multi_precision = attributes.at("multi_precision").dyn_cast<pir::BoolAttribute>().data();

  IR_ENFORCE(
      attributes.find("has_bias") != attributes.end(),
          "'has_bias' Attribute is expected for FusedLinearParamGradAddOp. ");
  bool has_bias = attributes.at("has_bias").dyn_cast<pir::BoolAttribute>().data();


  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_, dout_, dweight_, dbias_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_multi_precision = pir::BoolAttribute::get(pir::IrContext::Instance(), multi_precision);
  argument.AddAttribute("multi_precision", attr_multi_precision);
  pir::Attribute attr_has_bias = pir::BoolAttribute::get(pir::IrContext::Instance(), has_bias);
  argument.AddAttribute("has_bias", attr_has_bias);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;
  paddle::dialect::DenseTensorType dout = dout_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)dout;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);

  VLOG(4) << "Builder construction  dense_dout";
  paddle::dialect::IrTensor ir_tensor_dout(paddle::dialect::TransToPhiDataType(dout.dtype()),
                                                      dout.dims(),
                                                      dout.data_layout(),
                                                      dout.lod(),
                                                      dout.offset());
  VLOG(4) << "Builder construction  meta_dout";
  paddle::dialect::IrMetaTensor meta_dout(&ir_tensor_dout);

  paddle::dialect::IrMetaTensor meta_dweight;
  paddle::dialect::IrTensor ir_tensor_dweight;
  if (dweight_.impl() != nullptr) {
    paddle::dialect::DenseTensorType dweight = dweight_.type().dyn_cast<paddle::dialect::DenseTensorType>();
    VLOG(4) << "Builder construction  dense_dweight";
    ir_tensor_dweight = paddle::dialect::IrTensor(paddle::dialect::TransToPhiDataType(dweight.dtype()),
                                                        dweight.dims(),
                                                        dweight.data_layout(),
                                                        dweight.lod(),
                                                        dweight.offset());
    VLOG(4) << "Builder construction  meta_dweight";
    meta_dweight = paddle::dialect::IrMetaTensor(&ir_tensor_dweight);
  }


  paddle::dialect::IrMetaTensor meta_dbias;
  paddle::dialect::IrTensor ir_tensor_dbias;
  if (dbias_.impl() != nullptr) {
    paddle::dialect::DenseTensorType dbias = dbias_.type().dyn_cast<paddle::dialect::DenseTensorType>();
    VLOG(4) << "Builder construction  dense_dbias";
    ir_tensor_dbias = paddle::dialect::IrTensor(paddle::dialect::TransToPhiDataType(dbias.dtype()),
                                                        dbias.dims(),
                                                        dbias.data_layout(),
                                                        dbias.lod(),
                                                        dbias.offset());
    VLOG(4) << "Builder construction  meta_dbias";
    meta_dbias = paddle::dialect::IrMetaTensor(&ir_tensor_dbias);
  }

  paddle::dialect::IrTensor dense_dweight_out;
  paddle::dialect::IrMetaTensor meta_dweight_out(&dense_dweight_out);
  paddle::dialect::IrTensor dense_dbias_out;
  paddle::dialect::IrMetaTensor meta_dbias_out(&dense_dbias_out);

  phi::FusedLinearParamGradAddInferMeta(meta_x, meta_dout, meta_dweight, meta_dbias, multi_precision, has_bias, &meta_dweight_out, &meta_dbias_out);

  std::vector<pir::Type> argument_outputs;
  pir::Type dweight_out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_dweight_out.dtype()), dense_dweight_out.dims(), dense_dweight_out.layout(), dense_dweight_out.lod(), dense_dweight_out.offset());
  argument_outputs.push_back(dweight_out_dense_tensor_type);

  pir::Type dbias_out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_dbias_out.dtype()), dense_dbias_out.dims(), dense_dbias_out.layout(), dense_dbias_out.lod(), dense_dbias_out.offset());
  argument_outputs.push_back(dbias_out_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void FusedLinearParamGradAddOp::VerifySig() {
  VLOG(4) << "Start Verifying inputs, outputs and attributes for: FusedLinearParamGradAddOp.";
  VLOG(4) << "Verifying inputs:";
  {
  auto input_size = num_operands();
  IR_ENFORCE(input_size == 4u,
                    "The size %d of inputs must be equal to 4.", input_size);
  IR_ENFORCE((*this)->operand_source(0).type().isa<paddle::dialect::DenseTensorType>(),
                  "Type validation failed for the 0th input, got %s.", (*this)->operand_source(0).type());
  IR_ENFORCE((*this)->operand_source(1).type().isa<paddle::dialect::DenseTensorType>(),
                  "Type validation failed for the 1th input, got %s.", (*this)->operand_source(1).type());
  if (auto val = (*this)->operand(2)) {
    IR_ENFORCE(val.type().isa<paddle::dialect::DenseTensorType>(),
                   "Type validation failed for the 2th input, got %s.", (*this)->operand_source(2).type());
  }
  if (auto val = (*this)->operand(3)) {
    IR_ENFORCE(val.type().isa<paddle::dialect::DenseTensorType>(),
                   "Type validation failed for the 3th input, got %s.", (*this)->operand_source(3).type());
  }
  }
  VLOG(4) << "Verifying attributes:";
  {
  auto& attributes = this->attributes();
  IR_ENFORCE(attributes.count("multi_precision")>0,
                 "multi_precision does not exist.");
  IR_ENFORCE(attributes.at("multi_precision").isa<pir::BoolAttribute>(),
                 "Type of attribute: multi_precision is not pir::BoolAttribute.");

  IR_ENFORCE(attributes.count("has_bias")>0,
                 "has_bias does not exist.");
  IR_ENFORCE(attributes.at("has_bias").isa<pir::BoolAttribute>(),
                 "Type of attribute: has_bias is not pir::BoolAttribute.");

  }
  VLOG(4) << "Verifying outputs:";
  {
  auto output_size = num_results();
  IR_ENFORCE(output_size == 2u,
                    "The size %d of outputs must be equal to 2.", output_size);
  IR_ENFORCE((*this)->result(0).type().isa<paddle::dialect::DenseTensorType>(),
                 "Type validation failed for the 0th output.");
  IR_ENFORCE((*this)->result(1).type().isa<paddle::dialect::DenseTensorType>(),
                 "Type validation failed for the 1th output.");
  }
  VLOG(4) << "End Verifying for: FusedLinearParamGradAddOp.";
}

void FusedLinearParamGradAddOp::InferMeta( phi::InferMetaContext *infer_meta ) {
  auto fn = PD_INFER_META(phi::FusedLinearParamGradAddInferMeta);
  fn(infer_meta);
}

phi::DataType FusedLinearParamGradAddOp::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: FusedLinearParamGradAddOp";
  


  return expected_kernel_dtype;
}

const char *FusedMultiTransformerInt8XpuOp::attributes_name[10] = { "pre_layer_norm", "rotary_emb_dims", "epsilon", "dropout_rate", "is_test", "dropout_implementation", "act_method", "trans_qkvw", "ring_id", "gather_axis" };

OpInfoTuple FusedMultiTransformerInt8XpuOp::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("x", "paddle::dialect::DenseTensorType", false, false, false, false), paddle::dialect::OpInputInfo("ln_scale", "pir::VectorType<paddle::dialect::DenseTensorType>", false, false, false, false), paddle::dialect::OpInputInfo("ln_bias", "pir::VectorType<paddle::dialect::DenseTensorType>", false, false, false, false), paddle::dialect::OpInputInfo("qkv_in_max", "pir::VectorType<paddle::dialect::DenseTensorType>", false, false, false, false), paddle::dialect::OpInputInfo("qkvw", "pir::VectorType<paddle::dialect::DenseTensorType>", false, false, false, false), paddle::dialect::OpInputInfo("qkv_bias", "pir::VectorType<paddle::dialect::DenseTensorType>", false, false, false, false), paddle::dialect::OpInputInfo("qkv_scales", "pir::VectorType<paddle::dialect::DenseTensorType>", false, false, false, false), paddle::dialect::OpInputInfo("out_linear_in_max", "pir::VectorType<paddle::dialect::DenseTensorType>", false, false, false, false), paddle::dialect::OpInputInfo("out_linear_w", "pir::VectorType<paddle::dialect::DenseTensorType>", false, false, false, false), paddle::dialect::OpInputInfo("out_linear_bias", "pir::VectorType<paddle::dialect::DenseTensorType>", false, false, false, false), paddle::dialect::OpInputInfo("out_linear_scales", "pir::VectorType<paddle::dialect::DenseTensorType>", false, false, false, false), paddle::dialect::OpInputInfo("ffn_ln_scale", "pir::VectorType<paddle::dialect::DenseTensorType>", false, false, false, false), paddle::dialect::OpInputInfo("ffn_ln_bias", "pir::VectorType<paddle::dialect::DenseTensorType>", false, false, false, false), paddle::dialect::OpInputInfo("ffn1_in_max", "pir::VectorType<paddle::dialect::DenseTensorType>", false, false, false, false), paddle::dialect::OpInputInfo("ffn1_weight", "pir::VectorType<paddle::dialect::DenseTensorType>", false, false, false, false), paddle::dialect::OpInputInfo("ffn1_bias", "pir::VectorType<paddle::dialect::DenseTensorType>", false, false, false, false), paddle::dialect::OpInputInfo("ffn1_scales", "pir::VectorType<paddle::dialect::DenseTensorType>", false, false, false, false), paddle::dialect::OpInputInfo("ffn2_in_max", "pir::VectorType<paddle::dialect::DenseTensorType>", false, false, false, false), paddle::dialect::OpInputInfo("ffn2_weight", "pir::VectorType<paddle::dialect::DenseTensorType>", false, false, false, false), paddle::dialect::OpInputInfo("ffn2_bias", "pir::VectorType<paddle::dialect::DenseTensorType>", false, false, false, false), paddle::dialect::OpInputInfo("ffn2_scales", "pir::VectorType<paddle::dialect::DenseTensorType>", false, false, false, false), paddle::dialect::OpInputInfo("cache_kv", "pir::VectorType<paddle::dialect::DenseTensorType>", true, false, false, false), paddle::dialect::OpInputInfo("pre_caches", "pir::VectorType<paddle::dialect::DenseTensorType>", true, false, false, false), paddle::dialect::OpInputInfo("rotary_pos_emb", "paddle::dialect::DenseTensorType", true, false, false, false), paddle::dialect::OpInputInfo("time_step", "paddle::dialect::DenseTensorType", true, false, false, false), paddle::dialect::OpInputInfo("seq_lengths", "paddle::dialect::DenseTensorType", true, false, false, false), paddle::dialect::OpInputInfo("src_mask", "paddle::dialect::DenseTensorType", true, false, false, false), paddle::dialect::OpInputInfo("gather_index", "paddle::dialect::DenseTensorType", true, false, false, false), paddle::dialect::OpInputInfo("max_buffer", "paddle::dialect::DenseTensorType", false, false, false, false) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = { paddle::dialect::OpAttributeInfo("pre_layer_norm", "pir::BoolAttribute", ""), paddle::dialect::OpAttributeInfo("rotary_emb_dims", "pir::Int32Attribute", ""), paddle::dialect::OpAttributeInfo("epsilon", "pir::FloatAttribute", ""), paddle::dialect::OpAttributeInfo("dropout_rate", "pir::FloatAttribute", ""), paddle::dialect::OpAttributeInfo("is_test", "pir::BoolAttribute", ""), paddle::dialect::OpAttributeInfo("dropout_implementation", "pir::StrAttribute", ""), paddle::dialect::OpAttributeInfo("act_method", "pir::StrAttribute", ""), paddle::dialect::OpAttributeInfo("trans_qkvw", "pir::BoolAttribute", ""), paddle::dialect::OpAttributeInfo("ring_id", "pir::Int32Attribute", ""), paddle::dialect::OpAttributeInfo("gather_axis", "pir::Int32Attribute", "") };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("out", "paddle::dialect::DenseTensorType", false, false), paddle::dialect::OpOutputInfo("cache_kv_out", "pir::VectorType<paddle::dialect::DenseTensorType>", false, false) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("FusedMultiTransformerInt8XpuInferMeta", {"x", "ln_scale", "ln_bias", "qkv_in_max", "qkvw", "qkv_bias", "qkv_scales", "out_linear_in_max", "out_linear_w", "out_linear_bias", "out_linear_scales", "ffn_ln_scale", "ffn_ln_bias", "ffn1_in_max", "ffn1_weight", "ffn1_bias", "ffn1_scales", "ffn2_in_max", "ffn2_weight", "ffn2_bias", "ffn2_scales", "cache_kv", "pre_caches", "rotary_pos_emb", "time_step", "seq_lengths", "src_mask", "gather_index", "max_buffer", "pre_layer_norm", "rotary_emb_dims", "epsilon", "dropout_rate", "is_test", "dropout_implementation", "act_method", "trans_qkvw", "ring_id", "gather_axis"}, "fused_multi_transformer_int8_xpu", {"x", "ln_scale", "ln_bias", "qkv_in_max", "qkvw", "qkv_bias", "qkv_scales", "out_linear_in_max", "out_linear_w", "out_linear_bias", "out_linear_scales", "ffn_ln_scale", "ffn_ln_bias", "ffn1_in_max", "ffn1_weight", "ffn1_bias", "ffn1_scales", "ffn2_in_max", "ffn2_weight", "ffn2_bias", "ffn2_scales", "cache_kv", "pre_caches", "rotary_pos_emb", "time_step", "seq_lengths", "src_mask", "gather_index", "max_buffer", "pre_layer_norm", "rotary_emb_dims", "epsilon", "dropout_rate", "is_test", "dropout_implementation", "act_method", "trans_qkvw", "ring_id", "gather_axis"}, {"x"}, {}, {}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "fused_multi_transformer_int8_xpu");
}

void FusedMultiTransformerInt8XpuOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, pir::Value ln_scale_, pir::Value ln_bias_, pir::Value qkv_in_max_, pir::Value qkvw_, pir::Value qkv_bias_, pir::Value qkv_scales_, pir::Value out_linear_in_max_, pir::Value out_linear_w_, pir::Value out_linear_bias_, pir::Value out_linear_scales_, pir::Value ffn_ln_scale_, pir::Value ffn_ln_bias_, pir::Value ffn1_in_max_, pir::Value ffn1_weight_, pir::Value ffn1_bias_, pir::Value ffn1_scales_, pir::Value ffn2_in_max_, pir::Value ffn2_weight_, pir::Value ffn2_bias_, pir::Value ffn2_scales_, pir::Value cache_kv_, pir::Value pre_caches_, pir::Value rotary_pos_emb_, pir::Value time_step_, pir::Value seq_lengths_, pir::Value src_mask_, pir::Value gather_index_, pir::Value max_buffer_, bool pre_layer_norm, int rotary_emb_dims, float epsilon, float dropout_rate, bool is_test, const std::string& dropout_implementation, const std::string& act_method, bool trans_qkvw, int ring_id, int gather_axis) {
  VLOG(4) << "Start build FusedMultiTransformerInt8XpuOp";



  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_, ln_scale_, ln_bias_, qkv_in_max_, qkvw_, qkv_bias_, qkv_scales_, out_linear_in_max_, out_linear_w_, out_linear_bias_, out_linear_scales_, ffn_ln_scale_, ffn_ln_bias_, ffn1_in_max_, ffn1_weight_, ffn1_bias_, ffn1_scales_, ffn2_in_max_, ffn2_weight_, ffn2_bias_, ffn2_scales_, cache_kv_, pre_caches_, rotary_pos_emb_, time_step_, seq_lengths_, src_mask_, gather_index_, max_buffer_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_pre_layer_norm = pir::BoolAttribute::get(pir::IrContext::Instance(), pre_layer_norm);
  argument.AddAttribute("pre_layer_norm", attr_pre_layer_norm);
  pir::Attribute attr_rotary_emb_dims = pir::Int32Attribute::get(pir::IrContext::Instance(), rotary_emb_dims);
  argument.AddAttribute("rotary_emb_dims", attr_rotary_emb_dims);
  pir::Attribute attr_epsilon = pir::FloatAttribute::get(pir::IrContext::Instance(), epsilon);
  argument.AddAttribute("epsilon", attr_epsilon);
  pir::Attribute attr_dropout_rate = pir::FloatAttribute::get(pir::IrContext::Instance(), dropout_rate);
  argument.AddAttribute("dropout_rate", attr_dropout_rate);
  pir::Attribute attr_is_test = pir::BoolAttribute::get(pir::IrContext::Instance(), is_test);
  argument.AddAttribute("is_test", attr_is_test);
  pir::Attribute attr_dropout_implementation = pir::StrAttribute::get(pir::IrContext::Instance(), dropout_implementation);
  argument.AddAttribute("dropout_implementation", attr_dropout_implementation);
  pir::Attribute attr_act_method = pir::StrAttribute::get(pir::IrContext::Instance(), act_method);
  argument.AddAttribute("act_method", attr_act_method);
  pir::Attribute attr_trans_qkvw = pir::BoolAttribute::get(pir::IrContext::Instance(), trans_qkvw);
  argument.AddAttribute("trans_qkvw", attr_trans_qkvw);
  pir::Attribute attr_ring_id = pir::Int32Attribute::get(pir::IrContext::Instance(), ring_id);
  argument.AddAttribute("ring_id", attr_ring_id);
  pir::Attribute attr_gather_axis = pir::Int32Attribute::get(pir::IrContext::Instance(), gather_axis);
  argument.AddAttribute("gather_axis", attr_gather_axis);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;
  pir::VectorType ln_scale = ln_scale_.type().dyn_cast<pir::VectorType>(); (void)ln_scale;
  pir::VectorType ln_bias = ln_bias_.type().dyn_cast<pir::VectorType>(); (void)ln_bias;
  pir::VectorType qkv_in_max = qkv_in_max_.type().dyn_cast<pir::VectorType>(); (void)qkv_in_max;
  pir::VectorType qkvw = qkvw_.type().dyn_cast<pir::VectorType>(); (void)qkvw;
  pir::VectorType qkv_bias = qkv_bias_.type().dyn_cast<pir::VectorType>(); (void)qkv_bias;
  pir::VectorType qkv_scales = qkv_scales_.type().dyn_cast<pir::VectorType>(); (void)qkv_scales;
  pir::VectorType out_linear_in_max = out_linear_in_max_.type().dyn_cast<pir::VectorType>(); (void)out_linear_in_max;
  pir::VectorType out_linear_w = out_linear_w_.type().dyn_cast<pir::VectorType>(); (void)out_linear_w;
  pir::VectorType out_linear_bias = out_linear_bias_.type().dyn_cast<pir::VectorType>(); (void)out_linear_bias;
  pir::VectorType out_linear_scales = out_linear_scales_.type().dyn_cast<pir::VectorType>(); (void)out_linear_scales;
  pir::VectorType ffn_ln_scale = ffn_ln_scale_.type().dyn_cast<pir::VectorType>(); (void)ffn_ln_scale;
  pir::VectorType ffn_ln_bias = ffn_ln_bias_.type().dyn_cast<pir::VectorType>(); (void)ffn_ln_bias;
  pir::VectorType ffn1_in_max = ffn1_in_max_.type().dyn_cast<pir::VectorType>(); (void)ffn1_in_max;
  pir::VectorType ffn1_weight = ffn1_weight_.type().dyn_cast<pir::VectorType>(); (void)ffn1_weight;
  pir::VectorType ffn1_bias = ffn1_bias_.type().dyn_cast<pir::VectorType>(); (void)ffn1_bias;
  pir::VectorType ffn1_scales = ffn1_scales_.type().dyn_cast<pir::VectorType>(); (void)ffn1_scales;
  pir::VectorType ffn2_in_max = ffn2_in_max_.type().dyn_cast<pir::VectorType>(); (void)ffn2_in_max;
  pir::VectorType ffn2_weight = ffn2_weight_.type().dyn_cast<pir::VectorType>(); (void)ffn2_weight;
  pir::VectorType ffn2_bias = ffn2_bias_.type().dyn_cast<pir::VectorType>(); (void)ffn2_bias;
  pir::VectorType ffn2_scales = ffn2_scales_.type().dyn_cast<pir::VectorType>(); (void)ffn2_scales;
  paddle::dialect::DenseTensorType max_buffer = max_buffer_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)max_buffer;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);
  std::vector<paddle::dialect::IrTensor> vec_ir_tensor_ln_scale;
  for (size_t i=0; i < static_cast<size_t>(ln_scale.size()); i++) {
    vec_ir_tensor_ln_scale.push_back(paddle::dialect::IrTensor(paddle::dialect::TransToPhiDataType(ln_scale[i].dyn_cast<paddle::dialect::DenseTensorType>().dtype()),
                                                                     ln_scale[i].dyn_cast<paddle::dialect::DenseTensorType>().dims(),
                                                                     ln_scale[i].dyn_cast<paddle::dialect::DenseTensorType>().data_layout(),
                                                                     ln_scale[i].dyn_cast<paddle::dialect::DenseTensorType>().lod(),
                                                                     ln_scale[i].dyn_cast<paddle::dialect::DenseTensorType>().offset()));
  }
  std::vector<paddle::dialect::IrMetaTensor> vec_meta_ln_scale;
  for (size_t i=0; i < vec_ir_tensor_ln_scale.size(); i++) {
    vec_meta_ln_scale.push_back(paddle::dialect::IrMetaTensor(&vec_ir_tensor_ln_scale[i]));
  }

  std::vector<const phi::MetaTensor*> meta_ln_scale;
  for (size_t i=0; i < static_cast<size_t>(vec_meta_ln_scale.size()); i++) {
    meta_ln_scale.push_back(&vec_meta_ln_scale[i]);
  }
   std::vector<paddle::dialect::IrTensor> vec_ir_tensor_ln_bias;
  for (size_t i=0; i < static_cast<size_t>(ln_bias.size()); i++) {
    vec_ir_tensor_ln_bias.push_back(paddle::dialect::IrTensor(paddle::dialect::TransToPhiDataType(ln_bias[i].dyn_cast<paddle::dialect::DenseTensorType>().dtype()),
                                                                     ln_bias[i].dyn_cast<paddle::dialect::DenseTensorType>().dims(),
                                                                     ln_bias[i].dyn_cast<paddle::dialect::DenseTensorType>().data_layout(),
                                                                     ln_bias[i].dyn_cast<paddle::dialect::DenseTensorType>().lod(),
                                                                     ln_bias[i].dyn_cast<paddle::dialect::DenseTensorType>().offset()));
  }
  std::vector<paddle::dialect::IrMetaTensor> vec_meta_ln_bias;
  for (size_t i=0; i < vec_ir_tensor_ln_bias.size(); i++) {
    vec_meta_ln_bias.push_back(paddle::dialect::IrMetaTensor(&vec_ir_tensor_ln_bias[i]));
  }

  std::vector<const phi::MetaTensor*> meta_ln_bias;
  for (size_t i=0; i < static_cast<size_t>(vec_meta_ln_bias.size()); i++) {
    meta_ln_bias.push_back(&vec_meta_ln_bias[i]);
  }
   std::vector<paddle::dialect::IrTensor> vec_ir_tensor_qkv_in_max;
  for (size_t i=0; i < static_cast<size_t>(qkv_in_max.size()); i++) {
    vec_ir_tensor_qkv_in_max.push_back(paddle::dialect::IrTensor(paddle::dialect::TransToPhiDataType(qkv_in_max[i].dyn_cast<paddle::dialect::DenseTensorType>().dtype()),
                                                                     qkv_in_max[i].dyn_cast<paddle::dialect::DenseTensorType>().dims(),
                                                                     qkv_in_max[i].dyn_cast<paddle::dialect::DenseTensorType>().data_layout(),
                                                                     qkv_in_max[i].dyn_cast<paddle::dialect::DenseTensorType>().lod(),
                                                                     qkv_in_max[i].dyn_cast<paddle::dialect::DenseTensorType>().offset()));
  }
  std::vector<paddle::dialect::IrMetaTensor> vec_meta_qkv_in_max;
  for (size_t i=0; i < vec_ir_tensor_qkv_in_max.size(); i++) {
    vec_meta_qkv_in_max.push_back(paddle::dialect::IrMetaTensor(&vec_ir_tensor_qkv_in_max[i]));
  }

  std::vector<const phi::MetaTensor*> meta_qkv_in_max;
  for (size_t i=0; i < static_cast<size_t>(vec_meta_qkv_in_max.size()); i++) {
    meta_qkv_in_max.push_back(&vec_meta_qkv_in_max[i]);
  }
   std::vector<paddle::dialect::IrTensor> vec_ir_tensor_qkvw;
  for (size_t i=0; i < static_cast<size_t>(qkvw.size()); i++) {
    vec_ir_tensor_qkvw.push_back(paddle::dialect::IrTensor(paddle::dialect::TransToPhiDataType(qkvw[i].dyn_cast<paddle::dialect::DenseTensorType>().dtype()),
                                                                     qkvw[i].dyn_cast<paddle::dialect::DenseTensorType>().dims(),
                                                                     qkvw[i].dyn_cast<paddle::dialect::DenseTensorType>().data_layout(),
                                                                     qkvw[i].dyn_cast<paddle::dialect::DenseTensorType>().lod(),
                                                                     qkvw[i].dyn_cast<paddle::dialect::DenseTensorType>().offset()));
  }
  std::vector<paddle::dialect::IrMetaTensor> vec_meta_qkvw;
  for (size_t i=0; i < vec_ir_tensor_qkvw.size(); i++) {
    vec_meta_qkvw.push_back(paddle::dialect::IrMetaTensor(&vec_ir_tensor_qkvw[i]));
  }

  std::vector<const phi::MetaTensor*> meta_qkvw;
  for (size_t i=0; i < static_cast<size_t>(vec_meta_qkvw.size()); i++) {
    meta_qkvw.push_back(&vec_meta_qkvw[i]);
  }
   std::vector<paddle::dialect::IrTensor> vec_ir_tensor_qkv_bias;
  for (size_t i=0; i < static_cast<size_t>(qkv_bias.size()); i++) {
    vec_ir_tensor_qkv_bias.push_back(paddle::dialect::IrTensor(paddle::dialect::TransToPhiDataType(qkv_bias[i].dyn_cast<paddle::dialect::DenseTensorType>().dtype()),
                                                                     qkv_bias[i].dyn_cast<paddle::dialect::DenseTensorType>().dims(),
                                                                     qkv_bias[i].dyn_cast<paddle::dialect::DenseTensorType>().data_layout(),
                                                                     qkv_bias[i].dyn_cast<paddle::dialect::DenseTensorType>().lod(),
                                                                     qkv_bias[i].dyn_cast<paddle::dialect::DenseTensorType>().offset()));
  }
  std::vector<paddle::dialect::IrMetaTensor> vec_meta_qkv_bias;
  for (size_t i=0; i < vec_ir_tensor_qkv_bias.size(); i++) {
    vec_meta_qkv_bias.push_back(paddle::dialect::IrMetaTensor(&vec_ir_tensor_qkv_bias[i]));
  }

  std::vector<const phi::MetaTensor*> meta_qkv_bias;
  for (size_t i=0; i < static_cast<size_t>(vec_meta_qkv_bias.size()); i++) {
    meta_qkv_bias.push_back(&vec_meta_qkv_bias[i]);
  }
   std::vector<paddle::dialect::IrTensor> vec_ir_tensor_qkv_scales;
  for (size_t i=0; i < static_cast<size_t>(qkv_scales.size()); i++) {
    vec_ir_tensor_qkv_scales.push_back(paddle::dialect::IrTensor(paddle::dialect::TransToPhiDataType(qkv_scales[i].dyn_cast<paddle::dialect::DenseTensorType>().dtype()),
                                                                     qkv_scales[i].dyn_cast<paddle::dialect::DenseTensorType>().dims(),
                                                                     qkv_scales[i].dyn_cast<paddle::dialect::DenseTensorType>().data_layout(),
                                                                     qkv_scales[i].dyn_cast<paddle::dialect::DenseTensorType>().lod(),
                                                                     qkv_scales[i].dyn_cast<paddle::dialect::DenseTensorType>().offset()));
  }
  std::vector<paddle::dialect::IrMetaTensor> vec_meta_qkv_scales;
  for (size_t i=0; i < vec_ir_tensor_qkv_scales.size(); i++) {
    vec_meta_qkv_scales.push_back(paddle::dialect::IrMetaTensor(&vec_ir_tensor_qkv_scales[i]));
  }

  std::vector<const phi::MetaTensor*> meta_qkv_scales;
  for (size_t i=0; i < static_cast<size_t>(vec_meta_qkv_scales.size()); i++) {
    meta_qkv_scales.push_back(&vec_meta_qkv_scales[i]);
  }
   std::vector<paddle::dialect::IrTensor> vec_ir_tensor_out_linear_in_max;
  for (size_t i=0; i < static_cast<size_t>(out_linear_in_max.size()); i++) {
    vec_ir_tensor_out_linear_in_max.push_back(paddle::dialect::IrTensor(paddle::dialect::TransToPhiDataType(out_linear_in_max[i].dyn_cast<paddle::dialect::DenseTensorType>().dtype()),
                                                                     out_linear_in_max[i].dyn_cast<paddle::dialect::DenseTensorType>().dims(),
                                                                     out_linear_in_max[i].dyn_cast<paddle::dialect::DenseTensorType>().data_layout(),
                                                                     out_linear_in_max[i].dyn_cast<paddle::dialect::DenseTensorType>().lod(),
                                                                     out_linear_in_max[i].dyn_cast<paddle::dialect::DenseTensorType>().offset()));
  }
  std::vector<paddle::dialect::IrMetaTensor> vec_meta_out_linear_in_max;
  for (size_t i=0; i < vec_ir_tensor_out_linear_in_max.size(); i++) {
    vec_meta_out_linear_in_max.push_back(paddle::dialect::IrMetaTensor(&vec_ir_tensor_out_linear_in_max[i]));
  }

  std::vector<const phi::MetaTensor*> meta_out_linear_in_max;
  for (size_t i=0; i < static_cast<size_t>(vec_meta_out_linear_in_max.size()); i++) {
    meta_out_linear_in_max.push_back(&vec_meta_out_linear_in_max[i]);
  }
   std::vector<paddle::dialect::IrTensor> vec_ir_tensor_out_linear_w;
  for (size_t i=0; i < static_cast<size_t>(out_linear_w.size()); i++) {
    vec_ir_tensor_out_linear_w.push_back(paddle::dialect::IrTensor(paddle::dialect::TransToPhiDataType(out_linear_w[i].dyn_cast<paddle::dialect::DenseTensorType>().dtype()),
                                                                     out_linear_w[i].dyn_cast<paddle::dialect::DenseTensorType>().dims(),
                                                                     out_linear_w[i].dyn_cast<paddle::dialect::DenseTensorType>().data_layout(),
                                                                     out_linear_w[i].dyn_cast<paddle::dialect::DenseTensorType>().lod(),
                                                                     out_linear_w[i].dyn_cast<paddle::dialect::DenseTensorType>().offset()));
  }
  std::vector<paddle::dialect::IrMetaTensor> vec_meta_out_linear_w;
  for (size_t i=0; i < vec_ir_tensor_out_linear_w.size(); i++) {
    vec_meta_out_linear_w.push_back(paddle::dialect::IrMetaTensor(&vec_ir_tensor_out_linear_w[i]));
  }

  std::vector<const phi::MetaTensor*> meta_out_linear_w;
  for (size_t i=0; i < static_cast<size_t>(vec_meta_out_linear_w.size()); i++) {
    meta_out_linear_w.push_back(&vec_meta_out_linear_w[i]);
  }
   std::vector<paddle::dialect::IrTensor> vec_ir_tensor_out_linear_bias;
  for (size_t i=0; i < static_cast<size_t>(out_linear_bias.size()); i++) {
    vec_ir_tensor_out_linear_bias.push_back(paddle::dialect::IrTensor(paddle::dialect::TransToPhiDataType(out_linear_bias[i].dyn_cast<paddle::dialect::DenseTensorType>().dtype()),
                                                                     out_linear_bias[i].dyn_cast<paddle::dialect::DenseTensorType>().dims(),
                                                                     out_linear_bias[i].dyn_cast<paddle::dialect::DenseTensorType>().data_layout(),
                                                                     out_linear_bias[i].dyn_cast<paddle::dialect::DenseTensorType>().lod(),
                                                                     out_linear_bias[i].dyn_cast<paddle::dialect::DenseTensorType>().offset()));
  }
  std::vector<paddle::dialect::IrMetaTensor> vec_meta_out_linear_bias;
  for (size_t i=0; i < vec_ir_tensor_out_linear_bias.size(); i++) {
    vec_meta_out_linear_bias.push_back(paddle::dialect::IrMetaTensor(&vec_ir_tensor_out_linear_bias[i]));
  }

  std::vector<const phi::MetaTensor*> meta_out_linear_bias;
  for (size_t i=0; i < static_cast<size_t>(vec_meta_out_linear_bias.size()); i++) {
    meta_out_linear_bias.push_back(&vec_meta_out_linear_bias[i]);
  }
   std::vector<paddle::dialect::IrTensor> vec_ir_tensor_out_linear_scales;
  for (size_t i=0; i < static_cast<size_t>(out_linear_scales.size()); i++) {
    vec_ir_tensor_out_linear_scales.push_back(paddle::dialect::IrTensor(paddle::dialect::TransToPhiDataType(out_linear_scales[i].dyn_cast<paddle::dialect::DenseTensorType>().dtype()),
                                                                     out_linear_scales[i].dyn_cast<paddle::dialect::DenseTensorType>().dims(),
                                                                     out_linear_scales[i].dyn_cast<paddle::dialect::DenseTensorType>().data_layout(),
                                                                     out_linear_scales[i].dyn_cast<paddle::dialect::DenseTensorType>().lod(),
                                                                     out_linear_scales[i].dyn_cast<paddle::dialect::DenseTensorType>().offset()));
  }
  std::vector<paddle::dialect::IrMetaTensor> vec_meta_out_linear_scales;
  for (size_t i=0; i < vec_ir_tensor_out_linear_scales.size(); i++) {
    vec_meta_out_linear_scales.push_back(paddle::dialect::IrMetaTensor(&vec_ir_tensor_out_linear_scales[i]));
  }

  std::vector<const phi::MetaTensor*> meta_out_linear_scales;
  for (size_t i=0; i < static_cast<size_t>(vec_meta_out_linear_scales.size()); i++) {
    meta_out_linear_scales.push_back(&vec_meta_out_linear_scales[i]);
  }
   std::vector<paddle::dialect::IrTensor> vec_ir_tensor_ffn_ln_scale;
  for (size_t i=0; i < static_cast<size_t>(ffn_ln_scale.size()); i++) {
    vec_ir_tensor_ffn_ln_scale.push_back(paddle::dialect::IrTensor(paddle::dialect::TransToPhiDataType(ffn_ln_scale[i].dyn_cast<paddle::dialect::DenseTensorType>().dtype()),
                                                                     ffn_ln_scale[i].dyn_cast<paddle::dialect::DenseTensorType>().dims(),
                                                                     ffn_ln_scale[i].dyn_cast<paddle::dialect::DenseTensorType>().data_layout(),
                                                                     ffn_ln_scale[i].dyn_cast<paddle::dialect::DenseTensorType>().lod(),
                                                                     ffn_ln_scale[i].dyn_cast<paddle::dialect::DenseTensorType>().offset()));
  }
  std::vector<paddle::dialect::IrMetaTensor> vec_meta_ffn_ln_scale;
  for (size_t i=0; i < vec_ir_tensor_ffn_ln_scale.size(); i++) {
    vec_meta_ffn_ln_scale.push_back(paddle::dialect::IrMetaTensor(&vec_ir_tensor_ffn_ln_scale[i]));
  }

  std::vector<const phi::MetaTensor*> meta_ffn_ln_scale;
  for (size_t i=0; i < static_cast<size_t>(vec_meta_ffn_ln_scale.size()); i++) {
    meta_ffn_ln_scale.push_back(&vec_meta_ffn_ln_scale[i]);
  }
   std::vector<paddle::dialect::IrTensor> vec_ir_tensor_ffn_ln_bias;
  for (size_t i=0; i < static_cast<size_t>(ffn_ln_bias.size()); i++) {
    vec_ir_tensor_ffn_ln_bias.push_back(paddle::dialect::IrTensor(paddle::dialect::TransToPhiDataType(ffn_ln_bias[i].dyn_cast<paddle::dialect::DenseTensorType>().dtype()),
                                                                     ffn_ln_bias[i].dyn_cast<paddle::dialect::DenseTensorType>().dims(),
                                                                     ffn_ln_bias[i].dyn_cast<paddle::dialect::DenseTensorType>().data_layout(),
                                                                     ffn_ln_bias[i].dyn_cast<paddle::dialect::DenseTensorType>().lod(),
                                                                     ffn_ln_bias[i].dyn_cast<paddle::dialect::DenseTensorType>().offset()));
  }
  std::vector<paddle::dialect::IrMetaTensor> vec_meta_ffn_ln_bias;
  for (size_t i=0; i < vec_ir_tensor_ffn_ln_bias.size(); i++) {
    vec_meta_ffn_ln_bias.push_back(paddle::dialect::IrMetaTensor(&vec_ir_tensor_ffn_ln_bias[i]));
  }

  std::vector<const phi::MetaTensor*> meta_ffn_ln_bias;
  for (size_t i=0; i < static_cast<size_t>(vec_meta_ffn_ln_bias.size()); i++) {
    meta_ffn_ln_bias.push_back(&vec_meta_ffn_ln_bias[i]);
  }
   std::vector<paddle::dialect::IrTensor> vec_ir_tensor_ffn1_in_max;
  for (size_t i=0; i < static_cast<size_t>(ffn1_in_max.size()); i++) {
    vec_ir_tensor_ffn1_in_max.push_back(paddle::dialect::IrTensor(paddle::dialect::TransToPhiDataType(ffn1_in_max[i].dyn_cast<paddle::dialect::DenseTensorType>().dtype()),
                                                                     ffn1_in_max[i].dyn_cast<paddle::dialect::DenseTensorType>().dims(),
                                                                     ffn1_in_max[i].dyn_cast<paddle::dialect::DenseTensorType>().data_layout(),
                                                                     ffn1_in_max[i].dyn_cast<paddle::dialect::DenseTensorType>().lod(),
                                                                     ffn1_in_max[i].dyn_cast<paddle::dialect::DenseTensorType>().offset()));
  }
  std::vector<paddle::dialect::IrMetaTensor> vec_meta_ffn1_in_max;
  for (size_t i=0; i < vec_ir_tensor_ffn1_in_max.size(); i++) {
    vec_meta_ffn1_in_max.push_back(paddle::dialect::IrMetaTensor(&vec_ir_tensor_ffn1_in_max[i]));
  }

  std::vector<const phi::MetaTensor*> meta_ffn1_in_max;
  for (size_t i=0; i < static_cast<size_t>(vec_meta_ffn1_in_max.size()); i++) {
    meta_ffn1_in_max.push_back(&vec_meta_ffn1_in_max[i]);
  }
   std::vector<paddle::dialect::IrTensor> vec_ir_tensor_ffn1_weight;
  for (size_t i=0; i < static_cast<size_t>(ffn1_weight.size()); i++) {
    vec_ir_tensor_ffn1_weight.push_back(paddle::dialect::IrTensor(paddle::dialect::TransToPhiDataType(ffn1_weight[i].dyn_cast<paddle::dialect::DenseTensorType>().dtype()),
                                                                     ffn1_weight[i].dyn_cast<paddle::dialect::DenseTensorType>().dims(),
                                                                     ffn1_weight[i].dyn_cast<paddle::dialect::DenseTensorType>().data_layout(),
                                                                     ffn1_weight[i].dyn_cast<paddle::dialect::DenseTensorType>().lod(),
                                                                     ffn1_weight[i].dyn_cast<paddle::dialect::DenseTensorType>().offset()));
  }
  std::vector<paddle::dialect::IrMetaTensor> vec_meta_ffn1_weight;
  for (size_t i=0; i < vec_ir_tensor_ffn1_weight.size(); i++) {
    vec_meta_ffn1_weight.push_back(paddle::dialect::IrMetaTensor(&vec_ir_tensor_ffn1_weight[i]));
  }

  std::vector<const phi::MetaTensor*> meta_ffn1_weight;
  for (size_t i=0; i < static_cast<size_t>(vec_meta_ffn1_weight.size()); i++) {
    meta_ffn1_weight.push_back(&vec_meta_ffn1_weight[i]);
  }
   std::vector<paddle::dialect::IrTensor> vec_ir_tensor_ffn1_bias;
  for (size_t i=0; i < static_cast<size_t>(ffn1_bias.size()); i++) {
    vec_ir_tensor_ffn1_bias.push_back(paddle::dialect::IrTensor(paddle::dialect::TransToPhiDataType(ffn1_bias[i].dyn_cast<paddle::dialect::DenseTensorType>().dtype()),
                                                                     ffn1_bias[i].dyn_cast<paddle::dialect::DenseTensorType>().dims(),
                                                                     ffn1_bias[i].dyn_cast<paddle::dialect::DenseTensorType>().data_layout(),
                                                                     ffn1_bias[i].dyn_cast<paddle::dialect::DenseTensorType>().lod(),
                                                                     ffn1_bias[i].dyn_cast<paddle::dialect::DenseTensorType>().offset()));
  }
  std::vector<paddle::dialect::IrMetaTensor> vec_meta_ffn1_bias;
  for (size_t i=0; i < vec_ir_tensor_ffn1_bias.size(); i++) {
    vec_meta_ffn1_bias.push_back(paddle::dialect::IrMetaTensor(&vec_ir_tensor_ffn1_bias[i]));
  }

  std::vector<const phi::MetaTensor*> meta_ffn1_bias;
  for (size_t i=0; i < static_cast<size_t>(vec_meta_ffn1_bias.size()); i++) {
    meta_ffn1_bias.push_back(&vec_meta_ffn1_bias[i]);
  }
   std::vector<paddle::dialect::IrTensor> vec_ir_tensor_ffn1_scales;
  for (size_t i=0; i < static_cast<size_t>(ffn1_scales.size()); i++) {
    vec_ir_tensor_ffn1_scales.push_back(paddle::dialect::IrTensor(paddle::dialect::TransToPhiDataType(ffn1_scales[i].dyn_cast<paddle::dialect::DenseTensorType>().dtype()),
                                                                     ffn1_scales[i].dyn_cast<paddle::dialect::DenseTensorType>().dims(),
                                                                     ffn1_scales[i].dyn_cast<paddle::dialect::DenseTensorType>().data_layout(),
                                                                     ffn1_scales[i].dyn_cast<paddle::dialect::DenseTensorType>().lod(),
                                                                     ffn1_scales[i].dyn_cast<paddle::dialect::DenseTensorType>().offset()));
  }
  std::vector<paddle::dialect::IrMetaTensor> vec_meta_ffn1_scales;
  for (size_t i=0; i < vec_ir_tensor_ffn1_scales.size(); i++) {
    vec_meta_ffn1_scales.push_back(paddle::dialect::IrMetaTensor(&vec_ir_tensor_ffn1_scales[i]));
  }

  std::vector<const phi::MetaTensor*> meta_ffn1_scales;
  for (size_t i=0; i < static_cast<size_t>(vec_meta_ffn1_scales.size()); i++) {
    meta_ffn1_scales.push_back(&vec_meta_ffn1_scales[i]);
  }
   std::vector<paddle::dialect::IrTensor> vec_ir_tensor_ffn2_in_max;
  for (size_t i=0; i < static_cast<size_t>(ffn2_in_max.size()); i++) {
    vec_ir_tensor_ffn2_in_max.push_back(paddle::dialect::IrTensor(paddle::dialect::TransToPhiDataType(ffn2_in_max[i].dyn_cast<paddle::dialect::DenseTensorType>().dtype()),
                                                                     ffn2_in_max[i].dyn_cast<paddle::dialect::DenseTensorType>().dims(),
                                                                     ffn2_in_max[i].dyn_cast<paddle::dialect::DenseTensorType>().data_layout(),
                                                                     ffn2_in_max[i].dyn_cast<paddle::dialect::DenseTensorType>().lod(),
                                                                     ffn2_in_max[i].dyn_cast<paddle::dialect::DenseTensorType>().offset()));
  }
  std::vector<paddle::dialect::IrMetaTensor> vec_meta_ffn2_in_max;
  for (size_t i=0; i < vec_ir_tensor_ffn2_in_max.size(); i++) {
    vec_meta_ffn2_in_max.push_back(paddle::dialect::IrMetaTensor(&vec_ir_tensor_ffn2_in_max[i]));
  }

  std::vector<const phi::MetaTensor*> meta_ffn2_in_max;
  for (size_t i=0; i < static_cast<size_t>(vec_meta_ffn2_in_max.size()); i++) {
    meta_ffn2_in_max.push_back(&vec_meta_ffn2_in_max[i]);
  }
   std::vector<paddle::dialect::IrTensor> vec_ir_tensor_ffn2_weight;
  for (size_t i=0; i < static_cast<size_t>(ffn2_weight.size()); i++) {
    vec_ir_tensor_ffn2_weight.push_back(paddle::dialect::IrTensor(paddle::dialect::TransToPhiDataType(ffn2_weight[i].dyn_cast<paddle::dialect::DenseTensorType>().dtype()),
                                                                     ffn2_weight[i].dyn_cast<paddle::dialect::DenseTensorType>().dims(),
                                                                     ffn2_weight[i].dyn_cast<paddle::dialect::DenseTensorType>().data_layout(),
                                                                     ffn2_weight[i].dyn_cast<paddle::dialect::DenseTensorType>().lod(),
                                                                     ffn2_weight[i].dyn_cast<paddle::dialect::DenseTensorType>().offset()));
  }
  std::vector<paddle::dialect::IrMetaTensor> vec_meta_ffn2_weight;
  for (size_t i=0; i < vec_ir_tensor_ffn2_weight.size(); i++) {
    vec_meta_ffn2_weight.push_back(paddle::dialect::IrMetaTensor(&vec_ir_tensor_ffn2_weight[i]));
  }

  std::vector<const phi::MetaTensor*> meta_ffn2_weight;
  for (size_t i=0; i < static_cast<size_t>(vec_meta_ffn2_weight.size()); i++) {
    meta_ffn2_weight.push_back(&vec_meta_ffn2_weight[i]);
  }
   std::vector<paddle::dialect::IrTensor> vec_ir_tensor_ffn2_bias;
  for (size_t i=0; i < static_cast<size_t>(ffn2_bias.size()); i++) {
    vec_ir_tensor_ffn2_bias.push_back(paddle::dialect::IrTensor(paddle::dialect::TransToPhiDataType(ffn2_bias[i].dyn_cast<paddle::dialect::DenseTensorType>().dtype()),
                                                                     ffn2_bias[i].dyn_cast<paddle::dialect::DenseTensorType>().dims(),
                                                                     ffn2_bias[i].dyn_cast<paddle::dialect::DenseTensorType>().data_layout(),
                                                                     ffn2_bias[i].dyn_cast<paddle::dialect::DenseTensorType>().lod(),
                                                                     ffn2_bias[i].dyn_cast<paddle::dialect::DenseTensorType>().offset()));
  }
  std::vector<paddle::dialect::IrMetaTensor> vec_meta_ffn2_bias;
  for (size_t i=0; i < vec_ir_tensor_ffn2_bias.size(); i++) {
    vec_meta_ffn2_bias.push_back(paddle::dialect::IrMetaTensor(&vec_ir_tensor_ffn2_bias[i]));
  }

  std::vector<const phi::MetaTensor*> meta_ffn2_bias;
  for (size_t i=0; i < static_cast<size_t>(vec_meta_ffn2_bias.size()); i++) {
    meta_ffn2_bias.push_back(&vec_meta_ffn2_bias[i]);
  }
   std::vector<paddle::dialect::IrTensor> vec_ir_tensor_ffn2_scales;
  for (size_t i=0; i < static_cast<size_t>(ffn2_scales.size()); i++) {
    vec_ir_tensor_ffn2_scales.push_back(paddle::dialect::IrTensor(paddle::dialect::TransToPhiDataType(ffn2_scales[i].dyn_cast<paddle::dialect::DenseTensorType>().dtype()),
                                                                     ffn2_scales[i].dyn_cast<paddle::dialect::DenseTensorType>().dims(),
                                                                     ffn2_scales[i].dyn_cast<paddle::dialect::DenseTensorType>().data_layout(),
                                                                     ffn2_scales[i].dyn_cast<paddle::dialect::DenseTensorType>().lod(),
                                                                     ffn2_scales[i].dyn_cast<paddle::dialect::DenseTensorType>().offset()));
  }
  std::vector<paddle::dialect::IrMetaTensor> vec_meta_ffn2_scales;
  for (size_t i=0; i < vec_ir_tensor_ffn2_scales.size(); i++) {
    vec_meta_ffn2_scales.push_back(paddle::dialect::IrMetaTensor(&vec_ir_tensor_ffn2_scales[i]));
  }

  std::vector<const phi::MetaTensor*> meta_ffn2_scales;
  for (size_t i=0; i < static_cast<size_t>(vec_meta_ffn2_scales.size()); i++) {
    meta_ffn2_scales.push_back(&vec_meta_ffn2_scales[i]);
  }
   std::vector<paddle::dialect::IrTensor> vec_ir_tensor_cache_kv;
  if (cache_kv_.impl() != nullptr) {
    pir::VectorType cache_kv = cache_kv_.type().dyn_cast<pir::VectorType>();
    for (size_t i=0; i < static_cast<size_t>(cache_kv.size()); i++) {
        vec_ir_tensor_cache_kv.push_back(paddle::dialect::IrTensor(paddle::dialect::TransToPhiDataType(cache_kv[i].dyn_cast<paddle::dialect::DenseTensorType>().dtype()),
                                                                        cache_kv[i].dyn_cast<paddle::dialect::DenseTensorType>().dims(),
                                                                        cache_kv[i].dyn_cast<paddle::dialect::DenseTensorType>().data_layout(),
                                                                        cache_kv[i].dyn_cast<paddle::dialect::DenseTensorType>().lod(),
                                                                        cache_kv[i].dyn_cast<paddle::dialect::DenseTensorType>().offset()));
    }
  }

  std::vector<paddle::dialect::IrMetaTensor> vec_meta_cache_kv;
  for (size_t i=0; i < vec_ir_tensor_cache_kv.size(); i++) {
    vec_meta_cache_kv.push_back(paddle::dialect::IrMetaTensor(&vec_ir_tensor_cache_kv[i]));
  }

  std::vector<const phi::MetaTensor*> meta_cache_kv;
  for (size_t i=0; i < static_cast<size_t>(vec_meta_cache_kv.size()); i++) {
    meta_cache_kv.push_back(&vec_meta_cache_kv[i]);
  }

  std::vector<paddle::dialect::IrTensor> vec_ir_tensor_pre_caches;
  if (pre_caches_.impl() != nullptr) {
    pir::VectorType pre_caches = pre_caches_.type().dyn_cast<pir::VectorType>();
    for (size_t i=0; i < static_cast<size_t>(pre_caches.size()); i++) {
        vec_ir_tensor_pre_caches.push_back(paddle::dialect::IrTensor(paddle::dialect::TransToPhiDataType(pre_caches[i].dyn_cast<paddle::dialect::DenseTensorType>().dtype()),
                                                                        pre_caches[i].dyn_cast<paddle::dialect::DenseTensorType>().dims(),
                                                                        pre_caches[i].dyn_cast<paddle::dialect::DenseTensorType>().data_layout(),
                                                                        pre_caches[i].dyn_cast<paddle::dialect::DenseTensorType>().lod(),
                                                                        pre_caches[i].dyn_cast<paddle::dialect::DenseTensorType>().offset()));
    }
  }

  std::vector<paddle::dialect::IrMetaTensor> vec_meta_pre_caches;
  for (size_t i=0; i < vec_ir_tensor_pre_caches.size(); i++) {
    vec_meta_pre_caches.push_back(paddle::dialect::IrMetaTensor(&vec_ir_tensor_pre_caches[i]));
  }

  std::vector<const phi::MetaTensor*> meta_pre_caches;
  for (size_t i=0; i < static_cast<size_t>(vec_meta_pre_caches.size()); i++) {
    meta_pre_caches.push_back(&vec_meta_pre_caches[i]);
  }


  paddle::dialect::IrMetaTensor meta_rotary_pos_emb;
  paddle::dialect::IrTensor ir_tensor_rotary_pos_emb;
  if (rotary_pos_emb_.impl() != nullptr) {
    paddle::dialect::DenseTensorType rotary_pos_emb = rotary_pos_emb_.type().dyn_cast<paddle::dialect::DenseTensorType>();
    VLOG(4) << "Builder construction  dense_rotary_pos_emb";
    ir_tensor_rotary_pos_emb = paddle::dialect::IrTensor(paddle::dialect::TransToPhiDataType(rotary_pos_emb.dtype()),
                                                        rotary_pos_emb.dims(),
                                                        rotary_pos_emb.data_layout(),
                                                        rotary_pos_emb.lod(),
                                                        rotary_pos_emb.offset());
    VLOG(4) << "Builder construction  meta_rotary_pos_emb";
    meta_rotary_pos_emb = paddle::dialect::IrMetaTensor(&ir_tensor_rotary_pos_emb);
  }


  paddle::dialect::IrMetaTensor meta_time_step;
  paddle::dialect::IrTensor ir_tensor_time_step;
  if (time_step_.impl() != nullptr) {
    paddle::dialect::DenseTensorType time_step = time_step_.type().dyn_cast<paddle::dialect::DenseTensorType>();
    VLOG(4) << "Builder construction  dense_time_step";
    ir_tensor_time_step = paddle::dialect::IrTensor(paddle::dialect::TransToPhiDataType(time_step.dtype()),
                                                        time_step.dims(),
                                                        time_step.data_layout(),
                                                        time_step.lod(),
                                                        time_step.offset());
    VLOG(4) << "Builder construction  meta_time_step";
    meta_time_step = paddle::dialect::IrMetaTensor(&ir_tensor_time_step);
  }


  paddle::dialect::IrMetaTensor meta_seq_lengths;
  paddle::dialect::IrTensor ir_tensor_seq_lengths;
  if (seq_lengths_.impl() != nullptr) {
    paddle::dialect::DenseTensorType seq_lengths = seq_lengths_.type().dyn_cast<paddle::dialect::DenseTensorType>();
    VLOG(4) << "Builder construction  dense_seq_lengths";
    ir_tensor_seq_lengths = paddle::dialect::IrTensor(paddle::dialect::TransToPhiDataType(seq_lengths.dtype()),
                                                        seq_lengths.dims(),
                                                        seq_lengths.data_layout(),
                                                        seq_lengths.lod(),
                                                        seq_lengths.offset());
    VLOG(4) << "Builder construction  meta_seq_lengths";
    meta_seq_lengths = paddle::dialect::IrMetaTensor(&ir_tensor_seq_lengths);
  }


  paddle::dialect::IrMetaTensor meta_src_mask;
  paddle::dialect::IrTensor ir_tensor_src_mask;
  if (src_mask_.impl() != nullptr) {
    paddle::dialect::DenseTensorType src_mask = src_mask_.type().dyn_cast<paddle::dialect::DenseTensorType>();
    VLOG(4) << "Builder construction  dense_src_mask";
    ir_tensor_src_mask = paddle::dialect::IrTensor(paddle::dialect::TransToPhiDataType(src_mask.dtype()),
                                                        src_mask.dims(),
                                                        src_mask.data_layout(),
                                                        src_mask.lod(),
                                                        src_mask.offset());
    VLOG(4) << "Builder construction  meta_src_mask";
    meta_src_mask = paddle::dialect::IrMetaTensor(&ir_tensor_src_mask);
  }


  paddle::dialect::IrMetaTensor meta_gather_index;
  paddle::dialect::IrTensor ir_tensor_gather_index;
  if (gather_index_.impl() != nullptr) {
    paddle::dialect::DenseTensorType gather_index = gather_index_.type().dyn_cast<paddle::dialect::DenseTensorType>();
    VLOG(4) << "Builder construction  dense_gather_index";
    ir_tensor_gather_index = paddle::dialect::IrTensor(paddle::dialect::TransToPhiDataType(gather_index.dtype()),
                                                        gather_index.dims(),
                                                        gather_index.data_layout(),
                                                        gather_index.lod(),
                                                        gather_index.offset());
    VLOG(4) << "Builder construction  meta_gather_index";
    meta_gather_index = paddle::dialect::IrMetaTensor(&ir_tensor_gather_index);
  }


  VLOG(4) << "Builder construction  dense_max_buffer";
  paddle::dialect::IrTensor ir_tensor_max_buffer(paddle::dialect::TransToPhiDataType(max_buffer.dtype()),
                                                      max_buffer.dims(),
                                                      max_buffer.data_layout(),
                                                      max_buffer.lod(),
                                                      max_buffer.offset());
  VLOG(4) << "Builder construction  meta_max_buffer";
  paddle::dialect::IrMetaTensor meta_max_buffer(&ir_tensor_max_buffer);
  paddle::dialect::IrTensor dense_out;
  paddle::dialect::IrMetaTensor meta_out(&dense_out);
  std::vector<paddle::dialect::IrTensor> vec_dense_cache_kv_out((out_linear_w.size()), paddle::dialect::IrTensor());
  std::vector<paddle::dialect::IrMetaTensor> vec_meta_cache_kv_out;
  for (size_t i=0; i < static_cast<size_t>(out_linear_w.size()); i++) {
    vec_meta_cache_kv_out.push_back(paddle::dialect::IrMetaTensor(&vec_dense_cache_kv_out[i]));
  }
  std::vector<phi::MetaTensor*> meta_cache_kv_out;
  for (size_t i=0; i < static_cast<size_t>(vec_meta_cache_kv_out.size()); i++) {
    meta_cache_kv_out.push_back(&vec_meta_cache_kv_out[i]);
  }

  phi::FusedMultiTransformerInt8XpuInferMeta(meta_x, meta_ln_scale, meta_ln_bias, meta_qkv_in_max, meta_qkvw, meta_qkv_bias, meta_qkv_scales, meta_out_linear_in_max, meta_out_linear_w, meta_out_linear_bias, meta_out_linear_scales, meta_ffn_ln_scale, meta_ffn_ln_bias, meta_ffn1_in_max, meta_ffn1_weight, meta_ffn1_bias, meta_ffn1_scales, meta_ffn2_in_max, meta_ffn2_weight, meta_ffn2_bias, meta_ffn2_scales, meta_cache_kv, meta_pre_caches, meta_rotary_pos_emb, meta_time_step, meta_seq_lengths, meta_src_mask, meta_gather_index, meta_max_buffer, pre_layer_norm, rotary_emb_dims, epsilon, dropout_rate, is_test, dropout_implementation, act_method, trans_qkvw, ring_id, gather_axis, &meta_out, meta_cache_kv_out);

  std::vector<pir::Type> argument_outputs;
  pir::Type out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_out.dtype()), dense_out.dims(), dense_out.layout(), dense_out.lod(), dense_out.offset());
  argument_outputs.push_back(out_dense_tensor_type);

  std::vector<pir::Type> cache_kv_out_types;
  for (size_t i=0; i < static_cast<size_t>(out_linear_w.size()); i++) {
    cache_kv_out_types.push_back(paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(vec_dense_cache_kv_out[i].dtype()), vec_dense_cache_kv_out[i].dims(), vec_dense_cache_kv_out[i].layout(), vec_dense_cache_kv_out[i].lod(), vec_dense_cache_kv_out[i].offset()));
  }
  pir::Type cache_kv_out_vector_type = pir::VectorType::get(pir::IrContext::Instance(), cache_kv_out_types);
  argument_outputs.push_back(cache_kv_out_vector_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void FusedMultiTransformerInt8XpuOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, pir::Value ln_scale_, pir::Value ln_bias_, pir::Value qkv_in_max_, pir::Value qkvw_, pir::Value qkv_bias_, pir::Value qkv_scales_, pir::Value out_linear_in_max_, pir::Value out_linear_w_, pir::Value out_linear_bias_, pir::Value out_linear_scales_, pir::Value ffn_ln_scale_, pir::Value ffn_ln_bias_, pir::Value ffn1_in_max_, pir::Value ffn1_weight_, pir::Value ffn1_bias_, pir::Value ffn1_scales_, pir::Value ffn2_in_max_, pir::Value ffn2_weight_, pir::Value ffn2_bias_, pir::Value ffn2_scales_, pir::Value cache_kv_, pir::Value pre_caches_, pir::Value rotary_pos_emb_, pir::Value time_step_, pir::Value seq_lengths_, pir::Value src_mask_, pir::Value gather_index_, pir::Value max_buffer_, pir::AttributeMap attributes) {
  VLOG(4) << "Start build FusedMultiTransformerInt8XpuOp";


  IR_ENFORCE(
      attributes.find("pre_layer_norm") != attributes.end(),
          "'pre_layer_norm' Attribute is expected for FusedMultiTransformerInt8XpuOp. ");
  bool pre_layer_norm = attributes.at("pre_layer_norm").dyn_cast<pir::BoolAttribute>().data();

  IR_ENFORCE(
      attributes.find("rotary_emb_dims") != attributes.end(),
          "'rotary_emb_dims' Attribute is expected for FusedMultiTransformerInt8XpuOp. ");
  int rotary_emb_dims = attributes.at("rotary_emb_dims").dyn_cast<pir::Int32Attribute>().data();

  IR_ENFORCE(
      attributes.find("epsilon") != attributes.end(),
          "'epsilon' Attribute is expected for FusedMultiTransformerInt8XpuOp. ");
  float epsilon = attributes.at("epsilon").dyn_cast<pir::FloatAttribute>().data();

  IR_ENFORCE(
      attributes.find("dropout_rate") != attributes.end(),
          "'dropout_rate' Attribute is expected for FusedMultiTransformerInt8XpuOp. ");
  float dropout_rate = attributes.at("dropout_rate").dyn_cast<pir::FloatAttribute>().data();

  IR_ENFORCE(
      attributes.find("is_test") != attributes.end(),
          "'is_test' Attribute is expected for FusedMultiTransformerInt8XpuOp. ");
  bool is_test = attributes.at("is_test").dyn_cast<pir::BoolAttribute>().data();

  IR_ENFORCE(
      attributes.find("dropout_implementation") != attributes.end(),
          "'dropout_implementation' Attribute is expected for FusedMultiTransformerInt8XpuOp. ");
  std::string dropout_implementation = attributes.at("dropout_implementation").dyn_cast<pir::StrAttribute>().AsString();

  IR_ENFORCE(
      attributes.find("act_method") != attributes.end(),
          "'act_method' Attribute is expected for FusedMultiTransformerInt8XpuOp. ");
  std::string act_method = attributes.at("act_method").dyn_cast<pir::StrAttribute>().AsString();

  IR_ENFORCE(
      attributes.find("trans_qkvw") != attributes.end(),
          "'trans_qkvw' Attribute is expected for FusedMultiTransformerInt8XpuOp. ");
  bool trans_qkvw = attributes.at("trans_qkvw").dyn_cast<pir::BoolAttribute>().data();

  IR_ENFORCE(
      attributes.find("ring_id") != attributes.end(),
          "'ring_id' Attribute is expected for FusedMultiTransformerInt8XpuOp. ");
  int ring_id = attributes.at("ring_id").dyn_cast<pir::Int32Attribute>().data();

  IR_ENFORCE(
      attributes.find("gather_axis") != attributes.end(),
          "'gather_axis' Attribute is expected for FusedMultiTransformerInt8XpuOp. ");
  int gather_axis = attributes.at("gather_axis").dyn_cast<pir::Int32Attribute>().data();


  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_, ln_scale_, ln_bias_, qkv_in_max_, qkvw_, qkv_bias_, qkv_scales_, out_linear_in_max_, out_linear_w_, out_linear_bias_, out_linear_scales_, ffn_ln_scale_, ffn_ln_bias_, ffn1_in_max_, ffn1_weight_, ffn1_bias_, ffn1_scales_, ffn2_in_max_, ffn2_weight_, ffn2_bias_, ffn2_scales_, cache_kv_, pre_caches_, rotary_pos_emb_, time_step_, seq_lengths_, src_mask_, gather_index_, max_buffer_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_pre_layer_norm = pir::BoolAttribute::get(pir::IrContext::Instance(), pre_layer_norm);
  argument.AddAttribute("pre_layer_norm", attr_pre_layer_norm);
  pir::Attribute attr_rotary_emb_dims = pir::Int32Attribute::get(pir::IrContext::Instance(), rotary_emb_dims);
  argument.AddAttribute("rotary_emb_dims", attr_rotary_emb_dims);
  pir::Attribute attr_epsilon = pir::FloatAttribute::get(pir::IrContext::Instance(), epsilon);
  argument.AddAttribute("epsilon", attr_epsilon);
  pir::Attribute attr_dropout_rate = pir::FloatAttribute::get(pir::IrContext::Instance(), dropout_rate);
  argument.AddAttribute("dropout_rate", attr_dropout_rate);
  pir::Attribute attr_is_test = pir::BoolAttribute::get(pir::IrContext::Instance(), is_test);
  argument.AddAttribute("is_test", attr_is_test);
  pir::Attribute attr_dropout_implementation = pir::StrAttribute::get(pir::IrContext::Instance(), dropout_implementation);
  argument.AddAttribute("dropout_implementation", attr_dropout_implementation);
  pir::Attribute attr_act_method = pir::StrAttribute::get(pir::IrContext::Instance(), act_method);
  argument.AddAttribute("act_method", attr_act_method);
  pir::Attribute attr_trans_qkvw = pir::BoolAttribute::get(pir::IrContext::Instance(), trans_qkvw);
  argument.AddAttribute("trans_qkvw", attr_trans_qkvw);
  pir::Attribute attr_ring_id = pir::Int32Attribute::get(pir::IrContext::Instance(), ring_id);
  argument.AddAttribute("ring_id", attr_ring_id);
  pir::Attribute attr_gather_axis = pir::Int32Attribute::get(pir::IrContext::Instance(), gather_axis);
  argument.AddAttribute("gather_axis", attr_gather_axis);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;
  pir::VectorType ln_scale = ln_scale_.type().dyn_cast<pir::VectorType>(); (void)ln_scale;
  pir::VectorType ln_bias = ln_bias_.type().dyn_cast<pir::VectorType>(); (void)ln_bias;
  pir::VectorType qkv_in_max = qkv_in_max_.type().dyn_cast<pir::VectorType>(); (void)qkv_in_max;
  pir::VectorType qkvw = qkvw_.type().dyn_cast<pir::VectorType>(); (void)qkvw;
  pir::VectorType qkv_bias = qkv_bias_.type().dyn_cast<pir::VectorType>(); (void)qkv_bias;
  pir::VectorType qkv_scales = qkv_scales_.type().dyn_cast<pir::VectorType>(); (void)qkv_scales;
  pir::VectorType out_linear_in_max = out_linear_in_max_.type().dyn_cast<pir::VectorType>(); (void)out_linear_in_max;
  pir::VectorType out_linear_w = out_linear_w_.type().dyn_cast<pir::VectorType>(); (void)out_linear_w;
  pir::VectorType out_linear_bias = out_linear_bias_.type().dyn_cast<pir::VectorType>(); (void)out_linear_bias;
  pir::VectorType out_linear_scales = out_linear_scales_.type().dyn_cast<pir::VectorType>(); (void)out_linear_scales;
  pir::VectorType ffn_ln_scale = ffn_ln_scale_.type().dyn_cast<pir::VectorType>(); (void)ffn_ln_scale;
  pir::VectorType ffn_ln_bias = ffn_ln_bias_.type().dyn_cast<pir::VectorType>(); (void)ffn_ln_bias;
  pir::VectorType ffn1_in_max = ffn1_in_max_.type().dyn_cast<pir::VectorType>(); (void)ffn1_in_max;
  pir::VectorType ffn1_weight = ffn1_weight_.type().dyn_cast<pir::VectorType>(); (void)ffn1_weight;
  pir::VectorType ffn1_bias = ffn1_bias_.type().dyn_cast<pir::VectorType>(); (void)ffn1_bias;
  pir::VectorType ffn1_scales = ffn1_scales_.type().dyn_cast<pir::VectorType>(); (void)ffn1_scales;
  pir::VectorType ffn2_in_max = ffn2_in_max_.type().dyn_cast<pir::VectorType>(); (void)ffn2_in_max;
  pir::VectorType ffn2_weight = ffn2_weight_.type().dyn_cast<pir::VectorType>(); (void)ffn2_weight;
  pir::VectorType ffn2_bias = ffn2_bias_.type().dyn_cast<pir::VectorType>(); (void)ffn2_bias;
  pir::VectorType ffn2_scales = ffn2_scales_.type().dyn_cast<pir::VectorType>(); (void)ffn2_scales;
  paddle::dialect::DenseTensorType max_buffer = max_buffer_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)max_buffer;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);
  std::vector<paddle::dialect::IrTensor> vec_ir_tensor_ln_scale;
  for (size_t i=0; i < static_cast<size_t>(ln_scale.size()); i++) {
    vec_ir_tensor_ln_scale.push_back(paddle::dialect::IrTensor(paddle::dialect::TransToPhiDataType(ln_scale[i].dyn_cast<paddle::dialect::DenseTensorType>().dtype()),
                                                                     ln_scale[i].dyn_cast<paddle::dialect::DenseTensorType>().dims(),
                                                                     ln_scale[i].dyn_cast<paddle::dialect::DenseTensorType>().data_layout(),
                                                                     ln_scale[i].dyn_cast<paddle::dialect::DenseTensorType>().lod(),
                                                                     ln_scale[i].dyn_cast<paddle::dialect::DenseTensorType>().offset()));
  }
  std::vector<paddle::dialect::IrMetaTensor> vec_meta_ln_scale;
  for (size_t i=0; i < vec_ir_tensor_ln_scale.size(); i++) {
    vec_meta_ln_scale.push_back(paddle::dialect::IrMetaTensor(&vec_ir_tensor_ln_scale[i]));
  }

  std::vector<const phi::MetaTensor*> meta_ln_scale;
  for (size_t i=0; i < static_cast<size_t>(vec_meta_ln_scale.size()); i++) {
    meta_ln_scale.push_back(&vec_meta_ln_scale[i]);
  }
   std::vector<paddle::dialect::IrTensor> vec_ir_tensor_ln_bias;
  for (size_t i=0; i < static_cast<size_t>(ln_bias.size()); i++) {
    vec_ir_tensor_ln_bias.push_back(paddle::dialect::IrTensor(paddle::dialect::TransToPhiDataType(ln_bias[i].dyn_cast<paddle::dialect::DenseTensorType>().dtype()),
                                                                     ln_bias[i].dyn_cast<paddle::dialect::DenseTensorType>().dims(),
                                                                     ln_bias[i].dyn_cast<paddle::dialect::DenseTensorType>().data_layout(),
                                                                     ln_bias[i].dyn_cast<paddle::dialect::DenseTensorType>().lod(),
                                                                     ln_bias[i].dyn_cast<paddle::dialect::DenseTensorType>().offset()));
  }
  std::vector<paddle::dialect::IrMetaTensor> vec_meta_ln_bias;
  for (size_t i=0; i < vec_ir_tensor_ln_bias.size(); i++) {
    vec_meta_ln_bias.push_back(paddle::dialect::IrMetaTensor(&vec_ir_tensor_ln_bias[i]));
  }

  std::vector<const phi::MetaTensor*> meta_ln_bias;
  for (size_t i=0; i < static_cast<size_t>(vec_meta_ln_bias.size()); i++) {
    meta_ln_bias.push_back(&vec_meta_ln_bias[i]);
  }
   std::vector<paddle::dialect::IrTensor> vec_ir_tensor_qkv_in_max;
  for (size_t i=0; i < static_cast<size_t>(qkv_in_max.size()); i++) {
    vec_ir_tensor_qkv_in_max.push_back(paddle::dialect::IrTensor(paddle::dialect::TransToPhiDataType(qkv_in_max[i].dyn_cast<paddle::dialect::DenseTensorType>().dtype()),
                                                                     qkv_in_max[i].dyn_cast<paddle::dialect::DenseTensorType>().dims(),
                                                                     qkv_in_max[i].dyn_cast<paddle::dialect::DenseTensorType>().data_layout(),
                                                                     qkv_in_max[i].dyn_cast<paddle::dialect::DenseTensorType>().lod(),
                                                                     qkv_in_max[i].dyn_cast<paddle::dialect::DenseTensorType>().offset()));
  }
  std::vector<paddle::dialect::IrMetaTensor> vec_meta_qkv_in_max;
  for (size_t i=0; i < vec_ir_tensor_qkv_in_max.size(); i++) {
    vec_meta_qkv_in_max.push_back(paddle::dialect::IrMetaTensor(&vec_ir_tensor_qkv_in_max[i]));
  }

  std::vector<const phi::MetaTensor*> meta_qkv_in_max;
  for (size_t i=0; i < static_cast<size_t>(vec_meta_qkv_in_max.size()); i++) {
    meta_qkv_in_max.push_back(&vec_meta_qkv_in_max[i]);
  }
   std::vector<paddle::dialect::IrTensor> vec_ir_tensor_qkvw;
  for (size_t i=0; i < static_cast<size_t>(qkvw.size()); i++) {
    vec_ir_tensor_qkvw.push_back(paddle::dialect::IrTensor(paddle::dialect::TransToPhiDataType(qkvw[i].dyn_cast<paddle::dialect::DenseTensorType>().dtype()),
                                                                     qkvw[i].dyn_cast<paddle::dialect::DenseTensorType>().dims(),
                                                                     qkvw[i].dyn_cast<paddle::dialect::DenseTensorType>().data_layout(),
                                                                     qkvw[i].dyn_cast<paddle::dialect::DenseTensorType>().lod(),
                                                                     qkvw[i].dyn_cast<paddle::dialect::DenseTensorType>().offset()));
  }
  std::vector<paddle::dialect::IrMetaTensor> vec_meta_qkvw;
  for (size_t i=0; i < vec_ir_tensor_qkvw.size(); i++) {
    vec_meta_qkvw.push_back(paddle::dialect::IrMetaTensor(&vec_ir_tensor_qkvw[i]));
  }

  std::vector<const phi::MetaTensor*> meta_qkvw;
  for (size_t i=0; i < static_cast<size_t>(vec_meta_qkvw.size()); i++) {
    meta_qkvw.push_back(&vec_meta_qkvw[i]);
  }
   std::vector<paddle::dialect::IrTensor> vec_ir_tensor_qkv_bias;
  for (size_t i=0; i < static_cast<size_t>(qkv_bias.size()); i++) {
    vec_ir_tensor_qkv_bias.push_back(paddle::dialect::IrTensor(paddle::dialect::TransToPhiDataType(qkv_bias[i].dyn_cast<paddle::dialect::DenseTensorType>().dtype()),
                                                                     qkv_bias[i].dyn_cast<paddle::dialect::DenseTensorType>().dims(),
                                                                     qkv_bias[i].dyn_cast<paddle::dialect::DenseTensorType>().data_layout(),
                                                                     qkv_bias[i].dyn_cast<paddle::dialect::DenseTensorType>().lod(),
                                                                     qkv_bias[i].dyn_cast<paddle::dialect::DenseTensorType>().offset()));
  }
  std::vector<paddle::dialect::IrMetaTensor> vec_meta_qkv_bias;
  for (size_t i=0; i < vec_ir_tensor_qkv_bias.size(); i++) {
    vec_meta_qkv_bias.push_back(paddle::dialect::IrMetaTensor(&vec_ir_tensor_qkv_bias[i]));
  }

  std::vector<const phi::MetaTensor*> meta_qkv_bias;
  for (size_t i=0; i < static_cast<size_t>(vec_meta_qkv_bias.size()); i++) {
    meta_qkv_bias.push_back(&vec_meta_qkv_bias[i]);
  }
   std::vector<paddle::dialect::IrTensor> vec_ir_tensor_qkv_scales;
  for (size_t i=0; i < static_cast<size_t>(qkv_scales.size()); i++) {
    vec_ir_tensor_qkv_scales.push_back(paddle::dialect::IrTensor(paddle::dialect::TransToPhiDataType(qkv_scales[i].dyn_cast<paddle::dialect::DenseTensorType>().dtype()),
                                                                     qkv_scales[i].dyn_cast<paddle::dialect::DenseTensorType>().dims(),
                                                                     qkv_scales[i].dyn_cast<paddle::dialect::DenseTensorType>().data_layout(),
                                                                     qkv_scales[i].dyn_cast<paddle::dialect::DenseTensorType>().lod(),
                                                                     qkv_scales[i].dyn_cast<paddle::dialect::DenseTensorType>().offset()));
  }
  std::vector<paddle::dialect::IrMetaTensor> vec_meta_qkv_scales;
  for (size_t i=0; i < vec_ir_tensor_qkv_scales.size(); i++) {
    vec_meta_qkv_scales.push_back(paddle::dialect::IrMetaTensor(&vec_ir_tensor_qkv_scales[i]));
  }

  std::vector<const phi::MetaTensor*> meta_qkv_scales;
  for (size_t i=0; i < static_cast<size_t>(vec_meta_qkv_scales.size()); i++) {
    meta_qkv_scales.push_back(&vec_meta_qkv_scales[i]);
  }
   std::vector<paddle::dialect::IrTensor> vec_ir_tensor_out_linear_in_max;
  for (size_t i=0; i < static_cast<size_t>(out_linear_in_max.size()); i++) {
    vec_ir_tensor_out_linear_in_max.push_back(paddle::dialect::IrTensor(paddle::dialect::TransToPhiDataType(out_linear_in_max[i].dyn_cast<paddle::dialect::DenseTensorType>().dtype()),
                                                                     out_linear_in_max[i].dyn_cast<paddle::dialect::DenseTensorType>().dims(),
                                                                     out_linear_in_max[i].dyn_cast<paddle::dialect::DenseTensorType>().data_layout(),
                                                                     out_linear_in_max[i].dyn_cast<paddle::dialect::DenseTensorType>().lod(),
                                                                     out_linear_in_max[i].dyn_cast<paddle::dialect::DenseTensorType>().offset()));
  }
  std::vector<paddle::dialect::IrMetaTensor> vec_meta_out_linear_in_max;
  for (size_t i=0; i < vec_ir_tensor_out_linear_in_max.size(); i++) {
    vec_meta_out_linear_in_max.push_back(paddle::dialect::IrMetaTensor(&vec_ir_tensor_out_linear_in_max[i]));
  }

  std::vector<const phi::MetaTensor*> meta_out_linear_in_max;
  for (size_t i=0; i < static_cast<size_t>(vec_meta_out_linear_in_max.size()); i++) {
    meta_out_linear_in_max.push_back(&vec_meta_out_linear_in_max[i]);
  }
   std::vector<paddle::dialect::IrTensor> vec_ir_tensor_out_linear_w;
  for (size_t i=0; i < static_cast<size_t>(out_linear_w.size()); i++) {
    vec_ir_tensor_out_linear_w.push_back(paddle::dialect::IrTensor(paddle::dialect::TransToPhiDataType(out_linear_w[i].dyn_cast<paddle::dialect::DenseTensorType>().dtype()),
                                                                     out_linear_w[i].dyn_cast<paddle::dialect::DenseTensorType>().dims(),
                                                                     out_linear_w[i].dyn_cast<paddle::dialect::DenseTensorType>().data_layout(),
                                                                     out_linear_w[i].dyn_cast<paddle::dialect::DenseTensorType>().lod(),
                                                                     out_linear_w[i].dyn_cast<paddle::dialect::DenseTensorType>().offset()));
  }
  std::vector<paddle::dialect::IrMetaTensor> vec_meta_out_linear_w;
  for (size_t i=0; i < vec_ir_tensor_out_linear_w.size(); i++) {
    vec_meta_out_linear_w.push_back(paddle::dialect::IrMetaTensor(&vec_ir_tensor_out_linear_w[i]));
  }

  std::vector<const phi::MetaTensor*> meta_out_linear_w;
  for (size_t i=0; i < static_cast<size_t>(vec_meta_out_linear_w.size()); i++) {
    meta_out_linear_w.push_back(&vec_meta_out_linear_w[i]);
  }
   std::vector<paddle::dialect::IrTensor> vec_ir_tensor_out_linear_bias;
  for (size_t i=0; i < static_cast<size_t>(out_linear_bias.size()); i++) {
    vec_ir_tensor_out_linear_bias.push_back(paddle::dialect::IrTensor(paddle::dialect::TransToPhiDataType(out_linear_bias[i].dyn_cast<paddle::dialect::DenseTensorType>().dtype()),
                                                                     out_linear_bias[i].dyn_cast<paddle::dialect::DenseTensorType>().dims(),
                                                                     out_linear_bias[i].dyn_cast<paddle::dialect::DenseTensorType>().data_layout(),
                                                                     out_linear_bias[i].dyn_cast<paddle::dialect::DenseTensorType>().lod(),
                                                                     out_linear_bias[i].dyn_cast<paddle::dialect::DenseTensorType>().offset()));
  }
  std::vector<paddle::dialect::IrMetaTensor> vec_meta_out_linear_bias;
  for (size_t i=0; i < vec_ir_tensor_out_linear_bias.size(); i++) {
    vec_meta_out_linear_bias.push_back(paddle::dialect::IrMetaTensor(&vec_ir_tensor_out_linear_bias[i]));
  }

  std::vector<const phi::MetaTensor*> meta_out_linear_bias;
  for (size_t i=0; i < static_cast<size_t>(vec_meta_out_linear_bias.size()); i++) {
    meta_out_linear_bias.push_back(&vec_meta_out_linear_bias[i]);
  }
   std::vector<paddle::dialect::IrTensor> vec_ir_tensor_out_linear_scales;
  for (size_t i=0; i < static_cast<size_t>(out_linear_scales.size()); i++) {
    vec_ir_tensor_out_linear_scales.push_back(paddle::dialect::IrTensor(paddle::dialect::TransToPhiDataType(out_linear_scales[i].dyn_cast<paddle::dialect::DenseTensorType>().dtype()),
                                                                     out_linear_scales[i].dyn_cast<paddle::dialect::DenseTensorType>().dims(),
                                                                     out_linear_scales[i].dyn_cast<paddle::dialect::DenseTensorType>().data_layout(),
                                                                     out_linear_scales[i].dyn_cast<paddle::dialect::DenseTensorType>().lod(),
                                                                     out_linear_scales[i].dyn_cast<paddle::dialect::DenseTensorType>().offset()));
  }
  std::vector<paddle::dialect::IrMetaTensor> vec_meta_out_linear_scales;
  for (size_t i=0; i < vec_ir_tensor_out_linear_scales.size(); i++) {
    vec_meta_out_linear_scales.push_back(paddle::dialect::IrMetaTensor(&vec_ir_tensor_out_linear_scales[i]));
  }

  std::vector<const phi::MetaTensor*> meta_out_linear_scales;
  for (size_t i=0; i < static_cast<size_t>(vec_meta_out_linear_scales.size()); i++) {
    meta_out_linear_scales.push_back(&vec_meta_out_linear_scales[i]);
  }
   std::vector<paddle::dialect::IrTensor> vec_ir_tensor_ffn_ln_scale;
  for (size_t i=0; i < static_cast<size_t>(ffn_ln_scale.size()); i++) {
    vec_ir_tensor_ffn_ln_scale.push_back(paddle::dialect::IrTensor(paddle::dialect::TransToPhiDataType(ffn_ln_scale[i].dyn_cast<paddle::dialect::DenseTensorType>().dtype()),
                                                                     ffn_ln_scale[i].dyn_cast<paddle::dialect::DenseTensorType>().dims(),
                                                                     ffn_ln_scale[i].dyn_cast<paddle::dialect::DenseTensorType>().data_layout(),
                                                                     ffn_ln_scale[i].dyn_cast<paddle::dialect::DenseTensorType>().lod(),
                                                                     ffn_ln_scale[i].dyn_cast<paddle::dialect::DenseTensorType>().offset()));
  }
  std::vector<paddle::dialect::IrMetaTensor> vec_meta_ffn_ln_scale;
  for (size_t i=0; i < vec_ir_tensor_ffn_ln_scale.size(); i++) {
    vec_meta_ffn_ln_scale.push_back(paddle::dialect::IrMetaTensor(&vec_ir_tensor_ffn_ln_scale[i]));
  }

  std::vector<const phi::MetaTensor*> meta_ffn_ln_scale;
  for (size_t i=0; i < static_cast<size_t>(vec_meta_ffn_ln_scale.size()); i++) {
    meta_ffn_ln_scale.push_back(&vec_meta_ffn_ln_scale[i]);
  }
   std::vector<paddle::dialect::IrTensor> vec_ir_tensor_ffn_ln_bias;
  for (size_t i=0; i < static_cast<size_t>(ffn_ln_bias.size()); i++) {
    vec_ir_tensor_ffn_ln_bias.push_back(paddle::dialect::IrTensor(paddle::dialect::TransToPhiDataType(ffn_ln_bias[i].dyn_cast<paddle::dialect::DenseTensorType>().dtype()),
                                                                     ffn_ln_bias[i].dyn_cast<paddle::dialect::DenseTensorType>().dims(),
                                                                     ffn_ln_bias[i].dyn_cast<paddle::dialect::DenseTensorType>().data_layout(),
                                                                     ffn_ln_bias[i].dyn_cast<paddle::dialect::DenseTensorType>().lod(),
                                                                     ffn_ln_bias[i].dyn_cast<paddle::dialect::DenseTensorType>().offset()));
  }
  std::vector<paddle::dialect::IrMetaTensor> vec_meta_ffn_ln_bias;
  for (size_t i=0; i < vec_ir_tensor_ffn_ln_bias.size(); i++) {
    vec_meta_ffn_ln_bias.push_back(paddle::dialect::IrMetaTensor(&vec_ir_tensor_ffn_ln_bias[i]));
  }

  std::vector<const phi::MetaTensor*> meta_ffn_ln_bias;
  for (size_t i=0; i < static_cast<size_t>(vec_meta_ffn_ln_bias.size()); i++) {
    meta_ffn_ln_bias.push_back(&vec_meta_ffn_ln_bias[i]);
  }
   std::vector<paddle::dialect::IrTensor> vec_ir_tensor_ffn1_in_max;
  for (size_t i=0; i < static_cast<size_t>(ffn1_in_max.size()); i++) {
    vec_ir_tensor_ffn1_in_max.push_back(paddle::dialect::IrTensor(paddle::dialect::TransToPhiDataType(ffn1_in_max[i].dyn_cast<paddle::dialect::DenseTensorType>().dtype()),
                                                                     ffn1_in_max[i].dyn_cast<paddle::dialect::DenseTensorType>().dims(),
                                                                     ffn1_in_max[i].dyn_cast<paddle::dialect::DenseTensorType>().data_layout(),
                                                                     ffn1_in_max[i].dyn_cast<paddle::dialect::DenseTensorType>().lod(),
                                                                     ffn1_in_max[i].dyn_cast<paddle::dialect::DenseTensorType>().offset()));
  }
  std::vector<paddle::dialect::IrMetaTensor> vec_meta_ffn1_in_max;
  for (size_t i=0; i < vec_ir_tensor_ffn1_in_max.size(); i++) {
    vec_meta_ffn1_in_max.push_back(paddle::dialect::IrMetaTensor(&vec_ir_tensor_ffn1_in_max[i]));
  }

  std::vector<const phi::MetaTensor*> meta_ffn1_in_max;
  for (size_t i=0; i < static_cast<size_t>(vec_meta_ffn1_in_max.size()); i++) {
    meta_ffn1_in_max.push_back(&vec_meta_ffn1_in_max[i]);
  }
   std::vector<paddle::dialect::IrTensor> vec_ir_tensor_ffn1_weight;
  for (size_t i=0; i < static_cast<size_t>(ffn1_weight.size()); i++) {
    vec_ir_tensor_ffn1_weight.push_back(paddle::dialect::IrTensor(paddle::dialect::TransToPhiDataType(ffn1_weight[i].dyn_cast<paddle::dialect::DenseTensorType>().dtype()),
                                                                     ffn1_weight[i].dyn_cast<paddle::dialect::DenseTensorType>().dims(),
                                                                     ffn1_weight[i].dyn_cast<paddle::dialect::DenseTensorType>().data_layout(),
                                                                     ffn1_weight[i].dyn_cast<paddle::dialect::DenseTensorType>().lod(),
                                                                     ffn1_weight[i].dyn_cast<paddle::dialect::DenseTensorType>().offset()));
  }
  std::vector<paddle::dialect::IrMetaTensor> vec_meta_ffn1_weight;
  for (size_t i=0; i < vec_ir_tensor_ffn1_weight.size(); i++) {
    vec_meta_ffn1_weight.push_back(paddle::dialect::IrMetaTensor(&vec_ir_tensor_ffn1_weight[i]));
  }

  std::vector<const phi::MetaTensor*> meta_ffn1_weight;
  for (size_t i=0; i < static_cast<size_t>(vec_meta_ffn1_weight.size()); i++) {
    meta_ffn1_weight.push_back(&vec_meta_ffn1_weight[i]);
  }
   std::vector<paddle::dialect::IrTensor> vec_ir_tensor_ffn1_bias;
  for (size_t i=0; i < static_cast<size_t>(ffn1_bias.size()); i++) {
    vec_ir_tensor_ffn1_bias.push_back(paddle::dialect::IrTensor(paddle::dialect::TransToPhiDataType(ffn1_bias[i].dyn_cast<paddle::dialect::DenseTensorType>().dtype()),
                                                                     ffn1_bias[i].dyn_cast<paddle::dialect::DenseTensorType>().dims(),
                                                                     ffn1_bias[i].dyn_cast<paddle::dialect::DenseTensorType>().data_layout(),
                                                                     ffn1_bias[i].dyn_cast<paddle::dialect::DenseTensorType>().lod(),
                                                                     ffn1_bias[i].dyn_cast<paddle::dialect::DenseTensorType>().offset()));
  }
  std::vector<paddle::dialect::IrMetaTensor> vec_meta_ffn1_bias;
  for (size_t i=0; i < vec_ir_tensor_ffn1_bias.size(); i++) {
    vec_meta_ffn1_bias.push_back(paddle::dialect::IrMetaTensor(&vec_ir_tensor_ffn1_bias[i]));
  }

  std::vector<const phi::MetaTensor*> meta_ffn1_bias;
  for (size_t i=0; i < static_cast<size_t>(vec_meta_ffn1_bias.size()); i++) {
    meta_ffn1_bias.push_back(&vec_meta_ffn1_bias[i]);
  }
   std::vector<paddle::dialect::IrTensor> vec_ir_tensor_ffn1_scales;
  for (size_t i=0; i < static_cast<size_t>(ffn1_scales.size()); i++) {
    vec_ir_tensor_ffn1_scales.push_back(paddle::dialect::IrTensor(paddle::dialect::TransToPhiDataType(ffn1_scales[i].dyn_cast<paddle::dialect::DenseTensorType>().dtype()),
                                                                     ffn1_scales[i].dyn_cast<paddle::dialect::DenseTensorType>().dims(),
                                                                     ffn1_scales[i].dyn_cast<paddle::dialect::DenseTensorType>().data_layout(),
                                                                     ffn1_scales[i].dyn_cast<paddle::dialect::DenseTensorType>().lod(),
                                                                     ffn1_scales[i].dyn_cast<paddle::dialect::DenseTensorType>().offset()));
  }
  std::vector<paddle::dialect::IrMetaTensor> vec_meta_ffn1_scales;
  for (size_t i=0; i < vec_ir_tensor_ffn1_scales.size(); i++) {
    vec_meta_ffn1_scales.push_back(paddle::dialect::IrMetaTensor(&vec_ir_tensor_ffn1_scales[i]));
  }

  std::vector<const phi::MetaTensor*> meta_ffn1_scales;
  for (size_t i=0; i < static_cast<size_t>(vec_meta_ffn1_scales.size()); i++) {
    meta_ffn1_scales.push_back(&vec_meta_ffn1_scales[i]);
  }
   std::vector<paddle::dialect::IrTensor> vec_ir_tensor_ffn2_in_max;
  for (size_t i=0; i < static_cast<size_t>(ffn2_in_max.size()); i++) {
    vec_ir_tensor_ffn2_in_max.push_back(paddle::dialect::IrTensor(paddle::dialect::TransToPhiDataType(ffn2_in_max[i].dyn_cast<paddle::dialect::DenseTensorType>().dtype()),
                                                                     ffn2_in_max[i].dyn_cast<paddle::dialect::DenseTensorType>().dims(),
                                                                     ffn2_in_max[i].dyn_cast<paddle::dialect::DenseTensorType>().data_layout(),
                                                                     ffn2_in_max[i].dyn_cast<paddle::dialect::DenseTensorType>().lod(),
                                                                     ffn2_in_max[i].dyn_cast<paddle::dialect::DenseTensorType>().offset()));
  }
  std::vector<paddle::dialect::IrMetaTensor> vec_meta_ffn2_in_max;
  for (size_t i=0; i < vec_ir_tensor_ffn2_in_max.size(); i++) {
    vec_meta_ffn2_in_max.push_back(paddle::dialect::IrMetaTensor(&vec_ir_tensor_ffn2_in_max[i]));
  }

  std::vector<const phi::MetaTensor*> meta_ffn2_in_max;
  for (size_t i=0; i < static_cast<size_t>(vec_meta_ffn2_in_max.size()); i++) {
    meta_ffn2_in_max.push_back(&vec_meta_ffn2_in_max[i]);
  }
   std::vector<paddle::dialect::IrTensor> vec_ir_tensor_ffn2_weight;
  for (size_t i=0; i < static_cast<size_t>(ffn2_weight.size()); i++) {
    vec_ir_tensor_ffn2_weight.push_back(paddle::dialect::IrTensor(paddle::dialect::TransToPhiDataType(ffn2_weight[i].dyn_cast<paddle::dialect::DenseTensorType>().dtype()),
                                                                     ffn2_weight[i].dyn_cast<paddle::dialect::DenseTensorType>().dims(),
                                                                     ffn2_weight[i].dyn_cast<paddle::dialect::DenseTensorType>().data_layout(),
                                                                     ffn2_weight[i].dyn_cast<paddle::dialect::DenseTensorType>().lod(),
                                                                     ffn2_weight[i].dyn_cast<paddle::dialect::DenseTensorType>().offset()));
  }
  std::vector<paddle::dialect::IrMetaTensor> vec_meta_ffn2_weight;
  for (size_t i=0; i < vec_ir_tensor_ffn2_weight.size(); i++) {
    vec_meta_ffn2_weight.push_back(paddle::dialect::IrMetaTensor(&vec_ir_tensor_ffn2_weight[i]));
  }

  std::vector<const phi::MetaTensor*> meta_ffn2_weight;
  for (size_t i=0; i < static_cast<size_t>(vec_meta_ffn2_weight.size()); i++) {
    meta_ffn2_weight.push_back(&vec_meta_ffn2_weight[i]);
  }
   std::vector<paddle::dialect::IrTensor> vec_ir_tensor_ffn2_bias;
  for (size_t i=0; i < static_cast<size_t>(ffn2_bias.size()); i++) {
    vec_ir_tensor_ffn2_bias.push_back(paddle::dialect::IrTensor(paddle::dialect::TransToPhiDataType(ffn2_bias[i].dyn_cast<paddle::dialect::DenseTensorType>().dtype()),
                                                                     ffn2_bias[i].dyn_cast<paddle::dialect::DenseTensorType>().dims(),
                                                                     ffn2_bias[i].dyn_cast<paddle::dialect::DenseTensorType>().data_layout(),
                                                                     ffn2_bias[i].dyn_cast<paddle::dialect::DenseTensorType>().lod(),
                                                                     ffn2_bias[i].dyn_cast<paddle::dialect::DenseTensorType>().offset()));
  }
  std::vector<paddle::dialect::IrMetaTensor> vec_meta_ffn2_bias;
  for (size_t i=0; i < vec_ir_tensor_ffn2_bias.size(); i++) {
    vec_meta_ffn2_bias.push_back(paddle::dialect::IrMetaTensor(&vec_ir_tensor_ffn2_bias[i]));
  }

  std::vector<const phi::MetaTensor*> meta_ffn2_bias;
  for (size_t i=0; i < static_cast<size_t>(vec_meta_ffn2_bias.size()); i++) {
    meta_ffn2_bias.push_back(&vec_meta_ffn2_bias[i]);
  }
   std::vector<paddle::dialect::IrTensor> vec_ir_tensor_ffn2_scales;
  for (size_t i=0; i < static_cast<size_t>(ffn2_scales.size()); i++) {
    vec_ir_tensor_ffn2_scales.push_back(paddle::dialect::IrTensor(paddle::dialect::TransToPhiDataType(ffn2_scales[i].dyn_cast<paddle::dialect::DenseTensorType>().dtype()),
                                                                     ffn2_scales[i].dyn_cast<paddle::dialect::DenseTensorType>().dims(),
                                                                     ffn2_scales[i].dyn_cast<paddle::dialect::DenseTensorType>().data_layout(),
                                                                     ffn2_scales[i].dyn_cast<paddle::dialect::DenseTensorType>().lod(),
                                                                     ffn2_scales[i].dyn_cast<paddle::dialect::DenseTensorType>().offset()));
  }
  std::vector<paddle::dialect::IrMetaTensor> vec_meta_ffn2_scales;
  for (size_t i=0; i < vec_ir_tensor_ffn2_scales.size(); i++) {
    vec_meta_ffn2_scales.push_back(paddle::dialect::IrMetaTensor(&vec_ir_tensor_ffn2_scales[i]));
  }

  std::vector<const phi::MetaTensor*> meta_ffn2_scales;
  for (size_t i=0; i < static_cast<size_t>(vec_meta_ffn2_scales.size()); i++) {
    meta_ffn2_scales.push_back(&vec_meta_ffn2_scales[i]);
  }
   std::vector<paddle::dialect::IrTensor> vec_ir_tensor_cache_kv;
  if (cache_kv_.impl() != nullptr) {
    pir::VectorType cache_kv = cache_kv_.type().dyn_cast<pir::VectorType>();
    for (size_t i=0; i < static_cast<size_t>(cache_kv.size()); i++) {
        vec_ir_tensor_cache_kv.push_back(paddle::dialect::IrTensor(paddle::dialect::TransToPhiDataType(cache_kv[i].dyn_cast<paddle::dialect::DenseTensorType>().dtype()),
                                                                        cache_kv[i].dyn_cast<paddle::dialect::DenseTensorType>().dims(),
                                                                        cache_kv[i].dyn_cast<paddle::dialect::DenseTensorType>().data_layout(),
                                                                        cache_kv[i].dyn_cast<paddle::dialect::DenseTensorType>().lod(),
                                                                        cache_kv[i].dyn_cast<paddle::dialect::DenseTensorType>().offset()));
    }
  }

  std::vector<paddle::dialect::IrMetaTensor> vec_meta_cache_kv;
  for (size_t i=0; i < vec_ir_tensor_cache_kv.size(); i++) {
    vec_meta_cache_kv.push_back(paddle::dialect::IrMetaTensor(&vec_ir_tensor_cache_kv[i]));
  }

  std::vector<const phi::MetaTensor*> meta_cache_kv;
  for (size_t i=0; i < static_cast<size_t>(vec_meta_cache_kv.size()); i++) {
    meta_cache_kv.push_back(&vec_meta_cache_kv[i]);
  }

  std::vector<paddle::dialect::IrTensor> vec_ir_tensor_pre_caches;
  if (pre_caches_.impl() != nullptr) {
    pir::VectorType pre_caches = pre_caches_.type().dyn_cast<pir::VectorType>();
    for (size_t i=0; i < static_cast<size_t>(pre_caches.size()); i++) {
        vec_ir_tensor_pre_caches.push_back(paddle::dialect::IrTensor(paddle::dialect::TransToPhiDataType(pre_caches[i].dyn_cast<paddle::dialect::DenseTensorType>().dtype()),
                                                                        pre_caches[i].dyn_cast<paddle::dialect::DenseTensorType>().dims(),
                                                                        pre_caches[i].dyn_cast<paddle::dialect::DenseTensorType>().data_layout(),
                                                                        pre_caches[i].dyn_cast<paddle::dialect::DenseTensorType>().lod(),
                                                                        pre_caches[i].dyn_cast<paddle::dialect::DenseTensorType>().offset()));
    }
  }

  std::vector<paddle::dialect::IrMetaTensor> vec_meta_pre_caches;
  for (size_t i=0; i < vec_ir_tensor_pre_caches.size(); i++) {
    vec_meta_pre_caches.push_back(paddle::dialect::IrMetaTensor(&vec_ir_tensor_pre_caches[i]));
  }

  std::vector<const phi::MetaTensor*> meta_pre_caches;
  for (size_t i=0; i < static_cast<size_t>(vec_meta_pre_caches.size()); i++) {
    meta_pre_caches.push_back(&vec_meta_pre_caches[i]);
  }


  paddle::dialect::IrMetaTensor meta_rotary_pos_emb;
  paddle::dialect::IrTensor ir_tensor_rotary_pos_emb;
  if (rotary_pos_emb_.impl() != nullptr) {
    paddle::dialect::DenseTensorType rotary_pos_emb = rotary_pos_emb_.type().dyn_cast<paddle::dialect::DenseTensorType>();
    VLOG(4) << "Builder construction  dense_rotary_pos_emb";
    ir_tensor_rotary_pos_emb = paddle::dialect::IrTensor(paddle::dialect::TransToPhiDataType(rotary_pos_emb.dtype()),
                                                        rotary_pos_emb.dims(),
                                                        rotary_pos_emb.data_layout(),
                                                        rotary_pos_emb.lod(),
                                                        rotary_pos_emb.offset());
    VLOG(4) << "Builder construction  meta_rotary_pos_emb";
    meta_rotary_pos_emb = paddle::dialect::IrMetaTensor(&ir_tensor_rotary_pos_emb);
  }


  paddle::dialect::IrMetaTensor meta_time_step;
  paddle::dialect::IrTensor ir_tensor_time_step;
  if (time_step_.impl() != nullptr) {
    paddle::dialect::DenseTensorType time_step = time_step_.type().dyn_cast<paddle::dialect::DenseTensorType>();
    VLOG(4) << "Builder construction  dense_time_step";
    ir_tensor_time_step = paddle::dialect::IrTensor(paddle::dialect::TransToPhiDataType(time_step.dtype()),
                                                        time_step.dims(),
                                                        time_step.data_layout(),
                                                        time_step.lod(),
                                                        time_step.offset());
    VLOG(4) << "Builder construction  meta_time_step";
    meta_time_step = paddle::dialect::IrMetaTensor(&ir_tensor_time_step);
  }


  paddle::dialect::IrMetaTensor meta_seq_lengths;
  paddle::dialect::IrTensor ir_tensor_seq_lengths;
  if (seq_lengths_.impl() != nullptr) {
    paddle::dialect::DenseTensorType seq_lengths = seq_lengths_.type().dyn_cast<paddle::dialect::DenseTensorType>();
    VLOG(4) << "Builder construction  dense_seq_lengths";
    ir_tensor_seq_lengths = paddle::dialect::IrTensor(paddle::dialect::TransToPhiDataType(seq_lengths.dtype()),
                                                        seq_lengths.dims(),
                                                        seq_lengths.data_layout(),
                                                        seq_lengths.lod(),
                                                        seq_lengths.offset());
    VLOG(4) << "Builder construction  meta_seq_lengths";
    meta_seq_lengths = paddle::dialect::IrMetaTensor(&ir_tensor_seq_lengths);
  }


  paddle::dialect::IrMetaTensor meta_src_mask;
  paddle::dialect::IrTensor ir_tensor_src_mask;
  if (src_mask_.impl() != nullptr) {
    paddle::dialect::DenseTensorType src_mask = src_mask_.type().dyn_cast<paddle::dialect::DenseTensorType>();
    VLOG(4) << "Builder construction  dense_src_mask";
    ir_tensor_src_mask = paddle::dialect::IrTensor(paddle::dialect::TransToPhiDataType(src_mask.dtype()),
                                                        src_mask.dims(),
                                                        src_mask.data_layout(),
                                                        src_mask.lod(),
                                                        src_mask.offset());
    VLOG(4) << "Builder construction  meta_src_mask";
    meta_src_mask = paddle::dialect::IrMetaTensor(&ir_tensor_src_mask);
  }


  paddle::dialect::IrMetaTensor meta_gather_index;
  paddle::dialect::IrTensor ir_tensor_gather_index;
  if (gather_index_.impl() != nullptr) {
    paddle::dialect::DenseTensorType gather_index = gather_index_.type().dyn_cast<paddle::dialect::DenseTensorType>();
    VLOG(4) << "Builder construction  dense_gather_index";
    ir_tensor_gather_index = paddle::dialect::IrTensor(paddle::dialect::TransToPhiDataType(gather_index.dtype()),
                                                        gather_index.dims(),
                                                        gather_index.data_layout(),
                                                        gather_index.lod(),
                                                        gather_index.offset());
    VLOG(4) << "Builder construction  meta_gather_index";
    meta_gather_index = paddle::dialect::IrMetaTensor(&ir_tensor_gather_index);
  }


  VLOG(4) << "Builder construction  dense_max_buffer";
  paddle::dialect::IrTensor ir_tensor_max_buffer(paddle::dialect::TransToPhiDataType(max_buffer.dtype()),
                                                      max_buffer.dims(),
                                                      max_buffer.data_layout(),
                                                      max_buffer.lod(),
                                                      max_buffer.offset());
  VLOG(4) << "Builder construction  meta_max_buffer";
  paddle::dialect::IrMetaTensor meta_max_buffer(&ir_tensor_max_buffer);
  paddle::dialect::IrTensor dense_out;
  paddle::dialect::IrMetaTensor meta_out(&dense_out);
  std::vector<paddle::dialect::IrTensor> vec_dense_cache_kv_out((out_linear_w.size()), paddle::dialect::IrTensor());
  std::vector<paddle::dialect::IrMetaTensor> vec_meta_cache_kv_out;
  for (size_t i=0; i < static_cast<size_t>(out_linear_w.size()); i++) {
    vec_meta_cache_kv_out.push_back(paddle::dialect::IrMetaTensor(&vec_dense_cache_kv_out[i]));
  }
  std::vector<phi::MetaTensor*> meta_cache_kv_out;
  for (size_t i=0; i < static_cast<size_t>(vec_meta_cache_kv_out.size()); i++) {
    meta_cache_kv_out.push_back(&vec_meta_cache_kv_out[i]);
  }

  phi::FusedMultiTransformerInt8XpuInferMeta(meta_x, meta_ln_scale, meta_ln_bias, meta_qkv_in_max, meta_qkvw, meta_qkv_bias, meta_qkv_scales, meta_out_linear_in_max, meta_out_linear_w, meta_out_linear_bias, meta_out_linear_scales, meta_ffn_ln_scale, meta_ffn_ln_bias, meta_ffn1_in_max, meta_ffn1_weight, meta_ffn1_bias, meta_ffn1_scales, meta_ffn2_in_max, meta_ffn2_weight, meta_ffn2_bias, meta_ffn2_scales, meta_cache_kv, meta_pre_caches, meta_rotary_pos_emb, meta_time_step, meta_seq_lengths, meta_src_mask, meta_gather_index, meta_max_buffer, pre_layer_norm, rotary_emb_dims, epsilon, dropout_rate, is_test, dropout_implementation, act_method, trans_qkvw, ring_id, gather_axis, &meta_out, meta_cache_kv_out);

  std::vector<pir::Type> argument_outputs;
  pir::Type out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_out.dtype()), dense_out.dims(), dense_out.layout(), dense_out.lod(), dense_out.offset());
  argument_outputs.push_back(out_dense_tensor_type);

  std::vector<pir::Type> cache_kv_out_types;
  for (size_t i=0; i < static_cast<size_t>(out_linear_w.size()); i++) {
    cache_kv_out_types.push_back(paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(vec_dense_cache_kv_out[i].dtype()), vec_dense_cache_kv_out[i].dims(), vec_dense_cache_kv_out[i].layout(), vec_dense_cache_kv_out[i].lod(), vec_dense_cache_kv_out[i].offset()));
  }
  pir::Type cache_kv_out_vector_type = pir::VectorType::get(pir::IrContext::Instance(), cache_kv_out_types);
  argument_outputs.push_back(cache_kv_out_vector_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void FusedMultiTransformerInt8XpuOp::VerifySig() {
  VLOG(4) << "Start Verifying inputs, outputs and attributes for: FusedMultiTransformerInt8XpuOp.";
  VLOG(4) << "Verifying inputs:";
  {
  auto input_size = num_operands();
  IR_ENFORCE(input_size == 29u,
                    "The size %d of inputs must be equal to 29.", input_size);
  IR_ENFORCE((*this)->operand_source(0).type().isa<paddle::dialect::DenseTensorType>(),
                  "Type validation failed for the 0th input, got %s.", (*this)->operand_source(0).type());
  if (auto vec_type = (*this)->operand_source(1).type().dyn_cast<pir::VectorType>()) {
      for (size_t i = 0; i < vec_type.size(); ++i) {
        IR_ENFORCE(vec_type[i].isa<paddle::dialect::DenseTensorType>(),
                       "Type validation failed for the 1th input, got %s.", (*this)->operand_source(1).type());
      }
  }
  else {
    IR_ENFORCE((*this)->operand_source(1).type().isa<paddle::dialect::DenseTensorType>(),
                   "Type validation failed for the 1th input, got %s.", (*this)->operand_source(1).type());
  }
  if (auto vec_type = (*this)->operand_source(2).type().dyn_cast<pir::VectorType>()) {
      for (size_t i = 0; i < vec_type.size(); ++i) {
        IR_ENFORCE(vec_type[i].isa<paddle::dialect::DenseTensorType>(),
                       "Type validation failed for the 2th input, got %s.", (*this)->operand_source(2).type());
      }
  }
  else {
    IR_ENFORCE((*this)->operand_source(2).type().isa<paddle::dialect::DenseTensorType>(),
                   "Type validation failed for the 2th input, got %s.", (*this)->operand_source(2).type());
  }
  if (auto vec_type = (*this)->operand_source(3).type().dyn_cast<pir::VectorType>()) {
      for (size_t i = 0; i < vec_type.size(); ++i) {
        IR_ENFORCE(vec_type[i].isa<paddle::dialect::DenseTensorType>(),
                       "Type validation failed for the 3th input, got %s.", (*this)->operand_source(3).type());
      }
  }
  else {
    IR_ENFORCE((*this)->operand_source(3).type().isa<paddle::dialect::DenseTensorType>(),
                   "Type validation failed for the 3th input, got %s.", (*this)->operand_source(3).type());
  }
  if (auto vec_type = (*this)->operand_source(4).type().dyn_cast<pir::VectorType>()) {
      for (size_t i = 0; i < vec_type.size(); ++i) {
        IR_ENFORCE(vec_type[i].isa<paddle::dialect::DenseTensorType>(),
                       "Type validation failed for the 4th input, got %s.", (*this)->operand_source(4).type());
      }
  }
  else {
    IR_ENFORCE((*this)->operand_source(4).type().isa<paddle::dialect::DenseTensorType>(),
                   "Type validation failed for the 4th input, got %s.", (*this)->operand_source(4).type());
  }
  if (auto vec_type = (*this)->operand_source(5).type().dyn_cast<pir::VectorType>()) {
      for (size_t i = 0; i < vec_type.size(); ++i) {
        IR_ENFORCE(vec_type[i].isa<paddle::dialect::DenseTensorType>(),
                       "Type validation failed for the 5th input, got %s.", (*this)->operand_source(5).type());
      }
  }
  else {
    IR_ENFORCE((*this)->operand_source(5).type().isa<paddle::dialect::DenseTensorType>(),
                   "Type validation failed for the 5th input, got %s.", (*this)->operand_source(5).type());
  }
  if (auto vec_type = (*this)->operand_source(6).type().dyn_cast<pir::VectorType>()) {
      for (size_t i = 0; i < vec_type.size(); ++i) {
        IR_ENFORCE(vec_type[i].isa<paddle::dialect::DenseTensorType>(),
                       "Type validation failed for the 6th input, got %s.", (*this)->operand_source(6).type());
      }
  }
  else {
    IR_ENFORCE((*this)->operand_source(6).type().isa<paddle::dialect::DenseTensorType>(),
                   "Type validation failed for the 6th input, got %s.", (*this)->operand_source(6).type());
  }
  if (auto vec_type = (*this)->operand_source(7).type().dyn_cast<pir::VectorType>()) {
      for (size_t i = 0; i < vec_type.size(); ++i) {
        IR_ENFORCE(vec_type[i].isa<paddle::dialect::DenseTensorType>(),
                       "Type validation failed for the 7th input, got %s.", (*this)->operand_source(7).type());
      }
  }
  else {
    IR_ENFORCE((*this)->operand_source(7).type().isa<paddle::dialect::DenseTensorType>(),
                   "Type validation failed for the 7th input, got %s.", (*this)->operand_source(7).type());
  }
  if (auto vec_type = (*this)->operand_source(8).type().dyn_cast<pir::VectorType>()) {
      for (size_t i = 0; i < vec_type.size(); ++i) {
        IR_ENFORCE(vec_type[i].isa<paddle::dialect::DenseTensorType>(),
                       "Type validation failed for the 8th input, got %s.", (*this)->operand_source(8).type());
      }
  }
  else {
    IR_ENFORCE((*this)->operand_source(8).type().isa<paddle::dialect::DenseTensorType>(),
                   "Type validation failed for the 8th input, got %s.", (*this)->operand_source(8).type());
  }
  if (auto vec_type = (*this)->operand_source(9).type().dyn_cast<pir::VectorType>()) {
      for (size_t i = 0; i < vec_type.size(); ++i) {
        IR_ENFORCE(vec_type[i].isa<paddle::dialect::DenseTensorType>(),
                       "Type validation failed for the 9th input, got %s.", (*this)->operand_source(9).type());
      }
  }
  else {
    IR_ENFORCE((*this)->operand_source(9).type().isa<paddle::dialect::DenseTensorType>(),
                   "Type validation failed for the 9th input, got %s.", (*this)->operand_source(9).type());
  }
  if (auto vec_type = (*this)->operand_source(10).type().dyn_cast<pir::VectorType>()) {
      for (size_t i = 0; i < vec_type.size(); ++i) {
        IR_ENFORCE(vec_type[i].isa<paddle::dialect::DenseTensorType>(),
                       "Type validation failed for the 10th input, got %s.", (*this)->operand_source(10).type());
      }
  }
  else {
    IR_ENFORCE((*this)->operand_source(10).type().isa<paddle::dialect::DenseTensorType>(),
                   "Type validation failed for the 10th input, got %s.", (*this)->operand_source(10).type());
  }
  if (auto vec_type = (*this)->operand_source(11).type().dyn_cast<pir::VectorType>()) {
      for (size_t i = 0; i < vec_type.size(); ++i) {
        IR_ENFORCE(vec_type[i].isa<paddle::dialect::DenseTensorType>(),
                       "Type validation failed for the 11th input, got %s.", (*this)->operand_source(11).type());
      }
  }
  else {
    IR_ENFORCE((*this)->operand_source(11).type().isa<paddle::dialect::DenseTensorType>(),
                   "Type validation failed for the 11th input, got %s.", (*this)->operand_source(11).type());
  }
  if (auto vec_type = (*this)->operand_source(12).type().dyn_cast<pir::VectorType>()) {
      for (size_t i = 0; i < vec_type.size(); ++i) {
        IR_ENFORCE(vec_type[i].isa<paddle::dialect::DenseTensorType>(),
                       "Type validation failed for the 12th input, got %s.", (*this)->operand_source(12).type());
      }
  }
  else {
    IR_ENFORCE((*this)->operand_source(12).type().isa<paddle::dialect::DenseTensorType>(),
                   "Type validation failed for the 12th input, got %s.", (*this)->operand_source(12).type());
  }
  if (auto vec_type = (*this)->operand_source(13).type().dyn_cast<pir::VectorType>()) {
      for (size_t i = 0; i < vec_type.size(); ++i) {
        IR_ENFORCE(vec_type[i].isa<paddle::dialect::DenseTensorType>(),
                       "Type validation failed for the 13th input, got %s.", (*this)->operand_source(13).type());
      }
  }
  else {
    IR_ENFORCE((*this)->operand_source(13).type().isa<paddle::dialect::DenseTensorType>(),
                   "Type validation failed for the 13th input, got %s.", (*this)->operand_source(13).type());
  }
  if (auto vec_type = (*this)->operand_source(14).type().dyn_cast<pir::VectorType>()) {
      for (size_t i = 0; i < vec_type.size(); ++i) {
        IR_ENFORCE(vec_type[i].isa<paddle::dialect::DenseTensorType>(),
                       "Type validation failed for the 14th input, got %s.", (*this)->operand_source(14).type());
      }
  }
  else {
    IR_ENFORCE((*this)->operand_source(14).type().isa<paddle::dialect::DenseTensorType>(),
                   "Type validation failed for the 14th input, got %s.", (*this)->operand_source(14).type());
  }
  if (auto vec_type = (*this)->operand_source(15).type().dyn_cast<pir::VectorType>()) {
      for (size_t i = 0; i < vec_type.size(); ++i) {
        IR_ENFORCE(vec_type[i].isa<paddle::dialect::DenseTensorType>(),
                       "Type validation failed for the 15th input, got %s.", (*this)->operand_source(15).type());
      }
  }
  else {
    IR_ENFORCE((*this)->operand_source(15).type().isa<paddle::dialect::DenseTensorType>(),
                   "Type validation failed for the 15th input, got %s.", (*this)->operand_source(15).type());
  }
  if (auto vec_type = (*this)->operand_source(16).type().dyn_cast<pir::VectorType>()) {
      for (size_t i = 0; i < vec_type.size(); ++i) {
        IR_ENFORCE(vec_type[i].isa<paddle::dialect::DenseTensorType>(),
                       "Type validation failed for the 16th input, got %s.", (*this)->operand_source(16).type());
      }
  }
  else {
    IR_ENFORCE((*this)->operand_source(16).type().isa<paddle::dialect::DenseTensorType>(),
                   "Type validation failed for the 16th input, got %s.", (*this)->operand_source(16).type());
  }
  if (auto vec_type = (*this)->operand_source(17).type().dyn_cast<pir::VectorType>()) {
      for (size_t i = 0; i < vec_type.size(); ++i) {
        IR_ENFORCE(vec_type[i].isa<paddle::dialect::DenseTensorType>(),
                       "Type validation failed for the 17th input, got %s.", (*this)->operand_source(17).type());
      }
  }
  else {
    IR_ENFORCE((*this)->operand_source(17).type().isa<paddle::dialect::DenseTensorType>(),
                   "Type validation failed for the 17th input, got %s.", (*this)->operand_source(17).type());
  }
  if (auto vec_type = (*this)->operand_source(18).type().dyn_cast<pir::VectorType>()) {
      for (size_t i = 0; i < vec_type.size(); ++i) {
        IR_ENFORCE(vec_type[i].isa<paddle::dialect::DenseTensorType>(),
                       "Type validation failed for the 18th input, got %s.", (*this)->operand_source(18).type());
      }
  }
  else {
    IR_ENFORCE((*this)->operand_source(18).type().isa<paddle::dialect::DenseTensorType>(),
                   "Type validation failed for the 18th input, got %s.", (*this)->operand_source(18).type());
  }
  if (auto vec_type = (*this)->operand_source(19).type().dyn_cast<pir::VectorType>()) {
      for (size_t i = 0; i < vec_type.size(); ++i) {
        IR_ENFORCE(vec_type[i].isa<paddle::dialect::DenseTensorType>(),
                       "Type validation failed for the 19th input, got %s.", (*this)->operand_source(19).type());
      }
  }
  else {
    IR_ENFORCE((*this)->operand_source(19).type().isa<paddle::dialect::DenseTensorType>(),
                   "Type validation failed for the 19th input, got %s.", (*this)->operand_source(19).type());
  }
  if (auto vec_type = (*this)->operand_source(20).type().dyn_cast<pir::VectorType>()) {
      for (size_t i = 0; i < vec_type.size(); ++i) {
        IR_ENFORCE(vec_type[i].isa<paddle::dialect::DenseTensorType>(),
                       "Type validation failed for the 20th input, got %s.", (*this)->operand_source(20).type());
      }
  }
  else {
    IR_ENFORCE((*this)->operand_source(20).type().isa<paddle::dialect::DenseTensorType>(),
                   "Type validation failed for the 20th input, got %s.", (*this)->operand_source(20).type());
  }
  if (auto val =  (*this)->operand(21)) {
    if (auto vec_type = val.type().dyn_cast<pir::VectorType>()) {
      for (size_t i = 0; i < vec_type.size(); i++) {
        IR_ENFORCE(vec_type[i].isa<paddle::dialect::DenseTensorType>(),
                          "Type validation failed for the 21th input, got %s.", (*this)->operand_source(21).type());
      }
    }
    else {
      IR_ENFORCE(val.type().isa<paddle::dialect::DenseTensorType>(),
                        "Type validation failed for the 21th input, got %s.", (*this)->operand_source(21).type());
    }
  }
  if (auto val =  (*this)->operand(22)) {
    if (auto vec_type = val.type().dyn_cast<pir::VectorType>()) {
      for (size_t i = 0; i < vec_type.size(); i++) {
        IR_ENFORCE(vec_type[i].isa<paddle::dialect::DenseTensorType>(),
                          "Type validation failed for the 22th input, got %s.", (*this)->operand_source(22).type());
      }
    }
    else {
      IR_ENFORCE(val.type().isa<paddle::dialect::DenseTensorType>(),
                        "Type validation failed for the 22th input, got %s.", (*this)->operand_source(22).type());
    }
  }
  if (auto val = (*this)->operand(23)) {
    IR_ENFORCE(val.type().isa<paddle::dialect::DenseTensorType>(),
                   "Type validation failed for the 23th input, got %s.", (*this)->operand_source(23).type());
  }
  if (auto val = (*this)->operand(24)) {
    IR_ENFORCE(val.type().isa<paddle::dialect::DenseTensorType>(),
                   "Type validation failed for the 24th input, got %s.", (*this)->operand_source(24).type());
  }
  if (auto val = (*this)->operand(25)) {
    IR_ENFORCE(val.type().isa<paddle::dialect::DenseTensorType>(),
                   "Type validation failed for the 25th input, got %s.", (*this)->operand_source(25).type());
  }
  if (auto val = (*this)->operand(26)) {
    IR_ENFORCE(val.type().isa<paddle::dialect::DenseTensorType>(),
                   "Type validation failed for the 26th input, got %s.", (*this)->operand_source(26).type());
  }
  if (auto val = (*this)->operand(27)) {
    IR_ENFORCE(val.type().isa<paddle::dialect::DenseTensorType>(),
                   "Type validation failed for the 27th input, got %s.", (*this)->operand_source(27).type());
  }
  IR_ENFORCE((*this)->operand_source(28).type().isa<paddle::dialect::DenseTensorType>(),
                  "Type validation failed for the 28th input, got %s.", (*this)->operand_source(28).type());
  }
  VLOG(4) << "Verifying attributes:";
  {
  auto& attributes = this->attributes();
  IR_ENFORCE(attributes.count("pre_layer_norm")>0,
                 "pre_layer_norm does not exist.");
  IR_ENFORCE(attributes.at("pre_layer_norm").isa<pir::BoolAttribute>(),
                 "Type of attribute: pre_layer_norm is not pir::BoolAttribute.");

  IR_ENFORCE(attributes.count("rotary_emb_dims")>0,
                 "rotary_emb_dims does not exist.");
  IR_ENFORCE(attributes.at("rotary_emb_dims").isa<pir::Int32Attribute>(),
                 "Type of attribute: rotary_emb_dims is not pir::Int32Attribute.");

  IR_ENFORCE(attributes.count("epsilon")>0,
                 "epsilon does not exist.");
  IR_ENFORCE(attributes.at("epsilon").isa<pir::FloatAttribute>(),
                 "Type of attribute: epsilon is not pir::FloatAttribute.");

  IR_ENFORCE(attributes.count("dropout_rate")>0,
                 "dropout_rate does not exist.");
  IR_ENFORCE(attributes.at("dropout_rate").isa<pir::FloatAttribute>(),
                 "Type of attribute: dropout_rate is not pir::FloatAttribute.");

  IR_ENFORCE(attributes.count("is_test")>0,
                 "is_test does not exist.");
  IR_ENFORCE(attributes.at("is_test").isa<pir::BoolAttribute>(),
                 "Type of attribute: is_test is not pir::BoolAttribute.");

  IR_ENFORCE(attributes.count("dropout_implementation")>0,
                 "dropout_implementation does not exist.");
  IR_ENFORCE(attributes.at("dropout_implementation").isa<pir::StrAttribute>(),
                 "Type of attribute: dropout_implementation is not pir::StrAttribute.");

  IR_ENFORCE(attributes.count("act_method")>0,
                 "act_method does not exist.");
  IR_ENFORCE(attributes.at("act_method").isa<pir::StrAttribute>(),
                 "Type of attribute: act_method is not pir::StrAttribute.");

  IR_ENFORCE(attributes.count("trans_qkvw")>0,
                 "trans_qkvw does not exist.");
  IR_ENFORCE(attributes.at("trans_qkvw").isa<pir::BoolAttribute>(),
                 "Type of attribute: trans_qkvw is not pir::BoolAttribute.");

  IR_ENFORCE(attributes.count("ring_id")>0,
                 "ring_id does not exist.");
  IR_ENFORCE(attributes.at("ring_id").isa<pir::Int32Attribute>(),
                 "Type of attribute: ring_id is not pir::Int32Attribute.");

  IR_ENFORCE(attributes.count("gather_axis")>0,
                 "gather_axis does not exist.");
  IR_ENFORCE(attributes.at("gather_axis").isa<pir::Int32Attribute>(),
                 "Type of attribute: gather_axis is not pir::Int32Attribute.");

  }
  VLOG(4) << "Verifying outputs:";
  {
  auto output_size = num_results();
  IR_ENFORCE(output_size == 2u,
                    "The size %d of outputs must be equal to 2.", output_size);
  IR_ENFORCE((*this)->result(0).type().isa<paddle::dialect::DenseTensorType>(),
                 "Type validation failed for the 0th output.");
  auto output_1_type = (*this)->result(1).type();
  if (auto vec_type = output_1_type.dyn_cast<pir::VectorType>()) {
    for (size_t i = 0; i < vec_type.size(); i++) {
      IR_ENFORCE(vec_type[i].isa<paddle::dialect::DenseTensorType>(),
                     "Type validation failed for the 1th output.");
    }
  }
  else {
    IR_ENFORCE(output_1_type.isa<paddle::dialect::DenseTensorType>(),
                   "Type validation failed for the 1th output.");
  }
  }
  VLOG(4) << "End Verifying for: FusedMultiTransformerInt8XpuOp.";
}

void FusedMultiTransformerInt8XpuOp::InferMeta( phi::InferMetaContext *infer_meta ) {
  auto fn = PD_INFER_META(phi::FusedMultiTransformerInt8XpuInferMeta);
  fn(infer_meta);
}

phi::DataType FusedMultiTransformerInt8XpuOp::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: FusedMultiTransformerInt8XpuOp";
  


  return expected_kernel_dtype;
}

const char *FusedMultiTransformerXpuOp::attributes_name[10] = { "pre_layer_norm", "rotary_emb_dims", "epsilon", "dropout_rate", "is_test", "dropout_implementation", "act_method", "trans_qkvw", "ring_id", "gather_axis" };

OpInfoTuple FusedMultiTransformerXpuOp::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("x", "paddle::dialect::DenseTensorType", false, false, false, false), paddle::dialect::OpInputInfo("ln_scale", "pir::VectorType<paddle::dialect::DenseTensorType>", false, false, false, false), paddle::dialect::OpInputInfo("ln_bias", "pir::VectorType<paddle::dialect::DenseTensorType>", false, false, false, false), paddle::dialect::OpInputInfo("qkvw", "pir::VectorType<paddle::dialect::DenseTensorType>", false, false, false, false), paddle::dialect::OpInputInfo("qkvw_max", "pir::VectorType<paddle::dialect::DenseTensorType>", false, false, false, false), paddle::dialect::OpInputInfo("qkv_bias", "pir::VectorType<paddle::dialect::DenseTensorType>", false, false, false, false), paddle::dialect::OpInputInfo("out_linear_w", "pir::VectorType<paddle::dialect::DenseTensorType>", false, false, false, false), paddle::dialect::OpInputInfo("out_linear_wmax", "pir::VectorType<paddle::dialect::DenseTensorType>", false, false, false, false), paddle::dialect::OpInputInfo("out_linear_bias", "pir::VectorType<paddle::dialect::DenseTensorType>", false, false, false, false), paddle::dialect::OpInputInfo("ffn_ln_scale", "pir::VectorType<paddle::dialect::DenseTensorType>", false, false, false, false), paddle::dialect::OpInputInfo("ffn_ln_bias", "pir::VectorType<paddle::dialect::DenseTensorType>", false, false, false, false), paddle::dialect::OpInputInfo("ffn1_weight", "pir::VectorType<paddle::dialect::DenseTensorType>", false, false, false, false), paddle::dialect::OpInputInfo("ffn1_weight_max", "pir::VectorType<paddle::dialect::DenseTensorType>", false, false, false, false), paddle::dialect::OpInputInfo("ffn1_bias", "pir::VectorType<paddle::dialect::DenseTensorType>", false, false, false, false), paddle::dialect::OpInputInfo("ffn2_weight", "pir::VectorType<paddle::dialect::DenseTensorType>", false, false, false, false), paddle::dialect::OpInputInfo("ffn2_weight_max", "pir::VectorType<paddle::dialect::DenseTensorType>", false, false, false, false), paddle::dialect::OpInputInfo("ffn2_bias", "pir::VectorType<paddle::dialect::DenseTensorType>", false, false, false, false), paddle::dialect::OpInputInfo("cache_kv", "pir::VectorType<paddle::dialect::DenseTensorType>", true, false, false, false), paddle::dialect::OpInputInfo("pre_caches", "pir::VectorType<paddle::dialect::DenseTensorType>", true, false, false, false), paddle::dialect::OpInputInfo("rotary_pos_emb", "paddle::dialect::DenseTensorType", true, false, false, false), paddle::dialect::OpInputInfo("time_step", "paddle::dialect::DenseTensorType", true, false, false, false), paddle::dialect::OpInputInfo("seq_lengths", "paddle::dialect::DenseTensorType", true, false, false, false), paddle::dialect::OpInputInfo("src_mask", "paddle::dialect::DenseTensorType", true, false, false, false), paddle::dialect::OpInputInfo("gather_index", "paddle::dialect::DenseTensorType", true, false, false, false), paddle::dialect::OpInputInfo("max_buffer", "paddle::dialect::DenseTensorType", false, false, false, false) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = { paddle::dialect::OpAttributeInfo("pre_layer_norm", "pir::BoolAttribute", ""), paddle::dialect::OpAttributeInfo("rotary_emb_dims", "pir::Int32Attribute", ""), paddle::dialect::OpAttributeInfo("epsilon", "pir::FloatAttribute", ""), paddle::dialect::OpAttributeInfo("dropout_rate", "pir::FloatAttribute", ""), paddle::dialect::OpAttributeInfo("is_test", "pir::BoolAttribute", ""), paddle::dialect::OpAttributeInfo("dropout_implementation", "pir::StrAttribute", ""), paddle::dialect::OpAttributeInfo("act_method", "pir::StrAttribute", ""), paddle::dialect::OpAttributeInfo("trans_qkvw", "pir::BoolAttribute", ""), paddle::dialect::OpAttributeInfo("ring_id", "pir::Int32Attribute", ""), paddle::dialect::OpAttributeInfo("gather_axis", "pir::Int32Attribute", "") };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("out", "paddle::dialect::DenseTensorType", false, false), paddle::dialect::OpOutputInfo("cache_kv_out", "pir::VectorType<paddle::dialect::DenseTensorType>", false, false) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("FusedMultiTransformerXpuInferMeta", {"x", "ln_scale", "ln_bias", "qkvw", "qkvw_max", "qkv_bias", "out_linear_w", "out_linear_wmax", "out_linear_bias", "ffn_ln_scale", "ffn_ln_bias", "ffn1_weight", "ffn1_weight_max", "ffn1_bias", "ffn2_weight", "ffn2_weight_max", "ffn2_bias", "cache_kv", "pre_caches", "rotary_pos_emb", "time_step", "seq_lengths", "src_mask", "gather_index", "max_buffer", "pre_layer_norm", "rotary_emb_dims", "epsilon", "dropout_rate", "is_test", "dropout_implementation", "act_method", "trans_qkvw", "ring_id", "gather_axis"}, "fused_multi_transformer_xpu", {"x", "ln_scale", "ln_bias", "qkvw", "qkvw_max", "qkv_bias", "out_linear_w", "out_linear_wmax", "out_linear_bias", "ffn_ln_scale", "ffn_ln_bias", "ffn1_weight", "ffn1_weight_max", "ffn1_bias", "ffn2_weight", "ffn2_weight_max", "ffn2_bias", "cache_kv", "pre_caches", "rotary_pos_emb", "time_step", "seq_lengths", "src_mask", "gather_index", "max_buffer", "pre_layer_norm", "rotary_emb_dims", "epsilon", "dropout_rate", "is_test", "dropout_implementation", "act_method", "trans_qkvw", "ring_id", "gather_axis"}, {"x"}, {}, {}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "fused_multi_transformer_xpu");
}

void FusedMultiTransformerXpuOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, pir::Value ln_scale_, pir::Value ln_bias_, pir::Value qkvw_, pir::Value qkvw_max_, pir::Value qkv_bias_, pir::Value out_linear_w_, pir::Value out_linear_wmax_, pir::Value out_linear_bias_, pir::Value ffn_ln_scale_, pir::Value ffn_ln_bias_, pir::Value ffn1_weight_, pir::Value ffn1_weight_max_, pir::Value ffn1_bias_, pir::Value ffn2_weight_, pir::Value ffn2_weight_max_, pir::Value ffn2_bias_, pir::Value cache_kv_, pir::Value pre_caches_, pir::Value rotary_pos_emb_, pir::Value time_step_, pir::Value seq_lengths_, pir::Value src_mask_, pir::Value gather_index_, pir::Value max_buffer_, bool pre_layer_norm, int rotary_emb_dims, float epsilon, float dropout_rate, bool is_test, const std::string& dropout_implementation, const std::string& act_method, bool trans_qkvw, int ring_id, int gather_axis) {
  VLOG(4) << "Start build FusedMultiTransformerXpuOp";



  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_, ln_scale_, ln_bias_, qkvw_, qkvw_max_, qkv_bias_, out_linear_w_, out_linear_wmax_, out_linear_bias_, ffn_ln_scale_, ffn_ln_bias_, ffn1_weight_, ffn1_weight_max_, ffn1_bias_, ffn2_weight_, ffn2_weight_max_, ffn2_bias_, cache_kv_, pre_caches_, rotary_pos_emb_, time_step_, seq_lengths_, src_mask_, gather_index_, max_buffer_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_pre_layer_norm = pir::BoolAttribute::get(pir::IrContext::Instance(), pre_layer_norm);
  argument.AddAttribute("pre_layer_norm", attr_pre_layer_norm);
  pir::Attribute attr_rotary_emb_dims = pir::Int32Attribute::get(pir::IrContext::Instance(), rotary_emb_dims);
  argument.AddAttribute("rotary_emb_dims", attr_rotary_emb_dims);
  pir::Attribute attr_epsilon = pir::FloatAttribute::get(pir::IrContext::Instance(), epsilon);
  argument.AddAttribute("epsilon", attr_epsilon);
  pir::Attribute attr_dropout_rate = pir::FloatAttribute::get(pir::IrContext::Instance(), dropout_rate);
  argument.AddAttribute("dropout_rate", attr_dropout_rate);
  pir::Attribute attr_is_test = pir::BoolAttribute::get(pir::IrContext::Instance(), is_test);
  argument.AddAttribute("is_test", attr_is_test);
  pir::Attribute attr_dropout_implementation = pir::StrAttribute::get(pir::IrContext::Instance(), dropout_implementation);
  argument.AddAttribute("dropout_implementation", attr_dropout_implementation);
  pir::Attribute attr_act_method = pir::StrAttribute::get(pir::IrContext::Instance(), act_method);
  argument.AddAttribute("act_method", attr_act_method);
  pir::Attribute attr_trans_qkvw = pir::BoolAttribute::get(pir::IrContext::Instance(), trans_qkvw);
  argument.AddAttribute("trans_qkvw", attr_trans_qkvw);
  pir::Attribute attr_ring_id = pir::Int32Attribute::get(pir::IrContext::Instance(), ring_id);
  argument.AddAttribute("ring_id", attr_ring_id);
  pir::Attribute attr_gather_axis = pir::Int32Attribute::get(pir::IrContext::Instance(), gather_axis);
  argument.AddAttribute("gather_axis", attr_gather_axis);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;
  pir::VectorType ln_scale = ln_scale_.type().dyn_cast<pir::VectorType>(); (void)ln_scale;
  pir::VectorType ln_bias = ln_bias_.type().dyn_cast<pir::VectorType>(); (void)ln_bias;
  pir::VectorType qkvw = qkvw_.type().dyn_cast<pir::VectorType>(); (void)qkvw;
  pir::VectorType qkvw_max = qkvw_max_.type().dyn_cast<pir::VectorType>(); (void)qkvw_max;
  pir::VectorType qkv_bias = qkv_bias_.type().dyn_cast<pir::VectorType>(); (void)qkv_bias;
  pir::VectorType out_linear_w = out_linear_w_.type().dyn_cast<pir::VectorType>(); (void)out_linear_w;
  pir::VectorType out_linear_wmax = out_linear_wmax_.type().dyn_cast<pir::VectorType>(); (void)out_linear_wmax;
  pir::VectorType out_linear_bias = out_linear_bias_.type().dyn_cast<pir::VectorType>(); (void)out_linear_bias;
  pir::VectorType ffn_ln_scale = ffn_ln_scale_.type().dyn_cast<pir::VectorType>(); (void)ffn_ln_scale;
  pir::VectorType ffn_ln_bias = ffn_ln_bias_.type().dyn_cast<pir::VectorType>(); (void)ffn_ln_bias;
  pir::VectorType ffn1_weight = ffn1_weight_.type().dyn_cast<pir::VectorType>(); (void)ffn1_weight;
  pir::VectorType ffn1_weight_max = ffn1_weight_max_.type().dyn_cast<pir::VectorType>(); (void)ffn1_weight_max;
  pir::VectorType ffn1_bias = ffn1_bias_.type().dyn_cast<pir::VectorType>(); (void)ffn1_bias;
  pir::VectorType ffn2_weight = ffn2_weight_.type().dyn_cast<pir::VectorType>(); (void)ffn2_weight;
  pir::VectorType ffn2_weight_max = ffn2_weight_max_.type().dyn_cast<pir::VectorType>(); (void)ffn2_weight_max;
  pir::VectorType ffn2_bias = ffn2_bias_.type().dyn_cast<pir::VectorType>(); (void)ffn2_bias;
  paddle::dialect::DenseTensorType max_buffer = max_buffer_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)max_buffer;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);
  std::vector<paddle::dialect::IrTensor> vec_ir_tensor_ln_scale;
  for (size_t i=0; i < static_cast<size_t>(ln_scale.size()); i++) {
    vec_ir_tensor_ln_scale.push_back(paddle::dialect::IrTensor(paddle::dialect::TransToPhiDataType(ln_scale[i].dyn_cast<paddle::dialect::DenseTensorType>().dtype()),
                                                                     ln_scale[i].dyn_cast<paddle::dialect::DenseTensorType>().dims(),
                                                                     ln_scale[i].dyn_cast<paddle::dialect::DenseTensorType>().data_layout(),
                                                                     ln_scale[i].dyn_cast<paddle::dialect::DenseTensorType>().lod(),
                                                                     ln_scale[i].dyn_cast<paddle::dialect::DenseTensorType>().offset()));
  }
  std::vector<paddle::dialect::IrMetaTensor> vec_meta_ln_scale;
  for (size_t i=0; i < vec_ir_tensor_ln_scale.size(); i++) {
    vec_meta_ln_scale.push_back(paddle::dialect::IrMetaTensor(&vec_ir_tensor_ln_scale[i]));
  }

  std::vector<const phi::MetaTensor*> meta_ln_scale;
  for (size_t i=0; i < static_cast<size_t>(vec_meta_ln_scale.size()); i++) {
    meta_ln_scale.push_back(&vec_meta_ln_scale[i]);
  }
   std::vector<paddle::dialect::IrTensor> vec_ir_tensor_ln_bias;
  for (size_t i=0; i < static_cast<size_t>(ln_bias.size()); i++) {
    vec_ir_tensor_ln_bias.push_back(paddle::dialect::IrTensor(paddle::dialect::TransToPhiDataType(ln_bias[i].dyn_cast<paddle::dialect::DenseTensorType>().dtype()),
                                                                     ln_bias[i].dyn_cast<paddle::dialect::DenseTensorType>().dims(),
                                                                     ln_bias[i].dyn_cast<paddle::dialect::DenseTensorType>().data_layout(),
                                                                     ln_bias[i].dyn_cast<paddle::dialect::DenseTensorType>().lod(),
                                                                     ln_bias[i].dyn_cast<paddle::dialect::DenseTensorType>().offset()));
  }
  std::vector<paddle::dialect::IrMetaTensor> vec_meta_ln_bias;
  for (size_t i=0; i < vec_ir_tensor_ln_bias.size(); i++) {
    vec_meta_ln_bias.push_back(paddle::dialect::IrMetaTensor(&vec_ir_tensor_ln_bias[i]));
  }

  std::vector<const phi::MetaTensor*> meta_ln_bias;
  for (size_t i=0; i < static_cast<size_t>(vec_meta_ln_bias.size()); i++) {
    meta_ln_bias.push_back(&vec_meta_ln_bias[i]);
  }
   std::vector<paddle::dialect::IrTensor> vec_ir_tensor_qkvw;
  for (size_t i=0; i < static_cast<size_t>(qkvw.size()); i++) {
    vec_ir_tensor_qkvw.push_back(paddle::dialect::IrTensor(paddle::dialect::TransToPhiDataType(qkvw[i].dyn_cast<paddle::dialect::DenseTensorType>().dtype()),
                                                                     qkvw[i].dyn_cast<paddle::dialect::DenseTensorType>().dims(),
                                                                     qkvw[i].dyn_cast<paddle::dialect::DenseTensorType>().data_layout(),
                                                                     qkvw[i].dyn_cast<paddle::dialect::DenseTensorType>().lod(),
                                                                     qkvw[i].dyn_cast<paddle::dialect::DenseTensorType>().offset()));
  }
  std::vector<paddle::dialect::IrMetaTensor> vec_meta_qkvw;
  for (size_t i=0; i < vec_ir_tensor_qkvw.size(); i++) {
    vec_meta_qkvw.push_back(paddle::dialect::IrMetaTensor(&vec_ir_tensor_qkvw[i]));
  }

  std::vector<const phi::MetaTensor*> meta_qkvw;
  for (size_t i=0; i < static_cast<size_t>(vec_meta_qkvw.size()); i++) {
    meta_qkvw.push_back(&vec_meta_qkvw[i]);
  }
   std::vector<paddle::dialect::IrTensor> vec_ir_tensor_qkvw_max;
  for (size_t i=0; i < static_cast<size_t>(qkvw_max.size()); i++) {
    vec_ir_tensor_qkvw_max.push_back(paddle::dialect::IrTensor(paddle::dialect::TransToPhiDataType(qkvw_max[i].dyn_cast<paddle::dialect::DenseTensorType>().dtype()),
                                                                     qkvw_max[i].dyn_cast<paddle::dialect::DenseTensorType>().dims(),
                                                                     qkvw_max[i].dyn_cast<paddle::dialect::DenseTensorType>().data_layout(),
                                                                     qkvw_max[i].dyn_cast<paddle::dialect::DenseTensorType>().lod(),
                                                                     qkvw_max[i].dyn_cast<paddle::dialect::DenseTensorType>().offset()));
  }
  std::vector<paddle::dialect::IrMetaTensor> vec_meta_qkvw_max;
  for (size_t i=0; i < vec_ir_tensor_qkvw_max.size(); i++) {
    vec_meta_qkvw_max.push_back(paddle::dialect::IrMetaTensor(&vec_ir_tensor_qkvw_max[i]));
  }

  std::vector<const phi::MetaTensor*> meta_qkvw_max;
  for (size_t i=0; i < static_cast<size_t>(vec_meta_qkvw_max.size()); i++) {
    meta_qkvw_max.push_back(&vec_meta_qkvw_max[i]);
  }
   std::vector<paddle::dialect::IrTensor> vec_ir_tensor_qkv_bias;
  for (size_t i=0; i < static_cast<size_t>(qkv_bias.size()); i++) {
    vec_ir_tensor_qkv_bias.push_back(paddle::dialect::IrTensor(paddle::dialect::TransToPhiDataType(qkv_bias[i].dyn_cast<paddle::dialect::DenseTensorType>().dtype()),
                                                                     qkv_bias[i].dyn_cast<paddle::dialect::DenseTensorType>().dims(),
                                                                     qkv_bias[i].dyn_cast<paddle::dialect::DenseTensorType>().data_layout(),
                                                                     qkv_bias[i].dyn_cast<paddle::dialect::DenseTensorType>().lod(),
                                                                     qkv_bias[i].dyn_cast<paddle::dialect::DenseTensorType>().offset()));
  }
  std::vector<paddle::dialect::IrMetaTensor> vec_meta_qkv_bias;
  for (size_t i=0; i < vec_ir_tensor_qkv_bias.size(); i++) {
    vec_meta_qkv_bias.push_back(paddle::dialect::IrMetaTensor(&vec_ir_tensor_qkv_bias[i]));
  }

  std::vector<const phi::MetaTensor*> meta_qkv_bias;
  for (size_t i=0; i < static_cast<size_t>(vec_meta_qkv_bias.size()); i++) {
    meta_qkv_bias.push_back(&vec_meta_qkv_bias[i]);
  }
   std::vector<paddle::dialect::IrTensor> vec_ir_tensor_out_linear_w;
  for (size_t i=0; i < static_cast<size_t>(out_linear_w.size()); i++) {
    vec_ir_tensor_out_linear_w.push_back(paddle::dialect::IrTensor(paddle::dialect::TransToPhiDataType(out_linear_w[i].dyn_cast<paddle::dialect::DenseTensorType>().dtype()),
                                                                     out_linear_w[i].dyn_cast<paddle::dialect::DenseTensorType>().dims(),
                                                                     out_linear_w[i].dyn_cast<paddle::dialect::DenseTensorType>().data_layout(),
                                                                     out_linear_w[i].dyn_cast<paddle::dialect::DenseTensorType>().lod(),
                                                                     out_linear_w[i].dyn_cast<paddle::dialect::DenseTensorType>().offset()));
  }
  std::vector<paddle::dialect::IrMetaTensor> vec_meta_out_linear_w;
  for (size_t i=0; i < vec_ir_tensor_out_linear_w.size(); i++) {
    vec_meta_out_linear_w.push_back(paddle::dialect::IrMetaTensor(&vec_ir_tensor_out_linear_w[i]));
  }

  std::vector<const phi::MetaTensor*> meta_out_linear_w;
  for (size_t i=0; i < static_cast<size_t>(vec_meta_out_linear_w.size()); i++) {
    meta_out_linear_w.push_back(&vec_meta_out_linear_w[i]);
  }
   std::vector<paddle::dialect::IrTensor> vec_ir_tensor_out_linear_wmax;
  for (size_t i=0; i < static_cast<size_t>(out_linear_wmax.size()); i++) {
    vec_ir_tensor_out_linear_wmax.push_back(paddle::dialect::IrTensor(paddle::dialect::TransToPhiDataType(out_linear_wmax[i].dyn_cast<paddle::dialect::DenseTensorType>().dtype()),
                                                                     out_linear_wmax[i].dyn_cast<paddle::dialect::DenseTensorType>().dims(),
                                                                     out_linear_wmax[i].dyn_cast<paddle::dialect::DenseTensorType>().data_layout(),
                                                                     out_linear_wmax[i].dyn_cast<paddle::dialect::DenseTensorType>().lod(),
                                                                     out_linear_wmax[i].dyn_cast<paddle::dialect::DenseTensorType>().offset()));
  }
  std::vector<paddle::dialect::IrMetaTensor> vec_meta_out_linear_wmax;
  for (size_t i=0; i < vec_ir_tensor_out_linear_wmax.size(); i++) {
    vec_meta_out_linear_wmax.push_back(paddle::dialect::IrMetaTensor(&vec_ir_tensor_out_linear_wmax[i]));
  }

  std::vector<const phi::MetaTensor*> meta_out_linear_wmax;
  for (size_t i=0; i < static_cast<size_t>(vec_meta_out_linear_wmax.size()); i++) {
    meta_out_linear_wmax.push_back(&vec_meta_out_linear_wmax[i]);
  }
   std::vector<paddle::dialect::IrTensor> vec_ir_tensor_out_linear_bias;
  for (size_t i=0; i < static_cast<size_t>(out_linear_bias.size()); i++) {
    vec_ir_tensor_out_linear_bias.push_back(paddle::dialect::IrTensor(paddle::dialect::TransToPhiDataType(out_linear_bias[i].dyn_cast<paddle::dialect::DenseTensorType>().dtype()),
                                                                     out_linear_bias[i].dyn_cast<paddle::dialect::DenseTensorType>().dims(),
                                                                     out_linear_bias[i].dyn_cast<paddle::dialect::DenseTensorType>().data_layout(),
                                                                     out_linear_bias[i].dyn_cast<paddle::dialect::DenseTensorType>().lod(),
                                                                     out_linear_bias[i].dyn_cast<paddle::dialect::DenseTensorType>().offset()));
  }
  std::vector<paddle::dialect::IrMetaTensor> vec_meta_out_linear_bias;
  for (size_t i=0; i < vec_ir_tensor_out_linear_bias.size(); i++) {
    vec_meta_out_linear_bias.push_back(paddle::dialect::IrMetaTensor(&vec_ir_tensor_out_linear_bias[i]));
  }

  std::vector<const phi::MetaTensor*> meta_out_linear_bias;
  for (size_t i=0; i < static_cast<size_t>(vec_meta_out_linear_bias.size()); i++) {
    meta_out_linear_bias.push_back(&vec_meta_out_linear_bias[i]);
  }
   std::vector<paddle::dialect::IrTensor> vec_ir_tensor_ffn_ln_scale;
  for (size_t i=0; i < static_cast<size_t>(ffn_ln_scale.size()); i++) {
    vec_ir_tensor_ffn_ln_scale.push_back(paddle::dialect::IrTensor(paddle::dialect::TransToPhiDataType(ffn_ln_scale[i].dyn_cast<paddle::dialect::DenseTensorType>().dtype()),
                                                                     ffn_ln_scale[i].dyn_cast<paddle::dialect::DenseTensorType>().dims(),
                                                                     ffn_ln_scale[i].dyn_cast<paddle::dialect::DenseTensorType>().data_layout(),
                                                                     ffn_ln_scale[i].dyn_cast<paddle::dialect::DenseTensorType>().lod(),
                                                                     ffn_ln_scale[i].dyn_cast<paddle::dialect::DenseTensorType>().offset()));
  }
  std::vector<paddle::dialect::IrMetaTensor> vec_meta_ffn_ln_scale;
  for (size_t i=0; i < vec_ir_tensor_ffn_ln_scale.size(); i++) {
    vec_meta_ffn_ln_scale.push_back(paddle::dialect::IrMetaTensor(&vec_ir_tensor_ffn_ln_scale[i]));
  }

  std::vector<const phi::MetaTensor*> meta_ffn_ln_scale;
  for (size_t i=0; i < static_cast<size_t>(vec_meta_ffn_ln_scale.size()); i++) {
    meta_ffn_ln_scale.push_back(&vec_meta_ffn_ln_scale[i]);
  }
   std::vector<paddle::dialect::IrTensor> vec_ir_tensor_ffn_ln_bias;
  for (size_t i=0; i < static_cast<size_t>(ffn_ln_bias.size()); i++) {
    vec_ir_tensor_ffn_ln_bias.push_back(paddle::dialect::IrTensor(paddle::dialect::TransToPhiDataType(ffn_ln_bias[i].dyn_cast<paddle::dialect::DenseTensorType>().dtype()),
                                                                     ffn_ln_bias[i].dyn_cast<paddle::dialect::DenseTensorType>().dims(),
                                                                     ffn_ln_bias[i].dyn_cast<paddle::dialect::DenseTensorType>().data_layout(),
                                                                     ffn_ln_bias[i].dyn_cast<paddle::dialect::DenseTensorType>().lod(),
                                                                     ffn_ln_bias[i].dyn_cast<paddle::dialect::DenseTensorType>().offset()));
  }
  std::vector<paddle::dialect::IrMetaTensor> vec_meta_ffn_ln_bias;
  for (size_t i=0; i < vec_ir_tensor_ffn_ln_bias.size(); i++) {
    vec_meta_ffn_ln_bias.push_back(paddle::dialect::IrMetaTensor(&vec_ir_tensor_ffn_ln_bias[i]));
  }

  std::vector<const phi::MetaTensor*> meta_ffn_ln_bias;
  for (size_t i=0; i < static_cast<size_t>(vec_meta_ffn_ln_bias.size()); i++) {
    meta_ffn_ln_bias.push_back(&vec_meta_ffn_ln_bias[i]);
  }
   std::vector<paddle::dialect::IrTensor> vec_ir_tensor_ffn1_weight;
  for (size_t i=0; i < static_cast<size_t>(ffn1_weight.size()); i++) {
    vec_ir_tensor_ffn1_weight.push_back(paddle::dialect::IrTensor(paddle::dialect::TransToPhiDataType(ffn1_weight[i].dyn_cast<paddle::dialect::DenseTensorType>().dtype()),
                                                                     ffn1_weight[i].dyn_cast<paddle::dialect::DenseTensorType>().dims(),
                                                                     ffn1_weight[i].dyn_cast<paddle::dialect::DenseTensorType>().data_layout(),
                                                                     ffn1_weight[i].dyn_cast<paddle::dialect::DenseTensorType>().lod(),
                                                                     ffn1_weight[i].dyn_cast<paddle::dialect::DenseTensorType>().offset()));
  }
  std::vector<paddle::dialect::IrMetaTensor> vec_meta_ffn1_weight;
  for (size_t i=0; i < vec_ir_tensor_ffn1_weight.size(); i++) {
    vec_meta_ffn1_weight.push_back(paddle::dialect::IrMetaTensor(&vec_ir_tensor_ffn1_weight[i]));
  }

  std::vector<const phi::MetaTensor*> meta_ffn1_weight;
  for (size_t i=0; i < static_cast<size_t>(vec_meta_ffn1_weight.size()); i++) {
    meta_ffn1_weight.push_back(&vec_meta_ffn1_weight[i]);
  }
   std::vector<paddle::dialect::IrTensor> vec_ir_tensor_ffn1_weight_max;
  for (size_t i=0; i < static_cast<size_t>(ffn1_weight_max.size()); i++) {
    vec_ir_tensor_ffn1_weight_max.push_back(paddle::dialect::IrTensor(paddle::dialect::TransToPhiDataType(ffn1_weight_max[i].dyn_cast<paddle::dialect::DenseTensorType>().dtype()),
                                                                     ffn1_weight_max[i].dyn_cast<paddle::dialect::DenseTensorType>().dims(),
                                                                     ffn1_weight_max[i].dyn_cast<paddle::dialect::DenseTensorType>().data_layout(),
                                                                     ffn1_weight_max[i].dyn_cast<paddle::dialect::DenseTensorType>().lod(),
                                                                     ffn1_weight_max[i].dyn_cast<paddle::dialect::DenseTensorType>().offset()));
  }
  std::vector<paddle::dialect::IrMetaTensor> vec_meta_ffn1_weight_max;
  for (size_t i=0; i < vec_ir_tensor_ffn1_weight_max.size(); i++) {
    vec_meta_ffn1_weight_max.push_back(paddle::dialect::IrMetaTensor(&vec_ir_tensor_ffn1_weight_max[i]));
  }

  std::vector<const phi::MetaTensor*> meta_ffn1_weight_max;
  for (size_t i=0; i < static_cast<size_t>(vec_meta_ffn1_weight_max.size()); i++) {
    meta_ffn1_weight_max.push_back(&vec_meta_ffn1_weight_max[i]);
  }
   std::vector<paddle::dialect::IrTensor> vec_ir_tensor_ffn1_bias;
  for (size_t i=0; i < static_cast<size_t>(ffn1_bias.size()); i++) {
    vec_ir_tensor_ffn1_bias.push_back(paddle::dialect::IrTensor(paddle::dialect::TransToPhiDataType(ffn1_bias[i].dyn_cast<paddle::dialect::DenseTensorType>().dtype()),
                                                                     ffn1_bias[i].dyn_cast<paddle::dialect::DenseTensorType>().dims(),
                                                                     ffn1_bias[i].dyn_cast<paddle::dialect::DenseTensorType>().data_layout(),
                                                                     ffn1_bias[i].dyn_cast<paddle::dialect::DenseTensorType>().lod(),
                                                                     ffn1_bias[i].dyn_cast<paddle::dialect::DenseTensorType>().offset()));
  }
  std::vector<paddle::dialect::IrMetaTensor> vec_meta_ffn1_bias;
  for (size_t i=0; i < vec_ir_tensor_ffn1_bias.size(); i++) {
    vec_meta_ffn1_bias.push_back(paddle::dialect::IrMetaTensor(&vec_ir_tensor_ffn1_bias[i]));
  }

  std::vector<const phi::MetaTensor*> meta_ffn1_bias;
  for (size_t i=0; i < static_cast<size_t>(vec_meta_ffn1_bias.size()); i++) {
    meta_ffn1_bias.push_back(&vec_meta_ffn1_bias[i]);
  }
   std::vector<paddle::dialect::IrTensor> vec_ir_tensor_ffn2_weight;
  for (size_t i=0; i < static_cast<size_t>(ffn2_weight.size()); i++) {
    vec_ir_tensor_ffn2_weight.push_back(paddle::dialect::IrTensor(paddle::dialect::TransToPhiDataType(ffn2_weight[i].dyn_cast<paddle::dialect::DenseTensorType>().dtype()),
                                                                     ffn2_weight[i].dyn_cast<paddle::dialect::DenseTensorType>().dims(),
                                                                     ffn2_weight[i].dyn_cast<paddle::dialect::DenseTensorType>().data_layout(),
                                                                     ffn2_weight[i].dyn_cast<paddle::dialect::DenseTensorType>().lod(),
                                                                     ffn2_weight[i].dyn_cast<paddle::dialect::DenseTensorType>().offset()));
  }
  std::vector<paddle::dialect::IrMetaTensor> vec_meta_ffn2_weight;
  for (size_t i=0; i < vec_ir_tensor_ffn2_weight.size(); i++) {
    vec_meta_ffn2_weight.push_back(paddle::dialect::IrMetaTensor(&vec_ir_tensor_ffn2_weight[i]));
  }

  std::vector<const phi::MetaTensor*> meta_ffn2_weight;
  for (size_t i=0; i < static_cast<size_t>(vec_meta_ffn2_weight.size()); i++) {
    meta_ffn2_weight.push_back(&vec_meta_ffn2_weight[i]);
  }
   std::vector<paddle::dialect::IrTensor> vec_ir_tensor_ffn2_weight_max;
  for (size_t i=0; i < static_cast<size_t>(ffn2_weight_max.size()); i++) {
    vec_ir_tensor_ffn2_weight_max.push_back(paddle::dialect::IrTensor(paddle::dialect::TransToPhiDataType(ffn2_weight_max[i].dyn_cast<paddle::dialect::DenseTensorType>().dtype()),
                                                                     ffn2_weight_max[i].dyn_cast<paddle::dialect::DenseTensorType>().dims(),
                                                                     ffn2_weight_max[i].dyn_cast<paddle::dialect::DenseTensorType>().data_layout(),
                                                                     ffn2_weight_max[i].dyn_cast<paddle::dialect::DenseTensorType>().lod(),
                                                                     ffn2_weight_max[i].dyn_cast<paddle::dialect::DenseTensorType>().offset()));
  }
  std::vector<paddle::dialect::IrMetaTensor> vec_meta_ffn2_weight_max;
  for (size_t i=0; i < vec_ir_tensor_ffn2_weight_max.size(); i++) {
    vec_meta_ffn2_weight_max.push_back(paddle::dialect::IrMetaTensor(&vec_ir_tensor_ffn2_weight_max[i]));
  }

  std::vector<const phi::MetaTensor*> meta_ffn2_weight_max;
  for (size_t i=0; i < static_cast<size_t>(vec_meta_ffn2_weight_max.size()); i++) {
    meta_ffn2_weight_max.push_back(&vec_meta_ffn2_weight_max[i]);
  }
   std::vector<paddle::dialect::IrTensor> vec_ir_tensor_ffn2_bias;
  for (size_t i=0; i < static_cast<size_t>(ffn2_bias.size()); i++) {
    vec_ir_tensor_ffn2_bias.push_back(paddle::dialect::IrTensor(paddle::dialect::TransToPhiDataType(ffn2_bias[i].dyn_cast<paddle::dialect::DenseTensorType>().dtype()),
                                                                     ffn2_bias[i].dyn_cast<paddle::dialect::DenseTensorType>().dims(),
                                                                     ffn2_bias[i].dyn_cast<paddle::dialect::DenseTensorType>().data_layout(),
                                                                     ffn2_bias[i].dyn_cast<paddle::dialect::DenseTensorType>().lod(),
                                                                     ffn2_bias[i].dyn_cast<paddle::dialect::DenseTensorType>().offset()));
  }
  std::vector<paddle::dialect::IrMetaTensor> vec_meta_ffn2_bias;
  for (size_t i=0; i < vec_ir_tensor_ffn2_bias.size(); i++) {
    vec_meta_ffn2_bias.push_back(paddle::dialect::IrMetaTensor(&vec_ir_tensor_ffn2_bias[i]));
  }

  std::vector<const phi::MetaTensor*> meta_ffn2_bias;
  for (size_t i=0; i < static_cast<size_t>(vec_meta_ffn2_bias.size()); i++) {
    meta_ffn2_bias.push_back(&vec_meta_ffn2_bias[i]);
  }
   std::vector<paddle::dialect::IrTensor> vec_ir_tensor_cache_kv;
  if (cache_kv_.impl() != nullptr) {
    pir::VectorType cache_kv = cache_kv_.type().dyn_cast<pir::VectorType>();
    for (size_t i=0; i < static_cast<size_t>(cache_kv.size()); i++) {
        vec_ir_tensor_cache_kv.push_back(paddle::dialect::IrTensor(paddle::dialect::TransToPhiDataType(cache_kv[i].dyn_cast<paddle::dialect::DenseTensorType>().dtype()),
                                                                        cache_kv[i].dyn_cast<paddle::dialect::DenseTensorType>().dims(),
                                                                        cache_kv[i].dyn_cast<paddle::dialect::DenseTensorType>().data_layout(),
                                                                        cache_kv[i].dyn_cast<paddle::dialect::DenseTensorType>().lod(),
                                                                        cache_kv[i].dyn_cast<paddle::dialect::DenseTensorType>().offset()));
    }
  }

  std::vector<paddle::dialect::IrMetaTensor> vec_meta_cache_kv;
  for (size_t i=0; i < vec_ir_tensor_cache_kv.size(); i++) {
    vec_meta_cache_kv.push_back(paddle::dialect::IrMetaTensor(&vec_ir_tensor_cache_kv[i]));
  }

  std::vector<const phi::MetaTensor*> meta_cache_kv;
  for (size_t i=0; i < static_cast<size_t>(vec_meta_cache_kv.size()); i++) {
    meta_cache_kv.push_back(&vec_meta_cache_kv[i]);
  }

  std::vector<paddle::dialect::IrTensor> vec_ir_tensor_pre_caches;
  if (pre_caches_.impl() != nullptr) {
    pir::VectorType pre_caches = pre_caches_.type().dyn_cast<pir::VectorType>();
    for (size_t i=0; i < static_cast<size_t>(pre_caches.size()); i++) {
        vec_ir_tensor_pre_caches.push_back(paddle::dialect::IrTensor(paddle::dialect::TransToPhiDataType(pre_caches[i].dyn_cast<paddle::dialect::DenseTensorType>().dtype()),
                                                                        pre_caches[i].dyn_cast<paddle::dialect::DenseTensorType>().dims(),
                                                                        pre_caches[i].dyn_cast<paddle::dialect::DenseTensorType>().data_layout(),
                                                                        pre_caches[i].dyn_cast<paddle::dialect::DenseTensorType>().lod(),
                                                                        pre_caches[i].dyn_cast<paddle::dialect::DenseTensorType>().offset()));
    }
  }

  std::vector<paddle::dialect::IrMetaTensor> vec_meta_pre_caches;
  for (size_t i=0; i < vec_ir_tensor_pre_caches.size(); i++) {
    vec_meta_pre_caches.push_back(paddle::dialect::IrMetaTensor(&vec_ir_tensor_pre_caches[i]));
  }

  std::vector<const phi::MetaTensor*> meta_pre_caches;
  for (size_t i=0; i < static_cast<size_t>(vec_meta_pre_caches.size()); i++) {
    meta_pre_caches.push_back(&vec_meta_pre_caches[i]);
  }


  paddle::dialect::IrMetaTensor meta_rotary_pos_emb;
  paddle::dialect::IrTensor ir_tensor_rotary_pos_emb;
  if (rotary_pos_emb_.impl() != nullptr) {
    paddle::dialect::DenseTensorType rotary_pos_emb = rotary_pos_emb_.type().dyn_cast<paddle::dialect::DenseTensorType>();
    VLOG(4) << "Builder construction  dense_rotary_pos_emb";
    ir_tensor_rotary_pos_emb = paddle::dialect::IrTensor(paddle::dialect::TransToPhiDataType(rotary_pos_emb.dtype()),
                                                        rotary_pos_emb.dims(),
                                                        rotary_pos_emb.data_layout(),
                                                        rotary_pos_emb.lod(),
                                                        rotary_pos_emb.offset());
    VLOG(4) << "Builder construction  meta_rotary_pos_emb";
    meta_rotary_pos_emb = paddle::dialect::IrMetaTensor(&ir_tensor_rotary_pos_emb);
  }


  paddle::dialect::IrMetaTensor meta_time_step;
  paddle::dialect::IrTensor ir_tensor_time_step;
  if (time_step_.impl() != nullptr) {
    paddle::dialect::DenseTensorType time_step = time_step_.type().dyn_cast<paddle::dialect::DenseTensorType>();
    VLOG(4) << "Builder construction  dense_time_step";
    ir_tensor_time_step = paddle::dialect::IrTensor(paddle::dialect::TransToPhiDataType(time_step.dtype()),
                                                        time_step.dims(),
                                                        time_step.data_layout(),
                                                        time_step.lod(),
                                                        time_step.offset());
    VLOG(4) << "Builder construction  meta_time_step";
    meta_time_step = paddle::dialect::IrMetaTensor(&ir_tensor_time_step);
  }


  paddle::dialect::IrMetaTensor meta_seq_lengths;
  paddle::dialect::IrTensor ir_tensor_seq_lengths;
  if (seq_lengths_.impl() != nullptr) {
    paddle::dialect::DenseTensorType seq_lengths = seq_lengths_.type().dyn_cast<paddle::dialect::DenseTensorType>();
    VLOG(4) << "Builder construction  dense_seq_lengths";
    ir_tensor_seq_lengths = paddle::dialect::IrTensor(paddle::dialect::TransToPhiDataType(seq_lengths.dtype()),
                                                        seq_lengths.dims(),
                                                        seq_lengths.data_layout(),
                                                        seq_lengths.lod(),
                                                        seq_lengths.offset());
    VLOG(4) << "Builder construction  meta_seq_lengths";
    meta_seq_lengths = paddle::dialect::IrMetaTensor(&ir_tensor_seq_lengths);
  }


  paddle::dialect::IrMetaTensor meta_src_mask;
  paddle::dialect::IrTensor ir_tensor_src_mask;
  if (src_mask_.impl() != nullptr) {
    paddle::dialect::DenseTensorType src_mask = src_mask_.type().dyn_cast<paddle::dialect::DenseTensorType>();
    VLOG(4) << "Builder construction  dense_src_mask";
    ir_tensor_src_mask = paddle::dialect::IrTensor(paddle::dialect::TransToPhiDataType(src_mask.dtype()),
                                                        src_mask.dims(),
                                                        src_mask.data_layout(),
                                                        src_mask.lod(),
                                                        src_mask.offset());
    VLOG(4) << "Builder construction  meta_src_mask";
    meta_src_mask = paddle::dialect::IrMetaTensor(&ir_tensor_src_mask);
  }


  paddle::dialect::IrMetaTensor meta_gather_index;
  paddle::dialect::IrTensor ir_tensor_gather_index;
  if (gather_index_.impl() != nullptr) {
    paddle::dialect::DenseTensorType gather_index = gather_index_.type().dyn_cast<paddle::dialect::DenseTensorType>();
    VLOG(4) << "Builder construction  dense_gather_index";
    ir_tensor_gather_index = paddle::dialect::IrTensor(paddle::dialect::TransToPhiDataType(gather_index.dtype()),
                                                        gather_index.dims(),
                                                        gather_index.data_layout(),
                                                        gather_index.lod(),
                                                        gather_index.offset());
    VLOG(4) << "Builder construction  meta_gather_index";
    meta_gather_index = paddle::dialect::IrMetaTensor(&ir_tensor_gather_index);
  }


  VLOG(4) << "Builder construction  dense_max_buffer";
  paddle::dialect::IrTensor ir_tensor_max_buffer(paddle::dialect::TransToPhiDataType(max_buffer.dtype()),
                                                      max_buffer.dims(),
                                                      max_buffer.data_layout(),
                                                      max_buffer.lod(),
                                                      max_buffer.offset());
  VLOG(4) << "Builder construction  meta_max_buffer";
  paddle::dialect::IrMetaTensor meta_max_buffer(&ir_tensor_max_buffer);
  paddle::dialect::IrTensor dense_out;
  paddle::dialect::IrMetaTensor meta_out(&dense_out);
  std::vector<paddle::dialect::IrTensor> vec_dense_cache_kv_out((out_linear_w.size()), paddle::dialect::IrTensor());
  std::vector<paddle::dialect::IrMetaTensor> vec_meta_cache_kv_out;
  for (size_t i=0; i < static_cast<size_t>(out_linear_w.size()); i++) {
    vec_meta_cache_kv_out.push_back(paddle::dialect::IrMetaTensor(&vec_dense_cache_kv_out[i]));
  }
  std::vector<phi::MetaTensor*> meta_cache_kv_out;
  for (size_t i=0; i < static_cast<size_t>(vec_meta_cache_kv_out.size()); i++) {
    meta_cache_kv_out.push_back(&vec_meta_cache_kv_out[i]);
  }

  phi::FusedMultiTransformerXpuInferMeta(meta_x, meta_ln_scale, meta_ln_bias, meta_qkvw, meta_qkvw_max, meta_qkv_bias, meta_out_linear_w, meta_out_linear_wmax, meta_out_linear_bias, meta_ffn_ln_scale, meta_ffn_ln_bias, meta_ffn1_weight, meta_ffn1_weight_max, meta_ffn1_bias, meta_ffn2_weight, meta_ffn2_weight_max, meta_ffn2_bias, meta_cache_kv, meta_pre_caches, meta_rotary_pos_emb, meta_time_step, meta_seq_lengths, meta_src_mask, meta_gather_index, meta_max_buffer, pre_layer_norm, rotary_emb_dims, epsilon, dropout_rate, is_test, dropout_implementation, act_method, trans_qkvw, ring_id, gather_axis, &meta_out, meta_cache_kv_out);

  std::vector<pir::Type> argument_outputs;
  pir::Type out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_out.dtype()), dense_out.dims(), dense_out.layout(), dense_out.lod(), dense_out.offset());
  argument_outputs.push_back(out_dense_tensor_type);

  std::vector<pir::Type> cache_kv_out_types;
  for (size_t i=0; i < static_cast<size_t>(out_linear_w.size()); i++) {
    cache_kv_out_types.push_back(paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(vec_dense_cache_kv_out[i].dtype()), vec_dense_cache_kv_out[i].dims(), vec_dense_cache_kv_out[i].layout(), vec_dense_cache_kv_out[i].lod(), vec_dense_cache_kv_out[i].offset()));
  }
  pir::Type cache_kv_out_vector_type = pir::VectorType::get(pir::IrContext::Instance(), cache_kv_out_types);
  argument_outputs.push_back(cache_kv_out_vector_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void FusedMultiTransformerXpuOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, pir::Value ln_scale_, pir::Value ln_bias_, pir::Value qkvw_, pir::Value qkvw_max_, pir::Value qkv_bias_, pir::Value out_linear_w_, pir::Value out_linear_wmax_, pir::Value out_linear_bias_, pir::Value ffn_ln_scale_, pir::Value ffn_ln_bias_, pir::Value ffn1_weight_, pir::Value ffn1_weight_max_, pir::Value ffn1_bias_, pir::Value ffn2_weight_, pir::Value ffn2_weight_max_, pir::Value ffn2_bias_, pir::Value cache_kv_, pir::Value pre_caches_, pir::Value rotary_pos_emb_, pir::Value time_step_, pir::Value seq_lengths_, pir::Value src_mask_, pir::Value gather_index_, pir::Value max_buffer_, pir::AttributeMap attributes) {
  VLOG(4) << "Start build FusedMultiTransformerXpuOp";


  IR_ENFORCE(
      attributes.find("pre_layer_norm") != attributes.end(),
          "'pre_layer_norm' Attribute is expected for FusedMultiTransformerXpuOp. ");
  bool pre_layer_norm = attributes.at("pre_layer_norm").dyn_cast<pir::BoolAttribute>().data();

  IR_ENFORCE(
      attributes.find("rotary_emb_dims") != attributes.end(),
          "'rotary_emb_dims' Attribute is expected for FusedMultiTransformerXpuOp. ");
  int rotary_emb_dims = attributes.at("rotary_emb_dims").dyn_cast<pir::Int32Attribute>().data();

  IR_ENFORCE(
      attributes.find("epsilon") != attributes.end(),
          "'epsilon' Attribute is expected for FusedMultiTransformerXpuOp. ");
  float epsilon = attributes.at("epsilon").dyn_cast<pir::FloatAttribute>().data();

  IR_ENFORCE(
      attributes.find("dropout_rate") != attributes.end(),
          "'dropout_rate' Attribute is expected for FusedMultiTransformerXpuOp. ");
  float dropout_rate = attributes.at("dropout_rate").dyn_cast<pir::FloatAttribute>().data();

  IR_ENFORCE(
      attributes.find("is_test") != attributes.end(),
          "'is_test' Attribute is expected for FusedMultiTransformerXpuOp. ");
  bool is_test = attributes.at("is_test").dyn_cast<pir::BoolAttribute>().data();

  IR_ENFORCE(
      attributes.find("dropout_implementation") != attributes.end(),
          "'dropout_implementation' Attribute is expected for FusedMultiTransformerXpuOp. ");
  std::string dropout_implementation = attributes.at("dropout_implementation").dyn_cast<pir::StrAttribute>().AsString();

  IR_ENFORCE(
      attributes.find("act_method") != attributes.end(),
          "'act_method' Attribute is expected for FusedMultiTransformerXpuOp. ");
  std::string act_method = attributes.at("act_method").dyn_cast<pir::StrAttribute>().AsString();

  IR_ENFORCE(
      attributes.find("trans_qkvw") != attributes.end(),
          "'trans_qkvw' Attribute is expected for FusedMultiTransformerXpuOp. ");
  bool trans_qkvw = attributes.at("trans_qkvw").dyn_cast<pir::BoolAttribute>().data();

  IR_ENFORCE(
      attributes.find("ring_id") != attributes.end(),
          "'ring_id' Attribute is expected for FusedMultiTransformerXpuOp. ");
  int ring_id = attributes.at("ring_id").dyn_cast<pir::Int32Attribute>().data();

  IR_ENFORCE(
      attributes.find("gather_axis") != attributes.end(),
          "'gather_axis' Attribute is expected for FusedMultiTransformerXpuOp. ");
  int gather_axis = attributes.at("gather_axis").dyn_cast<pir::Int32Attribute>().data();


  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_, ln_scale_, ln_bias_, qkvw_, qkvw_max_, qkv_bias_, out_linear_w_, out_linear_wmax_, out_linear_bias_, ffn_ln_scale_, ffn_ln_bias_, ffn1_weight_, ffn1_weight_max_, ffn1_bias_, ffn2_weight_, ffn2_weight_max_, ffn2_bias_, cache_kv_, pre_caches_, rotary_pos_emb_, time_step_, seq_lengths_, src_mask_, gather_index_, max_buffer_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_pre_layer_norm = pir::BoolAttribute::get(pir::IrContext::Instance(), pre_layer_norm);
  argument.AddAttribute("pre_layer_norm", attr_pre_layer_norm);
  pir::Attribute attr_rotary_emb_dims = pir::Int32Attribute::get(pir::IrContext::Instance(), rotary_emb_dims);
  argument.AddAttribute("rotary_emb_dims", attr_rotary_emb_dims);
  pir::Attribute attr_epsilon = pir::FloatAttribute::get(pir::IrContext::Instance(), epsilon);
  argument.AddAttribute("epsilon", attr_epsilon);
  pir::Attribute attr_dropout_rate = pir::FloatAttribute::get(pir::IrContext::Instance(), dropout_rate);
  argument.AddAttribute("dropout_rate", attr_dropout_rate);
  pir::Attribute attr_is_test = pir::BoolAttribute::get(pir::IrContext::Instance(), is_test);
  argument.AddAttribute("is_test", attr_is_test);
  pir::Attribute attr_dropout_implementation = pir::StrAttribute::get(pir::IrContext::Instance(), dropout_implementation);
  argument.AddAttribute("dropout_implementation", attr_dropout_implementation);
  pir::Attribute attr_act_method = pir::StrAttribute::get(pir::IrContext::Instance(), act_method);
  argument.AddAttribute("act_method", attr_act_method);
  pir::Attribute attr_trans_qkvw = pir::BoolAttribute::get(pir::IrContext::Instance(), trans_qkvw);
  argument.AddAttribute("trans_qkvw", attr_trans_qkvw);
  pir::Attribute attr_ring_id = pir::Int32Attribute::get(pir::IrContext::Instance(), ring_id);
  argument.AddAttribute("ring_id", attr_ring_id);
  pir::Attribute attr_gather_axis = pir::Int32Attribute::get(pir::IrContext::Instance(), gather_axis);
  argument.AddAttribute("gather_axis", attr_gather_axis);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;
  pir::VectorType ln_scale = ln_scale_.type().dyn_cast<pir::VectorType>(); (void)ln_scale;
  pir::VectorType ln_bias = ln_bias_.type().dyn_cast<pir::VectorType>(); (void)ln_bias;
  pir::VectorType qkvw = qkvw_.type().dyn_cast<pir::VectorType>(); (void)qkvw;
  pir::VectorType qkvw_max = qkvw_max_.type().dyn_cast<pir::VectorType>(); (void)qkvw_max;
  pir::VectorType qkv_bias = qkv_bias_.type().dyn_cast<pir::VectorType>(); (void)qkv_bias;
  pir::VectorType out_linear_w = out_linear_w_.type().dyn_cast<pir::VectorType>(); (void)out_linear_w;
  pir::VectorType out_linear_wmax = out_linear_wmax_.type().dyn_cast<pir::VectorType>(); (void)out_linear_wmax;
  pir::VectorType out_linear_bias = out_linear_bias_.type().dyn_cast<pir::VectorType>(); (void)out_linear_bias;
  pir::VectorType ffn_ln_scale = ffn_ln_scale_.type().dyn_cast<pir::VectorType>(); (void)ffn_ln_scale;
  pir::VectorType ffn_ln_bias = ffn_ln_bias_.type().dyn_cast<pir::VectorType>(); (void)ffn_ln_bias;
  pir::VectorType ffn1_weight = ffn1_weight_.type().dyn_cast<pir::VectorType>(); (void)ffn1_weight;
  pir::VectorType ffn1_weight_max = ffn1_weight_max_.type().dyn_cast<pir::VectorType>(); (void)ffn1_weight_max;
  pir::VectorType ffn1_bias = ffn1_bias_.type().dyn_cast<pir::VectorType>(); (void)ffn1_bias;
  pir::VectorType ffn2_weight = ffn2_weight_.type().dyn_cast<pir::VectorType>(); (void)ffn2_weight;
  pir::VectorType ffn2_weight_max = ffn2_weight_max_.type().dyn_cast<pir::VectorType>(); (void)ffn2_weight_max;
  pir::VectorType ffn2_bias = ffn2_bias_.type().dyn_cast<pir::VectorType>(); (void)ffn2_bias;
  paddle::dialect::DenseTensorType max_buffer = max_buffer_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)max_buffer;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);
  std::vector<paddle::dialect::IrTensor> vec_ir_tensor_ln_scale;
  for (size_t i=0; i < static_cast<size_t>(ln_scale.size()); i++) {
    vec_ir_tensor_ln_scale.push_back(paddle::dialect::IrTensor(paddle::dialect::TransToPhiDataType(ln_scale[i].dyn_cast<paddle::dialect::DenseTensorType>().dtype()),
                                                                     ln_scale[i].dyn_cast<paddle::dialect::DenseTensorType>().dims(),
                                                                     ln_scale[i].dyn_cast<paddle::dialect::DenseTensorType>().data_layout(),
                                                                     ln_scale[i].dyn_cast<paddle::dialect::DenseTensorType>().lod(),
                                                                     ln_scale[i].dyn_cast<paddle::dialect::DenseTensorType>().offset()));
  }
  std::vector<paddle::dialect::IrMetaTensor> vec_meta_ln_scale;
  for (size_t i=0; i < vec_ir_tensor_ln_scale.size(); i++) {
    vec_meta_ln_scale.push_back(paddle::dialect::IrMetaTensor(&vec_ir_tensor_ln_scale[i]));
  }

  std::vector<const phi::MetaTensor*> meta_ln_scale;
  for (size_t i=0; i < static_cast<size_t>(vec_meta_ln_scale.size()); i++) {
    meta_ln_scale.push_back(&vec_meta_ln_scale[i]);
  }
   std::vector<paddle::dialect::IrTensor> vec_ir_tensor_ln_bias;
  for (size_t i=0; i < static_cast<size_t>(ln_bias.size()); i++) {
    vec_ir_tensor_ln_bias.push_back(paddle::dialect::IrTensor(paddle::dialect::TransToPhiDataType(ln_bias[i].dyn_cast<paddle::dialect::DenseTensorType>().dtype()),
                                                                     ln_bias[i].dyn_cast<paddle::dialect::DenseTensorType>().dims(),
                                                                     ln_bias[i].dyn_cast<paddle::dialect::DenseTensorType>().data_layout(),
                                                                     ln_bias[i].dyn_cast<paddle::dialect::DenseTensorType>().lod(),
                                                                     ln_bias[i].dyn_cast<paddle::dialect::DenseTensorType>().offset()));
  }
  std::vector<paddle::dialect::IrMetaTensor> vec_meta_ln_bias;
  for (size_t i=0; i < vec_ir_tensor_ln_bias.size(); i++) {
    vec_meta_ln_bias.push_back(paddle::dialect::IrMetaTensor(&vec_ir_tensor_ln_bias[i]));
  }

  std::vector<const phi::MetaTensor*> meta_ln_bias;
  for (size_t i=0; i < static_cast<size_t>(vec_meta_ln_bias.size()); i++) {
    meta_ln_bias.push_back(&vec_meta_ln_bias[i]);
  }
   std::vector<paddle::dialect::IrTensor> vec_ir_tensor_qkvw;
  for (size_t i=0; i < static_cast<size_t>(qkvw.size()); i++) {
    vec_ir_tensor_qkvw.push_back(paddle::dialect::IrTensor(paddle::dialect::TransToPhiDataType(qkvw[i].dyn_cast<paddle::dialect::DenseTensorType>().dtype()),
                                                                     qkvw[i].dyn_cast<paddle::dialect::DenseTensorType>().dims(),
                                                                     qkvw[i].dyn_cast<paddle::dialect::DenseTensorType>().data_layout(),
                                                                     qkvw[i].dyn_cast<paddle::dialect::DenseTensorType>().lod(),
                                                                     qkvw[i].dyn_cast<paddle::dialect::DenseTensorType>().offset()));
  }
  std::vector<paddle::dialect::IrMetaTensor> vec_meta_qkvw;
  for (size_t i=0; i < vec_ir_tensor_qkvw.size(); i++) {
    vec_meta_qkvw.push_back(paddle::dialect::IrMetaTensor(&vec_ir_tensor_qkvw[i]));
  }

  std::vector<const phi::MetaTensor*> meta_qkvw;
  for (size_t i=0; i < static_cast<size_t>(vec_meta_qkvw.size()); i++) {
    meta_qkvw.push_back(&vec_meta_qkvw[i]);
  }
   std::vector<paddle::dialect::IrTensor> vec_ir_tensor_qkvw_max;
  for (size_t i=0; i < static_cast<size_t>(qkvw_max.size()); i++) {
    vec_ir_tensor_qkvw_max.push_back(paddle::dialect::IrTensor(paddle::dialect::TransToPhiDataType(qkvw_max[i].dyn_cast<paddle::dialect::DenseTensorType>().dtype()),
                                                                     qkvw_max[i].dyn_cast<paddle::dialect::DenseTensorType>().dims(),
                                                                     qkvw_max[i].dyn_cast<paddle::dialect::DenseTensorType>().data_layout(),
                                                                     qkvw_max[i].dyn_cast<paddle::dialect::DenseTensorType>().lod(),
                                                                     qkvw_max[i].dyn_cast<paddle::dialect::DenseTensorType>().offset()));
  }
  std::vector<paddle::dialect::IrMetaTensor> vec_meta_qkvw_max;
  for (size_t i=0; i < vec_ir_tensor_qkvw_max.size(); i++) {
    vec_meta_qkvw_max.push_back(paddle::dialect::IrMetaTensor(&vec_ir_tensor_qkvw_max[i]));
  }

  std::vector<const phi::MetaTensor*> meta_qkvw_max;
  for (size_t i=0; i < static_cast<size_t>(vec_meta_qkvw_max.size()); i++) {
    meta_qkvw_max.push_back(&vec_meta_qkvw_max[i]);
  }
   std::vector<paddle::dialect::IrTensor> vec_ir_tensor_qkv_bias;
  for (size_t i=0; i < static_cast<size_t>(qkv_bias.size()); i++) {
    vec_ir_tensor_qkv_bias.push_back(paddle::dialect::IrTensor(paddle::dialect::TransToPhiDataType(qkv_bias[i].dyn_cast<paddle::dialect::DenseTensorType>().dtype()),
                                                                     qkv_bias[i].dyn_cast<paddle::dialect::DenseTensorType>().dims(),
                                                                     qkv_bias[i].dyn_cast<paddle::dialect::DenseTensorType>().data_layout(),
                                                                     qkv_bias[i].dyn_cast<paddle::dialect::DenseTensorType>().lod(),
                                                                     qkv_bias[i].dyn_cast<paddle::dialect::DenseTensorType>().offset()));
  }
  std::vector<paddle::dialect::IrMetaTensor> vec_meta_qkv_bias;
  for (size_t i=0; i < vec_ir_tensor_qkv_bias.size(); i++) {
    vec_meta_qkv_bias.push_back(paddle::dialect::IrMetaTensor(&vec_ir_tensor_qkv_bias[i]));
  }

  std::vector<const phi::MetaTensor*> meta_qkv_bias;
  for (size_t i=0; i < static_cast<size_t>(vec_meta_qkv_bias.size()); i++) {
    meta_qkv_bias.push_back(&vec_meta_qkv_bias[i]);
  }
   std::vector<paddle::dialect::IrTensor> vec_ir_tensor_out_linear_w;
  for (size_t i=0; i < static_cast<size_t>(out_linear_w.size()); i++) {
    vec_ir_tensor_out_linear_w.push_back(paddle::dialect::IrTensor(paddle::dialect::TransToPhiDataType(out_linear_w[i].dyn_cast<paddle::dialect::DenseTensorType>().dtype()),
                                                                     out_linear_w[i].dyn_cast<paddle::dialect::DenseTensorType>().dims(),
                                                                     out_linear_w[i].dyn_cast<paddle::dialect::DenseTensorType>().data_layout(),
                                                                     out_linear_w[i].dyn_cast<paddle::dialect::DenseTensorType>().lod(),
                                                                     out_linear_w[i].dyn_cast<paddle::dialect::DenseTensorType>().offset()));
  }
  std::vector<paddle::dialect::IrMetaTensor> vec_meta_out_linear_w;
  for (size_t i=0; i < vec_ir_tensor_out_linear_w.size(); i++) {
    vec_meta_out_linear_w.push_back(paddle::dialect::IrMetaTensor(&vec_ir_tensor_out_linear_w[i]));
  }

  std::vector<const phi::MetaTensor*> meta_out_linear_w;
  for (size_t i=0; i < static_cast<size_t>(vec_meta_out_linear_w.size()); i++) {
    meta_out_linear_w.push_back(&vec_meta_out_linear_w[i]);
  }
   std::vector<paddle::dialect::IrTensor> vec_ir_tensor_out_linear_wmax;
  for (size_t i=0; i < static_cast<size_t>(out_linear_wmax.size()); i++) {
    vec_ir_tensor_out_linear_wmax.push_back(paddle::dialect::IrTensor(paddle::dialect::TransToPhiDataType(out_linear_wmax[i].dyn_cast<paddle::dialect::DenseTensorType>().dtype()),
                                                                     out_linear_wmax[i].dyn_cast<paddle::dialect::DenseTensorType>().dims(),
                                                                     out_linear_wmax[i].dyn_cast<paddle::dialect::DenseTensorType>().data_layout(),
                                                                     out_linear_wmax[i].dyn_cast<paddle::dialect::DenseTensorType>().lod(),
                                                                     out_linear_wmax[i].dyn_cast<paddle::dialect::DenseTensorType>().offset()));
  }
  std::vector<paddle::dialect::IrMetaTensor> vec_meta_out_linear_wmax;
  for (size_t i=0; i < vec_ir_tensor_out_linear_wmax.size(); i++) {
    vec_meta_out_linear_wmax.push_back(paddle::dialect::IrMetaTensor(&vec_ir_tensor_out_linear_wmax[i]));
  }

  std::vector<const phi::MetaTensor*> meta_out_linear_wmax;
  for (size_t i=0; i < static_cast<size_t>(vec_meta_out_linear_wmax.size()); i++) {
    meta_out_linear_wmax.push_back(&vec_meta_out_linear_wmax[i]);
  }
   std::vector<paddle::dialect::IrTensor> vec_ir_tensor_out_linear_bias;
  for (size_t i=0; i < static_cast<size_t>(out_linear_bias.size()); i++) {
    vec_ir_tensor_out_linear_bias.push_back(paddle::dialect::IrTensor(paddle::dialect::TransToPhiDataType(out_linear_bias[i].dyn_cast<paddle::dialect::DenseTensorType>().dtype()),
                                                                     out_linear_bias[i].dyn_cast<paddle::dialect::DenseTensorType>().dims(),
                                                                     out_linear_bias[i].dyn_cast<paddle::dialect::DenseTensorType>().data_layout(),
                                                                     out_linear_bias[i].dyn_cast<paddle::dialect::DenseTensorType>().lod(),
                                                                     out_linear_bias[i].dyn_cast<paddle::dialect::DenseTensorType>().offset()));
  }
  std::vector<paddle::dialect::IrMetaTensor> vec_meta_out_linear_bias;
  for (size_t i=0; i < vec_ir_tensor_out_linear_bias.size(); i++) {
    vec_meta_out_linear_bias.push_back(paddle::dialect::IrMetaTensor(&vec_ir_tensor_out_linear_bias[i]));
  }

  std::vector<const phi::MetaTensor*> meta_out_linear_bias;
  for (size_t i=0; i < static_cast<size_t>(vec_meta_out_linear_bias.size()); i++) {
    meta_out_linear_bias.push_back(&vec_meta_out_linear_bias[i]);
  }
   std::vector<paddle::dialect::IrTensor> vec_ir_tensor_ffn_ln_scale;
  for (size_t i=0; i < static_cast<size_t>(ffn_ln_scale.size()); i++) {
    vec_ir_tensor_ffn_ln_scale.push_back(paddle::dialect::IrTensor(paddle::dialect::TransToPhiDataType(ffn_ln_scale[i].dyn_cast<paddle::dialect::DenseTensorType>().dtype()),
                                                                     ffn_ln_scale[i].dyn_cast<paddle::dialect::DenseTensorType>().dims(),
                                                                     ffn_ln_scale[i].dyn_cast<paddle::dialect::DenseTensorType>().data_layout(),
                                                                     ffn_ln_scale[i].dyn_cast<paddle::dialect::DenseTensorType>().lod(),
                                                                     ffn_ln_scale[i].dyn_cast<paddle::dialect::DenseTensorType>().offset()));
  }
  std::vector<paddle::dialect::IrMetaTensor> vec_meta_ffn_ln_scale;
  for (size_t i=0; i < vec_ir_tensor_ffn_ln_scale.size(); i++) {
    vec_meta_ffn_ln_scale.push_back(paddle::dialect::IrMetaTensor(&vec_ir_tensor_ffn_ln_scale[i]));
  }

  std::vector<const phi::MetaTensor*> meta_ffn_ln_scale;
  for (size_t i=0; i < static_cast<size_t>(vec_meta_ffn_ln_scale.size()); i++) {
    meta_ffn_ln_scale.push_back(&vec_meta_ffn_ln_scale[i]);
  }
   std::vector<paddle::dialect::IrTensor> vec_ir_tensor_ffn_ln_bias;
  for (size_t i=0; i < static_cast<size_t>(ffn_ln_bias.size()); i++) {
    vec_ir_tensor_ffn_ln_bias.push_back(paddle::dialect::IrTensor(paddle::dialect::TransToPhiDataType(ffn_ln_bias[i].dyn_cast<paddle::dialect::DenseTensorType>().dtype()),
                                                                     ffn_ln_bias[i].dyn_cast<paddle::dialect::DenseTensorType>().dims(),
                                                                     ffn_ln_bias[i].dyn_cast<paddle::dialect::DenseTensorType>().data_layout(),
                                                                     ffn_ln_bias[i].dyn_cast<paddle::dialect::DenseTensorType>().lod(),
                                                                     ffn_ln_bias[i].dyn_cast<paddle::dialect::DenseTensorType>().offset()));
  }
  std::vector<paddle::dialect::IrMetaTensor> vec_meta_ffn_ln_bias;
  for (size_t i=0; i < vec_ir_tensor_ffn_ln_bias.size(); i++) {
    vec_meta_ffn_ln_bias.push_back(paddle::dialect::IrMetaTensor(&vec_ir_tensor_ffn_ln_bias[i]));
  }

  std::vector<const phi::MetaTensor*> meta_ffn_ln_bias;
  for (size_t i=0; i < static_cast<size_t>(vec_meta_ffn_ln_bias.size()); i++) {
    meta_ffn_ln_bias.push_back(&vec_meta_ffn_ln_bias[i]);
  }
   std::vector<paddle::dialect::IrTensor> vec_ir_tensor_ffn1_weight;
  for (size_t i=0; i < static_cast<size_t>(ffn1_weight.size()); i++) {
    vec_ir_tensor_ffn1_weight.push_back(paddle::dialect::IrTensor(paddle::dialect::TransToPhiDataType(ffn1_weight[i].dyn_cast<paddle::dialect::DenseTensorType>().dtype()),
                                                                     ffn1_weight[i].dyn_cast<paddle::dialect::DenseTensorType>().dims(),
                                                                     ffn1_weight[i].dyn_cast<paddle::dialect::DenseTensorType>().data_layout(),
                                                                     ffn1_weight[i].dyn_cast<paddle::dialect::DenseTensorType>().lod(),
                                                                     ffn1_weight[i].dyn_cast<paddle::dialect::DenseTensorType>().offset()));
  }
  std::vector<paddle::dialect::IrMetaTensor> vec_meta_ffn1_weight;
  for (size_t i=0; i < vec_ir_tensor_ffn1_weight.size(); i++) {
    vec_meta_ffn1_weight.push_back(paddle::dialect::IrMetaTensor(&vec_ir_tensor_ffn1_weight[i]));
  }

  std::vector<const phi::MetaTensor*> meta_ffn1_weight;
  for (size_t i=0; i < static_cast<size_t>(vec_meta_ffn1_weight.size()); i++) {
    meta_ffn1_weight.push_back(&vec_meta_ffn1_weight[i]);
  }
   std::vector<paddle::dialect::IrTensor> vec_ir_tensor_ffn1_weight_max;
  for (size_t i=0; i < static_cast<size_t>(ffn1_weight_max.size()); i++) {
    vec_ir_tensor_ffn1_weight_max.push_back(paddle::dialect::IrTensor(paddle::dialect::TransToPhiDataType(ffn1_weight_max[i].dyn_cast<paddle::dialect::DenseTensorType>().dtype()),
                                                                     ffn1_weight_max[i].dyn_cast<paddle::dialect::DenseTensorType>().dims(),
                                                                     ffn1_weight_max[i].dyn_cast<paddle::dialect::DenseTensorType>().data_layout(),
                                                                     ffn1_weight_max[i].dyn_cast<paddle::dialect::DenseTensorType>().lod(),
                                                                     ffn1_weight_max[i].dyn_cast<paddle::dialect::DenseTensorType>().offset()));
  }
  std::vector<paddle::dialect::IrMetaTensor> vec_meta_ffn1_weight_max;
  for (size_t i=0; i < vec_ir_tensor_ffn1_weight_max.size(); i++) {
    vec_meta_ffn1_weight_max.push_back(paddle::dialect::IrMetaTensor(&vec_ir_tensor_ffn1_weight_max[i]));
  }

  std::vector<const phi::MetaTensor*> meta_ffn1_weight_max;
  for (size_t i=0; i < static_cast<size_t>(vec_meta_ffn1_weight_max.size()); i++) {
    meta_ffn1_weight_max.push_back(&vec_meta_ffn1_weight_max[i]);
  }
   std::vector<paddle::dialect::IrTensor> vec_ir_tensor_ffn1_bias;
  for (size_t i=0; i < static_cast<size_t>(ffn1_bias.size()); i++) {
    vec_ir_tensor_ffn1_bias.push_back(paddle::dialect::IrTensor(paddle::dialect::TransToPhiDataType(ffn1_bias[i].dyn_cast<paddle::dialect::DenseTensorType>().dtype()),
                                                                     ffn1_bias[i].dyn_cast<paddle::dialect::DenseTensorType>().dims(),
                                                                     ffn1_bias[i].dyn_cast<paddle::dialect::DenseTensorType>().data_layout(),
                                                                     ffn1_bias[i].dyn_cast<paddle::dialect::DenseTensorType>().lod(),
                                                                     ffn1_bias[i].dyn_cast<paddle::dialect::DenseTensorType>().offset()));
  }
  std::vector<paddle::dialect::IrMetaTensor> vec_meta_ffn1_bias;
  for (size_t i=0; i < vec_ir_tensor_ffn1_bias.size(); i++) {
    vec_meta_ffn1_bias.push_back(paddle::dialect::IrMetaTensor(&vec_ir_tensor_ffn1_bias[i]));
  }

  std::vector<const phi::MetaTensor*> meta_ffn1_bias;
  for (size_t i=0; i < static_cast<size_t>(vec_meta_ffn1_bias.size()); i++) {
    meta_ffn1_bias.push_back(&vec_meta_ffn1_bias[i]);
  }
   std::vector<paddle::dialect::IrTensor> vec_ir_tensor_ffn2_weight;
  for (size_t i=0; i < static_cast<size_t>(ffn2_weight.size()); i++) {
    vec_ir_tensor_ffn2_weight.push_back(paddle::dialect::IrTensor(paddle::dialect::TransToPhiDataType(ffn2_weight[i].dyn_cast<paddle::dialect::DenseTensorType>().dtype()),
                                                                     ffn2_weight[i].dyn_cast<paddle::dialect::DenseTensorType>().dims(),
                                                                     ffn2_weight[i].dyn_cast<paddle::dialect::DenseTensorType>().data_layout(),
                                                                     ffn2_weight[i].dyn_cast<paddle::dialect::DenseTensorType>().lod(),
                                                                     ffn2_weight[i].dyn_cast<paddle::dialect::DenseTensorType>().offset()));
  }
  std::vector<paddle::dialect::IrMetaTensor> vec_meta_ffn2_weight;
  for (size_t i=0; i < vec_ir_tensor_ffn2_weight.size(); i++) {
    vec_meta_ffn2_weight.push_back(paddle::dialect::IrMetaTensor(&vec_ir_tensor_ffn2_weight[i]));
  }

  std::vector<const phi::MetaTensor*> meta_ffn2_weight;
  for (size_t i=0; i < static_cast<size_t>(vec_meta_ffn2_weight.size()); i++) {
    meta_ffn2_weight.push_back(&vec_meta_ffn2_weight[i]);
  }
   std::vector<paddle::dialect::IrTensor> vec_ir_tensor_ffn2_weight_max;
  for (size_t i=0; i < static_cast<size_t>(ffn2_weight_max.size()); i++) {
    vec_ir_tensor_ffn2_weight_max.push_back(paddle::dialect::IrTensor(paddle::dialect::TransToPhiDataType(ffn2_weight_max[i].dyn_cast<paddle::dialect::DenseTensorType>().dtype()),
                                                                     ffn2_weight_max[i].dyn_cast<paddle::dialect::DenseTensorType>().dims(),
                                                                     ffn2_weight_max[i].dyn_cast<paddle::dialect::DenseTensorType>().data_layout(),
                                                                     ffn2_weight_max[i].dyn_cast<paddle::dialect::DenseTensorType>().lod(),
                                                                     ffn2_weight_max[i].dyn_cast<paddle::dialect::DenseTensorType>().offset()));
  }
  std::vector<paddle::dialect::IrMetaTensor> vec_meta_ffn2_weight_max;
  for (size_t i=0; i < vec_ir_tensor_ffn2_weight_max.size(); i++) {
    vec_meta_ffn2_weight_max.push_back(paddle::dialect::IrMetaTensor(&vec_ir_tensor_ffn2_weight_max[i]));
  }

  std::vector<const phi::MetaTensor*> meta_ffn2_weight_max;
  for (size_t i=0; i < static_cast<size_t>(vec_meta_ffn2_weight_max.size()); i++) {
    meta_ffn2_weight_max.push_back(&vec_meta_ffn2_weight_max[i]);
  }
   std::vector<paddle::dialect::IrTensor> vec_ir_tensor_ffn2_bias;
  for (size_t i=0; i < static_cast<size_t>(ffn2_bias.size()); i++) {
    vec_ir_tensor_ffn2_bias.push_back(paddle::dialect::IrTensor(paddle::dialect::TransToPhiDataType(ffn2_bias[i].dyn_cast<paddle::dialect::DenseTensorType>().dtype()),
                                                                     ffn2_bias[i].dyn_cast<paddle::dialect::DenseTensorType>().dims(),
                                                                     ffn2_bias[i].dyn_cast<paddle::dialect::DenseTensorType>().data_layout(),
                                                                     ffn2_bias[i].dyn_cast<paddle::dialect::DenseTensorType>().lod(),
                                                                     ffn2_bias[i].dyn_cast<paddle::dialect::DenseTensorType>().offset()));
  }
  std::vector<paddle::dialect::IrMetaTensor> vec_meta_ffn2_bias;
  for (size_t i=0; i < vec_ir_tensor_ffn2_bias.size(); i++) {
    vec_meta_ffn2_bias.push_back(paddle::dialect::IrMetaTensor(&vec_ir_tensor_ffn2_bias[i]));
  }

  std::vector<const phi::MetaTensor*> meta_ffn2_bias;
  for (size_t i=0; i < static_cast<size_t>(vec_meta_ffn2_bias.size()); i++) {
    meta_ffn2_bias.push_back(&vec_meta_ffn2_bias[i]);
  }
   std::vector<paddle::dialect::IrTensor> vec_ir_tensor_cache_kv;
  if (cache_kv_.impl() != nullptr) {
    pir::VectorType cache_kv = cache_kv_.type().dyn_cast<pir::VectorType>();
    for (size_t i=0; i < static_cast<size_t>(cache_kv.size()); i++) {
        vec_ir_tensor_cache_kv.push_back(paddle::dialect::IrTensor(paddle::dialect::TransToPhiDataType(cache_kv[i].dyn_cast<paddle::dialect::DenseTensorType>().dtype()),
                                                                        cache_kv[i].dyn_cast<paddle::dialect::DenseTensorType>().dims(),
                                                                        cache_kv[i].dyn_cast<paddle::dialect::DenseTensorType>().data_layout(),
                                                                        cache_kv[i].dyn_cast<paddle::dialect::DenseTensorType>().lod(),
                                                                        cache_kv[i].dyn_cast<paddle::dialect::DenseTensorType>().offset()));
    }
  }

  std::vector<paddle::dialect::IrMetaTensor> vec_meta_cache_kv;
  for (size_t i=0; i < vec_ir_tensor_cache_kv.size(); i++) {
    vec_meta_cache_kv.push_back(paddle::dialect::IrMetaTensor(&vec_ir_tensor_cache_kv[i]));
  }

  std::vector<const phi::MetaTensor*> meta_cache_kv;
  for (size_t i=0; i < static_cast<size_t>(vec_meta_cache_kv.size()); i++) {
    meta_cache_kv.push_back(&vec_meta_cache_kv[i]);
  }

  std::vector<paddle::dialect::IrTensor> vec_ir_tensor_pre_caches;
  if (pre_caches_.impl() != nullptr) {
    pir::VectorType pre_caches = pre_caches_.type().dyn_cast<pir::VectorType>();
    for (size_t i=0; i < static_cast<size_t>(pre_caches.size()); i++) {
        vec_ir_tensor_pre_caches.push_back(paddle::dialect::IrTensor(paddle::dialect::TransToPhiDataType(pre_caches[i].dyn_cast<paddle::dialect::DenseTensorType>().dtype()),
                                                                        pre_caches[i].dyn_cast<paddle::dialect::DenseTensorType>().dims(),
                                                                        pre_caches[i].dyn_cast<paddle::dialect::DenseTensorType>().data_layout(),
                                                                        pre_caches[i].dyn_cast<paddle::dialect::DenseTensorType>().lod(),
                                                                        pre_caches[i].dyn_cast<paddle::dialect::DenseTensorType>().offset()));
    }
  }

  std::vector<paddle::dialect::IrMetaTensor> vec_meta_pre_caches;
  for (size_t i=0; i < vec_ir_tensor_pre_caches.size(); i++) {
    vec_meta_pre_caches.push_back(paddle::dialect::IrMetaTensor(&vec_ir_tensor_pre_caches[i]));
  }

  std::vector<const phi::MetaTensor*> meta_pre_caches;
  for (size_t i=0; i < static_cast<size_t>(vec_meta_pre_caches.size()); i++) {
    meta_pre_caches.push_back(&vec_meta_pre_caches[i]);
  }


  paddle::dialect::IrMetaTensor meta_rotary_pos_emb;
  paddle::dialect::IrTensor ir_tensor_rotary_pos_emb;
  if (rotary_pos_emb_.impl() != nullptr) {
    paddle::dialect::DenseTensorType rotary_pos_emb = rotary_pos_emb_.type().dyn_cast<paddle::dialect::DenseTensorType>();
    VLOG(4) << "Builder construction  dense_rotary_pos_emb";
    ir_tensor_rotary_pos_emb = paddle::dialect::IrTensor(paddle::dialect::TransToPhiDataType(rotary_pos_emb.dtype()),
                                                        rotary_pos_emb.dims(),
                                                        rotary_pos_emb.data_layout(),
                                                        rotary_pos_emb.lod(),
                                                        rotary_pos_emb.offset());
    VLOG(4) << "Builder construction  meta_rotary_pos_emb";
    meta_rotary_pos_emb = paddle::dialect::IrMetaTensor(&ir_tensor_rotary_pos_emb);
  }


  paddle::dialect::IrMetaTensor meta_time_step;
  paddle::dialect::IrTensor ir_tensor_time_step;
  if (time_step_.impl() != nullptr) {
    paddle::dialect::DenseTensorType time_step = time_step_.type().dyn_cast<paddle::dialect::DenseTensorType>();
    VLOG(4) << "Builder construction  dense_time_step";
    ir_tensor_time_step = paddle::dialect::IrTensor(paddle::dialect::TransToPhiDataType(time_step.dtype()),
                                                        time_step.dims(),
                                                        time_step.data_layout(),
                                                        time_step.lod(),
                                                        time_step.offset());
    VLOG(4) << "Builder construction  meta_time_step";
    meta_time_step = paddle::dialect::IrMetaTensor(&ir_tensor_time_step);
  }


  paddle::dialect::IrMetaTensor meta_seq_lengths;
  paddle::dialect::IrTensor ir_tensor_seq_lengths;
  if (seq_lengths_.impl() != nullptr) {
    paddle::dialect::DenseTensorType seq_lengths = seq_lengths_.type().dyn_cast<paddle::dialect::DenseTensorType>();
    VLOG(4) << "Builder construction  dense_seq_lengths";
    ir_tensor_seq_lengths = paddle::dialect::IrTensor(paddle::dialect::TransToPhiDataType(seq_lengths.dtype()),
                                                        seq_lengths.dims(),
                                                        seq_lengths.data_layout(),
                                                        seq_lengths.lod(),
                                                        seq_lengths.offset());
    VLOG(4) << "Builder construction  meta_seq_lengths";
    meta_seq_lengths = paddle::dialect::IrMetaTensor(&ir_tensor_seq_lengths);
  }


  paddle::dialect::IrMetaTensor meta_src_mask;
  paddle::dialect::IrTensor ir_tensor_src_mask;
  if (src_mask_.impl() != nullptr) {
    paddle::dialect::DenseTensorType src_mask = src_mask_.type().dyn_cast<paddle::dialect::DenseTensorType>();
    VLOG(4) << "Builder construction  dense_src_mask";
    ir_tensor_src_mask = paddle::dialect::IrTensor(paddle::dialect::TransToPhiDataType(src_mask.dtype()),
                                                        src_mask.dims(),
                                                        src_mask.data_layout(),
                                                        src_mask.lod(),
                                                        src_mask.offset());
    VLOG(4) << "Builder construction  meta_src_mask";
    meta_src_mask = paddle::dialect::IrMetaTensor(&ir_tensor_src_mask);
  }


  paddle::dialect::IrMetaTensor meta_gather_index;
  paddle::dialect::IrTensor ir_tensor_gather_index;
  if (gather_index_.impl() != nullptr) {
    paddle::dialect::DenseTensorType gather_index = gather_index_.type().dyn_cast<paddle::dialect::DenseTensorType>();
    VLOG(4) << "Builder construction  dense_gather_index";
    ir_tensor_gather_index = paddle::dialect::IrTensor(paddle::dialect::TransToPhiDataType(gather_index.dtype()),
                                                        gather_index.dims(),
                                                        gather_index.data_layout(),
                                                        gather_index.lod(),
                                                        gather_index.offset());
    VLOG(4) << "Builder construction  meta_gather_index";
    meta_gather_index = paddle::dialect::IrMetaTensor(&ir_tensor_gather_index);
  }


  VLOG(4) << "Builder construction  dense_max_buffer";
  paddle::dialect::IrTensor ir_tensor_max_buffer(paddle::dialect::TransToPhiDataType(max_buffer.dtype()),
                                                      max_buffer.dims(),
                                                      max_buffer.data_layout(),
                                                      max_buffer.lod(),
                                                      max_buffer.offset());
  VLOG(4) << "Builder construction  meta_max_buffer";
  paddle::dialect::IrMetaTensor meta_max_buffer(&ir_tensor_max_buffer);
  paddle::dialect::IrTensor dense_out;
  paddle::dialect::IrMetaTensor meta_out(&dense_out);
  std::vector<paddle::dialect::IrTensor> vec_dense_cache_kv_out((out_linear_w.size()), paddle::dialect::IrTensor());
  std::vector<paddle::dialect::IrMetaTensor> vec_meta_cache_kv_out;
  for (size_t i=0; i < static_cast<size_t>(out_linear_w.size()); i++) {
    vec_meta_cache_kv_out.push_back(paddle::dialect::IrMetaTensor(&vec_dense_cache_kv_out[i]));
  }
  std::vector<phi::MetaTensor*> meta_cache_kv_out;
  for (size_t i=0; i < static_cast<size_t>(vec_meta_cache_kv_out.size()); i++) {
    meta_cache_kv_out.push_back(&vec_meta_cache_kv_out[i]);
  }

  phi::FusedMultiTransformerXpuInferMeta(meta_x, meta_ln_scale, meta_ln_bias, meta_qkvw, meta_qkvw_max, meta_qkv_bias, meta_out_linear_w, meta_out_linear_wmax, meta_out_linear_bias, meta_ffn_ln_scale, meta_ffn_ln_bias, meta_ffn1_weight, meta_ffn1_weight_max, meta_ffn1_bias, meta_ffn2_weight, meta_ffn2_weight_max, meta_ffn2_bias, meta_cache_kv, meta_pre_caches, meta_rotary_pos_emb, meta_time_step, meta_seq_lengths, meta_src_mask, meta_gather_index, meta_max_buffer, pre_layer_norm, rotary_emb_dims, epsilon, dropout_rate, is_test, dropout_implementation, act_method, trans_qkvw, ring_id, gather_axis, &meta_out, meta_cache_kv_out);

  std::vector<pir::Type> argument_outputs;
  pir::Type out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_out.dtype()), dense_out.dims(), dense_out.layout(), dense_out.lod(), dense_out.offset());
  argument_outputs.push_back(out_dense_tensor_type);

  std::vector<pir::Type> cache_kv_out_types;
  for (size_t i=0; i < static_cast<size_t>(out_linear_w.size()); i++) {
    cache_kv_out_types.push_back(paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(vec_dense_cache_kv_out[i].dtype()), vec_dense_cache_kv_out[i].dims(), vec_dense_cache_kv_out[i].layout(), vec_dense_cache_kv_out[i].lod(), vec_dense_cache_kv_out[i].offset()));
  }
  pir::Type cache_kv_out_vector_type = pir::VectorType::get(pir::IrContext::Instance(), cache_kv_out_types);
  argument_outputs.push_back(cache_kv_out_vector_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void FusedMultiTransformerXpuOp::VerifySig() {
  VLOG(4) << "Start Verifying inputs, outputs and attributes for: FusedMultiTransformerXpuOp.";
  VLOG(4) << "Verifying inputs:";
  {
  auto input_size = num_operands();
  IR_ENFORCE(input_size == 25u,
                    "The size %d of inputs must be equal to 25.", input_size);
  IR_ENFORCE((*this)->operand_source(0).type().isa<paddle::dialect::DenseTensorType>(),
                  "Type validation failed for the 0th input, got %s.", (*this)->operand_source(0).type());
  if (auto vec_type = (*this)->operand_source(1).type().dyn_cast<pir::VectorType>()) {
      for (size_t i = 0; i < vec_type.size(); ++i) {
        IR_ENFORCE(vec_type[i].isa<paddle::dialect::DenseTensorType>(),
                       "Type validation failed for the 1th input, got %s.", (*this)->operand_source(1).type());
      }
  }
  else {
    IR_ENFORCE((*this)->operand_source(1).type().isa<paddle::dialect::DenseTensorType>(),
                   "Type validation failed for the 1th input, got %s.", (*this)->operand_source(1).type());
  }
  if (auto vec_type = (*this)->operand_source(2).type().dyn_cast<pir::VectorType>()) {
      for (size_t i = 0; i < vec_type.size(); ++i) {
        IR_ENFORCE(vec_type[i].isa<paddle::dialect::DenseTensorType>(),
                       "Type validation failed for the 2th input, got %s.", (*this)->operand_source(2).type());
      }
  }
  else {
    IR_ENFORCE((*this)->operand_source(2).type().isa<paddle::dialect::DenseTensorType>(),
                   "Type validation failed for the 2th input, got %s.", (*this)->operand_source(2).type());
  }
  if (auto vec_type = (*this)->operand_source(3).type().dyn_cast<pir::VectorType>()) {
      for (size_t i = 0; i < vec_type.size(); ++i) {
        IR_ENFORCE(vec_type[i].isa<paddle::dialect::DenseTensorType>(),
                       "Type validation failed for the 3th input, got %s.", (*this)->operand_source(3).type());
      }
  }
  else {
    IR_ENFORCE((*this)->operand_source(3).type().isa<paddle::dialect::DenseTensorType>(),
                   "Type validation failed for the 3th input, got %s.", (*this)->operand_source(3).type());
  }
  if (auto vec_type = (*this)->operand_source(4).type().dyn_cast<pir::VectorType>()) {
      for (size_t i = 0; i < vec_type.size(); ++i) {
        IR_ENFORCE(vec_type[i].isa<paddle::dialect::DenseTensorType>(),
                       "Type validation failed for the 4th input, got %s.", (*this)->operand_source(4).type());
      }
  }
  else {
    IR_ENFORCE((*this)->operand_source(4).type().isa<paddle::dialect::DenseTensorType>(),
                   "Type validation failed for the 4th input, got %s.", (*this)->operand_source(4).type());
  }
  if (auto vec_type = (*this)->operand_source(5).type().dyn_cast<pir::VectorType>()) {
      for (size_t i = 0; i < vec_type.size(); ++i) {
        IR_ENFORCE(vec_type[i].isa<paddle::dialect::DenseTensorType>(),
                       "Type validation failed for the 5th input, got %s.", (*this)->operand_source(5).type());
      }
  }
  else {
    IR_ENFORCE((*this)->operand_source(5).type().isa<paddle::dialect::DenseTensorType>(),
                   "Type validation failed for the 5th input, got %s.", (*this)->operand_source(5).type());
  }
  if (auto vec_type = (*this)->operand_source(6).type().dyn_cast<pir::VectorType>()) {
      for (size_t i = 0; i < vec_type.size(); ++i) {
        IR_ENFORCE(vec_type[i].isa<paddle::dialect::DenseTensorType>(),
                       "Type validation failed for the 6th input, got %s.", (*this)->operand_source(6).type());
      }
  }
  else {
    IR_ENFORCE((*this)->operand_source(6).type().isa<paddle::dialect::DenseTensorType>(),
                   "Type validation failed for the 6th input, got %s.", (*this)->operand_source(6).type());
  }
  if (auto vec_type = (*this)->operand_source(7).type().dyn_cast<pir::VectorType>()) {
      for (size_t i = 0; i < vec_type.size(); ++i) {
        IR_ENFORCE(vec_type[i].isa<paddle::dialect::DenseTensorType>(),
                       "Type validation failed for the 7th input, got %s.", (*this)->operand_source(7).type());
      }
  }
  else {
    IR_ENFORCE((*this)->operand_source(7).type().isa<paddle::dialect::DenseTensorType>(),
                   "Type validation failed for the 7th input, got %s.", (*this)->operand_source(7).type());
  }
  if (auto vec_type = (*this)->operand_source(8).type().dyn_cast<pir::VectorType>()) {
      for (size_t i = 0; i < vec_type.size(); ++i) {
        IR_ENFORCE(vec_type[i].isa<paddle::dialect::DenseTensorType>(),
                       "Type validation failed for the 8th input, got %s.", (*this)->operand_source(8).type());
      }
  }
  else {
    IR_ENFORCE((*this)->operand_source(8).type().isa<paddle::dialect::DenseTensorType>(),
                   "Type validation failed for the 8th input, got %s.", (*this)->operand_source(8).type());
  }
  if (auto vec_type = (*this)->operand_source(9).type().dyn_cast<pir::VectorType>()) {
      for (size_t i = 0; i < vec_type.size(); ++i) {
        IR_ENFORCE(vec_type[i].isa<paddle::dialect::DenseTensorType>(),
                       "Type validation failed for the 9th input, got %s.", (*this)->operand_source(9).type());
      }
  }
  else {
    IR_ENFORCE((*this)->operand_source(9).type().isa<paddle::dialect::DenseTensorType>(),
                   "Type validation failed for the 9th input, got %s.", (*this)->operand_source(9).type());
  }
  if (auto vec_type = (*this)->operand_source(10).type().dyn_cast<pir::VectorType>()) {
      for (size_t i = 0; i < vec_type.size(); ++i) {
        IR_ENFORCE(vec_type[i].isa<paddle::dialect::DenseTensorType>(),
                       "Type validation failed for the 10th input, got %s.", (*this)->operand_source(10).type());
      }
  }
  else {
    IR_ENFORCE((*this)->operand_source(10).type().isa<paddle::dialect::DenseTensorType>(),
                   "Type validation failed for the 10th input, got %s.", (*this)->operand_source(10).type());
  }
  if (auto vec_type = (*this)->operand_source(11).type().dyn_cast<pir::VectorType>()) {
      for (size_t i = 0; i < vec_type.size(); ++i) {
        IR_ENFORCE(vec_type[i].isa<paddle::dialect::DenseTensorType>(),
                       "Type validation failed for the 11th input, got %s.", (*this)->operand_source(11).type());
      }
  }
  else {
    IR_ENFORCE((*this)->operand_source(11).type().isa<paddle::dialect::DenseTensorType>(),
                   "Type validation failed for the 11th input, got %s.", (*this)->operand_source(11).type());
  }
  if (auto vec_type = (*this)->operand_source(12).type().dyn_cast<pir::VectorType>()) {
      for (size_t i = 0; i < vec_type.size(); ++i) {
        IR_ENFORCE(vec_type[i].isa<paddle::dialect::DenseTensorType>(),
                       "Type validation failed for the 12th input, got %s.", (*this)->operand_source(12).type());
      }
  }
  else {
    IR_ENFORCE((*this)->operand_source(12).type().isa<paddle::dialect::DenseTensorType>(),
                   "Type validation failed for the 12th input, got %s.", (*this)->operand_source(12).type());
  }
  if (auto vec_type = (*this)->operand_source(13).type().dyn_cast<pir::VectorType>()) {
      for (size_t i = 0; i < vec_type.size(); ++i) {
        IR_ENFORCE(vec_type[i].isa<paddle::dialect::DenseTensorType>(),
                       "Type validation failed for the 13th input, got %s.", (*this)->operand_source(13).type());
      }
  }
  else {
    IR_ENFORCE((*this)->operand_source(13).type().isa<paddle::dialect::DenseTensorType>(),
                   "Type validation failed for the 13th input, got %s.", (*this)->operand_source(13).type());
  }
  if (auto vec_type = (*this)->operand_source(14).type().dyn_cast<pir::VectorType>()) {
      for (size_t i = 0; i < vec_type.size(); ++i) {
        IR_ENFORCE(vec_type[i].isa<paddle::dialect::DenseTensorType>(),
                       "Type validation failed for the 14th input, got %s.", (*this)->operand_source(14).type());
      }
  }
  else {
    IR_ENFORCE((*this)->operand_source(14).type().isa<paddle::dialect::DenseTensorType>(),
                   "Type validation failed for the 14th input, got %s.", (*this)->operand_source(14).type());
  }
  if (auto vec_type = (*this)->operand_source(15).type().dyn_cast<pir::VectorType>()) {
      for (size_t i = 0; i < vec_type.size(); ++i) {
        IR_ENFORCE(vec_type[i].isa<paddle::dialect::DenseTensorType>(),
                       "Type validation failed for the 15th input, got %s.", (*this)->operand_source(15).type());
      }
  }
  else {
    IR_ENFORCE((*this)->operand_source(15).type().isa<paddle::dialect::DenseTensorType>(),
                   "Type validation failed for the 15th input, got %s.", (*this)->operand_source(15).type());
  }
  if (auto vec_type = (*this)->operand_source(16).type().dyn_cast<pir::VectorType>()) {
      for (size_t i = 0; i < vec_type.size(); ++i) {
        IR_ENFORCE(vec_type[i].isa<paddle::dialect::DenseTensorType>(),
                       "Type validation failed for the 16th input, got %s.", (*this)->operand_source(16).type());
      }
  }
  else {
    IR_ENFORCE((*this)->operand_source(16).type().isa<paddle::dialect::DenseTensorType>(),
                   "Type validation failed for the 16th input, got %s.", (*this)->operand_source(16).type());
  }
  if (auto val =  (*this)->operand(17)) {
    if (auto vec_type = val.type().dyn_cast<pir::VectorType>()) {
      for (size_t i = 0; i < vec_type.size(); i++) {
        IR_ENFORCE(vec_type[i].isa<paddle::dialect::DenseTensorType>(),
                          "Type validation failed for the 17th input, got %s.", (*this)->operand_source(17).type());
      }
    }
    else {
      IR_ENFORCE(val.type().isa<paddle::dialect::DenseTensorType>(),
                        "Type validation failed for the 17th input, got %s.", (*this)->operand_source(17).type());
    }
  }
  if (auto val =  (*this)->operand(18)) {
    if (auto vec_type = val.type().dyn_cast<pir::VectorType>()) {
      for (size_t i = 0; i < vec_type.size(); i++) {
        IR_ENFORCE(vec_type[i].isa<paddle::dialect::DenseTensorType>(),
                          "Type validation failed for the 18th input, got %s.", (*this)->operand_source(18).type());
      }
    }
    else {
      IR_ENFORCE(val.type().isa<paddle::dialect::DenseTensorType>(),
                        "Type validation failed for the 18th input, got %s.", (*this)->operand_source(18).type());
    }
  }
  if (auto val = (*this)->operand(19)) {
    IR_ENFORCE(val.type().isa<paddle::dialect::DenseTensorType>(),
                   "Type validation failed for the 19th input, got %s.", (*this)->operand_source(19).type());
  }
  if (auto val = (*this)->operand(20)) {
    IR_ENFORCE(val.type().isa<paddle::dialect::DenseTensorType>(),
                   "Type validation failed for the 20th input, got %s.", (*this)->operand_source(20).type());
  }
  if (auto val = (*this)->operand(21)) {
    IR_ENFORCE(val.type().isa<paddle::dialect::DenseTensorType>(),
                   "Type validation failed for the 21th input, got %s.", (*this)->operand_source(21).type());
  }
  if (auto val = (*this)->operand(22)) {
    IR_ENFORCE(val.type().isa<paddle::dialect::DenseTensorType>(),
                   "Type validation failed for the 22th input, got %s.", (*this)->operand_source(22).type());
  }
  if (auto val = (*this)->operand(23)) {
    IR_ENFORCE(val.type().isa<paddle::dialect::DenseTensorType>(),
                   "Type validation failed for the 23th input, got %s.", (*this)->operand_source(23).type());
  }
  IR_ENFORCE((*this)->operand_source(24).type().isa<paddle::dialect::DenseTensorType>(),
                  "Type validation failed for the 24th input, got %s.", (*this)->operand_source(24).type());
  }
  VLOG(4) << "Verifying attributes:";
  {
  auto& attributes = this->attributes();
  IR_ENFORCE(attributes.count("pre_layer_norm")>0,
                 "pre_layer_norm does not exist.");
  IR_ENFORCE(attributes.at("pre_layer_norm").isa<pir::BoolAttribute>(),
                 "Type of attribute: pre_layer_norm is not pir::BoolAttribute.");

  IR_ENFORCE(attributes.count("rotary_emb_dims")>0,
                 "rotary_emb_dims does not exist.");
  IR_ENFORCE(attributes.at("rotary_emb_dims").isa<pir::Int32Attribute>(),
                 "Type of attribute: rotary_emb_dims is not pir::Int32Attribute.");

  IR_ENFORCE(attributes.count("epsilon")>0,
                 "epsilon does not exist.");
  IR_ENFORCE(attributes.at("epsilon").isa<pir::FloatAttribute>(),
                 "Type of attribute: epsilon is not pir::FloatAttribute.");

  IR_ENFORCE(attributes.count("dropout_rate")>0,
                 "dropout_rate does not exist.");
  IR_ENFORCE(attributes.at("dropout_rate").isa<pir::FloatAttribute>(),
                 "Type of attribute: dropout_rate is not pir::FloatAttribute.");

  IR_ENFORCE(attributes.count("is_test")>0,
                 "is_test does not exist.");
  IR_ENFORCE(attributes.at("is_test").isa<pir::BoolAttribute>(),
                 "Type of attribute: is_test is not pir::BoolAttribute.");

  IR_ENFORCE(attributes.count("dropout_implementation")>0,
                 "dropout_implementation does not exist.");
  IR_ENFORCE(attributes.at("dropout_implementation").isa<pir::StrAttribute>(),
                 "Type of attribute: dropout_implementation is not pir::StrAttribute.");

  IR_ENFORCE(attributes.count("act_method")>0,
                 "act_method does not exist.");
  IR_ENFORCE(attributes.at("act_method").isa<pir::StrAttribute>(),
                 "Type of attribute: act_method is not pir::StrAttribute.");

  IR_ENFORCE(attributes.count("trans_qkvw")>0,
                 "trans_qkvw does not exist.");
  IR_ENFORCE(attributes.at("trans_qkvw").isa<pir::BoolAttribute>(),
                 "Type of attribute: trans_qkvw is not pir::BoolAttribute.");

  IR_ENFORCE(attributes.count("ring_id")>0,
                 "ring_id does not exist.");
  IR_ENFORCE(attributes.at("ring_id").isa<pir::Int32Attribute>(),
                 "Type of attribute: ring_id is not pir::Int32Attribute.");

  IR_ENFORCE(attributes.count("gather_axis")>0,
                 "gather_axis does not exist.");
  IR_ENFORCE(attributes.at("gather_axis").isa<pir::Int32Attribute>(),
                 "Type of attribute: gather_axis is not pir::Int32Attribute.");

  }
  VLOG(4) << "Verifying outputs:";
  {
  auto output_size = num_results();
  IR_ENFORCE(output_size == 2u,
                    "The size %d of outputs must be equal to 2.", output_size);
  IR_ENFORCE((*this)->result(0).type().isa<paddle::dialect::DenseTensorType>(),
                 "Type validation failed for the 0th output.");
  auto output_1_type = (*this)->result(1).type();
  if (auto vec_type = output_1_type.dyn_cast<pir::VectorType>()) {
    for (size_t i = 0; i < vec_type.size(); i++) {
      IR_ENFORCE(vec_type[i].isa<paddle::dialect::DenseTensorType>(),
                     "Type validation failed for the 1th output.");
    }
  }
  else {
    IR_ENFORCE(output_1_type.isa<paddle::dialect::DenseTensorType>(),
                   "Type validation failed for the 1th output.");
  }
  }
  VLOG(4) << "End Verifying for: FusedMultiTransformerXpuOp.";
}

void FusedMultiTransformerXpuOp::InferMeta( phi::InferMetaContext *infer_meta ) {
  auto fn = PD_INFER_META(phi::FusedMultiTransformerXpuInferMeta);
  fn(infer_meta);
}

phi::DataType FusedMultiTransformerXpuOp::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: FusedMultiTransformerXpuOp";
  


  return expected_kernel_dtype;
}

const char *FusedRotaryPositionEmbeddingOp::attributes_name[1] = { "use_neox_rotary_style" };

OpInfoTuple FusedRotaryPositionEmbeddingOp::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("q", "paddle::dialect::DenseTensorType", false, false, false, true), paddle::dialect::OpInputInfo("k", "paddle::dialect::DenseTensorType", true, false, false, true), paddle::dialect::OpInputInfo("v", "paddle::dialect::DenseTensorType", true, false, false, true), paddle::dialect::OpInputInfo("sin", "paddle::dialect::DenseTensorType", true, false, false, false), paddle::dialect::OpInputInfo("cos", "paddle::dialect::DenseTensorType", true, false, false, false), paddle::dialect::OpInputInfo("position_ids", "paddle::dialect::DenseTensorType", true, false, false, false) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = { paddle::dialect::OpAttributeInfo("use_neox_rotary_style", "pir::BoolAttribute", "") };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("out_q", "paddle::dialect::DenseTensorType", false, false), paddle::dialect::OpOutputInfo("out_k", "paddle::dialect::DenseTensorType", true, false), paddle::dialect::OpOutputInfo("out_v", "paddle::dialect::DenseTensorType", true, false) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("FusedRopeInferMeta", {"q", "k", "v", "sin", "cos", "position_ids", "use_neox_rotary_style"}, "fused_rotary_position_embedding", {"q", "k", "v", "sin", "cos", "position_ids", "use_neox_rotary_style"}, {"q"}, {}, {}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "fused_rotary_position_embedding");
}

void FusedRotaryPositionEmbeddingOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value q_, pir::Value k_, pir::Value v_, pir::Value sin_, pir::Value cos_, pir::Value position_ids_, bool use_neox_rotary_style) {
  VLOG(4) << "Start build FusedRotaryPositionEmbeddingOp";



  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {q_, k_, v_, sin_, cos_, position_ids_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_use_neox_rotary_style = pir::BoolAttribute::get(pir::IrContext::Instance(), use_neox_rotary_style);
  argument.AddAttribute("use_neox_rotary_style", attr_use_neox_rotary_style);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType q = q_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)q;

  VLOG(4) << "Builder construction  dense_q";
  paddle::dialect::IrTensor ir_tensor_q(paddle::dialect::TransToPhiDataType(q.dtype()),
                                                      q.dims(),
                                                      q.data_layout(),
                                                      q.lod(),
                                                      q.offset());
  VLOG(4) << "Builder construction  meta_q";
  paddle::dialect::IrMetaTensor meta_q(&ir_tensor_q);

  paddle::dialect::IrMetaTensor meta_k;
  paddle::dialect::IrTensor ir_tensor_k;
  if (k_.impl() != nullptr) {
    paddle::dialect::DenseTensorType k = k_.type().dyn_cast<paddle::dialect::DenseTensorType>();
    VLOG(4) << "Builder construction  dense_k";
    ir_tensor_k = paddle::dialect::IrTensor(paddle::dialect::TransToPhiDataType(k.dtype()),
                                                        k.dims(),
                                                        k.data_layout(),
                                                        k.lod(),
                                                        k.offset());
    VLOG(4) << "Builder construction  meta_k";
    meta_k = paddle::dialect::IrMetaTensor(&ir_tensor_k);
  }


  paddle::dialect::IrMetaTensor meta_v;
  paddle::dialect::IrTensor ir_tensor_v;
  if (v_.impl() != nullptr) {
    paddle::dialect::DenseTensorType v = v_.type().dyn_cast<paddle::dialect::DenseTensorType>();
    VLOG(4) << "Builder construction  dense_v";
    ir_tensor_v = paddle::dialect::IrTensor(paddle::dialect::TransToPhiDataType(v.dtype()),
                                                        v.dims(),
                                                        v.data_layout(),
                                                        v.lod(),
                                                        v.offset());
    VLOG(4) << "Builder construction  meta_v";
    meta_v = paddle::dialect::IrMetaTensor(&ir_tensor_v);
  }


  paddle::dialect::IrMetaTensor meta_sin;
  paddle::dialect::IrTensor ir_tensor_sin;
  if (sin_.impl() != nullptr) {
    paddle::dialect::DenseTensorType sin = sin_.type().dyn_cast<paddle::dialect::DenseTensorType>();
    VLOG(4) << "Builder construction  dense_sin";
    ir_tensor_sin = paddle::dialect::IrTensor(paddle::dialect::TransToPhiDataType(sin.dtype()),
                                                        sin.dims(),
                                                        sin.data_layout(),
                                                        sin.lod(),
                                                        sin.offset());
    VLOG(4) << "Builder construction  meta_sin";
    meta_sin = paddle::dialect::IrMetaTensor(&ir_tensor_sin);
  }


  paddle::dialect::IrMetaTensor meta_cos;
  paddle::dialect::IrTensor ir_tensor_cos;
  if (cos_.impl() != nullptr) {
    paddle::dialect::DenseTensorType cos = cos_.type().dyn_cast<paddle::dialect::DenseTensorType>();
    VLOG(4) << "Builder construction  dense_cos";
    ir_tensor_cos = paddle::dialect::IrTensor(paddle::dialect::TransToPhiDataType(cos.dtype()),
                                                        cos.dims(),
                                                        cos.data_layout(),
                                                        cos.lod(),
                                                        cos.offset());
    VLOG(4) << "Builder construction  meta_cos";
    meta_cos = paddle::dialect::IrMetaTensor(&ir_tensor_cos);
  }


  paddle::dialect::IrMetaTensor meta_position_ids;
  paddle::dialect::IrTensor ir_tensor_position_ids;
  if (position_ids_.impl() != nullptr) {
    paddle::dialect::DenseTensorType position_ids = position_ids_.type().dyn_cast<paddle::dialect::DenseTensorType>();
    VLOG(4) << "Builder construction  dense_position_ids";
    ir_tensor_position_ids = paddle::dialect::IrTensor(paddle::dialect::TransToPhiDataType(position_ids.dtype()),
                                                        position_ids.dims(),
                                                        position_ids.data_layout(),
                                                        position_ids.lod(),
                                                        position_ids.offset());
    VLOG(4) << "Builder construction  meta_position_ids";
    meta_position_ids = paddle::dialect::IrMetaTensor(&ir_tensor_position_ids);
  }

  paddle::dialect::IrTensor dense_out_q;
  paddle::dialect::IrMetaTensor meta_out_q(&dense_out_q);
  paddle::dialect::IrTensor dense_out_k;
  paddle::dialect::IrMetaTensor meta_out_k(&dense_out_k);
  paddle::dialect::IrTensor dense_out_v;
  paddle::dialect::IrMetaTensor meta_out_v(&dense_out_v);

  phi::FusedRopeInferMeta(meta_q, meta_k, meta_v, meta_sin, meta_cos, meta_position_ids, use_neox_rotary_style, &meta_out_q, &meta_out_k, &meta_out_v);

  std::vector<pir::Type> argument_outputs;
  pir::Type out_q_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_out_q.dtype()), dense_out_q.dims(), dense_out_q.layout(), dense_out_q.lod(), dense_out_q.offset());
  argument_outputs.push_back(out_q_dense_tensor_type);

  pir::Type out_k_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_out_k.dtype()), dense_out_k.dims(), dense_out_k.layout(), dense_out_k.lod(), dense_out_k.offset());
  argument_outputs.push_back(out_k_dense_tensor_type);

  pir::Type out_v_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_out_v.dtype()), dense_out_v.dims(), dense_out_v.layout(), dense_out_v.lod(), dense_out_v.offset());
  argument_outputs.push_back(out_v_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void FusedRotaryPositionEmbeddingOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value q_, pir::Value k_, pir::Value v_, pir::Value sin_, pir::Value cos_, pir::Value position_ids_, pir::AttributeMap attributes) {
  VLOG(4) << "Start build FusedRotaryPositionEmbeddingOp";


  IR_ENFORCE(
      attributes.find("use_neox_rotary_style") != attributes.end(),
          "'use_neox_rotary_style' Attribute is expected for FusedRotaryPositionEmbeddingOp. ");
  bool use_neox_rotary_style = attributes.at("use_neox_rotary_style").dyn_cast<pir::BoolAttribute>().data();


  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {q_, k_, v_, sin_, cos_, position_ids_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_use_neox_rotary_style = pir::BoolAttribute::get(pir::IrContext::Instance(), use_neox_rotary_style);
  argument.AddAttribute("use_neox_rotary_style", attr_use_neox_rotary_style);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType q = q_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)q;

  VLOG(4) << "Builder construction  dense_q";
  paddle::dialect::IrTensor ir_tensor_q(paddle::dialect::TransToPhiDataType(q.dtype()),
                                                      q.dims(),
                                                      q.data_layout(),
                                                      q.lod(),
                                                      q.offset());
  VLOG(4) << "Builder construction  meta_q";
  paddle::dialect::IrMetaTensor meta_q(&ir_tensor_q);

  paddle::dialect::IrMetaTensor meta_k;
  paddle::dialect::IrTensor ir_tensor_k;
  if (k_.impl() != nullptr) {
    paddle::dialect::DenseTensorType k = k_.type().dyn_cast<paddle::dialect::DenseTensorType>();
    VLOG(4) << "Builder construction  dense_k";
    ir_tensor_k = paddle::dialect::IrTensor(paddle::dialect::TransToPhiDataType(k.dtype()),
                                                        k.dims(),
                                                        k.data_layout(),
                                                        k.lod(),
                                                        k.offset());
    VLOG(4) << "Builder construction  meta_k";
    meta_k = paddle::dialect::IrMetaTensor(&ir_tensor_k);
  }


  paddle::dialect::IrMetaTensor meta_v;
  paddle::dialect::IrTensor ir_tensor_v;
  if (v_.impl() != nullptr) {
    paddle::dialect::DenseTensorType v = v_.type().dyn_cast<paddle::dialect::DenseTensorType>();
    VLOG(4) << "Builder construction  dense_v";
    ir_tensor_v = paddle::dialect::IrTensor(paddle::dialect::TransToPhiDataType(v.dtype()),
                                                        v.dims(),
                                                        v.data_layout(),
                                                        v.lod(),
                                                        v.offset());
    VLOG(4) << "Builder construction  meta_v";
    meta_v = paddle::dialect::IrMetaTensor(&ir_tensor_v);
  }


  paddle::dialect::IrMetaTensor meta_sin;
  paddle::dialect::IrTensor ir_tensor_sin;
  if (sin_.impl() != nullptr) {
    paddle::dialect::DenseTensorType sin = sin_.type().dyn_cast<paddle::dialect::DenseTensorType>();
    VLOG(4) << "Builder construction  dense_sin";
    ir_tensor_sin = paddle::dialect::IrTensor(paddle::dialect::TransToPhiDataType(sin.dtype()),
                                                        sin.dims(),
                                                        sin.data_layout(),
                                                        sin.lod(),
                                                        sin.offset());
    VLOG(4) << "Builder construction  meta_sin";
    meta_sin = paddle::dialect::IrMetaTensor(&ir_tensor_sin);
  }


  paddle::dialect::IrMetaTensor meta_cos;
  paddle::dialect::IrTensor ir_tensor_cos;
  if (cos_.impl() != nullptr) {
    paddle::dialect::DenseTensorType cos = cos_.type().dyn_cast<paddle::dialect::DenseTensorType>();
    VLOG(4) << "Builder construction  dense_cos";
    ir_tensor_cos = paddle::dialect::IrTensor(paddle::dialect::TransToPhiDataType(cos.dtype()),
                                                        cos.dims(),
                                                        cos.data_layout(),
                                                        cos.lod(),
                                                        cos.offset());
    VLOG(4) << "Builder construction  meta_cos";
    meta_cos = paddle::dialect::IrMetaTensor(&ir_tensor_cos);
  }


  paddle::dialect::IrMetaTensor meta_position_ids;
  paddle::dialect::IrTensor ir_tensor_position_ids;
  if (position_ids_.impl() != nullptr) {
    paddle::dialect::DenseTensorType position_ids = position_ids_.type().dyn_cast<paddle::dialect::DenseTensorType>();
    VLOG(4) << "Builder construction  dense_position_ids";
    ir_tensor_position_ids = paddle::dialect::IrTensor(paddle::dialect::TransToPhiDataType(position_ids.dtype()),
                                                        position_ids.dims(),
                                                        position_ids.data_layout(),
                                                        position_ids.lod(),
                                                        position_ids.offset());
    VLOG(4) << "Builder construction  meta_position_ids";
    meta_position_ids = paddle::dialect::IrMetaTensor(&ir_tensor_position_ids);
  }

  paddle::dialect::IrTensor dense_out_q;
  paddle::dialect::IrMetaTensor meta_out_q(&dense_out_q);
  paddle::dialect::IrTensor dense_out_k;
  paddle::dialect::IrMetaTensor meta_out_k(&dense_out_k);
  paddle::dialect::IrTensor dense_out_v;
  paddle::dialect::IrMetaTensor meta_out_v(&dense_out_v);

  phi::FusedRopeInferMeta(meta_q, meta_k, meta_v, meta_sin, meta_cos, meta_position_ids, use_neox_rotary_style, &meta_out_q, &meta_out_k, &meta_out_v);

  std::vector<pir::Type> argument_outputs;
  pir::Type out_q_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_out_q.dtype()), dense_out_q.dims(), dense_out_q.layout(), dense_out_q.lod(), dense_out_q.offset());
  argument_outputs.push_back(out_q_dense_tensor_type);

  pir::Type out_k_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_out_k.dtype()), dense_out_k.dims(), dense_out_k.layout(), dense_out_k.lod(), dense_out_k.offset());
  argument_outputs.push_back(out_k_dense_tensor_type);

  pir::Type out_v_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_out_v.dtype()), dense_out_v.dims(), dense_out_v.layout(), dense_out_v.lod(), dense_out_v.offset());
  argument_outputs.push_back(out_v_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void FusedRotaryPositionEmbeddingOp::VerifySig() {
  VLOG(4) << "Start Verifying inputs, outputs and attributes for: FusedRotaryPositionEmbeddingOp.";
  VLOG(4) << "Verifying inputs:";
  {
  auto input_size = num_operands();
  IR_ENFORCE(input_size == 6u,
                    "The size %d of inputs must be equal to 6.", input_size);
  IR_ENFORCE((*this)->operand_source(0).type().isa<paddle::dialect::DenseTensorType>(),
                  "Type validation failed for the 0th input, got %s.", (*this)->operand_source(0).type());
  if (auto val = (*this)->operand(1)) {
    IR_ENFORCE(val.type().isa<paddle::dialect::DenseTensorType>(),
                   "Type validation failed for the 1th input, got %s.", (*this)->operand_source(1).type());
  }
  if (auto val = (*this)->operand(2)) {
    IR_ENFORCE(val.type().isa<paddle::dialect::DenseTensorType>(),
                   "Type validation failed for the 2th input, got %s.", (*this)->operand_source(2).type());
  }
  if (auto val = (*this)->operand(3)) {
    IR_ENFORCE(val.type().isa<paddle::dialect::DenseTensorType>(),
                   "Type validation failed for the 3th input, got %s.", (*this)->operand_source(3).type());
  }
  if (auto val = (*this)->operand(4)) {
    IR_ENFORCE(val.type().isa<paddle::dialect::DenseTensorType>(),
                   "Type validation failed for the 4th input, got %s.", (*this)->operand_source(4).type());
  }
  if (auto val = (*this)->operand(5)) {
    IR_ENFORCE(val.type().isa<paddle::dialect::DenseTensorType>(),
                   "Type validation failed for the 5th input, got %s.", (*this)->operand_source(5).type());
  }
  }
  VLOG(4) << "Verifying attributes:";
  {
  auto& attributes = this->attributes();
  IR_ENFORCE(attributes.count("use_neox_rotary_style")>0,
                 "use_neox_rotary_style does not exist.");
  IR_ENFORCE(attributes.at("use_neox_rotary_style").isa<pir::BoolAttribute>(),
                 "Type of attribute: use_neox_rotary_style is not pir::BoolAttribute.");

  }
  VLOG(4) << "Verifying outputs:";
  {
  auto output_size = num_results();
  IR_ENFORCE(output_size == 3u,
                    "The size %d of outputs must be equal to 3.", output_size);
  IR_ENFORCE((*this)->result(0).type().isa<paddle::dialect::DenseTensorType>(),
                 "Type validation failed for the 0th output.");
  if (auto output_1_type = (*this)->result(1).type()) {
    IR_ENFORCE(output_1_type.isa<paddle::dialect::DenseTensorType>(),
                   "Type validation failed for the 1th output.");
  }
  if (auto output_2_type = (*this)->result(2).type()) {
    IR_ENFORCE(output_2_type.isa<paddle::dialect::DenseTensorType>(),
                   "Type validation failed for the 2th output.");
  }
  }
  VLOG(4) << "End Verifying for: FusedRotaryPositionEmbeddingOp.";
}

void FusedRotaryPositionEmbeddingOp::InferMeta( phi::InferMetaContext *infer_meta ) {
  auto fn = PD_INFER_META(phi::FusedRopeInferMeta);
  fn(infer_meta);
}

phi::DataType FusedRotaryPositionEmbeddingOp::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: FusedRotaryPositionEmbeddingOp";
  


  return expected_kernel_dtype;
}

const char *FusedScaleBiasAddReluOp::attributes_name[2] = { "fuse_dual", "exhaustive_search" };

OpInfoTuple FusedScaleBiasAddReluOp::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("x1", "paddle::dialect::DenseTensorType", false, false, false, false), paddle::dialect::OpInputInfo("scale1", "paddle::dialect::DenseTensorType", false, false, false, false), paddle::dialect::OpInputInfo("bias1", "paddle::dialect::DenseTensorType", false, false, false, false), paddle::dialect::OpInputInfo("x2", "paddle::dialect::DenseTensorType", false, false, false, false), paddle::dialect::OpInputInfo("scale2", "paddle::dialect::DenseTensorType", true, false, false, false), paddle::dialect::OpInputInfo("bias2", "paddle::dialect::DenseTensorType", true, false, false, false) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = { paddle::dialect::OpAttributeInfo("fuse_dual", "pir::BoolAttribute", ""), paddle::dialect::OpAttributeInfo("exhaustive_search", "pir::BoolAttribute", "") };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("out", "paddle::dialect::DenseTensorType", false, false) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("FusedScaleBiasAddReluInferMeta", {"x1", "scale1", "bias1", "x2", "scale2", "bias2", "fuse_dual", "exhaustive_search"}, "fused_scale_bias_add_relu", {"x1", "scale1", "bias1", "x2", "scale2", "bias2", "fuse_dual", "exhaustive_search"}, {"x1"}, {}, {}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "fused_scale_bias_add_relu");
}

void FusedScaleBiasAddReluOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x1_, pir::Value scale1_, pir::Value bias1_, pir::Value x2_, pir::Value scale2_, pir::Value bias2_, bool fuse_dual, bool exhaustive_search) {
  VLOG(4) << "Start build FusedScaleBiasAddReluOp";



  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x1_, scale1_, bias1_, x2_, scale2_, bias2_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_fuse_dual = pir::BoolAttribute::get(pir::IrContext::Instance(), fuse_dual);
  argument.AddAttribute("fuse_dual", attr_fuse_dual);
  pir::Attribute attr_exhaustive_search = pir::BoolAttribute::get(pir::IrContext::Instance(), exhaustive_search);
  argument.AddAttribute("exhaustive_search", attr_exhaustive_search);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x1 = x1_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x1;
  paddle::dialect::DenseTensorType scale1 = scale1_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)scale1;
  paddle::dialect::DenseTensorType bias1 = bias1_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)bias1;
  paddle::dialect::DenseTensorType x2 = x2_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x2;

  VLOG(4) << "Builder construction  dense_x1";
  paddle::dialect::IrTensor ir_tensor_x1(paddle::dialect::TransToPhiDataType(x1.dtype()),
                                                      x1.dims(),
                                                      x1.data_layout(),
                                                      x1.lod(),
                                                      x1.offset());
  VLOG(4) << "Builder construction  meta_x1";
  paddle::dialect::IrMetaTensor meta_x1(&ir_tensor_x1);

  VLOG(4) << "Builder construction  dense_scale1";
  paddle::dialect::IrTensor ir_tensor_scale1(paddle::dialect::TransToPhiDataType(scale1.dtype()),
                                                      scale1.dims(),
                                                      scale1.data_layout(),
                                                      scale1.lod(),
                                                      scale1.offset());
  VLOG(4) << "Builder construction  meta_scale1";
  paddle::dialect::IrMetaTensor meta_scale1(&ir_tensor_scale1);

  VLOG(4) << "Builder construction  dense_bias1";
  paddle::dialect::IrTensor ir_tensor_bias1(paddle::dialect::TransToPhiDataType(bias1.dtype()),
                                                      bias1.dims(),
                                                      bias1.data_layout(),
                                                      bias1.lod(),
                                                      bias1.offset());
  VLOG(4) << "Builder construction  meta_bias1";
  paddle::dialect::IrMetaTensor meta_bias1(&ir_tensor_bias1);

  VLOG(4) << "Builder construction  dense_x2";
  paddle::dialect::IrTensor ir_tensor_x2(paddle::dialect::TransToPhiDataType(x2.dtype()),
                                                      x2.dims(),
                                                      x2.data_layout(),
                                                      x2.lod(),
                                                      x2.offset());
  VLOG(4) << "Builder construction  meta_x2";
  paddle::dialect::IrMetaTensor meta_x2(&ir_tensor_x2);

  paddle::dialect::IrMetaTensor meta_scale2;
  paddle::dialect::IrTensor ir_tensor_scale2;
  if (scale2_.impl() != nullptr) {
    paddle::dialect::DenseTensorType scale2 = scale2_.type().dyn_cast<paddle::dialect::DenseTensorType>();
    VLOG(4) << "Builder construction  dense_scale2";
    ir_tensor_scale2 = paddle::dialect::IrTensor(paddle::dialect::TransToPhiDataType(scale2.dtype()),
                                                        scale2.dims(),
                                                        scale2.data_layout(),
                                                        scale2.lod(),
                                                        scale2.offset());
    VLOG(4) << "Builder construction  meta_scale2";
    meta_scale2 = paddle::dialect::IrMetaTensor(&ir_tensor_scale2);
  }


  paddle::dialect::IrMetaTensor meta_bias2;
  paddle::dialect::IrTensor ir_tensor_bias2;
  if (bias2_.impl() != nullptr) {
    paddle::dialect::DenseTensorType bias2 = bias2_.type().dyn_cast<paddle::dialect::DenseTensorType>();
    VLOG(4) << "Builder construction  dense_bias2";
    ir_tensor_bias2 = paddle::dialect::IrTensor(paddle::dialect::TransToPhiDataType(bias2.dtype()),
                                                        bias2.dims(),
                                                        bias2.data_layout(),
                                                        bias2.lod(),
                                                        bias2.offset());
    VLOG(4) << "Builder construction  meta_bias2";
    meta_bias2 = paddle::dialect::IrMetaTensor(&ir_tensor_bias2);
  }

  paddle::dialect::IrTensor dense_out;
  paddle::dialect::IrMetaTensor meta_out(&dense_out);

  phi::FusedScaleBiasAddReluInferMeta(meta_x1, meta_scale1, meta_bias1, meta_x2, meta_scale2, meta_bias2, fuse_dual, exhaustive_search, &meta_out);

  std::vector<pir::Type> argument_outputs;
  pir::Type out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_out.dtype()), dense_out.dims(), dense_out.layout(), dense_out.lod(), dense_out.offset());
  argument_outputs.push_back(out_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void FusedScaleBiasAddReluOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x1_, pir::Value scale1_, pir::Value bias1_, pir::Value x2_, pir::Value scale2_, pir::Value bias2_, pir::AttributeMap attributes) {
  VLOG(4) << "Start build FusedScaleBiasAddReluOp";


  IR_ENFORCE(
      attributes.find("fuse_dual") != attributes.end(),
          "'fuse_dual' Attribute is expected for FusedScaleBiasAddReluOp. ");
  bool fuse_dual = attributes.at("fuse_dual").dyn_cast<pir::BoolAttribute>().data();

  IR_ENFORCE(
      attributes.find("exhaustive_search") != attributes.end(),
          "'exhaustive_search' Attribute is expected for FusedScaleBiasAddReluOp. ");
  bool exhaustive_search = attributes.at("exhaustive_search").dyn_cast<pir::BoolAttribute>().data();


  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x1_, scale1_, bias1_, x2_, scale2_, bias2_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_fuse_dual = pir::BoolAttribute::get(pir::IrContext::Instance(), fuse_dual);
  argument.AddAttribute("fuse_dual", attr_fuse_dual);
  pir::Attribute attr_exhaustive_search = pir::BoolAttribute::get(pir::IrContext::Instance(), exhaustive_search);
  argument.AddAttribute("exhaustive_search", attr_exhaustive_search);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x1 = x1_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x1;
  paddle::dialect::DenseTensorType scale1 = scale1_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)scale1;
  paddle::dialect::DenseTensorType bias1 = bias1_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)bias1;
  paddle::dialect::DenseTensorType x2 = x2_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x2;

  VLOG(4) << "Builder construction  dense_x1";
  paddle::dialect::IrTensor ir_tensor_x1(paddle::dialect::TransToPhiDataType(x1.dtype()),
                                                      x1.dims(),
                                                      x1.data_layout(),
                                                      x1.lod(),
                                                      x1.offset());
  VLOG(4) << "Builder construction  meta_x1";
  paddle::dialect::IrMetaTensor meta_x1(&ir_tensor_x1);

  VLOG(4) << "Builder construction  dense_scale1";
  paddle::dialect::IrTensor ir_tensor_scale1(paddle::dialect::TransToPhiDataType(scale1.dtype()),
                                                      scale1.dims(),
                                                      scale1.data_layout(),
                                                      scale1.lod(),
                                                      scale1.offset());
  VLOG(4) << "Builder construction  meta_scale1";
  paddle::dialect::IrMetaTensor meta_scale1(&ir_tensor_scale1);

  VLOG(4) << "Builder construction  dense_bias1";
  paddle::dialect::IrTensor ir_tensor_bias1(paddle::dialect::TransToPhiDataType(bias1.dtype()),
                                                      bias1.dims(),
                                                      bias1.data_layout(),
                                                      bias1.lod(),
                                                      bias1.offset());
  VLOG(4) << "Builder construction  meta_bias1";
  paddle::dialect::IrMetaTensor meta_bias1(&ir_tensor_bias1);

  VLOG(4) << "Builder construction  dense_x2";
  paddle::dialect::IrTensor ir_tensor_x2(paddle::dialect::TransToPhiDataType(x2.dtype()),
                                                      x2.dims(),
                                                      x2.data_layout(),
                                                      x2.lod(),
                                                      x2.offset());
  VLOG(4) << "Builder construction  meta_x2";
  paddle::dialect::IrMetaTensor meta_x2(&ir_tensor_x2);

  paddle::dialect::IrMetaTensor meta_scale2;
  paddle::dialect::IrTensor ir_tensor_scale2;
  if (scale2_.impl() != nullptr) {
    paddle::dialect::DenseTensorType scale2 = scale2_.type().dyn_cast<paddle::dialect::DenseTensorType>();
    VLOG(4) << "Builder construction  dense_scale2";
    ir_tensor_scale2 = paddle::dialect::IrTensor(paddle::dialect::TransToPhiDataType(scale2.dtype()),
                                                        scale2.dims(),
                                                        scale2.data_layout(),
                                                        scale2.lod(),
                                                        scale2.offset());
    VLOG(4) << "Builder construction  meta_scale2";
    meta_scale2 = paddle::dialect::IrMetaTensor(&ir_tensor_scale2);
  }


  paddle::dialect::IrMetaTensor meta_bias2;
  paddle::dialect::IrTensor ir_tensor_bias2;
  if (bias2_.impl() != nullptr) {
    paddle::dialect::DenseTensorType bias2 = bias2_.type().dyn_cast<paddle::dialect::DenseTensorType>();
    VLOG(4) << "Builder construction  dense_bias2";
    ir_tensor_bias2 = paddle::dialect::IrTensor(paddle::dialect::TransToPhiDataType(bias2.dtype()),
                                                        bias2.dims(),
                                                        bias2.data_layout(),
                                                        bias2.lod(),
                                                        bias2.offset());
    VLOG(4) << "Builder construction  meta_bias2";
    meta_bias2 = paddle::dialect::IrMetaTensor(&ir_tensor_bias2);
  }

  paddle::dialect::IrTensor dense_out;
  paddle::dialect::IrMetaTensor meta_out(&dense_out);

  phi::FusedScaleBiasAddReluInferMeta(meta_x1, meta_scale1, meta_bias1, meta_x2, meta_scale2, meta_bias2, fuse_dual, exhaustive_search, &meta_out);

  std::vector<pir::Type> argument_outputs;
  pir::Type out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_out.dtype()), dense_out.dims(), dense_out.layout(), dense_out.lod(), dense_out.offset());
  argument_outputs.push_back(out_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void FusedScaleBiasAddReluOp::VerifySig() {
  VLOG(4) << "Start Verifying inputs, outputs and attributes for: FusedScaleBiasAddReluOp.";
  VLOG(4) << "Verifying inputs:";
  {
  auto input_size = num_operands();
  IR_ENFORCE(input_size == 6u,
                    "The size %d of inputs must be equal to 6.", input_size);
  IR_ENFORCE((*this)->operand_source(0).type().isa<paddle::dialect::DenseTensorType>(),
                  "Type validation failed for the 0th input, got %s.", (*this)->operand_source(0).type());
  IR_ENFORCE((*this)->operand_source(1).type().isa<paddle::dialect::DenseTensorType>(),
                  "Type validation failed for the 1th input, got %s.", (*this)->operand_source(1).type());
  IR_ENFORCE((*this)->operand_source(2).type().isa<paddle::dialect::DenseTensorType>(),
                  "Type validation failed for the 2th input, got %s.", (*this)->operand_source(2).type());
  IR_ENFORCE((*this)->operand_source(3).type().isa<paddle::dialect::DenseTensorType>(),
                  "Type validation failed for the 3th input, got %s.", (*this)->operand_source(3).type());
  if (auto val = (*this)->operand(4)) {
    IR_ENFORCE(val.type().isa<paddle::dialect::DenseTensorType>(),
                   "Type validation failed for the 4th input, got %s.", (*this)->operand_source(4).type());
  }
  if (auto val = (*this)->operand(5)) {
    IR_ENFORCE(val.type().isa<paddle::dialect::DenseTensorType>(),
                   "Type validation failed for the 5th input, got %s.", (*this)->operand_source(5).type());
  }
  }
  VLOG(4) << "Verifying attributes:";
  {
  auto& attributes = this->attributes();
  IR_ENFORCE(attributes.count("fuse_dual")>0,
                 "fuse_dual does not exist.");
  IR_ENFORCE(attributes.at("fuse_dual").isa<pir::BoolAttribute>(),
                 "Type of attribute: fuse_dual is not pir::BoolAttribute.");

  IR_ENFORCE(attributes.count("exhaustive_search")>0,
                 "exhaustive_search does not exist.");
  IR_ENFORCE(attributes.at("exhaustive_search").isa<pir::BoolAttribute>(),
                 "Type of attribute: exhaustive_search is not pir::BoolAttribute.");

  }
  VLOG(4) << "Verifying outputs:";
  {
  auto output_size = num_results();
  IR_ENFORCE(output_size == 1u,
                    "The size %d of outputs must be equal to 1.", output_size);
  IR_ENFORCE((*this)->result(0).type().isa<paddle::dialect::DenseTensorType>(),
                 "Type validation failed for the 0th output.");
  }
  VLOG(4) << "End Verifying for: FusedScaleBiasAddReluOp.";
}

void FusedScaleBiasAddReluOp::InferMeta( phi::InferMetaContext *infer_meta ) {
  auto fn = PD_INFER_META(phi::FusedScaleBiasAddReluInferMeta);
  fn(infer_meta);
}

phi::DataType FusedScaleBiasAddReluOp::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: FusedScaleBiasAddReluOp";
  


  return expected_kernel_dtype;
}

const char *FusedScaleBiasReluConvBnOp::attributes_name[11] = { "paddings", "dilations", "strides", "padding_algorithm", "groups", "data_format", "momentum", "epsilon", "fuse_prologue", "exhaustive_search", "accumulation_count" };

OpInfoTuple FusedScaleBiasReluConvBnOp::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("x", "paddle::dialect::DenseTensorType", false, false, false, false), paddle::dialect::OpInputInfo("w", "paddle::dialect::DenseTensorType", false, false, false, false), paddle::dialect::OpInputInfo("scale", "paddle::dialect::DenseTensorType", true, false, false, false), paddle::dialect::OpInputInfo("bias", "paddle::dialect::DenseTensorType", true, false, false, false), paddle::dialect::OpInputInfo("bn_scale", "paddle::dialect::DenseTensorType", false, false, false, false), paddle::dialect::OpInputInfo("bn_bias", "paddle::dialect::DenseTensorType", false, false, false, false), paddle::dialect::OpInputInfo("input_running_mean", "paddle::dialect::DenseTensorType", false, false, false, false), paddle::dialect::OpInputInfo("input_running_var", "paddle::dialect::DenseTensorType", false, false, false, false) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = { paddle::dialect::OpAttributeInfo("paddings", "pir::ArrayAttribute<pir::Int32Attribute>", ""), paddle::dialect::OpAttributeInfo("dilations", "pir::ArrayAttribute<pir::Int32Attribute>", ""), paddle::dialect::OpAttributeInfo("strides", "pir::ArrayAttribute<pir::Int32Attribute>", ""), paddle::dialect::OpAttributeInfo("padding_algorithm", "pir::StrAttribute", ""), paddle::dialect::OpAttributeInfo("groups", "pir::Int32Attribute", ""), paddle::dialect::OpAttributeInfo("data_format", "pir::StrAttribute", ""), paddle::dialect::OpAttributeInfo("momentum", "pir::FloatAttribute", ""), paddle::dialect::OpAttributeInfo("epsilon", "pir::FloatAttribute", ""), paddle::dialect::OpAttributeInfo("fuse_prologue", "pir::BoolAttribute", ""), paddle::dialect::OpAttributeInfo("exhaustive_search", "pir::BoolAttribute", ""), paddle::dialect::OpAttributeInfo("accumulation_count", "pir::Int64Attribute", "") };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("out", "paddle::dialect::DenseTensorType", false, false), paddle::dialect::OpOutputInfo("out_running_mean", "paddle::dialect::DenseTensorType", false, false), paddle::dialect::OpOutputInfo("out_running_var", "paddle::dialect::DenseTensorType", false, false), paddle::dialect::OpOutputInfo("saved_mean", "paddle::dialect::DenseTensorType", false, false), paddle::dialect::OpOutputInfo("saved_var", "paddle::dialect::DenseTensorType", false, false), paddle::dialect::OpOutputInfo("eq_scale", "paddle::dialect::DenseTensorType", false, false), paddle::dialect::OpOutputInfo("eq_bias", "paddle::dialect::DenseTensorType", false, false) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("FusedScaleBiasReluConvBnInferMeta", {"x", "w", "scale", "bias", "bn_scale", "bn_bias", "input_running_mean", "input_running_var", "paddings", "dilations", "strides", "padding_algorithm", "groups", "data_format", "momentum", "epsilon", "fuse_prologue", "exhaustive_search", "accumulation_count"}, "fused_scale_bias_relu_conv_bn", {"x", "w", "scale", "bias", "bn_scale", "bn_bias", "input_running_mean", "input_running_var", "paddings", "dilations", "strides", "padding_algorithm", "groups", "data_format", "momentum", "epsilon", "fuse_prologue", "exhaustive_search", "accumulation_count"}, {"x"}, {}, {}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "fused_scale_bias_relu_conv_bn");
}

void FusedScaleBiasReluConvBnOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, pir::Value w_, pir::Value scale_, pir::Value bias_, pir::Value bn_scale_, pir::Value bn_bias_, pir::Value input_running_mean_, pir::Value input_running_var_, const std::vector<int>& paddings, const std::vector<int>& dilations, const std::vector<int>& strides, const std::string& padding_algorithm, int groups, const std::string& data_format, float momentum, float epsilon, bool fuse_prologue, bool exhaustive_search, int64_t accumulation_count) {
  VLOG(4) << "Start build FusedScaleBiasReluConvBnOp";



  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_, w_, scale_, bias_, bn_scale_, bn_bias_, input_running_mean_, input_running_var_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  std::vector<pir::Attribute> vec_paddings;
  for (size_t i = 0; i < static_cast<size_t>(paddings.size()); i++) {
      pir::Attribute attr_paddings = pir::Int32Attribute::get(pir::IrContext::Instance(), paddings[i]);

    vec_paddings.push_back(attr_paddings);
  }
  pir::Attribute attr_paddings = pir::ArrayAttribute::get(pir::IrContext::Instance(), vec_paddings);
  argument.AddAttribute("paddings", attr_paddings);
  std::vector<pir::Attribute> vec_dilations;
  for (size_t i = 0; i < static_cast<size_t>(dilations.size()); i++) {
      pir::Attribute attr_dilations = pir::Int32Attribute::get(pir::IrContext::Instance(), dilations[i]);

    vec_dilations.push_back(attr_dilations);
  }
  pir::Attribute attr_dilations = pir::ArrayAttribute::get(pir::IrContext::Instance(), vec_dilations);
  argument.AddAttribute("dilations", attr_dilations);
  std::vector<pir::Attribute> vec_strides;
  for (size_t i = 0; i < static_cast<size_t>(strides.size()); i++) {
      pir::Attribute attr_strides = pir::Int32Attribute::get(pir::IrContext::Instance(), strides[i]);

    vec_strides.push_back(attr_strides);
  }
  pir::Attribute attr_strides = pir::ArrayAttribute::get(pir::IrContext::Instance(), vec_strides);
  argument.AddAttribute("strides", attr_strides);
  pir::Attribute attr_padding_algorithm = pir::StrAttribute::get(pir::IrContext::Instance(), padding_algorithm);
  argument.AddAttribute("padding_algorithm", attr_padding_algorithm);
  pir::Attribute attr_groups = pir::Int32Attribute::get(pir::IrContext::Instance(), groups);
  argument.AddAttribute("groups", attr_groups);
  pir::Attribute attr_data_format = pir::StrAttribute::get(pir::IrContext::Instance(), data_format);
  argument.AddAttribute("data_format", attr_data_format);
  pir::Attribute attr_momentum = pir::FloatAttribute::get(pir::IrContext::Instance(), momentum);
  argument.AddAttribute("momentum", attr_momentum);
  pir::Attribute attr_epsilon = pir::FloatAttribute::get(pir::IrContext::Instance(), epsilon);
  argument.AddAttribute("epsilon", attr_epsilon);
  pir::Attribute attr_fuse_prologue = pir::BoolAttribute::get(pir::IrContext::Instance(), fuse_prologue);
  argument.AddAttribute("fuse_prologue", attr_fuse_prologue);
  pir::Attribute attr_exhaustive_search = pir::BoolAttribute::get(pir::IrContext::Instance(), exhaustive_search);
  argument.AddAttribute("exhaustive_search", attr_exhaustive_search);
  pir::Attribute attr_accumulation_count = pir::Int64Attribute::get(pir::IrContext::Instance(), accumulation_count);
  argument.AddAttribute("accumulation_count", attr_accumulation_count);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;
  paddle::dialect::DenseTensorType w = w_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)w;
  paddle::dialect::DenseTensorType bn_scale = bn_scale_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)bn_scale;
  paddle::dialect::DenseTensorType bn_bias = bn_bias_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)bn_bias;
  paddle::dialect::DenseTensorType input_running_mean = input_running_mean_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)input_running_mean;
  paddle::dialect::DenseTensorType input_running_var = input_running_var_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)input_running_var;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);

  VLOG(4) << "Builder construction  dense_w";
  paddle::dialect::IrTensor ir_tensor_w(paddle::dialect::TransToPhiDataType(w.dtype()),
                                                      w.dims(),
                                                      w.data_layout(),
                                                      w.lod(),
                                                      w.offset());
  VLOG(4) << "Builder construction  meta_w";
  paddle::dialect::IrMetaTensor meta_w(&ir_tensor_w);

  paddle::dialect::IrMetaTensor meta_scale;
  paddle::dialect::IrTensor ir_tensor_scale;
  if (scale_.impl() != nullptr) {
    paddle::dialect::DenseTensorType scale = scale_.type().dyn_cast<paddle::dialect::DenseTensorType>();
    VLOG(4) << "Builder construction  dense_scale";
    ir_tensor_scale = paddle::dialect::IrTensor(paddle::dialect::TransToPhiDataType(scale.dtype()),
                                                        scale.dims(),
                                                        scale.data_layout(),
                                                        scale.lod(),
                                                        scale.offset());
    VLOG(4) << "Builder construction  meta_scale";
    meta_scale = paddle::dialect::IrMetaTensor(&ir_tensor_scale);
  }


  paddle::dialect::IrMetaTensor meta_bias;
  paddle::dialect::IrTensor ir_tensor_bias;
  if (bias_.impl() != nullptr) {
    paddle::dialect::DenseTensorType bias = bias_.type().dyn_cast<paddle::dialect::DenseTensorType>();
    VLOG(4) << "Builder construction  dense_bias";
    ir_tensor_bias = paddle::dialect::IrTensor(paddle::dialect::TransToPhiDataType(bias.dtype()),
                                                        bias.dims(),
                                                        bias.data_layout(),
                                                        bias.lod(),
                                                        bias.offset());
    VLOG(4) << "Builder construction  meta_bias";
    meta_bias = paddle::dialect::IrMetaTensor(&ir_tensor_bias);
  }


  VLOG(4) << "Builder construction  dense_bn_scale";
  paddle::dialect::IrTensor ir_tensor_bn_scale(paddle::dialect::TransToPhiDataType(bn_scale.dtype()),
                                                      bn_scale.dims(),
                                                      bn_scale.data_layout(),
                                                      bn_scale.lod(),
                                                      bn_scale.offset());
  VLOG(4) << "Builder construction  meta_bn_scale";
  paddle::dialect::IrMetaTensor meta_bn_scale(&ir_tensor_bn_scale);

  VLOG(4) << "Builder construction  dense_bn_bias";
  paddle::dialect::IrTensor ir_tensor_bn_bias(paddle::dialect::TransToPhiDataType(bn_bias.dtype()),
                                                      bn_bias.dims(),
                                                      bn_bias.data_layout(),
                                                      bn_bias.lod(),
                                                      bn_bias.offset());
  VLOG(4) << "Builder construction  meta_bn_bias";
  paddle::dialect::IrMetaTensor meta_bn_bias(&ir_tensor_bn_bias);

  VLOG(4) << "Builder construction  dense_input_running_mean";
  paddle::dialect::IrTensor ir_tensor_input_running_mean(paddle::dialect::TransToPhiDataType(input_running_mean.dtype()),
                                                      input_running_mean.dims(),
                                                      input_running_mean.data_layout(),
                                                      input_running_mean.lod(),
                                                      input_running_mean.offset());
  VLOG(4) << "Builder construction  meta_input_running_mean";
  paddle::dialect::IrMetaTensor meta_input_running_mean(&ir_tensor_input_running_mean);

  VLOG(4) << "Builder construction  dense_input_running_var";
  paddle::dialect::IrTensor ir_tensor_input_running_var(paddle::dialect::TransToPhiDataType(input_running_var.dtype()),
                                                      input_running_var.dims(),
                                                      input_running_var.data_layout(),
                                                      input_running_var.lod(),
                                                      input_running_var.offset());
  VLOG(4) << "Builder construction  meta_input_running_var";
  paddle::dialect::IrMetaTensor meta_input_running_var(&ir_tensor_input_running_var);
  paddle::dialect::IrTensor dense_out;
  paddle::dialect::IrMetaTensor meta_out(&dense_out);
  paddle::dialect::IrTensor dense_out_running_mean;
  paddle::dialect::IrMetaTensor meta_out_running_mean(&dense_out_running_mean);
  paddle::dialect::IrTensor dense_out_running_var;
  paddle::dialect::IrMetaTensor meta_out_running_var(&dense_out_running_var);
  paddle::dialect::IrTensor dense_saved_mean;
  paddle::dialect::IrMetaTensor meta_saved_mean(&dense_saved_mean);
  paddle::dialect::IrTensor dense_saved_var;
  paddle::dialect::IrMetaTensor meta_saved_var(&dense_saved_var);
  paddle::dialect::IrTensor dense_eq_scale;
  paddle::dialect::IrMetaTensor meta_eq_scale(&dense_eq_scale);
  paddle::dialect::IrTensor dense_eq_bias;
  paddle::dialect::IrMetaTensor meta_eq_bias(&dense_eq_bias);

  phi::FusedScaleBiasReluConvBnInferMeta(meta_x, meta_w, meta_scale, meta_bias, meta_bn_scale, meta_bn_bias, meta_input_running_mean, meta_input_running_var, paddings, dilations, strides, padding_algorithm, groups, data_format, momentum, epsilon, fuse_prologue, exhaustive_search, accumulation_count, &meta_out, &meta_out_running_mean, &meta_out_running_var, &meta_saved_mean, &meta_saved_var, &meta_eq_scale, &meta_eq_bias);

  std::vector<pir::Type> argument_outputs;
  pir::Type out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_out.dtype()), dense_out.dims(), dense_out.layout(), dense_out.lod(), dense_out.offset());
  argument_outputs.push_back(out_dense_tensor_type);

  pir::Type out_running_mean_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_out_running_mean.dtype()), dense_out_running_mean.dims(), dense_out_running_mean.layout(), dense_out_running_mean.lod(), dense_out_running_mean.offset());
  argument_outputs.push_back(out_running_mean_dense_tensor_type);

  pir::Type out_running_var_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_out_running_var.dtype()), dense_out_running_var.dims(), dense_out_running_var.layout(), dense_out_running_var.lod(), dense_out_running_var.offset());
  argument_outputs.push_back(out_running_var_dense_tensor_type);

  pir::Type saved_mean_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_saved_mean.dtype()), dense_saved_mean.dims(), dense_saved_mean.layout(), dense_saved_mean.lod(), dense_saved_mean.offset());
  argument_outputs.push_back(saved_mean_dense_tensor_type);

  pir::Type saved_var_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_saved_var.dtype()), dense_saved_var.dims(), dense_saved_var.layout(), dense_saved_var.lod(), dense_saved_var.offset());
  argument_outputs.push_back(saved_var_dense_tensor_type);

  pir::Type eq_scale_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_eq_scale.dtype()), dense_eq_scale.dims(), dense_eq_scale.layout(), dense_eq_scale.lod(), dense_eq_scale.offset());
  argument_outputs.push_back(eq_scale_dense_tensor_type);

  pir::Type eq_bias_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_eq_bias.dtype()), dense_eq_bias.dims(), dense_eq_bias.layout(), dense_eq_bias.lod(), dense_eq_bias.offset());
  argument_outputs.push_back(eq_bias_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void FusedScaleBiasReluConvBnOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, pir::Value w_, pir::Value scale_, pir::Value bias_, pir::Value bn_scale_, pir::Value bn_bias_, pir::Value input_running_mean_, pir::Value input_running_var_, pir::AttributeMap attributes) {
  VLOG(4) << "Start build FusedScaleBiasReluConvBnOp";


  IR_ENFORCE(
      attributes.find("paddings") != attributes.end(),
          "'paddings' Attribute is expected for FusedScaleBiasReluConvBnOp. ");
  std::vector<int> paddings;
  for (size_t i = 0; i < attributes.at("paddings").dyn_cast<pir::ArrayAttribute>().size(); i++) {
    paddings.push_back(attributes.at("paddings").dyn_cast<pir::ArrayAttribute>().at(i).dyn_cast<pir::Int32Attribute>().data());
  }

  IR_ENFORCE(
      attributes.find("dilations") != attributes.end(),
          "'dilations' Attribute is expected for FusedScaleBiasReluConvBnOp. ");
  std::vector<int> dilations;
  for (size_t i = 0; i < attributes.at("dilations").dyn_cast<pir::ArrayAttribute>().size(); i++) {
    dilations.push_back(attributes.at("dilations").dyn_cast<pir::ArrayAttribute>().at(i).dyn_cast<pir::Int32Attribute>().data());
  }

  IR_ENFORCE(
      attributes.find("strides") != attributes.end(),
          "'strides' Attribute is expected for FusedScaleBiasReluConvBnOp. ");
  std::vector<int> strides;
  for (size_t i = 0; i < attributes.at("strides").dyn_cast<pir::ArrayAttribute>().size(); i++) {
    strides.push_back(attributes.at("strides").dyn_cast<pir::ArrayAttribute>().at(i).dyn_cast<pir::Int32Attribute>().data());
  }

  IR_ENFORCE(
      attributes.find("padding_algorithm") != attributes.end(),
          "'padding_algorithm' Attribute is expected for FusedScaleBiasReluConvBnOp. ");
  std::string padding_algorithm = attributes.at("padding_algorithm").dyn_cast<pir::StrAttribute>().AsString();

  IR_ENFORCE(
      attributes.find("groups") != attributes.end(),
          "'groups' Attribute is expected for FusedScaleBiasReluConvBnOp. ");
  int groups = attributes.at("groups").dyn_cast<pir::Int32Attribute>().data();

  IR_ENFORCE(
      attributes.find("data_format") != attributes.end(),
          "'data_format' Attribute is expected for FusedScaleBiasReluConvBnOp. ");
  std::string data_format = attributes.at("data_format").dyn_cast<pir::StrAttribute>().AsString();

  IR_ENFORCE(
      attributes.find("momentum") != attributes.end(),
          "'momentum' Attribute is expected for FusedScaleBiasReluConvBnOp. ");
  float momentum = attributes.at("momentum").dyn_cast<pir::FloatAttribute>().data();

  IR_ENFORCE(
      attributes.find("epsilon") != attributes.end(),
          "'epsilon' Attribute is expected for FusedScaleBiasReluConvBnOp. ");
  float epsilon = attributes.at("epsilon").dyn_cast<pir::FloatAttribute>().data();

  IR_ENFORCE(
      attributes.find("fuse_prologue") != attributes.end(),
          "'fuse_prologue' Attribute is expected for FusedScaleBiasReluConvBnOp. ");
  bool fuse_prologue = attributes.at("fuse_prologue").dyn_cast<pir::BoolAttribute>().data();

  IR_ENFORCE(
      attributes.find("exhaustive_search") != attributes.end(),
          "'exhaustive_search' Attribute is expected for FusedScaleBiasReluConvBnOp. ");
  bool exhaustive_search = attributes.at("exhaustive_search").dyn_cast<pir::BoolAttribute>().data();

  IR_ENFORCE(
      attributes.find("accumulation_count") != attributes.end(),
          "'accumulation_count' Attribute is expected for FusedScaleBiasReluConvBnOp. ");
  int64_t accumulation_count = attributes.at("accumulation_count").dyn_cast<pir::Int64Attribute>().data();


  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_, w_, scale_, bias_, bn_scale_, bn_bias_, input_running_mean_, input_running_var_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  std::vector<pir::Attribute> vec_paddings;
  for (size_t i = 0; i < static_cast<size_t>(paddings.size()); i++) {
      pir::Attribute attr_paddings = pir::Int32Attribute::get(pir::IrContext::Instance(), paddings[i]);

    vec_paddings.push_back(attr_paddings);
  }
  pir::Attribute attr_paddings = pir::ArrayAttribute::get(pir::IrContext::Instance(), vec_paddings);
  argument.AddAttribute("paddings", attr_paddings);
  std::vector<pir::Attribute> vec_dilations;
  for (size_t i = 0; i < static_cast<size_t>(dilations.size()); i++) {
      pir::Attribute attr_dilations = pir::Int32Attribute::get(pir::IrContext::Instance(), dilations[i]);

    vec_dilations.push_back(attr_dilations);
  }
  pir::Attribute attr_dilations = pir::ArrayAttribute::get(pir::IrContext::Instance(), vec_dilations);
  argument.AddAttribute("dilations", attr_dilations);
  std::vector<pir::Attribute> vec_strides;
  for (size_t i = 0; i < static_cast<size_t>(strides.size()); i++) {
      pir::Attribute attr_strides = pir::Int32Attribute::get(pir::IrContext::Instance(), strides[i]);

    vec_strides.push_back(attr_strides);
  }
  pir::Attribute attr_strides = pir::ArrayAttribute::get(pir::IrContext::Instance(), vec_strides);
  argument.AddAttribute("strides", attr_strides);
  pir::Attribute attr_padding_algorithm = pir::StrAttribute::get(pir::IrContext::Instance(), padding_algorithm);
  argument.AddAttribute("padding_algorithm", attr_padding_algorithm);
  pir::Attribute attr_groups = pir::Int32Attribute::get(pir::IrContext::Instance(), groups);
  argument.AddAttribute("groups", attr_groups);
  pir::Attribute attr_data_format = pir::StrAttribute::get(pir::IrContext::Instance(), data_format);
  argument.AddAttribute("data_format", attr_data_format);
  pir::Attribute attr_momentum = pir::FloatAttribute::get(pir::IrContext::Instance(), momentum);
  argument.AddAttribute("momentum", attr_momentum);
  pir::Attribute attr_epsilon = pir::FloatAttribute::get(pir::IrContext::Instance(), epsilon);
  argument.AddAttribute("epsilon", attr_epsilon);
  pir::Attribute attr_fuse_prologue = pir::BoolAttribute::get(pir::IrContext::Instance(), fuse_prologue);
  argument.AddAttribute("fuse_prologue", attr_fuse_prologue);
  pir::Attribute attr_exhaustive_search = pir::BoolAttribute::get(pir::IrContext::Instance(), exhaustive_search);
  argument.AddAttribute("exhaustive_search", attr_exhaustive_search);
  pir::Attribute attr_accumulation_count = pir::Int64Attribute::get(pir::IrContext::Instance(), accumulation_count);
  argument.AddAttribute("accumulation_count", attr_accumulation_count);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;
  paddle::dialect::DenseTensorType w = w_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)w;
  paddle::dialect::DenseTensorType bn_scale = bn_scale_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)bn_scale;
  paddle::dialect::DenseTensorType bn_bias = bn_bias_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)bn_bias;
  paddle::dialect::DenseTensorType input_running_mean = input_running_mean_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)input_running_mean;
  paddle::dialect::DenseTensorType input_running_var = input_running_var_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)input_running_var;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);

  VLOG(4) << "Builder construction  dense_w";
  paddle::dialect::IrTensor ir_tensor_w(paddle::dialect::TransToPhiDataType(w.dtype()),
                                                      w.dims(),
                                                      w.data_layout(),
                                                      w.lod(),
                                                      w.offset());
  VLOG(4) << "Builder construction  meta_w";
  paddle::dialect::IrMetaTensor meta_w(&ir_tensor_w);

  paddle::dialect::IrMetaTensor meta_scale;
  paddle::dialect::IrTensor ir_tensor_scale;
  if (scale_.impl() != nullptr) {
    paddle::dialect::DenseTensorType scale = scale_.type().dyn_cast<paddle::dialect::DenseTensorType>();
    VLOG(4) << "Builder construction  dense_scale";
    ir_tensor_scale = paddle::dialect::IrTensor(paddle::dialect::TransToPhiDataType(scale.dtype()),
                                                        scale.dims(),
                                                        scale.data_layout(),
                                                        scale.lod(),
                                                        scale.offset());
    VLOG(4) << "Builder construction  meta_scale";
    meta_scale = paddle::dialect::IrMetaTensor(&ir_tensor_scale);
  }


  paddle::dialect::IrMetaTensor meta_bias;
  paddle::dialect::IrTensor ir_tensor_bias;
  if (bias_.impl() != nullptr) {
    paddle::dialect::DenseTensorType bias = bias_.type().dyn_cast<paddle::dialect::DenseTensorType>();
    VLOG(4) << "Builder construction  dense_bias";
    ir_tensor_bias = paddle::dialect::IrTensor(paddle::dialect::TransToPhiDataType(bias.dtype()),
                                                        bias.dims(),
                                                        bias.data_layout(),
                                                        bias.lod(),
                                                        bias.offset());
    VLOG(4) << "Builder construction  meta_bias";
    meta_bias = paddle::dialect::IrMetaTensor(&ir_tensor_bias);
  }


  VLOG(4) << "Builder construction  dense_bn_scale";
  paddle::dialect::IrTensor ir_tensor_bn_scale(paddle::dialect::TransToPhiDataType(bn_scale.dtype()),
                                                      bn_scale.dims(),
                                                      bn_scale.data_layout(),
                                                      bn_scale.lod(),
                                                      bn_scale.offset());
  VLOG(4) << "Builder construction  meta_bn_scale";
  paddle::dialect::IrMetaTensor meta_bn_scale(&ir_tensor_bn_scale);

  VLOG(4) << "Builder construction  dense_bn_bias";
  paddle::dialect::IrTensor ir_tensor_bn_bias(paddle::dialect::TransToPhiDataType(bn_bias.dtype()),
                                                      bn_bias.dims(),
                                                      bn_bias.data_layout(),
                                                      bn_bias.lod(),
                                                      bn_bias.offset());
  VLOG(4) << "Builder construction  meta_bn_bias";
  paddle::dialect::IrMetaTensor meta_bn_bias(&ir_tensor_bn_bias);

  VLOG(4) << "Builder construction  dense_input_running_mean";
  paddle::dialect::IrTensor ir_tensor_input_running_mean(paddle::dialect::TransToPhiDataType(input_running_mean.dtype()),
                                                      input_running_mean.dims(),
                                                      input_running_mean.data_layout(),
                                                      input_running_mean.lod(),
                                                      input_running_mean.offset());
  VLOG(4) << "Builder construction  meta_input_running_mean";
  paddle::dialect::IrMetaTensor meta_input_running_mean(&ir_tensor_input_running_mean);

  VLOG(4) << "Builder construction  dense_input_running_var";
  paddle::dialect::IrTensor ir_tensor_input_running_var(paddle::dialect::TransToPhiDataType(input_running_var.dtype()),
                                                      input_running_var.dims(),
                                                      input_running_var.data_layout(),
                                                      input_running_var.lod(),
                                                      input_running_var.offset());
  VLOG(4) << "Builder construction  meta_input_running_var";
  paddle::dialect::IrMetaTensor meta_input_running_var(&ir_tensor_input_running_var);
  paddle::dialect::IrTensor dense_out;
  paddle::dialect::IrMetaTensor meta_out(&dense_out);
  paddle::dialect::IrTensor dense_out_running_mean;
  paddle::dialect::IrMetaTensor meta_out_running_mean(&dense_out_running_mean);
  paddle::dialect::IrTensor dense_out_running_var;
  paddle::dialect::IrMetaTensor meta_out_running_var(&dense_out_running_var);
  paddle::dialect::IrTensor dense_saved_mean;
  paddle::dialect::IrMetaTensor meta_saved_mean(&dense_saved_mean);
  paddle::dialect::IrTensor dense_saved_var;
  paddle::dialect::IrMetaTensor meta_saved_var(&dense_saved_var);
  paddle::dialect::IrTensor dense_eq_scale;
  paddle::dialect::IrMetaTensor meta_eq_scale(&dense_eq_scale);
  paddle::dialect::IrTensor dense_eq_bias;
  paddle::dialect::IrMetaTensor meta_eq_bias(&dense_eq_bias);

  phi::FusedScaleBiasReluConvBnInferMeta(meta_x, meta_w, meta_scale, meta_bias, meta_bn_scale, meta_bn_bias, meta_input_running_mean, meta_input_running_var, paddings, dilations, strides, padding_algorithm, groups, data_format, momentum, epsilon, fuse_prologue, exhaustive_search, accumulation_count, &meta_out, &meta_out_running_mean, &meta_out_running_var, &meta_saved_mean, &meta_saved_var, &meta_eq_scale, &meta_eq_bias);

  std::vector<pir::Type> argument_outputs;
  pir::Type out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_out.dtype()), dense_out.dims(), dense_out.layout(), dense_out.lod(), dense_out.offset());
  argument_outputs.push_back(out_dense_tensor_type);

  pir::Type out_running_mean_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_out_running_mean.dtype()), dense_out_running_mean.dims(), dense_out_running_mean.layout(), dense_out_running_mean.lod(), dense_out_running_mean.offset());
  argument_outputs.push_back(out_running_mean_dense_tensor_type);

  pir::Type out_running_var_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_out_running_var.dtype()), dense_out_running_var.dims(), dense_out_running_var.layout(), dense_out_running_var.lod(), dense_out_running_var.offset());
  argument_outputs.push_back(out_running_var_dense_tensor_type);

  pir::Type saved_mean_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_saved_mean.dtype()), dense_saved_mean.dims(), dense_saved_mean.layout(), dense_saved_mean.lod(), dense_saved_mean.offset());
  argument_outputs.push_back(saved_mean_dense_tensor_type);

  pir::Type saved_var_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_saved_var.dtype()), dense_saved_var.dims(), dense_saved_var.layout(), dense_saved_var.lod(), dense_saved_var.offset());
  argument_outputs.push_back(saved_var_dense_tensor_type);

  pir::Type eq_scale_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_eq_scale.dtype()), dense_eq_scale.dims(), dense_eq_scale.layout(), dense_eq_scale.lod(), dense_eq_scale.offset());
  argument_outputs.push_back(eq_scale_dense_tensor_type);

  pir::Type eq_bias_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_eq_bias.dtype()), dense_eq_bias.dims(), dense_eq_bias.layout(), dense_eq_bias.lod(), dense_eq_bias.offset());
  argument_outputs.push_back(eq_bias_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void FusedScaleBiasReluConvBnOp::VerifySig() {
  VLOG(4) << "Start Verifying inputs, outputs and attributes for: FusedScaleBiasReluConvBnOp.";
  VLOG(4) << "Verifying inputs:";
  {
  auto input_size = num_operands();
  IR_ENFORCE(input_size == 8u,
                    "The size %d of inputs must be equal to 8.", input_size);
  IR_ENFORCE((*this)->operand_source(0).type().isa<paddle::dialect::DenseTensorType>(),
                  "Type validation failed for the 0th input, got %s.", (*this)->operand_source(0).type());
  IR_ENFORCE((*this)->operand_source(1).type().isa<paddle::dialect::DenseTensorType>(),
                  "Type validation failed for the 1th input, got %s.", (*this)->operand_source(1).type());
  if (auto val = (*this)->operand(2)) {
    IR_ENFORCE(val.type().isa<paddle::dialect::DenseTensorType>(),
                   "Type validation failed for the 2th input, got %s.", (*this)->operand_source(2).type());
  }
  if (auto val = (*this)->operand(3)) {
    IR_ENFORCE(val.type().isa<paddle::dialect::DenseTensorType>(),
                   "Type validation failed for the 3th input, got %s.", (*this)->operand_source(3).type());
  }
  IR_ENFORCE((*this)->operand_source(4).type().isa<paddle::dialect::DenseTensorType>(),
                  "Type validation failed for the 4th input, got %s.", (*this)->operand_source(4).type());
  IR_ENFORCE((*this)->operand_source(5).type().isa<paddle::dialect::DenseTensorType>(),
                  "Type validation failed for the 5th input, got %s.", (*this)->operand_source(5).type());
  IR_ENFORCE((*this)->operand_source(6).type().isa<paddle::dialect::DenseTensorType>(),
                  "Type validation failed for the 6th input, got %s.", (*this)->operand_source(6).type());
  IR_ENFORCE((*this)->operand_source(7).type().isa<paddle::dialect::DenseTensorType>(),
                  "Type validation failed for the 7th input, got %s.", (*this)->operand_source(7).type());
  }
  VLOG(4) << "Verifying attributes:";
  {
  auto& attributes = this->attributes();
  IR_ENFORCE(attributes.count("paddings")>0,
                 "paddings does not exist.");
  IR_ENFORCE(attributes.at("paddings").isa<pir::ArrayAttribute>(),
                 "Type of attribute: paddings is not pir::ArrayAttribute.");
  for (size_t i = 0; i < attributes.at("paddings").dyn_cast<pir::ArrayAttribute>().size(); i++) {
    IR_ENFORCE(attributes.at("paddings").dyn_cast<pir::ArrayAttribute>().at(i).isa<pir::Int32Attribute>(),
                   "Type of attribute: paddings is not right.");
  }
  IR_ENFORCE(attributes.count("dilations")>0,
                 "dilations does not exist.");
  IR_ENFORCE(attributes.at("dilations").isa<pir::ArrayAttribute>(),
                 "Type of attribute: dilations is not pir::ArrayAttribute.");
  for (size_t i = 0; i < attributes.at("dilations").dyn_cast<pir::ArrayAttribute>().size(); i++) {
    IR_ENFORCE(attributes.at("dilations").dyn_cast<pir::ArrayAttribute>().at(i).isa<pir::Int32Attribute>(),
                   "Type of attribute: dilations is not right.");
  }
  IR_ENFORCE(attributes.count("strides")>0,
                 "strides does not exist.");
  IR_ENFORCE(attributes.at("strides").isa<pir::ArrayAttribute>(),
                 "Type of attribute: strides is not pir::ArrayAttribute.");
  for (size_t i = 0; i < attributes.at("strides").dyn_cast<pir::ArrayAttribute>().size(); i++) {
    IR_ENFORCE(attributes.at("strides").dyn_cast<pir::ArrayAttribute>().at(i).isa<pir::Int32Attribute>(),
                   "Type of attribute: strides is not right.");
  }
  IR_ENFORCE(attributes.count("padding_algorithm")>0,
                 "padding_algorithm does not exist.");
  IR_ENFORCE(attributes.at("padding_algorithm").isa<pir::StrAttribute>(),
                 "Type of attribute: padding_algorithm is not pir::StrAttribute.");

  IR_ENFORCE(attributes.count("groups")>0,
                 "groups does not exist.");
  IR_ENFORCE(attributes.at("groups").isa<pir::Int32Attribute>(),
                 "Type of attribute: groups is not pir::Int32Attribute.");

  IR_ENFORCE(attributes.count("data_format")>0,
                 "data_format does not exist.");
  IR_ENFORCE(attributes.at("data_format").isa<pir::StrAttribute>(),
                 "Type of attribute: data_format is not pir::StrAttribute.");

  IR_ENFORCE(attributes.count("momentum")>0,
                 "momentum does not exist.");
  IR_ENFORCE(attributes.at("momentum").isa<pir::FloatAttribute>(),
                 "Type of attribute: momentum is not pir::FloatAttribute.");

  IR_ENFORCE(attributes.count("epsilon")>0,
                 "epsilon does not exist.");
  IR_ENFORCE(attributes.at("epsilon").isa<pir::FloatAttribute>(),
                 "Type of attribute: epsilon is not pir::FloatAttribute.");

  IR_ENFORCE(attributes.count("fuse_prologue")>0,
                 "fuse_prologue does not exist.");
  IR_ENFORCE(attributes.at("fuse_prologue").isa<pir::BoolAttribute>(),
                 "Type of attribute: fuse_prologue is not pir::BoolAttribute.");

  IR_ENFORCE(attributes.count("exhaustive_search")>0,
                 "exhaustive_search does not exist.");
  IR_ENFORCE(attributes.at("exhaustive_search").isa<pir::BoolAttribute>(),
                 "Type of attribute: exhaustive_search is not pir::BoolAttribute.");

  IR_ENFORCE(attributes.count("accumulation_count")>0,
                 "accumulation_count does not exist.");
  IR_ENFORCE(attributes.at("accumulation_count").isa<pir::Int64Attribute>(),
                 "Type of attribute: accumulation_count is not pir::Int64Attribute.");

  }
  VLOG(4) << "Verifying outputs:";
  {
  auto output_size = num_results();
  IR_ENFORCE(output_size == 7u,
                    "The size %d of outputs must be equal to 7.", output_size);
  IR_ENFORCE((*this)->result(0).type().isa<paddle::dialect::DenseTensorType>(),
                 "Type validation failed for the 0th output.");
  IR_ENFORCE((*this)->result(1).type().isa<paddle::dialect::DenseTensorType>(),
                 "Type validation failed for the 1th output.");
  IR_ENFORCE((*this)->result(2).type().isa<paddle::dialect::DenseTensorType>(),
                 "Type validation failed for the 2th output.");
  IR_ENFORCE((*this)->result(3).type().isa<paddle::dialect::DenseTensorType>(),
                 "Type validation failed for the 3th output.");
  IR_ENFORCE((*this)->result(4).type().isa<paddle::dialect::DenseTensorType>(),
                 "Type validation failed for the 4th output.");
  IR_ENFORCE((*this)->result(5).type().isa<paddle::dialect::DenseTensorType>(),
                 "Type validation failed for the 5th output.");
  IR_ENFORCE((*this)->result(6).type().isa<paddle::dialect::DenseTensorType>(),
                 "Type validation failed for the 6th output.");
  }
  VLOG(4) << "End Verifying for: FusedScaleBiasReluConvBnOp.";
}

void FusedScaleBiasReluConvBnOp::InferMeta( phi::InferMetaContext *infer_meta ) {
  auto fn = PD_INFER_META(phi::FusedScaleBiasReluConvBnInferMeta);
  fn(infer_meta);
}

phi::DataType FusedScaleBiasReluConvBnOp::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: FusedScaleBiasReluConvBnOp";
  


  return expected_kernel_dtype;
}

const char *FusionGruOp::attributes_name[11] = { "activation", "gate_activation", "is_reverse", "use_seq", "origin_mode", "use_mkldnn", "mkldnn_data_type", "scale_data", "shift_data", "scale_weights", "force_fp32_output" };

OpInfoTuple FusionGruOp::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("x", "paddle::dialect::DenseTensorType", false, false, false, false), paddle::dialect::OpInputInfo("h0", "paddle::dialect::DenseTensorType", true, false, false, false), paddle::dialect::OpInputInfo("weight_x", "paddle::dialect::DenseTensorType", false, false, false, false), paddle::dialect::OpInputInfo("weight_h", "paddle::dialect::DenseTensorType", false, false, false, false), paddle::dialect::OpInputInfo("bias", "paddle::dialect::DenseTensorType", true, false, false, false) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = { paddle::dialect::OpAttributeInfo("activation", "pir::StrAttribute", ""), paddle::dialect::OpAttributeInfo("gate_activation", "pir::StrAttribute", ""), paddle::dialect::OpAttributeInfo("is_reverse", "pir::BoolAttribute", ""), paddle::dialect::OpAttributeInfo("use_seq", "pir::BoolAttribute", ""), paddle::dialect::OpAttributeInfo("origin_mode", "pir::BoolAttribute", ""), paddle::dialect::OpAttributeInfo("use_mkldnn", "pir::BoolAttribute", ""), paddle::dialect::OpAttributeInfo("mkldnn_data_type", "pir::StrAttribute", ""), paddle::dialect::OpAttributeInfo("scale_data", "pir::FloatAttribute", ""), paddle::dialect::OpAttributeInfo("shift_data", "pir::FloatAttribute", ""), paddle::dialect::OpAttributeInfo("scale_weights", "pir::ArrayAttribute<pir::FloatAttribute>", ""), paddle::dialect::OpAttributeInfo("force_fp32_output", "pir::BoolAttribute", "") };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("reordered_h0", "paddle::dialect::DenseTensorType", true, true), paddle::dialect::OpOutputInfo("xx", "paddle::dialect::DenseTensorType", true, true), paddle::dialect::OpOutputInfo("batched_input", "paddle::dialect::DenseTensorType", true, true), paddle::dialect::OpOutputInfo("batched_out", "paddle::dialect::DenseTensorType", true, true), paddle::dialect::OpOutputInfo("hidden", "paddle::dialect::DenseTensorType", false, false) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("FusionGRUInferMeta", {"x", "h0", "weight_x", "weight_h", "bias", "activation", "gate_activation", "is_reverse", "use_seq", "origin_mode", "use_mkldnn", "mkldnn_data_type", "scale_data", "shift_data", "scale_weights", "force_fp32_output"}, "fusion_gru", {"x", "h0", "weight_x", "weight_h", "bias", "activation", "gate_activation", "is_reverse", "use_seq", "origin_mode", "use_mkldnn", "mkldnn_data_type", "scale_data", "shift_data", "scale_weights", "force_fp32_output"}, {"x"}, {}, {}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "fusion_gru");
}

void FusionGruOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, pir::Value h0_, pir::Value weight_x_, pir::Value weight_h_, pir::Value bias_, const std::string& activation, const std::string& gate_activation, bool is_reverse, bool use_seq, bool origin_mode, bool use_mkldnn, const std::string& mkldnn_data_type, float scale_data, float shift_data, const std::vector<float>& scale_weights, bool force_fp32_output) {
  VLOG(4) << "Start build FusionGruOp";



  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_, h0_, weight_x_, weight_h_, bias_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_activation = pir::StrAttribute::get(pir::IrContext::Instance(), activation);
  argument.AddAttribute("activation", attr_activation);
  pir::Attribute attr_gate_activation = pir::StrAttribute::get(pir::IrContext::Instance(), gate_activation);
  argument.AddAttribute("gate_activation", attr_gate_activation);
  pir::Attribute attr_is_reverse = pir::BoolAttribute::get(pir::IrContext::Instance(), is_reverse);
  argument.AddAttribute("is_reverse", attr_is_reverse);
  pir::Attribute attr_use_seq = pir::BoolAttribute::get(pir::IrContext::Instance(), use_seq);
  argument.AddAttribute("use_seq", attr_use_seq);
  pir::Attribute attr_origin_mode = pir::BoolAttribute::get(pir::IrContext::Instance(), origin_mode);
  argument.AddAttribute("origin_mode", attr_origin_mode);
  pir::Attribute attr_use_mkldnn = pir::BoolAttribute::get(pir::IrContext::Instance(), use_mkldnn);
  argument.AddAttribute("use_mkldnn", attr_use_mkldnn);
  pir::Attribute attr_mkldnn_data_type = pir::StrAttribute::get(pir::IrContext::Instance(), mkldnn_data_type);
  argument.AddAttribute("mkldnn_data_type", attr_mkldnn_data_type);
  pir::Attribute attr_scale_data = pir::FloatAttribute::get(pir::IrContext::Instance(), scale_data);
  argument.AddAttribute("scale_data", attr_scale_data);
  pir::Attribute attr_shift_data = pir::FloatAttribute::get(pir::IrContext::Instance(), shift_data);
  argument.AddAttribute("shift_data", attr_shift_data);
  std::vector<pir::Attribute> vec_scale_weights;
  for (size_t i = 0; i < static_cast<size_t>(scale_weights.size()); i++) {
      pir::Attribute attr_scale_weights = pir::FloatAttribute::get(pir::IrContext::Instance(), scale_weights[i]);

    vec_scale_weights.push_back(attr_scale_weights);
  }
  pir::Attribute attr_scale_weights = pir::ArrayAttribute::get(pir::IrContext::Instance(), vec_scale_weights);
  argument.AddAttribute("scale_weights", attr_scale_weights);
  pir::Attribute attr_force_fp32_output = pir::BoolAttribute::get(pir::IrContext::Instance(), force_fp32_output);
  argument.AddAttribute("force_fp32_output", attr_force_fp32_output);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;
  paddle::dialect::DenseTensorType weight_x = weight_x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)weight_x;
  paddle::dialect::DenseTensorType weight_h = weight_h_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)weight_h;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);

  paddle::dialect::IrMetaTensor meta_h0;
  paddle::dialect::IrTensor ir_tensor_h0;
  if (h0_.impl() != nullptr) {
    paddle::dialect::DenseTensorType h0 = h0_.type().dyn_cast<paddle::dialect::DenseTensorType>();
    VLOG(4) << "Builder construction  dense_h0";
    ir_tensor_h0 = paddle::dialect::IrTensor(paddle::dialect::TransToPhiDataType(h0.dtype()),
                                                        h0.dims(),
                                                        h0.data_layout(),
                                                        h0.lod(),
                                                        h0.offset());
    VLOG(4) << "Builder construction  meta_h0";
    meta_h0 = paddle::dialect::IrMetaTensor(&ir_tensor_h0);
  }


  VLOG(4) << "Builder construction  dense_weight_x";
  paddle::dialect::IrTensor ir_tensor_weight_x(paddle::dialect::TransToPhiDataType(weight_x.dtype()),
                                                      weight_x.dims(),
                                                      weight_x.data_layout(),
                                                      weight_x.lod(),
                                                      weight_x.offset());
  VLOG(4) << "Builder construction  meta_weight_x";
  paddle::dialect::IrMetaTensor meta_weight_x(&ir_tensor_weight_x);

  VLOG(4) << "Builder construction  dense_weight_h";
  paddle::dialect::IrTensor ir_tensor_weight_h(paddle::dialect::TransToPhiDataType(weight_h.dtype()),
                                                      weight_h.dims(),
                                                      weight_h.data_layout(),
                                                      weight_h.lod(),
                                                      weight_h.offset());
  VLOG(4) << "Builder construction  meta_weight_h";
  paddle::dialect::IrMetaTensor meta_weight_h(&ir_tensor_weight_h);

  paddle::dialect::IrMetaTensor meta_bias;
  paddle::dialect::IrTensor ir_tensor_bias;
  if (bias_.impl() != nullptr) {
    paddle::dialect::DenseTensorType bias = bias_.type().dyn_cast<paddle::dialect::DenseTensorType>();
    VLOG(4) << "Builder construction  dense_bias";
    ir_tensor_bias = paddle::dialect::IrTensor(paddle::dialect::TransToPhiDataType(bias.dtype()),
                                                        bias.dims(),
                                                        bias.data_layout(),
                                                        bias.lod(),
                                                        bias.offset());
    VLOG(4) << "Builder construction  meta_bias";
    meta_bias = paddle::dialect::IrMetaTensor(&ir_tensor_bias);
  }

  paddle::dialect::IrTensor dense_reordered_h0;
  paddle::dialect::IrMetaTensor meta_reordered_h0(&dense_reordered_h0);
  paddle::dialect::IrTensor dense_xx;
  paddle::dialect::IrMetaTensor meta_xx(&dense_xx);
  paddle::dialect::IrTensor dense_batched_input;
  paddle::dialect::IrMetaTensor meta_batched_input(&dense_batched_input);
  paddle::dialect::IrTensor dense_batched_out;
  paddle::dialect::IrMetaTensor meta_batched_out(&dense_batched_out);
  paddle::dialect::IrTensor dense_hidden;
  paddle::dialect::IrMetaTensor meta_hidden(&dense_hidden);

  phi::FusionGRUInferMeta(meta_x, meta_h0, meta_weight_x, meta_weight_h, meta_bias, activation, gate_activation, is_reverse, use_seq, origin_mode, use_mkldnn, mkldnn_data_type, scale_data, shift_data, scale_weights, force_fp32_output, &meta_reordered_h0, &meta_xx, &meta_batched_input, &meta_batched_out, &meta_hidden);

  std::vector<pir::Type> argument_outputs;
  pir::Type reordered_h0_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_reordered_h0.dtype()), dense_reordered_h0.dims(), dense_reordered_h0.layout(), dense_reordered_h0.lod(), dense_reordered_h0.offset());
  argument_outputs.push_back(reordered_h0_dense_tensor_type);

  pir::Type xx_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_xx.dtype()), dense_xx.dims(), dense_xx.layout(), dense_xx.lod(), dense_xx.offset());
  argument_outputs.push_back(xx_dense_tensor_type);

  pir::Type batched_input_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_batched_input.dtype()), dense_batched_input.dims(), dense_batched_input.layout(), dense_batched_input.lod(), dense_batched_input.offset());
  argument_outputs.push_back(batched_input_dense_tensor_type);

  pir::Type batched_out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_batched_out.dtype()), dense_batched_out.dims(), dense_batched_out.layout(), dense_batched_out.lod(), dense_batched_out.offset());
  argument_outputs.push_back(batched_out_dense_tensor_type);

  pir::Type hidden_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_hidden.dtype()), dense_hidden.dims(), dense_hidden.layout(), dense_hidden.lod(), dense_hidden.offset());
  argument_outputs.push_back(hidden_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void FusionGruOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, pir::Value h0_, pir::Value weight_x_, pir::Value weight_h_, pir::Value bias_, pir::AttributeMap attributes) {
  VLOG(4) << "Start build FusionGruOp";


  IR_ENFORCE(
      attributes.find("activation") != attributes.end(),
          "'activation' Attribute is expected for FusionGruOp. ");
  std::string activation = attributes.at("activation").dyn_cast<pir::StrAttribute>().AsString();

  IR_ENFORCE(
      attributes.find("gate_activation") != attributes.end(),
          "'gate_activation' Attribute is expected for FusionGruOp. ");
  std::string gate_activation = attributes.at("gate_activation").dyn_cast<pir::StrAttribute>().AsString();

  IR_ENFORCE(
      attributes.find("is_reverse") != attributes.end(),
          "'is_reverse' Attribute is expected for FusionGruOp. ");
  bool is_reverse = attributes.at("is_reverse").dyn_cast<pir::BoolAttribute>().data();

  IR_ENFORCE(
      attributes.find("use_seq") != attributes.end(),
          "'use_seq' Attribute is expected for FusionGruOp. ");
  bool use_seq = attributes.at("use_seq").dyn_cast<pir::BoolAttribute>().data();

  IR_ENFORCE(
      attributes.find("origin_mode") != attributes.end(),
          "'origin_mode' Attribute is expected for FusionGruOp. ");
  bool origin_mode = attributes.at("origin_mode").dyn_cast<pir::BoolAttribute>().data();

  IR_ENFORCE(
      attributes.find("use_mkldnn") != attributes.end(),
          "'use_mkldnn' Attribute is expected for FusionGruOp. ");
  bool use_mkldnn = attributes.at("use_mkldnn").dyn_cast<pir::BoolAttribute>().data();

  IR_ENFORCE(
      attributes.find("mkldnn_data_type") != attributes.end(),
          "'mkldnn_data_type' Attribute is expected for FusionGruOp. ");
  std::string mkldnn_data_type = attributes.at("mkldnn_data_type").dyn_cast<pir::StrAttribute>().AsString();

  IR_ENFORCE(
      attributes.find("scale_data") != attributes.end(),
          "'scale_data' Attribute is expected for FusionGruOp. ");
  float scale_data = attributes.at("scale_data").dyn_cast<pir::FloatAttribute>().data();

  IR_ENFORCE(
      attributes.find("shift_data") != attributes.end(),
          "'shift_data' Attribute is expected for FusionGruOp. ");
  float shift_data = attributes.at("shift_data").dyn_cast<pir::FloatAttribute>().data();

  IR_ENFORCE(
      attributes.find("scale_weights") != attributes.end(),
          "'scale_weights' Attribute is expected for FusionGruOp. ");
  std::vector<float> scale_weights;
  for (size_t i = 0; i < attributes.at("scale_weights").dyn_cast<pir::ArrayAttribute>().size(); i++) {
    scale_weights.push_back(attributes.at("scale_weights").dyn_cast<pir::ArrayAttribute>().at(i).dyn_cast<pir::FloatAttribute>().data());
  }

  IR_ENFORCE(
      attributes.find("force_fp32_output") != attributes.end(),
          "'force_fp32_output' Attribute is expected for FusionGruOp. ");
  bool force_fp32_output = attributes.at("force_fp32_output").dyn_cast<pir::BoolAttribute>().data();


  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_, h0_, weight_x_, weight_h_, bias_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_activation = pir::StrAttribute::get(pir::IrContext::Instance(), activation);
  argument.AddAttribute("activation", attr_activation);
  pir::Attribute attr_gate_activation = pir::StrAttribute::get(pir::IrContext::Instance(), gate_activation);
  argument.AddAttribute("gate_activation", attr_gate_activation);
  pir::Attribute attr_is_reverse = pir::BoolAttribute::get(pir::IrContext::Instance(), is_reverse);
  argument.AddAttribute("is_reverse", attr_is_reverse);
  pir::Attribute attr_use_seq = pir::BoolAttribute::get(pir::IrContext::Instance(), use_seq);
  argument.AddAttribute("use_seq", attr_use_seq);
  pir::Attribute attr_origin_mode = pir::BoolAttribute::get(pir::IrContext::Instance(), origin_mode);
  argument.AddAttribute("origin_mode", attr_origin_mode);
  pir::Attribute attr_use_mkldnn = pir::BoolAttribute::get(pir::IrContext::Instance(), use_mkldnn);
  argument.AddAttribute("use_mkldnn", attr_use_mkldnn);
  pir::Attribute attr_mkldnn_data_type = pir::StrAttribute::get(pir::IrContext::Instance(), mkldnn_data_type);
  argument.AddAttribute("mkldnn_data_type", attr_mkldnn_data_type);
  pir::Attribute attr_scale_data = pir::FloatAttribute::get(pir::IrContext::Instance(), scale_data);
  argument.AddAttribute("scale_data", attr_scale_data);
  pir::Attribute attr_shift_data = pir::FloatAttribute::get(pir::IrContext::Instance(), shift_data);
  argument.AddAttribute("shift_data", attr_shift_data);
  std::vector<pir::Attribute> vec_scale_weights;
  for (size_t i = 0; i < static_cast<size_t>(scale_weights.size()); i++) {
      pir::Attribute attr_scale_weights = pir::FloatAttribute::get(pir::IrContext::Instance(), scale_weights[i]);

    vec_scale_weights.push_back(attr_scale_weights);
  }
  pir::Attribute attr_scale_weights = pir::ArrayAttribute::get(pir::IrContext::Instance(), vec_scale_weights);
  argument.AddAttribute("scale_weights", attr_scale_weights);
  pir::Attribute attr_force_fp32_output = pir::BoolAttribute::get(pir::IrContext::Instance(), force_fp32_output);
  argument.AddAttribute("force_fp32_output", attr_force_fp32_output);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;
  paddle::dialect::DenseTensorType weight_x = weight_x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)weight_x;
  paddle::dialect::DenseTensorType weight_h = weight_h_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)weight_h;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);

  paddle::dialect::IrMetaTensor meta_h0;
  paddle::dialect::IrTensor ir_tensor_h0;
  if (h0_.impl() != nullptr) {
    paddle::dialect::DenseTensorType h0 = h0_.type().dyn_cast<paddle::dialect::DenseTensorType>();
    VLOG(4) << "Builder construction  dense_h0";
    ir_tensor_h0 = paddle::dialect::IrTensor(paddle::dialect::TransToPhiDataType(h0.dtype()),
                                                        h0.dims(),
                                                        h0.data_layout(),
                                                        h0.lod(),
                                                        h0.offset());
    VLOG(4) << "Builder construction  meta_h0";
    meta_h0 = paddle::dialect::IrMetaTensor(&ir_tensor_h0);
  }


  VLOG(4) << "Builder construction  dense_weight_x";
  paddle::dialect::IrTensor ir_tensor_weight_x(paddle::dialect::TransToPhiDataType(weight_x.dtype()),
                                                      weight_x.dims(),
                                                      weight_x.data_layout(),
                                                      weight_x.lod(),
                                                      weight_x.offset());
  VLOG(4) << "Builder construction  meta_weight_x";
  paddle::dialect::IrMetaTensor meta_weight_x(&ir_tensor_weight_x);

  VLOG(4) << "Builder construction  dense_weight_h";
  paddle::dialect::IrTensor ir_tensor_weight_h(paddle::dialect::TransToPhiDataType(weight_h.dtype()),
                                                      weight_h.dims(),
                                                      weight_h.data_layout(),
                                                      weight_h.lod(),
                                                      weight_h.offset());
  VLOG(4) << "Builder construction  meta_weight_h";
  paddle::dialect::IrMetaTensor meta_weight_h(&ir_tensor_weight_h);

  paddle::dialect::IrMetaTensor meta_bias;
  paddle::dialect::IrTensor ir_tensor_bias;
  if (bias_.impl() != nullptr) {
    paddle::dialect::DenseTensorType bias = bias_.type().dyn_cast<paddle::dialect::DenseTensorType>();
    VLOG(4) << "Builder construction  dense_bias";
    ir_tensor_bias = paddle::dialect::IrTensor(paddle::dialect::TransToPhiDataType(bias.dtype()),
                                                        bias.dims(),
                                                        bias.data_layout(),
                                                        bias.lod(),
                                                        bias.offset());
    VLOG(4) << "Builder construction  meta_bias";
    meta_bias = paddle::dialect::IrMetaTensor(&ir_tensor_bias);
  }

  paddle::dialect::IrTensor dense_reordered_h0;
  paddle::dialect::IrMetaTensor meta_reordered_h0(&dense_reordered_h0);
  paddle::dialect::IrTensor dense_xx;
  paddle::dialect::IrMetaTensor meta_xx(&dense_xx);
  paddle::dialect::IrTensor dense_batched_input;
  paddle::dialect::IrMetaTensor meta_batched_input(&dense_batched_input);
  paddle::dialect::IrTensor dense_batched_out;
  paddle::dialect::IrMetaTensor meta_batched_out(&dense_batched_out);
  paddle::dialect::IrTensor dense_hidden;
  paddle::dialect::IrMetaTensor meta_hidden(&dense_hidden);

  phi::FusionGRUInferMeta(meta_x, meta_h0, meta_weight_x, meta_weight_h, meta_bias, activation, gate_activation, is_reverse, use_seq, origin_mode, use_mkldnn, mkldnn_data_type, scale_data, shift_data, scale_weights, force_fp32_output, &meta_reordered_h0, &meta_xx, &meta_batched_input, &meta_batched_out, &meta_hidden);

  std::vector<pir::Type> argument_outputs;
  pir::Type reordered_h0_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_reordered_h0.dtype()), dense_reordered_h0.dims(), dense_reordered_h0.layout(), dense_reordered_h0.lod(), dense_reordered_h0.offset());
  argument_outputs.push_back(reordered_h0_dense_tensor_type);

  pir::Type xx_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_xx.dtype()), dense_xx.dims(), dense_xx.layout(), dense_xx.lod(), dense_xx.offset());
  argument_outputs.push_back(xx_dense_tensor_type);

  pir::Type batched_input_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_batched_input.dtype()), dense_batched_input.dims(), dense_batched_input.layout(), dense_batched_input.lod(), dense_batched_input.offset());
  argument_outputs.push_back(batched_input_dense_tensor_type);

  pir::Type batched_out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_batched_out.dtype()), dense_batched_out.dims(), dense_batched_out.layout(), dense_batched_out.lod(), dense_batched_out.offset());
  argument_outputs.push_back(batched_out_dense_tensor_type);

  pir::Type hidden_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_hidden.dtype()), dense_hidden.dims(), dense_hidden.layout(), dense_hidden.lod(), dense_hidden.offset());
  argument_outputs.push_back(hidden_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void FusionGruOp::VerifySig() {
  VLOG(4) << "Start Verifying inputs, outputs and attributes for: FusionGruOp.";
  VLOG(4) << "Verifying inputs:";
  {
  auto input_size = num_operands();
  IR_ENFORCE(input_size == 5u,
                    "The size %d of inputs must be equal to 5.", input_size);
  IR_ENFORCE((*this)->operand_source(0).type().isa<paddle::dialect::DenseTensorType>(),
                  "Type validation failed for the 0th input, got %s.", (*this)->operand_source(0).type());
  if (auto val = (*this)->operand(1)) {
    IR_ENFORCE(val.type().isa<paddle::dialect::DenseTensorType>(),
                   "Type validation failed for the 1th input, got %s.", (*this)->operand_source(1).type());
  }
  IR_ENFORCE((*this)->operand_source(2).type().isa<paddle::dialect::DenseTensorType>(),
                  "Type validation failed for the 2th input, got %s.", (*this)->operand_source(2).type());
  IR_ENFORCE((*this)->operand_source(3).type().isa<paddle::dialect::DenseTensorType>(),
                  "Type validation failed for the 3th input, got %s.", (*this)->operand_source(3).type());
  if (auto val = (*this)->operand(4)) {
    IR_ENFORCE(val.type().isa<paddle::dialect::DenseTensorType>(),
                   "Type validation failed for the 4th input, got %s.", (*this)->operand_source(4).type());
  }
  }
  VLOG(4) << "Verifying attributes:";
  {
  auto& attributes = this->attributes();
  IR_ENFORCE(attributes.count("activation")>0,
                 "activation does not exist.");
  IR_ENFORCE(attributes.at("activation").isa<pir::StrAttribute>(),
                 "Type of attribute: activation is not pir::StrAttribute.");

  IR_ENFORCE(attributes.count("gate_activation")>0,
                 "gate_activation does not exist.");
  IR_ENFORCE(attributes.at("gate_activation").isa<pir::StrAttribute>(),
                 "Type of attribute: gate_activation is not pir::StrAttribute.");

  IR_ENFORCE(attributes.count("is_reverse")>0,
                 "is_reverse does not exist.");
  IR_ENFORCE(attributes.at("is_reverse").isa<pir::BoolAttribute>(),
                 "Type of attribute: is_reverse is not pir::BoolAttribute.");

  IR_ENFORCE(attributes.count("use_seq")>0,
                 "use_seq does not exist.");
  IR_ENFORCE(attributes.at("use_seq").isa<pir::BoolAttribute>(),
                 "Type of attribute: use_seq is not pir::BoolAttribute.");

  IR_ENFORCE(attributes.count("origin_mode")>0,
                 "origin_mode does not exist.");
  IR_ENFORCE(attributes.at("origin_mode").isa<pir::BoolAttribute>(),
                 "Type of attribute: origin_mode is not pir::BoolAttribute.");

  IR_ENFORCE(attributes.count("use_mkldnn")>0,
                 "use_mkldnn does not exist.");
  IR_ENFORCE(attributes.at("use_mkldnn").isa<pir::BoolAttribute>(),
                 "Type of attribute: use_mkldnn is not pir::BoolAttribute.");

  IR_ENFORCE(attributes.count("mkldnn_data_type")>0,
                 "mkldnn_data_type does not exist.");
  IR_ENFORCE(attributes.at("mkldnn_data_type").isa<pir::StrAttribute>(),
                 "Type of attribute: mkldnn_data_type is not pir::StrAttribute.");

  IR_ENFORCE(attributes.count("scale_data")>0,
                 "scale_data does not exist.");
  IR_ENFORCE(attributes.at("scale_data").isa<pir::FloatAttribute>(),
                 "Type of attribute: scale_data is not pir::FloatAttribute.");

  IR_ENFORCE(attributes.count("shift_data")>0,
                 "shift_data does not exist.");
  IR_ENFORCE(attributes.at("shift_data").isa<pir::FloatAttribute>(),
                 "Type of attribute: shift_data is not pir::FloatAttribute.");

  IR_ENFORCE(attributes.count("scale_weights")>0,
                 "scale_weights does not exist.");
  IR_ENFORCE(attributes.at("scale_weights").isa<pir::ArrayAttribute>(),
                 "Type of attribute: scale_weights is not pir::ArrayAttribute.");
  for (size_t i = 0; i < attributes.at("scale_weights").dyn_cast<pir::ArrayAttribute>().size(); i++) {
    IR_ENFORCE(attributes.at("scale_weights").dyn_cast<pir::ArrayAttribute>().at(i).isa<pir::FloatAttribute>(),
                   "Type of attribute: scale_weights is not right.");
  }
  IR_ENFORCE(attributes.count("force_fp32_output")>0,
                 "force_fp32_output does not exist.");
  IR_ENFORCE(attributes.at("force_fp32_output").isa<pir::BoolAttribute>(),
                 "Type of attribute: force_fp32_output is not pir::BoolAttribute.");

  }
  VLOG(4) << "Verifying outputs:";
  {
  auto output_size = num_results();
  IR_ENFORCE(output_size == 5u,
                    "The size %d of outputs must be equal to 5.", output_size);
  if (auto output_0_type = (*this)->result(0).type()) {
    IR_ENFORCE(output_0_type.isa<paddle::dialect::DenseTensorType>(),
                   "Type validation failed for the 0th output.");
  }
  if (auto output_1_type = (*this)->result(1).type()) {
    IR_ENFORCE(output_1_type.isa<paddle::dialect::DenseTensorType>(),
                   "Type validation failed for the 1th output.");
  }
  if (auto output_2_type = (*this)->result(2).type()) {
    IR_ENFORCE(output_2_type.isa<paddle::dialect::DenseTensorType>(),
                   "Type validation failed for the 2th output.");
  }
  if (auto output_3_type = (*this)->result(3).type()) {
    IR_ENFORCE(output_3_type.isa<paddle::dialect::DenseTensorType>(),
                   "Type validation failed for the 3th output.");
  }
  IR_ENFORCE((*this)->result(4).type().isa<paddle::dialect::DenseTensorType>(),
                 "Type validation failed for the 4th output.");
  }
  VLOG(4) << "End Verifying for: FusionGruOp.";
}

void FusionGruOp::InferMeta( phi::InferMetaContext *infer_meta ) {
  auto fn = PD_INFER_META(phi::FusionGRUInferMeta);
  fn(infer_meta);
}

phi::DataType FusionGruOp::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: FusionGruOp";
  


  return expected_kernel_dtype;
}

OpInfoTuple FusionRepeatedFcReluOp::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("x", "paddle::dialect::DenseTensorType", false, false, false, false), paddle::dialect::OpInputInfo("w", "pir::VectorType<paddle::dialect::DenseTensorType>", false, false, false, false), paddle::dialect::OpInputInfo("bias", "pir::VectorType<paddle::dialect::DenseTensorType>", false, false, false, false) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = {  };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("relu_out", "pir::VectorType<paddle::dialect::DenseTensorType>", true, true), paddle::dialect::OpOutputInfo("out", "paddle::dialect::DenseTensorType", false, false) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("FusionRepeatedFCReluInferMeta", {"x", "w", "bias"}, "fusion_repeated_fc_relu", {"x", "w", "bias"}, {"x"}, {}, {}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "fusion_repeated_fc_relu");
}

void FusionRepeatedFcReluOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, pir::Value w_, pir::Value bias_) {
  VLOG(4) << "Start build FusionRepeatedFcReluOp";



  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_, w_, bias_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;
  pir::VectorType w = w_.type().dyn_cast<pir::VectorType>(); (void)w;
  pir::VectorType bias = bias_.type().dyn_cast<pir::VectorType>(); (void)bias;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);
  std::vector<paddle::dialect::IrTensor> vec_ir_tensor_w;
  for (size_t i=0; i < static_cast<size_t>(w.size()); i++) {
    vec_ir_tensor_w.push_back(paddle::dialect::IrTensor(paddle::dialect::TransToPhiDataType(w[i].dyn_cast<paddle::dialect::DenseTensorType>().dtype()),
                                                                     w[i].dyn_cast<paddle::dialect::DenseTensorType>().dims(),
                                                                     w[i].dyn_cast<paddle::dialect::DenseTensorType>().data_layout(),
                                                                     w[i].dyn_cast<paddle::dialect::DenseTensorType>().lod(),
                                                                     w[i].dyn_cast<paddle::dialect::DenseTensorType>().offset()));
  }
  std::vector<paddle::dialect::IrMetaTensor> vec_meta_w;
  for (size_t i=0; i < vec_ir_tensor_w.size(); i++) {
    vec_meta_w.push_back(paddle::dialect::IrMetaTensor(&vec_ir_tensor_w[i]));
  }

  std::vector<const phi::MetaTensor*> meta_w;
  for (size_t i=0; i < static_cast<size_t>(vec_meta_w.size()); i++) {
    meta_w.push_back(&vec_meta_w[i]);
  }
   std::vector<paddle::dialect::IrTensor> vec_ir_tensor_bias;
  for (size_t i=0; i < static_cast<size_t>(bias.size()); i++) {
    vec_ir_tensor_bias.push_back(paddle::dialect::IrTensor(paddle::dialect::TransToPhiDataType(bias[i].dyn_cast<paddle::dialect::DenseTensorType>().dtype()),
                                                                     bias[i].dyn_cast<paddle::dialect::DenseTensorType>().dims(),
                                                                     bias[i].dyn_cast<paddle::dialect::DenseTensorType>().data_layout(),
                                                                     bias[i].dyn_cast<paddle::dialect::DenseTensorType>().lod(),
                                                                     bias[i].dyn_cast<paddle::dialect::DenseTensorType>().offset()));
  }
  std::vector<paddle::dialect::IrMetaTensor> vec_meta_bias;
  for (size_t i=0; i < vec_ir_tensor_bias.size(); i++) {
    vec_meta_bias.push_back(paddle::dialect::IrMetaTensor(&vec_ir_tensor_bias[i]));
  }

  std::vector<const phi::MetaTensor*> meta_bias;
  for (size_t i=0; i < static_cast<size_t>(vec_meta_bias.size()); i++) {
    meta_bias.push_back(&vec_meta_bias[i]);
  }
   std::vector<paddle::dialect::IrTensor> vec_dense_relu_out((w.size()-1), paddle::dialect::IrTensor());
  std::vector<paddle::dialect::IrMetaTensor> vec_meta_relu_out;
  for (size_t i=0; i < static_cast<size_t>(w.size()-1); i++) {
    vec_meta_relu_out.push_back(paddle::dialect::IrMetaTensor(&vec_dense_relu_out[i]));
  }
  std::vector<phi::MetaTensor*> meta_relu_out;
  for (size_t i=0; i < static_cast<size_t>(vec_meta_relu_out.size()); i++) {
    meta_relu_out.push_back(&vec_meta_relu_out[i]);
  }
  paddle::dialect::IrTensor dense_out;
  paddle::dialect::IrMetaTensor meta_out(&dense_out);

  phi::FusionRepeatedFCReluInferMeta(meta_x, meta_w, meta_bias, meta_relu_out, &meta_out);

  std::vector<pir::Type> argument_outputs;
  std::vector<pir::Type> relu_out_types;
  for (size_t i=0; i < static_cast<size_t>(w.size()-1); i++) {
    relu_out_types.push_back(paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(vec_dense_relu_out[i].dtype()), vec_dense_relu_out[i].dims(), vec_dense_relu_out[i].layout(), vec_dense_relu_out[i].lod(), vec_dense_relu_out[i].offset()));
  }
  pir::Type relu_out_vector_type = pir::VectorType::get(pir::IrContext::Instance(), relu_out_types);
  argument_outputs.push_back(relu_out_vector_type);

  pir::Type out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_out.dtype()), dense_out.dims(), dense_out.layout(), dense_out.lod(), dense_out.offset());
  argument_outputs.push_back(out_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void FusionRepeatedFcReluOp::VerifySig() {
  VLOG(4) << "Start Verifying inputs, outputs and attributes for: FusionRepeatedFcReluOp.";
  VLOG(4) << "Verifying inputs:";
  {
  auto input_size = num_operands();
  IR_ENFORCE(input_size == 3u,
                    "The size %d of inputs must be equal to 3.", input_size);
  IR_ENFORCE((*this)->operand_source(0).type().isa<paddle::dialect::DenseTensorType>(),
                  "Type validation failed for the 0th input, got %s.", (*this)->operand_source(0).type());
  if (auto vec_type = (*this)->operand_source(1).type().dyn_cast<pir::VectorType>()) {
      for (size_t i = 0; i < vec_type.size(); ++i) {
        IR_ENFORCE(vec_type[i].isa<paddle::dialect::DenseTensorType>(),
                       "Type validation failed for the 1th input, got %s.", (*this)->operand_source(1).type());
      }
  }
  else {
    IR_ENFORCE((*this)->operand_source(1).type().isa<paddle::dialect::DenseTensorType>(),
                   "Type validation failed for the 1th input, got %s.", (*this)->operand_source(1).type());
  }
  if (auto vec_type = (*this)->operand_source(2).type().dyn_cast<pir::VectorType>()) {
      for (size_t i = 0; i < vec_type.size(); ++i) {
        IR_ENFORCE(vec_type[i].isa<paddle::dialect::DenseTensorType>(),
                       "Type validation failed for the 2th input, got %s.", (*this)->operand_source(2).type());
      }
  }
  else {
    IR_ENFORCE((*this)->operand_source(2).type().isa<paddle::dialect::DenseTensorType>(),
                   "Type validation failed for the 2th input, got %s.", (*this)->operand_source(2).type());
  }
  }
  VLOG(4) << "Verifying attributes:";
  {
  // Attributes num is 0, not need to check attributes type.
  }
  VLOG(4) << "Verifying outputs:";
  {
  auto output_size = num_results();
  IR_ENFORCE(output_size == 2u,
                    "The size %d of outputs must be equal to 2.", output_size);
  if (auto output_0_type = (*this)->result(0).type()) {
    if (auto vec_type = output_0_type.dyn_cast<pir::VectorType>()) {
      for (size_t i = 0; i < vec_type.size(); ++i) {
        IR_ENFORCE(vec_type[i].isa<paddle::dialect::DenseTensorType>(),
                       "Type validation failed for the 0th output.");
      }
    }
    else {
      IR_ENFORCE(output_0_type.isa<paddle::dialect::DenseTensorType>(),
                     "Type validation failed for the 0th output.");
    }
  }
  IR_ENFORCE((*this)->result(1).type().isa<paddle::dialect::DenseTensorType>(),
                 "Type validation failed for the 1th output.");
  }
  VLOG(4) << "End Verifying for: FusionRepeatedFcReluOp.";
}

void FusionRepeatedFcReluOp::InferMeta( phi::InferMetaContext *infer_meta ) {
  auto fn = PD_INFER_META(phi::FusionRepeatedFCReluInferMeta);
  fn(infer_meta);
}

phi::DataType FusionRepeatedFcReluOp::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: FusionRepeatedFcReluOp";
  


  return expected_kernel_dtype;
}

const char *FusionSeqconvEltaddReluOp::attributes_name[3] = { "context_length", "context_start", "context_stride" };

OpInfoTuple FusionSeqconvEltaddReluOp::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("x", "paddle::dialect::DenseTensorType", false, false, false, false), paddle::dialect::OpInputInfo("filter", "paddle::dialect::DenseTensorType", false, false, false, false), paddle::dialect::OpInputInfo("bias", "paddle::dialect::DenseTensorType", false, false, false, false) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = { paddle::dialect::OpAttributeInfo("context_length", "pir::Int32Attribute", ""), paddle::dialect::OpAttributeInfo("context_start", "pir::Int32Attribute", ""), paddle::dialect::OpAttributeInfo("context_stride", "pir::Int32Attribute", "") };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("out", "paddle::dialect::DenseTensorType", false, false), paddle::dialect::OpOutputInfo("col_mat", "paddle::dialect::DenseTensorType", true, true) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("FusionSeqConvEltAddReluInferMeta", {"x", "filter", "bias", "context_length", "context_start", "context_stride"}, "fusion_seqconv_eltadd_relu", {"x", "filter", "bias", "context_length", "context_start", "context_stride"}, {"x"}, {}, {}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "fusion_seqconv_eltadd_relu");
}

void FusionSeqconvEltaddReluOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, pir::Value filter_, pir::Value bias_, int context_length, int context_start, int context_stride) {
  VLOG(4) << "Start build FusionSeqconvEltaddReluOp";



  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_, filter_, bias_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_context_length = pir::Int32Attribute::get(pir::IrContext::Instance(), context_length);
  argument.AddAttribute("context_length", attr_context_length);
  pir::Attribute attr_context_start = pir::Int32Attribute::get(pir::IrContext::Instance(), context_start);
  argument.AddAttribute("context_start", attr_context_start);
  pir::Attribute attr_context_stride = pir::Int32Attribute::get(pir::IrContext::Instance(), context_stride);
  argument.AddAttribute("context_stride", attr_context_stride);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;
  paddle::dialect::DenseTensorType filter = filter_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)filter;
  paddle::dialect::DenseTensorType bias = bias_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)bias;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);

  VLOG(4) << "Builder construction  dense_filter";
  paddle::dialect::IrTensor ir_tensor_filter(paddle::dialect::TransToPhiDataType(filter.dtype()),
                                                      filter.dims(),
                                                      filter.data_layout(),
                                                      filter.lod(),
                                                      filter.offset());
  VLOG(4) << "Builder construction  meta_filter";
  paddle::dialect::IrMetaTensor meta_filter(&ir_tensor_filter);

  VLOG(4) << "Builder construction  dense_bias";
  paddle::dialect::IrTensor ir_tensor_bias(paddle::dialect::TransToPhiDataType(bias.dtype()),
                                                      bias.dims(),
                                                      bias.data_layout(),
                                                      bias.lod(),
                                                      bias.offset());
  VLOG(4) << "Builder construction  meta_bias";
  paddle::dialect::IrMetaTensor meta_bias(&ir_tensor_bias);
  paddle::dialect::IrTensor dense_out;
  paddle::dialect::IrMetaTensor meta_out(&dense_out);
  paddle::dialect::IrTensor dense_col_mat;
  paddle::dialect::IrMetaTensor meta_col_mat(&dense_col_mat);

  phi::FusionSeqConvEltAddReluInferMeta(meta_x, meta_filter, meta_bias, context_length, context_start, context_stride, &meta_out, &meta_col_mat);

  std::vector<pir::Type> argument_outputs;
  pir::Type out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_out.dtype()), dense_out.dims(), dense_out.layout(), dense_out.lod(), dense_out.offset());
  argument_outputs.push_back(out_dense_tensor_type);

  pir::Type col_mat_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_col_mat.dtype()), dense_col_mat.dims(), dense_col_mat.layout(), dense_col_mat.lod(), dense_col_mat.offset());
  argument_outputs.push_back(col_mat_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void FusionSeqconvEltaddReluOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, pir::Value filter_, pir::Value bias_, pir::AttributeMap attributes) {
  VLOG(4) << "Start build FusionSeqconvEltaddReluOp";


  IR_ENFORCE(
      attributes.find("context_length") != attributes.end(),
          "'context_length' Attribute is expected for FusionSeqconvEltaddReluOp. ");
  int context_length = attributes.at("context_length").dyn_cast<pir::Int32Attribute>().data();

  IR_ENFORCE(
      attributes.find("context_start") != attributes.end(),
          "'context_start' Attribute is expected for FusionSeqconvEltaddReluOp. ");
  int context_start = attributes.at("context_start").dyn_cast<pir::Int32Attribute>().data();

  IR_ENFORCE(
      attributes.find("context_stride") != attributes.end(),
          "'context_stride' Attribute is expected for FusionSeqconvEltaddReluOp. ");
  int context_stride = attributes.at("context_stride").dyn_cast<pir::Int32Attribute>().data();


  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_, filter_, bias_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_context_length = pir::Int32Attribute::get(pir::IrContext::Instance(), context_length);
  argument.AddAttribute("context_length", attr_context_length);
  pir::Attribute attr_context_start = pir::Int32Attribute::get(pir::IrContext::Instance(), context_start);
  argument.AddAttribute("context_start", attr_context_start);
  pir::Attribute attr_context_stride = pir::Int32Attribute::get(pir::IrContext::Instance(), context_stride);
  argument.AddAttribute("context_stride", attr_context_stride);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;
  paddle::dialect::DenseTensorType filter = filter_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)filter;
  paddle::dialect::DenseTensorType bias = bias_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)bias;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);

  VLOG(4) << "Builder construction  dense_filter";
  paddle::dialect::IrTensor ir_tensor_filter(paddle::dialect::TransToPhiDataType(filter.dtype()),
                                                      filter.dims(),
                                                      filter.data_layout(),
                                                      filter.lod(),
                                                      filter.offset());
  VLOG(4) << "Builder construction  meta_filter";
  paddle::dialect::IrMetaTensor meta_filter(&ir_tensor_filter);

  VLOG(4) << "Builder construction  dense_bias";
  paddle::dialect::IrTensor ir_tensor_bias(paddle::dialect::TransToPhiDataType(bias.dtype()),
                                                      bias.dims(),
                                                      bias.data_layout(),
                                                      bias.lod(),
                                                      bias.offset());
  VLOG(4) << "Builder construction  meta_bias";
  paddle::dialect::IrMetaTensor meta_bias(&ir_tensor_bias);
  paddle::dialect::IrTensor dense_out;
  paddle::dialect::IrMetaTensor meta_out(&dense_out);
  paddle::dialect::IrTensor dense_col_mat;
  paddle::dialect::IrMetaTensor meta_col_mat(&dense_col_mat);

  phi::FusionSeqConvEltAddReluInferMeta(meta_x, meta_filter, meta_bias, context_length, context_start, context_stride, &meta_out, &meta_col_mat);

  std::vector<pir::Type> argument_outputs;
  pir::Type out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_out.dtype()), dense_out.dims(), dense_out.layout(), dense_out.lod(), dense_out.offset());
  argument_outputs.push_back(out_dense_tensor_type);

  pir::Type col_mat_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_col_mat.dtype()), dense_col_mat.dims(), dense_col_mat.layout(), dense_col_mat.lod(), dense_col_mat.offset());
  argument_outputs.push_back(col_mat_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void FusionSeqconvEltaddReluOp::VerifySig() {
  VLOG(4) << "Start Verifying inputs, outputs and attributes for: FusionSeqconvEltaddReluOp.";
  VLOG(4) << "Verifying inputs:";
  {
  auto input_size = num_operands();
  IR_ENFORCE(input_size == 3u,
                    "The size %d of inputs must be equal to 3.", input_size);
  IR_ENFORCE((*this)->operand_source(0).type().isa<paddle::dialect::DenseTensorType>(),
                  "Type validation failed for the 0th input, got %s.", (*this)->operand_source(0).type());
  IR_ENFORCE((*this)->operand_source(1).type().isa<paddle::dialect::DenseTensorType>(),
                  "Type validation failed for the 1th input, got %s.", (*this)->operand_source(1).type());
  IR_ENFORCE((*this)->operand_source(2).type().isa<paddle::dialect::DenseTensorType>(),
                  "Type validation failed for the 2th input, got %s.", (*this)->operand_source(2).type());
  }
  VLOG(4) << "Verifying attributes:";
  {
  auto& attributes = this->attributes();
  IR_ENFORCE(attributes.count("context_length")>0,
                 "context_length does not exist.");
  IR_ENFORCE(attributes.at("context_length").isa<pir::Int32Attribute>(),
                 "Type of attribute: context_length is not pir::Int32Attribute.");

  IR_ENFORCE(attributes.count("context_start")>0,
                 "context_start does not exist.");
  IR_ENFORCE(attributes.at("context_start").isa<pir::Int32Attribute>(),
                 "Type of attribute: context_start is not pir::Int32Attribute.");

  IR_ENFORCE(attributes.count("context_stride")>0,
                 "context_stride does not exist.");
  IR_ENFORCE(attributes.at("context_stride").isa<pir::Int32Attribute>(),
                 "Type of attribute: context_stride is not pir::Int32Attribute.");

  }
  VLOG(4) << "Verifying outputs:";
  {
  auto output_size = num_results();
  IR_ENFORCE(output_size == 2u,
                    "The size %d of outputs must be equal to 2.", output_size);
  IR_ENFORCE((*this)->result(0).type().isa<paddle::dialect::DenseTensorType>(),
                 "Type validation failed for the 0th output.");
  if (auto output_1_type = (*this)->result(1).type()) {
    IR_ENFORCE(output_1_type.isa<paddle::dialect::DenseTensorType>(),
                   "Type validation failed for the 1th output.");
  }
  }
  VLOG(4) << "End Verifying for: FusionSeqconvEltaddReluOp.";
}

void FusionSeqconvEltaddReluOp::InferMeta( phi::InferMetaContext *infer_meta ) {
  auto fn = PD_INFER_META(phi::FusionSeqConvEltAddReluInferMeta);
  fn(infer_meta);
}

phi::DataType FusionSeqconvEltaddReluOp::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: FusionSeqconvEltaddReluOp";
  


  return expected_kernel_dtype;
}

const char *FusionSeqexpandConcatFcOp::attributes_name[1] = { "fc_activation" };

OpInfoTuple FusionSeqexpandConcatFcOp::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("x", "pir::VectorType<paddle::dialect::DenseTensorType>", false, false, false, false), paddle::dialect::OpInputInfo("fc_weight", "paddle::dialect::DenseTensorType", false, false, false, false), paddle::dialect::OpInputInfo("fc_bias", "paddle::dialect::DenseTensorType", true, false, false, false) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = { paddle::dialect::OpAttributeInfo("fc_activation", "pir::StrAttribute", "") };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("out", "paddle::dialect::DenseTensorType", false, false), paddle::dialect::OpOutputInfo("fc_out", "paddle::dialect::DenseTensorType", true, true) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("FusionSeqExpandConcatFCInferMeta", {"x", "fc_weight", "fc_bias", "fc_activation"}, "fusion_seqexpand_concat_fc", {"x", "fc_weight", "fc_bias", "fc_activation"}, {"x"}, {}, {}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "fusion_seqexpand_concat_fc");
}

void FusionSeqexpandConcatFcOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, pir::Value fc_weight_, pir::Value fc_bias_, const std::string& fc_activation) {
  VLOG(4) << "Start build FusionSeqexpandConcatFcOp";



  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_, fc_weight_, fc_bias_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_fc_activation = pir::StrAttribute::get(pir::IrContext::Instance(), fc_activation);
  argument.AddAttribute("fc_activation", attr_fc_activation);

  VLOG(4) << "Builder construction outputs";
  pir::VectorType x = x_.type().dyn_cast<pir::VectorType>(); (void)x;
  paddle::dialect::DenseTensorType fc_weight = fc_weight_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)fc_weight;
  std::vector<paddle::dialect::IrTensor> vec_ir_tensor_x;
  for (size_t i=0; i < static_cast<size_t>(x.size()); i++) {
    vec_ir_tensor_x.push_back(paddle::dialect::IrTensor(paddle::dialect::TransToPhiDataType(x[i].dyn_cast<paddle::dialect::DenseTensorType>().dtype()),
                                                                     x[i].dyn_cast<paddle::dialect::DenseTensorType>().dims(),
                                                                     x[i].dyn_cast<paddle::dialect::DenseTensorType>().data_layout(),
                                                                     x[i].dyn_cast<paddle::dialect::DenseTensorType>().lod(),
                                                                     x[i].dyn_cast<paddle::dialect::DenseTensorType>().offset()));
  }
  std::vector<paddle::dialect::IrMetaTensor> vec_meta_x;
  for (size_t i=0; i < vec_ir_tensor_x.size(); i++) {
    vec_meta_x.push_back(paddle::dialect::IrMetaTensor(&vec_ir_tensor_x[i]));
  }

  std::vector<const phi::MetaTensor*> meta_x;
  for (size_t i=0; i < static_cast<size_t>(vec_meta_x.size()); i++) {
    meta_x.push_back(&vec_meta_x[i]);
  }
 
  VLOG(4) << "Builder construction  dense_fc_weight";
  paddle::dialect::IrTensor ir_tensor_fc_weight(paddle::dialect::TransToPhiDataType(fc_weight.dtype()),
                                                      fc_weight.dims(),
                                                      fc_weight.data_layout(),
                                                      fc_weight.lod(),
                                                      fc_weight.offset());
  VLOG(4) << "Builder construction  meta_fc_weight";
  paddle::dialect::IrMetaTensor meta_fc_weight(&ir_tensor_fc_weight);

  paddle::dialect::IrMetaTensor meta_fc_bias;
  paddle::dialect::IrTensor ir_tensor_fc_bias;
  if (fc_bias_.impl() != nullptr) {
    paddle::dialect::DenseTensorType fc_bias = fc_bias_.type().dyn_cast<paddle::dialect::DenseTensorType>();
    VLOG(4) << "Builder construction  dense_fc_bias";
    ir_tensor_fc_bias = paddle::dialect::IrTensor(paddle::dialect::TransToPhiDataType(fc_bias.dtype()),
                                                        fc_bias.dims(),
                                                        fc_bias.data_layout(),
                                                        fc_bias.lod(),
                                                        fc_bias.offset());
    VLOG(4) << "Builder construction  meta_fc_bias";
    meta_fc_bias = paddle::dialect::IrMetaTensor(&ir_tensor_fc_bias);
  }

  paddle::dialect::IrTensor dense_out;
  paddle::dialect::IrMetaTensor meta_out(&dense_out);
  paddle::dialect::IrTensor dense_fc_out;
  paddle::dialect::IrMetaTensor meta_fc_out(&dense_fc_out);

  phi::FusionSeqExpandConcatFCInferMeta(meta_x, meta_fc_weight, meta_fc_bias, fc_activation, &meta_out, &meta_fc_out);

  std::vector<pir::Type> argument_outputs;
  pir::Type out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_out.dtype()), dense_out.dims(), dense_out.layout(), dense_out.lod(), dense_out.offset());
  argument_outputs.push_back(out_dense_tensor_type);

  pir::Type fc_out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_fc_out.dtype()), dense_fc_out.dims(), dense_fc_out.layout(), dense_fc_out.lod(), dense_fc_out.offset());
  argument_outputs.push_back(fc_out_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void FusionSeqexpandConcatFcOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, pir::Value fc_weight_, pir::Value fc_bias_, pir::AttributeMap attributes) {
  VLOG(4) << "Start build FusionSeqexpandConcatFcOp";


  IR_ENFORCE(
      attributes.find("fc_activation") != attributes.end(),
          "'fc_activation' Attribute is expected for FusionSeqexpandConcatFcOp. ");
  std::string fc_activation = attributes.at("fc_activation").dyn_cast<pir::StrAttribute>().AsString();


  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_, fc_weight_, fc_bias_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_fc_activation = pir::StrAttribute::get(pir::IrContext::Instance(), fc_activation);
  argument.AddAttribute("fc_activation", attr_fc_activation);

  VLOG(4) << "Builder construction outputs";
  pir::VectorType x = x_.type().dyn_cast<pir::VectorType>(); (void)x;
  paddle::dialect::DenseTensorType fc_weight = fc_weight_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)fc_weight;
  std::vector<paddle::dialect::IrTensor> vec_ir_tensor_x;
  for (size_t i=0; i < static_cast<size_t>(x.size()); i++) {
    vec_ir_tensor_x.push_back(paddle::dialect::IrTensor(paddle::dialect::TransToPhiDataType(x[i].dyn_cast<paddle::dialect::DenseTensorType>().dtype()),
                                                                     x[i].dyn_cast<paddle::dialect::DenseTensorType>().dims(),
                                                                     x[i].dyn_cast<paddle::dialect::DenseTensorType>().data_layout(),
                                                                     x[i].dyn_cast<paddle::dialect::DenseTensorType>().lod(),
                                                                     x[i].dyn_cast<paddle::dialect::DenseTensorType>().offset()));
  }
  std::vector<paddle::dialect::IrMetaTensor> vec_meta_x;
  for (size_t i=0; i < vec_ir_tensor_x.size(); i++) {
    vec_meta_x.push_back(paddle::dialect::IrMetaTensor(&vec_ir_tensor_x[i]));
  }

  std::vector<const phi::MetaTensor*> meta_x;
  for (size_t i=0; i < static_cast<size_t>(vec_meta_x.size()); i++) {
    meta_x.push_back(&vec_meta_x[i]);
  }
 
  VLOG(4) << "Builder construction  dense_fc_weight";
  paddle::dialect::IrTensor ir_tensor_fc_weight(paddle::dialect::TransToPhiDataType(fc_weight.dtype()),
                                                      fc_weight.dims(),
                                                      fc_weight.data_layout(),
                                                      fc_weight.lod(),
                                                      fc_weight.offset());
  VLOG(4) << "Builder construction  meta_fc_weight";
  paddle::dialect::IrMetaTensor meta_fc_weight(&ir_tensor_fc_weight);

  paddle::dialect::IrMetaTensor meta_fc_bias;
  paddle::dialect::IrTensor ir_tensor_fc_bias;
  if (fc_bias_.impl() != nullptr) {
    paddle::dialect::DenseTensorType fc_bias = fc_bias_.type().dyn_cast<paddle::dialect::DenseTensorType>();
    VLOG(4) << "Builder construction  dense_fc_bias";
    ir_tensor_fc_bias = paddle::dialect::IrTensor(paddle::dialect::TransToPhiDataType(fc_bias.dtype()),
                                                        fc_bias.dims(),
                                                        fc_bias.data_layout(),
                                                        fc_bias.lod(),
                                                        fc_bias.offset());
    VLOG(4) << "Builder construction  meta_fc_bias";
    meta_fc_bias = paddle::dialect::IrMetaTensor(&ir_tensor_fc_bias);
  }

  paddle::dialect::IrTensor dense_out;
  paddle::dialect::IrMetaTensor meta_out(&dense_out);
  paddle::dialect::IrTensor dense_fc_out;
  paddle::dialect::IrMetaTensor meta_fc_out(&dense_fc_out);

  phi::FusionSeqExpandConcatFCInferMeta(meta_x, meta_fc_weight, meta_fc_bias, fc_activation, &meta_out, &meta_fc_out);

  std::vector<pir::Type> argument_outputs;
  pir::Type out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_out.dtype()), dense_out.dims(), dense_out.layout(), dense_out.lod(), dense_out.offset());
  argument_outputs.push_back(out_dense_tensor_type);

  pir::Type fc_out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_fc_out.dtype()), dense_fc_out.dims(), dense_fc_out.layout(), dense_fc_out.lod(), dense_fc_out.offset());
  argument_outputs.push_back(fc_out_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void FusionSeqexpandConcatFcOp::VerifySig() {
  VLOG(4) << "Start Verifying inputs, outputs and attributes for: FusionSeqexpandConcatFcOp.";
  VLOG(4) << "Verifying inputs:";
  {
  auto input_size = num_operands();
  IR_ENFORCE(input_size == 3u,
                    "The size %d of inputs must be equal to 3.", input_size);
  if (auto vec_type = (*this)->operand_source(0).type().dyn_cast<pir::VectorType>()) {
      for (size_t i = 0; i < vec_type.size(); ++i) {
        IR_ENFORCE(vec_type[i].isa<paddle::dialect::DenseTensorType>(),
                       "Type validation failed for the 0th input, got %s.", (*this)->operand_source(0).type());
      }
  }
  else {
    IR_ENFORCE((*this)->operand_source(0).type().isa<paddle::dialect::DenseTensorType>(),
                   "Type validation failed for the 0th input, got %s.", (*this)->operand_source(0).type());
  }
  IR_ENFORCE((*this)->operand_source(1).type().isa<paddle::dialect::DenseTensorType>(),
                  "Type validation failed for the 1th input, got %s.", (*this)->operand_source(1).type());
  if (auto val = (*this)->operand(2)) {
    IR_ENFORCE(val.type().isa<paddle::dialect::DenseTensorType>(),
                   "Type validation failed for the 2th input, got %s.", (*this)->operand_source(2).type());
  }
  }
  VLOG(4) << "Verifying attributes:";
  {
  auto& attributes = this->attributes();
  IR_ENFORCE(attributes.count("fc_activation")>0,
                 "fc_activation does not exist.");
  IR_ENFORCE(attributes.at("fc_activation").isa<pir::StrAttribute>(),
                 "Type of attribute: fc_activation is not pir::StrAttribute.");

  }
  VLOG(4) << "Verifying outputs:";
  {
  auto output_size = num_results();
  IR_ENFORCE(output_size == 2u,
                    "The size %d of outputs must be equal to 2.", output_size);
  IR_ENFORCE((*this)->result(0).type().isa<paddle::dialect::DenseTensorType>(),
                 "Type validation failed for the 0th output.");
  if (auto output_1_type = (*this)->result(1).type()) {
    IR_ENFORCE(output_1_type.isa<paddle::dialect::DenseTensorType>(),
                   "Type validation failed for the 1th output.");
  }
  }
  VLOG(4) << "End Verifying for: FusionSeqexpandConcatFcOp.";
}

void FusionSeqexpandConcatFcOp::InferMeta( phi::InferMetaContext *infer_meta ) {
  auto fn = PD_INFER_META(phi::FusionSeqExpandConcatFCInferMeta);
  fn(infer_meta);
}

phi::DataType FusionSeqexpandConcatFcOp::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: FusionSeqexpandConcatFcOp";
  


  return expected_kernel_dtype;
}

const char *FusionSquaredMatSubOp::attributes_name[1] = { "scalar" };

OpInfoTuple FusionSquaredMatSubOp::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("x", "paddle::dialect::DenseTensorType", false, false, false, false), paddle::dialect::OpInputInfo("y", "paddle::dialect::DenseTensorType", false, false, false, false) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = { paddle::dialect::OpAttributeInfo("scalar", "pir::FloatAttribute", "") };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("squared_x", "paddle::dialect::DenseTensorType", true, true), paddle::dialect::OpOutputInfo("squared_y", "paddle::dialect::DenseTensorType", true, true), paddle::dialect::OpOutputInfo("squared_xy", "paddle::dialect::DenseTensorType", true, true), paddle::dialect::OpOutputInfo("out", "paddle::dialect::DenseTensorType", false, false) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("FusionSquaredMatSubInferMeta", {"x", "y", "scalar"}, "fusion_squared_mat_sub", {"x", "y", "scalar"}, {"x"}, {}, {}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "fusion_squared_mat_sub");
}

void FusionSquaredMatSubOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, pir::Value y_, float scalar) {
  VLOG(4) << "Start build FusionSquaredMatSubOp";



  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_, y_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_scalar = pir::FloatAttribute::get(pir::IrContext::Instance(), scalar);
  argument.AddAttribute("scalar", attr_scalar);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;
  paddle::dialect::DenseTensorType y = y_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)y;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);

  VLOG(4) << "Builder construction  dense_y";
  paddle::dialect::IrTensor ir_tensor_y(paddle::dialect::TransToPhiDataType(y.dtype()),
                                                      y.dims(),
                                                      y.data_layout(),
                                                      y.lod(),
                                                      y.offset());
  VLOG(4) << "Builder construction  meta_y";
  paddle::dialect::IrMetaTensor meta_y(&ir_tensor_y);
  paddle::dialect::IrTensor dense_squared_x;
  paddle::dialect::IrMetaTensor meta_squared_x(&dense_squared_x);
  paddle::dialect::IrTensor dense_squared_y;
  paddle::dialect::IrMetaTensor meta_squared_y(&dense_squared_y);
  paddle::dialect::IrTensor dense_squared_xy;
  paddle::dialect::IrMetaTensor meta_squared_xy(&dense_squared_xy);
  paddle::dialect::IrTensor dense_out;
  paddle::dialect::IrMetaTensor meta_out(&dense_out);

  phi::FusionSquaredMatSubInferMeta(meta_x, meta_y, scalar, &meta_squared_x, &meta_squared_y, &meta_squared_xy, &meta_out);

  std::vector<pir::Type> argument_outputs;
  pir::Type squared_x_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_squared_x.dtype()), dense_squared_x.dims(), dense_squared_x.layout(), dense_squared_x.lod(), dense_squared_x.offset());
  argument_outputs.push_back(squared_x_dense_tensor_type);

  pir::Type squared_y_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_squared_y.dtype()), dense_squared_y.dims(), dense_squared_y.layout(), dense_squared_y.lod(), dense_squared_y.offset());
  argument_outputs.push_back(squared_y_dense_tensor_type);

  pir::Type squared_xy_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_squared_xy.dtype()), dense_squared_xy.dims(), dense_squared_xy.layout(), dense_squared_xy.lod(), dense_squared_xy.offset());
  argument_outputs.push_back(squared_xy_dense_tensor_type);

  pir::Type out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_out.dtype()), dense_out.dims(), dense_out.layout(), dense_out.lod(), dense_out.offset());
  argument_outputs.push_back(out_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void FusionSquaredMatSubOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, pir::Value y_, pir::AttributeMap attributes) {
  VLOG(4) << "Start build FusionSquaredMatSubOp";


  IR_ENFORCE(
      attributes.find("scalar") != attributes.end(),
          "'scalar' Attribute is expected for FusionSquaredMatSubOp. ");
  float scalar = attributes.at("scalar").dyn_cast<pir::FloatAttribute>().data();


  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_, y_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_scalar = pir::FloatAttribute::get(pir::IrContext::Instance(), scalar);
  argument.AddAttribute("scalar", attr_scalar);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;
  paddle::dialect::DenseTensorType y = y_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)y;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);

  VLOG(4) << "Builder construction  dense_y";
  paddle::dialect::IrTensor ir_tensor_y(paddle::dialect::TransToPhiDataType(y.dtype()),
                                                      y.dims(),
                                                      y.data_layout(),
                                                      y.lod(),
                                                      y.offset());
  VLOG(4) << "Builder construction  meta_y";
  paddle::dialect::IrMetaTensor meta_y(&ir_tensor_y);
  paddle::dialect::IrTensor dense_squared_x;
  paddle::dialect::IrMetaTensor meta_squared_x(&dense_squared_x);
  paddle::dialect::IrTensor dense_squared_y;
  paddle::dialect::IrMetaTensor meta_squared_y(&dense_squared_y);
  paddle::dialect::IrTensor dense_squared_xy;
  paddle::dialect::IrMetaTensor meta_squared_xy(&dense_squared_xy);
  paddle::dialect::IrTensor dense_out;
  paddle::dialect::IrMetaTensor meta_out(&dense_out);

  phi::FusionSquaredMatSubInferMeta(meta_x, meta_y, scalar, &meta_squared_x, &meta_squared_y, &meta_squared_xy, &meta_out);

  std::vector<pir::Type> argument_outputs;
  pir::Type squared_x_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_squared_x.dtype()), dense_squared_x.dims(), dense_squared_x.layout(), dense_squared_x.lod(), dense_squared_x.offset());
  argument_outputs.push_back(squared_x_dense_tensor_type);

  pir::Type squared_y_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_squared_y.dtype()), dense_squared_y.dims(), dense_squared_y.layout(), dense_squared_y.lod(), dense_squared_y.offset());
  argument_outputs.push_back(squared_y_dense_tensor_type);

  pir::Type squared_xy_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_squared_xy.dtype()), dense_squared_xy.dims(), dense_squared_xy.layout(), dense_squared_xy.lod(), dense_squared_xy.offset());
  argument_outputs.push_back(squared_xy_dense_tensor_type);

  pir::Type out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_out.dtype()), dense_out.dims(), dense_out.layout(), dense_out.lod(), dense_out.offset());
  argument_outputs.push_back(out_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void FusionSquaredMatSubOp::VerifySig() {
  VLOG(4) << "Start Verifying inputs, outputs and attributes for: FusionSquaredMatSubOp.";
  VLOG(4) << "Verifying inputs:";
  {
  auto input_size = num_operands();
  IR_ENFORCE(input_size == 2u,
                    "The size %d of inputs must be equal to 2.", input_size);
  IR_ENFORCE((*this)->operand_source(0).type().isa<paddle::dialect::DenseTensorType>(),
                  "Type validation failed for the 0th input, got %s.", (*this)->operand_source(0).type());
  IR_ENFORCE((*this)->operand_source(1).type().isa<paddle::dialect::DenseTensorType>(),
                  "Type validation failed for the 1th input, got %s.", (*this)->operand_source(1).type());
  }
  VLOG(4) << "Verifying attributes:";
  {
  auto& attributes = this->attributes();
  IR_ENFORCE(attributes.count("scalar")>0,
                 "scalar does not exist.");
  IR_ENFORCE(attributes.at("scalar").isa<pir::FloatAttribute>(),
                 "Type of attribute: scalar is not pir::FloatAttribute.");

  }
  VLOG(4) << "Verifying outputs:";
  {
  auto output_size = num_results();
  IR_ENFORCE(output_size == 4u,
                    "The size %d of outputs must be equal to 4.", output_size);
  if (auto output_0_type = (*this)->result(0).type()) {
    IR_ENFORCE(output_0_type.isa<paddle::dialect::DenseTensorType>(),
                   "Type validation failed for the 0th output.");
  }
  if (auto output_1_type = (*this)->result(1).type()) {
    IR_ENFORCE(output_1_type.isa<paddle::dialect::DenseTensorType>(),
                   "Type validation failed for the 1th output.");
  }
  if (auto output_2_type = (*this)->result(2).type()) {
    IR_ENFORCE(output_2_type.isa<paddle::dialect::DenseTensorType>(),
                   "Type validation failed for the 2th output.");
  }
  IR_ENFORCE((*this)->result(3).type().isa<paddle::dialect::DenseTensorType>(),
                 "Type validation failed for the 3th output.");
  }
  VLOG(4) << "End Verifying for: FusionSquaredMatSubOp.";
}

void FusionSquaredMatSubOp::InferMeta( phi::InferMetaContext *infer_meta ) {
  auto fn = PD_INFER_META(phi::FusionSquaredMatSubInferMeta);
  fn(infer_meta);
}

phi::DataType FusionSquaredMatSubOp::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: FusionSquaredMatSubOp";
  


  return expected_kernel_dtype;
}

const char *FusionTransposeFlattenConcatOp::attributes_name[3] = { "trans_axis", "flatten_axis", "concat_axis" };

OpInfoTuple FusionTransposeFlattenConcatOp::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("x", "pir::VectorType<paddle::dialect::DenseTensorType>", false, false, false, false) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = { paddle::dialect::OpAttributeInfo("trans_axis", "pir::ArrayAttribute<pir::Int32Attribute>", ""), paddle::dialect::OpAttributeInfo("flatten_axis", "pir::Int32Attribute", ""), paddle::dialect::OpAttributeInfo("concat_axis", "pir::Int32Attribute", "") };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("out", "paddle::dialect::DenseTensorType", false, false) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("FusionTransposeFlattenConcatInferMeta", {"x", "trans_axis", "flatten_axis", "concat_axis"}, "fusion_transpose_flatten_concat", {"x", "trans_axis", "flatten_axis", "concat_axis"}, {"x"}, {}, {}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "fusion_transpose_flatten_concat");
}

void FusionTransposeFlattenConcatOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, const std::vector<int>& trans_axis, int flatten_axis, int concat_axis) {
  VLOG(4) << "Start build FusionTransposeFlattenConcatOp";



  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  std::vector<pir::Attribute> vec_trans_axis;
  for (size_t i = 0; i < static_cast<size_t>(trans_axis.size()); i++) {
      pir::Attribute attr_trans_axis = pir::Int32Attribute::get(pir::IrContext::Instance(), trans_axis[i]);

    vec_trans_axis.push_back(attr_trans_axis);
  }
  pir::Attribute attr_trans_axis = pir::ArrayAttribute::get(pir::IrContext::Instance(), vec_trans_axis);
  argument.AddAttribute("trans_axis", attr_trans_axis);
  pir::Attribute attr_flatten_axis = pir::Int32Attribute::get(pir::IrContext::Instance(), flatten_axis);
  argument.AddAttribute("flatten_axis", attr_flatten_axis);
  pir::Attribute attr_concat_axis = pir::Int32Attribute::get(pir::IrContext::Instance(), concat_axis);
  argument.AddAttribute("concat_axis", attr_concat_axis);

  VLOG(4) << "Builder construction outputs";
  pir::VectorType x = x_.type().dyn_cast<pir::VectorType>(); (void)x;
  std::vector<paddle::dialect::IrTensor> vec_ir_tensor_x;
  for (size_t i=0; i < static_cast<size_t>(x.size()); i++) {
    vec_ir_tensor_x.push_back(paddle::dialect::IrTensor(paddle::dialect::TransToPhiDataType(x[i].dyn_cast<paddle::dialect::DenseTensorType>().dtype()),
                                                                     x[i].dyn_cast<paddle::dialect::DenseTensorType>().dims(),
                                                                     x[i].dyn_cast<paddle::dialect::DenseTensorType>().data_layout(),
                                                                     x[i].dyn_cast<paddle::dialect::DenseTensorType>().lod(),
                                                                     x[i].dyn_cast<paddle::dialect::DenseTensorType>().offset()));
  }
  std::vector<paddle::dialect::IrMetaTensor> vec_meta_x;
  for (size_t i=0; i < vec_ir_tensor_x.size(); i++) {
    vec_meta_x.push_back(paddle::dialect::IrMetaTensor(&vec_ir_tensor_x[i]));
  }

  std::vector<const phi::MetaTensor*> meta_x;
  for (size_t i=0; i < static_cast<size_t>(vec_meta_x.size()); i++) {
    meta_x.push_back(&vec_meta_x[i]);
  }
   paddle::dialect::IrTensor dense_out;
  paddle::dialect::IrMetaTensor meta_out(&dense_out);

  phi::FusionTransposeFlattenConcatInferMeta(meta_x, trans_axis, flatten_axis, concat_axis, &meta_out);

  std::vector<pir::Type> argument_outputs;
  pir::Type out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_out.dtype()), dense_out.dims(), dense_out.layout(), dense_out.lod(), dense_out.offset());
  argument_outputs.push_back(out_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void FusionTransposeFlattenConcatOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, pir::AttributeMap attributes) {
  VLOG(4) << "Start build FusionTransposeFlattenConcatOp";


  IR_ENFORCE(
      attributes.find("trans_axis") != attributes.end(),
          "'trans_axis' Attribute is expected for FusionTransposeFlattenConcatOp. ");
  std::vector<int> trans_axis;
  for (size_t i = 0; i < attributes.at("trans_axis").dyn_cast<pir::ArrayAttribute>().size(); i++) {
    trans_axis.push_back(attributes.at("trans_axis").dyn_cast<pir::ArrayAttribute>().at(i).dyn_cast<pir::Int32Attribute>().data());
  }

  IR_ENFORCE(
      attributes.find("flatten_axis") != attributes.end(),
          "'flatten_axis' Attribute is expected for FusionTransposeFlattenConcatOp. ");
  int flatten_axis = attributes.at("flatten_axis").dyn_cast<pir::Int32Attribute>().data();

  IR_ENFORCE(
      attributes.find("concat_axis") != attributes.end(),
          "'concat_axis' Attribute is expected for FusionTransposeFlattenConcatOp. ");
  int concat_axis = attributes.at("concat_axis").dyn_cast<pir::Int32Attribute>().data();


  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  std::vector<pir::Attribute> vec_trans_axis;
  for (size_t i = 0; i < static_cast<size_t>(trans_axis.size()); i++) {
      pir::Attribute attr_trans_axis = pir::Int32Attribute::get(pir::IrContext::Instance(), trans_axis[i]);

    vec_trans_axis.push_back(attr_trans_axis);
  }
  pir::Attribute attr_trans_axis = pir::ArrayAttribute::get(pir::IrContext::Instance(), vec_trans_axis);
  argument.AddAttribute("trans_axis", attr_trans_axis);
  pir::Attribute attr_flatten_axis = pir::Int32Attribute::get(pir::IrContext::Instance(), flatten_axis);
  argument.AddAttribute("flatten_axis", attr_flatten_axis);
  pir::Attribute attr_concat_axis = pir::Int32Attribute::get(pir::IrContext::Instance(), concat_axis);
  argument.AddAttribute("concat_axis", attr_concat_axis);

  VLOG(4) << "Builder construction outputs";
  pir::VectorType x = x_.type().dyn_cast<pir::VectorType>(); (void)x;
  std::vector<paddle::dialect::IrTensor> vec_ir_tensor_x;
  for (size_t i=0; i < static_cast<size_t>(x.size()); i++) {
    vec_ir_tensor_x.push_back(paddle::dialect::IrTensor(paddle::dialect::TransToPhiDataType(x[i].dyn_cast<paddle::dialect::DenseTensorType>().dtype()),
                                                                     x[i].dyn_cast<paddle::dialect::DenseTensorType>().dims(),
                                                                     x[i].dyn_cast<paddle::dialect::DenseTensorType>().data_layout(),
                                                                     x[i].dyn_cast<paddle::dialect::DenseTensorType>().lod(),
                                                                     x[i].dyn_cast<paddle::dialect::DenseTensorType>().offset()));
  }
  std::vector<paddle::dialect::IrMetaTensor> vec_meta_x;
  for (size_t i=0; i < vec_ir_tensor_x.size(); i++) {
    vec_meta_x.push_back(paddle::dialect::IrMetaTensor(&vec_ir_tensor_x[i]));
  }

  std::vector<const phi::MetaTensor*> meta_x;
  for (size_t i=0; i < static_cast<size_t>(vec_meta_x.size()); i++) {
    meta_x.push_back(&vec_meta_x[i]);
  }
   paddle::dialect::IrTensor dense_out;
  paddle::dialect::IrMetaTensor meta_out(&dense_out);

  phi::FusionTransposeFlattenConcatInferMeta(meta_x, trans_axis, flatten_axis, concat_axis, &meta_out);

  std::vector<pir::Type> argument_outputs;
  pir::Type out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_out.dtype()), dense_out.dims(), dense_out.layout(), dense_out.lod(), dense_out.offset());
  argument_outputs.push_back(out_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void FusionTransposeFlattenConcatOp::VerifySig() {
  VLOG(4) << "Start Verifying inputs, outputs and attributes for: FusionTransposeFlattenConcatOp.";
  VLOG(4) << "Verifying inputs:";
  {
  auto input_size = num_operands();
  IR_ENFORCE(input_size == 1u,
                    "The size %d of inputs must be equal to 1.", input_size);
  if (auto vec_type = (*this)->operand_source(0).type().dyn_cast<pir::VectorType>()) {
      for (size_t i = 0; i < vec_type.size(); ++i) {
        IR_ENFORCE(vec_type[i].isa<paddle::dialect::DenseTensorType>(),
                       "Type validation failed for the 0th input, got %s.", (*this)->operand_source(0).type());
      }
  }
  else {
    IR_ENFORCE((*this)->operand_source(0).type().isa<paddle::dialect::DenseTensorType>(),
                   "Type validation failed for the 0th input, got %s.", (*this)->operand_source(0).type());
  }
  }
  VLOG(4) << "Verifying attributes:";
  {
  auto& attributes = this->attributes();
  IR_ENFORCE(attributes.count("trans_axis")>0,
                 "trans_axis does not exist.");
  IR_ENFORCE(attributes.at("trans_axis").isa<pir::ArrayAttribute>(),
                 "Type of attribute: trans_axis is not pir::ArrayAttribute.");
  for (size_t i = 0; i < attributes.at("trans_axis").dyn_cast<pir::ArrayAttribute>().size(); i++) {
    IR_ENFORCE(attributes.at("trans_axis").dyn_cast<pir::ArrayAttribute>().at(i).isa<pir::Int32Attribute>(),
                   "Type of attribute: trans_axis is not right.");
  }
  IR_ENFORCE(attributes.count("flatten_axis")>0,
                 "flatten_axis does not exist.");
  IR_ENFORCE(attributes.at("flatten_axis").isa<pir::Int32Attribute>(),
                 "Type of attribute: flatten_axis is not pir::Int32Attribute.");

  IR_ENFORCE(attributes.count("concat_axis")>0,
                 "concat_axis does not exist.");
  IR_ENFORCE(attributes.at("concat_axis").isa<pir::Int32Attribute>(),
                 "Type of attribute: concat_axis is not pir::Int32Attribute.");

  }
  VLOG(4) << "Verifying outputs:";
  {
  auto output_size = num_results();
  IR_ENFORCE(output_size == 1u,
                    "The size %d of outputs must be equal to 1.", output_size);
  IR_ENFORCE((*this)->result(0).type().isa<paddle::dialect::DenseTensorType>(),
                 "Type validation failed for the 0th output.");
  }
  VLOG(4) << "End Verifying for: FusionTransposeFlattenConcatOp.";
}

void FusionTransposeFlattenConcatOp::InferMeta( phi::InferMetaContext *infer_meta ) {
  auto fn = PD_INFER_META(phi::FusionTransposeFlattenConcatInferMeta);
  fn(infer_meta);
}

phi::DataType FusionTransposeFlattenConcatOp::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: FusionTransposeFlattenConcatOp";
  


  return expected_kernel_dtype;
}

const char *GenerateSequenceXpuOp::attributes_name[1] = { "dtype" };

OpInfoTuple GenerateSequenceXpuOp::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("x", "paddle::dialect::DenseTensorType", false, false, false, false) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = { paddle::dialect::OpAttributeInfo("dtype", "paddle::dialect::DataTypeAttribute", "") };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("out", "paddle::dialect::DenseTensorType", false, false) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("GenerateSequenceXPUInferMeta", {"x", "dtype"}, "generate_sequence_xpu", {"x", "dtype"}, {"dtype"}, {}, {}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "generate_sequence_xpu");
}

void GenerateSequenceXpuOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, phi::DataType dtype) {
  VLOG(4) << "Start build GenerateSequenceXpuOp";



  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_dtype = paddle::dialect::DataTypeAttribute::get(pir::IrContext::Instance(), dtype);
  argument.AddAttribute("dtype", attr_dtype);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);
  paddle::dialect::IrTensor dense_out;
  paddle::dialect::IrMetaTensor meta_out(&dense_out);

  phi::GenerateSequenceXPUInferMeta(meta_x, dtype, &meta_out);

  std::vector<pir::Type> argument_outputs;
  pir::Type out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_out.dtype()), dense_out.dims(), dense_out.layout(), dense_out.lod(), dense_out.offset());
  argument_outputs.push_back(out_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void GenerateSequenceXpuOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, pir::AttributeMap attributes) {
  VLOG(4) << "Start build GenerateSequenceXpuOp";


  IR_ENFORCE(
      attributes.find("dtype") != attributes.end(),
          "'dtype' Attribute is expected for GenerateSequenceXpuOp. ");
  phi::DataType dtype = attributes.at("dtype").dyn_cast<paddle::dialect::DataTypeAttribute>().data();


  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_dtype = paddle::dialect::DataTypeAttribute::get(pir::IrContext::Instance(), dtype);
  argument.AddAttribute("dtype", attr_dtype);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);
  paddle::dialect::IrTensor dense_out;
  paddle::dialect::IrMetaTensor meta_out(&dense_out);

  phi::GenerateSequenceXPUInferMeta(meta_x, dtype, &meta_out);

  std::vector<pir::Type> argument_outputs;
  pir::Type out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_out.dtype()), dense_out.dims(), dense_out.layout(), dense_out.lod(), dense_out.offset());
  argument_outputs.push_back(out_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void GenerateSequenceXpuOp::VerifySig() {
  VLOG(4) << "Start Verifying inputs, outputs and attributes for: GenerateSequenceXpuOp.";
  VLOG(4) << "Verifying inputs:";
  {
  auto input_size = num_operands();
  IR_ENFORCE(input_size == 1u,
                    "The size %d of inputs must be equal to 1.", input_size);
  IR_ENFORCE((*this)->operand_source(0).type().isa<paddle::dialect::DenseTensorType>(),
                  "Type validation failed for the 0th input, got %s.", (*this)->operand_source(0).type());
  }
  VLOG(4) << "Verifying attributes:";
  {
  auto& attributes = this->attributes();
  IR_ENFORCE(attributes.count("dtype")>0,
                 "dtype does not exist.");
  IR_ENFORCE(attributes.at("dtype").isa<paddle::dialect::DataTypeAttribute>(),
                 "Type of attribute: dtype is not paddle::dialect::DataTypeAttribute.");

  }
  VLOG(4) << "Verifying outputs:";
  {
  auto output_size = num_results();
  IR_ENFORCE(output_size == 1u,
                    "The size %d of outputs must be equal to 1.", output_size);
  IR_ENFORCE((*this)->result(0).type().isa<paddle::dialect::DenseTensorType>(),
                 "Type validation failed for the 0th output.");
  }
  VLOG(4) << "End Verifying for: GenerateSequenceXpuOp.";
}

void GenerateSequenceXpuOp::InferMeta( phi::InferMetaContext *infer_meta ) {
  auto fn = PD_INFER_META(phi::GenerateSequenceXPUInferMeta);
  fn(infer_meta);
}

phi::DataType GenerateSequenceXpuOp::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: GenerateSequenceXpuOp";
  


  return expected_kernel_dtype;
}

const char *LayerNormActXpuOp::attributes_name[4] = { "begin_norm_axis", "epsilon", "act_type", "act_param" };

OpInfoTuple LayerNormActXpuOp::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("x", "paddle::dialect::DenseTensorType", false, false, false, false), paddle::dialect::OpInputInfo("scale", "paddle::dialect::DenseTensorType", false, false, false, false), paddle::dialect::OpInputInfo("bias", "paddle::dialect::DenseTensorType", false, false, false, false) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = { paddle::dialect::OpAttributeInfo("begin_norm_axis", "pir::Int32Attribute", ""), paddle::dialect::OpAttributeInfo("epsilon", "pir::FloatAttribute", ""), paddle::dialect::OpAttributeInfo("act_type", "pir::Int32Attribute", ""), paddle::dialect::OpAttributeInfo("act_param", "pir::FloatAttribute", "") };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("out", "paddle::dialect::DenseTensorType", false, false) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("LayerNormActXPUInferMeta", {"x", "scale", "bias", "begin_norm_axis", "epsilon", "act_type", "act_param"}, "layer_norm_act_xpu", {"x", "scale", "bias", "begin_norm_axis", "epsilon", "act_type", "act_param"}, {"x"}, {}, {}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "layer_norm_act_xpu");
}

void LayerNormActXpuOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, pir::Value scale_, pir::Value bias_, int begin_norm_axis, float epsilon, int act_type, float act_param) {
  VLOG(4) << "Start build LayerNormActXpuOp";



  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_, scale_, bias_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_begin_norm_axis = pir::Int32Attribute::get(pir::IrContext::Instance(), begin_norm_axis);
  argument.AddAttribute("begin_norm_axis", attr_begin_norm_axis);
  pir::Attribute attr_epsilon = pir::FloatAttribute::get(pir::IrContext::Instance(), epsilon);
  argument.AddAttribute("epsilon", attr_epsilon);
  pir::Attribute attr_act_type = pir::Int32Attribute::get(pir::IrContext::Instance(), act_type);
  argument.AddAttribute("act_type", attr_act_type);
  pir::Attribute attr_act_param = pir::FloatAttribute::get(pir::IrContext::Instance(), act_param);
  argument.AddAttribute("act_param", attr_act_param);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;
  paddle::dialect::DenseTensorType scale = scale_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)scale;
  paddle::dialect::DenseTensorType bias = bias_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)bias;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);

  VLOG(4) << "Builder construction  dense_scale";
  paddle::dialect::IrTensor ir_tensor_scale(paddle::dialect::TransToPhiDataType(scale.dtype()),
                                                      scale.dims(),
                                                      scale.data_layout(),
                                                      scale.lod(),
                                                      scale.offset());
  VLOG(4) << "Builder construction  meta_scale";
  paddle::dialect::IrMetaTensor meta_scale(&ir_tensor_scale);

  VLOG(4) << "Builder construction  dense_bias";
  paddle::dialect::IrTensor ir_tensor_bias(paddle::dialect::TransToPhiDataType(bias.dtype()),
                                                      bias.dims(),
                                                      bias.data_layout(),
                                                      bias.lod(),
                                                      bias.offset());
  VLOG(4) << "Builder construction  meta_bias";
  paddle::dialect::IrMetaTensor meta_bias(&ir_tensor_bias);
  paddle::dialect::IrTensor dense_out;
  paddle::dialect::IrMetaTensor meta_out(&dense_out);

  phi::LayerNormActXPUInferMeta(meta_x, meta_scale, meta_bias, begin_norm_axis, epsilon, act_type, act_param, &meta_out);

  std::vector<pir::Type> argument_outputs;
  pir::Type out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_out.dtype()), dense_out.dims(), dense_out.layout(), dense_out.lod(), dense_out.offset());
  argument_outputs.push_back(out_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void LayerNormActXpuOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, pir::Value scale_, pir::Value bias_, pir::AttributeMap attributes) {
  VLOG(4) << "Start build LayerNormActXpuOp";


  IR_ENFORCE(
      attributes.find("begin_norm_axis") != attributes.end(),
          "'begin_norm_axis' Attribute is expected for LayerNormActXpuOp. ");
  int begin_norm_axis = attributes.at("begin_norm_axis").dyn_cast<pir::Int32Attribute>().data();

  IR_ENFORCE(
      attributes.find("epsilon") != attributes.end(),
          "'epsilon' Attribute is expected for LayerNormActXpuOp. ");
  float epsilon = attributes.at("epsilon").dyn_cast<pir::FloatAttribute>().data();

  IR_ENFORCE(
      attributes.find("act_type") != attributes.end(),
          "'act_type' Attribute is expected for LayerNormActXpuOp. ");
  int act_type = attributes.at("act_type").dyn_cast<pir::Int32Attribute>().data();

  IR_ENFORCE(
      attributes.find("act_param") != attributes.end(),
          "'act_param' Attribute is expected for LayerNormActXpuOp. ");
  float act_param = attributes.at("act_param").dyn_cast<pir::FloatAttribute>().data();


  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_, scale_, bias_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_begin_norm_axis = pir::Int32Attribute::get(pir::IrContext::Instance(), begin_norm_axis);
  argument.AddAttribute("begin_norm_axis", attr_begin_norm_axis);
  pir::Attribute attr_epsilon = pir::FloatAttribute::get(pir::IrContext::Instance(), epsilon);
  argument.AddAttribute("epsilon", attr_epsilon);
  pir::Attribute attr_act_type = pir::Int32Attribute::get(pir::IrContext::Instance(), act_type);
  argument.AddAttribute("act_type", attr_act_type);
  pir::Attribute attr_act_param = pir::FloatAttribute::get(pir::IrContext::Instance(), act_param);
  argument.AddAttribute("act_param", attr_act_param);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;
  paddle::dialect::DenseTensorType scale = scale_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)scale;
  paddle::dialect::DenseTensorType bias = bias_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)bias;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);

  VLOG(4) << "Builder construction  dense_scale";
  paddle::dialect::IrTensor ir_tensor_scale(paddle::dialect::TransToPhiDataType(scale.dtype()),
                                                      scale.dims(),
                                                      scale.data_layout(),
                                                      scale.lod(),
                                                      scale.offset());
  VLOG(4) << "Builder construction  meta_scale";
  paddle::dialect::IrMetaTensor meta_scale(&ir_tensor_scale);

  VLOG(4) << "Builder construction  dense_bias";
  paddle::dialect::IrTensor ir_tensor_bias(paddle::dialect::TransToPhiDataType(bias.dtype()),
                                                      bias.dims(),
                                                      bias.data_layout(),
                                                      bias.lod(),
                                                      bias.offset());
  VLOG(4) << "Builder construction  meta_bias";
  paddle::dialect::IrMetaTensor meta_bias(&ir_tensor_bias);
  paddle::dialect::IrTensor dense_out;
  paddle::dialect::IrMetaTensor meta_out(&dense_out);

  phi::LayerNormActXPUInferMeta(meta_x, meta_scale, meta_bias, begin_norm_axis, epsilon, act_type, act_param, &meta_out);

  std::vector<pir::Type> argument_outputs;
  pir::Type out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_out.dtype()), dense_out.dims(), dense_out.layout(), dense_out.lod(), dense_out.offset());
  argument_outputs.push_back(out_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void LayerNormActXpuOp::VerifySig() {
  VLOG(4) << "Start Verifying inputs, outputs and attributes for: LayerNormActXpuOp.";
  VLOG(4) << "Verifying inputs:";
  {
  auto input_size = num_operands();
  IR_ENFORCE(input_size == 3u,
                    "The size %d of inputs must be equal to 3.", input_size);
  IR_ENFORCE((*this)->operand_source(0).type().isa<paddle::dialect::DenseTensorType>(),
                  "Type validation failed for the 0th input, got %s.", (*this)->operand_source(0).type());
  IR_ENFORCE((*this)->operand_source(1).type().isa<paddle::dialect::DenseTensorType>(),
                  "Type validation failed for the 1th input, got %s.", (*this)->operand_source(1).type());
  IR_ENFORCE((*this)->operand_source(2).type().isa<paddle::dialect::DenseTensorType>(),
                  "Type validation failed for the 2th input, got %s.", (*this)->operand_source(2).type());
  }
  VLOG(4) << "Verifying attributes:";
  {
  auto& attributes = this->attributes();
  IR_ENFORCE(attributes.count("begin_norm_axis")>0,
                 "begin_norm_axis does not exist.");
  IR_ENFORCE(attributes.at("begin_norm_axis").isa<pir::Int32Attribute>(),
                 "Type of attribute: begin_norm_axis is not pir::Int32Attribute.");

  IR_ENFORCE(attributes.count("epsilon")>0,
                 "epsilon does not exist.");
  IR_ENFORCE(attributes.at("epsilon").isa<pir::FloatAttribute>(),
                 "Type of attribute: epsilon is not pir::FloatAttribute.");

  IR_ENFORCE(attributes.count("act_type")>0,
                 "act_type does not exist.");
  IR_ENFORCE(attributes.at("act_type").isa<pir::Int32Attribute>(),
                 "Type of attribute: act_type is not pir::Int32Attribute.");

  IR_ENFORCE(attributes.count("act_param")>0,
                 "act_param does not exist.");
  IR_ENFORCE(attributes.at("act_param").isa<pir::FloatAttribute>(),
                 "Type of attribute: act_param is not pir::FloatAttribute.");

  }
  VLOG(4) << "Verifying outputs:";
  {
  auto output_size = num_results();
  IR_ENFORCE(output_size == 1u,
                    "The size %d of outputs must be equal to 1.", output_size);
  IR_ENFORCE((*this)->result(0).type().isa<paddle::dialect::DenseTensorType>(),
                 "Type validation failed for the 0th output.");
  }
  VLOG(4) << "End Verifying for: LayerNormActXpuOp.";
}

void LayerNormActXpuOp::InferMeta( phi::InferMetaContext *infer_meta ) {
  auto fn = PD_INFER_META(phi::LayerNormActXPUInferMeta);
  fn(infer_meta);
}

phi::DataType LayerNormActXpuOp::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: LayerNormActXpuOp";
  


  return expected_kernel_dtype;
}

const char *MaxPool2dV2Op::attributes_name[6] = { "kernel_size", "strides", "paddings", "data_format", "global_pooling", "adaptive" };

OpInfoTuple MaxPool2dV2Op::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("x", "paddle::dialect::DenseTensorType", false, false, false, true) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = { paddle::dialect::OpAttributeInfo("kernel_size", "pir::ArrayAttribute<pir::Int32Attribute>", ""), paddle::dialect::OpAttributeInfo("strides", "pir::ArrayAttribute<pir::Int32Attribute>", ""), paddle::dialect::OpAttributeInfo("paddings", "pir::ArrayAttribute<pir::Int32Attribute>", ""), paddle::dialect::OpAttributeInfo("data_format", "pir::StrAttribute", ""), paddle::dialect::OpAttributeInfo("global_pooling", "pir::BoolAttribute", ""), paddle::dialect::OpAttributeInfo("adaptive", "pir::BoolAttribute", "") };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("out", "paddle::dialect::DenseTensorType", false, false), paddle::dialect::OpOutputInfo("saved_idx", "paddle::dialect::DenseTensorType", true, true) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("MaxPoolV2InferMeta", {"x", "kernel_size", "strides", "paddings", "data_format", "global_pooling", "adaptive"}, "max_pool2d_v2", {"x", "kernel_size", "strides", "paddings", "data_format", "global_pooling", "adaptive"}, {}, {}, {}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "max_pool2d_v2");
}

void MaxPool2dV2Op::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, const std::vector<int>& kernel_size, const std::vector<int>& strides, const std::vector<int>& paddings, const std::string& data_format, bool global_pooling, bool adaptive) {
  VLOG(4) << "Start build MaxPool2dV2Op";



  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  std::vector<pir::Attribute> vec_kernel_size;
  for (size_t i = 0; i < static_cast<size_t>(kernel_size.size()); i++) {
      pir::Attribute attr_kernel_size = pir::Int32Attribute::get(pir::IrContext::Instance(), kernel_size[i]);

    vec_kernel_size.push_back(attr_kernel_size);
  }
  pir::Attribute attr_kernel_size = pir::ArrayAttribute::get(pir::IrContext::Instance(), vec_kernel_size);
  argument.AddAttribute("kernel_size", attr_kernel_size);
  std::vector<pir::Attribute> vec_strides;
  for (size_t i = 0; i < static_cast<size_t>(strides.size()); i++) {
      pir::Attribute attr_strides = pir::Int32Attribute::get(pir::IrContext::Instance(), strides[i]);

    vec_strides.push_back(attr_strides);
  }
  pir::Attribute attr_strides = pir::ArrayAttribute::get(pir::IrContext::Instance(), vec_strides);
  argument.AddAttribute("strides", attr_strides);
  std::vector<pir::Attribute> vec_paddings;
  for (size_t i = 0; i < static_cast<size_t>(paddings.size()); i++) {
      pir::Attribute attr_paddings = pir::Int32Attribute::get(pir::IrContext::Instance(), paddings[i]);

    vec_paddings.push_back(attr_paddings);
  }
  pir::Attribute attr_paddings = pir::ArrayAttribute::get(pir::IrContext::Instance(), vec_paddings);
  argument.AddAttribute("paddings", attr_paddings);
  pir::Attribute attr_data_format = pir::StrAttribute::get(pir::IrContext::Instance(), data_format);
  argument.AddAttribute("data_format", attr_data_format);
  pir::Attribute attr_global_pooling = pir::BoolAttribute::get(pir::IrContext::Instance(), global_pooling);
  argument.AddAttribute("global_pooling", attr_global_pooling);
  pir::Attribute attr_adaptive = pir::BoolAttribute::get(pir::IrContext::Instance(), adaptive);
  argument.AddAttribute("adaptive", attr_adaptive);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);
  paddle::dialect::IrTensor dense_out;
  paddle::dialect::IrMetaTensor meta_out(&dense_out);
  paddle::dialect::IrTensor dense_saved_idx;
  paddle::dialect::IrMetaTensor meta_saved_idx(&dense_saved_idx);

  phi::MaxPoolV2InferMeta(meta_x, kernel_size, strides, paddings, data_format, global_pooling, adaptive, &meta_out, &meta_saved_idx);

  std::vector<pir::Type> argument_outputs;
  pir::Type out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_out.dtype()), dense_out.dims(), dense_out.layout(), dense_out.lod(), dense_out.offset());
  argument_outputs.push_back(out_dense_tensor_type);

  pir::Type saved_idx_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_saved_idx.dtype()), dense_saved_idx.dims(), dense_saved_idx.layout(), dense_saved_idx.lod(), dense_saved_idx.offset());
  argument_outputs.push_back(saved_idx_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void MaxPool2dV2Op::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, pir::AttributeMap attributes) {
  VLOG(4) << "Start build MaxPool2dV2Op";


  IR_ENFORCE(
      attributes.find("kernel_size") != attributes.end(),
          "'kernel_size' Attribute is expected for MaxPool2dV2Op. ");
  std::vector<int> kernel_size;
  for (size_t i = 0; i < attributes.at("kernel_size").dyn_cast<pir::ArrayAttribute>().size(); i++) {
    kernel_size.push_back(attributes.at("kernel_size").dyn_cast<pir::ArrayAttribute>().at(i).dyn_cast<pir::Int32Attribute>().data());
  }

  IR_ENFORCE(
      attributes.find("strides") != attributes.end(),
          "'strides' Attribute is expected for MaxPool2dV2Op. ");
  std::vector<int> strides;
  for (size_t i = 0; i < attributes.at("strides").dyn_cast<pir::ArrayAttribute>().size(); i++) {
    strides.push_back(attributes.at("strides").dyn_cast<pir::ArrayAttribute>().at(i).dyn_cast<pir::Int32Attribute>().data());
  }

  IR_ENFORCE(
      attributes.find("paddings") != attributes.end(),
          "'paddings' Attribute is expected for MaxPool2dV2Op. ");
  std::vector<int> paddings;
  for (size_t i = 0; i < attributes.at("paddings").dyn_cast<pir::ArrayAttribute>().size(); i++) {
    paddings.push_back(attributes.at("paddings").dyn_cast<pir::ArrayAttribute>().at(i).dyn_cast<pir::Int32Attribute>().data());
  }

  IR_ENFORCE(
      attributes.find("data_format") != attributes.end(),
          "'data_format' Attribute is expected for MaxPool2dV2Op. ");
  std::string data_format = attributes.at("data_format").dyn_cast<pir::StrAttribute>().AsString();

  IR_ENFORCE(
      attributes.find("global_pooling") != attributes.end(),
          "'global_pooling' Attribute is expected for MaxPool2dV2Op. ");
  bool global_pooling = attributes.at("global_pooling").dyn_cast<pir::BoolAttribute>().data();

  IR_ENFORCE(
      attributes.find("adaptive") != attributes.end(),
          "'adaptive' Attribute is expected for MaxPool2dV2Op. ");
  bool adaptive = attributes.at("adaptive").dyn_cast<pir::BoolAttribute>().data();


  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  std::vector<pir::Attribute> vec_kernel_size;
  for (size_t i = 0; i < static_cast<size_t>(kernel_size.size()); i++) {
      pir::Attribute attr_kernel_size = pir::Int32Attribute::get(pir::IrContext::Instance(), kernel_size[i]);

    vec_kernel_size.push_back(attr_kernel_size);
  }
  pir::Attribute attr_kernel_size = pir::ArrayAttribute::get(pir::IrContext::Instance(), vec_kernel_size);
  argument.AddAttribute("kernel_size", attr_kernel_size);
  std::vector<pir::Attribute> vec_strides;
  for (size_t i = 0; i < static_cast<size_t>(strides.size()); i++) {
      pir::Attribute attr_strides = pir::Int32Attribute::get(pir::IrContext::Instance(), strides[i]);

    vec_strides.push_back(attr_strides);
  }
  pir::Attribute attr_strides = pir::ArrayAttribute::get(pir::IrContext::Instance(), vec_strides);
  argument.AddAttribute("strides", attr_strides);
  std::vector<pir::Attribute> vec_paddings;
  for (size_t i = 0; i < static_cast<size_t>(paddings.size()); i++) {
      pir::Attribute attr_paddings = pir::Int32Attribute::get(pir::IrContext::Instance(), paddings[i]);

    vec_paddings.push_back(attr_paddings);
  }
  pir::Attribute attr_paddings = pir::ArrayAttribute::get(pir::IrContext::Instance(), vec_paddings);
  argument.AddAttribute("paddings", attr_paddings);
  pir::Attribute attr_data_format = pir::StrAttribute::get(pir::IrContext::Instance(), data_format);
  argument.AddAttribute("data_format", attr_data_format);
  pir::Attribute attr_global_pooling = pir::BoolAttribute::get(pir::IrContext::Instance(), global_pooling);
  argument.AddAttribute("global_pooling", attr_global_pooling);
  pir::Attribute attr_adaptive = pir::BoolAttribute::get(pir::IrContext::Instance(), adaptive);
  argument.AddAttribute("adaptive", attr_adaptive);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);
  paddle::dialect::IrTensor dense_out;
  paddle::dialect::IrMetaTensor meta_out(&dense_out);
  paddle::dialect::IrTensor dense_saved_idx;
  paddle::dialect::IrMetaTensor meta_saved_idx(&dense_saved_idx);

  phi::MaxPoolV2InferMeta(meta_x, kernel_size, strides, paddings, data_format, global_pooling, adaptive, &meta_out, &meta_saved_idx);

  std::vector<pir::Type> argument_outputs;
  pir::Type out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_out.dtype()), dense_out.dims(), dense_out.layout(), dense_out.lod(), dense_out.offset());
  argument_outputs.push_back(out_dense_tensor_type);

  pir::Type saved_idx_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_saved_idx.dtype()), dense_saved_idx.dims(), dense_saved_idx.layout(), dense_saved_idx.lod(), dense_saved_idx.offset());
  argument_outputs.push_back(saved_idx_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void MaxPool2dV2Op::VerifySig() {
  VLOG(4) << "Start Verifying inputs, outputs and attributes for: MaxPool2dV2Op.";
  VLOG(4) << "Verifying inputs:";
  {
  auto input_size = num_operands();
  IR_ENFORCE(input_size == 1u,
                    "The size %d of inputs must be equal to 1.", input_size);
  IR_ENFORCE((*this)->operand_source(0).type().isa<paddle::dialect::DenseTensorType>(),
                  "Type validation failed for the 0th input, got %s.", (*this)->operand_source(0).type());
  }
  VLOG(4) << "Verifying attributes:";
  {
  auto& attributes = this->attributes();
  IR_ENFORCE(attributes.count("kernel_size")>0,
                 "kernel_size does not exist.");
  IR_ENFORCE(attributes.at("kernel_size").isa<pir::ArrayAttribute>(),
                 "Type of attribute: kernel_size is not pir::ArrayAttribute.");
  for (size_t i = 0; i < attributes.at("kernel_size").dyn_cast<pir::ArrayAttribute>().size(); i++) {
    IR_ENFORCE(attributes.at("kernel_size").dyn_cast<pir::ArrayAttribute>().at(i).isa<pir::Int32Attribute>(),
                   "Type of attribute: kernel_size is not right.");
  }
  IR_ENFORCE(attributes.count("strides")>0,
                 "strides does not exist.");
  IR_ENFORCE(attributes.at("strides").isa<pir::ArrayAttribute>(),
                 "Type of attribute: strides is not pir::ArrayAttribute.");
  for (size_t i = 0; i < attributes.at("strides").dyn_cast<pir::ArrayAttribute>().size(); i++) {
    IR_ENFORCE(attributes.at("strides").dyn_cast<pir::ArrayAttribute>().at(i).isa<pir::Int32Attribute>(),
                   "Type of attribute: strides is not right.");
  }
  IR_ENFORCE(attributes.count("paddings")>0,
                 "paddings does not exist.");
  IR_ENFORCE(attributes.at("paddings").isa<pir::ArrayAttribute>(),
                 "Type of attribute: paddings is not pir::ArrayAttribute.");
  for (size_t i = 0; i < attributes.at("paddings").dyn_cast<pir::ArrayAttribute>().size(); i++) {
    IR_ENFORCE(attributes.at("paddings").dyn_cast<pir::ArrayAttribute>().at(i).isa<pir::Int32Attribute>(),
                   "Type of attribute: paddings is not right.");
  }
  IR_ENFORCE(attributes.count("data_format")>0,
                 "data_format does not exist.");
  IR_ENFORCE(attributes.at("data_format").isa<pir::StrAttribute>(),
                 "Type of attribute: data_format is not pir::StrAttribute.");

  IR_ENFORCE(attributes.count("global_pooling")>0,
                 "global_pooling does not exist.");
  IR_ENFORCE(attributes.at("global_pooling").isa<pir::BoolAttribute>(),
                 "Type of attribute: global_pooling is not pir::BoolAttribute.");

  IR_ENFORCE(attributes.count("adaptive")>0,
                 "adaptive does not exist.");
  IR_ENFORCE(attributes.at("adaptive").isa<pir::BoolAttribute>(),
                 "Type of attribute: adaptive is not pir::BoolAttribute.");

  }
  VLOG(4) << "Verifying outputs:";
  {
  auto output_size = num_results();
  IR_ENFORCE(output_size == 2u,
                    "The size %d of outputs must be equal to 2.", output_size);
  IR_ENFORCE((*this)->result(0).type().isa<paddle::dialect::DenseTensorType>(),
                 "Type validation failed for the 0th output.");
  if (auto output_1_type = (*this)->result(1).type()) {
    IR_ENFORCE(output_1_type.isa<paddle::dialect::DenseTensorType>(),
                   "Type validation failed for the 1th output.");
  }
  }
  VLOG(4) << "End Verifying for: MaxPool2dV2Op.";
}

void MaxPool2dV2Op::InferMeta( phi::InferMetaContext *infer_meta ) {
  auto fn = PD_INFER_META(phi::MaxPoolV2InferMeta);
  fn(infer_meta);
}

phi::DataType MaxPool2dV2Op::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: MaxPool2dV2Op";
  


  return expected_kernel_dtype;
}

const char *MultiEncoderXpuOp::attributes_name[9] = { "layer_num", "norm_before", "hidden_dim", "head_num", "size_per_head", "ffn_hidden_dim_scale", "act_type", "relative_type", "slice_idx" };

OpInfoTuple MultiEncoderXpuOp::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("x", "paddle::dialect::DenseTensorType", false, false, false, false), paddle::dialect::OpInputInfo("fc_weight", "pir::VectorType<paddle::dialect::DenseTensorType>", false, false, false, false), paddle::dialect::OpInputInfo("fc_weight_max", "pir::VectorType<paddle::dialect::DenseTensorType>", false, false, false, false), paddle::dialect::OpInputInfo("fc_bias", "pir::VectorType<paddle::dialect::DenseTensorType>", false, false, false, false), paddle::dialect::OpInputInfo("ln_scale", "pir::VectorType<paddle::dialect::DenseTensorType>", false, false, false, false), paddle::dialect::OpInputInfo("ln_bias", "pir::VectorType<paddle::dialect::DenseTensorType>", false, false, false, false), paddle::dialect::OpInputInfo("mask", "paddle::dialect::DenseTensorType", true, false, false, false), paddle::dialect::OpInputInfo("seq_lod", "paddle::dialect::DenseTensorType", true, false, false, false), paddle::dialect::OpInputInfo("max_seq_len", "paddle::dialect::DenseTensorType", true, false, false, false) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = { paddle::dialect::OpAttributeInfo("layer_num", "pir::Int32Attribute", ""), paddle::dialect::OpAttributeInfo("norm_before", "pir::BoolAttribute", ""), paddle::dialect::OpAttributeInfo("hidden_dim", "pir::Int32Attribute", ""), paddle::dialect::OpAttributeInfo("head_num", "pir::Int32Attribute", ""), paddle::dialect::OpAttributeInfo("size_per_head", "pir::Int32Attribute", ""), paddle::dialect::OpAttributeInfo("ffn_hidden_dim_scale", "pir::Int32Attribute", ""), paddle::dialect::OpAttributeInfo("act_type", "pir::Int32Attribute", ""), paddle::dialect::OpAttributeInfo("relative_type", "pir::Int32Attribute", ""), paddle::dialect::OpAttributeInfo("slice_idx", "pir::Int32Attribute", "") };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("out", "paddle::dialect::DenseTensorType", false, false), paddle::dialect::OpOutputInfo("x_fp16", "paddle::dialect::DenseTensorType", true, false), paddle::dialect::OpOutputInfo("out_fp16", "paddle::dialect::DenseTensorType", true, false) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("MultiEncoderXPUInferMeta", {"x", "fc_weight", "fc_weight_max", "fc_bias", "ln_scale", "ln_bias", "mask", "seq_lod", "max_seq_len", "layer_num", "norm_before", "hidden_dim", "head_num", "size_per_head", "ffn_hidden_dim_scale", "act_type", "relative_type", "slice_idx"}, "multi_encoder_xpu", {"x", "fc_weight", "fc_weight_max", "fc_bias", "ln_scale", "ln_bias", "mask", "seq_lod", "max_seq_len", "layer_num", "norm_before", "hidden_dim", "head_num", "size_per_head", "ffn_hidden_dim_scale", "act_type", "relative_type", "slice_idx"}, {"x"}, {}, {}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "multi_encoder_xpu");
}

void MultiEncoderXpuOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, pir::Value fc_weight_, pir::Value fc_weight_max_, pir::Value fc_bias_, pir::Value ln_scale_, pir::Value ln_bias_, pir::Value mask_, pir::Value seq_lod_, pir::Value max_seq_len_, int layer_num, bool norm_before, int hidden_dim, int head_num, int size_per_head, int ffn_hidden_dim_scale, int act_type, int relative_type, int slice_idx) {
  VLOG(4) << "Start build MultiEncoderXpuOp";



  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_, fc_weight_, fc_weight_max_, fc_bias_, ln_scale_, ln_bias_, mask_, seq_lod_, max_seq_len_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_layer_num = pir::Int32Attribute::get(pir::IrContext::Instance(), layer_num);
  argument.AddAttribute("layer_num", attr_layer_num);
  pir::Attribute attr_norm_before = pir::BoolAttribute::get(pir::IrContext::Instance(), norm_before);
  argument.AddAttribute("norm_before", attr_norm_before);
  pir::Attribute attr_hidden_dim = pir::Int32Attribute::get(pir::IrContext::Instance(), hidden_dim);
  argument.AddAttribute("hidden_dim", attr_hidden_dim);
  pir::Attribute attr_head_num = pir::Int32Attribute::get(pir::IrContext::Instance(), head_num);
  argument.AddAttribute("head_num", attr_head_num);
  pir::Attribute attr_size_per_head = pir::Int32Attribute::get(pir::IrContext::Instance(), size_per_head);
  argument.AddAttribute("size_per_head", attr_size_per_head);
  pir::Attribute attr_ffn_hidden_dim_scale = pir::Int32Attribute::get(pir::IrContext::Instance(), ffn_hidden_dim_scale);
  argument.AddAttribute("ffn_hidden_dim_scale", attr_ffn_hidden_dim_scale);
  pir::Attribute attr_act_type = pir::Int32Attribute::get(pir::IrContext::Instance(), act_type);
  argument.AddAttribute("act_type", attr_act_type);
  pir::Attribute attr_relative_type = pir::Int32Attribute::get(pir::IrContext::Instance(), relative_type);
  argument.AddAttribute("relative_type", attr_relative_type);
  pir::Attribute attr_slice_idx = pir::Int32Attribute::get(pir::IrContext::Instance(), slice_idx);
  argument.AddAttribute("slice_idx", attr_slice_idx);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;
  pir::VectorType fc_weight = fc_weight_.type().dyn_cast<pir::VectorType>(); (void)fc_weight;
  pir::VectorType fc_weight_max = fc_weight_max_.type().dyn_cast<pir::VectorType>(); (void)fc_weight_max;
  pir::VectorType fc_bias = fc_bias_.type().dyn_cast<pir::VectorType>(); (void)fc_bias;
  pir::VectorType ln_scale = ln_scale_.type().dyn_cast<pir::VectorType>(); (void)ln_scale;
  pir::VectorType ln_bias = ln_bias_.type().dyn_cast<pir::VectorType>(); (void)ln_bias;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);
  std::vector<paddle::dialect::IrTensor> vec_ir_tensor_fc_weight;
  for (size_t i=0; i < static_cast<size_t>(fc_weight.size()); i++) {
    vec_ir_tensor_fc_weight.push_back(paddle::dialect::IrTensor(paddle::dialect::TransToPhiDataType(fc_weight[i].dyn_cast<paddle::dialect::DenseTensorType>().dtype()),
                                                                     fc_weight[i].dyn_cast<paddle::dialect::DenseTensorType>().dims(),
                                                                     fc_weight[i].dyn_cast<paddle::dialect::DenseTensorType>().data_layout(),
                                                                     fc_weight[i].dyn_cast<paddle::dialect::DenseTensorType>().lod(),
                                                                     fc_weight[i].dyn_cast<paddle::dialect::DenseTensorType>().offset()));
  }
  std::vector<paddle::dialect::IrMetaTensor> vec_meta_fc_weight;
  for (size_t i=0; i < vec_ir_tensor_fc_weight.size(); i++) {
    vec_meta_fc_weight.push_back(paddle::dialect::IrMetaTensor(&vec_ir_tensor_fc_weight[i]));
  }

  std::vector<const phi::MetaTensor*> meta_fc_weight;
  for (size_t i=0; i < static_cast<size_t>(vec_meta_fc_weight.size()); i++) {
    meta_fc_weight.push_back(&vec_meta_fc_weight[i]);
  }
   std::vector<paddle::dialect::IrTensor> vec_ir_tensor_fc_weight_max;
  for (size_t i=0; i < static_cast<size_t>(fc_weight_max.size()); i++) {
    vec_ir_tensor_fc_weight_max.push_back(paddle::dialect::IrTensor(paddle::dialect::TransToPhiDataType(fc_weight_max[i].dyn_cast<paddle::dialect::DenseTensorType>().dtype()),
                                                                     fc_weight_max[i].dyn_cast<paddle::dialect::DenseTensorType>().dims(),
                                                                     fc_weight_max[i].dyn_cast<paddle::dialect::DenseTensorType>().data_layout(),
                                                                     fc_weight_max[i].dyn_cast<paddle::dialect::DenseTensorType>().lod(),
                                                                     fc_weight_max[i].dyn_cast<paddle::dialect::DenseTensorType>().offset()));
  }
  std::vector<paddle::dialect::IrMetaTensor> vec_meta_fc_weight_max;
  for (size_t i=0; i < vec_ir_tensor_fc_weight_max.size(); i++) {
    vec_meta_fc_weight_max.push_back(paddle::dialect::IrMetaTensor(&vec_ir_tensor_fc_weight_max[i]));
  }

  std::vector<const phi::MetaTensor*> meta_fc_weight_max;
  for (size_t i=0; i < static_cast<size_t>(vec_meta_fc_weight_max.size()); i++) {
    meta_fc_weight_max.push_back(&vec_meta_fc_weight_max[i]);
  }
   std::vector<paddle::dialect::IrTensor> vec_ir_tensor_fc_bias;
  for (size_t i=0; i < static_cast<size_t>(fc_bias.size()); i++) {
    vec_ir_tensor_fc_bias.push_back(paddle::dialect::IrTensor(paddle::dialect::TransToPhiDataType(fc_bias[i].dyn_cast<paddle::dialect::DenseTensorType>().dtype()),
                                                                     fc_bias[i].dyn_cast<paddle::dialect::DenseTensorType>().dims(),
                                                                     fc_bias[i].dyn_cast<paddle::dialect::DenseTensorType>().data_layout(),
                                                                     fc_bias[i].dyn_cast<paddle::dialect::DenseTensorType>().lod(),
                                                                     fc_bias[i].dyn_cast<paddle::dialect::DenseTensorType>().offset()));
  }
  std::vector<paddle::dialect::IrMetaTensor> vec_meta_fc_bias;
  for (size_t i=0; i < vec_ir_tensor_fc_bias.size(); i++) {
    vec_meta_fc_bias.push_back(paddle::dialect::IrMetaTensor(&vec_ir_tensor_fc_bias[i]));
  }

  std::vector<const phi::MetaTensor*> meta_fc_bias;
  for (size_t i=0; i < static_cast<size_t>(vec_meta_fc_bias.size()); i++) {
    meta_fc_bias.push_back(&vec_meta_fc_bias[i]);
  }
   std::vector<paddle::dialect::IrTensor> vec_ir_tensor_ln_scale;
  for (size_t i=0; i < static_cast<size_t>(ln_scale.size()); i++) {
    vec_ir_tensor_ln_scale.push_back(paddle::dialect::IrTensor(paddle::dialect::TransToPhiDataType(ln_scale[i].dyn_cast<paddle::dialect::DenseTensorType>().dtype()),
                                                                     ln_scale[i].dyn_cast<paddle::dialect::DenseTensorType>().dims(),
                                                                     ln_scale[i].dyn_cast<paddle::dialect::DenseTensorType>().data_layout(),
                                                                     ln_scale[i].dyn_cast<paddle::dialect::DenseTensorType>().lod(),
                                                                     ln_scale[i].dyn_cast<paddle::dialect::DenseTensorType>().offset()));
  }
  std::vector<paddle::dialect::IrMetaTensor> vec_meta_ln_scale;
  for (size_t i=0; i < vec_ir_tensor_ln_scale.size(); i++) {
    vec_meta_ln_scale.push_back(paddle::dialect::IrMetaTensor(&vec_ir_tensor_ln_scale[i]));
  }

  std::vector<const phi::MetaTensor*> meta_ln_scale;
  for (size_t i=0; i < static_cast<size_t>(vec_meta_ln_scale.size()); i++) {
    meta_ln_scale.push_back(&vec_meta_ln_scale[i]);
  }
   std::vector<paddle::dialect::IrTensor> vec_ir_tensor_ln_bias;
  for (size_t i=0; i < static_cast<size_t>(ln_bias.size()); i++) {
    vec_ir_tensor_ln_bias.push_back(paddle::dialect::IrTensor(paddle::dialect::TransToPhiDataType(ln_bias[i].dyn_cast<paddle::dialect::DenseTensorType>().dtype()),
                                                                     ln_bias[i].dyn_cast<paddle::dialect::DenseTensorType>().dims(),
                                                                     ln_bias[i].dyn_cast<paddle::dialect::DenseTensorType>().data_layout(),
                                                                     ln_bias[i].dyn_cast<paddle::dialect::DenseTensorType>().lod(),
                                                                     ln_bias[i].dyn_cast<paddle::dialect::DenseTensorType>().offset()));
  }
  std::vector<paddle::dialect::IrMetaTensor> vec_meta_ln_bias;
  for (size_t i=0; i < vec_ir_tensor_ln_bias.size(); i++) {
    vec_meta_ln_bias.push_back(paddle::dialect::IrMetaTensor(&vec_ir_tensor_ln_bias[i]));
  }

  std::vector<const phi::MetaTensor*> meta_ln_bias;
  for (size_t i=0; i < static_cast<size_t>(vec_meta_ln_bias.size()); i++) {
    meta_ln_bias.push_back(&vec_meta_ln_bias[i]);
  }
 
  paddle::dialect::IrMetaTensor meta_mask;
  paddle::dialect::IrTensor ir_tensor_mask;
  if (mask_.impl() != nullptr) {
    paddle::dialect::DenseTensorType mask = mask_.type().dyn_cast<paddle::dialect::DenseTensorType>();
    VLOG(4) << "Builder construction  dense_mask";
    ir_tensor_mask = paddle::dialect::IrTensor(paddle::dialect::TransToPhiDataType(mask.dtype()),
                                                        mask.dims(),
                                                        mask.data_layout(),
                                                        mask.lod(),
                                                        mask.offset());
    VLOG(4) << "Builder construction  meta_mask";
    meta_mask = paddle::dialect::IrMetaTensor(&ir_tensor_mask);
  }


  paddle::dialect::IrMetaTensor meta_seq_lod;
  paddle::dialect::IrTensor ir_tensor_seq_lod;
  if (seq_lod_.impl() != nullptr) {
    paddle::dialect::DenseTensorType seq_lod = seq_lod_.type().dyn_cast<paddle::dialect::DenseTensorType>();
    VLOG(4) << "Builder construction  dense_seq_lod";
    ir_tensor_seq_lod = paddle::dialect::IrTensor(paddle::dialect::TransToPhiDataType(seq_lod.dtype()),
                                                        seq_lod.dims(),
                                                        seq_lod.data_layout(),
                                                        seq_lod.lod(),
                                                        seq_lod.offset());
    VLOG(4) << "Builder construction  meta_seq_lod";
    meta_seq_lod = paddle::dialect::IrMetaTensor(&ir_tensor_seq_lod);
  }


  paddle::dialect::IrMetaTensor meta_max_seq_len;
  paddle::dialect::IrTensor ir_tensor_max_seq_len;
  if (max_seq_len_.impl() != nullptr) {
    paddle::dialect::DenseTensorType max_seq_len = max_seq_len_.type().dyn_cast<paddle::dialect::DenseTensorType>();
    VLOG(4) << "Builder construction  dense_max_seq_len";
    ir_tensor_max_seq_len = paddle::dialect::IrTensor(paddle::dialect::TransToPhiDataType(max_seq_len.dtype()),
                                                        max_seq_len.dims(),
                                                        max_seq_len.data_layout(),
                                                        max_seq_len.lod(),
                                                        max_seq_len.offset());
    VLOG(4) << "Builder construction  meta_max_seq_len";
    meta_max_seq_len = paddle::dialect::IrMetaTensor(&ir_tensor_max_seq_len);
  }

  paddle::dialect::IrTensor dense_out;
  paddle::dialect::IrMetaTensor meta_out(&dense_out);
  paddle::dialect::IrTensor dense_x_fp16;
  paddle::dialect::IrMetaTensor meta_x_fp16(&dense_x_fp16);
  paddle::dialect::IrTensor dense_out_fp16;
  paddle::dialect::IrMetaTensor meta_out_fp16(&dense_out_fp16);

  phi::MultiEncoderXPUInferMeta(meta_x, meta_fc_weight, meta_fc_weight_max, meta_fc_bias, meta_ln_scale, meta_ln_bias, meta_mask, meta_seq_lod, meta_max_seq_len, layer_num, norm_before, hidden_dim, head_num, size_per_head, ffn_hidden_dim_scale, act_type, relative_type, slice_idx, &meta_out, &meta_x_fp16, &meta_out_fp16);

  std::vector<pir::Type> argument_outputs;
  pir::Type out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_out.dtype()), dense_out.dims(), dense_out.layout(), dense_out.lod(), dense_out.offset());
  argument_outputs.push_back(out_dense_tensor_type);

  pir::Type x_fp16_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_x_fp16.dtype()), dense_x_fp16.dims(), dense_x_fp16.layout(), dense_x_fp16.lod(), dense_x_fp16.offset());
  argument_outputs.push_back(x_fp16_dense_tensor_type);

  pir::Type out_fp16_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_out_fp16.dtype()), dense_out_fp16.dims(), dense_out_fp16.layout(), dense_out_fp16.lod(), dense_out_fp16.offset());
  argument_outputs.push_back(out_fp16_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void MultiEncoderXpuOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, pir::Value fc_weight_, pir::Value fc_weight_max_, pir::Value fc_bias_, pir::Value ln_scale_, pir::Value ln_bias_, pir::Value mask_, pir::Value seq_lod_, pir::Value max_seq_len_, pir::AttributeMap attributes) {
  VLOG(4) << "Start build MultiEncoderXpuOp";


  IR_ENFORCE(
      attributes.find("layer_num") != attributes.end(),
          "'layer_num' Attribute is expected for MultiEncoderXpuOp. ");
  int layer_num = attributes.at("layer_num").dyn_cast<pir::Int32Attribute>().data();

  IR_ENFORCE(
      attributes.find("norm_before") != attributes.end(),
          "'norm_before' Attribute is expected for MultiEncoderXpuOp. ");
  bool norm_before = attributes.at("norm_before").dyn_cast<pir::BoolAttribute>().data();

  IR_ENFORCE(
      attributes.find("hidden_dim") != attributes.end(),
          "'hidden_dim' Attribute is expected for MultiEncoderXpuOp. ");
  int hidden_dim = attributes.at("hidden_dim").dyn_cast<pir::Int32Attribute>().data();

  IR_ENFORCE(
      attributes.find("head_num") != attributes.end(),
          "'head_num' Attribute is expected for MultiEncoderXpuOp. ");
  int head_num = attributes.at("head_num").dyn_cast<pir::Int32Attribute>().data();

  IR_ENFORCE(
      attributes.find("size_per_head") != attributes.end(),
          "'size_per_head' Attribute is expected for MultiEncoderXpuOp. ");
  int size_per_head = attributes.at("size_per_head").dyn_cast<pir::Int32Attribute>().data();

  IR_ENFORCE(
      attributes.find("ffn_hidden_dim_scale") != attributes.end(),
          "'ffn_hidden_dim_scale' Attribute is expected for MultiEncoderXpuOp. ");
  int ffn_hidden_dim_scale = attributes.at("ffn_hidden_dim_scale").dyn_cast<pir::Int32Attribute>().data();

  IR_ENFORCE(
      attributes.find("act_type") != attributes.end(),
          "'act_type' Attribute is expected for MultiEncoderXpuOp. ");
  int act_type = attributes.at("act_type").dyn_cast<pir::Int32Attribute>().data();

  IR_ENFORCE(
      attributes.find("relative_type") != attributes.end(),
          "'relative_type' Attribute is expected for MultiEncoderXpuOp. ");
  int relative_type = attributes.at("relative_type").dyn_cast<pir::Int32Attribute>().data();

  IR_ENFORCE(
      attributes.find("slice_idx") != attributes.end(),
          "'slice_idx' Attribute is expected for MultiEncoderXpuOp. ");
  int slice_idx = attributes.at("slice_idx").dyn_cast<pir::Int32Attribute>().data();


  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_, fc_weight_, fc_weight_max_, fc_bias_, ln_scale_, ln_bias_, mask_, seq_lod_, max_seq_len_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_layer_num = pir::Int32Attribute::get(pir::IrContext::Instance(), layer_num);
  argument.AddAttribute("layer_num", attr_layer_num);
  pir::Attribute attr_norm_before = pir::BoolAttribute::get(pir::IrContext::Instance(), norm_before);
  argument.AddAttribute("norm_before", attr_norm_before);
  pir::Attribute attr_hidden_dim = pir::Int32Attribute::get(pir::IrContext::Instance(), hidden_dim);
  argument.AddAttribute("hidden_dim", attr_hidden_dim);
  pir::Attribute attr_head_num = pir::Int32Attribute::get(pir::IrContext::Instance(), head_num);
  argument.AddAttribute("head_num", attr_head_num);
  pir::Attribute attr_size_per_head = pir::Int32Attribute::get(pir::IrContext::Instance(), size_per_head);
  argument.AddAttribute("size_per_head", attr_size_per_head);
  pir::Attribute attr_ffn_hidden_dim_scale = pir::Int32Attribute::get(pir::IrContext::Instance(), ffn_hidden_dim_scale);
  argument.AddAttribute("ffn_hidden_dim_scale", attr_ffn_hidden_dim_scale);
  pir::Attribute attr_act_type = pir::Int32Attribute::get(pir::IrContext::Instance(), act_type);
  argument.AddAttribute("act_type", attr_act_type);
  pir::Attribute attr_relative_type = pir::Int32Attribute::get(pir::IrContext::Instance(), relative_type);
  argument.AddAttribute("relative_type", attr_relative_type);
  pir::Attribute attr_slice_idx = pir::Int32Attribute::get(pir::IrContext::Instance(), slice_idx);
  argument.AddAttribute("slice_idx", attr_slice_idx);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;
  pir::VectorType fc_weight = fc_weight_.type().dyn_cast<pir::VectorType>(); (void)fc_weight;
  pir::VectorType fc_weight_max = fc_weight_max_.type().dyn_cast<pir::VectorType>(); (void)fc_weight_max;
  pir::VectorType fc_bias = fc_bias_.type().dyn_cast<pir::VectorType>(); (void)fc_bias;
  pir::VectorType ln_scale = ln_scale_.type().dyn_cast<pir::VectorType>(); (void)ln_scale;
  pir::VectorType ln_bias = ln_bias_.type().dyn_cast<pir::VectorType>(); (void)ln_bias;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);
  std::vector<paddle::dialect::IrTensor> vec_ir_tensor_fc_weight;
  for (size_t i=0; i < static_cast<size_t>(fc_weight.size()); i++) {
    vec_ir_tensor_fc_weight.push_back(paddle::dialect::IrTensor(paddle::dialect::TransToPhiDataType(fc_weight[i].dyn_cast<paddle::dialect::DenseTensorType>().dtype()),
                                                                     fc_weight[i].dyn_cast<paddle::dialect::DenseTensorType>().dims(),
                                                                     fc_weight[i].dyn_cast<paddle::dialect::DenseTensorType>().data_layout(),
                                                                     fc_weight[i].dyn_cast<paddle::dialect::DenseTensorType>().lod(),
                                                                     fc_weight[i].dyn_cast<paddle::dialect::DenseTensorType>().offset()));
  }
  std::vector<paddle::dialect::IrMetaTensor> vec_meta_fc_weight;
  for (size_t i=0; i < vec_ir_tensor_fc_weight.size(); i++) {
    vec_meta_fc_weight.push_back(paddle::dialect::IrMetaTensor(&vec_ir_tensor_fc_weight[i]));
  }

  std::vector<const phi::MetaTensor*> meta_fc_weight;
  for (size_t i=0; i < static_cast<size_t>(vec_meta_fc_weight.size()); i++) {
    meta_fc_weight.push_back(&vec_meta_fc_weight[i]);
  }
   std::vector<paddle::dialect::IrTensor> vec_ir_tensor_fc_weight_max;
  for (size_t i=0; i < static_cast<size_t>(fc_weight_max.size()); i++) {
    vec_ir_tensor_fc_weight_max.push_back(paddle::dialect::IrTensor(paddle::dialect::TransToPhiDataType(fc_weight_max[i].dyn_cast<paddle::dialect::DenseTensorType>().dtype()),
                                                                     fc_weight_max[i].dyn_cast<paddle::dialect::DenseTensorType>().dims(),
                                                                     fc_weight_max[i].dyn_cast<paddle::dialect::DenseTensorType>().data_layout(),
                                                                     fc_weight_max[i].dyn_cast<paddle::dialect::DenseTensorType>().lod(),
                                                                     fc_weight_max[i].dyn_cast<paddle::dialect::DenseTensorType>().offset()));
  }
  std::vector<paddle::dialect::IrMetaTensor> vec_meta_fc_weight_max;
  for (size_t i=0; i < vec_ir_tensor_fc_weight_max.size(); i++) {
    vec_meta_fc_weight_max.push_back(paddle::dialect::IrMetaTensor(&vec_ir_tensor_fc_weight_max[i]));
  }

  std::vector<const phi::MetaTensor*> meta_fc_weight_max;
  for (size_t i=0; i < static_cast<size_t>(vec_meta_fc_weight_max.size()); i++) {
    meta_fc_weight_max.push_back(&vec_meta_fc_weight_max[i]);
  }
   std::vector<paddle::dialect::IrTensor> vec_ir_tensor_fc_bias;
  for (size_t i=0; i < static_cast<size_t>(fc_bias.size()); i++) {
    vec_ir_tensor_fc_bias.push_back(paddle::dialect::IrTensor(paddle::dialect::TransToPhiDataType(fc_bias[i].dyn_cast<paddle::dialect::DenseTensorType>().dtype()),
                                                                     fc_bias[i].dyn_cast<paddle::dialect::DenseTensorType>().dims(),
                                                                     fc_bias[i].dyn_cast<paddle::dialect::DenseTensorType>().data_layout(),
                                                                     fc_bias[i].dyn_cast<paddle::dialect::DenseTensorType>().lod(),
                                                                     fc_bias[i].dyn_cast<paddle::dialect::DenseTensorType>().offset()));
  }
  std::vector<paddle::dialect::IrMetaTensor> vec_meta_fc_bias;
  for (size_t i=0; i < vec_ir_tensor_fc_bias.size(); i++) {
    vec_meta_fc_bias.push_back(paddle::dialect::IrMetaTensor(&vec_ir_tensor_fc_bias[i]));
  }

  std::vector<const phi::MetaTensor*> meta_fc_bias;
  for (size_t i=0; i < static_cast<size_t>(vec_meta_fc_bias.size()); i++) {
    meta_fc_bias.push_back(&vec_meta_fc_bias[i]);
  }
   std::vector<paddle::dialect::IrTensor> vec_ir_tensor_ln_scale;
  for (size_t i=0; i < static_cast<size_t>(ln_scale.size()); i++) {
    vec_ir_tensor_ln_scale.push_back(paddle::dialect::IrTensor(paddle::dialect::TransToPhiDataType(ln_scale[i].dyn_cast<paddle::dialect::DenseTensorType>().dtype()),
                                                                     ln_scale[i].dyn_cast<paddle::dialect::DenseTensorType>().dims(),
                                                                     ln_scale[i].dyn_cast<paddle::dialect::DenseTensorType>().data_layout(),
                                                                     ln_scale[i].dyn_cast<paddle::dialect::DenseTensorType>().lod(),
                                                                     ln_scale[i].dyn_cast<paddle::dialect::DenseTensorType>().offset()));
  }
  std::vector<paddle::dialect::IrMetaTensor> vec_meta_ln_scale;
  for (size_t i=0; i < vec_ir_tensor_ln_scale.size(); i++) {
    vec_meta_ln_scale.push_back(paddle::dialect::IrMetaTensor(&vec_ir_tensor_ln_scale[i]));
  }

  std::vector<const phi::MetaTensor*> meta_ln_scale;
  for (size_t i=0; i < static_cast<size_t>(vec_meta_ln_scale.size()); i++) {
    meta_ln_scale.push_back(&vec_meta_ln_scale[i]);
  }
   std::vector<paddle::dialect::IrTensor> vec_ir_tensor_ln_bias;
  for (size_t i=0; i < static_cast<size_t>(ln_bias.size()); i++) {
    vec_ir_tensor_ln_bias.push_back(paddle::dialect::IrTensor(paddle::dialect::TransToPhiDataType(ln_bias[i].dyn_cast<paddle::dialect::DenseTensorType>().dtype()),
                                                                     ln_bias[i].dyn_cast<paddle::dialect::DenseTensorType>().dims(),
                                                                     ln_bias[i].dyn_cast<paddle::dialect::DenseTensorType>().data_layout(),
                                                                     ln_bias[i].dyn_cast<paddle::dialect::DenseTensorType>().lod(),
                                                                     ln_bias[i].dyn_cast<paddle::dialect::DenseTensorType>().offset()));
  }
  std::vector<paddle::dialect::IrMetaTensor> vec_meta_ln_bias;
  for (size_t i=0; i < vec_ir_tensor_ln_bias.size(); i++) {
    vec_meta_ln_bias.push_back(paddle::dialect::IrMetaTensor(&vec_ir_tensor_ln_bias[i]));
  }

  std::vector<const phi::MetaTensor*> meta_ln_bias;
  for (size_t i=0; i < static_cast<size_t>(vec_meta_ln_bias.size()); i++) {
    meta_ln_bias.push_back(&vec_meta_ln_bias[i]);
  }
 
  paddle::dialect::IrMetaTensor meta_mask;
  paddle::dialect::IrTensor ir_tensor_mask;
  if (mask_.impl() != nullptr) {
    paddle::dialect::DenseTensorType mask = mask_.type().dyn_cast<paddle::dialect::DenseTensorType>();
    VLOG(4) << "Builder construction  dense_mask";
    ir_tensor_mask = paddle::dialect::IrTensor(paddle::dialect::TransToPhiDataType(mask.dtype()),
                                                        mask.dims(),
                                                        mask.data_layout(),
                                                        mask.lod(),
                                                        mask.offset());
    VLOG(4) << "Builder construction  meta_mask";
    meta_mask = paddle::dialect::IrMetaTensor(&ir_tensor_mask);
  }


  paddle::dialect::IrMetaTensor meta_seq_lod;
  paddle::dialect::IrTensor ir_tensor_seq_lod;
  if (seq_lod_.impl() != nullptr) {
    paddle::dialect::DenseTensorType seq_lod = seq_lod_.type().dyn_cast<paddle::dialect::DenseTensorType>();
    VLOG(4) << "Builder construction  dense_seq_lod";
    ir_tensor_seq_lod = paddle::dialect::IrTensor(paddle::dialect::TransToPhiDataType(seq_lod.dtype()),
                                                        seq_lod.dims(),
                                                        seq_lod.data_layout(),
                                                        seq_lod.lod(),
                                                        seq_lod.offset());
    VLOG(4) << "Builder construction  meta_seq_lod";
    meta_seq_lod = paddle::dialect::IrMetaTensor(&ir_tensor_seq_lod);
  }


  paddle::dialect::IrMetaTensor meta_max_seq_len;
  paddle::dialect::IrTensor ir_tensor_max_seq_len;
  if (max_seq_len_.impl() != nullptr) {
    paddle::dialect::DenseTensorType max_seq_len = max_seq_len_.type().dyn_cast<paddle::dialect::DenseTensorType>();
    VLOG(4) << "Builder construction  dense_max_seq_len";
    ir_tensor_max_seq_len = paddle::dialect::IrTensor(paddle::dialect::TransToPhiDataType(max_seq_len.dtype()),
                                                        max_seq_len.dims(),
                                                        max_seq_len.data_layout(),
                                                        max_seq_len.lod(),
                                                        max_seq_len.offset());
    VLOG(4) << "Builder construction  meta_max_seq_len";
    meta_max_seq_len = paddle::dialect::IrMetaTensor(&ir_tensor_max_seq_len);
  }

  paddle::dialect::IrTensor dense_out;
  paddle::dialect::IrMetaTensor meta_out(&dense_out);
  paddle::dialect::IrTensor dense_x_fp16;
  paddle::dialect::IrMetaTensor meta_x_fp16(&dense_x_fp16);
  paddle::dialect::IrTensor dense_out_fp16;
  paddle::dialect::IrMetaTensor meta_out_fp16(&dense_out_fp16);

  phi::MultiEncoderXPUInferMeta(meta_x, meta_fc_weight, meta_fc_weight_max, meta_fc_bias, meta_ln_scale, meta_ln_bias, meta_mask, meta_seq_lod, meta_max_seq_len, layer_num, norm_before, hidden_dim, head_num, size_per_head, ffn_hidden_dim_scale, act_type, relative_type, slice_idx, &meta_out, &meta_x_fp16, &meta_out_fp16);

  std::vector<pir::Type> argument_outputs;
  pir::Type out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_out.dtype()), dense_out.dims(), dense_out.layout(), dense_out.lod(), dense_out.offset());
  argument_outputs.push_back(out_dense_tensor_type);

  pir::Type x_fp16_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_x_fp16.dtype()), dense_x_fp16.dims(), dense_x_fp16.layout(), dense_x_fp16.lod(), dense_x_fp16.offset());
  argument_outputs.push_back(x_fp16_dense_tensor_type);

  pir::Type out_fp16_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_out_fp16.dtype()), dense_out_fp16.dims(), dense_out_fp16.layout(), dense_out_fp16.lod(), dense_out_fp16.offset());
  argument_outputs.push_back(out_fp16_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void MultiEncoderXpuOp::VerifySig() {
  VLOG(4) << "Start Verifying inputs, outputs and attributes for: MultiEncoderXpuOp.";
  VLOG(4) << "Verifying inputs:";
  {
  auto input_size = num_operands();
  IR_ENFORCE(input_size == 9u,
                    "The size %d of inputs must be equal to 9.", input_size);
  IR_ENFORCE((*this)->operand_source(0).type().isa<paddle::dialect::DenseTensorType>(),
                  "Type validation failed for the 0th input, got %s.", (*this)->operand_source(0).type());
  if (auto vec_type = (*this)->operand_source(1).type().dyn_cast<pir::VectorType>()) {
      for (size_t i = 0; i < vec_type.size(); ++i) {
        IR_ENFORCE(vec_type[i].isa<paddle::dialect::DenseTensorType>(),
                       "Type validation failed for the 1th input, got %s.", (*this)->operand_source(1).type());
      }
  }
  else {
    IR_ENFORCE((*this)->operand_source(1).type().isa<paddle::dialect::DenseTensorType>(),
                   "Type validation failed for the 1th input, got %s.", (*this)->operand_source(1).type());
  }
  if (auto vec_type = (*this)->operand_source(2).type().dyn_cast<pir::VectorType>()) {
      for (size_t i = 0; i < vec_type.size(); ++i) {
        IR_ENFORCE(vec_type[i].isa<paddle::dialect::DenseTensorType>(),
                       "Type validation failed for the 2th input, got %s.", (*this)->operand_source(2).type());
      }
  }
  else {
    IR_ENFORCE((*this)->operand_source(2).type().isa<paddle::dialect::DenseTensorType>(),
                   "Type validation failed for the 2th input, got %s.", (*this)->operand_source(2).type());
  }
  if (auto vec_type = (*this)->operand_source(3).type().dyn_cast<pir::VectorType>()) {
      for (size_t i = 0; i < vec_type.size(); ++i) {
        IR_ENFORCE(vec_type[i].isa<paddle::dialect::DenseTensorType>(),
                       "Type validation failed for the 3th input, got %s.", (*this)->operand_source(3).type());
      }
  }
  else {
    IR_ENFORCE((*this)->operand_source(3).type().isa<paddle::dialect::DenseTensorType>(),
                   "Type validation failed for the 3th input, got %s.", (*this)->operand_source(3).type());
  }
  if (auto vec_type = (*this)->operand_source(4).type().dyn_cast<pir::VectorType>()) {
      for (size_t i = 0; i < vec_type.size(); ++i) {
        IR_ENFORCE(vec_type[i].isa<paddle::dialect::DenseTensorType>(),
                       "Type validation failed for the 4th input, got %s.", (*this)->operand_source(4).type());
      }
  }
  else {
    IR_ENFORCE((*this)->operand_source(4).type().isa<paddle::dialect::DenseTensorType>(),
                   "Type validation failed for the 4th input, got %s.", (*this)->operand_source(4).type());
  }
  if (auto vec_type = (*this)->operand_source(5).type().dyn_cast<pir::VectorType>()) {
      for (size_t i = 0; i < vec_type.size(); ++i) {
        IR_ENFORCE(vec_type[i].isa<paddle::dialect::DenseTensorType>(),
                       "Type validation failed for the 5th input, got %s.", (*this)->operand_source(5).type());
      }
  }
  else {
    IR_ENFORCE((*this)->operand_source(5).type().isa<paddle::dialect::DenseTensorType>(),
                   "Type validation failed for the 5th input, got %s.", (*this)->operand_source(5).type());
  }
  if (auto val = (*this)->operand(6)) {
    IR_ENFORCE(val.type().isa<paddle::dialect::DenseTensorType>(),
                   "Type validation failed for the 6th input, got %s.", (*this)->operand_source(6).type());
  }
  if (auto val = (*this)->operand(7)) {
    IR_ENFORCE(val.type().isa<paddle::dialect::DenseTensorType>(),
                   "Type validation failed for the 7th input, got %s.", (*this)->operand_source(7).type());
  }
  if (auto val = (*this)->operand(8)) {
    IR_ENFORCE(val.type().isa<paddle::dialect::DenseTensorType>(),
                   "Type validation failed for the 8th input, got %s.", (*this)->operand_source(8).type());
  }
  }
  VLOG(4) << "Verifying attributes:";
  {
  auto& attributes = this->attributes();
  IR_ENFORCE(attributes.count("layer_num")>0,
                 "layer_num does not exist.");
  IR_ENFORCE(attributes.at("layer_num").isa<pir::Int32Attribute>(),
                 "Type of attribute: layer_num is not pir::Int32Attribute.");

  IR_ENFORCE(attributes.count("norm_before")>0,
                 "norm_before does not exist.");
  IR_ENFORCE(attributes.at("norm_before").isa<pir::BoolAttribute>(),
                 "Type of attribute: norm_before is not pir::BoolAttribute.");

  IR_ENFORCE(attributes.count("hidden_dim")>0,
                 "hidden_dim does not exist.");
  IR_ENFORCE(attributes.at("hidden_dim").isa<pir::Int32Attribute>(),
                 "Type of attribute: hidden_dim is not pir::Int32Attribute.");

  IR_ENFORCE(attributes.count("head_num")>0,
                 "head_num does not exist.");
  IR_ENFORCE(attributes.at("head_num").isa<pir::Int32Attribute>(),
                 "Type of attribute: head_num is not pir::Int32Attribute.");

  IR_ENFORCE(attributes.count("size_per_head")>0,
                 "size_per_head does not exist.");
  IR_ENFORCE(attributes.at("size_per_head").isa<pir::Int32Attribute>(),
                 "Type of attribute: size_per_head is not pir::Int32Attribute.");

  IR_ENFORCE(attributes.count("ffn_hidden_dim_scale")>0,
                 "ffn_hidden_dim_scale does not exist.");
  IR_ENFORCE(attributes.at("ffn_hidden_dim_scale").isa<pir::Int32Attribute>(),
                 "Type of attribute: ffn_hidden_dim_scale is not pir::Int32Attribute.");

  IR_ENFORCE(attributes.count("act_type")>0,
                 "act_type does not exist.");
  IR_ENFORCE(attributes.at("act_type").isa<pir::Int32Attribute>(),
                 "Type of attribute: act_type is not pir::Int32Attribute.");

  IR_ENFORCE(attributes.count("relative_type")>0,
                 "relative_type does not exist.");
  IR_ENFORCE(attributes.at("relative_type").isa<pir::Int32Attribute>(),
                 "Type of attribute: relative_type is not pir::Int32Attribute.");

  IR_ENFORCE(attributes.count("slice_idx")>0,
                 "slice_idx does not exist.");
  IR_ENFORCE(attributes.at("slice_idx").isa<pir::Int32Attribute>(),
                 "Type of attribute: slice_idx is not pir::Int32Attribute.");

  }
  VLOG(4) << "Verifying outputs:";
  {
  auto output_size = num_results();
  IR_ENFORCE(output_size == 3u,
                    "The size %d of outputs must be equal to 3.", output_size);
  IR_ENFORCE((*this)->result(0).type().isa<paddle::dialect::DenseTensorType>(),
                 "Type validation failed for the 0th output.");
  if (auto output_1_type = (*this)->result(1).type()) {
    IR_ENFORCE(output_1_type.isa<paddle::dialect::DenseTensorType>(),
                   "Type validation failed for the 1th output.");
  }
  if (auto output_2_type = (*this)->result(2).type()) {
    IR_ENFORCE(output_2_type.isa<paddle::dialect::DenseTensorType>(),
                   "Type validation failed for the 2th output.");
  }
  }
  VLOG(4) << "End Verifying for: MultiEncoderXpuOp.";
}

void MultiEncoderXpuOp::InferMeta( phi::InferMetaContext *infer_meta ) {
  auto fn = PD_INFER_META(phi::MultiEncoderXPUInferMeta);
  fn(infer_meta);
}

phi::DataType MultiEncoderXpuOp::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: MultiEncoderXpuOp";
  


  return expected_kernel_dtype;
}

const char *MultiheadMatmulOp::attributes_name[5] = { "transpose_q", "transpose_k", "transpose_v", "alpha", "head_number" };

OpInfoTuple MultiheadMatmulOp::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("input", "paddle::dialect::DenseTensorType", false, false, false, false), paddle::dialect::OpInputInfo("w", "paddle::dialect::DenseTensorType", false, false, false, false), paddle::dialect::OpInputInfo("bias", "paddle::dialect::DenseTensorType", false, false, false, false), paddle::dialect::OpInputInfo("bias_qk", "paddle::dialect::DenseTensorType", true, false, false, false) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = { paddle::dialect::OpAttributeInfo("transpose_q", "pir::BoolAttribute", ""), paddle::dialect::OpAttributeInfo("transpose_k", "pir::BoolAttribute", ""), paddle::dialect::OpAttributeInfo("transpose_v", "pir::BoolAttribute", ""), paddle::dialect::OpAttributeInfo("alpha", "pir::FloatAttribute", ""), paddle::dialect::OpAttributeInfo("head_number", "pir::Int32Attribute", "") };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("out", "paddle::dialect::DenseTensorType", false, false) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("MultiheadMatmulInferMeta", {"input", "w", "bias", "bias_qk", "transpose_q", "transpose_k", "transpose_v", "alpha", "head_number"}, "multihead_matmul", {"input", "w", "bias", "bias_qk", "transpose_q", "transpose_k", "transpose_v", "alpha", "head_number"}, {"input"}, {}, {}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "multihead_matmul");
}

void MultiheadMatmulOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value input_, pir::Value w_, pir::Value bias_, pir::Value bias_qk_, bool transpose_q, bool transpose_k, bool transpose_v, float alpha, int head_number) {
  VLOG(4) << "Start build MultiheadMatmulOp";



  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {input_, w_, bias_, bias_qk_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_transpose_q = pir::BoolAttribute::get(pir::IrContext::Instance(), transpose_q);
  argument.AddAttribute("transpose_q", attr_transpose_q);
  pir::Attribute attr_transpose_k = pir::BoolAttribute::get(pir::IrContext::Instance(), transpose_k);
  argument.AddAttribute("transpose_k", attr_transpose_k);
  pir::Attribute attr_transpose_v = pir::BoolAttribute::get(pir::IrContext::Instance(), transpose_v);
  argument.AddAttribute("transpose_v", attr_transpose_v);
  pir::Attribute attr_alpha = pir::FloatAttribute::get(pir::IrContext::Instance(), alpha);
  argument.AddAttribute("alpha", attr_alpha);
  pir::Attribute attr_head_number = pir::Int32Attribute::get(pir::IrContext::Instance(), head_number);
  argument.AddAttribute("head_number", attr_head_number);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType input = input_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)input;
  paddle::dialect::DenseTensorType w = w_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)w;
  paddle::dialect::DenseTensorType bias = bias_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)bias;

  VLOG(4) << "Builder construction  dense_input";
  paddle::dialect::IrTensor ir_tensor_input(paddle::dialect::TransToPhiDataType(input.dtype()),
                                                      input.dims(),
                                                      input.data_layout(),
                                                      input.lod(),
                                                      input.offset());
  VLOG(4) << "Builder construction  meta_input";
  paddle::dialect::IrMetaTensor meta_input(&ir_tensor_input);

  VLOG(4) << "Builder construction  dense_w";
  paddle::dialect::IrTensor ir_tensor_w(paddle::dialect::TransToPhiDataType(w.dtype()),
                                                      w.dims(),
                                                      w.data_layout(),
                                                      w.lod(),
                                                      w.offset());
  VLOG(4) << "Builder construction  meta_w";
  paddle::dialect::IrMetaTensor meta_w(&ir_tensor_w);

  VLOG(4) << "Builder construction  dense_bias";
  paddle::dialect::IrTensor ir_tensor_bias(paddle::dialect::TransToPhiDataType(bias.dtype()),
                                                      bias.dims(),
                                                      bias.data_layout(),
                                                      bias.lod(),
                                                      bias.offset());
  VLOG(4) << "Builder construction  meta_bias";
  paddle::dialect::IrMetaTensor meta_bias(&ir_tensor_bias);

  paddle::dialect::IrMetaTensor meta_bias_qk;
  paddle::dialect::IrTensor ir_tensor_bias_qk;
  if (bias_qk_.impl() != nullptr) {
    paddle::dialect::DenseTensorType bias_qk = bias_qk_.type().dyn_cast<paddle::dialect::DenseTensorType>();
    VLOG(4) << "Builder construction  dense_bias_qk";
    ir_tensor_bias_qk = paddle::dialect::IrTensor(paddle::dialect::TransToPhiDataType(bias_qk.dtype()),
                                                        bias_qk.dims(),
                                                        bias_qk.data_layout(),
                                                        bias_qk.lod(),
                                                        bias_qk.offset());
    VLOG(4) << "Builder construction  meta_bias_qk";
    meta_bias_qk = paddle::dialect::IrMetaTensor(&ir_tensor_bias_qk);
  }

  paddle::dialect::IrTensor dense_out;
  paddle::dialect::IrMetaTensor meta_out(&dense_out);

  phi::MultiheadMatmulInferMeta(meta_input, meta_w, meta_bias, meta_bias_qk, transpose_q, transpose_k, transpose_v, alpha, head_number, &meta_out);

  std::vector<pir::Type> argument_outputs;
  pir::Type out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_out.dtype()), dense_out.dims(), dense_out.layout(), dense_out.lod(), dense_out.offset());
  argument_outputs.push_back(out_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void MultiheadMatmulOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value input_, pir::Value w_, pir::Value bias_, pir::Value bias_qk_, pir::AttributeMap attributes) {
  VLOG(4) << "Start build MultiheadMatmulOp";


  IR_ENFORCE(
      attributes.find("transpose_q") != attributes.end(),
          "'transpose_q' Attribute is expected for MultiheadMatmulOp. ");
  bool transpose_q = attributes.at("transpose_q").dyn_cast<pir::BoolAttribute>().data();

  IR_ENFORCE(
      attributes.find("transpose_k") != attributes.end(),
          "'transpose_k' Attribute is expected for MultiheadMatmulOp. ");
  bool transpose_k = attributes.at("transpose_k").dyn_cast<pir::BoolAttribute>().data();

  IR_ENFORCE(
      attributes.find("transpose_v") != attributes.end(),
          "'transpose_v' Attribute is expected for MultiheadMatmulOp. ");
  bool transpose_v = attributes.at("transpose_v").dyn_cast<pir::BoolAttribute>().data();

  IR_ENFORCE(
      attributes.find("alpha") != attributes.end(),
          "'alpha' Attribute is expected for MultiheadMatmulOp. ");
  float alpha = attributes.at("alpha").dyn_cast<pir::FloatAttribute>().data();

  IR_ENFORCE(
      attributes.find("head_number") != attributes.end(),
          "'head_number' Attribute is expected for MultiheadMatmulOp. ");
  int head_number = attributes.at("head_number").dyn_cast<pir::Int32Attribute>().data();


  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {input_, w_, bias_, bias_qk_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_transpose_q = pir::BoolAttribute::get(pir::IrContext::Instance(), transpose_q);
  argument.AddAttribute("transpose_q", attr_transpose_q);
  pir::Attribute attr_transpose_k = pir::BoolAttribute::get(pir::IrContext::Instance(), transpose_k);
  argument.AddAttribute("transpose_k", attr_transpose_k);
  pir::Attribute attr_transpose_v = pir::BoolAttribute::get(pir::IrContext::Instance(), transpose_v);
  argument.AddAttribute("transpose_v", attr_transpose_v);
  pir::Attribute attr_alpha = pir::FloatAttribute::get(pir::IrContext::Instance(), alpha);
  argument.AddAttribute("alpha", attr_alpha);
  pir::Attribute attr_head_number = pir::Int32Attribute::get(pir::IrContext::Instance(), head_number);
  argument.AddAttribute("head_number", attr_head_number);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType input = input_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)input;
  paddle::dialect::DenseTensorType w = w_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)w;
  paddle::dialect::DenseTensorType bias = bias_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)bias;

  VLOG(4) << "Builder construction  dense_input";
  paddle::dialect::IrTensor ir_tensor_input(paddle::dialect::TransToPhiDataType(input.dtype()),
                                                      input.dims(),
                                                      input.data_layout(),
                                                      input.lod(),
                                                      input.offset());
  VLOG(4) << "Builder construction  meta_input";
  paddle::dialect::IrMetaTensor meta_input(&ir_tensor_input);

  VLOG(4) << "Builder construction  dense_w";
  paddle::dialect::IrTensor ir_tensor_w(paddle::dialect::TransToPhiDataType(w.dtype()),
                                                      w.dims(),
                                                      w.data_layout(),
                                                      w.lod(),
                                                      w.offset());
  VLOG(4) << "Builder construction  meta_w";
  paddle::dialect::IrMetaTensor meta_w(&ir_tensor_w);

  VLOG(4) << "Builder construction  dense_bias";
  paddle::dialect::IrTensor ir_tensor_bias(paddle::dialect::TransToPhiDataType(bias.dtype()),
                                                      bias.dims(),
                                                      bias.data_layout(),
                                                      bias.lod(),
                                                      bias.offset());
  VLOG(4) << "Builder construction  meta_bias";
  paddle::dialect::IrMetaTensor meta_bias(&ir_tensor_bias);

  paddle::dialect::IrMetaTensor meta_bias_qk;
  paddle::dialect::IrTensor ir_tensor_bias_qk;
  if (bias_qk_.impl() != nullptr) {
    paddle::dialect::DenseTensorType bias_qk = bias_qk_.type().dyn_cast<paddle::dialect::DenseTensorType>();
    VLOG(4) << "Builder construction  dense_bias_qk";
    ir_tensor_bias_qk = paddle::dialect::IrTensor(paddle::dialect::TransToPhiDataType(bias_qk.dtype()),
                                                        bias_qk.dims(),
                                                        bias_qk.data_layout(),
                                                        bias_qk.lod(),
                                                        bias_qk.offset());
    VLOG(4) << "Builder construction  meta_bias_qk";
    meta_bias_qk = paddle::dialect::IrMetaTensor(&ir_tensor_bias_qk);
  }

  paddle::dialect::IrTensor dense_out;
  paddle::dialect::IrMetaTensor meta_out(&dense_out);

  phi::MultiheadMatmulInferMeta(meta_input, meta_w, meta_bias, meta_bias_qk, transpose_q, transpose_k, transpose_v, alpha, head_number, &meta_out);

  std::vector<pir::Type> argument_outputs;
  pir::Type out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_out.dtype()), dense_out.dims(), dense_out.layout(), dense_out.lod(), dense_out.offset());
  argument_outputs.push_back(out_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void MultiheadMatmulOp::VerifySig() {
  VLOG(4) << "Start Verifying inputs, outputs and attributes for: MultiheadMatmulOp.";
  VLOG(4) << "Verifying inputs:";
  {
  auto input_size = num_operands();
  IR_ENFORCE(input_size == 4u,
                    "The size %d of inputs must be equal to 4.", input_size);
  IR_ENFORCE((*this)->operand_source(0).type().isa<paddle::dialect::DenseTensorType>(),
                  "Type validation failed for the 0th input, got %s.", (*this)->operand_source(0).type());
  IR_ENFORCE((*this)->operand_source(1).type().isa<paddle::dialect::DenseTensorType>(),
                  "Type validation failed for the 1th input, got %s.", (*this)->operand_source(1).type());
  IR_ENFORCE((*this)->operand_source(2).type().isa<paddle::dialect::DenseTensorType>(),
                  "Type validation failed for the 2th input, got %s.", (*this)->operand_source(2).type());
  if (auto val = (*this)->operand(3)) {
    IR_ENFORCE(val.type().isa<paddle::dialect::DenseTensorType>(),
                   "Type validation failed for the 3th input, got %s.", (*this)->operand_source(3).type());
  }
  }
  VLOG(4) << "Verifying attributes:";
  {
  auto& attributes = this->attributes();
  IR_ENFORCE(attributes.count("transpose_q")>0,
                 "transpose_q does not exist.");
  IR_ENFORCE(attributes.at("transpose_q").isa<pir::BoolAttribute>(),
                 "Type of attribute: transpose_q is not pir::BoolAttribute.");

  IR_ENFORCE(attributes.count("transpose_k")>0,
                 "transpose_k does not exist.");
  IR_ENFORCE(attributes.at("transpose_k").isa<pir::BoolAttribute>(),
                 "Type of attribute: transpose_k is not pir::BoolAttribute.");

  IR_ENFORCE(attributes.count("transpose_v")>0,
                 "transpose_v does not exist.");
  IR_ENFORCE(attributes.at("transpose_v").isa<pir::BoolAttribute>(),
                 "Type of attribute: transpose_v is not pir::BoolAttribute.");

  IR_ENFORCE(attributes.count("alpha")>0,
                 "alpha does not exist.");
  IR_ENFORCE(attributes.at("alpha").isa<pir::FloatAttribute>(),
                 "Type of attribute: alpha is not pir::FloatAttribute.");

  IR_ENFORCE(attributes.count("head_number")>0,
                 "head_number does not exist.");
  IR_ENFORCE(attributes.at("head_number").isa<pir::Int32Attribute>(),
                 "Type of attribute: head_number is not pir::Int32Attribute.");

  }
  VLOG(4) << "Verifying outputs:";
  {
  auto output_size = num_results();
  IR_ENFORCE(output_size == 1u,
                    "The size %d of outputs must be equal to 1.", output_size);
  IR_ENFORCE((*this)->result(0).type().isa<paddle::dialect::DenseTensorType>(),
                 "Type validation failed for the 0th output.");
  }
  VLOG(4) << "End Verifying for: MultiheadMatmulOp.";
}

void MultiheadMatmulOp::InferMeta( phi::InferMetaContext *infer_meta ) {
  auto fn = PD_INFER_META(phi::MultiheadMatmulInferMeta);
  fn(infer_meta);
}

phi::DataType MultiheadMatmulOp::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: MultiheadMatmulOp";
  


  return expected_kernel_dtype;
}

const char *QkvAttentionXpuOp::attributes_name[5] = { "alpha", "head_num", "head_dim", "qkv_fc_fusion", "out_dtype" };

OpInfoTuple QkvAttentionXpuOp::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("q", "paddle::dialect::DenseTensorType", false, false, false, false), paddle::dialect::OpInputInfo("k", "paddle::dialect::DenseTensorType", false, false, false, false), paddle::dialect::OpInputInfo("v", "paddle::dialect::DenseTensorType", false, false, false, false), paddle::dialect::OpInputInfo("q_max", "paddle::dialect::DenseTensorType", true, false, false, false), paddle::dialect::OpInputInfo("k_max", "paddle::dialect::DenseTensorType", true, false, false, false), paddle::dialect::OpInputInfo("v_max", "paddle::dialect::DenseTensorType", true, false, false, false) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = { paddle::dialect::OpAttributeInfo("alpha", "pir::FloatAttribute", ""), paddle::dialect::OpAttributeInfo("head_num", "pir::Int32Attribute", ""), paddle::dialect::OpAttributeInfo("head_dim", "pir::Int32Attribute", ""), paddle::dialect::OpAttributeInfo("qkv_fc_fusion", "pir::BoolAttribute", ""), paddle::dialect::OpAttributeInfo("out_dtype", "paddle::dialect::DataTypeAttribute", "") };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("qkv", "paddle::dialect::DenseTensorType", false, false), paddle::dialect::OpOutputInfo("qkv_max", "paddle::dialect::DenseTensorType", false, false) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("QKVAttentionXPUInferMeta", {"q", "k", "v", "q_max", "k_max", "v_max", "alpha", "head_num", "head_dim", "qkv_fc_fusion", "out_dtype"}, "qkv_attention_xpu", {"q", "k", "v", "q_max", "k_max", "v_max", "alpha", "head_num", "head_dim", "qkv_fc_fusion", "out_dtype"}, {"q"}, {}, {}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "qkv_attention_xpu");
}

void QkvAttentionXpuOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value q_, pir::Value k_, pir::Value v_, pir::Value q_max_, pir::Value k_max_, pir::Value v_max_, float alpha, int head_num, int head_dim, bool qkv_fc_fusion, phi::DataType out_dtype) {
  VLOG(4) << "Start build QkvAttentionXpuOp";



  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {q_, k_, v_, q_max_, k_max_, v_max_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_alpha = pir::FloatAttribute::get(pir::IrContext::Instance(), alpha);
  argument.AddAttribute("alpha", attr_alpha);
  pir::Attribute attr_head_num = pir::Int32Attribute::get(pir::IrContext::Instance(), head_num);
  argument.AddAttribute("head_num", attr_head_num);
  pir::Attribute attr_head_dim = pir::Int32Attribute::get(pir::IrContext::Instance(), head_dim);
  argument.AddAttribute("head_dim", attr_head_dim);
  pir::Attribute attr_qkv_fc_fusion = pir::BoolAttribute::get(pir::IrContext::Instance(), qkv_fc_fusion);
  argument.AddAttribute("qkv_fc_fusion", attr_qkv_fc_fusion);
  pir::Attribute attr_out_dtype = paddle::dialect::DataTypeAttribute::get(pir::IrContext::Instance(), out_dtype);
  argument.AddAttribute("out_dtype", attr_out_dtype);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType q = q_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)q;
  paddle::dialect::DenseTensorType k = k_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)k;
  paddle::dialect::DenseTensorType v = v_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)v;

  VLOG(4) << "Builder construction  dense_q";
  paddle::dialect::IrTensor ir_tensor_q(paddle::dialect::TransToPhiDataType(q.dtype()),
                                                      q.dims(),
                                                      q.data_layout(),
                                                      q.lod(),
                                                      q.offset());
  VLOG(4) << "Builder construction  meta_q";
  paddle::dialect::IrMetaTensor meta_q(&ir_tensor_q);

  VLOG(4) << "Builder construction  dense_k";
  paddle::dialect::IrTensor ir_tensor_k(paddle::dialect::TransToPhiDataType(k.dtype()),
                                                      k.dims(),
                                                      k.data_layout(),
                                                      k.lod(),
                                                      k.offset());
  VLOG(4) << "Builder construction  meta_k";
  paddle::dialect::IrMetaTensor meta_k(&ir_tensor_k);

  VLOG(4) << "Builder construction  dense_v";
  paddle::dialect::IrTensor ir_tensor_v(paddle::dialect::TransToPhiDataType(v.dtype()),
                                                      v.dims(),
                                                      v.data_layout(),
                                                      v.lod(),
                                                      v.offset());
  VLOG(4) << "Builder construction  meta_v";
  paddle::dialect::IrMetaTensor meta_v(&ir_tensor_v);

  paddle::dialect::IrMetaTensor meta_q_max;
  paddle::dialect::IrTensor ir_tensor_q_max;
  if (q_max_.impl() != nullptr) {
    paddle::dialect::DenseTensorType q_max = q_max_.type().dyn_cast<paddle::dialect::DenseTensorType>();
    VLOG(4) << "Builder construction  dense_q_max";
    ir_tensor_q_max = paddle::dialect::IrTensor(paddle::dialect::TransToPhiDataType(q_max.dtype()),
                                                        q_max.dims(),
                                                        q_max.data_layout(),
                                                        q_max.lod(),
                                                        q_max.offset());
    VLOG(4) << "Builder construction  meta_q_max";
    meta_q_max = paddle::dialect::IrMetaTensor(&ir_tensor_q_max);
  }


  paddle::dialect::IrMetaTensor meta_k_max;
  paddle::dialect::IrTensor ir_tensor_k_max;
  if (k_max_.impl() != nullptr) {
    paddle::dialect::DenseTensorType k_max = k_max_.type().dyn_cast<paddle::dialect::DenseTensorType>();
    VLOG(4) << "Builder construction  dense_k_max";
    ir_tensor_k_max = paddle::dialect::IrTensor(paddle::dialect::TransToPhiDataType(k_max.dtype()),
                                                        k_max.dims(),
                                                        k_max.data_layout(),
                                                        k_max.lod(),
                                                        k_max.offset());
    VLOG(4) << "Builder construction  meta_k_max";
    meta_k_max = paddle::dialect::IrMetaTensor(&ir_tensor_k_max);
  }


  paddle::dialect::IrMetaTensor meta_v_max;
  paddle::dialect::IrTensor ir_tensor_v_max;
  if (v_max_.impl() != nullptr) {
    paddle::dialect::DenseTensorType v_max = v_max_.type().dyn_cast<paddle::dialect::DenseTensorType>();
    VLOG(4) << "Builder construction  dense_v_max";
    ir_tensor_v_max = paddle::dialect::IrTensor(paddle::dialect::TransToPhiDataType(v_max.dtype()),
                                                        v_max.dims(),
                                                        v_max.data_layout(),
                                                        v_max.lod(),
                                                        v_max.offset());
    VLOG(4) << "Builder construction  meta_v_max";
    meta_v_max = paddle::dialect::IrMetaTensor(&ir_tensor_v_max);
  }

  paddle::dialect::IrTensor dense_qkv;
  paddle::dialect::IrMetaTensor meta_qkv(&dense_qkv);
  paddle::dialect::IrTensor dense_qkv_max;
  paddle::dialect::IrMetaTensor meta_qkv_max(&dense_qkv_max);

  phi::QKVAttentionXPUInferMeta(meta_q, meta_k, meta_v, meta_q_max, meta_k_max, meta_v_max, alpha, head_num, head_dim, qkv_fc_fusion, out_dtype, &meta_qkv, &meta_qkv_max);

  std::vector<pir::Type> argument_outputs;
  pir::Type qkv_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_qkv.dtype()), dense_qkv.dims(), dense_qkv.layout(), dense_qkv.lod(), dense_qkv.offset());
  argument_outputs.push_back(qkv_dense_tensor_type);

  pir::Type qkv_max_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_qkv_max.dtype()), dense_qkv_max.dims(), dense_qkv_max.layout(), dense_qkv_max.lod(), dense_qkv_max.offset());
  argument_outputs.push_back(qkv_max_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void QkvAttentionXpuOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value q_, pir::Value k_, pir::Value v_, pir::Value q_max_, pir::Value k_max_, pir::Value v_max_, pir::AttributeMap attributes) {
  VLOG(4) << "Start build QkvAttentionXpuOp";


  IR_ENFORCE(
      attributes.find("alpha") != attributes.end(),
          "'alpha' Attribute is expected for QkvAttentionXpuOp. ");
  float alpha = attributes.at("alpha").dyn_cast<pir::FloatAttribute>().data();

  IR_ENFORCE(
      attributes.find("head_num") != attributes.end(),
          "'head_num' Attribute is expected for QkvAttentionXpuOp. ");
  int head_num = attributes.at("head_num").dyn_cast<pir::Int32Attribute>().data();

  IR_ENFORCE(
      attributes.find("head_dim") != attributes.end(),
          "'head_dim' Attribute is expected for QkvAttentionXpuOp. ");
  int head_dim = attributes.at("head_dim").dyn_cast<pir::Int32Attribute>().data();

  IR_ENFORCE(
      attributes.find("qkv_fc_fusion") != attributes.end(),
          "'qkv_fc_fusion' Attribute is expected for QkvAttentionXpuOp. ");
  bool qkv_fc_fusion = attributes.at("qkv_fc_fusion").dyn_cast<pir::BoolAttribute>().data();

  IR_ENFORCE(
      attributes.find("out_dtype") != attributes.end(),
          "'out_dtype' Attribute is expected for QkvAttentionXpuOp. ");
  phi::DataType out_dtype = attributes.at("out_dtype").dyn_cast<paddle::dialect::DataTypeAttribute>().data();


  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {q_, k_, v_, q_max_, k_max_, v_max_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_alpha = pir::FloatAttribute::get(pir::IrContext::Instance(), alpha);
  argument.AddAttribute("alpha", attr_alpha);
  pir::Attribute attr_head_num = pir::Int32Attribute::get(pir::IrContext::Instance(), head_num);
  argument.AddAttribute("head_num", attr_head_num);
  pir::Attribute attr_head_dim = pir::Int32Attribute::get(pir::IrContext::Instance(), head_dim);
  argument.AddAttribute("head_dim", attr_head_dim);
  pir::Attribute attr_qkv_fc_fusion = pir::BoolAttribute::get(pir::IrContext::Instance(), qkv_fc_fusion);
  argument.AddAttribute("qkv_fc_fusion", attr_qkv_fc_fusion);
  pir::Attribute attr_out_dtype = paddle::dialect::DataTypeAttribute::get(pir::IrContext::Instance(), out_dtype);
  argument.AddAttribute("out_dtype", attr_out_dtype);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType q = q_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)q;
  paddle::dialect::DenseTensorType k = k_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)k;
  paddle::dialect::DenseTensorType v = v_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)v;

  VLOG(4) << "Builder construction  dense_q";
  paddle::dialect::IrTensor ir_tensor_q(paddle::dialect::TransToPhiDataType(q.dtype()),
                                                      q.dims(),
                                                      q.data_layout(),
                                                      q.lod(),
                                                      q.offset());
  VLOG(4) << "Builder construction  meta_q";
  paddle::dialect::IrMetaTensor meta_q(&ir_tensor_q);

  VLOG(4) << "Builder construction  dense_k";
  paddle::dialect::IrTensor ir_tensor_k(paddle::dialect::TransToPhiDataType(k.dtype()),
                                                      k.dims(),
                                                      k.data_layout(),
                                                      k.lod(),
                                                      k.offset());
  VLOG(4) << "Builder construction  meta_k";
  paddle::dialect::IrMetaTensor meta_k(&ir_tensor_k);

  VLOG(4) << "Builder construction  dense_v";
  paddle::dialect::IrTensor ir_tensor_v(paddle::dialect::TransToPhiDataType(v.dtype()),
                                                      v.dims(),
                                                      v.data_layout(),
                                                      v.lod(),
                                                      v.offset());
  VLOG(4) << "Builder construction  meta_v";
  paddle::dialect::IrMetaTensor meta_v(&ir_tensor_v);

  paddle::dialect::IrMetaTensor meta_q_max;
  paddle::dialect::IrTensor ir_tensor_q_max;
  if (q_max_.impl() != nullptr) {
    paddle::dialect::DenseTensorType q_max = q_max_.type().dyn_cast<paddle::dialect::DenseTensorType>();
    VLOG(4) << "Builder construction  dense_q_max";
    ir_tensor_q_max = paddle::dialect::IrTensor(paddle::dialect::TransToPhiDataType(q_max.dtype()),
                                                        q_max.dims(),
                                                        q_max.data_layout(),
                                                        q_max.lod(),
                                                        q_max.offset());
    VLOG(4) << "Builder construction  meta_q_max";
    meta_q_max = paddle::dialect::IrMetaTensor(&ir_tensor_q_max);
  }


  paddle::dialect::IrMetaTensor meta_k_max;
  paddle::dialect::IrTensor ir_tensor_k_max;
  if (k_max_.impl() != nullptr) {
    paddle::dialect::DenseTensorType k_max = k_max_.type().dyn_cast<paddle::dialect::DenseTensorType>();
    VLOG(4) << "Builder construction  dense_k_max";
    ir_tensor_k_max = paddle::dialect::IrTensor(paddle::dialect::TransToPhiDataType(k_max.dtype()),
                                                        k_max.dims(),
                                                        k_max.data_layout(),
                                                        k_max.lod(),
                                                        k_max.offset());
    VLOG(4) << "Builder construction  meta_k_max";
    meta_k_max = paddle::dialect::IrMetaTensor(&ir_tensor_k_max);
  }


  paddle::dialect::IrMetaTensor meta_v_max;
  paddle::dialect::IrTensor ir_tensor_v_max;
  if (v_max_.impl() != nullptr) {
    paddle::dialect::DenseTensorType v_max = v_max_.type().dyn_cast<paddle::dialect::DenseTensorType>();
    VLOG(4) << "Builder construction  dense_v_max";
    ir_tensor_v_max = paddle::dialect::IrTensor(paddle::dialect::TransToPhiDataType(v_max.dtype()),
                                                        v_max.dims(),
                                                        v_max.data_layout(),
                                                        v_max.lod(),
                                                        v_max.offset());
    VLOG(4) << "Builder construction  meta_v_max";
    meta_v_max = paddle::dialect::IrMetaTensor(&ir_tensor_v_max);
  }

  paddle::dialect::IrTensor dense_qkv;
  paddle::dialect::IrMetaTensor meta_qkv(&dense_qkv);
  paddle::dialect::IrTensor dense_qkv_max;
  paddle::dialect::IrMetaTensor meta_qkv_max(&dense_qkv_max);

  phi::QKVAttentionXPUInferMeta(meta_q, meta_k, meta_v, meta_q_max, meta_k_max, meta_v_max, alpha, head_num, head_dim, qkv_fc_fusion, out_dtype, &meta_qkv, &meta_qkv_max);

  std::vector<pir::Type> argument_outputs;
  pir::Type qkv_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_qkv.dtype()), dense_qkv.dims(), dense_qkv.layout(), dense_qkv.lod(), dense_qkv.offset());
  argument_outputs.push_back(qkv_dense_tensor_type);

  pir::Type qkv_max_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_qkv_max.dtype()), dense_qkv_max.dims(), dense_qkv_max.layout(), dense_qkv_max.lod(), dense_qkv_max.offset());
  argument_outputs.push_back(qkv_max_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void QkvAttentionXpuOp::VerifySig() {
  VLOG(4) << "Start Verifying inputs, outputs and attributes for: QkvAttentionXpuOp.";
  VLOG(4) << "Verifying inputs:";
  {
  auto input_size = num_operands();
  IR_ENFORCE(input_size == 6u,
                    "The size %d of inputs must be equal to 6.", input_size);
  IR_ENFORCE((*this)->operand_source(0).type().isa<paddle::dialect::DenseTensorType>(),
                  "Type validation failed for the 0th input, got %s.", (*this)->operand_source(0).type());
  IR_ENFORCE((*this)->operand_source(1).type().isa<paddle::dialect::DenseTensorType>(),
                  "Type validation failed for the 1th input, got %s.", (*this)->operand_source(1).type());
  IR_ENFORCE((*this)->operand_source(2).type().isa<paddle::dialect::DenseTensorType>(),
                  "Type validation failed for the 2th input, got %s.", (*this)->operand_source(2).type());
  if (auto val = (*this)->operand(3)) {
    IR_ENFORCE(val.type().isa<paddle::dialect::DenseTensorType>(),
                   "Type validation failed for the 3th input, got %s.", (*this)->operand_source(3).type());
  }
  if (auto val = (*this)->operand(4)) {
    IR_ENFORCE(val.type().isa<paddle::dialect::DenseTensorType>(),
                   "Type validation failed for the 4th input, got %s.", (*this)->operand_source(4).type());
  }
  if (auto val = (*this)->operand(5)) {
    IR_ENFORCE(val.type().isa<paddle::dialect::DenseTensorType>(),
                   "Type validation failed for the 5th input, got %s.", (*this)->operand_source(5).type());
  }
  }
  VLOG(4) << "Verifying attributes:";
  {
  auto& attributes = this->attributes();
  IR_ENFORCE(attributes.count("alpha")>0,
                 "alpha does not exist.");
  IR_ENFORCE(attributes.at("alpha").isa<pir::FloatAttribute>(),
                 "Type of attribute: alpha is not pir::FloatAttribute.");

  IR_ENFORCE(attributes.count("head_num")>0,
                 "head_num does not exist.");
  IR_ENFORCE(attributes.at("head_num").isa<pir::Int32Attribute>(),
                 "Type of attribute: head_num is not pir::Int32Attribute.");

  IR_ENFORCE(attributes.count("head_dim")>0,
                 "head_dim does not exist.");
  IR_ENFORCE(attributes.at("head_dim").isa<pir::Int32Attribute>(),
                 "Type of attribute: head_dim is not pir::Int32Attribute.");

  IR_ENFORCE(attributes.count("qkv_fc_fusion")>0,
                 "qkv_fc_fusion does not exist.");
  IR_ENFORCE(attributes.at("qkv_fc_fusion").isa<pir::BoolAttribute>(),
                 "Type of attribute: qkv_fc_fusion is not pir::BoolAttribute.");

  IR_ENFORCE(attributes.count("out_dtype")>0,
                 "out_dtype does not exist.");
  IR_ENFORCE(attributes.at("out_dtype").isa<paddle::dialect::DataTypeAttribute>(),
                 "Type of attribute: out_dtype is not paddle::dialect::DataTypeAttribute.");

  }
  VLOG(4) << "Verifying outputs:";
  {
  auto output_size = num_results();
  IR_ENFORCE(output_size == 2u,
                    "The size %d of outputs must be equal to 2.", output_size);
  IR_ENFORCE((*this)->result(0).type().isa<paddle::dialect::DenseTensorType>(),
                 "Type validation failed for the 0th output.");
  IR_ENFORCE((*this)->result(1).type().isa<paddle::dialect::DenseTensorType>(),
                 "Type validation failed for the 1th output.");
  }
  VLOG(4) << "End Verifying for: QkvAttentionXpuOp.";
}

void QkvAttentionXpuOp::InferMeta( phi::InferMetaContext *infer_meta ) {
  auto fn = PD_INFER_META(phi::QKVAttentionXPUInferMeta);
  fn(infer_meta);
}

phi::DataType QkvAttentionXpuOp::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: QkvAttentionXpuOp";
  


  return expected_kernel_dtype;
}

const char *QuantizeXpuOp::attributes_name[2] = { "out_dtype", "scale" };

OpInfoTuple QuantizeXpuOp::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("x", "paddle::dialect::DenseTensorType", false, false, false, false) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = { paddle::dialect::OpAttributeInfo("out_dtype", "paddle::dialect::DataTypeAttribute", ""), paddle::dialect::OpAttributeInfo("scale", "pir::FloatAttribute", "") };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("y", "paddle::dialect::DenseTensorType", false, false) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("QuantizeXPUInferMeta", {"x", "out_dtype", "scale"}, "quantize_xpu", {"x", "out_dtype", "scale"}, {"x"}, {}, {}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "quantize_xpu");
}

void QuantizeXpuOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, phi::DataType out_dtype, float scale) {
  VLOG(4) << "Start build QuantizeXpuOp";



  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_out_dtype = paddle::dialect::DataTypeAttribute::get(pir::IrContext::Instance(), out_dtype);
  argument.AddAttribute("out_dtype", attr_out_dtype);
  pir::Attribute attr_scale = pir::FloatAttribute::get(pir::IrContext::Instance(), scale);
  argument.AddAttribute("scale", attr_scale);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);
  paddle::dialect::IrTensor dense_y;
  paddle::dialect::IrMetaTensor meta_y(&dense_y);

  phi::QuantizeXPUInferMeta(meta_x, out_dtype, scale, &meta_y);

  std::vector<pir::Type> argument_outputs;
  pir::Type y_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_y.dtype()), dense_y.dims(), dense_y.layout(), dense_y.lod(), dense_y.offset());
  argument_outputs.push_back(y_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void QuantizeXpuOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, pir::AttributeMap attributes) {
  VLOG(4) << "Start build QuantizeXpuOp";


  IR_ENFORCE(
      attributes.find("out_dtype") != attributes.end(),
          "'out_dtype' Attribute is expected for QuantizeXpuOp. ");
  phi::DataType out_dtype = attributes.at("out_dtype").dyn_cast<paddle::dialect::DataTypeAttribute>().data();

  IR_ENFORCE(
      attributes.find("scale") != attributes.end(),
          "'scale' Attribute is expected for QuantizeXpuOp. ");
  float scale = attributes.at("scale").dyn_cast<pir::FloatAttribute>().data();


  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_out_dtype = paddle::dialect::DataTypeAttribute::get(pir::IrContext::Instance(), out_dtype);
  argument.AddAttribute("out_dtype", attr_out_dtype);
  pir::Attribute attr_scale = pir::FloatAttribute::get(pir::IrContext::Instance(), scale);
  argument.AddAttribute("scale", attr_scale);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);
  paddle::dialect::IrTensor dense_y;
  paddle::dialect::IrMetaTensor meta_y(&dense_y);

  phi::QuantizeXPUInferMeta(meta_x, out_dtype, scale, &meta_y);

  std::vector<pir::Type> argument_outputs;
  pir::Type y_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_y.dtype()), dense_y.dims(), dense_y.layout(), dense_y.lod(), dense_y.offset());
  argument_outputs.push_back(y_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void QuantizeXpuOp::VerifySig() {
  VLOG(4) << "Start Verifying inputs, outputs and attributes for: QuantizeXpuOp.";
  VLOG(4) << "Verifying inputs:";
  {
  auto input_size = num_operands();
  IR_ENFORCE(input_size == 1u,
                    "The size %d of inputs must be equal to 1.", input_size);
  IR_ENFORCE((*this)->operand_source(0).type().isa<paddle::dialect::DenseTensorType>(),
                  "Type validation failed for the 0th input, got %s.", (*this)->operand_source(0).type());
  }
  VLOG(4) << "Verifying attributes:";
  {
  auto& attributes = this->attributes();
  IR_ENFORCE(attributes.count("out_dtype")>0,
                 "out_dtype does not exist.");
  IR_ENFORCE(attributes.at("out_dtype").isa<paddle::dialect::DataTypeAttribute>(),
                 "Type of attribute: out_dtype is not paddle::dialect::DataTypeAttribute.");

  IR_ENFORCE(attributes.count("scale")>0,
                 "scale does not exist.");
  IR_ENFORCE(attributes.at("scale").isa<pir::FloatAttribute>(),
                 "Type of attribute: scale is not pir::FloatAttribute.");

  }
  VLOG(4) << "Verifying outputs:";
  {
  auto output_size = num_results();
  IR_ENFORCE(output_size == 1u,
                    "The size %d of outputs must be equal to 1.", output_size);
  IR_ENFORCE((*this)->result(0).type().isa<paddle::dialect::DenseTensorType>(),
                 "Type validation failed for the 0th output.");
  }
  VLOG(4) << "End Verifying for: QuantizeXpuOp.";
}

void QuantizeXpuOp::InferMeta( phi::InferMetaContext *infer_meta ) {
  auto fn = PD_INFER_META(phi::QuantizeXPUInferMeta);
  fn(infer_meta);
}

phi::DataType QuantizeXpuOp::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: QuantizeXpuOp";
  


  return expected_kernel_dtype;
}

const char *SelfDpAttentionOp::attributes_name[2] = { "alpha", "head_number" };

OpInfoTuple SelfDpAttentionOp::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("x", "paddle::dialect::DenseTensorType", false, false, false, false) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = { paddle::dialect::OpAttributeInfo("alpha", "pir::FloatAttribute", ""), paddle::dialect::OpAttributeInfo("head_number", "pir::Int32Attribute", "") };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("out", "paddle::dialect::DenseTensorType", false, false) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("SelfDPAttenInferMeta", {"x", "alpha", "head_number"}, "self_dp_attention", {"x", "alpha", "head_number"}, {"x"}, {}, {}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "self_dp_attention");
}

void SelfDpAttentionOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, float alpha, int head_number) {
  VLOG(4) << "Start build SelfDpAttentionOp";



  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_alpha = pir::FloatAttribute::get(pir::IrContext::Instance(), alpha);
  argument.AddAttribute("alpha", attr_alpha);
  pir::Attribute attr_head_number = pir::Int32Attribute::get(pir::IrContext::Instance(), head_number);
  argument.AddAttribute("head_number", attr_head_number);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);
  paddle::dialect::IrTensor dense_out;
  paddle::dialect::IrMetaTensor meta_out(&dense_out);

  phi::SelfDPAttenInferMeta(meta_x, alpha, head_number, &meta_out);

  std::vector<pir::Type> argument_outputs;
  pir::Type out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_out.dtype()), dense_out.dims(), dense_out.layout(), dense_out.lod(), dense_out.offset());
  argument_outputs.push_back(out_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void SelfDpAttentionOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, pir::AttributeMap attributes) {
  VLOG(4) << "Start build SelfDpAttentionOp";


  IR_ENFORCE(
      attributes.find("alpha") != attributes.end(),
          "'alpha' Attribute is expected for SelfDpAttentionOp. ");
  float alpha = attributes.at("alpha").dyn_cast<pir::FloatAttribute>().data();

  IR_ENFORCE(
      attributes.find("head_number") != attributes.end(),
          "'head_number' Attribute is expected for SelfDpAttentionOp. ");
  int head_number = attributes.at("head_number").dyn_cast<pir::Int32Attribute>().data();


  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_alpha = pir::FloatAttribute::get(pir::IrContext::Instance(), alpha);
  argument.AddAttribute("alpha", attr_alpha);
  pir::Attribute attr_head_number = pir::Int32Attribute::get(pir::IrContext::Instance(), head_number);
  argument.AddAttribute("head_number", attr_head_number);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);
  paddle::dialect::IrTensor dense_out;
  paddle::dialect::IrMetaTensor meta_out(&dense_out);

  phi::SelfDPAttenInferMeta(meta_x, alpha, head_number, &meta_out);

  std::vector<pir::Type> argument_outputs;
  pir::Type out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_out.dtype()), dense_out.dims(), dense_out.layout(), dense_out.lod(), dense_out.offset());
  argument_outputs.push_back(out_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void SelfDpAttentionOp::VerifySig() {
  VLOG(4) << "Start Verifying inputs, outputs and attributes for: SelfDpAttentionOp.";
  VLOG(4) << "Verifying inputs:";
  {
  auto input_size = num_operands();
  IR_ENFORCE(input_size == 1u,
                    "The size %d of inputs must be equal to 1.", input_size);
  IR_ENFORCE((*this)->operand_source(0).type().isa<paddle::dialect::DenseTensorType>(),
                  "Type validation failed for the 0th input, got %s.", (*this)->operand_source(0).type());
  }
  VLOG(4) << "Verifying attributes:";
  {
  auto& attributes = this->attributes();
  IR_ENFORCE(attributes.count("alpha")>0,
                 "alpha does not exist.");
  IR_ENFORCE(attributes.at("alpha").isa<pir::FloatAttribute>(),
                 "Type of attribute: alpha is not pir::FloatAttribute.");

  IR_ENFORCE(attributes.count("head_number")>0,
                 "head_number does not exist.");
  IR_ENFORCE(attributes.at("head_number").isa<pir::Int32Attribute>(),
                 "Type of attribute: head_number is not pir::Int32Attribute.");

  }
  VLOG(4) << "Verifying outputs:";
  {
  auto output_size = num_results();
  IR_ENFORCE(output_size == 1u,
                    "The size %d of outputs must be equal to 1.", output_size);
  IR_ENFORCE((*this)->result(0).type().isa<paddle::dialect::DenseTensorType>(),
                 "Type validation failed for the 0th output.");
  }
  VLOG(4) << "End Verifying for: SelfDpAttentionOp.";
}

void SelfDpAttentionOp::InferMeta( phi::InferMetaContext *infer_meta ) {
  auto fn = PD_INFER_META(phi::SelfDPAttenInferMeta);
  fn(infer_meta);
}

phi::DataType SelfDpAttentionOp::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: SelfDpAttentionOp";
  


  return expected_kernel_dtype;
}

OpInfoTuple SinePosXpuOp::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("x", "paddle::dialect::DenseTensorType", false, false, false, false), paddle::dialect::OpInputInfo("y", "paddle::dialect::DenseTensorType", false, false, false, false) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = {  };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("out", "paddle::dialect::DenseTensorType", false, false) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("SinePosXPUInferMeta", {"x", "y"}, "sine_pos_xpu", {"x", "y"}, {"x"}, {}, {}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "sine_pos_xpu");
}

void SinePosXpuOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, pir::Value y_) {
  VLOG(4) << "Start build SinePosXpuOp";



  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_, y_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;
  paddle::dialect::DenseTensorType y = y_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)y;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);

  VLOG(4) << "Builder construction  dense_y";
  paddle::dialect::IrTensor ir_tensor_y(paddle::dialect::TransToPhiDataType(y.dtype()),
                                                      y.dims(),
                                                      y.data_layout(),
                                                      y.lod(),
                                                      y.offset());
  VLOG(4) << "Builder construction  meta_y";
  paddle::dialect::IrMetaTensor meta_y(&ir_tensor_y);
  paddle::dialect::IrTensor dense_out;
  paddle::dialect::IrMetaTensor meta_out(&dense_out);

  phi::SinePosXPUInferMeta(meta_x, meta_y, &meta_out);

  std::vector<pir::Type> argument_outputs;
  pir::Type out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_out.dtype()), dense_out.dims(), dense_out.layout(), dense_out.lod(), dense_out.offset());
  argument_outputs.push_back(out_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void SinePosXpuOp::VerifySig() {
  VLOG(4) << "Start Verifying inputs, outputs and attributes for: SinePosXpuOp.";
  VLOG(4) << "Verifying inputs:";
  {
  auto input_size = num_operands();
  IR_ENFORCE(input_size == 2u,
                    "The size %d of inputs must be equal to 2.", input_size);
  IR_ENFORCE((*this)->operand_source(0).type().isa<paddle::dialect::DenseTensorType>(),
                  "Type validation failed for the 0th input, got %s.", (*this)->operand_source(0).type());
  IR_ENFORCE((*this)->operand_source(1).type().isa<paddle::dialect::DenseTensorType>(),
                  "Type validation failed for the 1th input, got %s.", (*this)->operand_source(1).type());
  }
  VLOG(4) << "Verifying attributes:";
  {
  // Attributes num is 0, not need to check attributes type.
  }
  VLOG(4) << "Verifying outputs:";
  {
  auto output_size = num_results();
  IR_ENFORCE(output_size == 1u,
                    "The size %d of outputs must be equal to 1.", output_size);
  IR_ENFORCE((*this)->result(0).type().isa<paddle::dialect::DenseTensorType>(),
                 "Type validation failed for the 0th output.");
  }
  VLOG(4) << "End Verifying for: SinePosXpuOp.";
}

void SinePosXpuOp::InferMeta( phi::InferMetaContext *infer_meta ) {
  auto fn = PD_INFER_META(phi::SinePosXPUInferMeta);
  fn(infer_meta);
}

phi::DataType SinePosXpuOp::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: SinePosXpuOp";
  


  return expected_kernel_dtype;
}

const char *SkipLayernormOp::attributes_name[2] = { "epsilon", "begin_norm_axis" };

OpInfoTuple SkipLayernormOp::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("x", "paddle::dialect::DenseTensorType", false, false, false, false), paddle::dialect::OpInputInfo("y", "paddle::dialect::DenseTensorType", false, false, false, false), paddle::dialect::OpInputInfo("scale", "paddle::dialect::DenseTensorType", false, false, false, false), paddle::dialect::OpInputInfo("bias", "paddle::dialect::DenseTensorType", false, false, false, false) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = { paddle::dialect::OpAttributeInfo("epsilon", "pir::FloatAttribute", ""), paddle::dialect::OpAttributeInfo("begin_norm_axis", "pir::Int32Attribute", "") };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("out", "paddle::dialect::DenseTensorType", false, false) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("SkipLayerNormInferMeta", {"x", "y", "scale", "bias", "epsilon", "begin_norm_axis"}, "skip_layernorm", {"x", "y", "scale", "bias", "epsilon", "begin_norm_axis"}, {"x"}, {}, {}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "skip_layernorm");
}

void SkipLayernormOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, pir::Value y_, pir::Value scale_, pir::Value bias_, float epsilon, int begin_norm_axis) {
  VLOG(4) << "Start build SkipLayernormOp";



  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_, y_, scale_, bias_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_epsilon = pir::FloatAttribute::get(pir::IrContext::Instance(), epsilon);
  argument.AddAttribute("epsilon", attr_epsilon);
  pir::Attribute attr_begin_norm_axis = pir::Int32Attribute::get(pir::IrContext::Instance(), begin_norm_axis);
  argument.AddAttribute("begin_norm_axis", attr_begin_norm_axis);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;
  paddle::dialect::DenseTensorType y = y_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)y;
  paddle::dialect::DenseTensorType scale = scale_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)scale;
  paddle::dialect::DenseTensorType bias = bias_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)bias;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);

  VLOG(4) << "Builder construction  dense_y";
  paddle::dialect::IrTensor ir_tensor_y(paddle::dialect::TransToPhiDataType(y.dtype()),
                                                      y.dims(),
                                                      y.data_layout(),
                                                      y.lod(),
                                                      y.offset());
  VLOG(4) << "Builder construction  meta_y";
  paddle::dialect::IrMetaTensor meta_y(&ir_tensor_y);

  VLOG(4) << "Builder construction  dense_scale";
  paddle::dialect::IrTensor ir_tensor_scale(paddle::dialect::TransToPhiDataType(scale.dtype()),
                                                      scale.dims(),
                                                      scale.data_layout(),
                                                      scale.lod(),
                                                      scale.offset());
  VLOG(4) << "Builder construction  meta_scale";
  paddle::dialect::IrMetaTensor meta_scale(&ir_tensor_scale);

  VLOG(4) << "Builder construction  dense_bias";
  paddle::dialect::IrTensor ir_tensor_bias(paddle::dialect::TransToPhiDataType(bias.dtype()),
                                                      bias.dims(),
                                                      bias.data_layout(),
                                                      bias.lod(),
                                                      bias.offset());
  VLOG(4) << "Builder construction  meta_bias";
  paddle::dialect::IrMetaTensor meta_bias(&ir_tensor_bias);
  paddle::dialect::IrTensor dense_out;
  paddle::dialect::IrMetaTensor meta_out(&dense_out);

  phi::SkipLayerNormInferMeta(meta_x, meta_y, meta_scale, meta_bias, epsilon, begin_norm_axis, &meta_out);

  std::vector<pir::Type> argument_outputs;
  pir::Type out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_out.dtype()), dense_out.dims(), dense_out.layout(), dense_out.lod(), dense_out.offset());
  argument_outputs.push_back(out_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void SkipLayernormOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, pir::Value y_, pir::Value scale_, pir::Value bias_, pir::AttributeMap attributes) {
  VLOG(4) << "Start build SkipLayernormOp";


  IR_ENFORCE(
      attributes.find("epsilon") != attributes.end(),
          "'epsilon' Attribute is expected for SkipLayernormOp. ");
  float epsilon = attributes.at("epsilon").dyn_cast<pir::FloatAttribute>().data();

  IR_ENFORCE(
      attributes.find("begin_norm_axis") != attributes.end(),
          "'begin_norm_axis' Attribute is expected for SkipLayernormOp. ");
  int begin_norm_axis = attributes.at("begin_norm_axis").dyn_cast<pir::Int32Attribute>().data();


  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_, y_, scale_, bias_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_epsilon = pir::FloatAttribute::get(pir::IrContext::Instance(), epsilon);
  argument.AddAttribute("epsilon", attr_epsilon);
  pir::Attribute attr_begin_norm_axis = pir::Int32Attribute::get(pir::IrContext::Instance(), begin_norm_axis);
  argument.AddAttribute("begin_norm_axis", attr_begin_norm_axis);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;
  paddle::dialect::DenseTensorType y = y_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)y;
  paddle::dialect::DenseTensorType scale = scale_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)scale;
  paddle::dialect::DenseTensorType bias = bias_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)bias;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);

  VLOG(4) << "Builder construction  dense_y";
  paddle::dialect::IrTensor ir_tensor_y(paddle::dialect::TransToPhiDataType(y.dtype()),
                                                      y.dims(),
                                                      y.data_layout(),
                                                      y.lod(),
                                                      y.offset());
  VLOG(4) << "Builder construction  meta_y";
  paddle::dialect::IrMetaTensor meta_y(&ir_tensor_y);

  VLOG(4) << "Builder construction  dense_scale";
  paddle::dialect::IrTensor ir_tensor_scale(paddle::dialect::TransToPhiDataType(scale.dtype()),
                                                      scale.dims(),
                                                      scale.data_layout(),
                                                      scale.lod(),
                                                      scale.offset());
  VLOG(4) << "Builder construction  meta_scale";
  paddle::dialect::IrMetaTensor meta_scale(&ir_tensor_scale);

  VLOG(4) << "Builder construction  dense_bias";
  paddle::dialect::IrTensor ir_tensor_bias(paddle::dialect::TransToPhiDataType(bias.dtype()),
                                                      bias.dims(),
                                                      bias.data_layout(),
                                                      bias.lod(),
                                                      bias.offset());
  VLOG(4) << "Builder construction  meta_bias";
  paddle::dialect::IrMetaTensor meta_bias(&ir_tensor_bias);
  paddle::dialect::IrTensor dense_out;
  paddle::dialect::IrMetaTensor meta_out(&dense_out);

  phi::SkipLayerNormInferMeta(meta_x, meta_y, meta_scale, meta_bias, epsilon, begin_norm_axis, &meta_out);

  std::vector<pir::Type> argument_outputs;
  pir::Type out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_out.dtype()), dense_out.dims(), dense_out.layout(), dense_out.lod(), dense_out.offset());
  argument_outputs.push_back(out_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void SkipLayernormOp::VerifySig() {
  VLOG(4) << "Start Verifying inputs, outputs and attributes for: SkipLayernormOp.";
  VLOG(4) << "Verifying inputs:";
  {
  auto input_size = num_operands();
  IR_ENFORCE(input_size == 4u,
                    "The size %d of inputs must be equal to 4.", input_size);
  IR_ENFORCE((*this)->operand_source(0).type().isa<paddle::dialect::DenseTensorType>(),
                  "Type validation failed for the 0th input, got %s.", (*this)->operand_source(0).type());
  IR_ENFORCE((*this)->operand_source(1).type().isa<paddle::dialect::DenseTensorType>(),
                  "Type validation failed for the 1th input, got %s.", (*this)->operand_source(1).type());
  IR_ENFORCE((*this)->operand_source(2).type().isa<paddle::dialect::DenseTensorType>(),
                  "Type validation failed for the 2th input, got %s.", (*this)->operand_source(2).type());
  IR_ENFORCE((*this)->operand_source(3).type().isa<paddle::dialect::DenseTensorType>(),
                  "Type validation failed for the 3th input, got %s.", (*this)->operand_source(3).type());
  }
  VLOG(4) << "Verifying attributes:";
  {
  auto& attributes = this->attributes();
  IR_ENFORCE(attributes.count("epsilon")>0,
                 "epsilon does not exist.");
  IR_ENFORCE(attributes.at("epsilon").isa<pir::FloatAttribute>(),
                 "Type of attribute: epsilon is not pir::FloatAttribute.");

  IR_ENFORCE(attributes.count("begin_norm_axis")>0,
                 "begin_norm_axis does not exist.");
  IR_ENFORCE(attributes.at("begin_norm_axis").isa<pir::Int32Attribute>(),
                 "Type of attribute: begin_norm_axis is not pir::Int32Attribute.");

  }
  VLOG(4) << "Verifying outputs:";
  {
  auto output_size = num_results();
  IR_ENFORCE(output_size == 1u,
                    "The size %d of outputs must be equal to 1.", output_size);
  IR_ENFORCE((*this)->result(0).type().isa<paddle::dialect::DenseTensorType>(),
                 "Type validation failed for the 0th output.");
  }
  VLOG(4) << "End Verifying for: SkipLayernormOp.";
}

void SkipLayernormOp::InferMeta( phi::InferMetaContext *infer_meta ) {
  auto fn = PD_INFER_META(phi::SkipLayerNormInferMeta);
  fn(infer_meta);
}

phi::DataType SkipLayernormOp::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: SkipLayernormOp";
  


  return expected_kernel_dtype;
}

const char *SqueezeExcitationBlockOp::attributes_name[3] = { "act_type", "act_param", "filter_dims" };

OpInfoTuple SqueezeExcitationBlockOp::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("x", "paddle::dialect::DenseTensorType", false, false, false, false), paddle::dialect::OpInputInfo("filter", "paddle::dialect::DenseTensorType", false, false, false, false), paddle::dialect::OpInputInfo("filter_max", "paddle::dialect::DenseTensorType", false, false, false, false), paddle::dialect::OpInputInfo("bias", "paddle::dialect::DenseTensorType", true, false, false, false), paddle::dialect::OpInputInfo("branch", "paddle::dialect::DenseTensorType", true, false, false, false) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = { paddle::dialect::OpAttributeInfo("act_type", "pir::ArrayAttribute<pir::Int32Attribute>", ""), paddle::dialect::OpAttributeInfo("act_param", "pir::ArrayAttribute<pir::FloatAttribute>", ""), paddle::dialect::OpAttributeInfo("filter_dims", "pir::ArrayAttribute<pir::Int32Attribute>", "") };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("out", "paddle::dialect::DenseTensorType", false, false) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("SqueezeExcitationInferMeta", {"x", "filter", "filter_max", "bias", "branch", "act_type", "act_param", "filter_dims"}, "squeeze_excitation_block", {"x", "filter", "filter_max", "bias", "branch", "act_type", "act_param", "filter_dims"}, {"x"}, {}, {}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "squeeze_excitation_block");
}

void SqueezeExcitationBlockOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, pir::Value filter_, pir::Value filter_max_, pir::Value bias_, pir::Value branch_, const std::vector<int>& act_type, const std::vector<float>& act_param, const std::vector<int>& filter_dims) {
  VLOG(4) << "Start build SqueezeExcitationBlockOp";



  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_, filter_, filter_max_, bias_, branch_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  std::vector<pir::Attribute> vec_act_type;
  for (size_t i = 0; i < static_cast<size_t>(act_type.size()); i++) {
      pir::Attribute attr_act_type = pir::Int32Attribute::get(pir::IrContext::Instance(), act_type[i]);

    vec_act_type.push_back(attr_act_type);
  }
  pir::Attribute attr_act_type = pir::ArrayAttribute::get(pir::IrContext::Instance(), vec_act_type);
  argument.AddAttribute("act_type", attr_act_type);
  std::vector<pir::Attribute> vec_act_param;
  for (size_t i = 0; i < static_cast<size_t>(act_param.size()); i++) {
      pir::Attribute attr_act_param = pir::FloatAttribute::get(pir::IrContext::Instance(), act_param[i]);

    vec_act_param.push_back(attr_act_param);
  }
  pir::Attribute attr_act_param = pir::ArrayAttribute::get(pir::IrContext::Instance(), vec_act_param);
  argument.AddAttribute("act_param", attr_act_param);
  std::vector<pir::Attribute> vec_filter_dims;
  for (size_t i = 0; i < static_cast<size_t>(filter_dims.size()); i++) {
      pir::Attribute attr_filter_dims = pir::Int32Attribute::get(pir::IrContext::Instance(), filter_dims[i]);

    vec_filter_dims.push_back(attr_filter_dims);
  }
  pir::Attribute attr_filter_dims = pir::ArrayAttribute::get(pir::IrContext::Instance(), vec_filter_dims);
  argument.AddAttribute("filter_dims", attr_filter_dims);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;
  paddle::dialect::DenseTensorType filter = filter_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)filter;
  paddle::dialect::DenseTensorType filter_max = filter_max_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)filter_max;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);

  VLOG(4) << "Builder construction  dense_filter";
  paddle::dialect::IrTensor ir_tensor_filter(paddle::dialect::TransToPhiDataType(filter.dtype()),
                                                      filter.dims(),
                                                      filter.data_layout(),
                                                      filter.lod(),
                                                      filter.offset());
  VLOG(4) << "Builder construction  meta_filter";
  paddle::dialect::IrMetaTensor meta_filter(&ir_tensor_filter);

  VLOG(4) << "Builder construction  dense_filter_max";
  paddle::dialect::IrTensor ir_tensor_filter_max(paddle::dialect::TransToPhiDataType(filter_max.dtype()),
                                                      filter_max.dims(),
                                                      filter_max.data_layout(),
                                                      filter_max.lod(),
                                                      filter_max.offset());
  VLOG(4) << "Builder construction  meta_filter_max";
  paddle::dialect::IrMetaTensor meta_filter_max(&ir_tensor_filter_max);

  paddle::dialect::IrMetaTensor meta_bias;
  paddle::dialect::IrTensor ir_tensor_bias;
  if (bias_.impl() != nullptr) {
    paddle::dialect::DenseTensorType bias = bias_.type().dyn_cast<paddle::dialect::DenseTensorType>();
    VLOG(4) << "Builder construction  dense_bias";
    ir_tensor_bias = paddle::dialect::IrTensor(paddle::dialect::TransToPhiDataType(bias.dtype()),
                                                        bias.dims(),
                                                        bias.data_layout(),
                                                        bias.lod(),
                                                        bias.offset());
    VLOG(4) << "Builder construction  meta_bias";
    meta_bias = paddle::dialect::IrMetaTensor(&ir_tensor_bias);
  }


  paddle::dialect::IrMetaTensor meta_branch;
  paddle::dialect::IrTensor ir_tensor_branch;
  if (branch_.impl() != nullptr) {
    paddle::dialect::DenseTensorType branch = branch_.type().dyn_cast<paddle::dialect::DenseTensorType>();
    VLOG(4) << "Builder construction  dense_branch";
    ir_tensor_branch = paddle::dialect::IrTensor(paddle::dialect::TransToPhiDataType(branch.dtype()),
                                                        branch.dims(),
                                                        branch.data_layout(),
                                                        branch.lod(),
                                                        branch.offset());
    VLOG(4) << "Builder construction  meta_branch";
    meta_branch = paddle::dialect::IrMetaTensor(&ir_tensor_branch);
  }

  paddle::dialect::IrTensor dense_out;
  paddle::dialect::IrMetaTensor meta_out(&dense_out);

  phi::SqueezeExcitationInferMeta(meta_x, meta_filter, meta_filter_max, meta_bias, meta_branch, act_type, act_param, filter_dims, &meta_out);

  std::vector<pir::Type> argument_outputs;
  pir::Type out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_out.dtype()), dense_out.dims(), dense_out.layout(), dense_out.lod(), dense_out.offset());
  argument_outputs.push_back(out_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void SqueezeExcitationBlockOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, pir::Value filter_, pir::Value filter_max_, pir::Value bias_, pir::Value branch_, pir::AttributeMap attributes) {
  VLOG(4) << "Start build SqueezeExcitationBlockOp";


  IR_ENFORCE(
      attributes.find("act_type") != attributes.end(),
          "'act_type' Attribute is expected for SqueezeExcitationBlockOp. ");
  std::vector<int> act_type;
  for (size_t i = 0; i < attributes.at("act_type").dyn_cast<pir::ArrayAttribute>().size(); i++) {
    act_type.push_back(attributes.at("act_type").dyn_cast<pir::ArrayAttribute>().at(i).dyn_cast<pir::Int32Attribute>().data());
  }

  IR_ENFORCE(
      attributes.find("act_param") != attributes.end(),
          "'act_param' Attribute is expected for SqueezeExcitationBlockOp. ");
  std::vector<float> act_param;
  for (size_t i = 0; i < attributes.at("act_param").dyn_cast<pir::ArrayAttribute>().size(); i++) {
    act_param.push_back(attributes.at("act_param").dyn_cast<pir::ArrayAttribute>().at(i).dyn_cast<pir::FloatAttribute>().data());
  }

  IR_ENFORCE(
      attributes.find("filter_dims") != attributes.end(),
          "'filter_dims' Attribute is expected for SqueezeExcitationBlockOp. ");
  std::vector<int> filter_dims;
  for (size_t i = 0; i < attributes.at("filter_dims").dyn_cast<pir::ArrayAttribute>().size(); i++) {
    filter_dims.push_back(attributes.at("filter_dims").dyn_cast<pir::ArrayAttribute>().at(i).dyn_cast<pir::Int32Attribute>().data());
  }


  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_, filter_, filter_max_, bias_, branch_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  std::vector<pir::Attribute> vec_act_type;
  for (size_t i = 0; i < static_cast<size_t>(act_type.size()); i++) {
      pir::Attribute attr_act_type = pir::Int32Attribute::get(pir::IrContext::Instance(), act_type[i]);

    vec_act_type.push_back(attr_act_type);
  }
  pir::Attribute attr_act_type = pir::ArrayAttribute::get(pir::IrContext::Instance(), vec_act_type);
  argument.AddAttribute("act_type", attr_act_type);
  std::vector<pir::Attribute> vec_act_param;
  for (size_t i = 0; i < static_cast<size_t>(act_param.size()); i++) {
      pir::Attribute attr_act_param = pir::FloatAttribute::get(pir::IrContext::Instance(), act_param[i]);

    vec_act_param.push_back(attr_act_param);
  }
  pir::Attribute attr_act_param = pir::ArrayAttribute::get(pir::IrContext::Instance(), vec_act_param);
  argument.AddAttribute("act_param", attr_act_param);
  std::vector<pir::Attribute> vec_filter_dims;
  for (size_t i = 0; i < static_cast<size_t>(filter_dims.size()); i++) {
      pir::Attribute attr_filter_dims = pir::Int32Attribute::get(pir::IrContext::Instance(), filter_dims[i]);

    vec_filter_dims.push_back(attr_filter_dims);
  }
  pir::Attribute attr_filter_dims = pir::ArrayAttribute::get(pir::IrContext::Instance(), vec_filter_dims);
  argument.AddAttribute("filter_dims", attr_filter_dims);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;
  paddle::dialect::DenseTensorType filter = filter_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)filter;
  paddle::dialect::DenseTensorType filter_max = filter_max_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)filter_max;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);

  VLOG(4) << "Builder construction  dense_filter";
  paddle::dialect::IrTensor ir_tensor_filter(paddle::dialect::TransToPhiDataType(filter.dtype()),
                                                      filter.dims(),
                                                      filter.data_layout(),
                                                      filter.lod(),
                                                      filter.offset());
  VLOG(4) << "Builder construction  meta_filter";
  paddle::dialect::IrMetaTensor meta_filter(&ir_tensor_filter);

  VLOG(4) << "Builder construction  dense_filter_max";
  paddle::dialect::IrTensor ir_tensor_filter_max(paddle::dialect::TransToPhiDataType(filter_max.dtype()),
                                                      filter_max.dims(),
                                                      filter_max.data_layout(),
                                                      filter_max.lod(),
                                                      filter_max.offset());
  VLOG(4) << "Builder construction  meta_filter_max";
  paddle::dialect::IrMetaTensor meta_filter_max(&ir_tensor_filter_max);

  paddle::dialect::IrMetaTensor meta_bias;
  paddle::dialect::IrTensor ir_tensor_bias;
  if (bias_.impl() != nullptr) {
    paddle::dialect::DenseTensorType bias = bias_.type().dyn_cast<paddle::dialect::DenseTensorType>();
    VLOG(4) << "Builder construction  dense_bias";
    ir_tensor_bias = paddle::dialect::IrTensor(paddle::dialect::TransToPhiDataType(bias.dtype()),
                                                        bias.dims(),
                                                        bias.data_layout(),
                                                        bias.lod(),
                                                        bias.offset());
    VLOG(4) << "Builder construction  meta_bias";
    meta_bias = paddle::dialect::IrMetaTensor(&ir_tensor_bias);
  }


  paddle::dialect::IrMetaTensor meta_branch;
  paddle::dialect::IrTensor ir_tensor_branch;
  if (branch_.impl() != nullptr) {
    paddle::dialect::DenseTensorType branch = branch_.type().dyn_cast<paddle::dialect::DenseTensorType>();
    VLOG(4) << "Builder construction  dense_branch";
    ir_tensor_branch = paddle::dialect::IrTensor(paddle::dialect::TransToPhiDataType(branch.dtype()),
                                                        branch.dims(),
                                                        branch.data_layout(),
                                                        branch.lod(),
                                                        branch.offset());
    VLOG(4) << "Builder construction  meta_branch";
    meta_branch = paddle::dialect::IrMetaTensor(&ir_tensor_branch);
  }

  paddle::dialect::IrTensor dense_out;
  paddle::dialect::IrMetaTensor meta_out(&dense_out);

  phi::SqueezeExcitationInferMeta(meta_x, meta_filter, meta_filter_max, meta_bias, meta_branch, act_type, act_param, filter_dims, &meta_out);

  std::vector<pir::Type> argument_outputs;
  pir::Type out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_out.dtype()), dense_out.dims(), dense_out.layout(), dense_out.lod(), dense_out.offset());
  argument_outputs.push_back(out_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void SqueezeExcitationBlockOp::VerifySig() {
  VLOG(4) << "Start Verifying inputs, outputs and attributes for: SqueezeExcitationBlockOp.";
  VLOG(4) << "Verifying inputs:";
  {
  auto input_size = num_operands();
  IR_ENFORCE(input_size == 5u,
                    "The size %d of inputs must be equal to 5.", input_size);
  IR_ENFORCE((*this)->operand_source(0).type().isa<paddle::dialect::DenseTensorType>(),
                  "Type validation failed for the 0th input, got %s.", (*this)->operand_source(0).type());
  IR_ENFORCE((*this)->operand_source(1).type().isa<paddle::dialect::DenseTensorType>(),
                  "Type validation failed for the 1th input, got %s.", (*this)->operand_source(1).type());
  IR_ENFORCE((*this)->operand_source(2).type().isa<paddle::dialect::DenseTensorType>(),
                  "Type validation failed for the 2th input, got %s.", (*this)->operand_source(2).type());
  if (auto val = (*this)->operand(3)) {
    IR_ENFORCE(val.type().isa<paddle::dialect::DenseTensorType>(),
                   "Type validation failed for the 3th input, got %s.", (*this)->operand_source(3).type());
  }
  if (auto val = (*this)->operand(4)) {
    IR_ENFORCE(val.type().isa<paddle::dialect::DenseTensorType>(),
                   "Type validation failed for the 4th input, got %s.", (*this)->operand_source(4).type());
  }
  }
  VLOG(4) << "Verifying attributes:";
  {
  auto& attributes = this->attributes();
  IR_ENFORCE(attributes.count("act_type")>0,
                 "act_type does not exist.");
  IR_ENFORCE(attributes.at("act_type").isa<pir::ArrayAttribute>(),
                 "Type of attribute: act_type is not pir::ArrayAttribute.");
  for (size_t i = 0; i < attributes.at("act_type").dyn_cast<pir::ArrayAttribute>().size(); i++) {
    IR_ENFORCE(attributes.at("act_type").dyn_cast<pir::ArrayAttribute>().at(i).isa<pir::Int32Attribute>(),
                   "Type of attribute: act_type is not right.");
  }
  IR_ENFORCE(attributes.count("act_param")>0,
                 "act_param does not exist.");
  IR_ENFORCE(attributes.at("act_param").isa<pir::ArrayAttribute>(),
                 "Type of attribute: act_param is not pir::ArrayAttribute.");
  for (size_t i = 0; i < attributes.at("act_param").dyn_cast<pir::ArrayAttribute>().size(); i++) {
    IR_ENFORCE(attributes.at("act_param").dyn_cast<pir::ArrayAttribute>().at(i).isa<pir::FloatAttribute>(),
                   "Type of attribute: act_param is not right.");
  }
  IR_ENFORCE(attributes.count("filter_dims")>0,
                 "filter_dims does not exist.");
  IR_ENFORCE(attributes.at("filter_dims").isa<pir::ArrayAttribute>(),
                 "Type of attribute: filter_dims is not pir::ArrayAttribute.");
  for (size_t i = 0; i < attributes.at("filter_dims").dyn_cast<pir::ArrayAttribute>().size(); i++) {
    IR_ENFORCE(attributes.at("filter_dims").dyn_cast<pir::ArrayAttribute>().at(i).isa<pir::Int32Attribute>(),
                   "Type of attribute: filter_dims is not right.");
  }
  }
  VLOG(4) << "Verifying outputs:";
  {
  auto output_size = num_results();
  IR_ENFORCE(output_size == 1u,
                    "The size %d of outputs must be equal to 1.", output_size);
  IR_ENFORCE((*this)->result(0).type().isa<paddle::dialect::DenseTensorType>(),
                 "Type validation failed for the 0th output.");
  }
  VLOG(4) << "End Verifying for: SqueezeExcitationBlockOp.";
}

void SqueezeExcitationBlockOp::InferMeta( phi::InferMetaContext *infer_meta ) {
  auto fn = PD_INFER_META(phi::SqueezeExcitationInferMeta);
  fn(infer_meta);
}

phi::DataType SqueezeExcitationBlockOp::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: SqueezeExcitationBlockOp";
  


  return expected_kernel_dtype;
}

const char *VariableLengthMemoryEfficientAttentionOp::attributes_name[3] = { "scale", "causal", "pre_cache_length" };

OpInfoTuple VariableLengthMemoryEfficientAttentionOp::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("query", "paddle::dialect::DenseTensorType", false, false, false, false), paddle::dialect::OpInputInfo("key", "paddle::dialect::DenseTensorType", false, false, false, false), paddle::dialect::OpInputInfo("value", "paddle::dialect::DenseTensorType", false, false, false, false), paddle::dialect::OpInputInfo("seq_lens", "paddle::dialect::DenseTensorType", false, false, false, false), paddle::dialect::OpInputInfo("kv_seq_lens", "paddle::dialect::DenseTensorType", false, false, false, false), paddle::dialect::OpInputInfo("mask", "paddle::dialect::DenseTensorType", true, false, false, false) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = { paddle::dialect::OpAttributeInfo("scale", "pir::FloatAttribute", ""), paddle::dialect::OpAttributeInfo("causal", "pir::BoolAttribute", ""), paddle::dialect::OpAttributeInfo("pre_cache_length", "pir::Int32Attribute", "") };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("out", "paddle::dialect::DenseTensorType", false, false) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("VariableLengthMemoryEfficientAttentionInferMeta", {"query", "key", "value", "seq_lens", "kv_seq_lens", "mask", "scale", "causal", "pre_cache_length"}, "variable_length_memory_efficient_attention", {"query", "key", "value", "seq_lens", "kv_seq_lens", "mask", "scale", "causal", "pre_cache_length"}, {"query"}, {}, {}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "variable_length_memory_efficient_attention");
}

void VariableLengthMemoryEfficientAttentionOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value query_, pir::Value key_, pir::Value value_, pir::Value seq_lens_, pir::Value kv_seq_lens_, pir::Value mask_, float scale, bool causal, int pre_cache_length) {
  VLOG(4) << "Start build VariableLengthMemoryEfficientAttentionOp";



  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {query_, key_, value_, seq_lens_, kv_seq_lens_, mask_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_scale = pir::FloatAttribute::get(pir::IrContext::Instance(), scale);
  argument.AddAttribute("scale", attr_scale);
  pir::Attribute attr_causal = pir::BoolAttribute::get(pir::IrContext::Instance(), causal);
  argument.AddAttribute("causal", attr_causal);
  pir::Attribute attr_pre_cache_length = pir::Int32Attribute::get(pir::IrContext::Instance(), pre_cache_length);
  argument.AddAttribute("pre_cache_length", attr_pre_cache_length);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType query = query_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)query;
  paddle::dialect::DenseTensorType key = key_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)key;
  paddle::dialect::DenseTensorType value = value_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)value;
  paddle::dialect::DenseTensorType seq_lens = seq_lens_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)seq_lens;
  paddle::dialect::DenseTensorType kv_seq_lens = kv_seq_lens_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)kv_seq_lens;

  VLOG(4) << "Builder construction  dense_query";
  paddle::dialect::IrTensor ir_tensor_query(paddle::dialect::TransToPhiDataType(query.dtype()),
                                                      query.dims(),
                                                      query.data_layout(),
                                                      query.lod(),
                                                      query.offset());
  VLOG(4) << "Builder construction  meta_query";
  paddle::dialect::IrMetaTensor meta_query(&ir_tensor_query);

  VLOG(4) << "Builder construction  dense_key";
  paddle::dialect::IrTensor ir_tensor_key(paddle::dialect::TransToPhiDataType(key.dtype()),
                                                      key.dims(),
                                                      key.data_layout(),
                                                      key.lod(),
                                                      key.offset());
  VLOG(4) << "Builder construction  meta_key";
  paddle::dialect::IrMetaTensor meta_key(&ir_tensor_key);

  VLOG(4) << "Builder construction  dense_value";
  paddle::dialect::IrTensor ir_tensor_value(paddle::dialect::TransToPhiDataType(value.dtype()),
                                                      value.dims(),
                                                      value.data_layout(),
                                                      value.lod(),
                                                      value.offset());
  VLOG(4) << "Builder construction  meta_value";
  paddle::dialect::IrMetaTensor meta_value(&ir_tensor_value);

  VLOG(4) << "Builder construction  dense_seq_lens";
  paddle::dialect::IrTensor ir_tensor_seq_lens(paddle::dialect::TransToPhiDataType(seq_lens.dtype()),
                                                      seq_lens.dims(),
                                                      seq_lens.data_layout(),
                                                      seq_lens.lod(),
                                                      seq_lens.offset());
  VLOG(4) << "Builder construction  meta_seq_lens";
  paddle::dialect::IrMetaTensor meta_seq_lens(&ir_tensor_seq_lens);

  VLOG(4) << "Builder construction  dense_kv_seq_lens";
  paddle::dialect::IrTensor ir_tensor_kv_seq_lens(paddle::dialect::TransToPhiDataType(kv_seq_lens.dtype()),
                                                      kv_seq_lens.dims(),
                                                      kv_seq_lens.data_layout(),
                                                      kv_seq_lens.lod(),
                                                      kv_seq_lens.offset());
  VLOG(4) << "Builder construction  meta_kv_seq_lens";
  paddle::dialect::IrMetaTensor meta_kv_seq_lens(&ir_tensor_kv_seq_lens);

  paddle::dialect::IrMetaTensor meta_mask;
  paddle::dialect::IrTensor ir_tensor_mask;
  if (mask_.impl() != nullptr) {
    paddle::dialect::DenseTensorType mask = mask_.type().dyn_cast<paddle::dialect::DenseTensorType>();
    VLOG(4) << "Builder construction  dense_mask";
    ir_tensor_mask = paddle::dialect::IrTensor(paddle::dialect::TransToPhiDataType(mask.dtype()),
                                                        mask.dims(),
                                                        mask.data_layout(),
                                                        mask.lod(),
                                                        mask.offset());
    VLOG(4) << "Builder construction  meta_mask";
    meta_mask = paddle::dialect::IrMetaTensor(&ir_tensor_mask);
  }

  paddle::dialect::IrTensor dense_out;
  paddle::dialect::IrMetaTensor meta_out(&dense_out);

  phi::VariableLengthMemoryEfficientAttentionInferMeta(meta_query, meta_key, meta_value, meta_seq_lens, meta_kv_seq_lens, meta_mask, scale, causal, pre_cache_length, &meta_out);

  std::vector<pir::Type> argument_outputs;
  pir::Type out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_out.dtype()), dense_out.dims(), dense_out.layout(), dense_out.lod(), dense_out.offset());
  argument_outputs.push_back(out_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void VariableLengthMemoryEfficientAttentionOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value query_, pir::Value key_, pir::Value value_, pir::Value seq_lens_, pir::Value kv_seq_lens_, pir::Value mask_, pir::AttributeMap attributes) {
  VLOG(4) << "Start build VariableLengthMemoryEfficientAttentionOp";


  IR_ENFORCE(
      attributes.find("scale") != attributes.end(),
          "'scale' Attribute is expected for VariableLengthMemoryEfficientAttentionOp. ");
  float scale = attributes.at("scale").dyn_cast<pir::FloatAttribute>().data();

  IR_ENFORCE(
      attributes.find("causal") != attributes.end(),
          "'causal' Attribute is expected for VariableLengthMemoryEfficientAttentionOp. ");
  bool causal = attributes.at("causal").dyn_cast<pir::BoolAttribute>().data();

  IR_ENFORCE(
      attributes.find("pre_cache_length") != attributes.end(),
          "'pre_cache_length' Attribute is expected for VariableLengthMemoryEfficientAttentionOp. ");
  int pre_cache_length = attributes.at("pre_cache_length").dyn_cast<pir::Int32Attribute>().data();


  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {query_, key_, value_, seq_lens_, kv_seq_lens_, mask_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_scale = pir::FloatAttribute::get(pir::IrContext::Instance(), scale);
  argument.AddAttribute("scale", attr_scale);
  pir::Attribute attr_causal = pir::BoolAttribute::get(pir::IrContext::Instance(), causal);
  argument.AddAttribute("causal", attr_causal);
  pir::Attribute attr_pre_cache_length = pir::Int32Attribute::get(pir::IrContext::Instance(), pre_cache_length);
  argument.AddAttribute("pre_cache_length", attr_pre_cache_length);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType query = query_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)query;
  paddle::dialect::DenseTensorType key = key_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)key;
  paddle::dialect::DenseTensorType value = value_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)value;
  paddle::dialect::DenseTensorType seq_lens = seq_lens_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)seq_lens;
  paddle::dialect::DenseTensorType kv_seq_lens = kv_seq_lens_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)kv_seq_lens;

  VLOG(4) << "Builder construction  dense_query";
  paddle::dialect::IrTensor ir_tensor_query(paddle::dialect::TransToPhiDataType(query.dtype()),
                                                      query.dims(),
                                                      query.data_layout(),
                                                      query.lod(),
                                                      query.offset());
  VLOG(4) << "Builder construction  meta_query";
  paddle::dialect::IrMetaTensor meta_query(&ir_tensor_query);

  VLOG(4) << "Builder construction  dense_key";
  paddle::dialect::IrTensor ir_tensor_key(paddle::dialect::TransToPhiDataType(key.dtype()),
                                                      key.dims(),
                                                      key.data_layout(),
                                                      key.lod(),
                                                      key.offset());
  VLOG(4) << "Builder construction  meta_key";
  paddle::dialect::IrMetaTensor meta_key(&ir_tensor_key);

  VLOG(4) << "Builder construction  dense_value";
  paddle::dialect::IrTensor ir_tensor_value(paddle::dialect::TransToPhiDataType(value.dtype()),
                                                      value.dims(),
                                                      value.data_layout(),
                                                      value.lod(),
                                                      value.offset());
  VLOG(4) << "Builder construction  meta_value";
  paddle::dialect::IrMetaTensor meta_value(&ir_tensor_value);

  VLOG(4) << "Builder construction  dense_seq_lens";
  paddle::dialect::IrTensor ir_tensor_seq_lens(paddle::dialect::TransToPhiDataType(seq_lens.dtype()),
                                                      seq_lens.dims(),
                                                      seq_lens.data_layout(),
                                                      seq_lens.lod(),
                                                      seq_lens.offset());
  VLOG(4) << "Builder construction  meta_seq_lens";
  paddle::dialect::IrMetaTensor meta_seq_lens(&ir_tensor_seq_lens);

  VLOG(4) << "Builder construction  dense_kv_seq_lens";
  paddle::dialect::IrTensor ir_tensor_kv_seq_lens(paddle::dialect::TransToPhiDataType(kv_seq_lens.dtype()),
                                                      kv_seq_lens.dims(),
                                                      kv_seq_lens.data_layout(),
                                                      kv_seq_lens.lod(),
                                                      kv_seq_lens.offset());
  VLOG(4) << "Builder construction  meta_kv_seq_lens";
  paddle::dialect::IrMetaTensor meta_kv_seq_lens(&ir_tensor_kv_seq_lens);

  paddle::dialect::IrMetaTensor meta_mask;
  paddle::dialect::IrTensor ir_tensor_mask;
  if (mask_.impl() != nullptr) {
    paddle::dialect::DenseTensorType mask = mask_.type().dyn_cast<paddle::dialect::DenseTensorType>();
    VLOG(4) << "Builder construction  dense_mask";
    ir_tensor_mask = paddle::dialect::IrTensor(paddle::dialect::TransToPhiDataType(mask.dtype()),
                                                        mask.dims(),
                                                        mask.data_layout(),
                                                        mask.lod(),
                                                        mask.offset());
    VLOG(4) << "Builder construction  meta_mask";
    meta_mask = paddle::dialect::IrMetaTensor(&ir_tensor_mask);
  }

  paddle::dialect::IrTensor dense_out;
  paddle::dialect::IrMetaTensor meta_out(&dense_out);

  phi::VariableLengthMemoryEfficientAttentionInferMeta(meta_query, meta_key, meta_value, meta_seq_lens, meta_kv_seq_lens, meta_mask, scale, causal, pre_cache_length, &meta_out);

  std::vector<pir::Type> argument_outputs;
  pir::Type out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_out.dtype()), dense_out.dims(), dense_out.layout(), dense_out.lod(), dense_out.offset());
  argument_outputs.push_back(out_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void VariableLengthMemoryEfficientAttentionOp::VerifySig() {
  VLOG(4) << "Start Verifying inputs, outputs and attributes for: VariableLengthMemoryEfficientAttentionOp.";
  VLOG(4) << "Verifying inputs:";
  {
  auto input_size = num_operands();
  IR_ENFORCE(input_size == 6u,
                    "The size %d of inputs must be equal to 6.", input_size);
  IR_ENFORCE((*this)->operand_source(0).type().isa<paddle::dialect::DenseTensorType>(),
                  "Type validation failed for the 0th input, got %s.", (*this)->operand_source(0).type());
  IR_ENFORCE((*this)->operand_source(1).type().isa<paddle::dialect::DenseTensorType>(),
                  "Type validation failed for the 1th input, got %s.", (*this)->operand_source(1).type());
  IR_ENFORCE((*this)->operand_source(2).type().isa<paddle::dialect::DenseTensorType>(),
                  "Type validation failed for the 2th input, got %s.", (*this)->operand_source(2).type());
  IR_ENFORCE((*this)->operand_source(3).type().isa<paddle::dialect::DenseTensorType>(),
                  "Type validation failed for the 3th input, got %s.", (*this)->operand_source(3).type());
  IR_ENFORCE((*this)->operand_source(4).type().isa<paddle::dialect::DenseTensorType>(),
                  "Type validation failed for the 4th input, got %s.", (*this)->operand_source(4).type());
  if (auto val = (*this)->operand(5)) {
    IR_ENFORCE(val.type().isa<paddle::dialect::DenseTensorType>(),
                   "Type validation failed for the 5th input, got %s.", (*this)->operand_source(5).type());
  }
  }
  VLOG(4) << "Verifying attributes:";
  {
  auto& attributes = this->attributes();
  IR_ENFORCE(attributes.count("scale")>0,
                 "scale does not exist.");
  IR_ENFORCE(attributes.at("scale").isa<pir::FloatAttribute>(),
                 "Type of attribute: scale is not pir::FloatAttribute.");

  IR_ENFORCE(attributes.count("causal")>0,
                 "causal does not exist.");
  IR_ENFORCE(attributes.at("causal").isa<pir::BoolAttribute>(),
                 "Type of attribute: causal is not pir::BoolAttribute.");

  IR_ENFORCE(attributes.count("pre_cache_length")>0,
                 "pre_cache_length does not exist.");
  IR_ENFORCE(attributes.at("pre_cache_length").isa<pir::Int32Attribute>(),
                 "Type of attribute: pre_cache_length is not pir::Int32Attribute.");

  }
  VLOG(4) << "Verifying outputs:";
  {
  auto output_size = num_results();
  IR_ENFORCE(output_size == 1u,
                    "The size %d of outputs must be equal to 1.", output_size);
  IR_ENFORCE((*this)->result(0).type().isa<paddle::dialect::DenseTensorType>(),
                 "Type validation failed for the 0th output.");
  }
  VLOG(4) << "End Verifying for: VariableLengthMemoryEfficientAttentionOp.";
}

void VariableLengthMemoryEfficientAttentionOp::InferMeta( phi::InferMetaContext *infer_meta ) {
  auto fn = PD_INFER_META(phi::VariableLengthMemoryEfficientAttentionInferMeta);
  fn(infer_meta);
}

phi::DataType VariableLengthMemoryEfficientAttentionOp::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: VariableLengthMemoryEfficientAttentionOp";
  


  return expected_kernel_dtype;
}

const char *YoloBoxXpuOp::attributes_name[1] = { "offset" };

OpInfoTuple YoloBoxXpuOp::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("x", "paddle::dialect::DenseTensorType", false, false, false, false), paddle::dialect::OpInputInfo("x_max", "paddle::dialect::DenseTensorType", true, false, false, false), paddle::dialect::OpInputInfo("grid", "paddle::dialect::DenseTensorType", false, false, false, false), paddle::dialect::OpInputInfo("stride", "paddle::dialect::DenseTensorType", false, false, false, false), paddle::dialect::OpInputInfo("anchor_grid", "paddle::dialect::DenseTensorType", false, false, false, false) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = { paddle::dialect::OpAttributeInfo("offset", "pir::FloatAttribute", "") };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("out", "paddle::dialect::DenseTensorType", false, false), paddle::dialect::OpOutputInfo("out_max", "paddle::dialect::DenseTensorType", false, false) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("YoloBoxXPUInferMeta", {"x", "x_max", "grid", "stride", "anchor_grid", "offset"}, "yolo_box_xpu", {"x", "x_max", "grid", "stride", "anchor_grid", "offset"}, {"x"}, {}, {}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "yolo_box_xpu");
}

void YoloBoxXpuOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, pir::Value x_max_, pir::Value grid_, pir::Value stride_, pir::Value anchor_grid_, float offset) {
  VLOG(4) << "Start build YoloBoxXpuOp";



  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_, x_max_, grid_, stride_, anchor_grid_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_offset = pir::FloatAttribute::get(pir::IrContext::Instance(), offset);
  argument.AddAttribute("offset", attr_offset);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;
  paddle::dialect::DenseTensorType grid = grid_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)grid;
  paddle::dialect::DenseTensorType stride = stride_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)stride;
  paddle::dialect::DenseTensorType anchor_grid = anchor_grid_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)anchor_grid;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);

  paddle::dialect::IrMetaTensor meta_x_max;
  paddle::dialect::IrTensor ir_tensor_x_max;
  if (x_max_.impl() != nullptr) {
    paddle::dialect::DenseTensorType x_max = x_max_.type().dyn_cast<paddle::dialect::DenseTensorType>();
    VLOG(4) << "Builder construction  dense_x_max";
    ir_tensor_x_max = paddle::dialect::IrTensor(paddle::dialect::TransToPhiDataType(x_max.dtype()),
                                                        x_max.dims(),
                                                        x_max.data_layout(),
                                                        x_max.lod(),
                                                        x_max.offset());
    VLOG(4) << "Builder construction  meta_x_max";
    meta_x_max = paddle::dialect::IrMetaTensor(&ir_tensor_x_max);
  }


  VLOG(4) << "Builder construction  dense_grid";
  paddle::dialect::IrTensor ir_tensor_grid(paddle::dialect::TransToPhiDataType(grid.dtype()),
                                                      grid.dims(),
                                                      grid.data_layout(),
                                                      grid.lod(),
                                                      grid.offset());
  VLOG(4) << "Builder construction  meta_grid";
  paddle::dialect::IrMetaTensor meta_grid(&ir_tensor_grid);

  VLOG(4) << "Builder construction  dense_stride";
  paddle::dialect::IrTensor ir_tensor_stride(paddle::dialect::TransToPhiDataType(stride.dtype()),
                                                      stride.dims(),
                                                      stride.data_layout(),
                                                      stride.lod(),
                                                      stride.offset());
  VLOG(4) << "Builder construction  meta_stride";
  paddle::dialect::IrMetaTensor meta_stride(&ir_tensor_stride);

  VLOG(4) << "Builder construction  dense_anchor_grid";
  paddle::dialect::IrTensor ir_tensor_anchor_grid(paddle::dialect::TransToPhiDataType(anchor_grid.dtype()),
                                                      anchor_grid.dims(),
                                                      anchor_grid.data_layout(),
                                                      anchor_grid.lod(),
                                                      anchor_grid.offset());
  VLOG(4) << "Builder construction  meta_anchor_grid";
  paddle::dialect::IrMetaTensor meta_anchor_grid(&ir_tensor_anchor_grid);
  paddle::dialect::IrTensor dense_out;
  paddle::dialect::IrMetaTensor meta_out(&dense_out);
  paddle::dialect::IrTensor dense_out_max;
  paddle::dialect::IrMetaTensor meta_out_max(&dense_out_max);

  phi::YoloBoxXPUInferMeta(meta_x, meta_x_max, meta_grid, meta_stride, meta_anchor_grid, offset, &meta_out, &meta_out_max);

  std::vector<pir::Type> argument_outputs;
  pir::Type out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_out.dtype()), dense_out.dims(), dense_out.layout(), dense_out.lod(), dense_out.offset());
  argument_outputs.push_back(out_dense_tensor_type);

  pir::Type out_max_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_out_max.dtype()), dense_out_max.dims(), dense_out_max.layout(), dense_out_max.lod(), dense_out_max.offset());
  argument_outputs.push_back(out_max_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void YoloBoxXpuOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, pir::Value x_max_, pir::Value grid_, pir::Value stride_, pir::Value anchor_grid_, pir::AttributeMap attributes) {
  VLOG(4) << "Start build YoloBoxXpuOp";


  IR_ENFORCE(
      attributes.find("offset") != attributes.end(),
          "'offset' Attribute is expected for YoloBoxXpuOp. ");
  float offset = attributes.at("offset").dyn_cast<pir::FloatAttribute>().data();


  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_, x_max_, grid_, stride_, anchor_grid_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_offset = pir::FloatAttribute::get(pir::IrContext::Instance(), offset);
  argument.AddAttribute("offset", attr_offset);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;
  paddle::dialect::DenseTensorType grid = grid_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)grid;
  paddle::dialect::DenseTensorType stride = stride_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)stride;
  paddle::dialect::DenseTensorType anchor_grid = anchor_grid_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)anchor_grid;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);

  paddle::dialect::IrMetaTensor meta_x_max;
  paddle::dialect::IrTensor ir_tensor_x_max;
  if (x_max_.impl() != nullptr) {
    paddle::dialect::DenseTensorType x_max = x_max_.type().dyn_cast<paddle::dialect::DenseTensorType>();
    VLOG(4) << "Builder construction  dense_x_max";
    ir_tensor_x_max = paddle::dialect::IrTensor(paddle::dialect::TransToPhiDataType(x_max.dtype()),
                                                        x_max.dims(),
                                                        x_max.data_layout(),
                                                        x_max.lod(),
                                                        x_max.offset());
    VLOG(4) << "Builder construction  meta_x_max";
    meta_x_max = paddle::dialect::IrMetaTensor(&ir_tensor_x_max);
  }


  VLOG(4) << "Builder construction  dense_grid";
  paddle::dialect::IrTensor ir_tensor_grid(paddle::dialect::TransToPhiDataType(grid.dtype()),
                                                      grid.dims(),
                                                      grid.data_layout(),
                                                      grid.lod(),
                                                      grid.offset());
  VLOG(4) << "Builder construction  meta_grid";
  paddle::dialect::IrMetaTensor meta_grid(&ir_tensor_grid);

  VLOG(4) << "Builder construction  dense_stride";
  paddle::dialect::IrTensor ir_tensor_stride(paddle::dialect::TransToPhiDataType(stride.dtype()),
                                                      stride.dims(),
                                                      stride.data_layout(),
                                                      stride.lod(),
                                                      stride.offset());
  VLOG(4) << "Builder construction  meta_stride";
  paddle::dialect::IrMetaTensor meta_stride(&ir_tensor_stride);

  VLOG(4) << "Builder construction  dense_anchor_grid";
  paddle::dialect::IrTensor ir_tensor_anchor_grid(paddle::dialect::TransToPhiDataType(anchor_grid.dtype()),
                                                      anchor_grid.dims(),
                                                      anchor_grid.data_layout(),
                                                      anchor_grid.lod(),
                                                      anchor_grid.offset());
  VLOG(4) << "Builder construction  meta_anchor_grid";
  paddle::dialect::IrMetaTensor meta_anchor_grid(&ir_tensor_anchor_grid);
  paddle::dialect::IrTensor dense_out;
  paddle::dialect::IrMetaTensor meta_out(&dense_out);
  paddle::dialect::IrTensor dense_out_max;
  paddle::dialect::IrMetaTensor meta_out_max(&dense_out_max);

  phi::YoloBoxXPUInferMeta(meta_x, meta_x_max, meta_grid, meta_stride, meta_anchor_grid, offset, &meta_out, &meta_out_max);

  std::vector<pir::Type> argument_outputs;
  pir::Type out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_out.dtype()), dense_out.dims(), dense_out.layout(), dense_out.lod(), dense_out.offset());
  argument_outputs.push_back(out_dense_tensor_type);

  pir::Type out_max_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_out_max.dtype()), dense_out_max.dims(), dense_out_max.layout(), dense_out_max.lod(), dense_out_max.offset());
  argument_outputs.push_back(out_max_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void YoloBoxXpuOp::VerifySig() {
  VLOG(4) << "Start Verifying inputs, outputs and attributes for: YoloBoxXpuOp.";
  VLOG(4) << "Verifying inputs:";
  {
  auto input_size = num_operands();
  IR_ENFORCE(input_size == 5u,
                    "The size %d of inputs must be equal to 5.", input_size);
  IR_ENFORCE((*this)->operand_source(0).type().isa<paddle::dialect::DenseTensorType>(),
                  "Type validation failed for the 0th input, got %s.", (*this)->operand_source(0).type());
  if (auto val = (*this)->operand(1)) {
    IR_ENFORCE(val.type().isa<paddle::dialect::DenseTensorType>(),
                   "Type validation failed for the 1th input, got %s.", (*this)->operand_source(1).type());
  }
  IR_ENFORCE((*this)->operand_source(2).type().isa<paddle::dialect::DenseTensorType>(),
                  "Type validation failed for the 2th input, got %s.", (*this)->operand_source(2).type());
  IR_ENFORCE((*this)->operand_source(3).type().isa<paddle::dialect::DenseTensorType>(),
                  "Type validation failed for the 3th input, got %s.", (*this)->operand_source(3).type());
  IR_ENFORCE((*this)->operand_source(4).type().isa<paddle::dialect::DenseTensorType>(),
                  "Type validation failed for the 4th input, got %s.", (*this)->operand_source(4).type());
  }
  VLOG(4) << "Verifying attributes:";
  {
  auto& attributes = this->attributes();
  IR_ENFORCE(attributes.count("offset")>0,
                 "offset does not exist.");
  IR_ENFORCE(attributes.at("offset").isa<pir::FloatAttribute>(),
                 "Type of attribute: offset is not pir::FloatAttribute.");

  }
  VLOG(4) << "Verifying outputs:";
  {
  auto output_size = num_results();
  IR_ENFORCE(output_size == 2u,
                    "The size %d of outputs must be equal to 2.", output_size);
  IR_ENFORCE((*this)->result(0).type().isa<paddle::dialect::DenseTensorType>(),
                 "Type validation failed for the 0th output.");
  IR_ENFORCE((*this)->result(1).type().isa<paddle::dialect::DenseTensorType>(),
                 "Type validation failed for the 1th output.");
  }
  VLOG(4) << "End Verifying for: YoloBoxXpuOp.";
}

void YoloBoxXpuOp::InferMeta( phi::InferMetaContext *infer_meta ) {
  auto fn = PD_INFER_META(phi::YoloBoxXPUInferMeta);
  fn(infer_meta);
}

phi::DataType YoloBoxXpuOp::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: YoloBoxXpuOp";
  


  return expected_kernel_dtype;
}

} // namespace dialect
} // namespace paddle


IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::AddActXpuOp)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::AddLayernormXpuOp)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::AddcmulXpuOp)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::BlockMultiheadAttention_Op)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::BnActXpuOp)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::Conv1dXpuOp)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::Conv2dTransposeXpuOp)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::Conv2dXpuOp)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::DequantizeXpuOp)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::EmbeddingWithEltwiseAddXpuOp)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::FastLayernormXpuOp)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::FastWhereXpuOp)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::FcOp)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::FcXpuOp)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::FusedBiasActOp)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::FusedBiasDropoutResidualLayerNormOp)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::FusedBiasResidualLayernormOp)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::FusedConv2dAddActOp)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::FusedDconvDreluDbnOp)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::FusedDotProductAttentionOp)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::FusedDropoutAddOp)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::FusedEmbeddingEltwiseLayernormOp)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::FusedFcElementwiseLayernormOp)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::FusedLinearParamGradAddOp)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::FusedMultiTransformerInt8XpuOp)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::FusedMultiTransformerXpuOp)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::FusedRotaryPositionEmbeddingOp)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::FusedScaleBiasAddReluOp)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::FusedScaleBiasReluConvBnOp)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::FusionGruOp)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::FusionRepeatedFcReluOp)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::FusionSeqconvEltaddReluOp)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::FusionSeqexpandConcatFcOp)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::FusionSquaredMatSubOp)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::FusionTransposeFlattenConcatOp)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::GenerateSequenceXpuOp)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::LayerNormActXpuOp)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::MaxPool2dV2Op)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::MultiEncoderXpuOp)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::MultiheadMatmulOp)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::QkvAttentionXpuOp)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::QuantizeXpuOp)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::SelfDpAttentionOp)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::SinePosXpuOp)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::SkipLayernormOp)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::SqueezeExcitationBlockOp)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::VariableLengthMemoryEfficientAttentionOp)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::YoloBoxXpuOp)

