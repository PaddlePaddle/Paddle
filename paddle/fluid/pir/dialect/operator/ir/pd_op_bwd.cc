// This file is generated by "paddle/fluid/pir/dialect/op_generator/op_gen.py"
#include "/home/aistudio/fix_op/Paddle/paddle/fluid/pir/dialect/operator/ir/pd_op.h"
#include "paddle/fluid/pir/dialect/operator/ir/op_type.h"
#include "paddle/fluid/pir/dialect/operator/ir/op_attribute.h"
#include "paddle/fluid/pir/dialect/operator/ir/ir_tensor.h"
#include "paddle/fluid/pir/dialect/operator/ir/ir_selected_rows.h"
#include "paddle/fluid/pir/dialect/operator/ir/ir_meta_tensor.h"
#include "paddle/pir/core/builtin_attribute.h"
#include "paddle/pir/core/builtin_type.h"
#include "paddle/pir/core/builtin_op.h"
#include "paddle/pir/core/ir_context.h"
#include "paddle/phi/core/enforce.h"
#include "paddle/phi/core/dense_tensor.h"
#include "paddle/phi/infermeta/binary.h"
#include "paddle/phi/infermeta/multiary.h"
#include "paddle/phi/infermeta/nullary.h"
#include "paddle/phi/infermeta/unary.h"
#include "paddle/phi/infermeta/ternary.h"
#include "paddle/phi/infermeta/backward.h"
#include "paddle/phi/infermeta/fusion.h"
#include "paddle/phi/api/lib/utils/allocator.h"
#include "paddle/fluid/primitive/rule/vjp/vjp.h"
#include "paddle/pir/core/op_base.h"

namespace paddle {
namespace dialect {

OpInfoTuple AbsDoubleGradOp::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("x", "paddle::dialect::DenseTensorType", false, false, false, false), paddle::dialect::OpInputInfo("grad_x_grad", "paddle::dialect::DenseTensorType", false, false, false, false) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = {  };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("grad_out_grad", "paddle::dialect::DenseTensorType", false, false) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("UnchangedInferMeta", {"x"}, "abs_double_grad", {"x", "grad_x_grad"}, {"grad_x_grad"}, {}, {}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "abs_double_grad");
}

void AbsDoubleGradOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, pir::Value grad_x_grad_) {
  VLOG(4) << "Start build AbsDoubleGradOp";



  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_, grad_x_grad_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;
  paddle::dialect::DenseTensorType grad_x_grad = grad_x_grad_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)grad_x_grad;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);
  paddle::dialect::IrTensor dense_grad_out_grad;
  paddle::dialect::IrMetaTensor meta_grad_out_grad(&dense_grad_out_grad);

  phi::UnchangedInferMeta(meta_x, &meta_grad_out_grad);

  std::vector<pir::Type> argument_outputs;
  pir::Type grad_out_grad_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_grad_out_grad.dtype()), dense_grad_out_grad.dims(), dense_grad_out_grad.layout(), dense_grad_out_grad.lod(), dense_grad_out_grad.offset());
  argument_outputs.push_back(grad_out_grad_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void AbsDoubleGradOp::VerifySig() {}

void AbsDoubleGradOp::InferMeta( phi::InferMetaContext *infer_meta ) {
  auto fn = PD_INFER_META(phi::UnchangedInferMeta);
  fn(infer_meta);
}

phi::DataType AbsDoubleGradOp::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: AbsDoubleGradOp";
  

  // deal support data transform
  VLOG(8) << "SUPPORT_TRANSFORM: " << "['x', 'grad_x_grad'];";
  return tensor_dtype;


  return expected_kernel_dtype;
}

OpInfoTuple AbsGradOp::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("x", "paddle::dialect::DenseTensorType", false, false, false, false), paddle::dialect::OpInputInfo("out_grad", "paddle::dialect::DenseTensorType", false, false, false, true) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = {  };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("x_grad", "paddle::dialect::DenseTensorType", false, false) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("UnchangedInferMeta", {"x"}, "abs_grad", {"x", "out_grad"}, {"x"}, {}, {}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "abs_grad");
}

void AbsGradOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, pir::Value out_grad_) {
  VLOG(4) << "Start build AbsGradOp";



  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_, out_grad_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;
  paddle::dialect::DenseTensorType out_grad = out_grad_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)out_grad;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);
  paddle::dialect::IrTensor dense_x_grad;
  paddle::dialect::IrMetaTensor meta_x_grad(&dense_x_grad);

  phi::UnchangedInferMeta(meta_x, &meta_x_grad);

  std::vector<pir::Type> argument_outputs;
  pir::Type x_grad_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_x_grad.dtype()), dense_x_grad.dims(), dense_x_grad.layout(), dense_x_grad.lod(), dense_x_grad.offset());
  argument_outputs.push_back(x_grad_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void AbsGradOp::VerifySig() {}

void AbsGradOp::InferMeta( phi::InferMetaContext *infer_meta ) {
  auto fn = PD_INFER_META(phi::UnchangedInferMeta);
  fn(infer_meta);
}

phi::DataType AbsGradOp::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: AbsGradOp";
  


  return expected_kernel_dtype;
}

OpInfoTuple AcosGradOp::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("x", "paddle::dialect::DenseTensorType", false, false, false, false), paddle::dialect::OpInputInfo("out_grad", "paddle::dialect::DenseTensorType", false, false, false, false) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = {  };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("x_grad", "paddle::dialect::DenseTensorType", false, false) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("UnchangedInferMeta", {"x"}, "acos_grad", {"x", "out_grad"}, {}, {}, {}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "acos_grad");
}

void AcosGradOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, pir::Value out_grad_) {
  VLOG(4) << "Start build AcosGradOp";



  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_, out_grad_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;
  paddle::dialect::DenseTensorType out_grad = out_grad_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)out_grad;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);
  paddle::dialect::IrTensor dense_x_grad;
  paddle::dialect::IrMetaTensor meta_x_grad(&dense_x_grad);

  phi::UnchangedInferMeta(meta_x, &meta_x_grad);

  std::vector<pir::Type> argument_outputs;
  pir::Type x_grad_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_x_grad.dtype()), dense_x_grad.dims(), dense_x_grad.layout(), dense_x_grad.lod(), dense_x_grad.offset());
  argument_outputs.push_back(x_grad_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void AcosGradOp::VerifySig() {}

void AcosGradOp::InferMeta( phi::InferMetaContext *infer_meta ) {
  auto fn = PD_INFER_META(phi::UnchangedInferMeta);
  fn(infer_meta);
}

phi::DataType AcosGradOp::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: AcosGradOp";
  


  return expected_kernel_dtype;
}

OpInfoTuple AcosGrad_Op::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("x", "paddle::dialect::DenseTensorType", false, false, false, false), paddle::dialect::OpInputInfo("out_grad", "paddle::dialect::DenseTensorType", false, false, false, false) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = {  };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("x_grad", "paddle::dialect::DenseTensorType", false, false) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("UnchangedInferMeta", {"x"}, "acos_grad", {"x", "out_grad"}, {}, {}, {{"x_grad", "out_grad"}}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "acos_grad");
}

void AcosGrad_Op::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, pir::Value out_grad_) {
  VLOG(4) << "Start build AcosGrad_Op";



  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_, out_grad_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;
  paddle::dialect::DenseTensorType out_grad = out_grad_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)out_grad;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);
  paddle::dialect::IrTensor dense_x_grad;
  paddle::dialect::IrMetaTensor meta_x_grad(&dense_x_grad);

  phi::UnchangedInferMeta(meta_x, &meta_x_grad);

  std::vector<pir::Type> argument_outputs;
  pir::Type x_grad_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_x_grad.dtype()), dense_x_grad.dims(), dense_x_grad.layout(), dense_x_grad.lod(), dense_x_grad.offset());
  argument_outputs.push_back(x_grad_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void AcosGrad_Op::VerifySig() {}

void AcosGrad_Op::InferMeta( phi::InferMetaContext *infer_meta ) {
  auto fn = PD_INFER_META(phi::UnchangedInferMeta);
  fn(infer_meta);
}

phi::DataType AcosGrad_Op::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: AcosGrad_Op";
  


  return expected_kernel_dtype;
}

OpInfoTuple AcoshGradOp::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("x", "paddle::dialect::DenseTensorType", false, false, false, false), paddle::dialect::OpInputInfo("out_grad", "paddle::dialect::DenseTensorType", false, false, false, false) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = {  };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("x_grad", "paddle::dialect::DenseTensorType", false, false) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("UnchangedInferMeta", {"x"}, "acosh_grad", {"x", "out_grad"}, {}, {}, {}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "acosh_grad");
}

void AcoshGradOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, pir::Value out_grad_) {
  VLOG(4) << "Start build AcoshGradOp";



  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_, out_grad_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;
  paddle::dialect::DenseTensorType out_grad = out_grad_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)out_grad;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);
  paddle::dialect::IrTensor dense_x_grad;
  paddle::dialect::IrMetaTensor meta_x_grad(&dense_x_grad);

  phi::UnchangedInferMeta(meta_x, &meta_x_grad);

  std::vector<pir::Type> argument_outputs;
  pir::Type x_grad_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_x_grad.dtype()), dense_x_grad.dims(), dense_x_grad.layout(), dense_x_grad.lod(), dense_x_grad.offset());
  argument_outputs.push_back(x_grad_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void AcoshGradOp::VerifySig() {}

void AcoshGradOp::InferMeta( phi::InferMetaContext *infer_meta ) {
  auto fn = PD_INFER_META(phi::UnchangedInferMeta);
  fn(infer_meta);
}

phi::DataType AcoshGradOp::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: AcoshGradOp";
  


  return expected_kernel_dtype;
}

OpInfoTuple AcoshGrad_Op::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("x", "paddle::dialect::DenseTensorType", false, false, false, false), paddle::dialect::OpInputInfo("out_grad", "paddle::dialect::DenseTensorType", false, false, false, false) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = {  };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("x_grad", "paddle::dialect::DenseTensorType", false, false) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("UnchangedInferMeta", {"x"}, "acosh_grad", {"x", "out_grad"}, {}, {}, {{"x_grad", "out_grad"}}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "acosh_grad");
}

void AcoshGrad_Op::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, pir::Value out_grad_) {
  VLOG(4) << "Start build AcoshGrad_Op";



  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_, out_grad_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;
  paddle::dialect::DenseTensorType out_grad = out_grad_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)out_grad;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);
  paddle::dialect::IrTensor dense_x_grad;
  paddle::dialect::IrMetaTensor meta_x_grad(&dense_x_grad);

  phi::UnchangedInferMeta(meta_x, &meta_x_grad);

  std::vector<pir::Type> argument_outputs;
  pir::Type x_grad_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_x_grad.dtype()), dense_x_grad.dims(), dense_x_grad.layout(), dense_x_grad.lod(), dense_x_grad.offset());
  argument_outputs.push_back(x_grad_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void AcoshGrad_Op::VerifySig() {}

void AcoshGrad_Op::InferMeta( phi::InferMetaContext *infer_meta ) {
  auto fn = PD_INFER_META(phi::UnchangedInferMeta);
  fn(infer_meta);
}

phi::DataType AcoshGrad_Op::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: AcoshGrad_Op";
  


  return expected_kernel_dtype;
}

const char *AddmmGradOp::attributes_name[2] = { "alpha", "beta" };

OpInfoTuple AddmmGradOp::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("input", "paddle::dialect::DenseTensorType", false, false, false, false), paddle::dialect::OpInputInfo("x", "paddle::dialect::DenseTensorType", false, false, false, false), paddle::dialect::OpInputInfo("y", "paddle::dialect::DenseTensorType", false, false, false, false), paddle::dialect::OpInputInfo("out_grad", "paddle::dialect::DenseTensorType", false, false, false, false) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = { paddle::dialect::OpAttributeInfo("alpha", "pir::FloatAttribute", ""), paddle::dialect::OpAttributeInfo("beta", "pir::FloatAttribute", "") };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("input_grad", "paddle::dialect::DenseTensorType", false, false), paddle::dialect::OpOutputInfo("x_grad", "paddle::dialect::DenseTensorType", false, false), paddle::dialect::OpOutputInfo("y_grad", "paddle::dialect::DenseTensorType", false, false) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("GeneralTernaryGradInferMeta", {"input", "x", "y"}, "addmm_grad", {"input", "x", "y", "out_grad", "alpha", "beta"}, {}, {}, {}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "addmm_grad");
}

void AddmmGradOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value input_, pir::Value x_, pir::Value y_, pir::Value out_grad_, float alpha, float beta) {
  VLOG(4) << "Start build AddmmGradOp";



  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {input_, x_, y_, out_grad_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_alpha = pir::FloatAttribute::get(pir::IrContext::Instance(), alpha);
  argument.AddAttribute("alpha", attr_alpha);
  pir::Attribute attr_beta = pir::FloatAttribute::get(pir::IrContext::Instance(), beta);
  argument.AddAttribute("beta", attr_beta);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType input = input_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)input;
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;
  paddle::dialect::DenseTensorType y = y_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)y;
  paddle::dialect::DenseTensorType out_grad = out_grad_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)out_grad;

  VLOG(4) << "Builder construction  dense_input";
  paddle::dialect::IrTensor ir_tensor_input(paddle::dialect::TransToPhiDataType(input.dtype()),
                                                      input.dims(),
                                                      input.data_layout(),
                                                      input.lod(),
                                                      input.offset());
  VLOG(4) << "Builder construction  meta_input";
  paddle::dialect::IrMetaTensor meta_input(&ir_tensor_input);

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);

  VLOG(4) << "Builder construction  dense_y";
  paddle::dialect::IrTensor ir_tensor_y(paddle::dialect::TransToPhiDataType(y.dtype()),
                                                      y.dims(),
                                                      y.data_layout(),
                                                      y.lod(),
                                                      y.offset());
  VLOG(4) << "Builder construction  meta_y";
  paddle::dialect::IrMetaTensor meta_y(&ir_tensor_y);
  paddle::dialect::IrTensor dense_input_grad;
  paddle::dialect::IrMetaTensor meta_input_grad(&dense_input_grad);
  paddle::dialect::IrTensor dense_x_grad;
  paddle::dialect::IrMetaTensor meta_x_grad(&dense_x_grad);
  paddle::dialect::IrTensor dense_y_grad;
  paddle::dialect::IrMetaTensor meta_y_grad(&dense_y_grad);

  phi::GeneralTernaryGradInferMeta(meta_input, meta_x, meta_y, &meta_input_grad, &meta_x_grad, &meta_y_grad);

  std::vector<pir::Type> argument_outputs;
  pir::Type input_grad_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_input_grad.dtype()), dense_input_grad.dims(), dense_input_grad.layout(), dense_input_grad.lod(), dense_input_grad.offset());
  argument_outputs.push_back(input_grad_dense_tensor_type);

  pir::Type x_grad_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_x_grad.dtype()), dense_x_grad.dims(), dense_x_grad.layout(), dense_x_grad.lod(), dense_x_grad.offset());
  argument_outputs.push_back(x_grad_dense_tensor_type);

  pir::Type y_grad_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_y_grad.dtype()), dense_y_grad.dims(), dense_y_grad.layout(), dense_y_grad.lod(), dense_y_grad.offset());
  argument_outputs.push_back(y_grad_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void AddmmGradOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value input_, pir::Value x_, pir::Value y_, pir::Value out_grad_, pir::AttributeMap attributes) {
  VLOG(4) << "Start build AddmmGradOp";


  IR_ENFORCE(
      attributes.find("alpha") != attributes.end(),
          "'alpha' Attribute is expected for AddmmGradOp. ");
  float alpha = attributes.at("alpha").dyn_cast<pir::FloatAttribute>().data();

  IR_ENFORCE(
      attributes.find("beta") != attributes.end(),
          "'beta' Attribute is expected for AddmmGradOp. ");
  float beta = attributes.at("beta").dyn_cast<pir::FloatAttribute>().data();


  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {input_, x_, y_, out_grad_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_alpha = pir::FloatAttribute::get(pir::IrContext::Instance(), alpha);
  argument.AddAttribute("alpha", attr_alpha);
  pir::Attribute attr_beta = pir::FloatAttribute::get(pir::IrContext::Instance(), beta);
  argument.AddAttribute("beta", attr_beta);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType input = input_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)input;
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;
  paddle::dialect::DenseTensorType y = y_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)y;
  paddle::dialect::DenseTensorType out_grad = out_grad_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)out_grad;

  VLOG(4) << "Builder construction  dense_input";
  paddle::dialect::IrTensor ir_tensor_input(paddle::dialect::TransToPhiDataType(input.dtype()),
                                                      input.dims(),
                                                      input.data_layout(),
                                                      input.lod(),
                                                      input.offset());
  VLOG(4) << "Builder construction  meta_input";
  paddle::dialect::IrMetaTensor meta_input(&ir_tensor_input);

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);

  VLOG(4) << "Builder construction  dense_y";
  paddle::dialect::IrTensor ir_tensor_y(paddle::dialect::TransToPhiDataType(y.dtype()),
                                                      y.dims(),
                                                      y.data_layout(),
                                                      y.lod(),
                                                      y.offset());
  VLOG(4) << "Builder construction  meta_y";
  paddle::dialect::IrMetaTensor meta_y(&ir_tensor_y);
  paddle::dialect::IrTensor dense_input_grad;
  paddle::dialect::IrMetaTensor meta_input_grad(&dense_input_grad);
  paddle::dialect::IrTensor dense_x_grad;
  paddle::dialect::IrMetaTensor meta_x_grad(&dense_x_grad);
  paddle::dialect::IrTensor dense_y_grad;
  paddle::dialect::IrMetaTensor meta_y_grad(&dense_y_grad);

  phi::GeneralTernaryGradInferMeta(meta_input, meta_x, meta_y, &meta_input_grad, &meta_x_grad, &meta_y_grad);

  std::vector<pir::Type> argument_outputs;
  pir::Type input_grad_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_input_grad.dtype()), dense_input_grad.dims(), dense_input_grad.layout(), dense_input_grad.lod(), dense_input_grad.offset());
  argument_outputs.push_back(input_grad_dense_tensor_type);

  pir::Type x_grad_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_x_grad.dtype()), dense_x_grad.dims(), dense_x_grad.layout(), dense_x_grad.lod(), dense_x_grad.offset());
  argument_outputs.push_back(x_grad_dense_tensor_type);

  pir::Type y_grad_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_y_grad.dtype()), dense_y_grad.dims(), dense_y_grad.layout(), dense_y_grad.lod(), dense_y_grad.offset());
  argument_outputs.push_back(y_grad_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void AddmmGradOp::VerifySig() {}

void AddmmGradOp::InferMeta( phi::InferMetaContext *infer_meta ) {
  auto fn = PD_INFER_META(phi::GeneralTernaryGradInferMeta);
  fn(infer_meta);
}

phi::DataType AddmmGradOp::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: AddmmGradOp";
  


  return expected_kernel_dtype;
}

const char *AffineGridGradOp::attributes_name[1] = { "align_corners" };

OpInfoTuple AffineGridGradOp::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("input", "paddle::dialect::DenseTensorType", false, false, false, false), paddle::dialect::OpInputInfo("output_grad", "paddle::dialect::DenseTensorType", false, false, false, false), paddle::dialect::OpInputInfo("output_shape", "paddle::dialect::IntArrayAttribute", false, false, true, false) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = { paddle::dialect::OpAttributeInfo("align_corners", "pir::BoolAttribute", "") };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("input_grad", "paddle::dialect::DenseTensorType", false, false) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("AffineGridGradInferMeta", {"output_grad", "output_shape", "align_corners"}, "affine_grid_grad", {"output_grad", "output_shape", "align_corners"}, {}, {}, {}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "affine_grid_grad");
}

void AffineGridGradOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value input_, pir::Value output_grad_, const std::vector<int64_t>& output_shape, bool align_corners) {
  VLOG(4) << "Start build AffineGridGradOp";


  // Generate int_array mutable attribute: output_shape
  paddle::dialect::FullIntArrayOp full_output_shape_op = builder.Build<paddle::dialect::FullIntArrayOp>(output_shape, phi::DataType::INT64, phi::CPUPlace());
  pir::OpResult output_shape_ = full_output_shape_op->result(0);
    
  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {input_, output_grad_, output_shape_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_align_corners = pir::BoolAttribute::get(pir::IrContext::Instance(), align_corners);
  argument.AddAttribute("align_corners", attr_align_corners);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType input = input_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)input;
  paddle::dialect::DenseTensorType output_grad = output_grad_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)output_grad;

  VLOG(4) << "Builder construction  dense_output_grad";
  paddle::dialect::IrTensor ir_tensor_output_grad(paddle::dialect::TransToPhiDataType(output_grad.dtype()),
                                                      output_grad.dims(),
                                                      output_grad.data_layout(),
                                                      output_grad.lod(),
                                                      output_grad.offset());
  VLOG(4) << "Builder construction  meta_output_grad";
  paddle::dialect::IrMetaTensor meta_output_grad(&ir_tensor_output_grad);
  paddle::dialect::IrTensor dense_input_grad;
  paddle::dialect::IrMetaTensor meta_input_grad(&dense_input_grad);

  phi::AffineGridGradInferMeta(meta_output_grad, output_shape, align_corners, &meta_input_grad);

  std::vector<pir::Type> argument_outputs;
  pir::Type input_grad_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_input_grad.dtype()), dense_input_grad.dims(), dense_input_grad.layout(), dense_input_grad.lod(), dense_input_grad.offset());
  argument_outputs.push_back(input_grad_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void AffineGridGradOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value input_, pir::Value output_grad_, pir::AttributeMap attributes) {
  VLOG(4) << "Start build AffineGridGradOp";


  IR_ENFORCE(
      attributes.find("output_shape") != attributes.end(),
          "'output_shape' Attribute is expected for AffineGridGradOp. ");
  std::vector<int64_t> output_shape = attributes.at("output_shape").dyn_cast<paddle::dialect::IntArrayAttribute>().data().GetData();

  IR_ENFORCE(
      attributes.find("align_corners") != attributes.end(),
          "'align_corners' Attribute is expected for AffineGridGradOp. ");
  bool align_corners = attributes.at("align_corners").dyn_cast<pir::BoolAttribute>().data();

  // Generate int_array mutable attribute: output_shape
  paddle::dialect::FullIntArrayOp full_output_shape_op = builder.Build<paddle::dialect::FullIntArrayOp>(output_shape, phi::DataType::INT64, phi::CPUPlace());
  pir::OpResult output_shape_ = full_output_shape_op->result(0);
    
  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {input_, output_grad_, output_shape_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_align_corners = pir::BoolAttribute::get(pir::IrContext::Instance(), align_corners);
  argument.AddAttribute("align_corners", attr_align_corners);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType input = input_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)input;
  paddle::dialect::DenseTensorType output_grad = output_grad_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)output_grad;

  VLOG(4) << "Builder construction  dense_output_grad";
  paddle::dialect::IrTensor ir_tensor_output_grad(paddle::dialect::TransToPhiDataType(output_grad.dtype()),
                                                      output_grad.dims(),
                                                      output_grad.data_layout(),
                                                      output_grad.lod(),
                                                      output_grad.offset());
  VLOG(4) << "Builder construction  meta_output_grad";
  paddle::dialect::IrMetaTensor meta_output_grad(&ir_tensor_output_grad);
  paddle::dialect::IrTensor dense_input_grad;
  paddle::dialect::IrMetaTensor meta_input_grad(&dense_input_grad);

  phi::AffineGridGradInferMeta(meta_output_grad, output_shape, align_corners, &meta_input_grad);

  std::vector<pir::Type> argument_outputs;
  pir::Type input_grad_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_input_grad.dtype()), dense_input_grad.dims(), dense_input_grad.layout(), dense_input_grad.lod(), dense_input_grad.offset());
  argument_outputs.push_back(input_grad_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void AffineGridGradOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value input_, pir::Value output_grad_, pir::Value output_shape_, bool align_corners) {
  VLOG(4) << "Start build AffineGridGradOp";



  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {input_, output_grad_, output_shape_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_align_corners = pir::BoolAttribute::get(pir::IrContext::Instance(), align_corners);
  argument.AddAttribute("align_corners", attr_align_corners);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType input = input_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)input;
  paddle::dialect::DenseTensorType output_grad = output_grad_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)output_grad;
  phi::IntArray output_shape;
  if (output_shape_.dyn_cast<pir::OpResult>() && output_shape_.dyn_cast<pir::OpResult>().owner()->isa<paddle::dialect::FullIntArrayOp>()) {
    output_shape = std::move(phi::IntArray(paddle::dialect::GetInt64Vector(
                          output_shape_.dyn_cast<pir::OpResult>().owner()
                          ->dyn_cast<paddle::dialect::FullIntArrayOp>()
                          .attribute("value"))));
  } else if (output_shape_.type().isa<pir::VectorType>()) {
    size_t output_shape_size = output_shape_.type().dyn_cast<pir::VectorType>().size();
    output_shape = std::move(phi::IntArray(std::vector<int64_t>(output_shape_size, -1)));
    output_shape.SetFromTensor(true);
  } else if (output_shape_.type().isa<paddle::dialect::DenseTensorType>()) {
    common::DDim output_shape_dim = output_shape_.type().dyn_cast<paddle::dialect::DenseTensorType>().dims();
    size_t output_shape_size = common::product(output_shape_dim);
    if (common::contain_unknown_dim(output_shape_dim)) {
      output_shape_size = 1;
    }
    output_shape = std::move(phi::IntArray(std::vector<int64_t>(output_shape_size, -1)));
    output_shape.SetFromTensor(true);
  } else {
    PADDLE_THROW(phi::errors::Unimplemented("Only support VectorType or DenseTensorType"));
  }


  VLOG(4) << "Builder construction  dense_output_grad";
  paddle::dialect::IrTensor ir_tensor_output_grad(paddle::dialect::TransToPhiDataType(output_grad.dtype()),
                                                      output_grad.dims(),
                                                      output_grad.data_layout(),
                                                      output_grad.lod(),
                                                      output_grad.offset());
  VLOG(4) << "Builder construction  meta_output_grad";
  paddle::dialect::IrMetaTensor meta_output_grad(&ir_tensor_output_grad);
  paddle::dialect::IrTensor dense_input_grad;
  paddle::dialect::IrMetaTensor meta_input_grad(&dense_input_grad);

  phi::AffineGridGradInferMeta(meta_output_grad, output_shape, align_corners, &meta_input_grad);

  std::vector<pir::Type> argument_outputs;
  pir::Type input_grad_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_input_grad.dtype()), dense_input_grad.dims(), dense_input_grad.layout(), dense_input_grad.lod(), dense_input_grad.offset());
  argument_outputs.push_back(input_grad_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void AffineGridGradOp::VerifySig() {}

void AffineGridGradOp::InferMeta( phi::InferMetaContext *infer_meta ) {
  auto fn = PD_INFER_META(phi::AffineGridGradInferMeta);
  fn(infer_meta);
}

phi::DataType AffineGridGradOp::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: AffineGridGradOp";
  


  return expected_kernel_dtype;
}

OpInfoTuple AngleGradOp::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("x", "paddle::dialect::DenseTensorType", false, false, false, false), paddle::dialect::OpInputInfo("out_grad", "paddle::dialect::DenseTensorType", false, false, false, false) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = {  };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("x_grad", "paddle::dialect::DenseTensorType", false, false) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("UnchangedInferMeta", {"x"}, "angle_grad", {"x", "out_grad"}, {}, {}, {}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "angle_grad");
}

void AngleGradOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, pir::Value out_grad_) {
  VLOG(4) << "Start build AngleGradOp";



  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_, out_grad_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;
  paddle::dialect::DenseTensorType out_grad = out_grad_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)out_grad;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);
  paddle::dialect::IrTensor dense_x_grad;
  paddle::dialect::IrMetaTensor meta_x_grad(&dense_x_grad);

  phi::UnchangedInferMeta(meta_x, &meta_x_grad);

  std::vector<pir::Type> argument_outputs;
  pir::Type x_grad_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_x_grad.dtype()), dense_x_grad.dims(), dense_x_grad.layout(), dense_x_grad.lod(), dense_x_grad.offset());
  argument_outputs.push_back(x_grad_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void AngleGradOp::VerifySig() {}

void AngleGradOp::InferMeta( phi::InferMetaContext *infer_meta ) {
  auto fn = PD_INFER_META(phi::UnchangedInferMeta);
  fn(infer_meta);
}

phi::DataType AngleGradOp::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: AngleGradOp";
  


  return expected_kernel_dtype;
}

const char *ArgsortGradOp::attributes_name[2] = { "axis", "descending" };

OpInfoTuple ArgsortGradOp::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("indices", "paddle::dialect::DenseTensorType", false, false, false, false), paddle::dialect::OpInputInfo("x", "paddle::dialect::DenseTensorType", false, true, false, false), paddle::dialect::OpInputInfo("out_grad", "paddle::dialect::DenseTensorType", false, false, false, false) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = { paddle::dialect::OpAttributeInfo("axis", "pir::Int32Attribute", ""), paddle::dialect::OpAttributeInfo("descending", "pir::BoolAttribute", "") };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("x_grad", "paddle::dialect::DenseTensorType", false, false) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("UnchangedInferMeta", {"x"}, "argsort_grad", {"indices", "x", "out_grad", "axis", "descending"}, {"out_grad"}, {}, {}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "argsort_grad");
}

void ArgsortGradOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value indices_, pir::Value x_, pir::Value out_grad_, int axis, bool descending) {
  VLOG(4) << "Start build ArgsortGradOp";



  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {indices_, x_, out_grad_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_axis = pir::Int32Attribute::get(pir::IrContext::Instance(), axis);
  argument.AddAttribute("axis", attr_axis);
  pir::Attribute attr_descending = pir::BoolAttribute::get(pir::IrContext::Instance(), descending);
  argument.AddAttribute("descending", attr_descending);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType indices = indices_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)indices;
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;
  paddle::dialect::DenseTensorType out_grad = out_grad_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)out_grad;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);
  paddle::dialect::IrTensor dense_x_grad;
  paddle::dialect::IrMetaTensor meta_x_grad(&dense_x_grad);

  phi::UnchangedInferMeta(meta_x, &meta_x_grad);

  std::vector<pir::Type> argument_outputs;
  pir::Type x_grad_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_x_grad.dtype()), dense_x_grad.dims(), dense_x_grad.layout(), dense_x_grad.lod(), dense_x_grad.offset());
  argument_outputs.push_back(x_grad_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void ArgsortGradOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value indices_, pir::Value x_, pir::Value out_grad_, pir::AttributeMap attributes) {
  VLOG(4) << "Start build ArgsortGradOp";


  IR_ENFORCE(
      attributes.find("axis") != attributes.end(),
          "'axis' Attribute is expected for ArgsortGradOp. ");
  int axis = attributes.at("axis").dyn_cast<pir::Int32Attribute>().data();

  IR_ENFORCE(
      attributes.find("descending") != attributes.end(),
          "'descending' Attribute is expected for ArgsortGradOp. ");
  bool descending = attributes.at("descending").dyn_cast<pir::BoolAttribute>().data();


  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {indices_, x_, out_grad_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_axis = pir::Int32Attribute::get(pir::IrContext::Instance(), axis);
  argument.AddAttribute("axis", attr_axis);
  pir::Attribute attr_descending = pir::BoolAttribute::get(pir::IrContext::Instance(), descending);
  argument.AddAttribute("descending", attr_descending);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType indices = indices_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)indices;
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;
  paddle::dialect::DenseTensorType out_grad = out_grad_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)out_grad;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);
  paddle::dialect::IrTensor dense_x_grad;
  paddle::dialect::IrMetaTensor meta_x_grad(&dense_x_grad);

  phi::UnchangedInferMeta(meta_x, &meta_x_grad);

  std::vector<pir::Type> argument_outputs;
  pir::Type x_grad_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_x_grad.dtype()), dense_x_grad.dims(), dense_x_grad.layout(), dense_x_grad.lod(), dense_x_grad.offset());
  argument_outputs.push_back(x_grad_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void ArgsortGradOp::VerifySig() {}

void ArgsortGradOp::InferMeta( phi::InferMetaContext *infer_meta ) {
  auto fn = PD_INFER_META(phi::UnchangedInferMeta);
  fn(infer_meta);
}

phi::DataType ArgsortGradOp::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: ArgsortGradOp";
  


  return expected_kernel_dtype;
}

OpInfoTuple AsComplexGradOp::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("out_grad", "paddle::dialect::DenseTensorType", false, false, false, false) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = {  };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("x_grad", "paddle::dialect::DenseTensorType", false, false) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("", {""}, "", {""}, {}, {}, {}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "as_complex_grad");
}

void AsComplexGradOp::VerifySig() {}

phi::DataType AsComplexGradOp::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: AsComplexGradOp";
  


  return expected_kernel_dtype;
}

OpInfoTuple AsRealGradOp::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("out_grad", "paddle::dialect::DenseTensorType", false, false, false, false) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = {  };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("x_grad", "paddle::dialect::DenseTensorType", false, false) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("", {""}, "", {""}, {}, {}, {}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "as_real_grad");
}

void AsRealGradOp::VerifySig() {}

phi::DataType AsRealGradOp::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: AsRealGradOp";
  


  return expected_kernel_dtype;
}

const char *AsStridedGradOp::attributes_name[3] = { "dims", "stride", "offset" };

OpInfoTuple AsStridedGradOp::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("input", "paddle::dialect::DenseTensorType", false, false, false, false), paddle::dialect::OpInputInfo("out_grad", "paddle::dialect::DenseTensorType", false, false, false, false) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = { paddle::dialect::OpAttributeInfo("dims", "pir::ArrayAttribute<pir::Int64Attribute>", ""), paddle::dialect::OpAttributeInfo("stride", "pir::ArrayAttribute<pir::Int64Attribute>", ""), paddle::dialect::OpAttributeInfo("offset", "pir::Int64Attribute", "") };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("input_grad", "paddle::dialect::DenseTensorType", false, false) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("StridedUnChangedInferMeta", {"input"}, "as_strided_grad", {"input", "out_grad", "dims", "stride", "offset"}, {}, {}, {}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "as_strided_grad");
}

void AsStridedGradOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value input_, pir::Value out_grad_, const std::vector<int64_t>& dims, const std::vector<int64_t>& stride, int64_t offset) {
  VLOG(4) << "Start build AsStridedGradOp";



  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {input_, out_grad_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  std::vector<pir::Attribute> vec_dims;
  for (size_t i = 0; i < static_cast<size_t>(dims.size()); i++) {
      pir::Attribute attr_dims = pir::Int64Attribute::get(pir::IrContext::Instance(), dims[i]);

    vec_dims.push_back(attr_dims);
  }
  pir::Attribute attr_dims = pir::ArrayAttribute::get(pir::IrContext::Instance(), vec_dims);
  argument.AddAttribute("dims", attr_dims);
  std::vector<pir::Attribute> vec_stride;
  for (size_t i = 0; i < static_cast<size_t>(stride.size()); i++) {
      pir::Attribute attr_stride = pir::Int64Attribute::get(pir::IrContext::Instance(), stride[i]);

    vec_stride.push_back(attr_stride);
  }
  pir::Attribute attr_stride = pir::ArrayAttribute::get(pir::IrContext::Instance(), vec_stride);
  argument.AddAttribute("stride", attr_stride);
  pir::Attribute attr_offset = pir::Int64Attribute::get(pir::IrContext::Instance(), offset);
  argument.AddAttribute("offset", attr_offset);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType input = input_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)input;
  paddle::dialect::DenseTensorType out_grad = out_grad_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)out_grad;

  VLOG(4) << "Builder construction  dense_input";
  paddle::dialect::IrTensor ir_tensor_input(paddle::dialect::TransToPhiDataType(input.dtype()),
                                                      input.dims(),
                                                      input.data_layout(),
                                                      input.lod(),
                                                      input.offset());
  VLOG(4) << "Builder construction  meta_input";
  paddle::dialect::IrMetaTensor meta_input(&ir_tensor_input);
  paddle::dialect::IrTensor dense_input_grad;
  paddle::dialect::IrMetaTensor meta_input_grad(&dense_input_grad);

  phi::StridedUnChangedInferMeta(meta_input, &meta_input_grad);

  std::vector<pir::Type> argument_outputs;
  pir::Type input_grad_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_input_grad.dtype()), dense_input_grad.dims(), dense_input_grad.layout(), dense_input_grad.lod(), dense_input_grad.offset());
  argument_outputs.push_back(input_grad_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void AsStridedGradOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value input_, pir::Value out_grad_, pir::AttributeMap attributes) {
  VLOG(4) << "Start build AsStridedGradOp";


  IR_ENFORCE(
      attributes.find("dims") != attributes.end(),
          "'dims' Attribute is expected for AsStridedGradOp. ");
  std::vector<int64_t> dims;
  for (size_t i = 0; i < attributes.at("dims").dyn_cast<pir::ArrayAttribute>().size(); i++) {
    dims.push_back(attributes.at("dims").dyn_cast<pir::ArrayAttribute>().at(i).dyn_cast<pir::Int64Attribute>().data());
  }

  IR_ENFORCE(
      attributes.find("stride") != attributes.end(),
          "'stride' Attribute is expected for AsStridedGradOp. ");
  std::vector<int64_t> stride;
  for (size_t i = 0; i < attributes.at("stride").dyn_cast<pir::ArrayAttribute>().size(); i++) {
    stride.push_back(attributes.at("stride").dyn_cast<pir::ArrayAttribute>().at(i).dyn_cast<pir::Int64Attribute>().data());
  }

  IR_ENFORCE(
      attributes.find("offset") != attributes.end(),
          "'offset' Attribute is expected for AsStridedGradOp. ");
  int64_t offset = attributes.at("offset").dyn_cast<pir::Int64Attribute>().data();


  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {input_, out_grad_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  std::vector<pir::Attribute> vec_dims;
  for (size_t i = 0; i < static_cast<size_t>(dims.size()); i++) {
      pir::Attribute attr_dims = pir::Int64Attribute::get(pir::IrContext::Instance(), dims[i]);

    vec_dims.push_back(attr_dims);
  }
  pir::Attribute attr_dims = pir::ArrayAttribute::get(pir::IrContext::Instance(), vec_dims);
  argument.AddAttribute("dims", attr_dims);
  std::vector<pir::Attribute> vec_stride;
  for (size_t i = 0; i < static_cast<size_t>(stride.size()); i++) {
      pir::Attribute attr_stride = pir::Int64Attribute::get(pir::IrContext::Instance(), stride[i]);

    vec_stride.push_back(attr_stride);
  }
  pir::Attribute attr_stride = pir::ArrayAttribute::get(pir::IrContext::Instance(), vec_stride);
  argument.AddAttribute("stride", attr_stride);
  pir::Attribute attr_offset = pir::Int64Attribute::get(pir::IrContext::Instance(), offset);
  argument.AddAttribute("offset", attr_offset);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType input = input_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)input;
  paddle::dialect::DenseTensorType out_grad = out_grad_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)out_grad;

  VLOG(4) << "Builder construction  dense_input";
  paddle::dialect::IrTensor ir_tensor_input(paddle::dialect::TransToPhiDataType(input.dtype()),
                                                      input.dims(),
                                                      input.data_layout(),
                                                      input.lod(),
                                                      input.offset());
  VLOG(4) << "Builder construction  meta_input";
  paddle::dialect::IrMetaTensor meta_input(&ir_tensor_input);
  paddle::dialect::IrTensor dense_input_grad;
  paddle::dialect::IrMetaTensor meta_input_grad(&dense_input_grad);

  phi::StridedUnChangedInferMeta(meta_input, &meta_input_grad);

  std::vector<pir::Type> argument_outputs;
  pir::Type input_grad_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_input_grad.dtype()), dense_input_grad.dims(), dense_input_grad.layout(), dense_input_grad.lod(), dense_input_grad.offset());
  argument_outputs.push_back(input_grad_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void AsStridedGradOp::VerifySig() {}

void AsStridedGradOp::InferMeta( phi::InferMetaContext *infer_meta ) {
  auto fn = PD_INFER_META(phi::StridedUnChangedInferMeta);
  fn(infer_meta);
}

phi::DataType AsStridedGradOp::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: AsStridedGradOp";
  


  return expected_kernel_dtype;
}

OpInfoTuple AsinGradOp::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("x", "paddle::dialect::DenseTensorType", false, false, false, false), paddle::dialect::OpInputInfo("out_grad", "paddle::dialect::DenseTensorType", false, false, false, false) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = {  };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("x_grad", "paddle::dialect::DenseTensorType", false, false) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("UnchangedInferMeta", {"x"}, "asin_grad", {"x", "out_grad"}, {}, {}, {}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "asin_grad");
}

void AsinGradOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, pir::Value out_grad_) {
  VLOG(4) << "Start build AsinGradOp";



  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_, out_grad_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;
  paddle::dialect::DenseTensorType out_grad = out_grad_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)out_grad;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);
  paddle::dialect::IrTensor dense_x_grad;
  paddle::dialect::IrMetaTensor meta_x_grad(&dense_x_grad);

  phi::UnchangedInferMeta(meta_x, &meta_x_grad);

  std::vector<pir::Type> argument_outputs;
  pir::Type x_grad_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_x_grad.dtype()), dense_x_grad.dims(), dense_x_grad.layout(), dense_x_grad.lod(), dense_x_grad.offset());
  argument_outputs.push_back(x_grad_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void AsinGradOp::VerifySig() {}

void AsinGradOp::InferMeta( phi::InferMetaContext *infer_meta ) {
  auto fn = PD_INFER_META(phi::UnchangedInferMeta);
  fn(infer_meta);
}

phi::DataType AsinGradOp::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: AsinGradOp";
  


  return expected_kernel_dtype;
}

OpInfoTuple AsinGrad_Op::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("x", "paddle::dialect::DenseTensorType", false, false, false, false), paddle::dialect::OpInputInfo("out_grad", "paddle::dialect::DenseTensorType", false, false, false, false) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = {  };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("x_grad", "paddle::dialect::DenseTensorType", false, false) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("UnchangedInferMeta", {"x"}, "asin_grad", {"x", "out_grad"}, {}, {}, {{"x_grad", "out_grad"}}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "asin_grad");
}

void AsinGrad_Op::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, pir::Value out_grad_) {
  VLOG(4) << "Start build AsinGrad_Op";



  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_, out_grad_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;
  paddle::dialect::DenseTensorType out_grad = out_grad_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)out_grad;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);
  paddle::dialect::IrTensor dense_x_grad;
  paddle::dialect::IrMetaTensor meta_x_grad(&dense_x_grad);

  phi::UnchangedInferMeta(meta_x, &meta_x_grad);

  std::vector<pir::Type> argument_outputs;
  pir::Type x_grad_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_x_grad.dtype()), dense_x_grad.dims(), dense_x_grad.layout(), dense_x_grad.lod(), dense_x_grad.offset());
  argument_outputs.push_back(x_grad_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void AsinGrad_Op::VerifySig() {}

void AsinGrad_Op::InferMeta( phi::InferMetaContext *infer_meta ) {
  auto fn = PD_INFER_META(phi::UnchangedInferMeta);
  fn(infer_meta);
}

phi::DataType AsinGrad_Op::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: AsinGrad_Op";
  


  return expected_kernel_dtype;
}

OpInfoTuple AsinhGradOp::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("x", "paddle::dialect::DenseTensorType", false, false, false, false), paddle::dialect::OpInputInfo("out_grad", "paddle::dialect::DenseTensorType", false, false, false, false) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = {  };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("x_grad", "paddle::dialect::DenseTensorType", false, false) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("UnchangedInferMeta", {"x"}, "asinh_grad", {"x", "out_grad"}, {}, {}, {}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "asinh_grad");
}

void AsinhGradOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, pir::Value out_grad_) {
  VLOG(4) << "Start build AsinhGradOp";



  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_, out_grad_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;
  paddle::dialect::DenseTensorType out_grad = out_grad_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)out_grad;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);
  paddle::dialect::IrTensor dense_x_grad;
  paddle::dialect::IrMetaTensor meta_x_grad(&dense_x_grad);

  phi::UnchangedInferMeta(meta_x, &meta_x_grad);

  std::vector<pir::Type> argument_outputs;
  pir::Type x_grad_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_x_grad.dtype()), dense_x_grad.dims(), dense_x_grad.layout(), dense_x_grad.lod(), dense_x_grad.offset());
  argument_outputs.push_back(x_grad_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void AsinhGradOp::VerifySig() {}

void AsinhGradOp::InferMeta( phi::InferMetaContext *infer_meta ) {
  auto fn = PD_INFER_META(phi::UnchangedInferMeta);
  fn(infer_meta);
}

phi::DataType AsinhGradOp::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: AsinhGradOp";
  


  return expected_kernel_dtype;
}

OpInfoTuple AsinhGrad_Op::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("x", "paddle::dialect::DenseTensorType", false, false, false, false), paddle::dialect::OpInputInfo("out_grad", "paddle::dialect::DenseTensorType", false, false, false, false) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = {  };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("x_grad", "paddle::dialect::DenseTensorType", false, false) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("UnchangedInferMeta", {"x"}, "asinh_grad", {"x", "out_grad"}, {}, {}, {{"x_grad", "out_grad"}}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "asinh_grad");
}

void AsinhGrad_Op::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, pir::Value out_grad_) {
  VLOG(4) << "Start build AsinhGrad_Op";



  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_, out_grad_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;
  paddle::dialect::DenseTensorType out_grad = out_grad_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)out_grad;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);
  paddle::dialect::IrTensor dense_x_grad;
  paddle::dialect::IrMetaTensor meta_x_grad(&dense_x_grad);

  phi::UnchangedInferMeta(meta_x, &meta_x_grad);

  std::vector<pir::Type> argument_outputs;
  pir::Type x_grad_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_x_grad.dtype()), dense_x_grad.dims(), dense_x_grad.layout(), dense_x_grad.lod(), dense_x_grad.offset());
  argument_outputs.push_back(x_grad_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void AsinhGrad_Op::VerifySig() {}

void AsinhGrad_Op::InferMeta( phi::InferMetaContext *infer_meta ) {
  auto fn = PD_INFER_META(phi::UnchangedInferMeta);
  fn(infer_meta);
}

phi::DataType AsinhGrad_Op::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: AsinhGrad_Op";
  


  return expected_kernel_dtype;
}

OpInfoTuple Atan2GradOp::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("x", "paddle::dialect::DenseTensorType", false, false, false, false), paddle::dialect::OpInputInfo("y", "paddle::dialect::DenseTensorType", false, false, false, false), paddle::dialect::OpInputInfo("out_grad", "paddle::dialect::DenseTensorType", false, false, false, false) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = {  };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("x_grad", "paddle::dialect::DenseTensorType", false, false), paddle::dialect::OpOutputInfo("y_grad", "paddle::dialect::DenseTensorType", false, false) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("GeneralBinaryGradInferMeta", {"x", "y"}, "atan2_grad", {"x", "y", "out_grad"}, {}, {}, {}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "atan2_grad");
}

void Atan2GradOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, pir::Value y_, pir::Value out_grad_) {
  VLOG(4) << "Start build Atan2GradOp";



  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_, y_, out_grad_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;
  paddle::dialect::DenseTensorType y = y_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)y;
  paddle::dialect::DenseTensorType out_grad = out_grad_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)out_grad;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);

  VLOG(4) << "Builder construction  dense_y";
  paddle::dialect::IrTensor ir_tensor_y(paddle::dialect::TransToPhiDataType(y.dtype()),
                                                      y.dims(),
                                                      y.data_layout(),
                                                      y.lod(),
                                                      y.offset());
  VLOG(4) << "Builder construction  meta_y";
  paddle::dialect::IrMetaTensor meta_y(&ir_tensor_y);
  paddle::dialect::IrTensor dense_x_grad;
  paddle::dialect::IrMetaTensor meta_x_grad(&dense_x_grad);
  paddle::dialect::IrTensor dense_y_grad;
  paddle::dialect::IrMetaTensor meta_y_grad(&dense_y_grad);

  phi::GeneralBinaryGradInferMeta(meta_x, meta_y, &meta_x_grad, &meta_y_grad);

  std::vector<pir::Type> argument_outputs;
  pir::Type x_grad_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_x_grad.dtype()), dense_x_grad.dims(), dense_x_grad.layout(), dense_x_grad.lod(), dense_x_grad.offset());
  argument_outputs.push_back(x_grad_dense_tensor_type);

  pir::Type y_grad_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_y_grad.dtype()), dense_y_grad.dims(), dense_y_grad.layout(), dense_y_grad.lod(), dense_y_grad.offset());
  argument_outputs.push_back(y_grad_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void Atan2GradOp::VerifySig() {}

void Atan2GradOp::InferMeta( phi::InferMetaContext *infer_meta ) {
  auto fn = PD_INFER_META(phi::GeneralBinaryGradInferMeta);
  fn(infer_meta);
}

phi::DataType Atan2GradOp::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: Atan2GradOp";
  


  return expected_kernel_dtype;
}

OpInfoTuple AtanGradOp::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("x", "paddle::dialect::DenseTensorType", false, false, false, false), paddle::dialect::OpInputInfo("out_grad", "paddle::dialect::DenseTensorType", false, false, false, false) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = {  };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("x_grad", "paddle::dialect::DenseTensorType", false, false) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("UnchangedInferMeta", {"x"}, "atan_grad", {"x", "out_grad"}, {}, {}, {}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "atan_grad");
}

void AtanGradOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, pir::Value out_grad_) {
  VLOG(4) << "Start build AtanGradOp";



  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_, out_grad_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;
  paddle::dialect::DenseTensorType out_grad = out_grad_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)out_grad;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);
  paddle::dialect::IrTensor dense_x_grad;
  paddle::dialect::IrMetaTensor meta_x_grad(&dense_x_grad);

  phi::UnchangedInferMeta(meta_x, &meta_x_grad);

  std::vector<pir::Type> argument_outputs;
  pir::Type x_grad_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_x_grad.dtype()), dense_x_grad.dims(), dense_x_grad.layout(), dense_x_grad.lod(), dense_x_grad.offset());
  argument_outputs.push_back(x_grad_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void AtanGradOp::VerifySig() {}

void AtanGradOp::InferMeta( phi::InferMetaContext *infer_meta ) {
  auto fn = PD_INFER_META(phi::UnchangedInferMeta);
  fn(infer_meta);
}

phi::DataType AtanGradOp::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: AtanGradOp";
  


  return expected_kernel_dtype;
}

OpInfoTuple AtanGrad_Op::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("x", "paddle::dialect::DenseTensorType", false, false, false, false), paddle::dialect::OpInputInfo("out_grad", "paddle::dialect::DenseTensorType", false, false, false, false) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = {  };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("x_grad", "paddle::dialect::DenseTensorType", false, false) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("UnchangedInferMeta", {"x"}, "atan_grad", {"x", "out_grad"}, {}, {}, {{"x_grad", "out_grad"}}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "atan_grad");
}

void AtanGrad_Op::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, pir::Value out_grad_) {
  VLOG(4) << "Start build AtanGrad_Op";



  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_, out_grad_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;
  paddle::dialect::DenseTensorType out_grad = out_grad_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)out_grad;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);
  paddle::dialect::IrTensor dense_x_grad;
  paddle::dialect::IrMetaTensor meta_x_grad(&dense_x_grad);

  phi::UnchangedInferMeta(meta_x, &meta_x_grad);

  std::vector<pir::Type> argument_outputs;
  pir::Type x_grad_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_x_grad.dtype()), dense_x_grad.dims(), dense_x_grad.layout(), dense_x_grad.lod(), dense_x_grad.offset());
  argument_outputs.push_back(x_grad_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void AtanGrad_Op::VerifySig() {}

void AtanGrad_Op::InferMeta( phi::InferMetaContext *infer_meta ) {
  auto fn = PD_INFER_META(phi::UnchangedInferMeta);
  fn(infer_meta);
}

phi::DataType AtanGrad_Op::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: AtanGrad_Op";
  


  return expected_kernel_dtype;
}

OpInfoTuple AtanhGradOp::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("x", "paddle::dialect::DenseTensorType", false, false, false, false), paddle::dialect::OpInputInfo("out_grad", "paddle::dialect::DenseTensorType", false, false, false, false) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = {  };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("x_grad", "paddle::dialect::DenseTensorType", false, false) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("UnchangedInferMeta", {"x"}, "atanh_grad", {"x", "out_grad"}, {}, {}, {}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "atanh_grad");
}

void AtanhGradOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, pir::Value out_grad_) {
  VLOG(4) << "Start build AtanhGradOp";



  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_, out_grad_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;
  paddle::dialect::DenseTensorType out_grad = out_grad_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)out_grad;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);
  paddle::dialect::IrTensor dense_x_grad;
  paddle::dialect::IrMetaTensor meta_x_grad(&dense_x_grad);

  phi::UnchangedInferMeta(meta_x, &meta_x_grad);

  std::vector<pir::Type> argument_outputs;
  pir::Type x_grad_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_x_grad.dtype()), dense_x_grad.dims(), dense_x_grad.layout(), dense_x_grad.lod(), dense_x_grad.offset());
  argument_outputs.push_back(x_grad_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void AtanhGradOp::VerifySig() {}

void AtanhGradOp::InferMeta( phi::InferMetaContext *infer_meta ) {
  auto fn = PD_INFER_META(phi::UnchangedInferMeta);
  fn(infer_meta);
}

phi::DataType AtanhGradOp::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: AtanhGradOp";
  


  return expected_kernel_dtype;
}

OpInfoTuple AtanhGrad_Op::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("x", "paddle::dialect::DenseTensorType", false, false, false, false), paddle::dialect::OpInputInfo("out_grad", "paddle::dialect::DenseTensorType", false, false, false, false) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = {  };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("x_grad", "paddle::dialect::DenseTensorType", false, false) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("UnchangedInferMeta", {"x"}, "atanh_grad", {"x", "out_grad"}, {}, {}, {{"x_grad", "out_grad"}}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "atanh_grad");
}

void AtanhGrad_Op::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, pir::Value out_grad_) {
  VLOG(4) << "Start build AtanhGrad_Op";



  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_, out_grad_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;
  paddle::dialect::DenseTensorType out_grad = out_grad_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)out_grad;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);
  paddle::dialect::IrTensor dense_x_grad;
  paddle::dialect::IrMetaTensor meta_x_grad(&dense_x_grad);

  phi::UnchangedInferMeta(meta_x, &meta_x_grad);

  std::vector<pir::Type> argument_outputs;
  pir::Type x_grad_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_x_grad.dtype()), dense_x_grad.dims(), dense_x_grad.layout(), dense_x_grad.lod(), dense_x_grad.offset());
  argument_outputs.push_back(x_grad_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void AtanhGrad_Op::VerifySig() {}

void AtanhGrad_Op::InferMeta( phi::InferMetaContext *infer_meta ) {
  auto fn = PD_INFER_META(phi::UnchangedInferMeta);
  fn(infer_meta);
}

phi::DataType AtanhGrad_Op::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: AtanhGrad_Op";
  


  return expected_kernel_dtype;
}

OpInfoTuple BceLossGradOp::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("input", "paddle::dialect::DenseTensorType", false, false, false, false), paddle::dialect::OpInputInfo("label", "paddle::dialect::DenseTensorType", false, false, false, false), paddle::dialect::OpInputInfo("out_grad", "paddle::dialect::DenseTensorType", false, false, false, false) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = {  };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("input_grad", "paddle::dialect::DenseTensorType", false, false) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("UnchangedInferMeta", {"input"}, "bce_loss_grad", {"input", "label", "out_grad"}, {}, {}, {}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "bce_loss_grad");
}

void BceLossGradOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value input_, pir::Value label_, pir::Value out_grad_) {
  VLOG(4) << "Start build BceLossGradOp";



  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {input_, label_, out_grad_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType input = input_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)input;
  paddle::dialect::DenseTensorType label = label_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)label;
  paddle::dialect::DenseTensorType out_grad = out_grad_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)out_grad;

  VLOG(4) << "Builder construction  dense_input";
  paddle::dialect::IrTensor ir_tensor_input(paddle::dialect::TransToPhiDataType(input.dtype()),
                                                      input.dims(),
                                                      input.data_layout(),
                                                      input.lod(),
                                                      input.offset());
  VLOG(4) << "Builder construction  meta_input";
  paddle::dialect::IrMetaTensor meta_input(&ir_tensor_input);
  paddle::dialect::IrTensor dense_input_grad;
  paddle::dialect::IrMetaTensor meta_input_grad(&dense_input_grad);

  phi::UnchangedInferMeta(meta_input, &meta_input_grad);

  std::vector<pir::Type> argument_outputs;
  pir::Type input_grad_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_input_grad.dtype()), dense_input_grad.dims(), dense_input_grad.layout(), dense_input_grad.lod(), dense_input_grad.offset());
  argument_outputs.push_back(input_grad_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void BceLossGradOp::VerifySig() {}

void BceLossGradOp::InferMeta( phi::InferMetaContext *infer_meta ) {
  auto fn = PD_INFER_META(phi::UnchangedInferMeta);
  fn(infer_meta);
}

phi::DataType BceLossGradOp::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: BceLossGradOp";
  


  return expected_kernel_dtype;
}

OpInfoTuple BceLossGrad_Op::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("input", "paddle::dialect::DenseTensorType", false, false, false, false), paddle::dialect::OpInputInfo("label", "paddle::dialect::DenseTensorType", false, false, false, false), paddle::dialect::OpInputInfo("out_grad", "paddle::dialect::DenseTensorType", false, false, false, false) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = {  };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("input_grad", "paddle::dialect::DenseTensorType", false, false) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("UnchangedInferMeta", {"input"}, "bce_loss_grad", {"input", "label", "out_grad"}, {}, {}, {{"input_grad", "out_grad"}}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "bce_loss_grad");
}

void BceLossGrad_Op::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value input_, pir::Value label_, pir::Value out_grad_) {
  VLOG(4) << "Start build BceLossGrad_Op";



  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {input_, label_, out_grad_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType input = input_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)input;
  paddle::dialect::DenseTensorType label = label_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)label;
  paddle::dialect::DenseTensorType out_grad = out_grad_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)out_grad;

  VLOG(4) << "Builder construction  dense_input";
  paddle::dialect::IrTensor ir_tensor_input(paddle::dialect::TransToPhiDataType(input.dtype()),
                                                      input.dims(),
                                                      input.data_layout(),
                                                      input.lod(),
                                                      input.offset());
  VLOG(4) << "Builder construction  meta_input";
  paddle::dialect::IrMetaTensor meta_input(&ir_tensor_input);
  paddle::dialect::IrTensor dense_input_grad;
  paddle::dialect::IrMetaTensor meta_input_grad(&dense_input_grad);

  phi::UnchangedInferMeta(meta_input, &meta_input_grad);

  std::vector<pir::Type> argument_outputs;
  pir::Type input_grad_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_input_grad.dtype()), dense_input_grad.dims(), dense_input_grad.layout(), dense_input_grad.lod(), dense_input_grad.offset());
  argument_outputs.push_back(input_grad_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void BceLossGrad_Op::VerifySig() {}

void BceLossGrad_Op::InferMeta( phi::InferMetaContext *infer_meta ) {
  auto fn = PD_INFER_META(phi::UnchangedInferMeta);
  fn(infer_meta);
}

phi::DataType BceLossGrad_Op::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: BceLossGrad_Op";
  


  return expected_kernel_dtype;
}

const char *BicubicInterpGradOp::attributes_name[8] = { "data_layout", "out_d", "out_h", "out_w", "scale", "interp_method", "align_corners", "align_mode" };

OpInfoTuple BicubicInterpGradOp::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("x", "paddle::dialect::DenseTensorType", false, true, false, false), paddle::dialect::OpInputInfo("out_size", "paddle::dialect::DenseTensorType", true, false, false, false), paddle::dialect::OpInputInfo("size_tensor", "pir::VectorType<paddle::dialect::DenseTensorType>", true, false, false, false), paddle::dialect::OpInputInfo("scale_tensor", "paddle::dialect::DenseTensorType", true, false, false, false), paddle::dialect::OpInputInfo("output_grad", "paddle::dialect::DenseTensorType", false, false, false, false) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = { paddle::dialect::OpAttributeInfo("data_layout", "pir::StrAttribute", ""), paddle::dialect::OpAttributeInfo("out_d", "pir::Int32Attribute", ""), paddle::dialect::OpAttributeInfo("out_h", "pir::Int32Attribute", ""), paddle::dialect::OpAttributeInfo("out_w", "pir::Int32Attribute", ""), paddle::dialect::OpAttributeInfo("scale", "pir::ArrayAttribute<pir::FloatAttribute>", ""), paddle::dialect::OpAttributeInfo("interp_method", "pir::StrAttribute", ""), paddle::dialect::OpAttributeInfo("align_corners", "pir::BoolAttribute", ""), paddle::dialect::OpAttributeInfo("align_mode", "pir::Int32Attribute", "") };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("x_grad", "paddle::dialect::DenseTensorType", false, false) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("UnchangedInferMeta", {"x"}, "bicubic_interp_grad", {"x", "out_size", "size_tensor", "scale_tensor", "output_grad", "data_layout", "out_d", "out_h", "out_w", "scale", "interp_method", "align_corners", "align_mode"}, {"output_grad"}, {}, {}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "bicubic_interp_grad");
}

void BicubicInterpGradOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, pir::Value out_size_, pir::Value size_tensor_, pir::Value scale_tensor_, pir::Value output_grad_, const std::string& data_layout, int out_d, int out_h, int out_w, const std::vector<float>& scale, const std::string& interp_method, bool align_corners, int align_mode) {
  VLOG(4) << "Start build BicubicInterpGradOp";



  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_, out_size_, size_tensor_, scale_tensor_, output_grad_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_data_layout = pir::StrAttribute::get(pir::IrContext::Instance(), data_layout);
  argument.AddAttribute("data_layout", attr_data_layout);
  pir::Attribute attr_out_d = pir::Int32Attribute::get(pir::IrContext::Instance(), out_d);
  argument.AddAttribute("out_d", attr_out_d);
  pir::Attribute attr_out_h = pir::Int32Attribute::get(pir::IrContext::Instance(), out_h);
  argument.AddAttribute("out_h", attr_out_h);
  pir::Attribute attr_out_w = pir::Int32Attribute::get(pir::IrContext::Instance(), out_w);
  argument.AddAttribute("out_w", attr_out_w);
  std::vector<pir::Attribute> vec_scale;
  for (size_t i = 0; i < static_cast<size_t>(scale.size()); i++) {
      pir::Attribute attr_scale = pir::FloatAttribute::get(pir::IrContext::Instance(), scale[i]);

    vec_scale.push_back(attr_scale);
  }
  pir::Attribute attr_scale = pir::ArrayAttribute::get(pir::IrContext::Instance(), vec_scale);
  argument.AddAttribute("scale", attr_scale);
  pir::Attribute attr_interp_method = pir::StrAttribute::get(pir::IrContext::Instance(), interp_method);
  argument.AddAttribute("interp_method", attr_interp_method);
  pir::Attribute attr_align_corners = pir::BoolAttribute::get(pir::IrContext::Instance(), align_corners);
  argument.AddAttribute("align_corners", attr_align_corners);
  pir::Attribute attr_align_mode = pir::Int32Attribute::get(pir::IrContext::Instance(), align_mode);
  argument.AddAttribute("align_mode", attr_align_mode);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;
  paddle::dialect::DenseTensorType output_grad = output_grad_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)output_grad;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);
  paddle::dialect::IrTensor dense_x_grad;
  paddle::dialect::IrMetaTensor meta_x_grad(&dense_x_grad);

  phi::UnchangedInferMeta(meta_x, &meta_x_grad);

  std::vector<pir::Type> argument_outputs;
  pir::Type x_grad_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_x_grad.dtype()), dense_x_grad.dims(), dense_x_grad.layout(), dense_x_grad.lod(), dense_x_grad.offset());
  argument_outputs.push_back(x_grad_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void BicubicInterpGradOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, pir::Value out_size_, pir::Value size_tensor_, pir::Value scale_tensor_, pir::Value output_grad_, pir::AttributeMap attributes) {
  VLOG(4) << "Start build BicubicInterpGradOp";


  IR_ENFORCE(
      attributes.find("data_layout") != attributes.end(),
          "'data_layout' Attribute is expected for BicubicInterpGradOp. ");
  std::string data_layout = attributes.at("data_layout").dyn_cast<pir::StrAttribute>().AsString();

  IR_ENFORCE(
      attributes.find("out_d") != attributes.end(),
          "'out_d' Attribute is expected for BicubicInterpGradOp. ");
  int out_d = attributes.at("out_d").dyn_cast<pir::Int32Attribute>().data();

  IR_ENFORCE(
      attributes.find("out_h") != attributes.end(),
          "'out_h' Attribute is expected for BicubicInterpGradOp. ");
  int out_h = attributes.at("out_h").dyn_cast<pir::Int32Attribute>().data();

  IR_ENFORCE(
      attributes.find("out_w") != attributes.end(),
          "'out_w' Attribute is expected for BicubicInterpGradOp. ");
  int out_w = attributes.at("out_w").dyn_cast<pir::Int32Attribute>().data();

  IR_ENFORCE(
      attributes.find("scale") != attributes.end(),
          "'scale' Attribute is expected for BicubicInterpGradOp. ");
  std::vector<float> scale;
  for (size_t i = 0; i < attributes.at("scale").dyn_cast<pir::ArrayAttribute>().size(); i++) {
    scale.push_back(attributes.at("scale").dyn_cast<pir::ArrayAttribute>().at(i).dyn_cast<pir::FloatAttribute>().data());
  }

  IR_ENFORCE(
      attributes.find("interp_method") != attributes.end(),
          "'interp_method' Attribute is expected for BicubicInterpGradOp. ");
  std::string interp_method = attributes.at("interp_method").dyn_cast<pir::StrAttribute>().AsString();

  IR_ENFORCE(
      attributes.find("align_corners") != attributes.end(),
          "'align_corners' Attribute is expected for BicubicInterpGradOp. ");
  bool align_corners = attributes.at("align_corners").dyn_cast<pir::BoolAttribute>().data();

  IR_ENFORCE(
      attributes.find("align_mode") != attributes.end(),
          "'align_mode' Attribute is expected for BicubicInterpGradOp. ");
  int align_mode = attributes.at("align_mode").dyn_cast<pir::Int32Attribute>().data();


  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_, out_size_, size_tensor_, scale_tensor_, output_grad_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_data_layout = pir::StrAttribute::get(pir::IrContext::Instance(), data_layout);
  argument.AddAttribute("data_layout", attr_data_layout);
  pir::Attribute attr_out_d = pir::Int32Attribute::get(pir::IrContext::Instance(), out_d);
  argument.AddAttribute("out_d", attr_out_d);
  pir::Attribute attr_out_h = pir::Int32Attribute::get(pir::IrContext::Instance(), out_h);
  argument.AddAttribute("out_h", attr_out_h);
  pir::Attribute attr_out_w = pir::Int32Attribute::get(pir::IrContext::Instance(), out_w);
  argument.AddAttribute("out_w", attr_out_w);
  std::vector<pir::Attribute> vec_scale;
  for (size_t i = 0; i < static_cast<size_t>(scale.size()); i++) {
      pir::Attribute attr_scale = pir::FloatAttribute::get(pir::IrContext::Instance(), scale[i]);

    vec_scale.push_back(attr_scale);
  }
  pir::Attribute attr_scale = pir::ArrayAttribute::get(pir::IrContext::Instance(), vec_scale);
  argument.AddAttribute("scale", attr_scale);
  pir::Attribute attr_interp_method = pir::StrAttribute::get(pir::IrContext::Instance(), interp_method);
  argument.AddAttribute("interp_method", attr_interp_method);
  pir::Attribute attr_align_corners = pir::BoolAttribute::get(pir::IrContext::Instance(), align_corners);
  argument.AddAttribute("align_corners", attr_align_corners);
  pir::Attribute attr_align_mode = pir::Int32Attribute::get(pir::IrContext::Instance(), align_mode);
  argument.AddAttribute("align_mode", attr_align_mode);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;
  paddle::dialect::DenseTensorType output_grad = output_grad_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)output_grad;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);
  paddle::dialect::IrTensor dense_x_grad;
  paddle::dialect::IrMetaTensor meta_x_grad(&dense_x_grad);

  phi::UnchangedInferMeta(meta_x, &meta_x_grad);

  std::vector<pir::Type> argument_outputs;
  pir::Type x_grad_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_x_grad.dtype()), dense_x_grad.dims(), dense_x_grad.layout(), dense_x_grad.lod(), dense_x_grad.offset());
  argument_outputs.push_back(x_grad_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void BicubicInterpGradOp::VerifySig() {}

void BicubicInterpGradOp::InferMeta( phi::InferMetaContext *infer_meta ) {
  auto fn = PD_INFER_META(phi::UnchangedInferMeta);
  fn(infer_meta);
}

phi::DataType BicubicInterpGradOp::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: BicubicInterpGradOp";
  

  // deal skip data transform
  if (var_name == "out_size" || var_name == "size_tensor" || var_name == "scale_tensor"){
    return expected_kernel_dtype;
  }


  return expected_kernel_dtype;
}

OpInfoTuple BilinearGradOp::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("x", "paddle::dialect::DenseTensorType", false, false, false, false), paddle::dialect::OpInputInfo("y", "paddle::dialect::DenseTensorType", false, false, false, false), paddle::dialect::OpInputInfo("weight", "paddle::dialect::DenseTensorType", false, false, false, false), paddle::dialect::OpInputInfo("out_grad", "paddle::dialect::DenseTensorType", false, false, false, false) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = {  };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("x_grad", "paddle::dialect::DenseTensorType", false, false), paddle::dialect::OpOutputInfo("y_grad", "paddle::dialect::DenseTensorType", false, false), paddle::dialect::OpOutputInfo("weight_grad", "paddle::dialect::DenseTensorType", false, false), paddle::dialect::OpOutputInfo("bias_grad", "paddle::dialect::DenseTensorType", false, false) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("BilinearGradInferMeta", {"x", "y", "weight", "out_grad"}, "bilinear_grad", {"x", "y", "weight", "out_grad"}, {}, {}, {}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "bilinear_grad");
}

void BilinearGradOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, pir::Value y_, pir::Value weight_, pir::Value out_grad_) {
  VLOG(4) << "Start build BilinearGradOp";



  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_, y_, weight_, out_grad_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;
  paddle::dialect::DenseTensorType y = y_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)y;
  paddle::dialect::DenseTensorType weight = weight_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)weight;
  paddle::dialect::DenseTensorType out_grad = out_grad_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)out_grad;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);

  VLOG(4) << "Builder construction  dense_y";
  paddle::dialect::IrTensor ir_tensor_y(paddle::dialect::TransToPhiDataType(y.dtype()),
                                                      y.dims(),
                                                      y.data_layout(),
                                                      y.lod(),
                                                      y.offset());
  VLOG(4) << "Builder construction  meta_y";
  paddle::dialect::IrMetaTensor meta_y(&ir_tensor_y);

  VLOG(4) << "Builder construction  dense_weight";
  paddle::dialect::IrTensor ir_tensor_weight(paddle::dialect::TransToPhiDataType(weight.dtype()),
                                                      weight.dims(),
                                                      weight.data_layout(),
                                                      weight.lod(),
                                                      weight.offset());
  VLOG(4) << "Builder construction  meta_weight";
  paddle::dialect::IrMetaTensor meta_weight(&ir_tensor_weight);

  VLOG(4) << "Builder construction  dense_out_grad";
  paddle::dialect::IrTensor ir_tensor_out_grad(paddle::dialect::TransToPhiDataType(out_grad.dtype()),
                                                      out_grad.dims(),
                                                      out_grad.data_layout(),
                                                      out_grad.lod(),
                                                      out_grad.offset());
  VLOG(4) << "Builder construction  meta_out_grad";
  paddle::dialect::IrMetaTensor meta_out_grad(&ir_tensor_out_grad);
  paddle::dialect::IrTensor dense_x_grad;
  paddle::dialect::IrMetaTensor meta_x_grad(&dense_x_grad);
  paddle::dialect::IrTensor dense_y_grad;
  paddle::dialect::IrMetaTensor meta_y_grad(&dense_y_grad);
  paddle::dialect::IrTensor dense_weight_grad;
  paddle::dialect::IrMetaTensor meta_weight_grad(&dense_weight_grad);
  paddle::dialect::IrTensor dense_bias_grad;
  paddle::dialect::IrMetaTensor meta_bias_grad(&dense_bias_grad);

  phi::BilinearGradInferMeta(meta_x, meta_y, meta_weight, meta_out_grad, &meta_x_grad, &meta_y_grad, &meta_weight_grad, &meta_bias_grad);

  std::vector<pir::Type> argument_outputs;
  pir::Type x_grad_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_x_grad.dtype()), dense_x_grad.dims(), dense_x_grad.layout(), dense_x_grad.lod(), dense_x_grad.offset());
  argument_outputs.push_back(x_grad_dense_tensor_type);

  pir::Type y_grad_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_y_grad.dtype()), dense_y_grad.dims(), dense_y_grad.layout(), dense_y_grad.lod(), dense_y_grad.offset());
  argument_outputs.push_back(y_grad_dense_tensor_type);

  pir::Type weight_grad_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_weight_grad.dtype()), dense_weight_grad.dims(), dense_weight_grad.layout(), dense_weight_grad.lod(), dense_weight_grad.offset());
  argument_outputs.push_back(weight_grad_dense_tensor_type);

  pir::Type bias_grad_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_bias_grad.dtype()), dense_bias_grad.dims(), dense_bias_grad.layout(), dense_bias_grad.lod(), dense_bias_grad.offset());
  argument_outputs.push_back(bias_grad_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void BilinearGradOp::VerifySig() {}

void BilinearGradOp::InferMeta( phi::InferMetaContext *infer_meta ) {
  auto fn = PD_INFER_META(phi::BilinearGradInferMeta);
  fn(infer_meta);
}

phi::DataType BilinearGradOp::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: BilinearGradOp";
  


  return expected_kernel_dtype;
}

const char *BilinearInterpGradOp::attributes_name[8] = { "data_layout", "out_d", "out_h", "out_w", "scale", "interp_method", "align_corners", "align_mode" };

OpInfoTuple BilinearInterpGradOp::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("x", "paddle::dialect::DenseTensorType", false, true, false, false), paddle::dialect::OpInputInfo("out_size", "paddle::dialect::DenseTensorType", true, false, false, false), paddle::dialect::OpInputInfo("size_tensor", "pir::VectorType<paddle::dialect::DenseTensorType>", true, false, false, false), paddle::dialect::OpInputInfo("scale_tensor", "paddle::dialect::DenseTensorType", true, false, false, false), paddle::dialect::OpInputInfo("output_grad", "paddle::dialect::DenseTensorType", false, false, false, false) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = { paddle::dialect::OpAttributeInfo("data_layout", "pir::StrAttribute", ""), paddle::dialect::OpAttributeInfo("out_d", "pir::Int32Attribute", ""), paddle::dialect::OpAttributeInfo("out_h", "pir::Int32Attribute", ""), paddle::dialect::OpAttributeInfo("out_w", "pir::Int32Attribute", ""), paddle::dialect::OpAttributeInfo("scale", "pir::ArrayAttribute<pir::FloatAttribute>", ""), paddle::dialect::OpAttributeInfo("interp_method", "pir::StrAttribute", ""), paddle::dialect::OpAttributeInfo("align_corners", "pir::BoolAttribute", ""), paddle::dialect::OpAttributeInfo("align_mode", "pir::Int32Attribute", "") };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("x_grad", "paddle::dialect::DenseTensorType", false, false) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("UnchangedInferMeta", {"x"}, "bilinear_interp_grad", {"x", "out_size", "size_tensor", "scale_tensor", "output_grad", "data_layout", "out_d", "out_h", "out_w", "scale", "interp_method", "align_corners", "align_mode"}, {"output_grad"}, {}, {}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "bilinear_interp_grad");
}

void BilinearInterpGradOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, pir::Value out_size_, pir::Value size_tensor_, pir::Value scale_tensor_, pir::Value output_grad_, const std::string& data_layout, int out_d, int out_h, int out_w, const std::vector<float>& scale, const std::string& interp_method, bool align_corners, int align_mode) {
  VLOG(4) << "Start build BilinearInterpGradOp";



  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_, out_size_, size_tensor_, scale_tensor_, output_grad_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_data_layout = pir::StrAttribute::get(pir::IrContext::Instance(), data_layout);
  argument.AddAttribute("data_layout", attr_data_layout);
  pir::Attribute attr_out_d = pir::Int32Attribute::get(pir::IrContext::Instance(), out_d);
  argument.AddAttribute("out_d", attr_out_d);
  pir::Attribute attr_out_h = pir::Int32Attribute::get(pir::IrContext::Instance(), out_h);
  argument.AddAttribute("out_h", attr_out_h);
  pir::Attribute attr_out_w = pir::Int32Attribute::get(pir::IrContext::Instance(), out_w);
  argument.AddAttribute("out_w", attr_out_w);
  std::vector<pir::Attribute> vec_scale;
  for (size_t i = 0; i < static_cast<size_t>(scale.size()); i++) {
      pir::Attribute attr_scale = pir::FloatAttribute::get(pir::IrContext::Instance(), scale[i]);

    vec_scale.push_back(attr_scale);
  }
  pir::Attribute attr_scale = pir::ArrayAttribute::get(pir::IrContext::Instance(), vec_scale);
  argument.AddAttribute("scale", attr_scale);
  pir::Attribute attr_interp_method = pir::StrAttribute::get(pir::IrContext::Instance(), interp_method);
  argument.AddAttribute("interp_method", attr_interp_method);
  pir::Attribute attr_align_corners = pir::BoolAttribute::get(pir::IrContext::Instance(), align_corners);
  argument.AddAttribute("align_corners", attr_align_corners);
  pir::Attribute attr_align_mode = pir::Int32Attribute::get(pir::IrContext::Instance(), align_mode);
  argument.AddAttribute("align_mode", attr_align_mode);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;
  paddle::dialect::DenseTensorType output_grad = output_grad_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)output_grad;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);
  paddle::dialect::IrTensor dense_x_grad;
  paddle::dialect::IrMetaTensor meta_x_grad(&dense_x_grad);

  phi::UnchangedInferMeta(meta_x, &meta_x_grad);

  std::vector<pir::Type> argument_outputs;
  pir::Type x_grad_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_x_grad.dtype()), dense_x_grad.dims(), dense_x_grad.layout(), dense_x_grad.lod(), dense_x_grad.offset());
  argument_outputs.push_back(x_grad_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void BilinearInterpGradOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, pir::Value out_size_, pir::Value size_tensor_, pir::Value scale_tensor_, pir::Value output_grad_, pir::AttributeMap attributes) {
  VLOG(4) << "Start build BilinearInterpGradOp";


  IR_ENFORCE(
      attributes.find("data_layout") != attributes.end(),
          "'data_layout' Attribute is expected for BilinearInterpGradOp. ");
  std::string data_layout = attributes.at("data_layout").dyn_cast<pir::StrAttribute>().AsString();

  IR_ENFORCE(
      attributes.find("out_d") != attributes.end(),
          "'out_d' Attribute is expected for BilinearInterpGradOp. ");
  int out_d = attributes.at("out_d").dyn_cast<pir::Int32Attribute>().data();

  IR_ENFORCE(
      attributes.find("out_h") != attributes.end(),
          "'out_h' Attribute is expected for BilinearInterpGradOp. ");
  int out_h = attributes.at("out_h").dyn_cast<pir::Int32Attribute>().data();

  IR_ENFORCE(
      attributes.find("out_w") != attributes.end(),
          "'out_w' Attribute is expected for BilinearInterpGradOp. ");
  int out_w = attributes.at("out_w").dyn_cast<pir::Int32Attribute>().data();

  IR_ENFORCE(
      attributes.find("scale") != attributes.end(),
          "'scale' Attribute is expected for BilinearInterpGradOp. ");
  std::vector<float> scale;
  for (size_t i = 0; i < attributes.at("scale").dyn_cast<pir::ArrayAttribute>().size(); i++) {
    scale.push_back(attributes.at("scale").dyn_cast<pir::ArrayAttribute>().at(i).dyn_cast<pir::FloatAttribute>().data());
  }

  IR_ENFORCE(
      attributes.find("interp_method") != attributes.end(),
          "'interp_method' Attribute is expected for BilinearInterpGradOp. ");
  std::string interp_method = attributes.at("interp_method").dyn_cast<pir::StrAttribute>().AsString();

  IR_ENFORCE(
      attributes.find("align_corners") != attributes.end(),
          "'align_corners' Attribute is expected for BilinearInterpGradOp. ");
  bool align_corners = attributes.at("align_corners").dyn_cast<pir::BoolAttribute>().data();

  IR_ENFORCE(
      attributes.find("align_mode") != attributes.end(),
          "'align_mode' Attribute is expected for BilinearInterpGradOp. ");
  int align_mode = attributes.at("align_mode").dyn_cast<pir::Int32Attribute>().data();


  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_, out_size_, size_tensor_, scale_tensor_, output_grad_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_data_layout = pir::StrAttribute::get(pir::IrContext::Instance(), data_layout);
  argument.AddAttribute("data_layout", attr_data_layout);
  pir::Attribute attr_out_d = pir::Int32Attribute::get(pir::IrContext::Instance(), out_d);
  argument.AddAttribute("out_d", attr_out_d);
  pir::Attribute attr_out_h = pir::Int32Attribute::get(pir::IrContext::Instance(), out_h);
  argument.AddAttribute("out_h", attr_out_h);
  pir::Attribute attr_out_w = pir::Int32Attribute::get(pir::IrContext::Instance(), out_w);
  argument.AddAttribute("out_w", attr_out_w);
  std::vector<pir::Attribute> vec_scale;
  for (size_t i = 0; i < static_cast<size_t>(scale.size()); i++) {
      pir::Attribute attr_scale = pir::FloatAttribute::get(pir::IrContext::Instance(), scale[i]);

    vec_scale.push_back(attr_scale);
  }
  pir::Attribute attr_scale = pir::ArrayAttribute::get(pir::IrContext::Instance(), vec_scale);
  argument.AddAttribute("scale", attr_scale);
  pir::Attribute attr_interp_method = pir::StrAttribute::get(pir::IrContext::Instance(), interp_method);
  argument.AddAttribute("interp_method", attr_interp_method);
  pir::Attribute attr_align_corners = pir::BoolAttribute::get(pir::IrContext::Instance(), align_corners);
  argument.AddAttribute("align_corners", attr_align_corners);
  pir::Attribute attr_align_mode = pir::Int32Attribute::get(pir::IrContext::Instance(), align_mode);
  argument.AddAttribute("align_mode", attr_align_mode);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;
  paddle::dialect::DenseTensorType output_grad = output_grad_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)output_grad;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);
  paddle::dialect::IrTensor dense_x_grad;
  paddle::dialect::IrMetaTensor meta_x_grad(&dense_x_grad);

  phi::UnchangedInferMeta(meta_x, &meta_x_grad);

  std::vector<pir::Type> argument_outputs;
  pir::Type x_grad_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_x_grad.dtype()), dense_x_grad.dims(), dense_x_grad.layout(), dense_x_grad.lod(), dense_x_grad.offset());
  argument_outputs.push_back(x_grad_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void BilinearInterpGradOp::VerifySig() {}

void BilinearInterpGradOp::InferMeta( phi::InferMetaContext *infer_meta ) {
  auto fn = PD_INFER_META(phi::UnchangedInferMeta);
  fn(infer_meta);
}

phi::DataType BilinearInterpGradOp::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: BilinearInterpGradOp";
  

  // deal skip data transform
  if (var_name == "out_size" || var_name == "size_tensor" || var_name == "scale_tensor"){
    return expected_kernel_dtype;
  }


  return expected_kernel_dtype;
}

OpInfoTuple BmmGradOp::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("x", "paddle::dialect::DenseTensorType", false, false, false, false), paddle::dialect::OpInputInfo("y", "paddle::dialect::DenseTensorType", false, false, false, false), paddle::dialect::OpInputInfo("out_grad", "paddle::dialect::DenseTensorType", false, false, false, false) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = {  };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("x_grad", "paddle::dialect::DenseTensorType", false, false), paddle::dialect::OpOutputInfo("y_grad", "paddle::dialect::DenseTensorType", false, false) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("BmmGradInferMeta", {"x", "y", "out_grad"}, "bmm_grad", {"x", "y", "out_grad"}, {"out_grad"}, {}, {}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "bmm_grad");
}

void BmmGradOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, pir::Value y_, pir::Value out_grad_) {
  VLOG(4) << "Start build BmmGradOp";



  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_, y_, out_grad_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;
  paddle::dialect::DenseTensorType y = y_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)y;
  paddle::dialect::DenseTensorType out_grad = out_grad_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)out_grad;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);

  VLOG(4) << "Builder construction  dense_y";
  paddle::dialect::IrTensor ir_tensor_y(paddle::dialect::TransToPhiDataType(y.dtype()),
                                                      y.dims(),
                                                      y.data_layout(),
                                                      y.lod(),
                                                      y.offset());
  VLOG(4) << "Builder construction  meta_y";
  paddle::dialect::IrMetaTensor meta_y(&ir_tensor_y);

  VLOG(4) << "Builder construction  dense_out_grad";
  paddle::dialect::IrTensor ir_tensor_out_grad(paddle::dialect::TransToPhiDataType(out_grad.dtype()),
                                                      out_grad.dims(),
                                                      out_grad.data_layout(),
                                                      out_grad.lod(),
                                                      out_grad.offset());
  VLOG(4) << "Builder construction  meta_out_grad";
  paddle::dialect::IrMetaTensor meta_out_grad(&ir_tensor_out_grad);
  paddle::dialect::IrTensor dense_x_grad;
  paddle::dialect::IrMetaTensor meta_x_grad(&dense_x_grad);
  paddle::dialect::IrTensor dense_y_grad;
  paddle::dialect::IrMetaTensor meta_y_grad(&dense_y_grad);

  phi::BmmGradInferMeta(meta_x, meta_y, meta_out_grad, &meta_x_grad, &meta_y_grad);

  std::vector<pir::Type> argument_outputs;
  pir::Type x_grad_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_x_grad.dtype()), dense_x_grad.dims(), dense_x_grad.layout(), dense_x_grad.lod(), dense_x_grad.offset());
  argument_outputs.push_back(x_grad_dense_tensor_type);

  pir::Type y_grad_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_y_grad.dtype()), dense_y_grad.dims(), dense_y_grad.layout(), dense_y_grad.lod(), dense_y_grad.offset());
  argument_outputs.push_back(y_grad_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void BmmGradOp::VerifySig() {}

void BmmGradOp::InferMeta( phi::InferMetaContext *infer_meta ) {
  auto fn = PD_INFER_META(phi::BmmGradInferMeta);
  fn(infer_meta);
}

phi::DataType BmmGradOp::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: BmmGradOp";
  


  return expected_kernel_dtype;
}

OpInfoTuple BroadcastTensorsGradOp::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("input", "pir::VectorType<paddle::dialect::DenseTensorType>", false, true, false, false), paddle::dialect::OpInputInfo("out_grad", "pir::VectorType<paddle::dialect::DenseTensorType>", false, false, false, false) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = {  };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("input_grad", "pir::VectorType<paddle::dialect::DenseTensorType>", false, false) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("UnchangedMultiInferMeta", {"input"}, "broadcast_tensors_grad", {"input", "out_grad"}, {"out_grad"}, {}, {}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "broadcast_tensors_grad");
}

void BroadcastTensorsGradOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value input_, pir::Value out_grad_) {
  VLOG(4) << "Start build BroadcastTensorsGradOp";



  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {input_, out_grad_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";

  VLOG(4) << "Builder construction outputs";
  pir::VectorType input = input_.type().dyn_cast<pir::VectorType>(); (void)input;
  pir::VectorType out_grad = out_grad_.type().dyn_cast<pir::VectorType>(); (void)out_grad;
  std::vector<paddle::dialect::IrTensor> vec_ir_tensor_input;
  for (size_t i=0; i < static_cast<size_t>(input.size()); i++) {
    vec_ir_tensor_input.push_back(paddle::dialect::IrTensor(paddle::dialect::TransToPhiDataType(input[i].dyn_cast<paddle::dialect::DenseTensorType>().dtype()),
                                                                     input[i].dyn_cast<paddle::dialect::DenseTensorType>().dims(),
                                                                     input[i].dyn_cast<paddle::dialect::DenseTensorType>().data_layout(),
                                                                     input[i].dyn_cast<paddle::dialect::DenseTensorType>().lod(),
                                                                     input[i].dyn_cast<paddle::dialect::DenseTensorType>().offset()));
  }
  std::vector<paddle::dialect::IrMetaTensor> vec_meta_input;
  for (size_t i=0; i < vec_ir_tensor_input.size(); i++) {
    vec_meta_input.push_back(paddle::dialect::IrMetaTensor(&vec_ir_tensor_input[i]));
  }

  std::vector<const phi::MetaTensor*> meta_input;
  for (size_t i=0; i < static_cast<size_t>(vec_meta_input.size()); i++) {
    meta_input.push_back(&vec_meta_input[i]);
  }
   std::vector<paddle::dialect::IrTensor> vec_dense_input_grad((input.size()), paddle::dialect::IrTensor());
  std::vector<paddle::dialect::IrMetaTensor> vec_meta_input_grad;
  for (size_t i=0; i < static_cast<size_t>(input.size()); i++) {
    vec_meta_input_grad.push_back(paddle::dialect::IrMetaTensor(&vec_dense_input_grad[i]));
  }
  std::vector<phi::MetaTensor*> meta_input_grad;
  for (size_t i=0; i < static_cast<size_t>(vec_meta_input_grad.size()); i++) {
    meta_input_grad.push_back(&vec_meta_input_grad[i]);
  }

  phi::UnchangedMultiInferMeta(meta_input, meta_input_grad);

  std::vector<pir::Type> argument_outputs;
  std::vector<pir::Type> input_grad_types;
  for (size_t i=0; i < static_cast<size_t>(input.size()); i++) {
    input_grad_types.push_back(paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(vec_dense_input_grad[i].dtype()), vec_dense_input_grad[i].dims(), vec_dense_input_grad[i].layout(), vec_dense_input_grad[i].lod(), vec_dense_input_grad[i].offset()));
  }
  pir::Type input_grad_vector_type = pir::VectorType::get(pir::IrContext::Instance(), input_grad_types);
  argument_outputs.push_back(input_grad_vector_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void BroadcastTensorsGradOp::VerifySig() {}

void BroadcastTensorsGradOp::InferMeta( phi::InferMetaContext *infer_meta ) {
  auto fn = PD_INFER_META(phi::UnchangedMultiInferMeta);
  fn(infer_meta);
}

phi::DataType BroadcastTensorsGradOp::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: BroadcastTensorsGradOp";
  


  return expected_kernel_dtype;
}

OpInfoTuple CeilGradOp::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("out_grad", "paddle::dialect::DenseTensorType", false, false, false, false) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = {  };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("x_grad", "paddle::dialect::DenseTensorType", false, false) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("UnchangedInferMeta", {"out_grad"}, "ceil_grad", {"out_grad"}, {}, {}, {}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "ceil_grad");
}

void CeilGradOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value out_grad_) {
  VLOG(4) << "Start build CeilGradOp";



  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {out_grad_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType out_grad = out_grad_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)out_grad;

  VLOG(4) << "Builder construction  dense_out_grad";
  paddle::dialect::IrTensor ir_tensor_out_grad(paddle::dialect::TransToPhiDataType(out_grad.dtype()),
                                                      out_grad.dims(),
                                                      out_grad.data_layout(),
                                                      out_grad.lod(),
                                                      out_grad.offset());
  VLOG(4) << "Builder construction  meta_out_grad";
  paddle::dialect::IrMetaTensor meta_out_grad(&ir_tensor_out_grad);
  paddle::dialect::IrTensor dense_x_grad;
  paddle::dialect::IrMetaTensor meta_x_grad(&dense_x_grad);

  phi::UnchangedInferMeta(meta_out_grad, &meta_x_grad);

  std::vector<pir::Type> argument_outputs;
  pir::Type x_grad_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_x_grad.dtype()), dense_x_grad.dims(), dense_x_grad.layout(), dense_x_grad.lod(), dense_x_grad.offset());
  argument_outputs.push_back(x_grad_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void CeilGradOp::VerifySig() {}

void CeilGradOp::InferMeta( phi::InferMetaContext *infer_meta ) {
  auto fn = PD_INFER_META(phi::UnchangedInferMeta);
  fn(infer_meta);
}

phi::DataType CeilGradOp::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: CeilGradOp";
  


  return expected_kernel_dtype;
}

OpInfoTuple CeilGrad_Op::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("out_grad", "paddle::dialect::DenseTensorType", false, false, false, false) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = {  };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("x_grad", "paddle::dialect::DenseTensorType", false, false) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("UnchangedInferMeta", {"out_grad"}, "ceil_grad", {"out_grad"}, {}, {}, {{"x_grad", "out_grad"}}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "ceil_grad");
}

void CeilGrad_Op::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value out_grad_) {
  VLOG(4) << "Start build CeilGrad_Op";



  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {out_grad_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType out_grad = out_grad_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)out_grad;

  VLOG(4) << "Builder construction  dense_out_grad";
  paddle::dialect::IrTensor ir_tensor_out_grad(paddle::dialect::TransToPhiDataType(out_grad.dtype()),
                                                      out_grad.dims(),
                                                      out_grad.data_layout(),
                                                      out_grad.lod(),
                                                      out_grad.offset());
  VLOG(4) << "Builder construction  meta_out_grad";
  paddle::dialect::IrMetaTensor meta_out_grad(&ir_tensor_out_grad);
  paddle::dialect::IrTensor dense_x_grad;
  paddle::dialect::IrMetaTensor meta_x_grad(&dense_x_grad);

  phi::UnchangedInferMeta(meta_out_grad, &meta_x_grad);

  std::vector<pir::Type> argument_outputs;
  pir::Type x_grad_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_x_grad.dtype()), dense_x_grad.dims(), dense_x_grad.layout(), dense_x_grad.lod(), dense_x_grad.offset());
  argument_outputs.push_back(x_grad_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void CeilGrad_Op::VerifySig() {}

void CeilGrad_Op::InferMeta( phi::InferMetaContext *infer_meta ) {
  auto fn = PD_INFER_META(phi::UnchangedInferMeta);
  fn(infer_meta);
}

phi::DataType CeilGrad_Op::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: CeilGrad_Op";
  


  return expected_kernel_dtype;
}

const char *CeluDoubleGradOp::attributes_name[1] = { "alpha" };

OpInfoTuple CeluDoubleGradOp::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("x", "paddle::dialect::DenseTensorType", false, false, false, false), paddle::dialect::OpInputInfo("grad_out", "paddle::dialect::DenseTensorType", false, false, false, false), paddle::dialect::OpInputInfo("grad_x_grad", "paddle::dialect::DenseTensorType", false, false, false, false) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = { paddle::dialect::OpAttributeInfo("alpha", "pir::FloatAttribute", "") };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("x_grad", "paddle::dialect::DenseTensorType", false, false), paddle::dialect::OpOutputInfo("grad_out_grad", "paddle::dialect::DenseTensorType", false, false) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("GeneralBinaryGradInferMeta", {"x", "x"}, "celu_double_grad", {"x", "grad_out", "grad_x_grad", "alpha"}, {}, {}, {}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "celu_double_grad");
}

void CeluDoubleGradOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, pir::Value grad_out_, pir::Value grad_x_grad_, float alpha) {
  VLOG(4) << "Start build CeluDoubleGradOp";



  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_, grad_out_, grad_x_grad_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_alpha = pir::FloatAttribute::get(pir::IrContext::Instance(), alpha);
  argument.AddAttribute("alpha", attr_alpha);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;
  paddle::dialect::DenseTensorType grad_out = grad_out_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)grad_out;
  paddle::dialect::DenseTensorType grad_x_grad = grad_x_grad_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)grad_x_grad;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);
  paddle::dialect::IrTensor dense_x_grad;
  paddle::dialect::IrMetaTensor meta_x_grad(&dense_x_grad);
  paddle::dialect::IrTensor dense_grad_out_grad;
  paddle::dialect::IrMetaTensor meta_grad_out_grad(&dense_grad_out_grad);

  phi::GeneralBinaryGradInferMeta(meta_x, meta_x, &meta_x_grad, &meta_grad_out_grad);

  std::vector<pir::Type> argument_outputs;
  pir::Type x_grad_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_x_grad.dtype()), dense_x_grad.dims(), dense_x_grad.layout(), dense_x_grad.lod(), dense_x_grad.offset());
  argument_outputs.push_back(x_grad_dense_tensor_type);

  pir::Type grad_out_grad_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_grad_out_grad.dtype()), dense_grad_out_grad.dims(), dense_grad_out_grad.layout(), dense_grad_out_grad.lod(), dense_grad_out_grad.offset());
  argument_outputs.push_back(grad_out_grad_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void CeluDoubleGradOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, pir::Value grad_out_, pir::Value grad_x_grad_, pir::AttributeMap attributes) {
  VLOG(4) << "Start build CeluDoubleGradOp";


  IR_ENFORCE(
      attributes.find("alpha") != attributes.end(),
          "'alpha' Attribute is expected for CeluDoubleGradOp. ");
  float alpha = attributes.at("alpha").dyn_cast<pir::FloatAttribute>().data();


  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_, grad_out_, grad_x_grad_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_alpha = pir::FloatAttribute::get(pir::IrContext::Instance(), alpha);
  argument.AddAttribute("alpha", attr_alpha);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;
  paddle::dialect::DenseTensorType grad_out = grad_out_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)grad_out;
  paddle::dialect::DenseTensorType grad_x_grad = grad_x_grad_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)grad_x_grad;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);
  paddle::dialect::IrTensor dense_x_grad;
  paddle::dialect::IrMetaTensor meta_x_grad(&dense_x_grad);
  paddle::dialect::IrTensor dense_grad_out_grad;
  paddle::dialect::IrMetaTensor meta_grad_out_grad(&dense_grad_out_grad);

  phi::GeneralBinaryGradInferMeta(meta_x, meta_x, &meta_x_grad, &meta_grad_out_grad);

  std::vector<pir::Type> argument_outputs;
  pir::Type x_grad_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_x_grad.dtype()), dense_x_grad.dims(), dense_x_grad.layout(), dense_x_grad.lod(), dense_x_grad.offset());
  argument_outputs.push_back(x_grad_dense_tensor_type);

  pir::Type grad_out_grad_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_grad_out_grad.dtype()), dense_grad_out_grad.dims(), dense_grad_out_grad.layout(), dense_grad_out_grad.lod(), dense_grad_out_grad.offset());
  argument_outputs.push_back(grad_out_grad_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void CeluDoubleGradOp::VerifySig() {}

void CeluDoubleGradOp::InferMeta( phi::InferMetaContext *infer_meta ) {
  auto fn = PD_INFER_META(phi::GeneralBinaryGradInferMeta);
  fn(infer_meta);
}

phi::DataType CeluDoubleGradOp::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: CeluDoubleGradOp";
  


  return expected_kernel_dtype;
}

const char *CeluDoubleGrad_Op::attributes_name[1] = { "alpha" };

OpInfoTuple CeluDoubleGrad_Op::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("x", "paddle::dialect::DenseTensorType", false, false, false, false), paddle::dialect::OpInputInfo("grad_out", "paddle::dialect::DenseTensorType", false, false, false, false), paddle::dialect::OpInputInfo("grad_x_grad", "paddle::dialect::DenseTensorType", false, false, false, false) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = { paddle::dialect::OpAttributeInfo("alpha", "pir::FloatAttribute", "") };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("x_grad", "paddle::dialect::DenseTensorType", false, false), paddle::dialect::OpOutputInfo("grad_out_grad", "paddle::dialect::DenseTensorType", false, false) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("GeneralBinaryGradInferMeta", {"x", "x"}, "celu_double_grad", {"x", "grad_out", "grad_x_grad", "alpha"}, {}, {}, {{"grad_out_grad", "grad_x_grad"}}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "celu_double_grad");
}

void CeluDoubleGrad_Op::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, pir::Value grad_out_, pir::Value grad_x_grad_, float alpha) {
  VLOG(4) << "Start build CeluDoubleGrad_Op";



  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_, grad_out_, grad_x_grad_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_alpha = pir::FloatAttribute::get(pir::IrContext::Instance(), alpha);
  argument.AddAttribute("alpha", attr_alpha);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;
  paddle::dialect::DenseTensorType grad_out = grad_out_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)grad_out;
  paddle::dialect::DenseTensorType grad_x_grad = grad_x_grad_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)grad_x_grad;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);
  paddle::dialect::IrTensor dense_x_grad;
  paddle::dialect::IrMetaTensor meta_x_grad(&dense_x_grad);
  paddle::dialect::IrTensor dense_grad_out_grad;
  paddle::dialect::IrMetaTensor meta_grad_out_grad(&dense_grad_out_grad);

  phi::GeneralBinaryGradInferMeta(meta_x, meta_x, &meta_x_grad, &meta_grad_out_grad);

  std::vector<pir::Type> argument_outputs;
  pir::Type x_grad_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_x_grad.dtype()), dense_x_grad.dims(), dense_x_grad.layout(), dense_x_grad.lod(), dense_x_grad.offset());
  argument_outputs.push_back(x_grad_dense_tensor_type);

  pir::Type grad_out_grad_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_grad_out_grad.dtype()), dense_grad_out_grad.dims(), dense_grad_out_grad.layout(), dense_grad_out_grad.lod(), dense_grad_out_grad.offset());
  argument_outputs.push_back(grad_out_grad_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void CeluDoubleGrad_Op::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, pir::Value grad_out_, pir::Value grad_x_grad_, pir::AttributeMap attributes) {
  VLOG(4) << "Start build CeluDoubleGrad_Op";


  IR_ENFORCE(
      attributes.find("alpha") != attributes.end(),
          "'alpha' Attribute is expected for CeluDoubleGrad_Op. ");
  float alpha = attributes.at("alpha").dyn_cast<pir::FloatAttribute>().data();


  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_, grad_out_, grad_x_grad_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_alpha = pir::FloatAttribute::get(pir::IrContext::Instance(), alpha);
  argument.AddAttribute("alpha", attr_alpha);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;
  paddle::dialect::DenseTensorType grad_out = grad_out_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)grad_out;
  paddle::dialect::DenseTensorType grad_x_grad = grad_x_grad_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)grad_x_grad;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);
  paddle::dialect::IrTensor dense_x_grad;
  paddle::dialect::IrMetaTensor meta_x_grad(&dense_x_grad);
  paddle::dialect::IrTensor dense_grad_out_grad;
  paddle::dialect::IrMetaTensor meta_grad_out_grad(&dense_grad_out_grad);

  phi::GeneralBinaryGradInferMeta(meta_x, meta_x, &meta_x_grad, &meta_grad_out_grad);

  std::vector<pir::Type> argument_outputs;
  pir::Type x_grad_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_x_grad.dtype()), dense_x_grad.dims(), dense_x_grad.layout(), dense_x_grad.lod(), dense_x_grad.offset());
  argument_outputs.push_back(x_grad_dense_tensor_type);

  pir::Type grad_out_grad_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_grad_out_grad.dtype()), dense_grad_out_grad.dims(), dense_grad_out_grad.layout(), dense_grad_out_grad.lod(), dense_grad_out_grad.offset());
  argument_outputs.push_back(grad_out_grad_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void CeluDoubleGrad_Op::VerifySig() {}

void CeluDoubleGrad_Op::InferMeta( phi::InferMetaContext *infer_meta ) {
  auto fn = PD_INFER_META(phi::GeneralBinaryGradInferMeta);
  fn(infer_meta);
}

phi::DataType CeluDoubleGrad_Op::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: CeluDoubleGrad_Op";
  


  return expected_kernel_dtype;
}

const char *CeluGradOp::attributes_name[1] = { "alpha" };

OpInfoTuple CeluGradOp::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("x", "paddle::dialect::DenseTensorType", false, false, false, true), paddle::dialect::OpInputInfo("out_grad", "paddle::dialect::DenseTensorType", false, false, false, true) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = { paddle::dialect::OpAttributeInfo("alpha", "pir::FloatAttribute", "") };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("x_grad", "paddle::dialect::DenseTensorType", false, false) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("UnchangedInferMeta", {"x"}, "celu_grad", {"x", "out_grad", "alpha"}, {}, {}, {}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "celu_grad");
}

void CeluGradOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, pir::Value out_grad_, float alpha) {
  VLOG(4) << "Start build CeluGradOp";



  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_, out_grad_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_alpha = pir::FloatAttribute::get(pir::IrContext::Instance(), alpha);
  argument.AddAttribute("alpha", attr_alpha);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;
  paddle::dialect::DenseTensorType out_grad = out_grad_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)out_grad;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);
  paddle::dialect::IrTensor dense_x_grad;
  paddle::dialect::IrMetaTensor meta_x_grad(&dense_x_grad);

  phi::UnchangedInferMeta(meta_x, &meta_x_grad);

  std::vector<pir::Type> argument_outputs;
  pir::Type x_grad_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_x_grad.dtype()), dense_x_grad.dims(), dense_x_grad.layout(), dense_x_grad.lod(), dense_x_grad.offset());
  argument_outputs.push_back(x_grad_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void CeluGradOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, pir::Value out_grad_, pir::AttributeMap attributes) {
  VLOG(4) << "Start build CeluGradOp";


  IR_ENFORCE(
      attributes.find("alpha") != attributes.end(),
          "'alpha' Attribute is expected for CeluGradOp. ");
  float alpha = attributes.at("alpha").dyn_cast<pir::FloatAttribute>().data();


  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_, out_grad_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_alpha = pir::FloatAttribute::get(pir::IrContext::Instance(), alpha);
  argument.AddAttribute("alpha", attr_alpha);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;
  paddle::dialect::DenseTensorType out_grad = out_grad_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)out_grad;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);
  paddle::dialect::IrTensor dense_x_grad;
  paddle::dialect::IrMetaTensor meta_x_grad(&dense_x_grad);

  phi::UnchangedInferMeta(meta_x, &meta_x_grad);

  std::vector<pir::Type> argument_outputs;
  pir::Type x_grad_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_x_grad.dtype()), dense_x_grad.dims(), dense_x_grad.layout(), dense_x_grad.lod(), dense_x_grad.offset());
  argument_outputs.push_back(x_grad_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void CeluGradOp::VerifySig() {}

void CeluGradOp::InferMeta( phi::InferMetaContext *infer_meta ) {
  auto fn = PD_INFER_META(phi::UnchangedInferMeta);
  fn(infer_meta);
}

phi::DataType CeluGradOp::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: CeluGradOp";
  


  return expected_kernel_dtype;
}

const char *CeluGrad_Op::attributes_name[1] = { "alpha" };

OpInfoTuple CeluGrad_Op::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("x", "paddle::dialect::DenseTensorType", false, false, false, true), paddle::dialect::OpInputInfo("out_grad", "paddle::dialect::DenseTensorType", false, false, false, true) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = { paddle::dialect::OpAttributeInfo("alpha", "pir::FloatAttribute", "") };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("x_grad", "paddle::dialect::DenseTensorType", false, false) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("UnchangedInferMeta", {"x"}, "celu_grad", {"x", "out_grad", "alpha"}, {}, {}, {{"x_grad", "out_grad"}}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "celu_grad");
}

void CeluGrad_Op::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, pir::Value out_grad_, float alpha) {
  VLOG(4) << "Start build CeluGrad_Op";



  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_, out_grad_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_alpha = pir::FloatAttribute::get(pir::IrContext::Instance(), alpha);
  argument.AddAttribute("alpha", attr_alpha);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;
  paddle::dialect::DenseTensorType out_grad = out_grad_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)out_grad;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);
  paddle::dialect::IrTensor dense_x_grad;
  paddle::dialect::IrMetaTensor meta_x_grad(&dense_x_grad);

  phi::UnchangedInferMeta(meta_x, &meta_x_grad);

  std::vector<pir::Type> argument_outputs;
  pir::Type x_grad_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_x_grad.dtype()), dense_x_grad.dims(), dense_x_grad.layout(), dense_x_grad.lod(), dense_x_grad.offset());
  argument_outputs.push_back(x_grad_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void CeluGrad_Op::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, pir::Value out_grad_, pir::AttributeMap attributes) {
  VLOG(4) << "Start build CeluGrad_Op";


  IR_ENFORCE(
      attributes.find("alpha") != attributes.end(),
          "'alpha' Attribute is expected for CeluGrad_Op. ");
  float alpha = attributes.at("alpha").dyn_cast<pir::FloatAttribute>().data();


  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_, out_grad_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_alpha = pir::FloatAttribute::get(pir::IrContext::Instance(), alpha);
  argument.AddAttribute("alpha", attr_alpha);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;
  paddle::dialect::DenseTensorType out_grad = out_grad_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)out_grad;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);
  paddle::dialect::IrTensor dense_x_grad;
  paddle::dialect::IrMetaTensor meta_x_grad(&dense_x_grad);

  phi::UnchangedInferMeta(meta_x, &meta_x_grad);

  std::vector<pir::Type> argument_outputs;
  pir::Type x_grad_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_x_grad.dtype()), dense_x_grad.dims(), dense_x_grad.layout(), dense_x_grad.lod(), dense_x_grad.offset());
  argument_outputs.push_back(x_grad_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void CeluGrad_Op::VerifySig() {}

void CeluGrad_Op::InferMeta( phi::InferMetaContext *infer_meta ) {
  auto fn = PD_INFER_META(phi::UnchangedInferMeta);
  fn(infer_meta);
}

phi::DataType CeluGrad_Op::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: CeluGrad_Op";
  


  return expected_kernel_dtype;
}

const char *CholeskyGradOp::attributes_name[1] = { "upper" };

OpInfoTuple CholeskyGradOp::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("out", "paddle::dialect::DenseTensorType", false, false, false, false), paddle::dialect::OpInputInfo("out_grad", "paddle::dialect::DenseTensorType", false, false, false, false) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = { paddle::dialect::OpAttributeInfo("upper", "pir::BoolAttribute", "") };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("x_grad", "paddle::dialect::DenseTensorType", false, false) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("UnchangedInferMeta", {"out"}, "cholesky_grad", {"out", "out_grad", "upper"}, {}, {}, {}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "cholesky_grad");
}

void CholeskyGradOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value out_, pir::Value out_grad_, bool upper) {
  VLOG(4) << "Start build CholeskyGradOp";



  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {out_, out_grad_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_upper = pir::BoolAttribute::get(pir::IrContext::Instance(), upper);
  argument.AddAttribute("upper", attr_upper);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType out = out_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)out;
  paddle::dialect::DenseTensorType out_grad = out_grad_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)out_grad;

  VLOG(4) << "Builder construction  dense_out";
  paddle::dialect::IrTensor ir_tensor_out(paddle::dialect::TransToPhiDataType(out.dtype()),
                                                      out.dims(),
                                                      out.data_layout(),
                                                      out.lod(),
                                                      out.offset());
  VLOG(4) << "Builder construction  meta_out";
  paddle::dialect::IrMetaTensor meta_out(&ir_tensor_out);
  paddle::dialect::IrTensor dense_x_grad;
  paddle::dialect::IrMetaTensor meta_x_grad(&dense_x_grad);

  phi::UnchangedInferMeta(meta_out, &meta_x_grad);

  std::vector<pir::Type> argument_outputs;
  pir::Type x_grad_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_x_grad.dtype()), dense_x_grad.dims(), dense_x_grad.layout(), dense_x_grad.lod(), dense_x_grad.offset());
  argument_outputs.push_back(x_grad_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void CholeskyGradOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value out_, pir::Value out_grad_, pir::AttributeMap attributes) {
  VLOG(4) << "Start build CholeskyGradOp";


  IR_ENFORCE(
      attributes.find("upper") != attributes.end(),
          "'upper' Attribute is expected for CholeskyGradOp. ");
  bool upper = attributes.at("upper").dyn_cast<pir::BoolAttribute>().data();


  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {out_, out_grad_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_upper = pir::BoolAttribute::get(pir::IrContext::Instance(), upper);
  argument.AddAttribute("upper", attr_upper);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType out = out_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)out;
  paddle::dialect::DenseTensorType out_grad = out_grad_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)out_grad;

  VLOG(4) << "Builder construction  dense_out";
  paddle::dialect::IrTensor ir_tensor_out(paddle::dialect::TransToPhiDataType(out.dtype()),
                                                      out.dims(),
                                                      out.data_layout(),
                                                      out.lod(),
                                                      out.offset());
  VLOG(4) << "Builder construction  meta_out";
  paddle::dialect::IrMetaTensor meta_out(&ir_tensor_out);
  paddle::dialect::IrTensor dense_x_grad;
  paddle::dialect::IrMetaTensor meta_x_grad(&dense_x_grad);

  phi::UnchangedInferMeta(meta_out, &meta_x_grad);

  std::vector<pir::Type> argument_outputs;
  pir::Type x_grad_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_x_grad.dtype()), dense_x_grad.dims(), dense_x_grad.layout(), dense_x_grad.lod(), dense_x_grad.offset());
  argument_outputs.push_back(x_grad_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void CholeskyGradOp::VerifySig() {}

void CholeskyGradOp::InferMeta( phi::InferMetaContext *infer_meta ) {
  auto fn = PD_INFER_META(phi::UnchangedInferMeta);
  fn(infer_meta);
}

phi::DataType CholeskyGradOp::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: CholeskyGradOp";
  


  return expected_kernel_dtype;
}

const char *CholeskySolveGradOp::attributes_name[1] = { "upper" };

OpInfoTuple CholeskySolveGradOp::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("x", "paddle::dialect::DenseTensorType", false, false, false, false), paddle::dialect::OpInputInfo("y", "paddle::dialect::DenseTensorType", false, false, false, false), paddle::dialect::OpInputInfo("out", "paddle::dialect::DenseTensorType", false, false, false, false), paddle::dialect::OpInputInfo("out_grad", "paddle::dialect::DenseTensorType", false, false, false, false) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = { paddle::dialect::OpAttributeInfo("upper", "pir::BoolAttribute", "") };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("x_grad", "paddle::dialect::DenseTensorType", false, false), paddle::dialect::OpOutputInfo("y_grad", "paddle::dialect::DenseTensorType", false, false) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("GeneralBinaryGradInferMeta", {"x", "y"}, "cholesky_solve_grad", {"x", "y", "out", "out_grad", "upper"}, {}, {}, {}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "cholesky_solve_grad");
}

void CholeskySolveGradOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, pir::Value y_, pir::Value out_, pir::Value out_grad_, bool upper) {
  VLOG(4) << "Start build CholeskySolveGradOp";



  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_, y_, out_, out_grad_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_upper = pir::BoolAttribute::get(pir::IrContext::Instance(), upper);
  argument.AddAttribute("upper", attr_upper);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;
  paddle::dialect::DenseTensorType y = y_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)y;
  paddle::dialect::DenseTensorType out = out_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)out;
  paddle::dialect::DenseTensorType out_grad = out_grad_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)out_grad;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);

  VLOG(4) << "Builder construction  dense_y";
  paddle::dialect::IrTensor ir_tensor_y(paddle::dialect::TransToPhiDataType(y.dtype()),
                                                      y.dims(),
                                                      y.data_layout(),
                                                      y.lod(),
                                                      y.offset());
  VLOG(4) << "Builder construction  meta_y";
  paddle::dialect::IrMetaTensor meta_y(&ir_tensor_y);
  paddle::dialect::IrTensor dense_x_grad;
  paddle::dialect::IrMetaTensor meta_x_grad(&dense_x_grad);
  paddle::dialect::IrTensor dense_y_grad;
  paddle::dialect::IrMetaTensor meta_y_grad(&dense_y_grad);

  phi::GeneralBinaryGradInferMeta(meta_x, meta_y, &meta_x_grad, &meta_y_grad);

  std::vector<pir::Type> argument_outputs;
  pir::Type x_grad_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_x_grad.dtype()), dense_x_grad.dims(), dense_x_grad.layout(), dense_x_grad.lod(), dense_x_grad.offset());
  argument_outputs.push_back(x_grad_dense_tensor_type);

  pir::Type y_grad_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_y_grad.dtype()), dense_y_grad.dims(), dense_y_grad.layout(), dense_y_grad.lod(), dense_y_grad.offset());
  argument_outputs.push_back(y_grad_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void CholeskySolveGradOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, pir::Value y_, pir::Value out_, pir::Value out_grad_, pir::AttributeMap attributes) {
  VLOG(4) << "Start build CholeskySolveGradOp";


  IR_ENFORCE(
      attributes.find("upper") != attributes.end(),
          "'upper' Attribute is expected for CholeskySolveGradOp. ");
  bool upper = attributes.at("upper").dyn_cast<pir::BoolAttribute>().data();


  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_, y_, out_, out_grad_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_upper = pir::BoolAttribute::get(pir::IrContext::Instance(), upper);
  argument.AddAttribute("upper", attr_upper);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;
  paddle::dialect::DenseTensorType y = y_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)y;
  paddle::dialect::DenseTensorType out = out_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)out;
  paddle::dialect::DenseTensorType out_grad = out_grad_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)out_grad;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);

  VLOG(4) << "Builder construction  dense_y";
  paddle::dialect::IrTensor ir_tensor_y(paddle::dialect::TransToPhiDataType(y.dtype()),
                                                      y.dims(),
                                                      y.data_layout(),
                                                      y.lod(),
                                                      y.offset());
  VLOG(4) << "Builder construction  meta_y";
  paddle::dialect::IrMetaTensor meta_y(&ir_tensor_y);
  paddle::dialect::IrTensor dense_x_grad;
  paddle::dialect::IrMetaTensor meta_x_grad(&dense_x_grad);
  paddle::dialect::IrTensor dense_y_grad;
  paddle::dialect::IrMetaTensor meta_y_grad(&dense_y_grad);

  phi::GeneralBinaryGradInferMeta(meta_x, meta_y, &meta_x_grad, &meta_y_grad);

  std::vector<pir::Type> argument_outputs;
  pir::Type x_grad_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_x_grad.dtype()), dense_x_grad.dims(), dense_x_grad.layout(), dense_x_grad.lod(), dense_x_grad.offset());
  argument_outputs.push_back(x_grad_dense_tensor_type);

  pir::Type y_grad_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_y_grad.dtype()), dense_y_grad.dims(), dense_y_grad.layout(), dense_y_grad.lod(), dense_y_grad.offset());
  argument_outputs.push_back(y_grad_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void CholeskySolveGradOp::VerifySig() {}

void CholeskySolveGradOp::InferMeta( phi::InferMetaContext *infer_meta ) {
  auto fn = PD_INFER_META(phi::GeneralBinaryGradInferMeta);
  fn(infer_meta);
}

phi::DataType CholeskySolveGradOp::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: CholeskySolveGradOp";
  


  return expected_kernel_dtype;
}

OpInfoTuple ClipDoubleGradOp::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("x", "paddle::dialect::DenseTensorType", false, false, false, false), paddle::dialect::OpInputInfo("grad_x_grad", "paddle::dialect::DenseTensorType", false, false, false, false), paddle::dialect::OpInputInfo("min", "paddle::dialect::ScalarAttribute", false, false, true, false), paddle::dialect::OpInputInfo("max", "paddle::dialect::ScalarAttribute", false, false, true, false) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = {  };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("grad_out_grad", "paddle::dialect::DenseTensorType", false, false) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("UnchangedInferMeta", {"x"}, "clip_grad", {"x", "grad_x_grad", "min", "max"}, {"x"}, {}, {}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "clip_double_grad");
}

void ClipDoubleGradOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, pir::Value grad_x_grad_, float min, float max) {
  VLOG(4) << "Start build ClipDoubleGradOp";


  // Generate scalar mutable attribute: min
  paddle::dialect::FullOp full_min_op = builder.Build<paddle::dialect::FullOp>(std::vector<int64_t>{1}, min, phi::DataType::FLOAT32, phi::CPUPlace());
  pir::OpResult min_ = full_min_op->result(0);
      // Generate scalar mutable attribute: max
  paddle::dialect::FullOp full_max_op = builder.Build<paddle::dialect::FullOp>(std::vector<int64_t>{1}, max, phi::DataType::FLOAT32, phi::CPUPlace());
  pir::OpResult max_ = full_max_op->result(0);
    
  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_, grad_x_grad_, min_, max_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;
  paddle::dialect::DenseTensorType grad_x_grad = grad_x_grad_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)grad_x_grad;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);
  paddle::dialect::IrTensor dense_grad_out_grad;
  paddle::dialect::IrMetaTensor meta_grad_out_grad(&dense_grad_out_grad);

  phi::UnchangedInferMeta(meta_x, &meta_grad_out_grad);

  std::vector<pir::Type> argument_outputs;
  pir::Type grad_out_grad_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_grad_out_grad.dtype()), dense_grad_out_grad.dims(), dense_grad_out_grad.layout(), dense_grad_out_grad.lod(), dense_grad_out_grad.offset());
  argument_outputs.push_back(grad_out_grad_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void ClipDoubleGradOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, pir::Value grad_x_grad_, pir::AttributeMap attributes) {
  VLOG(4) << "Start build ClipDoubleGradOp";


  IR_ENFORCE(
      attributes.find("min") != attributes.end(),
          "'min' Attribute is expected for ClipDoubleGradOp. ");
  float min = attributes.at("min").dyn_cast<paddle::dialect::ScalarAttribute>().data().to<float>();

  IR_ENFORCE(
      attributes.find("max") != attributes.end(),
          "'max' Attribute is expected for ClipDoubleGradOp. ");
  float max = attributes.at("max").dyn_cast<paddle::dialect::ScalarAttribute>().data().to<float>();

  // Generate scalar mutable attribute: min
  paddle::dialect::FullOp full_min_op = builder.Build<paddle::dialect::FullOp>(std::vector<int64_t>{1}, min, phi::DataType::FLOAT32, phi::CPUPlace());
  pir::OpResult min_ = full_min_op->result(0);
      // Generate scalar mutable attribute: max
  paddle::dialect::FullOp full_max_op = builder.Build<paddle::dialect::FullOp>(std::vector<int64_t>{1}, max, phi::DataType::FLOAT32, phi::CPUPlace());
  pir::OpResult max_ = full_max_op->result(0);
    
  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_, grad_x_grad_, min_, max_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;
  paddle::dialect::DenseTensorType grad_x_grad = grad_x_grad_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)grad_x_grad;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);
  paddle::dialect::IrTensor dense_grad_out_grad;
  paddle::dialect::IrMetaTensor meta_grad_out_grad(&dense_grad_out_grad);

  phi::UnchangedInferMeta(meta_x, &meta_grad_out_grad);

  std::vector<pir::Type> argument_outputs;
  pir::Type grad_out_grad_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_grad_out_grad.dtype()), dense_grad_out_grad.dims(), dense_grad_out_grad.layout(), dense_grad_out_grad.lod(), dense_grad_out_grad.offset());
  argument_outputs.push_back(grad_out_grad_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void ClipDoubleGradOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, pir::Value grad_x_grad_, pir::Value min_, pir::Value max_) {
  VLOG(4) << "Start build ClipDoubleGradOp";



  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_, grad_x_grad_, min_, max_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;
  paddle::dialect::DenseTensorType grad_x_grad = grad_x_grad_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)grad_x_grad;
  phi::Scalar min;
  if (min_.dyn_cast<pir::OpResult>() && min_.dyn_cast<pir::OpResult>().owner()->isa<paddle::dialect::FullOp>()) {
    min = std::move(phi::Scalar(min_.dyn_cast<pir::OpResult>().owner()
                                  ->dyn_cast<paddle::dialect::FullOp>()
                                  .attribute("value")
                                  .dyn_cast<paddle::dialect::ScalarAttribute>()
                                  .data()
                                  .to<int>()));
  }
  else {
    min = std::move(phi::Scalar(-1));
    min.SetFromTensor(true);
  }
  phi::Scalar max;
  if (max_.dyn_cast<pir::OpResult>() && max_.dyn_cast<pir::OpResult>().owner()->isa<paddle::dialect::FullOp>()) {
    max = std::move(phi::Scalar(max_.dyn_cast<pir::OpResult>().owner()
                                  ->dyn_cast<paddle::dialect::FullOp>()
                                  .attribute("value")
                                  .dyn_cast<paddle::dialect::ScalarAttribute>()
                                  .data()
                                  .to<int>()));
  }
  else {
    max = std::move(phi::Scalar(-1));
    max.SetFromTensor(true);
  }


  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);
  paddle::dialect::IrTensor dense_grad_out_grad;
  paddle::dialect::IrMetaTensor meta_grad_out_grad(&dense_grad_out_grad);

  phi::UnchangedInferMeta(meta_x, &meta_grad_out_grad);

  std::vector<pir::Type> argument_outputs;
  pir::Type grad_out_grad_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_grad_out_grad.dtype()), dense_grad_out_grad.dims(), dense_grad_out_grad.layout(), dense_grad_out_grad.lod(), dense_grad_out_grad.offset());
  argument_outputs.push_back(grad_out_grad_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void ClipDoubleGradOp::VerifySig() {}

void ClipDoubleGradOp::InferMeta( phi::InferMetaContext *infer_meta ) {
  auto fn = PD_INFER_META(phi::UnchangedInferMeta);
  fn(infer_meta);
}

phi::DataType ClipDoubleGradOp::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: ClipDoubleGradOp";
  


  return expected_kernel_dtype;
}

OpInfoTuple ClipGradOp::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("x", "paddle::dialect::DenseTensorType", false, false, false, false), paddle::dialect::OpInputInfo("out_grad", "paddle::dialect::DenseTensorType", false, false, false, true), paddle::dialect::OpInputInfo("min", "paddle::dialect::ScalarAttribute", false, false, true, false), paddle::dialect::OpInputInfo("max", "paddle::dialect::ScalarAttribute", false, false, true, false) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = {  };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("x_grad", "paddle::dialect::DenseTensorType", false, false) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("UnchangedInferMeta", {"x"}, "clip_grad", {"x", "out_grad", "min", "max"}, {}, {}, {}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "clip_grad");
}

void ClipGradOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, pir::Value out_grad_, float min, float max) {
  VLOG(4) << "Start build ClipGradOp";


  // Generate scalar mutable attribute: min
  paddle::dialect::FullOp full_min_op = builder.Build<paddle::dialect::FullOp>(std::vector<int64_t>{1}, min, phi::DataType::FLOAT32, phi::CPUPlace());
  pir::OpResult min_ = full_min_op->result(0);
      // Generate scalar mutable attribute: max
  paddle::dialect::FullOp full_max_op = builder.Build<paddle::dialect::FullOp>(std::vector<int64_t>{1}, max, phi::DataType::FLOAT32, phi::CPUPlace());
  pir::OpResult max_ = full_max_op->result(0);
    
  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_, out_grad_, min_, max_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;
  paddle::dialect::DenseTensorType out_grad = out_grad_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)out_grad;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);
  paddle::dialect::IrTensor dense_x_grad;
  paddle::dialect::IrMetaTensor meta_x_grad(&dense_x_grad);

  phi::UnchangedInferMeta(meta_x, &meta_x_grad);

  std::vector<pir::Type> argument_outputs;
  pir::Type x_grad_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_x_grad.dtype()), dense_x_grad.dims(), dense_x_grad.layout(), dense_x_grad.lod(), dense_x_grad.offset());
  argument_outputs.push_back(x_grad_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void ClipGradOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, pir::Value out_grad_, pir::AttributeMap attributes) {
  VLOG(4) << "Start build ClipGradOp";


  IR_ENFORCE(
      attributes.find("min") != attributes.end(),
          "'min' Attribute is expected for ClipGradOp. ");
  float min = attributes.at("min").dyn_cast<paddle::dialect::ScalarAttribute>().data().to<float>();

  IR_ENFORCE(
      attributes.find("max") != attributes.end(),
          "'max' Attribute is expected for ClipGradOp. ");
  float max = attributes.at("max").dyn_cast<paddle::dialect::ScalarAttribute>().data().to<float>();

  // Generate scalar mutable attribute: min
  paddle::dialect::FullOp full_min_op = builder.Build<paddle::dialect::FullOp>(std::vector<int64_t>{1}, min, phi::DataType::FLOAT32, phi::CPUPlace());
  pir::OpResult min_ = full_min_op->result(0);
      // Generate scalar mutable attribute: max
  paddle::dialect::FullOp full_max_op = builder.Build<paddle::dialect::FullOp>(std::vector<int64_t>{1}, max, phi::DataType::FLOAT32, phi::CPUPlace());
  pir::OpResult max_ = full_max_op->result(0);
    
  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_, out_grad_, min_, max_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;
  paddle::dialect::DenseTensorType out_grad = out_grad_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)out_grad;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);
  paddle::dialect::IrTensor dense_x_grad;
  paddle::dialect::IrMetaTensor meta_x_grad(&dense_x_grad);

  phi::UnchangedInferMeta(meta_x, &meta_x_grad);

  std::vector<pir::Type> argument_outputs;
  pir::Type x_grad_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_x_grad.dtype()), dense_x_grad.dims(), dense_x_grad.layout(), dense_x_grad.lod(), dense_x_grad.offset());
  argument_outputs.push_back(x_grad_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void ClipGradOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, pir::Value out_grad_, pir::Value min_, pir::Value max_) {
  VLOG(4) << "Start build ClipGradOp";



  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_, out_grad_, min_, max_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;
  paddle::dialect::DenseTensorType out_grad = out_grad_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)out_grad;
  phi::Scalar min;
  if (min_.dyn_cast<pir::OpResult>() && min_.dyn_cast<pir::OpResult>().owner()->isa<paddle::dialect::FullOp>()) {
    min = std::move(phi::Scalar(min_.dyn_cast<pir::OpResult>().owner()
                                  ->dyn_cast<paddle::dialect::FullOp>()
                                  .attribute("value")
                                  .dyn_cast<paddle::dialect::ScalarAttribute>()
                                  .data()
                                  .to<int>()));
  }
  else {
    min = std::move(phi::Scalar(-1));
    min.SetFromTensor(true);
  }
  phi::Scalar max;
  if (max_.dyn_cast<pir::OpResult>() && max_.dyn_cast<pir::OpResult>().owner()->isa<paddle::dialect::FullOp>()) {
    max = std::move(phi::Scalar(max_.dyn_cast<pir::OpResult>().owner()
                                  ->dyn_cast<paddle::dialect::FullOp>()
                                  .attribute("value")
                                  .dyn_cast<paddle::dialect::ScalarAttribute>()
                                  .data()
                                  .to<int>()));
  }
  else {
    max = std::move(phi::Scalar(-1));
    max.SetFromTensor(true);
  }


  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);
  paddle::dialect::IrTensor dense_x_grad;
  paddle::dialect::IrMetaTensor meta_x_grad(&dense_x_grad);

  phi::UnchangedInferMeta(meta_x, &meta_x_grad);

  std::vector<pir::Type> argument_outputs;
  pir::Type x_grad_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_x_grad.dtype()), dense_x_grad.dims(), dense_x_grad.layout(), dense_x_grad.lod(), dense_x_grad.offset());
  argument_outputs.push_back(x_grad_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void ClipGradOp::VerifySig() {}

void ClipGradOp::InferMeta( phi::InferMetaContext *infer_meta ) {
  auto fn = PD_INFER_META(phi::UnchangedInferMeta);
  fn(infer_meta);
}

phi::DataType ClipGradOp::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: ClipGradOp";
  


  return expected_kernel_dtype;
}

OpInfoTuple ClipGrad_Op::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("x", "paddle::dialect::DenseTensorType", false, false, false, false), paddle::dialect::OpInputInfo("out_grad", "paddle::dialect::DenseTensorType", false, false, false, true), paddle::dialect::OpInputInfo("min", "paddle::dialect::ScalarAttribute", false, false, true, false), paddle::dialect::OpInputInfo("max", "paddle::dialect::ScalarAttribute", false, false, true, false) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = {  };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("x_grad", "paddle::dialect::DenseTensorType", false, false) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("UnchangedInferMeta", {"x"}, "clip_grad", {"x", "out_grad", "min", "max"}, {}, {}, {{"x_grad", "out_grad"}}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "clip_grad");
}

void ClipGrad_Op::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, pir::Value out_grad_, float min, float max) {
  VLOG(4) << "Start build ClipGrad_Op";


  // Generate scalar mutable attribute: min
  paddle::dialect::FullOp full_min_op = builder.Build<paddle::dialect::FullOp>(std::vector<int64_t>{1}, min, phi::DataType::FLOAT32, phi::CPUPlace());
  pir::OpResult min_ = full_min_op->result(0);
      // Generate scalar mutable attribute: max
  paddle::dialect::FullOp full_max_op = builder.Build<paddle::dialect::FullOp>(std::vector<int64_t>{1}, max, phi::DataType::FLOAT32, phi::CPUPlace());
  pir::OpResult max_ = full_max_op->result(0);
    
  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_, out_grad_, min_, max_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;
  paddle::dialect::DenseTensorType out_grad = out_grad_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)out_grad;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);
  paddle::dialect::IrTensor dense_x_grad;
  paddle::dialect::IrMetaTensor meta_x_grad(&dense_x_grad);

  phi::UnchangedInferMeta(meta_x, &meta_x_grad);

  std::vector<pir::Type> argument_outputs;
  pir::Type x_grad_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_x_grad.dtype()), dense_x_grad.dims(), dense_x_grad.layout(), dense_x_grad.lod(), dense_x_grad.offset());
  argument_outputs.push_back(x_grad_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void ClipGrad_Op::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, pir::Value out_grad_, pir::AttributeMap attributes) {
  VLOG(4) << "Start build ClipGrad_Op";


  IR_ENFORCE(
      attributes.find("min") != attributes.end(),
          "'min' Attribute is expected for ClipGrad_Op. ");
  float min = attributes.at("min").dyn_cast<paddle::dialect::ScalarAttribute>().data().to<float>();

  IR_ENFORCE(
      attributes.find("max") != attributes.end(),
          "'max' Attribute is expected for ClipGrad_Op. ");
  float max = attributes.at("max").dyn_cast<paddle::dialect::ScalarAttribute>().data().to<float>();

  // Generate scalar mutable attribute: min
  paddle::dialect::FullOp full_min_op = builder.Build<paddle::dialect::FullOp>(std::vector<int64_t>{1}, min, phi::DataType::FLOAT32, phi::CPUPlace());
  pir::OpResult min_ = full_min_op->result(0);
      // Generate scalar mutable attribute: max
  paddle::dialect::FullOp full_max_op = builder.Build<paddle::dialect::FullOp>(std::vector<int64_t>{1}, max, phi::DataType::FLOAT32, phi::CPUPlace());
  pir::OpResult max_ = full_max_op->result(0);
    
  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_, out_grad_, min_, max_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;
  paddle::dialect::DenseTensorType out_grad = out_grad_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)out_grad;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);
  paddle::dialect::IrTensor dense_x_grad;
  paddle::dialect::IrMetaTensor meta_x_grad(&dense_x_grad);

  phi::UnchangedInferMeta(meta_x, &meta_x_grad);

  std::vector<pir::Type> argument_outputs;
  pir::Type x_grad_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_x_grad.dtype()), dense_x_grad.dims(), dense_x_grad.layout(), dense_x_grad.lod(), dense_x_grad.offset());
  argument_outputs.push_back(x_grad_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void ClipGrad_Op::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, pir::Value out_grad_, pir::Value min_, pir::Value max_) {
  VLOG(4) << "Start build ClipGrad_Op";



  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_, out_grad_, min_, max_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;
  paddle::dialect::DenseTensorType out_grad = out_grad_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)out_grad;
  phi::Scalar min;
  if (min_.dyn_cast<pir::OpResult>() && min_.dyn_cast<pir::OpResult>().owner()->isa<paddle::dialect::FullOp>()) {
    min = std::move(phi::Scalar(min_.dyn_cast<pir::OpResult>().owner()
                                  ->dyn_cast<paddle::dialect::FullOp>()
                                  .attribute("value")
                                  .dyn_cast<paddle::dialect::ScalarAttribute>()
                                  .data()
                                  .to<int>()));
  }
  else {
    min = std::move(phi::Scalar(-1));
    min.SetFromTensor(true);
  }
  phi::Scalar max;
  if (max_.dyn_cast<pir::OpResult>() && max_.dyn_cast<pir::OpResult>().owner()->isa<paddle::dialect::FullOp>()) {
    max = std::move(phi::Scalar(max_.dyn_cast<pir::OpResult>().owner()
                                  ->dyn_cast<paddle::dialect::FullOp>()
                                  .attribute("value")
                                  .dyn_cast<paddle::dialect::ScalarAttribute>()
                                  .data()
                                  .to<int>()));
  }
  else {
    max = std::move(phi::Scalar(-1));
    max.SetFromTensor(true);
  }


  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);
  paddle::dialect::IrTensor dense_x_grad;
  paddle::dialect::IrMetaTensor meta_x_grad(&dense_x_grad);

  phi::UnchangedInferMeta(meta_x, &meta_x_grad);

  std::vector<pir::Type> argument_outputs;
  pir::Type x_grad_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_x_grad.dtype()), dense_x_grad.dims(), dense_x_grad.layout(), dense_x_grad.lod(), dense_x_grad.offset());
  argument_outputs.push_back(x_grad_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void ClipGrad_Op::VerifySig() {}

void ClipGrad_Op::InferMeta( phi::InferMetaContext *infer_meta ) {
  auto fn = PD_INFER_META(phi::UnchangedInferMeta);
  fn(infer_meta);
}

phi::DataType ClipGrad_Op::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: ClipGrad_Op";
  


  return expected_kernel_dtype;
}

OpInfoTuple ComplexGradOp::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("real", "paddle::dialect::DenseTensorType", false, false, false, false), paddle::dialect::OpInputInfo("imag", "paddle::dialect::DenseTensorType", false, false, false, false), paddle::dialect::OpInputInfo("out_grad", "paddle::dialect::DenseTensorType", false, false, false, false) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = {  };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("real_grad", "paddle::dialect::DenseTensorType", false, false), paddle::dialect::OpOutputInfo("imag_grad", "paddle::dialect::DenseTensorType", false, false) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("ComplexGradInferMeta", {"real", "imag", "out_grad"}, "complex_grad", {"real", "imag", "out_grad"}, {"real"}, {}, {}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "complex_grad");
}

void ComplexGradOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value real_, pir::Value imag_, pir::Value out_grad_) {
  VLOG(4) << "Start build ComplexGradOp";



  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {real_, imag_, out_grad_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType real = real_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)real;
  paddle::dialect::DenseTensorType imag = imag_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)imag;
  paddle::dialect::DenseTensorType out_grad = out_grad_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)out_grad;

  VLOG(4) << "Builder construction  dense_real";
  paddle::dialect::IrTensor ir_tensor_real(paddle::dialect::TransToPhiDataType(real.dtype()),
                                                      real.dims(),
                                                      real.data_layout(),
                                                      real.lod(),
                                                      real.offset());
  VLOG(4) << "Builder construction  meta_real";
  paddle::dialect::IrMetaTensor meta_real(&ir_tensor_real);

  VLOG(4) << "Builder construction  dense_imag";
  paddle::dialect::IrTensor ir_tensor_imag(paddle::dialect::TransToPhiDataType(imag.dtype()),
                                                      imag.dims(),
                                                      imag.data_layout(),
                                                      imag.lod(),
                                                      imag.offset());
  VLOG(4) << "Builder construction  meta_imag";
  paddle::dialect::IrMetaTensor meta_imag(&ir_tensor_imag);

  VLOG(4) << "Builder construction  dense_out_grad";
  paddle::dialect::IrTensor ir_tensor_out_grad(paddle::dialect::TransToPhiDataType(out_grad.dtype()),
                                                      out_grad.dims(),
                                                      out_grad.data_layout(),
                                                      out_grad.lod(),
                                                      out_grad.offset());
  VLOG(4) << "Builder construction  meta_out_grad";
  paddle::dialect::IrMetaTensor meta_out_grad(&ir_tensor_out_grad);
  paddle::dialect::IrTensor dense_real_grad;
  paddle::dialect::IrMetaTensor meta_real_grad(&dense_real_grad);
  paddle::dialect::IrTensor dense_imag_grad;
  paddle::dialect::IrMetaTensor meta_imag_grad(&dense_imag_grad);

  phi::ComplexGradInferMeta(meta_real, meta_imag, meta_out_grad, &meta_real_grad, &meta_imag_grad);

  std::vector<pir::Type> argument_outputs;
  pir::Type real_grad_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_real_grad.dtype()), dense_real_grad.dims(), dense_real_grad.layout(), dense_real_grad.lod(), dense_real_grad.offset());
  argument_outputs.push_back(real_grad_dense_tensor_type);

  pir::Type imag_grad_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_imag_grad.dtype()), dense_imag_grad.dims(), dense_imag_grad.layout(), dense_imag_grad.lod(), dense_imag_grad.offset());
  argument_outputs.push_back(imag_grad_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void ComplexGradOp::VerifySig() {}

void ComplexGradOp::InferMeta( phi::InferMetaContext *infer_meta ) {
  auto fn = PD_INFER_META(phi::ComplexGradInferMeta);
  fn(infer_meta);
}

phi::DataType ComplexGradOp::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: ComplexGradOp";
  


  return expected_kernel_dtype;
}

OpInfoTuple ConcatDoubleGradOp::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("grad_x_grad", "pir::VectorType<paddle::dialect::DenseTensorType>", false, false, false, false), paddle::dialect::OpInputInfo("axis", "paddle::dialect::ScalarAttribute", false, false, true, false) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = {  };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("grad_out_grad", "paddle::dialect::DenseTensorType", false, false) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("", {""}, "", {""}, {}, {}, {}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "concat_double_grad");
}

void ConcatDoubleGradOp::VerifySig() {}

phi::DataType ConcatDoubleGradOp::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: ConcatDoubleGradOp";
  


  return expected_kernel_dtype;
}

OpInfoTuple ConcatGradOp::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("x", "pir::VectorType<paddle::dialect::DenseTensorType>", false, true, false, false), paddle::dialect::OpInputInfo("out_grad", "paddle::dialect::DenseTensorType", false, false, false, true), paddle::dialect::OpInputInfo("axis", "paddle::dialect::ScalarAttribute", false, false, true, false) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = {  };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("x_grad", "pir::VectorType<paddle::dialect::DenseTensorType>", false, false) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("UnchangedMultiInferMeta", {"x"}, "concat_grad", {"x", "out_grad", "axis"}, {"out_grad"}, {}, {}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "concat_grad");
}

void ConcatGradOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, pir::Value out_grad_, int axis) {
  VLOG(4) << "Start build ConcatGradOp";


  // Generate scalar mutable attribute: axis
  paddle::dialect::FullOp full_axis_op = builder.Build<paddle::dialect::FullOp>(std::vector<int64_t>{1}, axis, phi::DataType::INT32, phi::CPUPlace());
  pir::OpResult axis_ = full_axis_op->result(0);
    
  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_, out_grad_, axis_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";

  VLOG(4) << "Builder construction outputs";
  pir::VectorType x = x_.type().dyn_cast<pir::VectorType>(); (void)x;
  paddle::dialect::DenseTensorType out_grad = out_grad_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)out_grad;
  std::vector<paddle::dialect::IrTensor> vec_ir_tensor_x;
  for (size_t i=0; i < static_cast<size_t>(x.size()); i++) {
    vec_ir_tensor_x.push_back(paddle::dialect::IrTensor(paddle::dialect::TransToPhiDataType(x[i].dyn_cast<paddle::dialect::DenseTensorType>().dtype()),
                                                                     x[i].dyn_cast<paddle::dialect::DenseTensorType>().dims(),
                                                                     x[i].dyn_cast<paddle::dialect::DenseTensorType>().data_layout(),
                                                                     x[i].dyn_cast<paddle::dialect::DenseTensorType>().lod(),
                                                                     x[i].dyn_cast<paddle::dialect::DenseTensorType>().offset()));
  }
  std::vector<paddle::dialect::IrMetaTensor> vec_meta_x;
  for (size_t i=0; i < vec_ir_tensor_x.size(); i++) {
    vec_meta_x.push_back(paddle::dialect::IrMetaTensor(&vec_ir_tensor_x[i]));
  }

  std::vector<const phi::MetaTensor*> meta_x;
  for (size_t i=0; i < static_cast<size_t>(vec_meta_x.size()); i++) {
    meta_x.push_back(&vec_meta_x[i]);
  }
   std::vector<paddle::dialect::IrTensor> vec_dense_x_grad((x.size()), paddle::dialect::IrTensor());
  std::vector<paddle::dialect::IrMetaTensor> vec_meta_x_grad;
  for (size_t i=0; i < static_cast<size_t>(x.size()); i++) {
    vec_meta_x_grad.push_back(paddle::dialect::IrMetaTensor(&vec_dense_x_grad[i]));
  }
  std::vector<phi::MetaTensor*> meta_x_grad;
  for (size_t i=0; i < static_cast<size_t>(vec_meta_x_grad.size()); i++) {
    meta_x_grad.push_back(&vec_meta_x_grad[i]);
  }

  phi::UnchangedMultiInferMeta(meta_x, meta_x_grad);

  std::vector<pir::Type> argument_outputs;
  std::vector<pir::Type> x_grad_types;
  for (size_t i=0; i < static_cast<size_t>(x.size()); i++) {
    x_grad_types.push_back(paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(vec_dense_x_grad[i].dtype()), vec_dense_x_grad[i].dims(), vec_dense_x_grad[i].layout(), vec_dense_x_grad[i].lod(), vec_dense_x_grad[i].offset()));
  }
  pir::Type x_grad_vector_type = pir::VectorType::get(pir::IrContext::Instance(), x_grad_types);
  argument_outputs.push_back(x_grad_vector_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void ConcatGradOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, pir::Value out_grad_, pir::AttributeMap attributes) {
  VLOG(4) << "Start build ConcatGradOp";


  IR_ENFORCE(
      attributes.find("axis") != attributes.end(),
          "'axis' Attribute is expected for ConcatGradOp. ");
  int axis = attributes.at("axis").dyn_cast<paddle::dialect::ScalarAttribute>().data().to<int>();

  // Generate scalar mutable attribute: axis
  paddle::dialect::FullOp full_axis_op = builder.Build<paddle::dialect::FullOp>(std::vector<int64_t>{1}, axis, phi::DataType::INT32, phi::CPUPlace());
  pir::OpResult axis_ = full_axis_op->result(0);
    
  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_, out_grad_, axis_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";

  VLOG(4) << "Builder construction outputs";
  pir::VectorType x = x_.type().dyn_cast<pir::VectorType>(); (void)x;
  paddle::dialect::DenseTensorType out_grad = out_grad_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)out_grad;
  std::vector<paddle::dialect::IrTensor> vec_ir_tensor_x;
  for (size_t i=0; i < static_cast<size_t>(x.size()); i++) {
    vec_ir_tensor_x.push_back(paddle::dialect::IrTensor(paddle::dialect::TransToPhiDataType(x[i].dyn_cast<paddle::dialect::DenseTensorType>().dtype()),
                                                                     x[i].dyn_cast<paddle::dialect::DenseTensorType>().dims(),
                                                                     x[i].dyn_cast<paddle::dialect::DenseTensorType>().data_layout(),
                                                                     x[i].dyn_cast<paddle::dialect::DenseTensorType>().lod(),
                                                                     x[i].dyn_cast<paddle::dialect::DenseTensorType>().offset()));
  }
  std::vector<paddle::dialect::IrMetaTensor> vec_meta_x;
  for (size_t i=0; i < vec_ir_tensor_x.size(); i++) {
    vec_meta_x.push_back(paddle::dialect::IrMetaTensor(&vec_ir_tensor_x[i]));
  }

  std::vector<const phi::MetaTensor*> meta_x;
  for (size_t i=0; i < static_cast<size_t>(vec_meta_x.size()); i++) {
    meta_x.push_back(&vec_meta_x[i]);
  }
   std::vector<paddle::dialect::IrTensor> vec_dense_x_grad((x.size()), paddle::dialect::IrTensor());
  std::vector<paddle::dialect::IrMetaTensor> vec_meta_x_grad;
  for (size_t i=0; i < static_cast<size_t>(x.size()); i++) {
    vec_meta_x_grad.push_back(paddle::dialect::IrMetaTensor(&vec_dense_x_grad[i]));
  }
  std::vector<phi::MetaTensor*> meta_x_grad;
  for (size_t i=0; i < static_cast<size_t>(vec_meta_x_grad.size()); i++) {
    meta_x_grad.push_back(&vec_meta_x_grad[i]);
  }

  phi::UnchangedMultiInferMeta(meta_x, meta_x_grad);

  std::vector<pir::Type> argument_outputs;
  std::vector<pir::Type> x_grad_types;
  for (size_t i=0; i < static_cast<size_t>(x.size()); i++) {
    x_grad_types.push_back(paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(vec_dense_x_grad[i].dtype()), vec_dense_x_grad[i].dims(), vec_dense_x_grad[i].layout(), vec_dense_x_grad[i].lod(), vec_dense_x_grad[i].offset()));
  }
  pir::Type x_grad_vector_type = pir::VectorType::get(pir::IrContext::Instance(), x_grad_types);
  argument_outputs.push_back(x_grad_vector_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void ConcatGradOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, pir::Value out_grad_, pir::Value axis_) {
  VLOG(4) << "Start build ConcatGradOp";



  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_, out_grad_, axis_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";

  VLOG(4) << "Builder construction outputs";
  pir::VectorType x = x_.type().dyn_cast<pir::VectorType>(); (void)x;
  paddle::dialect::DenseTensorType out_grad = out_grad_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)out_grad;
  phi::Scalar axis;
  if (axis_.dyn_cast<pir::OpResult>() && axis_.dyn_cast<pir::OpResult>().owner()->isa<paddle::dialect::FullOp>()) {
    axis = std::move(phi::Scalar(axis_.dyn_cast<pir::OpResult>().owner()
                                  ->dyn_cast<paddle::dialect::FullOp>()
                                  .attribute("value")
                                  .dyn_cast<paddle::dialect::ScalarAttribute>()
                                  .data()
                                  .to<int>()));
  }
  else {
    axis = std::move(phi::Scalar(-1));
    axis.SetFromTensor(true);
  }

  std::vector<paddle::dialect::IrTensor> vec_ir_tensor_x;
  for (size_t i=0; i < static_cast<size_t>(x.size()); i++) {
    vec_ir_tensor_x.push_back(paddle::dialect::IrTensor(paddle::dialect::TransToPhiDataType(x[i].dyn_cast<paddle::dialect::DenseTensorType>().dtype()),
                                                                     x[i].dyn_cast<paddle::dialect::DenseTensorType>().dims(),
                                                                     x[i].dyn_cast<paddle::dialect::DenseTensorType>().data_layout(),
                                                                     x[i].dyn_cast<paddle::dialect::DenseTensorType>().lod(),
                                                                     x[i].dyn_cast<paddle::dialect::DenseTensorType>().offset()));
  }
  std::vector<paddle::dialect::IrMetaTensor> vec_meta_x;
  for (size_t i=0; i < vec_ir_tensor_x.size(); i++) {
    vec_meta_x.push_back(paddle::dialect::IrMetaTensor(&vec_ir_tensor_x[i]));
  }

  std::vector<const phi::MetaTensor*> meta_x;
  for (size_t i=0; i < static_cast<size_t>(vec_meta_x.size()); i++) {
    meta_x.push_back(&vec_meta_x[i]);
  }
   std::vector<paddle::dialect::IrTensor> vec_dense_x_grad((x.size()), paddle::dialect::IrTensor());
  std::vector<paddle::dialect::IrMetaTensor> vec_meta_x_grad;
  for (size_t i=0; i < static_cast<size_t>(x.size()); i++) {
    vec_meta_x_grad.push_back(paddle::dialect::IrMetaTensor(&vec_dense_x_grad[i]));
  }
  std::vector<phi::MetaTensor*> meta_x_grad;
  for (size_t i=0; i < static_cast<size_t>(vec_meta_x_grad.size()); i++) {
    meta_x_grad.push_back(&vec_meta_x_grad[i]);
  }

  phi::UnchangedMultiInferMeta(meta_x, meta_x_grad);

  std::vector<pir::Type> argument_outputs;
  std::vector<pir::Type> x_grad_types;
  for (size_t i=0; i < static_cast<size_t>(x.size()); i++) {
    x_grad_types.push_back(paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(vec_dense_x_grad[i].dtype()), vec_dense_x_grad[i].dims(), vec_dense_x_grad[i].layout(), vec_dense_x_grad[i].lod(), vec_dense_x_grad[i].offset()));
  }
  pir::Type x_grad_vector_type = pir::VectorType::get(pir::IrContext::Instance(), x_grad_types);
  argument_outputs.push_back(x_grad_vector_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void ConcatGradOp::VerifySig() {}

void ConcatGradOp::InferMeta( phi::InferMetaContext *infer_meta ) {
  auto fn = PD_INFER_META(phi::UnchangedMultiInferMeta);
  fn(infer_meta);
}

phi::DataType ConcatGradOp::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: ConcatGradOp";
  


  return expected_kernel_dtype;
}

OpInfoTuple ConjGradOp::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("out_grad", "paddle::dialect::DenseTensorType", false, false, false, false) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = {  };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("x_grad", "paddle::dialect::DenseTensorType", false, false) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("", {""}, "", {""}, {}, {}, {}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "conj_grad");
}

void ConjGradOp::VerifySig() {}

phi::DataType ConjGradOp::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: ConjGradOp";
  


  return expected_kernel_dtype;
}

const char *Conv2dGradOp::attributes_name[6] = { "strides", "paddings", "padding_algorithm", "dilations", "groups", "data_format" };

OpInfoTuple Conv2dGradOp::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("input", "paddle::dialect::DenseTensorType", false, false, false, true), paddle::dialect::OpInputInfo("filter", "paddle::dialect::DenseTensorType", false, false, false, true), paddle::dialect::OpInputInfo("out_grad", "paddle::dialect::DenseTensorType", false, false, false, true) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = { paddle::dialect::OpAttributeInfo("strides", "pir::ArrayAttribute<pir::Int32Attribute>", ""), paddle::dialect::OpAttributeInfo("paddings", "pir::ArrayAttribute<pir::Int32Attribute>", ""), paddle::dialect::OpAttributeInfo("padding_algorithm", "pir::StrAttribute", ""), paddle::dialect::OpAttributeInfo("dilations", "pir::ArrayAttribute<pir::Int32Attribute>", ""), paddle::dialect::OpAttributeInfo("groups", "pir::Int32Attribute", ""), paddle::dialect::OpAttributeInfo("data_format", "pir::StrAttribute", "") };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("input_grad", "paddle::dialect::DenseTensorType", false, false), paddle::dialect::OpOutputInfo("filter_grad", "paddle::dialect::DenseTensorType", false, false) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("GeneralBinaryGradInferMeta", {"input", "filter"}, "conv2d_grad", {"input", "filter", "out_grad", "strides", "paddings", "padding_algorithm", "dilations", "groups", "data_format"}, {"input"}, {}, {}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "conv2d_grad");
}

void Conv2dGradOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value input_, pir::Value filter_, pir::Value out_grad_, const std::vector<int>& strides, const std::vector<int>& paddings, const std::string& padding_algorithm, const std::vector<int>& dilations, int groups, const std::string& data_format) {
  VLOG(4) << "Start build Conv2dGradOp";



  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {input_, filter_, out_grad_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  std::vector<pir::Attribute> vec_strides;
  for (size_t i = 0; i < static_cast<size_t>(strides.size()); i++) {
      pir::Attribute attr_strides = pir::Int32Attribute::get(pir::IrContext::Instance(), strides[i]);

    vec_strides.push_back(attr_strides);
  }
  pir::Attribute attr_strides = pir::ArrayAttribute::get(pir::IrContext::Instance(), vec_strides);
  argument.AddAttribute("strides", attr_strides);
  std::vector<pir::Attribute> vec_paddings;
  for (size_t i = 0; i < static_cast<size_t>(paddings.size()); i++) {
      pir::Attribute attr_paddings = pir::Int32Attribute::get(pir::IrContext::Instance(), paddings[i]);

    vec_paddings.push_back(attr_paddings);
  }
  pir::Attribute attr_paddings = pir::ArrayAttribute::get(pir::IrContext::Instance(), vec_paddings);
  argument.AddAttribute("paddings", attr_paddings);
  pir::Attribute attr_padding_algorithm = pir::StrAttribute::get(pir::IrContext::Instance(), padding_algorithm);
  argument.AddAttribute("padding_algorithm", attr_padding_algorithm);
  std::vector<pir::Attribute> vec_dilations;
  for (size_t i = 0; i < static_cast<size_t>(dilations.size()); i++) {
      pir::Attribute attr_dilations = pir::Int32Attribute::get(pir::IrContext::Instance(), dilations[i]);

    vec_dilations.push_back(attr_dilations);
  }
  pir::Attribute attr_dilations = pir::ArrayAttribute::get(pir::IrContext::Instance(), vec_dilations);
  argument.AddAttribute("dilations", attr_dilations);
  pir::Attribute attr_groups = pir::Int32Attribute::get(pir::IrContext::Instance(), groups);
  argument.AddAttribute("groups", attr_groups);
  pir::Attribute attr_data_format = pir::StrAttribute::get(pir::IrContext::Instance(), data_format);
  argument.AddAttribute("data_format", attr_data_format);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType input = input_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)input;
  paddle::dialect::DenseTensorType filter = filter_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)filter;
  paddle::dialect::DenseTensorType out_grad = out_grad_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)out_grad;

  VLOG(4) << "Builder construction  dense_input";
  paddle::dialect::IrTensor ir_tensor_input(paddle::dialect::TransToPhiDataType(input.dtype()),
                                                      input.dims(),
                                                      input.data_layout(),
                                                      input.lod(),
                                                      input.offset());
  VLOG(4) << "Builder construction  meta_input";
  paddle::dialect::IrMetaTensor meta_input(&ir_tensor_input);

  VLOG(4) << "Builder construction  dense_filter";
  paddle::dialect::IrTensor ir_tensor_filter(paddle::dialect::TransToPhiDataType(filter.dtype()),
                                                      filter.dims(),
                                                      filter.data_layout(),
                                                      filter.lod(),
                                                      filter.offset());
  VLOG(4) << "Builder construction  meta_filter";
  paddle::dialect::IrMetaTensor meta_filter(&ir_tensor_filter);
  paddle::dialect::IrTensor dense_input_grad;
  paddle::dialect::IrMetaTensor meta_input_grad(&dense_input_grad);
  paddle::dialect::IrTensor dense_filter_grad;
  paddle::dialect::IrMetaTensor meta_filter_grad(&dense_filter_grad);

  phi::GeneralBinaryGradInferMeta(meta_input, meta_filter, &meta_input_grad, &meta_filter_grad);

  std::vector<pir::Type> argument_outputs;
  pir::Type input_grad_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_input_grad.dtype()), dense_input_grad.dims(), dense_input_grad.layout(), dense_input_grad.lod(), dense_input_grad.offset());
  argument_outputs.push_back(input_grad_dense_tensor_type);

  pir::Type filter_grad_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_filter_grad.dtype()), dense_filter_grad.dims(), dense_filter_grad.layout(), dense_filter_grad.lod(), dense_filter_grad.offset());
  argument_outputs.push_back(filter_grad_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void Conv2dGradOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value input_, pir::Value filter_, pir::Value out_grad_, pir::AttributeMap attributes) {
  VLOG(4) << "Start build Conv2dGradOp";


  IR_ENFORCE(
      attributes.find("strides") != attributes.end(),
          "'strides' Attribute is expected for Conv2dGradOp. ");
  std::vector<int> strides;
  for (size_t i = 0; i < attributes.at("strides").dyn_cast<pir::ArrayAttribute>().size(); i++) {
    strides.push_back(attributes.at("strides").dyn_cast<pir::ArrayAttribute>().at(i).dyn_cast<pir::Int32Attribute>().data());
  }

  IR_ENFORCE(
      attributes.find("paddings") != attributes.end(),
          "'paddings' Attribute is expected for Conv2dGradOp. ");
  std::vector<int> paddings;
  for (size_t i = 0; i < attributes.at("paddings").dyn_cast<pir::ArrayAttribute>().size(); i++) {
    paddings.push_back(attributes.at("paddings").dyn_cast<pir::ArrayAttribute>().at(i).dyn_cast<pir::Int32Attribute>().data());
  }

  IR_ENFORCE(
      attributes.find("padding_algorithm") != attributes.end(),
          "'padding_algorithm' Attribute is expected for Conv2dGradOp. ");
  std::string padding_algorithm = attributes.at("padding_algorithm").dyn_cast<pir::StrAttribute>().AsString();

  IR_ENFORCE(
      attributes.find("dilations") != attributes.end(),
          "'dilations' Attribute is expected for Conv2dGradOp. ");
  std::vector<int> dilations;
  for (size_t i = 0; i < attributes.at("dilations").dyn_cast<pir::ArrayAttribute>().size(); i++) {
    dilations.push_back(attributes.at("dilations").dyn_cast<pir::ArrayAttribute>().at(i).dyn_cast<pir::Int32Attribute>().data());
  }

  IR_ENFORCE(
      attributes.find("groups") != attributes.end(),
          "'groups' Attribute is expected for Conv2dGradOp. ");
  int groups = attributes.at("groups").dyn_cast<pir::Int32Attribute>().data();

  IR_ENFORCE(
      attributes.find("data_format") != attributes.end(),
          "'data_format' Attribute is expected for Conv2dGradOp. ");
  std::string data_format = attributes.at("data_format").dyn_cast<pir::StrAttribute>().AsString();


  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {input_, filter_, out_grad_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  std::vector<pir::Attribute> vec_strides;
  for (size_t i = 0; i < static_cast<size_t>(strides.size()); i++) {
      pir::Attribute attr_strides = pir::Int32Attribute::get(pir::IrContext::Instance(), strides[i]);

    vec_strides.push_back(attr_strides);
  }
  pir::Attribute attr_strides = pir::ArrayAttribute::get(pir::IrContext::Instance(), vec_strides);
  argument.AddAttribute("strides", attr_strides);
  std::vector<pir::Attribute> vec_paddings;
  for (size_t i = 0; i < static_cast<size_t>(paddings.size()); i++) {
      pir::Attribute attr_paddings = pir::Int32Attribute::get(pir::IrContext::Instance(), paddings[i]);

    vec_paddings.push_back(attr_paddings);
  }
  pir::Attribute attr_paddings = pir::ArrayAttribute::get(pir::IrContext::Instance(), vec_paddings);
  argument.AddAttribute("paddings", attr_paddings);
  pir::Attribute attr_padding_algorithm = pir::StrAttribute::get(pir::IrContext::Instance(), padding_algorithm);
  argument.AddAttribute("padding_algorithm", attr_padding_algorithm);
  std::vector<pir::Attribute> vec_dilations;
  for (size_t i = 0; i < static_cast<size_t>(dilations.size()); i++) {
      pir::Attribute attr_dilations = pir::Int32Attribute::get(pir::IrContext::Instance(), dilations[i]);

    vec_dilations.push_back(attr_dilations);
  }
  pir::Attribute attr_dilations = pir::ArrayAttribute::get(pir::IrContext::Instance(), vec_dilations);
  argument.AddAttribute("dilations", attr_dilations);
  pir::Attribute attr_groups = pir::Int32Attribute::get(pir::IrContext::Instance(), groups);
  argument.AddAttribute("groups", attr_groups);
  pir::Attribute attr_data_format = pir::StrAttribute::get(pir::IrContext::Instance(), data_format);
  argument.AddAttribute("data_format", attr_data_format);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType input = input_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)input;
  paddle::dialect::DenseTensorType filter = filter_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)filter;
  paddle::dialect::DenseTensorType out_grad = out_grad_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)out_grad;

  VLOG(4) << "Builder construction  dense_input";
  paddle::dialect::IrTensor ir_tensor_input(paddle::dialect::TransToPhiDataType(input.dtype()),
                                                      input.dims(),
                                                      input.data_layout(),
                                                      input.lod(),
                                                      input.offset());
  VLOG(4) << "Builder construction  meta_input";
  paddle::dialect::IrMetaTensor meta_input(&ir_tensor_input);

  VLOG(4) << "Builder construction  dense_filter";
  paddle::dialect::IrTensor ir_tensor_filter(paddle::dialect::TransToPhiDataType(filter.dtype()),
                                                      filter.dims(),
                                                      filter.data_layout(),
                                                      filter.lod(),
                                                      filter.offset());
  VLOG(4) << "Builder construction  meta_filter";
  paddle::dialect::IrMetaTensor meta_filter(&ir_tensor_filter);
  paddle::dialect::IrTensor dense_input_grad;
  paddle::dialect::IrMetaTensor meta_input_grad(&dense_input_grad);
  paddle::dialect::IrTensor dense_filter_grad;
  paddle::dialect::IrMetaTensor meta_filter_grad(&dense_filter_grad);

  phi::GeneralBinaryGradInferMeta(meta_input, meta_filter, &meta_input_grad, &meta_filter_grad);

  std::vector<pir::Type> argument_outputs;
  pir::Type input_grad_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_input_grad.dtype()), dense_input_grad.dims(), dense_input_grad.layout(), dense_input_grad.lod(), dense_input_grad.offset());
  argument_outputs.push_back(input_grad_dense_tensor_type);

  pir::Type filter_grad_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_filter_grad.dtype()), dense_filter_grad.dims(), dense_filter_grad.layout(), dense_filter_grad.lod(), dense_filter_grad.offset());
  argument_outputs.push_back(filter_grad_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void Conv2dGradOp::VerifySig() {}

void Conv2dGradOp::InferMeta( phi::InferMetaContext *infer_meta ) {
  auto fn = PD_INFER_META(phi::GeneralBinaryGradInferMeta);
  fn(infer_meta);
}

phi::DataType Conv2dGradOp::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: Conv2dGradOp";
  


  return expected_kernel_dtype;
}

const char *Conv2dGradGradOp::attributes_name[6] = { "strides", "paddings", "padding_algorithm", "dilations", "groups", "data_format" };

OpInfoTuple Conv2dGradGradOp::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("input", "paddle::dialect::DenseTensorType", false, false, false, false), paddle::dialect::OpInputInfo("filter", "paddle::dialect::DenseTensorType", false, false, false, false), paddle::dialect::OpInputInfo("grad_out", "paddle::dialect::DenseTensorType", false, false, false, false), paddle::dialect::OpInputInfo("grad_input_grad", "paddle::dialect::DenseTensorType", true, false, false, false), paddle::dialect::OpInputInfo("grad_filter_grad", "paddle::dialect::DenseTensorType", true, false, false, false) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = { paddle::dialect::OpAttributeInfo("strides", "pir::ArrayAttribute<pir::Int32Attribute>", ""), paddle::dialect::OpAttributeInfo("paddings", "pir::ArrayAttribute<pir::Int32Attribute>", ""), paddle::dialect::OpAttributeInfo("padding_algorithm", "pir::StrAttribute", ""), paddle::dialect::OpAttributeInfo("dilations", "pir::ArrayAttribute<pir::Int32Attribute>", ""), paddle::dialect::OpAttributeInfo("groups", "pir::Int32Attribute", ""), paddle::dialect::OpAttributeInfo("data_format", "pir::StrAttribute", "") };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("input_grad", "paddle::dialect::DenseTensorType", false, false), paddle::dialect::OpOutputInfo("filter_grad", "paddle::dialect::DenseTensorType", false, false), paddle::dialect::OpOutputInfo("grad_out_grad", "paddle::dialect::DenseTensorType", false, false) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("GeneralTernaryGradInferMeta", {"input", "filter", "grad_out"}, "conv2d_double_grad", {"input", "filter", "grad_out", "grad_input_grad", "grad_filter_grad", "strides", "paddings", "padding_algorithm", "dilations", "groups", "data_format"}, {"input"}, {}, {}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "conv2d_grad_grad");
}

void Conv2dGradGradOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value input_, pir::Value filter_, pir::Value grad_out_, pir::Value grad_input_grad_, pir::Value grad_filter_grad_, const std::vector<int>& strides, const std::vector<int>& paddings, const std::string& padding_algorithm, const std::vector<int>& dilations, int groups, const std::string& data_format) {
  VLOG(4) << "Start build Conv2dGradGradOp";



  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {input_, filter_, grad_out_, grad_input_grad_, grad_filter_grad_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  std::vector<pir::Attribute> vec_strides;
  for (size_t i = 0; i < static_cast<size_t>(strides.size()); i++) {
      pir::Attribute attr_strides = pir::Int32Attribute::get(pir::IrContext::Instance(), strides[i]);

    vec_strides.push_back(attr_strides);
  }
  pir::Attribute attr_strides = pir::ArrayAttribute::get(pir::IrContext::Instance(), vec_strides);
  argument.AddAttribute("strides", attr_strides);
  std::vector<pir::Attribute> vec_paddings;
  for (size_t i = 0; i < static_cast<size_t>(paddings.size()); i++) {
      pir::Attribute attr_paddings = pir::Int32Attribute::get(pir::IrContext::Instance(), paddings[i]);

    vec_paddings.push_back(attr_paddings);
  }
  pir::Attribute attr_paddings = pir::ArrayAttribute::get(pir::IrContext::Instance(), vec_paddings);
  argument.AddAttribute("paddings", attr_paddings);
  pir::Attribute attr_padding_algorithm = pir::StrAttribute::get(pir::IrContext::Instance(), padding_algorithm);
  argument.AddAttribute("padding_algorithm", attr_padding_algorithm);
  std::vector<pir::Attribute> vec_dilations;
  for (size_t i = 0; i < static_cast<size_t>(dilations.size()); i++) {
      pir::Attribute attr_dilations = pir::Int32Attribute::get(pir::IrContext::Instance(), dilations[i]);

    vec_dilations.push_back(attr_dilations);
  }
  pir::Attribute attr_dilations = pir::ArrayAttribute::get(pir::IrContext::Instance(), vec_dilations);
  argument.AddAttribute("dilations", attr_dilations);
  pir::Attribute attr_groups = pir::Int32Attribute::get(pir::IrContext::Instance(), groups);
  argument.AddAttribute("groups", attr_groups);
  pir::Attribute attr_data_format = pir::StrAttribute::get(pir::IrContext::Instance(), data_format);
  argument.AddAttribute("data_format", attr_data_format);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType input = input_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)input;
  paddle::dialect::DenseTensorType filter = filter_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)filter;
  paddle::dialect::DenseTensorType grad_out = grad_out_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)grad_out;

  VLOG(4) << "Builder construction  dense_input";
  paddle::dialect::IrTensor ir_tensor_input(paddle::dialect::TransToPhiDataType(input.dtype()),
                                                      input.dims(),
                                                      input.data_layout(),
                                                      input.lod(),
                                                      input.offset());
  VLOG(4) << "Builder construction  meta_input";
  paddle::dialect::IrMetaTensor meta_input(&ir_tensor_input);

  VLOG(4) << "Builder construction  dense_filter";
  paddle::dialect::IrTensor ir_tensor_filter(paddle::dialect::TransToPhiDataType(filter.dtype()),
                                                      filter.dims(),
                                                      filter.data_layout(),
                                                      filter.lod(),
                                                      filter.offset());
  VLOG(4) << "Builder construction  meta_filter";
  paddle::dialect::IrMetaTensor meta_filter(&ir_tensor_filter);

  VLOG(4) << "Builder construction  dense_grad_out";
  paddle::dialect::IrTensor ir_tensor_grad_out(paddle::dialect::TransToPhiDataType(grad_out.dtype()),
                                                      grad_out.dims(),
                                                      grad_out.data_layout(),
                                                      grad_out.lod(),
                                                      grad_out.offset());
  VLOG(4) << "Builder construction  meta_grad_out";
  paddle::dialect::IrMetaTensor meta_grad_out(&ir_tensor_grad_out);
  paddle::dialect::IrTensor dense_input_grad;
  paddle::dialect::IrMetaTensor meta_input_grad(&dense_input_grad);
  paddle::dialect::IrTensor dense_filter_grad;
  paddle::dialect::IrMetaTensor meta_filter_grad(&dense_filter_grad);
  paddle::dialect::IrTensor dense_grad_out_grad;
  paddle::dialect::IrMetaTensor meta_grad_out_grad(&dense_grad_out_grad);

  phi::GeneralTernaryGradInferMeta(meta_input, meta_filter, meta_grad_out, &meta_input_grad, &meta_filter_grad, &meta_grad_out_grad);

  std::vector<pir::Type> argument_outputs;
  pir::Type input_grad_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_input_grad.dtype()), dense_input_grad.dims(), dense_input_grad.layout(), dense_input_grad.lod(), dense_input_grad.offset());
  argument_outputs.push_back(input_grad_dense_tensor_type);

  pir::Type filter_grad_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_filter_grad.dtype()), dense_filter_grad.dims(), dense_filter_grad.layout(), dense_filter_grad.lod(), dense_filter_grad.offset());
  argument_outputs.push_back(filter_grad_dense_tensor_type);

  pir::Type grad_out_grad_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_grad_out_grad.dtype()), dense_grad_out_grad.dims(), dense_grad_out_grad.layout(), dense_grad_out_grad.lod(), dense_grad_out_grad.offset());
  argument_outputs.push_back(grad_out_grad_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void Conv2dGradGradOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value input_, pir::Value filter_, pir::Value grad_out_, pir::Value grad_input_grad_, pir::Value grad_filter_grad_, pir::AttributeMap attributes) {
  VLOG(4) << "Start build Conv2dGradGradOp";


  IR_ENFORCE(
      attributes.find("strides") != attributes.end(),
          "'strides' Attribute is expected for Conv2dGradGradOp. ");
  std::vector<int> strides;
  for (size_t i = 0; i < attributes.at("strides").dyn_cast<pir::ArrayAttribute>().size(); i++) {
    strides.push_back(attributes.at("strides").dyn_cast<pir::ArrayAttribute>().at(i).dyn_cast<pir::Int32Attribute>().data());
  }

  IR_ENFORCE(
      attributes.find("paddings") != attributes.end(),
          "'paddings' Attribute is expected for Conv2dGradGradOp. ");
  std::vector<int> paddings;
  for (size_t i = 0; i < attributes.at("paddings").dyn_cast<pir::ArrayAttribute>().size(); i++) {
    paddings.push_back(attributes.at("paddings").dyn_cast<pir::ArrayAttribute>().at(i).dyn_cast<pir::Int32Attribute>().data());
  }

  IR_ENFORCE(
      attributes.find("padding_algorithm") != attributes.end(),
          "'padding_algorithm' Attribute is expected for Conv2dGradGradOp. ");
  std::string padding_algorithm = attributes.at("padding_algorithm").dyn_cast<pir::StrAttribute>().AsString();

  IR_ENFORCE(
      attributes.find("dilations") != attributes.end(),
          "'dilations' Attribute is expected for Conv2dGradGradOp. ");
  std::vector<int> dilations;
  for (size_t i = 0; i < attributes.at("dilations").dyn_cast<pir::ArrayAttribute>().size(); i++) {
    dilations.push_back(attributes.at("dilations").dyn_cast<pir::ArrayAttribute>().at(i).dyn_cast<pir::Int32Attribute>().data());
  }

  IR_ENFORCE(
      attributes.find("groups") != attributes.end(),
          "'groups' Attribute is expected for Conv2dGradGradOp. ");
  int groups = attributes.at("groups").dyn_cast<pir::Int32Attribute>().data();

  IR_ENFORCE(
      attributes.find("data_format") != attributes.end(),
          "'data_format' Attribute is expected for Conv2dGradGradOp. ");
  std::string data_format = attributes.at("data_format").dyn_cast<pir::StrAttribute>().AsString();


  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {input_, filter_, grad_out_, grad_input_grad_, grad_filter_grad_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  std::vector<pir::Attribute> vec_strides;
  for (size_t i = 0; i < static_cast<size_t>(strides.size()); i++) {
      pir::Attribute attr_strides = pir::Int32Attribute::get(pir::IrContext::Instance(), strides[i]);

    vec_strides.push_back(attr_strides);
  }
  pir::Attribute attr_strides = pir::ArrayAttribute::get(pir::IrContext::Instance(), vec_strides);
  argument.AddAttribute("strides", attr_strides);
  std::vector<pir::Attribute> vec_paddings;
  for (size_t i = 0; i < static_cast<size_t>(paddings.size()); i++) {
      pir::Attribute attr_paddings = pir::Int32Attribute::get(pir::IrContext::Instance(), paddings[i]);

    vec_paddings.push_back(attr_paddings);
  }
  pir::Attribute attr_paddings = pir::ArrayAttribute::get(pir::IrContext::Instance(), vec_paddings);
  argument.AddAttribute("paddings", attr_paddings);
  pir::Attribute attr_padding_algorithm = pir::StrAttribute::get(pir::IrContext::Instance(), padding_algorithm);
  argument.AddAttribute("padding_algorithm", attr_padding_algorithm);
  std::vector<pir::Attribute> vec_dilations;
  for (size_t i = 0; i < static_cast<size_t>(dilations.size()); i++) {
      pir::Attribute attr_dilations = pir::Int32Attribute::get(pir::IrContext::Instance(), dilations[i]);

    vec_dilations.push_back(attr_dilations);
  }
  pir::Attribute attr_dilations = pir::ArrayAttribute::get(pir::IrContext::Instance(), vec_dilations);
  argument.AddAttribute("dilations", attr_dilations);
  pir::Attribute attr_groups = pir::Int32Attribute::get(pir::IrContext::Instance(), groups);
  argument.AddAttribute("groups", attr_groups);
  pir::Attribute attr_data_format = pir::StrAttribute::get(pir::IrContext::Instance(), data_format);
  argument.AddAttribute("data_format", attr_data_format);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType input = input_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)input;
  paddle::dialect::DenseTensorType filter = filter_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)filter;
  paddle::dialect::DenseTensorType grad_out = grad_out_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)grad_out;

  VLOG(4) << "Builder construction  dense_input";
  paddle::dialect::IrTensor ir_tensor_input(paddle::dialect::TransToPhiDataType(input.dtype()),
                                                      input.dims(),
                                                      input.data_layout(),
                                                      input.lod(),
                                                      input.offset());
  VLOG(4) << "Builder construction  meta_input";
  paddle::dialect::IrMetaTensor meta_input(&ir_tensor_input);

  VLOG(4) << "Builder construction  dense_filter";
  paddle::dialect::IrTensor ir_tensor_filter(paddle::dialect::TransToPhiDataType(filter.dtype()),
                                                      filter.dims(),
                                                      filter.data_layout(),
                                                      filter.lod(),
                                                      filter.offset());
  VLOG(4) << "Builder construction  meta_filter";
  paddle::dialect::IrMetaTensor meta_filter(&ir_tensor_filter);

  VLOG(4) << "Builder construction  dense_grad_out";
  paddle::dialect::IrTensor ir_tensor_grad_out(paddle::dialect::TransToPhiDataType(grad_out.dtype()),
                                                      grad_out.dims(),
                                                      grad_out.data_layout(),
                                                      grad_out.lod(),
                                                      grad_out.offset());
  VLOG(4) << "Builder construction  meta_grad_out";
  paddle::dialect::IrMetaTensor meta_grad_out(&ir_tensor_grad_out);
  paddle::dialect::IrTensor dense_input_grad;
  paddle::dialect::IrMetaTensor meta_input_grad(&dense_input_grad);
  paddle::dialect::IrTensor dense_filter_grad;
  paddle::dialect::IrMetaTensor meta_filter_grad(&dense_filter_grad);
  paddle::dialect::IrTensor dense_grad_out_grad;
  paddle::dialect::IrMetaTensor meta_grad_out_grad(&dense_grad_out_grad);

  phi::GeneralTernaryGradInferMeta(meta_input, meta_filter, meta_grad_out, &meta_input_grad, &meta_filter_grad, &meta_grad_out_grad);

  std::vector<pir::Type> argument_outputs;
  pir::Type input_grad_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_input_grad.dtype()), dense_input_grad.dims(), dense_input_grad.layout(), dense_input_grad.lod(), dense_input_grad.offset());
  argument_outputs.push_back(input_grad_dense_tensor_type);

  pir::Type filter_grad_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_filter_grad.dtype()), dense_filter_grad.dims(), dense_filter_grad.layout(), dense_filter_grad.lod(), dense_filter_grad.offset());
  argument_outputs.push_back(filter_grad_dense_tensor_type);

  pir::Type grad_out_grad_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_grad_out_grad.dtype()), dense_grad_out_grad.dims(), dense_grad_out_grad.layout(), dense_grad_out_grad.lod(), dense_grad_out_grad.offset());
  argument_outputs.push_back(grad_out_grad_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void Conv2dGradGradOp::VerifySig() {}

void Conv2dGradGradOp::InferMeta( phi::InferMetaContext *infer_meta ) {
  auto fn = PD_INFER_META(phi::GeneralTernaryGradInferMeta);
  fn(infer_meta);
}

phi::DataType Conv2dGradGradOp::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: Conv2dGradGradOp";
  


  return expected_kernel_dtype;
}

const char *Conv3dDoubleGradOp::attributes_name[6] = { "strides", "paddings", "padding_algorithm", "groups", "dilations", "data_format" };

OpInfoTuple Conv3dDoubleGradOp::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("input", "paddle::dialect::DenseTensorType", false, false, false, false), paddle::dialect::OpInputInfo("filter", "paddle::dialect::DenseTensorType", false, false, false, false), paddle::dialect::OpInputInfo("grad_out", "paddle::dialect::DenseTensorType", false, false, false, false), paddle::dialect::OpInputInfo("grad_input_grad", "paddle::dialect::DenseTensorType", true, false, false, false), paddle::dialect::OpInputInfo("grad_filter_grad", "paddle::dialect::DenseTensorType", true, false, false, false) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = { paddle::dialect::OpAttributeInfo("strides", "pir::ArrayAttribute<pir::Int32Attribute>", ""), paddle::dialect::OpAttributeInfo("paddings", "pir::ArrayAttribute<pir::Int32Attribute>", ""), paddle::dialect::OpAttributeInfo("padding_algorithm", "pir::StrAttribute", ""), paddle::dialect::OpAttributeInfo("groups", "pir::Int32Attribute", ""), paddle::dialect::OpAttributeInfo("dilations", "pir::ArrayAttribute<pir::Int32Attribute>", ""), paddle::dialect::OpAttributeInfo("data_format", "pir::StrAttribute", "") };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("input_grad", "paddle::dialect::DenseTensorType", false, false), paddle::dialect::OpOutputInfo("filter_grad", "paddle::dialect::DenseTensorType", false, false), paddle::dialect::OpOutputInfo("grad_out_grad", "paddle::dialect::DenseTensorType", false, false) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("GeneralTernaryGradInferMeta", {"input", "filter", "grad_out"}, "conv3d_double_grad", {"input", "filter", "grad_out", "grad_input_grad", "grad_filter_grad", "strides", "paddings", "padding_algorithm", "groups", "dilations", "data_format"}, {"input"}, {}, {}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "conv3d_double_grad");
}

void Conv3dDoubleGradOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value input_, pir::Value filter_, pir::Value grad_out_, pir::Value grad_input_grad_, pir::Value grad_filter_grad_, const std::vector<int>& strides, const std::vector<int>& paddings, const std::string& padding_algorithm, int groups, const std::vector<int>& dilations, const std::string& data_format) {
  VLOG(4) << "Start build Conv3dDoubleGradOp";



  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {input_, filter_, grad_out_, grad_input_grad_, grad_filter_grad_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  std::vector<pir::Attribute> vec_strides;
  for (size_t i = 0; i < static_cast<size_t>(strides.size()); i++) {
      pir::Attribute attr_strides = pir::Int32Attribute::get(pir::IrContext::Instance(), strides[i]);

    vec_strides.push_back(attr_strides);
  }
  pir::Attribute attr_strides = pir::ArrayAttribute::get(pir::IrContext::Instance(), vec_strides);
  argument.AddAttribute("strides", attr_strides);
  std::vector<pir::Attribute> vec_paddings;
  for (size_t i = 0; i < static_cast<size_t>(paddings.size()); i++) {
      pir::Attribute attr_paddings = pir::Int32Attribute::get(pir::IrContext::Instance(), paddings[i]);

    vec_paddings.push_back(attr_paddings);
  }
  pir::Attribute attr_paddings = pir::ArrayAttribute::get(pir::IrContext::Instance(), vec_paddings);
  argument.AddAttribute("paddings", attr_paddings);
  pir::Attribute attr_padding_algorithm = pir::StrAttribute::get(pir::IrContext::Instance(), padding_algorithm);
  argument.AddAttribute("padding_algorithm", attr_padding_algorithm);
  pir::Attribute attr_groups = pir::Int32Attribute::get(pir::IrContext::Instance(), groups);
  argument.AddAttribute("groups", attr_groups);
  std::vector<pir::Attribute> vec_dilations;
  for (size_t i = 0; i < static_cast<size_t>(dilations.size()); i++) {
      pir::Attribute attr_dilations = pir::Int32Attribute::get(pir::IrContext::Instance(), dilations[i]);

    vec_dilations.push_back(attr_dilations);
  }
  pir::Attribute attr_dilations = pir::ArrayAttribute::get(pir::IrContext::Instance(), vec_dilations);
  argument.AddAttribute("dilations", attr_dilations);
  pir::Attribute attr_data_format = pir::StrAttribute::get(pir::IrContext::Instance(), data_format);
  argument.AddAttribute("data_format", attr_data_format);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType input = input_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)input;
  paddle::dialect::DenseTensorType filter = filter_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)filter;
  paddle::dialect::DenseTensorType grad_out = grad_out_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)grad_out;

  VLOG(4) << "Builder construction  dense_input";
  paddle::dialect::IrTensor ir_tensor_input(paddle::dialect::TransToPhiDataType(input.dtype()),
                                                      input.dims(),
                                                      input.data_layout(),
                                                      input.lod(),
                                                      input.offset());
  VLOG(4) << "Builder construction  meta_input";
  paddle::dialect::IrMetaTensor meta_input(&ir_tensor_input);

  VLOG(4) << "Builder construction  dense_filter";
  paddle::dialect::IrTensor ir_tensor_filter(paddle::dialect::TransToPhiDataType(filter.dtype()),
                                                      filter.dims(),
                                                      filter.data_layout(),
                                                      filter.lod(),
                                                      filter.offset());
  VLOG(4) << "Builder construction  meta_filter";
  paddle::dialect::IrMetaTensor meta_filter(&ir_tensor_filter);

  VLOG(4) << "Builder construction  dense_grad_out";
  paddle::dialect::IrTensor ir_tensor_grad_out(paddle::dialect::TransToPhiDataType(grad_out.dtype()),
                                                      grad_out.dims(),
                                                      grad_out.data_layout(),
                                                      grad_out.lod(),
                                                      grad_out.offset());
  VLOG(4) << "Builder construction  meta_grad_out";
  paddle::dialect::IrMetaTensor meta_grad_out(&ir_tensor_grad_out);
  paddle::dialect::IrTensor dense_input_grad;
  paddle::dialect::IrMetaTensor meta_input_grad(&dense_input_grad);
  paddle::dialect::IrTensor dense_filter_grad;
  paddle::dialect::IrMetaTensor meta_filter_grad(&dense_filter_grad);
  paddle::dialect::IrTensor dense_grad_out_grad;
  paddle::dialect::IrMetaTensor meta_grad_out_grad(&dense_grad_out_grad);

  phi::GeneralTernaryGradInferMeta(meta_input, meta_filter, meta_grad_out, &meta_input_grad, &meta_filter_grad, &meta_grad_out_grad);

  std::vector<pir::Type> argument_outputs;
  pir::Type input_grad_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_input_grad.dtype()), dense_input_grad.dims(), dense_input_grad.layout(), dense_input_grad.lod(), dense_input_grad.offset());
  argument_outputs.push_back(input_grad_dense_tensor_type);

  pir::Type filter_grad_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_filter_grad.dtype()), dense_filter_grad.dims(), dense_filter_grad.layout(), dense_filter_grad.lod(), dense_filter_grad.offset());
  argument_outputs.push_back(filter_grad_dense_tensor_type);

  pir::Type grad_out_grad_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_grad_out_grad.dtype()), dense_grad_out_grad.dims(), dense_grad_out_grad.layout(), dense_grad_out_grad.lod(), dense_grad_out_grad.offset());
  argument_outputs.push_back(grad_out_grad_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void Conv3dDoubleGradOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value input_, pir::Value filter_, pir::Value grad_out_, pir::Value grad_input_grad_, pir::Value grad_filter_grad_, pir::AttributeMap attributes) {
  VLOG(4) << "Start build Conv3dDoubleGradOp";


  IR_ENFORCE(
      attributes.find("strides") != attributes.end(),
          "'strides' Attribute is expected for Conv3dDoubleGradOp. ");
  std::vector<int> strides;
  for (size_t i = 0; i < attributes.at("strides").dyn_cast<pir::ArrayAttribute>().size(); i++) {
    strides.push_back(attributes.at("strides").dyn_cast<pir::ArrayAttribute>().at(i).dyn_cast<pir::Int32Attribute>().data());
  }

  IR_ENFORCE(
      attributes.find("paddings") != attributes.end(),
          "'paddings' Attribute is expected for Conv3dDoubleGradOp. ");
  std::vector<int> paddings;
  for (size_t i = 0; i < attributes.at("paddings").dyn_cast<pir::ArrayAttribute>().size(); i++) {
    paddings.push_back(attributes.at("paddings").dyn_cast<pir::ArrayAttribute>().at(i).dyn_cast<pir::Int32Attribute>().data());
  }

  IR_ENFORCE(
      attributes.find("padding_algorithm") != attributes.end(),
          "'padding_algorithm' Attribute is expected for Conv3dDoubleGradOp. ");
  std::string padding_algorithm = attributes.at("padding_algorithm").dyn_cast<pir::StrAttribute>().AsString();

  IR_ENFORCE(
      attributes.find("groups") != attributes.end(),
          "'groups' Attribute is expected for Conv3dDoubleGradOp. ");
  int groups = attributes.at("groups").dyn_cast<pir::Int32Attribute>().data();

  IR_ENFORCE(
      attributes.find("dilations") != attributes.end(),
          "'dilations' Attribute is expected for Conv3dDoubleGradOp. ");
  std::vector<int> dilations;
  for (size_t i = 0; i < attributes.at("dilations").dyn_cast<pir::ArrayAttribute>().size(); i++) {
    dilations.push_back(attributes.at("dilations").dyn_cast<pir::ArrayAttribute>().at(i).dyn_cast<pir::Int32Attribute>().data());
  }

  IR_ENFORCE(
      attributes.find("data_format") != attributes.end(),
          "'data_format' Attribute is expected for Conv3dDoubleGradOp. ");
  std::string data_format = attributes.at("data_format").dyn_cast<pir::StrAttribute>().AsString();


  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {input_, filter_, grad_out_, grad_input_grad_, grad_filter_grad_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  std::vector<pir::Attribute> vec_strides;
  for (size_t i = 0; i < static_cast<size_t>(strides.size()); i++) {
      pir::Attribute attr_strides = pir::Int32Attribute::get(pir::IrContext::Instance(), strides[i]);

    vec_strides.push_back(attr_strides);
  }
  pir::Attribute attr_strides = pir::ArrayAttribute::get(pir::IrContext::Instance(), vec_strides);
  argument.AddAttribute("strides", attr_strides);
  std::vector<pir::Attribute> vec_paddings;
  for (size_t i = 0; i < static_cast<size_t>(paddings.size()); i++) {
      pir::Attribute attr_paddings = pir::Int32Attribute::get(pir::IrContext::Instance(), paddings[i]);

    vec_paddings.push_back(attr_paddings);
  }
  pir::Attribute attr_paddings = pir::ArrayAttribute::get(pir::IrContext::Instance(), vec_paddings);
  argument.AddAttribute("paddings", attr_paddings);
  pir::Attribute attr_padding_algorithm = pir::StrAttribute::get(pir::IrContext::Instance(), padding_algorithm);
  argument.AddAttribute("padding_algorithm", attr_padding_algorithm);
  pir::Attribute attr_groups = pir::Int32Attribute::get(pir::IrContext::Instance(), groups);
  argument.AddAttribute("groups", attr_groups);
  std::vector<pir::Attribute> vec_dilations;
  for (size_t i = 0; i < static_cast<size_t>(dilations.size()); i++) {
      pir::Attribute attr_dilations = pir::Int32Attribute::get(pir::IrContext::Instance(), dilations[i]);

    vec_dilations.push_back(attr_dilations);
  }
  pir::Attribute attr_dilations = pir::ArrayAttribute::get(pir::IrContext::Instance(), vec_dilations);
  argument.AddAttribute("dilations", attr_dilations);
  pir::Attribute attr_data_format = pir::StrAttribute::get(pir::IrContext::Instance(), data_format);
  argument.AddAttribute("data_format", attr_data_format);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType input = input_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)input;
  paddle::dialect::DenseTensorType filter = filter_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)filter;
  paddle::dialect::DenseTensorType grad_out = grad_out_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)grad_out;

  VLOG(4) << "Builder construction  dense_input";
  paddle::dialect::IrTensor ir_tensor_input(paddle::dialect::TransToPhiDataType(input.dtype()),
                                                      input.dims(),
                                                      input.data_layout(),
                                                      input.lod(),
                                                      input.offset());
  VLOG(4) << "Builder construction  meta_input";
  paddle::dialect::IrMetaTensor meta_input(&ir_tensor_input);

  VLOG(4) << "Builder construction  dense_filter";
  paddle::dialect::IrTensor ir_tensor_filter(paddle::dialect::TransToPhiDataType(filter.dtype()),
                                                      filter.dims(),
                                                      filter.data_layout(),
                                                      filter.lod(),
                                                      filter.offset());
  VLOG(4) << "Builder construction  meta_filter";
  paddle::dialect::IrMetaTensor meta_filter(&ir_tensor_filter);

  VLOG(4) << "Builder construction  dense_grad_out";
  paddle::dialect::IrTensor ir_tensor_grad_out(paddle::dialect::TransToPhiDataType(grad_out.dtype()),
                                                      grad_out.dims(),
                                                      grad_out.data_layout(),
                                                      grad_out.lod(),
                                                      grad_out.offset());
  VLOG(4) << "Builder construction  meta_grad_out";
  paddle::dialect::IrMetaTensor meta_grad_out(&ir_tensor_grad_out);
  paddle::dialect::IrTensor dense_input_grad;
  paddle::dialect::IrMetaTensor meta_input_grad(&dense_input_grad);
  paddle::dialect::IrTensor dense_filter_grad;
  paddle::dialect::IrMetaTensor meta_filter_grad(&dense_filter_grad);
  paddle::dialect::IrTensor dense_grad_out_grad;
  paddle::dialect::IrMetaTensor meta_grad_out_grad(&dense_grad_out_grad);

  phi::GeneralTernaryGradInferMeta(meta_input, meta_filter, meta_grad_out, &meta_input_grad, &meta_filter_grad, &meta_grad_out_grad);

  std::vector<pir::Type> argument_outputs;
  pir::Type input_grad_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_input_grad.dtype()), dense_input_grad.dims(), dense_input_grad.layout(), dense_input_grad.lod(), dense_input_grad.offset());
  argument_outputs.push_back(input_grad_dense_tensor_type);

  pir::Type filter_grad_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_filter_grad.dtype()), dense_filter_grad.dims(), dense_filter_grad.layout(), dense_filter_grad.lod(), dense_filter_grad.offset());
  argument_outputs.push_back(filter_grad_dense_tensor_type);

  pir::Type grad_out_grad_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_grad_out_grad.dtype()), dense_grad_out_grad.dims(), dense_grad_out_grad.layout(), dense_grad_out_grad.lod(), dense_grad_out_grad.offset());
  argument_outputs.push_back(grad_out_grad_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void Conv3dDoubleGradOp::VerifySig() {}

void Conv3dDoubleGradOp::InferMeta( phi::InferMetaContext *infer_meta ) {
  auto fn = PD_INFER_META(phi::GeneralTernaryGradInferMeta);
  fn(infer_meta);
}

phi::DataType Conv3dDoubleGradOp::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: Conv3dDoubleGradOp";
  


  return expected_kernel_dtype;
}

const char *Conv3dGradOp::attributes_name[6] = { "strides", "paddings", "padding_algorithm", "groups", "dilations", "data_format" };

OpInfoTuple Conv3dGradOp::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("input", "paddle::dialect::DenseTensorType", false, false, false, true), paddle::dialect::OpInputInfo("filter", "paddle::dialect::DenseTensorType", false, false, false, true), paddle::dialect::OpInputInfo("out_grad", "paddle::dialect::DenseTensorType", false, false, false, true) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = { paddle::dialect::OpAttributeInfo("strides", "pir::ArrayAttribute<pir::Int32Attribute>", ""), paddle::dialect::OpAttributeInfo("paddings", "pir::ArrayAttribute<pir::Int32Attribute>", ""), paddle::dialect::OpAttributeInfo("padding_algorithm", "pir::StrAttribute", ""), paddle::dialect::OpAttributeInfo("groups", "pir::Int32Attribute", ""), paddle::dialect::OpAttributeInfo("dilations", "pir::ArrayAttribute<pir::Int32Attribute>", ""), paddle::dialect::OpAttributeInfo("data_format", "pir::StrAttribute", "") };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("input_grad", "paddle::dialect::DenseTensorType", false, false), paddle::dialect::OpOutputInfo("filter_grad", "paddle::dialect::DenseTensorType", false, false) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("GeneralBinaryGradInferMeta", {"input", "filter"}, "conv3d_grad", {"input", "filter", "out_grad", "strides", "paddings", "padding_algorithm", "groups", "dilations", "data_format"}, {"input"}, {}, {}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "conv3d_grad");
}

void Conv3dGradOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value input_, pir::Value filter_, pir::Value out_grad_, const std::vector<int>& strides, const std::vector<int>& paddings, const std::string& padding_algorithm, int groups, const std::vector<int>& dilations, const std::string& data_format) {
  VLOG(4) << "Start build Conv3dGradOp";



  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {input_, filter_, out_grad_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  std::vector<pir::Attribute> vec_strides;
  for (size_t i = 0; i < static_cast<size_t>(strides.size()); i++) {
      pir::Attribute attr_strides = pir::Int32Attribute::get(pir::IrContext::Instance(), strides[i]);

    vec_strides.push_back(attr_strides);
  }
  pir::Attribute attr_strides = pir::ArrayAttribute::get(pir::IrContext::Instance(), vec_strides);
  argument.AddAttribute("strides", attr_strides);
  std::vector<pir::Attribute> vec_paddings;
  for (size_t i = 0; i < static_cast<size_t>(paddings.size()); i++) {
      pir::Attribute attr_paddings = pir::Int32Attribute::get(pir::IrContext::Instance(), paddings[i]);

    vec_paddings.push_back(attr_paddings);
  }
  pir::Attribute attr_paddings = pir::ArrayAttribute::get(pir::IrContext::Instance(), vec_paddings);
  argument.AddAttribute("paddings", attr_paddings);
  pir::Attribute attr_padding_algorithm = pir::StrAttribute::get(pir::IrContext::Instance(), padding_algorithm);
  argument.AddAttribute("padding_algorithm", attr_padding_algorithm);
  pir::Attribute attr_groups = pir::Int32Attribute::get(pir::IrContext::Instance(), groups);
  argument.AddAttribute("groups", attr_groups);
  std::vector<pir::Attribute> vec_dilations;
  for (size_t i = 0; i < static_cast<size_t>(dilations.size()); i++) {
      pir::Attribute attr_dilations = pir::Int32Attribute::get(pir::IrContext::Instance(), dilations[i]);

    vec_dilations.push_back(attr_dilations);
  }
  pir::Attribute attr_dilations = pir::ArrayAttribute::get(pir::IrContext::Instance(), vec_dilations);
  argument.AddAttribute("dilations", attr_dilations);
  pir::Attribute attr_data_format = pir::StrAttribute::get(pir::IrContext::Instance(), data_format);
  argument.AddAttribute("data_format", attr_data_format);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType input = input_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)input;
  paddle::dialect::DenseTensorType filter = filter_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)filter;
  paddle::dialect::DenseTensorType out_grad = out_grad_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)out_grad;

  VLOG(4) << "Builder construction  dense_input";
  paddle::dialect::IrTensor ir_tensor_input(paddle::dialect::TransToPhiDataType(input.dtype()),
                                                      input.dims(),
                                                      input.data_layout(),
                                                      input.lod(),
                                                      input.offset());
  VLOG(4) << "Builder construction  meta_input";
  paddle::dialect::IrMetaTensor meta_input(&ir_tensor_input);

  VLOG(4) << "Builder construction  dense_filter";
  paddle::dialect::IrTensor ir_tensor_filter(paddle::dialect::TransToPhiDataType(filter.dtype()),
                                                      filter.dims(),
                                                      filter.data_layout(),
                                                      filter.lod(),
                                                      filter.offset());
  VLOG(4) << "Builder construction  meta_filter";
  paddle::dialect::IrMetaTensor meta_filter(&ir_tensor_filter);
  paddle::dialect::IrTensor dense_input_grad;
  paddle::dialect::IrMetaTensor meta_input_grad(&dense_input_grad);
  paddle::dialect::IrTensor dense_filter_grad;
  paddle::dialect::IrMetaTensor meta_filter_grad(&dense_filter_grad);

  phi::GeneralBinaryGradInferMeta(meta_input, meta_filter, &meta_input_grad, &meta_filter_grad);

  std::vector<pir::Type> argument_outputs;
  pir::Type input_grad_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_input_grad.dtype()), dense_input_grad.dims(), dense_input_grad.layout(), dense_input_grad.lod(), dense_input_grad.offset());
  argument_outputs.push_back(input_grad_dense_tensor_type);

  pir::Type filter_grad_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_filter_grad.dtype()), dense_filter_grad.dims(), dense_filter_grad.layout(), dense_filter_grad.lod(), dense_filter_grad.offset());
  argument_outputs.push_back(filter_grad_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void Conv3dGradOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value input_, pir::Value filter_, pir::Value out_grad_, pir::AttributeMap attributes) {
  VLOG(4) << "Start build Conv3dGradOp";


  IR_ENFORCE(
      attributes.find("strides") != attributes.end(),
          "'strides' Attribute is expected for Conv3dGradOp. ");
  std::vector<int> strides;
  for (size_t i = 0; i < attributes.at("strides").dyn_cast<pir::ArrayAttribute>().size(); i++) {
    strides.push_back(attributes.at("strides").dyn_cast<pir::ArrayAttribute>().at(i).dyn_cast<pir::Int32Attribute>().data());
  }

  IR_ENFORCE(
      attributes.find("paddings") != attributes.end(),
          "'paddings' Attribute is expected for Conv3dGradOp. ");
  std::vector<int> paddings;
  for (size_t i = 0; i < attributes.at("paddings").dyn_cast<pir::ArrayAttribute>().size(); i++) {
    paddings.push_back(attributes.at("paddings").dyn_cast<pir::ArrayAttribute>().at(i).dyn_cast<pir::Int32Attribute>().data());
  }

  IR_ENFORCE(
      attributes.find("padding_algorithm") != attributes.end(),
          "'padding_algorithm' Attribute is expected for Conv3dGradOp. ");
  std::string padding_algorithm = attributes.at("padding_algorithm").dyn_cast<pir::StrAttribute>().AsString();

  IR_ENFORCE(
      attributes.find("groups") != attributes.end(),
          "'groups' Attribute is expected for Conv3dGradOp. ");
  int groups = attributes.at("groups").dyn_cast<pir::Int32Attribute>().data();

  IR_ENFORCE(
      attributes.find("dilations") != attributes.end(),
          "'dilations' Attribute is expected for Conv3dGradOp. ");
  std::vector<int> dilations;
  for (size_t i = 0; i < attributes.at("dilations").dyn_cast<pir::ArrayAttribute>().size(); i++) {
    dilations.push_back(attributes.at("dilations").dyn_cast<pir::ArrayAttribute>().at(i).dyn_cast<pir::Int32Attribute>().data());
  }

  IR_ENFORCE(
      attributes.find("data_format") != attributes.end(),
          "'data_format' Attribute is expected for Conv3dGradOp. ");
  std::string data_format = attributes.at("data_format").dyn_cast<pir::StrAttribute>().AsString();


  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {input_, filter_, out_grad_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  std::vector<pir::Attribute> vec_strides;
  for (size_t i = 0; i < static_cast<size_t>(strides.size()); i++) {
      pir::Attribute attr_strides = pir::Int32Attribute::get(pir::IrContext::Instance(), strides[i]);

    vec_strides.push_back(attr_strides);
  }
  pir::Attribute attr_strides = pir::ArrayAttribute::get(pir::IrContext::Instance(), vec_strides);
  argument.AddAttribute("strides", attr_strides);
  std::vector<pir::Attribute> vec_paddings;
  for (size_t i = 0; i < static_cast<size_t>(paddings.size()); i++) {
      pir::Attribute attr_paddings = pir::Int32Attribute::get(pir::IrContext::Instance(), paddings[i]);

    vec_paddings.push_back(attr_paddings);
  }
  pir::Attribute attr_paddings = pir::ArrayAttribute::get(pir::IrContext::Instance(), vec_paddings);
  argument.AddAttribute("paddings", attr_paddings);
  pir::Attribute attr_padding_algorithm = pir::StrAttribute::get(pir::IrContext::Instance(), padding_algorithm);
  argument.AddAttribute("padding_algorithm", attr_padding_algorithm);
  pir::Attribute attr_groups = pir::Int32Attribute::get(pir::IrContext::Instance(), groups);
  argument.AddAttribute("groups", attr_groups);
  std::vector<pir::Attribute> vec_dilations;
  for (size_t i = 0; i < static_cast<size_t>(dilations.size()); i++) {
      pir::Attribute attr_dilations = pir::Int32Attribute::get(pir::IrContext::Instance(), dilations[i]);

    vec_dilations.push_back(attr_dilations);
  }
  pir::Attribute attr_dilations = pir::ArrayAttribute::get(pir::IrContext::Instance(), vec_dilations);
  argument.AddAttribute("dilations", attr_dilations);
  pir::Attribute attr_data_format = pir::StrAttribute::get(pir::IrContext::Instance(), data_format);
  argument.AddAttribute("data_format", attr_data_format);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType input = input_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)input;
  paddle::dialect::DenseTensorType filter = filter_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)filter;
  paddle::dialect::DenseTensorType out_grad = out_grad_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)out_grad;

  VLOG(4) << "Builder construction  dense_input";
  paddle::dialect::IrTensor ir_tensor_input(paddle::dialect::TransToPhiDataType(input.dtype()),
                                                      input.dims(),
                                                      input.data_layout(),
                                                      input.lod(),
                                                      input.offset());
  VLOG(4) << "Builder construction  meta_input";
  paddle::dialect::IrMetaTensor meta_input(&ir_tensor_input);

  VLOG(4) << "Builder construction  dense_filter";
  paddle::dialect::IrTensor ir_tensor_filter(paddle::dialect::TransToPhiDataType(filter.dtype()),
                                                      filter.dims(),
                                                      filter.data_layout(),
                                                      filter.lod(),
                                                      filter.offset());
  VLOG(4) << "Builder construction  meta_filter";
  paddle::dialect::IrMetaTensor meta_filter(&ir_tensor_filter);
  paddle::dialect::IrTensor dense_input_grad;
  paddle::dialect::IrMetaTensor meta_input_grad(&dense_input_grad);
  paddle::dialect::IrTensor dense_filter_grad;
  paddle::dialect::IrMetaTensor meta_filter_grad(&dense_filter_grad);

  phi::GeneralBinaryGradInferMeta(meta_input, meta_filter, &meta_input_grad, &meta_filter_grad);

  std::vector<pir::Type> argument_outputs;
  pir::Type input_grad_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_input_grad.dtype()), dense_input_grad.dims(), dense_input_grad.layout(), dense_input_grad.lod(), dense_input_grad.offset());
  argument_outputs.push_back(input_grad_dense_tensor_type);

  pir::Type filter_grad_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_filter_grad.dtype()), dense_filter_grad.dims(), dense_filter_grad.layout(), dense_filter_grad.lod(), dense_filter_grad.offset());
  argument_outputs.push_back(filter_grad_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void Conv3dGradOp::VerifySig() {}

void Conv3dGradOp::InferMeta( phi::InferMetaContext *infer_meta ) {
  auto fn = PD_INFER_META(phi::GeneralBinaryGradInferMeta);
  fn(infer_meta);
}

phi::DataType Conv3dGradOp::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: Conv3dGradOp";
  


  return expected_kernel_dtype;
}

const char *Conv3dTransposeGradOp::attributes_name[8] = { "strides", "paddings", "output_padding", "output_size", "padding_algorithm", "groups", "dilations", "data_format" };

OpInfoTuple Conv3dTransposeGradOp::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("x", "paddle::dialect::DenseTensorType", false, false, false, false), paddle::dialect::OpInputInfo("filter", "paddle::dialect::DenseTensorType", false, false, false, false), paddle::dialect::OpInputInfo("out_grad", "paddle::dialect::DenseTensorType", false, false, false, false) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = { paddle::dialect::OpAttributeInfo("strides", "pir::ArrayAttribute<pir::Int32Attribute>", ""), paddle::dialect::OpAttributeInfo("paddings", "pir::ArrayAttribute<pir::Int32Attribute>", ""), paddle::dialect::OpAttributeInfo("output_padding", "pir::ArrayAttribute<pir::Int32Attribute>", ""), paddle::dialect::OpAttributeInfo("output_size", "pir::ArrayAttribute<pir::Int32Attribute>", ""), paddle::dialect::OpAttributeInfo("padding_algorithm", "pir::StrAttribute", ""), paddle::dialect::OpAttributeInfo("groups", "pir::Int32Attribute", ""), paddle::dialect::OpAttributeInfo("dilations", "pir::ArrayAttribute<pir::Int32Attribute>", ""), paddle::dialect::OpAttributeInfo("data_format", "pir::StrAttribute", "") };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("x_grad", "paddle::dialect::DenseTensorType", false, false), paddle::dialect::OpOutputInfo("filter_grad", "paddle::dialect::DenseTensorType", false, false) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("ConvTransposeGradInferMeta", {"x", "filter", "out_grad", "strides", "paddings", "output_padding", "output_size", "padding_algorithm", "groups", "dilations", "data_format"}, "conv3d_transpose_grad", {"x", "filter", "out_grad", "strides", "paddings", "output_padding", "output_size", "padding_algorithm", "groups", "dilations", "data_format"}, {"x"}, {}, {}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "conv3d_transpose_grad");
}

void Conv3dTransposeGradOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, pir::Value filter_, pir::Value out_grad_, const std::vector<int>& strides, const std::vector<int>& paddings, const std::vector<int>& output_padding, const std::vector<int>& output_size, const std::string& padding_algorithm, int groups, const std::vector<int>& dilations, const std::string& data_format) {
  VLOG(4) << "Start build Conv3dTransposeGradOp";



  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_, filter_, out_grad_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  std::vector<pir::Attribute> vec_strides;
  for (size_t i = 0; i < static_cast<size_t>(strides.size()); i++) {
      pir::Attribute attr_strides = pir::Int32Attribute::get(pir::IrContext::Instance(), strides[i]);

    vec_strides.push_back(attr_strides);
  }
  pir::Attribute attr_strides = pir::ArrayAttribute::get(pir::IrContext::Instance(), vec_strides);
  argument.AddAttribute("strides", attr_strides);
  std::vector<pir::Attribute> vec_paddings;
  for (size_t i = 0; i < static_cast<size_t>(paddings.size()); i++) {
      pir::Attribute attr_paddings = pir::Int32Attribute::get(pir::IrContext::Instance(), paddings[i]);

    vec_paddings.push_back(attr_paddings);
  }
  pir::Attribute attr_paddings = pir::ArrayAttribute::get(pir::IrContext::Instance(), vec_paddings);
  argument.AddAttribute("paddings", attr_paddings);
  std::vector<pir::Attribute> vec_output_padding;
  for (size_t i = 0; i < static_cast<size_t>(output_padding.size()); i++) {
      pir::Attribute attr_output_padding = pir::Int32Attribute::get(pir::IrContext::Instance(), output_padding[i]);

    vec_output_padding.push_back(attr_output_padding);
  }
  pir::Attribute attr_output_padding = pir::ArrayAttribute::get(pir::IrContext::Instance(), vec_output_padding);
  argument.AddAttribute("output_padding", attr_output_padding);
  std::vector<pir::Attribute> vec_output_size;
  for (size_t i = 0; i < static_cast<size_t>(output_size.size()); i++) {
      pir::Attribute attr_output_size = pir::Int32Attribute::get(pir::IrContext::Instance(), output_size[i]);

    vec_output_size.push_back(attr_output_size);
  }
  pir::Attribute attr_output_size = pir::ArrayAttribute::get(pir::IrContext::Instance(), vec_output_size);
  argument.AddAttribute("output_size", attr_output_size);
  pir::Attribute attr_padding_algorithm = pir::StrAttribute::get(pir::IrContext::Instance(), padding_algorithm);
  argument.AddAttribute("padding_algorithm", attr_padding_algorithm);
  pir::Attribute attr_groups = pir::Int32Attribute::get(pir::IrContext::Instance(), groups);
  argument.AddAttribute("groups", attr_groups);
  std::vector<pir::Attribute> vec_dilations;
  for (size_t i = 0; i < static_cast<size_t>(dilations.size()); i++) {
      pir::Attribute attr_dilations = pir::Int32Attribute::get(pir::IrContext::Instance(), dilations[i]);

    vec_dilations.push_back(attr_dilations);
  }
  pir::Attribute attr_dilations = pir::ArrayAttribute::get(pir::IrContext::Instance(), vec_dilations);
  argument.AddAttribute("dilations", attr_dilations);
  pir::Attribute attr_data_format = pir::StrAttribute::get(pir::IrContext::Instance(), data_format);
  argument.AddAttribute("data_format", attr_data_format);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;
  paddle::dialect::DenseTensorType filter = filter_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)filter;
  paddle::dialect::DenseTensorType out_grad = out_grad_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)out_grad;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);

  VLOG(4) << "Builder construction  dense_filter";
  paddle::dialect::IrTensor ir_tensor_filter(paddle::dialect::TransToPhiDataType(filter.dtype()),
                                                      filter.dims(),
                                                      filter.data_layout(),
                                                      filter.lod(),
                                                      filter.offset());
  VLOG(4) << "Builder construction  meta_filter";
  paddle::dialect::IrMetaTensor meta_filter(&ir_tensor_filter);

  VLOG(4) << "Builder construction  dense_out_grad";
  paddle::dialect::IrTensor ir_tensor_out_grad(paddle::dialect::TransToPhiDataType(out_grad.dtype()),
                                                      out_grad.dims(),
                                                      out_grad.data_layout(),
                                                      out_grad.lod(),
                                                      out_grad.offset());
  VLOG(4) << "Builder construction  meta_out_grad";
  paddle::dialect::IrMetaTensor meta_out_grad(&ir_tensor_out_grad);
  paddle::dialect::IrTensor dense_x_grad;
  paddle::dialect::IrMetaTensor meta_x_grad(&dense_x_grad);
  paddle::dialect::IrTensor dense_filter_grad;
  paddle::dialect::IrMetaTensor meta_filter_grad(&dense_filter_grad);

  phi::ConvTransposeGradInferMeta(meta_x, meta_filter, meta_out_grad, strides, paddings, output_padding, output_size, padding_algorithm, groups, dilations, data_format, &meta_x_grad, &meta_filter_grad);

  std::vector<pir::Type> argument_outputs;
  pir::Type x_grad_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_x_grad.dtype()), dense_x_grad.dims(), dense_x_grad.layout(), dense_x_grad.lod(), dense_x_grad.offset());
  argument_outputs.push_back(x_grad_dense_tensor_type);

  pir::Type filter_grad_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_filter_grad.dtype()), dense_filter_grad.dims(), dense_filter_grad.layout(), dense_filter_grad.lod(), dense_filter_grad.offset());
  argument_outputs.push_back(filter_grad_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void Conv3dTransposeGradOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, pir::Value filter_, pir::Value out_grad_, pir::AttributeMap attributes) {
  VLOG(4) << "Start build Conv3dTransposeGradOp";


  IR_ENFORCE(
      attributes.find("strides") != attributes.end(),
          "'strides' Attribute is expected for Conv3dTransposeGradOp. ");
  std::vector<int> strides;
  for (size_t i = 0; i < attributes.at("strides").dyn_cast<pir::ArrayAttribute>().size(); i++) {
    strides.push_back(attributes.at("strides").dyn_cast<pir::ArrayAttribute>().at(i).dyn_cast<pir::Int32Attribute>().data());
  }

  IR_ENFORCE(
      attributes.find("paddings") != attributes.end(),
          "'paddings' Attribute is expected for Conv3dTransposeGradOp. ");
  std::vector<int> paddings;
  for (size_t i = 0; i < attributes.at("paddings").dyn_cast<pir::ArrayAttribute>().size(); i++) {
    paddings.push_back(attributes.at("paddings").dyn_cast<pir::ArrayAttribute>().at(i).dyn_cast<pir::Int32Attribute>().data());
  }

  IR_ENFORCE(
      attributes.find("output_padding") != attributes.end(),
          "'output_padding' Attribute is expected for Conv3dTransposeGradOp. ");
  std::vector<int> output_padding;
  for (size_t i = 0; i < attributes.at("output_padding").dyn_cast<pir::ArrayAttribute>().size(); i++) {
    output_padding.push_back(attributes.at("output_padding").dyn_cast<pir::ArrayAttribute>().at(i).dyn_cast<pir::Int32Attribute>().data());
  }

  IR_ENFORCE(
      attributes.find("output_size") != attributes.end(),
          "'output_size' Attribute is expected for Conv3dTransposeGradOp. ");
  std::vector<int> output_size;
  for (size_t i = 0; i < attributes.at("output_size").dyn_cast<pir::ArrayAttribute>().size(); i++) {
    output_size.push_back(attributes.at("output_size").dyn_cast<pir::ArrayAttribute>().at(i).dyn_cast<pir::Int32Attribute>().data());
  }

  IR_ENFORCE(
      attributes.find("padding_algorithm") != attributes.end(),
          "'padding_algorithm' Attribute is expected for Conv3dTransposeGradOp. ");
  std::string padding_algorithm = attributes.at("padding_algorithm").dyn_cast<pir::StrAttribute>().AsString();

  IR_ENFORCE(
      attributes.find("groups") != attributes.end(),
          "'groups' Attribute is expected for Conv3dTransposeGradOp. ");
  int groups = attributes.at("groups").dyn_cast<pir::Int32Attribute>().data();

  IR_ENFORCE(
      attributes.find("dilations") != attributes.end(),
          "'dilations' Attribute is expected for Conv3dTransposeGradOp. ");
  std::vector<int> dilations;
  for (size_t i = 0; i < attributes.at("dilations").dyn_cast<pir::ArrayAttribute>().size(); i++) {
    dilations.push_back(attributes.at("dilations").dyn_cast<pir::ArrayAttribute>().at(i).dyn_cast<pir::Int32Attribute>().data());
  }

  IR_ENFORCE(
      attributes.find("data_format") != attributes.end(),
          "'data_format' Attribute is expected for Conv3dTransposeGradOp. ");
  std::string data_format = attributes.at("data_format").dyn_cast<pir::StrAttribute>().AsString();


  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_, filter_, out_grad_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  std::vector<pir::Attribute> vec_strides;
  for (size_t i = 0; i < static_cast<size_t>(strides.size()); i++) {
      pir::Attribute attr_strides = pir::Int32Attribute::get(pir::IrContext::Instance(), strides[i]);

    vec_strides.push_back(attr_strides);
  }
  pir::Attribute attr_strides = pir::ArrayAttribute::get(pir::IrContext::Instance(), vec_strides);
  argument.AddAttribute("strides", attr_strides);
  std::vector<pir::Attribute> vec_paddings;
  for (size_t i = 0; i < static_cast<size_t>(paddings.size()); i++) {
      pir::Attribute attr_paddings = pir::Int32Attribute::get(pir::IrContext::Instance(), paddings[i]);

    vec_paddings.push_back(attr_paddings);
  }
  pir::Attribute attr_paddings = pir::ArrayAttribute::get(pir::IrContext::Instance(), vec_paddings);
  argument.AddAttribute("paddings", attr_paddings);
  std::vector<pir::Attribute> vec_output_padding;
  for (size_t i = 0; i < static_cast<size_t>(output_padding.size()); i++) {
      pir::Attribute attr_output_padding = pir::Int32Attribute::get(pir::IrContext::Instance(), output_padding[i]);

    vec_output_padding.push_back(attr_output_padding);
  }
  pir::Attribute attr_output_padding = pir::ArrayAttribute::get(pir::IrContext::Instance(), vec_output_padding);
  argument.AddAttribute("output_padding", attr_output_padding);
  std::vector<pir::Attribute> vec_output_size;
  for (size_t i = 0; i < static_cast<size_t>(output_size.size()); i++) {
      pir::Attribute attr_output_size = pir::Int32Attribute::get(pir::IrContext::Instance(), output_size[i]);

    vec_output_size.push_back(attr_output_size);
  }
  pir::Attribute attr_output_size = pir::ArrayAttribute::get(pir::IrContext::Instance(), vec_output_size);
  argument.AddAttribute("output_size", attr_output_size);
  pir::Attribute attr_padding_algorithm = pir::StrAttribute::get(pir::IrContext::Instance(), padding_algorithm);
  argument.AddAttribute("padding_algorithm", attr_padding_algorithm);
  pir::Attribute attr_groups = pir::Int32Attribute::get(pir::IrContext::Instance(), groups);
  argument.AddAttribute("groups", attr_groups);
  std::vector<pir::Attribute> vec_dilations;
  for (size_t i = 0; i < static_cast<size_t>(dilations.size()); i++) {
      pir::Attribute attr_dilations = pir::Int32Attribute::get(pir::IrContext::Instance(), dilations[i]);

    vec_dilations.push_back(attr_dilations);
  }
  pir::Attribute attr_dilations = pir::ArrayAttribute::get(pir::IrContext::Instance(), vec_dilations);
  argument.AddAttribute("dilations", attr_dilations);
  pir::Attribute attr_data_format = pir::StrAttribute::get(pir::IrContext::Instance(), data_format);
  argument.AddAttribute("data_format", attr_data_format);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;
  paddle::dialect::DenseTensorType filter = filter_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)filter;
  paddle::dialect::DenseTensorType out_grad = out_grad_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)out_grad;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);

  VLOG(4) << "Builder construction  dense_filter";
  paddle::dialect::IrTensor ir_tensor_filter(paddle::dialect::TransToPhiDataType(filter.dtype()),
                                                      filter.dims(),
                                                      filter.data_layout(),
                                                      filter.lod(),
                                                      filter.offset());
  VLOG(4) << "Builder construction  meta_filter";
  paddle::dialect::IrMetaTensor meta_filter(&ir_tensor_filter);

  VLOG(4) << "Builder construction  dense_out_grad";
  paddle::dialect::IrTensor ir_tensor_out_grad(paddle::dialect::TransToPhiDataType(out_grad.dtype()),
                                                      out_grad.dims(),
                                                      out_grad.data_layout(),
                                                      out_grad.lod(),
                                                      out_grad.offset());
  VLOG(4) << "Builder construction  meta_out_grad";
  paddle::dialect::IrMetaTensor meta_out_grad(&ir_tensor_out_grad);
  paddle::dialect::IrTensor dense_x_grad;
  paddle::dialect::IrMetaTensor meta_x_grad(&dense_x_grad);
  paddle::dialect::IrTensor dense_filter_grad;
  paddle::dialect::IrMetaTensor meta_filter_grad(&dense_filter_grad);

  phi::ConvTransposeGradInferMeta(meta_x, meta_filter, meta_out_grad, strides, paddings, output_padding, output_size, padding_algorithm, groups, dilations, data_format, &meta_x_grad, &meta_filter_grad);

  std::vector<pir::Type> argument_outputs;
  pir::Type x_grad_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_x_grad.dtype()), dense_x_grad.dims(), dense_x_grad.layout(), dense_x_grad.lod(), dense_x_grad.offset());
  argument_outputs.push_back(x_grad_dense_tensor_type);

  pir::Type filter_grad_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_filter_grad.dtype()), dense_filter_grad.dims(), dense_filter_grad.layout(), dense_filter_grad.lod(), dense_filter_grad.offset());
  argument_outputs.push_back(filter_grad_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void Conv3dTransposeGradOp::VerifySig() {}

void Conv3dTransposeGradOp::InferMeta( phi::InferMetaContext *infer_meta ) {
  auto fn = PD_INFER_META(phi::ConvTransposeGradInferMeta);
  fn(infer_meta);
}

phi::DataType Conv3dTransposeGradOp::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: Conv3dTransposeGradOp";
  


  return expected_kernel_dtype;
}

OpInfoTuple CosDoubleGradOp::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("x", "paddle::dialect::DenseTensorType", false, false, false, true), paddle::dialect::OpInputInfo("grad_out", "paddle::dialect::DenseTensorType", true, false, false, true), paddle::dialect::OpInputInfo("grad_x_grad", "paddle::dialect::DenseTensorType", false, false, false, true) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = {  };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("x_grad", "paddle::dialect::DenseTensorType", false, false), paddle::dialect::OpOutputInfo("grad_out_grad", "paddle::dialect::DenseTensorType", false, false) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("GeneralBinaryGradInferMeta", {"x", "x"}, "cos_double_grad", {"x", "grad_out", "grad_x_grad"}, {}, {}, {}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "cos_double_grad");
}

void CosDoubleGradOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, pir::Value grad_out_, pir::Value grad_x_grad_) {
  VLOG(4) << "Start build CosDoubleGradOp";



  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_, grad_out_, grad_x_grad_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;
  paddle::dialect::DenseTensorType grad_x_grad = grad_x_grad_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)grad_x_grad;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);
  paddle::dialect::IrTensor dense_x_grad;
  paddle::dialect::IrMetaTensor meta_x_grad(&dense_x_grad);
  paddle::dialect::IrTensor dense_grad_out_grad;
  paddle::dialect::IrMetaTensor meta_grad_out_grad(&dense_grad_out_grad);

  phi::GeneralBinaryGradInferMeta(meta_x, meta_x, &meta_x_grad, &meta_grad_out_grad);

  std::vector<pir::Type> argument_outputs;
  pir::Type x_grad_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_x_grad.dtype()), dense_x_grad.dims(), dense_x_grad.layout(), dense_x_grad.lod(), dense_x_grad.offset());
  argument_outputs.push_back(x_grad_dense_tensor_type);

  pir::Type grad_out_grad_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_grad_out_grad.dtype()), dense_grad_out_grad.dims(), dense_grad_out_grad.layout(), dense_grad_out_grad.lod(), dense_grad_out_grad.offset());
  argument_outputs.push_back(grad_out_grad_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void CosDoubleGradOp::VerifySig() {}

void CosDoubleGradOp::InferMeta( phi::InferMetaContext *infer_meta ) {
  auto fn = PD_INFER_META(phi::GeneralBinaryGradInferMeta);
  fn(infer_meta);
}

phi::DataType CosDoubleGradOp::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: CosDoubleGradOp";
  


  return expected_kernel_dtype;
}

OpInfoTuple CosDoubleGrad_Op::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("x", "paddle::dialect::DenseTensorType", false, false, false, true), paddle::dialect::OpInputInfo("grad_out", "paddle::dialect::DenseTensorType", true, false, false, true), paddle::dialect::OpInputInfo("grad_x_grad", "paddle::dialect::DenseTensorType", false, false, false, true) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = {  };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("x_grad", "paddle::dialect::DenseTensorType", false, false), paddle::dialect::OpOutputInfo("grad_out_grad", "paddle::dialect::DenseTensorType", false, false) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("GeneralBinaryGradInferMeta", {"x", "x"}, "cos_double_grad", {"x", "grad_out", "grad_x_grad"}, {}, {}, {{"grad_out_grad", "grad_x_grad"}}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "cos_double_grad");
}

void CosDoubleGrad_Op::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, pir::Value grad_out_, pir::Value grad_x_grad_) {
  VLOG(4) << "Start build CosDoubleGrad_Op";



  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_, grad_out_, grad_x_grad_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;
  paddle::dialect::DenseTensorType grad_x_grad = grad_x_grad_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)grad_x_grad;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);
  paddle::dialect::IrTensor dense_x_grad;
  paddle::dialect::IrMetaTensor meta_x_grad(&dense_x_grad);
  paddle::dialect::IrTensor dense_grad_out_grad;
  paddle::dialect::IrMetaTensor meta_grad_out_grad(&dense_grad_out_grad);

  phi::GeneralBinaryGradInferMeta(meta_x, meta_x, &meta_x_grad, &meta_grad_out_grad);

  std::vector<pir::Type> argument_outputs;
  pir::Type x_grad_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_x_grad.dtype()), dense_x_grad.dims(), dense_x_grad.layout(), dense_x_grad.lod(), dense_x_grad.offset());
  argument_outputs.push_back(x_grad_dense_tensor_type);

  pir::Type grad_out_grad_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_grad_out_grad.dtype()), dense_grad_out_grad.dims(), dense_grad_out_grad.layout(), dense_grad_out_grad.lod(), dense_grad_out_grad.offset());
  argument_outputs.push_back(grad_out_grad_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void CosDoubleGrad_Op::VerifySig() {}

void CosDoubleGrad_Op::InferMeta( phi::InferMetaContext *infer_meta ) {
  auto fn = PD_INFER_META(phi::GeneralBinaryGradInferMeta);
  fn(infer_meta);
}

phi::DataType CosDoubleGrad_Op::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: CosDoubleGrad_Op";
  


  return expected_kernel_dtype;
}

OpInfoTuple CosGradOp::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("x", "paddle::dialect::DenseTensorType", false, false, false, true), paddle::dialect::OpInputInfo("out_grad", "paddle::dialect::DenseTensorType", false, false, false, true) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = {  };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("x_grad", "paddle::dialect::DenseTensorType", false, false) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("UnchangedInferMeta", {"x"}, "cos_grad", {"x", "out_grad"}, {}, {}, {}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "cos_grad");
}

void CosGradOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, pir::Value out_grad_) {
  VLOG(4) << "Start build CosGradOp";



  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_, out_grad_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;
  paddle::dialect::DenseTensorType out_grad = out_grad_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)out_grad;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);
  paddle::dialect::IrTensor dense_x_grad;
  paddle::dialect::IrMetaTensor meta_x_grad(&dense_x_grad);

  phi::UnchangedInferMeta(meta_x, &meta_x_grad);

  std::vector<pir::Type> argument_outputs;
  pir::Type x_grad_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_x_grad.dtype()), dense_x_grad.dims(), dense_x_grad.layout(), dense_x_grad.lod(), dense_x_grad.offset());
  argument_outputs.push_back(x_grad_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void CosGradOp::VerifySig() {}

void CosGradOp::InferMeta( phi::InferMetaContext *infer_meta ) {
  auto fn = PD_INFER_META(phi::UnchangedInferMeta);
  fn(infer_meta);
}

phi::DataType CosGradOp::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: CosGradOp";
  


  return expected_kernel_dtype;
}

OpInfoTuple CosGrad_Op::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("x", "paddle::dialect::DenseTensorType", false, false, false, true), paddle::dialect::OpInputInfo("out_grad", "paddle::dialect::DenseTensorType", false, false, false, true) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = {  };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("x_grad", "paddle::dialect::DenseTensorType", false, false) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("UnchangedInferMeta", {"x"}, "cos_grad", {"x", "out_grad"}, {}, {}, {{"x_grad", "out_grad"}}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "cos_grad");
}

void CosGrad_Op::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, pir::Value out_grad_) {
  VLOG(4) << "Start build CosGrad_Op";



  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_, out_grad_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;
  paddle::dialect::DenseTensorType out_grad = out_grad_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)out_grad;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);
  paddle::dialect::IrTensor dense_x_grad;
  paddle::dialect::IrMetaTensor meta_x_grad(&dense_x_grad);

  phi::UnchangedInferMeta(meta_x, &meta_x_grad);

  std::vector<pir::Type> argument_outputs;
  pir::Type x_grad_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_x_grad.dtype()), dense_x_grad.dims(), dense_x_grad.layout(), dense_x_grad.lod(), dense_x_grad.offset());
  argument_outputs.push_back(x_grad_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void CosGrad_Op::VerifySig() {}

void CosGrad_Op::InferMeta( phi::InferMetaContext *infer_meta ) {
  auto fn = PD_INFER_META(phi::UnchangedInferMeta);
  fn(infer_meta);
}

phi::DataType CosGrad_Op::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: CosGrad_Op";
  


  return expected_kernel_dtype;
}

OpInfoTuple CosTripleGradOp::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("x", "paddle::dialect::DenseTensorType", false, false, false, false), paddle::dialect::OpInputInfo("grad_out_forward", "paddle::dialect::DenseTensorType", true, false, false, false), paddle::dialect::OpInputInfo("grad_x_grad_forward", "paddle::dialect::DenseTensorType", true, false, false, false), paddle::dialect::OpInputInfo("grad_x_grad", "paddle::dialect::DenseTensorType", false, false, false, false), paddle::dialect::OpInputInfo("grad_out_grad_grad", "paddle::dialect::DenseTensorType", true, false, false, false) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = {  };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("x_grad", "paddle::dialect::DenseTensorType", false, false), paddle::dialect::OpOutputInfo("grad_out_forward_grad", "paddle::dialect::DenseTensorType", false, false), paddle::dialect::OpOutputInfo("grad_x_grad_forward_grad", "paddle::dialect::DenseTensorType", false, false) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("GeneralTernaryGradInferMeta", {"x", "x", "grad_x_grad_forward"}, "cos_triple_grad", {"x", "grad_out_forward", "grad_x_grad_forward", "grad_x_grad", "grad_out_grad_grad"}, {}, {}, {}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "cos_triple_grad");
}

void CosTripleGradOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, pir::Value grad_out_forward_, pir::Value grad_x_grad_forward_, pir::Value grad_x_grad_, pir::Value grad_out_grad_grad_) {
  VLOG(4) << "Start build CosTripleGradOp";



  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_, grad_out_forward_, grad_x_grad_forward_, grad_x_grad_, grad_out_grad_grad_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;
  paddle::dialect::DenseTensorType grad_x_grad = grad_x_grad_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)grad_x_grad;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);

  paddle::dialect::IrMetaTensor meta_grad_x_grad_forward;
  paddle::dialect::IrTensor ir_tensor_grad_x_grad_forward;
  if (grad_x_grad_forward_.impl() != nullptr) {
    paddle::dialect::DenseTensorType grad_x_grad_forward = grad_x_grad_forward_.type().dyn_cast<paddle::dialect::DenseTensorType>();
    VLOG(4) << "Builder construction  dense_grad_x_grad_forward";
    ir_tensor_grad_x_grad_forward = paddle::dialect::IrTensor(paddle::dialect::TransToPhiDataType(grad_x_grad_forward.dtype()),
                                                        grad_x_grad_forward.dims(),
                                                        grad_x_grad_forward.data_layout(),
                                                        grad_x_grad_forward.lod(),
                                                        grad_x_grad_forward.offset());
    VLOG(4) << "Builder construction  meta_grad_x_grad_forward";
    meta_grad_x_grad_forward = paddle::dialect::IrMetaTensor(&ir_tensor_grad_x_grad_forward);
  }

  paddle::dialect::IrTensor dense_x_grad;
  paddle::dialect::IrMetaTensor meta_x_grad(&dense_x_grad);
  paddle::dialect::IrTensor dense_grad_out_forward_grad;
  paddle::dialect::IrMetaTensor meta_grad_out_forward_grad(&dense_grad_out_forward_grad);
  paddle::dialect::IrTensor dense_grad_x_grad_forward_grad;
  paddle::dialect::IrMetaTensor meta_grad_x_grad_forward_grad(&dense_grad_x_grad_forward_grad);

  phi::GeneralTernaryGradInferMeta(meta_x, meta_x, meta_grad_x_grad_forward, &meta_x_grad, &meta_grad_out_forward_grad, &meta_grad_x_grad_forward_grad);

  std::vector<pir::Type> argument_outputs;
  pir::Type x_grad_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_x_grad.dtype()), dense_x_grad.dims(), dense_x_grad.layout(), dense_x_grad.lod(), dense_x_grad.offset());
  argument_outputs.push_back(x_grad_dense_tensor_type);

  pir::Type grad_out_forward_grad_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_grad_out_forward_grad.dtype()), dense_grad_out_forward_grad.dims(), dense_grad_out_forward_grad.layout(), dense_grad_out_forward_grad.lod(), dense_grad_out_forward_grad.offset());
  argument_outputs.push_back(grad_out_forward_grad_dense_tensor_type);

  pir::Type grad_x_grad_forward_grad_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_grad_x_grad_forward_grad.dtype()), dense_grad_x_grad_forward_grad.dims(), dense_grad_x_grad_forward_grad.layout(), dense_grad_x_grad_forward_grad.lod(), dense_grad_x_grad_forward_grad.offset());
  argument_outputs.push_back(grad_x_grad_forward_grad_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void CosTripleGradOp::VerifySig() {}

void CosTripleGradOp::InferMeta( phi::InferMetaContext *infer_meta ) {
  auto fn = PD_INFER_META(phi::GeneralTernaryGradInferMeta);
  fn(infer_meta);
}

phi::DataType CosTripleGradOp::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: CosTripleGradOp";
  


  return expected_kernel_dtype;
}

OpInfoTuple CosTripleGrad_Op::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("x", "paddle::dialect::DenseTensorType", false, false, false, false), paddle::dialect::OpInputInfo("grad_out_forward", "paddle::dialect::DenseTensorType", true, false, false, false), paddle::dialect::OpInputInfo("grad_x_grad_forward", "paddle::dialect::DenseTensorType", true, false, false, false), paddle::dialect::OpInputInfo("grad_x_grad", "paddle::dialect::DenseTensorType", false, false, false, false), paddle::dialect::OpInputInfo("grad_out_grad_grad", "paddle::dialect::DenseTensorType", true, false, false, false) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = {  };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("x_grad", "paddle::dialect::DenseTensorType", false, false), paddle::dialect::OpOutputInfo("grad_out_forward_grad", "paddle::dialect::DenseTensorType", false, false), paddle::dialect::OpOutputInfo("grad_x_grad_forward_grad", "paddle::dialect::DenseTensorType", false, false) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("GeneralTernaryGradInferMeta", {"x", "x", "grad_x_grad_forward"}, "cos_triple_grad", {"x", "grad_out_forward", "grad_x_grad_forward", "grad_x_grad", "grad_out_grad_grad"}, {}, {}, {{"grad_out_forward_grad", "grad_x_grad_forward"}}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "cos_triple_grad");
}

void CosTripleGrad_Op::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, pir::Value grad_out_forward_, pir::Value grad_x_grad_forward_, pir::Value grad_x_grad_, pir::Value grad_out_grad_grad_) {
  VLOG(4) << "Start build CosTripleGrad_Op";



  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_, grad_out_forward_, grad_x_grad_forward_, grad_x_grad_, grad_out_grad_grad_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;
  paddle::dialect::DenseTensorType grad_x_grad = grad_x_grad_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)grad_x_grad;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);

  paddle::dialect::IrMetaTensor meta_grad_x_grad_forward;
  paddle::dialect::IrTensor ir_tensor_grad_x_grad_forward;
  if (grad_x_grad_forward_.impl() != nullptr) {
    paddle::dialect::DenseTensorType grad_x_grad_forward = grad_x_grad_forward_.type().dyn_cast<paddle::dialect::DenseTensorType>();
    VLOG(4) << "Builder construction  dense_grad_x_grad_forward";
    ir_tensor_grad_x_grad_forward = paddle::dialect::IrTensor(paddle::dialect::TransToPhiDataType(grad_x_grad_forward.dtype()),
                                                        grad_x_grad_forward.dims(),
                                                        grad_x_grad_forward.data_layout(),
                                                        grad_x_grad_forward.lod(),
                                                        grad_x_grad_forward.offset());
    VLOG(4) << "Builder construction  meta_grad_x_grad_forward";
    meta_grad_x_grad_forward = paddle::dialect::IrMetaTensor(&ir_tensor_grad_x_grad_forward);
  }

  paddle::dialect::IrTensor dense_x_grad;
  paddle::dialect::IrMetaTensor meta_x_grad(&dense_x_grad);
  paddle::dialect::IrTensor dense_grad_out_forward_grad;
  paddle::dialect::IrMetaTensor meta_grad_out_forward_grad(&dense_grad_out_forward_grad);
  paddle::dialect::IrTensor dense_grad_x_grad_forward_grad;
  paddle::dialect::IrMetaTensor meta_grad_x_grad_forward_grad(&dense_grad_x_grad_forward_grad);

  phi::GeneralTernaryGradInferMeta(meta_x, meta_x, meta_grad_x_grad_forward, &meta_x_grad, &meta_grad_out_forward_grad, &meta_grad_x_grad_forward_grad);

  std::vector<pir::Type> argument_outputs;
  pir::Type x_grad_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_x_grad.dtype()), dense_x_grad.dims(), dense_x_grad.layout(), dense_x_grad.lod(), dense_x_grad.offset());
  argument_outputs.push_back(x_grad_dense_tensor_type);

  pir::Type grad_out_forward_grad_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_grad_out_forward_grad.dtype()), dense_grad_out_forward_grad.dims(), dense_grad_out_forward_grad.layout(), dense_grad_out_forward_grad.lod(), dense_grad_out_forward_grad.offset());
  argument_outputs.push_back(grad_out_forward_grad_dense_tensor_type);

  pir::Type grad_x_grad_forward_grad_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_grad_x_grad_forward_grad.dtype()), dense_grad_x_grad_forward_grad.dims(), dense_grad_x_grad_forward_grad.layout(), dense_grad_x_grad_forward_grad.lod(), dense_grad_x_grad_forward_grad.offset());
  argument_outputs.push_back(grad_x_grad_forward_grad_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void CosTripleGrad_Op::VerifySig() {}

void CosTripleGrad_Op::InferMeta( phi::InferMetaContext *infer_meta ) {
  auto fn = PD_INFER_META(phi::GeneralTernaryGradInferMeta);
  fn(infer_meta);
}

phi::DataType CosTripleGrad_Op::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: CosTripleGrad_Op";
  


  return expected_kernel_dtype;
}

OpInfoTuple CoshGradOp::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("x", "paddle::dialect::DenseTensorType", false, false, false, false), paddle::dialect::OpInputInfo("out_grad", "paddle::dialect::DenseTensorType", false, false, false, false) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = {  };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("x_grad", "paddle::dialect::DenseTensorType", false, false) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("UnchangedInferMeta", {"x"}, "cosh_grad", {"x", "out_grad"}, {}, {}, {}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "cosh_grad");
}

void CoshGradOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, pir::Value out_grad_) {
  VLOG(4) << "Start build CoshGradOp";



  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_, out_grad_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;
  paddle::dialect::DenseTensorType out_grad = out_grad_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)out_grad;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);
  paddle::dialect::IrTensor dense_x_grad;
  paddle::dialect::IrMetaTensor meta_x_grad(&dense_x_grad);

  phi::UnchangedInferMeta(meta_x, &meta_x_grad);

  std::vector<pir::Type> argument_outputs;
  pir::Type x_grad_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_x_grad.dtype()), dense_x_grad.dims(), dense_x_grad.layout(), dense_x_grad.lod(), dense_x_grad.offset());
  argument_outputs.push_back(x_grad_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void CoshGradOp::VerifySig() {}

void CoshGradOp::InferMeta( phi::InferMetaContext *infer_meta ) {
  auto fn = PD_INFER_META(phi::UnchangedInferMeta);
  fn(infer_meta);
}

phi::DataType CoshGradOp::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: CoshGradOp";
  


  return expected_kernel_dtype;
}

OpInfoTuple CoshGrad_Op::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("x", "paddle::dialect::DenseTensorType", false, false, false, false), paddle::dialect::OpInputInfo("out_grad", "paddle::dialect::DenseTensorType", false, false, false, false) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = {  };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("x_grad", "paddle::dialect::DenseTensorType", false, false) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("UnchangedInferMeta", {"x"}, "cosh_grad", {"x", "out_grad"}, {}, {}, {{"x_grad", "out_grad"}}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "cosh_grad");
}

void CoshGrad_Op::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, pir::Value out_grad_) {
  VLOG(4) << "Start build CoshGrad_Op";



  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_, out_grad_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;
  paddle::dialect::DenseTensorType out_grad = out_grad_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)out_grad;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);
  paddle::dialect::IrTensor dense_x_grad;
  paddle::dialect::IrMetaTensor meta_x_grad(&dense_x_grad);

  phi::UnchangedInferMeta(meta_x, &meta_x_grad);

  std::vector<pir::Type> argument_outputs;
  pir::Type x_grad_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_x_grad.dtype()), dense_x_grad.dims(), dense_x_grad.layout(), dense_x_grad.lod(), dense_x_grad.offset());
  argument_outputs.push_back(x_grad_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void CoshGrad_Op::VerifySig() {}

void CoshGrad_Op::InferMeta( phi::InferMetaContext *infer_meta ) {
  auto fn = PD_INFER_META(phi::UnchangedInferMeta);
  fn(infer_meta);
}

phi::DataType CoshGrad_Op::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: CoshGrad_Op";
  


  return expected_kernel_dtype;
}

OpInfoTuple CropGradOp::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("x", "paddle::dialect::DenseTensorType", false, false, false, false), paddle::dialect::OpInputInfo("out_grad", "paddle::dialect::DenseTensorType", false, false, false, false), paddle::dialect::OpInputInfo("offsets", "paddle::dialect::IntArrayAttribute", false, false, true, false) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = {  };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("x_grad", "paddle::dialect::DenseTensorType", false, false) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("CropGradInferMeta", {"x", "out_grad", "offsets"}, "crop_grad", {"x", "out_grad", "offsets"}, {"x"}, {}, {}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "crop_grad");
}

void CropGradOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, pir::Value out_grad_, const std::vector<int64_t>& offsets) {
  VLOG(4) << "Start build CropGradOp";


  // Generate int_array mutable attribute: offsets
  paddle::dialect::FullIntArrayOp full_offsets_op = builder.Build<paddle::dialect::FullIntArrayOp>(offsets, phi::DataType::INT64, phi::CPUPlace());
  pir::OpResult offsets_ = full_offsets_op->result(0);
    
  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_, out_grad_, offsets_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;
  paddle::dialect::DenseTensorType out_grad = out_grad_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)out_grad;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);

  VLOG(4) << "Builder construction  dense_out_grad";
  paddle::dialect::IrTensor ir_tensor_out_grad(paddle::dialect::TransToPhiDataType(out_grad.dtype()),
                                                      out_grad.dims(),
                                                      out_grad.data_layout(),
                                                      out_grad.lod(),
                                                      out_grad.offset());
  VLOG(4) << "Builder construction  meta_out_grad";
  paddle::dialect::IrMetaTensor meta_out_grad(&ir_tensor_out_grad);
  paddle::dialect::IrTensor dense_x_grad;
  paddle::dialect::IrMetaTensor meta_x_grad(&dense_x_grad);

  phi::CropGradInferMeta(meta_x, meta_out_grad, offsets, &meta_x_grad);

  std::vector<pir::Type> argument_outputs;
  pir::Type x_grad_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_x_grad.dtype()), dense_x_grad.dims(), dense_x_grad.layout(), dense_x_grad.lod(), dense_x_grad.offset());
  argument_outputs.push_back(x_grad_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void CropGradOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, pir::Value out_grad_, pir::AttributeMap attributes) {
  VLOG(4) << "Start build CropGradOp";


  IR_ENFORCE(
      attributes.find("offsets") != attributes.end(),
          "'offsets' Attribute is expected for CropGradOp. ");
  std::vector<int64_t> offsets = attributes.at("offsets").dyn_cast<paddle::dialect::IntArrayAttribute>().data().GetData();

  // Generate int_array mutable attribute: offsets
  paddle::dialect::FullIntArrayOp full_offsets_op = builder.Build<paddle::dialect::FullIntArrayOp>(offsets, phi::DataType::INT64, phi::CPUPlace());
  pir::OpResult offsets_ = full_offsets_op->result(0);
    
  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_, out_grad_, offsets_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;
  paddle::dialect::DenseTensorType out_grad = out_grad_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)out_grad;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);

  VLOG(4) << "Builder construction  dense_out_grad";
  paddle::dialect::IrTensor ir_tensor_out_grad(paddle::dialect::TransToPhiDataType(out_grad.dtype()),
                                                      out_grad.dims(),
                                                      out_grad.data_layout(),
                                                      out_grad.lod(),
                                                      out_grad.offset());
  VLOG(4) << "Builder construction  meta_out_grad";
  paddle::dialect::IrMetaTensor meta_out_grad(&ir_tensor_out_grad);
  paddle::dialect::IrTensor dense_x_grad;
  paddle::dialect::IrMetaTensor meta_x_grad(&dense_x_grad);

  phi::CropGradInferMeta(meta_x, meta_out_grad, offsets, &meta_x_grad);

  std::vector<pir::Type> argument_outputs;
  pir::Type x_grad_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_x_grad.dtype()), dense_x_grad.dims(), dense_x_grad.layout(), dense_x_grad.lod(), dense_x_grad.offset());
  argument_outputs.push_back(x_grad_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void CropGradOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, pir::Value out_grad_, pir::Value offsets_) {
  VLOG(4) << "Start build CropGradOp";



  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_, out_grad_, offsets_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;
  paddle::dialect::DenseTensorType out_grad = out_grad_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)out_grad;
  phi::IntArray offsets;
  if (offsets_.dyn_cast<pir::OpResult>() && offsets_.dyn_cast<pir::OpResult>().owner()->isa<paddle::dialect::FullIntArrayOp>()) {
    offsets = std::move(phi::IntArray(paddle::dialect::GetInt64Vector(
                          offsets_.dyn_cast<pir::OpResult>().owner()
                          ->dyn_cast<paddle::dialect::FullIntArrayOp>()
                          .attribute("value"))));
  } else if (offsets_.type().isa<pir::VectorType>()) {
    size_t offsets_size = offsets_.type().dyn_cast<pir::VectorType>().size();
    offsets = std::move(phi::IntArray(std::vector<int64_t>(offsets_size, -1)));
    offsets.SetFromTensor(true);
  } else if (offsets_.type().isa<paddle::dialect::DenseTensorType>()) {
    common::DDim offsets_dim = offsets_.type().dyn_cast<paddle::dialect::DenseTensorType>().dims();
    size_t offsets_size = common::product(offsets_dim);
    if (common::contain_unknown_dim(offsets_dim)) {
      offsets_size = 1;
    }
    offsets = std::move(phi::IntArray(std::vector<int64_t>(offsets_size, -1)));
    offsets.SetFromTensor(true);
  } else {
    PADDLE_THROW(phi::errors::Unimplemented("Only support VectorType or DenseTensorType"));
  }


  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);

  VLOG(4) << "Builder construction  dense_out_grad";
  paddle::dialect::IrTensor ir_tensor_out_grad(paddle::dialect::TransToPhiDataType(out_grad.dtype()),
                                                      out_grad.dims(),
                                                      out_grad.data_layout(),
                                                      out_grad.lod(),
                                                      out_grad.offset());
  VLOG(4) << "Builder construction  meta_out_grad";
  paddle::dialect::IrMetaTensor meta_out_grad(&ir_tensor_out_grad);
  paddle::dialect::IrTensor dense_x_grad;
  paddle::dialect::IrMetaTensor meta_x_grad(&dense_x_grad);

  phi::CropGradInferMeta(meta_x, meta_out_grad, offsets, &meta_x_grad);

  std::vector<pir::Type> argument_outputs;
  pir::Type x_grad_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_x_grad.dtype()), dense_x_grad.dims(), dense_x_grad.layout(), dense_x_grad.lod(), dense_x_grad.offset());
  argument_outputs.push_back(x_grad_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void CropGradOp::VerifySig() {}

void CropGradOp::InferMeta( phi::InferMetaContext *infer_meta ) {
  auto fn = PD_INFER_META(phi::CropGradInferMeta);
  fn(infer_meta);
}

phi::DataType CropGradOp::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: CropGradOp";
  


  return expected_kernel_dtype;
}

const char *CrossEntropyWithSoftmaxGradOp::attributes_name[5] = { "soft_label", "use_softmax", "numeric_stable_mode", "ignore_index", "axis" };

OpInfoTuple CrossEntropyWithSoftmaxGradOp::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("label", "paddle::dialect::DenseTensorType", false, false, false, false), paddle::dialect::OpInputInfo("softmax", "paddle::dialect::DenseTensorType", false, false, false, false), paddle::dialect::OpInputInfo("loss_grad", "paddle::dialect::DenseTensorType", false, false, false, false) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = { paddle::dialect::OpAttributeInfo("soft_label", "pir::BoolAttribute", ""), paddle::dialect::OpAttributeInfo("use_softmax", "pir::BoolAttribute", ""), paddle::dialect::OpAttributeInfo("numeric_stable_mode", "pir::BoolAttribute", ""), paddle::dialect::OpAttributeInfo("ignore_index", "pir::Int32Attribute", ""), paddle::dialect::OpAttributeInfo("axis", "pir::Int32Attribute", "") };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("input_grad", "paddle::dialect::DenseTensorType", false, false) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("CrossEntropyWithSoftmaxGradInferMeta", {"label", "softmax", "loss_grad", "soft_label", "use_softmax", "numeric_stable_mode", "ignore_index", "axis"}, "cross_entropy_with_softmax_grad", {"label", "softmax", "loss_grad", "soft_label", "use_softmax", "numeric_stable_mode", "ignore_index", "axis"}, {"loss_grad"}, {}, {}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "cross_entropy_with_softmax_grad");
}

void CrossEntropyWithSoftmaxGradOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value label_, pir::Value softmax_, pir::Value loss_grad_, bool soft_label, bool use_softmax, bool numeric_stable_mode, int ignore_index, int axis) {
  VLOG(4) << "Start build CrossEntropyWithSoftmaxGradOp";



  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {label_, softmax_, loss_grad_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_soft_label = pir::BoolAttribute::get(pir::IrContext::Instance(), soft_label);
  argument.AddAttribute("soft_label", attr_soft_label);
  pir::Attribute attr_use_softmax = pir::BoolAttribute::get(pir::IrContext::Instance(), use_softmax);
  argument.AddAttribute("use_softmax", attr_use_softmax);
  pir::Attribute attr_numeric_stable_mode = pir::BoolAttribute::get(pir::IrContext::Instance(), numeric_stable_mode);
  argument.AddAttribute("numeric_stable_mode", attr_numeric_stable_mode);
  pir::Attribute attr_ignore_index = pir::Int32Attribute::get(pir::IrContext::Instance(), ignore_index);
  argument.AddAttribute("ignore_index", attr_ignore_index);
  pir::Attribute attr_axis = pir::Int32Attribute::get(pir::IrContext::Instance(), axis);
  argument.AddAttribute("axis", attr_axis);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType label = label_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)label;
  paddle::dialect::DenseTensorType softmax = softmax_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)softmax;
  paddle::dialect::DenseTensorType loss_grad = loss_grad_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)loss_grad;

  VLOG(4) << "Builder construction  dense_label";
  paddle::dialect::IrTensor ir_tensor_label(paddle::dialect::TransToPhiDataType(label.dtype()),
                                                      label.dims(),
                                                      label.data_layout(),
                                                      label.lod(),
                                                      label.offset());
  VLOG(4) << "Builder construction  meta_label";
  paddle::dialect::IrMetaTensor meta_label(&ir_tensor_label);

  VLOG(4) << "Builder construction  dense_softmax";
  paddle::dialect::IrTensor ir_tensor_softmax(paddle::dialect::TransToPhiDataType(softmax.dtype()),
                                                      softmax.dims(),
                                                      softmax.data_layout(),
                                                      softmax.lod(),
                                                      softmax.offset());
  VLOG(4) << "Builder construction  meta_softmax";
  paddle::dialect::IrMetaTensor meta_softmax(&ir_tensor_softmax);

  VLOG(4) << "Builder construction  dense_loss_grad";
  paddle::dialect::IrTensor ir_tensor_loss_grad(paddle::dialect::TransToPhiDataType(loss_grad.dtype()),
                                                      loss_grad.dims(),
                                                      loss_grad.data_layout(),
                                                      loss_grad.lod(),
                                                      loss_grad.offset());
  VLOG(4) << "Builder construction  meta_loss_grad";
  paddle::dialect::IrMetaTensor meta_loss_grad(&ir_tensor_loss_grad);
  paddle::dialect::IrTensor dense_input_grad;
  paddle::dialect::IrMetaTensor meta_input_grad(&dense_input_grad);

  phi::CrossEntropyWithSoftmaxGradInferMeta(meta_label, meta_softmax, meta_loss_grad, soft_label, use_softmax, numeric_stable_mode, ignore_index, axis, &meta_input_grad);

  std::vector<pir::Type> argument_outputs;
  pir::Type input_grad_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_input_grad.dtype()), dense_input_grad.dims(), dense_input_grad.layout(), dense_input_grad.lod(), dense_input_grad.offset());
  argument_outputs.push_back(input_grad_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void CrossEntropyWithSoftmaxGradOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value label_, pir::Value softmax_, pir::Value loss_grad_, pir::AttributeMap attributes) {
  VLOG(4) << "Start build CrossEntropyWithSoftmaxGradOp";


  IR_ENFORCE(
      attributes.find("soft_label") != attributes.end(),
          "'soft_label' Attribute is expected for CrossEntropyWithSoftmaxGradOp. ");
  bool soft_label = attributes.at("soft_label").dyn_cast<pir::BoolAttribute>().data();

  IR_ENFORCE(
      attributes.find("use_softmax") != attributes.end(),
          "'use_softmax' Attribute is expected for CrossEntropyWithSoftmaxGradOp. ");
  bool use_softmax = attributes.at("use_softmax").dyn_cast<pir::BoolAttribute>().data();

  IR_ENFORCE(
      attributes.find("numeric_stable_mode") != attributes.end(),
          "'numeric_stable_mode' Attribute is expected for CrossEntropyWithSoftmaxGradOp. ");
  bool numeric_stable_mode = attributes.at("numeric_stable_mode").dyn_cast<pir::BoolAttribute>().data();

  IR_ENFORCE(
      attributes.find("ignore_index") != attributes.end(),
          "'ignore_index' Attribute is expected for CrossEntropyWithSoftmaxGradOp. ");
  int ignore_index = attributes.at("ignore_index").dyn_cast<pir::Int32Attribute>().data();

  IR_ENFORCE(
      attributes.find("axis") != attributes.end(),
          "'axis' Attribute is expected for CrossEntropyWithSoftmaxGradOp. ");
  int axis = attributes.at("axis").dyn_cast<pir::Int32Attribute>().data();


  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {label_, softmax_, loss_grad_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_soft_label = pir::BoolAttribute::get(pir::IrContext::Instance(), soft_label);
  argument.AddAttribute("soft_label", attr_soft_label);
  pir::Attribute attr_use_softmax = pir::BoolAttribute::get(pir::IrContext::Instance(), use_softmax);
  argument.AddAttribute("use_softmax", attr_use_softmax);
  pir::Attribute attr_numeric_stable_mode = pir::BoolAttribute::get(pir::IrContext::Instance(), numeric_stable_mode);
  argument.AddAttribute("numeric_stable_mode", attr_numeric_stable_mode);
  pir::Attribute attr_ignore_index = pir::Int32Attribute::get(pir::IrContext::Instance(), ignore_index);
  argument.AddAttribute("ignore_index", attr_ignore_index);
  pir::Attribute attr_axis = pir::Int32Attribute::get(pir::IrContext::Instance(), axis);
  argument.AddAttribute("axis", attr_axis);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType label = label_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)label;
  paddle::dialect::DenseTensorType softmax = softmax_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)softmax;
  paddle::dialect::DenseTensorType loss_grad = loss_grad_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)loss_grad;

  VLOG(4) << "Builder construction  dense_label";
  paddle::dialect::IrTensor ir_tensor_label(paddle::dialect::TransToPhiDataType(label.dtype()),
                                                      label.dims(),
                                                      label.data_layout(),
                                                      label.lod(),
                                                      label.offset());
  VLOG(4) << "Builder construction  meta_label";
  paddle::dialect::IrMetaTensor meta_label(&ir_tensor_label);

  VLOG(4) << "Builder construction  dense_softmax";
  paddle::dialect::IrTensor ir_tensor_softmax(paddle::dialect::TransToPhiDataType(softmax.dtype()),
                                                      softmax.dims(),
                                                      softmax.data_layout(),
                                                      softmax.lod(),
                                                      softmax.offset());
  VLOG(4) << "Builder construction  meta_softmax";
  paddle::dialect::IrMetaTensor meta_softmax(&ir_tensor_softmax);

  VLOG(4) << "Builder construction  dense_loss_grad";
  paddle::dialect::IrTensor ir_tensor_loss_grad(paddle::dialect::TransToPhiDataType(loss_grad.dtype()),
                                                      loss_grad.dims(),
                                                      loss_grad.data_layout(),
                                                      loss_grad.lod(),
                                                      loss_grad.offset());
  VLOG(4) << "Builder construction  meta_loss_grad";
  paddle::dialect::IrMetaTensor meta_loss_grad(&ir_tensor_loss_grad);
  paddle::dialect::IrTensor dense_input_grad;
  paddle::dialect::IrMetaTensor meta_input_grad(&dense_input_grad);

  phi::CrossEntropyWithSoftmaxGradInferMeta(meta_label, meta_softmax, meta_loss_grad, soft_label, use_softmax, numeric_stable_mode, ignore_index, axis, &meta_input_grad);

  std::vector<pir::Type> argument_outputs;
  pir::Type input_grad_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_input_grad.dtype()), dense_input_grad.dims(), dense_input_grad.layout(), dense_input_grad.lod(), dense_input_grad.offset());
  argument_outputs.push_back(input_grad_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void CrossEntropyWithSoftmaxGradOp::VerifySig() {}

void CrossEntropyWithSoftmaxGradOp::InferMeta( phi::InferMetaContext *infer_meta ) {
  auto fn = PD_INFER_META(phi::CrossEntropyWithSoftmaxGradInferMeta);
  fn(infer_meta);
}

phi::DataType CrossEntropyWithSoftmaxGradOp::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: CrossEntropyWithSoftmaxGradOp";
  


  return expected_kernel_dtype;
}

const char *CrossEntropyWithSoftmaxGrad_Op::attributes_name[5] = { "soft_label", "use_softmax", "numeric_stable_mode", "ignore_index", "axis" };

OpInfoTuple CrossEntropyWithSoftmaxGrad_Op::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("label", "paddle::dialect::DenseTensorType", false, false, false, false), paddle::dialect::OpInputInfo("softmax", "paddle::dialect::DenseTensorType", false, false, false, false), paddle::dialect::OpInputInfo("loss_grad", "paddle::dialect::DenseTensorType", false, false, false, false) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = { paddle::dialect::OpAttributeInfo("soft_label", "pir::BoolAttribute", ""), paddle::dialect::OpAttributeInfo("use_softmax", "pir::BoolAttribute", ""), paddle::dialect::OpAttributeInfo("numeric_stable_mode", "pir::BoolAttribute", ""), paddle::dialect::OpAttributeInfo("ignore_index", "pir::Int32Attribute", ""), paddle::dialect::OpAttributeInfo("axis", "pir::Int32Attribute", "") };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("input_grad", "paddle::dialect::DenseTensorType", false, false) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("CrossEntropyWithSoftmaxGradInferMeta", {"label", "softmax", "loss_grad", "soft_label", "use_softmax", "numeric_stable_mode", "ignore_index", "axis"}, "cross_entropy_with_softmax_grad", {"label", "softmax", "loss_grad", "soft_label", "use_softmax", "numeric_stable_mode", "ignore_index", "axis"}, {"loss_grad"}, {}, {{"input_grad", "softmax"}}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "cross_entropy_with_softmax_grad");
}

void CrossEntropyWithSoftmaxGrad_Op::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value label_, pir::Value softmax_, pir::Value loss_grad_, bool soft_label, bool use_softmax, bool numeric_stable_mode, int ignore_index, int axis) {
  VLOG(4) << "Start build CrossEntropyWithSoftmaxGrad_Op";



  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {label_, softmax_, loss_grad_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_soft_label = pir::BoolAttribute::get(pir::IrContext::Instance(), soft_label);
  argument.AddAttribute("soft_label", attr_soft_label);
  pir::Attribute attr_use_softmax = pir::BoolAttribute::get(pir::IrContext::Instance(), use_softmax);
  argument.AddAttribute("use_softmax", attr_use_softmax);
  pir::Attribute attr_numeric_stable_mode = pir::BoolAttribute::get(pir::IrContext::Instance(), numeric_stable_mode);
  argument.AddAttribute("numeric_stable_mode", attr_numeric_stable_mode);
  pir::Attribute attr_ignore_index = pir::Int32Attribute::get(pir::IrContext::Instance(), ignore_index);
  argument.AddAttribute("ignore_index", attr_ignore_index);
  pir::Attribute attr_axis = pir::Int32Attribute::get(pir::IrContext::Instance(), axis);
  argument.AddAttribute("axis", attr_axis);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType label = label_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)label;
  paddle::dialect::DenseTensorType softmax = softmax_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)softmax;
  paddle::dialect::DenseTensorType loss_grad = loss_grad_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)loss_grad;

  VLOG(4) << "Builder construction  dense_label";
  paddle::dialect::IrTensor ir_tensor_label(paddle::dialect::TransToPhiDataType(label.dtype()),
                                                      label.dims(),
                                                      label.data_layout(),
                                                      label.lod(),
                                                      label.offset());
  VLOG(4) << "Builder construction  meta_label";
  paddle::dialect::IrMetaTensor meta_label(&ir_tensor_label);

  VLOG(4) << "Builder construction  dense_softmax";
  paddle::dialect::IrTensor ir_tensor_softmax(paddle::dialect::TransToPhiDataType(softmax.dtype()),
                                                      softmax.dims(),
                                                      softmax.data_layout(),
                                                      softmax.lod(),
                                                      softmax.offset());
  VLOG(4) << "Builder construction  meta_softmax";
  paddle::dialect::IrMetaTensor meta_softmax(&ir_tensor_softmax);

  VLOG(4) << "Builder construction  dense_loss_grad";
  paddle::dialect::IrTensor ir_tensor_loss_grad(paddle::dialect::TransToPhiDataType(loss_grad.dtype()),
                                                      loss_grad.dims(),
                                                      loss_grad.data_layout(),
                                                      loss_grad.lod(),
                                                      loss_grad.offset());
  VLOG(4) << "Builder construction  meta_loss_grad";
  paddle::dialect::IrMetaTensor meta_loss_grad(&ir_tensor_loss_grad);
  paddle::dialect::IrTensor dense_input_grad;
  paddle::dialect::IrMetaTensor meta_input_grad(&dense_input_grad);

  phi::CrossEntropyWithSoftmaxGradInferMeta(meta_label, meta_softmax, meta_loss_grad, soft_label, use_softmax, numeric_stable_mode, ignore_index, axis, &meta_input_grad);

  std::vector<pir::Type> argument_outputs;
  pir::Type input_grad_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_input_grad.dtype()), dense_input_grad.dims(), dense_input_grad.layout(), dense_input_grad.lod(), dense_input_grad.offset());
  argument_outputs.push_back(input_grad_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void CrossEntropyWithSoftmaxGrad_Op::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value label_, pir::Value softmax_, pir::Value loss_grad_, pir::AttributeMap attributes) {
  VLOG(4) << "Start build CrossEntropyWithSoftmaxGrad_Op";


  IR_ENFORCE(
      attributes.find("soft_label") != attributes.end(),
          "'soft_label' Attribute is expected for CrossEntropyWithSoftmaxGrad_Op. ");
  bool soft_label = attributes.at("soft_label").dyn_cast<pir::BoolAttribute>().data();

  IR_ENFORCE(
      attributes.find("use_softmax") != attributes.end(),
          "'use_softmax' Attribute is expected for CrossEntropyWithSoftmaxGrad_Op. ");
  bool use_softmax = attributes.at("use_softmax").dyn_cast<pir::BoolAttribute>().data();

  IR_ENFORCE(
      attributes.find("numeric_stable_mode") != attributes.end(),
          "'numeric_stable_mode' Attribute is expected for CrossEntropyWithSoftmaxGrad_Op. ");
  bool numeric_stable_mode = attributes.at("numeric_stable_mode").dyn_cast<pir::BoolAttribute>().data();

  IR_ENFORCE(
      attributes.find("ignore_index") != attributes.end(),
          "'ignore_index' Attribute is expected for CrossEntropyWithSoftmaxGrad_Op. ");
  int ignore_index = attributes.at("ignore_index").dyn_cast<pir::Int32Attribute>().data();

  IR_ENFORCE(
      attributes.find("axis") != attributes.end(),
          "'axis' Attribute is expected for CrossEntropyWithSoftmaxGrad_Op. ");
  int axis = attributes.at("axis").dyn_cast<pir::Int32Attribute>().data();


  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {label_, softmax_, loss_grad_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_soft_label = pir::BoolAttribute::get(pir::IrContext::Instance(), soft_label);
  argument.AddAttribute("soft_label", attr_soft_label);
  pir::Attribute attr_use_softmax = pir::BoolAttribute::get(pir::IrContext::Instance(), use_softmax);
  argument.AddAttribute("use_softmax", attr_use_softmax);
  pir::Attribute attr_numeric_stable_mode = pir::BoolAttribute::get(pir::IrContext::Instance(), numeric_stable_mode);
  argument.AddAttribute("numeric_stable_mode", attr_numeric_stable_mode);
  pir::Attribute attr_ignore_index = pir::Int32Attribute::get(pir::IrContext::Instance(), ignore_index);
  argument.AddAttribute("ignore_index", attr_ignore_index);
  pir::Attribute attr_axis = pir::Int32Attribute::get(pir::IrContext::Instance(), axis);
  argument.AddAttribute("axis", attr_axis);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType label = label_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)label;
  paddle::dialect::DenseTensorType softmax = softmax_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)softmax;
  paddle::dialect::DenseTensorType loss_grad = loss_grad_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)loss_grad;

  VLOG(4) << "Builder construction  dense_label";
  paddle::dialect::IrTensor ir_tensor_label(paddle::dialect::TransToPhiDataType(label.dtype()),
                                                      label.dims(),
                                                      label.data_layout(),
                                                      label.lod(),
                                                      label.offset());
  VLOG(4) << "Builder construction  meta_label";
  paddle::dialect::IrMetaTensor meta_label(&ir_tensor_label);

  VLOG(4) << "Builder construction  dense_softmax";
  paddle::dialect::IrTensor ir_tensor_softmax(paddle::dialect::TransToPhiDataType(softmax.dtype()),
                                                      softmax.dims(),
                                                      softmax.data_layout(),
                                                      softmax.lod(),
                                                      softmax.offset());
  VLOG(4) << "Builder construction  meta_softmax";
  paddle::dialect::IrMetaTensor meta_softmax(&ir_tensor_softmax);

  VLOG(4) << "Builder construction  dense_loss_grad";
  paddle::dialect::IrTensor ir_tensor_loss_grad(paddle::dialect::TransToPhiDataType(loss_grad.dtype()),
                                                      loss_grad.dims(),
                                                      loss_grad.data_layout(),
                                                      loss_grad.lod(),
                                                      loss_grad.offset());
  VLOG(4) << "Builder construction  meta_loss_grad";
  paddle::dialect::IrMetaTensor meta_loss_grad(&ir_tensor_loss_grad);
  paddle::dialect::IrTensor dense_input_grad;
  paddle::dialect::IrMetaTensor meta_input_grad(&dense_input_grad);

  phi::CrossEntropyWithSoftmaxGradInferMeta(meta_label, meta_softmax, meta_loss_grad, soft_label, use_softmax, numeric_stable_mode, ignore_index, axis, &meta_input_grad);

  std::vector<pir::Type> argument_outputs;
  pir::Type input_grad_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_input_grad.dtype()), dense_input_grad.dims(), dense_input_grad.layout(), dense_input_grad.lod(), dense_input_grad.offset());
  argument_outputs.push_back(input_grad_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void CrossEntropyWithSoftmaxGrad_Op::VerifySig() {}

void CrossEntropyWithSoftmaxGrad_Op::InferMeta( phi::InferMetaContext *infer_meta ) {
  auto fn = PD_INFER_META(phi::CrossEntropyWithSoftmaxGradInferMeta);
  fn(infer_meta);
}

phi::DataType CrossEntropyWithSoftmaxGrad_Op::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: CrossEntropyWithSoftmaxGrad_Op";
  


  return expected_kernel_dtype;
}

const char *CrossGradOp::attributes_name[1] = { "axis" };

OpInfoTuple CrossGradOp::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("x", "paddle::dialect::DenseTensorType", false, false, false, false), paddle::dialect::OpInputInfo("y", "paddle::dialect::DenseTensorType", false, false, false, false), paddle::dialect::OpInputInfo("out_grad", "paddle::dialect::DenseTensorType", false, false, false, false) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = { paddle::dialect::OpAttributeInfo("axis", "pir::Int32Attribute", "") };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("x_grad", "paddle::dialect::DenseTensorType", false, false), paddle::dialect::OpOutputInfo("y_grad", "paddle::dialect::DenseTensorType", false, false) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("GeneralBinaryGradInferMeta", {"x", "y"}, "cross_grad", {"x", "y", "out_grad", "axis"}, {"out_grad"}, {}, {}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "cross_grad");
}

void CrossGradOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, pir::Value y_, pir::Value out_grad_, int axis) {
  VLOG(4) << "Start build CrossGradOp";



  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_, y_, out_grad_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_axis = pir::Int32Attribute::get(pir::IrContext::Instance(), axis);
  argument.AddAttribute("axis", attr_axis);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;
  paddle::dialect::DenseTensorType y = y_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)y;
  paddle::dialect::DenseTensorType out_grad = out_grad_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)out_grad;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);

  VLOG(4) << "Builder construction  dense_y";
  paddle::dialect::IrTensor ir_tensor_y(paddle::dialect::TransToPhiDataType(y.dtype()),
                                                      y.dims(),
                                                      y.data_layout(),
                                                      y.lod(),
                                                      y.offset());
  VLOG(4) << "Builder construction  meta_y";
  paddle::dialect::IrMetaTensor meta_y(&ir_tensor_y);
  paddle::dialect::IrTensor dense_x_grad;
  paddle::dialect::IrMetaTensor meta_x_grad(&dense_x_grad);
  paddle::dialect::IrTensor dense_y_grad;
  paddle::dialect::IrMetaTensor meta_y_grad(&dense_y_grad);

  phi::GeneralBinaryGradInferMeta(meta_x, meta_y, &meta_x_grad, &meta_y_grad);

  std::vector<pir::Type> argument_outputs;
  pir::Type x_grad_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_x_grad.dtype()), dense_x_grad.dims(), dense_x_grad.layout(), dense_x_grad.lod(), dense_x_grad.offset());
  argument_outputs.push_back(x_grad_dense_tensor_type);

  pir::Type y_grad_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_y_grad.dtype()), dense_y_grad.dims(), dense_y_grad.layout(), dense_y_grad.lod(), dense_y_grad.offset());
  argument_outputs.push_back(y_grad_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void CrossGradOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, pir::Value y_, pir::Value out_grad_, pir::AttributeMap attributes) {
  VLOG(4) << "Start build CrossGradOp";


  IR_ENFORCE(
      attributes.find("axis") != attributes.end(),
          "'axis' Attribute is expected for CrossGradOp. ");
  int axis = attributes.at("axis").dyn_cast<pir::Int32Attribute>().data();


  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_, y_, out_grad_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_axis = pir::Int32Attribute::get(pir::IrContext::Instance(), axis);
  argument.AddAttribute("axis", attr_axis);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;
  paddle::dialect::DenseTensorType y = y_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)y;
  paddle::dialect::DenseTensorType out_grad = out_grad_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)out_grad;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);

  VLOG(4) << "Builder construction  dense_y";
  paddle::dialect::IrTensor ir_tensor_y(paddle::dialect::TransToPhiDataType(y.dtype()),
                                                      y.dims(),
                                                      y.data_layout(),
                                                      y.lod(),
                                                      y.offset());
  VLOG(4) << "Builder construction  meta_y";
  paddle::dialect::IrMetaTensor meta_y(&ir_tensor_y);
  paddle::dialect::IrTensor dense_x_grad;
  paddle::dialect::IrMetaTensor meta_x_grad(&dense_x_grad);
  paddle::dialect::IrTensor dense_y_grad;
  paddle::dialect::IrMetaTensor meta_y_grad(&dense_y_grad);

  phi::GeneralBinaryGradInferMeta(meta_x, meta_y, &meta_x_grad, &meta_y_grad);

  std::vector<pir::Type> argument_outputs;
  pir::Type x_grad_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_x_grad.dtype()), dense_x_grad.dims(), dense_x_grad.layout(), dense_x_grad.lod(), dense_x_grad.offset());
  argument_outputs.push_back(x_grad_dense_tensor_type);

  pir::Type y_grad_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_y_grad.dtype()), dense_y_grad.dims(), dense_y_grad.layout(), dense_y_grad.lod(), dense_y_grad.offset());
  argument_outputs.push_back(y_grad_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void CrossGradOp::VerifySig() {}

void CrossGradOp::InferMeta( phi::InferMetaContext *infer_meta ) {
  auto fn = PD_INFER_META(phi::GeneralBinaryGradInferMeta);
  fn(infer_meta);
}

phi::DataType CrossGradOp::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: CrossGradOp";
  


  return expected_kernel_dtype;
}

const char *CummaxGradOp::attributes_name[2] = { "axis", "dtype" };

OpInfoTuple CummaxGradOp::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("x", "paddle::dialect::DenseTensorType", false, false, false, false), paddle::dialect::OpInputInfo("indices", "paddle::dialect::DenseTensorType", false, false, false, false), paddle::dialect::OpInputInfo("out_grad", "paddle::dialect::DenseTensorType", false, false, false, false) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = { paddle::dialect::OpAttributeInfo("axis", "pir::Int32Attribute", ""), paddle::dialect::OpAttributeInfo("dtype", "paddle::dialect::DataTypeAttribute", "") };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("x_grad", "paddle::dialect::DenseTensorType", false, false) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("UnchangedInferMeta", {"x"}, "cummax_grad", {"x", "indices", "out_grad", "axis", "dtype"}, {"out_grad"}, {}, {}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "cummax_grad");
}

void CummaxGradOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, pir::Value indices_, pir::Value out_grad_, int axis, phi::DataType dtype) {
  VLOG(4) << "Start build CummaxGradOp";



  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_, indices_, out_grad_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_axis = pir::Int32Attribute::get(pir::IrContext::Instance(), axis);
  argument.AddAttribute("axis", attr_axis);
  pir::Attribute attr_dtype = paddle::dialect::DataTypeAttribute::get(pir::IrContext::Instance(), dtype);
  argument.AddAttribute("dtype", attr_dtype);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;
  paddle::dialect::DenseTensorType indices = indices_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)indices;
  paddle::dialect::DenseTensorType out_grad = out_grad_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)out_grad;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);
  paddle::dialect::IrTensor dense_x_grad;
  paddle::dialect::IrMetaTensor meta_x_grad(&dense_x_grad);

  phi::UnchangedInferMeta(meta_x, &meta_x_grad);

  std::vector<pir::Type> argument_outputs;
  pir::Type x_grad_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_x_grad.dtype()), dense_x_grad.dims(), dense_x_grad.layout(), dense_x_grad.lod(), dense_x_grad.offset());
  argument_outputs.push_back(x_grad_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void CummaxGradOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, pir::Value indices_, pir::Value out_grad_, pir::AttributeMap attributes) {
  VLOG(4) << "Start build CummaxGradOp";


  IR_ENFORCE(
      attributes.find("axis") != attributes.end(),
          "'axis' Attribute is expected for CummaxGradOp. ");
  int axis = attributes.at("axis").dyn_cast<pir::Int32Attribute>().data();

  IR_ENFORCE(
      attributes.find("dtype") != attributes.end(),
          "'dtype' Attribute is expected for CummaxGradOp. ");
  phi::DataType dtype = attributes.at("dtype").dyn_cast<paddle::dialect::DataTypeAttribute>().data();


  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_, indices_, out_grad_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_axis = pir::Int32Attribute::get(pir::IrContext::Instance(), axis);
  argument.AddAttribute("axis", attr_axis);
  pir::Attribute attr_dtype = paddle::dialect::DataTypeAttribute::get(pir::IrContext::Instance(), dtype);
  argument.AddAttribute("dtype", attr_dtype);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;
  paddle::dialect::DenseTensorType indices = indices_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)indices;
  paddle::dialect::DenseTensorType out_grad = out_grad_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)out_grad;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);
  paddle::dialect::IrTensor dense_x_grad;
  paddle::dialect::IrMetaTensor meta_x_grad(&dense_x_grad);

  phi::UnchangedInferMeta(meta_x, &meta_x_grad);

  std::vector<pir::Type> argument_outputs;
  pir::Type x_grad_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_x_grad.dtype()), dense_x_grad.dims(), dense_x_grad.layout(), dense_x_grad.lod(), dense_x_grad.offset());
  argument_outputs.push_back(x_grad_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void CummaxGradOp::VerifySig() {}

void CummaxGradOp::InferMeta( phi::InferMetaContext *infer_meta ) {
  auto fn = PD_INFER_META(phi::UnchangedInferMeta);
  fn(infer_meta);
}

phi::DataType CummaxGradOp::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: CummaxGradOp";
  


  return expected_kernel_dtype;
}

const char *CumminGradOp::attributes_name[2] = { "axis", "dtype" };

OpInfoTuple CumminGradOp::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("x", "paddle::dialect::DenseTensorType", false, false, false, false), paddle::dialect::OpInputInfo("indices", "paddle::dialect::DenseTensorType", false, false, false, false), paddle::dialect::OpInputInfo("out_grad", "paddle::dialect::DenseTensorType", false, false, false, false) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = { paddle::dialect::OpAttributeInfo("axis", "pir::Int32Attribute", ""), paddle::dialect::OpAttributeInfo("dtype", "paddle::dialect::DataTypeAttribute", "") };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("x_grad", "paddle::dialect::DenseTensorType", false, false) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("UnchangedInferMeta", {"x"}, "cummin_grad", {"x", "indices", "out_grad", "axis", "dtype"}, {"out_grad"}, {}, {}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "cummin_grad");
}

void CumminGradOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, pir::Value indices_, pir::Value out_grad_, int axis, phi::DataType dtype) {
  VLOG(4) << "Start build CumminGradOp";



  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_, indices_, out_grad_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_axis = pir::Int32Attribute::get(pir::IrContext::Instance(), axis);
  argument.AddAttribute("axis", attr_axis);
  pir::Attribute attr_dtype = paddle::dialect::DataTypeAttribute::get(pir::IrContext::Instance(), dtype);
  argument.AddAttribute("dtype", attr_dtype);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;
  paddle::dialect::DenseTensorType indices = indices_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)indices;
  paddle::dialect::DenseTensorType out_grad = out_grad_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)out_grad;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);
  paddle::dialect::IrTensor dense_x_grad;
  paddle::dialect::IrMetaTensor meta_x_grad(&dense_x_grad);

  phi::UnchangedInferMeta(meta_x, &meta_x_grad);

  std::vector<pir::Type> argument_outputs;
  pir::Type x_grad_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_x_grad.dtype()), dense_x_grad.dims(), dense_x_grad.layout(), dense_x_grad.lod(), dense_x_grad.offset());
  argument_outputs.push_back(x_grad_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void CumminGradOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, pir::Value indices_, pir::Value out_grad_, pir::AttributeMap attributes) {
  VLOG(4) << "Start build CumminGradOp";


  IR_ENFORCE(
      attributes.find("axis") != attributes.end(),
          "'axis' Attribute is expected for CumminGradOp. ");
  int axis = attributes.at("axis").dyn_cast<pir::Int32Attribute>().data();

  IR_ENFORCE(
      attributes.find("dtype") != attributes.end(),
          "'dtype' Attribute is expected for CumminGradOp. ");
  phi::DataType dtype = attributes.at("dtype").dyn_cast<paddle::dialect::DataTypeAttribute>().data();


  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_, indices_, out_grad_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_axis = pir::Int32Attribute::get(pir::IrContext::Instance(), axis);
  argument.AddAttribute("axis", attr_axis);
  pir::Attribute attr_dtype = paddle::dialect::DataTypeAttribute::get(pir::IrContext::Instance(), dtype);
  argument.AddAttribute("dtype", attr_dtype);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;
  paddle::dialect::DenseTensorType indices = indices_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)indices;
  paddle::dialect::DenseTensorType out_grad = out_grad_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)out_grad;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);
  paddle::dialect::IrTensor dense_x_grad;
  paddle::dialect::IrMetaTensor meta_x_grad(&dense_x_grad);

  phi::UnchangedInferMeta(meta_x, &meta_x_grad);

  std::vector<pir::Type> argument_outputs;
  pir::Type x_grad_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_x_grad.dtype()), dense_x_grad.dims(), dense_x_grad.layout(), dense_x_grad.lod(), dense_x_grad.offset());
  argument_outputs.push_back(x_grad_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void CumminGradOp::VerifySig() {}

void CumminGradOp::InferMeta( phi::InferMetaContext *infer_meta ) {
  auto fn = PD_INFER_META(phi::UnchangedInferMeta);
  fn(infer_meta);
}

phi::DataType CumminGradOp::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: CumminGradOp";
  


  return expected_kernel_dtype;
}

const char *CumprodGradOp::attributes_name[1] = { "dim" };

OpInfoTuple CumprodGradOp::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("x", "paddle::dialect::DenseTensorType", false, false, false, false), paddle::dialect::OpInputInfo("out", "paddle::dialect::DenseTensorType", false, false, false, false), paddle::dialect::OpInputInfo("out_grad", "paddle::dialect::DenseTensorType", false, false, false, false) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = { paddle::dialect::OpAttributeInfo("dim", "pir::Int32Attribute", "") };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("x_grad", "paddle::dialect::DenseTensorType", false, false) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("UnchangedInferMeta", {"x"}, "cumprod_grad", {"x", "out", "out_grad", "dim"}, {}, {}, {}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "cumprod_grad");
}

void CumprodGradOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, pir::Value out_, pir::Value out_grad_, int dim) {
  VLOG(4) << "Start build CumprodGradOp";



  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_, out_, out_grad_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_dim = pir::Int32Attribute::get(pir::IrContext::Instance(), dim);
  argument.AddAttribute("dim", attr_dim);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;
  paddle::dialect::DenseTensorType out = out_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)out;
  paddle::dialect::DenseTensorType out_grad = out_grad_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)out_grad;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);
  paddle::dialect::IrTensor dense_x_grad;
  paddle::dialect::IrMetaTensor meta_x_grad(&dense_x_grad);

  phi::UnchangedInferMeta(meta_x, &meta_x_grad);

  std::vector<pir::Type> argument_outputs;
  pir::Type x_grad_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_x_grad.dtype()), dense_x_grad.dims(), dense_x_grad.layout(), dense_x_grad.lod(), dense_x_grad.offset());
  argument_outputs.push_back(x_grad_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void CumprodGradOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, pir::Value out_, pir::Value out_grad_, pir::AttributeMap attributes) {
  VLOG(4) << "Start build CumprodGradOp";


  IR_ENFORCE(
      attributes.find("dim") != attributes.end(),
          "'dim' Attribute is expected for CumprodGradOp. ");
  int dim = attributes.at("dim").dyn_cast<pir::Int32Attribute>().data();


  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_, out_, out_grad_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_dim = pir::Int32Attribute::get(pir::IrContext::Instance(), dim);
  argument.AddAttribute("dim", attr_dim);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;
  paddle::dialect::DenseTensorType out = out_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)out;
  paddle::dialect::DenseTensorType out_grad = out_grad_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)out_grad;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);
  paddle::dialect::IrTensor dense_x_grad;
  paddle::dialect::IrMetaTensor meta_x_grad(&dense_x_grad);

  phi::UnchangedInferMeta(meta_x, &meta_x_grad);

  std::vector<pir::Type> argument_outputs;
  pir::Type x_grad_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_x_grad.dtype()), dense_x_grad.dims(), dense_x_grad.layout(), dense_x_grad.lod(), dense_x_grad.offset());
  argument_outputs.push_back(x_grad_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void CumprodGradOp::VerifySig() {}

void CumprodGradOp::InferMeta( phi::InferMetaContext *infer_meta ) {
  auto fn = PD_INFER_META(phi::UnchangedInferMeta);
  fn(infer_meta);
}

phi::DataType CumprodGradOp::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: CumprodGradOp";
  


  return expected_kernel_dtype;
}

const char *CumsumGradOp::attributes_name[3] = { "flatten", "exclusive", "reverse" };

OpInfoTuple CumsumGradOp::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("x", "paddle::dialect::DenseTensorType", false, false, false, false), paddle::dialect::OpInputInfo("out_grad", "paddle::dialect::DenseTensorType", false, false, false, false), paddle::dialect::OpInputInfo("axis", "paddle::dialect::ScalarAttribute", false, false, true, false) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = { paddle::dialect::OpAttributeInfo("flatten", "pir::BoolAttribute", ""), paddle::dialect::OpAttributeInfo("exclusive", "pir::BoolAttribute", ""), paddle::dialect::OpAttributeInfo("reverse", "pir::BoolAttribute", "") };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("x_grad", "paddle::dialect::DenseTensorType", false, false) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("UnchangedInferMeta", {"x"}, "cumsum_grad", {"x", "out_grad", "axis", "flatten", "exclusive", "reverse"}, {"x"}, {}, {}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "cumsum_grad");
}

void CumsumGradOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, pir::Value out_grad_, int axis, bool flatten, bool exclusive, bool reverse) {
  VLOG(4) << "Start build CumsumGradOp";


  // Generate scalar mutable attribute: axis
  paddle::dialect::FullOp full_axis_op = builder.Build<paddle::dialect::FullOp>(std::vector<int64_t>{1}, axis, phi::DataType::INT32, phi::CPUPlace());
  pir::OpResult axis_ = full_axis_op->result(0);
    
  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_, out_grad_, axis_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_flatten = pir::BoolAttribute::get(pir::IrContext::Instance(), flatten);
  argument.AddAttribute("flatten", attr_flatten);
  pir::Attribute attr_exclusive = pir::BoolAttribute::get(pir::IrContext::Instance(), exclusive);
  argument.AddAttribute("exclusive", attr_exclusive);
  pir::Attribute attr_reverse = pir::BoolAttribute::get(pir::IrContext::Instance(), reverse);
  argument.AddAttribute("reverse", attr_reverse);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;
  paddle::dialect::DenseTensorType out_grad = out_grad_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)out_grad;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);
  paddle::dialect::IrTensor dense_x_grad;
  paddle::dialect::IrMetaTensor meta_x_grad(&dense_x_grad);

  phi::UnchangedInferMeta(meta_x, &meta_x_grad);

  std::vector<pir::Type> argument_outputs;
  pir::Type x_grad_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_x_grad.dtype()), dense_x_grad.dims(), dense_x_grad.layout(), dense_x_grad.lod(), dense_x_grad.offset());
  argument_outputs.push_back(x_grad_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void CumsumGradOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, pir::Value out_grad_, pir::AttributeMap attributes) {
  VLOG(4) << "Start build CumsumGradOp";


  IR_ENFORCE(
      attributes.find("axis") != attributes.end(),
          "'axis' Attribute is expected for CumsumGradOp. ");
  int axis = attributes.at("axis").dyn_cast<paddle::dialect::ScalarAttribute>().data().to<int>();

  IR_ENFORCE(
      attributes.find("flatten") != attributes.end(),
          "'flatten' Attribute is expected for CumsumGradOp. ");
  bool flatten = attributes.at("flatten").dyn_cast<pir::BoolAttribute>().data();

  IR_ENFORCE(
      attributes.find("exclusive") != attributes.end(),
          "'exclusive' Attribute is expected for CumsumGradOp. ");
  bool exclusive = attributes.at("exclusive").dyn_cast<pir::BoolAttribute>().data();

  IR_ENFORCE(
      attributes.find("reverse") != attributes.end(),
          "'reverse' Attribute is expected for CumsumGradOp. ");
  bool reverse = attributes.at("reverse").dyn_cast<pir::BoolAttribute>().data();

  // Generate scalar mutable attribute: axis
  paddle::dialect::FullOp full_axis_op = builder.Build<paddle::dialect::FullOp>(std::vector<int64_t>{1}, axis, phi::DataType::INT32, phi::CPUPlace());
  pir::OpResult axis_ = full_axis_op->result(0);
    
  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_, out_grad_, axis_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_flatten = pir::BoolAttribute::get(pir::IrContext::Instance(), flatten);
  argument.AddAttribute("flatten", attr_flatten);
  pir::Attribute attr_exclusive = pir::BoolAttribute::get(pir::IrContext::Instance(), exclusive);
  argument.AddAttribute("exclusive", attr_exclusive);
  pir::Attribute attr_reverse = pir::BoolAttribute::get(pir::IrContext::Instance(), reverse);
  argument.AddAttribute("reverse", attr_reverse);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;
  paddle::dialect::DenseTensorType out_grad = out_grad_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)out_grad;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);
  paddle::dialect::IrTensor dense_x_grad;
  paddle::dialect::IrMetaTensor meta_x_grad(&dense_x_grad);

  phi::UnchangedInferMeta(meta_x, &meta_x_grad);

  std::vector<pir::Type> argument_outputs;
  pir::Type x_grad_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_x_grad.dtype()), dense_x_grad.dims(), dense_x_grad.layout(), dense_x_grad.lod(), dense_x_grad.offset());
  argument_outputs.push_back(x_grad_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void CumsumGradOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, pir::Value out_grad_, pir::Value axis_, bool flatten, bool exclusive, bool reverse) {
  VLOG(4) << "Start build CumsumGradOp";



  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_, out_grad_, axis_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_flatten = pir::BoolAttribute::get(pir::IrContext::Instance(), flatten);
  argument.AddAttribute("flatten", attr_flatten);
  pir::Attribute attr_exclusive = pir::BoolAttribute::get(pir::IrContext::Instance(), exclusive);
  argument.AddAttribute("exclusive", attr_exclusive);
  pir::Attribute attr_reverse = pir::BoolAttribute::get(pir::IrContext::Instance(), reverse);
  argument.AddAttribute("reverse", attr_reverse);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;
  paddle::dialect::DenseTensorType out_grad = out_grad_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)out_grad;
  phi::Scalar axis;
  if (axis_.dyn_cast<pir::OpResult>() && axis_.dyn_cast<pir::OpResult>().owner()->isa<paddle::dialect::FullOp>()) {
    axis = std::move(phi::Scalar(axis_.dyn_cast<pir::OpResult>().owner()
                                  ->dyn_cast<paddle::dialect::FullOp>()
                                  .attribute("value")
                                  .dyn_cast<paddle::dialect::ScalarAttribute>()
                                  .data()
                                  .to<int>()));
  }
  else {
    axis = std::move(phi::Scalar(-1));
    axis.SetFromTensor(true);
  }


  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);
  paddle::dialect::IrTensor dense_x_grad;
  paddle::dialect::IrMetaTensor meta_x_grad(&dense_x_grad);

  phi::UnchangedInferMeta(meta_x, &meta_x_grad);

  std::vector<pir::Type> argument_outputs;
  pir::Type x_grad_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_x_grad.dtype()), dense_x_grad.dims(), dense_x_grad.layout(), dense_x_grad.lod(), dense_x_grad.offset());
  argument_outputs.push_back(x_grad_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void CumsumGradOp::VerifySig() {}

void CumsumGradOp::InferMeta( phi::InferMetaContext *infer_meta ) {
  auto fn = PD_INFER_META(phi::UnchangedInferMeta);
  fn(infer_meta);
}

phi::DataType CumsumGradOp::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: CumsumGradOp";
  


  return expected_kernel_dtype;
}

const char *DepthwiseConv2dDoubleGradOp::attributes_name[6] = { "strides", "paddings", "padding_algorithm", "groups", "dilations", "data_format" };

OpInfoTuple DepthwiseConv2dDoubleGradOp::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("input", "paddle::dialect::DenseTensorType", false, false, false, false), paddle::dialect::OpInputInfo("filter", "paddle::dialect::DenseTensorType", false, false, false, false), paddle::dialect::OpInputInfo("grad_out", "paddle::dialect::DenseTensorType", false, false, false, false), paddle::dialect::OpInputInfo("grad_input_grad", "paddle::dialect::DenseTensorType", true, false, false, false), paddle::dialect::OpInputInfo("grad_filter_grad", "paddle::dialect::DenseTensorType", true, false, false, false) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = { paddle::dialect::OpAttributeInfo("strides", "pir::ArrayAttribute<pir::Int32Attribute>", ""), paddle::dialect::OpAttributeInfo("paddings", "pir::ArrayAttribute<pir::Int32Attribute>", ""), paddle::dialect::OpAttributeInfo("padding_algorithm", "pir::StrAttribute", ""), paddle::dialect::OpAttributeInfo("groups", "pir::Int32Attribute", ""), paddle::dialect::OpAttributeInfo("dilations", "pir::ArrayAttribute<pir::Int32Attribute>", ""), paddle::dialect::OpAttributeInfo("data_format", "pir::StrAttribute", "") };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("input_grad", "paddle::dialect::DenseTensorType", false, false), paddle::dialect::OpOutputInfo("filter_grad", "paddle::dialect::DenseTensorType", false, false), paddle::dialect::OpOutputInfo("grad_out_grad", "paddle::dialect::DenseTensorType", false, false) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("GeneralTernaryGradInferMeta", {"input", "filter", "grad_out"}, "depthwise_conv2d_double_grad", {"input", "filter", "grad_out", "grad_input_grad", "grad_filter_grad", "strides", "paddings", "padding_algorithm", "groups", "dilations", "data_format"}, {"input"}, {}, {}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "depthwise_conv2d_double_grad");
}

void DepthwiseConv2dDoubleGradOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value input_, pir::Value filter_, pir::Value grad_out_, pir::Value grad_input_grad_, pir::Value grad_filter_grad_, const std::vector<int>& strides, const std::vector<int>& paddings, const std::string& padding_algorithm, int groups, const std::vector<int>& dilations, const std::string& data_format) {
  VLOG(4) << "Start build DepthwiseConv2dDoubleGradOp";



  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {input_, filter_, grad_out_, grad_input_grad_, grad_filter_grad_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  std::vector<pir::Attribute> vec_strides;
  for (size_t i = 0; i < static_cast<size_t>(strides.size()); i++) {
      pir::Attribute attr_strides = pir::Int32Attribute::get(pir::IrContext::Instance(), strides[i]);

    vec_strides.push_back(attr_strides);
  }
  pir::Attribute attr_strides = pir::ArrayAttribute::get(pir::IrContext::Instance(), vec_strides);
  argument.AddAttribute("strides", attr_strides);
  std::vector<pir::Attribute> vec_paddings;
  for (size_t i = 0; i < static_cast<size_t>(paddings.size()); i++) {
      pir::Attribute attr_paddings = pir::Int32Attribute::get(pir::IrContext::Instance(), paddings[i]);

    vec_paddings.push_back(attr_paddings);
  }
  pir::Attribute attr_paddings = pir::ArrayAttribute::get(pir::IrContext::Instance(), vec_paddings);
  argument.AddAttribute("paddings", attr_paddings);
  pir::Attribute attr_padding_algorithm = pir::StrAttribute::get(pir::IrContext::Instance(), padding_algorithm);
  argument.AddAttribute("padding_algorithm", attr_padding_algorithm);
  pir::Attribute attr_groups = pir::Int32Attribute::get(pir::IrContext::Instance(), groups);
  argument.AddAttribute("groups", attr_groups);
  std::vector<pir::Attribute> vec_dilations;
  for (size_t i = 0; i < static_cast<size_t>(dilations.size()); i++) {
      pir::Attribute attr_dilations = pir::Int32Attribute::get(pir::IrContext::Instance(), dilations[i]);

    vec_dilations.push_back(attr_dilations);
  }
  pir::Attribute attr_dilations = pir::ArrayAttribute::get(pir::IrContext::Instance(), vec_dilations);
  argument.AddAttribute("dilations", attr_dilations);
  pir::Attribute attr_data_format = pir::StrAttribute::get(pir::IrContext::Instance(), data_format);
  argument.AddAttribute("data_format", attr_data_format);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType input = input_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)input;
  paddle::dialect::DenseTensorType filter = filter_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)filter;
  paddle::dialect::DenseTensorType grad_out = grad_out_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)grad_out;

  VLOG(4) << "Builder construction  dense_input";
  paddle::dialect::IrTensor ir_tensor_input(paddle::dialect::TransToPhiDataType(input.dtype()),
                                                      input.dims(),
                                                      input.data_layout(),
                                                      input.lod(),
                                                      input.offset());
  VLOG(4) << "Builder construction  meta_input";
  paddle::dialect::IrMetaTensor meta_input(&ir_tensor_input);

  VLOG(4) << "Builder construction  dense_filter";
  paddle::dialect::IrTensor ir_tensor_filter(paddle::dialect::TransToPhiDataType(filter.dtype()),
                                                      filter.dims(),
                                                      filter.data_layout(),
                                                      filter.lod(),
                                                      filter.offset());
  VLOG(4) << "Builder construction  meta_filter";
  paddle::dialect::IrMetaTensor meta_filter(&ir_tensor_filter);

  VLOG(4) << "Builder construction  dense_grad_out";
  paddle::dialect::IrTensor ir_tensor_grad_out(paddle::dialect::TransToPhiDataType(grad_out.dtype()),
                                                      grad_out.dims(),
                                                      grad_out.data_layout(),
                                                      grad_out.lod(),
                                                      grad_out.offset());
  VLOG(4) << "Builder construction  meta_grad_out";
  paddle::dialect::IrMetaTensor meta_grad_out(&ir_tensor_grad_out);
  paddle::dialect::IrTensor dense_input_grad;
  paddle::dialect::IrMetaTensor meta_input_grad(&dense_input_grad);
  paddle::dialect::IrTensor dense_filter_grad;
  paddle::dialect::IrMetaTensor meta_filter_grad(&dense_filter_grad);
  paddle::dialect::IrTensor dense_grad_out_grad;
  paddle::dialect::IrMetaTensor meta_grad_out_grad(&dense_grad_out_grad);

  phi::GeneralTernaryGradInferMeta(meta_input, meta_filter, meta_grad_out, &meta_input_grad, &meta_filter_grad, &meta_grad_out_grad);

  std::vector<pir::Type> argument_outputs;
  pir::Type input_grad_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_input_grad.dtype()), dense_input_grad.dims(), dense_input_grad.layout(), dense_input_grad.lod(), dense_input_grad.offset());
  argument_outputs.push_back(input_grad_dense_tensor_type);

  pir::Type filter_grad_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_filter_grad.dtype()), dense_filter_grad.dims(), dense_filter_grad.layout(), dense_filter_grad.lod(), dense_filter_grad.offset());
  argument_outputs.push_back(filter_grad_dense_tensor_type);

  pir::Type grad_out_grad_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_grad_out_grad.dtype()), dense_grad_out_grad.dims(), dense_grad_out_grad.layout(), dense_grad_out_grad.lod(), dense_grad_out_grad.offset());
  argument_outputs.push_back(grad_out_grad_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void DepthwiseConv2dDoubleGradOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value input_, pir::Value filter_, pir::Value grad_out_, pir::Value grad_input_grad_, pir::Value grad_filter_grad_, pir::AttributeMap attributes) {
  VLOG(4) << "Start build DepthwiseConv2dDoubleGradOp";


  IR_ENFORCE(
      attributes.find("strides") != attributes.end(),
          "'strides' Attribute is expected for DepthwiseConv2dDoubleGradOp. ");
  std::vector<int> strides;
  for (size_t i = 0; i < attributes.at("strides").dyn_cast<pir::ArrayAttribute>().size(); i++) {
    strides.push_back(attributes.at("strides").dyn_cast<pir::ArrayAttribute>().at(i).dyn_cast<pir::Int32Attribute>().data());
  }

  IR_ENFORCE(
      attributes.find("paddings") != attributes.end(),
          "'paddings' Attribute is expected for DepthwiseConv2dDoubleGradOp. ");
  std::vector<int> paddings;
  for (size_t i = 0; i < attributes.at("paddings").dyn_cast<pir::ArrayAttribute>().size(); i++) {
    paddings.push_back(attributes.at("paddings").dyn_cast<pir::ArrayAttribute>().at(i).dyn_cast<pir::Int32Attribute>().data());
  }

  IR_ENFORCE(
      attributes.find("padding_algorithm") != attributes.end(),
          "'padding_algorithm' Attribute is expected for DepthwiseConv2dDoubleGradOp. ");
  std::string padding_algorithm = attributes.at("padding_algorithm").dyn_cast<pir::StrAttribute>().AsString();

  IR_ENFORCE(
      attributes.find("groups") != attributes.end(),
          "'groups' Attribute is expected for DepthwiseConv2dDoubleGradOp. ");
  int groups = attributes.at("groups").dyn_cast<pir::Int32Attribute>().data();

  IR_ENFORCE(
      attributes.find("dilations") != attributes.end(),
          "'dilations' Attribute is expected for DepthwiseConv2dDoubleGradOp. ");
  std::vector<int> dilations;
  for (size_t i = 0; i < attributes.at("dilations").dyn_cast<pir::ArrayAttribute>().size(); i++) {
    dilations.push_back(attributes.at("dilations").dyn_cast<pir::ArrayAttribute>().at(i).dyn_cast<pir::Int32Attribute>().data());
  }

  IR_ENFORCE(
      attributes.find("data_format") != attributes.end(),
          "'data_format' Attribute is expected for DepthwiseConv2dDoubleGradOp. ");
  std::string data_format = attributes.at("data_format").dyn_cast<pir::StrAttribute>().AsString();


  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {input_, filter_, grad_out_, grad_input_grad_, grad_filter_grad_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  std::vector<pir::Attribute> vec_strides;
  for (size_t i = 0; i < static_cast<size_t>(strides.size()); i++) {
      pir::Attribute attr_strides = pir::Int32Attribute::get(pir::IrContext::Instance(), strides[i]);

    vec_strides.push_back(attr_strides);
  }
  pir::Attribute attr_strides = pir::ArrayAttribute::get(pir::IrContext::Instance(), vec_strides);
  argument.AddAttribute("strides", attr_strides);
  std::vector<pir::Attribute> vec_paddings;
  for (size_t i = 0; i < static_cast<size_t>(paddings.size()); i++) {
      pir::Attribute attr_paddings = pir::Int32Attribute::get(pir::IrContext::Instance(), paddings[i]);

    vec_paddings.push_back(attr_paddings);
  }
  pir::Attribute attr_paddings = pir::ArrayAttribute::get(pir::IrContext::Instance(), vec_paddings);
  argument.AddAttribute("paddings", attr_paddings);
  pir::Attribute attr_padding_algorithm = pir::StrAttribute::get(pir::IrContext::Instance(), padding_algorithm);
  argument.AddAttribute("padding_algorithm", attr_padding_algorithm);
  pir::Attribute attr_groups = pir::Int32Attribute::get(pir::IrContext::Instance(), groups);
  argument.AddAttribute("groups", attr_groups);
  std::vector<pir::Attribute> vec_dilations;
  for (size_t i = 0; i < static_cast<size_t>(dilations.size()); i++) {
      pir::Attribute attr_dilations = pir::Int32Attribute::get(pir::IrContext::Instance(), dilations[i]);

    vec_dilations.push_back(attr_dilations);
  }
  pir::Attribute attr_dilations = pir::ArrayAttribute::get(pir::IrContext::Instance(), vec_dilations);
  argument.AddAttribute("dilations", attr_dilations);
  pir::Attribute attr_data_format = pir::StrAttribute::get(pir::IrContext::Instance(), data_format);
  argument.AddAttribute("data_format", attr_data_format);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType input = input_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)input;
  paddle::dialect::DenseTensorType filter = filter_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)filter;
  paddle::dialect::DenseTensorType grad_out = grad_out_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)grad_out;

  VLOG(4) << "Builder construction  dense_input";
  paddle::dialect::IrTensor ir_tensor_input(paddle::dialect::TransToPhiDataType(input.dtype()),
                                                      input.dims(),
                                                      input.data_layout(),
                                                      input.lod(),
                                                      input.offset());
  VLOG(4) << "Builder construction  meta_input";
  paddle::dialect::IrMetaTensor meta_input(&ir_tensor_input);

  VLOG(4) << "Builder construction  dense_filter";
  paddle::dialect::IrTensor ir_tensor_filter(paddle::dialect::TransToPhiDataType(filter.dtype()),
                                                      filter.dims(),
                                                      filter.data_layout(),
                                                      filter.lod(),
                                                      filter.offset());
  VLOG(4) << "Builder construction  meta_filter";
  paddle::dialect::IrMetaTensor meta_filter(&ir_tensor_filter);

  VLOG(4) << "Builder construction  dense_grad_out";
  paddle::dialect::IrTensor ir_tensor_grad_out(paddle::dialect::TransToPhiDataType(grad_out.dtype()),
                                                      grad_out.dims(),
                                                      grad_out.data_layout(),
                                                      grad_out.lod(),
                                                      grad_out.offset());
  VLOG(4) << "Builder construction  meta_grad_out";
  paddle::dialect::IrMetaTensor meta_grad_out(&ir_tensor_grad_out);
  paddle::dialect::IrTensor dense_input_grad;
  paddle::dialect::IrMetaTensor meta_input_grad(&dense_input_grad);
  paddle::dialect::IrTensor dense_filter_grad;
  paddle::dialect::IrMetaTensor meta_filter_grad(&dense_filter_grad);
  paddle::dialect::IrTensor dense_grad_out_grad;
  paddle::dialect::IrMetaTensor meta_grad_out_grad(&dense_grad_out_grad);

  phi::GeneralTernaryGradInferMeta(meta_input, meta_filter, meta_grad_out, &meta_input_grad, &meta_filter_grad, &meta_grad_out_grad);

  std::vector<pir::Type> argument_outputs;
  pir::Type input_grad_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_input_grad.dtype()), dense_input_grad.dims(), dense_input_grad.layout(), dense_input_grad.lod(), dense_input_grad.offset());
  argument_outputs.push_back(input_grad_dense_tensor_type);

  pir::Type filter_grad_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_filter_grad.dtype()), dense_filter_grad.dims(), dense_filter_grad.layout(), dense_filter_grad.lod(), dense_filter_grad.offset());
  argument_outputs.push_back(filter_grad_dense_tensor_type);

  pir::Type grad_out_grad_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_grad_out_grad.dtype()), dense_grad_out_grad.dims(), dense_grad_out_grad.layout(), dense_grad_out_grad.lod(), dense_grad_out_grad.offset());
  argument_outputs.push_back(grad_out_grad_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void DepthwiseConv2dDoubleGradOp::VerifySig() {}

void DepthwiseConv2dDoubleGradOp::InferMeta( phi::InferMetaContext *infer_meta ) {
  auto fn = PD_INFER_META(phi::GeneralTernaryGradInferMeta);
  fn(infer_meta);
}

phi::DataType DepthwiseConv2dDoubleGradOp::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: DepthwiseConv2dDoubleGradOp";
  


  return expected_kernel_dtype;
}

const char *DepthwiseConv2dGradOp::attributes_name[6] = { "strides", "paddings", "padding_algorithm", "groups", "dilations", "data_format" };

OpInfoTuple DepthwiseConv2dGradOp::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("input", "paddle::dialect::DenseTensorType", false, false, false, true), paddle::dialect::OpInputInfo("filter", "paddle::dialect::DenseTensorType", false, false, false, true), paddle::dialect::OpInputInfo("out_grad", "paddle::dialect::DenseTensorType", false, false, false, true) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = { paddle::dialect::OpAttributeInfo("strides", "pir::ArrayAttribute<pir::Int32Attribute>", ""), paddle::dialect::OpAttributeInfo("paddings", "pir::ArrayAttribute<pir::Int32Attribute>", ""), paddle::dialect::OpAttributeInfo("padding_algorithm", "pir::StrAttribute", ""), paddle::dialect::OpAttributeInfo("groups", "pir::Int32Attribute", ""), paddle::dialect::OpAttributeInfo("dilations", "pir::ArrayAttribute<pir::Int32Attribute>", ""), paddle::dialect::OpAttributeInfo("data_format", "pir::StrAttribute", "") };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("input_grad", "paddle::dialect::DenseTensorType", false, false), paddle::dialect::OpOutputInfo("filter_grad", "paddle::dialect::DenseTensorType", false, false) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("GeneralBinaryGradInferMeta", {"input", "filter"}, "depthwise_conv2d_grad", {"input", "filter", "out_grad", "strides", "paddings", "padding_algorithm", "groups", "dilations", "data_format"}, {"input"}, {}, {}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "depthwise_conv2d_grad");
}

void DepthwiseConv2dGradOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value input_, pir::Value filter_, pir::Value out_grad_, const std::vector<int>& strides, const std::vector<int>& paddings, const std::string& padding_algorithm, int groups, const std::vector<int>& dilations, const std::string& data_format) {
  VLOG(4) << "Start build DepthwiseConv2dGradOp";



  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {input_, filter_, out_grad_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  std::vector<pir::Attribute> vec_strides;
  for (size_t i = 0; i < static_cast<size_t>(strides.size()); i++) {
      pir::Attribute attr_strides = pir::Int32Attribute::get(pir::IrContext::Instance(), strides[i]);

    vec_strides.push_back(attr_strides);
  }
  pir::Attribute attr_strides = pir::ArrayAttribute::get(pir::IrContext::Instance(), vec_strides);
  argument.AddAttribute("strides", attr_strides);
  std::vector<pir::Attribute> vec_paddings;
  for (size_t i = 0; i < static_cast<size_t>(paddings.size()); i++) {
      pir::Attribute attr_paddings = pir::Int32Attribute::get(pir::IrContext::Instance(), paddings[i]);

    vec_paddings.push_back(attr_paddings);
  }
  pir::Attribute attr_paddings = pir::ArrayAttribute::get(pir::IrContext::Instance(), vec_paddings);
  argument.AddAttribute("paddings", attr_paddings);
  pir::Attribute attr_padding_algorithm = pir::StrAttribute::get(pir::IrContext::Instance(), padding_algorithm);
  argument.AddAttribute("padding_algorithm", attr_padding_algorithm);
  pir::Attribute attr_groups = pir::Int32Attribute::get(pir::IrContext::Instance(), groups);
  argument.AddAttribute("groups", attr_groups);
  std::vector<pir::Attribute> vec_dilations;
  for (size_t i = 0; i < static_cast<size_t>(dilations.size()); i++) {
      pir::Attribute attr_dilations = pir::Int32Attribute::get(pir::IrContext::Instance(), dilations[i]);

    vec_dilations.push_back(attr_dilations);
  }
  pir::Attribute attr_dilations = pir::ArrayAttribute::get(pir::IrContext::Instance(), vec_dilations);
  argument.AddAttribute("dilations", attr_dilations);
  pir::Attribute attr_data_format = pir::StrAttribute::get(pir::IrContext::Instance(), data_format);
  argument.AddAttribute("data_format", attr_data_format);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType input = input_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)input;
  paddle::dialect::DenseTensorType filter = filter_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)filter;
  paddle::dialect::DenseTensorType out_grad = out_grad_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)out_grad;

  VLOG(4) << "Builder construction  dense_input";
  paddle::dialect::IrTensor ir_tensor_input(paddle::dialect::TransToPhiDataType(input.dtype()),
                                                      input.dims(),
                                                      input.data_layout(),
                                                      input.lod(),
                                                      input.offset());
  VLOG(4) << "Builder construction  meta_input";
  paddle::dialect::IrMetaTensor meta_input(&ir_tensor_input);

  VLOG(4) << "Builder construction  dense_filter";
  paddle::dialect::IrTensor ir_tensor_filter(paddle::dialect::TransToPhiDataType(filter.dtype()),
                                                      filter.dims(),
                                                      filter.data_layout(),
                                                      filter.lod(),
                                                      filter.offset());
  VLOG(4) << "Builder construction  meta_filter";
  paddle::dialect::IrMetaTensor meta_filter(&ir_tensor_filter);
  paddle::dialect::IrTensor dense_input_grad;
  paddle::dialect::IrMetaTensor meta_input_grad(&dense_input_grad);
  paddle::dialect::IrTensor dense_filter_grad;
  paddle::dialect::IrMetaTensor meta_filter_grad(&dense_filter_grad);

  phi::GeneralBinaryGradInferMeta(meta_input, meta_filter, &meta_input_grad, &meta_filter_grad);

  std::vector<pir::Type> argument_outputs;
  pir::Type input_grad_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_input_grad.dtype()), dense_input_grad.dims(), dense_input_grad.layout(), dense_input_grad.lod(), dense_input_grad.offset());
  argument_outputs.push_back(input_grad_dense_tensor_type);

  pir::Type filter_grad_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_filter_grad.dtype()), dense_filter_grad.dims(), dense_filter_grad.layout(), dense_filter_grad.lod(), dense_filter_grad.offset());
  argument_outputs.push_back(filter_grad_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void DepthwiseConv2dGradOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value input_, pir::Value filter_, pir::Value out_grad_, pir::AttributeMap attributes) {
  VLOG(4) << "Start build DepthwiseConv2dGradOp";


  IR_ENFORCE(
      attributes.find("strides") != attributes.end(),
          "'strides' Attribute is expected for DepthwiseConv2dGradOp. ");
  std::vector<int> strides;
  for (size_t i = 0; i < attributes.at("strides").dyn_cast<pir::ArrayAttribute>().size(); i++) {
    strides.push_back(attributes.at("strides").dyn_cast<pir::ArrayAttribute>().at(i).dyn_cast<pir::Int32Attribute>().data());
  }

  IR_ENFORCE(
      attributes.find("paddings") != attributes.end(),
          "'paddings' Attribute is expected for DepthwiseConv2dGradOp. ");
  std::vector<int> paddings;
  for (size_t i = 0; i < attributes.at("paddings").dyn_cast<pir::ArrayAttribute>().size(); i++) {
    paddings.push_back(attributes.at("paddings").dyn_cast<pir::ArrayAttribute>().at(i).dyn_cast<pir::Int32Attribute>().data());
  }

  IR_ENFORCE(
      attributes.find("padding_algorithm") != attributes.end(),
          "'padding_algorithm' Attribute is expected for DepthwiseConv2dGradOp. ");
  std::string padding_algorithm = attributes.at("padding_algorithm").dyn_cast<pir::StrAttribute>().AsString();

  IR_ENFORCE(
      attributes.find("groups") != attributes.end(),
          "'groups' Attribute is expected for DepthwiseConv2dGradOp. ");
  int groups = attributes.at("groups").dyn_cast<pir::Int32Attribute>().data();

  IR_ENFORCE(
      attributes.find("dilations") != attributes.end(),
          "'dilations' Attribute is expected for DepthwiseConv2dGradOp. ");
  std::vector<int> dilations;
  for (size_t i = 0; i < attributes.at("dilations").dyn_cast<pir::ArrayAttribute>().size(); i++) {
    dilations.push_back(attributes.at("dilations").dyn_cast<pir::ArrayAttribute>().at(i).dyn_cast<pir::Int32Attribute>().data());
  }

  IR_ENFORCE(
      attributes.find("data_format") != attributes.end(),
          "'data_format' Attribute is expected for DepthwiseConv2dGradOp. ");
  std::string data_format = attributes.at("data_format").dyn_cast<pir::StrAttribute>().AsString();


  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {input_, filter_, out_grad_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  std::vector<pir::Attribute> vec_strides;
  for (size_t i = 0; i < static_cast<size_t>(strides.size()); i++) {
      pir::Attribute attr_strides = pir::Int32Attribute::get(pir::IrContext::Instance(), strides[i]);

    vec_strides.push_back(attr_strides);
  }
  pir::Attribute attr_strides = pir::ArrayAttribute::get(pir::IrContext::Instance(), vec_strides);
  argument.AddAttribute("strides", attr_strides);
  std::vector<pir::Attribute> vec_paddings;
  for (size_t i = 0; i < static_cast<size_t>(paddings.size()); i++) {
      pir::Attribute attr_paddings = pir::Int32Attribute::get(pir::IrContext::Instance(), paddings[i]);

    vec_paddings.push_back(attr_paddings);
  }
  pir::Attribute attr_paddings = pir::ArrayAttribute::get(pir::IrContext::Instance(), vec_paddings);
  argument.AddAttribute("paddings", attr_paddings);
  pir::Attribute attr_padding_algorithm = pir::StrAttribute::get(pir::IrContext::Instance(), padding_algorithm);
  argument.AddAttribute("padding_algorithm", attr_padding_algorithm);
  pir::Attribute attr_groups = pir::Int32Attribute::get(pir::IrContext::Instance(), groups);
  argument.AddAttribute("groups", attr_groups);
  std::vector<pir::Attribute> vec_dilations;
  for (size_t i = 0; i < static_cast<size_t>(dilations.size()); i++) {
      pir::Attribute attr_dilations = pir::Int32Attribute::get(pir::IrContext::Instance(), dilations[i]);

    vec_dilations.push_back(attr_dilations);
  }
  pir::Attribute attr_dilations = pir::ArrayAttribute::get(pir::IrContext::Instance(), vec_dilations);
  argument.AddAttribute("dilations", attr_dilations);
  pir::Attribute attr_data_format = pir::StrAttribute::get(pir::IrContext::Instance(), data_format);
  argument.AddAttribute("data_format", attr_data_format);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType input = input_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)input;
  paddle::dialect::DenseTensorType filter = filter_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)filter;
  paddle::dialect::DenseTensorType out_grad = out_grad_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)out_grad;

  VLOG(4) << "Builder construction  dense_input";
  paddle::dialect::IrTensor ir_tensor_input(paddle::dialect::TransToPhiDataType(input.dtype()),
                                                      input.dims(),
                                                      input.data_layout(),
                                                      input.lod(),
                                                      input.offset());
  VLOG(4) << "Builder construction  meta_input";
  paddle::dialect::IrMetaTensor meta_input(&ir_tensor_input);

  VLOG(4) << "Builder construction  dense_filter";
  paddle::dialect::IrTensor ir_tensor_filter(paddle::dialect::TransToPhiDataType(filter.dtype()),
                                                      filter.dims(),
                                                      filter.data_layout(),
                                                      filter.lod(),
                                                      filter.offset());
  VLOG(4) << "Builder construction  meta_filter";
  paddle::dialect::IrMetaTensor meta_filter(&ir_tensor_filter);
  paddle::dialect::IrTensor dense_input_grad;
  paddle::dialect::IrMetaTensor meta_input_grad(&dense_input_grad);
  paddle::dialect::IrTensor dense_filter_grad;
  paddle::dialect::IrMetaTensor meta_filter_grad(&dense_filter_grad);

  phi::GeneralBinaryGradInferMeta(meta_input, meta_filter, &meta_input_grad, &meta_filter_grad);

  std::vector<pir::Type> argument_outputs;
  pir::Type input_grad_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_input_grad.dtype()), dense_input_grad.dims(), dense_input_grad.layout(), dense_input_grad.lod(), dense_input_grad.offset());
  argument_outputs.push_back(input_grad_dense_tensor_type);

  pir::Type filter_grad_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_filter_grad.dtype()), dense_filter_grad.dims(), dense_filter_grad.layout(), dense_filter_grad.lod(), dense_filter_grad.offset());
  argument_outputs.push_back(filter_grad_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void DepthwiseConv2dGradOp::VerifySig() {}

void DepthwiseConv2dGradOp::InferMeta( phi::InferMetaContext *infer_meta ) {
  auto fn = PD_INFER_META(phi::GeneralBinaryGradInferMeta);
  fn(infer_meta);
}

phi::DataType DepthwiseConv2dGradOp::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: DepthwiseConv2dGradOp";
  


  return expected_kernel_dtype;
}

OpInfoTuple DetGradOp::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("x", "paddle::dialect::DenseTensorType", false, false, false, false), paddle::dialect::OpInputInfo("out", "paddle::dialect::DenseTensorType", false, false, false, false), paddle::dialect::OpInputInfo("out_grad", "paddle::dialect::DenseTensorType", false, false, false, false) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = {  };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("x_grad", "paddle::dialect::DenseTensorType", false, false) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("UnchangedInferMeta", {"x"}, "determinant_grad", {"x", "out", "out_grad"}, {"out_grad"}, {}, {}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "det_grad");
}

void DetGradOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, pir::Value out_, pir::Value out_grad_) {
  VLOG(4) << "Start build DetGradOp";



  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_, out_, out_grad_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;
  paddle::dialect::DenseTensorType out = out_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)out;
  paddle::dialect::DenseTensorType out_grad = out_grad_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)out_grad;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);
  paddle::dialect::IrTensor dense_x_grad;
  paddle::dialect::IrMetaTensor meta_x_grad(&dense_x_grad);

  phi::UnchangedInferMeta(meta_x, &meta_x_grad);

  std::vector<pir::Type> argument_outputs;
  pir::Type x_grad_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_x_grad.dtype()), dense_x_grad.dims(), dense_x_grad.layout(), dense_x_grad.lod(), dense_x_grad.offset());
  argument_outputs.push_back(x_grad_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void DetGradOp::VerifySig() {}

void DetGradOp::InferMeta( phi::InferMetaContext *infer_meta ) {
  auto fn = PD_INFER_META(phi::UnchangedInferMeta);
  fn(infer_meta);
}

phi::DataType DetGradOp::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: DetGradOp";
  


  return expected_kernel_dtype;
}

const char *DiagGradOp::attributes_name[1] = { "offset" };

OpInfoTuple DiagGradOp::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("x", "paddle::dialect::DenseTensorType", false, true, false, false), paddle::dialect::OpInputInfo("out_grad", "paddle::dialect::DenseTensorType", false, false, false, false) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = { paddle::dialect::OpAttributeInfo("offset", "pir::Int32Attribute", "") };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("x_grad", "paddle::dialect::DenseTensorType", false, false) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("UnchangedInferMeta", {"x"}, "diag_grad", {"x", "out_grad", "offset"}, {"out_grad"}, {}, {}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "diag_grad");
}

void DiagGradOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, pir::Value out_grad_, int offset) {
  VLOG(4) << "Start build DiagGradOp";



  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_, out_grad_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_offset = pir::Int32Attribute::get(pir::IrContext::Instance(), offset);
  argument.AddAttribute("offset", attr_offset);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;
  paddle::dialect::DenseTensorType out_grad = out_grad_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)out_grad;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);
  paddle::dialect::IrTensor dense_x_grad;
  paddle::dialect::IrMetaTensor meta_x_grad(&dense_x_grad);

  phi::UnchangedInferMeta(meta_x, &meta_x_grad);

  std::vector<pir::Type> argument_outputs;
  pir::Type x_grad_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_x_grad.dtype()), dense_x_grad.dims(), dense_x_grad.layout(), dense_x_grad.lod(), dense_x_grad.offset());
  argument_outputs.push_back(x_grad_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void DiagGradOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, pir::Value out_grad_, pir::AttributeMap attributes) {
  VLOG(4) << "Start build DiagGradOp";


  IR_ENFORCE(
      attributes.find("offset") != attributes.end(),
          "'offset' Attribute is expected for DiagGradOp. ");
  int offset = attributes.at("offset").dyn_cast<pir::Int32Attribute>().data();


  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_, out_grad_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_offset = pir::Int32Attribute::get(pir::IrContext::Instance(), offset);
  argument.AddAttribute("offset", attr_offset);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;
  paddle::dialect::DenseTensorType out_grad = out_grad_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)out_grad;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);
  paddle::dialect::IrTensor dense_x_grad;
  paddle::dialect::IrMetaTensor meta_x_grad(&dense_x_grad);

  phi::UnchangedInferMeta(meta_x, &meta_x_grad);

  std::vector<pir::Type> argument_outputs;
  pir::Type x_grad_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_x_grad.dtype()), dense_x_grad.dims(), dense_x_grad.layout(), dense_x_grad.lod(), dense_x_grad.offset());
  argument_outputs.push_back(x_grad_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void DiagGradOp::VerifySig() {}

void DiagGradOp::InferMeta( phi::InferMetaContext *infer_meta ) {
  auto fn = PD_INFER_META(phi::UnchangedInferMeta);
  fn(infer_meta);
}

phi::DataType DiagGradOp::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: DiagGradOp";
  


  return expected_kernel_dtype;
}

const char *DiagonalGradOp::attributes_name[3] = { "offset", "axis1", "axis2" };

OpInfoTuple DiagonalGradOp::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("x", "paddle::dialect::DenseTensorType", false, true, false, false), paddle::dialect::OpInputInfo("out_grad", "paddle::dialect::DenseTensorType", false, false, false, false) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = { paddle::dialect::OpAttributeInfo("offset", "pir::Int32Attribute", ""), paddle::dialect::OpAttributeInfo("axis1", "pir::Int32Attribute", ""), paddle::dialect::OpAttributeInfo("axis2", "pir::Int32Attribute", "") };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("x_grad", "paddle::dialect::DenseTensorType", false, false) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("UnchangedInferMeta", {"x"}, "diagonal_grad", {"x", "out_grad", "offset", "axis1", "axis2"}, {"out_grad"}, {}, {}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "diagonal_grad");
}

void DiagonalGradOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, pir::Value out_grad_, int offset, int axis1, int axis2) {
  VLOG(4) << "Start build DiagonalGradOp";



  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_, out_grad_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_offset = pir::Int32Attribute::get(pir::IrContext::Instance(), offset);
  argument.AddAttribute("offset", attr_offset);
  pir::Attribute attr_axis1 = pir::Int32Attribute::get(pir::IrContext::Instance(), axis1);
  argument.AddAttribute("axis1", attr_axis1);
  pir::Attribute attr_axis2 = pir::Int32Attribute::get(pir::IrContext::Instance(), axis2);
  argument.AddAttribute("axis2", attr_axis2);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;
  paddle::dialect::DenseTensorType out_grad = out_grad_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)out_grad;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);
  paddle::dialect::IrTensor dense_x_grad;
  paddle::dialect::IrMetaTensor meta_x_grad(&dense_x_grad);

  phi::UnchangedInferMeta(meta_x, &meta_x_grad);

  std::vector<pir::Type> argument_outputs;
  pir::Type x_grad_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_x_grad.dtype()), dense_x_grad.dims(), dense_x_grad.layout(), dense_x_grad.lod(), dense_x_grad.offset());
  argument_outputs.push_back(x_grad_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void DiagonalGradOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, pir::Value out_grad_, pir::AttributeMap attributes) {
  VLOG(4) << "Start build DiagonalGradOp";


  IR_ENFORCE(
      attributes.find("offset") != attributes.end(),
          "'offset' Attribute is expected for DiagonalGradOp. ");
  int offset = attributes.at("offset").dyn_cast<pir::Int32Attribute>().data();

  IR_ENFORCE(
      attributes.find("axis1") != attributes.end(),
          "'axis1' Attribute is expected for DiagonalGradOp. ");
  int axis1 = attributes.at("axis1").dyn_cast<pir::Int32Attribute>().data();

  IR_ENFORCE(
      attributes.find("axis2") != attributes.end(),
          "'axis2' Attribute is expected for DiagonalGradOp. ");
  int axis2 = attributes.at("axis2").dyn_cast<pir::Int32Attribute>().data();


  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_, out_grad_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_offset = pir::Int32Attribute::get(pir::IrContext::Instance(), offset);
  argument.AddAttribute("offset", attr_offset);
  pir::Attribute attr_axis1 = pir::Int32Attribute::get(pir::IrContext::Instance(), axis1);
  argument.AddAttribute("axis1", attr_axis1);
  pir::Attribute attr_axis2 = pir::Int32Attribute::get(pir::IrContext::Instance(), axis2);
  argument.AddAttribute("axis2", attr_axis2);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;
  paddle::dialect::DenseTensorType out_grad = out_grad_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)out_grad;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);
  paddle::dialect::IrTensor dense_x_grad;
  paddle::dialect::IrMetaTensor meta_x_grad(&dense_x_grad);

  phi::UnchangedInferMeta(meta_x, &meta_x_grad);

  std::vector<pir::Type> argument_outputs;
  pir::Type x_grad_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_x_grad.dtype()), dense_x_grad.dims(), dense_x_grad.layout(), dense_x_grad.lod(), dense_x_grad.offset());
  argument_outputs.push_back(x_grad_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void DiagonalGradOp::VerifySig() {}

void DiagonalGradOp::InferMeta( phi::InferMetaContext *infer_meta ) {
  auto fn = PD_INFER_META(phi::UnchangedInferMeta);
  fn(infer_meta);
}

phi::DataType DiagonalGradOp::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: DiagonalGradOp";
  


  return expected_kernel_dtype;
}

OpInfoTuple DigammaGradOp::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("x", "paddle::dialect::DenseTensorType", false, false, false, false), paddle::dialect::OpInputInfo("out_grad", "paddle::dialect::DenseTensorType", false, false, false, false) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = {  };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("x_grad", "paddle::dialect::DenseTensorType", false, false) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("UnchangedInferMeta", {"x"}, "digamma_grad", {"x", "out_grad"}, {}, {}, {}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "digamma_grad");
}

void DigammaGradOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, pir::Value out_grad_) {
  VLOG(4) << "Start build DigammaGradOp";



  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_, out_grad_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;
  paddle::dialect::DenseTensorType out_grad = out_grad_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)out_grad;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);
  paddle::dialect::IrTensor dense_x_grad;
  paddle::dialect::IrMetaTensor meta_x_grad(&dense_x_grad);

  phi::UnchangedInferMeta(meta_x, &meta_x_grad);

  std::vector<pir::Type> argument_outputs;
  pir::Type x_grad_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_x_grad.dtype()), dense_x_grad.dims(), dense_x_grad.layout(), dense_x_grad.lod(), dense_x_grad.offset());
  argument_outputs.push_back(x_grad_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void DigammaGradOp::VerifySig() {}

void DigammaGradOp::InferMeta( phi::InferMetaContext *infer_meta ) {
  auto fn = PD_INFER_META(phi::UnchangedInferMeta);
  fn(infer_meta);
}

phi::DataType DigammaGradOp::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: DigammaGradOp";
  


  return expected_kernel_dtype;
}

const char *DistGradOp::attributes_name[1] = { "p" };

OpInfoTuple DistGradOp::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("x", "paddle::dialect::DenseTensorType", false, false, false, false), paddle::dialect::OpInputInfo("y", "paddle::dialect::DenseTensorType", false, false, false, false), paddle::dialect::OpInputInfo("out", "paddle::dialect::DenseTensorType", false, false, false, false), paddle::dialect::OpInputInfo("out_grad", "paddle::dialect::DenseTensorType", false, false, false, false) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = { paddle::dialect::OpAttributeInfo("p", "pir::FloatAttribute", "") };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("x_grad", "paddle::dialect::DenseTensorType", false, false), paddle::dialect::OpOutputInfo("y_grad", "paddle::dialect::DenseTensorType", false, false) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("GeneralBinaryGradInferMeta", {"x", "y"}, "dist_grad", {"x", "y", "out", "out_grad", "p"}, {}, {}, {}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "dist_grad");
}

void DistGradOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, pir::Value y_, pir::Value out_, pir::Value out_grad_, float p) {
  VLOG(4) << "Start build DistGradOp";



  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_, y_, out_, out_grad_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_p = pir::FloatAttribute::get(pir::IrContext::Instance(), p);
  argument.AddAttribute("p", attr_p);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;
  paddle::dialect::DenseTensorType y = y_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)y;
  paddle::dialect::DenseTensorType out = out_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)out;
  paddle::dialect::DenseTensorType out_grad = out_grad_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)out_grad;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);

  VLOG(4) << "Builder construction  dense_y";
  paddle::dialect::IrTensor ir_tensor_y(paddle::dialect::TransToPhiDataType(y.dtype()),
                                                      y.dims(),
                                                      y.data_layout(),
                                                      y.lod(),
                                                      y.offset());
  VLOG(4) << "Builder construction  meta_y";
  paddle::dialect::IrMetaTensor meta_y(&ir_tensor_y);
  paddle::dialect::IrTensor dense_x_grad;
  paddle::dialect::IrMetaTensor meta_x_grad(&dense_x_grad);
  paddle::dialect::IrTensor dense_y_grad;
  paddle::dialect::IrMetaTensor meta_y_grad(&dense_y_grad);

  phi::GeneralBinaryGradInferMeta(meta_x, meta_y, &meta_x_grad, &meta_y_grad);

  std::vector<pir::Type> argument_outputs;
  pir::Type x_grad_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_x_grad.dtype()), dense_x_grad.dims(), dense_x_grad.layout(), dense_x_grad.lod(), dense_x_grad.offset());
  argument_outputs.push_back(x_grad_dense_tensor_type);

  pir::Type y_grad_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_y_grad.dtype()), dense_y_grad.dims(), dense_y_grad.layout(), dense_y_grad.lod(), dense_y_grad.offset());
  argument_outputs.push_back(y_grad_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void DistGradOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, pir::Value y_, pir::Value out_, pir::Value out_grad_, pir::AttributeMap attributes) {
  VLOG(4) << "Start build DistGradOp";


  IR_ENFORCE(
      attributes.find("p") != attributes.end(),
          "'p' Attribute is expected for DistGradOp. ");
  float p = attributes.at("p").dyn_cast<pir::FloatAttribute>().data();


  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_, y_, out_, out_grad_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_p = pir::FloatAttribute::get(pir::IrContext::Instance(), p);
  argument.AddAttribute("p", attr_p);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;
  paddle::dialect::DenseTensorType y = y_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)y;
  paddle::dialect::DenseTensorType out = out_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)out;
  paddle::dialect::DenseTensorType out_grad = out_grad_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)out_grad;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);

  VLOG(4) << "Builder construction  dense_y";
  paddle::dialect::IrTensor ir_tensor_y(paddle::dialect::TransToPhiDataType(y.dtype()),
                                                      y.dims(),
                                                      y.data_layout(),
                                                      y.lod(),
                                                      y.offset());
  VLOG(4) << "Builder construction  meta_y";
  paddle::dialect::IrMetaTensor meta_y(&ir_tensor_y);
  paddle::dialect::IrTensor dense_x_grad;
  paddle::dialect::IrMetaTensor meta_x_grad(&dense_x_grad);
  paddle::dialect::IrTensor dense_y_grad;
  paddle::dialect::IrMetaTensor meta_y_grad(&dense_y_grad);

  phi::GeneralBinaryGradInferMeta(meta_x, meta_y, &meta_x_grad, &meta_y_grad);

  std::vector<pir::Type> argument_outputs;
  pir::Type x_grad_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_x_grad.dtype()), dense_x_grad.dims(), dense_x_grad.layout(), dense_x_grad.lod(), dense_x_grad.offset());
  argument_outputs.push_back(x_grad_dense_tensor_type);

  pir::Type y_grad_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_y_grad.dtype()), dense_y_grad.dims(), dense_y_grad.layout(), dense_y_grad.lod(), dense_y_grad.offset());
  argument_outputs.push_back(y_grad_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void DistGradOp::VerifySig() {}

void DistGradOp::InferMeta( phi::InferMetaContext *infer_meta ) {
  auto fn = PD_INFER_META(phi::GeneralBinaryGradInferMeta);
  fn(infer_meta);
}

phi::DataType DistGradOp::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: DistGradOp";
  


  return expected_kernel_dtype;
}

OpInfoTuple DotGradOp::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("x", "paddle::dialect::DenseTensorType", false, false, false, false), paddle::dialect::OpInputInfo("y", "paddle::dialect::DenseTensorType", false, false, false, false), paddle::dialect::OpInputInfo("out_grad", "paddle::dialect::DenseTensorType", false, false, false, false) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = {  };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("x_grad", "paddle::dialect::DenseTensorType", false, false), paddle::dialect::OpOutputInfo("y_grad", "paddle::dialect::DenseTensorType", false, false) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("GeneralBinaryGradInferMeta", {"x", "y"}, "dot_grad", {"x", "y", "out_grad"}, {"out_grad"}, {}, {}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "dot_grad");
}

void DotGradOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, pir::Value y_, pir::Value out_grad_) {
  VLOG(4) << "Start build DotGradOp";



  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_, y_, out_grad_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;
  paddle::dialect::DenseTensorType y = y_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)y;
  paddle::dialect::DenseTensorType out_grad = out_grad_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)out_grad;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);

  VLOG(4) << "Builder construction  dense_y";
  paddle::dialect::IrTensor ir_tensor_y(paddle::dialect::TransToPhiDataType(y.dtype()),
                                                      y.dims(),
                                                      y.data_layout(),
                                                      y.lod(),
                                                      y.offset());
  VLOG(4) << "Builder construction  meta_y";
  paddle::dialect::IrMetaTensor meta_y(&ir_tensor_y);
  paddle::dialect::IrTensor dense_x_grad;
  paddle::dialect::IrMetaTensor meta_x_grad(&dense_x_grad);
  paddle::dialect::IrTensor dense_y_grad;
  paddle::dialect::IrMetaTensor meta_y_grad(&dense_y_grad);

  phi::GeneralBinaryGradInferMeta(meta_x, meta_y, &meta_x_grad, &meta_y_grad);

  std::vector<pir::Type> argument_outputs;
  pir::Type x_grad_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_x_grad.dtype()), dense_x_grad.dims(), dense_x_grad.layout(), dense_x_grad.lod(), dense_x_grad.offset());
  argument_outputs.push_back(x_grad_dense_tensor_type);

  pir::Type y_grad_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_y_grad.dtype()), dense_y_grad.dims(), dense_y_grad.layout(), dense_y_grad.lod(), dense_y_grad.offset());
  argument_outputs.push_back(y_grad_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void DotGradOp::VerifySig() {}

void DotGradOp::InferMeta( phi::InferMetaContext *infer_meta ) {
  auto fn = PD_INFER_META(phi::GeneralBinaryGradInferMeta);
  fn(infer_meta);
}

phi::DataType DotGradOp::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: DotGradOp";
  


  return expected_kernel_dtype;
}

OpInfoTuple EigGradOp::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("out_w", "paddle::dialect::DenseTensorType", false, false, false, false), paddle::dialect::OpInputInfo("out_v", "paddle::dialect::DenseTensorType", false, false, false, false), paddle::dialect::OpInputInfo("out_w_grad", "paddle::dialect::DenseTensorType", false, false, false, false), paddle::dialect::OpInputInfo("out_v_grad", "paddle::dialect::DenseTensorType", false, false, false, false) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = {  };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("x_grad", "paddle::dialect::DenseTensorType", false, false) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("EigGradInferMeta", {"out_w", "out_v", "out_w_grad", "out_v_grad"}, "eig_grad", {"out_w", "out_v", "out_w_grad", "out_v_grad"}, {"out_v"}, {}, {}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "eig_grad");
}

void EigGradOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value out_w_, pir::Value out_v_, pir::Value out_w_grad_, pir::Value out_v_grad_) {
  VLOG(4) << "Start build EigGradOp";



  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {out_w_, out_v_, out_w_grad_, out_v_grad_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType out_w = out_w_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)out_w;
  paddle::dialect::DenseTensorType out_v = out_v_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)out_v;
  paddle::dialect::DenseTensorType out_w_grad = out_w_grad_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)out_w_grad;
  paddle::dialect::DenseTensorType out_v_grad = out_v_grad_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)out_v_grad;

  VLOG(4) << "Builder construction  dense_out_w";
  paddle::dialect::IrTensor ir_tensor_out_w(paddle::dialect::TransToPhiDataType(out_w.dtype()),
                                                      out_w.dims(),
                                                      out_w.data_layout(),
                                                      out_w.lod(),
                                                      out_w.offset());
  VLOG(4) << "Builder construction  meta_out_w";
  paddle::dialect::IrMetaTensor meta_out_w(&ir_tensor_out_w);

  VLOG(4) << "Builder construction  dense_out_v";
  paddle::dialect::IrTensor ir_tensor_out_v(paddle::dialect::TransToPhiDataType(out_v.dtype()),
                                                      out_v.dims(),
                                                      out_v.data_layout(),
                                                      out_v.lod(),
                                                      out_v.offset());
  VLOG(4) << "Builder construction  meta_out_v";
  paddle::dialect::IrMetaTensor meta_out_v(&ir_tensor_out_v);

  VLOG(4) << "Builder construction  dense_out_w_grad";
  paddle::dialect::IrTensor ir_tensor_out_w_grad(paddle::dialect::TransToPhiDataType(out_w_grad.dtype()),
                                                      out_w_grad.dims(),
                                                      out_w_grad.data_layout(),
                                                      out_w_grad.lod(),
                                                      out_w_grad.offset());
  VLOG(4) << "Builder construction  meta_out_w_grad";
  paddle::dialect::IrMetaTensor meta_out_w_grad(&ir_tensor_out_w_grad);

  VLOG(4) << "Builder construction  dense_out_v_grad";
  paddle::dialect::IrTensor ir_tensor_out_v_grad(paddle::dialect::TransToPhiDataType(out_v_grad.dtype()),
                                                      out_v_grad.dims(),
                                                      out_v_grad.data_layout(),
                                                      out_v_grad.lod(),
                                                      out_v_grad.offset());
  VLOG(4) << "Builder construction  meta_out_v_grad";
  paddle::dialect::IrMetaTensor meta_out_v_grad(&ir_tensor_out_v_grad);
  paddle::dialect::IrTensor dense_x_grad;
  paddle::dialect::IrMetaTensor meta_x_grad(&dense_x_grad);

  phi::EigGradInferMeta(meta_out_w, meta_out_v, meta_out_w_grad, meta_out_v_grad, &meta_x_grad);

  std::vector<pir::Type> argument_outputs;
  pir::Type x_grad_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_x_grad.dtype()), dense_x_grad.dims(), dense_x_grad.layout(), dense_x_grad.lod(), dense_x_grad.offset());
  argument_outputs.push_back(x_grad_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void EigGradOp::VerifySig() {}

void EigGradOp::InferMeta( phi::InferMetaContext *infer_meta ) {
  auto fn = PD_INFER_META(phi::EigGradInferMeta);
  fn(infer_meta);
}

phi::DataType EigGradOp::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: EigGradOp";
  


  return expected_kernel_dtype;
}

OpInfoTuple EighGradOp::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("out_w", "paddle::dialect::DenseTensorType", false, false, false, false), paddle::dialect::OpInputInfo("out_v", "paddle::dialect::DenseTensorType", false, false, false, false), paddle::dialect::OpInputInfo("out_w_grad", "paddle::dialect::DenseTensorType", false, false, false, false), paddle::dialect::OpInputInfo("out_v_grad", "paddle::dialect::DenseTensorType", false, false, false, false) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = {  };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("x_grad", "paddle::dialect::DenseTensorType", false, false) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("UnchangedInferMeta", {"out_v"}, "eigh_grad", {"out_w", "out_v", "out_w_grad", "out_v_grad"}, {"out_v"}, {}, {}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "eigh_grad");
}

void EighGradOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value out_w_, pir::Value out_v_, pir::Value out_w_grad_, pir::Value out_v_grad_) {
  VLOG(4) << "Start build EighGradOp";



  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {out_w_, out_v_, out_w_grad_, out_v_grad_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType out_w = out_w_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)out_w;
  paddle::dialect::DenseTensorType out_v = out_v_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)out_v;
  paddle::dialect::DenseTensorType out_w_grad = out_w_grad_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)out_w_grad;
  paddle::dialect::DenseTensorType out_v_grad = out_v_grad_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)out_v_grad;

  VLOG(4) << "Builder construction  dense_out_v";
  paddle::dialect::IrTensor ir_tensor_out_v(paddle::dialect::TransToPhiDataType(out_v.dtype()),
                                                      out_v.dims(),
                                                      out_v.data_layout(),
                                                      out_v.lod(),
                                                      out_v.offset());
  VLOG(4) << "Builder construction  meta_out_v";
  paddle::dialect::IrMetaTensor meta_out_v(&ir_tensor_out_v);
  paddle::dialect::IrTensor dense_x_grad;
  paddle::dialect::IrMetaTensor meta_x_grad(&dense_x_grad);

  phi::UnchangedInferMeta(meta_out_v, &meta_x_grad);

  std::vector<pir::Type> argument_outputs;
  pir::Type x_grad_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_x_grad.dtype()), dense_x_grad.dims(), dense_x_grad.layout(), dense_x_grad.lod(), dense_x_grad.offset());
  argument_outputs.push_back(x_grad_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void EighGradOp::VerifySig() {}

void EighGradOp::InferMeta( phi::InferMetaContext *infer_meta ) {
  auto fn = PD_INFER_META(phi::UnchangedInferMeta);
  fn(infer_meta);
}

phi::DataType EighGradOp::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: EighGradOp";
  


  return expected_kernel_dtype;
}

const char *EigvalshGradOp::attributes_name[2] = { "uplo", "is_test" };

OpInfoTuple EigvalshGradOp::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("eigenvectors", "paddle::dialect::DenseTensorType", false, false, false, false), paddle::dialect::OpInputInfo("eigenvalues_grad", "paddle::dialect::DenseTensorType", false, false, false, false) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = { paddle::dialect::OpAttributeInfo("uplo", "pir::StrAttribute", ""), paddle::dialect::OpAttributeInfo("is_test", "pir::BoolAttribute", "") };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("x_grad", "paddle::dialect::DenseTensorType", false, false) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("EigvalshGradInferMeta", {"eigenvectors", "eigenvalues_grad", "uplo", "is_test"}, "eigvalsh_grad", {"eigenvectors", "eigenvalues_grad", "uplo", "is_test"}, {"eigenvectors"}, {}, {}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "eigvalsh_grad");
}

void EigvalshGradOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value eigenvectors_, pir::Value eigenvalues_grad_, const std::string& uplo, bool is_test) {
  VLOG(4) << "Start build EigvalshGradOp";



  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {eigenvectors_, eigenvalues_grad_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_uplo = pir::StrAttribute::get(pir::IrContext::Instance(), uplo);
  argument.AddAttribute("uplo", attr_uplo);
  pir::Attribute attr_is_test = pir::BoolAttribute::get(pir::IrContext::Instance(), is_test);
  argument.AddAttribute("is_test", attr_is_test);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType eigenvectors = eigenvectors_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)eigenvectors;
  paddle::dialect::DenseTensorType eigenvalues_grad = eigenvalues_grad_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)eigenvalues_grad;

  VLOG(4) << "Builder construction  dense_eigenvectors";
  paddle::dialect::IrTensor ir_tensor_eigenvectors(paddle::dialect::TransToPhiDataType(eigenvectors.dtype()),
                                                      eigenvectors.dims(),
                                                      eigenvectors.data_layout(),
                                                      eigenvectors.lod(),
                                                      eigenvectors.offset());
  VLOG(4) << "Builder construction  meta_eigenvectors";
  paddle::dialect::IrMetaTensor meta_eigenvectors(&ir_tensor_eigenvectors);

  VLOG(4) << "Builder construction  dense_eigenvalues_grad";
  paddle::dialect::IrTensor ir_tensor_eigenvalues_grad(paddle::dialect::TransToPhiDataType(eigenvalues_grad.dtype()),
                                                      eigenvalues_grad.dims(),
                                                      eigenvalues_grad.data_layout(),
                                                      eigenvalues_grad.lod(),
                                                      eigenvalues_grad.offset());
  VLOG(4) << "Builder construction  meta_eigenvalues_grad";
  paddle::dialect::IrMetaTensor meta_eigenvalues_grad(&ir_tensor_eigenvalues_grad);
  paddle::dialect::IrTensor dense_x_grad;
  paddle::dialect::IrMetaTensor meta_x_grad(&dense_x_grad);

  phi::EigvalshGradInferMeta(meta_eigenvectors, meta_eigenvalues_grad, uplo, is_test, &meta_x_grad);

  std::vector<pir::Type> argument_outputs;
  pir::Type x_grad_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_x_grad.dtype()), dense_x_grad.dims(), dense_x_grad.layout(), dense_x_grad.lod(), dense_x_grad.offset());
  argument_outputs.push_back(x_grad_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void EigvalshGradOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value eigenvectors_, pir::Value eigenvalues_grad_, pir::AttributeMap attributes) {
  VLOG(4) << "Start build EigvalshGradOp";


  IR_ENFORCE(
      attributes.find("uplo") != attributes.end(),
          "'uplo' Attribute is expected for EigvalshGradOp. ");
  std::string uplo = attributes.at("uplo").dyn_cast<pir::StrAttribute>().AsString();

  IR_ENFORCE(
      attributes.find("is_test") != attributes.end(),
          "'is_test' Attribute is expected for EigvalshGradOp. ");
  bool is_test = attributes.at("is_test").dyn_cast<pir::BoolAttribute>().data();


  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {eigenvectors_, eigenvalues_grad_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_uplo = pir::StrAttribute::get(pir::IrContext::Instance(), uplo);
  argument.AddAttribute("uplo", attr_uplo);
  pir::Attribute attr_is_test = pir::BoolAttribute::get(pir::IrContext::Instance(), is_test);
  argument.AddAttribute("is_test", attr_is_test);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType eigenvectors = eigenvectors_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)eigenvectors;
  paddle::dialect::DenseTensorType eigenvalues_grad = eigenvalues_grad_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)eigenvalues_grad;

  VLOG(4) << "Builder construction  dense_eigenvectors";
  paddle::dialect::IrTensor ir_tensor_eigenvectors(paddle::dialect::TransToPhiDataType(eigenvectors.dtype()),
                                                      eigenvectors.dims(),
                                                      eigenvectors.data_layout(),
                                                      eigenvectors.lod(),
                                                      eigenvectors.offset());
  VLOG(4) << "Builder construction  meta_eigenvectors";
  paddle::dialect::IrMetaTensor meta_eigenvectors(&ir_tensor_eigenvectors);

  VLOG(4) << "Builder construction  dense_eigenvalues_grad";
  paddle::dialect::IrTensor ir_tensor_eigenvalues_grad(paddle::dialect::TransToPhiDataType(eigenvalues_grad.dtype()),
                                                      eigenvalues_grad.dims(),
                                                      eigenvalues_grad.data_layout(),
                                                      eigenvalues_grad.lod(),
                                                      eigenvalues_grad.offset());
  VLOG(4) << "Builder construction  meta_eigenvalues_grad";
  paddle::dialect::IrMetaTensor meta_eigenvalues_grad(&ir_tensor_eigenvalues_grad);
  paddle::dialect::IrTensor dense_x_grad;
  paddle::dialect::IrMetaTensor meta_x_grad(&dense_x_grad);

  phi::EigvalshGradInferMeta(meta_eigenvectors, meta_eigenvalues_grad, uplo, is_test, &meta_x_grad);

  std::vector<pir::Type> argument_outputs;
  pir::Type x_grad_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_x_grad.dtype()), dense_x_grad.dims(), dense_x_grad.layout(), dense_x_grad.lod(), dense_x_grad.offset());
  argument_outputs.push_back(x_grad_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void EigvalshGradOp::VerifySig() {}

void EigvalshGradOp::InferMeta( phi::InferMetaContext *infer_meta ) {
  auto fn = PD_INFER_META(phi::EigvalshGradInferMeta);
  fn(infer_meta);
}

phi::DataType EigvalshGradOp::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: EigvalshGradOp";
  


  return expected_kernel_dtype;
}

const char *EluDoubleGradOp::attributes_name[1] = { "alpha" };

OpInfoTuple EluDoubleGradOp::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("x", "paddle::dialect::DenseTensorType", false, false, false, false), paddle::dialect::OpInputInfo("grad_out", "paddle::dialect::DenseTensorType", false, false, false, false), paddle::dialect::OpInputInfo("grad_x_grad", "paddle::dialect::DenseTensorType", false, false, false, false) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = { paddle::dialect::OpAttributeInfo("alpha", "pir::FloatAttribute", "") };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("x_grad", "paddle::dialect::DenseTensorType", false, false), paddle::dialect::OpOutputInfo("grad_out_grad", "paddle::dialect::DenseTensorType", false, false) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("GeneralBinaryGradInferMeta", {"x", "x"}, "elu_double_grad", {"x", "grad_out", "grad_x_grad", "alpha"}, {}, {}, {}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "elu_double_grad");
}

void EluDoubleGradOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, pir::Value grad_out_, pir::Value grad_x_grad_, float alpha) {
  VLOG(4) << "Start build EluDoubleGradOp";



  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_, grad_out_, grad_x_grad_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_alpha = pir::FloatAttribute::get(pir::IrContext::Instance(), alpha);
  argument.AddAttribute("alpha", attr_alpha);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;
  paddle::dialect::DenseTensorType grad_out = grad_out_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)grad_out;
  paddle::dialect::DenseTensorType grad_x_grad = grad_x_grad_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)grad_x_grad;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);
  paddle::dialect::IrTensor dense_x_grad;
  paddle::dialect::IrMetaTensor meta_x_grad(&dense_x_grad);
  paddle::dialect::IrTensor dense_grad_out_grad;
  paddle::dialect::IrMetaTensor meta_grad_out_grad(&dense_grad_out_grad);

  phi::GeneralBinaryGradInferMeta(meta_x, meta_x, &meta_x_grad, &meta_grad_out_grad);

  std::vector<pir::Type> argument_outputs;
  pir::Type x_grad_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_x_grad.dtype()), dense_x_grad.dims(), dense_x_grad.layout(), dense_x_grad.lod(), dense_x_grad.offset());
  argument_outputs.push_back(x_grad_dense_tensor_type);

  pir::Type grad_out_grad_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_grad_out_grad.dtype()), dense_grad_out_grad.dims(), dense_grad_out_grad.layout(), dense_grad_out_grad.lod(), dense_grad_out_grad.offset());
  argument_outputs.push_back(grad_out_grad_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void EluDoubleGradOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, pir::Value grad_out_, pir::Value grad_x_grad_, pir::AttributeMap attributes) {
  VLOG(4) << "Start build EluDoubleGradOp";


  IR_ENFORCE(
      attributes.find("alpha") != attributes.end(),
          "'alpha' Attribute is expected for EluDoubleGradOp. ");
  float alpha = attributes.at("alpha").dyn_cast<pir::FloatAttribute>().data();


  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_, grad_out_, grad_x_grad_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_alpha = pir::FloatAttribute::get(pir::IrContext::Instance(), alpha);
  argument.AddAttribute("alpha", attr_alpha);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;
  paddle::dialect::DenseTensorType grad_out = grad_out_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)grad_out;
  paddle::dialect::DenseTensorType grad_x_grad = grad_x_grad_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)grad_x_grad;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);
  paddle::dialect::IrTensor dense_x_grad;
  paddle::dialect::IrMetaTensor meta_x_grad(&dense_x_grad);
  paddle::dialect::IrTensor dense_grad_out_grad;
  paddle::dialect::IrMetaTensor meta_grad_out_grad(&dense_grad_out_grad);

  phi::GeneralBinaryGradInferMeta(meta_x, meta_x, &meta_x_grad, &meta_grad_out_grad);

  std::vector<pir::Type> argument_outputs;
  pir::Type x_grad_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_x_grad.dtype()), dense_x_grad.dims(), dense_x_grad.layout(), dense_x_grad.lod(), dense_x_grad.offset());
  argument_outputs.push_back(x_grad_dense_tensor_type);

  pir::Type grad_out_grad_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_grad_out_grad.dtype()), dense_grad_out_grad.dims(), dense_grad_out_grad.layout(), dense_grad_out_grad.lod(), dense_grad_out_grad.offset());
  argument_outputs.push_back(grad_out_grad_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void EluDoubleGradOp::VerifySig() {}

void EluDoubleGradOp::InferMeta( phi::InferMetaContext *infer_meta ) {
  auto fn = PD_INFER_META(phi::GeneralBinaryGradInferMeta);
  fn(infer_meta);
}

phi::DataType EluDoubleGradOp::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: EluDoubleGradOp";
  


  return expected_kernel_dtype;
}

const char *EluDoubleGrad_Op::attributes_name[1] = { "alpha" };

OpInfoTuple EluDoubleGrad_Op::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("x", "paddle::dialect::DenseTensorType", false, false, false, false), paddle::dialect::OpInputInfo("grad_out", "paddle::dialect::DenseTensorType", false, false, false, false), paddle::dialect::OpInputInfo("grad_x_grad", "paddle::dialect::DenseTensorType", false, false, false, false) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = { paddle::dialect::OpAttributeInfo("alpha", "pir::FloatAttribute", "") };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("x_grad", "paddle::dialect::DenseTensorType", false, false), paddle::dialect::OpOutputInfo("grad_out_grad", "paddle::dialect::DenseTensorType", false, false) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("GeneralBinaryGradInferMeta", {"x", "x"}, "elu_double_grad", {"x", "grad_out", "grad_x_grad", "alpha"}, {}, {}, {{"grad_out_grad", "grad_x_grad"}}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "elu_double_grad");
}

void EluDoubleGrad_Op::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, pir::Value grad_out_, pir::Value grad_x_grad_, float alpha) {
  VLOG(4) << "Start build EluDoubleGrad_Op";



  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_, grad_out_, grad_x_grad_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_alpha = pir::FloatAttribute::get(pir::IrContext::Instance(), alpha);
  argument.AddAttribute("alpha", attr_alpha);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;
  paddle::dialect::DenseTensorType grad_out = grad_out_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)grad_out;
  paddle::dialect::DenseTensorType grad_x_grad = grad_x_grad_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)grad_x_grad;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);
  paddle::dialect::IrTensor dense_x_grad;
  paddle::dialect::IrMetaTensor meta_x_grad(&dense_x_grad);
  paddle::dialect::IrTensor dense_grad_out_grad;
  paddle::dialect::IrMetaTensor meta_grad_out_grad(&dense_grad_out_grad);

  phi::GeneralBinaryGradInferMeta(meta_x, meta_x, &meta_x_grad, &meta_grad_out_grad);

  std::vector<pir::Type> argument_outputs;
  pir::Type x_grad_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_x_grad.dtype()), dense_x_grad.dims(), dense_x_grad.layout(), dense_x_grad.lod(), dense_x_grad.offset());
  argument_outputs.push_back(x_grad_dense_tensor_type);

  pir::Type grad_out_grad_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_grad_out_grad.dtype()), dense_grad_out_grad.dims(), dense_grad_out_grad.layout(), dense_grad_out_grad.lod(), dense_grad_out_grad.offset());
  argument_outputs.push_back(grad_out_grad_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void EluDoubleGrad_Op::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, pir::Value grad_out_, pir::Value grad_x_grad_, pir::AttributeMap attributes) {
  VLOG(4) << "Start build EluDoubleGrad_Op";


  IR_ENFORCE(
      attributes.find("alpha") != attributes.end(),
          "'alpha' Attribute is expected for EluDoubleGrad_Op. ");
  float alpha = attributes.at("alpha").dyn_cast<pir::FloatAttribute>().data();


  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_, grad_out_, grad_x_grad_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_alpha = pir::FloatAttribute::get(pir::IrContext::Instance(), alpha);
  argument.AddAttribute("alpha", attr_alpha);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;
  paddle::dialect::DenseTensorType grad_out = grad_out_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)grad_out;
  paddle::dialect::DenseTensorType grad_x_grad = grad_x_grad_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)grad_x_grad;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);
  paddle::dialect::IrTensor dense_x_grad;
  paddle::dialect::IrMetaTensor meta_x_grad(&dense_x_grad);
  paddle::dialect::IrTensor dense_grad_out_grad;
  paddle::dialect::IrMetaTensor meta_grad_out_grad(&dense_grad_out_grad);

  phi::GeneralBinaryGradInferMeta(meta_x, meta_x, &meta_x_grad, &meta_grad_out_grad);

  std::vector<pir::Type> argument_outputs;
  pir::Type x_grad_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_x_grad.dtype()), dense_x_grad.dims(), dense_x_grad.layout(), dense_x_grad.lod(), dense_x_grad.offset());
  argument_outputs.push_back(x_grad_dense_tensor_type);

  pir::Type grad_out_grad_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_grad_out_grad.dtype()), dense_grad_out_grad.dims(), dense_grad_out_grad.layout(), dense_grad_out_grad.lod(), dense_grad_out_grad.offset());
  argument_outputs.push_back(grad_out_grad_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void EluDoubleGrad_Op::VerifySig() {}

void EluDoubleGrad_Op::InferMeta( phi::InferMetaContext *infer_meta ) {
  auto fn = PD_INFER_META(phi::GeneralBinaryGradInferMeta);
  fn(infer_meta);
}

phi::DataType EluDoubleGrad_Op::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: EluDoubleGrad_Op";
  


  return expected_kernel_dtype;
}

const char *EluGradOp::attributes_name[1] = { "alpha" };

OpInfoTuple EluGradOp::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("x", "paddle::dialect::DenseTensorType", false, false, false, true), paddle::dialect::OpInputInfo("out", "paddle::dialect::DenseTensorType", false, false, false, false), paddle::dialect::OpInputInfo("out_grad", "paddle::dialect::DenseTensorType", false, false, false, true) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = { paddle::dialect::OpAttributeInfo("alpha", "pir::FloatAttribute", "") };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("x_grad", "paddle::dialect::DenseTensorType", false, false) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("UnchangedInferMeta", {"x"}, "elu_grad", {"x", "out", "out_grad", "alpha"}, {}, {}, {}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "elu_grad");
}

void EluGradOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, pir::Value out_, pir::Value out_grad_, float alpha) {
  VLOG(4) << "Start build EluGradOp";



  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_, out_, out_grad_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_alpha = pir::FloatAttribute::get(pir::IrContext::Instance(), alpha);
  argument.AddAttribute("alpha", attr_alpha);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;
  paddle::dialect::DenseTensorType out = out_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)out;
  paddle::dialect::DenseTensorType out_grad = out_grad_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)out_grad;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);
  paddle::dialect::IrTensor dense_x_grad;
  paddle::dialect::IrMetaTensor meta_x_grad(&dense_x_grad);

  phi::UnchangedInferMeta(meta_x, &meta_x_grad);

  std::vector<pir::Type> argument_outputs;
  pir::Type x_grad_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_x_grad.dtype()), dense_x_grad.dims(), dense_x_grad.layout(), dense_x_grad.lod(), dense_x_grad.offset());
  argument_outputs.push_back(x_grad_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void EluGradOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, pir::Value out_, pir::Value out_grad_, pir::AttributeMap attributes) {
  VLOG(4) << "Start build EluGradOp";


  IR_ENFORCE(
      attributes.find("alpha") != attributes.end(),
          "'alpha' Attribute is expected for EluGradOp. ");
  float alpha = attributes.at("alpha").dyn_cast<pir::FloatAttribute>().data();


  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_, out_, out_grad_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_alpha = pir::FloatAttribute::get(pir::IrContext::Instance(), alpha);
  argument.AddAttribute("alpha", attr_alpha);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;
  paddle::dialect::DenseTensorType out = out_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)out;
  paddle::dialect::DenseTensorType out_grad = out_grad_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)out_grad;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);
  paddle::dialect::IrTensor dense_x_grad;
  paddle::dialect::IrMetaTensor meta_x_grad(&dense_x_grad);

  phi::UnchangedInferMeta(meta_x, &meta_x_grad);

  std::vector<pir::Type> argument_outputs;
  pir::Type x_grad_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_x_grad.dtype()), dense_x_grad.dims(), dense_x_grad.layout(), dense_x_grad.lod(), dense_x_grad.offset());
  argument_outputs.push_back(x_grad_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void EluGradOp::VerifySig() {}

void EluGradOp::InferMeta( phi::InferMetaContext *infer_meta ) {
  auto fn = PD_INFER_META(phi::UnchangedInferMeta);
  fn(infer_meta);
}

phi::DataType EluGradOp::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: EluGradOp";
  


  return expected_kernel_dtype;
}

const char *EluGrad_Op::attributes_name[1] = { "alpha" };

OpInfoTuple EluGrad_Op::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("x", "paddle::dialect::DenseTensorType", false, false, false, true), paddle::dialect::OpInputInfo("out", "paddle::dialect::DenseTensorType", false, false, false, false), paddle::dialect::OpInputInfo("out_grad", "paddle::dialect::DenseTensorType", false, false, false, true) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = { paddle::dialect::OpAttributeInfo("alpha", "pir::FloatAttribute", "") };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("x_grad", "paddle::dialect::DenseTensorType", false, false) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("UnchangedInferMeta", {"x"}, "elu_grad", {"x", "out", "out_grad", "alpha"}, {}, {}, {{"x_grad", "out_grad"}}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "elu_grad");
}

void EluGrad_Op::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, pir::Value out_, pir::Value out_grad_, float alpha) {
  VLOG(4) << "Start build EluGrad_Op";



  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_, out_, out_grad_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_alpha = pir::FloatAttribute::get(pir::IrContext::Instance(), alpha);
  argument.AddAttribute("alpha", attr_alpha);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;
  paddle::dialect::DenseTensorType out = out_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)out;
  paddle::dialect::DenseTensorType out_grad = out_grad_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)out_grad;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);
  paddle::dialect::IrTensor dense_x_grad;
  paddle::dialect::IrMetaTensor meta_x_grad(&dense_x_grad);

  phi::UnchangedInferMeta(meta_x, &meta_x_grad);

  std::vector<pir::Type> argument_outputs;
  pir::Type x_grad_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_x_grad.dtype()), dense_x_grad.dims(), dense_x_grad.layout(), dense_x_grad.lod(), dense_x_grad.offset());
  argument_outputs.push_back(x_grad_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void EluGrad_Op::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, pir::Value out_, pir::Value out_grad_, pir::AttributeMap attributes) {
  VLOG(4) << "Start build EluGrad_Op";


  IR_ENFORCE(
      attributes.find("alpha") != attributes.end(),
          "'alpha' Attribute is expected for EluGrad_Op. ");
  float alpha = attributes.at("alpha").dyn_cast<pir::FloatAttribute>().data();


  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_, out_, out_grad_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_alpha = pir::FloatAttribute::get(pir::IrContext::Instance(), alpha);
  argument.AddAttribute("alpha", attr_alpha);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;
  paddle::dialect::DenseTensorType out = out_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)out;
  paddle::dialect::DenseTensorType out_grad = out_grad_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)out_grad;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);
  paddle::dialect::IrTensor dense_x_grad;
  paddle::dialect::IrMetaTensor meta_x_grad(&dense_x_grad);

  phi::UnchangedInferMeta(meta_x, &meta_x_grad);

  std::vector<pir::Type> argument_outputs;
  pir::Type x_grad_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_x_grad.dtype()), dense_x_grad.dims(), dense_x_grad.layout(), dense_x_grad.lod(), dense_x_grad.offset());
  argument_outputs.push_back(x_grad_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void EluGrad_Op::VerifySig() {}

void EluGrad_Op::InferMeta( phi::InferMetaContext *infer_meta ) {
  auto fn = PD_INFER_META(phi::UnchangedInferMeta);
  fn(infer_meta);
}

phi::DataType EluGrad_Op::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: EluGrad_Op";
  


  return expected_kernel_dtype;
}

OpInfoTuple ErfGradOp::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("x", "paddle::dialect::DenseTensorType", false, false, false, false), paddle::dialect::OpInputInfo("out_grad", "paddle::dialect::DenseTensorType", false, false, false, false) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = {  };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("x_grad", "paddle::dialect::DenseTensorType", false, false) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("UnchangedInferMeta", {"x"}, "erf_grad", {"x", "out_grad"}, {"out_grad"}, {}, {}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "erf_grad");
}

void ErfGradOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, pir::Value out_grad_) {
  VLOG(4) << "Start build ErfGradOp";



  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_, out_grad_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;
  paddle::dialect::DenseTensorType out_grad = out_grad_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)out_grad;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);
  paddle::dialect::IrTensor dense_x_grad;
  paddle::dialect::IrMetaTensor meta_x_grad(&dense_x_grad);

  phi::UnchangedInferMeta(meta_x, &meta_x_grad);

  std::vector<pir::Type> argument_outputs;
  pir::Type x_grad_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_x_grad.dtype()), dense_x_grad.dims(), dense_x_grad.layout(), dense_x_grad.lod(), dense_x_grad.offset());
  argument_outputs.push_back(x_grad_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void ErfGradOp::VerifySig() {}

void ErfGradOp::InferMeta( phi::InferMetaContext *infer_meta ) {
  auto fn = PD_INFER_META(phi::UnchangedInferMeta);
  fn(infer_meta);
}

phi::DataType ErfGradOp::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: ErfGradOp";
  


  return expected_kernel_dtype;
}

OpInfoTuple ErfinvGradOp::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("out", "paddle::dialect::DenseTensorType", false, false, false, false), paddle::dialect::OpInputInfo("out_grad", "paddle::dialect::DenseTensorType", false, false, false, false) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = {  };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("x_grad", "paddle::dialect::DenseTensorType", false, false) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("UnchangedInferMeta", {"out"}, "erfinv_grad", {"out", "out_grad"}, {}, {}, {}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "erfinv_grad");
}

void ErfinvGradOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value out_, pir::Value out_grad_) {
  VLOG(4) << "Start build ErfinvGradOp";



  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {out_, out_grad_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType out = out_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)out;
  paddle::dialect::DenseTensorType out_grad = out_grad_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)out_grad;

  VLOG(4) << "Builder construction  dense_out";
  paddle::dialect::IrTensor ir_tensor_out(paddle::dialect::TransToPhiDataType(out.dtype()),
                                                      out.dims(),
                                                      out.data_layout(),
                                                      out.lod(),
                                                      out.offset());
  VLOG(4) << "Builder construction  meta_out";
  paddle::dialect::IrMetaTensor meta_out(&ir_tensor_out);
  paddle::dialect::IrTensor dense_x_grad;
  paddle::dialect::IrMetaTensor meta_x_grad(&dense_x_grad);

  phi::UnchangedInferMeta(meta_out, &meta_x_grad);

  std::vector<pir::Type> argument_outputs;
  pir::Type x_grad_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_x_grad.dtype()), dense_x_grad.dims(), dense_x_grad.layout(), dense_x_grad.lod(), dense_x_grad.offset());
  argument_outputs.push_back(x_grad_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void ErfinvGradOp::VerifySig() {}

void ErfinvGradOp::InferMeta( phi::InferMetaContext *infer_meta ) {
  auto fn = PD_INFER_META(phi::UnchangedInferMeta);
  fn(infer_meta);
}

phi::DataType ErfinvGradOp::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: ErfinvGradOp";
  


  return expected_kernel_dtype;
}

OpInfoTuple ExpGradOp::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("out", "paddle::dialect::DenseTensorType", false, false, false, false), paddle::dialect::OpInputInfo("out_grad", "paddle::dialect::DenseTensorType", false, false, false, false) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = {  };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("x_grad", "paddle::dialect::DenseTensorType", false, false) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("UnchangedInferMeta", {"out"}, "exp_grad", {"out", "out_grad"}, {}, {}, {}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "exp_grad");
}

void ExpGradOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value out_, pir::Value out_grad_) {
  VLOG(4) << "Start build ExpGradOp";



  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {out_, out_grad_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType out = out_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)out;
  paddle::dialect::DenseTensorType out_grad = out_grad_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)out_grad;

  VLOG(4) << "Builder construction  dense_out";
  paddle::dialect::IrTensor ir_tensor_out(paddle::dialect::TransToPhiDataType(out.dtype()),
                                                      out.dims(),
                                                      out.data_layout(),
                                                      out.lod(),
                                                      out.offset());
  VLOG(4) << "Builder construction  meta_out";
  paddle::dialect::IrMetaTensor meta_out(&ir_tensor_out);
  paddle::dialect::IrTensor dense_x_grad;
  paddle::dialect::IrMetaTensor meta_x_grad(&dense_x_grad);

  phi::UnchangedInferMeta(meta_out, &meta_x_grad);

  std::vector<pir::Type> argument_outputs;
  pir::Type x_grad_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_x_grad.dtype()), dense_x_grad.dims(), dense_x_grad.layout(), dense_x_grad.lod(), dense_x_grad.offset());
  argument_outputs.push_back(x_grad_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void ExpGradOp::VerifySig() {}

void ExpGradOp::InferMeta( phi::InferMetaContext *infer_meta ) {
  auto fn = PD_INFER_META(phi::UnchangedInferMeta);
  fn(infer_meta);
}

phi::DataType ExpGradOp::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: ExpGradOp";
  


  return expected_kernel_dtype;
}

OpInfoTuple ExpGrad_Op::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("out", "paddle::dialect::DenseTensorType", false, false, false, false), paddle::dialect::OpInputInfo("out_grad", "paddle::dialect::DenseTensorType", false, false, false, false) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = {  };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("x_grad", "paddle::dialect::DenseTensorType", false, false) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("UnchangedInferMeta", {"out"}, "exp_grad", {"out", "out_grad"}, {}, {}, {{"x_grad", "out_grad"}}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "exp_grad");
}

void ExpGrad_Op::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value out_, pir::Value out_grad_) {
  VLOG(4) << "Start build ExpGrad_Op";



  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {out_, out_grad_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType out = out_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)out;
  paddle::dialect::DenseTensorType out_grad = out_grad_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)out_grad;

  VLOG(4) << "Builder construction  dense_out";
  paddle::dialect::IrTensor ir_tensor_out(paddle::dialect::TransToPhiDataType(out.dtype()),
                                                      out.dims(),
                                                      out.data_layout(),
                                                      out.lod(),
                                                      out.offset());
  VLOG(4) << "Builder construction  meta_out";
  paddle::dialect::IrMetaTensor meta_out(&ir_tensor_out);
  paddle::dialect::IrTensor dense_x_grad;
  paddle::dialect::IrMetaTensor meta_x_grad(&dense_x_grad);

  phi::UnchangedInferMeta(meta_out, &meta_x_grad);

  std::vector<pir::Type> argument_outputs;
  pir::Type x_grad_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_x_grad.dtype()), dense_x_grad.dims(), dense_x_grad.layout(), dense_x_grad.lod(), dense_x_grad.offset());
  argument_outputs.push_back(x_grad_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void ExpGrad_Op::VerifySig() {}

void ExpGrad_Op::InferMeta( phi::InferMetaContext *infer_meta ) {
  auto fn = PD_INFER_META(phi::UnchangedInferMeta);
  fn(infer_meta);
}

phi::DataType ExpGrad_Op::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: ExpGrad_Op";
  


  return expected_kernel_dtype;
}

const char *ExpandAsGradOp::attributes_name[1] = { "target_shape" };

OpInfoTuple ExpandAsGradOp::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("x", "paddle::dialect::DenseTensorType", false, true, false, false), paddle::dialect::OpInputInfo("out_grad", "paddle::dialect::DenseTensorType", false, false, false, false) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = { paddle::dialect::OpAttributeInfo("target_shape", "pir::ArrayAttribute<pir::Int32Attribute>", "") };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("x_grad", "paddle::dialect::DenseTensorType", false, false) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("UnchangedInferMeta", {"x"}, "expand_as_grad", {"x", "out_grad", "target_shape"}, {}, {}, {}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "expand_as_grad");
}

void ExpandAsGradOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, pir::Value out_grad_, const std::vector<int>& target_shape) {
  VLOG(4) << "Start build ExpandAsGradOp";



  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_, out_grad_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  std::vector<pir::Attribute> vec_target_shape;
  for (size_t i = 0; i < static_cast<size_t>(target_shape.size()); i++) {
      pir::Attribute attr_target_shape = pir::Int32Attribute::get(pir::IrContext::Instance(), target_shape[i]);

    vec_target_shape.push_back(attr_target_shape);
  }
  pir::Attribute attr_target_shape = pir::ArrayAttribute::get(pir::IrContext::Instance(), vec_target_shape);
  argument.AddAttribute("target_shape", attr_target_shape);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;
  paddle::dialect::DenseTensorType out_grad = out_grad_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)out_grad;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);
  paddle::dialect::IrTensor dense_x_grad;
  paddle::dialect::IrMetaTensor meta_x_grad(&dense_x_grad);

  phi::UnchangedInferMeta(meta_x, &meta_x_grad);

  std::vector<pir::Type> argument_outputs;
  pir::Type x_grad_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_x_grad.dtype()), dense_x_grad.dims(), dense_x_grad.layout(), dense_x_grad.lod(), dense_x_grad.offset());
  argument_outputs.push_back(x_grad_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void ExpandAsGradOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, pir::Value out_grad_, pir::AttributeMap attributes) {
  VLOG(4) << "Start build ExpandAsGradOp";


  IR_ENFORCE(
      attributes.find("target_shape") != attributes.end(),
          "'target_shape' Attribute is expected for ExpandAsGradOp. ");
  std::vector<int> target_shape;
  for (size_t i = 0; i < attributes.at("target_shape").dyn_cast<pir::ArrayAttribute>().size(); i++) {
    target_shape.push_back(attributes.at("target_shape").dyn_cast<pir::ArrayAttribute>().at(i).dyn_cast<pir::Int32Attribute>().data());
  }


  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_, out_grad_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  std::vector<pir::Attribute> vec_target_shape;
  for (size_t i = 0; i < static_cast<size_t>(target_shape.size()); i++) {
      pir::Attribute attr_target_shape = pir::Int32Attribute::get(pir::IrContext::Instance(), target_shape[i]);

    vec_target_shape.push_back(attr_target_shape);
  }
  pir::Attribute attr_target_shape = pir::ArrayAttribute::get(pir::IrContext::Instance(), vec_target_shape);
  argument.AddAttribute("target_shape", attr_target_shape);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;
  paddle::dialect::DenseTensorType out_grad = out_grad_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)out_grad;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);
  paddle::dialect::IrTensor dense_x_grad;
  paddle::dialect::IrMetaTensor meta_x_grad(&dense_x_grad);

  phi::UnchangedInferMeta(meta_x, &meta_x_grad);

  std::vector<pir::Type> argument_outputs;
  pir::Type x_grad_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_x_grad.dtype()), dense_x_grad.dims(), dense_x_grad.layout(), dense_x_grad.lod(), dense_x_grad.offset());
  argument_outputs.push_back(x_grad_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void ExpandAsGradOp::VerifySig() {}

void ExpandAsGradOp::InferMeta( phi::InferMetaContext *infer_meta ) {
  auto fn = PD_INFER_META(phi::UnchangedInferMeta);
  fn(infer_meta);
}

phi::DataType ExpandAsGradOp::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: ExpandAsGradOp";
  


  return expected_kernel_dtype;
}

OpInfoTuple ExpandDoubleGradOp::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("grad_x_grad", "paddle::dialect::DenseTensorType", false, false, false, false), paddle::dialect::OpInputInfo("shape", "paddle::dialect::IntArrayAttribute", false, false, true, false) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = {  };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("grad_out_grad", "paddle::dialect::DenseTensorType", false, false) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("", {""}, "", {""}, {}, {}, {}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "expand_double_grad");
}

void ExpandDoubleGradOp::VerifySig() {}

phi::DataType ExpandDoubleGradOp::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: ExpandDoubleGradOp";
  


  return expected_kernel_dtype;
}

OpInfoTuple ExpandGradOp::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("x", "paddle::dialect::DenseTensorType", false, true, false, false), paddle::dialect::OpInputInfo("out_grad", "paddle::dialect::DenseTensorType", false, false, false, true), paddle::dialect::OpInputInfo("shape", "paddle::dialect::IntArrayAttribute", false, false, true, false) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = {  };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("x_grad", "paddle::dialect::DenseTensorType", false, false) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("UnchangedInferMeta", {"x"}, "expand_grad", {"x", "out_grad", "shape"}, {"out_grad"}, {}, {}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "expand_grad");
}

void ExpandGradOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, pir::Value out_grad_, const std::vector<int64_t>& shape) {
  VLOG(4) << "Start build ExpandGradOp";


  // Generate int_array mutable attribute: shape
  paddle::dialect::FullIntArrayOp full_shape_op = builder.Build<paddle::dialect::FullIntArrayOp>(shape, phi::DataType::INT64, phi::CPUPlace());
  pir::OpResult shape_ = full_shape_op->result(0);
    
  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_, out_grad_, shape_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;
  paddle::dialect::DenseTensorType out_grad = out_grad_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)out_grad;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);
  paddle::dialect::IrTensor dense_x_grad;
  paddle::dialect::IrMetaTensor meta_x_grad(&dense_x_grad);

  phi::UnchangedInferMeta(meta_x, &meta_x_grad);

  std::vector<pir::Type> argument_outputs;
  pir::Type x_grad_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_x_grad.dtype()), dense_x_grad.dims(), dense_x_grad.layout(), dense_x_grad.lod(), dense_x_grad.offset());
  argument_outputs.push_back(x_grad_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void ExpandGradOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, pir::Value out_grad_, pir::AttributeMap attributes) {
  VLOG(4) << "Start build ExpandGradOp";


  IR_ENFORCE(
      attributes.find("shape") != attributes.end(),
          "'shape' Attribute is expected for ExpandGradOp. ");
  std::vector<int64_t> shape = attributes.at("shape").dyn_cast<paddle::dialect::IntArrayAttribute>().data().GetData();

  // Generate int_array mutable attribute: shape
  paddle::dialect::FullIntArrayOp full_shape_op = builder.Build<paddle::dialect::FullIntArrayOp>(shape, phi::DataType::INT64, phi::CPUPlace());
  pir::OpResult shape_ = full_shape_op->result(0);
    
  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_, out_grad_, shape_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;
  paddle::dialect::DenseTensorType out_grad = out_grad_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)out_grad;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);
  paddle::dialect::IrTensor dense_x_grad;
  paddle::dialect::IrMetaTensor meta_x_grad(&dense_x_grad);

  phi::UnchangedInferMeta(meta_x, &meta_x_grad);

  std::vector<pir::Type> argument_outputs;
  pir::Type x_grad_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_x_grad.dtype()), dense_x_grad.dims(), dense_x_grad.layout(), dense_x_grad.lod(), dense_x_grad.offset());
  argument_outputs.push_back(x_grad_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void ExpandGradOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, pir::Value out_grad_, pir::Value shape_) {
  VLOG(4) << "Start build ExpandGradOp";



  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_, out_grad_, shape_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;
  paddle::dialect::DenseTensorType out_grad = out_grad_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)out_grad;
  phi::IntArray shape;
  if (shape_.dyn_cast<pir::OpResult>() && shape_.dyn_cast<pir::OpResult>().owner()->isa<paddle::dialect::FullIntArrayOp>()) {
    shape = std::move(phi::IntArray(paddle::dialect::GetInt64Vector(
                          shape_.dyn_cast<pir::OpResult>().owner()
                          ->dyn_cast<paddle::dialect::FullIntArrayOp>()
                          .attribute("value"))));
  } else if (shape_.type().isa<pir::VectorType>()) {
    size_t shape_size = shape_.type().dyn_cast<pir::VectorType>().size();
    shape = std::move(phi::IntArray(std::vector<int64_t>(shape_size, -1)));
    shape.SetFromTensor(true);
  } else if (shape_.type().isa<paddle::dialect::DenseTensorType>()) {
    common::DDim shape_dim = shape_.type().dyn_cast<paddle::dialect::DenseTensorType>().dims();
    size_t shape_size = common::product(shape_dim);
    if (common::contain_unknown_dim(shape_dim)) {
      shape_size = 1;
    }
    shape = std::move(phi::IntArray(std::vector<int64_t>(shape_size, -1)));
    shape.SetFromTensor(true);
  } else {
    PADDLE_THROW(phi::errors::Unimplemented("Only support VectorType or DenseTensorType"));
  }


  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);
  paddle::dialect::IrTensor dense_x_grad;
  paddle::dialect::IrMetaTensor meta_x_grad(&dense_x_grad);

  phi::UnchangedInferMeta(meta_x, &meta_x_grad);

  std::vector<pir::Type> argument_outputs;
  pir::Type x_grad_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_x_grad.dtype()), dense_x_grad.dims(), dense_x_grad.layout(), dense_x_grad.lod(), dense_x_grad.offset());
  argument_outputs.push_back(x_grad_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void ExpandGradOp::VerifySig() {}

void ExpandGradOp::InferMeta( phi::InferMetaContext *infer_meta ) {
  auto fn = PD_INFER_META(phi::UnchangedInferMeta);
  fn(infer_meta);
}

phi::DataType ExpandGradOp::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: ExpandGradOp";
  


  return expected_kernel_dtype;
}

OpInfoTuple Expm1GradOp::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("out", "paddle::dialect::DenseTensorType", false, false, false, false), paddle::dialect::OpInputInfo("out_grad", "paddle::dialect::DenseTensorType", false, false, false, false) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = {  };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("x_grad", "paddle::dialect::DenseTensorType", false, false) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("UnchangedInferMeta", {"out"}, "expm1_grad", {"out", "out_grad"}, {}, {}, {}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "expm1_grad");
}

void Expm1GradOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value out_, pir::Value out_grad_) {
  VLOG(4) << "Start build Expm1GradOp";



  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {out_, out_grad_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType out = out_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)out;
  paddle::dialect::DenseTensorType out_grad = out_grad_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)out_grad;

  VLOG(4) << "Builder construction  dense_out";
  paddle::dialect::IrTensor ir_tensor_out(paddle::dialect::TransToPhiDataType(out.dtype()),
                                                      out.dims(),
                                                      out.data_layout(),
                                                      out.lod(),
                                                      out.offset());
  VLOG(4) << "Builder construction  meta_out";
  paddle::dialect::IrMetaTensor meta_out(&ir_tensor_out);
  paddle::dialect::IrTensor dense_x_grad;
  paddle::dialect::IrMetaTensor meta_x_grad(&dense_x_grad);

  phi::UnchangedInferMeta(meta_out, &meta_x_grad);

  std::vector<pir::Type> argument_outputs;
  pir::Type x_grad_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_x_grad.dtype()), dense_x_grad.dims(), dense_x_grad.layout(), dense_x_grad.lod(), dense_x_grad.offset());
  argument_outputs.push_back(x_grad_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void Expm1GradOp::VerifySig() {}

void Expm1GradOp::InferMeta( phi::InferMetaContext *infer_meta ) {
  auto fn = PD_INFER_META(phi::UnchangedInferMeta);
  fn(infer_meta);
}

phi::DataType Expm1GradOp::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: Expm1GradOp";
  


  return expected_kernel_dtype;
}

OpInfoTuple Expm1Grad_Op::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("out", "paddle::dialect::DenseTensorType", false, false, false, false), paddle::dialect::OpInputInfo("out_grad", "paddle::dialect::DenseTensorType", false, false, false, false) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = {  };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("x_grad", "paddle::dialect::DenseTensorType", false, false) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("UnchangedInferMeta", {"out"}, "expm1_grad", {"out", "out_grad"}, {}, {}, {{"x_grad", "out_grad"}}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "expm1_grad");
}

void Expm1Grad_Op::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value out_, pir::Value out_grad_) {
  VLOG(4) << "Start build Expm1Grad_Op";



  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {out_, out_grad_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType out = out_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)out;
  paddle::dialect::DenseTensorType out_grad = out_grad_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)out_grad;

  VLOG(4) << "Builder construction  dense_out";
  paddle::dialect::IrTensor ir_tensor_out(paddle::dialect::TransToPhiDataType(out.dtype()),
                                                      out.dims(),
                                                      out.data_layout(),
                                                      out.lod(),
                                                      out.offset());
  VLOG(4) << "Builder construction  meta_out";
  paddle::dialect::IrMetaTensor meta_out(&ir_tensor_out);
  paddle::dialect::IrTensor dense_x_grad;
  paddle::dialect::IrMetaTensor meta_x_grad(&dense_x_grad);

  phi::UnchangedInferMeta(meta_out, &meta_x_grad);

  std::vector<pir::Type> argument_outputs;
  pir::Type x_grad_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_x_grad.dtype()), dense_x_grad.dims(), dense_x_grad.layout(), dense_x_grad.lod(), dense_x_grad.offset());
  argument_outputs.push_back(x_grad_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void Expm1Grad_Op::VerifySig() {}

void Expm1Grad_Op::InferMeta( phi::InferMetaContext *infer_meta ) {
  auto fn = PD_INFER_META(phi::UnchangedInferMeta);
  fn(infer_meta);
}

phi::DataType Expm1Grad_Op::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: Expm1Grad_Op";
  


  return expected_kernel_dtype;
}

const char *FftC2cGradOp::attributes_name[3] = { "axes", "normalization", "forward" };

OpInfoTuple FftC2cGradOp::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("out_grad", "paddle::dialect::DenseTensorType", false, false, false, false) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = { paddle::dialect::OpAttributeInfo("axes", "pir::ArrayAttribute<pir::Int64Attribute>", ""), paddle::dialect::OpAttributeInfo("normalization", "pir::StrAttribute", ""), paddle::dialect::OpAttributeInfo("forward", "pir::BoolAttribute", "") };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("x_grad", "paddle::dialect::DenseTensorType", false, false) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("UnchangedInferMeta", {"out_grad"}, "fft_c2c_grad", {"out_grad", "axes", "normalization", "forward"}, {}, {}, {}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "fft_c2c_grad");
}

void FftC2cGradOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value out_grad_, const std::vector<int64_t>& axes, const std::string& normalization, bool forward) {
  VLOG(4) << "Start build FftC2cGradOp";



  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {out_grad_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  std::vector<pir::Attribute> vec_axes;
  for (size_t i = 0; i < static_cast<size_t>(axes.size()); i++) {
      pir::Attribute attr_axes = pir::Int64Attribute::get(pir::IrContext::Instance(), axes[i]);

    vec_axes.push_back(attr_axes);
  }
  pir::Attribute attr_axes = pir::ArrayAttribute::get(pir::IrContext::Instance(), vec_axes);
  argument.AddAttribute("axes", attr_axes);
  pir::Attribute attr_normalization = pir::StrAttribute::get(pir::IrContext::Instance(), normalization);
  argument.AddAttribute("normalization", attr_normalization);
  pir::Attribute attr_forward = pir::BoolAttribute::get(pir::IrContext::Instance(), forward);
  argument.AddAttribute("forward", attr_forward);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType out_grad = out_grad_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)out_grad;

  VLOG(4) << "Builder construction  dense_out_grad";
  paddle::dialect::IrTensor ir_tensor_out_grad(paddle::dialect::TransToPhiDataType(out_grad.dtype()),
                                                      out_grad.dims(),
                                                      out_grad.data_layout(),
                                                      out_grad.lod(),
                                                      out_grad.offset());
  VLOG(4) << "Builder construction  meta_out_grad";
  paddle::dialect::IrMetaTensor meta_out_grad(&ir_tensor_out_grad);
  paddle::dialect::IrTensor dense_x_grad;
  paddle::dialect::IrMetaTensor meta_x_grad(&dense_x_grad);

  phi::UnchangedInferMeta(meta_out_grad, &meta_x_grad);

  std::vector<pir::Type> argument_outputs;
  pir::Type x_grad_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_x_grad.dtype()), dense_x_grad.dims(), dense_x_grad.layout(), dense_x_grad.lod(), dense_x_grad.offset());
  argument_outputs.push_back(x_grad_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void FftC2cGradOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value out_grad_, pir::AttributeMap attributes) {
  VLOG(4) << "Start build FftC2cGradOp";


  IR_ENFORCE(
      attributes.find("axes") != attributes.end(),
          "'axes' Attribute is expected for FftC2cGradOp. ");
  std::vector<int64_t> axes;
  for (size_t i = 0; i < attributes.at("axes").dyn_cast<pir::ArrayAttribute>().size(); i++) {
    axes.push_back(attributes.at("axes").dyn_cast<pir::ArrayAttribute>().at(i).dyn_cast<pir::Int64Attribute>().data());
  }

  IR_ENFORCE(
      attributes.find("normalization") != attributes.end(),
          "'normalization' Attribute is expected for FftC2cGradOp. ");
  std::string normalization = attributes.at("normalization").dyn_cast<pir::StrAttribute>().AsString();

  IR_ENFORCE(
      attributes.find("forward") != attributes.end(),
          "'forward' Attribute is expected for FftC2cGradOp. ");
  bool forward = attributes.at("forward").dyn_cast<pir::BoolAttribute>().data();


  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {out_grad_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  std::vector<pir::Attribute> vec_axes;
  for (size_t i = 0; i < static_cast<size_t>(axes.size()); i++) {
      pir::Attribute attr_axes = pir::Int64Attribute::get(pir::IrContext::Instance(), axes[i]);

    vec_axes.push_back(attr_axes);
  }
  pir::Attribute attr_axes = pir::ArrayAttribute::get(pir::IrContext::Instance(), vec_axes);
  argument.AddAttribute("axes", attr_axes);
  pir::Attribute attr_normalization = pir::StrAttribute::get(pir::IrContext::Instance(), normalization);
  argument.AddAttribute("normalization", attr_normalization);
  pir::Attribute attr_forward = pir::BoolAttribute::get(pir::IrContext::Instance(), forward);
  argument.AddAttribute("forward", attr_forward);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType out_grad = out_grad_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)out_grad;

  VLOG(4) << "Builder construction  dense_out_grad";
  paddle::dialect::IrTensor ir_tensor_out_grad(paddle::dialect::TransToPhiDataType(out_grad.dtype()),
                                                      out_grad.dims(),
                                                      out_grad.data_layout(),
                                                      out_grad.lod(),
                                                      out_grad.offset());
  VLOG(4) << "Builder construction  meta_out_grad";
  paddle::dialect::IrMetaTensor meta_out_grad(&ir_tensor_out_grad);
  paddle::dialect::IrTensor dense_x_grad;
  paddle::dialect::IrMetaTensor meta_x_grad(&dense_x_grad);

  phi::UnchangedInferMeta(meta_out_grad, &meta_x_grad);

  std::vector<pir::Type> argument_outputs;
  pir::Type x_grad_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_x_grad.dtype()), dense_x_grad.dims(), dense_x_grad.layout(), dense_x_grad.lod(), dense_x_grad.offset());
  argument_outputs.push_back(x_grad_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void FftC2cGradOp::VerifySig() {}

void FftC2cGradOp::InferMeta( phi::InferMetaContext *infer_meta ) {
  auto fn = PD_INFER_META(phi::UnchangedInferMeta);
  fn(infer_meta);
}

phi::DataType FftC2cGradOp::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: FftC2cGradOp";
  


  return expected_kernel_dtype;
}

const char *FftC2rGradOp::attributes_name[4] = { "axes", "normalization", "forward", "last_dim_size" };

OpInfoTuple FftC2rGradOp::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("out_grad", "paddle::dialect::DenseTensorType", false, false, false, false) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = { paddle::dialect::OpAttributeInfo("axes", "pir::ArrayAttribute<pir::Int64Attribute>", ""), paddle::dialect::OpAttributeInfo("normalization", "pir::StrAttribute", ""), paddle::dialect::OpAttributeInfo("forward", "pir::BoolAttribute", ""), paddle::dialect::OpAttributeInfo("last_dim_size", "pir::Int64Attribute", "") };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("x_grad", "paddle::dialect::DenseTensorType", false, false) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("FFTC2RGradInferMeta", {"out_grad", "axes", "normalization", "forward", "last_dim_size"}, "fft_c2r_grad", {"out_grad", "axes", "normalization", "forward", "last_dim_size"}, {"out_grad"}, {}, {}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "fft_c2r_grad");
}

void FftC2rGradOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value out_grad_, const std::vector<int64_t>& axes, const std::string& normalization, bool forward, int64_t last_dim_size) {
  VLOG(4) << "Start build FftC2rGradOp";



  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {out_grad_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  std::vector<pir::Attribute> vec_axes;
  for (size_t i = 0; i < static_cast<size_t>(axes.size()); i++) {
      pir::Attribute attr_axes = pir::Int64Attribute::get(pir::IrContext::Instance(), axes[i]);

    vec_axes.push_back(attr_axes);
  }
  pir::Attribute attr_axes = pir::ArrayAttribute::get(pir::IrContext::Instance(), vec_axes);
  argument.AddAttribute("axes", attr_axes);
  pir::Attribute attr_normalization = pir::StrAttribute::get(pir::IrContext::Instance(), normalization);
  argument.AddAttribute("normalization", attr_normalization);
  pir::Attribute attr_forward = pir::BoolAttribute::get(pir::IrContext::Instance(), forward);
  argument.AddAttribute("forward", attr_forward);
  pir::Attribute attr_last_dim_size = pir::Int64Attribute::get(pir::IrContext::Instance(), last_dim_size);
  argument.AddAttribute("last_dim_size", attr_last_dim_size);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType out_grad = out_grad_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)out_grad;

  VLOG(4) << "Builder construction  dense_out_grad";
  paddle::dialect::IrTensor ir_tensor_out_grad(paddle::dialect::TransToPhiDataType(out_grad.dtype()),
                                                      out_grad.dims(),
                                                      out_grad.data_layout(),
                                                      out_grad.lod(),
                                                      out_grad.offset());
  VLOG(4) << "Builder construction  meta_out_grad";
  paddle::dialect::IrMetaTensor meta_out_grad(&ir_tensor_out_grad);
  paddle::dialect::IrTensor dense_x_grad;
  paddle::dialect::IrMetaTensor meta_x_grad(&dense_x_grad);

  phi::FFTC2RGradInferMeta(meta_out_grad, axes, normalization, forward, last_dim_size, &meta_x_grad);

  std::vector<pir::Type> argument_outputs;
  pir::Type x_grad_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_x_grad.dtype()), dense_x_grad.dims(), dense_x_grad.layout(), dense_x_grad.lod(), dense_x_grad.offset());
  argument_outputs.push_back(x_grad_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void FftC2rGradOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value out_grad_, pir::AttributeMap attributes) {
  VLOG(4) << "Start build FftC2rGradOp";


  IR_ENFORCE(
      attributes.find("axes") != attributes.end(),
          "'axes' Attribute is expected for FftC2rGradOp. ");
  std::vector<int64_t> axes;
  for (size_t i = 0; i < attributes.at("axes").dyn_cast<pir::ArrayAttribute>().size(); i++) {
    axes.push_back(attributes.at("axes").dyn_cast<pir::ArrayAttribute>().at(i).dyn_cast<pir::Int64Attribute>().data());
  }

  IR_ENFORCE(
      attributes.find("normalization") != attributes.end(),
          "'normalization' Attribute is expected for FftC2rGradOp. ");
  std::string normalization = attributes.at("normalization").dyn_cast<pir::StrAttribute>().AsString();

  IR_ENFORCE(
      attributes.find("forward") != attributes.end(),
          "'forward' Attribute is expected for FftC2rGradOp. ");
  bool forward = attributes.at("forward").dyn_cast<pir::BoolAttribute>().data();

  IR_ENFORCE(
      attributes.find("last_dim_size") != attributes.end(),
          "'last_dim_size' Attribute is expected for FftC2rGradOp. ");
  int64_t last_dim_size = attributes.at("last_dim_size").dyn_cast<pir::Int64Attribute>().data();


  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {out_grad_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  std::vector<pir::Attribute> vec_axes;
  for (size_t i = 0; i < static_cast<size_t>(axes.size()); i++) {
      pir::Attribute attr_axes = pir::Int64Attribute::get(pir::IrContext::Instance(), axes[i]);

    vec_axes.push_back(attr_axes);
  }
  pir::Attribute attr_axes = pir::ArrayAttribute::get(pir::IrContext::Instance(), vec_axes);
  argument.AddAttribute("axes", attr_axes);
  pir::Attribute attr_normalization = pir::StrAttribute::get(pir::IrContext::Instance(), normalization);
  argument.AddAttribute("normalization", attr_normalization);
  pir::Attribute attr_forward = pir::BoolAttribute::get(pir::IrContext::Instance(), forward);
  argument.AddAttribute("forward", attr_forward);
  pir::Attribute attr_last_dim_size = pir::Int64Attribute::get(pir::IrContext::Instance(), last_dim_size);
  argument.AddAttribute("last_dim_size", attr_last_dim_size);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType out_grad = out_grad_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)out_grad;

  VLOG(4) << "Builder construction  dense_out_grad";
  paddle::dialect::IrTensor ir_tensor_out_grad(paddle::dialect::TransToPhiDataType(out_grad.dtype()),
                                                      out_grad.dims(),
                                                      out_grad.data_layout(),
                                                      out_grad.lod(),
                                                      out_grad.offset());
  VLOG(4) << "Builder construction  meta_out_grad";
  paddle::dialect::IrMetaTensor meta_out_grad(&ir_tensor_out_grad);
  paddle::dialect::IrTensor dense_x_grad;
  paddle::dialect::IrMetaTensor meta_x_grad(&dense_x_grad);

  phi::FFTC2RGradInferMeta(meta_out_grad, axes, normalization, forward, last_dim_size, &meta_x_grad);

  std::vector<pir::Type> argument_outputs;
  pir::Type x_grad_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_x_grad.dtype()), dense_x_grad.dims(), dense_x_grad.layout(), dense_x_grad.lod(), dense_x_grad.offset());
  argument_outputs.push_back(x_grad_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void FftC2rGradOp::VerifySig() {}

void FftC2rGradOp::InferMeta( phi::InferMetaContext *infer_meta ) {
  auto fn = PD_INFER_META(phi::FFTC2RGradInferMeta);
  fn(infer_meta);
}

phi::DataType FftC2rGradOp::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: FftC2rGradOp";
  


  return expected_kernel_dtype;
}

const char *FftR2cGradOp::attributes_name[4] = { "axes", "normalization", "forward", "onesided" };

OpInfoTuple FftR2cGradOp::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("x", "paddle::dialect::DenseTensorType", false, true, false, false), paddle::dialect::OpInputInfo("out_grad", "paddle::dialect::DenseTensorType", false, false, false, false) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = { paddle::dialect::OpAttributeInfo("axes", "pir::ArrayAttribute<pir::Int64Attribute>", ""), paddle::dialect::OpAttributeInfo("normalization", "pir::StrAttribute", ""), paddle::dialect::OpAttributeInfo("forward", "pir::BoolAttribute", ""), paddle::dialect::OpAttributeInfo("onesided", "pir::BoolAttribute", "") };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("x_grad", "paddle::dialect::DenseTensorType", false, false) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("UnchangedInferMeta", {"x"}, "fft_r2c_grad", {"x", "out_grad", "axes", "normalization", "forward", "onesided"}, {"out_grad"}, {}, {}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "fft_r2c_grad");
}

void FftR2cGradOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, pir::Value out_grad_, const std::vector<int64_t>& axes, const std::string& normalization, bool forward, bool onesided) {
  VLOG(4) << "Start build FftR2cGradOp";



  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_, out_grad_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  std::vector<pir::Attribute> vec_axes;
  for (size_t i = 0; i < static_cast<size_t>(axes.size()); i++) {
      pir::Attribute attr_axes = pir::Int64Attribute::get(pir::IrContext::Instance(), axes[i]);

    vec_axes.push_back(attr_axes);
  }
  pir::Attribute attr_axes = pir::ArrayAttribute::get(pir::IrContext::Instance(), vec_axes);
  argument.AddAttribute("axes", attr_axes);
  pir::Attribute attr_normalization = pir::StrAttribute::get(pir::IrContext::Instance(), normalization);
  argument.AddAttribute("normalization", attr_normalization);
  pir::Attribute attr_forward = pir::BoolAttribute::get(pir::IrContext::Instance(), forward);
  argument.AddAttribute("forward", attr_forward);
  pir::Attribute attr_onesided = pir::BoolAttribute::get(pir::IrContext::Instance(), onesided);
  argument.AddAttribute("onesided", attr_onesided);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;
  paddle::dialect::DenseTensorType out_grad = out_grad_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)out_grad;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);
  paddle::dialect::IrTensor dense_x_grad;
  paddle::dialect::IrMetaTensor meta_x_grad(&dense_x_grad);

  phi::UnchangedInferMeta(meta_x, &meta_x_grad);

  std::vector<pir::Type> argument_outputs;
  pir::Type x_grad_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_x_grad.dtype()), dense_x_grad.dims(), dense_x_grad.layout(), dense_x_grad.lod(), dense_x_grad.offset());
  argument_outputs.push_back(x_grad_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void FftR2cGradOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, pir::Value out_grad_, pir::AttributeMap attributes) {
  VLOG(4) << "Start build FftR2cGradOp";


  IR_ENFORCE(
      attributes.find("axes") != attributes.end(),
          "'axes' Attribute is expected for FftR2cGradOp. ");
  std::vector<int64_t> axes;
  for (size_t i = 0; i < attributes.at("axes").dyn_cast<pir::ArrayAttribute>().size(); i++) {
    axes.push_back(attributes.at("axes").dyn_cast<pir::ArrayAttribute>().at(i).dyn_cast<pir::Int64Attribute>().data());
  }

  IR_ENFORCE(
      attributes.find("normalization") != attributes.end(),
          "'normalization' Attribute is expected for FftR2cGradOp. ");
  std::string normalization = attributes.at("normalization").dyn_cast<pir::StrAttribute>().AsString();

  IR_ENFORCE(
      attributes.find("forward") != attributes.end(),
          "'forward' Attribute is expected for FftR2cGradOp. ");
  bool forward = attributes.at("forward").dyn_cast<pir::BoolAttribute>().data();

  IR_ENFORCE(
      attributes.find("onesided") != attributes.end(),
          "'onesided' Attribute is expected for FftR2cGradOp. ");
  bool onesided = attributes.at("onesided").dyn_cast<pir::BoolAttribute>().data();


  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_, out_grad_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  std::vector<pir::Attribute> vec_axes;
  for (size_t i = 0; i < static_cast<size_t>(axes.size()); i++) {
      pir::Attribute attr_axes = pir::Int64Attribute::get(pir::IrContext::Instance(), axes[i]);

    vec_axes.push_back(attr_axes);
  }
  pir::Attribute attr_axes = pir::ArrayAttribute::get(pir::IrContext::Instance(), vec_axes);
  argument.AddAttribute("axes", attr_axes);
  pir::Attribute attr_normalization = pir::StrAttribute::get(pir::IrContext::Instance(), normalization);
  argument.AddAttribute("normalization", attr_normalization);
  pir::Attribute attr_forward = pir::BoolAttribute::get(pir::IrContext::Instance(), forward);
  argument.AddAttribute("forward", attr_forward);
  pir::Attribute attr_onesided = pir::BoolAttribute::get(pir::IrContext::Instance(), onesided);
  argument.AddAttribute("onesided", attr_onesided);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;
  paddle::dialect::DenseTensorType out_grad = out_grad_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)out_grad;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);
  paddle::dialect::IrTensor dense_x_grad;
  paddle::dialect::IrMetaTensor meta_x_grad(&dense_x_grad);

  phi::UnchangedInferMeta(meta_x, &meta_x_grad);

  std::vector<pir::Type> argument_outputs;
  pir::Type x_grad_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_x_grad.dtype()), dense_x_grad.dims(), dense_x_grad.layout(), dense_x_grad.lod(), dense_x_grad.offset());
  argument_outputs.push_back(x_grad_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void FftR2cGradOp::VerifySig() {}

void FftR2cGradOp::InferMeta( phi::InferMetaContext *infer_meta ) {
  auto fn = PD_INFER_META(phi::UnchangedInferMeta);
  fn(infer_meta);
}

phi::DataType FftR2cGradOp::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: FftR2cGradOp";
  


  return expected_kernel_dtype;
}

const char *FillDiagonalGradOp::attributes_name[3] = { "value", "offset", "wrap" };

OpInfoTuple FillDiagonalGradOp::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("out_grad", "paddle::dialect::DenseTensorType", false, false, false, false) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = { paddle::dialect::OpAttributeInfo("value", "pir::FloatAttribute", ""), paddle::dialect::OpAttributeInfo("offset", "pir::Int32Attribute", ""), paddle::dialect::OpAttributeInfo("wrap", "pir::BoolAttribute", "") };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("x_grad", "paddle::dialect::DenseTensorType", false, false) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("FillDiagonalGradInferMeta", {"out_grad", "value", "offset", "wrap"}, "fill_diagonal_grad", {"out_grad", "value", "offset", "wrap"}, {}, {}, {}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "fill_diagonal_grad");
}

void FillDiagonalGradOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value out_grad_, float value, int offset, bool wrap) {
  VLOG(4) << "Start build FillDiagonalGradOp";



  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {out_grad_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_value = pir::FloatAttribute::get(pir::IrContext::Instance(), value);
  argument.AddAttribute("value", attr_value);
  pir::Attribute attr_offset = pir::Int32Attribute::get(pir::IrContext::Instance(), offset);
  argument.AddAttribute("offset", attr_offset);
  pir::Attribute attr_wrap = pir::BoolAttribute::get(pir::IrContext::Instance(), wrap);
  argument.AddAttribute("wrap", attr_wrap);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType out_grad = out_grad_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)out_grad;

  VLOG(4) << "Builder construction  dense_out_grad";
  paddle::dialect::IrTensor ir_tensor_out_grad(paddle::dialect::TransToPhiDataType(out_grad.dtype()),
                                                      out_grad.dims(),
                                                      out_grad.data_layout(),
                                                      out_grad.lod(),
                                                      out_grad.offset());
  VLOG(4) << "Builder construction  meta_out_grad";
  paddle::dialect::IrMetaTensor meta_out_grad(&ir_tensor_out_grad);
  paddle::dialect::IrTensor dense_x_grad;
  paddle::dialect::IrMetaTensor meta_x_grad(&dense_x_grad);

  phi::FillDiagonalGradInferMeta(meta_out_grad, value, offset, wrap, &meta_x_grad);

  std::vector<pir::Type> argument_outputs;
  pir::Type x_grad_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_x_grad.dtype()), dense_x_grad.dims(), dense_x_grad.layout(), dense_x_grad.lod(), dense_x_grad.offset());
  argument_outputs.push_back(x_grad_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void FillDiagonalGradOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value out_grad_, pir::AttributeMap attributes) {
  VLOG(4) << "Start build FillDiagonalGradOp";


  IR_ENFORCE(
      attributes.find("value") != attributes.end(),
          "'value' Attribute is expected for FillDiagonalGradOp. ");
  float value = attributes.at("value").dyn_cast<pir::FloatAttribute>().data();

  IR_ENFORCE(
      attributes.find("offset") != attributes.end(),
          "'offset' Attribute is expected for FillDiagonalGradOp. ");
  int offset = attributes.at("offset").dyn_cast<pir::Int32Attribute>().data();

  IR_ENFORCE(
      attributes.find("wrap") != attributes.end(),
          "'wrap' Attribute is expected for FillDiagonalGradOp. ");
  bool wrap = attributes.at("wrap").dyn_cast<pir::BoolAttribute>().data();


  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {out_grad_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_value = pir::FloatAttribute::get(pir::IrContext::Instance(), value);
  argument.AddAttribute("value", attr_value);
  pir::Attribute attr_offset = pir::Int32Attribute::get(pir::IrContext::Instance(), offset);
  argument.AddAttribute("offset", attr_offset);
  pir::Attribute attr_wrap = pir::BoolAttribute::get(pir::IrContext::Instance(), wrap);
  argument.AddAttribute("wrap", attr_wrap);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType out_grad = out_grad_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)out_grad;

  VLOG(4) << "Builder construction  dense_out_grad";
  paddle::dialect::IrTensor ir_tensor_out_grad(paddle::dialect::TransToPhiDataType(out_grad.dtype()),
                                                      out_grad.dims(),
                                                      out_grad.data_layout(),
                                                      out_grad.lod(),
                                                      out_grad.offset());
  VLOG(4) << "Builder construction  meta_out_grad";
  paddle::dialect::IrMetaTensor meta_out_grad(&ir_tensor_out_grad);
  paddle::dialect::IrTensor dense_x_grad;
  paddle::dialect::IrMetaTensor meta_x_grad(&dense_x_grad);

  phi::FillDiagonalGradInferMeta(meta_out_grad, value, offset, wrap, &meta_x_grad);

  std::vector<pir::Type> argument_outputs;
  pir::Type x_grad_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_x_grad.dtype()), dense_x_grad.dims(), dense_x_grad.layout(), dense_x_grad.lod(), dense_x_grad.offset());
  argument_outputs.push_back(x_grad_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void FillDiagonalGradOp::VerifySig() {}

void FillDiagonalGradOp::InferMeta( phi::InferMetaContext *infer_meta ) {
  auto fn = PD_INFER_META(phi::FillDiagonalGradInferMeta);
  fn(infer_meta);
}

phi::DataType FillDiagonalGradOp::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: FillDiagonalGradOp";
  


  return expected_kernel_dtype;
}

const char *FillDiagonalTensorGradOp::attributes_name[3] = { "offset", "dim1", "dim2" };

OpInfoTuple FillDiagonalTensorGradOp::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("out_grad", "paddle::dialect::DenseTensorType", false, false, false, false) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = { paddle::dialect::OpAttributeInfo("offset", "pir::Int64Attribute", ""), paddle::dialect::OpAttributeInfo("dim1", "pir::Int32Attribute", ""), paddle::dialect::OpAttributeInfo("dim2", "pir::Int32Attribute", "") };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("x_grad", "paddle::dialect::DenseTensorType", false, false) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("FillDiagonalTensorGradInferMeta", {"out_grad", "offset", "dim1", "dim2"}, "fill_diagonal_tensor_grad", {"out_grad", "offset", "dim1", "dim2"}, {}, {}, {}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "fill_diagonal_tensor_grad");
}

void FillDiagonalTensorGradOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value out_grad_, int64_t offset, int dim1, int dim2) {
  VLOG(4) << "Start build FillDiagonalTensorGradOp";



  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {out_grad_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_offset = pir::Int64Attribute::get(pir::IrContext::Instance(), offset);
  argument.AddAttribute("offset", attr_offset);
  pir::Attribute attr_dim1 = pir::Int32Attribute::get(pir::IrContext::Instance(), dim1);
  argument.AddAttribute("dim1", attr_dim1);
  pir::Attribute attr_dim2 = pir::Int32Attribute::get(pir::IrContext::Instance(), dim2);
  argument.AddAttribute("dim2", attr_dim2);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType out_grad = out_grad_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)out_grad;

  VLOG(4) << "Builder construction  dense_out_grad";
  paddle::dialect::IrTensor ir_tensor_out_grad(paddle::dialect::TransToPhiDataType(out_grad.dtype()),
                                                      out_grad.dims(),
                                                      out_grad.data_layout(),
                                                      out_grad.lod(),
                                                      out_grad.offset());
  VLOG(4) << "Builder construction  meta_out_grad";
  paddle::dialect::IrMetaTensor meta_out_grad(&ir_tensor_out_grad);
  paddle::dialect::IrTensor dense_x_grad;
  paddle::dialect::IrMetaTensor meta_x_grad(&dense_x_grad);

  phi::FillDiagonalTensorGradInferMeta(meta_out_grad, offset, dim1, dim2, &meta_x_grad);

  std::vector<pir::Type> argument_outputs;
  pir::Type x_grad_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_x_grad.dtype()), dense_x_grad.dims(), dense_x_grad.layout(), dense_x_grad.lod(), dense_x_grad.offset());
  argument_outputs.push_back(x_grad_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void FillDiagonalTensorGradOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value out_grad_, pir::AttributeMap attributes) {
  VLOG(4) << "Start build FillDiagonalTensorGradOp";


  IR_ENFORCE(
      attributes.find("offset") != attributes.end(),
          "'offset' Attribute is expected for FillDiagonalTensorGradOp. ");
  int64_t offset = attributes.at("offset").dyn_cast<pir::Int64Attribute>().data();

  IR_ENFORCE(
      attributes.find("dim1") != attributes.end(),
          "'dim1' Attribute is expected for FillDiagonalTensorGradOp. ");
  int dim1 = attributes.at("dim1").dyn_cast<pir::Int32Attribute>().data();

  IR_ENFORCE(
      attributes.find("dim2") != attributes.end(),
          "'dim2' Attribute is expected for FillDiagonalTensorGradOp. ");
  int dim2 = attributes.at("dim2").dyn_cast<pir::Int32Attribute>().data();


  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {out_grad_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_offset = pir::Int64Attribute::get(pir::IrContext::Instance(), offset);
  argument.AddAttribute("offset", attr_offset);
  pir::Attribute attr_dim1 = pir::Int32Attribute::get(pir::IrContext::Instance(), dim1);
  argument.AddAttribute("dim1", attr_dim1);
  pir::Attribute attr_dim2 = pir::Int32Attribute::get(pir::IrContext::Instance(), dim2);
  argument.AddAttribute("dim2", attr_dim2);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType out_grad = out_grad_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)out_grad;

  VLOG(4) << "Builder construction  dense_out_grad";
  paddle::dialect::IrTensor ir_tensor_out_grad(paddle::dialect::TransToPhiDataType(out_grad.dtype()),
                                                      out_grad.dims(),
                                                      out_grad.data_layout(),
                                                      out_grad.lod(),
                                                      out_grad.offset());
  VLOG(4) << "Builder construction  meta_out_grad";
  paddle::dialect::IrMetaTensor meta_out_grad(&ir_tensor_out_grad);
  paddle::dialect::IrTensor dense_x_grad;
  paddle::dialect::IrMetaTensor meta_x_grad(&dense_x_grad);

  phi::FillDiagonalTensorGradInferMeta(meta_out_grad, offset, dim1, dim2, &meta_x_grad);

  std::vector<pir::Type> argument_outputs;
  pir::Type x_grad_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_x_grad.dtype()), dense_x_grad.dims(), dense_x_grad.layout(), dense_x_grad.lod(), dense_x_grad.offset());
  argument_outputs.push_back(x_grad_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void FillDiagonalTensorGradOp::VerifySig() {}

void FillDiagonalTensorGradOp::InferMeta( phi::InferMetaContext *infer_meta ) {
  auto fn = PD_INFER_META(phi::FillDiagonalTensorGradInferMeta);
  fn(infer_meta);
}

phi::DataType FillDiagonalTensorGradOp::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: FillDiagonalTensorGradOp";
  


  return expected_kernel_dtype;
}

const char *FillDiagonalTensorGrad_Op::attributes_name[3] = { "offset", "dim1", "dim2" };

OpInfoTuple FillDiagonalTensorGrad_Op::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("out_grad", "paddle::dialect::DenseTensorType", false, false, false, false) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = { paddle::dialect::OpAttributeInfo("offset", "pir::Int64Attribute", ""), paddle::dialect::OpAttributeInfo("dim1", "pir::Int32Attribute", ""), paddle::dialect::OpAttributeInfo("dim2", "pir::Int32Attribute", "") };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("x_grad", "paddle::dialect::DenseTensorType", false, false) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("FillDiagonalTensorGradInferMeta", {"out_grad", "offset", "dim1", "dim2"}, "fill_diagonal_tensor_grad", {"out_grad", "offset", "dim1", "dim2"}, {}, {}, {{"x_grad", "out_grad"}}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "fill_diagonal_tensor_grad");
}

void FillDiagonalTensorGrad_Op::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value out_grad_, int64_t offset, int dim1, int dim2) {
  VLOG(4) << "Start build FillDiagonalTensorGrad_Op";



  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {out_grad_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_offset = pir::Int64Attribute::get(pir::IrContext::Instance(), offset);
  argument.AddAttribute("offset", attr_offset);
  pir::Attribute attr_dim1 = pir::Int32Attribute::get(pir::IrContext::Instance(), dim1);
  argument.AddAttribute("dim1", attr_dim1);
  pir::Attribute attr_dim2 = pir::Int32Attribute::get(pir::IrContext::Instance(), dim2);
  argument.AddAttribute("dim2", attr_dim2);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType out_grad = out_grad_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)out_grad;

  VLOG(4) << "Builder construction  dense_out_grad";
  paddle::dialect::IrTensor ir_tensor_out_grad(paddle::dialect::TransToPhiDataType(out_grad.dtype()),
                                                      out_grad.dims(),
                                                      out_grad.data_layout(),
                                                      out_grad.lod(),
                                                      out_grad.offset());
  VLOG(4) << "Builder construction  meta_out_grad";
  paddle::dialect::IrMetaTensor meta_out_grad(&ir_tensor_out_grad);
  paddle::dialect::IrTensor dense_x_grad;
  paddle::dialect::IrMetaTensor meta_x_grad(&dense_x_grad);

  phi::FillDiagonalTensorGradInferMeta(meta_out_grad, offset, dim1, dim2, &meta_x_grad);

  std::vector<pir::Type> argument_outputs;
  pir::Type x_grad_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_x_grad.dtype()), dense_x_grad.dims(), dense_x_grad.layout(), dense_x_grad.lod(), dense_x_grad.offset());
  argument_outputs.push_back(x_grad_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void FillDiagonalTensorGrad_Op::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value out_grad_, pir::AttributeMap attributes) {
  VLOG(4) << "Start build FillDiagonalTensorGrad_Op";


  IR_ENFORCE(
      attributes.find("offset") != attributes.end(),
          "'offset' Attribute is expected for FillDiagonalTensorGrad_Op. ");
  int64_t offset = attributes.at("offset").dyn_cast<pir::Int64Attribute>().data();

  IR_ENFORCE(
      attributes.find("dim1") != attributes.end(),
          "'dim1' Attribute is expected for FillDiagonalTensorGrad_Op. ");
  int dim1 = attributes.at("dim1").dyn_cast<pir::Int32Attribute>().data();

  IR_ENFORCE(
      attributes.find("dim2") != attributes.end(),
          "'dim2' Attribute is expected for FillDiagonalTensorGrad_Op. ");
  int dim2 = attributes.at("dim2").dyn_cast<pir::Int32Attribute>().data();


  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {out_grad_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_offset = pir::Int64Attribute::get(pir::IrContext::Instance(), offset);
  argument.AddAttribute("offset", attr_offset);
  pir::Attribute attr_dim1 = pir::Int32Attribute::get(pir::IrContext::Instance(), dim1);
  argument.AddAttribute("dim1", attr_dim1);
  pir::Attribute attr_dim2 = pir::Int32Attribute::get(pir::IrContext::Instance(), dim2);
  argument.AddAttribute("dim2", attr_dim2);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType out_grad = out_grad_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)out_grad;

  VLOG(4) << "Builder construction  dense_out_grad";
  paddle::dialect::IrTensor ir_tensor_out_grad(paddle::dialect::TransToPhiDataType(out_grad.dtype()),
                                                      out_grad.dims(),
                                                      out_grad.data_layout(),
                                                      out_grad.lod(),
                                                      out_grad.offset());
  VLOG(4) << "Builder construction  meta_out_grad";
  paddle::dialect::IrMetaTensor meta_out_grad(&ir_tensor_out_grad);
  paddle::dialect::IrTensor dense_x_grad;
  paddle::dialect::IrMetaTensor meta_x_grad(&dense_x_grad);

  phi::FillDiagonalTensorGradInferMeta(meta_out_grad, offset, dim1, dim2, &meta_x_grad);

  std::vector<pir::Type> argument_outputs;
  pir::Type x_grad_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_x_grad.dtype()), dense_x_grad.dims(), dense_x_grad.layout(), dense_x_grad.lod(), dense_x_grad.offset());
  argument_outputs.push_back(x_grad_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void FillDiagonalTensorGrad_Op::VerifySig() {}

void FillDiagonalTensorGrad_Op::InferMeta( phi::InferMetaContext *infer_meta ) {
  auto fn = PD_INFER_META(phi::FillDiagonalTensorGradInferMeta);
  fn(infer_meta);
}

phi::DataType FillDiagonalTensorGrad_Op::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: FillDiagonalTensorGrad_Op";
  


  return expected_kernel_dtype;
}

OpInfoTuple FillGradOp::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("out_grad", "paddle::dialect::DenseTensorType", false, false, false, false), paddle::dialect::OpInputInfo("value", "paddle::dialect::ScalarAttribute", false, false, true, false) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = {  };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("x_grad", "paddle::dialect::DenseTensorType", false, false) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("UnchangedInferMeta", {"out_grad"}, "fill_grad", {"out_grad", "value"}, {}, {}, {}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "fill_grad");
}

void FillGradOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value out_grad_, float value) {
  VLOG(4) << "Start build FillGradOp";


  // Generate scalar mutable attribute: value
  paddle::dialect::FullOp full_value_op = builder.Build<paddle::dialect::FullOp>(std::vector<int64_t>{1}, value, phi::DataType::FLOAT32, phi::CPUPlace());
  pir::OpResult value_ = full_value_op->result(0);
    
  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {out_grad_, value_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType out_grad = out_grad_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)out_grad;

  VLOG(4) << "Builder construction  dense_out_grad";
  paddle::dialect::IrTensor ir_tensor_out_grad(paddle::dialect::TransToPhiDataType(out_grad.dtype()),
                                                      out_grad.dims(),
                                                      out_grad.data_layout(),
                                                      out_grad.lod(),
                                                      out_grad.offset());
  VLOG(4) << "Builder construction  meta_out_grad";
  paddle::dialect::IrMetaTensor meta_out_grad(&ir_tensor_out_grad);
  paddle::dialect::IrTensor dense_x_grad;
  paddle::dialect::IrMetaTensor meta_x_grad(&dense_x_grad);

  phi::UnchangedInferMeta(meta_out_grad, &meta_x_grad);

  std::vector<pir::Type> argument_outputs;
  pir::Type x_grad_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_x_grad.dtype()), dense_x_grad.dims(), dense_x_grad.layout(), dense_x_grad.lod(), dense_x_grad.offset());
  argument_outputs.push_back(x_grad_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void FillGradOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value out_grad_, pir::AttributeMap attributes) {
  VLOG(4) << "Start build FillGradOp";


  IR_ENFORCE(
      attributes.find("value") != attributes.end(),
          "'value' Attribute is expected for FillGradOp. ");
  float value = attributes.at("value").dyn_cast<paddle::dialect::ScalarAttribute>().data().to<float>();

  // Generate scalar mutable attribute: value
  paddle::dialect::FullOp full_value_op = builder.Build<paddle::dialect::FullOp>(std::vector<int64_t>{1}, value, phi::DataType::FLOAT32, phi::CPUPlace());
  pir::OpResult value_ = full_value_op->result(0);
    
  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {out_grad_, value_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType out_grad = out_grad_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)out_grad;

  VLOG(4) << "Builder construction  dense_out_grad";
  paddle::dialect::IrTensor ir_tensor_out_grad(paddle::dialect::TransToPhiDataType(out_grad.dtype()),
                                                      out_grad.dims(),
                                                      out_grad.data_layout(),
                                                      out_grad.lod(),
                                                      out_grad.offset());
  VLOG(4) << "Builder construction  meta_out_grad";
  paddle::dialect::IrMetaTensor meta_out_grad(&ir_tensor_out_grad);
  paddle::dialect::IrTensor dense_x_grad;
  paddle::dialect::IrMetaTensor meta_x_grad(&dense_x_grad);

  phi::UnchangedInferMeta(meta_out_grad, &meta_x_grad);

  std::vector<pir::Type> argument_outputs;
  pir::Type x_grad_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_x_grad.dtype()), dense_x_grad.dims(), dense_x_grad.layout(), dense_x_grad.lod(), dense_x_grad.offset());
  argument_outputs.push_back(x_grad_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void FillGradOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value out_grad_, pir::Value value_) {
  VLOG(4) << "Start build FillGradOp";



  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {out_grad_, value_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType out_grad = out_grad_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)out_grad;
  phi::Scalar value;
  if (value_.dyn_cast<pir::OpResult>() && value_.dyn_cast<pir::OpResult>().owner()->isa<paddle::dialect::FullOp>()) {
    value = std::move(phi::Scalar(value_.dyn_cast<pir::OpResult>().owner()
                                  ->dyn_cast<paddle::dialect::FullOp>()
                                  .attribute("value")
                                  .dyn_cast<paddle::dialect::ScalarAttribute>()
                                  .data()
                                  .to<int>()));
  }
  else {
    value = std::move(phi::Scalar(-1));
    value.SetFromTensor(true);
  }


  VLOG(4) << "Builder construction  dense_out_grad";
  paddle::dialect::IrTensor ir_tensor_out_grad(paddle::dialect::TransToPhiDataType(out_grad.dtype()),
                                                      out_grad.dims(),
                                                      out_grad.data_layout(),
                                                      out_grad.lod(),
                                                      out_grad.offset());
  VLOG(4) << "Builder construction  meta_out_grad";
  paddle::dialect::IrMetaTensor meta_out_grad(&ir_tensor_out_grad);
  paddle::dialect::IrTensor dense_x_grad;
  paddle::dialect::IrMetaTensor meta_x_grad(&dense_x_grad);

  phi::UnchangedInferMeta(meta_out_grad, &meta_x_grad);

  std::vector<pir::Type> argument_outputs;
  pir::Type x_grad_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_x_grad.dtype()), dense_x_grad.dims(), dense_x_grad.layout(), dense_x_grad.lod(), dense_x_grad.offset());
  argument_outputs.push_back(x_grad_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void FillGradOp::VerifySig() {}

void FillGradOp::InferMeta( phi::InferMetaContext *infer_meta ) {
  auto fn = PD_INFER_META(phi::UnchangedInferMeta);
  fn(infer_meta);
}

phi::DataType FillGradOp::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: FillGradOp";
  


  return expected_kernel_dtype;
}

OpInfoTuple FillGrad_Op::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("out_grad", "paddle::dialect::DenseTensorType", false, false, false, false), paddle::dialect::OpInputInfo("value", "paddle::dialect::ScalarAttribute", false, false, true, false) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = {  };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("x_grad", "paddle::dialect::DenseTensorType", false, false) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("UnchangedInferMeta", {"out_grad"}, "fill_grad", {"out_grad", "value"}, {}, {}, {{"x_grad", "out_grad"}}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "fill_grad");
}

void FillGrad_Op::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value out_grad_, float value) {
  VLOG(4) << "Start build FillGrad_Op";


  // Generate scalar mutable attribute: value
  paddle::dialect::FullOp full_value_op = builder.Build<paddle::dialect::FullOp>(std::vector<int64_t>{1}, value, phi::DataType::FLOAT32, phi::CPUPlace());
  pir::OpResult value_ = full_value_op->result(0);
    
  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {out_grad_, value_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType out_grad = out_grad_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)out_grad;

  VLOG(4) << "Builder construction  dense_out_grad";
  paddle::dialect::IrTensor ir_tensor_out_grad(paddle::dialect::TransToPhiDataType(out_grad.dtype()),
                                                      out_grad.dims(),
                                                      out_grad.data_layout(),
                                                      out_grad.lod(),
                                                      out_grad.offset());
  VLOG(4) << "Builder construction  meta_out_grad";
  paddle::dialect::IrMetaTensor meta_out_grad(&ir_tensor_out_grad);
  paddle::dialect::IrTensor dense_x_grad;
  paddle::dialect::IrMetaTensor meta_x_grad(&dense_x_grad);

  phi::UnchangedInferMeta(meta_out_grad, &meta_x_grad);

  std::vector<pir::Type> argument_outputs;
  pir::Type x_grad_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_x_grad.dtype()), dense_x_grad.dims(), dense_x_grad.layout(), dense_x_grad.lod(), dense_x_grad.offset());
  argument_outputs.push_back(x_grad_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void FillGrad_Op::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value out_grad_, pir::AttributeMap attributes) {
  VLOG(4) << "Start build FillGrad_Op";


  IR_ENFORCE(
      attributes.find("value") != attributes.end(),
          "'value' Attribute is expected for FillGrad_Op. ");
  float value = attributes.at("value").dyn_cast<paddle::dialect::ScalarAttribute>().data().to<float>();

  // Generate scalar mutable attribute: value
  paddle::dialect::FullOp full_value_op = builder.Build<paddle::dialect::FullOp>(std::vector<int64_t>{1}, value, phi::DataType::FLOAT32, phi::CPUPlace());
  pir::OpResult value_ = full_value_op->result(0);
    
  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {out_grad_, value_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType out_grad = out_grad_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)out_grad;

  VLOG(4) << "Builder construction  dense_out_grad";
  paddle::dialect::IrTensor ir_tensor_out_grad(paddle::dialect::TransToPhiDataType(out_grad.dtype()),
                                                      out_grad.dims(),
                                                      out_grad.data_layout(),
                                                      out_grad.lod(),
                                                      out_grad.offset());
  VLOG(4) << "Builder construction  meta_out_grad";
  paddle::dialect::IrMetaTensor meta_out_grad(&ir_tensor_out_grad);
  paddle::dialect::IrTensor dense_x_grad;
  paddle::dialect::IrMetaTensor meta_x_grad(&dense_x_grad);

  phi::UnchangedInferMeta(meta_out_grad, &meta_x_grad);

  std::vector<pir::Type> argument_outputs;
  pir::Type x_grad_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_x_grad.dtype()), dense_x_grad.dims(), dense_x_grad.layout(), dense_x_grad.lod(), dense_x_grad.offset());
  argument_outputs.push_back(x_grad_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void FillGrad_Op::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value out_grad_, pir::Value value_) {
  VLOG(4) << "Start build FillGrad_Op";



  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {out_grad_, value_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType out_grad = out_grad_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)out_grad;
  phi::Scalar value;
  if (value_.dyn_cast<pir::OpResult>() && value_.dyn_cast<pir::OpResult>().owner()->isa<paddle::dialect::FullOp>()) {
    value = std::move(phi::Scalar(value_.dyn_cast<pir::OpResult>().owner()
                                  ->dyn_cast<paddle::dialect::FullOp>()
                                  .attribute("value")
                                  .dyn_cast<paddle::dialect::ScalarAttribute>()
                                  .data()
                                  .to<int>()));
  }
  else {
    value = std::move(phi::Scalar(-1));
    value.SetFromTensor(true);
  }


  VLOG(4) << "Builder construction  dense_out_grad";
  paddle::dialect::IrTensor ir_tensor_out_grad(paddle::dialect::TransToPhiDataType(out_grad.dtype()),
                                                      out_grad.dims(),
                                                      out_grad.data_layout(),
                                                      out_grad.lod(),
                                                      out_grad.offset());
  VLOG(4) << "Builder construction  meta_out_grad";
  paddle::dialect::IrMetaTensor meta_out_grad(&ir_tensor_out_grad);
  paddle::dialect::IrTensor dense_x_grad;
  paddle::dialect::IrMetaTensor meta_x_grad(&dense_x_grad);

  phi::UnchangedInferMeta(meta_out_grad, &meta_x_grad);

  std::vector<pir::Type> argument_outputs;
  pir::Type x_grad_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_x_grad.dtype()), dense_x_grad.dims(), dense_x_grad.layout(), dense_x_grad.lod(), dense_x_grad.offset());
  argument_outputs.push_back(x_grad_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void FillGrad_Op::VerifySig() {}

void FillGrad_Op::InferMeta( phi::InferMetaContext *infer_meta ) {
  auto fn = PD_INFER_META(phi::UnchangedInferMeta);
  fn(infer_meta);
}

phi::DataType FillGrad_Op::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: FillGrad_Op";
  


  return expected_kernel_dtype;
}

const char *FlashAttnGradOp::attributes_name[2] = { "dropout", "causal" };

OpInfoTuple FlashAttnGradOp::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("q", "paddle::dialect::DenseTensorType", false, false, false, false), paddle::dialect::OpInputInfo("k", "paddle::dialect::DenseTensorType", false, false, false, false), paddle::dialect::OpInputInfo("v", "paddle::dialect::DenseTensorType", false, false, false, false), paddle::dialect::OpInputInfo("out", "paddle::dialect::DenseTensorType", false, false, false, false), paddle::dialect::OpInputInfo("softmax_lse", "paddle::dialect::DenseTensorType", false, false, false, false), paddle::dialect::OpInputInfo("seed_offset", "paddle::dialect::DenseTensorType", false, false, false, false), paddle::dialect::OpInputInfo("attn_mask", "paddle::dialect::DenseTensorType", true, false, false, false), paddle::dialect::OpInputInfo("out_grad", "paddle::dialect::DenseTensorType", false, false, false, false) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = { paddle::dialect::OpAttributeInfo("dropout", "pir::FloatAttribute", ""), paddle::dialect::OpAttributeInfo("causal", "pir::BoolAttribute", "") };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("q_grad", "paddle::dialect::DenseTensorType", false, false), paddle::dialect::OpOutputInfo("k_grad", "paddle::dialect::DenseTensorType", false, false), paddle::dialect::OpOutputInfo("v_grad", "paddle::dialect::DenseTensorType", false, false) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("FlashAttnGradInferMeta", {"q", "k", "v"}, "flash_attn_grad", {"q", "k", "v", "out", "softmax_lse", "seed_offset", "attn_mask", "out_grad", "dropout", "causal"}, {"q"}, {}, {}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "flash_attn_grad");
}

void FlashAttnGradOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value q_, pir::Value k_, pir::Value v_, pir::Value out_, pir::Value softmax_lse_, pir::Value seed_offset_, pir::Value attn_mask_, pir::Value out_grad_, float dropout, bool causal) {
  VLOG(4) << "Start build FlashAttnGradOp";



  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {q_, k_, v_, out_, softmax_lse_, seed_offset_, attn_mask_, out_grad_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_dropout = pir::FloatAttribute::get(pir::IrContext::Instance(), dropout);
  argument.AddAttribute("dropout", attr_dropout);
  pir::Attribute attr_causal = pir::BoolAttribute::get(pir::IrContext::Instance(), causal);
  argument.AddAttribute("causal", attr_causal);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType q = q_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)q;
  paddle::dialect::DenseTensorType k = k_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)k;
  paddle::dialect::DenseTensorType v = v_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)v;
  paddle::dialect::DenseTensorType out = out_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)out;
  paddle::dialect::DenseTensorType softmax_lse = softmax_lse_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)softmax_lse;
  paddle::dialect::DenseTensorType seed_offset = seed_offset_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)seed_offset;
  paddle::dialect::DenseTensorType out_grad = out_grad_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)out_grad;

  VLOG(4) << "Builder construction  dense_q";
  paddle::dialect::IrTensor ir_tensor_q(paddle::dialect::TransToPhiDataType(q.dtype()),
                                                      q.dims(),
                                                      q.data_layout(),
                                                      q.lod(),
                                                      q.offset());
  VLOG(4) << "Builder construction  meta_q";
  paddle::dialect::IrMetaTensor meta_q(&ir_tensor_q);

  VLOG(4) << "Builder construction  dense_k";
  paddle::dialect::IrTensor ir_tensor_k(paddle::dialect::TransToPhiDataType(k.dtype()),
                                                      k.dims(),
                                                      k.data_layout(),
                                                      k.lod(),
                                                      k.offset());
  VLOG(4) << "Builder construction  meta_k";
  paddle::dialect::IrMetaTensor meta_k(&ir_tensor_k);

  VLOG(4) << "Builder construction  dense_v";
  paddle::dialect::IrTensor ir_tensor_v(paddle::dialect::TransToPhiDataType(v.dtype()),
                                                      v.dims(),
                                                      v.data_layout(),
                                                      v.lod(),
                                                      v.offset());
  VLOG(4) << "Builder construction  meta_v";
  paddle::dialect::IrMetaTensor meta_v(&ir_tensor_v);
  paddle::dialect::IrTensor dense_q_grad;
  paddle::dialect::IrMetaTensor meta_q_grad(&dense_q_grad);
  paddle::dialect::IrTensor dense_k_grad;
  paddle::dialect::IrMetaTensor meta_k_grad(&dense_k_grad);
  paddle::dialect::IrTensor dense_v_grad;
  paddle::dialect::IrMetaTensor meta_v_grad(&dense_v_grad);

  phi::FlashAttnGradInferMeta(meta_q, meta_k, meta_v, &meta_q_grad, &meta_k_grad, &meta_v_grad);

  std::vector<pir::Type> argument_outputs;
  pir::Type q_grad_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_q_grad.dtype()), dense_q_grad.dims(), dense_q_grad.layout(), dense_q_grad.lod(), dense_q_grad.offset());
  argument_outputs.push_back(q_grad_dense_tensor_type);

  pir::Type k_grad_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_k_grad.dtype()), dense_k_grad.dims(), dense_k_grad.layout(), dense_k_grad.lod(), dense_k_grad.offset());
  argument_outputs.push_back(k_grad_dense_tensor_type);

  pir::Type v_grad_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_v_grad.dtype()), dense_v_grad.dims(), dense_v_grad.layout(), dense_v_grad.lod(), dense_v_grad.offset());
  argument_outputs.push_back(v_grad_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void FlashAttnGradOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value q_, pir::Value k_, pir::Value v_, pir::Value out_, pir::Value softmax_lse_, pir::Value seed_offset_, pir::Value attn_mask_, pir::Value out_grad_, pir::AttributeMap attributes) {
  VLOG(4) << "Start build FlashAttnGradOp";


  IR_ENFORCE(
      attributes.find("dropout") != attributes.end(),
          "'dropout' Attribute is expected for FlashAttnGradOp. ");
  float dropout = attributes.at("dropout").dyn_cast<pir::FloatAttribute>().data();

  IR_ENFORCE(
      attributes.find("causal") != attributes.end(),
          "'causal' Attribute is expected for FlashAttnGradOp. ");
  bool causal = attributes.at("causal").dyn_cast<pir::BoolAttribute>().data();


  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {q_, k_, v_, out_, softmax_lse_, seed_offset_, attn_mask_, out_grad_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_dropout = pir::FloatAttribute::get(pir::IrContext::Instance(), dropout);
  argument.AddAttribute("dropout", attr_dropout);
  pir::Attribute attr_causal = pir::BoolAttribute::get(pir::IrContext::Instance(), causal);
  argument.AddAttribute("causal", attr_causal);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType q = q_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)q;
  paddle::dialect::DenseTensorType k = k_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)k;
  paddle::dialect::DenseTensorType v = v_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)v;
  paddle::dialect::DenseTensorType out = out_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)out;
  paddle::dialect::DenseTensorType softmax_lse = softmax_lse_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)softmax_lse;
  paddle::dialect::DenseTensorType seed_offset = seed_offset_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)seed_offset;
  paddle::dialect::DenseTensorType out_grad = out_grad_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)out_grad;

  VLOG(4) << "Builder construction  dense_q";
  paddle::dialect::IrTensor ir_tensor_q(paddle::dialect::TransToPhiDataType(q.dtype()),
                                                      q.dims(),
                                                      q.data_layout(),
                                                      q.lod(),
                                                      q.offset());
  VLOG(4) << "Builder construction  meta_q";
  paddle::dialect::IrMetaTensor meta_q(&ir_tensor_q);

  VLOG(4) << "Builder construction  dense_k";
  paddle::dialect::IrTensor ir_tensor_k(paddle::dialect::TransToPhiDataType(k.dtype()),
                                                      k.dims(),
                                                      k.data_layout(),
                                                      k.lod(),
                                                      k.offset());
  VLOG(4) << "Builder construction  meta_k";
  paddle::dialect::IrMetaTensor meta_k(&ir_tensor_k);

  VLOG(4) << "Builder construction  dense_v";
  paddle::dialect::IrTensor ir_tensor_v(paddle::dialect::TransToPhiDataType(v.dtype()),
                                                      v.dims(),
                                                      v.data_layout(),
                                                      v.lod(),
                                                      v.offset());
  VLOG(4) << "Builder construction  meta_v";
  paddle::dialect::IrMetaTensor meta_v(&ir_tensor_v);
  paddle::dialect::IrTensor dense_q_grad;
  paddle::dialect::IrMetaTensor meta_q_grad(&dense_q_grad);
  paddle::dialect::IrTensor dense_k_grad;
  paddle::dialect::IrMetaTensor meta_k_grad(&dense_k_grad);
  paddle::dialect::IrTensor dense_v_grad;
  paddle::dialect::IrMetaTensor meta_v_grad(&dense_v_grad);

  phi::FlashAttnGradInferMeta(meta_q, meta_k, meta_v, &meta_q_grad, &meta_k_grad, &meta_v_grad);

  std::vector<pir::Type> argument_outputs;
  pir::Type q_grad_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_q_grad.dtype()), dense_q_grad.dims(), dense_q_grad.layout(), dense_q_grad.lod(), dense_q_grad.offset());
  argument_outputs.push_back(q_grad_dense_tensor_type);

  pir::Type k_grad_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_k_grad.dtype()), dense_k_grad.dims(), dense_k_grad.layout(), dense_k_grad.lod(), dense_k_grad.offset());
  argument_outputs.push_back(k_grad_dense_tensor_type);

  pir::Type v_grad_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_v_grad.dtype()), dense_v_grad.dims(), dense_v_grad.layout(), dense_v_grad.lod(), dense_v_grad.offset());
  argument_outputs.push_back(v_grad_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void FlashAttnGradOp::VerifySig() {}

void FlashAttnGradOp::InferMeta( phi::InferMetaContext *infer_meta ) {
  auto fn = PD_INFER_META(phi::FlashAttnGradInferMeta);
  fn(infer_meta);
}

phi::DataType FlashAttnGradOp::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: FlashAttnGradOp";
  


  return expected_kernel_dtype;
}

const char *FlashAttnUnpaddedGradOp::attributes_name[5] = { "max_seqlen_q", "max_seqlen_k", "scale", "dropout", "causal" };

OpInfoTuple FlashAttnUnpaddedGradOp::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("q", "paddle::dialect::DenseTensorType", false, false, false, false), paddle::dialect::OpInputInfo("k", "paddle::dialect::DenseTensorType", false, false, false, false), paddle::dialect::OpInputInfo("v", "paddle::dialect::DenseTensorType", false, false, false, false), paddle::dialect::OpInputInfo("cu_seqlens_q", "paddle::dialect::DenseTensorType", false, false, false, false), paddle::dialect::OpInputInfo("cu_seqlens_k", "paddle::dialect::DenseTensorType", false, false, false, false), paddle::dialect::OpInputInfo("out", "paddle::dialect::DenseTensorType", false, false, false, false), paddle::dialect::OpInputInfo("softmax_lse", "paddle::dialect::DenseTensorType", false, false, false, false), paddle::dialect::OpInputInfo("seed_offset", "paddle::dialect::DenseTensorType", false, false, false, false), paddle::dialect::OpInputInfo("attn_mask", "paddle::dialect::DenseTensorType", true, false, false, false), paddle::dialect::OpInputInfo("out_grad", "paddle::dialect::DenseTensorType", false, false, false, false) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = { paddle::dialect::OpAttributeInfo("max_seqlen_q", "pir::Int64Attribute", ""), paddle::dialect::OpAttributeInfo("max_seqlen_k", "pir::Int64Attribute", ""), paddle::dialect::OpAttributeInfo("scale", "pir::FloatAttribute", ""), paddle::dialect::OpAttributeInfo("dropout", "pir::FloatAttribute", ""), paddle::dialect::OpAttributeInfo("causal", "pir::BoolAttribute", "") };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("q_grad", "paddle::dialect::DenseTensorType", false, false), paddle::dialect::OpOutputInfo("k_grad", "paddle::dialect::DenseTensorType", false, false), paddle::dialect::OpOutputInfo("v_grad", "paddle::dialect::DenseTensorType", false, false) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("FlashAttnGradInferMeta", {"q", "k", "v"}, "flash_attn_unpadded_grad", {"q", "k", "v", "cu_seqlens_q", "cu_seqlens_k", "out", "softmax_lse", "seed_offset", "attn_mask", "out_grad", "max_seqlen_q", "max_seqlen_k", "scale", "dropout", "causal"}, {"q"}, {}, {}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "flash_attn_unpadded_grad");
}

void FlashAttnUnpaddedGradOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value q_, pir::Value k_, pir::Value v_, pir::Value cu_seqlens_q_, pir::Value cu_seqlens_k_, pir::Value out_, pir::Value softmax_lse_, pir::Value seed_offset_, pir::Value attn_mask_, pir::Value out_grad_, int64_t max_seqlen_q, int64_t max_seqlen_k, float scale, float dropout, bool causal) {
  VLOG(4) << "Start build FlashAttnUnpaddedGradOp";



  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {q_, k_, v_, cu_seqlens_q_, cu_seqlens_k_, out_, softmax_lse_, seed_offset_, attn_mask_, out_grad_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_max_seqlen_q = pir::Int64Attribute::get(pir::IrContext::Instance(), max_seqlen_q);
  argument.AddAttribute("max_seqlen_q", attr_max_seqlen_q);
  pir::Attribute attr_max_seqlen_k = pir::Int64Attribute::get(pir::IrContext::Instance(), max_seqlen_k);
  argument.AddAttribute("max_seqlen_k", attr_max_seqlen_k);
  pir::Attribute attr_scale = pir::FloatAttribute::get(pir::IrContext::Instance(), scale);
  argument.AddAttribute("scale", attr_scale);
  pir::Attribute attr_dropout = pir::FloatAttribute::get(pir::IrContext::Instance(), dropout);
  argument.AddAttribute("dropout", attr_dropout);
  pir::Attribute attr_causal = pir::BoolAttribute::get(pir::IrContext::Instance(), causal);
  argument.AddAttribute("causal", attr_causal);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType q = q_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)q;
  paddle::dialect::DenseTensorType k = k_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)k;
  paddle::dialect::DenseTensorType v = v_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)v;
  paddle::dialect::DenseTensorType cu_seqlens_q = cu_seqlens_q_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)cu_seqlens_q;
  paddle::dialect::DenseTensorType cu_seqlens_k = cu_seqlens_k_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)cu_seqlens_k;
  paddle::dialect::DenseTensorType out = out_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)out;
  paddle::dialect::DenseTensorType softmax_lse = softmax_lse_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)softmax_lse;
  paddle::dialect::DenseTensorType seed_offset = seed_offset_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)seed_offset;
  paddle::dialect::DenseTensorType out_grad = out_grad_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)out_grad;

  VLOG(4) << "Builder construction  dense_q";
  paddle::dialect::IrTensor ir_tensor_q(paddle::dialect::TransToPhiDataType(q.dtype()),
                                                      q.dims(),
                                                      q.data_layout(),
                                                      q.lod(),
                                                      q.offset());
  VLOG(4) << "Builder construction  meta_q";
  paddle::dialect::IrMetaTensor meta_q(&ir_tensor_q);

  VLOG(4) << "Builder construction  dense_k";
  paddle::dialect::IrTensor ir_tensor_k(paddle::dialect::TransToPhiDataType(k.dtype()),
                                                      k.dims(),
                                                      k.data_layout(),
                                                      k.lod(),
                                                      k.offset());
  VLOG(4) << "Builder construction  meta_k";
  paddle::dialect::IrMetaTensor meta_k(&ir_tensor_k);

  VLOG(4) << "Builder construction  dense_v";
  paddle::dialect::IrTensor ir_tensor_v(paddle::dialect::TransToPhiDataType(v.dtype()),
                                                      v.dims(),
                                                      v.data_layout(),
                                                      v.lod(),
                                                      v.offset());
  VLOG(4) << "Builder construction  meta_v";
  paddle::dialect::IrMetaTensor meta_v(&ir_tensor_v);
  paddle::dialect::IrTensor dense_q_grad;
  paddle::dialect::IrMetaTensor meta_q_grad(&dense_q_grad);
  paddle::dialect::IrTensor dense_k_grad;
  paddle::dialect::IrMetaTensor meta_k_grad(&dense_k_grad);
  paddle::dialect::IrTensor dense_v_grad;
  paddle::dialect::IrMetaTensor meta_v_grad(&dense_v_grad);

  phi::FlashAttnGradInferMeta(meta_q, meta_k, meta_v, &meta_q_grad, &meta_k_grad, &meta_v_grad);

  std::vector<pir::Type> argument_outputs;
  pir::Type q_grad_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_q_grad.dtype()), dense_q_grad.dims(), dense_q_grad.layout(), dense_q_grad.lod(), dense_q_grad.offset());
  argument_outputs.push_back(q_grad_dense_tensor_type);

  pir::Type k_grad_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_k_grad.dtype()), dense_k_grad.dims(), dense_k_grad.layout(), dense_k_grad.lod(), dense_k_grad.offset());
  argument_outputs.push_back(k_grad_dense_tensor_type);

  pir::Type v_grad_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_v_grad.dtype()), dense_v_grad.dims(), dense_v_grad.layout(), dense_v_grad.lod(), dense_v_grad.offset());
  argument_outputs.push_back(v_grad_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void FlashAttnUnpaddedGradOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value q_, pir::Value k_, pir::Value v_, pir::Value cu_seqlens_q_, pir::Value cu_seqlens_k_, pir::Value out_, pir::Value softmax_lse_, pir::Value seed_offset_, pir::Value attn_mask_, pir::Value out_grad_, pir::AttributeMap attributes) {
  VLOG(4) << "Start build FlashAttnUnpaddedGradOp";


  IR_ENFORCE(
      attributes.find("max_seqlen_q") != attributes.end(),
          "'max_seqlen_q' Attribute is expected for FlashAttnUnpaddedGradOp. ");
  int64_t max_seqlen_q = attributes.at("max_seqlen_q").dyn_cast<pir::Int64Attribute>().data();

  IR_ENFORCE(
      attributes.find("max_seqlen_k") != attributes.end(),
          "'max_seqlen_k' Attribute is expected for FlashAttnUnpaddedGradOp. ");
  int64_t max_seqlen_k = attributes.at("max_seqlen_k").dyn_cast<pir::Int64Attribute>().data();

  IR_ENFORCE(
      attributes.find("scale") != attributes.end(),
          "'scale' Attribute is expected for FlashAttnUnpaddedGradOp. ");
  float scale = attributes.at("scale").dyn_cast<pir::FloatAttribute>().data();

  IR_ENFORCE(
      attributes.find("dropout") != attributes.end(),
          "'dropout' Attribute is expected for FlashAttnUnpaddedGradOp. ");
  float dropout = attributes.at("dropout").dyn_cast<pir::FloatAttribute>().data();

  IR_ENFORCE(
      attributes.find("causal") != attributes.end(),
          "'causal' Attribute is expected for FlashAttnUnpaddedGradOp. ");
  bool causal = attributes.at("causal").dyn_cast<pir::BoolAttribute>().data();


  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {q_, k_, v_, cu_seqlens_q_, cu_seqlens_k_, out_, softmax_lse_, seed_offset_, attn_mask_, out_grad_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_max_seqlen_q = pir::Int64Attribute::get(pir::IrContext::Instance(), max_seqlen_q);
  argument.AddAttribute("max_seqlen_q", attr_max_seqlen_q);
  pir::Attribute attr_max_seqlen_k = pir::Int64Attribute::get(pir::IrContext::Instance(), max_seqlen_k);
  argument.AddAttribute("max_seqlen_k", attr_max_seqlen_k);
  pir::Attribute attr_scale = pir::FloatAttribute::get(pir::IrContext::Instance(), scale);
  argument.AddAttribute("scale", attr_scale);
  pir::Attribute attr_dropout = pir::FloatAttribute::get(pir::IrContext::Instance(), dropout);
  argument.AddAttribute("dropout", attr_dropout);
  pir::Attribute attr_causal = pir::BoolAttribute::get(pir::IrContext::Instance(), causal);
  argument.AddAttribute("causal", attr_causal);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType q = q_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)q;
  paddle::dialect::DenseTensorType k = k_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)k;
  paddle::dialect::DenseTensorType v = v_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)v;
  paddle::dialect::DenseTensorType cu_seqlens_q = cu_seqlens_q_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)cu_seqlens_q;
  paddle::dialect::DenseTensorType cu_seqlens_k = cu_seqlens_k_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)cu_seqlens_k;
  paddle::dialect::DenseTensorType out = out_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)out;
  paddle::dialect::DenseTensorType softmax_lse = softmax_lse_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)softmax_lse;
  paddle::dialect::DenseTensorType seed_offset = seed_offset_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)seed_offset;
  paddle::dialect::DenseTensorType out_grad = out_grad_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)out_grad;

  VLOG(4) << "Builder construction  dense_q";
  paddle::dialect::IrTensor ir_tensor_q(paddle::dialect::TransToPhiDataType(q.dtype()),
                                                      q.dims(),
                                                      q.data_layout(),
                                                      q.lod(),
                                                      q.offset());
  VLOG(4) << "Builder construction  meta_q";
  paddle::dialect::IrMetaTensor meta_q(&ir_tensor_q);

  VLOG(4) << "Builder construction  dense_k";
  paddle::dialect::IrTensor ir_tensor_k(paddle::dialect::TransToPhiDataType(k.dtype()),
                                                      k.dims(),
                                                      k.data_layout(),
                                                      k.lod(),
                                                      k.offset());
  VLOG(4) << "Builder construction  meta_k";
  paddle::dialect::IrMetaTensor meta_k(&ir_tensor_k);

  VLOG(4) << "Builder construction  dense_v";
  paddle::dialect::IrTensor ir_tensor_v(paddle::dialect::TransToPhiDataType(v.dtype()),
                                                      v.dims(),
                                                      v.data_layout(),
                                                      v.lod(),
                                                      v.offset());
  VLOG(4) << "Builder construction  meta_v";
  paddle::dialect::IrMetaTensor meta_v(&ir_tensor_v);
  paddle::dialect::IrTensor dense_q_grad;
  paddle::dialect::IrMetaTensor meta_q_grad(&dense_q_grad);
  paddle::dialect::IrTensor dense_k_grad;
  paddle::dialect::IrMetaTensor meta_k_grad(&dense_k_grad);
  paddle::dialect::IrTensor dense_v_grad;
  paddle::dialect::IrMetaTensor meta_v_grad(&dense_v_grad);

  phi::FlashAttnGradInferMeta(meta_q, meta_k, meta_v, &meta_q_grad, &meta_k_grad, &meta_v_grad);

  std::vector<pir::Type> argument_outputs;
  pir::Type q_grad_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_q_grad.dtype()), dense_q_grad.dims(), dense_q_grad.layout(), dense_q_grad.lod(), dense_q_grad.offset());
  argument_outputs.push_back(q_grad_dense_tensor_type);

  pir::Type k_grad_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_k_grad.dtype()), dense_k_grad.dims(), dense_k_grad.layout(), dense_k_grad.lod(), dense_k_grad.offset());
  argument_outputs.push_back(k_grad_dense_tensor_type);

  pir::Type v_grad_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_v_grad.dtype()), dense_v_grad.dims(), dense_v_grad.layout(), dense_v_grad.lod(), dense_v_grad.offset());
  argument_outputs.push_back(v_grad_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void FlashAttnUnpaddedGradOp::VerifySig() {}

void FlashAttnUnpaddedGradOp::InferMeta( phi::InferMetaContext *infer_meta ) {
  auto fn = PD_INFER_META(phi::FlashAttnGradInferMeta);
  fn(infer_meta);
}

phi::DataType FlashAttnUnpaddedGradOp::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: FlashAttnUnpaddedGradOp";
  


  return expected_kernel_dtype;
}

OpInfoTuple FlattenGradOp::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("xshape", "paddle::dialect::DenseTensorType", false, false, false, false), paddle::dialect::OpInputInfo("out_grad", "paddle::dialect::DenseTensorType", false, false, false, false) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = {  };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("x_grad", "paddle::dialect::DenseTensorType", false, false) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("KernelWithXShapeInferMeta", {"xshape", "out_grad"}, "flatten_grad", {"xshape", "out_grad"}, {"out_grad"}, {}, {}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "flatten_grad");
}

void FlattenGradOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value xshape_, pir::Value out_grad_) {
  VLOG(4) << "Start build FlattenGradOp";



  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {xshape_, out_grad_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType xshape = xshape_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)xshape;
  paddle::dialect::DenseTensorType out_grad = out_grad_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)out_grad;

  VLOG(4) << "Builder construction  dense_xshape";
  paddle::dialect::IrTensor ir_tensor_xshape(paddle::dialect::TransToPhiDataType(xshape.dtype()),
                                                      xshape.dims(),
                                                      xshape.data_layout(),
                                                      xshape.lod(),
                                                      xshape.offset());
  VLOG(4) << "Builder construction  meta_xshape";
  paddle::dialect::IrMetaTensor meta_xshape(&ir_tensor_xshape);

  VLOG(4) << "Builder construction  dense_out_grad";
  paddle::dialect::IrTensor ir_tensor_out_grad(paddle::dialect::TransToPhiDataType(out_grad.dtype()),
                                                      out_grad.dims(),
                                                      out_grad.data_layout(),
                                                      out_grad.lod(),
                                                      out_grad.offset());
  VLOG(4) << "Builder construction  meta_out_grad";
  paddle::dialect::IrMetaTensor meta_out_grad(&ir_tensor_out_grad);
  paddle::dialect::IrTensor dense_x_grad;
  paddle::dialect::IrMetaTensor meta_x_grad(&dense_x_grad);

  phi::KernelWithXShapeInferMeta(meta_xshape, meta_out_grad, &meta_x_grad);

  std::vector<pir::Type> argument_outputs;
  pir::Type x_grad_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_x_grad.dtype()), dense_x_grad.dims(), dense_x_grad.layout(), dense_x_grad.lod(), dense_x_grad.offset());
  argument_outputs.push_back(x_grad_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void FlattenGradOp::VerifySig() {}

void FlattenGradOp::InferMeta( phi::InferMetaContext *infer_meta ) {
  auto fn = PD_INFER_META(phi::KernelWithXShapeInferMeta);
  fn(infer_meta);
}

phi::DataType FlattenGradOp::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: FlattenGradOp";
  


  return expected_kernel_dtype;
}

OpInfoTuple FlattenGrad_Op::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("xshape", "paddle::dialect::DenseTensorType", false, false, false, false), paddle::dialect::OpInputInfo("out_grad", "paddle::dialect::DenseTensorType", false, false, false, false) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = {  };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("x_grad", "paddle::dialect::DenseTensorType", false, false) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("KernelWithXShapeInferMeta", {"xshape", "out_grad"}, "flatten_grad", {"xshape", "out_grad"}, {"out_grad"}, {}, {{"x_grad", "out_grad"}}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "flatten_grad");
}

void FlattenGrad_Op::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value xshape_, pir::Value out_grad_) {
  VLOG(4) << "Start build FlattenGrad_Op";



  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {xshape_, out_grad_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType xshape = xshape_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)xshape;
  paddle::dialect::DenseTensorType out_grad = out_grad_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)out_grad;

  VLOG(4) << "Builder construction  dense_xshape";
  paddle::dialect::IrTensor ir_tensor_xshape(paddle::dialect::TransToPhiDataType(xshape.dtype()),
                                                      xshape.dims(),
                                                      xshape.data_layout(),
                                                      xshape.lod(),
                                                      xshape.offset());
  VLOG(4) << "Builder construction  meta_xshape";
  paddle::dialect::IrMetaTensor meta_xshape(&ir_tensor_xshape);

  VLOG(4) << "Builder construction  dense_out_grad";
  paddle::dialect::IrTensor ir_tensor_out_grad(paddle::dialect::TransToPhiDataType(out_grad.dtype()),
                                                      out_grad.dims(),
                                                      out_grad.data_layout(),
                                                      out_grad.lod(),
                                                      out_grad.offset());
  VLOG(4) << "Builder construction  meta_out_grad";
  paddle::dialect::IrMetaTensor meta_out_grad(&ir_tensor_out_grad);
  paddle::dialect::IrTensor dense_x_grad;
  paddle::dialect::IrMetaTensor meta_x_grad(&dense_x_grad);

  phi::KernelWithXShapeInferMeta(meta_xshape, meta_out_grad, &meta_x_grad);

  std::vector<pir::Type> argument_outputs;
  pir::Type x_grad_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_x_grad.dtype()), dense_x_grad.dims(), dense_x_grad.layout(), dense_x_grad.lod(), dense_x_grad.offset());
  argument_outputs.push_back(x_grad_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void FlattenGrad_Op::VerifySig() {}

void FlattenGrad_Op::InferMeta( phi::InferMetaContext *infer_meta ) {
  auto fn = PD_INFER_META(phi::KernelWithXShapeInferMeta);
  fn(infer_meta);
}

phi::DataType FlattenGrad_Op::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: FlattenGrad_Op";
  


  return expected_kernel_dtype;
}

const char *FlipGradOp::attributes_name[1] = { "axis" };

OpInfoTuple FlipGradOp::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("out_grad", "paddle::dialect::DenseTensorType", false, false, false, false) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = { paddle::dialect::OpAttributeInfo("axis", "pir::ArrayAttribute<pir::Int32Attribute>", "") };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("x_grad", "paddle::dialect::DenseTensorType", false, false) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("", {""}, "", {""}, {}, {}, {}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "flip_grad");
}

void FlipGradOp::VerifySig() {}

phi::DataType FlipGradOp::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: FlipGradOp";
  


  return expected_kernel_dtype;
}

OpInfoTuple FloorGradOp::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("out_grad", "paddle::dialect::DenseTensorType", false, false, false, false) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = {  };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("x_grad", "paddle::dialect::DenseTensorType", false, false) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("UnchangedInferMeta", {"out_grad"}, "floor_grad", {"out_grad"}, {}, {}, {}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "floor_grad");
}

void FloorGradOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value out_grad_) {
  VLOG(4) << "Start build FloorGradOp";



  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {out_grad_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType out_grad = out_grad_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)out_grad;

  VLOG(4) << "Builder construction  dense_out_grad";
  paddle::dialect::IrTensor ir_tensor_out_grad(paddle::dialect::TransToPhiDataType(out_grad.dtype()),
                                                      out_grad.dims(),
                                                      out_grad.data_layout(),
                                                      out_grad.lod(),
                                                      out_grad.offset());
  VLOG(4) << "Builder construction  meta_out_grad";
  paddle::dialect::IrMetaTensor meta_out_grad(&ir_tensor_out_grad);
  paddle::dialect::IrTensor dense_x_grad;
  paddle::dialect::IrMetaTensor meta_x_grad(&dense_x_grad);

  phi::UnchangedInferMeta(meta_out_grad, &meta_x_grad);

  std::vector<pir::Type> argument_outputs;
  pir::Type x_grad_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_x_grad.dtype()), dense_x_grad.dims(), dense_x_grad.layout(), dense_x_grad.lod(), dense_x_grad.offset());
  argument_outputs.push_back(x_grad_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void FloorGradOp::VerifySig() {}

void FloorGradOp::InferMeta( phi::InferMetaContext *infer_meta ) {
  auto fn = PD_INFER_META(phi::UnchangedInferMeta);
  fn(infer_meta);
}

phi::DataType FloorGradOp::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: FloorGradOp";
  


  return expected_kernel_dtype;
}

OpInfoTuple FloorGrad_Op::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("out_grad", "paddle::dialect::DenseTensorType", false, false, false, false) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = {  };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("x_grad", "paddle::dialect::DenseTensorType", false, false) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("UnchangedInferMeta", {"out_grad"}, "floor_grad", {"out_grad"}, {}, {}, {{"x_grad", "out_grad"}}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "floor_grad");
}

void FloorGrad_Op::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value out_grad_) {
  VLOG(4) << "Start build FloorGrad_Op";



  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {out_grad_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType out_grad = out_grad_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)out_grad;

  VLOG(4) << "Builder construction  dense_out_grad";
  paddle::dialect::IrTensor ir_tensor_out_grad(paddle::dialect::TransToPhiDataType(out_grad.dtype()),
                                                      out_grad.dims(),
                                                      out_grad.data_layout(),
                                                      out_grad.lod(),
                                                      out_grad.offset());
  VLOG(4) << "Builder construction  meta_out_grad";
  paddle::dialect::IrMetaTensor meta_out_grad(&ir_tensor_out_grad);
  paddle::dialect::IrTensor dense_x_grad;
  paddle::dialect::IrMetaTensor meta_x_grad(&dense_x_grad);

  phi::UnchangedInferMeta(meta_out_grad, &meta_x_grad);

  std::vector<pir::Type> argument_outputs;
  pir::Type x_grad_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_x_grad.dtype()), dense_x_grad.dims(), dense_x_grad.layout(), dense_x_grad.lod(), dense_x_grad.offset());
  argument_outputs.push_back(x_grad_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void FloorGrad_Op::VerifySig() {}

void FloorGrad_Op::InferMeta( phi::InferMetaContext *infer_meta ) {
  auto fn = PD_INFER_META(phi::UnchangedInferMeta);
  fn(infer_meta);
}

phi::DataType FloorGrad_Op::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: FloorGrad_Op";
  


  return expected_kernel_dtype;
}

OpInfoTuple FmaxGradOp::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("x", "paddle::dialect::DenseTensorType", false, false, false, false), paddle::dialect::OpInputInfo("y", "paddle::dialect::DenseTensorType", false, false, false, false), paddle::dialect::OpInputInfo("out_grad", "paddle::dialect::DenseTensorType", false, false, false, false) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = {  };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("x_grad", "paddle::dialect::DenseTensorType", false, false), paddle::dialect::OpOutputInfo("y_grad", "paddle::dialect::DenseTensorType", false, false) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("GeneralBinaryGradInferMeta", {"x", "y"}, "fmax_grad", {"x", "y", "out_grad"}, {"out_grad"}, {}, {}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "fmax_grad");
}

void FmaxGradOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, pir::Value y_, pir::Value out_grad_) {
  VLOG(4) << "Start build FmaxGradOp";



  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_, y_, out_grad_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;
  paddle::dialect::DenseTensorType y = y_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)y;
  paddle::dialect::DenseTensorType out_grad = out_grad_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)out_grad;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);

  VLOG(4) << "Builder construction  dense_y";
  paddle::dialect::IrTensor ir_tensor_y(paddle::dialect::TransToPhiDataType(y.dtype()),
                                                      y.dims(),
                                                      y.data_layout(),
                                                      y.lod(),
                                                      y.offset());
  VLOG(4) << "Builder construction  meta_y";
  paddle::dialect::IrMetaTensor meta_y(&ir_tensor_y);
  paddle::dialect::IrTensor dense_x_grad;
  paddle::dialect::IrMetaTensor meta_x_grad(&dense_x_grad);
  paddle::dialect::IrTensor dense_y_grad;
  paddle::dialect::IrMetaTensor meta_y_grad(&dense_y_grad);

  phi::GeneralBinaryGradInferMeta(meta_x, meta_y, &meta_x_grad, &meta_y_grad);

  std::vector<pir::Type> argument_outputs;
  pir::Type x_grad_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_x_grad.dtype()), dense_x_grad.dims(), dense_x_grad.layout(), dense_x_grad.lod(), dense_x_grad.offset());
  argument_outputs.push_back(x_grad_dense_tensor_type);

  pir::Type y_grad_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_y_grad.dtype()), dense_y_grad.dims(), dense_y_grad.layout(), dense_y_grad.lod(), dense_y_grad.offset());
  argument_outputs.push_back(y_grad_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void FmaxGradOp::VerifySig() {}

void FmaxGradOp::InferMeta( phi::InferMetaContext *infer_meta ) {
  auto fn = PD_INFER_META(phi::GeneralBinaryGradInferMeta);
  fn(infer_meta);
}

phi::DataType FmaxGradOp::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: FmaxGradOp";
  


  // deal complex_promote
  if (framework::IsComplexType(expected_kernel_dtype)) {{
    // only promote inputs’s types when contains complex input
    return tensor_dtype;
  }}

  return expected_kernel_dtype;
}

OpInfoTuple FminGradOp::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("x", "paddle::dialect::DenseTensorType", false, false, false, false), paddle::dialect::OpInputInfo("y", "paddle::dialect::DenseTensorType", false, false, false, false), paddle::dialect::OpInputInfo("out_grad", "paddle::dialect::DenseTensorType", false, false, false, false) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = {  };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("x_grad", "paddle::dialect::DenseTensorType", false, false), paddle::dialect::OpOutputInfo("y_grad", "paddle::dialect::DenseTensorType", false, false) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("GeneralBinaryGradInferMeta", {"x", "y"}, "fmin_grad", {"x", "y", "out_grad"}, {"out_grad"}, {}, {}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "fmin_grad");
}

void FminGradOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, pir::Value y_, pir::Value out_grad_) {
  VLOG(4) << "Start build FminGradOp";



  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_, y_, out_grad_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;
  paddle::dialect::DenseTensorType y = y_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)y;
  paddle::dialect::DenseTensorType out_grad = out_grad_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)out_grad;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);

  VLOG(4) << "Builder construction  dense_y";
  paddle::dialect::IrTensor ir_tensor_y(paddle::dialect::TransToPhiDataType(y.dtype()),
                                                      y.dims(),
                                                      y.data_layout(),
                                                      y.lod(),
                                                      y.offset());
  VLOG(4) << "Builder construction  meta_y";
  paddle::dialect::IrMetaTensor meta_y(&ir_tensor_y);
  paddle::dialect::IrTensor dense_x_grad;
  paddle::dialect::IrMetaTensor meta_x_grad(&dense_x_grad);
  paddle::dialect::IrTensor dense_y_grad;
  paddle::dialect::IrMetaTensor meta_y_grad(&dense_y_grad);

  phi::GeneralBinaryGradInferMeta(meta_x, meta_y, &meta_x_grad, &meta_y_grad);

  std::vector<pir::Type> argument_outputs;
  pir::Type x_grad_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_x_grad.dtype()), dense_x_grad.dims(), dense_x_grad.layout(), dense_x_grad.lod(), dense_x_grad.offset());
  argument_outputs.push_back(x_grad_dense_tensor_type);

  pir::Type y_grad_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_y_grad.dtype()), dense_y_grad.dims(), dense_y_grad.layout(), dense_y_grad.lod(), dense_y_grad.offset());
  argument_outputs.push_back(y_grad_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void FminGradOp::VerifySig() {}

void FminGradOp::InferMeta( phi::InferMetaContext *infer_meta ) {
  auto fn = PD_INFER_META(phi::GeneralBinaryGradInferMeta);
  fn(infer_meta);
}

phi::DataType FminGradOp::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: FminGradOp";
  


  // deal complex_promote
  if (framework::IsComplexType(expected_kernel_dtype)) {{
    // only promote inputs’s types when contains complex input
    return tensor_dtype;
  }}

  return expected_kernel_dtype;
}

const char *FoldGradOp::attributes_name[5] = { "output_sizes", "kernel_sizes", "strides", "paddings", "dilations" };

OpInfoTuple FoldGradOp::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("x", "paddle::dialect::DenseTensorType", false, true, false, false), paddle::dialect::OpInputInfo("out_grad", "paddle::dialect::DenseTensorType", false, false, false, false) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = { paddle::dialect::OpAttributeInfo("output_sizes", "pir::ArrayAttribute<pir::Int32Attribute>", ""), paddle::dialect::OpAttributeInfo("kernel_sizes", "pir::ArrayAttribute<pir::Int32Attribute>", ""), paddle::dialect::OpAttributeInfo("strides", "pir::ArrayAttribute<pir::Int32Attribute>", ""), paddle::dialect::OpAttributeInfo("paddings", "pir::ArrayAttribute<pir::Int32Attribute>", ""), paddle::dialect::OpAttributeInfo("dilations", "pir::ArrayAttribute<pir::Int32Attribute>", "") };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("x_grad", "paddle::dialect::DenseTensorType", false, false) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("UnchangedInferMeta", {"x"}, "fold_grad", {"x", "out_grad", "output_sizes", "kernel_sizes", "strides", "paddings", "dilations"}, {"out_grad"}, {}, {}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "fold_grad");
}

void FoldGradOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, pir::Value out_grad_, const std::vector<int>& output_sizes, const std::vector<int>& kernel_sizes, const std::vector<int>& strides, const std::vector<int>& paddings, const std::vector<int>& dilations) {
  VLOG(4) << "Start build FoldGradOp";



  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_, out_grad_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  std::vector<pir::Attribute> vec_output_sizes;
  for (size_t i = 0; i < static_cast<size_t>(output_sizes.size()); i++) {
      pir::Attribute attr_output_sizes = pir::Int32Attribute::get(pir::IrContext::Instance(), output_sizes[i]);

    vec_output_sizes.push_back(attr_output_sizes);
  }
  pir::Attribute attr_output_sizes = pir::ArrayAttribute::get(pir::IrContext::Instance(), vec_output_sizes);
  argument.AddAttribute("output_sizes", attr_output_sizes);
  std::vector<pir::Attribute> vec_kernel_sizes;
  for (size_t i = 0; i < static_cast<size_t>(kernel_sizes.size()); i++) {
      pir::Attribute attr_kernel_sizes = pir::Int32Attribute::get(pir::IrContext::Instance(), kernel_sizes[i]);

    vec_kernel_sizes.push_back(attr_kernel_sizes);
  }
  pir::Attribute attr_kernel_sizes = pir::ArrayAttribute::get(pir::IrContext::Instance(), vec_kernel_sizes);
  argument.AddAttribute("kernel_sizes", attr_kernel_sizes);
  std::vector<pir::Attribute> vec_strides;
  for (size_t i = 0; i < static_cast<size_t>(strides.size()); i++) {
      pir::Attribute attr_strides = pir::Int32Attribute::get(pir::IrContext::Instance(), strides[i]);

    vec_strides.push_back(attr_strides);
  }
  pir::Attribute attr_strides = pir::ArrayAttribute::get(pir::IrContext::Instance(), vec_strides);
  argument.AddAttribute("strides", attr_strides);
  std::vector<pir::Attribute> vec_paddings;
  for (size_t i = 0; i < static_cast<size_t>(paddings.size()); i++) {
      pir::Attribute attr_paddings = pir::Int32Attribute::get(pir::IrContext::Instance(), paddings[i]);

    vec_paddings.push_back(attr_paddings);
  }
  pir::Attribute attr_paddings = pir::ArrayAttribute::get(pir::IrContext::Instance(), vec_paddings);
  argument.AddAttribute("paddings", attr_paddings);
  std::vector<pir::Attribute> vec_dilations;
  for (size_t i = 0; i < static_cast<size_t>(dilations.size()); i++) {
      pir::Attribute attr_dilations = pir::Int32Attribute::get(pir::IrContext::Instance(), dilations[i]);

    vec_dilations.push_back(attr_dilations);
  }
  pir::Attribute attr_dilations = pir::ArrayAttribute::get(pir::IrContext::Instance(), vec_dilations);
  argument.AddAttribute("dilations", attr_dilations);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;
  paddle::dialect::DenseTensorType out_grad = out_grad_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)out_grad;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);
  paddle::dialect::IrTensor dense_x_grad;
  paddle::dialect::IrMetaTensor meta_x_grad(&dense_x_grad);

  phi::UnchangedInferMeta(meta_x, &meta_x_grad);

  std::vector<pir::Type> argument_outputs;
  pir::Type x_grad_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_x_grad.dtype()), dense_x_grad.dims(), dense_x_grad.layout(), dense_x_grad.lod(), dense_x_grad.offset());
  argument_outputs.push_back(x_grad_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void FoldGradOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, pir::Value out_grad_, pir::AttributeMap attributes) {
  VLOG(4) << "Start build FoldGradOp";


  IR_ENFORCE(
      attributes.find("output_sizes") != attributes.end(),
          "'output_sizes' Attribute is expected for FoldGradOp. ");
  std::vector<int> output_sizes;
  for (size_t i = 0; i < attributes.at("output_sizes").dyn_cast<pir::ArrayAttribute>().size(); i++) {
    output_sizes.push_back(attributes.at("output_sizes").dyn_cast<pir::ArrayAttribute>().at(i).dyn_cast<pir::Int32Attribute>().data());
  }

  IR_ENFORCE(
      attributes.find("kernel_sizes") != attributes.end(),
          "'kernel_sizes' Attribute is expected for FoldGradOp. ");
  std::vector<int> kernel_sizes;
  for (size_t i = 0; i < attributes.at("kernel_sizes").dyn_cast<pir::ArrayAttribute>().size(); i++) {
    kernel_sizes.push_back(attributes.at("kernel_sizes").dyn_cast<pir::ArrayAttribute>().at(i).dyn_cast<pir::Int32Attribute>().data());
  }

  IR_ENFORCE(
      attributes.find("strides") != attributes.end(),
          "'strides' Attribute is expected for FoldGradOp. ");
  std::vector<int> strides;
  for (size_t i = 0; i < attributes.at("strides").dyn_cast<pir::ArrayAttribute>().size(); i++) {
    strides.push_back(attributes.at("strides").dyn_cast<pir::ArrayAttribute>().at(i).dyn_cast<pir::Int32Attribute>().data());
  }

  IR_ENFORCE(
      attributes.find("paddings") != attributes.end(),
          "'paddings' Attribute is expected for FoldGradOp. ");
  std::vector<int> paddings;
  for (size_t i = 0; i < attributes.at("paddings").dyn_cast<pir::ArrayAttribute>().size(); i++) {
    paddings.push_back(attributes.at("paddings").dyn_cast<pir::ArrayAttribute>().at(i).dyn_cast<pir::Int32Attribute>().data());
  }

  IR_ENFORCE(
      attributes.find("dilations") != attributes.end(),
          "'dilations' Attribute is expected for FoldGradOp. ");
  std::vector<int> dilations;
  for (size_t i = 0; i < attributes.at("dilations").dyn_cast<pir::ArrayAttribute>().size(); i++) {
    dilations.push_back(attributes.at("dilations").dyn_cast<pir::ArrayAttribute>().at(i).dyn_cast<pir::Int32Attribute>().data());
  }


  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_, out_grad_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  std::vector<pir::Attribute> vec_output_sizes;
  for (size_t i = 0; i < static_cast<size_t>(output_sizes.size()); i++) {
      pir::Attribute attr_output_sizes = pir::Int32Attribute::get(pir::IrContext::Instance(), output_sizes[i]);

    vec_output_sizes.push_back(attr_output_sizes);
  }
  pir::Attribute attr_output_sizes = pir::ArrayAttribute::get(pir::IrContext::Instance(), vec_output_sizes);
  argument.AddAttribute("output_sizes", attr_output_sizes);
  std::vector<pir::Attribute> vec_kernel_sizes;
  for (size_t i = 0; i < static_cast<size_t>(kernel_sizes.size()); i++) {
      pir::Attribute attr_kernel_sizes = pir::Int32Attribute::get(pir::IrContext::Instance(), kernel_sizes[i]);

    vec_kernel_sizes.push_back(attr_kernel_sizes);
  }
  pir::Attribute attr_kernel_sizes = pir::ArrayAttribute::get(pir::IrContext::Instance(), vec_kernel_sizes);
  argument.AddAttribute("kernel_sizes", attr_kernel_sizes);
  std::vector<pir::Attribute> vec_strides;
  for (size_t i = 0; i < static_cast<size_t>(strides.size()); i++) {
      pir::Attribute attr_strides = pir::Int32Attribute::get(pir::IrContext::Instance(), strides[i]);

    vec_strides.push_back(attr_strides);
  }
  pir::Attribute attr_strides = pir::ArrayAttribute::get(pir::IrContext::Instance(), vec_strides);
  argument.AddAttribute("strides", attr_strides);
  std::vector<pir::Attribute> vec_paddings;
  for (size_t i = 0; i < static_cast<size_t>(paddings.size()); i++) {
      pir::Attribute attr_paddings = pir::Int32Attribute::get(pir::IrContext::Instance(), paddings[i]);

    vec_paddings.push_back(attr_paddings);
  }
  pir::Attribute attr_paddings = pir::ArrayAttribute::get(pir::IrContext::Instance(), vec_paddings);
  argument.AddAttribute("paddings", attr_paddings);
  std::vector<pir::Attribute> vec_dilations;
  for (size_t i = 0; i < static_cast<size_t>(dilations.size()); i++) {
      pir::Attribute attr_dilations = pir::Int32Attribute::get(pir::IrContext::Instance(), dilations[i]);

    vec_dilations.push_back(attr_dilations);
  }
  pir::Attribute attr_dilations = pir::ArrayAttribute::get(pir::IrContext::Instance(), vec_dilations);
  argument.AddAttribute("dilations", attr_dilations);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;
  paddle::dialect::DenseTensorType out_grad = out_grad_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)out_grad;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);
  paddle::dialect::IrTensor dense_x_grad;
  paddle::dialect::IrMetaTensor meta_x_grad(&dense_x_grad);

  phi::UnchangedInferMeta(meta_x, &meta_x_grad);

  std::vector<pir::Type> argument_outputs;
  pir::Type x_grad_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_x_grad.dtype()), dense_x_grad.dims(), dense_x_grad.layout(), dense_x_grad.lod(), dense_x_grad.offset());
  argument_outputs.push_back(x_grad_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void FoldGradOp::VerifySig() {}

void FoldGradOp::InferMeta( phi::InferMetaContext *infer_meta ) {
  auto fn = PD_INFER_META(phi::UnchangedInferMeta);
  fn(infer_meta);
}

phi::DataType FoldGradOp::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: FoldGradOp";
  


  return expected_kernel_dtype;
}

const char *FrameGradOp::attributes_name[3] = { "frame_length", "hop_length", "axis" };

OpInfoTuple FrameGradOp::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("x", "paddle::dialect::DenseTensorType", false, false, false, false), paddle::dialect::OpInputInfo("out_grad", "paddle::dialect::DenseTensorType", false, false, false, false) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = { paddle::dialect::OpAttributeInfo("frame_length", "pir::Int32Attribute", ""), paddle::dialect::OpAttributeInfo("hop_length", "pir::Int32Attribute", ""), paddle::dialect::OpAttributeInfo("axis", "pir::Int32Attribute", "") };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("x_grad", "paddle::dialect::DenseTensorType", false, false) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("UnchangedInferMeta", {"x"}, "frame_grad", {"x", "out_grad", "frame_length", "hop_length", "axis"}, {}, {}, {}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "frame_grad");
}

void FrameGradOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, pir::Value out_grad_, int frame_length, int hop_length, int axis) {
  VLOG(4) << "Start build FrameGradOp";



  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_, out_grad_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_frame_length = pir::Int32Attribute::get(pir::IrContext::Instance(), frame_length);
  argument.AddAttribute("frame_length", attr_frame_length);
  pir::Attribute attr_hop_length = pir::Int32Attribute::get(pir::IrContext::Instance(), hop_length);
  argument.AddAttribute("hop_length", attr_hop_length);
  pir::Attribute attr_axis = pir::Int32Attribute::get(pir::IrContext::Instance(), axis);
  argument.AddAttribute("axis", attr_axis);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;
  paddle::dialect::DenseTensorType out_grad = out_grad_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)out_grad;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);
  paddle::dialect::IrTensor dense_x_grad;
  paddle::dialect::IrMetaTensor meta_x_grad(&dense_x_grad);

  phi::UnchangedInferMeta(meta_x, &meta_x_grad);

  std::vector<pir::Type> argument_outputs;
  pir::Type x_grad_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_x_grad.dtype()), dense_x_grad.dims(), dense_x_grad.layout(), dense_x_grad.lod(), dense_x_grad.offset());
  argument_outputs.push_back(x_grad_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void FrameGradOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, pir::Value out_grad_, pir::AttributeMap attributes) {
  VLOG(4) << "Start build FrameGradOp";


  IR_ENFORCE(
      attributes.find("frame_length") != attributes.end(),
          "'frame_length' Attribute is expected for FrameGradOp. ");
  int frame_length = attributes.at("frame_length").dyn_cast<pir::Int32Attribute>().data();

  IR_ENFORCE(
      attributes.find("hop_length") != attributes.end(),
          "'hop_length' Attribute is expected for FrameGradOp. ");
  int hop_length = attributes.at("hop_length").dyn_cast<pir::Int32Attribute>().data();

  IR_ENFORCE(
      attributes.find("axis") != attributes.end(),
          "'axis' Attribute is expected for FrameGradOp. ");
  int axis = attributes.at("axis").dyn_cast<pir::Int32Attribute>().data();


  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_, out_grad_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_frame_length = pir::Int32Attribute::get(pir::IrContext::Instance(), frame_length);
  argument.AddAttribute("frame_length", attr_frame_length);
  pir::Attribute attr_hop_length = pir::Int32Attribute::get(pir::IrContext::Instance(), hop_length);
  argument.AddAttribute("hop_length", attr_hop_length);
  pir::Attribute attr_axis = pir::Int32Attribute::get(pir::IrContext::Instance(), axis);
  argument.AddAttribute("axis", attr_axis);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;
  paddle::dialect::DenseTensorType out_grad = out_grad_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)out_grad;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);
  paddle::dialect::IrTensor dense_x_grad;
  paddle::dialect::IrMetaTensor meta_x_grad(&dense_x_grad);

  phi::UnchangedInferMeta(meta_x, &meta_x_grad);

  std::vector<pir::Type> argument_outputs;
  pir::Type x_grad_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_x_grad.dtype()), dense_x_grad.dims(), dense_x_grad.layout(), dense_x_grad.lod(), dense_x_grad.offset());
  argument_outputs.push_back(x_grad_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void FrameGradOp::VerifySig() {}

void FrameGradOp::InferMeta( phi::InferMetaContext *infer_meta ) {
  auto fn = PD_INFER_META(phi::UnchangedInferMeta);
  fn(infer_meta);
}

phi::DataType FrameGradOp::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: FrameGradOp";
  


  return expected_kernel_dtype;
}

OpInfoTuple GammalnGradOp::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("x", "paddle::dialect::DenseTensorType", false, false, false, false), paddle::dialect::OpInputInfo("out_grad", "paddle::dialect::DenseTensorType", false, false, false, false) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = {  };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("x_grad", "paddle::dialect::DenseTensorType", false, false) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("UnchangedInferMeta", {"x"}, "gammaln_grad", {"x", "out_grad"}, {}, {}, {}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "gammaln_grad");
}

void GammalnGradOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, pir::Value out_grad_) {
  VLOG(4) << "Start build GammalnGradOp";



  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_, out_grad_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;
  paddle::dialect::DenseTensorType out_grad = out_grad_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)out_grad;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);
  paddle::dialect::IrTensor dense_x_grad;
  paddle::dialect::IrMetaTensor meta_x_grad(&dense_x_grad);

  phi::UnchangedInferMeta(meta_x, &meta_x_grad);

  std::vector<pir::Type> argument_outputs;
  pir::Type x_grad_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_x_grad.dtype()), dense_x_grad.dims(), dense_x_grad.layout(), dense_x_grad.lod(), dense_x_grad.offset());
  argument_outputs.push_back(x_grad_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void GammalnGradOp::VerifySig() {}

void GammalnGradOp::InferMeta( phi::InferMetaContext *infer_meta ) {
  auto fn = PD_INFER_META(phi::UnchangedInferMeta);
  fn(infer_meta);
}

phi::DataType GammalnGradOp::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: GammalnGradOp";
  


  return expected_kernel_dtype;
}

OpInfoTuple GatherGradOp::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("x", "paddle::dialect::DenseTensorType", false, true, false, false), paddle::dialect::OpInputInfo("index", "paddle::dialect::DenseTensorType", false, false, false, false), paddle::dialect::OpInputInfo("out_grad", "paddle::dialect::DenseTensorType", false, false, false, false), paddle::dialect::OpInputInfo("axis", "paddle::dialect::ScalarAttribute", false, false, true, false) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = {  };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("x_grad", "paddle::dialect::DenseTensorType", false, false) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("GeneralUnaryGradInferMeta", {"x"}, "gather_grad", {"x", "index", "out_grad", "axis"}, {"out_grad"}, {}, {}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "gather_grad");
}

void GatherGradOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, pir::Value index_, pir::Value out_grad_, int axis) {
  VLOG(4) << "Start build GatherGradOp";


  // Generate scalar mutable attribute: axis
  paddle::dialect::FullOp full_axis_op = builder.Build<paddle::dialect::FullOp>(std::vector<int64_t>{1}, axis, phi::DataType::INT32, phi::CPUPlace());
  pir::OpResult axis_ = full_axis_op->result(0);
    
  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_, index_, out_grad_, axis_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;
  paddle::dialect::DenseTensorType index = index_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)index;
  paddle::dialect::DenseTensorType out_grad = out_grad_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)out_grad;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);
  paddle::dialect::IrTensor dense_x_grad;
  paddle::dialect::IrMetaTensor meta_x_grad(&dense_x_grad);

  phi::GeneralUnaryGradInferMeta(meta_x, &meta_x_grad);

  std::vector<pir::Type> argument_outputs;
  pir::Type x_grad_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_x_grad.dtype()), dense_x_grad.dims(), dense_x_grad.layout(), dense_x_grad.lod(), dense_x_grad.offset());
  argument_outputs.push_back(x_grad_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void GatherGradOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, pir::Value index_, pir::Value out_grad_, pir::AttributeMap attributes) {
  VLOG(4) << "Start build GatherGradOp";


  IR_ENFORCE(
      attributes.find("axis") != attributes.end(),
          "'axis' Attribute is expected for GatherGradOp. ");
  int axis = attributes.at("axis").dyn_cast<paddle::dialect::ScalarAttribute>().data().to<int>();

  // Generate scalar mutable attribute: axis
  paddle::dialect::FullOp full_axis_op = builder.Build<paddle::dialect::FullOp>(std::vector<int64_t>{1}, axis, phi::DataType::INT32, phi::CPUPlace());
  pir::OpResult axis_ = full_axis_op->result(0);
    
  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_, index_, out_grad_, axis_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;
  paddle::dialect::DenseTensorType index = index_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)index;
  paddle::dialect::DenseTensorType out_grad = out_grad_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)out_grad;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);
  paddle::dialect::IrTensor dense_x_grad;
  paddle::dialect::IrMetaTensor meta_x_grad(&dense_x_grad);

  phi::GeneralUnaryGradInferMeta(meta_x, &meta_x_grad);

  std::vector<pir::Type> argument_outputs;
  pir::Type x_grad_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_x_grad.dtype()), dense_x_grad.dims(), dense_x_grad.layout(), dense_x_grad.lod(), dense_x_grad.offset());
  argument_outputs.push_back(x_grad_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void GatherGradOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, pir::Value index_, pir::Value out_grad_, pir::Value axis_) {
  VLOG(4) << "Start build GatherGradOp";



  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_, index_, out_grad_, axis_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;
  paddle::dialect::DenseTensorType index = index_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)index;
  paddle::dialect::DenseTensorType out_grad = out_grad_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)out_grad;
  phi::Scalar axis;
  if (axis_.dyn_cast<pir::OpResult>() && axis_.dyn_cast<pir::OpResult>().owner()->isa<paddle::dialect::FullOp>()) {
    axis = std::move(phi::Scalar(axis_.dyn_cast<pir::OpResult>().owner()
                                  ->dyn_cast<paddle::dialect::FullOp>()
                                  .attribute("value")
                                  .dyn_cast<paddle::dialect::ScalarAttribute>()
                                  .data()
                                  .to<int>()));
  }
  else {
    axis = std::move(phi::Scalar(-1));
    axis.SetFromTensor(true);
  }


  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);
  paddle::dialect::IrTensor dense_x_grad;
  paddle::dialect::IrMetaTensor meta_x_grad(&dense_x_grad);

  phi::GeneralUnaryGradInferMeta(meta_x, &meta_x_grad);

  std::vector<pir::Type> argument_outputs;
  pir::Type x_grad_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_x_grad.dtype()), dense_x_grad.dims(), dense_x_grad.layout(), dense_x_grad.lod(), dense_x_grad.offset());
  argument_outputs.push_back(x_grad_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void GatherGradOp::VerifySig() {}

void GatherGradOp::InferMeta( phi::InferMetaContext *infer_meta ) {
  auto fn = PD_INFER_META(phi::GeneralUnaryGradInferMeta);
  fn(infer_meta);
}

phi::DataType GatherGradOp::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: GatherGradOp";
  


  return expected_kernel_dtype;
}

OpInfoTuple GatherNdGradOp::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("x", "paddle::dialect::DenseTensorType", false, true, false, false), paddle::dialect::OpInputInfo("index", "paddle::dialect::DenseTensorType", false, false, false, false), paddle::dialect::OpInputInfo("out_grad", "paddle::dialect::DenseTensorType", false, false, false, false) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = {  };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("x_grad", "paddle::dialect::DenseTensorType", false, false) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("GatherNdGradInferMeta", {"x", "index", "out_grad"}, "gather_nd_grad", {"x", "index", "out_grad"}, {}, {}, {}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "gather_nd_grad");
}

void GatherNdGradOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, pir::Value index_, pir::Value out_grad_) {
  VLOG(4) << "Start build GatherNdGradOp";



  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_, index_, out_grad_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;
  paddle::dialect::DenseTensorType index = index_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)index;
  paddle::dialect::DenseTensorType out_grad = out_grad_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)out_grad;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);

  VLOG(4) << "Builder construction  dense_index";
  paddle::dialect::IrTensor ir_tensor_index(paddle::dialect::TransToPhiDataType(index.dtype()),
                                                      index.dims(),
                                                      index.data_layout(),
                                                      index.lod(),
                                                      index.offset());
  VLOG(4) << "Builder construction  meta_index";
  paddle::dialect::IrMetaTensor meta_index(&ir_tensor_index);

  VLOG(4) << "Builder construction  dense_out_grad";
  paddle::dialect::IrTensor ir_tensor_out_grad(paddle::dialect::TransToPhiDataType(out_grad.dtype()),
                                                      out_grad.dims(),
                                                      out_grad.data_layout(),
                                                      out_grad.lod(),
                                                      out_grad.offset());
  VLOG(4) << "Builder construction  meta_out_grad";
  paddle::dialect::IrMetaTensor meta_out_grad(&ir_tensor_out_grad);
  paddle::dialect::IrTensor dense_x_grad;
  paddle::dialect::IrMetaTensor meta_x_grad(&dense_x_grad);

  phi::GatherNdGradInferMeta(meta_x, meta_index, meta_out_grad, &meta_x_grad);

  std::vector<pir::Type> argument_outputs;
  pir::Type x_grad_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_x_grad.dtype()), dense_x_grad.dims(), dense_x_grad.layout(), dense_x_grad.lod(), dense_x_grad.offset());
  argument_outputs.push_back(x_grad_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void GatherNdGradOp::VerifySig() {}

void GatherNdGradOp::InferMeta( phi::InferMetaContext *infer_meta ) {
  auto fn = PD_INFER_META(phi::GatherNdGradInferMeta);
  fn(infer_meta);
}

phi::DataType GatherNdGradOp::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: GatherNdGradOp";
  

  // deal skip data transform
  if (var_name == "index"){
    return expected_kernel_dtype;
  }


  return expected_kernel_dtype;
}

const char *GaussianInplaceGradOp::attributes_name[3] = { "mean", "std", "seed" };

OpInfoTuple GaussianInplaceGradOp::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("out_grad", "paddle::dialect::DenseTensorType", false, false, false, false) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = { paddle::dialect::OpAttributeInfo("mean", "pir::FloatAttribute", ""), paddle::dialect::OpAttributeInfo("std", "pir::FloatAttribute", ""), paddle::dialect::OpAttributeInfo("seed", "pir::Int32Attribute", "") };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("x_grad", "paddle::dialect::DenseTensorType", false, false) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("UnchangedInferMeta", {"out_grad"}, "gaussian_inplace_grad", {"out_grad", "mean", "std", "seed"}, {}, {}, {}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "gaussian_inplace_grad");
}

void GaussianInplaceGradOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value out_grad_, float mean, float std, int seed) {
  VLOG(4) << "Start build GaussianInplaceGradOp";



  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {out_grad_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_mean = pir::FloatAttribute::get(pir::IrContext::Instance(), mean);
  argument.AddAttribute("mean", attr_mean);
  pir::Attribute attr_std = pir::FloatAttribute::get(pir::IrContext::Instance(), std);
  argument.AddAttribute("std", attr_std);
  pir::Attribute attr_seed = pir::Int32Attribute::get(pir::IrContext::Instance(), seed);
  argument.AddAttribute("seed", attr_seed);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType out_grad = out_grad_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)out_grad;

  VLOG(4) << "Builder construction  dense_out_grad";
  paddle::dialect::IrTensor ir_tensor_out_grad(paddle::dialect::TransToPhiDataType(out_grad.dtype()),
                                                      out_grad.dims(),
                                                      out_grad.data_layout(),
                                                      out_grad.lod(),
                                                      out_grad.offset());
  VLOG(4) << "Builder construction  meta_out_grad";
  paddle::dialect::IrMetaTensor meta_out_grad(&ir_tensor_out_grad);
  paddle::dialect::IrTensor dense_x_grad;
  paddle::dialect::IrMetaTensor meta_x_grad(&dense_x_grad);

  phi::UnchangedInferMeta(meta_out_grad, &meta_x_grad);

  std::vector<pir::Type> argument_outputs;
  pir::Type x_grad_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_x_grad.dtype()), dense_x_grad.dims(), dense_x_grad.layout(), dense_x_grad.lod(), dense_x_grad.offset());
  argument_outputs.push_back(x_grad_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void GaussianInplaceGradOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value out_grad_, pir::AttributeMap attributes) {
  VLOG(4) << "Start build GaussianInplaceGradOp";


  IR_ENFORCE(
      attributes.find("mean") != attributes.end(),
          "'mean' Attribute is expected for GaussianInplaceGradOp. ");
  float mean = attributes.at("mean").dyn_cast<pir::FloatAttribute>().data();

  IR_ENFORCE(
      attributes.find("std") != attributes.end(),
          "'std' Attribute is expected for GaussianInplaceGradOp. ");
  float std = attributes.at("std").dyn_cast<pir::FloatAttribute>().data();

  IR_ENFORCE(
      attributes.find("seed") != attributes.end(),
          "'seed' Attribute is expected for GaussianInplaceGradOp. ");
  int seed = attributes.at("seed").dyn_cast<pir::Int32Attribute>().data();


  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {out_grad_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_mean = pir::FloatAttribute::get(pir::IrContext::Instance(), mean);
  argument.AddAttribute("mean", attr_mean);
  pir::Attribute attr_std = pir::FloatAttribute::get(pir::IrContext::Instance(), std);
  argument.AddAttribute("std", attr_std);
  pir::Attribute attr_seed = pir::Int32Attribute::get(pir::IrContext::Instance(), seed);
  argument.AddAttribute("seed", attr_seed);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType out_grad = out_grad_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)out_grad;

  VLOG(4) << "Builder construction  dense_out_grad";
  paddle::dialect::IrTensor ir_tensor_out_grad(paddle::dialect::TransToPhiDataType(out_grad.dtype()),
                                                      out_grad.dims(),
                                                      out_grad.data_layout(),
                                                      out_grad.lod(),
                                                      out_grad.offset());
  VLOG(4) << "Builder construction  meta_out_grad";
  paddle::dialect::IrMetaTensor meta_out_grad(&ir_tensor_out_grad);
  paddle::dialect::IrTensor dense_x_grad;
  paddle::dialect::IrMetaTensor meta_x_grad(&dense_x_grad);

  phi::UnchangedInferMeta(meta_out_grad, &meta_x_grad);

  std::vector<pir::Type> argument_outputs;
  pir::Type x_grad_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_x_grad.dtype()), dense_x_grad.dims(), dense_x_grad.layout(), dense_x_grad.lod(), dense_x_grad.offset());
  argument_outputs.push_back(x_grad_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void GaussianInplaceGradOp::VerifySig() {}

void GaussianInplaceGradOp::InferMeta( phi::InferMetaContext *infer_meta ) {
  auto fn = PD_INFER_META(phi::UnchangedInferMeta);
  fn(infer_meta);
}

phi::DataType GaussianInplaceGradOp::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: GaussianInplaceGradOp";
  


  return expected_kernel_dtype;
}

const char *GaussianInplaceGrad_Op::attributes_name[3] = { "mean", "std", "seed" };

OpInfoTuple GaussianInplaceGrad_Op::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("out_grad", "paddle::dialect::DenseTensorType", false, false, false, false) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = { paddle::dialect::OpAttributeInfo("mean", "pir::FloatAttribute", ""), paddle::dialect::OpAttributeInfo("std", "pir::FloatAttribute", ""), paddle::dialect::OpAttributeInfo("seed", "pir::Int32Attribute", "") };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("x_grad", "paddle::dialect::DenseTensorType", false, false) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("UnchangedInferMeta", {"out_grad"}, "gaussian_inplace_grad", {"out_grad", "mean", "std", "seed"}, {}, {}, {{"x_grad", "out_grad"}}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "gaussian_inplace_grad");
}

void GaussianInplaceGrad_Op::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value out_grad_, float mean, float std, int seed) {
  VLOG(4) << "Start build GaussianInplaceGrad_Op";



  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {out_grad_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_mean = pir::FloatAttribute::get(pir::IrContext::Instance(), mean);
  argument.AddAttribute("mean", attr_mean);
  pir::Attribute attr_std = pir::FloatAttribute::get(pir::IrContext::Instance(), std);
  argument.AddAttribute("std", attr_std);
  pir::Attribute attr_seed = pir::Int32Attribute::get(pir::IrContext::Instance(), seed);
  argument.AddAttribute("seed", attr_seed);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType out_grad = out_grad_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)out_grad;

  VLOG(4) << "Builder construction  dense_out_grad";
  paddle::dialect::IrTensor ir_tensor_out_grad(paddle::dialect::TransToPhiDataType(out_grad.dtype()),
                                                      out_grad.dims(),
                                                      out_grad.data_layout(),
                                                      out_grad.lod(),
                                                      out_grad.offset());
  VLOG(4) << "Builder construction  meta_out_grad";
  paddle::dialect::IrMetaTensor meta_out_grad(&ir_tensor_out_grad);
  paddle::dialect::IrTensor dense_x_grad;
  paddle::dialect::IrMetaTensor meta_x_grad(&dense_x_grad);

  phi::UnchangedInferMeta(meta_out_grad, &meta_x_grad);

  std::vector<pir::Type> argument_outputs;
  pir::Type x_grad_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_x_grad.dtype()), dense_x_grad.dims(), dense_x_grad.layout(), dense_x_grad.lod(), dense_x_grad.offset());
  argument_outputs.push_back(x_grad_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void GaussianInplaceGrad_Op::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value out_grad_, pir::AttributeMap attributes) {
  VLOG(4) << "Start build GaussianInplaceGrad_Op";


  IR_ENFORCE(
      attributes.find("mean") != attributes.end(),
          "'mean' Attribute is expected for GaussianInplaceGrad_Op. ");
  float mean = attributes.at("mean").dyn_cast<pir::FloatAttribute>().data();

  IR_ENFORCE(
      attributes.find("std") != attributes.end(),
          "'std' Attribute is expected for GaussianInplaceGrad_Op. ");
  float std = attributes.at("std").dyn_cast<pir::FloatAttribute>().data();

  IR_ENFORCE(
      attributes.find("seed") != attributes.end(),
          "'seed' Attribute is expected for GaussianInplaceGrad_Op. ");
  int seed = attributes.at("seed").dyn_cast<pir::Int32Attribute>().data();


  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {out_grad_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_mean = pir::FloatAttribute::get(pir::IrContext::Instance(), mean);
  argument.AddAttribute("mean", attr_mean);
  pir::Attribute attr_std = pir::FloatAttribute::get(pir::IrContext::Instance(), std);
  argument.AddAttribute("std", attr_std);
  pir::Attribute attr_seed = pir::Int32Attribute::get(pir::IrContext::Instance(), seed);
  argument.AddAttribute("seed", attr_seed);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType out_grad = out_grad_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)out_grad;

  VLOG(4) << "Builder construction  dense_out_grad";
  paddle::dialect::IrTensor ir_tensor_out_grad(paddle::dialect::TransToPhiDataType(out_grad.dtype()),
                                                      out_grad.dims(),
                                                      out_grad.data_layout(),
                                                      out_grad.lod(),
                                                      out_grad.offset());
  VLOG(4) << "Builder construction  meta_out_grad";
  paddle::dialect::IrMetaTensor meta_out_grad(&ir_tensor_out_grad);
  paddle::dialect::IrTensor dense_x_grad;
  paddle::dialect::IrMetaTensor meta_x_grad(&dense_x_grad);

  phi::UnchangedInferMeta(meta_out_grad, &meta_x_grad);

  std::vector<pir::Type> argument_outputs;
  pir::Type x_grad_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_x_grad.dtype()), dense_x_grad.dims(), dense_x_grad.layout(), dense_x_grad.lod(), dense_x_grad.offset());
  argument_outputs.push_back(x_grad_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void GaussianInplaceGrad_Op::VerifySig() {}

void GaussianInplaceGrad_Op::InferMeta( phi::InferMetaContext *infer_meta ) {
  auto fn = PD_INFER_META(phi::UnchangedInferMeta);
  fn(infer_meta);
}

phi::DataType GaussianInplaceGrad_Op::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: GaussianInplaceGrad_Op";
  


  return expected_kernel_dtype;
}

const char *GeluGradOp::attributes_name[1] = { "approximate" };

OpInfoTuple GeluGradOp::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("x", "paddle::dialect::DenseTensorType", false, false, false, false), paddle::dialect::OpInputInfo("out_grad", "paddle::dialect::DenseTensorType", false, false, false, false) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = { paddle::dialect::OpAttributeInfo("approximate", "pir::BoolAttribute", "") };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("x_grad", "paddle::dialect::DenseTensorType", false, false) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("UnchangedInferMeta", {"x"}, "gelu_grad", {"x", "out_grad", "approximate"}, {}, {}, {}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "gelu_grad");
}

void GeluGradOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, pir::Value out_grad_, bool approximate) {
  VLOG(4) << "Start build GeluGradOp";



  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_, out_grad_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_approximate = pir::BoolAttribute::get(pir::IrContext::Instance(), approximate);
  argument.AddAttribute("approximate", attr_approximate);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;
  paddle::dialect::DenseTensorType out_grad = out_grad_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)out_grad;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);
  paddle::dialect::IrTensor dense_x_grad;
  paddle::dialect::IrMetaTensor meta_x_grad(&dense_x_grad);

  phi::UnchangedInferMeta(meta_x, &meta_x_grad);

  std::vector<pir::Type> argument_outputs;
  pir::Type x_grad_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_x_grad.dtype()), dense_x_grad.dims(), dense_x_grad.layout(), dense_x_grad.lod(), dense_x_grad.offset());
  argument_outputs.push_back(x_grad_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void GeluGradOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, pir::Value out_grad_, pir::AttributeMap attributes) {
  VLOG(4) << "Start build GeluGradOp";


  IR_ENFORCE(
      attributes.find("approximate") != attributes.end(),
          "'approximate' Attribute is expected for GeluGradOp. ");
  bool approximate = attributes.at("approximate").dyn_cast<pir::BoolAttribute>().data();


  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_, out_grad_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_approximate = pir::BoolAttribute::get(pir::IrContext::Instance(), approximate);
  argument.AddAttribute("approximate", attr_approximate);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;
  paddle::dialect::DenseTensorType out_grad = out_grad_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)out_grad;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);
  paddle::dialect::IrTensor dense_x_grad;
  paddle::dialect::IrMetaTensor meta_x_grad(&dense_x_grad);

  phi::UnchangedInferMeta(meta_x, &meta_x_grad);

  std::vector<pir::Type> argument_outputs;
  pir::Type x_grad_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_x_grad.dtype()), dense_x_grad.dims(), dense_x_grad.layout(), dense_x_grad.lod(), dense_x_grad.offset());
  argument_outputs.push_back(x_grad_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void GeluGradOp::VerifySig() {}

void GeluGradOp::InferMeta( phi::InferMetaContext *infer_meta ) {
  auto fn = PD_INFER_META(phi::UnchangedInferMeta);
  fn(infer_meta);
}

phi::DataType GeluGradOp::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: GeluGradOp";
  


  return expected_kernel_dtype;
}

const char *GridSampleGradOp::attributes_name[3] = { "mode", "padding_mode", "align_corners" };

OpInfoTuple GridSampleGradOp::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("x", "paddle::dialect::DenseTensorType", false, false, false, false), paddle::dialect::OpInputInfo("grid", "paddle::dialect::DenseTensorType", false, false, false, false), paddle::dialect::OpInputInfo("out_grad", "paddle::dialect::DenseTensorType", false, false, false, false) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = { paddle::dialect::OpAttributeInfo("mode", "pir::StrAttribute", ""), paddle::dialect::OpAttributeInfo("padding_mode", "pir::StrAttribute", ""), paddle::dialect::OpAttributeInfo("align_corners", "pir::BoolAttribute", "") };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("x_grad", "paddle::dialect::DenseTensorType", false, false), paddle::dialect::OpOutputInfo("grid_grad", "paddle::dialect::DenseTensorType", false, false) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("GeneralBinaryGradInferMeta", {"x", "grid"}, "grid_sample_grad", {"x", "grid", "out_grad", "mode", "padding_mode", "align_corners"}, {"x"}, {}, {}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "grid_sample_grad");
}

void GridSampleGradOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, pir::Value grid_, pir::Value out_grad_, const std::string& mode, const std::string& padding_mode, bool align_corners) {
  VLOG(4) << "Start build GridSampleGradOp";



  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_, grid_, out_grad_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_mode = pir::StrAttribute::get(pir::IrContext::Instance(), mode);
  argument.AddAttribute("mode", attr_mode);
  pir::Attribute attr_padding_mode = pir::StrAttribute::get(pir::IrContext::Instance(), padding_mode);
  argument.AddAttribute("padding_mode", attr_padding_mode);
  pir::Attribute attr_align_corners = pir::BoolAttribute::get(pir::IrContext::Instance(), align_corners);
  argument.AddAttribute("align_corners", attr_align_corners);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;
  paddle::dialect::DenseTensorType grid = grid_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)grid;
  paddle::dialect::DenseTensorType out_grad = out_grad_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)out_grad;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);

  VLOG(4) << "Builder construction  dense_grid";
  paddle::dialect::IrTensor ir_tensor_grid(paddle::dialect::TransToPhiDataType(grid.dtype()),
                                                      grid.dims(),
                                                      grid.data_layout(),
                                                      grid.lod(),
                                                      grid.offset());
  VLOG(4) << "Builder construction  meta_grid";
  paddle::dialect::IrMetaTensor meta_grid(&ir_tensor_grid);
  paddle::dialect::IrTensor dense_x_grad;
  paddle::dialect::IrMetaTensor meta_x_grad(&dense_x_grad);
  paddle::dialect::IrTensor dense_grid_grad;
  paddle::dialect::IrMetaTensor meta_grid_grad(&dense_grid_grad);

  phi::GeneralBinaryGradInferMeta(meta_x, meta_grid, &meta_x_grad, &meta_grid_grad);

  std::vector<pir::Type> argument_outputs;
  pir::Type x_grad_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_x_grad.dtype()), dense_x_grad.dims(), dense_x_grad.layout(), dense_x_grad.lod(), dense_x_grad.offset());
  argument_outputs.push_back(x_grad_dense_tensor_type);

  pir::Type grid_grad_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_grid_grad.dtype()), dense_grid_grad.dims(), dense_grid_grad.layout(), dense_grid_grad.lod(), dense_grid_grad.offset());
  argument_outputs.push_back(grid_grad_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void GridSampleGradOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, pir::Value grid_, pir::Value out_grad_, pir::AttributeMap attributes) {
  VLOG(4) << "Start build GridSampleGradOp";


  IR_ENFORCE(
      attributes.find("mode") != attributes.end(),
          "'mode' Attribute is expected for GridSampleGradOp. ");
  std::string mode = attributes.at("mode").dyn_cast<pir::StrAttribute>().AsString();

  IR_ENFORCE(
      attributes.find("padding_mode") != attributes.end(),
          "'padding_mode' Attribute is expected for GridSampleGradOp. ");
  std::string padding_mode = attributes.at("padding_mode").dyn_cast<pir::StrAttribute>().AsString();

  IR_ENFORCE(
      attributes.find("align_corners") != attributes.end(),
          "'align_corners' Attribute is expected for GridSampleGradOp. ");
  bool align_corners = attributes.at("align_corners").dyn_cast<pir::BoolAttribute>().data();


  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_, grid_, out_grad_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_mode = pir::StrAttribute::get(pir::IrContext::Instance(), mode);
  argument.AddAttribute("mode", attr_mode);
  pir::Attribute attr_padding_mode = pir::StrAttribute::get(pir::IrContext::Instance(), padding_mode);
  argument.AddAttribute("padding_mode", attr_padding_mode);
  pir::Attribute attr_align_corners = pir::BoolAttribute::get(pir::IrContext::Instance(), align_corners);
  argument.AddAttribute("align_corners", attr_align_corners);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;
  paddle::dialect::DenseTensorType grid = grid_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)grid;
  paddle::dialect::DenseTensorType out_grad = out_grad_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)out_grad;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);

  VLOG(4) << "Builder construction  dense_grid";
  paddle::dialect::IrTensor ir_tensor_grid(paddle::dialect::TransToPhiDataType(grid.dtype()),
                                                      grid.dims(),
                                                      grid.data_layout(),
                                                      grid.lod(),
                                                      grid.offset());
  VLOG(4) << "Builder construction  meta_grid";
  paddle::dialect::IrMetaTensor meta_grid(&ir_tensor_grid);
  paddle::dialect::IrTensor dense_x_grad;
  paddle::dialect::IrMetaTensor meta_x_grad(&dense_x_grad);
  paddle::dialect::IrTensor dense_grid_grad;
  paddle::dialect::IrMetaTensor meta_grid_grad(&dense_grid_grad);

  phi::GeneralBinaryGradInferMeta(meta_x, meta_grid, &meta_x_grad, &meta_grid_grad);

  std::vector<pir::Type> argument_outputs;
  pir::Type x_grad_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_x_grad.dtype()), dense_x_grad.dims(), dense_x_grad.layout(), dense_x_grad.lod(), dense_x_grad.offset());
  argument_outputs.push_back(x_grad_dense_tensor_type);

  pir::Type grid_grad_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_grid_grad.dtype()), dense_grid_grad.dims(), dense_grid_grad.layout(), dense_grid_grad.lod(), dense_grid_grad.offset());
  argument_outputs.push_back(grid_grad_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void GridSampleGradOp::VerifySig() {}

void GridSampleGradOp::InferMeta( phi::InferMetaContext *infer_meta ) {
  auto fn = PD_INFER_META(phi::GeneralBinaryGradInferMeta);
  fn(infer_meta);
}

phi::DataType GridSampleGradOp::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: GridSampleGradOp";
  


  return expected_kernel_dtype;
}

const char *GroupNormGradOp::attributes_name[3] = { "epsilon", "groups", "data_layout" };

OpInfoTuple GroupNormGradOp::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("x", "paddle::dialect::DenseTensorType", false, false, false, false), paddle::dialect::OpInputInfo("scale", "paddle::dialect::DenseTensorType", true, false, false, false), paddle::dialect::OpInputInfo("bias", "paddle::dialect::DenseTensorType", true, false, false, false), paddle::dialect::OpInputInfo("y", "paddle::dialect::DenseTensorType", false, false, false, false), paddle::dialect::OpInputInfo("mean", "paddle::dialect::DenseTensorType", false, false, false, false), paddle::dialect::OpInputInfo("variance", "paddle::dialect::DenseTensorType", false, false, false, false), paddle::dialect::OpInputInfo("y_grad", "paddle::dialect::DenseTensorType", false, false, false, false) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = { paddle::dialect::OpAttributeInfo("epsilon", "pir::FloatAttribute", ""), paddle::dialect::OpAttributeInfo("groups", "pir::Int32Attribute", ""), paddle::dialect::OpAttributeInfo("data_layout", "pir::StrAttribute", "") };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("x_grad", "paddle::dialect::DenseTensorType", false, false), paddle::dialect::OpOutputInfo("scale_grad", "paddle::dialect::DenseTensorType", false, false), paddle::dialect::OpOutputInfo("bias_grad", "paddle::dialect::DenseTensorType", false, false) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("GeneralTernaryGradInferMeta", {"y", "scale", "bias"}, "group_norm_grad", {"x", "scale", "bias", "y", "mean", "variance", "y_grad", "epsilon", "groups", "data_layout"}, {"y_grad"}, {}, {}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "group_norm_grad");
}

void GroupNormGradOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, pir::Value scale_, pir::Value bias_, pir::Value y_, pir::Value mean_, pir::Value variance_, pir::Value y_grad_, float epsilon, int groups, const std::string& data_layout) {
  VLOG(4) << "Start build GroupNormGradOp";



  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_, scale_, bias_, y_, mean_, variance_, y_grad_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_epsilon = pir::FloatAttribute::get(pir::IrContext::Instance(), epsilon);
  argument.AddAttribute("epsilon", attr_epsilon);
  pir::Attribute attr_groups = pir::Int32Attribute::get(pir::IrContext::Instance(), groups);
  argument.AddAttribute("groups", attr_groups);
  pir::Attribute attr_data_layout = pir::StrAttribute::get(pir::IrContext::Instance(), data_layout);
  argument.AddAttribute("data_layout", attr_data_layout);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;
  paddle::dialect::DenseTensorType y = y_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)y;
  paddle::dialect::DenseTensorType mean = mean_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)mean;
  paddle::dialect::DenseTensorType variance = variance_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)variance;
  paddle::dialect::DenseTensorType y_grad = y_grad_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)y_grad;

  VLOG(4) << "Builder construction  dense_y";
  paddle::dialect::IrTensor ir_tensor_y(paddle::dialect::TransToPhiDataType(y.dtype()),
                                                      y.dims(),
                                                      y.data_layout(),
                                                      y.lod(),
                                                      y.offset());
  VLOG(4) << "Builder construction  meta_y";
  paddle::dialect::IrMetaTensor meta_y(&ir_tensor_y);

  paddle::dialect::IrMetaTensor meta_scale;
  paddle::dialect::IrTensor ir_tensor_scale;
  if (scale_.impl() != nullptr) {
    paddle::dialect::DenseTensorType scale = scale_.type().dyn_cast<paddle::dialect::DenseTensorType>();
    VLOG(4) << "Builder construction  dense_scale";
    ir_tensor_scale = paddle::dialect::IrTensor(paddle::dialect::TransToPhiDataType(scale.dtype()),
                                                        scale.dims(),
                                                        scale.data_layout(),
                                                        scale.lod(),
                                                        scale.offset());
    VLOG(4) << "Builder construction  meta_scale";
    meta_scale = paddle::dialect::IrMetaTensor(&ir_tensor_scale);
  }


  paddle::dialect::IrMetaTensor meta_bias;
  paddle::dialect::IrTensor ir_tensor_bias;
  if (bias_.impl() != nullptr) {
    paddle::dialect::DenseTensorType bias = bias_.type().dyn_cast<paddle::dialect::DenseTensorType>();
    VLOG(4) << "Builder construction  dense_bias";
    ir_tensor_bias = paddle::dialect::IrTensor(paddle::dialect::TransToPhiDataType(bias.dtype()),
                                                        bias.dims(),
                                                        bias.data_layout(),
                                                        bias.lod(),
                                                        bias.offset());
    VLOG(4) << "Builder construction  meta_bias";
    meta_bias = paddle::dialect::IrMetaTensor(&ir_tensor_bias);
  }

  paddle::dialect::IrTensor dense_x_grad;
  paddle::dialect::IrMetaTensor meta_x_grad(&dense_x_grad);
  paddle::dialect::IrTensor dense_scale_grad;
  paddle::dialect::IrMetaTensor meta_scale_grad(&dense_scale_grad);
  paddle::dialect::IrTensor dense_bias_grad;
  paddle::dialect::IrMetaTensor meta_bias_grad(&dense_bias_grad);

  phi::GeneralTernaryGradInferMeta(meta_y, meta_scale, meta_bias, &meta_x_grad, &meta_scale_grad, &meta_bias_grad);

  std::vector<pir::Type> argument_outputs;
  pir::Type x_grad_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_x_grad.dtype()), dense_x_grad.dims(), dense_x_grad.layout(), dense_x_grad.lod(), dense_x_grad.offset());
  argument_outputs.push_back(x_grad_dense_tensor_type);

  pir::Type scale_grad_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_scale_grad.dtype()), dense_scale_grad.dims(), dense_scale_grad.layout(), dense_scale_grad.lod(), dense_scale_grad.offset());
  argument_outputs.push_back(scale_grad_dense_tensor_type);

  pir::Type bias_grad_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_bias_grad.dtype()), dense_bias_grad.dims(), dense_bias_grad.layout(), dense_bias_grad.lod(), dense_bias_grad.offset());
  argument_outputs.push_back(bias_grad_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void GroupNormGradOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, pir::Value scale_, pir::Value bias_, pir::Value y_, pir::Value mean_, pir::Value variance_, pir::Value y_grad_, pir::AttributeMap attributes) {
  VLOG(4) << "Start build GroupNormGradOp";


  IR_ENFORCE(
      attributes.find("epsilon") != attributes.end(),
          "'epsilon' Attribute is expected for GroupNormGradOp. ");
  float epsilon = attributes.at("epsilon").dyn_cast<pir::FloatAttribute>().data();

  IR_ENFORCE(
      attributes.find("groups") != attributes.end(),
          "'groups' Attribute is expected for GroupNormGradOp. ");
  int groups = attributes.at("groups").dyn_cast<pir::Int32Attribute>().data();

  IR_ENFORCE(
      attributes.find("data_layout") != attributes.end(),
          "'data_layout' Attribute is expected for GroupNormGradOp. ");
  std::string data_layout = attributes.at("data_layout").dyn_cast<pir::StrAttribute>().AsString();


  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_, scale_, bias_, y_, mean_, variance_, y_grad_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_epsilon = pir::FloatAttribute::get(pir::IrContext::Instance(), epsilon);
  argument.AddAttribute("epsilon", attr_epsilon);
  pir::Attribute attr_groups = pir::Int32Attribute::get(pir::IrContext::Instance(), groups);
  argument.AddAttribute("groups", attr_groups);
  pir::Attribute attr_data_layout = pir::StrAttribute::get(pir::IrContext::Instance(), data_layout);
  argument.AddAttribute("data_layout", attr_data_layout);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;
  paddle::dialect::DenseTensorType y = y_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)y;
  paddle::dialect::DenseTensorType mean = mean_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)mean;
  paddle::dialect::DenseTensorType variance = variance_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)variance;
  paddle::dialect::DenseTensorType y_grad = y_grad_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)y_grad;

  VLOG(4) << "Builder construction  dense_y";
  paddle::dialect::IrTensor ir_tensor_y(paddle::dialect::TransToPhiDataType(y.dtype()),
                                                      y.dims(),
                                                      y.data_layout(),
                                                      y.lod(),
                                                      y.offset());
  VLOG(4) << "Builder construction  meta_y";
  paddle::dialect::IrMetaTensor meta_y(&ir_tensor_y);

  paddle::dialect::IrMetaTensor meta_scale;
  paddle::dialect::IrTensor ir_tensor_scale;
  if (scale_.impl() != nullptr) {
    paddle::dialect::DenseTensorType scale = scale_.type().dyn_cast<paddle::dialect::DenseTensorType>();
    VLOG(4) << "Builder construction  dense_scale";
    ir_tensor_scale = paddle::dialect::IrTensor(paddle::dialect::TransToPhiDataType(scale.dtype()),
                                                        scale.dims(),
                                                        scale.data_layout(),
                                                        scale.lod(),
                                                        scale.offset());
    VLOG(4) << "Builder construction  meta_scale";
    meta_scale = paddle::dialect::IrMetaTensor(&ir_tensor_scale);
  }


  paddle::dialect::IrMetaTensor meta_bias;
  paddle::dialect::IrTensor ir_tensor_bias;
  if (bias_.impl() != nullptr) {
    paddle::dialect::DenseTensorType bias = bias_.type().dyn_cast<paddle::dialect::DenseTensorType>();
    VLOG(4) << "Builder construction  dense_bias";
    ir_tensor_bias = paddle::dialect::IrTensor(paddle::dialect::TransToPhiDataType(bias.dtype()),
                                                        bias.dims(),
                                                        bias.data_layout(),
                                                        bias.lod(),
                                                        bias.offset());
    VLOG(4) << "Builder construction  meta_bias";
    meta_bias = paddle::dialect::IrMetaTensor(&ir_tensor_bias);
  }

  paddle::dialect::IrTensor dense_x_grad;
  paddle::dialect::IrMetaTensor meta_x_grad(&dense_x_grad);
  paddle::dialect::IrTensor dense_scale_grad;
  paddle::dialect::IrMetaTensor meta_scale_grad(&dense_scale_grad);
  paddle::dialect::IrTensor dense_bias_grad;
  paddle::dialect::IrMetaTensor meta_bias_grad(&dense_bias_grad);

  phi::GeneralTernaryGradInferMeta(meta_y, meta_scale, meta_bias, &meta_x_grad, &meta_scale_grad, &meta_bias_grad);

  std::vector<pir::Type> argument_outputs;
  pir::Type x_grad_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_x_grad.dtype()), dense_x_grad.dims(), dense_x_grad.layout(), dense_x_grad.lod(), dense_x_grad.offset());
  argument_outputs.push_back(x_grad_dense_tensor_type);

  pir::Type scale_grad_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_scale_grad.dtype()), dense_scale_grad.dims(), dense_scale_grad.layout(), dense_scale_grad.lod(), dense_scale_grad.offset());
  argument_outputs.push_back(scale_grad_dense_tensor_type);

  pir::Type bias_grad_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_bias_grad.dtype()), dense_bias_grad.dims(), dense_bias_grad.layout(), dense_bias_grad.lod(), dense_bias_grad.offset());
  argument_outputs.push_back(bias_grad_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void GroupNormGradOp::VerifySig() {}

void GroupNormGradOp::InferMeta( phi::InferMetaContext *infer_meta ) {
  auto fn = PD_INFER_META(phi::GeneralTernaryGradInferMeta);
  fn(infer_meta);
}

phi::DataType GroupNormGradOp::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: GroupNormGradOp";
  


  return expected_kernel_dtype;
}

const char *GroupNormGrad_Op::attributes_name[3] = { "epsilon", "groups", "data_layout" };

OpInfoTuple GroupNormGrad_Op::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("x", "paddle::dialect::DenseTensorType", false, false, false, false), paddle::dialect::OpInputInfo("scale", "paddle::dialect::DenseTensorType", true, false, false, false), paddle::dialect::OpInputInfo("bias", "paddle::dialect::DenseTensorType", true, false, false, false), paddle::dialect::OpInputInfo("y", "paddle::dialect::DenseTensorType", false, false, false, false), paddle::dialect::OpInputInfo("mean", "paddle::dialect::DenseTensorType", false, false, false, false), paddle::dialect::OpInputInfo("variance", "paddle::dialect::DenseTensorType", false, false, false, false), paddle::dialect::OpInputInfo("y_grad", "paddle::dialect::DenseTensorType", false, false, false, false) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = { paddle::dialect::OpAttributeInfo("epsilon", "pir::FloatAttribute", ""), paddle::dialect::OpAttributeInfo("groups", "pir::Int32Attribute", ""), paddle::dialect::OpAttributeInfo("data_layout", "pir::StrAttribute", "") };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("x_grad", "paddle::dialect::DenseTensorType", false, false), paddle::dialect::OpOutputInfo("scale_grad", "paddle::dialect::DenseTensorType", false, false), paddle::dialect::OpOutputInfo("bias_grad", "paddle::dialect::DenseTensorType", false, false) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("GeneralTernaryGradInferMeta", {"y", "scale", "bias"}, "group_norm_grad", {"x", "scale", "bias", "y", "mean", "variance", "y_grad", "epsilon", "groups", "data_layout"}, {"y_grad"}, {}, {{"x_grad", "y_grad"}}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "group_norm_grad");
}

void GroupNormGrad_Op::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, pir::Value scale_, pir::Value bias_, pir::Value y_, pir::Value mean_, pir::Value variance_, pir::Value y_grad_, float epsilon, int groups, const std::string& data_layout) {
  VLOG(4) << "Start build GroupNormGrad_Op";



  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_, scale_, bias_, y_, mean_, variance_, y_grad_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_epsilon = pir::FloatAttribute::get(pir::IrContext::Instance(), epsilon);
  argument.AddAttribute("epsilon", attr_epsilon);
  pir::Attribute attr_groups = pir::Int32Attribute::get(pir::IrContext::Instance(), groups);
  argument.AddAttribute("groups", attr_groups);
  pir::Attribute attr_data_layout = pir::StrAttribute::get(pir::IrContext::Instance(), data_layout);
  argument.AddAttribute("data_layout", attr_data_layout);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;
  paddle::dialect::DenseTensorType y = y_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)y;
  paddle::dialect::DenseTensorType mean = mean_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)mean;
  paddle::dialect::DenseTensorType variance = variance_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)variance;
  paddle::dialect::DenseTensorType y_grad = y_grad_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)y_grad;

  VLOG(4) << "Builder construction  dense_y";
  paddle::dialect::IrTensor ir_tensor_y(paddle::dialect::TransToPhiDataType(y.dtype()),
                                                      y.dims(),
                                                      y.data_layout(),
                                                      y.lod(),
                                                      y.offset());
  VLOG(4) << "Builder construction  meta_y";
  paddle::dialect::IrMetaTensor meta_y(&ir_tensor_y);

  paddle::dialect::IrMetaTensor meta_scale;
  paddle::dialect::IrTensor ir_tensor_scale;
  if (scale_.impl() != nullptr) {
    paddle::dialect::DenseTensorType scale = scale_.type().dyn_cast<paddle::dialect::DenseTensorType>();
    VLOG(4) << "Builder construction  dense_scale";
    ir_tensor_scale = paddle::dialect::IrTensor(paddle::dialect::TransToPhiDataType(scale.dtype()),
                                                        scale.dims(),
                                                        scale.data_layout(),
                                                        scale.lod(),
                                                        scale.offset());
    VLOG(4) << "Builder construction  meta_scale";
    meta_scale = paddle::dialect::IrMetaTensor(&ir_tensor_scale);
  }


  paddle::dialect::IrMetaTensor meta_bias;
  paddle::dialect::IrTensor ir_tensor_bias;
  if (bias_.impl() != nullptr) {
    paddle::dialect::DenseTensorType bias = bias_.type().dyn_cast<paddle::dialect::DenseTensorType>();
    VLOG(4) << "Builder construction  dense_bias";
    ir_tensor_bias = paddle::dialect::IrTensor(paddle::dialect::TransToPhiDataType(bias.dtype()),
                                                        bias.dims(),
                                                        bias.data_layout(),
                                                        bias.lod(),
                                                        bias.offset());
    VLOG(4) << "Builder construction  meta_bias";
    meta_bias = paddle::dialect::IrMetaTensor(&ir_tensor_bias);
  }

  paddle::dialect::IrTensor dense_x_grad;
  paddle::dialect::IrMetaTensor meta_x_grad(&dense_x_grad);
  paddle::dialect::IrTensor dense_scale_grad;
  paddle::dialect::IrMetaTensor meta_scale_grad(&dense_scale_grad);
  paddle::dialect::IrTensor dense_bias_grad;
  paddle::dialect::IrMetaTensor meta_bias_grad(&dense_bias_grad);

  phi::GeneralTernaryGradInferMeta(meta_y, meta_scale, meta_bias, &meta_x_grad, &meta_scale_grad, &meta_bias_grad);

  std::vector<pir::Type> argument_outputs;
  pir::Type x_grad_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_x_grad.dtype()), dense_x_grad.dims(), dense_x_grad.layout(), dense_x_grad.lod(), dense_x_grad.offset());
  argument_outputs.push_back(x_grad_dense_tensor_type);

  pir::Type scale_grad_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_scale_grad.dtype()), dense_scale_grad.dims(), dense_scale_grad.layout(), dense_scale_grad.lod(), dense_scale_grad.offset());
  argument_outputs.push_back(scale_grad_dense_tensor_type);

  pir::Type bias_grad_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_bias_grad.dtype()), dense_bias_grad.dims(), dense_bias_grad.layout(), dense_bias_grad.lod(), dense_bias_grad.offset());
  argument_outputs.push_back(bias_grad_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void GroupNormGrad_Op::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, pir::Value scale_, pir::Value bias_, pir::Value y_, pir::Value mean_, pir::Value variance_, pir::Value y_grad_, pir::AttributeMap attributes) {
  VLOG(4) << "Start build GroupNormGrad_Op";


  IR_ENFORCE(
      attributes.find("epsilon") != attributes.end(),
          "'epsilon' Attribute is expected for GroupNormGrad_Op. ");
  float epsilon = attributes.at("epsilon").dyn_cast<pir::FloatAttribute>().data();

  IR_ENFORCE(
      attributes.find("groups") != attributes.end(),
          "'groups' Attribute is expected for GroupNormGrad_Op. ");
  int groups = attributes.at("groups").dyn_cast<pir::Int32Attribute>().data();

  IR_ENFORCE(
      attributes.find("data_layout") != attributes.end(),
          "'data_layout' Attribute is expected for GroupNormGrad_Op. ");
  std::string data_layout = attributes.at("data_layout").dyn_cast<pir::StrAttribute>().AsString();


  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_, scale_, bias_, y_, mean_, variance_, y_grad_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_epsilon = pir::FloatAttribute::get(pir::IrContext::Instance(), epsilon);
  argument.AddAttribute("epsilon", attr_epsilon);
  pir::Attribute attr_groups = pir::Int32Attribute::get(pir::IrContext::Instance(), groups);
  argument.AddAttribute("groups", attr_groups);
  pir::Attribute attr_data_layout = pir::StrAttribute::get(pir::IrContext::Instance(), data_layout);
  argument.AddAttribute("data_layout", attr_data_layout);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;
  paddle::dialect::DenseTensorType y = y_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)y;
  paddle::dialect::DenseTensorType mean = mean_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)mean;
  paddle::dialect::DenseTensorType variance = variance_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)variance;
  paddle::dialect::DenseTensorType y_grad = y_grad_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)y_grad;

  VLOG(4) << "Builder construction  dense_y";
  paddle::dialect::IrTensor ir_tensor_y(paddle::dialect::TransToPhiDataType(y.dtype()),
                                                      y.dims(),
                                                      y.data_layout(),
                                                      y.lod(),
                                                      y.offset());
  VLOG(4) << "Builder construction  meta_y";
  paddle::dialect::IrMetaTensor meta_y(&ir_tensor_y);

  paddle::dialect::IrMetaTensor meta_scale;
  paddle::dialect::IrTensor ir_tensor_scale;
  if (scale_.impl() != nullptr) {
    paddle::dialect::DenseTensorType scale = scale_.type().dyn_cast<paddle::dialect::DenseTensorType>();
    VLOG(4) << "Builder construction  dense_scale";
    ir_tensor_scale = paddle::dialect::IrTensor(paddle::dialect::TransToPhiDataType(scale.dtype()),
                                                        scale.dims(),
                                                        scale.data_layout(),
                                                        scale.lod(),
                                                        scale.offset());
    VLOG(4) << "Builder construction  meta_scale";
    meta_scale = paddle::dialect::IrMetaTensor(&ir_tensor_scale);
  }


  paddle::dialect::IrMetaTensor meta_bias;
  paddle::dialect::IrTensor ir_tensor_bias;
  if (bias_.impl() != nullptr) {
    paddle::dialect::DenseTensorType bias = bias_.type().dyn_cast<paddle::dialect::DenseTensorType>();
    VLOG(4) << "Builder construction  dense_bias";
    ir_tensor_bias = paddle::dialect::IrTensor(paddle::dialect::TransToPhiDataType(bias.dtype()),
                                                        bias.dims(),
                                                        bias.data_layout(),
                                                        bias.lod(),
                                                        bias.offset());
    VLOG(4) << "Builder construction  meta_bias";
    meta_bias = paddle::dialect::IrMetaTensor(&ir_tensor_bias);
  }

  paddle::dialect::IrTensor dense_x_grad;
  paddle::dialect::IrMetaTensor meta_x_grad(&dense_x_grad);
  paddle::dialect::IrTensor dense_scale_grad;
  paddle::dialect::IrMetaTensor meta_scale_grad(&dense_scale_grad);
  paddle::dialect::IrTensor dense_bias_grad;
  paddle::dialect::IrMetaTensor meta_bias_grad(&dense_bias_grad);

  phi::GeneralTernaryGradInferMeta(meta_y, meta_scale, meta_bias, &meta_x_grad, &meta_scale_grad, &meta_bias_grad);

  std::vector<pir::Type> argument_outputs;
  pir::Type x_grad_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_x_grad.dtype()), dense_x_grad.dims(), dense_x_grad.layout(), dense_x_grad.lod(), dense_x_grad.offset());
  argument_outputs.push_back(x_grad_dense_tensor_type);

  pir::Type scale_grad_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_scale_grad.dtype()), dense_scale_grad.dims(), dense_scale_grad.layout(), dense_scale_grad.lod(), dense_scale_grad.offset());
  argument_outputs.push_back(scale_grad_dense_tensor_type);

  pir::Type bias_grad_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_bias_grad.dtype()), dense_bias_grad.dims(), dense_bias_grad.layout(), dense_bias_grad.lod(), dense_bias_grad.offset());
  argument_outputs.push_back(bias_grad_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void GroupNormGrad_Op::VerifySig() {}

void GroupNormGrad_Op::InferMeta( phi::InferMetaContext *infer_meta ) {
  auto fn = PD_INFER_META(phi::GeneralTernaryGradInferMeta);
  fn(infer_meta);
}

phi::DataType GroupNormGrad_Op::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: GroupNormGrad_Op";
  


  return expected_kernel_dtype;
}

const char *GumbelSoftmaxGradOp::attributes_name[1] = { "axis" };

OpInfoTuple GumbelSoftmaxGradOp::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("out", "paddle::dialect::DenseTensorType", false, false, false, false), paddle::dialect::OpInputInfo("out_grad", "paddle::dialect::DenseTensorType", false, false, false, false) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = { paddle::dialect::OpAttributeInfo("axis", "pir::Int32Attribute", "") };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("x_grad", "paddle::dialect::DenseTensorType", false, false) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("GumbelSoftmaxGradInferMeta", {"out", "out_grad", "axis"}, "gumbel_softmax_grad", {"out", "out_grad", "axis"}, {}, {}, {}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "gumbel_softmax_grad");
}

void GumbelSoftmaxGradOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value out_, pir::Value out_grad_, int axis) {
  VLOG(4) << "Start build GumbelSoftmaxGradOp";



  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {out_, out_grad_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_axis = pir::Int32Attribute::get(pir::IrContext::Instance(), axis);
  argument.AddAttribute("axis", attr_axis);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType out = out_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)out;
  paddle::dialect::DenseTensorType out_grad = out_grad_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)out_grad;

  VLOG(4) << "Builder construction  dense_out";
  paddle::dialect::IrTensor ir_tensor_out(paddle::dialect::TransToPhiDataType(out.dtype()),
                                                      out.dims(),
                                                      out.data_layout(),
                                                      out.lod(),
                                                      out.offset());
  VLOG(4) << "Builder construction  meta_out";
  paddle::dialect::IrMetaTensor meta_out(&ir_tensor_out);

  VLOG(4) << "Builder construction  dense_out_grad";
  paddle::dialect::IrTensor ir_tensor_out_grad(paddle::dialect::TransToPhiDataType(out_grad.dtype()),
                                                      out_grad.dims(),
                                                      out_grad.data_layout(),
                                                      out_grad.lod(),
                                                      out_grad.offset());
  VLOG(4) << "Builder construction  meta_out_grad";
  paddle::dialect::IrMetaTensor meta_out_grad(&ir_tensor_out_grad);
  paddle::dialect::IrTensor dense_x_grad;
  paddle::dialect::IrMetaTensor meta_x_grad(&dense_x_grad);

  phi::GumbelSoftmaxGradInferMeta(meta_out, meta_out_grad, axis, &meta_x_grad);

  std::vector<pir::Type> argument_outputs;
  pir::Type x_grad_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_x_grad.dtype()), dense_x_grad.dims(), dense_x_grad.layout(), dense_x_grad.lod(), dense_x_grad.offset());
  argument_outputs.push_back(x_grad_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void GumbelSoftmaxGradOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value out_, pir::Value out_grad_, pir::AttributeMap attributes) {
  VLOG(4) << "Start build GumbelSoftmaxGradOp";


  IR_ENFORCE(
      attributes.find("axis") != attributes.end(),
          "'axis' Attribute is expected for GumbelSoftmaxGradOp. ");
  int axis = attributes.at("axis").dyn_cast<pir::Int32Attribute>().data();


  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {out_, out_grad_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_axis = pir::Int32Attribute::get(pir::IrContext::Instance(), axis);
  argument.AddAttribute("axis", attr_axis);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType out = out_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)out;
  paddle::dialect::DenseTensorType out_grad = out_grad_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)out_grad;

  VLOG(4) << "Builder construction  dense_out";
  paddle::dialect::IrTensor ir_tensor_out(paddle::dialect::TransToPhiDataType(out.dtype()),
                                                      out.dims(),
                                                      out.data_layout(),
                                                      out.lod(),
                                                      out.offset());
  VLOG(4) << "Builder construction  meta_out";
  paddle::dialect::IrMetaTensor meta_out(&ir_tensor_out);

  VLOG(4) << "Builder construction  dense_out_grad";
  paddle::dialect::IrTensor ir_tensor_out_grad(paddle::dialect::TransToPhiDataType(out_grad.dtype()),
                                                      out_grad.dims(),
                                                      out_grad.data_layout(),
                                                      out_grad.lod(),
                                                      out_grad.offset());
  VLOG(4) << "Builder construction  meta_out_grad";
  paddle::dialect::IrMetaTensor meta_out_grad(&ir_tensor_out_grad);
  paddle::dialect::IrTensor dense_x_grad;
  paddle::dialect::IrMetaTensor meta_x_grad(&dense_x_grad);

  phi::GumbelSoftmaxGradInferMeta(meta_out, meta_out_grad, axis, &meta_x_grad);

  std::vector<pir::Type> argument_outputs;
  pir::Type x_grad_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_x_grad.dtype()), dense_x_grad.dims(), dense_x_grad.layout(), dense_x_grad.lod(), dense_x_grad.offset());
  argument_outputs.push_back(x_grad_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void GumbelSoftmaxGradOp::VerifySig() {}

void GumbelSoftmaxGradOp::InferMeta( phi::InferMetaContext *infer_meta ) {
  auto fn = PD_INFER_META(phi::GumbelSoftmaxGradInferMeta);
  fn(infer_meta);
}

phi::DataType GumbelSoftmaxGradOp::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: GumbelSoftmaxGradOp";
  


  return expected_kernel_dtype;
}

const char *HardshrinkGradOp::attributes_name[1] = { "threshold" };

OpInfoTuple HardshrinkGradOp::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("x", "paddle::dialect::DenseTensorType", false, false, false, false), paddle::dialect::OpInputInfo("out_grad", "paddle::dialect::DenseTensorType", false, false, false, false) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = { paddle::dialect::OpAttributeInfo("threshold", "pir::FloatAttribute", "") };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("x_grad", "paddle::dialect::DenseTensorType", false, false) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("UnchangedInferMeta", {"x"}, "hard_shrink_grad", {"x", "out_grad", "threshold"}, {}, {}, {}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "hardshrink_grad");
}

void HardshrinkGradOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, pir::Value out_grad_, float threshold) {
  VLOG(4) << "Start build HardshrinkGradOp";



  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_, out_grad_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_threshold = pir::FloatAttribute::get(pir::IrContext::Instance(), threshold);
  argument.AddAttribute("threshold", attr_threshold);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;
  paddle::dialect::DenseTensorType out_grad = out_grad_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)out_grad;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);
  paddle::dialect::IrTensor dense_x_grad;
  paddle::dialect::IrMetaTensor meta_x_grad(&dense_x_grad);

  phi::UnchangedInferMeta(meta_x, &meta_x_grad);

  std::vector<pir::Type> argument_outputs;
  pir::Type x_grad_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_x_grad.dtype()), dense_x_grad.dims(), dense_x_grad.layout(), dense_x_grad.lod(), dense_x_grad.offset());
  argument_outputs.push_back(x_grad_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void HardshrinkGradOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, pir::Value out_grad_, pir::AttributeMap attributes) {
  VLOG(4) << "Start build HardshrinkGradOp";


  IR_ENFORCE(
      attributes.find("threshold") != attributes.end(),
          "'threshold' Attribute is expected for HardshrinkGradOp. ");
  float threshold = attributes.at("threshold").dyn_cast<pir::FloatAttribute>().data();


  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_, out_grad_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_threshold = pir::FloatAttribute::get(pir::IrContext::Instance(), threshold);
  argument.AddAttribute("threshold", attr_threshold);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;
  paddle::dialect::DenseTensorType out_grad = out_grad_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)out_grad;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);
  paddle::dialect::IrTensor dense_x_grad;
  paddle::dialect::IrMetaTensor meta_x_grad(&dense_x_grad);

  phi::UnchangedInferMeta(meta_x, &meta_x_grad);

  std::vector<pir::Type> argument_outputs;
  pir::Type x_grad_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_x_grad.dtype()), dense_x_grad.dims(), dense_x_grad.layout(), dense_x_grad.lod(), dense_x_grad.offset());
  argument_outputs.push_back(x_grad_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void HardshrinkGradOp::VerifySig() {}

void HardshrinkGradOp::InferMeta( phi::InferMetaContext *infer_meta ) {
  auto fn = PD_INFER_META(phi::UnchangedInferMeta);
  fn(infer_meta);
}

phi::DataType HardshrinkGradOp::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: HardshrinkGradOp";
  


  return expected_kernel_dtype;
}

const char *HardshrinkGrad_Op::attributes_name[1] = { "threshold" };

OpInfoTuple HardshrinkGrad_Op::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("x", "paddle::dialect::DenseTensorType", false, false, false, false), paddle::dialect::OpInputInfo("out_grad", "paddle::dialect::DenseTensorType", false, false, false, false) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = { paddle::dialect::OpAttributeInfo("threshold", "pir::FloatAttribute", "") };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("x_grad", "paddle::dialect::DenseTensorType", false, false) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("UnchangedInferMeta", {"x"}, "hard_shrink_grad", {"x", "out_grad", "threshold"}, {}, {}, {{"x_grad", "out_grad"}}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "hardshrink_grad");
}

void HardshrinkGrad_Op::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, pir::Value out_grad_, float threshold) {
  VLOG(4) << "Start build HardshrinkGrad_Op";



  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_, out_grad_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_threshold = pir::FloatAttribute::get(pir::IrContext::Instance(), threshold);
  argument.AddAttribute("threshold", attr_threshold);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;
  paddle::dialect::DenseTensorType out_grad = out_grad_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)out_grad;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);
  paddle::dialect::IrTensor dense_x_grad;
  paddle::dialect::IrMetaTensor meta_x_grad(&dense_x_grad);

  phi::UnchangedInferMeta(meta_x, &meta_x_grad);

  std::vector<pir::Type> argument_outputs;
  pir::Type x_grad_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_x_grad.dtype()), dense_x_grad.dims(), dense_x_grad.layout(), dense_x_grad.lod(), dense_x_grad.offset());
  argument_outputs.push_back(x_grad_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void HardshrinkGrad_Op::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, pir::Value out_grad_, pir::AttributeMap attributes) {
  VLOG(4) << "Start build HardshrinkGrad_Op";


  IR_ENFORCE(
      attributes.find("threshold") != attributes.end(),
          "'threshold' Attribute is expected for HardshrinkGrad_Op. ");
  float threshold = attributes.at("threshold").dyn_cast<pir::FloatAttribute>().data();


  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_, out_grad_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_threshold = pir::FloatAttribute::get(pir::IrContext::Instance(), threshold);
  argument.AddAttribute("threshold", attr_threshold);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;
  paddle::dialect::DenseTensorType out_grad = out_grad_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)out_grad;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);
  paddle::dialect::IrTensor dense_x_grad;
  paddle::dialect::IrMetaTensor meta_x_grad(&dense_x_grad);

  phi::UnchangedInferMeta(meta_x, &meta_x_grad);

  std::vector<pir::Type> argument_outputs;
  pir::Type x_grad_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_x_grad.dtype()), dense_x_grad.dims(), dense_x_grad.layout(), dense_x_grad.lod(), dense_x_grad.offset());
  argument_outputs.push_back(x_grad_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void HardshrinkGrad_Op::VerifySig() {}

void HardshrinkGrad_Op::InferMeta( phi::InferMetaContext *infer_meta ) {
  auto fn = PD_INFER_META(phi::UnchangedInferMeta);
  fn(infer_meta);
}

phi::DataType HardshrinkGrad_Op::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: HardshrinkGrad_Op";
  


  return expected_kernel_dtype;
}

const char *HardsigmoidGradOp::attributes_name[2] = { "slope", "offset" };

OpInfoTuple HardsigmoidGradOp::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("out", "paddle::dialect::DenseTensorType", false, false, false, false), paddle::dialect::OpInputInfo("out_grad", "paddle::dialect::DenseTensorType", false, false, false, false) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = { paddle::dialect::OpAttributeInfo("slope", "pir::FloatAttribute", ""), paddle::dialect::OpAttributeInfo("offset", "pir::FloatAttribute", "") };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("x_grad", "paddle::dialect::DenseTensorType", false, false) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("UnchangedInferMeta", {"out"}, "hardsigmoid_grad", {"out", "out_grad", "slope", "offset"}, {}, {}, {}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "hardsigmoid_grad");
}

void HardsigmoidGradOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value out_, pir::Value out_grad_, float slope, float offset) {
  VLOG(4) << "Start build HardsigmoidGradOp";



  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {out_, out_grad_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_slope = pir::FloatAttribute::get(pir::IrContext::Instance(), slope);
  argument.AddAttribute("slope", attr_slope);
  pir::Attribute attr_offset = pir::FloatAttribute::get(pir::IrContext::Instance(), offset);
  argument.AddAttribute("offset", attr_offset);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType out = out_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)out;
  paddle::dialect::DenseTensorType out_grad = out_grad_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)out_grad;

  VLOG(4) << "Builder construction  dense_out";
  paddle::dialect::IrTensor ir_tensor_out(paddle::dialect::TransToPhiDataType(out.dtype()),
                                                      out.dims(),
                                                      out.data_layout(),
                                                      out.lod(),
                                                      out.offset());
  VLOG(4) << "Builder construction  meta_out";
  paddle::dialect::IrMetaTensor meta_out(&ir_tensor_out);
  paddle::dialect::IrTensor dense_x_grad;
  paddle::dialect::IrMetaTensor meta_x_grad(&dense_x_grad);

  phi::UnchangedInferMeta(meta_out, &meta_x_grad);

  std::vector<pir::Type> argument_outputs;
  pir::Type x_grad_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_x_grad.dtype()), dense_x_grad.dims(), dense_x_grad.layout(), dense_x_grad.lod(), dense_x_grad.offset());
  argument_outputs.push_back(x_grad_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void HardsigmoidGradOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value out_, pir::Value out_grad_, pir::AttributeMap attributes) {
  VLOG(4) << "Start build HardsigmoidGradOp";


  IR_ENFORCE(
      attributes.find("slope") != attributes.end(),
          "'slope' Attribute is expected for HardsigmoidGradOp. ");
  float slope = attributes.at("slope").dyn_cast<pir::FloatAttribute>().data();

  IR_ENFORCE(
      attributes.find("offset") != attributes.end(),
          "'offset' Attribute is expected for HardsigmoidGradOp. ");
  float offset = attributes.at("offset").dyn_cast<pir::FloatAttribute>().data();


  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {out_, out_grad_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_slope = pir::FloatAttribute::get(pir::IrContext::Instance(), slope);
  argument.AddAttribute("slope", attr_slope);
  pir::Attribute attr_offset = pir::FloatAttribute::get(pir::IrContext::Instance(), offset);
  argument.AddAttribute("offset", attr_offset);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType out = out_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)out;
  paddle::dialect::DenseTensorType out_grad = out_grad_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)out_grad;

  VLOG(4) << "Builder construction  dense_out";
  paddle::dialect::IrTensor ir_tensor_out(paddle::dialect::TransToPhiDataType(out.dtype()),
                                                      out.dims(),
                                                      out.data_layout(),
                                                      out.lod(),
                                                      out.offset());
  VLOG(4) << "Builder construction  meta_out";
  paddle::dialect::IrMetaTensor meta_out(&ir_tensor_out);
  paddle::dialect::IrTensor dense_x_grad;
  paddle::dialect::IrMetaTensor meta_x_grad(&dense_x_grad);

  phi::UnchangedInferMeta(meta_out, &meta_x_grad);

  std::vector<pir::Type> argument_outputs;
  pir::Type x_grad_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_x_grad.dtype()), dense_x_grad.dims(), dense_x_grad.layout(), dense_x_grad.lod(), dense_x_grad.offset());
  argument_outputs.push_back(x_grad_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void HardsigmoidGradOp::VerifySig() {}

void HardsigmoidGradOp::InferMeta( phi::InferMetaContext *infer_meta ) {
  auto fn = PD_INFER_META(phi::UnchangedInferMeta);
  fn(infer_meta);
}

phi::DataType HardsigmoidGradOp::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: HardsigmoidGradOp";
  


  return expected_kernel_dtype;
}

const char *HardsigmoidGrad_Op::attributes_name[2] = { "slope", "offset" };

OpInfoTuple HardsigmoidGrad_Op::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("out", "paddle::dialect::DenseTensorType", false, false, false, false), paddle::dialect::OpInputInfo("out_grad", "paddle::dialect::DenseTensorType", false, false, false, false) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = { paddle::dialect::OpAttributeInfo("slope", "pir::FloatAttribute", ""), paddle::dialect::OpAttributeInfo("offset", "pir::FloatAttribute", "") };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("x_grad", "paddle::dialect::DenseTensorType", false, false) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("UnchangedInferMeta", {"out"}, "hardsigmoid_grad", {"out", "out_grad", "slope", "offset"}, {}, {}, {{"x_grad", "out_grad"}}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "hardsigmoid_grad");
}

void HardsigmoidGrad_Op::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value out_, pir::Value out_grad_, float slope, float offset) {
  VLOG(4) << "Start build HardsigmoidGrad_Op";



  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {out_, out_grad_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_slope = pir::FloatAttribute::get(pir::IrContext::Instance(), slope);
  argument.AddAttribute("slope", attr_slope);
  pir::Attribute attr_offset = pir::FloatAttribute::get(pir::IrContext::Instance(), offset);
  argument.AddAttribute("offset", attr_offset);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType out = out_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)out;
  paddle::dialect::DenseTensorType out_grad = out_grad_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)out_grad;

  VLOG(4) << "Builder construction  dense_out";
  paddle::dialect::IrTensor ir_tensor_out(paddle::dialect::TransToPhiDataType(out.dtype()),
                                                      out.dims(),
                                                      out.data_layout(),
                                                      out.lod(),
                                                      out.offset());
  VLOG(4) << "Builder construction  meta_out";
  paddle::dialect::IrMetaTensor meta_out(&ir_tensor_out);
  paddle::dialect::IrTensor dense_x_grad;
  paddle::dialect::IrMetaTensor meta_x_grad(&dense_x_grad);

  phi::UnchangedInferMeta(meta_out, &meta_x_grad);

  std::vector<pir::Type> argument_outputs;
  pir::Type x_grad_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_x_grad.dtype()), dense_x_grad.dims(), dense_x_grad.layout(), dense_x_grad.lod(), dense_x_grad.offset());
  argument_outputs.push_back(x_grad_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void HardsigmoidGrad_Op::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value out_, pir::Value out_grad_, pir::AttributeMap attributes) {
  VLOG(4) << "Start build HardsigmoidGrad_Op";


  IR_ENFORCE(
      attributes.find("slope") != attributes.end(),
          "'slope' Attribute is expected for HardsigmoidGrad_Op. ");
  float slope = attributes.at("slope").dyn_cast<pir::FloatAttribute>().data();

  IR_ENFORCE(
      attributes.find("offset") != attributes.end(),
          "'offset' Attribute is expected for HardsigmoidGrad_Op. ");
  float offset = attributes.at("offset").dyn_cast<pir::FloatAttribute>().data();


  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {out_, out_grad_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_slope = pir::FloatAttribute::get(pir::IrContext::Instance(), slope);
  argument.AddAttribute("slope", attr_slope);
  pir::Attribute attr_offset = pir::FloatAttribute::get(pir::IrContext::Instance(), offset);
  argument.AddAttribute("offset", attr_offset);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType out = out_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)out;
  paddle::dialect::DenseTensorType out_grad = out_grad_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)out_grad;

  VLOG(4) << "Builder construction  dense_out";
  paddle::dialect::IrTensor ir_tensor_out(paddle::dialect::TransToPhiDataType(out.dtype()),
                                                      out.dims(),
                                                      out.data_layout(),
                                                      out.lod(),
                                                      out.offset());
  VLOG(4) << "Builder construction  meta_out";
  paddle::dialect::IrMetaTensor meta_out(&ir_tensor_out);
  paddle::dialect::IrTensor dense_x_grad;
  paddle::dialect::IrMetaTensor meta_x_grad(&dense_x_grad);

  phi::UnchangedInferMeta(meta_out, &meta_x_grad);

  std::vector<pir::Type> argument_outputs;
  pir::Type x_grad_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_x_grad.dtype()), dense_x_grad.dims(), dense_x_grad.layout(), dense_x_grad.lod(), dense_x_grad.offset());
  argument_outputs.push_back(x_grad_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void HardsigmoidGrad_Op::VerifySig() {}

void HardsigmoidGrad_Op::InferMeta( phi::InferMetaContext *infer_meta ) {
  auto fn = PD_INFER_META(phi::UnchangedInferMeta);
  fn(infer_meta);
}

phi::DataType HardsigmoidGrad_Op::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: HardsigmoidGrad_Op";
  


  return expected_kernel_dtype;
}

const char *HardtanhGradOp::attributes_name[2] = { "t_min", "t_max" };

OpInfoTuple HardtanhGradOp::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("x", "paddle::dialect::DenseTensorType", false, false, false, false), paddle::dialect::OpInputInfo("out_grad", "paddle::dialect::DenseTensorType", false, false, false, false) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = { paddle::dialect::OpAttributeInfo("t_min", "pir::FloatAttribute", ""), paddle::dialect::OpAttributeInfo("t_max", "pir::FloatAttribute", "") };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("x_grad", "paddle::dialect::DenseTensorType", false, false) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("UnchangedInferMeta", {"x"}, "hardtanh_grad", {"x", "out_grad", "t_min", "t_max"}, {}, {}, {}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "hardtanh_grad");
}

void HardtanhGradOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, pir::Value out_grad_, float t_min, float t_max) {
  VLOG(4) << "Start build HardtanhGradOp";



  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_, out_grad_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_t_min = pir::FloatAttribute::get(pir::IrContext::Instance(), t_min);
  argument.AddAttribute("t_min", attr_t_min);
  pir::Attribute attr_t_max = pir::FloatAttribute::get(pir::IrContext::Instance(), t_max);
  argument.AddAttribute("t_max", attr_t_max);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;
  paddle::dialect::DenseTensorType out_grad = out_grad_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)out_grad;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);
  paddle::dialect::IrTensor dense_x_grad;
  paddle::dialect::IrMetaTensor meta_x_grad(&dense_x_grad);

  phi::UnchangedInferMeta(meta_x, &meta_x_grad);

  std::vector<pir::Type> argument_outputs;
  pir::Type x_grad_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_x_grad.dtype()), dense_x_grad.dims(), dense_x_grad.layout(), dense_x_grad.lod(), dense_x_grad.offset());
  argument_outputs.push_back(x_grad_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void HardtanhGradOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, pir::Value out_grad_, pir::AttributeMap attributes) {
  VLOG(4) << "Start build HardtanhGradOp";


  IR_ENFORCE(
      attributes.find("t_min") != attributes.end(),
          "'t_min' Attribute is expected for HardtanhGradOp. ");
  float t_min = attributes.at("t_min").dyn_cast<pir::FloatAttribute>().data();

  IR_ENFORCE(
      attributes.find("t_max") != attributes.end(),
          "'t_max' Attribute is expected for HardtanhGradOp. ");
  float t_max = attributes.at("t_max").dyn_cast<pir::FloatAttribute>().data();


  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_, out_grad_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_t_min = pir::FloatAttribute::get(pir::IrContext::Instance(), t_min);
  argument.AddAttribute("t_min", attr_t_min);
  pir::Attribute attr_t_max = pir::FloatAttribute::get(pir::IrContext::Instance(), t_max);
  argument.AddAttribute("t_max", attr_t_max);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;
  paddle::dialect::DenseTensorType out_grad = out_grad_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)out_grad;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);
  paddle::dialect::IrTensor dense_x_grad;
  paddle::dialect::IrMetaTensor meta_x_grad(&dense_x_grad);

  phi::UnchangedInferMeta(meta_x, &meta_x_grad);

  std::vector<pir::Type> argument_outputs;
  pir::Type x_grad_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_x_grad.dtype()), dense_x_grad.dims(), dense_x_grad.layout(), dense_x_grad.lod(), dense_x_grad.offset());
  argument_outputs.push_back(x_grad_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void HardtanhGradOp::VerifySig() {}

void HardtanhGradOp::InferMeta( phi::InferMetaContext *infer_meta ) {
  auto fn = PD_INFER_META(phi::UnchangedInferMeta);
  fn(infer_meta);
}

phi::DataType HardtanhGradOp::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: HardtanhGradOp";
  


  return expected_kernel_dtype;
}

const char *HardtanhGrad_Op::attributes_name[2] = { "t_min", "t_max" };

OpInfoTuple HardtanhGrad_Op::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("x", "paddle::dialect::DenseTensorType", false, false, false, false), paddle::dialect::OpInputInfo("out_grad", "paddle::dialect::DenseTensorType", false, false, false, false) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = { paddle::dialect::OpAttributeInfo("t_min", "pir::FloatAttribute", ""), paddle::dialect::OpAttributeInfo("t_max", "pir::FloatAttribute", "") };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("x_grad", "paddle::dialect::DenseTensorType", false, false) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("UnchangedInferMeta", {"x"}, "hardtanh_grad", {"x", "out_grad", "t_min", "t_max"}, {}, {}, {{"x_grad", "out_grad"}}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "hardtanh_grad");
}

void HardtanhGrad_Op::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, pir::Value out_grad_, float t_min, float t_max) {
  VLOG(4) << "Start build HardtanhGrad_Op";



  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_, out_grad_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_t_min = pir::FloatAttribute::get(pir::IrContext::Instance(), t_min);
  argument.AddAttribute("t_min", attr_t_min);
  pir::Attribute attr_t_max = pir::FloatAttribute::get(pir::IrContext::Instance(), t_max);
  argument.AddAttribute("t_max", attr_t_max);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;
  paddle::dialect::DenseTensorType out_grad = out_grad_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)out_grad;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);
  paddle::dialect::IrTensor dense_x_grad;
  paddle::dialect::IrMetaTensor meta_x_grad(&dense_x_grad);

  phi::UnchangedInferMeta(meta_x, &meta_x_grad);

  std::vector<pir::Type> argument_outputs;
  pir::Type x_grad_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_x_grad.dtype()), dense_x_grad.dims(), dense_x_grad.layout(), dense_x_grad.lod(), dense_x_grad.offset());
  argument_outputs.push_back(x_grad_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void HardtanhGrad_Op::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, pir::Value out_grad_, pir::AttributeMap attributes) {
  VLOG(4) << "Start build HardtanhGrad_Op";


  IR_ENFORCE(
      attributes.find("t_min") != attributes.end(),
          "'t_min' Attribute is expected for HardtanhGrad_Op. ");
  float t_min = attributes.at("t_min").dyn_cast<pir::FloatAttribute>().data();

  IR_ENFORCE(
      attributes.find("t_max") != attributes.end(),
          "'t_max' Attribute is expected for HardtanhGrad_Op. ");
  float t_max = attributes.at("t_max").dyn_cast<pir::FloatAttribute>().data();


  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_, out_grad_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_t_min = pir::FloatAttribute::get(pir::IrContext::Instance(), t_min);
  argument.AddAttribute("t_min", attr_t_min);
  pir::Attribute attr_t_max = pir::FloatAttribute::get(pir::IrContext::Instance(), t_max);
  argument.AddAttribute("t_max", attr_t_max);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;
  paddle::dialect::DenseTensorType out_grad = out_grad_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)out_grad;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);
  paddle::dialect::IrTensor dense_x_grad;
  paddle::dialect::IrMetaTensor meta_x_grad(&dense_x_grad);

  phi::UnchangedInferMeta(meta_x, &meta_x_grad);

  std::vector<pir::Type> argument_outputs;
  pir::Type x_grad_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_x_grad.dtype()), dense_x_grad.dims(), dense_x_grad.layout(), dense_x_grad.lod(), dense_x_grad.offset());
  argument_outputs.push_back(x_grad_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void HardtanhGrad_Op::VerifySig() {}

void HardtanhGrad_Op::InferMeta( phi::InferMetaContext *infer_meta ) {
  auto fn = PD_INFER_META(phi::UnchangedInferMeta);
  fn(infer_meta);
}

phi::DataType HardtanhGrad_Op::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: HardtanhGrad_Op";
  


  return expected_kernel_dtype;
}

OpInfoTuple HeavisideGradOp::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("x", "paddle::dialect::DenseTensorType", false, false, false, false), paddle::dialect::OpInputInfo("y", "paddle::dialect::DenseTensorType", false, false, false, false), paddle::dialect::OpInputInfo("out_grad", "paddle::dialect::DenseTensorType", false, false, false, false) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = {  };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("x_grad", "paddle::dialect::DenseTensorType", false, false), paddle::dialect::OpOutputInfo("y_grad", "paddle::dialect::DenseTensorType", false, false) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("GeneralBinaryGradInferMeta", {"x", "y"}, "heaviside_grad", {"x", "y", "out_grad"}, {"out_grad"}, {}, {}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "heaviside_grad");
}

void HeavisideGradOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, pir::Value y_, pir::Value out_grad_) {
  VLOG(4) << "Start build HeavisideGradOp";



  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_, y_, out_grad_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;
  paddle::dialect::DenseTensorType y = y_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)y;
  paddle::dialect::DenseTensorType out_grad = out_grad_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)out_grad;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);

  VLOG(4) << "Builder construction  dense_y";
  paddle::dialect::IrTensor ir_tensor_y(paddle::dialect::TransToPhiDataType(y.dtype()),
                                                      y.dims(),
                                                      y.data_layout(),
                                                      y.lod(),
                                                      y.offset());
  VLOG(4) << "Builder construction  meta_y";
  paddle::dialect::IrMetaTensor meta_y(&ir_tensor_y);
  paddle::dialect::IrTensor dense_x_grad;
  paddle::dialect::IrMetaTensor meta_x_grad(&dense_x_grad);
  paddle::dialect::IrTensor dense_y_grad;
  paddle::dialect::IrMetaTensor meta_y_grad(&dense_y_grad);

  phi::GeneralBinaryGradInferMeta(meta_x, meta_y, &meta_x_grad, &meta_y_grad);

  std::vector<pir::Type> argument_outputs;
  pir::Type x_grad_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_x_grad.dtype()), dense_x_grad.dims(), dense_x_grad.layout(), dense_x_grad.lod(), dense_x_grad.offset());
  argument_outputs.push_back(x_grad_dense_tensor_type);

  pir::Type y_grad_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_y_grad.dtype()), dense_y_grad.dims(), dense_y_grad.layout(), dense_y_grad.lod(), dense_y_grad.offset());
  argument_outputs.push_back(y_grad_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void HeavisideGradOp::VerifySig() {}

void HeavisideGradOp::InferMeta( phi::InferMetaContext *infer_meta ) {
  auto fn = PD_INFER_META(phi::GeneralBinaryGradInferMeta);
  fn(infer_meta);
}

phi::DataType HeavisideGradOp::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: HeavisideGradOp";
  


  // deal complex_promote
  if (framework::IsComplexType(expected_kernel_dtype)) {{
    // only promote inputs’s types when contains complex input
    return tensor_dtype;
  }}

  return expected_kernel_dtype;
}

const char *HuberLossGradOp::attributes_name[1] = { "delta" };

OpInfoTuple HuberLossGradOp::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("residual", "paddle::dialect::DenseTensorType", false, false, false, false), paddle::dialect::OpInputInfo("out_grad", "paddle::dialect::DenseTensorType", false, false, false, false) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = { paddle::dialect::OpAttributeInfo("delta", "pir::FloatAttribute", "") };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("input_grad", "paddle::dialect::DenseTensorType", false, false), paddle::dialect::OpOutputInfo("label_grad", "paddle::dialect::DenseTensorType", false, false) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("GeneralBinaryGradInferMeta", {"residual", "residual"}, "huber_loss_grad", {"residual", "out_grad", "delta"}, {}, {}, {}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "huber_loss_grad");
}

void HuberLossGradOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value residual_, pir::Value out_grad_, float delta) {
  VLOG(4) << "Start build HuberLossGradOp";



  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {residual_, out_grad_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_delta = pir::FloatAttribute::get(pir::IrContext::Instance(), delta);
  argument.AddAttribute("delta", attr_delta);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType residual = residual_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)residual;
  paddle::dialect::DenseTensorType out_grad = out_grad_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)out_grad;

  VLOG(4) << "Builder construction  dense_residual";
  paddle::dialect::IrTensor ir_tensor_residual(paddle::dialect::TransToPhiDataType(residual.dtype()),
                                                      residual.dims(),
                                                      residual.data_layout(),
                                                      residual.lod(),
                                                      residual.offset());
  VLOG(4) << "Builder construction  meta_residual";
  paddle::dialect::IrMetaTensor meta_residual(&ir_tensor_residual);
  paddle::dialect::IrTensor dense_input_grad;
  paddle::dialect::IrMetaTensor meta_input_grad(&dense_input_grad);
  paddle::dialect::IrTensor dense_label_grad;
  paddle::dialect::IrMetaTensor meta_label_grad(&dense_label_grad);

  phi::GeneralBinaryGradInferMeta(meta_residual, meta_residual, &meta_input_grad, &meta_label_grad);

  std::vector<pir::Type> argument_outputs;
  pir::Type input_grad_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_input_grad.dtype()), dense_input_grad.dims(), dense_input_grad.layout(), dense_input_grad.lod(), dense_input_grad.offset());
  argument_outputs.push_back(input_grad_dense_tensor_type);

  pir::Type label_grad_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_label_grad.dtype()), dense_label_grad.dims(), dense_label_grad.layout(), dense_label_grad.lod(), dense_label_grad.offset());
  argument_outputs.push_back(label_grad_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void HuberLossGradOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value residual_, pir::Value out_grad_, pir::AttributeMap attributes) {
  VLOG(4) << "Start build HuberLossGradOp";


  IR_ENFORCE(
      attributes.find("delta") != attributes.end(),
          "'delta' Attribute is expected for HuberLossGradOp. ");
  float delta = attributes.at("delta").dyn_cast<pir::FloatAttribute>().data();


  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {residual_, out_grad_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_delta = pir::FloatAttribute::get(pir::IrContext::Instance(), delta);
  argument.AddAttribute("delta", attr_delta);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType residual = residual_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)residual;
  paddle::dialect::DenseTensorType out_grad = out_grad_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)out_grad;

  VLOG(4) << "Builder construction  dense_residual";
  paddle::dialect::IrTensor ir_tensor_residual(paddle::dialect::TransToPhiDataType(residual.dtype()),
                                                      residual.dims(),
                                                      residual.data_layout(),
                                                      residual.lod(),
                                                      residual.offset());
  VLOG(4) << "Builder construction  meta_residual";
  paddle::dialect::IrMetaTensor meta_residual(&ir_tensor_residual);
  paddle::dialect::IrTensor dense_input_grad;
  paddle::dialect::IrMetaTensor meta_input_grad(&dense_input_grad);
  paddle::dialect::IrTensor dense_label_grad;
  paddle::dialect::IrMetaTensor meta_label_grad(&dense_label_grad);

  phi::GeneralBinaryGradInferMeta(meta_residual, meta_residual, &meta_input_grad, &meta_label_grad);

  std::vector<pir::Type> argument_outputs;
  pir::Type input_grad_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_input_grad.dtype()), dense_input_grad.dims(), dense_input_grad.layout(), dense_input_grad.lod(), dense_input_grad.offset());
  argument_outputs.push_back(input_grad_dense_tensor_type);

  pir::Type label_grad_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_label_grad.dtype()), dense_label_grad.dims(), dense_label_grad.layout(), dense_label_grad.lod(), dense_label_grad.offset());
  argument_outputs.push_back(label_grad_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void HuberLossGradOp::VerifySig() {}

void HuberLossGradOp::InferMeta( phi::InferMetaContext *infer_meta ) {
  auto fn = PD_INFER_META(phi::GeneralBinaryGradInferMeta);
  fn(infer_meta);
}

phi::DataType HuberLossGradOp::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: HuberLossGradOp";
  


  return expected_kernel_dtype;
}

OpInfoTuple I0GradOp::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("x", "paddle::dialect::DenseTensorType", false, false, false, false), paddle::dialect::OpInputInfo("out_grad", "paddle::dialect::DenseTensorType", false, false, false, false) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = {  };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("x_grad", "paddle::dialect::DenseTensorType", false, false) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("UnchangedInferMeta", {"x"}, "i0_grad", {"x", "out_grad"}, {}, {}, {}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "i0_grad");
}

void I0GradOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, pir::Value out_grad_) {
  VLOG(4) << "Start build I0GradOp";



  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_, out_grad_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;
  paddle::dialect::DenseTensorType out_grad = out_grad_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)out_grad;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);
  paddle::dialect::IrTensor dense_x_grad;
  paddle::dialect::IrMetaTensor meta_x_grad(&dense_x_grad);

  phi::UnchangedInferMeta(meta_x, &meta_x_grad);

  std::vector<pir::Type> argument_outputs;
  pir::Type x_grad_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_x_grad.dtype()), dense_x_grad.dims(), dense_x_grad.layout(), dense_x_grad.lod(), dense_x_grad.offset());
  argument_outputs.push_back(x_grad_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void I0GradOp::VerifySig() {}

void I0GradOp::InferMeta( phi::InferMetaContext *infer_meta ) {
  auto fn = PD_INFER_META(phi::UnchangedInferMeta);
  fn(infer_meta);
}

phi::DataType I0GradOp::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: I0GradOp";
  


  return expected_kernel_dtype;
}

OpInfoTuple I0eGradOp::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("x", "paddle::dialect::DenseTensorType", false, false, false, false), paddle::dialect::OpInputInfo("out", "paddle::dialect::DenseTensorType", false, false, false, false), paddle::dialect::OpInputInfo("out_grad", "paddle::dialect::DenseTensorType", false, false, false, false) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = {  };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("x_grad", "paddle::dialect::DenseTensorType", false, false) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("UnchangedInferMeta", {"x"}, "i0e_grad", {"x", "out", "out_grad"}, {}, {}, {}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "i0e_grad");
}

void I0eGradOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, pir::Value out_, pir::Value out_grad_) {
  VLOG(4) << "Start build I0eGradOp";



  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_, out_, out_grad_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;
  paddle::dialect::DenseTensorType out = out_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)out;
  paddle::dialect::DenseTensorType out_grad = out_grad_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)out_grad;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);
  paddle::dialect::IrTensor dense_x_grad;
  paddle::dialect::IrMetaTensor meta_x_grad(&dense_x_grad);

  phi::UnchangedInferMeta(meta_x, &meta_x_grad);

  std::vector<pir::Type> argument_outputs;
  pir::Type x_grad_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_x_grad.dtype()), dense_x_grad.dims(), dense_x_grad.layout(), dense_x_grad.lod(), dense_x_grad.offset());
  argument_outputs.push_back(x_grad_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void I0eGradOp::VerifySig() {}

void I0eGradOp::InferMeta( phi::InferMetaContext *infer_meta ) {
  auto fn = PD_INFER_META(phi::UnchangedInferMeta);
  fn(infer_meta);
}

phi::DataType I0eGradOp::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: I0eGradOp";
  


  return expected_kernel_dtype;
}

OpInfoTuple I1GradOp::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("x", "paddle::dialect::DenseTensorType", false, false, false, false), paddle::dialect::OpInputInfo("out", "paddle::dialect::DenseTensorType", false, false, false, false), paddle::dialect::OpInputInfo("out_grad", "paddle::dialect::DenseTensorType", false, false, false, false) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = {  };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("x_grad", "paddle::dialect::DenseTensorType", false, false) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("UnchangedInferMeta", {"x"}, "i1_grad", {"x", "out", "out_grad"}, {}, {}, {}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "i1_grad");
}

void I1GradOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, pir::Value out_, pir::Value out_grad_) {
  VLOG(4) << "Start build I1GradOp";



  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_, out_, out_grad_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;
  paddle::dialect::DenseTensorType out = out_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)out;
  paddle::dialect::DenseTensorType out_grad = out_grad_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)out_grad;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);
  paddle::dialect::IrTensor dense_x_grad;
  paddle::dialect::IrMetaTensor meta_x_grad(&dense_x_grad);

  phi::UnchangedInferMeta(meta_x, &meta_x_grad);

  std::vector<pir::Type> argument_outputs;
  pir::Type x_grad_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_x_grad.dtype()), dense_x_grad.dims(), dense_x_grad.layout(), dense_x_grad.lod(), dense_x_grad.offset());
  argument_outputs.push_back(x_grad_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void I1GradOp::VerifySig() {}

void I1GradOp::InferMeta( phi::InferMetaContext *infer_meta ) {
  auto fn = PD_INFER_META(phi::UnchangedInferMeta);
  fn(infer_meta);
}

phi::DataType I1GradOp::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: I1GradOp";
  


  return expected_kernel_dtype;
}

OpInfoTuple I1eGradOp::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("x", "paddle::dialect::DenseTensorType", false, false, false, false), paddle::dialect::OpInputInfo("out", "paddle::dialect::DenseTensorType", false, false, false, false), paddle::dialect::OpInputInfo("out_grad", "paddle::dialect::DenseTensorType", false, false, false, false) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = {  };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("x_grad", "paddle::dialect::DenseTensorType", false, false) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("UnchangedInferMeta", {"x"}, "i1e_grad", {"x", "out", "out_grad"}, {}, {}, {}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "i1e_grad");
}

void I1eGradOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, pir::Value out_, pir::Value out_grad_) {
  VLOG(4) << "Start build I1eGradOp";



  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_, out_, out_grad_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;
  paddle::dialect::DenseTensorType out = out_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)out;
  paddle::dialect::DenseTensorType out_grad = out_grad_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)out_grad;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);
  paddle::dialect::IrTensor dense_x_grad;
  paddle::dialect::IrMetaTensor meta_x_grad(&dense_x_grad);

  phi::UnchangedInferMeta(meta_x, &meta_x_grad);

  std::vector<pir::Type> argument_outputs;
  pir::Type x_grad_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_x_grad.dtype()), dense_x_grad.dims(), dense_x_grad.layout(), dense_x_grad.lod(), dense_x_grad.offset());
  argument_outputs.push_back(x_grad_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void I1eGradOp::VerifySig() {}

void I1eGradOp::InferMeta( phi::InferMetaContext *infer_meta ) {
  auto fn = PD_INFER_META(phi::UnchangedInferMeta);
  fn(infer_meta);
}

phi::DataType I1eGradOp::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: I1eGradOp";
  


  return expected_kernel_dtype;
}

const char *IdentityLossGradOp::attributes_name[1] = { "reduction" };

OpInfoTuple IdentityLossGradOp::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("x", "paddle::dialect::DenseTensorType", false, false, false, false), paddle::dialect::OpInputInfo("out_grad", "paddle::dialect::DenseTensorType", false, false, false, false) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = { paddle::dialect::OpAttributeInfo("reduction", "pir::Int32Attribute", "") };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("x_grad", "paddle::dialect::DenseTensorType", false, false) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("IdentityLossGradInferMeta", {"x", "out_grad", "reduction"}, "identity_loss_grad", {"x", "out_grad", "reduction"}, {"out_grad"}, {}, {}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "identity_loss_grad");
}

void IdentityLossGradOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, pir::Value out_grad_, int reduction) {
  VLOG(4) << "Start build IdentityLossGradOp";



  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_, out_grad_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_reduction = pir::Int32Attribute::get(pir::IrContext::Instance(), reduction);
  argument.AddAttribute("reduction", attr_reduction);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;
  paddle::dialect::DenseTensorType out_grad = out_grad_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)out_grad;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);

  VLOG(4) << "Builder construction  dense_out_grad";
  paddle::dialect::IrTensor ir_tensor_out_grad(paddle::dialect::TransToPhiDataType(out_grad.dtype()),
                                                      out_grad.dims(),
                                                      out_grad.data_layout(),
                                                      out_grad.lod(),
                                                      out_grad.offset());
  VLOG(4) << "Builder construction  meta_out_grad";
  paddle::dialect::IrMetaTensor meta_out_grad(&ir_tensor_out_grad);
  paddle::dialect::IrTensor dense_x_grad;
  paddle::dialect::IrMetaTensor meta_x_grad(&dense_x_grad);

  phi::IdentityLossGradInferMeta(meta_x, meta_out_grad, reduction, &meta_x_grad);

  std::vector<pir::Type> argument_outputs;
  pir::Type x_grad_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_x_grad.dtype()), dense_x_grad.dims(), dense_x_grad.layout(), dense_x_grad.lod(), dense_x_grad.offset());
  argument_outputs.push_back(x_grad_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void IdentityLossGradOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, pir::Value out_grad_, pir::AttributeMap attributes) {
  VLOG(4) << "Start build IdentityLossGradOp";


  IR_ENFORCE(
      attributes.find("reduction") != attributes.end(),
          "'reduction' Attribute is expected for IdentityLossGradOp. ");
  int reduction = attributes.at("reduction").dyn_cast<pir::Int32Attribute>().data();


  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_, out_grad_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_reduction = pir::Int32Attribute::get(pir::IrContext::Instance(), reduction);
  argument.AddAttribute("reduction", attr_reduction);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;
  paddle::dialect::DenseTensorType out_grad = out_grad_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)out_grad;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);

  VLOG(4) << "Builder construction  dense_out_grad";
  paddle::dialect::IrTensor ir_tensor_out_grad(paddle::dialect::TransToPhiDataType(out_grad.dtype()),
                                                      out_grad.dims(),
                                                      out_grad.data_layout(),
                                                      out_grad.lod(),
                                                      out_grad.offset());
  VLOG(4) << "Builder construction  meta_out_grad";
  paddle::dialect::IrMetaTensor meta_out_grad(&ir_tensor_out_grad);
  paddle::dialect::IrTensor dense_x_grad;
  paddle::dialect::IrMetaTensor meta_x_grad(&dense_x_grad);

  phi::IdentityLossGradInferMeta(meta_x, meta_out_grad, reduction, &meta_x_grad);

  std::vector<pir::Type> argument_outputs;
  pir::Type x_grad_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_x_grad.dtype()), dense_x_grad.dims(), dense_x_grad.layout(), dense_x_grad.lod(), dense_x_grad.offset());
  argument_outputs.push_back(x_grad_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void IdentityLossGradOp::VerifySig() {}

void IdentityLossGradOp::InferMeta( phi::InferMetaContext *infer_meta ) {
  auto fn = PD_INFER_META(phi::IdentityLossGradInferMeta);
  fn(infer_meta);
}

phi::DataType IdentityLossGradOp::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: IdentityLossGradOp";
  


  return expected_kernel_dtype;
}

const char *IdentityLossGrad_Op::attributes_name[1] = { "reduction" };

OpInfoTuple IdentityLossGrad_Op::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("x", "paddle::dialect::DenseTensorType", false, false, false, false), paddle::dialect::OpInputInfo("out_grad", "paddle::dialect::DenseTensorType", false, false, false, false) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = { paddle::dialect::OpAttributeInfo("reduction", "pir::Int32Attribute", "") };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("x_grad", "paddle::dialect::DenseTensorType", false, false) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("IdentityLossGradInferMeta", {"x", "out_grad", "reduction"}, "identity_loss_grad", {"x", "out_grad", "reduction"}, {"out_grad"}, {}, {{"x_grad", "out_grad"}}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "identity_loss_grad");
}

void IdentityLossGrad_Op::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, pir::Value out_grad_, int reduction) {
  VLOG(4) << "Start build IdentityLossGrad_Op";



  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_, out_grad_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_reduction = pir::Int32Attribute::get(pir::IrContext::Instance(), reduction);
  argument.AddAttribute("reduction", attr_reduction);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;
  paddle::dialect::DenseTensorType out_grad = out_grad_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)out_grad;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);

  VLOG(4) << "Builder construction  dense_out_grad";
  paddle::dialect::IrTensor ir_tensor_out_grad(paddle::dialect::TransToPhiDataType(out_grad.dtype()),
                                                      out_grad.dims(),
                                                      out_grad.data_layout(),
                                                      out_grad.lod(),
                                                      out_grad.offset());
  VLOG(4) << "Builder construction  meta_out_grad";
  paddle::dialect::IrMetaTensor meta_out_grad(&ir_tensor_out_grad);
  paddle::dialect::IrTensor dense_x_grad;
  paddle::dialect::IrMetaTensor meta_x_grad(&dense_x_grad);

  phi::IdentityLossGradInferMeta(meta_x, meta_out_grad, reduction, &meta_x_grad);

  std::vector<pir::Type> argument_outputs;
  pir::Type x_grad_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_x_grad.dtype()), dense_x_grad.dims(), dense_x_grad.layout(), dense_x_grad.lod(), dense_x_grad.offset());
  argument_outputs.push_back(x_grad_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void IdentityLossGrad_Op::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, pir::Value out_grad_, pir::AttributeMap attributes) {
  VLOG(4) << "Start build IdentityLossGrad_Op";


  IR_ENFORCE(
      attributes.find("reduction") != attributes.end(),
          "'reduction' Attribute is expected for IdentityLossGrad_Op. ");
  int reduction = attributes.at("reduction").dyn_cast<pir::Int32Attribute>().data();


  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_, out_grad_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_reduction = pir::Int32Attribute::get(pir::IrContext::Instance(), reduction);
  argument.AddAttribute("reduction", attr_reduction);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;
  paddle::dialect::DenseTensorType out_grad = out_grad_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)out_grad;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);

  VLOG(4) << "Builder construction  dense_out_grad";
  paddle::dialect::IrTensor ir_tensor_out_grad(paddle::dialect::TransToPhiDataType(out_grad.dtype()),
                                                      out_grad.dims(),
                                                      out_grad.data_layout(),
                                                      out_grad.lod(),
                                                      out_grad.offset());
  VLOG(4) << "Builder construction  meta_out_grad";
  paddle::dialect::IrMetaTensor meta_out_grad(&ir_tensor_out_grad);
  paddle::dialect::IrTensor dense_x_grad;
  paddle::dialect::IrMetaTensor meta_x_grad(&dense_x_grad);

  phi::IdentityLossGradInferMeta(meta_x, meta_out_grad, reduction, &meta_x_grad);

  std::vector<pir::Type> argument_outputs;
  pir::Type x_grad_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_x_grad.dtype()), dense_x_grad.dims(), dense_x_grad.layout(), dense_x_grad.lod(), dense_x_grad.offset());
  argument_outputs.push_back(x_grad_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void IdentityLossGrad_Op::VerifySig() {}

void IdentityLossGrad_Op::InferMeta( phi::InferMetaContext *infer_meta ) {
  auto fn = PD_INFER_META(phi::IdentityLossGradInferMeta);
  fn(infer_meta);
}

phi::DataType IdentityLossGrad_Op::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: IdentityLossGrad_Op";
  


  return expected_kernel_dtype;
}

OpInfoTuple ImagGradOp::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("out_grad", "paddle::dialect::DenseTensorType", false, false, false, false) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = {  };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("x_grad", "paddle::dialect::DenseTensorType", false, false) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("RealAndImagGradInferMeta", {"out_grad"}, "imag_grad", {"out_grad"}, {"complex:out_grad"}, {}, {}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "imag_grad");
}

void ImagGradOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value out_grad_) {
  VLOG(4) << "Start build ImagGradOp";



  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {out_grad_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType out_grad = out_grad_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)out_grad;

  VLOG(4) << "Builder construction  dense_out_grad";
  paddle::dialect::IrTensor ir_tensor_out_grad(paddle::dialect::TransToPhiDataType(out_grad.dtype()),
                                                      out_grad.dims(),
                                                      out_grad.data_layout(),
                                                      out_grad.lod(),
                                                      out_grad.offset());
  VLOG(4) << "Builder construction  meta_out_grad";
  paddle::dialect::IrMetaTensor meta_out_grad(&ir_tensor_out_grad);
  paddle::dialect::IrTensor dense_x_grad;
  paddle::dialect::IrMetaTensor meta_x_grad(&dense_x_grad);

  phi::RealAndImagGradInferMeta(meta_out_grad, &meta_x_grad);

  std::vector<pir::Type> argument_outputs;
  pir::Type x_grad_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_x_grad.dtype()), dense_x_grad.dims(), dense_x_grad.layout(), dense_x_grad.lod(), dense_x_grad.offset());
  argument_outputs.push_back(x_grad_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void ImagGradOp::VerifySig() {}

void ImagGradOp::InferMeta( phi::InferMetaContext *infer_meta ) {
  auto fn = PD_INFER_META(phi::RealAndImagGradInferMeta);
  fn(infer_meta);
}

phi::DataType ImagGradOp::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: ImagGradOp";
  


  return expected_kernel_dtype;
}

const char *IndexAddGradOp::attributes_name[1] = { "axis" };

OpInfoTuple IndexAddGradOp::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("index", "paddle::dialect::DenseTensorType", false, false, false, false), paddle::dialect::OpInputInfo("add_value", "paddle::dialect::DenseTensorType", false, false, false, false), paddle::dialect::OpInputInfo("out_grad", "paddle::dialect::DenseTensorType", false, false, false, false) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = { paddle::dialect::OpAttributeInfo("axis", "pir::Int32Attribute", "") };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("x_grad", "paddle::dialect::DenseTensorType", false, false), paddle::dialect::OpOutputInfo("add_value_grad", "paddle::dialect::DenseTensorType", false, false) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("IndexAddGradInferMeta", {"index", "add_value", "out_grad", "axis"}, "index_add_grad", {"index", "add_value", "out_grad", "axis"}, {"out_grad"}, {}, {}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "index_add_grad");
}

void IndexAddGradOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value index_, pir::Value add_value_, pir::Value out_grad_, int axis) {
  VLOG(4) << "Start build IndexAddGradOp";



  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {index_, add_value_, out_grad_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_axis = pir::Int32Attribute::get(pir::IrContext::Instance(), axis);
  argument.AddAttribute("axis", attr_axis);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType index = index_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)index;
  paddle::dialect::DenseTensorType add_value = add_value_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)add_value;
  paddle::dialect::DenseTensorType out_grad = out_grad_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)out_grad;

  VLOG(4) << "Builder construction  dense_index";
  paddle::dialect::IrTensor ir_tensor_index(paddle::dialect::TransToPhiDataType(index.dtype()),
                                                      index.dims(),
                                                      index.data_layout(),
                                                      index.lod(),
                                                      index.offset());
  VLOG(4) << "Builder construction  meta_index";
  paddle::dialect::IrMetaTensor meta_index(&ir_tensor_index);

  VLOG(4) << "Builder construction  dense_add_value";
  paddle::dialect::IrTensor ir_tensor_add_value(paddle::dialect::TransToPhiDataType(add_value.dtype()),
                                                      add_value.dims(),
                                                      add_value.data_layout(),
                                                      add_value.lod(),
                                                      add_value.offset());
  VLOG(4) << "Builder construction  meta_add_value";
  paddle::dialect::IrMetaTensor meta_add_value(&ir_tensor_add_value);

  VLOG(4) << "Builder construction  dense_out_grad";
  paddle::dialect::IrTensor ir_tensor_out_grad(paddle::dialect::TransToPhiDataType(out_grad.dtype()),
                                                      out_grad.dims(),
                                                      out_grad.data_layout(),
                                                      out_grad.lod(),
                                                      out_grad.offset());
  VLOG(4) << "Builder construction  meta_out_grad";
  paddle::dialect::IrMetaTensor meta_out_grad(&ir_tensor_out_grad);
  paddle::dialect::IrTensor dense_x_grad;
  paddle::dialect::IrMetaTensor meta_x_grad(&dense_x_grad);
  paddle::dialect::IrTensor dense_add_value_grad;
  paddle::dialect::IrMetaTensor meta_add_value_grad(&dense_add_value_grad);

  phi::IndexAddGradInferMeta(meta_index, meta_add_value, meta_out_grad, axis, &meta_x_grad, &meta_add_value_grad);

  std::vector<pir::Type> argument_outputs;
  pir::Type x_grad_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_x_grad.dtype()), dense_x_grad.dims(), dense_x_grad.layout(), dense_x_grad.lod(), dense_x_grad.offset());
  argument_outputs.push_back(x_grad_dense_tensor_type);

  pir::Type add_value_grad_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_add_value_grad.dtype()), dense_add_value_grad.dims(), dense_add_value_grad.layout(), dense_add_value_grad.lod(), dense_add_value_grad.offset());
  argument_outputs.push_back(add_value_grad_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void IndexAddGradOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value index_, pir::Value add_value_, pir::Value out_grad_, pir::AttributeMap attributes) {
  VLOG(4) << "Start build IndexAddGradOp";


  IR_ENFORCE(
      attributes.find("axis") != attributes.end(),
          "'axis' Attribute is expected for IndexAddGradOp. ");
  int axis = attributes.at("axis").dyn_cast<pir::Int32Attribute>().data();


  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {index_, add_value_, out_grad_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_axis = pir::Int32Attribute::get(pir::IrContext::Instance(), axis);
  argument.AddAttribute("axis", attr_axis);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType index = index_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)index;
  paddle::dialect::DenseTensorType add_value = add_value_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)add_value;
  paddle::dialect::DenseTensorType out_grad = out_grad_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)out_grad;

  VLOG(4) << "Builder construction  dense_index";
  paddle::dialect::IrTensor ir_tensor_index(paddle::dialect::TransToPhiDataType(index.dtype()),
                                                      index.dims(),
                                                      index.data_layout(),
                                                      index.lod(),
                                                      index.offset());
  VLOG(4) << "Builder construction  meta_index";
  paddle::dialect::IrMetaTensor meta_index(&ir_tensor_index);

  VLOG(4) << "Builder construction  dense_add_value";
  paddle::dialect::IrTensor ir_tensor_add_value(paddle::dialect::TransToPhiDataType(add_value.dtype()),
                                                      add_value.dims(),
                                                      add_value.data_layout(),
                                                      add_value.lod(),
                                                      add_value.offset());
  VLOG(4) << "Builder construction  meta_add_value";
  paddle::dialect::IrMetaTensor meta_add_value(&ir_tensor_add_value);

  VLOG(4) << "Builder construction  dense_out_grad";
  paddle::dialect::IrTensor ir_tensor_out_grad(paddle::dialect::TransToPhiDataType(out_grad.dtype()),
                                                      out_grad.dims(),
                                                      out_grad.data_layout(),
                                                      out_grad.lod(),
                                                      out_grad.offset());
  VLOG(4) << "Builder construction  meta_out_grad";
  paddle::dialect::IrMetaTensor meta_out_grad(&ir_tensor_out_grad);
  paddle::dialect::IrTensor dense_x_grad;
  paddle::dialect::IrMetaTensor meta_x_grad(&dense_x_grad);
  paddle::dialect::IrTensor dense_add_value_grad;
  paddle::dialect::IrMetaTensor meta_add_value_grad(&dense_add_value_grad);

  phi::IndexAddGradInferMeta(meta_index, meta_add_value, meta_out_grad, axis, &meta_x_grad, &meta_add_value_grad);

  std::vector<pir::Type> argument_outputs;
  pir::Type x_grad_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_x_grad.dtype()), dense_x_grad.dims(), dense_x_grad.layout(), dense_x_grad.lod(), dense_x_grad.offset());
  argument_outputs.push_back(x_grad_dense_tensor_type);

  pir::Type add_value_grad_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_add_value_grad.dtype()), dense_add_value_grad.dims(), dense_add_value_grad.layout(), dense_add_value_grad.lod(), dense_add_value_grad.offset());
  argument_outputs.push_back(add_value_grad_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void IndexAddGradOp::VerifySig() {}

void IndexAddGradOp::InferMeta( phi::InferMetaContext *infer_meta ) {
  auto fn = PD_INFER_META(phi::IndexAddGradInferMeta);
  fn(infer_meta);
}

phi::DataType IndexAddGradOp::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: IndexAddGradOp";
  


  return expected_kernel_dtype;
}

const char *IndexAddGrad_Op::attributes_name[1] = { "axis" };

OpInfoTuple IndexAddGrad_Op::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("index", "paddle::dialect::DenseTensorType", false, false, false, false), paddle::dialect::OpInputInfo("add_value", "paddle::dialect::DenseTensorType", false, false, false, false), paddle::dialect::OpInputInfo("out_grad", "paddle::dialect::DenseTensorType", false, false, false, false) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = { paddle::dialect::OpAttributeInfo("axis", "pir::Int32Attribute", "") };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("x_grad", "paddle::dialect::DenseTensorType", false, false), paddle::dialect::OpOutputInfo("add_value_grad", "paddle::dialect::DenseTensorType", false, false) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("IndexAddGradInferMeta", {"index", "add_value", "out_grad", "axis"}, "index_add_grad", {"index", "add_value", "out_grad", "axis"}, {"out_grad"}, {}, {{"x_grad", "out_grad"}}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "index_add_grad");
}

void IndexAddGrad_Op::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value index_, pir::Value add_value_, pir::Value out_grad_, int axis) {
  VLOG(4) << "Start build IndexAddGrad_Op";



  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {index_, add_value_, out_grad_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_axis = pir::Int32Attribute::get(pir::IrContext::Instance(), axis);
  argument.AddAttribute("axis", attr_axis);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType index = index_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)index;
  paddle::dialect::DenseTensorType add_value = add_value_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)add_value;
  paddle::dialect::DenseTensorType out_grad = out_grad_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)out_grad;

  VLOG(4) << "Builder construction  dense_index";
  paddle::dialect::IrTensor ir_tensor_index(paddle::dialect::TransToPhiDataType(index.dtype()),
                                                      index.dims(),
                                                      index.data_layout(),
                                                      index.lod(),
                                                      index.offset());
  VLOG(4) << "Builder construction  meta_index";
  paddle::dialect::IrMetaTensor meta_index(&ir_tensor_index);

  VLOG(4) << "Builder construction  dense_add_value";
  paddle::dialect::IrTensor ir_tensor_add_value(paddle::dialect::TransToPhiDataType(add_value.dtype()),
                                                      add_value.dims(),
                                                      add_value.data_layout(),
                                                      add_value.lod(),
                                                      add_value.offset());
  VLOG(4) << "Builder construction  meta_add_value";
  paddle::dialect::IrMetaTensor meta_add_value(&ir_tensor_add_value);

  VLOG(4) << "Builder construction  dense_out_grad";
  paddle::dialect::IrTensor ir_tensor_out_grad(paddle::dialect::TransToPhiDataType(out_grad.dtype()),
                                                      out_grad.dims(),
                                                      out_grad.data_layout(),
                                                      out_grad.lod(),
                                                      out_grad.offset());
  VLOG(4) << "Builder construction  meta_out_grad";
  paddle::dialect::IrMetaTensor meta_out_grad(&ir_tensor_out_grad);
  paddle::dialect::IrTensor dense_x_grad;
  paddle::dialect::IrMetaTensor meta_x_grad(&dense_x_grad);
  paddle::dialect::IrTensor dense_add_value_grad;
  paddle::dialect::IrMetaTensor meta_add_value_grad(&dense_add_value_grad);

  phi::IndexAddGradInferMeta(meta_index, meta_add_value, meta_out_grad, axis, &meta_x_grad, &meta_add_value_grad);

  std::vector<pir::Type> argument_outputs;
  pir::Type x_grad_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_x_grad.dtype()), dense_x_grad.dims(), dense_x_grad.layout(), dense_x_grad.lod(), dense_x_grad.offset());
  argument_outputs.push_back(x_grad_dense_tensor_type);

  pir::Type add_value_grad_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_add_value_grad.dtype()), dense_add_value_grad.dims(), dense_add_value_grad.layout(), dense_add_value_grad.lod(), dense_add_value_grad.offset());
  argument_outputs.push_back(add_value_grad_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void IndexAddGrad_Op::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value index_, pir::Value add_value_, pir::Value out_grad_, pir::AttributeMap attributes) {
  VLOG(4) << "Start build IndexAddGrad_Op";


  IR_ENFORCE(
      attributes.find("axis") != attributes.end(),
          "'axis' Attribute is expected for IndexAddGrad_Op. ");
  int axis = attributes.at("axis").dyn_cast<pir::Int32Attribute>().data();


  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {index_, add_value_, out_grad_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_axis = pir::Int32Attribute::get(pir::IrContext::Instance(), axis);
  argument.AddAttribute("axis", attr_axis);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType index = index_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)index;
  paddle::dialect::DenseTensorType add_value = add_value_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)add_value;
  paddle::dialect::DenseTensorType out_grad = out_grad_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)out_grad;

  VLOG(4) << "Builder construction  dense_index";
  paddle::dialect::IrTensor ir_tensor_index(paddle::dialect::TransToPhiDataType(index.dtype()),
                                                      index.dims(),
                                                      index.data_layout(),
                                                      index.lod(),
                                                      index.offset());
  VLOG(4) << "Builder construction  meta_index";
  paddle::dialect::IrMetaTensor meta_index(&ir_tensor_index);

  VLOG(4) << "Builder construction  dense_add_value";
  paddle::dialect::IrTensor ir_tensor_add_value(paddle::dialect::TransToPhiDataType(add_value.dtype()),
                                                      add_value.dims(),
                                                      add_value.data_layout(),
                                                      add_value.lod(),
                                                      add_value.offset());
  VLOG(4) << "Builder construction  meta_add_value";
  paddle::dialect::IrMetaTensor meta_add_value(&ir_tensor_add_value);

  VLOG(4) << "Builder construction  dense_out_grad";
  paddle::dialect::IrTensor ir_tensor_out_grad(paddle::dialect::TransToPhiDataType(out_grad.dtype()),
                                                      out_grad.dims(),
                                                      out_grad.data_layout(),
                                                      out_grad.lod(),
                                                      out_grad.offset());
  VLOG(4) << "Builder construction  meta_out_grad";
  paddle::dialect::IrMetaTensor meta_out_grad(&ir_tensor_out_grad);
  paddle::dialect::IrTensor dense_x_grad;
  paddle::dialect::IrMetaTensor meta_x_grad(&dense_x_grad);
  paddle::dialect::IrTensor dense_add_value_grad;
  paddle::dialect::IrMetaTensor meta_add_value_grad(&dense_add_value_grad);

  phi::IndexAddGradInferMeta(meta_index, meta_add_value, meta_out_grad, axis, &meta_x_grad, &meta_add_value_grad);

  std::vector<pir::Type> argument_outputs;
  pir::Type x_grad_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_x_grad.dtype()), dense_x_grad.dims(), dense_x_grad.layout(), dense_x_grad.lod(), dense_x_grad.offset());
  argument_outputs.push_back(x_grad_dense_tensor_type);

  pir::Type add_value_grad_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_add_value_grad.dtype()), dense_add_value_grad.dims(), dense_add_value_grad.layout(), dense_add_value_grad.lod(), dense_add_value_grad.offset());
  argument_outputs.push_back(add_value_grad_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void IndexAddGrad_Op::VerifySig() {}

void IndexAddGrad_Op::InferMeta( phi::InferMetaContext *infer_meta ) {
  auto fn = PD_INFER_META(phi::IndexAddGradInferMeta);
  fn(infer_meta);
}

phi::DataType IndexAddGrad_Op::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: IndexAddGrad_Op";
  


  return expected_kernel_dtype;
}

const char *IndexPutGradOp::attributes_name[1] = { "accumulate" };

OpInfoTuple IndexPutGradOp::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("x", "paddle::dialect::DenseTensorType", false, false, false, false), paddle::dialect::OpInputInfo("indices", "pir::VectorType<paddle::dialect::DenseTensorType>", false, false, false, false), paddle::dialect::OpInputInfo("value", "paddle::dialect::DenseTensorType", false, false, false, false), paddle::dialect::OpInputInfo("out_grad", "paddle::dialect::DenseTensorType", false, false, false, false) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = { paddle::dialect::OpAttributeInfo("accumulate", "pir::BoolAttribute", "") };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("x_grad", "paddle::dialect::DenseTensorType", false, false), paddle::dialect::OpOutputInfo("value_grad", "paddle::dialect::DenseTensorType", false, false) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("IndexPutGradInferMeta", {"x", "indices", "value", "out_grad", "accumulate"}, "index_put_grad", {"x", "indices", "value", "out_grad", "accumulate"}, {"out_grad"}, {}, {}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "index_put_grad");
}

void IndexPutGradOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, pir::Value indices_, pir::Value value_, pir::Value out_grad_, bool accumulate) {
  VLOG(4) << "Start build IndexPutGradOp";



  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_, indices_, value_, out_grad_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_accumulate = pir::BoolAttribute::get(pir::IrContext::Instance(), accumulate);
  argument.AddAttribute("accumulate", attr_accumulate);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;
  pir::VectorType indices = indices_.type().dyn_cast<pir::VectorType>(); (void)indices;
  paddle::dialect::DenseTensorType value = value_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)value;
  paddle::dialect::DenseTensorType out_grad = out_grad_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)out_grad;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);
  std::vector<paddle::dialect::IrTensor> vec_ir_tensor_indices;
  for (size_t i=0; i < static_cast<size_t>(indices.size()); i++) {
    vec_ir_tensor_indices.push_back(paddle::dialect::IrTensor(paddle::dialect::TransToPhiDataType(indices[i].dyn_cast<paddle::dialect::DenseTensorType>().dtype()),
                                                                     indices[i].dyn_cast<paddle::dialect::DenseTensorType>().dims(),
                                                                     indices[i].dyn_cast<paddle::dialect::DenseTensorType>().data_layout(),
                                                                     indices[i].dyn_cast<paddle::dialect::DenseTensorType>().lod(),
                                                                     indices[i].dyn_cast<paddle::dialect::DenseTensorType>().offset()));
  }
  std::vector<paddle::dialect::IrMetaTensor> vec_meta_indices;
  for (size_t i=0; i < vec_ir_tensor_indices.size(); i++) {
    vec_meta_indices.push_back(paddle::dialect::IrMetaTensor(&vec_ir_tensor_indices[i]));
  }

  std::vector<const phi::MetaTensor*> meta_indices;
  for (size_t i=0; i < static_cast<size_t>(vec_meta_indices.size()); i++) {
    meta_indices.push_back(&vec_meta_indices[i]);
  }
 
  VLOG(4) << "Builder construction  dense_value";
  paddle::dialect::IrTensor ir_tensor_value(paddle::dialect::TransToPhiDataType(value.dtype()),
                                                      value.dims(),
                                                      value.data_layout(),
                                                      value.lod(),
                                                      value.offset());
  VLOG(4) << "Builder construction  meta_value";
  paddle::dialect::IrMetaTensor meta_value(&ir_tensor_value);

  VLOG(4) << "Builder construction  dense_out_grad";
  paddle::dialect::IrTensor ir_tensor_out_grad(paddle::dialect::TransToPhiDataType(out_grad.dtype()),
                                                      out_grad.dims(),
                                                      out_grad.data_layout(),
                                                      out_grad.lod(),
                                                      out_grad.offset());
  VLOG(4) << "Builder construction  meta_out_grad";
  paddle::dialect::IrMetaTensor meta_out_grad(&ir_tensor_out_grad);
  paddle::dialect::IrTensor dense_x_grad;
  paddle::dialect::IrMetaTensor meta_x_grad(&dense_x_grad);
  paddle::dialect::IrTensor dense_value_grad;
  paddle::dialect::IrMetaTensor meta_value_grad(&dense_value_grad);

  phi::IndexPutGradInferMeta(meta_x, meta_indices, meta_value, meta_out_grad, accumulate, &meta_x_grad, &meta_value_grad);

  std::vector<pir::Type> argument_outputs;
  pir::Type x_grad_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_x_grad.dtype()), dense_x_grad.dims(), dense_x_grad.layout(), dense_x_grad.lod(), dense_x_grad.offset());
  argument_outputs.push_back(x_grad_dense_tensor_type);

  pir::Type value_grad_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_value_grad.dtype()), dense_value_grad.dims(), dense_value_grad.layout(), dense_value_grad.lod(), dense_value_grad.offset());
  argument_outputs.push_back(value_grad_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void IndexPutGradOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, pir::Value indices_, pir::Value value_, pir::Value out_grad_, pir::AttributeMap attributes) {
  VLOG(4) << "Start build IndexPutGradOp";


  IR_ENFORCE(
      attributes.find("accumulate") != attributes.end(),
          "'accumulate' Attribute is expected for IndexPutGradOp. ");
  bool accumulate = attributes.at("accumulate").dyn_cast<pir::BoolAttribute>().data();


  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_, indices_, value_, out_grad_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_accumulate = pir::BoolAttribute::get(pir::IrContext::Instance(), accumulate);
  argument.AddAttribute("accumulate", attr_accumulate);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;
  pir::VectorType indices = indices_.type().dyn_cast<pir::VectorType>(); (void)indices;
  paddle::dialect::DenseTensorType value = value_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)value;
  paddle::dialect::DenseTensorType out_grad = out_grad_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)out_grad;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);
  std::vector<paddle::dialect::IrTensor> vec_ir_tensor_indices;
  for (size_t i=0; i < static_cast<size_t>(indices.size()); i++) {
    vec_ir_tensor_indices.push_back(paddle::dialect::IrTensor(paddle::dialect::TransToPhiDataType(indices[i].dyn_cast<paddle::dialect::DenseTensorType>().dtype()),
                                                                     indices[i].dyn_cast<paddle::dialect::DenseTensorType>().dims(),
                                                                     indices[i].dyn_cast<paddle::dialect::DenseTensorType>().data_layout(),
                                                                     indices[i].dyn_cast<paddle::dialect::DenseTensorType>().lod(),
                                                                     indices[i].dyn_cast<paddle::dialect::DenseTensorType>().offset()));
  }
  std::vector<paddle::dialect::IrMetaTensor> vec_meta_indices;
  for (size_t i=0; i < vec_ir_tensor_indices.size(); i++) {
    vec_meta_indices.push_back(paddle::dialect::IrMetaTensor(&vec_ir_tensor_indices[i]));
  }

  std::vector<const phi::MetaTensor*> meta_indices;
  for (size_t i=0; i < static_cast<size_t>(vec_meta_indices.size()); i++) {
    meta_indices.push_back(&vec_meta_indices[i]);
  }
 
  VLOG(4) << "Builder construction  dense_value";
  paddle::dialect::IrTensor ir_tensor_value(paddle::dialect::TransToPhiDataType(value.dtype()),
                                                      value.dims(),
                                                      value.data_layout(),
                                                      value.lod(),
                                                      value.offset());
  VLOG(4) << "Builder construction  meta_value";
  paddle::dialect::IrMetaTensor meta_value(&ir_tensor_value);

  VLOG(4) << "Builder construction  dense_out_grad";
  paddle::dialect::IrTensor ir_tensor_out_grad(paddle::dialect::TransToPhiDataType(out_grad.dtype()),
                                                      out_grad.dims(),
                                                      out_grad.data_layout(),
                                                      out_grad.lod(),
                                                      out_grad.offset());
  VLOG(4) << "Builder construction  meta_out_grad";
  paddle::dialect::IrMetaTensor meta_out_grad(&ir_tensor_out_grad);
  paddle::dialect::IrTensor dense_x_grad;
  paddle::dialect::IrMetaTensor meta_x_grad(&dense_x_grad);
  paddle::dialect::IrTensor dense_value_grad;
  paddle::dialect::IrMetaTensor meta_value_grad(&dense_value_grad);

  phi::IndexPutGradInferMeta(meta_x, meta_indices, meta_value, meta_out_grad, accumulate, &meta_x_grad, &meta_value_grad);

  std::vector<pir::Type> argument_outputs;
  pir::Type x_grad_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_x_grad.dtype()), dense_x_grad.dims(), dense_x_grad.layout(), dense_x_grad.lod(), dense_x_grad.offset());
  argument_outputs.push_back(x_grad_dense_tensor_type);

  pir::Type value_grad_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_value_grad.dtype()), dense_value_grad.dims(), dense_value_grad.layout(), dense_value_grad.lod(), dense_value_grad.offset());
  argument_outputs.push_back(value_grad_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void IndexPutGradOp::VerifySig() {}

void IndexPutGradOp::InferMeta( phi::InferMetaContext *infer_meta ) {
  auto fn = PD_INFER_META(phi::IndexPutGradInferMeta);
  fn(infer_meta);
}

phi::DataType IndexPutGradOp::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: IndexPutGradOp";
  

  // deal skip data transform
  if (var_name == "indices"){
    return expected_kernel_dtype;
  }


  return expected_kernel_dtype;
}

OpInfoTuple IndexSampleGradOp::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("x", "paddle::dialect::DenseTensorType", false, true, false, false), paddle::dialect::OpInputInfo("index", "paddle::dialect::DenseTensorType", false, false, false, false), paddle::dialect::OpInputInfo("out_grad", "paddle::dialect::DenseTensorType", false, false, false, false) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = {  };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("x_grad", "paddle::dialect::DenseTensorType", false, false) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("UnchangedInferMeta", {"x"}, "index_sample_grad", {"x", "index", "out_grad"}, {"out_grad"}, {}, {}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "index_sample_grad");
}

void IndexSampleGradOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, pir::Value index_, pir::Value out_grad_) {
  VLOG(4) << "Start build IndexSampleGradOp";



  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_, index_, out_grad_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;
  paddle::dialect::DenseTensorType index = index_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)index;
  paddle::dialect::DenseTensorType out_grad = out_grad_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)out_grad;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);
  paddle::dialect::IrTensor dense_x_grad;
  paddle::dialect::IrMetaTensor meta_x_grad(&dense_x_grad);

  phi::UnchangedInferMeta(meta_x, &meta_x_grad);

  std::vector<pir::Type> argument_outputs;
  pir::Type x_grad_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_x_grad.dtype()), dense_x_grad.dims(), dense_x_grad.layout(), dense_x_grad.lod(), dense_x_grad.offset());
  argument_outputs.push_back(x_grad_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void IndexSampleGradOp::VerifySig() {}

void IndexSampleGradOp::InferMeta( phi::InferMetaContext *infer_meta ) {
  auto fn = PD_INFER_META(phi::UnchangedInferMeta);
  fn(infer_meta);
}

phi::DataType IndexSampleGradOp::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: IndexSampleGradOp";
  

  // deal skip data transform
  if (var_name == "index"){
    return expected_kernel_dtype;
  }


  return expected_kernel_dtype;
}

const char *IndexSelectGradOp::attributes_name[1] = { "axis" };

OpInfoTuple IndexSelectGradOp::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("x", "paddle::dialect::DenseTensorType", false, true, false, false), paddle::dialect::OpInputInfo("index", "paddle::dialect::DenseTensorType", false, false, false, false), paddle::dialect::OpInputInfo("out_grad", "paddle::dialect::DenseTensorType", false, false, false, false) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = { paddle::dialect::OpAttributeInfo("axis", "pir::Int32Attribute", "") };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("x_grad", "paddle::dialect::DenseTensorType", false, false) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("UnchangedInferMeta", {"x"}, "index_select_grad", {"x", "index", "out_grad", "axis"}, {"out_grad"}, {}, {}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "index_select_grad");
}

void IndexSelectGradOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, pir::Value index_, pir::Value out_grad_, int axis) {
  VLOG(4) << "Start build IndexSelectGradOp";



  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_, index_, out_grad_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_axis = pir::Int32Attribute::get(pir::IrContext::Instance(), axis);
  argument.AddAttribute("axis", attr_axis);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;
  paddle::dialect::DenseTensorType index = index_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)index;
  paddle::dialect::DenseTensorType out_grad = out_grad_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)out_grad;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);
  paddle::dialect::IrTensor dense_x_grad;
  paddle::dialect::IrMetaTensor meta_x_grad(&dense_x_grad);

  phi::UnchangedInferMeta(meta_x, &meta_x_grad);

  std::vector<pir::Type> argument_outputs;
  pir::Type x_grad_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_x_grad.dtype()), dense_x_grad.dims(), dense_x_grad.layout(), dense_x_grad.lod(), dense_x_grad.offset());
  argument_outputs.push_back(x_grad_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void IndexSelectGradOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, pir::Value index_, pir::Value out_grad_, pir::AttributeMap attributes) {
  VLOG(4) << "Start build IndexSelectGradOp";


  IR_ENFORCE(
      attributes.find("axis") != attributes.end(),
          "'axis' Attribute is expected for IndexSelectGradOp. ");
  int axis = attributes.at("axis").dyn_cast<pir::Int32Attribute>().data();


  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_, index_, out_grad_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_axis = pir::Int32Attribute::get(pir::IrContext::Instance(), axis);
  argument.AddAttribute("axis", attr_axis);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;
  paddle::dialect::DenseTensorType index = index_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)index;
  paddle::dialect::DenseTensorType out_grad = out_grad_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)out_grad;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);
  paddle::dialect::IrTensor dense_x_grad;
  paddle::dialect::IrMetaTensor meta_x_grad(&dense_x_grad);

  phi::UnchangedInferMeta(meta_x, &meta_x_grad);

  std::vector<pir::Type> argument_outputs;
  pir::Type x_grad_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_x_grad.dtype()), dense_x_grad.dims(), dense_x_grad.layout(), dense_x_grad.lod(), dense_x_grad.offset());
  argument_outputs.push_back(x_grad_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void IndexSelectGradOp::VerifySig() {}

void IndexSelectGradOp::InferMeta( phi::InferMetaContext *infer_meta ) {
  auto fn = PD_INFER_META(phi::UnchangedInferMeta);
  fn(infer_meta);
}

phi::DataType IndexSelectGradOp::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: IndexSelectGradOp";
  

  // deal skip data transform
  if (var_name == "index"){
    return expected_kernel_dtype;
  }


  return expected_kernel_dtype;
}

const char *IndexSelectStridedGradOp::attributes_name[2] = { "index", "axis" };

OpInfoTuple IndexSelectStridedGradOp::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("x", "paddle::dialect::DenseTensorType", false, true, false, false), paddle::dialect::OpInputInfo("out_grad", "paddle::dialect::DenseTensorType", false, false, false, false) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = { paddle::dialect::OpAttributeInfo("index", "pir::Int64Attribute", ""), paddle::dialect::OpAttributeInfo("axis", "pir::Int32Attribute", "") };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("x_grad", "paddle::dialect::DenseTensorType", false, false) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("UnchangedInferMeta", {"x"}, "index_select_strided_grad", {"x", "out_grad", "index", "axis"}, {"out_grad"}, {}, {}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "index_select_strided_grad");
}

void IndexSelectStridedGradOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, pir::Value out_grad_, int64_t index, int axis) {
  VLOG(4) << "Start build IndexSelectStridedGradOp";



  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_, out_grad_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_index = pir::Int64Attribute::get(pir::IrContext::Instance(), index);
  argument.AddAttribute("index", attr_index);
  pir::Attribute attr_axis = pir::Int32Attribute::get(pir::IrContext::Instance(), axis);
  argument.AddAttribute("axis", attr_axis);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;
  paddle::dialect::DenseTensorType out_grad = out_grad_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)out_grad;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);
  paddle::dialect::IrTensor dense_x_grad;
  paddle::dialect::IrMetaTensor meta_x_grad(&dense_x_grad);

  phi::UnchangedInferMeta(meta_x, &meta_x_grad);

  std::vector<pir::Type> argument_outputs;
  pir::Type x_grad_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_x_grad.dtype()), dense_x_grad.dims(), dense_x_grad.layout(), dense_x_grad.lod(), dense_x_grad.offset());
  argument_outputs.push_back(x_grad_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void IndexSelectStridedGradOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, pir::Value out_grad_, pir::AttributeMap attributes) {
  VLOG(4) << "Start build IndexSelectStridedGradOp";


  IR_ENFORCE(
      attributes.find("index") != attributes.end(),
          "'index' Attribute is expected for IndexSelectStridedGradOp. ");
  int64_t index = attributes.at("index").dyn_cast<pir::Int64Attribute>().data();

  IR_ENFORCE(
      attributes.find("axis") != attributes.end(),
          "'axis' Attribute is expected for IndexSelectStridedGradOp. ");
  int axis = attributes.at("axis").dyn_cast<pir::Int32Attribute>().data();


  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_, out_grad_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_index = pir::Int64Attribute::get(pir::IrContext::Instance(), index);
  argument.AddAttribute("index", attr_index);
  pir::Attribute attr_axis = pir::Int32Attribute::get(pir::IrContext::Instance(), axis);
  argument.AddAttribute("axis", attr_axis);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;
  paddle::dialect::DenseTensorType out_grad = out_grad_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)out_grad;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);
  paddle::dialect::IrTensor dense_x_grad;
  paddle::dialect::IrMetaTensor meta_x_grad(&dense_x_grad);

  phi::UnchangedInferMeta(meta_x, &meta_x_grad);

  std::vector<pir::Type> argument_outputs;
  pir::Type x_grad_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_x_grad.dtype()), dense_x_grad.dims(), dense_x_grad.layout(), dense_x_grad.lod(), dense_x_grad.offset());
  argument_outputs.push_back(x_grad_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void IndexSelectStridedGradOp::VerifySig() {}

void IndexSelectStridedGradOp::InferMeta( phi::InferMetaContext *infer_meta ) {
  auto fn = PD_INFER_META(phi::UnchangedInferMeta);
  fn(infer_meta);
}

phi::DataType IndexSelectStridedGradOp::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: IndexSelectStridedGradOp";
  


  return expected_kernel_dtype;
}

const char *InstanceNormDoubleGradOp::attributes_name[1] = { "epsilon" };

OpInfoTuple InstanceNormDoubleGradOp::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("x", "paddle::dialect::DenseTensorType", false, false, false, false), paddle::dialect::OpInputInfo("fwd_scale", "paddle::dialect::DenseTensorType", true, false, false, false), paddle::dialect::OpInputInfo("saved_mean", "paddle::dialect::DenseTensorType", false, false, false, false), paddle::dialect::OpInputInfo("saved_variance", "paddle::dialect::DenseTensorType", false, false, false, false), paddle::dialect::OpInputInfo("grad_y", "paddle::dialect::DenseTensorType", false, false, false, false), paddle::dialect::OpInputInfo("grad_x_grad", "paddle::dialect::DenseTensorType", true, false, false, false), paddle::dialect::OpInputInfo("grad_scale_grad", "paddle::dialect::DenseTensorType", true, false, false, false), paddle::dialect::OpInputInfo("grad_bias_grad", "paddle::dialect::DenseTensorType", true, false, false, false) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = { paddle::dialect::OpAttributeInfo("epsilon", "pir::FloatAttribute", "") };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("x_grad", "paddle::dialect::DenseTensorType", false, false), paddle::dialect::OpOutputInfo("fwd_scale_grad", "paddle::dialect::DenseTensorType", false, false), paddle::dialect::OpOutputInfo("grad_y_grad", "paddle::dialect::DenseTensorType", false, false) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("InstanceNormDoubleGradInferMeta", {"x", "fwd_scale", "saved_mean", "saved_variance", "grad_y", "grad_x_grad", "grad_scale_grad", "grad_bias_grad", "epsilon"}, "instance_norm_double_grad", {"x", "fwd_scale", "saved_mean", "saved_variance", "grad_y", "grad_x_grad", "grad_scale_grad", "grad_bias_grad", "epsilon"}, {"x"}, {}, {}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "instance_norm_double_grad");
}

void InstanceNormDoubleGradOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, pir::Value fwd_scale_, pir::Value saved_mean_, pir::Value saved_variance_, pir::Value grad_y_, pir::Value grad_x_grad_, pir::Value grad_scale_grad_, pir::Value grad_bias_grad_, float epsilon) {
  VLOG(4) << "Start build InstanceNormDoubleGradOp";



  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_, fwd_scale_, saved_mean_, saved_variance_, grad_y_, grad_x_grad_, grad_scale_grad_, grad_bias_grad_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_epsilon = pir::FloatAttribute::get(pir::IrContext::Instance(), epsilon);
  argument.AddAttribute("epsilon", attr_epsilon);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;
  paddle::dialect::DenseTensorType saved_mean = saved_mean_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)saved_mean;
  paddle::dialect::DenseTensorType saved_variance = saved_variance_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)saved_variance;
  paddle::dialect::DenseTensorType grad_y = grad_y_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)grad_y;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);

  paddle::dialect::IrMetaTensor meta_fwd_scale;
  paddle::dialect::IrTensor ir_tensor_fwd_scale;
  if (fwd_scale_.impl() != nullptr) {
    paddle::dialect::DenseTensorType fwd_scale = fwd_scale_.type().dyn_cast<paddle::dialect::DenseTensorType>();
    VLOG(4) << "Builder construction  dense_fwd_scale";
    ir_tensor_fwd_scale = paddle::dialect::IrTensor(paddle::dialect::TransToPhiDataType(fwd_scale.dtype()),
                                                        fwd_scale.dims(),
                                                        fwd_scale.data_layout(),
                                                        fwd_scale.lod(),
                                                        fwd_scale.offset());
    VLOG(4) << "Builder construction  meta_fwd_scale";
    meta_fwd_scale = paddle::dialect::IrMetaTensor(&ir_tensor_fwd_scale);
  }


  VLOG(4) << "Builder construction  dense_saved_mean";
  paddle::dialect::IrTensor ir_tensor_saved_mean(paddle::dialect::TransToPhiDataType(saved_mean.dtype()),
                                                      saved_mean.dims(),
                                                      saved_mean.data_layout(),
                                                      saved_mean.lod(),
                                                      saved_mean.offset());
  VLOG(4) << "Builder construction  meta_saved_mean";
  paddle::dialect::IrMetaTensor meta_saved_mean(&ir_tensor_saved_mean);

  VLOG(4) << "Builder construction  dense_saved_variance";
  paddle::dialect::IrTensor ir_tensor_saved_variance(paddle::dialect::TransToPhiDataType(saved_variance.dtype()),
                                                      saved_variance.dims(),
                                                      saved_variance.data_layout(),
                                                      saved_variance.lod(),
                                                      saved_variance.offset());
  VLOG(4) << "Builder construction  meta_saved_variance";
  paddle::dialect::IrMetaTensor meta_saved_variance(&ir_tensor_saved_variance);

  VLOG(4) << "Builder construction  dense_grad_y";
  paddle::dialect::IrTensor ir_tensor_grad_y(paddle::dialect::TransToPhiDataType(grad_y.dtype()),
                                                      grad_y.dims(),
                                                      grad_y.data_layout(),
                                                      grad_y.lod(),
                                                      grad_y.offset());
  VLOG(4) << "Builder construction  meta_grad_y";
  paddle::dialect::IrMetaTensor meta_grad_y(&ir_tensor_grad_y);

  paddle::dialect::IrMetaTensor meta_grad_x_grad;
  paddle::dialect::IrTensor ir_tensor_grad_x_grad;
  if (grad_x_grad_.impl() != nullptr) {
    paddle::dialect::DenseTensorType grad_x_grad = grad_x_grad_.type().dyn_cast<paddle::dialect::DenseTensorType>();
    VLOG(4) << "Builder construction  dense_grad_x_grad";
    ir_tensor_grad_x_grad = paddle::dialect::IrTensor(paddle::dialect::TransToPhiDataType(grad_x_grad.dtype()),
                                                        grad_x_grad.dims(),
                                                        grad_x_grad.data_layout(),
                                                        grad_x_grad.lod(),
                                                        grad_x_grad.offset());
    VLOG(4) << "Builder construction  meta_grad_x_grad";
    meta_grad_x_grad = paddle::dialect::IrMetaTensor(&ir_tensor_grad_x_grad);
  }


  paddle::dialect::IrMetaTensor meta_grad_scale_grad;
  paddle::dialect::IrTensor ir_tensor_grad_scale_grad;
  if (grad_scale_grad_.impl() != nullptr) {
    paddle::dialect::DenseTensorType grad_scale_grad = grad_scale_grad_.type().dyn_cast<paddle::dialect::DenseTensorType>();
    VLOG(4) << "Builder construction  dense_grad_scale_grad";
    ir_tensor_grad_scale_grad = paddle::dialect::IrTensor(paddle::dialect::TransToPhiDataType(grad_scale_grad.dtype()),
                                                        grad_scale_grad.dims(),
                                                        grad_scale_grad.data_layout(),
                                                        grad_scale_grad.lod(),
                                                        grad_scale_grad.offset());
    VLOG(4) << "Builder construction  meta_grad_scale_grad";
    meta_grad_scale_grad = paddle::dialect::IrMetaTensor(&ir_tensor_grad_scale_grad);
  }


  paddle::dialect::IrMetaTensor meta_grad_bias_grad;
  paddle::dialect::IrTensor ir_tensor_grad_bias_grad;
  if (grad_bias_grad_.impl() != nullptr) {
    paddle::dialect::DenseTensorType grad_bias_grad = grad_bias_grad_.type().dyn_cast<paddle::dialect::DenseTensorType>();
    VLOG(4) << "Builder construction  dense_grad_bias_grad";
    ir_tensor_grad_bias_grad = paddle::dialect::IrTensor(paddle::dialect::TransToPhiDataType(grad_bias_grad.dtype()),
                                                        grad_bias_grad.dims(),
                                                        grad_bias_grad.data_layout(),
                                                        grad_bias_grad.lod(),
                                                        grad_bias_grad.offset());
    VLOG(4) << "Builder construction  meta_grad_bias_grad";
    meta_grad_bias_grad = paddle::dialect::IrMetaTensor(&ir_tensor_grad_bias_grad);
  }

  paddle::dialect::IrTensor dense_x_grad;
  paddle::dialect::IrMetaTensor meta_x_grad(&dense_x_grad);
  paddle::dialect::IrTensor dense_fwd_scale_grad;
  paddle::dialect::IrMetaTensor meta_fwd_scale_grad(&dense_fwd_scale_grad);
  paddle::dialect::IrTensor dense_grad_y_grad;
  paddle::dialect::IrMetaTensor meta_grad_y_grad(&dense_grad_y_grad);

  phi::InstanceNormDoubleGradInferMeta(meta_x, meta_fwd_scale, meta_saved_mean, meta_saved_variance, meta_grad_y, meta_grad_x_grad, meta_grad_scale_grad, meta_grad_bias_grad, epsilon, &meta_x_grad, &meta_fwd_scale_grad, &meta_grad_y_grad);

  std::vector<pir::Type> argument_outputs;
  pir::Type x_grad_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_x_grad.dtype()), dense_x_grad.dims(), dense_x_grad.layout(), dense_x_grad.lod(), dense_x_grad.offset());
  argument_outputs.push_back(x_grad_dense_tensor_type);

  pir::Type fwd_scale_grad_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_fwd_scale_grad.dtype()), dense_fwd_scale_grad.dims(), dense_fwd_scale_grad.layout(), dense_fwd_scale_grad.lod(), dense_fwd_scale_grad.offset());
  argument_outputs.push_back(fwd_scale_grad_dense_tensor_type);

  pir::Type grad_y_grad_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_grad_y_grad.dtype()), dense_grad_y_grad.dims(), dense_grad_y_grad.layout(), dense_grad_y_grad.lod(), dense_grad_y_grad.offset());
  argument_outputs.push_back(grad_y_grad_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void InstanceNormDoubleGradOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, pir::Value fwd_scale_, pir::Value saved_mean_, pir::Value saved_variance_, pir::Value grad_y_, pir::Value grad_x_grad_, pir::Value grad_scale_grad_, pir::Value grad_bias_grad_, pir::AttributeMap attributes) {
  VLOG(4) << "Start build InstanceNormDoubleGradOp";


  IR_ENFORCE(
      attributes.find("epsilon") != attributes.end(),
          "'epsilon' Attribute is expected for InstanceNormDoubleGradOp. ");
  float epsilon = attributes.at("epsilon").dyn_cast<pir::FloatAttribute>().data();


  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_, fwd_scale_, saved_mean_, saved_variance_, grad_y_, grad_x_grad_, grad_scale_grad_, grad_bias_grad_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_epsilon = pir::FloatAttribute::get(pir::IrContext::Instance(), epsilon);
  argument.AddAttribute("epsilon", attr_epsilon);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;
  paddle::dialect::DenseTensorType saved_mean = saved_mean_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)saved_mean;
  paddle::dialect::DenseTensorType saved_variance = saved_variance_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)saved_variance;
  paddle::dialect::DenseTensorType grad_y = grad_y_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)grad_y;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);

  paddle::dialect::IrMetaTensor meta_fwd_scale;
  paddle::dialect::IrTensor ir_tensor_fwd_scale;
  if (fwd_scale_.impl() != nullptr) {
    paddle::dialect::DenseTensorType fwd_scale = fwd_scale_.type().dyn_cast<paddle::dialect::DenseTensorType>();
    VLOG(4) << "Builder construction  dense_fwd_scale";
    ir_tensor_fwd_scale = paddle::dialect::IrTensor(paddle::dialect::TransToPhiDataType(fwd_scale.dtype()),
                                                        fwd_scale.dims(),
                                                        fwd_scale.data_layout(),
                                                        fwd_scale.lod(),
                                                        fwd_scale.offset());
    VLOG(4) << "Builder construction  meta_fwd_scale";
    meta_fwd_scale = paddle::dialect::IrMetaTensor(&ir_tensor_fwd_scale);
  }


  VLOG(4) << "Builder construction  dense_saved_mean";
  paddle::dialect::IrTensor ir_tensor_saved_mean(paddle::dialect::TransToPhiDataType(saved_mean.dtype()),
                                                      saved_mean.dims(),
                                                      saved_mean.data_layout(),
                                                      saved_mean.lod(),
                                                      saved_mean.offset());
  VLOG(4) << "Builder construction  meta_saved_mean";
  paddle::dialect::IrMetaTensor meta_saved_mean(&ir_tensor_saved_mean);

  VLOG(4) << "Builder construction  dense_saved_variance";
  paddle::dialect::IrTensor ir_tensor_saved_variance(paddle::dialect::TransToPhiDataType(saved_variance.dtype()),
                                                      saved_variance.dims(),
                                                      saved_variance.data_layout(),
                                                      saved_variance.lod(),
                                                      saved_variance.offset());
  VLOG(4) << "Builder construction  meta_saved_variance";
  paddle::dialect::IrMetaTensor meta_saved_variance(&ir_tensor_saved_variance);

  VLOG(4) << "Builder construction  dense_grad_y";
  paddle::dialect::IrTensor ir_tensor_grad_y(paddle::dialect::TransToPhiDataType(grad_y.dtype()),
                                                      grad_y.dims(),
                                                      grad_y.data_layout(),
                                                      grad_y.lod(),
                                                      grad_y.offset());
  VLOG(4) << "Builder construction  meta_grad_y";
  paddle::dialect::IrMetaTensor meta_grad_y(&ir_tensor_grad_y);

  paddle::dialect::IrMetaTensor meta_grad_x_grad;
  paddle::dialect::IrTensor ir_tensor_grad_x_grad;
  if (grad_x_grad_.impl() != nullptr) {
    paddle::dialect::DenseTensorType grad_x_grad = grad_x_grad_.type().dyn_cast<paddle::dialect::DenseTensorType>();
    VLOG(4) << "Builder construction  dense_grad_x_grad";
    ir_tensor_grad_x_grad = paddle::dialect::IrTensor(paddle::dialect::TransToPhiDataType(grad_x_grad.dtype()),
                                                        grad_x_grad.dims(),
                                                        grad_x_grad.data_layout(),
                                                        grad_x_grad.lod(),
                                                        grad_x_grad.offset());
    VLOG(4) << "Builder construction  meta_grad_x_grad";
    meta_grad_x_grad = paddle::dialect::IrMetaTensor(&ir_tensor_grad_x_grad);
  }


  paddle::dialect::IrMetaTensor meta_grad_scale_grad;
  paddle::dialect::IrTensor ir_tensor_grad_scale_grad;
  if (grad_scale_grad_.impl() != nullptr) {
    paddle::dialect::DenseTensorType grad_scale_grad = grad_scale_grad_.type().dyn_cast<paddle::dialect::DenseTensorType>();
    VLOG(4) << "Builder construction  dense_grad_scale_grad";
    ir_tensor_grad_scale_grad = paddle::dialect::IrTensor(paddle::dialect::TransToPhiDataType(grad_scale_grad.dtype()),
                                                        grad_scale_grad.dims(),
                                                        grad_scale_grad.data_layout(),
                                                        grad_scale_grad.lod(),
                                                        grad_scale_grad.offset());
    VLOG(4) << "Builder construction  meta_grad_scale_grad";
    meta_grad_scale_grad = paddle::dialect::IrMetaTensor(&ir_tensor_grad_scale_grad);
  }


  paddle::dialect::IrMetaTensor meta_grad_bias_grad;
  paddle::dialect::IrTensor ir_tensor_grad_bias_grad;
  if (grad_bias_grad_.impl() != nullptr) {
    paddle::dialect::DenseTensorType grad_bias_grad = grad_bias_grad_.type().dyn_cast<paddle::dialect::DenseTensorType>();
    VLOG(4) << "Builder construction  dense_grad_bias_grad";
    ir_tensor_grad_bias_grad = paddle::dialect::IrTensor(paddle::dialect::TransToPhiDataType(grad_bias_grad.dtype()),
                                                        grad_bias_grad.dims(),
                                                        grad_bias_grad.data_layout(),
                                                        grad_bias_grad.lod(),
                                                        grad_bias_grad.offset());
    VLOG(4) << "Builder construction  meta_grad_bias_grad";
    meta_grad_bias_grad = paddle::dialect::IrMetaTensor(&ir_tensor_grad_bias_grad);
  }

  paddle::dialect::IrTensor dense_x_grad;
  paddle::dialect::IrMetaTensor meta_x_grad(&dense_x_grad);
  paddle::dialect::IrTensor dense_fwd_scale_grad;
  paddle::dialect::IrMetaTensor meta_fwd_scale_grad(&dense_fwd_scale_grad);
  paddle::dialect::IrTensor dense_grad_y_grad;
  paddle::dialect::IrMetaTensor meta_grad_y_grad(&dense_grad_y_grad);

  phi::InstanceNormDoubleGradInferMeta(meta_x, meta_fwd_scale, meta_saved_mean, meta_saved_variance, meta_grad_y, meta_grad_x_grad, meta_grad_scale_grad, meta_grad_bias_grad, epsilon, &meta_x_grad, &meta_fwd_scale_grad, &meta_grad_y_grad);

  std::vector<pir::Type> argument_outputs;
  pir::Type x_grad_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_x_grad.dtype()), dense_x_grad.dims(), dense_x_grad.layout(), dense_x_grad.lod(), dense_x_grad.offset());
  argument_outputs.push_back(x_grad_dense_tensor_type);

  pir::Type fwd_scale_grad_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_fwd_scale_grad.dtype()), dense_fwd_scale_grad.dims(), dense_fwd_scale_grad.layout(), dense_fwd_scale_grad.lod(), dense_fwd_scale_grad.offset());
  argument_outputs.push_back(fwd_scale_grad_dense_tensor_type);

  pir::Type grad_y_grad_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_grad_y_grad.dtype()), dense_grad_y_grad.dims(), dense_grad_y_grad.layout(), dense_grad_y_grad.lod(), dense_grad_y_grad.offset());
  argument_outputs.push_back(grad_y_grad_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void InstanceNormDoubleGradOp::VerifySig() {}

void InstanceNormDoubleGradOp::InferMeta( phi::InferMetaContext *infer_meta ) {
  auto fn = PD_INFER_META(phi::InstanceNormDoubleGradInferMeta);
  fn(infer_meta);
}

phi::DataType InstanceNormDoubleGradOp::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: InstanceNormDoubleGradOp";
  


  return expected_kernel_dtype;
}

const char *InstanceNormGradOp::attributes_name[1] = { "epsilon" };

OpInfoTuple InstanceNormGradOp::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("x", "paddle::dialect::DenseTensorType", false, false, false, true), paddle::dialect::OpInputInfo("scale", "paddle::dialect::DenseTensorType", true, false, false, true), paddle::dialect::OpInputInfo("saved_mean", "paddle::dialect::DenseTensorType", false, false, false, false), paddle::dialect::OpInputInfo("saved_variance", "paddle::dialect::DenseTensorType", false, false, false, false), paddle::dialect::OpInputInfo("y_grad", "paddle::dialect::DenseTensorType", false, false, false, true) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = { paddle::dialect::OpAttributeInfo("epsilon", "pir::FloatAttribute", "") };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("x_grad", "paddle::dialect::DenseTensorType", false, false), paddle::dialect::OpOutputInfo("scale_grad", "paddle::dialect::DenseTensorType", false, false), paddle::dialect::OpOutputInfo("bias_grad", "paddle::dialect::DenseTensorType", false, false) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("InstanceNormGradInferMeta", {"x", "scale", "saved_mean", "saved_variance", "y_grad", "epsilon"}, "instance_norm_grad", {"x", "scale", "saved_mean", "saved_variance", "y_grad", "epsilon"}, {"x"}, {}, {}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "instance_norm_grad");
}

void InstanceNormGradOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, pir::Value scale_, pir::Value saved_mean_, pir::Value saved_variance_, pir::Value y_grad_, float epsilon) {
  VLOG(4) << "Start build InstanceNormGradOp";



  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_, scale_, saved_mean_, saved_variance_, y_grad_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_epsilon = pir::FloatAttribute::get(pir::IrContext::Instance(), epsilon);
  argument.AddAttribute("epsilon", attr_epsilon);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;
  paddle::dialect::DenseTensorType saved_mean = saved_mean_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)saved_mean;
  paddle::dialect::DenseTensorType saved_variance = saved_variance_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)saved_variance;
  paddle::dialect::DenseTensorType y_grad = y_grad_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)y_grad;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);

  paddle::dialect::IrMetaTensor meta_scale;
  paddle::dialect::IrTensor ir_tensor_scale;
  if (scale_.impl() != nullptr) {
    paddle::dialect::DenseTensorType scale = scale_.type().dyn_cast<paddle::dialect::DenseTensorType>();
    VLOG(4) << "Builder construction  dense_scale";
    ir_tensor_scale = paddle::dialect::IrTensor(paddle::dialect::TransToPhiDataType(scale.dtype()),
                                                        scale.dims(),
                                                        scale.data_layout(),
                                                        scale.lod(),
                                                        scale.offset());
    VLOG(4) << "Builder construction  meta_scale";
    meta_scale = paddle::dialect::IrMetaTensor(&ir_tensor_scale);
  }


  VLOG(4) << "Builder construction  dense_saved_mean";
  paddle::dialect::IrTensor ir_tensor_saved_mean(paddle::dialect::TransToPhiDataType(saved_mean.dtype()),
                                                      saved_mean.dims(),
                                                      saved_mean.data_layout(),
                                                      saved_mean.lod(),
                                                      saved_mean.offset());
  VLOG(4) << "Builder construction  meta_saved_mean";
  paddle::dialect::IrMetaTensor meta_saved_mean(&ir_tensor_saved_mean);

  VLOG(4) << "Builder construction  dense_saved_variance";
  paddle::dialect::IrTensor ir_tensor_saved_variance(paddle::dialect::TransToPhiDataType(saved_variance.dtype()),
                                                      saved_variance.dims(),
                                                      saved_variance.data_layout(),
                                                      saved_variance.lod(),
                                                      saved_variance.offset());
  VLOG(4) << "Builder construction  meta_saved_variance";
  paddle::dialect::IrMetaTensor meta_saved_variance(&ir_tensor_saved_variance);

  VLOG(4) << "Builder construction  dense_y_grad";
  paddle::dialect::IrTensor ir_tensor_y_grad(paddle::dialect::TransToPhiDataType(y_grad.dtype()),
                                                      y_grad.dims(),
                                                      y_grad.data_layout(),
                                                      y_grad.lod(),
                                                      y_grad.offset());
  VLOG(4) << "Builder construction  meta_y_grad";
  paddle::dialect::IrMetaTensor meta_y_grad(&ir_tensor_y_grad);
  paddle::dialect::IrTensor dense_x_grad;
  paddle::dialect::IrMetaTensor meta_x_grad(&dense_x_grad);
  paddle::dialect::IrTensor dense_scale_grad;
  paddle::dialect::IrMetaTensor meta_scale_grad(&dense_scale_grad);
  paddle::dialect::IrTensor dense_bias_grad;
  paddle::dialect::IrMetaTensor meta_bias_grad(&dense_bias_grad);

  phi::InstanceNormGradInferMeta(meta_x, meta_scale, meta_saved_mean, meta_saved_variance, meta_y_grad, epsilon, &meta_x_grad, &meta_scale_grad, &meta_bias_grad);

  std::vector<pir::Type> argument_outputs;
  pir::Type x_grad_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_x_grad.dtype()), dense_x_grad.dims(), dense_x_grad.layout(), dense_x_grad.lod(), dense_x_grad.offset());
  argument_outputs.push_back(x_grad_dense_tensor_type);

  pir::Type scale_grad_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_scale_grad.dtype()), dense_scale_grad.dims(), dense_scale_grad.layout(), dense_scale_grad.lod(), dense_scale_grad.offset());
  argument_outputs.push_back(scale_grad_dense_tensor_type);

  pir::Type bias_grad_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_bias_grad.dtype()), dense_bias_grad.dims(), dense_bias_grad.layout(), dense_bias_grad.lod(), dense_bias_grad.offset());
  argument_outputs.push_back(bias_grad_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void InstanceNormGradOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, pir::Value scale_, pir::Value saved_mean_, pir::Value saved_variance_, pir::Value y_grad_, pir::AttributeMap attributes) {
  VLOG(4) << "Start build InstanceNormGradOp";


  IR_ENFORCE(
      attributes.find("epsilon") != attributes.end(),
          "'epsilon' Attribute is expected for InstanceNormGradOp. ");
  float epsilon = attributes.at("epsilon").dyn_cast<pir::FloatAttribute>().data();


  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_, scale_, saved_mean_, saved_variance_, y_grad_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_epsilon = pir::FloatAttribute::get(pir::IrContext::Instance(), epsilon);
  argument.AddAttribute("epsilon", attr_epsilon);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;
  paddle::dialect::DenseTensorType saved_mean = saved_mean_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)saved_mean;
  paddle::dialect::DenseTensorType saved_variance = saved_variance_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)saved_variance;
  paddle::dialect::DenseTensorType y_grad = y_grad_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)y_grad;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);

  paddle::dialect::IrMetaTensor meta_scale;
  paddle::dialect::IrTensor ir_tensor_scale;
  if (scale_.impl() != nullptr) {
    paddle::dialect::DenseTensorType scale = scale_.type().dyn_cast<paddle::dialect::DenseTensorType>();
    VLOG(4) << "Builder construction  dense_scale";
    ir_tensor_scale = paddle::dialect::IrTensor(paddle::dialect::TransToPhiDataType(scale.dtype()),
                                                        scale.dims(),
                                                        scale.data_layout(),
                                                        scale.lod(),
                                                        scale.offset());
    VLOG(4) << "Builder construction  meta_scale";
    meta_scale = paddle::dialect::IrMetaTensor(&ir_tensor_scale);
  }


  VLOG(4) << "Builder construction  dense_saved_mean";
  paddle::dialect::IrTensor ir_tensor_saved_mean(paddle::dialect::TransToPhiDataType(saved_mean.dtype()),
                                                      saved_mean.dims(),
                                                      saved_mean.data_layout(),
                                                      saved_mean.lod(),
                                                      saved_mean.offset());
  VLOG(4) << "Builder construction  meta_saved_mean";
  paddle::dialect::IrMetaTensor meta_saved_mean(&ir_tensor_saved_mean);

  VLOG(4) << "Builder construction  dense_saved_variance";
  paddle::dialect::IrTensor ir_tensor_saved_variance(paddle::dialect::TransToPhiDataType(saved_variance.dtype()),
                                                      saved_variance.dims(),
                                                      saved_variance.data_layout(),
                                                      saved_variance.lod(),
                                                      saved_variance.offset());
  VLOG(4) << "Builder construction  meta_saved_variance";
  paddle::dialect::IrMetaTensor meta_saved_variance(&ir_tensor_saved_variance);

  VLOG(4) << "Builder construction  dense_y_grad";
  paddle::dialect::IrTensor ir_tensor_y_grad(paddle::dialect::TransToPhiDataType(y_grad.dtype()),
                                                      y_grad.dims(),
                                                      y_grad.data_layout(),
                                                      y_grad.lod(),
                                                      y_grad.offset());
  VLOG(4) << "Builder construction  meta_y_grad";
  paddle::dialect::IrMetaTensor meta_y_grad(&ir_tensor_y_grad);
  paddle::dialect::IrTensor dense_x_grad;
  paddle::dialect::IrMetaTensor meta_x_grad(&dense_x_grad);
  paddle::dialect::IrTensor dense_scale_grad;
  paddle::dialect::IrMetaTensor meta_scale_grad(&dense_scale_grad);
  paddle::dialect::IrTensor dense_bias_grad;
  paddle::dialect::IrMetaTensor meta_bias_grad(&dense_bias_grad);

  phi::InstanceNormGradInferMeta(meta_x, meta_scale, meta_saved_mean, meta_saved_variance, meta_y_grad, epsilon, &meta_x_grad, &meta_scale_grad, &meta_bias_grad);

  std::vector<pir::Type> argument_outputs;
  pir::Type x_grad_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_x_grad.dtype()), dense_x_grad.dims(), dense_x_grad.layout(), dense_x_grad.lod(), dense_x_grad.offset());
  argument_outputs.push_back(x_grad_dense_tensor_type);

  pir::Type scale_grad_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_scale_grad.dtype()), dense_scale_grad.dims(), dense_scale_grad.layout(), dense_scale_grad.lod(), dense_scale_grad.offset());
  argument_outputs.push_back(scale_grad_dense_tensor_type);

  pir::Type bias_grad_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_bias_grad.dtype()), dense_bias_grad.dims(), dense_bias_grad.layout(), dense_bias_grad.lod(), dense_bias_grad.offset());
  argument_outputs.push_back(bias_grad_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void InstanceNormGradOp::VerifySig() {}

void InstanceNormGradOp::InferMeta( phi::InferMetaContext *infer_meta ) {
  auto fn = PD_INFER_META(phi::InstanceNormGradInferMeta);
  fn(infer_meta);
}

phi::DataType InstanceNormGradOp::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: InstanceNormGradOp";
  


  return expected_kernel_dtype;
}

OpInfoTuple InverseGradOp::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("out", "paddle::dialect::DenseTensorType", false, false, false, false), paddle::dialect::OpInputInfo("out_grad", "paddle::dialect::DenseTensorType", false, false, false, false) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = {  };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("x_grad", "paddle::dialect::DenseTensorType", false, false) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("InverseGradInferMeta", {"out", "out_grad"}, "inverse_grad", {"out", "out_grad"}, {}, {}, {}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "inverse_grad");
}

void InverseGradOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value out_, pir::Value out_grad_) {
  VLOG(4) << "Start build InverseGradOp";



  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {out_, out_grad_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType out = out_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)out;
  paddle::dialect::DenseTensorType out_grad = out_grad_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)out_grad;

  VLOG(4) << "Builder construction  dense_out";
  paddle::dialect::IrTensor ir_tensor_out(paddle::dialect::TransToPhiDataType(out.dtype()),
                                                      out.dims(),
                                                      out.data_layout(),
                                                      out.lod(),
                                                      out.offset());
  VLOG(4) << "Builder construction  meta_out";
  paddle::dialect::IrMetaTensor meta_out(&ir_tensor_out);

  VLOG(4) << "Builder construction  dense_out_grad";
  paddle::dialect::IrTensor ir_tensor_out_grad(paddle::dialect::TransToPhiDataType(out_grad.dtype()),
                                                      out_grad.dims(),
                                                      out_grad.data_layout(),
                                                      out_grad.lod(),
                                                      out_grad.offset());
  VLOG(4) << "Builder construction  meta_out_grad";
  paddle::dialect::IrMetaTensor meta_out_grad(&ir_tensor_out_grad);
  paddle::dialect::IrTensor dense_x_grad;
  paddle::dialect::IrMetaTensor meta_x_grad(&dense_x_grad);

  phi::InverseGradInferMeta(meta_out, meta_out_grad, &meta_x_grad);

  std::vector<pir::Type> argument_outputs;
  pir::Type x_grad_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_x_grad.dtype()), dense_x_grad.dims(), dense_x_grad.layout(), dense_x_grad.lod(), dense_x_grad.offset());
  argument_outputs.push_back(x_grad_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void InverseGradOp::VerifySig() {}

void InverseGradOp::InferMeta( phi::InferMetaContext *infer_meta ) {
  auto fn = PD_INFER_META(phi::InverseGradInferMeta);
  fn(infer_meta);
}

phi::DataType InverseGradOp::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: InverseGradOp";
  


  return expected_kernel_dtype;
}

const char *KldivLossGradOp::attributes_name[1] = { "reduction" };

OpInfoTuple KldivLossGradOp::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("x", "paddle::dialect::DenseTensorType", false, true, false, false), paddle::dialect::OpInputInfo("label", "paddle::dialect::DenseTensorType", false, false, false, false), paddle::dialect::OpInputInfo("out_grad", "paddle::dialect::DenseTensorType", false, false, false, false) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = { paddle::dialect::OpAttributeInfo("reduction", "pir::StrAttribute", "") };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("x_grad", "paddle::dialect::DenseTensorType", false, false) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("UnchangedInferMeta", {"x"}, "kldiv_loss_grad", {"x", "label", "out_grad", "reduction"}, {}, {}, {}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "kldiv_loss_grad");
}

void KldivLossGradOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, pir::Value label_, pir::Value out_grad_, const std::string& reduction) {
  VLOG(4) << "Start build KldivLossGradOp";



  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_, label_, out_grad_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_reduction = pir::StrAttribute::get(pir::IrContext::Instance(), reduction);
  argument.AddAttribute("reduction", attr_reduction);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;
  paddle::dialect::DenseTensorType label = label_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)label;
  paddle::dialect::DenseTensorType out_grad = out_grad_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)out_grad;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);
  paddle::dialect::IrTensor dense_x_grad;
  paddle::dialect::IrMetaTensor meta_x_grad(&dense_x_grad);

  phi::UnchangedInferMeta(meta_x, &meta_x_grad);

  std::vector<pir::Type> argument_outputs;
  pir::Type x_grad_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_x_grad.dtype()), dense_x_grad.dims(), dense_x_grad.layout(), dense_x_grad.lod(), dense_x_grad.offset());
  argument_outputs.push_back(x_grad_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void KldivLossGradOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, pir::Value label_, pir::Value out_grad_, pir::AttributeMap attributes) {
  VLOG(4) << "Start build KldivLossGradOp";


  IR_ENFORCE(
      attributes.find("reduction") != attributes.end(),
          "'reduction' Attribute is expected for KldivLossGradOp. ");
  std::string reduction = attributes.at("reduction").dyn_cast<pir::StrAttribute>().AsString();


  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_, label_, out_grad_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_reduction = pir::StrAttribute::get(pir::IrContext::Instance(), reduction);
  argument.AddAttribute("reduction", attr_reduction);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;
  paddle::dialect::DenseTensorType label = label_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)label;
  paddle::dialect::DenseTensorType out_grad = out_grad_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)out_grad;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);
  paddle::dialect::IrTensor dense_x_grad;
  paddle::dialect::IrMetaTensor meta_x_grad(&dense_x_grad);

  phi::UnchangedInferMeta(meta_x, &meta_x_grad);

  std::vector<pir::Type> argument_outputs;
  pir::Type x_grad_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_x_grad.dtype()), dense_x_grad.dims(), dense_x_grad.layout(), dense_x_grad.lod(), dense_x_grad.offset());
  argument_outputs.push_back(x_grad_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void KldivLossGradOp::VerifySig() {}

void KldivLossGradOp::InferMeta( phi::InferMetaContext *infer_meta ) {
  auto fn = PD_INFER_META(phi::UnchangedInferMeta);
  fn(infer_meta);
}

phi::DataType KldivLossGradOp::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: KldivLossGradOp";
  


  return expected_kernel_dtype;
}

OpInfoTuple KronGradOp::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("x", "paddle::dialect::DenseTensorType", false, false, false, false), paddle::dialect::OpInputInfo("y", "paddle::dialect::DenseTensorType", false, false, false, false), paddle::dialect::OpInputInfo("out_grad", "paddle::dialect::DenseTensorType", false, false, false, false) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = {  };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("x_grad", "paddle::dialect::DenseTensorType", false, false), paddle::dialect::OpOutputInfo("y_grad", "paddle::dialect::DenseTensorType", false, false) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("GeneralBinaryGradInferMeta", {"x", "y"}, "kron_grad", {"x", "y", "out_grad"}, {"out_grad"}, {}, {}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "kron_grad");
}

void KronGradOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, pir::Value y_, pir::Value out_grad_) {
  VLOG(4) << "Start build KronGradOp";



  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_, y_, out_grad_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;
  paddle::dialect::DenseTensorType y = y_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)y;
  paddle::dialect::DenseTensorType out_grad = out_grad_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)out_grad;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);

  VLOG(4) << "Builder construction  dense_y";
  paddle::dialect::IrTensor ir_tensor_y(paddle::dialect::TransToPhiDataType(y.dtype()),
                                                      y.dims(),
                                                      y.data_layout(),
                                                      y.lod(),
                                                      y.offset());
  VLOG(4) << "Builder construction  meta_y";
  paddle::dialect::IrMetaTensor meta_y(&ir_tensor_y);
  paddle::dialect::IrTensor dense_x_grad;
  paddle::dialect::IrMetaTensor meta_x_grad(&dense_x_grad);
  paddle::dialect::IrTensor dense_y_grad;
  paddle::dialect::IrMetaTensor meta_y_grad(&dense_y_grad);

  phi::GeneralBinaryGradInferMeta(meta_x, meta_y, &meta_x_grad, &meta_y_grad);

  std::vector<pir::Type> argument_outputs;
  pir::Type x_grad_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_x_grad.dtype()), dense_x_grad.dims(), dense_x_grad.layout(), dense_x_grad.lod(), dense_x_grad.offset());
  argument_outputs.push_back(x_grad_dense_tensor_type);

  pir::Type y_grad_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_y_grad.dtype()), dense_y_grad.dims(), dense_y_grad.layout(), dense_y_grad.lod(), dense_y_grad.offset());
  argument_outputs.push_back(y_grad_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void KronGradOp::VerifySig() {}

void KronGradOp::InferMeta( phi::InferMetaContext *infer_meta ) {
  auto fn = PD_INFER_META(phi::GeneralBinaryGradInferMeta);
  fn(infer_meta);
}

phi::DataType KronGradOp::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: KronGradOp";
  


  // deal complex_promote
  if (framework::IsComplexType(expected_kernel_dtype)) {{
    // only promote inputs’s types when contains complex input
    return tensor_dtype;
  }}

  return expected_kernel_dtype;
}

const char *KthvalueGradOp::attributes_name[3] = { "k", "axis", "keepdim" };

OpInfoTuple KthvalueGradOp::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("x", "paddle::dialect::DenseTensorType", false, false, false, false), paddle::dialect::OpInputInfo("indices", "paddle::dialect::DenseTensorType", false, false, false, false), paddle::dialect::OpInputInfo("out_grad", "paddle::dialect::DenseTensorType", false, false, false, false) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = { paddle::dialect::OpAttributeInfo("k", "pir::Int32Attribute", ""), paddle::dialect::OpAttributeInfo("axis", "pir::Int32Attribute", ""), paddle::dialect::OpAttributeInfo("keepdim", "pir::BoolAttribute", "") };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("x_grad", "paddle::dialect::DenseTensorType", false, false) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("UnchangedInferMeta", {"x"}, "kthvalue_grad", {"x", "indices", "out_grad", "k", "axis", "keepdim"}, {"out_grad"}, {}, {}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "kthvalue_grad");
}

void KthvalueGradOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, pir::Value indices_, pir::Value out_grad_, int k, int axis, bool keepdim) {
  VLOG(4) << "Start build KthvalueGradOp";



  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_, indices_, out_grad_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_k = pir::Int32Attribute::get(pir::IrContext::Instance(), k);
  argument.AddAttribute("k", attr_k);
  pir::Attribute attr_axis = pir::Int32Attribute::get(pir::IrContext::Instance(), axis);
  argument.AddAttribute("axis", attr_axis);
  pir::Attribute attr_keepdim = pir::BoolAttribute::get(pir::IrContext::Instance(), keepdim);
  argument.AddAttribute("keepdim", attr_keepdim);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;
  paddle::dialect::DenseTensorType indices = indices_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)indices;
  paddle::dialect::DenseTensorType out_grad = out_grad_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)out_grad;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);
  paddle::dialect::IrTensor dense_x_grad;
  paddle::dialect::IrMetaTensor meta_x_grad(&dense_x_grad);

  phi::UnchangedInferMeta(meta_x, &meta_x_grad);

  std::vector<pir::Type> argument_outputs;
  pir::Type x_grad_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_x_grad.dtype()), dense_x_grad.dims(), dense_x_grad.layout(), dense_x_grad.lod(), dense_x_grad.offset());
  argument_outputs.push_back(x_grad_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void KthvalueGradOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, pir::Value indices_, pir::Value out_grad_, pir::AttributeMap attributes) {
  VLOG(4) << "Start build KthvalueGradOp";


  IR_ENFORCE(
      attributes.find("k") != attributes.end(),
          "'k' Attribute is expected for KthvalueGradOp. ");
  int k = attributes.at("k").dyn_cast<pir::Int32Attribute>().data();

  IR_ENFORCE(
      attributes.find("axis") != attributes.end(),
          "'axis' Attribute is expected for KthvalueGradOp. ");
  int axis = attributes.at("axis").dyn_cast<pir::Int32Attribute>().data();

  IR_ENFORCE(
      attributes.find("keepdim") != attributes.end(),
          "'keepdim' Attribute is expected for KthvalueGradOp. ");
  bool keepdim = attributes.at("keepdim").dyn_cast<pir::BoolAttribute>().data();


  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_, indices_, out_grad_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_k = pir::Int32Attribute::get(pir::IrContext::Instance(), k);
  argument.AddAttribute("k", attr_k);
  pir::Attribute attr_axis = pir::Int32Attribute::get(pir::IrContext::Instance(), axis);
  argument.AddAttribute("axis", attr_axis);
  pir::Attribute attr_keepdim = pir::BoolAttribute::get(pir::IrContext::Instance(), keepdim);
  argument.AddAttribute("keepdim", attr_keepdim);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;
  paddle::dialect::DenseTensorType indices = indices_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)indices;
  paddle::dialect::DenseTensorType out_grad = out_grad_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)out_grad;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);
  paddle::dialect::IrTensor dense_x_grad;
  paddle::dialect::IrMetaTensor meta_x_grad(&dense_x_grad);

  phi::UnchangedInferMeta(meta_x, &meta_x_grad);

  std::vector<pir::Type> argument_outputs;
  pir::Type x_grad_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_x_grad.dtype()), dense_x_grad.dims(), dense_x_grad.layout(), dense_x_grad.lod(), dense_x_grad.offset());
  argument_outputs.push_back(x_grad_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void KthvalueGradOp::VerifySig() {}

void KthvalueGradOp::InferMeta( phi::InferMetaContext *infer_meta ) {
  auto fn = PD_INFER_META(phi::UnchangedInferMeta);
  fn(infer_meta);
}

phi::DataType KthvalueGradOp::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: KthvalueGradOp";
  


  return expected_kernel_dtype;
}

const char *LabelSmoothGradOp::attributes_name[1] = { "epsilon" };

OpInfoTuple LabelSmoothGradOp::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("out_grad", "paddle::dialect::DenseTensorType", false, false, false, false) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = { paddle::dialect::OpAttributeInfo("epsilon", "pir::FloatAttribute", "") };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("label_grad", "paddle::dialect::DenseTensorType", false, false) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("UnchangedInferMeta", {"out_grad"}, "label_smooth_grad", {"out_grad", "epsilon"}, {}, {}, {}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "label_smooth_grad");
}

void LabelSmoothGradOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value out_grad_, float epsilon) {
  VLOG(4) << "Start build LabelSmoothGradOp";



  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {out_grad_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_epsilon = pir::FloatAttribute::get(pir::IrContext::Instance(), epsilon);
  argument.AddAttribute("epsilon", attr_epsilon);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType out_grad = out_grad_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)out_grad;

  VLOG(4) << "Builder construction  dense_out_grad";
  paddle::dialect::IrTensor ir_tensor_out_grad(paddle::dialect::TransToPhiDataType(out_grad.dtype()),
                                                      out_grad.dims(),
                                                      out_grad.data_layout(),
                                                      out_grad.lod(),
                                                      out_grad.offset());
  VLOG(4) << "Builder construction  meta_out_grad";
  paddle::dialect::IrMetaTensor meta_out_grad(&ir_tensor_out_grad);
  paddle::dialect::IrTensor dense_label_grad;
  paddle::dialect::IrMetaTensor meta_label_grad(&dense_label_grad);

  phi::UnchangedInferMeta(meta_out_grad, &meta_label_grad);

  std::vector<pir::Type> argument_outputs;
  pir::Type label_grad_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_label_grad.dtype()), dense_label_grad.dims(), dense_label_grad.layout(), dense_label_grad.lod(), dense_label_grad.offset());
  argument_outputs.push_back(label_grad_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void LabelSmoothGradOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value out_grad_, pir::AttributeMap attributes) {
  VLOG(4) << "Start build LabelSmoothGradOp";


  IR_ENFORCE(
      attributes.find("epsilon") != attributes.end(),
          "'epsilon' Attribute is expected for LabelSmoothGradOp. ");
  float epsilon = attributes.at("epsilon").dyn_cast<pir::FloatAttribute>().data();


  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {out_grad_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_epsilon = pir::FloatAttribute::get(pir::IrContext::Instance(), epsilon);
  argument.AddAttribute("epsilon", attr_epsilon);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType out_grad = out_grad_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)out_grad;

  VLOG(4) << "Builder construction  dense_out_grad";
  paddle::dialect::IrTensor ir_tensor_out_grad(paddle::dialect::TransToPhiDataType(out_grad.dtype()),
                                                      out_grad.dims(),
                                                      out_grad.data_layout(),
                                                      out_grad.lod(),
                                                      out_grad.offset());
  VLOG(4) << "Builder construction  meta_out_grad";
  paddle::dialect::IrMetaTensor meta_out_grad(&ir_tensor_out_grad);
  paddle::dialect::IrTensor dense_label_grad;
  paddle::dialect::IrMetaTensor meta_label_grad(&dense_label_grad);

  phi::UnchangedInferMeta(meta_out_grad, &meta_label_grad);

  std::vector<pir::Type> argument_outputs;
  pir::Type label_grad_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_label_grad.dtype()), dense_label_grad.dims(), dense_label_grad.layout(), dense_label_grad.lod(), dense_label_grad.offset());
  argument_outputs.push_back(label_grad_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void LabelSmoothGradOp::VerifySig() {}

void LabelSmoothGradOp::InferMeta( phi::InferMetaContext *infer_meta ) {
  auto fn = PD_INFER_META(phi::UnchangedInferMeta);
  fn(infer_meta);
}

phi::DataType LabelSmoothGradOp::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: LabelSmoothGradOp";
  


  return expected_kernel_dtype;
}

const char *LayerNormGradOp::attributes_name[2] = { "epsilon", "begin_norm_axis" };

OpInfoTuple LayerNormGradOp::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("x", "paddle::dialect::DenseTensorType", false, false, false, false), paddle::dialect::OpInputInfo("scale", "paddle::dialect::DenseTensorType", true, false, false, false), paddle::dialect::OpInputInfo("bias", "paddle::dialect::DenseTensorType", true, true, false, false), paddle::dialect::OpInputInfo("mean", "paddle::dialect::DenseTensorType", false, false, false, false), paddle::dialect::OpInputInfo("variance", "paddle::dialect::DenseTensorType", false, false, false, false), paddle::dialect::OpInputInfo("out_grad", "paddle::dialect::DenseTensorType", false, false, false, false) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = { paddle::dialect::OpAttributeInfo("epsilon", "pir::FloatAttribute", ""), paddle::dialect::OpAttributeInfo("begin_norm_axis", "pir::Int32Attribute", "") };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("x_grad", "paddle::dialect::DenseTensorType", false, false), paddle::dialect::OpOutputInfo("scale_grad", "paddle::dialect::DenseTensorType", false, false), paddle::dialect::OpOutputInfo("bias_grad", "paddle::dialect::DenseTensorType", false, false) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("LayerNormGradInferMeta", {"x", "scale", "bias"}, "layer_norm_grad", {"x", "scale", "bias", "mean", "variance", "out_grad", "epsilon", "begin_norm_axis"}, {"x"}, {}, {}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "layer_norm_grad");
}

void LayerNormGradOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, pir::Value scale_, pir::Value bias_, pir::Value mean_, pir::Value variance_, pir::Value out_grad_, float epsilon, int begin_norm_axis) {
  VLOG(4) << "Start build LayerNormGradOp";



  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_, scale_, bias_, mean_, variance_, out_grad_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_epsilon = pir::FloatAttribute::get(pir::IrContext::Instance(), epsilon);
  argument.AddAttribute("epsilon", attr_epsilon);
  pir::Attribute attr_begin_norm_axis = pir::Int32Attribute::get(pir::IrContext::Instance(), begin_norm_axis);
  argument.AddAttribute("begin_norm_axis", attr_begin_norm_axis);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;
  paddle::dialect::DenseTensorType mean = mean_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)mean;
  paddle::dialect::DenseTensorType variance = variance_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)variance;
  paddle::dialect::DenseTensorType out_grad = out_grad_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)out_grad;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);

  paddle::dialect::IrMetaTensor meta_scale;
  paddle::dialect::IrTensor ir_tensor_scale;
  if (scale_.impl() != nullptr) {
    paddle::dialect::DenseTensorType scale = scale_.type().dyn_cast<paddle::dialect::DenseTensorType>();
    VLOG(4) << "Builder construction  dense_scale";
    ir_tensor_scale = paddle::dialect::IrTensor(paddle::dialect::TransToPhiDataType(scale.dtype()),
                                                        scale.dims(),
                                                        scale.data_layout(),
                                                        scale.lod(),
                                                        scale.offset());
    VLOG(4) << "Builder construction  meta_scale";
    meta_scale = paddle::dialect::IrMetaTensor(&ir_tensor_scale);
  }


  paddle::dialect::IrMetaTensor meta_bias;
  paddle::dialect::IrTensor ir_tensor_bias;
  if (bias_.impl() != nullptr) {
    paddle::dialect::DenseTensorType bias = bias_.type().dyn_cast<paddle::dialect::DenseTensorType>();
    VLOG(4) << "Builder construction  dense_bias";
    ir_tensor_bias = paddle::dialect::IrTensor(paddle::dialect::TransToPhiDataType(bias.dtype()),
                                                        bias.dims(),
                                                        bias.data_layout(),
                                                        bias.lod(),
                                                        bias.offset());
    VLOG(4) << "Builder construction  meta_bias";
    meta_bias = paddle::dialect::IrMetaTensor(&ir_tensor_bias);
  }

  paddle::dialect::IrTensor dense_x_grad;
  paddle::dialect::IrMetaTensor meta_x_grad(&dense_x_grad);
  paddle::dialect::IrTensor dense_scale_grad;
  paddle::dialect::IrMetaTensor meta_scale_grad(&dense_scale_grad);
  paddle::dialect::IrTensor dense_bias_grad;
  paddle::dialect::IrMetaTensor meta_bias_grad(&dense_bias_grad);

  phi::LayerNormGradInferMeta(meta_x, meta_scale, meta_bias, &meta_x_grad, &meta_scale_grad, &meta_bias_grad);

  std::vector<pir::Type> argument_outputs;
  pir::Type x_grad_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_x_grad.dtype()), dense_x_grad.dims(), dense_x_grad.layout(), dense_x_grad.lod(), dense_x_grad.offset());
  argument_outputs.push_back(x_grad_dense_tensor_type);

  pir::Type scale_grad_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_scale_grad.dtype()), dense_scale_grad.dims(), dense_scale_grad.layout(), dense_scale_grad.lod(), dense_scale_grad.offset());
  argument_outputs.push_back(scale_grad_dense_tensor_type);

  pir::Type bias_grad_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_bias_grad.dtype()), dense_bias_grad.dims(), dense_bias_grad.layout(), dense_bias_grad.lod(), dense_bias_grad.offset());
  argument_outputs.push_back(bias_grad_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void LayerNormGradOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, pir::Value scale_, pir::Value bias_, pir::Value mean_, pir::Value variance_, pir::Value out_grad_, pir::AttributeMap attributes) {
  VLOG(4) << "Start build LayerNormGradOp";


  IR_ENFORCE(
      attributes.find("epsilon") != attributes.end(),
          "'epsilon' Attribute is expected for LayerNormGradOp. ");
  float epsilon = attributes.at("epsilon").dyn_cast<pir::FloatAttribute>().data();

  IR_ENFORCE(
      attributes.find("begin_norm_axis") != attributes.end(),
          "'begin_norm_axis' Attribute is expected for LayerNormGradOp. ");
  int begin_norm_axis = attributes.at("begin_norm_axis").dyn_cast<pir::Int32Attribute>().data();


  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_, scale_, bias_, mean_, variance_, out_grad_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_epsilon = pir::FloatAttribute::get(pir::IrContext::Instance(), epsilon);
  argument.AddAttribute("epsilon", attr_epsilon);
  pir::Attribute attr_begin_norm_axis = pir::Int32Attribute::get(pir::IrContext::Instance(), begin_norm_axis);
  argument.AddAttribute("begin_norm_axis", attr_begin_norm_axis);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;
  paddle::dialect::DenseTensorType mean = mean_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)mean;
  paddle::dialect::DenseTensorType variance = variance_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)variance;
  paddle::dialect::DenseTensorType out_grad = out_grad_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)out_grad;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);

  paddle::dialect::IrMetaTensor meta_scale;
  paddle::dialect::IrTensor ir_tensor_scale;
  if (scale_.impl() != nullptr) {
    paddle::dialect::DenseTensorType scale = scale_.type().dyn_cast<paddle::dialect::DenseTensorType>();
    VLOG(4) << "Builder construction  dense_scale";
    ir_tensor_scale = paddle::dialect::IrTensor(paddle::dialect::TransToPhiDataType(scale.dtype()),
                                                        scale.dims(),
                                                        scale.data_layout(),
                                                        scale.lod(),
                                                        scale.offset());
    VLOG(4) << "Builder construction  meta_scale";
    meta_scale = paddle::dialect::IrMetaTensor(&ir_tensor_scale);
  }


  paddle::dialect::IrMetaTensor meta_bias;
  paddle::dialect::IrTensor ir_tensor_bias;
  if (bias_.impl() != nullptr) {
    paddle::dialect::DenseTensorType bias = bias_.type().dyn_cast<paddle::dialect::DenseTensorType>();
    VLOG(4) << "Builder construction  dense_bias";
    ir_tensor_bias = paddle::dialect::IrTensor(paddle::dialect::TransToPhiDataType(bias.dtype()),
                                                        bias.dims(),
                                                        bias.data_layout(),
                                                        bias.lod(),
                                                        bias.offset());
    VLOG(4) << "Builder construction  meta_bias";
    meta_bias = paddle::dialect::IrMetaTensor(&ir_tensor_bias);
  }

  paddle::dialect::IrTensor dense_x_grad;
  paddle::dialect::IrMetaTensor meta_x_grad(&dense_x_grad);
  paddle::dialect::IrTensor dense_scale_grad;
  paddle::dialect::IrMetaTensor meta_scale_grad(&dense_scale_grad);
  paddle::dialect::IrTensor dense_bias_grad;
  paddle::dialect::IrMetaTensor meta_bias_grad(&dense_bias_grad);

  phi::LayerNormGradInferMeta(meta_x, meta_scale, meta_bias, &meta_x_grad, &meta_scale_grad, &meta_bias_grad);

  std::vector<pir::Type> argument_outputs;
  pir::Type x_grad_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_x_grad.dtype()), dense_x_grad.dims(), dense_x_grad.layout(), dense_x_grad.lod(), dense_x_grad.offset());
  argument_outputs.push_back(x_grad_dense_tensor_type);

  pir::Type scale_grad_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_scale_grad.dtype()), dense_scale_grad.dims(), dense_scale_grad.layout(), dense_scale_grad.lod(), dense_scale_grad.offset());
  argument_outputs.push_back(scale_grad_dense_tensor_type);

  pir::Type bias_grad_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_bias_grad.dtype()), dense_bias_grad.dims(), dense_bias_grad.layout(), dense_bias_grad.lod(), dense_bias_grad.offset());
  argument_outputs.push_back(bias_grad_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void LayerNormGradOp::VerifySig() {}

void LayerNormGradOp::InferMeta( phi::InferMetaContext *infer_meta ) {
  auto fn = PD_INFER_META(phi::LayerNormGradInferMeta);
  fn(infer_meta);
}

phi::DataType LayerNormGradOp::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: LayerNormGradOp";
  


  return expected_kernel_dtype;
}

const char *LeakyReluDoubleGradOp::attributes_name[1] = { "negative_slope" };

OpInfoTuple LeakyReluDoubleGradOp::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("x", "paddle::dialect::DenseTensorType", false, false, false, false), paddle::dialect::OpInputInfo("grad_x_grad", "paddle::dialect::DenseTensorType", false, false, false, false) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = { paddle::dialect::OpAttributeInfo("negative_slope", "pir::FloatAttribute", "") };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("grad_out_grad", "paddle::dialect::DenseTensorType", false, false) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("UnchangedInferMeta", {"grad_x_grad"}, "leaky_relu_double_grad", {"x", "grad_x_grad", "negative_slope"}, {}, {}, {}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "leaky_relu_double_grad");
}

void LeakyReluDoubleGradOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, pir::Value grad_x_grad_, float negative_slope) {
  VLOG(4) << "Start build LeakyReluDoubleGradOp";



  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_, grad_x_grad_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_negative_slope = pir::FloatAttribute::get(pir::IrContext::Instance(), negative_slope);
  argument.AddAttribute("negative_slope", attr_negative_slope);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;
  paddle::dialect::DenseTensorType grad_x_grad = grad_x_grad_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)grad_x_grad;

  VLOG(4) << "Builder construction  dense_grad_x_grad";
  paddle::dialect::IrTensor ir_tensor_grad_x_grad(paddle::dialect::TransToPhiDataType(grad_x_grad.dtype()),
                                                      grad_x_grad.dims(),
                                                      grad_x_grad.data_layout(),
                                                      grad_x_grad.lod(),
                                                      grad_x_grad.offset());
  VLOG(4) << "Builder construction  meta_grad_x_grad";
  paddle::dialect::IrMetaTensor meta_grad_x_grad(&ir_tensor_grad_x_grad);
  paddle::dialect::IrTensor dense_grad_out_grad;
  paddle::dialect::IrMetaTensor meta_grad_out_grad(&dense_grad_out_grad);

  phi::UnchangedInferMeta(meta_grad_x_grad, &meta_grad_out_grad);

  std::vector<pir::Type> argument_outputs;
  pir::Type grad_out_grad_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_grad_out_grad.dtype()), dense_grad_out_grad.dims(), dense_grad_out_grad.layout(), dense_grad_out_grad.lod(), dense_grad_out_grad.offset());
  argument_outputs.push_back(grad_out_grad_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void LeakyReluDoubleGradOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, pir::Value grad_x_grad_, pir::AttributeMap attributes) {
  VLOG(4) << "Start build LeakyReluDoubleGradOp";


  IR_ENFORCE(
      attributes.find("negative_slope") != attributes.end(),
          "'negative_slope' Attribute is expected for LeakyReluDoubleGradOp. ");
  float negative_slope = attributes.at("negative_slope").dyn_cast<pir::FloatAttribute>().data();


  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_, grad_x_grad_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_negative_slope = pir::FloatAttribute::get(pir::IrContext::Instance(), negative_slope);
  argument.AddAttribute("negative_slope", attr_negative_slope);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;
  paddle::dialect::DenseTensorType grad_x_grad = grad_x_grad_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)grad_x_grad;

  VLOG(4) << "Builder construction  dense_grad_x_grad";
  paddle::dialect::IrTensor ir_tensor_grad_x_grad(paddle::dialect::TransToPhiDataType(grad_x_grad.dtype()),
                                                      grad_x_grad.dims(),
                                                      grad_x_grad.data_layout(),
                                                      grad_x_grad.lod(),
                                                      grad_x_grad.offset());
  VLOG(4) << "Builder construction  meta_grad_x_grad";
  paddle::dialect::IrMetaTensor meta_grad_x_grad(&ir_tensor_grad_x_grad);
  paddle::dialect::IrTensor dense_grad_out_grad;
  paddle::dialect::IrMetaTensor meta_grad_out_grad(&dense_grad_out_grad);

  phi::UnchangedInferMeta(meta_grad_x_grad, &meta_grad_out_grad);

  std::vector<pir::Type> argument_outputs;
  pir::Type grad_out_grad_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_grad_out_grad.dtype()), dense_grad_out_grad.dims(), dense_grad_out_grad.layout(), dense_grad_out_grad.lod(), dense_grad_out_grad.offset());
  argument_outputs.push_back(grad_out_grad_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void LeakyReluDoubleGradOp::VerifySig() {}

void LeakyReluDoubleGradOp::InferMeta( phi::InferMetaContext *infer_meta ) {
  auto fn = PD_INFER_META(phi::UnchangedInferMeta);
  fn(infer_meta);
}

phi::DataType LeakyReluDoubleGradOp::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: LeakyReluDoubleGradOp";
  


  return expected_kernel_dtype;
}

const char *LeakyReluDoubleGrad_Op::attributes_name[1] = { "negative_slope" };

OpInfoTuple LeakyReluDoubleGrad_Op::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("x", "paddle::dialect::DenseTensorType", false, false, false, false), paddle::dialect::OpInputInfo("grad_x_grad", "paddle::dialect::DenseTensorType", false, false, false, false) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = { paddle::dialect::OpAttributeInfo("negative_slope", "pir::FloatAttribute", "") };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("grad_out_grad", "paddle::dialect::DenseTensorType", false, false) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("UnchangedInferMeta", {"grad_x_grad"}, "leaky_relu_double_grad", {"x", "grad_x_grad", "negative_slope"}, {}, {}, {{"grad_out_grad", "grad_x_grad"}}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "leaky_relu_double_grad");
}

void LeakyReluDoubleGrad_Op::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, pir::Value grad_x_grad_, float negative_slope) {
  VLOG(4) << "Start build LeakyReluDoubleGrad_Op";



  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_, grad_x_grad_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_negative_slope = pir::FloatAttribute::get(pir::IrContext::Instance(), negative_slope);
  argument.AddAttribute("negative_slope", attr_negative_slope);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;
  paddle::dialect::DenseTensorType grad_x_grad = grad_x_grad_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)grad_x_grad;

  VLOG(4) << "Builder construction  dense_grad_x_grad";
  paddle::dialect::IrTensor ir_tensor_grad_x_grad(paddle::dialect::TransToPhiDataType(grad_x_grad.dtype()),
                                                      grad_x_grad.dims(),
                                                      grad_x_grad.data_layout(),
                                                      grad_x_grad.lod(),
                                                      grad_x_grad.offset());
  VLOG(4) << "Builder construction  meta_grad_x_grad";
  paddle::dialect::IrMetaTensor meta_grad_x_grad(&ir_tensor_grad_x_grad);
  paddle::dialect::IrTensor dense_grad_out_grad;
  paddle::dialect::IrMetaTensor meta_grad_out_grad(&dense_grad_out_grad);

  phi::UnchangedInferMeta(meta_grad_x_grad, &meta_grad_out_grad);

  std::vector<pir::Type> argument_outputs;
  pir::Type grad_out_grad_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_grad_out_grad.dtype()), dense_grad_out_grad.dims(), dense_grad_out_grad.layout(), dense_grad_out_grad.lod(), dense_grad_out_grad.offset());
  argument_outputs.push_back(grad_out_grad_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void LeakyReluDoubleGrad_Op::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, pir::Value grad_x_grad_, pir::AttributeMap attributes) {
  VLOG(4) << "Start build LeakyReluDoubleGrad_Op";


  IR_ENFORCE(
      attributes.find("negative_slope") != attributes.end(),
          "'negative_slope' Attribute is expected for LeakyReluDoubleGrad_Op. ");
  float negative_slope = attributes.at("negative_slope").dyn_cast<pir::FloatAttribute>().data();


  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_, grad_x_grad_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_negative_slope = pir::FloatAttribute::get(pir::IrContext::Instance(), negative_slope);
  argument.AddAttribute("negative_slope", attr_negative_slope);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;
  paddle::dialect::DenseTensorType grad_x_grad = grad_x_grad_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)grad_x_grad;

  VLOG(4) << "Builder construction  dense_grad_x_grad";
  paddle::dialect::IrTensor ir_tensor_grad_x_grad(paddle::dialect::TransToPhiDataType(grad_x_grad.dtype()),
                                                      grad_x_grad.dims(),
                                                      grad_x_grad.data_layout(),
                                                      grad_x_grad.lod(),
                                                      grad_x_grad.offset());
  VLOG(4) << "Builder construction  meta_grad_x_grad";
  paddle::dialect::IrMetaTensor meta_grad_x_grad(&ir_tensor_grad_x_grad);
  paddle::dialect::IrTensor dense_grad_out_grad;
  paddle::dialect::IrMetaTensor meta_grad_out_grad(&dense_grad_out_grad);

  phi::UnchangedInferMeta(meta_grad_x_grad, &meta_grad_out_grad);

  std::vector<pir::Type> argument_outputs;
  pir::Type grad_out_grad_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_grad_out_grad.dtype()), dense_grad_out_grad.dims(), dense_grad_out_grad.layout(), dense_grad_out_grad.lod(), dense_grad_out_grad.offset());
  argument_outputs.push_back(grad_out_grad_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void LeakyReluDoubleGrad_Op::VerifySig() {}

void LeakyReluDoubleGrad_Op::InferMeta( phi::InferMetaContext *infer_meta ) {
  auto fn = PD_INFER_META(phi::UnchangedInferMeta);
  fn(infer_meta);
}

phi::DataType LeakyReluDoubleGrad_Op::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: LeakyReluDoubleGrad_Op";
  


  return expected_kernel_dtype;
}

const char *LeakyReluGradOp::attributes_name[1] = { "negative_slope" };

OpInfoTuple LeakyReluGradOp::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("x", "paddle::dialect::DenseTensorType", false, false, false, false), paddle::dialect::OpInputInfo("out_grad", "paddle::dialect::DenseTensorType", false, false, false, true) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = { paddle::dialect::OpAttributeInfo("negative_slope", "pir::FloatAttribute", "") };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("x_grad", "paddle::dialect::DenseTensorType", false, false) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("UnchangedInferMeta", {"x"}, "leaky_relu_grad", {"x", "out_grad", "negative_slope"}, {}, {}, {}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "leaky_relu_grad");
}

void LeakyReluGradOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, pir::Value out_grad_, float negative_slope) {
  VLOG(4) << "Start build LeakyReluGradOp";



  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_, out_grad_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_negative_slope = pir::FloatAttribute::get(pir::IrContext::Instance(), negative_slope);
  argument.AddAttribute("negative_slope", attr_negative_slope);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;
  paddle::dialect::DenseTensorType out_grad = out_grad_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)out_grad;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);
  paddle::dialect::IrTensor dense_x_grad;
  paddle::dialect::IrMetaTensor meta_x_grad(&dense_x_grad);

  phi::UnchangedInferMeta(meta_x, &meta_x_grad);

  std::vector<pir::Type> argument_outputs;
  pir::Type x_grad_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_x_grad.dtype()), dense_x_grad.dims(), dense_x_grad.layout(), dense_x_grad.lod(), dense_x_grad.offset());
  argument_outputs.push_back(x_grad_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void LeakyReluGradOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, pir::Value out_grad_, pir::AttributeMap attributes) {
  VLOG(4) << "Start build LeakyReluGradOp";


  IR_ENFORCE(
      attributes.find("negative_slope") != attributes.end(),
          "'negative_slope' Attribute is expected for LeakyReluGradOp. ");
  float negative_slope = attributes.at("negative_slope").dyn_cast<pir::FloatAttribute>().data();


  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_, out_grad_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_negative_slope = pir::FloatAttribute::get(pir::IrContext::Instance(), negative_slope);
  argument.AddAttribute("negative_slope", attr_negative_slope);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;
  paddle::dialect::DenseTensorType out_grad = out_grad_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)out_grad;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);
  paddle::dialect::IrTensor dense_x_grad;
  paddle::dialect::IrMetaTensor meta_x_grad(&dense_x_grad);

  phi::UnchangedInferMeta(meta_x, &meta_x_grad);

  std::vector<pir::Type> argument_outputs;
  pir::Type x_grad_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_x_grad.dtype()), dense_x_grad.dims(), dense_x_grad.layout(), dense_x_grad.lod(), dense_x_grad.offset());
  argument_outputs.push_back(x_grad_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void LeakyReluGradOp::VerifySig() {}

void LeakyReluGradOp::InferMeta( phi::InferMetaContext *infer_meta ) {
  auto fn = PD_INFER_META(phi::UnchangedInferMeta);
  fn(infer_meta);
}

phi::DataType LeakyReluGradOp::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: LeakyReluGradOp";
  


  return expected_kernel_dtype;
}

const char *LeakyReluGrad_Op::attributes_name[1] = { "negative_slope" };

OpInfoTuple LeakyReluGrad_Op::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("x", "paddle::dialect::DenseTensorType", false, false, false, false), paddle::dialect::OpInputInfo("out_grad", "paddle::dialect::DenseTensorType", false, false, false, true) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = { paddle::dialect::OpAttributeInfo("negative_slope", "pir::FloatAttribute", "") };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("x_grad", "paddle::dialect::DenseTensorType", false, false) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("UnchangedInferMeta", {"x"}, "leaky_relu_grad", {"x", "out_grad", "negative_slope"}, {}, {}, {{"x_grad", "out_grad"}}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "leaky_relu_grad");
}

void LeakyReluGrad_Op::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, pir::Value out_grad_, float negative_slope) {
  VLOG(4) << "Start build LeakyReluGrad_Op";



  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_, out_grad_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_negative_slope = pir::FloatAttribute::get(pir::IrContext::Instance(), negative_slope);
  argument.AddAttribute("negative_slope", attr_negative_slope);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;
  paddle::dialect::DenseTensorType out_grad = out_grad_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)out_grad;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);
  paddle::dialect::IrTensor dense_x_grad;
  paddle::dialect::IrMetaTensor meta_x_grad(&dense_x_grad);

  phi::UnchangedInferMeta(meta_x, &meta_x_grad);

  std::vector<pir::Type> argument_outputs;
  pir::Type x_grad_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_x_grad.dtype()), dense_x_grad.dims(), dense_x_grad.layout(), dense_x_grad.lod(), dense_x_grad.offset());
  argument_outputs.push_back(x_grad_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void LeakyReluGrad_Op::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, pir::Value out_grad_, pir::AttributeMap attributes) {
  VLOG(4) << "Start build LeakyReluGrad_Op";


  IR_ENFORCE(
      attributes.find("negative_slope") != attributes.end(),
          "'negative_slope' Attribute is expected for LeakyReluGrad_Op. ");
  float negative_slope = attributes.at("negative_slope").dyn_cast<pir::FloatAttribute>().data();


  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_, out_grad_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_negative_slope = pir::FloatAttribute::get(pir::IrContext::Instance(), negative_slope);
  argument.AddAttribute("negative_slope", attr_negative_slope);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;
  paddle::dialect::DenseTensorType out_grad = out_grad_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)out_grad;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);
  paddle::dialect::IrTensor dense_x_grad;
  paddle::dialect::IrMetaTensor meta_x_grad(&dense_x_grad);

  phi::UnchangedInferMeta(meta_x, &meta_x_grad);

  std::vector<pir::Type> argument_outputs;
  pir::Type x_grad_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_x_grad.dtype()), dense_x_grad.dims(), dense_x_grad.layout(), dense_x_grad.lod(), dense_x_grad.offset());
  argument_outputs.push_back(x_grad_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void LeakyReluGrad_Op::VerifySig() {}

void LeakyReluGrad_Op::InferMeta( phi::InferMetaContext *infer_meta ) {
  auto fn = PD_INFER_META(phi::UnchangedInferMeta);
  fn(infer_meta);
}

phi::DataType LeakyReluGrad_Op::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: LeakyReluGrad_Op";
  


  return expected_kernel_dtype;
}

OpInfoTuple LerpGradOp::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("x", "paddle::dialect::DenseTensorType", false, false, false, false), paddle::dialect::OpInputInfo("y", "paddle::dialect::DenseTensorType", false, false, false, false), paddle::dialect::OpInputInfo("weight", "paddle::dialect::DenseTensorType", false, false, false, false), paddle::dialect::OpInputInfo("out", "paddle::dialect::DenseTensorType", false, false, false, false), paddle::dialect::OpInputInfo("out_grad", "paddle::dialect::DenseTensorType", false, false, false, false) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = {  };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("x_grad", "paddle::dialect::DenseTensorType", false, false), paddle::dialect::OpOutputInfo("y_grad", "paddle::dialect::DenseTensorType", false, false) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("GeneralBinaryGradInferMeta", {"x", "y"}, "lerp_grad", {"x", "y", "weight", "out", "out_grad"}, {}, {}, {}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "lerp_grad");
}

void LerpGradOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, pir::Value y_, pir::Value weight_, pir::Value out_, pir::Value out_grad_) {
  VLOG(4) << "Start build LerpGradOp";



  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_, y_, weight_, out_, out_grad_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;
  paddle::dialect::DenseTensorType y = y_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)y;
  paddle::dialect::DenseTensorType weight = weight_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)weight;
  paddle::dialect::DenseTensorType out = out_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)out;
  paddle::dialect::DenseTensorType out_grad = out_grad_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)out_grad;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);

  VLOG(4) << "Builder construction  dense_y";
  paddle::dialect::IrTensor ir_tensor_y(paddle::dialect::TransToPhiDataType(y.dtype()),
                                                      y.dims(),
                                                      y.data_layout(),
                                                      y.lod(),
                                                      y.offset());
  VLOG(4) << "Builder construction  meta_y";
  paddle::dialect::IrMetaTensor meta_y(&ir_tensor_y);
  paddle::dialect::IrTensor dense_x_grad;
  paddle::dialect::IrMetaTensor meta_x_grad(&dense_x_grad);
  paddle::dialect::IrTensor dense_y_grad;
  paddle::dialect::IrMetaTensor meta_y_grad(&dense_y_grad);

  phi::GeneralBinaryGradInferMeta(meta_x, meta_y, &meta_x_grad, &meta_y_grad);

  std::vector<pir::Type> argument_outputs;
  pir::Type x_grad_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_x_grad.dtype()), dense_x_grad.dims(), dense_x_grad.layout(), dense_x_grad.lod(), dense_x_grad.offset());
  argument_outputs.push_back(x_grad_dense_tensor_type);

  pir::Type y_grad_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_y_grad.dtype()), dense_y_grad.dims(), dense_y_grad.layout(), dense_y_grad.lod(), dense_y_grad.offset());
  argument_outputs.push_back(y_grad_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void LerpGradOp::VerifySig() {}

void LerpGradOp::InferMeta( phi::InferMetaContext *infer_meta ) {
  auto fn = PD_INFER_META(phi::GeneralBinaryGradInferMeta);
  fn(infer_meta);
}

phi::DataType LerpGradOp::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: LerpGradOp";
  


  return expected_kernel_dtype;
}

OpInfoTuple LgammaGradOp::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("x", "paddle::dialect::DenseTensorType", false, false, false, false), paddle::dialect::OpInputInfo("out_grad", "paddle::dialect::DenseTensorType", false, false, false, false) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = {  };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("x_grad", "paddle::dialect::DenseTensorType", false, false) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("UnchangedInferMeta", {"x"}, "lgamma_grad", {"x", "out_grad"}, {}, {}, {}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "lgamma_grad");
}

void LgammaGradOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, pir::Value out_grad_) {
  VLOG(4) << "Start build LgammaGradOp";



  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_, out_grad_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;
  paddle::dialect::DenseTensorType out_grad = out_grad_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)out_grad;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);
  paddle::dialect::IrTensor dense_x_grad;
  paddle::dialect::IrMetaTensor meta_x_grad(&dense_x_grad);

  phi::UnchangedInferMeta(meta_x, &meta_x_grad);

  std::vector<pir::Type> argument_outputs;
  pir::Type x_grad_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_x_grad.dtype()), dense_x_grad.dims(), dense_x_grad.layout(), dense_x_grad.lod(), dense_x_grad.offset());
  argument_outputs.push_back(x_grad_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void LgammaGradOp::VerifySig() {}

void LgammaGradOp::InferMeta( phi::InferMetaContext *infer_meta ) {
  auto fn = PD_INFER_META(phi::UnchangedInferMeta);
  fn(infer_meta);
}

phi::DataType LgammaGradOp::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: LgammaGradOp";
  


  return expected_kernel_dtype;
}

const char *LinearInterpGradOp::attributes_name[8] = { "data_layout", "out_d", "out_h", "out_w", "scale", "interp_method", "align_corners", "align_mode" };

OpInfoTuple LinearInterpGradOp::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("x", "paddle::dialect::DenseTensorType", false, true, false, false), paddle::dialect::OpInputInfo("out_size", "paddle::dialect::DenseTensorType", true, false, false, false), paddle::dialect::OpInputInfo("size_tensor", "pir::VectorType<paddle::dialect::DenseTensorType>", true, false, false, false), paddle::dialect::OpInputInfo("scale_tensor", "paddle::dialect::DenseTensorType", true, false, false, false), paddle::dialect::OpInputInfo("output_grad", "paddle::dialect::DenseTensorType", false, false, false, false) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = { paddle::dialect::OpAttributeInfo("data_layout", "pir::StrAttribute", ""), paddle::dialect::OpAttributeInfo("out_d", "pir::Int32Attribute", ""), paddle::dialect::OpAttributeInfo("out_h", "pir::Int32Attribute", ""), paddle::dialect::OpAttributeInfo("out_w", "pir::Int32Attribute", ""), paddle::dialect::OpAttributeInfo("scale", "pir::ArrayAttribute<pir::FloatAttribute>", ""), paddle::dialect::OpAttributeInfo("interp_method", "pir::StrAttribute", ""), paddle::dialect::OpAttributeInfo("align_corners", "pir::BoolAttribute", ""), paddle::dialect::OpAttributeInfo("align_mode", "pir::Int32Attribute", "") };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("x_grad", "paddle::dialect::DenseTensorType", false, false) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("UnchangedInferMeta", {"x"}, "linear_interp_grad", {"x", "out_size", "size_tensor", "scale_tensor", "output_grad", "data_layout", "out_d", "out_h", "out_w", "scale", "interp_method", "align_corners", "align_mode"}, {"output_grad"}, {}, {}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "linear_interp_grad");
}

void LinearInterpGradOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, pir::Value out_size_, pir::Value size_tensor_, pir::Value scale_tensor_, pir::Value output_grad_, const std::string& data_layout, int out_d, int out_h, int out_w, const std::vector<float>& scale, const std::string& interp_method, bool align_corners, int align_mode) {
  VLOG(4) << "Start build LinearInterpGradOp";



  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_, out_size_, size_tensor_, scale_tensor_, output_grad_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_data_layout = pir::StrAttribute::get(pir::IrContext::Instance(), data_layout);
  argument.AddAttribute("data_layout", attr_data_layout);
  pir::Attribute attr_out_d = pir::Int32Attribute::get(pir::IrContext::Instance(), out_d);
  argument.AddAttribute("out_d", attr_out_d);
  pir::Attribute attr_out_h = pir::Int32Attribute::get(pir::IrContext::Instance(), out_h);
  argument.AddAttribute("out_h", attr_out_h);
  pir::Attribute attr_out_w = pir::Int32Attribute::get(pir::IrContext::Instance(), out_w);
  argument.AddAttribute("out_w", attr_out_w);
  std::vector<pir::Attribute> vec_scale;
  for (size_t i = 0; i < static_cast<size_t>(scale.size()); i++) {
      pir::Attribute attr_scale = pir::FloatAttribute::get(pir::IrContext::Instance(), scale[i]);

    vec_scale.push_back(attr_scale);
  }
  pir::Attribute attr_scale = pir::ArrayAttribute::get(pir::IrContext::Instance(), vec_scale);
  argument.AddAttribute("scale", attr_scale);
  pir::Attribute attr_interp_method = pir::StrAttribute::get(pir::IrContext::Instance(), interp_method);
  argument.AddAttribute("interp_method", attr_interp_method);
  pir::Attribute attr_align_corners = pir::BoolAttribute::get(pir::IrContext::Instance(), align_corners);
  argument.AddAttribute("align_corners", attr_align_corners);
  pir::Attribute attr_align_mode = pir::Int32Attribute::get(pir::IrContext::Instance(), align_mode);
  argument.AddAttribute("align_mode", attr_align_mode);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;
  paddle::dialect::DenseTensorType output_grad = output_grad_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)output_grad;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);
  paddle::dialect::IrTensor dense_x_grad;
  paddle::dialect::IrMetaTensor meta_x_grad(&dense_x_grad);

  phi::UnchangedInferMeta(meta_x, &meta_x_grad);

  std::vector<pir::Type> argument_outputs;
  pir::Type x_grad_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_x_grad.dtype()), dense_x_grad.dims(), dense_x_grad.layout(), dense_x_grad.lod(), dense_x_grad.offset());
  argument_outputs.push_back(x_grad_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void LinearInterpGradOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, pir::Value out_size_, pir::Value size_tensor_, pir::Value scale_tensor_, pir::Value output_grad_, pir::AttributeMap attributes) {
  VLOG(4) << "Start build LinearInterpGradOp";


  IR_ENFORCE(
      attributes.find("data_layout") != attributes.end(),
          "'data_layout' Attribute is expected for LinearInterpGradOp. ");
  std::string data_layout = attributes.at("data_layout").dyn_cast<pir::StrAttribute>().AsString();

  IR_ENFORCE(
      attributes.find("out_d") != attributes.end(),
          "'out_d' Attribute is expected for LinearInterpGradOp. ");
  int out_d = attributes.at("out_d").dyn_cast<pir::Int32Attribute>().data();

  IR_ENFORCE(
      attributes.find("out_h") != attributes.end(),
          "'out_h' Attribute is expected for LinearInterpGradOp. ");
  int out_h = attributes.at("out_h").dyn_cast<pir::Int32Attribute>().data();

  IR_ENFORCE(
      attributes.find("out_w") != attributes.end(),
          "'out_w' Attribute is expected for LinearInterpGradOp. ");
  int out_w = attributes.at("out_w").dyn_cast<pir::Int32Attribute>().data();

  IR_ENFORCE(
      attributes.find("scale") != attributes.end(),
          "'scale' Attribute is expected for LinearInterpGradOp. ");
  std::vector<float> scale;
  for (size_t i = 0; i < attributes.at("scale").dyn_cast<pir::ArrayAttribute>().size(); i++) {
    scale.push_back(attributes.at("scale").dyn_cast<pir::ArrayAttribute>().at(i).dyn_cast<pir::FloatAttribute>().data());
  }

  IR_ENFORCE(
      attributes.find("interp_method") != attributes.end(),
          "'interp_method' Attribute is expected for LinearInterpGradOp. ");
  std::string interp_method = attributes.at("interp_method").dyn_cast<pir::StrAttribute>().AsString();

  IR_ENFORCE(
      attributes.find("align_corners") != attributes.end(),
          "'align_corners' Attribute is expected for LinearInterpGradOp. ");
  bool align_corners = attributes.at("align_corners").dyn_cast<pir::BoolAttribute>().data();

  IR_ENFORCE(
      attributes.find("align_mode") != attributes.end(),
          "'align_mode' Attribute is expected for LinearInterpGradOp. ");
  int align_mode = attributes.at("align_mode").dyn_cast<pir::Int32Attribute>().data();


  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_, out_size_, size_tensor_, scale_tensor_, output_grad_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_data_layout = pir::StrAttribute::get(pir::IrContext::Instance(), data_layout);
  argument.AddAttribute("data_layout", attr_data_layout);
  pir::Attribute attr_out_d = pir::Int32Attribute::get(pir::IrContext::Instance(), out_d);
  argument.AddAttribute("out_d", attr_out_d);
  pir::Attribute attr_out_h = pir::Int32Attribute::get(pir::IrContext::Instance(), out_h);
  argument.AddAttribute("out_h", attr_out_h);
  pir::Attribute attr_out_w = pir::Int32Attribute::get(pir::IrContext::Instance(), out_w);
  argument.AddAttribute("out_w", attr_out_w);
  std::vector<pir::Attribute> vec_scale;
  for (size_t i = 0; i < static_cast<size_t>(scale.size()); i++) {
      pir::Attribute attr_scale = pir::FloatAttribute::get(pir::IrContext::Instance(), scale[i]);

    vec_scale.push_back(attr_scale);
  }
  pir::Attribute attr_scale = pir::ArrayAttribute::get(pir::IrContext::Instance(), vec_scale);
  argument.AddAttribute("scale", attr_scale);
  pir::Attribute attr_interp_method = pir::StrAttribute::get(pir::IrContext::Instance(), interp_method);
  argument.AddAttribute("interp_method", attr_interp_method);
  pir::Attribute attr_align_corners = pir::BoolAttribute::get(pir::IrContext::Instance(), align_corners);
  argument.AddAttribute("align_corners", attr_align_corners);
  pir::Attribute attr_align_mode = pir::Int32Attribute::get(pir::IrContext::Instance(), align_mode);
  argument.AddAttribute("align_mode", attr_align_mode);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;
  paddle::dialect::DenseTensorType output_grad = output_grad_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)output_grad;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);
  paddle::dialect::IrTensor dense_x_grad;
  paddle::dialect::IrMetaTensor meta_x_grad(&dense_x_grad);

  phi::UnchangedInferMeta(meta_x, &meta_x_grad);

  std::vector<pir::Type> argument_outputs;
  pir::Type x_grad_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_x_grad.dtype()), dense_x_grad.dims(), dense_x_grad.layout(), dense_x_grad.lod(), dense_x_grad.offset());
  argument_outputs.push_back(x_grad_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void LinearInterpGradOp::VerifySig() {}

void LinearInterpGradOp::InferMeta( phi::InferMetaContext *infer_meta ) {
  auto fn = PD_INFER_META(phi::UnchangedInferMeta);
  fn(infer_meta);
}

phi::DataType LinearInterpGradOp::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: LinearInterpGradOp";
  

  // deal skip data transform
  if (var_name == "out_size" || var_name == "size_tensor" || var_name == "scale_tensor"){
    return expected_kernel_dtype;
  }


  return expected_kernel_dtype;
}

OpInfoTuple Log10GradOp::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("x", "paddle::dialect::DenseTensorType", false, false, false, false), paddle::dialect::OpInputInfo("out_grad", "paddle::dialect::DenseTensorType", false, false, false, false) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = {  };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("x_grad", "paddle::dialect::DenseTensorType", false, false) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("UnchangedInferMeta", {"x"}, "log10_grad", {"x", "out_grad"}, {}, {}, {}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "log10_grad");
}

void Log10GradOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, pir::Value out_grad_) {
  VLOG(4) << "Start build Log10GradOp";



  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_, out_grad_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;
  paddle::dialect::DenseTensorType out_grad = out_grad_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)out_grad;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);
  paddle::dialect::IrTensor dense_x_grad;
  paddle::dialect::IrMetaTensor meta_x_grad(&dense_x_grad);

  phi::UnchangedInferMeta(meta_x, &meta_x_grad);

  std::vector<pir::Type> argument_outputs;
  pir::Type x_grad_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_x_grad.dtype()), dense_x_grad.dims(), dense_x_grad.layout(), dense_x_grad.lod(), dense_x_grad.offset());
  argument_outputs.push_back(x_grad_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void Log10GradOp::VerifySig() {}

void Log10GradOp::InferMeta( phi::InferMetaContext *infer_meta ) {
  auto fn = PD_INFER_META(phi::UnchangedInferMeta);
  fn(infer_meta);
}

phi::DataType Log10GradOp::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: Log10GradOp";
  


  return expected_kernel_dtype;
}

OpInfoTuple Log10Grad_Op::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("x", "paddle::dialect::DenseTensorType", false, false, false, false), paddle::dialect::OpInputInfo("out_grad", "paddle::dialect::DenseTensorType", false, false, false, false) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = {  };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("x_grad", "paddle::dialect::DenseTensorType", false, false) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("UnchangedInferMeta", {"x"}, "log10_grad", {"x", "out_grad"}, {}, {}, {{"x_grad", "out_grad"}}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "log10_grad");
}

void Log10Grad_Op::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, pir::Value out_grad_) {
  VLOG(4) << "Start build Log10Grad_Op";



  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_, out_grad_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;
  paddle::dialect::DenseTensorType out_grad = out_grad_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)out_grad;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);
  paddle::dialect::IrTensor dense_x_grad;
  paddle::dialect::IrMetaTensor meta_x_grad(&dense_x_grad);

  phi::UnchangedInferMeta(meta_x, &meta_x_grad);

  std::vector<pir::Type> argument_outputs;
  pir::Type x_grad_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_x_grad.dtype()), dense_x_grad.dims(), dense_x_grad.layout(), dense_x_grad.lod(), dense_x_grad.offset());
  argument_outputs.push_back(x_grad_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void Log10Grad_Op::VerifySig() {}

void Log10Grad_Op::InferMeta( phi::InferMetaContext *infer_meta ) {
  auto fn = PD_INFER_META(phi::UnchangedInferMeta);
  fn(infer_meta);
}

phi::DataType Log10Grad_Op::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: Log10Grad_Op";
  


  return expected_kernel_dtype;
}

OpInfoTuple Log1pGradOp::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("x", "paddle::dialect::DenseTensorType", false, false, false, false), paddle::dialect::OpInputInfo("out_grad", "paddle::dialect::DenseTensorType", false, false, false, false) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = {  };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("x_grad", "paddle::dialect::DenseTensorType", false, false) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("UnchangedInferMeta", {"x"}, "log1p_grad", {"x", "out_grad"}, {}, {}, {}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "log1p_grad");
}

void Log1pGradOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, pir::Value out_grad_) {
  VLOG(4) << "Start build Log1pGradOp";



  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_, out_grad_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;
  paddle::dialect::DenseTensorType out_grad = out_grad_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)out_grad;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);
  paddle::dialect::IrTensor dense_x_grad;
  paddle::dialect::IrMetaTensor meta_x_grad(&dense_x_grad);

  phi::UnchangedInferMeta(meta_x, &meta_x_grad);

  std::vector<pir::Type> argument_outputs;
  pir::Type x_grad_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_x_grad.dtype()), dense_x_grad.dims(), dense_x_grad.layout(), dense_x_grad.lod(), dense_x_grad.offset());
  argument_outputs.push_back(x_grad_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void Log1pGradOp::VerifySig() {}

void Log1pGradOp::InferMeta( phi::InferMetaContext *infer_meta ) {
  auto fn = PD_INFER_META(phi::UnchangedInferMeta);
  fn(infer_meta);
}

phi::DataType Log1pGradOp::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: Log1pGradOp";
  


  return expected_kernel_dtype;
}

OpInfoTuple Log1pGrad_Op::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("x", "paddle::dialect::DenseTensorType", false, false, false, false), paddle::dialect::OpInputInfo("out_grad", "paddle::dialect::DenseTensorType", false, false, false, false) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = {  };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("x_grad", "paddle::dialect::DenseTensorType", false, false) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("UnchangedInferMeta", {"x"}, "log1p_grad", {"x", "out_grad"}, {}, {}, {{"x_grad", "out_grad"}}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "log1p_grad");
}

void Log1pGrad_Op::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, pir::Value out_grad_) {
  VLOG(4) << "Start build Log1pGrad_Op";



  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_, out_grad_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;
  paddle::dialect::DenseTensorType out_grad = out_grad_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)out_grad;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);
  paddle::dialect::IrTensor dense_x_grad;
  paddle::dialect::IrMetaTensor meta_x_grad(&dense_x_grad);

  phi::UnchangedInferMeta(meta_x, &meta_x_grad);

  std::vector<pir::Type> argument_outputs;
  pir::Type x_grad_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_x_grad.dtype()), dense_x_grad.dims(), dense_x_grad.layout(), dense_x_grad.lod(), dense_x_grad.offset());
  argument_outputs.push_back(x_grad_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void Log1pGrad_Op::VerifySig() {}

void Log1pGrad_Op::InferMeta( phi::InferMetaContext *infer_meta ) {
  auto fn = PD_INFER_META(phi::UnchangedInferMeta);
  fn(infer_meta);
}

phi::DataType Log1pGrad_Op::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: Log1pGrad_Op";
  


  return expected_kernel_dtype;
}

OpInfoTuple Log2GradOp::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("x", "paddle::dialect::DenseTensorType", false, false, false, false), paddle::dialect::OpInputInfo("out_grad", "paddle::dialect::DenseTensorType", false, false, false, false) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = {  };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("x_grad", "paddle::dialect::DenseTensorType", false, false) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("UnchangedInferMeta", {"x"}, "log2_grad", {"x", "out_grad"}, {}, {}, {}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "log2_grad");
}

void Log2GradOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, pir::Value out_grad_) {
  VLOG(4) << "Start build Log2GradOp";



  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_, out_grad_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;
  paddle::dialect::DenseTensorType out_grad = out_grad_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)out_grad;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);
  paddle::dialect::IrTensor dense_x_grad;
  paddle::dialect::IrMetaTensor meta_x_grad(&dense_x_grad);

  phi::UnchangedInferMeta(meta_x, &meta_x_grad);

  std::vector<pir::Type> argument_outputs;
  pir::Type x_grad_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_x_grad.dtype()), dense_x_grad.dims(), dense_x_grad.layout(), dense_x_grad.lod(), dense_x_grad.offset());
  argument_outputs.push_back(x_grad_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void Log2GradOp::VerifySig() {}

void Log2GradOp::InferMeta( phi::InferMetaContext *infer_meta ) {
  auto fn = PD_INFER_META(phi::UnchangedInferMeta);
  fn(infer_meta);
}

phi::DataType Log2GradOp::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: Log2GradOp";
  


  return expected_kernel_dtype;
}

OpInfoTuple Log2Grad_Op::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("x", "paddle::dialect::DenseTensorType", false, false, false, false), paddle::dialect::OpInputInfo("out_grad", "paddle::dialect::DenseTensorType", false, false, false, false) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = {  };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("x_grad", "paddle::dialect::DenseTensorType", false, false) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("UnchangedInferMeta", {"x"}, "log2_grad", {"x", "out_grad"}, {}, {}, {{"x_grad", "out_grad"}}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "log2_grad");
}

void Log2Grad_Op::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, pir::Value out_grad_) {
  VLOG(4) << "Start build Log2Grad_Op";



  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_, out_grad_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;
  paddle::dialect::DenseTensorType out_grad = out_grad_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)out_grad;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);
  paddle::dialect::IrTensor dense_x_grad;
  paddle::dialect::IrMetaTensor meta_x_grad(&dense_x_grad);

  phi::UnchangedInferMeta(meta_x, &meta_x_grad);

  std::vector<pir::Type> argument_outputs;
  pir::Type x_grad_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_x_grad.dtype()), dense_x_grad.dims(), dense_x_grad.layout(), dense_x_grad.lod(), dense_x_grad.offset());
  argument_outputs.push_back(x_grad_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void Log2Grad_Op::VerifySig() {}

void Log2Grad_Op::InferMeta( phi::InferMetaContext *infer_meta ) {
  auto fn = PD_INFER_META(phi::UnchangedInferMeta);
  fn(infer_meta);
}

phi::DataType Log2Grad_Op::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: Log2Grad_Op";
  


  return expected_kernel_dtype;
}

OpInfoTuple LogDoubleGradOp::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("x", "paddle::dialect::DenseTensorType", false, false, false, false), paddle::dialect::OpInputInfo("grad_out", "paddle::dialect::DenseTensorType", false, false, false, false), paddle::dialect::OpInputInfo("grad_x_grad", "paddle::dialect::DenseTensorType", false, false, false, false) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = {  };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("x_grad", "paddle::dialect::DenseTensorType", false, false), paddle::dialect::OpOutputInfo("grad_out_grad", "paddle::dialect::DenseTensorType", false, false) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("GeneralBinaryGradInferMeta", {"x", "x"}, "log_double_grad", {"x", "grad_out", "grad_x_grad"}, {}, {}, {}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "log_double_grad");
}

void LogDoubleGradOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, pir::Value grad_out_, pir::Value grad_x_grad_) {
  VLOG(4) << "Start build LogDoubleGradOp";



  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_, grad_out_, grad_x_grad_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;
  paddle::dialect::DenseTensorType grad_out = grad_out_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)grad_out;
  paddle::dialect::DenseTensorType grad_x_grad = grad_x_grad_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)grad_x_grad;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);
  paddle::dialect::IrTensor dense_x_grad;
  paddle::dialect::IrMetaTensor meta_x_grad(&dense_x_grad);
  paddle::dialect::IrTensor dense_grad_out_grad;
  paddle::dialect::IrMetaTensor meta_grad_out_grad(&dense_grad_out_grad);

  phi::GeneralBinaryGradInferMeta(meta_x, meta_x, &meta_x_grad, &meta_grad_out_grad);

  std::vector<pir::Type> argument_outputs;
  pir::Type x_grad_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_x_grad.dtype()), dense_x_grad.dims(), dense_x_grad.layout(), dense_x_grad.lod(), dense_x_grad.offset());
  argument_outputs.push_back(x_grad_dense_tensor_type);

  pir::Type grad_out_grad_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_grad_out_grad.dtype()), dense_grad_out_grad.dims(), dense_grad_out_grad.layout(), dense_grad_out_grad.lod(), dense_grad_out_grad.offset());
  argument_outputs.push_back(grad_out_grad_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void LogDoubleGradOp::VerifySig() {}

void LogDoubleGradOp::InferMeta( phi::InferMetaContext *infer_meta ) {
  auto fn = PD_INFER_META(phi::GeneralBinaryGradInferMeta);
  fn(infer_meta);
}

phi::DataType LogDoubleGradOp::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: LogDoubleGradOp";
  


  return expected_kernel_dtype;
}

OpInfoTuple LogDoubleGrad_Op::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("x", "paddle::dialect::DenseTensorType", false, false, false, false), paddle::dialect::OpInputInfo("grad_out", "paddle::dialect::DenseTensorType", false, false, false, false), paddle::dialect::OpInputInfo("grad_x_grad", "paddle::dialect::DenseTensorType", false, false, false, false) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = {  };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("x_grad", "paddle::dialect::DenseTensorType", false, false), paddle::dialect::OpOutputInfo("grad_out_grad", "paddle::dialect::DenseTensorType", false, false) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("GeneralBinaryGradInferMeta", {"x", "x"}, "log_double_grad", {"x", "grad_out", "grad_x_grad"}, {}, {}, {{"grad_out_grad", "grad_x_grad"}}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "log_double_grad");
}

void LogDoubleGrad_Op::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, pir::Value grad_out_, pir::Value grad_x_grad_) {
  VLOG(4) << "Start build LogDoubleGrad_Op";



  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_, grad_out_, grad_x_grad_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;
  paddle::dialect::DenseTensorType grad_out = grad_out_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)grad_out;
  paddle::dialect::DenseTensorType grad_x_grad = grad_x_grad_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)grad_x_grad;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);
  paddle::dialect::IrTensor dense_x_grad;
  paddle::dialect::IrMetaTensor meta_x_grad(&dense_x_grad);
  paddle::dialect::IrTensor dense_grad_out_grad;
  paddle::dialect::IrMetaTensor meta_grad_out_grad(&dense_grad_out_grad);

  phi::GeneralBinaryGradInferMeta(meta_x, meta_x, &meta_x_grad, &meta_grad_out_grad);

  std::vector<pir::Type> argument_outputs;
  pir::Type x_grad_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_x_grad.dtype()), dense_x_grad.dims(), dense_x_grad.layout(), dense_x_grad.lod(), dense_x_grad.offset());
  argument_outputs.push_back(x_grad_dense_tensor_type);

  pir::Type grad_out_grad_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_grad_out_grad.dtype()), dense_grad_out_grad.dims(), dense_grad_out_grad.layout(), dense_grad_out_grad.lod(), dense_grad_out_grad.offset());
  argument_outputs.push_back(grad_out_grad_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void LogDoubleGrad_Op::VerifySig() {}

void LogDoubleGrad_Op::InferMeta( phi::InferMetaContext *infer_meta ) {
  auto fn = PD_INFER_META(phi::GeneralBinaryGradInferMeta);
  fn(infer_meta);
}

phi::DataType LogDoubleGrad_Op::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: LogDoubleGrad_Op";
  


  return expected_kernel_dtype;
}

OpInfoTuple LogGradOp::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("x", "paddle::dialect::DenseTensorType", false, false, false, true), paddle::dialect::OpInputInfo("out_grad", "paddle::dialect::DenseTensorType", false, false, false, true) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = {  };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("x_grad", "paddle::dialect::DenseTensorType", false, false) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("UnchangedInferMeta", {"x"}, "log_grad", {"x", "out_grad"}, {}, {}, {}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "log_grad");
}

void LogGradOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, pir::Value out_grad_) {
  VLOG(4) << "Start build LogGradOp";



  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_, out_grad_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;
  paddle::dialect::DenseTensorType out_grad = out_grad_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)out_grad;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);
  paddle::dialect::IrTensor dense_x_grad;
  paddle::dialect::IrMetaTensor meta_x_grad(&dense_x_grad);

  phi::UnchangedInferMeta(meta_x, &meta_x_grad);

  std::vector<pir::Type> argument_outputs;
  pir::Type x_grad_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_x_grad.dtype()), dense_x_grad.dims(), dense_x_grad.layout(), dense_x_grad.lod(), dense_x_grad.offset());
  argument_outputs.push_back(x_grad_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void LogGradOp::VerifySig() {}

void LogGradOp::InferMeta( phi::InferMetaContext *infer_meta ) {
  auto fn = PD_INFER_META(phi::UnchangedInferMeta);
  fn(infer_meta);
}

phi::DataType LogGradOp::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: LogGradOp";
  


  return expected_kernel_dtype;
}

OpInfoTuple LogGrad_Op::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("x", "paddle::dialect::DenseTensorType", false, false, false, true), paddle::dialect::OpInputInfo("out_grad", "paddle::dialect::DenseTensorType", false, false, false, true) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = {  };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("x_grad", "paddle::dialect::DenseTensorType", false, false) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("UnchangedInferMeta", {"x"}, "log_grad", {"x", "out_grad"}, {}, {}, {{"x_grad", "out_grad"}}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "log_grad");
}

void LogGrad_Op::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, pir::Value out_grad_) {
  VLOG(4) << "Start build LogGrad_Op";



  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_, out_grad_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;
  paddle::dialect::DenseTensorType out_grad = out_grad_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)out_grad;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);
  paddle::dialect::IrTensor dense_x_grad;
  paddle::dialect::IrMetaTensor meta_x_grad(&dense_x_grad);

  phi::UnchangedInferMeta(meta_x, &meta_x_grad);

  std::vector<pir::Type> argument_outputs;
  pir::Type x_grad_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_x_grad.dtype()), dense_x_grad.dims(), dense_x_grad.layout(), dense_x_grad.lod(), dense_x_grad.offset());
  argument_outputs.push_back(x_grad_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void LogGrad_Op::VerifySig() {}

void LogGrad_Op::InferMeta( phi::InferMetaContext *infer_meta ) {
  auto fn = PD_INFER_META(phi::UnchangedInferMeta);
  fn(infer_meta);
}

phi::DataType LogGrad_Op::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: LogGrad_Op";
  


  return expected_kernel_dtype;
}

const char *LogLossGradOp::attributes_name[1] = { "epsilon" };

OpInfoTuple LogLossGradOp::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("input", "paddle::dialect::DenseTensorType", false, false, false, false), paddle::dialect::OpInputInfo("label", "paddle::dialect::DenseTensorType", false, false, false, false), paddle::dialect::OpInputInfo("out_grad", "paddle::dialect::DenseTensorType", false, false, false, false) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = { paddle::dialect::OpAttributeInfo("epsilon", "pir::FloatAttribute", "") };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("input_grad", "paddle::dialect::DenseTensorType", false, false) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("UnchangedInferMeta", {"input"}, "log_loss_grad", {"input", "label", "out_grad", "epsilon"}, {}, {}, {}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "log_loss_grad");
}

void LogLossGradOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value input_, pir::Value label_, pir::Value out_grad_, float epsilon) {
  VLOG(4) << "Start build LogLossGradOp";



  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {input_, label_, out_grad_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_epsilon = pir::FloatAttribute::get(pir::IrContext::Instance(), epsilon);
  argument.AddAttribute("epsilon", attr_epsilon);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType input = input_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)input;
  paddle::dialect::DenseTensorType label = label_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)label;
  paddle::dialect::DenseTensorType out_grad = out_grad_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)out_grad;

  VLOG(4) << "Builder construction  dense_input";
  paddle::dialect::IrTensor ir_tensor_input(paddle::dialect::TransToPhiDataType(input.dtype()),
                                                      input.dims(),
                                                      input.data_layout(),
                                                      input.lod(),
                                                      input.offset());
  VLOG(4) << "Builder construction  meta_input";
  paddle::dialect::IrMetaTensor meta_input(&ir_tensor_input);
  paddle::dialect::IrTensor dense_input_grad;
  paddle::dialect::IrMetaTensor meta_input_grad(&dense_input_grad);

  phi::UnchangedInferMeta(meta_input, &meta_input_grad);

  std::vector<pir::Type> argument_outputs;
  pir::Type input_grad_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_input_grad.dtype()), dense_input_grad.dims(), dense_input_grad.layout(), dense_input_grad.lod(), dense_input_grad.offset());
  argument_outputs.push_back(input_grad_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void LogLossGradOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value input_, pir::Value label_, pir::Value out_grad_, pir::AttributeMap attributes) {
  VLOG(4) << "Start build LogLossGradOp";


  IR_ENFORCE(
      attributes.find("epsilon") != attributes.end(),
          "'epsilon' Attribute is expected for LogLossGradOp. ");
  float epsilon = attributes.at("epsilon").dyn_cast<pir::FloatAttribute>().data();


  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {input_, label_, out_grad_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_epsilon = pir::FloatAttribute::get(pir::IrContext::Instance(), epsilon);
  argument.AddAttribute("epsilon", attr_epsilon);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType input = input_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)input;
  paddle::dialect::DenseTensorType label = label_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)label;
  paddle::dialect::DenseTensorType out_grad = out_grad_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)out_grad;

  VLOG(4) << "Builder construction  dense_input";
  paddle::dialect::IrTensor ir_tensor_input(paddle::dialect::TransToPhiDataType(input.dtype()),
                                                      input.dims(),
                                                      input.data_layout(),
                                                      input.lod(),
                                                      input.offset());
  VLOG(4) << "Builder construction  meta_input";
  paddle::dialect::IrMetaTensor meta_input(&ir_tensor_input);
  paddle::dialect::IrTensor dense_input_grad;
  paddle::dialect::IrMetaTensor meta_input_grad(&dense_input_grad);

  phi::UnchangedInferMeta(meta_input, &meta_input_grad);

  std::vector<pir::Type> argument_outputs;
  pir::Type input_grad_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_input_grad.dtype()), dense_input_grad.dims(), dense_input_grad.layout(), dense_input_grad.lod(), dense_input_grad.offset());
  argument_outputs.push_back(input_grad_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void LogLossGradOp::VerifySig() {}

void LogLossGradOp::InferMeta( phi::InferMetaContext *infer_meta ) {
  auto fn = PD_INFER_META(phi::UnchangedInferMeta);
  fn(infer_meta);
}

phi::DataType LogLossGradOp::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: LogLossGradOp";
  


  return expected_kernel_dtype;
}

const char *LogSoftmaxGradOp::attributes_name[1] = { "axis" };

OpInfoTuple LogSoftmaxGradOp::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("out", "paddle::dialect::DenseTensorType", false, false, false, false), paddle::dialect::OpInputInfo("out_grad", "paddle::dialect::DenseTensorType", false, false, false, false) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = { paddle::dialect::OpAttributeInfo("axis", "pir::Int32Attribute", "") };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("x_grad", "paddle::dialect::DenseTensorType", false, false) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("UnchangedInferMeta", {"out"}, "log_softmax_grad", {"out", "out_grad", "axis"}, {"out_grad"}, {}, {}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "log_softmax_grad");
}

void LogSoftmaxGradOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value out_, pir::Value out_grad_, int axis) {
  VLOG(4) << "Start build LogSoftmaxGradOp";



  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {out_, out_grad_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_axis = pir::Int32Attribute::get(pir::IrContext::Instance(), axis);
  argument.AddAttribute("axis", attr_axis);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType out = out_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)out;
  paddle::dialect::DenseTensorType out_grad = out_grad_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)out_grad;

  VLOG(4) << "Builder construction  dense_out";
  paddle::dialect::IrTensor ir_tensor_out(paddle::dialect::TransToPhiDataType(out.dtype()),
                                                      out.dims(),
                                                      out.data_layout(),
                                                      out.lod(),
                                                      out.offset());
  VLOG(4) << "Builder construction  meta_out";
  paddle::dialect::IrMetaTensor meta_out(&ir_tensor_out);
  paddle::dialect::IrTensor dense_x_grad;
  paddle::dialect::IrMetaTensor meta_x_grad(&dense_x_grad);

  phi::UnchangedInferMeta(meta_out, &meta_x_grad);

  std::vector<pir::Type> argument_outputs;
  pir::Type x_grad_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_x_grad.dtype()), dense_x_grad.dims(), dense_x_grad.layout(), dense_x_grad.lod(), dense_x_grad.offset());
  argument_outputs.push_back(x_grad_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void LogSoftmaxGradOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value out_, pir::Value out_grad_, pir::AttributeMap attributes) {
  VLOG(4) << "Start build LogSoftmaxGradOp";


  IR_ENFORCE(
      attributes.find("axis") != attributes.end(),
          "'axis' Attribute is expected for LogSoftmaxGradOp. ");
  int axis = attributes.at("axis").dyn_cast<pir::Int32Attribute>().data();


  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {out_, out_grad_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_axis = pir::Int32Attribute::get(pir::IrContext::Instance(), axis);
  argument.AddAttribute("axis", attr_axis);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType out = out_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)out;
  paddle::dialect::DenseTensorType out_grad = out_grad_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)out_grad;

  VLOG(4) << "Builder construction  dense_out";
  paddle::dialect::IrTensor ir_tensor_out(paddle::dialect::TransToPhiDataType(out.dtype()),
                                                      out.dims(),
                                                      out.data_layout(),
                                                      out.lod(),
                                                      out.offset());
  VLOG(4) << "Builder construction  meta_out";
  paddle::dialect::IrMetaTensor meta_out(&ir_tensor_out);
  paddle::dialect::IrTensor dense_x_grad;
  paddle::dialect::IrMetaTensor meta_x_grad(&dense_x_grad);

  phi::UnchangedInferMeta(meta_out, &meta_x_grad);

  std::vector<pir::Type> argument_outputs;
  pir::Type x_grad_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_x_grad.dtype()), dense_x_grad.dims(), dense_x_grad.layout(), dense_x_grad.lod(), dense_x_grad.offset());
  argument_outputs.push_back(x_grad_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void LogSoftmaxGradOp::VerifySig() {}

void LogSoftmaxGradOp::InferMeta( phi::InferMetaContext *infer_meta ) {
  auto fn = PD_INFER_META(phi::UnchangedInferMeta);
  fn(infer_meta);
}

phi::DataType LogSoftmaxGradOp::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: LogSoftmaxGradOp";
  


  return expected_kernel_dtype;
}

const char *LogcumsumexpGradOp::attributes_name[4] = { "axis", "flatten", "exclusive", "reverse" };

OpInfoTuple LogcumsumexpGradOp::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("x", "paddle::dialect::DenseTensorType", false, false, false, false), paddle::dialect::OpInputInfo("out", "paddle::dialect::DenseTensorType", false, false, false, false), paddle::dialect::OpInputInfo("out_grad", "paddle::dialect::DenseTensorType", false, false, false, false) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = { paddle::dialect::OpAttributeInfo("axis", "pir::Int32Attribute", ""), paddle::dialect::OpAttributeInfo("flatten", "pir::BoolAttribute", ""), paddle::dialect::OpAttributeInfo("exclusive", "pir::BoolAttribute", ""), paddle::dialect::OpAttributeInfo("reverse", "pir::BoolAttribute", "") };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("x_grad", "paddle::dialect::DenseTensorType", false, false) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("UnchangedInferMeta", {"x"}, "logcumsumexp_grad", {"x", "out", "out_grad", "axis", "flatten", "exclusive", "reverse"}, {}, {}, {}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "logcumsumexp_grad");
}

void LogcumsumexpGradOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, pir::Value out_, pir::Value out_grad_, int axis, bool flatten, bool exclusive, bool reverse) {
  VLOG(4) << "Start build LogcumsumexpGradOp";



  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_, out_, out_grad_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_axis = pir::Int32Attribute::get(pir::IrContext::Instance(), axis);
  argument.AddAttribute("axis", attr_axis);
  pir::Attribute attr_flatten = pir::BoolAttribute::get(pir::IrContext::Instance(), flatten);
  argument.AddAttribute("flatten", attr_flatten);
  pir::Attribute attr_exclusive = pir::BoolAttribute::get(pir::IrContext::Instance(), exclusive);
  argument.AddAttribute("exclusive", attr_exclusive);
  pir::Attribute attr_reverse = pir::BoolAttribute::get(pir::IrContext::Instance(), reverse);
  argument.AddAttribute("reverse", attr_reverse);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;
  paddle::dialect::DenseTensorType out = out_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)out;
  paddle::dialect::DenseTensorType out_grad = out_grad_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)out_grad;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);
  paddle::dialect::IrTensor dense_x_grad;
  paddle::dialect::IrMetaTensor meta_x_grad(&dense_x_grad);

  phi::UnchangedInferMeta(meta_x, &meta_x_grad);

  std::vector<pir::Type> argument_outputs;
  pir::Type x_grad_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_x_grad.dtype()), dense_x_grad.dims(), dense_x_grad.layout(), dense_x_grad.lod(), dense_x_grad.offset());
  argument_outputs.push_back(x_grad_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void LogcumsumexpGradOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, pir::Value out_, pir::Value out_grad_, pir::AttributeMap attributes) {
  VLOG(4) << "Start build LogcumsumexpGradOp";


  IR_ENFORCE(
      attributes.find("axis") != attributes.end(),
          "'axis' Attribute is expected for LogcumsumexpGradOp. ");
  int axis = attributes.at("axis").dyn_cast<pir::Int32Attribute>().data();

  IR_ENFORCE(
      attributes.find("flatten") != attributes.end(),
          "'flatten' Attribute is expected for LogcumsumexpGradOp. ");
  bool flatten = attributes.at("flatten").dyn_cast<pir::BoolAttribute>().data();

  IR_ENFORCE(
      attributes.find("exclusive") != attributes.end(),
          "'exclusive' Attribute is expected for LogcumsumexpGradOp. ");
  bool exclusive = attributes.at("exclusive").dyn_cast<pir::BoolAttribute>().data();

  IR_ENFORCE(
      attributes.find("reverse") != attributes.end(),
          "'reverse' Attribute is expected for LogcumsumexpGradOp. ");
  bool reverse = attributes.at("reverse").dyn_cast<pir::BoolAttribute>().data();


  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_, out_, out_grad_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_axis = pir::Int32Attribute::get(pir::IrContext::Instance(), axis);
  argument.AddAttribute("axis", attr_axis);
  pir::Attribute attr_flatten = pir::BoolAttribute::get(pir::IrContext::Instance(), flatten);
  argument.AddAttribute("flatten", attr_flatten);
  pir::Attribute attr_exclusive = pir::BoolAttribute::get(pir::IrContext::Instance(), exclusive);
  argument.AddAttribute("exclusive", attr_exclusive);
  pir::Attribute attr_reverse = pir::BoolAttribute::get(pir::IrContext::Instance(), reverse);
  argument.AddAttribute("reverse", attr_reverse);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;
  paddle::dialect::DenseTensorType out = out_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)out;
  paddle::dialect::DenseTensorType out_grad = out_grad_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)out_grad;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);
  paddle::dialect::IrTensor dense_x_grad;
  paddle::dialect::IrMetaTensor meta_x_grad(&dense_x_grad);

  phi::UnchangedInferMeta(meta_x, &meta_x_grad);

  std::vector<pir::Type> argument_outputs;
  pir::Type x_grad_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_x_grad.dtype()), dense_x_grad.dims(), dense_x_grad.layout(), dense_x_grad.lod(), dense_x_grad.offset());
  argument_outputs.push_back(x_grad_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void LogcumsumexpGradOp::VerifySig() {}

void LogcumsumexpGradOp::InferMeta( phi::InferMetaContext *infer_meta ) {
  auto fn = PD_INFER_META(phi::UnchangedInferMeta);
  fn(infer_meta);
}

phi::DataType LogcumsumexpGradOp::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: LogcumsumexpGradOp";
  


  return expected_kernel_dtype;
}

const char *LogitGradOp::attributes_name[1] = { "eps" };

OpInfoTuple LogitGradOp::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("x", "paddle::dialect::DenseTensorType", false, false, false, false), paddle::dialect::OpInputInfo("out_grad", "paddle::dialect::DenseTensorType", false, false, false, false) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = { paddle::dialect::OpAttributeInfo("eps", "pir::FloatAttribute", "") };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("x_grad", "paddle::dialect::DenseTensorType", false, false) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("UnchangedInferMeta", {"x"}, "logit_grad", {"x", "out_grad", "eps"}, {}, {}, {}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "logit_grad");
}

void LogitGradOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, pir::Value out_grad_, float eps) {
  VLOG(4) << "Start build LogitGradOp";



  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_, out_grad_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_eps = pir::FloatAttribute::get(pir::IrContext::Instance(), eps);
  argument.AddAttribute("eps", attr_eps);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;
  paddle::dialect::DenseTensorType out_grad = out_grad_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)out_grad;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);
  paddle::dialect::IrTensor dense_x_grad;
  paddle::dialect::IrMetaTensor meta_x_grad(&dense_x_grad);

  phi::UnchangedInferMeta(meta_x, &meta_x_grad);

  std::vector<pir::Type> argument_outputs;
  pir::Type x_grad_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_x_grad.dtype()), dense_x_grad.dims(), dense_x_grad.layout(), dense_x_grad.lod(), dense_x_grad.offset());
  argument_outputs.push_back(x_grad_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void LogitGradOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, pir::Value out_grad_, pir::AttributeMap attributes) {
  VLOG(4) << "Start build LogitGradOp";


  IR_ENFORCE(
      attributes.find("eps") != attributes.end(),
          "'eps' Attribute is expected for LogitGradOp. ");
  float eps = attributes.at("eps").dyn_cast<pir::FloatAttribute>().data();


  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_, out_grad_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_eps = pir::FloatAttribute::get(pir::IrContext::Instance(), eps);
  argument.AddAttribute("eps", attr_eps);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;
  paddle::dialect::DenseTensorType out_grad = out_grad_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)out_grad;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);
  paddle::dialect::IrTensor dense_x_grad;
  paddle::dialect::IrMetaTensor meta_x_grad(&dense_x_grad);

  phi::UnchangedInferMeta(meta_x, &meta_x_grad);

  std::vector<pir::Type> argument_outputs;
  pir::Type x_grad_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_x_grad.dtype()), dense_x_grad.dims(), dense_x_grad.layout(), dense_x_grad.lod(), dense_x_grad.offset());
  argument_outputs.push_back(x_grad_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void LogitGradOp::VerifySig() {}

void LogitGradOp::InferMeta( phi::InferMetaContext *infer_meta ) {
  auto fn = PD_INFER_META(phi::UnchangedInferMeta);
  fn(infer_meta);
}

phi::DataType LogitGradOp::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: LogitGradOp";
  


  return expected_kernel_dtype;
}

OpInfoTuple LogsigmoidGradOp::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("x", "paddle::dialect::DenseTensorType", false, false, false, false), paddle::dialect::OpInputInfo("out_grad", "paddle::dialect::DenseTensorType", false, false, false, false) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = {  };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("x_grad", "paddle::dialect::DenseTensorType", false, false) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("UnchangedInferMeta", {"x"}, "logsigmoid_grad", {"x", "out_grad"}, {}, {}, {}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "logsigmoid_grad");
}

void LogsigmoidGradOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, pir::Value out_grad_) {
  VLOG(4) << "Start build LogsigmoidGradOp";



  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_, out_grad_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;
  paddle::dialect::DenseTensorType out_grad = out_grad_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)out_grad;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);
  paddle::dialect::IrTensor dense_x_grad;
  paddle::dialect::IrMetaTensor meta_x_grad(&dense_x_grad);

  phi::UnchangedInferMeta(meta_x, &meta_x_grad);

  std::vector<pir::Type> argument_outputs;
  pir::Type x_grad_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_x_grad.dtype()), dense_x_grad.dims(), dense_x_grad.layout(), dense_x_grad.lod(), dense_x_grad.offset());
  argument_outputs.push_back(x_grad_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void LogsigmoidGradOp::VerifySig() {}

void LogsigmoidGradOp::InferMeta( phi::InferMetaContext *infer_meta ) {
  auto fn = PD_INFER_META(phi::UnchangedInferMeta);
  fn(infer_meta);
}

phi::DataType LogsigmoidGradOp::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: LogsigmoidGradOp";
  


  return expected_kernel_dtype;
}

OpInfoTuple LogsigmoidGrad_Op::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("x", "paddle::dialect::DenseTensorType", false, false, false, false), paddle::dialect::OpInputInfo("out_grad", "paddle::dialect::DenseTensorType", false, false, false, false) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = {  };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("x_grad", "paddle::dialect::DenseTensorType", false, false) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("UnchangedInferMeta", {"x"}, "logsigmoid_grad", {"x", "out_grad"}, {}, {}, {{"x_grad", "out_grad"}}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "logsigmoid_grad");
}

void LogsigmoidGrad_Op::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, pir::Value out_grad_) {
  VLOG(4) << "Start build LogsigmoidGrad_Op";



  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_, out_grad_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;
  paddle::dialect::DenseTensorType out_grad = out_grad_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)out_grad;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);
  paddle::dialect::IrTensor dense_x_grad;
  paddle::dialect::IrMetaTensor meta_x_grad(&dense_x_grad);

  phi::UnchangedInferMeta(meta_x, &meta_x_grad);

  std::vector<pir::Type> argument_outputs;
  pir::Type x_grad_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_x_grad.dtype()), dense_x_grad.dims(), dense_x_grad.layout(), dense_x_grad.lod(), dense_x_grad.offset());
  argument_outputs.push_back(x_grad_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void LogsigmoidGrad_Op::VerifySig() {}

void LogsigmoidGrad_Op::InferMeta( phi::InferMetaContext *infer_meta ) {
  auto fn = PD_INFER_META(phi::UnchangedInferMeta);
  fn(infer_meta);
}

phi::DataType LogsigmoidGrad_Op::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: LogsigmoidGrad_Op";
  


  return expected_kernel_dtype;
}

const char *LuGradOp::attributes_name[1] = { "pivot" };

OpInfoTuple LuGradOp::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("x", "paddle::dialect::DenseTensorType", false, false, false, false), paddle::dialect::OpInputInfo("out", "paddle::dialect::DenseTensorType", false, false, false, false), paddle::dialect::OpInputInfo("pivots", "paddle::dialect::DenseTensorType", false, false, false, false), paddle::dialect::OpInputInfo("out_grad", "paddle::dialect::DenseTensorType", false, false, false, false) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = { paddle::dialect::OpAttributeInfo("pivot", "pir::BoolAttribute", "") };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("x_grad", "paddle::dialect::DenseTensorType", false, false) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("LUGradInferMeta", {"x", "out", "pivots", "out_grad", "pivot"}, "lu_grad", {"x", "out", "pivots", "out_grad", "pivot"}, {}, {}, {}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "lu_grad");
}

void LuGradOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, pir::Value out_, pir::Value pivots_, pir::Value out_grad_, bool pivot) {
  VLOG(4) << "Start build LuGradOp";



  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_, out_, pivots_, out_grad_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_pivot = pir::BoolAttribute::get(pir::IrContext::Instance(), pivot);
  argument.AddAttribute("pivot", attr_pivot);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;
  paddle::dialect::DenseTensorType out = out_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)out;
  paddle::dialect::DenseTensorType pivots = pivots_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)pivots;
  paddle::dialect::DenseTensorType out_grad = out_grad_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)out_grad;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);

  VLOG(4) << "Builder construction  dense_out";
  paddle::dialect::IrTensor ir_tensor_out(paddle::dialect::TransToPhiDataType(out.dtype()),
                                                      out.dims(),
                                                      out.data_layout(),
                                                      out.lod(),
                                                      out.offset());
  VLOG(4) << "Builder construction  meta_out";
  paddle::dialect::IrMetaTensor meta_out(&ir_tensor_out);

  VLOG(4) << "Builder construction  dense_pivots";
  paddle::dialect::IrTensor ir_tensor_pivots(paddle::dialect::TransToPhiDataType(pivots.dtype()),
                                                      pivots.dims(),
                                                      pivots.data_layout(),
                                                      pivots.lod(),
                                                      pivots.offset());
  VLOG(4) << "Builder construction  meta_pivots";
  paddle::dialect::IrMetaTensor meta_pivots(&ir_tensor_pivots);

  VLOG(4) << "Builder construction  dense_out_grad";
  paddle::dialect::IrTensor ir_tensor_out_grad(paddle::dialect::TransToPhiDataType(out_grad.dtype()),
                                                      out_grad.dims(),
                                                      out_grad.data_layout(),
                                                      out_grad.lod(),
                                                      out_grad.offset());
  VLOG(4) << "Builder construction  meta_out_grad";
  paddle::dialect::IrMetaTensor meta_out_grad(&ir_tensor_out_grad);
  paddle::dialect::IrTensor dense_x_grad;
  paddle::dialect::IrMetaTensor meta_x_grad(&dense_x_grad);

  phi::LUGradInferMeta(meta_x, meta_out, meta_pivots, meta_out_grad, pivot, &meta_x_grad);

  std::vector<pir::Type> argument_outputs;
  pir::Type x_grad_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_x_grad.dtype()), dense_x_grad.dims(), dense_x_grad.layout(), dense_x_grad.lod(), dense_x_grad.offset());
  argument_outputs.push_back(x_grad_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void LuGradOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, pir::Value out_, pir::Value pivots_, pir::Value out_grad_, pir::AttributeMap attributes) {
  VLOG(4) << "Start build LuGradOp";


  IR_ENFORCE(
      attributes.find("pivot") != attributes.end(),
          "'pivot' Attribute is expected for LuGradOp. ");
  bool pivot = attributes.at("pivot").dyn_cast<pir::BoolAttribute>().data();


  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_, out_, pivots_, out_grad_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_pivot = pir::BoolAttribute::get(pir::IrContext::Instance(), pivot);
  argument.AddAttribute("pivot", attr_pivot);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;
  paddle::dialect::DenseTensorType out = out_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)out;
  paddle::dialect::DenseTensorType pivots = pivots_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)pivots;
  paddle::dialect::DenseTensorType out_grad = out_grad_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)out_grad;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);

  VLOG(4) << "Builder construction  dense_out";
  paddle::dialect::IrTensor ir_tensor_out(paddle::dialect::TransToPhiDataType(out.dtype()),
                                                      out.dims(),
                                                      out.data_layout(),
                                                      out.lod(),
                                                      out.offset());
  VLOG(4) << "Builder construction  meta_out";
  paddle::dialect::IrMetaTensor meta_out(&ir_tensor_out);

  VLOG(4) << "Builder construction  dense_pivots";
  paddle::dialect::IrTensor ir_tensor_pivots(paddle::dialect::TransToPhiDataType(pivots.dtype()),
                                                      pivots.dims(),
                                                      pivots.data_layout(),
                                                      pivots.lod(),
                                                      pivots.offset());
  VLOG(4) << "Builder construction  meta_pivots";
  paddle::dialect::IrMetaTensor meta_pivots(&ir_tensor_pivots);

  VLOG(4) << "Builder construction  dense_out_grad";
  paddle::dialect::IrTensor ir_tensor_out_grad(paddle::dialect::TransToPhiDataType(out_grad.dtype()),
                                                      out_grad.dims(),
                                                      out_grad.data_layout(),
                                                      out_grad.lod(),
                                                      out_grad.offset());
  VLOG(4) << "Builder construction  meta_out_grad";
  paddle::dialect::IrMetaTensor meta_out_grad(&ir_tensor_out_grad);
  paddle::dialect::IrTensor dense_x_grad;
  paddle::dialect::IrMetaTensor meta_x_grad(&dense_x_grad);

  phi::LUGradInferMeta(meta_x, meta_out, meta_pivots, meta_out_grad, pivot, &meta_x_grad);

  std::vector<pir::Type> argument_outputs;
  pir::Type x_grad_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_x_grad.dtype()), dense_x_grad.dims(), dense_x_grad.layout(), dense_x_grad.lod(), dense_x_grad.offset());
  argument_outputs.push_back(x_grad_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void LuGradOp::VerifySig() {}

void LuGradOp::InferMeta( phi::InferMetaContext *infer_meta ) {
  auto fn = PD_INFER_META(phi::LUGradInferMeta);
  fn(infer_meta);
}

phi::DataType LuGradOp::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: LuGradOp";
  


  return expected_kernel_dtype;
}

const char *LuGrad_Op::attributes_name[1] = { "pivot" };

OpInfoTuple LuGrad_Op::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("x", "paddle::dialect::DenseTensorType", false, false, false, false), paddle::dialect::OpInputInfo("out", "paddle::dialect::DenseTensorType", false, false, false, false), paddle::dialect::OpInputInfo("pivots", "paddle::dialect::DenseTensorType", false, false, false, false), paddle::dialect::OpInputInfo("out_grad", "paddle::dialect::DenseTensorType", false, false, false, false) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = { paddle::dialect::OpAttributeInfo("pivot", "pir::BoolAttribute", "") };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("x_grad", "paddle::dialect::DenseTensorType", false, false) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("LUGradInferMeta", {"x", "out", "pivots", "out_grad", "pivot"}, "lu_grad", {"x", "out", "pivots", "out_grad", "pivot"}, {}, {}, {{"x_grad", "out_grad"}}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "lu_grad");
}

void LuGrad_Op::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, pir::Value out_, pir::Value pivots_, pir::Value out_grad_, bool pivot) {
  VLOG(4) << "Start build LuGrad_Op";



  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_, out_, pivots_, out_grad_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_pivot = pir::BoolAttribute::get(pir::IrContext::Instance(), pivot);
  argument.AddAttribute("pivot", attr_pivot);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;
  paddle::dialect::DenseTensorType out = out_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)out;
  paddle::dialect::DenseTensorType pivots = pivots_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)pivots;
  paddle::dialect::DenseTensorType out_grad = out_grad_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)out_grad;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);

  VLOG(4) << "Builder construction  dense_out";
  paddle::dialect::IrTensor ir_tensor_out(paddle::dialect::TransToPhiDataType(out.dtype()),
                                                      out.dims(),
                                                      out.data_layout(),
                                                      out.lod(),
                                                      out.offset());
  VLOG(4) << "Builder construction  meta_out";
  paddle::dialect::IrMetaTensor meta_out(&ir_tensor_out);

  VLOG(4) << "Builder construction  dense_pivots";
  paddle::dialect::IrTensor ir_tensor_pivots(paddle::dialect::TransToPhiDataType(pivots.dtype()),
                                                      pivots.dims(),
                                                      pivots.data_layout(),
                                                      pivots.lod(),
                                                      pivots.offset());
  VLOG(4) << "Builder construction  meta_pivots";
  paddle::dialect::IrMetaTensor meta_pivots(&ir_tensor_pivots);

  VLOG(4) << "Builder construction  dense_out_grad";
  paddle::dialect::IrTensor ir_tensor_out_grad(paddle::dialect::TransToPhiDataType(out_grad.dtype()),
                                                      out_grad.dims(),
                                                      out_grad.data_layout(),
                                                      out_grad.lod(),
                                                      out_grad.offset());
  VLOG(4) << "Builder construction  meta_out_grad";
  paddle::dialect::IrMetaTensor meta_out_grad(&ir_tensor_out_grad);
  paddle::dialect::IrTensor dense_x_grad;
  paddle::dialect::IrMetaTensor meta_x_grad(&dense_x_grad);

  phi::LUGradInferMeta(meta_x, meta_out, meta_pivots, meta_out_grad, pivot, &meta_x_grad);

  std::vector<pir::Type> argument_outputs;
  pir::Type x_grad_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_x_grad.dtype()), dense_x_grad.dims(), dense_x_grad.layout(), dense_x_grad.lod(), dense_x_grad.offset());
  argument_outputs.push_back(x_grad_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void LuGrad_Op::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, pir::Value out_, pir::Value pivots_, pir::Value out_grad_, pir::AttributeMap attributes) {
  VLOG(4) << "Start build LuGrad_Op";


  IR_ENFORCE(
      attributes.find("pivot") != attributes.end(),
          "'pivot' Attribute is expected for LuGrad_Op. ");
  bool pivot = attributes.at("pivot").dyn_cast<pir::BoolAttribute>().data();


  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_, out_, pivots_, out_grad_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_pivot = pir::BoolAttribute::get(pir::IrContext::Instance(), pivot);
  argument.AddAttribute("pivot", attr_pivot);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;
  paddle::dialect::DenseTensorType out = out_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)out;
  paddle::dialect::DenseTensorType pivots = pivots_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)pivots;
  paddle::dialect::DenseTensorType out_grad = out_grad_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)out_grad;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);

  VLOG(4) << "Builder construction  dense_out";
  paddle::dialect::IrTensor ir_tensor_out(paddle::dialect::TransToPhiDataType(out.dtype()),
                                                      out.dims(),
                                                      out.data_layout(),
                                                      out.lod(),
                                                      out.offset());
  VLOG(4) << "Builder construction  meta_out";
  paddle::dialect::IrMetaTensor meta_out(&ir_tensor_out);

  VLOG(4) << "Builder construction  dense_pivots";
  paddle::dialect::IrTensor ir_tensor_pivots(paddle::dialect::TransToPhiDataType(pivots.dtype()),
                                                      pivots.dims(),
                                                      pivots.data_layout(),
                                                      pivots.lod(),
                                                      pivots.offset());
  VLOG(4) << "Builder construction  meta_pivots";
  paddle::dialect::IrMetaTensor meta_pivots(&ir_tensor_pivots);

  VLOG(4) << "Builder construction  dense_out_grad";
  paddle::dialect::IrTensor ir_tensor_out_grad(paddle::dialect::TransToPhiDataType(out_grad.dtype()),
                                                      out_grad.dims(),
                                                      out_grad.data_layout(),
                                                      out_grad.lod(),
                                                      out_grad.offset());
  VLOG(4) << "Builder construction  meta_out_grad";
  paddle::dialect::IrMetaTensor meta_out_grad(&ir_tensor_out_grad);
  paddle::dialect::IrTensor dense_x_grad;
  paddle::dialect::IrMetaTensor meta_x_grad(&dense_x_grad);

  phi::LUGradInferMeta(meta_x, meta_out, meta_pivots, meta_out_grad, pivot, &meta_x_grad);

  std::vector<pir::Type> argument_outputs;
  pir::Type x_grad_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_x_grad.dtype()), dense_x_grad.dims(), dense_x_grad.layout(), dense_x_grad.lod(), dense_x_grad.offset());
  argument_outputs.push_back(x_grad_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void LuGrad_Op::VerifySig() {}

void LuGrad_Op::InferMeta( phi::InferMetaContext *infer_meta ) {
  auto fn = PD_INFER_META(phi::LUGradInferMeta);
  fn(infer_meta);
}

phi::DataType LuGrad_Op::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: LuGrad_Op";
  


  return expected_kernel_dtype;
}

const char *LuUnpackGradOp::attributes_name[2] = { "unpack_ludata", "unpack_pivots" };

OpInfoTuple LuUnpackGradOp::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("x", "paddle::dialect::DenseTensorType", false, false, false, false), paddle::dialect::OpInputInfo("y", "paddle::dialect::DenseTensorType", false, false, false, false), paddle::dialect::OpInputInfo("l", "paddle::dialect::DenseTensorType", false, false, false, false), paddle::dialect::OpInputInfo("u", "paddle::dialect::DenseTensorType", false, false, false, false), paddle::dialect::OpInputInfo("pmat", "paddle::dialect::DenseTensorType", false, false, false, false), paddle::dialect::OpInputInfo("l_grad", "paddle::dialect::DenseTensorType", false, false, false, false), paddle::dialect::OpInputInfo("u_grad", "paddle::dialect::DenseTensorType", false, false, false, false) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = { paddle::dialect::OpAttributeInfo("unpack_ludata", "pir::BoolAttribute", ""), paddle::dialect::OpAttributeInfo("unpack_pivots", "pir::BoolAttribute", "") };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("x_grad", "paddle::dialect::DenseTensorType", false, false) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("LUUnpackGradInferMeta", {"x", "y", "l", "u", "pmat", "l_grad", "u_grad", "unpack_ludata", "unpack_pivots"}, "lu_unpack_grad", {"x", "y", "l", "u", "pmat", "l_grad", "u_grad", "unpack_ludata", "unpack_pivots"}, {}, {}, {}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "lu_unpack_grad");
}

void LuUnpackGradOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, pir::Value y_, pir::Value l_, pir::Value u_, pir::Value pmat_, pir::Value l_grad_, pir::Value u_grad_, bool unpack_ludata, bool unpack_pivots) {
  VLOG(4) << "Start build LuUnpackGradOp";



  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_, y_, l_, u_, pmat_, l_grad_, u_grad_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_unpack_ludata = pir::BoolAttribute::get(pir::IrContext::Instance(), unpack_ludata);
  argument.AddAttribute("unpack_ludata", attr_unpack_ludata);
  pir::Attribute attr_unpack_pivots = pir::BoolAttribute::get(pir::IrContext::Instance(), unpack_pivots);
  argument.AddAttribute("unpack_pivots", attr_unpack_pivots);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;
  paddle::dialect::DenseTensorType y = y_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)y;
  paddle::dialect::DenseTensorType l = l_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)l;
  paddle::dialect::DenseTensorType u = u_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)u;
  paddle::dialect::DenseTensorType pmat = pmat_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)pmat;
  paddle::dialect::DenseTensorType l_grad = l_grad_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)l_grad;
  paddle::dialect::DenseTensorType u_grad = u_grad_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)u_grad;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);

  VLOG(4) << "Builder construction  dense_y";
  paddle::dialect::IrTensor ir_tensor_y(paddle::dialect::TransToPhiDataType(y.dtype()),
                                                      y.dims(),
                                                      y.data_layout(),
                                                      y.lod(),
                                                      y.offset());
  VLOG(4) << "Builder construction  meta_y";
  paddle::dialect::IrMetaTensor meta_y(&ir_tensor_y);

  VLOG(4) << "Builder construction  dense_l";
  paddle::dialect::IrTensor ir_tensor_l(paddle::dialect::TransToPhiDataType(l.dtype()),
                                                      l.dims(),
                                                      l.data_layout(),
                                                      l.lod(),
                                                      l.offset());
  VLOG(4) << "Builder construction  meta_l";
  paddle::dialect::IrMetaTensor meta_l(&ir_tensor_l);

  VLOG(4) << "Builder construction  dense_u";
  paddle::dialect::IrTensor ir_tensor_u(paddle::dialect::TransToPhiDataType(u.dtype()),
                                                      u.dims(),
                                                      u.data_layout(),
                                                      u.lod(),
                                                      u.offset());
  VLOG(4) << "Builder construction  meta_u";
  paddle::dialect::IrMetaTensor meta_u(&ir_tensor_u);

  VLOG(4) << "Builder construction  dense_pmat";
  paddle::dialect::IrTensor ir_tensor_pmat(paddle::dialect::TransToPhiDataType(pmat.dtype()),
                                                      pmat.dims(),
                                                      pmat.data_layout(),
                                                      pmat.lod(),
                                                      pmat.offset());
  VLOG(4) << "Builder construction  meta_pmat";
  paddle::dialect::IrMetaTensor meta_pmat(&ir_tensor_pmat);

  VLOG(4) << "Builder construction  dense_l_grad";
  paddle::dialect::IrTensor ir_tensor_l_grad(paddle::dialect::TransToPhiDataType(l_grad.dtype()),
                                                      l_grad.dims(),
                                                      l_grad.data_layout(),
                                                      l_grad.lod(),
                                                      l_grad.offset());
  VLOG(4) << "Builder construction  meta_l_grad";
  paddle::dialect::IrMetaTensor meta_l_grad(&ir_tensor_l_grad);

  VLOG(4) << "Builder construction  dense_u_grad";
  paddle::dialect::IrTensor ir_tensor_u_grad(paddle::dialect::TransToPhiDataType(u_grad.dtype()),
                                                      u_grad.dims(),
                                                      u_grad.data_layout(),
                                                      u_grad.lod(),
                                                      u_grad.offset());
  VLOG(4) << "Builder construction  meta_u_grad";
  paddle::dialect::IrMetaTensor meta_u_grad(&ir_tensor_u_grad);
  paddle::dialect::IrTensor dense_x_grad;
  paddle::dialect::IrMetaTensor meta_x_grad(&dense_x_grad);

  phi::LUUnpackGradInferMeta(meta_x, meta_y, meta_l, meta_u, meta_pmat, meta_l_grad, meta_u_grad, unpack_ludata, unpack_pivots, &meta_x_grad);

  std::vector<pir::Type> argument_outputs;
  pir::Type x_grad_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_x_grad.dtype()), dense_x_grad.dims(), dense_x_grad.layout(), dense_x_grad.lod(), dense_x_grad.offset());
  argument_outputs.push_back(x_grad_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void LuUnpackGradOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, pir::Value y_, pir::Value l_, pir::Value u_, pir::Value pmat_, pir::Value l_grad_, pir::Value u_grad_, pir::AttributeMap attributes) {
  VLOG(4) << "Start build LuUnpackGradOp";


  IR_ENFORCE(
      attributes.find("unpack_ludata") != attributes.end(),
          "'unpack_ludata' Attribute is expected for LuUnpackGradOp. ");
  bool unpack_ludata = attributes.at("unpack_ludata").dyn_cast<pir::BoolAttribute>().data();

  IR_ENFORCE(
      attributes.find("unpack_pivots") != attributes.end(),
          "'unpack_pivots' Attribute is expected for LuUnpackGradOp. ");
  bool unpack_pivots = attributes.at("unpack_pivots").dyn_cast<pir::BoolAttribute>().data();


  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_, y_, l_, u_, pmat_, l_grad_, u_grad_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_unpack_ludata = pir::BoolAttribute::get(pir::IrContext::Instance(), unpack_ludata);
  argument.AddAttribute("unpack_ludata", attr_unpack_ludata);
  pir::Attribute attr_unpack_pivots = pir::BoolAttribute::get(pir::IrContext::Instance(), unpack_pivots);
  argument.AddAttribute("unpack_pivots", attr_unpack_pivots);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;
  paddle::dialect::DenseTensorType y = y_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)y;
  paddle::dialect::DenseTensorType l = l_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)l;
  paddle::dialect::DenseTensorType u = u_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)u;
  paddle::dialect::DenseTensorType pmat = pmat_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)pmat;
  paddle::dialect::DenseTensorType l_grad = l_grad_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)l_grad;
  paddle::dialect::DenseTensorType u_grad = u_grad_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)u_grad;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);

  VLOG(4) << "Builder construction  dense_y";
  paddle::dialect::IrTensor ir_tensor_y(paddle::dialect::TransToPhiDataType(y.dtype()),
                                                      y.dims(),
                                                      y.data_layout(),
                                                      y.lod(),
                                                      y.offset());
  VLOG(4) << "Builder construction  meta_y";
  paddle::dialect::IrMetaTensor meta_y(&ir_tensor_y);

  VLOG(4) << "Builder construction  dense_l";
  paddle::dialect::IrTensor ir_tensor_l(paddle::dialect::TransToPhiDataType(l.dtype()),
                                                      l.dims(),
                                                      l.data_layout(),
                                                      l.lod(),
                                                      l.offset());
  VLOG(4) << "Builder construction  meta_l";
  paddle::dialect::IrMetaTensor meta_l(&ir_tensor_l);

  VLOG(4) << "Builder construction  dense_u";
  paddle::dialect::IrTensor ir_tensor_u(paddle::dialect::TransToPhiDataType(u.dtype()),
                                                      u.dims(),
                                                      u.data_layout(),
                                                      u.lod(),
                                                      u.offset());
  VLOG(4) << "Builder construction  meta_u";
  paddle::dialect::IrMetaTensor meta_u(&ir_tensor_u);

  VLOG(4) << "Builder construction  dense_pmat";
  paddle::dialect::IrTensor ir_tensor_pmat(paddle::dialect::TransToPhiDataType(pmat.dtype()),
                                                      pmat.dims(),
                                                      pmat.data_layout(),
                                                      pmat.lod(),
                                                      pmat.offset());
  VLOG(4) << "Builder construction  meta_pmat";
  paddle::dialect::IrMetaTensor meta_pmat(&ir_tensor_pmat);

  VLOG(4) << "Builder construction  dense_l_grad";
  paddle::dialect::IrTensor ir_tensor_l_grad(paddle::dialect::TransToPhiDataType(l_grad.dtype()),
                                                      l_grad.dims(),
                                                      l_grad.data_layout(),
                                                      l_grad.lod(),
                                                      l_grad.offset());
  VLOG(4) << "Builder construction  meta_l_grad";
  paddle::dialect::IrMetaTensor meta_l_grad(&ir_tensor_l_grad);

  VLOG(4) << "Builder construction  dense_u_grad";
  paddle::dialect::IrTensor ir_tensor_u_grad(paddle::dialect::TransToPhiDataType(u_grad.dtype()),
                                                      u_grad.dims(),
                                                      u_grad.data_layout(),
                                                      u_grad.lod(),
                                                      u_grad.offset());
  VLOG(4) << "Builder construction  meta_u_grad";
  paddle::dialect::IrMetaTensor meta_u_grad(&ir_tensor_u_grad);
  paddle::dialect::IrTensor dense_x_grad;
  paddle::dialect::IrMetaTensor meta_x_grad(&dense_x_grad);

  phi::LUUnpackGradInferMeta(meta_x, meta_y, meta_l, meta_u, meta_pmat, meta_l_grad, meta_u_grad, unpack_ludata, unpack_pivots, &meta_x_grad);

  std::vector<pir::Type> argument_outputs;
  pir::Type x_grad_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_x_grad.dtype()), dense_x_grad.dims(), dense_x_grad.layout(), dense_x_grad.lod(), dense_x_grad.offset());
  argument_outputs.push_back(x_grad_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void LuUnpackGradOp::VerifySig() {}

void LuUnpackGradOp::InferMeta( phi::InferMetaContext *infer_meta ) {
  auto fn = PD_INFER_META(phi::LUUnpackGradInferMeta);
  fn(infer_meta);
}

phi::DataType LuUnpackGradOp::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: LuUnpackGradOp";
  


  return expected_kernel_dtype;
}

const char *MarginCrossEntropyGradOp::attributes_name[8] = { "return_softmax", "ring_id", "rank", "nranks", "margin1", "margin2", "margin3", "scale" };

OpInfoTuple MarginCrossEntropyGradOp::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("logits", "paddle::dialect::DenseTensorType", false, false, false, false), paddle::dialect::OpInputInfo("label", "paddle::dialect::DenseTensorType", false, false, false, false), paddle::dialect::OpInputInfo("softmax", "paddle::dialect::DenseTensorType", false, false, false, false), paddle::dialect::OpInputInfo("loss_grad", "paddle::dialect::DenseTensorType", false, false, false, false) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = { paddle::dialect::OpAttributeInfo("return_softmax", "pir::BoolAttribute", ""), paddle::dialect::OpAttributeInfo("ring_id", "pir::Int32Attribute", ""), paddle::dialect::OpAttributeInfo("rank", "pir::Int32Attribute", ""), paddle::dialect::OpAttributeInfo("nranks", "pir::Int32Attribute", ""), paddle::dialect::OpAttributeInfo("margin1", "pir::FloatAttribute", ""), paddle::dialect::OpAttributeInfo("margin2", "pir::FloatAttribute", ""), paddle::dialect::OpAttributeInfo("margin3", "pir::FloatAttribute", ""), paddle::dialect::OpAttributeInfo("scale", "pir::FloatAttribute", "") };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("logits_grad", "paddle::dialect::DenseTensorType", false, false) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("MarginCrossEntropyGradInferMeta", {"logits", "label", "softmax", "loss_grad", "return_softmax", "ring_id", "rank", "nranks", "margin1", "margin2", "margin3", "scale"}, "margin_cross_entropy_grad", {"logits", "label", "softmax", "loss_grad", "return_softmax", "ring_id", "rank", "nranks", "margin1", "margin2", "margin3", "scale"}, {"softmax"}, {}, {}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "margin_cross_entropy_grad");
}

void MarginCrossEntropyGradOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value logits_, pir::Value label_, pir::Value softmax_, pir::Value loss_grad_, bool return_softmax, int ring_id, int rank, int nranks, float margin1, float margin2, float margin3, float scale) {
  VLOG(4) << "Start build MarginCrossEntropyGradOp";



  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {logits_, label_, softmax_, loss_grad_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_return_softmax = pir::BoolAttribute::get(pir::IrContext::Instance(), return_softmax);
  argument.AddAttribute("return_softmax", attr_return_softmax);
  pir::Attribute attr_ring_id = pir::Int32Attribute::get(pir::IrContext::Instance(), ring_id);
  argument.AddAttribute("ring_id", attr_ring_id);
  pir::Attribute attr_rank = pir::Int32Attribute::get(pir::IrContext::Instance(), rank);
  argument.AddAttribute("rank", attr_rank);
  pir::Attribute attr_nranks = pir::Int32Attribute::get(pir::IrContext::Instance(), nranks);
  argument.AddAttribute("nranks", attr_nranks);
  pir::Attribute attr_margin1 = pir::FloatAttribute::get(pir::IrContext::Instance(), margin1);
  argument.AddAttribute("margin1", attr_margin1);
  pir::Attribute attr_margin2 = pir::FloatAttribute::get(pir::IrContext::Instance(), margin2);
  argument.AddAttribute("margin2", attr_margin2);
  pir::Attribute attr_margin3 = pir::FloatAttribute::get(pir::IrContext::Instance(), margin3);
  argument.AddAttribute("margin3", attr_margin3);
  pir::Attribute attr_scale = pir::FloatAttribute::get(pir::IrContext::Instance(), scale);
  argument.AddAttribute("scale", attr_scale);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType logits = logits_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)logits;
  paddle::dialect::DenseTensorType label = label_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)label;
  paddle::dialect::DenseTensorType softmax = softmax_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)softmax;
  paddle::dialect::DenseTensorType loss_grad = loss_grad_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)loss_grad;

  VLOG(4) << "Builder construction  dense_logits";
  paddle::dialect::IrTensor ir_tensor_logits(paddle::dialect::TransToPhiDataType(logits.dtype()),
                                                      logits.dims(),
                                                      logits.data_layout(),
                                                      logits.lod(),
                                                      logits.offset());
  VLOG(4) << "Builder construction  meta_logits";
  paddle::dialect::IrMetaTensor meta_logits(&ir_tensor_logits);

  VLOG(4) << "Builder construction  dense_label";
  paddle::dialect::IrTensor ir_tensor_label(paddle::dialect::TransToPhiDataType(label.dtype()),
                                                      label.dims(),
                                                      label.data_layout(),
                                                      label.lod(),
                                                      label.offset());
  VLOG(4) << "Builder construction  meta_label";
  paddle::dialect::IrMetaTensor meta_label(&ir_tensor_label);

  VLOG(4) << "Builder construction  dense_softmax";
  paddle::dialect::IrTensor ir_tensor_softmax(paddle::dialect::TransToPhiDataType(softmax.dtype()),
                                                      softmax.dims(),
                                                      softmax.data_layout(),
                                                      softmax.lod(),
                                                      softmax.offset());
  VLOG(4) << "Builder construction  meta_softmax";
  paddle::dialect::IrMetaTensor meta_softmax(&ir_tensor_softmax);

  VLOG(4) << "Builder construction  dense_loss_grad";
  paddle::dialect::IrTensor ir_tensor_loss_grad(paddle::dialect::TransToPhiDataType(loss_grad.dtype()),
                                                      loss_grad.dims(),
                                                      loss_grad.data_layout(),
                                                      loss_grad.lod(),
                                                      loss_grad.offset());
  VLOG(4) << "Builder construction  meta_loss_grad";
  paddle::dialect::IrMetaTensor meta_loss_grad(&ir_tensor_loss_grad);
  paddle::dialect::IrTensor dense_logits_grad;
  paddle::dialect::IrMetaTensor meta_logits_grad(&dense_logits_grad);

  phi::MarginCrossEntropyGradInferMeta(meta_logits, meta_label, meta_softmax, meta_loss_grad, return_softmax, ring_id, rank, nranks, margin1, margin2, margin3, scale, &meta_logits_grad);

  std::vector<pir::Type> argument_outputs;
  pir::Type logits_grad_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_logits_grad.dtype()), dense_logits_grad.dims(), dense_logits_grad.layout(), dense_logits_grad.lod(), dense_logits_grad.offset());
  argument_outputs.push_back(logits_grad_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void MarginCrossEntropyGradOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value logits_, pir::Value label_, pir::Value softmax_, pir::Value loss_grad_, pir::AttributeMap attributes) {
  VLOG(4) << "Start build MarginCrossEntropyGradOp";


  IR_ENFORCE(
      attributes.find("return_softmax") != attributes.end(),
          "'return_softmax' Attribute is expected for MarginCrossEntropyGradOp. ");
  bool return_softmax = attributes.at("return_softmax").dyn_cast<pir::BoolAttribute>().data();

  IR_ENFORCE(
      attributes.find("ring_id") != attributes.end(),
          "'ring_id' Attribute is expected for MarginCrossEntropyGradOp. ");
  int ring_id = attributes.at("ring_id").dyn_cast<pir::Int32Attribute>().data();

  IR_ENFORCE(
      attributes.find("rank") != attributes.end(),
          "'rank' Attribute is expected for MarginCrossEntropyGradOp. ");
  int rank = attributes.at("rank").dyn_cast<pir::Int32Attribute>().data();

  IR_ENFORCE(
      attributes.find("nranks") != attributes.end(),
          "'nranks' Attribute is expected for MarginCrossEntropyGradOp. ");
  int nranks = attributes.at("nranks").dyn_cast<pir::Int32Attribute>().data();

  IR_ENFORCE(
      attributes.find("margin1") != attributes.end(),
          "'margin1' Attribute is expected for MarginCrossEntropyGradOp. ");
  float margin1 = attributes.at("margin1").dyn_cast<pir::FloatAttribute>().data();

  IR_ENFORCE(
      attributes.find("margin2") != attributes.end(),
          "'margin2' Attribute is expected for MarginCrossEntropyGradOp. ");
  float margin2 = attributes.at("margin2").dyn_cast<pir::FloatAttribute>().data();

  IR_ENFORCE(
      attributes.find("margin3") != attributes.end(),
          "'margin3' Attribute is expected for MarginCrossEntropyGradOp. ");
  float margin3 = attributes.at("margin3").dyn_cast<pir::FloatAttribute>().data();

  IR_ENFORCE(
      attributes.find("scale") != attributes.end(),
          "'scale' Attribute is expected for MarginCrossEntropyGradOp. ");
  float scale = attributes.at("scale").dyn_cast<pir::FloatAttribute>().data();


  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {logits_, label_, softmax_, loss_grad_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_return_softmax = pir::BoolAttribute::get(pir::IrContext::Instance(), return_softmax);
  argument.AddAttribute("return_softmax", attr_return_softmax);
  pir::Attribute attr_ring_id = pir::Int32Attribute::get(pir::IrContext::Instance(), ring_id);
  argument.AddAttribute("ring_id", attr_ring_id);
  pir::Attribute attr_rank = pir::Int32Attribute::get(pir::IrContext::Instance(), rank);
  argument.AddAttribute("rank", attr_rank);
  pir::Attribute attr_nranks = pir::Int32Attribute::get(pir::IrContext::Instance(), nranks);
  argument.AddAttribute("nranks", attr_nranks);
  pir::Attribute attr_margin1 = pir::FloatAttribute::get(pir::IrContext::Instance(), margin1);
  argument.AddAttribute("margin1", attr_margin1);
  pir::Attribute attr_margin2 = pir::FloatAttribute::get(pir::IrContext::Instance(), margin2);
  argument.AddAttribute("margin2", attr_margin2);
  pir::Attribute attr_margin3 = pir::FloatAttribute::get(pir::IrContext::Instance(), margin3);
  argument.AddAttribute("margin3", attr_margin3);
  pir::Attribute attr_scale = pir::FloatAttribute::get(pir::IrContext::Instance(), scale);
  argument.AddAttribute("scale", attr_scale);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType logits = logits_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)logits;
  paddle::dialect::DenseTensorType label = label_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)label;
  paddle::dialect::DenseTensorType softmax = softmax_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)softmax;
  paddle::dialect::DenseTensorType loss_grad = loss_grad_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)loss_grad;

  VLOG(4) << "Builder construction  dense_logits";
  paddle::dialect::IrTensor ir_tensor_logits(paddle::dialect::TransToPhiDataType(logits.dtype()),
                                                      logits.dims(),
                                                      logits.data_layout(),
                                                      logits.lod(),
                                                      logits.offset());
  VLOG(4) << "Builder construction  meta_logits";
  paddle::dialect::IrMetaTensor meta_logits(&ir_tensor_logits);

  VLOG(4) << "Builder construction  dense_label";
  paddle::dialect::IrTensor ir_tensor_label(paddle::dialect::TransToPhiDataType(label.dtype()),
                                                      label.dims(),
                                                      label.data_layout(),
                                                      label.lod(),
                                                      label.offset());
  VLOG(4) << "Builder construction  meta_label";
  paddle::dialect::IrMetaTensor meta_label(&ir_tensor_label);

  VLOG(4) << "Builder construction  dense_softmax";
  paddle::dialect::IrTensor ir_tensor_softmax(paddle::dialect::TransToPhiDataType(softmax.dtype()),
                                                      softmax.dims(),
                                                      softmax.data_layout(),
                                                      softmax.lod(),
                                                      softmax.offset());
  VLOG(4) << "Builder construction  meta_softmax";
  paddle::dialect::IrMetaTensor meta_softmax(&ir_tensor_softmax);

  VLOG(4) << "Builder construction  dense_loss_grad";
  paddle::dialect::IrTensor ir_tensor_loss_grad(paddle::dialect::TransToPhiDataType(loss_grad.dtype()),
                                                      loss_grad.dims(),
                                                      loss_grad.data_layout(),
                                                      loss_grad.lod(),
                                                      loss_grad.offset());
  VLOG(4) << "Builder construction  meta_loss_grad";
  paddle::dialect::IrMetaTensor meta_loss_grad(&ir_tensor_loss_grad);
  paddle::dialect::IrTensor dense_logits_grad;
  paddle::dialect::IrMetaTensor meta_logits_grad(&dense_logits_grad);

  phi::MarginCrossEntropyGradInferMeta(meta_logits, meta_label, meta_softmax, meta_loss_grad, return_softmax, ring_id, rank, nranks, margin1, margin2, margin3, scale, &meta_logits_grad);

  std::vector<pir::Type> argument_outputs;
  pir::Type logits_grad_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_logits_grad.dtype()), dense_logits_grad.dims(), dense_logits_grad.layout(), dense_logits_grad.lod(), dense_logits_grad.offset());
  argument_outputs.push_back(logits_grad_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void MarginCrossEntropyGradOp::VerifySig() {}

void MarginCrossEntropyGradOp::InferMeta( phi::InferMetaContext *infer_meta ) {
  auto fn = PD_INFER_META(phi::MarginCrossEntropyGradInferMeta);
  fn(infer_meta);
}

phi::DataType MarginCrossEntropyGradOp::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: MarginCrossEntropyGradOp";
  


  return expected_kernel_dtype;
}

const char *MarginCrossEntropyGrad_Op::attributes_name[8] = { "return_softmax", "ring_id", "rank", "nranks", "margin1", "margin2", "margin3", "scale" };

OpInfoTuple MarginCrossEntropyGrad_Op::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("logits", "paddle::dialect::DenseTensorType", false, false, false, false), paddle::dialect::OpInputInfo("label", "paddle::dialect::DenseTensorType", false, false, false, false), paddle::dialect::OpInputInfo("softmax", "paddle::dialect::DenseTensorType", false, false, false, false), paddle::dialect::OpInputInfo("loss_grad", "paddle::dialect::DenseTensorType", false, false, false, false) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = { paddle::dialect::OpAttributeInfo("return_softmax", "pir::BoolAttribute", ""), paddle::dialect::OpAttributeInfo("ring_id", "pir::Int32Attribute", ""), paddle::dialect::OpAttributeInfo("rank", "pir::Int32Attribute", ""), paddle::dialect::OpAttributeInfo("nranks", "pir::Int32Attribute", ""), paddle::dialect::OpAttributeInfo("margin1", "pir::FloatAttribute", ""), paddle::dialect::OpAttributeInfo("margin2", "pir::FloatAttribute", ""), paddle::dialect::OpAttributeInfo("margin3", "pir::FloatAttribute", ""), paddle::dialect::OpAttributeInfo("scale", "pir::FloatAttribute", "") };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("logits_grad", "paddle::dialect::DenseTensorType", false, false) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("MarginCrossEntropyGradInferMeta", {"logits", "label", "softmax", "loss_grad", "return_softmax", "ring_id", "rank", "nranks", "margin1", "margin2", "margin3", "scale"}, "margin_cross_entropy_grad", {"logits", "label", "softmax", "loss_grad", "return_softmax", "ring_id", "rank", "nranks", "margin1", "margin2", "margin3", "scale"}, {"softmax"}, {}, {{"logits_grad", "softmax"}}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "margin_cross_entropy_grad");
}

void MarginCrossEntropyGrad_Op::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value logits_, pir::Value label_, pir::Value softmax_, pir::Value loss_grad_, bool return_softmax, int ring_id, int rank, int nranks, float margin1, float margin2, float margin3, float scale) {
  VLOG(4) << "Start build MarginCrossEntropyGrad_Op";



  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {logits_, label_, softmax_, loss_grad_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_return_softmax = pir::BoolAttribute::get(pir::IrContext::Instance(), return_softmax);
  argument.AddAttribute("return_softmax", attr_return_softmax);
  pir::Attribute attr_ring_id = pir::Int32Attribute::get(pir::IrContext::Instance(), ring_id);
  argument.AddAttribute("ring_id", attr_ring_id);
  pir::Attribute attr_rank = pir::Int32Attribute::get(pir::IrContext::Instance(), rank);
  argument.AddAttribute("rank", attr_rank);
  pir::Attribute attr_nranks = pir::Int32Attribute::get(pir::IrContext::Instance(), nranks);
  argument.AddAttribute("nranks", attr_nranks);
  pir::Attribute attr_margin1 = pir::FloatAttribute::get(pir::IrContext::Instance(), margin1);
  argument.AddAttribute("margin1", attr_margin1);
  pir::Attribute attr_margin2 = pir::FloatAttribute::get(pir::IrContext::Instance(), margin2);
  argument.AddAttribute("margin2", attr_margin2);
  pir::Attribute attr_margin3 = pir::FloatAttribute::get(pir::IrContext::Instance(), margin3);
  argument.AddAttribute("margin3", attr_margin3);
  pir::Attribute attr_scale = pir::FloatAttribute::get(pir::IrContext::Instance(), scale);
  argument.AddAttribute("scale", attr_scale);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType logits = logits_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)logits;
  paddle::dialect::DenseTensorType label = label_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)label;
  paddle::dialect::DenseTensorType softmax = softmax_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)softmax;
  paddle::dialect::DenseTensorType loss_grad = loss_grad_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)loss_grad;

  VLOG(4) << "Builder construction  dense_logits";
  paddle::dialect::IrTensor ir_tensor_logits(paddle::dialect::TransToPhiDataType(logits.dtype()),
                                                      logits.dims(),
                                                      logits.data_layout(),
                                                      logits.lod(),
                                                      logits.offset());
  VLOG(4) << "Builder construction  meta_logits";
  paddle::dialect::IrMetaTensor meta_logits(&ir_tensor_logits);

  VLOG(4) << "Builder construction  dense_label";
  paddle::dialect::IrTensor ir_tensor_label(paddle::dialect::TransToPhiDataType(label.dtype()),
                                                      label.dims(),
                                                      label.data_layout(),
                                                      label.lod(),
                                                      label.offset());
  VLOG(4) << "Builder construction  meta_label";
  paddle::dialect::IrMetaTensor meta_label(&ir_tensor_label);

  VLOG(4) << "Builder construction  dense_softmax";
  paddle::dialect::IrTensor ir_tensor_softmax(paddle::dialect::TransToPhiDataType(softmax.dtype()),
                                                      softmax.dims(),
                                                      softmax.data_layout(),
                                                      softmax.lod(),
                                                      softmax.offset());
  VLOG(4) << "Builder construction  meta_softmax";
  paddle::dialect::IrMetaTensor meta_softmax(&ir_tensor_softmax);

  VLOG(4) << "Builder construction  dense_loss_grad";
  paddle::dialect::IrTensor ir_tensor_loss_grad(paddle::dialect::TransToPhiDataType(loss_grad.dtype()),
                                                      loss_grad.dims(),
                                                      loss_grad.data_layout(),
                                                      loss_grad.lod(),
                                                      loss_grad.offset());
  VLOG(4) << "Builder construction  meta_loss_grad";
  paddle::dialect::IrMetaTensor meta_loss_grad(&ir_tensor_loss_grad);
  paddle::dialect::IrTensor dense_logits_grad;
  paddle::dialect::IrMetaTensor meta_logits_grad(&dense_logits_grad);

  phi::MarginCrossEntropyGradInferMeta(meta_logits, meta_label, meta_softmax, meta_loss_grad, return_softmax, ring_id, rank, nranks, margin1, margin2, margin3, scale, &meta_logits_grad);

  std::vector<pir::Type> argument_outputs;
  pir::Type logits_grad_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_logits_grad.dtype()), dense_logits_grad.dims(), dense_logits_grad.layout(), dense_logits_grad.lod(), dense_logits_grad.offset());
  argument_outputs.push_back(logits_grad_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void MarginCrossEntropyGrad_Op::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value logits_, pir::Value label_, pir::Value softmax_, pir::Value loss_grad_, pir::AttributeMap attributes) {
  VLOG(4) << "Start build MarginCrossEntropyGrad_Op";


  IR_ENFORCE(
      attributes.find("return_softmax") != attributes.end(),
          "'return_softmax' Attribute is expected for MarginCrossEntropyGrad_Op. ");
  bool return_softmax = attributes.at("return_softmax").dyn_cast<pir::BoolAttribute>().data();

  IR_ENFORCE(
      attributes.find("ring_id") != attributes.end(),
          "'ring_id' Attribute is expected for MarginCrossEntropyGrad_Op. ");
  int ring_id = attributes.at("ring_id").dyn_cast<pir::Int32Attribute>().data();

  IR_ENFORCE(
      attributes.find("rank") != attributes.end(),
          "'rank' Attribute is expected for MarginCrossEntropyGrad_Op. ");
  int rank = attributes.at("rank").dyn_cast<pir::Int32Attribute>().data();

  IR_ENFORCE(
      attributes.find("nranks") != attributes.end(),
          "'nranks' Attribute is expected for MarginCrossEntropyGrad_Op. ");
  int nranks = attributes.at("nranks").dyn_cast<pir::Int32Attribute>().data();

  IR_ENFORCE(
      attributes.find("margin1") != attributes.end(),
          "'margin1' Attribute is expected for MarginCrossEntropyGrad_Op. ");
  float margin1 = attributes.at("margin1").dyn_cast<pir::FloatAttribute>().data();

  IR_ENFORCE(
      attributes.find("margin2") != attributes.end(),
          "'margin2' Attribute is expected for MarginCrossEntropyGrad_Op. ");
  float margin2 = attributes.at("margin2").dyn_cast<pir::FloatAttribute>().data();

  IR_ENFORCE(
      attributes.find("margin3") != attributes.end(),
          "'margin3' Attribute is expected for MarginCrossEntropyGrad_Op. ");
  float margin3 = attributes.at("margin3").dyn_cast<pir::FloatAttribute>().data();

  IR_ENFORCE(
      attributes.find("scale") != attributes.end(),
          "'scale' Attribute is expected for MarginCrossEntropyGrad_Op. ");
  float scale = attributes.at("scale").dyn_cast<pir::FloatAttribute>().data();


  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {logits_, label_, softmax_, loss_grad_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_return_softmax = pir::BoolAttribute::get(pir::IrContext::Instance(), return_softmax);
  argument.AddAttribute("return_softmax", attr_return_softmax);
  pir::Attribute attr_ring_id = pir::Int32Attribute::get(pir::IrContext::Instance(), ring_id);
  argument.AddAttribute("ring_id", attr_ring_id);
  pir::Attribute attr_rank = pir::Int32Attribute::get(pir::IrContext::Instance(), rank);
  argument.AddAttribute("rank", attr_rank);
  pir::Attribute attr_nranks = pir::Int32Attribute::get(pir::IrContext::Instance(), nranks);
  argument.AddAttribute("nranks", attr_nranks);
  pir::Attribute attr_margin1 = pir::FloatAttribute::get(pir::IrContext::Instance(), margin1);
  argument.AddAttribute("margin1", attr_margin1);
  pir::Attribute attr_margin2 = pir::FloatAttribute::get(pir::IrContext::Instance(), margin2);
  argument.AddAttribute("margin2", attr_margin2);
  pir::Attribute attr_margin3 = pir::FloatAttribute::get(pir::IrContext::Instance(), margin3);
  argument.AddAttribute("margin3", attr_margin3);
  pir::Attribute attr_scale = pir::FloatAttribute::get(pir::IrContext::Instance(), scale);
  argument.AddAttribute("scale", attr_scale);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType logits = logits_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)logits;
  paddle::dialect::DenseTensorType label = label_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)label;
  paddle::dialect::DenseTensorType softmax = softmax_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)softmax;
  paddle::dialect::DenseTensorType loss_grad = loss_grad_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)loss_grad;

  VLOG(4) << "Builder construction  dense_logits";
  paddle::dialect::IrTensor ir_tensor_logits(paddle::dialect::TransToPhiDataType(logits.dtype()),
                                                      logits.dims(),
                                                      logits.data_layout(),
                                                      logits.lod(),
                                                      logits.offset());
  VLOG(4) << "Builder construction  meta_logits";
  paddle::dialect::IrMetaTensor meta_logits(&ir_tensor_logits);

  VLOG(4) << "Builder construction  dense_label";
  paddle::dialect::IrTensor ir_tensor_label(paddle::dialect::TransToPhiDataType(label.dtype()),
                                                      label.dims(),
                                                      label.data_layout(),
                                                      label.lod(),
                                                      label.offset());
  VLOG(4) << "Builder construction  meta_label";
  paddle::dialect::IrMetaTensor meta_label(&ir_tensor_label);

  VLOG(4) << "Builder construction  dense_softmax";
  paddle::dialect::IrTensor ir_tensor_softmax(paddle::dialect::TransToPhiDataType(softmax.dtype()),
                                                      softmax.dims(),
                                                      softmax.data_layout(),
                                                      softmax.lod(),
                                                      softmax.offset());
  VLOG(4) << "Builder construction  meta_softmax";
  paddle::dialect::IrMetaTensor meta_softmax(&ir_tensor_softmax);

  VLOG(4) << "Builder construction  dense_loss_grad";
  paddle::dialect::IrTensor ir_tensor_loss_grad(paddle::dialect::TransToPhiDataType(loss_grad.dtype()),
                                                      loss_grad.dims(),
                                                      loss_grad.data_layout(),
                                                      loss_grad.lod(),
                                                      loss_grad.offset());
  VLOG(4) << "Builder construction  meta_loss_grad";
  paddle::dialect::IrMetaTensor meta_loss_grad(&ir_tensor_loss_grad);
  paddle::dialect::IrTensor dense_logits_grad;
  paddle::dialect::IrMetaTensor meta_logits_grad(&dense_logits_grad);

  phi::MarginCrossEntropyGradInferMeta(meta_logits, meta_label, meta_softmax, meta_loss_grad, return_softmax, ring_id, rank, nranks, margin1, margin2, margin3, scale, &meta_logits_grad);

  std::vector<pir::Type> argument_outputs;
  pir::Type logits_grad_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_logits_grad.dtype()), dense_logits_grad.dims(), dense_logits_grad.layout(), dense_logits_grad.lod(), dense_logits_grad.offset());
  argument_outputs.push_back(logits_grad_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void MarginCrossEntropyGrad_Op::VerifySig() {}

void MarginCrossEntropyGrad_Op::InferMeta( phi::InferMetaContext *infer_meta ) {
  auto fn = PD_INFER_META(phi::MarginCrossEntropyGradInferMeta);
  fn(infer_meta);
}

phi::DataType MarginCrossEntropyGrad_Op::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: MarginCrossEntropyGrad_Op";
  


  return expected_kernel_dtype;
}

OpInfoTuple MaskedSelectGradOp::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("x", "paddle::dialect::DenseTensorType", false, true, false, false), paddle::dialect::OpInputInfo("mask", "paddle::dialect::DenseTensorType", false, false, false, false), paddle::dialect::OpInputInfo("out_grad", "paddle::dialect::DenseTensorType", false, false, false, false) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = {  };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("x_grad", "paddle::dialect::DenseTensorType", false, false) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("UnchangedInferMeta", {"x"}, "masked_select_grad", {"x", "mask", "out_grad"}, {"x"}, {}, {}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "masked_select_grad");
}

void MaskedSelectGradOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, pir::Value mask_, pir::Value out_grad_) {
  VLOG(4) << "Start build MaskedSelectGradOp";



  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_, mask_, out_grad_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;
  paddle::dialect::DenseTensorType mask = mask_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)mask;
  paddle::dialect::DenseTensorType out_grad = out_grad_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)out_grad;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);
  paddle::dialect::IrTensor dense_x_grad;
  paddle::dialect::IrMetaTensor meta_x_grad(&dense_x_grad);

  phi::UnchangedInferMeta(meta_x, &meta_x_grad);

  std::vector<pir::Type> argument_outputs;
  pir::Type x_grad_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_x_grad.dtype()), dense_x_grad.dims(), dense_x_grad.layout(), dense_x_grad.lod(), dense_x_grad.offset());
  argument_outputs.push_back(x_grad_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void MaskedSelectGradOp::VerifySig() {}

void MaskedSelectGradOp::InferMeta( phi::InferMetaContext *infer_meta ) {
  auto fn = PD_INFER_META(phi::UnchangedInferMeta);
  fn(infer_meta);
}

phi::DataType MaskedSelectGradOp::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: MaskedSelectGradOp";
  


  return expected_kernel_dtype;
}

const char *MatrixPowerGradOp::attributes_name[1] = { "n" };

OpInfoTuple MatrixPowerGradOp::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("x", "paddle::dialect::DenseTensorType", false, false, false, false), paddle::dialect::OpInputInfo("out", "paddle::dialect::DenseTensorType", false, false, false, false), paddle::dialect::OpInputInfo("out_grad", "paddle::dialect::DenseTensorType", false, false, false, false) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = { paddle::dialect::OpAttributeInfo("n", "pir::Int32Attribute", "") };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("x_grad", "paddle::dialect::DenseTensorType", false, false) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("UnchangedInferMeta", {"x"}, "matrix_power_grad", {"x", "out", "out_grad", "n"}, {}, {}, {}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "matrix_power_grad");
}

void MatrixPowerGradOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, pir::Value out_, pir::Value out_grad_, int n) {
  VLOG(4) << "Start build MatrixPowerGradOp";



  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_, out_, out_grad_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_n = pir::Int32Attribute::get(pir::IrContext::Instance(), n);
  argument.AddAttribute("n", attr_n);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;
  paddle::dialect::DenseTensorType out = out_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)out;
  paddle::dialect::DenseTensorType out_grad = out_grad_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)out_grad;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);
  paddle::dialect::IrTensor dense_x_grad;
  paddle::dialect::IrMetaTensor meta_x_grad(&dense_x_grad);

  phi::UnchangedInferMeta(meta_x, &meta_x_grad);

  std::vector<pir::Type> argument_outputs;
  pir::Type x_grad_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_x_grad.dtype()), dense_x_grad.dims(), dense_x_grad.layout(), dense_x_grad.lod(), dense_x_grad.offset());
  argument_outputs.push_back(x_grad_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void MatrixPowerGradOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, pir::Value out_, pir::Value out_grad_, pir::AttributeMap attributes) {
  VLOG(4) << "Start build MatrixPowerGradOp";


  IR_ENFORCE(
      attributes.find("n") != attributes.end(),
          "'n' Attribute is expected for MatrixPowerGradOp. ");
  int n = attributes.at("n").dyn_cast<pir::Int32Attribute>().data();


  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_, out_, out_grad_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_n = pir::Int32Attribute::get(pir::IrContext::Instance(), n);
  argument.AddAttribute("n", attr_n);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;
  paddle::dialect::DenseTensorType out = out_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)out;
  paddle::dialect::DenseTensorType out_grad = out_grad_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)out_grad;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);
  paddle::dialect::IrTensor dense_x_grad;
  paddle::dialect::IrMetaTensor meta_x_grad(&dense_x_grad);

  phi::UnchangedInferMeta(meta_x, &meta_x_grad);

  std::vector<pir::Type> argument_outputs;
  pir::Type x_grad_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_x_grad.dtype()), dense_x_grad.dims(), dense_x_grad.layout(), dense_x_grad.lod(), dense_x_grad.offset());
  argument_outputs.push_back(x_grad_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void MatrixPowerGradOp::VerifySig() {}

void MatrixPowerGradOp::InferMeta( phi::InferMetaContext *infer_meta ) {
  auto fn = PD_INFER_META(phi::UnchangedInferMeta);
  fn(infer_meta);
}

phi::DataType MatrixPowerGradOp::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: MatrixPowerGradOp";
  


  return expected_kernel_dtype;
}

const char *MaxPool2dWithIndexGradOp::attributes_name[5] = { "kernel_size", "strides", "paddings", "global_pooling", "adaptive" };

OpInfoTuple MaxPool2dWithIndexGradOp::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("x", "paddle::dialect::DenseTensorType", false, false, false, false), paddle::dialect::OpInputInfo("mask", "paddle::dialect::DenseTensorType", false, false, false, false), paddle::dialect::OpInputInfo("out_grad", "paddle::dialect::DenseTensorType", false, false, false, false) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = { paddle::dialect::OpAttributeInfo("kernel_size", "pir::ArrayAttribute<pir::Int32Attribute>", ""), paddle::dialect::OpAttributeInfo("strides", "pir::ArrayAttribute<pir::Int32Attribute>", ""), paddle::dialect::OpAttributeInfo("paddings", "pir::ArrayAttribute<pir::Int32Attribute>", ""), paddle::dialect::OpAttributeInfo("global_pooling", "pir::BoolAttribute", ""), paddle::dialect::OpAttributeInfo("adaptive", "pir::BoolAttribute", "") };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("x_grad", "paddle::dialect::DenseTensorType", false, false) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("MaxPoolWithIndexGradInferMeta", {"x", "mask", "out_grad", "kernel_size", "strides", "paddings", "global_pooling", "adaptive"}, "max_pool2d_with_index_grad", {"x", "mask", "out_grad", "kernel_size", "strides", "paddings", "global_pooling", "adaptive"}, {}, {}, {}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "max_pool2d_with_index_grad");
}

void MaxPool2dWithIndexGradOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, pir::Value mask_, pir::Value out_grad_, const std::vector<int>& kernel_size, const std::vector<int>& strides, const std::vector<int>& paddings, bool global_pooling, bool adaptive) {
  VLOG(4) << "Start build MaxPool2dWithIndexGradOp";



  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_, mask_, out_grad_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  std::vector<pir::Attribute> vec_kernel_size;
  for (size_t i = 0; i < static_cast<size_t>(kernel_size.size()); i++) {
      pir::Attribute attr_kernel_size = pir::Int32Attribute::get(pir::IrContext::Instance(), kernel_size[i]);

    vec_kernel_size.push_back(attr_kernel_size);
  }
  pir::Attribute attr_kernel_size = pir::ArrayAttribute::get(pir::IrContext::Instance(), vec_kernel_size);
  argument.AddAttribute("kernel_size", attr_kernel_size);
  std::vector<pir::Attribute> vec_strides;
  for (size_t i = 0; i < static_cast<size_t>(strides.size()); i++) {
      pir::Attribute attr_strides = pir::Int32Attribute::get(pir::IrContext::Instance(), strides[i]);

    vec_strides.push_back(attr_strides);
  }
  pir::Attribute attr_strides = pir::ArrayAttribute::get(pir::IrContext::Instance(), vec_strides);
  argument.AddAttribute("strides", attr_strides);
  std::vector<pir::Attribute> vec_paddings;
  for (size_t i = 0; i < static_cast<size_t>(paddings.size()); i++) {
      pir::Attribute attr_paddings = pir::Int32Attribute::get(pir::IrContext::Instance(), paddings[i]);

    vec_paddings.push_back(attr_paddings);
  }
  pir::Attribute attr_paddings = pir::ArrayAttribute::get(pir::IrContext::Instance(), vec_paddings);
  argument.AddAttribute("paddings", attr_paddings);
  pir::Attribute attr_global_pooling = pir::BoolAttribute::get(pir::IrContext::Instance(), global_pooling);
  argument.AddAttribute("global_pooling", attr_global_pooling);
  pir::Attribute attr_adaptive = pir::BoolAttribute::get(pir::IrContext::Instance(), adaptive);
  argument.AddAttribute("adaptive", attr_adaptive);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;
  paddle::dialect::DenseTensorType mask = mask_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)mask;
  paddle::dialect::DenseTensorType out_grad = out_grad_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)out_grad;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);

  VLOG(4) << "Builder construction  dense_mask";
  paddle::dialect::IrTensor ir_tensor_mask(paddle::dialect::TransToPhiDataType(mask.dtype()),
                                                      mask.dims(),
                                                      mask.data_layout(),
                                                      mask.lod(),
                                                      mask.offset());
  VLOG(4) << "Builder construction  meta_mask";
  paddle::dialect::IrMetaTensor meta_mask(&ir_tensor_mask);

  VLOG(4) << "Builder construction  dense_out_grad";
  paddle::dialect::IrTensor ir_tensor_out_grad(paddle::dialect::TransToPhiDataType(out_grad.dtype()),
                                                      out_grad.dims(),
                                                      out_grad.data_layout(),
                                                      out_grad.lod(),
                                                      out_grad.offset());
  VLOG(4) << "Builder construction  meta_out_grad";
  paddle::dialect::IrMetaTensor meta_out_grad(&ir_tensor_out_grad);
  paddle::dialect::IrTensor dense_x_grad;
  paddle::dialect::IrMetaTensor meta_x_grad(&dense_x_grad);

  phi::MaxPoolWithIndexGradInferMeta(meta_x, meta_mask, meta_out_grad, kernel_size, strides, paddings, global_pooling, adaptive, &meta_x_grad);

  std::vector<pir::Type> argument_outputs;
  pir::Type x_grad_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_x_grad.dtype()), dense_x_grad.dims(), dense_x_grad.layout(), dense_x_grad.lod(), dense_x_grad.offset());
  argument_outputs.push_back(x_grad_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void MaxPool2dWithIndexGradOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, pir::Value mask_, pir::Value out_grad_, pir::AttributeMap attributes) {
  VLOG(4) << "Start build MaxPool2dWithIndexGradOp";


  IR_ENFORCE(
      attributes.find("kernel_size") != attributes.end(),
          "'kernel_size' Attribute is expected for MaxPool2dWithIndexGradOp. ");
  std::vector<int> kernel_size;
  for (size_t i = 0; i < attributes.at("kernel_size").dyn_cast<pir::ArrayAttribute>().size(); i++) {
    kernel_size.push_back(attributes.at("kernel_size").dyn_cast<pir::ArrayAttribute>().at(i).dyn_cast<pir::Int32Attribute>().data());
  }

  IR_ENFORCE(
      attributes.find("strides") != attributes.end(),
          "'strides' Attribute is expected for MaxPool2dWithIndexGradOp. ");
  std::vector<int> strides;
  for (size_t i = 0; i < attributes.at("strides").dyn_cast<pir::ArrayAttribute>().size(); i++) {
    strides.push_back(attributes.at("strides").dyn_cast<pir::ArrayAttribute>().at(i).dyn_cast<pir::Int32Attribute>().data());
  }

  IR_ENFORCE(
      attributes.find("paddings") != attributes.end(),
          "'paddings' Attribute is expected for MaxPool2dWithIndexGradOp. ");
  std::vector<int> paddings;
  for (size_t i = 0; i < attributes.at("paddings").dyn_cast<pir::ArrayAttribute>().size(); i++) {
    paddings.push_back(attributes.at("paddings").dyn_cast<pir::ArrayAttribute>().at(i).dyn_cast<pir::Int32Attribute>().data());
  }

  IR_ENFORCE(
      attributes.find("global_pooling") != attributes.end(),
          "'global_pooling' Attribute is expected for MaxPool2dWithIndexGradOp. ");
  bool global_pooling = attributes.at("global_pooling").dyn_cast<pir::BoolAttribute>().data();

  IR_ENFORCE(
      attributes.find("adaptive") != attributes.end(),
          "'adaptive' Attribute is expected for MaxPool2dWithIndexGradOp. ");
  bool adaptive = attributes.at("adaptive").dyn_cast<pir::BoolAttribute>().data();


  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_, mask_, out_grad_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  std::vector<pir::Attribute> vec_kernel_size;
  for (size_t i = 0; i < static_cast<size_t>(kernel_size.size()); i++) {
      pir::Attribute attr_kernel_size = pir::Int32Attribute::get(pir::IrContext::Instance(), kernel_size[i]);

    vec_kernel_size.push_back(attr_kernel_size);
  }
  pir::Attribute attr_kernel_size = pir::ArrayAttribute::get(pir::IrContext::Instance(), vec_kernel_size);
  argument.AddAttribute("kernel_size", attr_kernel_size);
  std::vector<pir::Attribute> vec_strides;
  for (size_t i = 0; i < static_cast<size_t>(strides.size()); i++) {
      pir::Attribute attr_strides = pir::Int32Attribute::get(pir::IrContext::Instance(), strides[i]);

    vec_strides.push_back(attr_strides);
  }
  pir::Attribute attr_strides = pir::ArrayAttribute::get(pir::IrContext::Instance(), vec_strides);
  argument.AddAttribute("strides", attr_strides);
  std::vector<pir::Attribute> vec_paddings;
  for (size_t i = 0; i < static_cast<size_t>(paddings.size()); i++) {
      pir::Attribute attr_paddings = pir::Int32Attribute::get(pir::IrContext::Instance(), paddings[i]);

    vec_paddings.push_back(attr_paddings);
  }
  pir::Attribute attr_paddings = pir::ArrayAttribute::get(pir::IrContext::Instance(), vec_paddings);
  argument.AddAttribute("paddings", attr_paddings);
  pir::Attribute attr_global_pooling = pir::BoolAttribute::get(pir::IrContext::Instance(), global_pooling);
  argument.AddAttribute("global_pooling", attr_global_pooling);
  pir::Attribute attr_adaptive = pir::BoolAttribute::get(pir::IrContext::Instance(), adaptive);
  argument.AddAttribute("adaptive", attr_adaptive);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;
  paddle::dialect::DenseTensorType mask = mask_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)mask;
  paddle::dialect::DenseTensorType out_grad = out_grad_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)out_grad;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);

  VLOG(4) << "Builder construction  dense_mask";
  paddle::dialect::IrTensor ir_tensor_mask(paddle::dialect::TransToPhiDataType(mask.dtype()),
                                                      mask.dims(),
                                                      mask.data_layout(),
                                                      mask.lod(),
                                                      mask.offset());
  VLOG(4) << "Builder construction  meta_mask";
  paddle::dialect::IrMetaTensor meta_mask(&ir_tensor_mask);

  VLOG(4) << "Builder construction  dense_out_grad";
  paddle::dialect::IrTensor ir_tensor_out_grad(paddle::dialect::TransToPhiDataType(out_grad.dtype()),
                                                      out_grad.dims(),
                                                      out_grad.data_layout(),
                                                      out_grad.lod(),
                                                      out_grad.offset());
  VLOG(4) << "Builder construction  meta_out_grad";
  paddle::dialect::IrMetaTensor meta_out_grad(&ir_tensor_out_grad);
  paddle::dialect::IrTensor dense_x_grad;
  paddle::dialect::IrMetaTensor meta_x_grad(&dense_x_grad);

  phi::MaxPoolWithIndexGradInferMeta(meta_x, meta_mask, meta_out_grad, kernel_size, strides, paddings, global_pooling, adaptive, &meta_x_grad);

  std::vector<pir::Type> argument_outputs;
  pir::Type x_grad_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_x_grad.dtype()), dense_x_grad.dims(), dense_x_grad.layout(), dense_x_grad.lod(), dense_x_grad.offset());
  argument_outputs.push_back(x_grad_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void MaxPool2dWithIndexGradOp::VerifySig() {}

void MaxPool2dWithIndexGradOp::InferMeta( phi::InferMetaContext *infer_meta ) {
  auto fn = PD_INFER_META(phi::MaxPoolWithIndexGradInferMeta);
  fn(infer_meta);
}

phi::DataType MaxPool2dWithIndexGradOp::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: MaxPool2dWithIndexGradOp";
  


  return expected_kernel_dtype;
}

const char *MaxPool3dWithIndexGradOp::attributes_name[5] = { "kernel_size", "strides", "paddings", "global_pooling", "adaptive" };

OpInfoTuple MaxPool3dWithIndexGradOp::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("x", "paddle::dialect::DenseTensorType", false, false, false, false), paddle::dialect::OpInputInfo("mask", "paddle::dialect::DenseTensorType", false, false, false, false), paddle::dialect::OpInputInfo("out_grad", "paddle::dialect::DenseTensorType", false, false, false, false) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = { paddle::dialect::OpAttributeInfo("kernel_size", "pir::ArrayAttribute<pir::Int32Attribute>", ""), paddle::dialect::OpAttributeInfo("strides", "pir::ArrayAttribute<pir::Int32Attribute>", ""), paddle::dialect::OpAttributeInfo("paddings", "pir::ArrayAttribute<pir::Int32Attribute>", ""), paddle::dialect::OpAttributeInfo("global_pooling", "pir::BoolAttribute", ""), paddle::dialect::OpAttributeInfo("adaptive", "pir::BoolAttribute", "") };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("x_grad", "paddle::dialect::DenseTensorType", false, false) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("MaxPoolWithIndexGradInferMeta", {"x", "mask", "out_grad", "kernel_size", "strides", "paddings", "global_pooling", "adaptive"}, "max_pool3d_with_index_grad", {"x", "mask", "out_grad", "kernel_size", "strides", "paddings", "global_pooling", "adaptive"}, {}, {}, {}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "max_pool3d_with_index_grad");
}

void MaxPool3dWithIndexGradOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, pir::Value mask_, pir::Value out_grad_, const std::vector<int>& kernel_size, const std::vector<int>& strides, const std::vector<int>& paddings, bool global_pooling, bool adaptive) {
  VLOG(4) << "Start build MaxPool3dWithIndexGradOp";



  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_, mask_, out_grad_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  std::vector<pir::Attribute> vec_kernel_size;
  for (size_t i = 0; i < static_cast<size_t>(kernel_size.size()); i++) {
      pir::Attribute attr_kernel_size = pir::Int32Attribute::get(pir::IrContext::Instance(), kernel_size[i]);

    vec_kernel_size.push_back(attr_kernel_size);
  }
  pir::Attribute attr_kernel_size = pir::ArrayAttribute::get(pir::IrContext::Instance(), vec_kernel_size);
  argument.AddAttribute("kernel_size", attr_kernel_size);
  std::vector<pir::Attribute> vec_strides;
  for (size_t i = 0; i < static_cast<size_t>(strides.size()); i++) {
      pir::Attribute attr_strides = pir::Int32Attribute::get(pir::IrContext::Instance(), strides[i]);

    vec_strides.push_back(attr_strides);
  }
  pir::Attribute attr_strides = pir::ArrayAttribute::get(pir::IrContext::Instance(), vec_strides);
  argument.AddAttribute("strides", attr_strides);
  std::vector<pir::Attribute> vec_paddings;
  for (size_t i = 0; i < static_cast<size_t>(paddings.size()); i++) {
      pir::Attribute attr_paddings = pir::Int32Attribute::get(pir::IrContext::Instance(), paddings[i]);

    vec_paddings.push_back(attr_paddings);
  }
  pir::Attribute attr_paddings = pir::ArrayAttribute::get(pir::IrContext::Instance(), vec_paddings);
  argument.AddAttribute("paddings", attr_paddings);
  pir::Attribute attr_global_pooling = pir::BoolAttribute::get(pir::IrContext::Instance(), global_pooling);
  argument.AddAttribute("global_pooling", attr_global_pooling);
  pir::Attribute attr_adaptive = pir::BoolAttribute::get(pir::IrContext::Instance(), adaptive);
  argument.AddAttribute("adaptive", attr_adaptive);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;
  paddle::dialect::DenseTensorType mask = mask_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)mask;
  paddle::dialect::DenseTensorType out_grad = out_grad_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)out_grad;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);

  VLOG(4) << "Builder construction  dense_mask";
  paddle::dialect::IrTensor ir_tensor_mask(paddle::dialect::TransToPhiDataType(mask.dtype()),
                                                      mask.dims(),
                                                      mask.data_layout(),
                                                      mask.lod(),
                                                      mask.offset());
  VLOG(4) << "Builder construction  meta_mask";
  paddle::dialect::IrMetaTensor meta_mask(&ir_tensor_mask);

  VLOG(4) << "Builder construction  dense_out_grad";
  paddle::dialect::IrTensor ir_tensor_out_grad(paddle::dialect::TransToPhiDataType(out_grad.dtype()),
                                                      out_grad.dims(),
                                                      out_grad.data_layout(),
                                                      out_grad.lod(),
                                                      out_grad.offset());
  VLOG(4) << "Builder construction  meta_out_grad";
  paddle::dialect::IrMetaTensor meta_out_grad(&ir_tensor_out_grad);
  paddle::dialect::IrTensor dense_x_grad;
  paddle::dialect::IrMetaTensor meta_x_grad(&dense_x_grad);

  phi::MaxPoolWithIndexGradInferMeta(meta_x, meta_mask, meta_out_grad, kernel_size, strides, paddings, global_pooling, adaptive, &meta_x_grad);

  std::vector<pir::Type> argument_outputs;
  pir::Type x_grad_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_x_grad.dtype()), dense_x_grad.dims(), dense_x_grad.layout(), dense_x_grad.lod(), dense_x_grad.offset());
  argument_outputs.push_back(x_grad_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void MaxPool3dWithIndexGradOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, pir::Value mask_, pir::Value out_grad_, pir::AttributeMap attributes) {
  VLOG(4) << "Start build MaxPool3dWithIndexGradOp";


  IR_ENFORCE(
      attributes.find("kernel_size") != attributes.end(),
          "'kernel_size' Attribute is expected for MaxPool3dWithIndexGradOp. ");
  std::vector<int> kernel_size;
  for (size_t i = 0; i < attributes.at("kernel_size").dyn_cast<pir::ArrayAttribute>().size(); i++) {
    kernel_size.push_back(attributes.at("kernel_size").dyn_cast<pir::ArrayAttribute>().at(i).dyn_cast<pir::Int32Attribute>().data());
  }

  IR_ENFORCE(
      attributes.find("strides") != attributes.end(),
          "'strides' Attribute is expected for MaxPool3dWithIndexGradOp. ");
  std::vector<int> strides;
  for (size_t i = 0; i < attributes.at("strides").dyn_cast<pir::ArrayAttribute>().size(); i++) {
    strides.push_back(attributes.at("strides").dyn_cast<pir::ArrayAttribute>().at(i).dyn_cast<pir::Int32Attribute>().data());
  }

  IR_ENFORCE(
      attributes.find("paddings") != attributes.end(),
          "'paddings' Attribute is expected for MaxPool3dWithIndexGradOp. ");
  std::vector<int> paddings;
  for (size_t i = 0; i < attributes.at("paddings").dyn_cast<pir::ArrayAttribute>().size(); i++) {
    paddings.push_back(attributes.at("paddings").dyn_cast<pir::ArrayAttribute>().at(i).dyn_cast<pir::Int32Attribute>().data());
  }

  IR_ENFORCE(
      attributes.find("global_pooling") != attributes.end(),
          "'global_pooling' Attribute is expected for MaxPool3dWithIndexGradOp. ");
  bool global_pooling = attributes.at("global_pooling").dyn_cast<pir::BoolAttribute>().data();

  IR_ENFORCE(
      attributes.find("adaptive") != attributes.end(),
          "'adaptive' Attribute is expected for MaxPool3dWithIndexGradOp. ");
  bool adaptive = attributes.at("adaptive").dyn_cast<pir::BoolAttribute>().data();


  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_, mask_, out_grad_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  std::vector<pir::Attribute> vec_kernel_size;
  for (size_t i = 0; i < static_cast<size_t>(kernel_size.size()); i++) {
      pir::Attribute attr_kernel_size = pir::Int32Attribute::get(pir::IrContext::Instance(), kernel_size[i]);

    vec_kernel_size.push_back(attr_kernel_size);
  }
  pir::Attribute attr_kernel_size = pir::ArrayAttribute::get(pir::IrContext::Instance(), vec_kernel_size);
  argument.AddAttribute("kernel_size", attr_kernel_size);
  std::vector<pir::Attribute> vec_strides;
  for (size_t i = 0; i < static_cast<size_t>(strides.size()); i++) {
      pir::Attribute attr_strides = pir::Int32Attribute::get(pir::IrContext::Instance(), strides[i]);

    vec_strides.push_back(attr_strides);
  }
  pir::Attribute attr_strides = pir::ArrayAttribute::get(pir::IrContext::Instance(), vec_strides);
  argument.AddAttribute("strides", attr_strides);
  std::vector<pir::Attribute> vec_paddings;
  for (size_t i = 0; i < static_cast<size_t>(paddings.size()); i++) {
      pir::Attribute attr_paddings = pir::Int32Attribute::get(pir::IrContext::Instance(), paddings[i]);

    vec_paddings.push_back(attr_paddings);
  }
  pir::Attribute attr_paddings = pir::ArrayAttribute::get(pir::IrContext::Instance(), vec_paddings);
  argument.AddAttribute("paddings", attr_paddings);
  pir::Attribute attr_global_pooling = pir::BoolAttribute::get(pir::IrContext::Instance(), global_pooling);
  argument.AddAttribute("global_pooling", attr_global_pooling);
  pir::Attribute attr_adaptive = pir::BoolAttribute::get(pir::IrContext::Instance(), adaptive);
  argument.AddAttribute("adaptive", attr_adaptive);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;
  paddle::dialect::DenseTensorType mask = mask_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)mask;
  paddle::dialect::DenseTensorType out_grad = out_grad_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)out_grad;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);

  VLOG(4) << "Builder construction  dense_mask";
  paddle::dialect::IrTensor ir_tensor_mask(paddle::dialect::TransToPhiDataType(mask.dtype()),
                                                      mask.dims(),
                                                      mask.data_layout(),
                                                      mask.lod(),
                                                      mask.offset());
  VLOG(4) << "Builder construction  meta_mask";
  paddle::dialect::IrMetaTensor meta_mask(&ir_tensor_mask);

  VLOG(4) << "Builder construction  dense_out_grad";
  paddle::dialect::IrTensor ir_tensor_out_grad(paddle::dialect::TransToPhiDataType(out_grad.dtype()),
                                                      out_grad.dims(),
                                                      out_grad.data_layout(),
                                                      out_grad.lod(),
                                                      out_grad.offset());
  VLOG(4) << "Builder construction  meta_out_grad";
  paddle::dialect::IrMetaTensor meta_out_grad(&ir_tensor_out_grad);
  paddle::dialect::IrTensor dense_x_grad;
  paddle::dialect::IrMetaTensor meta_x_grad(&dense_x_grad);

  phi::MaxPoolWithIndexGradInferMeta(meta_x, meta_mask, meta_out_grad, kernel_size, strides, paddings, global_pooling, adaptive, &meta_x_grad);

  std::vector<pir::Type> argument_outputs;
  pir::Type x_grad_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_x_grad.dtype()), dense_x_grad.dims(), dense_x_grad.layout(), dense_x_grad.lod(), dense_x_grad.offset());
  argument_outputs.push_back(x_grad_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void MaxPool3dWithIndexGradOp::VerifySig() {}

void MaxPool3dWithIndexGradOp::InferMeta( phi::InferMetaContext *infer_meta ) {
  auto fn = PD_INFER_META(phi::MaxPoolWithIndexGradInferMeta);
  fn(infer_meta);
}

phi::DataType MaxPool3dWithIndexGradOp::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: MaxPool3dWithIndexGradOp";
  


  return expected_kernel_dtype;
}

const char *MaxoutGradOp::attributes_name[2] = { "groups", "axis" };

OpInfoTuple MaxoutGradOp::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("x", "paddle::dialect::DenseTensorType", false, false, false, false), paddle::dialect::OpInputInfo("out", "paddle::dialect::DenseTensorType", false, false, false, false), paddle::dialect::OpInputInfo("out_grad", "paddle::dialect::DenseTensorType", false, false, false, false) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = { paddle::dialect::OpAttributeInfo("groups", "pir::Int32Attribute", ""), paddle::dialect::OpAttributeInfo("axis", "pir::Int32Attribute", "") };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("x_grad", "paddle::dialect::DenseTensorType", false, false) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("GeneralUnaryGradInferMeta", {"x"}, "maxout_grad", {"x", "out", "out_grad", "groups", "axis"}, {}, {}, {}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "maxout_grad");
}

void MaxoutGradOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, pir::Value out_, pir::Value out_grad_, int groups, int axis) {
  VLOG(4) << "Start build MaxoutGradOp";



  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_, out_, out_grad_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_groups = pir::Int32Attribute::get(pir::IrContext::Instance(), groups);
  argument.AddAttribute("groups", attr_groups);
  pir::Attribute attr_axis = pir::Int32Attribute::get(pir::IrContext::Instance(), axis);
  argument.AddAttribute("axis", attr_axis);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;
  paddle::dialect::DenseTensorType out = out_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)out;
  paddle::dialect::DenseTensorType out_grad = out_grad_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)out_grad;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);
  paddle::dialect::IrTensor dense_x_grad;
  paddle::dialect::IrMetaTensor meta_x_grad(&dense_x_grad);

  phi::GeneralUnaryGradInferMeta(meta_x, &meta_x_grad);

  std::vector<pir::Type> argument_outputs;
  pir::Type x_grad_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_x_grad.dtype()), dense_x_grad.dims(), dense_x_grad.layout(), dense_x_grad.lod(), dense_x_grad.offset());
  argument_outputs.push_back(x_grad_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void MaxoutGradOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, pir::Value out_, pir::Value out_grad_, pir::AttributeMap attributes) {
  VLOG(4) << "Start build MaxoutGradOp";


  IR_ENFORCE(
      attributes.find("groups") != attributes.end(),
          "'groups' Attribute is expected for MaxoutGradOp. ");
  int groups = attributes.at("groups").dyn_cast<pir::Int32Attribute>().data();

  IR_ENFORCE(
      attributes.find("axis") != attributes.end(),
          "'axis' Attribute is expected for MaxoutGradOp. ");
  int axis = attributes.at("axis").dyn_cast<pir::Int32Attribute>().data();


  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_, out_, out_grad_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_groups = pir::Int32Attribute::get(pir::IrContext::Instance(), groups);
  argument.AddAttribute("groups", attr_groups);
  pir::Attribute attr_axis = pir::Int32Attribute::get(pir::IrContext::Instance(), axis);
  argument.AddAttribute("axis", attr_axis);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;
  paddle::dialect::DenseTensorType out = out_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)out;
  paddle::dialect::DenseTensorType out_grad = out_grad_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)out_grad;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);
  paddle::dialect::IrTensor dense_x_grad;
  paddle::dialect::IrMetaTensor meta_x_grad(&dense_x_grad);

  phi::GeneralUnaryGradInferMeta(meta_x, &meta_x_grad);

  std::vector<pir::Type> argument_outputs;
  pir::Type x_grad_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_x_grad.dtype()), dense_x_grad.dims(), dense_x_grad.layout(), dense_x_grad.lod(), dense_x_grad.offset());
  argument_outputs.push_back(x_grad_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void MaxoutGradOp::VerifySig() {}

void MaxoutGradOp::InferMeta( phi::InferMetaContext *infer_meta ) {
  auto fn = PD_INFER_META(phi::GeneralUnaryGradInferMeta);
  fn(infer_meta);
}

phi::DataType MaxoutGradOp::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: MaxoutGradOp";
  


  return expected_kernel_dtype;
}

OpInfoTuple MeanAllGradOp::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("x", "paddle::dialect::DenseTensorType", false, true, false, false), paddle::dialect::OpInputInfo("out_grad", "paddle::dialect::DenseTensorType", false, false, false, false) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = {  };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("x_grad", "paddle::dialect::DenseTensorType", false, false) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("UnchangedExceptLayoutInferMeta", {"x"}, "mean_all_grad", {"x", "out_grad"}, {"out_grad"}, {}, {}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "mean_all_grad");
}

void MeanAllGradOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, pir::Value out_grad_) {
  VLOG(4) << "Start build MeanAllGradOp";



  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_, out_grad_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;
  paddle::dialect::DenseTensorType out_grad = out_grad_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)out_grad;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);
  paddle::dialect::IrTensor dense_x_grad;
  paddle::dialect::IrMetaTensor meta_x_grad(&dense_x_grad);

  phi::UnchangedExceptLayoutInferMeta(meta_x, &meta_x_grad);

  std::vector<pir::Type> argument_outputs;
  pir::Type x_grad_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_x_grad.dtype()), dense_x_grad.dims(), dense_x_grad.layout(), dense_x_grad.lod(), dense_x_grad.offset());
  argument_outputs.push_back(x_grad_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void MeanAllGradOp::VerifySig() {}

void MeanAllGradOp::InferMeta( phi::InferMetaContext *infer_meta ) {
  auto fn = PD_INFER_META(phi::UnchangedExceptLayoutInferMeta);
  fn(infer_meta);
}

phi::DataType MeanAllGradOp::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: MeanAllGradOp";
  


  return expected_kernel_dtype;
}

const char *MemoryEfficientAttentionGradOp::attributes_name[5] = { "max_seqlen_q", "max_seqlen_k", "causal", "dropout_p", "scale" };

OpInfoTuple MemoryEfficientAttentionGradOp::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("query", "paddle::dialect::DenseTensorType", false, false, false, false), paddle::dialect::OpInputInfo("key", "paddle::dialect::DenseTensorType", false, false, false, false), paddle::dialect::OpInputInfo("value", "paddle::dialect::DenseTensorType", false, false, false, false), paddle::dialect::OpInputInfo("bias", "paddle::dialect::DenseTensorType", true, false, false, false), paddle::dialect::OpInputInfo("cu_seqlens_q", "paddle::dialect::DenseTensorType", true, false, false, false), paddle::dialect::OpInputInfo("cu_seqlens_k", "paddle::dialect::DenseTensorType", true, false, false, false), paddle::dialect::OpInputInfo("output", "paddle::dialect::DenseTensorType", false, false, false, false), paddle::dialect::OpInputInfo("logsumexp", "paddle::dialect::DenseTensorType", false, false, false, false), paddle::dialect::OpInputInfo("seed_and_offset", "paddle::dialect::DenseTensorType", false, false, false, false), paddle::dialect::OpInputInfo("output_grad", "paddle::dialect::DenseTensorType", false, false, false, false) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = { paddle::dialect::OpAttributeInfo("max_seqlen_q", "paddle::dialect::ScalarAttribute", "float"), paddle::dialect::OpAttributeInfo("max_seqlen_k", "paddle::dialect::ScalarAttribute", "float"), paddle::dialect::OpAttributeInfo("causal", "pir::BoolAttribute", ""), paddle::dialect::OpAttributeInfo("dropout_p", "pir::DoubleAttribute", ""), paddle::dialect::OpAttributeInfo("scale", "pir::FloatAttribute", "") };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("query_grad", "paddle::dialect::DenseTensorType", false, false), paddle::dialect::OpOutputInfo("key_grad", "paddle::dialect::DenseTensorType", false, false), paddle::dialect::OpOutputInfo("value_grad", "paddle::dialect::DenseTensorType", false, false), paddle::dialect::OpOutputInfo("bias_grad", "paddle::dialect::DenseTensorType", false, false) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("MemoryEfficientAttentionGradInferMeta", {"query", "key", "value", "bias", "cu_seqlens_q", "cu_seqlens_k", "output", "logsumexp", "seed_and_offset", "output_grad", "max_seqlen_q", "max_seqlen_k", "causal", "dropout_p", "scale"}, "memory_efficient_attention_grad", {"query", "key", "value", "bias", "cu_seqlens_q", "cu_seqlens_k", "output", "logsumexp", "seed_and_offset", "output_grad", "max_seqlen_q", "max_seqlen_k", "causal", "dropout_p", "scale"}, {"output_grad"}, {}, {}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "memory_efficient_attention_grad");
}

void MemoryEfficientAttentionGradOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value query_, pir::Value key_, pir::Value value_, pir::Value bias_, pir::Value cu_seqlens_q_, pir::Value cu_seqlens_k_, pir::Value output_, pir::Value logsumexp_, pir::Value seed_and_offset_, pir::Value output_grad_, float max_seqlen_q, float max_seqlen_k, bool causal, double dropout_p, float scale) {
  VLOG(4) << "Start build MemoryEfficientAttentionGradOp";



  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {query_, key_, value_, bias_, cu_seqlens_q_, cu_seqlens_k_, output_, logsumexp_, seed_and_offset_, output_grad_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_max_seqlen_q = paddle::dialect::TransToIrAttribute(max_seqlen_q, pir::IrContext::Instance());
  argument.AddAttribute("max_seqlen_q", attr_max_seqlen_q);
  pir::Attribute attr_max_seqlen_k = paddle::dialect::TransToIrAttribute(max_seqlen_k, pir::IrContext::Instance());
  argument.AddAttribute("max_seqlen_k", attr_max_seqlen_k);
  pir::Attribute attr_causal = pir::BoolAttribute::get(pir::IrContext::Instance(), causal);
  argument.AddAttribute("causal", attr_causal);
  pir::Attribute attr_dropout_p = pir::DoubleAttribute::get(pir::IrContext::Instance(), dropout_p);
  argument.AddAttribute("dropout_p", attr_dropout_p);
  pir::Attribute attr_scale = pir::FloatAttribute::get(pir::IrContext::Instance(), scale);
  argument.AddAttribute("scale", attr_scale);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType query = query_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)query;
  paddle::dialect::DenseTensorType key = key_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)key;
  paddle::dialect::DenseTensorType value = value_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)value;
  paddle::dialect::DenseTensorType output = output_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)output;
  paddle::dialect::DenseTensorType logsumexp = logsumexp_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)logsumexp;
  paddle::dialect::DenseTensorType seed_and_offset = seed_and_offset_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)seed_and_offset;
  paddle::dialect::DenseTensorType output_grad = output_grad_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)output_grad;

  VLOG(4) << "Builder construction  dense_query";
  paddle::dialect::IrTensor ir_tensor_query(paddle::dialect::TransToPhiDataType(query.dtype()),
                                                      query.dims(),
                                                      query.data_layout(),
                                                      query.lod(),
                                                      query.offset());
  VLOG(4) << "Builder construction  meta_query";
  paddle::dialect::IrMetaTensor meta_query(&ir_tensor_query);

  VLOG(4) << "Builder construction  dense_key";
  paddle::dialect::IrTensor ir_tensor_key(paddle::dialect::TransToPhiDataType(key.dtype()),
                                                      key.dims(),
                                                      key.data_layout(),
                                                      key.lod(),
                                                      key.offset());
  VLOG(4) << "Builder construction  meta_key";
  paddle::dialect::IrMetaTensor meta_key(&ir_tensor_key);

  VLOG(4) << "Builder construction  dense_value";
  paddle::dialect::IrTensor ir_tensor_value(paddle::dialect::TransToPhiDataType(value.dtype()),
                                                      value.dims(),
                                                      value.data_layout(),
                                                      value.lod(),
                                                      value.offset());
  VLOG(4) << "Builder construction  meta_value";
  paddle::dialect::IrMetaTensor meta_value(&ir_tensor_value);

  paddle::dialect::IrMetaTensor meta_bias;
  paddle::dialect::IrTensor ir_tensor_bias;
  if (bias_.impl() != nullptr) {
    paddle::dialect::DenseTensorType bias = bias_.type().dyn_cast<paddle::dialect::DenseTensorType>();
    VLOG(4) << "Builder construction  dense_bias";
    ir_tensor_bias = paddle::dialect::IrTensor(paddle::dialect::TransToPhiDataType(bias.dtype()),
                                                        bias.dims(),
                                                        bias.data_layout(),
                                                        bias.lod(),
                                                        bias.offset());
    VLOG(4) << "Builder construction  meta_bias";
    meta_bias = paddle::dialect::IrMetaTensor(&ir_tensor_bias);
  }


  paddle::dialect::IrMetaTensor meta_cu_seqlens_q;
  paddle::dialect::IrTensor ir_tensor_cu_seqlens_q;
  if (cu_seqlens_q_.impl() != nullptr) {
    paddle::dialect::DenseTensorType cu_seqlens_q = cu_seqlens_q_.type().dyn_cast<paddle::dialect::DenseTensorType>();
    VLOG(4) << "Builder construction  dense_cu_seqlens_q";
    ir_tensor_cu_seqlens_q = paddle::dialect::IrTensor(paddle::dialect::TransToPhiDataType(cu_seqlens_q.dtype()),
                                                        cu_seqlens_q.dims(),
                                                        cu_seqlens_q.data_layout(),
                                                        cu_seqlens_q.lod(),
                                                        cu_seqlens_q.offset());
    VLOG(4) << "Builder construction  meta_cu_seqlens_q";
    meta_cu_seqlens_q = paddle::dialect::IrMetaTensor(&ir_tensor_cu_seqlens_q);
  }


  paddle::dialect::IrMetaTensor meta_cu_seqlens_k;
  paddle::dialect::IrTensor ir_tensor_cu_seqlens_k;
  if (cu_seqlens_k_.impl() != nullptr) {
    paddle::dialect::DenseTensorType cu_seqlens_k = cu_seqlens_k_.type().dyn_cast<paddle::dialect::DenseTensorType>();
    VLOG(4) << "Builder construction  dense_cu_seqlens_k";
    ir_tensor_cu_seqlens_k = paddle::dialect::IrTensor(paddle::dialect::TransToPhiDataType(cu_seqlens_k.dtype()),
                                                        cu_seqlens_k.dims(),
                                                        cu_seqlens_k.data_layout(),
                                                        cu_seqlens_k.lod(),
                                                        cu_seqlens_k.offset());
    VLOG(4) << "Builder construction  meta_cu_seqlens_k";
    meta_cu_seqlens_k = paddle::dialect::IrMetaTensor(&ir_tensor_cu_seqlens_k);
  }


  VLOG(4) << "Builder construction  dense_output";
  paddle::dialect::IrTensor ir_tensor_output(paddle::dialect::TransToPhiDataType(output.dtype()),
                                                      output.dims(),
                                                      output.data_layout(),
                                                      output.lod(),
                                                      output.offset());
  VLOG(4) << "Builder construction  meta_output";
  paddle::dialect::IrMetaTensor meta_output(&ir_tensor_output);

  VLOG(4) << "Builder construction  dense_logsumexp";
  paddle::dialect::IrTensor ir_tensor_logsumexp(paddle::dialect::TransToPhiDataType(logsumexp.dtype()),
                                                      logsumexp.dims(),
                                                      logsumexp.data_layout(),
                                                      logsumexp.lod(),
                                                      logsumexp.offset());
  VLOG(4) << "Builder construction  meta_logsumexp";
  paddle::dialect::IrMetaTensor meta_logsumexp(&ir_tensor_logsumexp);

  VLOG(4) << "Builder construction  dense_seed_and_offset";
  paddle::dialect::IrTensor ir_tensor_seed_and_offset(paddle::dialect::TransToPhiDataType(seed_and_offset.dtype()),
                                                      seed_and_offset.dims(),
                                                      seed_and_offset.data_layout(),
                                                      seed_and_offset.lod(),
                                                      seed_and_offset.offset());
  VLOG(4) << "Builder construction  meta_seed_and_offset";
  paddle::dialect::IrMetaTensor meta_seed_and_offset(&ir_tensor_seed_and_offset);

  VLOG(4) << "Builder construction  dense_output_grad";
  paddle::dialect::IrTensor ir_tensor_output_grad(paddle::dialect::TransToPhiDataType(output_grad.dtype()),
                                                      output_grad.dims(),
                                                      output_grad.data_layout(),
                                                      output_grad.lod(),
                                                      output_grad.offset());
  VLOG(4) << "Builder construction  meta_output_grad";
  paddle::dialect::IrMetaTensor meta_output_grad(&ir_tensor_output_grad);
  paddle::dialect::IrTensor dense_query_grad;
  paddle::dialect::IrMetaTensor meta_query_grad(&dense_query_grad);
  paddle::dialect::IrTensor dense_key_grad;
  paddle::dialect::IrMetaTensor meta_key_grad(&dense_key_grad);
  paddle::dialect::IrTensor dense_value_grad;
  paddle::dialect::IrMetaTensor meta_value_grad(&dense_value_grad);
  paddle::dialect::IrTensor dense_bias_grad;
  paddle::dialect::IrMetaTensor meta_bias_grad(&dense_bias_grad);

  phi::MemoryEfficientAttentionGradInferMeta(meta_query, meta_key, meta_value, meta_bias, meta_cu_seqlens_q, meta_cu_seqlens_k, meta_output, meta_logsumexp, meta_seed_and_offset, meta_output_grad, max_seqlen_q, max_seqlen_k, causal, dropout_p, scale, &meta_query_grad, &meta_key_grad, &meta_value_grad, &meta_bias_grad);

  std::vector<pir::Type> argument_outputs;
  pir::Type query_grad_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_query_grad.dtype()), dense_query_grad.dims(), dense_query_grad.layout(), dense_query_grad.lod(), dense_query_grad.offset());
  argument_outputs.push_back(query_grad_dense_tensor_type);

  pir::Type key_grad_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_key_grad.dtype()), dense_key_grad.dims(), dense_key_grad.layout(), dense_key_grad.lod(), dense_key_grad.offset());
  argument_outputs.push_back(key_grad_dense_tensor_type);

  pir::Type value_grad_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_value_grad.dtype()), dense_value_grad.dims(), dense_value_grad.layout(), dense_value_grad.lod(), dense_value_grad.offset());
  argument_outputs.push_back(value_grad_dense_tensor_type);

  pir::Type bias_grad_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_bias_grad.dtype()), dense_bias_grad.dims(), dense_bias_grad.layout(), dense_bias_grad.lod(), dense_bias_grad.offset());
  argument_outputs.push_back(bias_grad_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void MemoryEfficientAttentionGradOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value query_, pir::Value key_, pir::Value value_, pir::Value bias_, pir::Value cu_seqlens_q_, pir::Value cu_seqlens_k_, pir::Value output_, pir::Value logsumexp_, pir::Value seed_and_offset_, pir::Value output_grad_, pir::AttributeMap attributes) {
  VLOG(4) << "Start build MemoryEfficientAttentionGradOp";


  IR_ENFORCE(
      attributes.find("max_seqlen_q") != attributes.end(),
          "'max_seqlen_q' Attribute is expected for MemoryEfficientAttentionGradOp. ");
  float max_seqlen_q = attributes.at("max_seqlen_q").dyn_cast<paddle::dialect::ScalarAttribute>().data().to<float>();

  IR_ENFORCE(
      attributes.find("max_seqlen_k") != attributes.end(),
          "'max_seqlen_k' Attribute is expected for MemoryEfficientAttentionGradOp. ");
  float max_seqlen_k = attributes.at("max_seqlen_k").dyn_cast<paddle::dialect::ScalarAttribute>().data().to<float>();

  IR_ENFORCE(
      attributes.find("causal") != attributes.end(),
          "'causal' Attribute is expected for MemoryEfficientAttentionGradOp. ");
  bool causal = attributes.at("causal").dyn_cast<pir::BoolAttribute>().data();

  IR_ENFORCE(
      attributes.find("dropout_p") != attributes.end(),
          "'dropout_p' Attribute is expected for MemoryEfficientAttentionGradOp. ");
  double dropout_p = attributes.at("dropout_p").dyn_cast<pir::DoubleAttribute>().data();

  IR_ENFORCE(
      attributes.find("scale") != attributes.end(),
          "'scale' Attribute is expected for MemoryEfficientAttentionGradOp. ");
  float scale = attributes.at("scale").dyn_cast<pir::FloatAttribute>().data();


  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {query_, key_, value_, bias_, cu_seqlens_q_, cu_seqlens_k_, output_, logsumexp_, seed_and_offset_, output_grad_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_max_seqlen_q = paddle::dialect::TransToIrAttribute(max_seqlen_q, pir::IrContext::Instance());
  argument.AddAttribute("max_seqlen_q", attr_max_seqlen_q);
  pir::Attribute attr_max_seqlen_k = paddle::dialect::TransToIrAttribute(max_seqlen_k, pir::IrContext::Instance());
  argument.AddAttribute("max_seqlen_k", attr_max_seqlen_k);
  pir::Attribute attr_causal = pir::BoolAttribute::get(pir::IrContext::Instance(), causal);
  argument.AddAttribute("causal", attr_causal);
  pir::Attribute attr_dropout_p = pir::DoubleAttribute::get(pir::IrContext::Instance(), dropout_p);
  argument.AddAttribute("dropout_p", attr_dropout_p);
  pir::Attribute attr_scale = pir::FloatAttribute::get(pir::IrContext::Instance(), scale);
  argument.AddAttribute("scale", attr_scale);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType query = query_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)query;
  paddle::dialect::DenseTensorType key = key_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)key;
  paddle::dialect::DenseTensorType value = value_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)value;
  paddle::dialect::DenseTensorType output = output_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)output;
  paddle::dialect::DenseTensorType logsumexp = logsumexp_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)logsumexp;
  paddle::dialect::DenseTensorType seed_and_offset = seed_and_offset_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)seed_and_offset;
  paddle::dialect::DenseTensorType output_grad = output_grad_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)output_grad;

  VLOG(4) << "Builder construction  dense_query";
  paddle::dialect::IrTensor ir_tensor_query(paddle::dialect::TransToPhiDataType(query.dtype()),
                                                      query.dims(),
                                                      query.data_layout(),
                                                      query.lod(),
                                                      query.offset());
  VLOG(4) << "Builder construction  meta_query";
  paddle::dialect::IrMetaTensor meta_query(&ir_tensor_query);

  VLOG(4) << "Builder construction  dense_key";
  paddle::dialect::IrTensor ir_tensor_key(paddle::dialect::TransToPhiDataType(key.dtype()),
                                                      key.dims(),
                                                      key.data_layout(),
                                                      key.lod(),
                                                      key.offset());
  VLOG(4) << "Builder construction  meta_key";
  paddle::dialect::IrMetaTensor meta_key(&ir_tensor_key);

  VLOG(4) << "Builder construction  dense_value";
  paddle::dialect::IrTensor ir_tensor_value(paddle::dialect::TransToPhiDataType(value.dtype()),
                                                      value.dims(),
                                                      value.data_layout(),
                                                      value.lod(),
                                                      value.offset());
  VLOG(4) << "Builder construction  meta_value";
  paddle::dialect::IrMetaTensor meta_value(&ir_tensor_value);

  paddle::dialect::IrMetaTensor meta_bias;
  paddle::dialect::IrTensor ir_tensor_bias;
  if (bias_.impl() != nullptr) {
    paddle::dialect::DenseTensorType bias = bias_.type().dyn_cast<paddle::dialect::DenseTensorType>();
    VLOG(4) << "Builder construction  dense_bias";
    ir_tensor_bias = paddle::dialect::IrTensor(paddle::dialect::TransToPhiDataType(bias.dtype()),
                                                        bias.dims(),
                                                        bias.data_layout(),
                                                        bias.lod(),
                                                        bias.offset());
    VLOG(4) << "Builder construction  meta_bias";
    meta_bias = paddle::dialect::IrMetaTensor(&ir_tensor_bias);
  }


  paddle::dialect::IrMetaTensor meta_cu_seqlens_q;
  paddle::dialect::IrTensor ir_tensor_cu_seqlens_q;
  if (cu_seqlens_q_.impl() != nullptr) {
    paddle::dialect::DenseTensorType cu_seqlens_q = cu_seqlens_q_.type().dyn_cast<paddle::dialect::DenseTensorType>();
    VLOG(4) << "Builder construction  dense_cu_seqlens_q";
    ir_tensor_cu_seqlens_q = paddle::dialect::IrTensor(paddle::dialect::TransToPhiDataType(cu_seqlens_q.dtype()),
                                                        cu_seqlens_q.dims(),
                                                        cu_seqlens_q.data_layout(),
                                                        cu_seqlens_q.lod(),
                                                        cu_seqlens_q.offset());
    VLOG(4) << "Builder construction  meta_cu_seqlens_q";
    meta_cu_seqlens_q = paddle::dialect::IrMetaTensor(&ir_tensor_cu_seqlens_q);
  }


  paddle::dialect::IrMetaTensor meta_cu_seqlens_k;
  paddle::dialect::IrTensor ir_tensor_cu_seqlens_k;
  if (cu_seqlens_k_.impl() != nullptr) {
    paddle::dialect::DenseTensorType cu_seqlens_k = cu_seqlens_k_.type().dyn_cast<paddle::dialect::DenseTensorType>();
    VLOG(4) << "Builder construction  dense_cu_seqlens_k";
    ir_tensor_cu_seqlens_k = paddle::dialect::IrTensor(paddle::dialect::TransToPhiDataType(cu_seqlens_k.dtype()),
                                                        cu_seqlens_k.dims(),
                                                        cu_seqlens_k.data_layout(),
                                                        cu_seqlens_k.lod(),
                                                        cu_seqlens_k.offset());
    VLOG(4) << "Builder construction  meta_cu_seqlens_k";
    meta_cu_seqlens_k = paddle::dialect::IrMetaTensor(&ir_tensor_cu_seqlens_k);
  }


  VLOG(4) << "Builder construction  dense_output";
  paddle::dialect::IrTensor ir_tensor_output(paddle::dialect::TransToPhiDataType(output.dtype()),
                                                      output.dims(),
                                                      output.data_layout(),
                                                      output.lod(),
                                                      output.offset());
  VLOG(4) << "Builder construction  meta_output";
  paddle::dialect::IrMetaTensor meta_output(&ir_tensor_output);

  VLOG(4) << "Builder construction  dense_logsumexp";
  paddle::dialect::IrTensor ir_tensor_logsumexp(paddle::dialect::TransToPhiDataType(logsumexp.dtype()),
                                                      logsumexp.dims(),
                                                      logsumexp.data_layout(),
                                                      logsumexp.lod(),
                                                      logsumexp.offset());
  VLOG(4) << "Builder construction  meta_logsumexp";
  paddle::dialect::IrMetaTensor meta_logsumexp(&ir_tensor_logsumexp);

  VLOG(4) << "Builder construction  dense_seed_and_offset";
  paddle::dialect::IrTensor ir_tensor_seed_and_offset(paddle::dialect::TransToPhiDataType(seed_and_offset.dtype()),
                                                      seed_and_offset.dims(),
                                                      seed_and_offset.data_layout(),
                                                      seed_and_offset.lod(),
                                                      seed_and_offset.offset());
  VLOG(4) << "Builder construction  meta_seed_and_offset";
  paddle::dialect::IrMetaTensor meta_seed_and_offset(&ir_tensor_seed_and_offset);

  VLOG(4) << "Builder construction  dense_output_grad";
  paddle::dialect::IrTensor ir_tensor_output_grad(paddle::dialect::TransToPhiDataType(output_grad.dtype()),
                                                      output_grad.dims(),
                                                      output_grad.data_layout(),
                                                      output_grad.lod(),
                                                      output_grad.offset());
  VLOG(4) << "Builder construction  meta_output_grad";
  paddle::dialect::IrMetaTensor meta_output_grad(&ir_tensor_output_grad);
  paddle::dialect::IrTensor dense_query_grad;
  paddle::dialect::IrMetaTensor meta_query_grad(&dense_query_grad);
  paddle::dialect::IrTensor dense_key_grad;
  paddle::dialect::IrMetaTensor meta_key_grad(&dense_key_grad);
  paddle::dialect::IrTensor dense_value_grad;
  paddle::dialect::IrMetaTensor meta_value_grad(&dense_value_grad);
  paddle::dialect::IrTensor dense_bias_grad;
  paddle::dialect::IrMetaTensor meta_bias_grad(&dense_bias_grad);

  phi::MemoryEfficientAttentionGradInferMeta(meta_query, meta_key, meta_value, meta_bias, meta_cu_seqlens_q, meta_cu_seqlens_k, meta_output, meta_logsumexp, meta_seed_and_offset, meta_output_grad, max_seqlen_q, max_seqlen_k, causal, dropout_p, scale, &meta_query_grad, &meta_key_grad, &meta_value_grad, &meta_bias_grad);

  std::vector<pir::Type> argument_outputs;
  pir::Type query_grad_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_query_grad.dtype()), dense_query_grad.dims(), dense_query_grad.layout(), dense_query_grad.lod(), dense_query_grad.offset());
  argument_outputs.push_back(query_grad_dense_tensor_type);

  pir::Type key_grad_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_key_grad.dtype()), dense_key_grad.dims(), dense_key_grad.layout(), dense_key_grad.lod(), dense_key_grad.offset());
  argument_outputs.push_back(key_grad_dense_tensor_type);

  pir::Type value_grad_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_value_grad.dtype()), dense_value_grad.dims(), dense_value_grad.layout(), dense_value_grad.lod(), dense_value_grad.offset());
  argument_outputs.push_back(value_grad_dense_tensor_type);

  pir::Type bias_grad_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_bias_grad.dtype()), dense_bias_grad.dims(), dense_bias_grad.layout(), dense_bias_grad.lod(), dense_bias_grad.offset());
  argument_outputs.push_back(bias_grad_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void MemoryEfficientAttentionGradOp::VerifySig() {}

void MemoryEfficientAttentionGradOp::InferMeta( phi::InferMetaContext *infer_meta ) {
  auto fn = PD_INFER_META(phi::MemoryEfficientAttentionGradInferMeta);
  fn(infer_meta);
}

phi::DataType MemoryEfficientAttentionGradOp::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: MemoryEfficientAttentionGradOp";
  


  return expected_kernel_dtype;
}

OpInfoTuple MeshgridGradOp::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("inputs", "pir::VectorType<paddle::dialect::DenseTensorType>", false, false, false, false), paddle::dialect::OpInputInfo("outputs_grad", "pir::VectorType<paddle::dialect::DenseTensorType>", false, false, false, false) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = {  };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("inputs_grad", "pir::VectorType<paddle::dialect::DenseTensorType>", false, false) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("MeshgridGradInferMeta", {"inputs", "outputs_grad"}, "meshgrid_grad", {"inputs", "outputs_grad"}, {"outputs_grad"}, {}, {}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "meshgrid_grad");
}

void MeshgridGradOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value inputs_, pir::Value outputs_grad_) {
  VLOG(4) << "Start build MeshgridGradOp";



  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {inputs_, outputs_grad_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";

  VLOG(4) << "Builder construction outputs";
  pir::VectorType inputs = inputs_.type().dyn_cast<pir::VectorType>(); (void)inputs;
  pir::VectorType outputs_grad = outputs_grad_.type().dyn_cast<pir::VectorType>(); (void)outputs_grad;
  std::vector<paddle::dialect::IrTensor> vec_ir_tensor_inputs;
  for (size_t i=0; i < static_cast<size_t>(inputs.size()); i++) {
    vec_ir_tensor_inputs.push_back(paddle::dialect::IrTensor(paddle::dialect::TransToPhiDataType(inputs[i].dyn_cast<paddle::dialect::DenseTensorType>().dtype()),
                                                                     inputs[i].dyn_cast<paddle::dialect::DenseTensorType>().dims(),
                                                                     inputs[i].dyn_cast<paddle::dialect::DenseTensorType>().data_layout(),
                                                                     inputs[i].dyn_cast<paddle::dialect::DenseTensorType>().lod(),
                                                                     inputs[i].dyn_cast<paddle::dialect::DenseTensorType>().offset()));
  }
  std::vector<paddle::dialect::IrMetaTensor> vec_meta_inputs;
  for (size_t i=0; i < vec_ir_tensor_inputs.size(); i++) {
    vec_meta_inputs.push_back(paddle::dialect::IrMetaTensor(&vec_ir_tensor_inputs[i]));
  }

  std::vector<const phi::MetaTensor*> meta_inputs;
  for (size_t i=0; i < static_cast<size_t>(vec_meta_inputs.size()); i++) {
    meta_inputs.push_back(&vec_meta_inputs[i]);
  }
   std::vector<paddle::dialect::IrTensor> vec_ir_tensor_outputs_grad;
  for (size_t i=0; i < static_cast<size_t>(outputs_grad.size()); i++) {
    vec_ir_tensor_outputs_grad.push_back(paddle::dialect::IrTensor(paddle::dialect::TransToPhiDataType(outputs_grad[i].dyn_cast<paddle::dialect::DenseTensorType>().dtype()),
                                                                     outputs_grad[i].dyn_cast<paddle::dialect::DenseTensorType>().dims(),
                                                                     outputs_grad[i].dyn_cast<paddle::dialect::DenseTensorType>().data_layout(),
                                                                     outputs_grad[i].dyn_cast<paddle::dialect::DenseTensorType>().lod(),
                                                                     outputs_grad[i].dyn_cast<paddle::dialect::DenseTensorType>().offset()));
  }
  std::vector<paddle::dialect::IrMetaTensor> vec_meta_outputs_grad;
  for (size_t i=0; i < vec_ir_tensor_outputs_grad.size(); i++) {
    vec_meta_outputs_grad.push_back(paddle::dialect::IrMetaTensor(&vec_ir_tensor_outputs_grad[i]));
  }

  std::vector<const phi::MetaTensor*> meta_outputs_grad;
  for (size_t i=0; i < static_cast<size_t>(vec_meta_outputs_grad.size()); i++) {
    meta_outputs_grad.push_back(&vec_meta_outputs_grad[i]);
  }
   std::vector<paddle::dialect::IrTensor> vec_dense_inputs_grad((inputs.size()), paddle::dialect::IrTensor());
  std::vector<paddle::dialect::IrMetaTensor> vec_meta_inputs_grad;
  for (size_t i=0; i < static_cast<size_t>(inputs.size()); i++) {
    vec_meta_inputs_grad.push_back(paddle::dialect::IrMetaTensor(&vec_dense_inputs_grad[i]));
  }
  std::vector<phi::MetaTensor*> meta_inputs_grad;
  for (size_t i=0; i < static_cast<size_t>(vec_meta_inputs_grad.size()); i++) {
    meta_inputs_grad.push_back(&vec_meta_inputs_grad[i]);
  }

  phi::MeshgridGradInferMeta(meta_inputs, meta_outputs_grad, meta_inputs_grad);

  std::vector<pir::Type> argument_outputs;
  std::vector<pir::Type> inputs_grad_types;
  for (size_t i=0; i < static_cast<size_t>(inputs.size()); i++) {
    inputs_grad_types.push_back(paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(vec_dense_inputs_grad[i].dtype()), vec_dense_inputs_grad[i].dims(), vec_dense_inputs_grad[i].layout(), vec_dense_inputs_grad[i].lod(), vec_dense_inputs_grad[i].offset()));
  }
  pir::Type inputs_grad_vector_type = pir::VectorType::get(pir::IrContext::Instance(), inputs_grad_types);
  argument_outputs.push_back(inputs_grad_vector_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void MeshgridGradOp::VerifySig() {}

void MeshgridGradOp::InferMeta( phi::InferMetaContext *infer_meta ) {
  auto fn = PD_INFER_META(phi::MeshgridGradInferMeta);
  fn(infer_meta);
}

phi::DataType MeshgridGradOp::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: MeshgridGradOp";
  


  return expected_kernel_dtype;
}

const char *ModeGradOp::attributes_name[2] = { "axis", "keepdim" };

OpInfoTuple ModeGradOp::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("x", "paddle::dialect::DenseTensorType", false, false, false, false), paddle::dialect::OpInputInfo("indices", "paddle::dialect::DenseTensorType", false, false, false, false), paddle::dialect::OpInputInfo("out_grad", "paddle::dialect::DenseTensorType", false, false, false, false) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = { paddle::dialect::OpAttributeInfo("axis", "pir::Int32Attribute", ""), paddle::dialect::OpAttributeInfo("keepdim", "pir::BoolAttribute", "") };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("x_grad", "paddle::dialect::DenseTensorType", false, false) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("UnchangedInferMeta", {"x"}, "mode_grad", {"x", "indices", "out_grad", "axis", "keepdim"}, {}, {}, {}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "mode_grad");
}

void ModeGradOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, pir::Value indices_, pir::Value out_grad_, int axis, bool keepdim) {
  VLOG(4) << "Start build ModeGradOp";



  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_, indices_, out_grad_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_axis = pir::Int32Attribute::get(pir::IrContext::Instance(), axis);
  argument.AddAttribute("axis", attr_axis);
  pir::Attribute attr_keepdim = pir::BoolAttribute::get(pir::IrContext::Instance(), keepdim);
  argument.AddAttribute("keepdim", attr_keepdim);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;
  paddle::dialect::DenseTensorType indices = indices_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)indices;
  paddle::dialect::DenseTensorType out_grad = out_grad_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)out_grad;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);
  paddle::dialect::IrTensor dense_x_grad;
  paddle::dialect::IrMetaTensor meta_x_grad(&dense_x_grad);

  phi::UnchangedInferMeta(meta_x, &meta_x_grad);

  std::vector<pir::Type> argument_outputs;
  pir::Type x_grad_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_x_grad.dtype()), dense_x_grad.dims(), dense_x_grad.layout(), dense_x_grad.lod(), dense_x_grad.offset());
  argument_outputs.push_back(x_grad_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void ModeGradOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, pir::Value indices_, pir::Value out_grad_, pir::AttributeMap attributes) {
  VLOG(4) << "Start build ModeGradOp";


  IR_ENFORCE(
      attributes.find("axis") != attributes.end(),
          "'axis' Attribute is expected for ModeGradOp. ");
  int axis = attributes.at("axis").dyn_cast<pir::Int32Attribute>().data();

  IR_ENFORCE(
      attributes.find("keepdim") != attributes.end(),
          "'keepdim' Attribute is expected for ModeGradOp. ");
  bool keepdim = attributes.at("keepdim").dyn_cast<pir::BoolAttribute>().data();


  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_, indices_, out_grad_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_axis = pir::Int32Attribute::get(pir::IrContext::Instance(), axis);
  argument.AddAttribute("axis", attr_axis);
  pir::Attribute attr_keepdim = pir::BoolAttribute::get(pir::IrContext::Instance(), keepdim);
  argument.AddAttribute("keepdim", attr_keepdim);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;
  paddle::dialect::DenseTensorType indices = indices_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)indices;
  paddle::dialect::DenseTensorType out_grad = out_grad_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)out_grad;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);
  paddle::dialect::IrTensor dense_x_grad;
  paddle::dialect::IrMetaTensor meta_x_grad(&dense_x_grad);

  phi::UnchangedInferMeta(meta_x, &meta_x_grad);

  std::vector<pir::Type> argument_outputs;
  pir::Type x_grad_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_x_grad.dtype()), dense_x_grad.dims(), dense_x_grad.layout(), dense_x_grad.lod(), dense_x_grad.offset());
  argument_outputs.push_back(x_grad_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void ModeGradOp::VerifySig() {}

void ModeGradOp::InferMeta( phi::InferMetaContext *infer_meta ) {
  auto fn = PD_INFER_META(phi::UnchangedInferMeta);
  fn(infer_meta);
}

phi::DataType ModeGradOp::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: ModeGradOp";
  


  return expected_kernel_dtype;
}

OpInfoTuple MultiDotGradOp::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("x", "pir::VectorType<paddle::dialect::DenseTensorType>", false, false, false, false), paddle::dialect::OpInputInfo("out_grad", "paddle::dialect::DenseTensorType", false, false, false, false) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = {  };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("x_grad", "pir::VectorType<paddle::dialect::DenseTensorType>", false, false) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("MultiDotGradInferMeta", {"x", "out_grad"}, "multi_dot_grad", {"x", "out_grad"}, {}, {}, {}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "multi_dot_grad");
}

void MultiDotGradOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, pir::Value out_grad_) {
  VLOG(4) << "Start build MultiDotGradOp";



  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_, out_grad_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";

  VLOG(4) << "Builder construction outputs";
  pir::VectorType x = x_.type().dyn_cast<pir::VectorType>(); (void)x;
  paddle::dialect::DenseTensorType out_grad = out_grad_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)out_grad;
  std::vector<paddle::dialect::IrTensor> vec_ir_tensor_x;
  for (size_t i=0; i < static_cast<size_t>(x.size()); i++) {
    vec_ir_tensor_x.push_back(paddle::dialect::IrTensor(paddle::dialect::TransToPhiDataType(x[i].dyn_cast<paddle::dialect::DenseTensorType>().dtype()),
                                                                     x[i].dyn_cast<paddle::dialect::DenseTensorType>().dims(),
                                                                     x[i].dyn_cast<paddle::dialect::DenseTensorType>().data_layout(),
                                                                     x[i].dyn_cast<paddle::dialect::DenseTensorType>().lod(),
                                                                     x[i].dyn_cast<paddle::dialect::DenseTensorType>().offset()));
  }
  std::vector<paddle::dialect::IrMetaTensor> vec_meta_x;
  for (size_t i=0; i < vec_ir_tensor_x.size(); i++) {
    vec_meta_x.push_back(paddle::dialect::IrMetaTensor(&vec_ir_tensor_x[i]));
  }

  std::vector<const phi::MetaTensor*> meta_x;
  for (size_t i=0; i < static_cast<size_t>(vec_meta_x.size()); i++) {
    meta_x.push_back(&vec_meta_x[i]);
  }
 
  VLOG(4) << "Builder construction  dense_out_grad";
  paddle::dialect::IrTensor ir_tensor_out_grad(paddle::dialect::TransToPhiDataType(out_grad.dtype()),
                                                      out_grad.dims(),
                                                      out_grad.data_layout(),
                                                      out_grad.lod(),
                                                      out_grad.offset());
  VLOG(4) << "Builder construction  meta_out_grad";
  paddle::dialect::IrMetaTensor meta_out_grad(&ir_tensor_out_grad);
  std::vector<paddle::dialect::IrTensor> vec_dense_x_grad((x.size()), paddle::dialect::IrTensor());
  std::vector<paddle::dialect::IrMetaTensor> vec_meta_x_grad;
  for (size_t i=0; i < static_cast<size_t>(x.size()); i++) {
    vec_meta_x_grad.push_back(paddle::dialect::IrMetaTensor(&vec_dense_x_grad[i]));
  }
  std::vector<phi::MetaTensor*> meta_x_grad;
  for (size_t i=0; i < static_cast<size_t>(vec_meta_x_grad.size()); i++) {
    meta_x_grad.push_back(&vec_meta_x_grad[i]);
  }

  phi::MultiDotGradInferMeta(meta_x, meta_out_grad, meta_x_grad);

  std::vector<pir::Type> argument_outputs;
  std::vector<pir::Type> x_grad_types;
  for (size_t i=0; i < static_cast<size_t>(x.size()); i++) {
    x_grad_types.push_back(paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(vec_dense_x_grad[i].dtype()), vec_dense_x_grad[i].dims(), vec_dense_x_grad[i].layout(), vec_dense_x_grad[i].lod(), vec_dense_x_grad[i].offset()));
  }
  pir::Type x_grad_vector_type = pir::VectorType::get(pir::IrContext::Instance(), x_grad_types);
  argument_outputs.push_back(x_grad_vector_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void MultiDotGradOp::VerifySig() {}

void MultiDotGradOp::InferMeta( phi::InferMetaContext *infer_meta ) {
  auto fn = PD_INFER_META(phi::MultiDotGradInferMeta);
  fn(infer_meta);
}

phi::DataType MultiDotGradOp::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: MultiDotGradOp";
  


  return expected_kernel_dtype;
}

OpInfoTuple MultiplexGradOp::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("inputs", "pir::VectorType<paddle::dialect::DenseTensorType>", false, false, false, false), paddle::dialect::OpInputInfo("index", "paddle::dialect::DenseTensorType", false, false, false, false), paddle::dialect::OpInputInfo("out_grad", "paddle::dialect::DenseTensorType", false, false, false, false) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = {  };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("inputs_grad", "pir::VectorType<paddle::dialect::DenseTensorType>", false, false) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("MultiplexGradInferMeta", {"index", "out_grad"}, "multiplex_grad", {"index", "out_grad"}, {"out_grad"}, {}, {}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "multiplex_grad");
}

void MultiplexGradOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value inputs_, pir::Value index_, pir::Value out_grad_) {
  VLOG(4) << "Start build MultiplexGradOp";



  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {inputs_, index_, out_grad_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";

  VLOG(4) << "Builder construction outputs";
  pir::VectorType inputs = inputs_.type().dyn_cast<pir::VectorType>(); (void)inputs;
  paddle::dialect::DenseTensorType index = index_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)index;
  paddle::dialect::DenseTensorType out_grad = out_grad_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)out_grad;

  VLOG(4) << "Builder construction  dense_index";
  paddle::dialect::IrTensor ir_tensor_index(paddle::dialect::TransToPhiDataType(index.dtype()),
                                                      index.dims(),
                                                      index.data_layout(),
                                                      index.lod(),
                                                      index.offset());
  VLOG(4) << "Builder construction  meta_index";
  paddle::dialect::IrMetaTensor meta_index(&ir_tensor_index);

  VLOG(4) << "Builder construction  dense_out_grad";
  paddle::dialect::IrTensor ir_tensor_out_grad(paddle::dialect::TransToPhiDataType(out_grad.dtype()),
                                                      out_grad.dims(),
                                                      out_grad.data_layout(),
                                                      out_grad.lod(),
                                                      out_grad.offset());
  VLOG(4) << "Builder construction  meta_out_grad";
  paddle::dialect::IrMetaTensor meta_out_grad(&ir_tensor_out_grad);
  std::vector<paddle::dialect::IrTensor> vec_dense_inputs_grad((inputs.size()), paddle::dialect::IrTensor());
  std::vector<paddle::dialect::IrMetaTensor> vec_meta_inputs_grad;
  for (size_t i=0; i < static_cast<size_t>(inputs.size()); i++) {
    vec_meta_inputs_grad.push_back(paddle::dialect::IrMetaTensor(&vec_dense_inputs_grad[i]));
  }
  std::vector<phi::MetaTensor*> meta_inputs_grad;
  for (size_t i=0; i < static_cast<size_t>(vec_meta_inputs_grad.size()); i++) {
    meta_inputs_grad.push_back(&vec_meta_inputs_grad[i]);
  }

  phi::MultiplexGradInferMeta(meta_index, meta_out_grad, meta_inputs_grad);

  std::vector<pir::Type> argument_outputs;
  std::vector<pir::Type> inputs_grad_types;
  for (size_t i=0; i < static_cast<size_t>(inputs.size()); i++) {
    inputs_grad_types.push_back(paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(vec_dense_inputs_grad[i].dtype()), vec_dense_inputs_grad[i].dims(), vec_dense_inputs_grad[i].layout(), vec_dense_inputs_grad[i].lod(), vec_dense_inputs_grad[i].offset()));
  }
  pir::Type inputs_grad_vector_type = pir::VectorType::get(pir::IrContext::Instance(), inputs_grad_types);
  argument_outputs.push_back(inputs_grad_vector_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void MultiplexGradOp::VerifySig() {}

void MultiplexGradOp::InferMeta( phi::InferMetaContext *infer_meta ) {
  auto fn = PD_INFER_META(phi::MultiplexGradInferMeta);
  fn(infer_meta);
}

phi::DataType MultiplexGradOp::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: MultiplexGradOp";
  

  // deal skip data transform
  if (var_name == "index"){
    return expected_kernel_dtype;
  }


  return expected_kernel_dtype;
}

OpInfoTuple MvGradOp::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("x", "paddle::dialect::DenseTensorType", false, false, false, false), paddle::dialect::OpInputInfo("vec", "paddle::dialect::DenseTensorType", false, false, false, false), paddle::dialect::OpInputInfo("out_grad", "paddle::dialect::DenseTensorType", false, false, false, false) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = {  };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("x_grad", "paddle::dialect::DenseTensorType", false, false), paddle::dialect::OpOutputInfo("vec_grad", "paddle::dialect::DenseTensorType", false, false) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("GeneralBinaryGradInferMeta", {"x", "vec"}, "mv_grad", {"x", "vec", "out_grad"}, {}, {}, {}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "mv_grad");
}

void MvGradOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, pir::Value vec_, pir::Value out_grad_) {
  VLOG(4) << "Start build MvGradOp";



  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_, vec_, out_grad_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;
  paddle::dialect::DenseTensorType vec = vec_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)vec;
  paddle::dialect::DenseTensorType out_grad = out_grad_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)out_grad;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);

  VLOG(4) << "Builder construction  dense_vec";
  paddle::dialect::IrTensor ir_tensor_vec(paddle::dialect::TransToPhiDataType(vec.dtype()),
                                                      vec.dims(),
                                                      vec.data_layout(),
                                                      vec.lod(),
                                                      vec.offset());
  VLOG(4) << "Builder construction  meta_vec";
  paddle::dialect::IrMetaTensor meta_vec(&ir_tensor_vec);
  paddle::dialect::IrTensor dense_x_grad;
  paddle::dialect::IrMetaTensor meta_x_grad(&dense_x_grad);
  paddle::dialect::IrTensor dense_vec_grad;
  paddle::dialect::IrMetaTensor meta_vec_grad(&dense_vec_grad);

  phi::GeneralBinaryGradInferMeta(meta_x, meta_vec, &meta_x_grad, &meta_vec_grad);

  std::vector<pir::Type> argument_outputs;
  pir::Type x_grad_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_x_grad.dtype()), dense_x_grad.dims(), dense_x_grad.layout(), dense_x_grad.lod(), dense_x_grad.offset());
  argument_outputs.push_back(x_grad_dense_tensor_type);

  pir::Type vec_grad_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_vec_grad.dtype()), dense_vec_grad.dims(), dense_vec_grad.layout(), dense_vec_grad.lod(), dense_vec_grad.offset());
  argument_outputs.push_back(vec_grad_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void MvGradOp::VerifySig() {}

void MvGradOp::InferMeta( phi::InferMetaContext *infer_meta ) {
  auto fn = PD_INFER_META(phi::GeneralBinaryGradInferMeta);
  fn(infer_meta);
}

phi::DataType MvGradOp::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: MvGradOp";
  


  return expected_kernel_dtype;
}

const char *NanmedianGradOp::attributes_name[2] = { "axis", "keepdim" };

OpInfoTuple NanmedianGradOp::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("x", "paddle::dialect::DenseTensorType", false, false, false, false), paddle::dialect::OpInputInfo("medians", "paddle::dialect::DenseTensorType", false, false, false, false), paddle::dialect::OpInputInfo("out_grad", "paddle::dialect::DenseTensorType", false, false, false, false) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = { paddle::dialect::OpAttributeInfo("axis", "paddle::dialect::IntArrayAttribute", "std::vector<int64_t>"), paddle::dialect::OpAttributeInfo("keepdim", "pir::BoolAttribute", "") };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("x_grad", "paddle::dialect::DenseTensorType", false, false) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("NanmedianGradInferMeta", {"x", "medians", "out_grad", "axis", "keepdim"}, "nanmedian_grad", {"x", "medians", "out_grad", "axis", "keepdim"}, {}, {}, {}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "nanmedian_grad");
}

void NanmedianGradOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, pir::Value medians_, pir::Value out_grad_, const std::vector<int64_t>& axis, bool keepdim) {
  VLOG(4) << "Start build NanmedianGradOp";



  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_, medians_, out_grad_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_axis = paddle::dialect::IntArrayAttribute::get(pir::IrContext::Instance(), phi::IntArray(axis));
  argument.AddAttribute("axis", attr_axis);
  pir::Attribute attr_keepdim = pir::BoolAttribute::get(pir::IrContext::Instance(), keepdim);
  argument.AddAttribute("keepdim", attr_keepdim);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;
  paddle::dialect::DenseTensorType medians = medians_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)medians;
  paddle::dialect::DenseTensorType out_grad = out_grad_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)out_grad;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);

  VLOG(4) << "Builder construction  dense_medians";
  paddle::dialect::IrTensor ir_tensor_medians(paddle::dialect::TransToPhiDataType(medians.dtype()),
                                                      medians.dims(),
                                                      medians.data_layout(),
                                                      medians.lod(),
                                                      medians.offset());
  VLOG(4) << "Builder construction  meta_medians";
  paddle::dialect::IrMetaTensor meta_medians(&ir_tensor_medians);

  VLOG(4) << "Builder construction  dense_out_grad";
  paddle::dialect::IrTensor ir_tensor_out_grad(paddle::dialect::TransToPhiDataType(out_grad.dtype()),
                                                      out_grad.dims(),
                                                      out_grad.data_layout(),
                                                      out_grad.lod(),
                                                      out_grad.offset());
  VLOG(4) << "Builder construction  meta_out_grad";
  paddle::dialect::IrMetaTensor meta_out_grad(&ir_tensor_out_grad);
  paddle::dialect::IrTensor dense_x_grad;
  paddle::dialect::IrMetaTensor meta_x_grad(&dense_x_grad);

  phi::NanmedianGradInferMeta(meta_x, meta_medians, meta_out_grad, axis, keepdim, &meta_x_grad);

  std::vector<pir::Type> argument_outputs;
  pir::Type x_grad_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_x_grad.dtype()), dense_x_grad.dims(), dense_x_grad.layout(), dense_x_grad.lod(), dense_x_grad.offset());
  argument_outputs.push_back(x_grad_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void NanmedianGradOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, pir::Value medians_, pir::Value out_grad_, pir::AttributeMap attributes) {
  VLOG(4) << "Start build NanmedianGradOp";


  IR_ENFORCE(
      attributes.find("axis") != attributes.end(),
          "'axis' Attribute is expected for NanmedianGradOp. ");
  std::vector<int64_t> axis = attributes.at("axis").dyn_cast<paddle::dialect::IntArrayAttribute>().data().GetData();

  IR_ENFORCE(
      attributes.find("keepdim") != attributes.end(),
          "'keepdim' Attribute is expected for NanmedianGradOp. ");
  bool keepdim = attributes.at("keepdim").dyn_cast<pir::BoolAttribute>().data();


  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_, medians_, out_grad_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_axis = paddle::dialect::IntArrayAttribute::get(pir::IrContext::Instance(), phi::IntArray(axis));
  argument.AddAttribute("axis", attr_axis);
  pir::Attribute attr_keepdim = pir::BoolAttribute::get(pir::IrContext::Instance(), keepdim);
  argument.AddAttribute("keepdim", attr_keepdim);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;
  paddle::dialect::DenseTensorType medians = medians_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)medians;
  paddle::dialect::DenseTensorType out_grad = out_grad_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)out_grad;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);

  VLOG(4) << "Builder construction  dense_medians";
  paddle::dialect::IrTensor ir_tensor_medians(paddle::dialect::TransToPhiDataType(medians.dtype()),
                                                      medians.dims(),
                                                      medians.data_layout(),
                                                      medians.lod(),
                                                      medians.offset());
  VLOG(4) << "Builder construction  meta_medians";
  paddle::dialect::IrMetaTensor meta_medians(&ir_tensor_medians);

  VLOG(4) << "Builder construction  dense_out_grad";
  paddle::dialect::IrTensor ir_tensor_out_grad(paddle::dialect::TransToPhiDataType(out_grad.dtype()),
                                                      out_grad.dims(),
                                                      out_grad.data_layout(),
                                                      out_grad.lod(),
                                                      out_grad.offset());
  VLOG(4) << "Builder construction  meta_out_grad";
  paddle::dialect::IrMetaTensor meta_out_grad(&ir_tensor_out_grad);
  paddle::dialect::IrTensor dense_x_grad;
  paddle::dialect::IrMetaTensor meta_x_grad(&dense_x_grad);

  phi::NanmedianGradInferMeta(meta_x, meta_medians, meta_out_grad, axis, keepdim, &meta_x_grad);

  std::vector<pir::Type> argument_outputs;
  pir::Type x_grad_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_x_grad.dtype()), dense_x_grad.dims(), dense_x_grad.layout(), dense_x_grad.lod(), dense_x_grad.offset());
  argument_outputs.push_back(x_grad_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void NanmedianGradOp::VerifySig() {}

void NanmedianGradOp::InferMeta( phi::InferMetaContext *infer_meta ) {
  auto fn = PD_INFER_META(phi::NanmedianGradInferMeta);
  fn(infer_meta);
}

phi::DataType NanmedianGradOp::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: NanmedianGradOp";
  


  return expected_kernel_dtype;
}

const char *NearestInterpGradOp::attributes_name[8] = { "data_layout", "out_d", "out_h", "out_w", "scale", "interp_method", "align_corners", "align_mode" };

OpInfoTuple NearestInterpGradOp::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("x", "paddle::dialect::DenseTensorType", false, true, false, false), paddle::dialect::OpInputInfo("out_size", "paddle::dialect::DenseTensorType", true, false, false, false), paddle::dialect::OpInputInfo("size_tensor", "pir::VectorType<paddle::dialect::DenseTensorType>", true, false, false, false), paddle::dialect::OpInputInfo("scale_tensor", "paddle::dialect::DenseTensorType", true, false, false, false), paddle::dialect::OpInputInfo("output_grad", "paddle::dialect::DenseTensorType", false, false, false, false) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = { paddle::dialect::OpAttributeInfo("data_layout", "pir::StrAttribute", ""), paddle::dialect::OpAttributeInfo("out_d", "pir::Int32Attribute", ""), paddle::dialect::OpAttributeInfo("out_h", "pir::Int32Attribute", ""), paddle::dialect::OpAttributeInfo("out_w", "pir::Int32Attribute", ""), paddle::dialect::OpAttributeInfo("scale", "pir::ArrayAttribute<pir::FloatAttribute>", ""), paddle::dialect::OpAttributeInfo("interp_method", "pir::StrAttribute", ""), paddle::dialect::OpAttributeInfo("align_corners", "pir::BoolAttribute", ""), paddle::dialect::OpAttributeInfo("align_mode", "pir::Int32Attribute", "") };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("x_grad", "paddle::dialect::DenseTensorType", false, false) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("UnchangedInferMeta", {"x"}, "nearest_interp_grad", {"x", "out_size", "size_tensor", "scale_tensor", "output_grad", "data_layout", "out_d", "out_h", "out_w", "scale", "interp_method", "align_corners", "align_mode"}, {"output_grad"}, {}, {}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "nearest_interp_grad");
}

void NearestInterpGradOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, pir::Value out_size_, pir::Value size_tensor_, pir::Value scale_tensor_, pir::Value output_grad_, const std::string& data_layout, int out_d, int out_h, int out_w, const std::vector<float>& scale, const std::string& interp_method, bool align_corners, int align_mode) {
  VLOG(4) << "Start build NearestInterpGradOp";



  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_, out_size_, size_tensor_, scale_tensor_, output_grad_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_data_layout = pir::StrAttribute::get(pir::IrContext::Instance(), data_layout);
  argument.AddAttribute("data_layout", attr_data_layout);
  pir::Attribute attr_out_d = pir::Int32Attribute::get(pir::IrContext::Instance(), out_d);
  argument.AddAttribute("out_d", attr_out_d);
  pir::Attribute attr_out_h = pir::Int32Attribute::get(pir::IrContext::Instance(), out_h);
  argument.AddAttribute("out_h", attr_out_h);
  pir::Attribute attr_out_w = pir::Int32Attribute::get(pir::IrContext::Instance(), out_w);
  argument.AddAttribute("out_w", attr_out_w);
  std::vector<pir::Attribute> vec_scale;
  for (size_t i = 0; i < static_cast<size_t>(scale.size()); i++) {
      pir::Attribute attr_scale = pir::FloatAttribute::get(pir::IrContext::Instance(), scale[i]);

    vec_scale.push_back(attr_scale);
  }
  pir::Attribute attr_scale = pir::ArrayAttribute::get(pir::IrContext::Instance(), vec_scale);
  argument.AddAttribute("scale", attr_scale);
  pir::Attribute attr_interp_method = pir::StrAttribute::get(pir::IrContext::Instance(), interp_method);
  argument.AddAttribute("interp_method", attr_interp_method);
  pir::Attribute attr_align_corners = pir::BoolAttribute::get(pir::IrContext::Instance(), align_corners);
  argument.AddAttribute("align_corners", attr_align_corners);
  pir::Attribute attr_align_mode = pir::Int32Attribute::get(pir::IrContext::Instance(), align_mode);
  argument.AddAttribute("align_mode", attr_align_mode);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;
  paddle::dialect::DenseTensorType output_grad = output_grad_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)output_grad;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);
  paddle::dialect::IrTensor dense_x_grad;
  paddle::dialect::IrMetaTensor meta_x_grad(&dense_x_grad);

  phi::UnchangedInferMeta(meta_x, &meta_x_grad);

  std::vector<pir::Type> argument_outputs;
  pir::Type x_grad_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_x_grad.dtype()), dense_x_grad.dims(), dense_x_grad.layout(), dense_x_grad.lod(), dense_x_grad.offset());
  argument_outputs.push_back(x_grad_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void NearestInterpGradOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, pir::Value out_size_, pir::Value size_tensor_, pir::Value scale_tensor_, pir::Value output_grad_, pir::AttributeMap attributes) {
  VLOG(4) << "Start build NearestInterpGradOp";


  IR_ENFORCE(
      attributes.find("data_layout") != attributes.end(),
          "'data_layout' Attribute is expected for NearestInterpGradOp. ");
  std::string data_layout = attributes.at("data_layout").dyn_cast<pir::StrAttribute>().AsString();

  IR_ENFORCE(
      attributes.find("out_d") != attributes.end(),
          "'out_d' Attribute is expected for NearestInterpGradOp. ");
  int out_d = attributes.at("out_d").dyn_cast<pir::Int32Attribute>().data();

  IR_ENFORCE(
      attributes.find("out_h") != attributes.end(),
          "'out_h' Attribute is expected for NearestInterpGradOp. ");
  int out_h = attributes.at("out_h").dyn_cast<pir::Int32Attribute>().data();

  IR_ENFORCE(
      attributes.find("out_w") != attributes.end(),
          "'out_w' Attribute is expected for NearestInterpGradOp. ");
  int out_w = attributes.at("out_w").dyn_cast<pir::Int32Attribute>().data();

  IR_ENFORCE(
      attributes.find("scale") != attributes.end(),
          "'scale' Attribute is expected for NearestInterpGradOp. ");
  std::vector<float> scale;
  for (size_t i = 0; i < attributes.at("scale").dyn_cast<pir::ArrayAttribute>().size(); i++) {
    scale.push_back(attributes.at("scale").dyn_cast<pir::ArrayAttribute>().at(i).dyn_cast<pir::FloatAttribute>().data());
  }

  IR_ENFORCE(
      attributes.find("interp_method") != attributes.end(),
          "'interp_method' Attribute is expected for NearestInterpGradOp. ");
  std::string interp_method = attributes.at("interp_method").dyn_cast<pir::StrAttribute>().AsString();

  IR_ENFORCE(
      attributes.find("align_corners") != attributes.end(),
          "'align_corners' Attribute is expected for NearestInterpGradOp. ");
  bool align_corners = attributes.at("align_corners").dyn_cast<pir::BoolAttribute>().data();

  IR_ENFORCE(
      attributes.find("align_mode") != attributes.end(),
          "'align_mode' Attribute is expected for NearestInterpGradOp. ");
  int align_mode = attributes.at("align_mode").dyn_cast<pir::Int32Attribute>().data();


  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_, out_size_, size_tensor_, scale_tensor_, output_grad_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_data_layout = pir::StrAttribute::get(pir::IrContext::Instance(), data_layout);
  argument.AddAttribute("data_layout", attr_data_layout);
  pir::Attribute attr_out_d = pir::Int32Attribute::get(pir::IrContext::Instance(), out_d);
  argument.AddAttribute("out_d", attr_out_d);
  pir::Attribute attr_out_h = pir::Int32Attribute::get(pir::IrContext::Instance(), out_h);
  argument.AddAttribute("out_h", attr_out_h);
  pir::Attribute attr_out_w = pir::Int32Attribute::get(pir::IrContext::Instance(), out_w);
  argument.AddAttribute("out_w", attr_out_w);
  std::vector<pir::Attribute> vec_scale;
  for (size_t i = 0; i < static_cast<size_t>(scale.size()); i++) {
      pir::Attribute attr_scale = pir::FloatAttribute::get(pir::IrContext::Instance(), scale[i]);

    vec_scale.push_back(attr_scale);
  }
  pir::Attribute attr_scale = pir::ArrayAttribute::get(pir::IrContext::Instance(), vec_scale);
  argument.AddAttribute("scale", attr_scale);
  pir::Attribute attr_interp_method = pir::StrAttribute::get(pir::IrContext::Instance(), interp_method);
  argument.AddAttribute("interp_method", attr_interp_method);
  pir::Attribute attr_align_corners = pir::BoolAttribute::get(pir::IrContext::Instance(), align_corners);
  argument.AddAttribute("align_corners", attr_align_corners);
  pir::Attribute attr_align_mode = pir::Int32Attribute::get(pir::IrContext::Instance(), align_mode);
  argument.AddAttribute("align_mode", attr_align_mode);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;
  paddle::dialect::DenseTensorType output_grad = output_grad_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)output_grad;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);
  paddle::dialect::IrTensor dense_x_grad;
  paddle::dialect::IrMetaTensor meta_x_grad(&dense_x_grad);

  phi::UnchangedInferMeta(meta_x, &meta_x_grad);

  std::vector<pir::Type> argument_outputs;
  pir::Type x_grad_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_x_grad.dtype()), dense_x_grad.dims(), dense_x_grad.layout(), dense_x_grad.lod(), dense_x_grad.offset());
  argument_outputs.push_back(x_grad_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void NearestInterpGradOp::VerifySig() {}

void NearestInterpGradOp::InferMeta( phi::InferMetaContext *infer_meta ) {
  auto fn = PD_INFER_META(phi::UnchangedInferMeta);
  fn(infer_meta);
}

phi::DataType NearestInterpGradOp::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: NearestInterpGradOp";
  

  // deal skip data transform
  if (var_name == "out_size" || var_name == "size_tensor" || var_name == "scale_tensor"){
    return expected_kernel_dtype;
  }


  return expected_kernel_dtype;
}

const char *NllLossGradOp::attributes_name[2] = { "ignore_index", "reduction" };

OpInfoTuple NllLossGradOp::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("input", "paddle::dialect::DenseTensorType", false, false, false, false), paddle::dialect::OpInputInfo("label", "paddle::dialect::DenseTensorType", false, false, false, false), paddle::dialect::OpInputInfo("weight", "paddle::dialect::DenseTensorType", true, false, false, false), paddle::dialect::OpInputInfo("total_weight", "paddle::dialect::DenseTensorType", false, false, false, false), paddle::dialect::OpInputInfo("out_grad", "paddle::dialect::DenseTensorType", false, false, false, false) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = { paddle::dialect::OpAttributeInfo("ignore_index", "pir::Int64Attribute", ""), paddle::dialect::OpAttributeInfo("reduction", "pir::StrAttribute", "") };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("input_grad", "paddle::dialect::DenseTensorType", false, false) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("NllLossGradInferMeta", {"input", "label", "weight", "total_weight", "out_grad", "ignore_index", "reduction"}, "nll_loss_grad", {"input", "label", "weight", "total_weight", "out_grad", "ignore_index", "reduction"}, {"input"}, {}, {}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "nll_loss_grad");
}

void NllLossGradOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value input_, pir::Value label_, pir::Value weight_, pir::Value total_weight_, pir::Value out_grad_, int64_t ignore_index, const std::string& reduction) {
  VLOG(4) << "Start build NllLossGradOp";



  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {input_, label_, weight_, total_weight_, out_grad_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_ignore_index = pir::Int64Attribute::get(pir::IrContext::Instance(), ignore_index);
  argument.AddAttribute("ignore_index", attr_ignore_index);
  pir::Attribute attr_reduction = pir::StrAttribute::get(pir::IrContext::Instance(), reduction);
  argument.AddAttribute("reduction", attr_reduction);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType input = input_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)input;
  paddle::dialect::DenseTensorType label = label_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)label;
  paddle::dialect::DenseTensorType total_weight = total_weight_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)total_weight;
  paddle::dialect::DenseTensorType out_grad = out_grad_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)out_grad;

  VLOG(4) << "Builder construction  dense_input";
  paddle::dialect::IrTensor ir_tensor_input(paddle::dialect::TransToPhiDataType(input.dtype()),
                                                      input.dims(),
                                                      input.data_layout(),
                                                      input.lod(),
                                                      input.offset());
  VLOG(4) << "Builder construction  meta_input";
  paddle::dialect::IrMetaTensor meta_input(&ir_tensor_input);

  VLOG(4) << "Builder construction  dense_label";
  paddle::dialect::IrTensor ir_tensor_label(paddle::dialect::TransToPhiDataType(label.dtype()),
                                                      label.dims(),
                                                      label.data_layout(),
                                                      label.lod(),
                                                      label.offset());
  VLOG(4) << "Builder construction  meta_label";
  paddle::dialect::IrMetaTensor meta_label(&ir_tensor_label);

  paddle::dialect::IrMetaTensor meta_weight;
  paddle::dialect::IrTensor ir_tensor_weight;
  if (weight_.impl() != nullptr) {
    paddle::dialect::DenseTensorType weight = weight_.type().dyn_cast<paddle::dialect::DenseTensorType>();
    VLOG(4) << "Builder construction  dense_weight";
    ir_tensor_weight = paddle::dialect::IrTensor(paddle::dialect::TransToPhiDataType(weight.dtype()),
                                                        weight.dims(),
                                                        weight.data_layout(),
                                                        weight.lod(),
                                                        weight.offset());
    VLOG(4) << "Builder construction  meta_weight";
    meta_weight = paddle::dialect::IrMetaTensor(&ir_tensor_weight);
  }


  VLOG(4) << "Builder construction  dense_total_weight";
  paddle::dialect::IrTensor ir_tensor_total_weight(paddle::dialect::TransToPhiDataType(total_weight.dtype()),
                                                      total_weight.dims(),
                                                      total_weight.data_layout(),
                                                      total_weight.lod(),
                                                      total_weight.offset());
  VLOG(4) << "Builder construction  meta_total_weight";
  paddle::dialect::IrMetaTensor meta_total_weight(&ir_tensor_total_weight);

  VLOG(4) << "Builder construction  dense_out_grad";
  paddle::dialect::IrTensor ir_tensor_out_grad(paddle::dialect::TransToPhiDataType(out_grad.dtype()),
                                                      out_grad.dims(),
                                                      out_grad.data_layout(),
                                                      out_grad.lod(),
                                                      out_grad.offset());
  VLOG(4) << "Builder construction  meta_out_grad";
  paddle::dialect::IrMetaTensor meta_out_grad(&ir_tensor_out_grad);
  paddle::dialect::IrTensor dense_input_grad;
  paddle::dialect::IrMetaTensor meta_input_grad(&dense_input_grad);

  phi::NllLossGradInferMeta(meta_input, meta_label, meta_weight, meta_total_weight, meta_out_grad, ignore_index, reduction, &meta_input_grad);

  std::vector<pir::Type> argument_outputs;
  pir::Type input_grad_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_input_grad.dtype()), dense_input_grad.dims(), dense_input_grad.layout(), dense_input_grad.lod(), dense_input_grad.offset());
  argument_outputs.push_back(input_grad_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void NllLossGradOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value input_, pir::Value label_, pir::Value weight_, pir::Value total_weight_, pir::Value out_grad_, pir::AttributeMap attributes) {
  VLOG(4) << "Start build NllLossGradOp";


  IR_ENFORCE(
      attributes.find("ignore_index") != attributes.end(),
          "'ignore_index' Attribute is expected for NllLossGradOp. ");
  int64_t ignore_index = attributes.at("ignore_index").dyn_cast<pir::Int64Attribute>().data();

  IR_ENFORCE(
      attributes.find("reduction") != attributes.end(),
          "'reduction' Attribute is expected for NllLossGradOp. ");
  std::string reduction = attributes.at("reduction").dyn_cast<pir::StrAttribute>().AsString();


  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {input_, label_, weight_, total_weight_, out_grad_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_ignore_index = pir::Int64Attribute::get(pir::IrContext::Instance(), ignore_index);
  argument.AddAttribute("ignore_index", attr_ignore_index);
  pir::Attribute attr_reduction = pir::StrAttribute::get(pir::IrContext::Instance(), reduction);
  argument.AddAttribute("reduction", attr_reduction);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType input = input_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)input;
  paddle::dialect::DenseTensorType label = label_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)label;
  paddle::dialect::DenseTensorType total_weight = total_weight_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)total_weight;
  paddle::dialect::DenseTensorType out_grad = out_grad_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)out_grad;

  VLOG(4) << "Builder construction  dense_input";
  paddle::dialect::IrTensor ir_tensor_input(paddle::dialect::TransToPhiDataType(input.dtype()),
                                                      input.dims(),
                                                      input.data_layout(),
                                                      input.lod(),
                                                      input.offset());
  VLOG(4) << "Builder construction  meta_input";
  paddle::dialect::IrMetaTensor meta_input(&ir_tensor_input);

  VLOG(4) << "Builder construction  dense_label";
  paddle::dialect::IrTensor ir_tensor_label(paddle::dialect::TransToPhiDataType(label.dtype()),
                                                      label.dims(),
                                                      label.data_layout(),
                                                      label.lod(),
                                                      label.offset());
  VLOG(4) << "Builder construction  meta_label";
  paddle::dialect::IrMetaTensor meta_label(&ir_tensor_label);

  paddle::dialect::IrMetaTensor meta_weight;
  paddle::dialect::IrTensor ir_tensor_weight;
  if (weight_.impl() != nullptr) {
    paddle::dialect::DenseTensorType weight = weight_.type().dyn_cast<paddle::dialect::DenseTensorType>();
    VLOG(4) << "Builder construction  dense_weight";
    ir_tensor_weight = paddle::dialect::IrTensor(paddle::dialect::TransToPhiDataType(weight.dtype()),
                                                        weight.dims(),
                                                        weight.data_layout(),
                                                        weight.lod(),
                                                        weight.offset());
    VLOG(4) << "Builder construction  meta_weight";
    meta_weight = paddle::dialect::IrMetaTensor(&ir_tensor_weight);
  }


  VLOG(4) << "Builder construction  dense_total_weight";
  paddle::dialect::IrTensor ir_tensor_total_weight(paddle::dialect::TransToPhiDataType(total_weight.dtype()),
                                                      total_weight.dims(),
                                                      total_weight.data_layout(),
                                                      total_weight.lod(),
                                                      total_weight.offset());
  VLOG(4) << "Builder construction  meta_total_weight";
  paddle::dialect::IrMetaTensor meta_total_weight(&ir_tensor_total_weight);

  VLOG(4) << "Builder construction  dense_out_grad";
  paddle::dialect::IrTensor ir_tensor_out_grad(paddle::dialect::TransToPhiDataType(out_grad.dtype()),
                                                      out_grad.dims(),
                                                      out_grad.data_layout(),
                                                      out_grad.lod(),
                                                      out_grad.offset());
  VLOG(4) << "Builder construction  meta_out_grad";
  paddle::dialect::IrMetaTensor meta_out_grad(&ir_tensor_out_grad);
  paddle::dialect::IrTensor dense_input_grad;
  paddle::dialect::IrMetaTensor meta_input_grad(&dense_input_grad);

  phi::NllLossGradInferMeta(meta_input, meta_label, meta_weight, meta_total_weight, meta_out_grad, ignore_index, reduction, &meta_input_grad);

  std::vector<pir::Type> argument_outputs;
  pir::Type input_grad_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_input_grad.dtype()), dense_input_grad.dims(), dense_input_grad.layout(), dense_input_grad.lod(), dense_input_grad.offset());
  argument_outputs.push_back(input_grad_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void NllLossGradOp::VerifySig() {}

void NllLossGradOp::InferMeta( phi::InferMetaContext *infer_meta ) {
  auto fn = PD_INFER_META(phi::NllLossGradInferMeta);
  fn(infer_meta);
}

phi::DataType NllLossGradOp::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: NllLossGradOp";
  


  return expected_kernel_dtype;
}

const char *OverlapAddGradOp::attributes_name[2] = { "hop_length", "axis" };

OpInfoTuple OverlapAddGradOp::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("x", "paddle::dialect::DenseTensorType", false, false, false, false), paddle::dialect::OpInputInfo("out_grad", "paddle::dialect::DenseTensorType", false, false, false, false) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = { paddle::dialect::OpAttributeInfo("hop_length", "pir::Int32Attribute", ""), paddle::dialect::OpAttributeInfo("axis", "pir::Int32Attribute", "") };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("x_grad", "paddle::dialect::DenseTensorType", false, false) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("OverlapAddGradInferMeta", {"x", "out_grad", "hop_length", "axis"}, "overlap_add_grad", {"x", "out_grad", "hop_length", "axis"}, {"x"}, {}, {}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "overlap_add_grad");
}

void OverlapAddGradOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, pir::Value out_grad_, int hop_length, int axis) {
  VLOG(4) << "Start build OverlapAddGradOp";



  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_, out_grad_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_hop_length = pir::Int32Attribute::get(pir::IrContext::Instance(), hop_length);
  argument.AddAttribute("hop_length", attr_hop_length);
  pir::Attribute attr_axis = pir::Int32Attribute::get(pir::IrContext::Instance(), axis);
  argument.AddAttribute("axis", attr_axis);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;
  paddle::dialect::DenseTensorType out_grad = out_grad_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)out_grad;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);

  VLOG(4) << "Builder construction  dense_out_grad";
  paddle::dialect::IrTensor ir_tensor_out_grad(paddle::dialect::TransToPhiDataType(out_grad.dtype()),
                                                      out_grad.dims(),
                                                      out_grad.data_layout(),
                                                      out_grad.lod(),
                                                      out_grad.offset());
  VLOG(4) << "Builder construction  meta_out_grad";
  paddle::dialect::IrMetaTensor meta_out_grad(&ir_tensor_out_grad);
  paddle::dialect::IrTensor dense_x_grad;
  paddle::dialect::IrMetaTensor meta_x_grad(&dense_x_grad);

  phi::OverlapAddGradInferMeta(meta_x, meta_out_grad, hop_length, axis, &meta_x_grad);

  std::vector<pir::Type> argument_outputs;
  pir::Type x_grad_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_x_grad.dtype()), dense_x_grad.dims(), dense_x_grad.layout(), dense_x_grad.lod(), dense_x_grad.offset());
  argument_outputs.push_back(x_grad_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void OverlapAddGradOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, pir::Value out_grad_, pir::AttributeMap attributes) {
  VLOG(4) << "Start build OverlapAddGradOp";


  IR_ENFORCE(
      attributes.find("hop_length") != attributes.end(),
          "'hop_length' Attribute is expected for OverlapAddGradOp. ");
  int hop_length = attributes.at("hop_length").dyn_cast<pir::Int32Attribute>().data();

  IR_ENFORCE(
      attributes.find("axis") != attributes.end(),
          "'axis' Attribute is expected for OverlapAddGradOp. ");
  int axis = attributes.at("axis").dyn_cast<pir::Int32Attribute>().data();


  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_, out_grad_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_hop_length = pir::Int32Attribute::get(pir::IrContext::Instance(), hop_length);
  argument.AddAttribute("hop_length", attr_hop_length);
  pir::Attribute attr_axis = pir::Int32Attribute::get(pir::IrContext::Instance(), axis);
  argument.AddAttribute("axis", attr_axis);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;
  paddle::dialect::DenseTensorType out_grad = out_grad_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)out_grad;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);

  VLOG(4) << "Builder construction  dense_out_grad";
  paddle::dialect::IrTensor ir_tensor_out_grad(paddle::dialect::TransToPhiDataType(out_grad.dtype()),
                                                      out_grad.dims(),
                                                      out_grad.data_layout(),
                                                      out_grad.lod(),
                                                      out_grad.offset());
  VLOG(4) << "Builder construction  meta_out_grad";
  paddle::dialect::IrMetaTensor meta_out_grad(&ir_tensor_out_grad);
  paddle::dialect::IrTensor dense_x_grad;
  paddle::dialect::IrMetaTensor meta_x_grad(&dense_x_grad);

  phi::OverlapAddGradInferMeta(meta_x, meta_out_grad, hop_length, axis, &meta_x_grad);

  std::vector<pir::Type> argument_outputs;
  pir::Type x_grad_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_x_grad.dtype()), dense_x_grad.dims(), dense_x_grad.layout(), dense_x_grad.lod(), dense_x_grad.offset());
  argument_outputs.push_back(x_grad_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void OverlapAddGradOp::VerifySig() {}

void OverlapAddGradOp::InferMeta( phi::InferMetaContext *infer_meta ) {
  auto fn = PD_INFER_META(phi::OverlapAddGradInferMeta);
  fn(infer_meta);
}

phi::DataType OverlapAddGradOp::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: OverlapAddGradOp";
  


  return expected_kernel_dtype;
}

const char *PNormGradOp::attributes_name[5] = { "porder", "axis", "epsilon", "keepdim", "asvector" };

OpInfoTuple PNormGradOp::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("x", "paddle::dialect::DenseTensorType", false, false, false, false), paddle::dialect::OpInputInfo("out", "paddle::dialect::DenseTensorType", false, false, false, false), paddle::dialect::OpInputInfo("out_grad", "paddle::dialect::DenseTensorType", false, false, false, false) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = { paddle::dialect::OpAttributeInfo("porder", "pir::FloatAttribute", ""), paddle::dialect::OpAttributeInfo("axis", "pir::Int32Attribute", ""), paddle::dialect::OpAttributeInfo("epsilon", "pir::FloatAttribute", ""), paddle::dialect::OpAttributeInfo("keepdim", "pir::BoolAttribute", ""), paddle::dialect::OpAttributeInfo("asvector", "pir::BoolAttribute", "") };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("x_grad", "paddle::dialect::DenseTensorType", false, false) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("GeneralUnaryGradInferMeta", {"x"}, "p_norm_grad", {"x", "out", "out_grad", "porder", "axis", "epsilon", "keepdim", "asvector"}, {}, {}, {}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "p_norm_grad");
}

void PNormGradOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, pir::Value out_, pir::Value out_grad_, float porder, int axis, float epsilon, bool keepdim, bool asvector) {
  VLOG(4) << "Start build PNormGradOp";



  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_, out_, out_grad_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_porder = pir::FloatAttribute::get(pir::IrContext::Instance(), porder);
  argument.AddAttribute("porder", attr_porder);
  pir::Attribute attr_axis = pir::Int32Attribute::get(pir::IrContext::Instance(), axis);
  argument.AddAttribute("axis", attr_axis);
  pir::Attribute attr_epsilon = pir::FloatAttribute::get(pir::IrContext::Instance(), epsilon);
  argument.AddAttribute("epsilon", attr_epsilon);
  pir::Attribute attr_keepdim = pir::BoolAttribute::get(pir::IrContext::Instance(), keepdim);
  argument.AddAttribute("keepdim", attr_keepdim);
  pir::Attribute attr_asvector = pir::BoolAttribute::get(pir::IrContext::Instance(), asvector);
  argument.AddAttribute("asvector", attr_asvector);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;
  paddle::dialect::DenseTensorType out = out_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)out;
  paddle::dialect::DenseTensorType out_grad = out_grad_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)out_grad;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);
  paddle::dialect::IrTensor dense_x_grad;
  paddle::dialect::IrMetaTensor meta_x_grad(&dense_x_grad);

  phi::GeneralUnaryGradInferMeta(meta_x, &meta_x_grad);

  std::vector<pir::Type> argument_outputs;
  pir::Type x_grad_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_x_grad.dtype()), dense_x_grad.dims(), dense_x_grad.layout(), dense_x_grad.lod(), dense_x_grad.offset());
  argument_outputs.push_back(x_grad_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void PNormGradOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, pir::Value out_, pir::Value out_grad_, pir::AttributeMap attributes) {
  VLOG(4) << "Start build PNormGradOp";


  IR_ENFORCE(
      attributes.find("porder") != attributes.end(),
          "'porder' Attribute is expected for PNormGradOp. ");
  float porder = attributes.at("porder").dyn_cast<pir::FloatAttribute>().data();

  IR_ENFORCE(
      attributes.find("axis") != attributes.end(),
          "'axis' Attribute is expected for PNormGradOp. ");
  int axis = attributes.at("axis").dyn_cast<pir::Int32Attribute>().data();

  IR_ENFORCE(
      attributes.find("epsilon") != attributes.end(),
          "'epsilon' Attribute is expected for PNormGradOp. ");
  float epsilon = attributes.at("epsilon").dyn_cast<pir::FloatAttribute>().data();

  IR_ENFORCE(
      attributes.find("keepdim") != attributes.end(),
          "'keepdim' Attribute is expected for PNormGradOp. ");
  bool keepdim = attributes.at("keepdim").dyn_cast<pir::BoolAttribute>().data();

  IR_ENFORCE(
      attributes.find("asvector") != attributes.end(),
          "'asvector' Attribute is expected for PNormGradOp. ");
  bool asvector = attributes.at("asvector").dyn_cast<pir::BoolAttribute>().data();


  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_, out_, out_grad_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_porder = pir::FloatAttribute::get(pir::IrContext::Instance(), porder);
  argument.AddAttribute("porder", attr_porder);
  pir::Attribute attr_axis = pir::Int32Attribute::get(pir::IrContext::Instance(), axis);
  argument.AddAttribute("axis", attr_axis);
  pir::Attribute attr_epsilon = pir::FloatAttribute::get(pir::IrContext::Instance(), epsilon);
  argument.AddAttribute("epsilon", attr_epsilon);
  pir::Attribute attr_keepdim = pir::BoolAttribute::get(pir::IrContext::Instance(), keepdim);
  argument.AddAttribute("keepdim", attr_keepdim);
  pir::Attribute attr_asvector = pir::BoolAttribute::get(pir::IrContext::Instance(), asvector);
  argument.AddAttribute("asvector", attr_asvector);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;
  paddle::dialect::DenseTensorType out = out_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)out;
  paddle::dialect::DenseTensorType out_grad = out_grad_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)out_grad;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);
  paddle::dialect::IrTensor dense_x_grad;
  paddle::dialect::IrMetaTensor meta_x_grad(&dense_x_grad);

  phi::GeneralUnaryGradInferMeta(meta_x, &meta_x_grad);

  std::vector<pir::Type> argument_outputs;
  pir::Type x_grad_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_x_grad.dtype()), dense_x_grad.dims(), dense_x_grad.layout(), dense_x_grad.lod(), dense_x_grad.offset());
  argument_outputs.push_back(x_grad_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void PNormGradOp::VerifySig() {}

void PNormGradOp::InferMeta( phi::InferMetaContext *infer_meta ) {
  auto fn = PD_INFER_META(phi::GeneralUnaryGradInferMeta);
  fn(infer_meta);
}

phi::DataType PNormGradOp::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: PNormGradOp";
  


  return expected_kernel_dtype;
}

const char *Pad3dDoubleGradOp::attributes_name[3] = { "mode", "pad_value", "data_format" };

OpInfoTuple Pad3dDoubleGradOp::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("grad_x_grad", "paddle::dialect::DenseTensorType", false, false, false, false), paddle::dialect::OpInputInfo("paddings", "paddle::dialect::IntArrayAttribute", false, false, true, false) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = { paddle::dialect::OpAttributeInfo("mode", "pir::StrAttribute", ""), paddle::dialect::OpAttributeInfo("pad_value", "pir::FloatAttribute", ""), paddle::dialect::OpAttributeInfo("data_format", "pir::StrAttribute", "") };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("grad_out_grad", "paddle::dialect::DenseTensorType", false, false) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("Pad3dInferMeta", {"grad_x_grad", "paddings", "mode", "pad_value", "data_format"}, "pad3d", {"grad_x_grad", "paddings", "mode", "pad_value", "data_format"}, {}, {}, {}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "pad3d_double_grad");
}

void Pad3dDoubleGradOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value grad_x_grad_, const std::vector<int64_t>& paddings, const std::string& mode, float pad_value, const std::string& data_format) {
  VLOG(4) << "Start build Pad3dDoubleGradOp";


  // Generate int_array mutable attribute: paddings
  paddle::dialect::FullIntArrayOp full_paddings_op = builder.Build<paddle::dialect::FullIntArrayOp>(paddings, phi::DataType::INT64, phi::CPUPlace());
  pir::OpResult paddings_ = full_paddings_op->result(0);
    
  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {grad_x_grad_, paddings_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_mode = pir::StrAttribute::get(pir::IrContext::Instance(), mode);
  argument.AddAttribute("mode", attr_mode);
  pir::Attribute attr_pad_value = pir::FloatAttribute::get(pir::IrContext::Instance(), pad_value);
  argument.AddAttribute("pad_value", attr_pad_value);
  pir::Attribute attr_data_format = pir::StrAttribute::get(pir::IrContext::Instance(), data_format);
  argument.AddAttribute("data_format", attr_data_format);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType grad_x_grad = grad_x_grad_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)grad_x_grad;

  VLOG(4) << "Builder construction  dense_grad_x_grad";
  paddle::dialect::IrTensor ir_tensor_grad_x_grad(paddle::dialect::TransToPhiDataType(grad_x_grad.dtype()),
                                                      grad_x_grad.dims(),
                                                      grad_x_grad.data_layout(),
                                                      grad_x_grad.lod(),
                                                      grad_x_grad.offset());
  VLOG(4) << "Builder construction  meta_grad_x_grad";
  paddle::dialect::IrMetaTensor meta_grad_x_grad(&ir_tensor_grad_x_grad);
  paddle::dialect::IrTensor dense_grad_out_grad;
  paddle::dialect::IrMetaTensor meta_grad_out_grad(&dense_grad_out_grad);

  phi::Pad3dInferMeta(meta_grad_x_grad, paddings, mode, pad_value, data_format, &meta_grad_out_grad);

  std::vector<pir::Type> argument_outputs;
  pir::Type grad_out_grad_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_grad_out_grad.dtype()), dense_grad_out_grad.dims(), dense_grad_out_grad.layout(), dense_grad_out_grad.lod(), dense_grad_out_grad.offset());
  argument_outputs.push_back(grad_out_grad_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void Pad3dDoubleGradOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value grad_x_grad_, pir::AttributeMap attributes) {
  VLOG(4) << "Start build Pad3dDoubleGradOp";


  IR_ENFORCE(
      attributes.find("paddings") != attributes.end(),
          "'paddings' Attribute is expected for Pad3dDoubleGradOp. ");
  std::vector<int64_t> paddings = attributes.at("paddings").dyn_cast<paddle::dialect::IntArrayAttribute>().data().GetData();

  IR_ENFORCE(
      attributes.find("mode") != attributes.end(),
          "'mode' Attribute is expected for Pad3dDoubleGradOp. ");
  std::string mode = attributes.at("mode").dyn_cast<pir::StrAttribute>().AsString();

  IR_ENFORCE(
      attributes.find("pad_value") != attributes.end(),
          "'pad_value' Attribute is expected for Pad3dDoubleGradOp. ");
  float pad_value = attributes.at("pad_value").dyn_cast<pir::FloatAttribute>().data();

  IR_ENFORCE(
      attributes.find("data_format") != attributes.end(),
          "'data_format' Attribute is expected for Pad3dDoubleGradOp. ");
  std::string data_format = attributes.at("data_format").dyn_cast<pir::StrAttribute>().AsString();

  // Generate int_array mutable attribute: paddings
  paddle::dialect::FullIntArrayOp full_paddings_op = builder.Build<paddle::dialect::FullIntArrayOp>(paddings, phi::DataType::INT64, phi::CPUPlace());
  pir::OpResult paddings_ = full_paddings_op->result(0);
    
  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {grad_x_grad_, paddings_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_mode = pir::StrAttribute::get(pir::IrContext::Instance(), mode);
  argument.AddAttribute("mode", attr_mode);
  pir::Attribute attr_pad_value = pir::FloatAttribute::get(pir::IrContext::Instance(), pad_value);
  argument.AddAttribute("pad_value", attr_pad_value);
  pir::Attribute attr_data_format = pir::StrAttribute::get(pir::IrContext::Instance(), data_format);
  argument.AddAttribute("data_format", attr_data_format);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType grad_x_grad = grad_x_grad_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)grad_x_grad;

  VLOG(4) << "Builder construction  dense_grad_x_grad";
  paddle::dialect::IrTensor ir_tensor_grad_x_grad(paddle::dialect::TransToPhiDataType(grad_x_grad.dtype()),
                                                      grad_x_grad.dims(),
                                                      grad_x_grad.data_layout(),
                                                      grad_x_grad.lod(),
                                                      grad_x_grad.offset());
  VLOG(4) << "Builder construction  meta_grad_x_grad";
  paddle::dialect::IrMetaTensor meta_grad_x_grad(&ir_tensor_grad_x_grad);
  paddle::dialect::IrTensor dense_grad_out_grad;
  paddle::dialect::IrMetaTensor meta_grad_out_grad(&dense_grad_out_grad);

  phi::Pad3dInferMeta(meta_grad_x_grad, paddings, mode, pad_value, data_format, &meta_grad_out_grad);

  std::vector<pir::Type> argument_outputs;
  pir::Type grad_out_grad_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_grad_out_grad.dtype()), dense_grad_out_grad.dims(), dense_grad_out_grad.layout(), dense_grad_out_grad.lod(), dense_grad_out_grad.offset());
  argument_outputs.push_back(grad_out_grad_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void Pad3dDoubleGradOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value grad_x_grad_, pir::Value paddings_, const std::string& mode, float pad_value, const std::string& data_format) {
  VLOG(4) << "Start build Pad3dDoubleGradOp";



  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {grad_x_grad_, paddings_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_mode = pir::StrAttribute::get(pir::IrContext::Instance(), mode);
  argument.AddAttribute("mode", attr_mode);
  pir::Attribute attr_pad_value = pir::FloatAttribute::get(pir::IrContext::Instance(), pad_value);
  argument.AddAttribute("pad_value", attr_pad_value);
  pir::Attribute attr_data_format = pir::StrAttribute::get(pir::IrContext::Instance(), data_format);
  argument.AddAttribute("data_format", attr_data_format);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType grad_x_grad = grad_x_grad_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)grad_x_grad;
  phi::IntArray paddings;
  if (paddings_.dyn_cast<pir::OpResult>() && paddings_.dyn_cast<pir::OpResult>().owner()->isa<paddle::dialect::FullIntArrayOp>()) {
    paddings = std::move(phi::IntArray(paddle::dialect::GetInt64Vector(
                          paddings_.dyn_cast<pir::OpResult>().owner()
                          ->dyn_cast<paddle::dialect::FullIntArrayOp>()
                          .attribute("value"))));
  } else if (paddings_.type().isa<pir::VectorType>()) {
    size_t paddings_size = paddings_.type().dyn_cast<pir::VectorType>().size();
    paddings = std::move(phi::IntArray(std::vector<int64_t>(paddings_size, -1)));
    paddings.SetFromTensor(true);
  } else if (paddings_.type().isa<paddle::dialect::DenseTensorType>()) {
    common::DDim paddings_dim = paddings_.type().dyn_cast<paddle::dialect::DenseTensorType>().dims();
    size_t paddings_size = common::product(paddings_dim);
    if (common::contain_unknown_dim(paddings_dim)) {
      paddings_size = 1;
    }
    paddings = std::move(phi::IntArray(std::vector<int64_t>(paddings_size, -1)));
    paddings.SetFromTensor(true);
  } else {
    PADDLE_THROW(phi::errors::Unimplemented("Only support VectorType or DenseTensorType"));
  }


  VLOG(4) << "Builder construction  dense_grad_x_grad";
  paddle::dialect::IrTensor ir_tensor_grad_x_grad(paddle::dialect::TransToPhiDataType(grad_x_grad.dtype()),
                                                      grad_x_grad.dims(),
                                                      grad_x_grad.data_layout(),
                                                      grad_x_grad.lod(),
                                                      grad_x_grad.offset());
  VLOG(4) << "Builder construction  meta_grad_x_grad";
  paddle::dialect::IrMetaTensor meta_grad_x_grad(&ir_tensor_grad_x_grad);
  paddle::dialect::IrTensor dense_grad_out_grad;
  paddle::dialect::IrMetaTensor meta_grad_out_grad(&dense_grad_out_grad);

  phi::Pad3dInferMeta(meta_grad_x_grad, paddings, mode, pad_value, data_format, &meta_grad_out_grad);

  std::vector<pir::Type> argument_outputs;
  pir::Type grad_out_grad_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_grad_out_grad.dtype()), dense_grad_out_grad.dims(), dense_grad_out_grad.layout(), dense_grad_out_grad.lod(), dense_grad_out_grad.offset());
  argument_outputs.push_back(grad_out_grad_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void Pad3dDoubleGradOp::VerifySig() {}

void Pad3dDoubleGradOp::InferMeta( phi::InferMetaContext *infer_meta ) {
  auto fn = PD_INFER_META(phi::Pad3dInferMeta);
  fn(infer_meta);
}

phi::DataType Pad3dDoubleGradOp::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: Pad3dDoubleGradOp";
  


  return expected_kernel_dtype;
}

const char *Pad3dGradOp::attributes_name[3] = { "mode", "pad_value", "data_format" };

OpInfoTuple Pad3dGradOp::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("x", "paddle::dialect::DenseTensorType", false, true, false, false), paddle::dialect::OpInputInfo("out_grad", "paddle::dialect::DenseTensorType", false, false, false, true), paddle::dialect::OpInputInfo("paddings", "paddle::dialect::IntArrayAttribute", false, false, true, false) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = { paddle::dialect::OpAttributeInfo("mode", "pir::StrAttribute", ""), paddle::dialect::OpAttributeInfo("pad_value", "pir::FloatAttribute", ""), paddle::dialect::OpAttributeInfo("data_format", "pir::StrAttribute", "") };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("x_grad", "paddle::dialect::DenseTensorType", false, false) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("UnchangedInferMeta", {"x"}, "pad3d_grad", {"x", "out_grad", "paddings", "mode", "pad_value", "data_format"}, {}, {}, {}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "pad3d_grad");
}

void Pad3dGradOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, pir::Value out_grad_, const std::vector<int64_t>& paddings, const std::string& mode, float pad_value, const std::string& data_format) {
  VLOG(4) << "Start build Pad3dGradOp";


  // Generate int_array mutable attribute: paddings
  paddle::dialect::FullIntArrayOp full_paddings_op = builder.Build<paddle::dialect::FullIntArrayOp>(paddings, phi::DataType::INT64, phi::CPUPlace());
  pir::OpResult paddings_ = full_paddings_op->result(0);
    
  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_, out_grad_, paddings_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_mode = pir::StrAttribute::get(pir::IrContext::Instance(), mode);
  argument.AddAttribute("mode", attr_mode);
  pir::Attribute attr_pad_value = pir::FloatAttribute::get(pir::IrContext::Instance(), pad_value);
  argument.AddAttribute("pad_value", attr_pad_value);
  pir::Attribute attr_data_format = pir::StrAttribute::get(pir::IrContext::Instance(), data_format);
  argument.AddAttribute("data_format", attr_data_format);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;
  paddle::dialect::DenseTensorType out_grad = out_grad_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)out_grad;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);
  paddle::dialect::IrTensor dense_x_grad;
  paddle::dialect::IrMetaTensor meta_x_grad(&dense_x_grad);

  phi::UnchangedInferMeta(meta_x, &meta_x_grad);

  std::vector<pir::Type> argument_outputs;
  pir::Type x_grad_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_x_grad.dtype()), dense_x_grad.dims(), dense_x_grad.layout(), dense_x_grad.lod(), dense_x_grad.offset());
  argument_outputs.push_back(x_grad_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void Pad3dGradOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, pir::Value out_grad_, pir::AttributeMap attributes) {
  VLOG(4) << "Start build Pad3dGradOp";


  IR_ENFORCE(
      attributes.find("paddings") != attributes.end(),
          "'paddings' Attribute is expected for Pad3dGradOp. ");
  std::vector<int64_t> paddings = attributes.at("paddings").dyn_cast<paddle::dialect::IntArrayAttribute>().data().GetData();

  IR_ENFORCE(
      attributes.find("mode") != attributes.end(),
          "'mode' Attribute is expected for Pad3dGradOp. ");
  std::string mode = attributes.at("mode").dyn_cast<pir::StrAttribute>().AsString();

  IR_ENFORCE(
      attributes.find("pad_value") != attributes.end(),
          "'pad_value' Attribute is expected for Pad3dGradOp. ");
  float pad_value = attributes.at("pad_value").dyn_cast<pir::FloatAttribute>().data();

  IR_ENFORCE(
      attributes.find("data_format") != attributes.end(),
          "'data_format' Attribute is expected for Pad3dGradOp. ");
  std::string data_format = attributes.at("data_format").dyn_cast<pir::StrAttribute>().AsString();

  // Generate int_array mutable attribute: paddings
  paddle::dialect::FullIntArrayOp full_paddings_op = builder.Build<paddle::dialect::FullIntArrayOp>(paddings, phi::DataType::INT64, phi::CPUPlace());
  pir::OpResult paddings_ = full_paddings_op->result(0);
    
  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_, out_grad_, paddings_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_mode = pir::StrAttribute::get(pir::IrContext::Instance(), mode);
  argument.AddAttribute("mode", attr_mode);
  pir::Attribute attr_pad_value = pir::FloatAttribute::get(pir::IrContext::Instance(), pad_value);
  argument.AddAttribute("pad_value", attr_pad_value);
  pir::Attribute attr_data_format = pir::StrAttribute::get(pir::IrContext::Instance(), data_format);
  argument.AddAttribute("data_format", attr_data_format);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;
  paddle::dialect::DenseTensorType out_grad = out_grad_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)out_grad;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);
  paddle::dialect::IrTensor dense_x_grad;
  paddle::dialect::IrMetaTensor meta_x_grad(&dense_x_grad);

  phi::UnchangedInferMeta(meta_x, &meta_x_grad);

  std::vector<pir::Type> argument_outputs;
  pir::Type x_grad_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_x_grad.dtype()), dense_x_grad.dims(), dense_x_grad.layout(), dense_x_grad.lod(), dense_x_grad.offset());
  argument_outputs.push_back(x_grad_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void Pad3dGradOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, pir::Value out_grad_, pir::Value paddings_, const std::string& mode, float pad_value, const std::string& data_format) {
  VLOG(4) << "Start build Pad3dGradOp";



  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_, out_grad_, paddings_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_mode = pir::StrAttribute::get(pir::IrContext::Instance(), mode);
  argument.AddAttribute("mode", attr_mode);
  pir::Attribute attr_pad_value = pir::FloatAttribute::get(pir::IrContext::Instance(), pad_value);
  argument.AddAttribute("pad_value", attr_pad_value);
  pir::Attribute attr_data_format = pir::StrAttribute::get(pir::IrContext::Instance(), data_format);
  argument.AddAttribute("data_format", attr_data_format);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;
  paddle::dialect::DenseTensorType out_grad = out_grad_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)out_grad;
  phi::IntArray paddings;
  if (paddings_.dyn_cast<pir::OpResult>() && paddings_.dyn_cast<pir::OpResult>().owner()->isa<paddle::dialect::FullIntArrayOp>()) {
    paddings = std::move(phi::IntArray(paddle::dialect::GetInt64Vector(
                          paddings_.dyn_cast<pir::OpResult>().owner()
                          ->dyn_cast<paddle::dialect::FullIntArrayOp>()
                          .attribute("value"))));
  } else if (paddings_.type().isa<pir::VectorType>()) {
    size_t paddings_size = paddings_.type().dyn_cast<pir::VectorType>().size();
    paddings = std::move(phi::IntArray(std::vector<int64_t>(paddings_size, -1)));
    paddings.SetFromTensor(true);
  } else if (paddings_.type().isa<paddle::dialect::DenseTensorType>()) {
    common::DDim paddings_dim = paddings_.type().dyn_cast<paddle::dialect::DenseTensorType>().dims();
    size_t paddings_size = common::product(paddings_dim);
    if (common::contain_unknown_dim(paddings_dim)) {
      paddings_size = 1;
    }
    paddings = std::move(phi::IntArray(std::vector<int64_t>(paddings_size, -1)));
    paddings.SetFromTensor(true);
  } else {
    PADDLE_THROW(phi::errors::Unimplemented("Only support VectorType or DenseTensorType"));
  }


  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);
  paddle::dialect::IrTensor dense_x_grad;
  paddle::dialect::IrMetaTensor meta_x_grad(&dense_x_grad);

  phi::UnchangedInferMeta(meta_x, &meta_x_grad);

  std::vector<pir::Type> argument_outputs;
  pir::Type x_grad_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_x_grad.dtype()), dense_x_grad.dims(), dense_x_grad.layout(), dense_x_grad.lod(), dense_x_grad.offset());
  argument_outputs.push_back(x_grad_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void Pad3dGradOp::VerifySig() {}

void Pad3dGradOp::InferMeta( phi::InferMetaContext *infer_meta ) {
  auto fn = PD_INFER_META(phi::UnchangedInferMeta);
  fn(infer_meta);
}

phi::DataType Pad3dGradOp::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: Pad3dGradOp";
  


  return expected_kernel_dtype;
}

const char *PixelShuffleGradOp::attributes_name[2] = { "upscale_factor", "data_format" };

OpInfoTuple PixelShuffleGradOp::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("out_grad", "paddle::dialect::DenseTensorType", false, false, false, false) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = { paddle::dialect::OpAttributeInfo("upscale_factor", "pir::Int32Attribute", ""), paddle::dialect::OpAttributeInfo("data_format", "pir::StrAttribute", "") };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("x_grad", "paddle::dialect::DenseTensorType", false, false) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("PixelShuffleGradInferMeta", {"out_grad", "upscale_factor", "data_format"}, "pixel_shuffle_grad", {"out_grad", "upscale_factor", "data_format"}, {}, {}, {}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "pixel_shuffle_grad");
}

void PixelShuffleGradOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value out_grad_, int upscale_factor, const std::string& data_format) {
  VLOG(4) << "Start build PixelShuffleGradOp";



  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {out_grad_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_upscale_factor = pir::Int32Attribute::get(pir::IrContext::Instance(), upscale_factor);
  argument.AddAttribute("upscale_factor", attr_upscale_factor);
  pir::Attribute attr_data_format = pir::StrAttribute::get(pir::IrContext::Instance(), data_format);
  argument.AddAttribute("data_format", attr_data_format);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType out_grad = out_grad_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)out_grad;

  VLOG(4) << "Builder construction  dense_out_grad";
  paddle::dialect::IrTensor ir_tensor_out_grad(paddle::dialect::TransToPhiDataType(out_grad.dtype()),
                                                      out_grad.dims(),
                                                      out_grad.data_layout(),
                                                      out_grad.lod(),
                                                      out_grad.offset());
  VLOG(4) << "Builder construction  meta_out_grad";
  paddle::dialect::IrMetaTensor meta_out_grad(&ir_tensor_out_grad);
  paddle::dialect::IrTensor dense_x_grad;
  paddle::dialect::IrMetaTensor meta_x_grad(&dense_x_grad);

  phi::PixelShuffleGradInferMeta(meta_out_grad, upscale_factor, data_format, &meta_x_grad);

  std::vector<pir::Type> argument_outputs;
  pir::Type x_grad_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_x_grad.dtype()), dense_x_grad.dims(), dense_x_grad.layout(), dense_x_grad.lod(), dense_x_grad.offset());
  argument_outputs.push_back(x_grad_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void PixelShuffleGradOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value out_grad_, pir::AttributeMap attributes) {
  VLOG(4) << "Start build PixelShuffleGradOp";


  IR_ENFORCE(
      attributes.find("upscale_factor") != attributes.end(),
          "'upscale_factor' Attribute is expected for PixelShuffleGradOp. ");
  int upscale_factor = attributes.at("upscale_factor").dyn_cast<pir::Int32Attribute>().data();

  IR_ENFORCE(
      attributes.find("data_format") != attributes.end(),
          "'data_format' Attribute is expected for PixelShuffleGradOp. ");
  std::string data_format = attributes.at("data_format").dyn_cast<pir::StrAttribute>().AsString();


  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {out_grad_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_upscale_factor = pir::Int32Attribute::get(pir::IrContext::Instance(), upscale_factor);
  argument.AddAttribute("upscale_factor", attr_upscale_factor);
  pir::Attribute attr_data_format = pir::StrAttribute::get(pir::IrContext::Instance(), data_format);
  argument.AddAttribute("data_format", attr_data_format);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType out_grad = out_grad_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)out_grad;

  VLOG(4) << "Builder construction  dense_out_grad";
  paddle::dialect::IrTensor ir_tensor_out_grad(paddle::dialect::TransToPhiDataType(out_grad.dtype()),
                                                      out_grad.dims(),
                                                      out_grad.data_layout(),
                                                      out_grad.lod(),
                                                      out_grad.offset());
  VLOG(4) << "Builder construction  meta_out_grad";
  paddle::dialect::IrMetaTensor meta_out_grad(&ir_tensor_out_grad);
  paddle::dialect::IrTensor dense_x_grad;
  paddle::dialect::IrMetaTensor meta_x_grad(&dense_x_grad);

  phi::PixelShuffleGradInferMeta(meta_out_grad, upscale_factor, data_format, &meta_x_grad);

  std::vector<pir::Type> argument_outputs;
  pir::Type x_grad_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_x_grad.dtype()), dense_x_grad.dims(), dense_x_grad.layout(), dense_x_grad.lod(), dense_x_grad.offset());
  argument_outputs.push_back(x_grad_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void PixelShuffleGradOp::VerifySig() {}

void PixelShuffleGradOp::InferMeta( phi::InferMetaContext *infer_meta ) {
  auto fn = PD_INFER_META(phi::PixelShuffleGradInferMeta);
  fn(infer_meta);
}

phi::DataType PixelShuffleGradOp::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: PixelShuffleGradOp";
  


  return expected_kernel_dtype;
}

const char *PixelUnshuffleGradOp::attributes_name[2] = { "downscale_factor", "data_format" };

OpInfoTuple PixelUnshuffleGradOp::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("out_grad", "paddle::dialect::DenseTensorType", false, false, false, false) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = { paddle::dialect::OpAttributeInfo("downscale_factor", "pir::Int32Attribute", ""), paddle::dialect::OpAttributeInfo("data_format", "pir::StrAttribute", "") };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("x_grad", "paddle::dialect::DenseTensorType", false, false) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("PixelUnshuffleGradInferMeta", {"out_grad", "downscale_factor", "data_format"}, "pixel_unshuffle_grad", {"out_grad", "downscale_factor", "data_format"}, {}, {}, {}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "pixel_unshuffle_grad");
}

void PixelUnshuffleGradOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value out_grad_, int downscale_factor, const std::string& data_format) {
  VLOG(4) << "Start build PixelUnshuffleGradOp";



  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {out_grad_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_downscale_factor = pir::Int32Attribute::get(pir::IrContext::Instance(), downscale_factor);
  argument.AddAttribute("downscale_factor", attr_downscale_factor);
  pir::Attribute attr_data_format = pir::StrAttribute::get(pir::IrContext::Instance(), data_format);
  argument.AddAttribute("data_format", attr_data_format);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType out_grad = out_grad_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)out_grad;

  VLOG(4) << "Builder construction  dense_out_grad";
  paddle::dialect::IrTensor ir_tensor_out_grad(paddle::dialect::TransToPhiDataType(out_grad.dtype()),
                                                      out_grad.dims(),
                                                      out_grad.data_layout(),
                                                      out_grad.lod(),
                                                      out_grad.offset());
  VLOG(4) << "Builder construction  meta_out_grad";
  paddle::dialect::IrMetaTensor meta_out_grad(&ir_tensor_out_grad);
  paddle::dialect::IrTensor dense_x_grad;
  paddle::dialect::IrMetaTensor meta_x_grad(&dense_x_grad);

  phi::PixelUnshuffleGradInferMeta(meta_out_grad, downscale_factor, data_format, &meta_x_grad);

  std::vector<pir::Type> argument_outputs;
  pir::Type x_grad_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_x_grad.dtype()), dense_x_grad.dims(), dense_x_grad.layout(), dense_x_grad.lod(), dense_x_grad.offset());
  argument_outputs.push_back(x_grad_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void PixelUnshuffleGradOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value out_grad_, pir::AttributeMap attributes) {
  VLOG(4) << "Start build PixelUnshuffleGradOp";


  IR_ENFORCE(
      attributes.find("downscale_factor") != attributes.end(),
          "'downscale_factor' Attribute is expected for PixelUnshuffleGradOp. ");
  int downscale_factor = attributes.at("downscale_factor").dyn_cast<pir::Int32Attribute>().data();

  IR_ENFORCE(
      attributes.find("data_format") != attributes.end(),
          "'data_format' Attribute is expected for PixelUnshuffleGradOp. ");
  std::string data_format = attributes.at("data_format").dyn_cast<pir::StrAttribute>().AsString();


  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {out_grad_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_downscale_factor = pir::Int32Attribute::get(pir::IrContext::Instance(), downscale_factor);
  argument.AddAttribute("downscale_factor", attr_downscale_factor);
  pir::Attribute attr_data_format = pir::StrAttribute::get(pir::IrContext::Instance(), data_format);
  argument.AddAttribute("data_format", attr_data_format);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType out_grad = out_grad_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)out_grad;

  VLOG(4) << "Builder construction  dense_out_grad";
  paddle::dialect::IrTensor ir_tensor_out_grad(paddle::dialect::TransToPhiDataType(out_grad.dtype()),
                                                      out_grad.dims(),
                                                      out_grad.data_layout(),
                                                      out_grad.lod(),
                                                      out_grad.offset());
  VLOG(4) << "Builder construction  meta_out_grad";
  paddle::dialect::IrMetaTensor meta_out_grad(&ir_tensor_out_grad);
  paddle::dialect::IrTensor dense_x_grad;
  paddle::dialect::IrMetaTensor meta_x_grad(&dense_x_grad);

  phi::PixelUnshuffleGradInferMeta(meta_out_grad, downscale_factor, data_format, &meta_x_grad);

  std::vector<pir::Type> argument_outputs;
  pir::Type x_grad_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_x_grad.dtype()), dense_x_grad.dims(), dense_x_grad.layout(), dense_x_grad.lod(), dense_x_grad.offset());
  argument_outputs.push_back(x_grad_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void PixelUnshuffleGradOp::VerifySig() {}

void PixelUnshuffleGradOp::InferMeta( phi::InferMetaContext *infer_meta ) {
  auto fn = PD_INFER_META(phi::PixelUnshuffleGradInferMeta);
  fn(infer_meta);
}

phi::DataType PixelUnshuffleGradOp::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: PixelUnshuffleGradOp";
  


  return expected_kernel_dtype;
}

OpInfoTuple PoissonGradOp::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("out_grad", "paddle::dialect::DenseTensorType", false, false, false, false) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = {  };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("x_grad", "paddle::dialect::DenseTensorType", false, false) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("UnchangedInferMeta", {"out_grad"}, "poisson_grad", {"out_grad"}, {}, {}, {}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "poisson_grad");
}

void PoissonGradOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value out_grad_) {
  VLOG(4) << "Start build PoissonGradOp";



  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {out_grad_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType out_grad = out_grad_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)out_grad;

  VLOG(4) << "Builder construction  dense_out_grad";
  paddle::dialect::IrTensor ir_tensor_out_grad(paddle::dialect::TransToPhiDataType(out_grad.dtype()),
                                                      out_grad.dims(),
                                                      out_grad.data_layout(),
                                                      out_grad.lod(),
                                                      out_grad.offset());
  VLOG(4) << "Builder construction  meta_out_grad";
  paddle::dialect::IrMetaTensor meta_out_grad(&ir_tensor_out_grad);
  paddle::dialect::IrTensor dense_x_grad;
  paddle::dialect::IrMetaTensor meta_x_grad(&dense_x_grad);

  phi::UnchangedInferMeta(meta_out_grad, &meta_x_grad);

  std::vector<pir::Type> argument_outputs;
  pir::Type x_grad_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_x_grad.dtype()), dense_x_grad.dims(), dense_x_grad.layout(), dense_x_grad.lod(), dense_x_grad.offset());
  argument_outputs.push_back(x_grad_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void PoissonGradOp::VerifySig() {}

void PoissonGradOp::InferMeta( phi::InferMetaContext *infer_meta ) {
  auto fn = PD_INFER_META(phi::UnchangedInferMeta);
  fn(infer_meta);
}

phi::DataType PoissonGradOp::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: PoissonGradOp";
  


  return expected_kernel_dtype;
}

const char *PolygammaGradOp::attributes_name[1] = { "n" };

OpInfoTuple PolygammaGradOp::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("x", "paddle::dialect::DenseTensorType", false, false, false, false), paddle::dialect::OpInputInfo("out_grad", "paddle::dialect::DenseTensorType", false, false, false, false) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = { paddle::dialect::OpAttributeInfo("n", "pir::Int32Attribute", "") };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("x_grad", "paddle::dialect::DenseTensorType", false, false) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("UnchangedInferMeta", {"x"}, "polygamma_grad", {"x", "out_grad", "n"}, {}, {}, {}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "polygamma_grad");
}

void PolygammaGradOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, pir::Value out_grad_, int n) {
  VLOG(4) << "Start build PolygammaGradOp";



  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_, out_grad_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_n = pir::Int32Attribute::get(pir::IrContext::Instance(), n);
  argument.AddAttribute("n", attr_n);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;
  paddle::dialect::DenseTensorType out_grad = out_grad_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)out_grad;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);
  paddle::dialect::IrTensor dense_x_grad;
  paddle::dialect::IrMetaTensor meta_x_grad(&dense_x_grad);

  phi::UnchangedInferMeta(meta_x, &meta_x_grad);

  std::vector<pir::Type> argument_outputs;
  pir::Type x_grad_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_x_grad.dtype()), dense_x_grad.dims(), dense_x_grad.layout(), dense_x_grad.lod(), dense_x_grad.offset());
  argument_outputs.push_back(x_grad_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void PolygammaGradOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, pir::Value out_grad_, pir::AttributeMap attributes) {
  VLOG(4) << "Start build PolygammaGradOp";


  IR_ENFORCE(
      attributes.find("n") != attributes.end(),
          "'n' Attribute is expected for PolygammaGradOp. ");
  int n = attributes.at("n").dyn_cast<pir::Int32Attribute>().data();


  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_, out_grad_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_n = pir::Int32Attribute::get(pir::IrContext::Instance(), n);
  argument.AddAttribute("n", attr_n);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;
  paddle::dialect::DenseTensorType out_grad = out_grad_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)out_grad;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);
  paddle::dialect::IrTensor dense_x_grad;
  paddle::dialect::IrMetaTensor meta_x_grad(&dense_x_grad);

  phi::UnchangedInferMeta(meta_x, &meta_x_grad);

  std::vector<pir::Type> argument_outputs;
  pir::Type x_grad_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_x_grad.dtype()), dense_x_grad.dims(), dense_x_grad.layout(), dense_x_grad.lod(), dense_x_grad.offset());
  argument_outputs.push_back(x_grad_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void PolygammaGradOp::VerifySig() {}

void PolygammaGradOp::InferMeta( phi::InferMetaContext *infer_meta ) {
  auto fn = PD_INFER_META(phi::UnchangedInferMeta);
  fn(infer_meta);
}

phi::DataType PolygammaGradOp::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: PolygammaGradOp";
  


  return expected_kernel_dtype;
}

const char *PowDoubleGradOp::attributes_name[1] = { "y" };

OpInfoTuple PowDoubleGradOp::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("x", "paddle::dialect::DenseTensorType", false, false, false, true), paddle::dialect::OpInputInfo("grad_out", "paddle::dialect::DenseTensorType", false, false, false, true), paddle::dialect::OpInputInfo("grad_x_grad", "paddle::dialect::DenseTensorType", false, false, false, true) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = { paddle::dialect::OpAttributeInfo("y", "paddle::dialect::ScalarAttribute", "float") };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("x_grad", "paddle::dialect::DenseTensorType", false, false), paddle::dialect::OpOutputInfo("grad_out_grad", "paddle::dialect::DenseTensorType", false, false) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("GeneralBinaryGradInferMeta", {"x", "grad_out"}, "pow_double_grad", {"x", "grad_out", "grad_x_grad", "y"}, {"x"}, {}, {}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "pow_double_grad");
}

void PowDoubleGradOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, pir::Value grad_out_, pir::Value grad_x_grad_, float y) {
  VLOG(4) << "Start build PowDoubleGradOp";



  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_, grad_out_, grad_x_grad_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_y = paddle::dialect::TransToIrAttribute(y, pir::IrContext::Instance());
  argument.AddAttribute("y", attr_y);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;
  paddle::dialect::DenseTensorType grad_out = grad_out_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)grad_out;
  paddle::dialect::DenseTensorType grad_x_grad = grad_x_grad_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)grad_x_grad;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);

  VLOG(4) << "Builder construction  dense_grad_out";
  paddle::dialect::IrTensor ir_tensor_grad_out(paddle::dialect::TransToPhiDataType(grad_out.dtype()),
                                                      grad_out.dims(),
                                                      grad_out.data_layout(),
                                                      grad_out.lod(),
                                                      grad_out.offset());
  VLOG(4) << "Builder construction  meta_grad_out";
  paddle::dialect::IrMetaTensor meta_grad_out(&ir_tensor_grad_out);
  paddle::dialect::IrTensor dense_x_grad;
  paddle::dialect::IrMetaTensor meta_x_grad(&dense_x_grad);
  paddle::dialect::IrTensor dense_grad_out_grad;
  paddle::dialect::IrMetaTensor meta_grad_out_grad(&dense_grad_out_grad);

  phi::GeneralBinaryGradInferMeta(meta_x, meta_grad_out, &meta_x_grad, &meta_grad_out_grad);

  std::vector<pir::Type> argument_outputs;
  pir::Type x_grad_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_x_grad.dtype()), dense_x_grad.dims(), dense_x_grad.layout(), dense_x_grad.lod(), dense_x_grad.offset());
  argument_outputs.push_back(x_grad_dense_tensor_type);

  pir::Type grad_out_grad_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_grad_out_grad.dtype()), dense_grad_out_grad.dims(), dense_grad_out_grad.layout(), dense_grad_out_grad.lod(), dense_grad_out_grad.offset());
  argument_outputs.push_back(grad_out_grad_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void PowDoubleGradOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, pir::Value grad_out_, pir::Value grad_x_grad_, pir::AttributeMap attributes) {
  VLOG(4) << "Start build PowDoubleGradOp";


  IR_ENFORCE(
      attributes.find("y") != attributes.end(),
          "'y' Attribute is expected for PowDoubleGradOp. ");
  float y = attributes.at("y").dyn_cast<paddle::dialect::ScalarAttribute>().data().to<float>();


  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_, grad_out_, grad_x_grad_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_y = paddle::dialect::TransToIrAttribute(y, pir::IrContext::Instance());
  argument.AddAttribute("y", attr_y);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;
  paddle::dialect::DenseTensorType grad_out = grad_out_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)grad_out;
  paddle::dialect::DenseTensorType grad_x_grad = grad_x_grad_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)grad_x_grad;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);

  VLOG(4) << "Builder construction  dense_grad_out";
  paddle::dialect::IrTensor ir_tensor_grad_out(paddle::dialect::TransToPhiDataType(grad_out.dtype()),
                                                      grad_out.dims(),
                                                      grad_out.data_layout(),
                                                      grad_out.lod(),
                                                      grad_out.offset());
  VLOG(4) << "Builder construction  meta_grad_out";
  paddle::dialect::IrMetaTensor meta_grad_out(&ir_tensor_grad_out);
  paddle::dialect::IrTensor dense_x_grad;
  paddle::dialect::IrMetaTensor meta_x_grad(&dense_x_grad);
  paddle::dialect::IrTensor dense_grad_out_grad;
  paddle::dialect::IrMetaTensor meta_grad_out_grad(&dense_grad_out_grad);

  phi::GeneralBinaryGradInferMeta(meta_x, meta_grad_out, &meta_x_grad, &meta_grad_out_grad);

  std::vector<pir::Type> argument_outputs;
  pir::Type x_grad_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_x_grad.dtype()), dense_x_grad.dims(), dense_x_grad.layout(), dense_x_grad.lod(), dense_x_grad.offset());
  argument_outputs.push_back(x_grad_dense_tensor_type);

  pir::Type grad_out_grad_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_grad_out_grad.dtype()), dense_grad_out_grad.dims(), dense_grad_out_grad.layout(), dense_grad_out_grad.lod(), dense_grad_out_grad.offset());
  argument_outputs.push_back(grad_out_grad_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void PowDoubleGradOp::VerifySig() {}

void PowDoubleGradOp::InferMeta( phi::InferMetaContext *infer_meta ) {
  auto fn = PD_INFER_META(phi::GeneralBinaryGradInferMeta);
  fn(infer_meta);
}

phi::DataType PowDoubleGradOp::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: PowDoubleGradOp";
  


  return expected_kernel_dtype;
}

const char *PowDoubleGrad_Op::attributes_name[1] = { "y" };

OpInfoTuple PowDoubleGrad_Op::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("x", "paddle::dialect::DenseTensorType", false, false, false, true), paddle::dialect::OpInputInfo("grad_out", "paddle::dialect::DenseTensorType", false, false, false, true), paddle::dialect::OpInputInfo("grad_x_grad", "paddle::dialect::DenseTensorType", false, false, false, true) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = { paddle::dialect::OpAttributeInfo("y", "paddle::dialect::ScalarAttribute", "float") };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("x_grad", "paddle::dialect::DenseTensorType", false, false), paddle::dialect::OpOutputInfo("grad_out_grad", "paddle::dialect::DenseTensorType", false, false) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("GeneralBinaryGradInferMeta", {"x", "grad_out"}, "pow_double_grad", {"x", "grad_out", "grad_x_grad", "y"}, {"x"}, {}, {{"x_grad", "grad_x_grad"}}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "pow_double_grad");
}

void PowDoubleGrad_Op::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, pir::Value grad_out_, pir::Value grad_x_grad_, float y) {
  VLOG(4) << "Start build PowDoubleGrad_Op";



  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_, grad_out_, grad_x_grad_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_y = paddle::dialect::TransToIrAttribute(y, pir::IrContext::Instance());
  argument.AddAttribute("y", attr_y);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;
  paddle::dialect::DenseTensorType grad_out = grad_out_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)grad_out;
  paddle::dialect::DenseTensorType grad_x_grad = grad_x_grad_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)grad_x_grad;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);

  VLOG(4) << "Builder construction  dense_grad_out";
  paddle::dialect::IrTensor ir_tensor_grad_out(paddle::dialect::TransToPhiDataType(grad_out.dtype()),
                                                      grad_out.dims(),
                                                      grad_out.data_layout(),
                                                      grad_out.lod(),
                                                      grad_out.offset());
  VLOG(4) << "Builder construction  meta_grad_out";
  paddle::dialect::IrMetaTensor meta_grad_out(&ir_tensor_grad_out);
  paddle::dialect::IrTensor dense_x_grad;
  paddle::dialect::IrMetaTensor meta_x_grad(&dense_x_grad);
  paddle::dialect::IrTensor dense_grad_out_grad;
  paddle::dialect::IrMetaTensor meta_grad_out_grad(&dense_grad_out_grad);

  phi::GeneralBinaryGradInferMeta(meta_x, meta_grad_out, &meta_x_grad, &meta_grad_out_grad);

  std::vector<pir::Type> argument_outputs;
  pir::Type x_grad_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_x_grad.dtype()), dense_x_grad.dims(), dense_x_grad.layout(), dense_x_grad.lod(), dense_x_grad.offset());
  argument_outputs.push_back(x_grad_dense_tensor_type);

  pir::Type grad_out_grad_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_grad_out_grad.dtype()), dense_grad_out_grad.dims(), dense_grad_out_grad.layout(), dense_grad_out_grad.lod(), dense_grad_out_grad.offset());
  argument_outputs.push_back(grad_out_grad_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void PowDoubleGrad_Op::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, pir::Value grad_out_, pir::Value grad_x_grad_, pir::AttributeMap attributes) {
  VLOG(4) << "Start build PowDoubleGrad_Op";


  IR_ENFORCE(
      attributes.find("y") != attributes.end(),
          "'y' Attribute is expected for PowDoubleGrad_Op. ");
  float y = attributes.at("y").dyn_cast<paddle::dialect::ScalarAttribute>().data().to<float>();


  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_, grad_out_, grad_x_grad_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_y = paddle::dialect::TransToIrAttribute(y, pir::IrContext::Instance());
  argument.AddAttribute("y", attr_y);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;
  paddle::dialect::DenseTensorType grad_out = grad_out_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)grad_out;
  paddle::dialect::DenseTensorType grad_x_grad = grad_x_grad_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)grad_x_grad;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);

  VLOG(4) << "Builder construction  dense_grad_out";
  paddle::dialect::IrTensor ir_tensor_grad_out(paddle::dialect::TransToPhiDataType(grad_out.dtype()),
                                                      grad_out.dims(),
                                                      grad_out.data_layout(),
                                                      grad_out.lod(),
                                                      grad_out.offset());
  VLOG(4) << "Builder construction  meta_grad_out";
  paddle::dialect::IrMetaTensor meta_grad_out(&ir_tensor_grad_out);
  paddle::dialect::IrTensor dense_x_grad;
  paddle::dialect::IrMetaTensor meta_x_grad(&dense_x_grad);
  paddle::dialect::IrTensor dense_grad_out_grad;
  paddle::dialect::IrMetaTensor meta_grad_out_grad(&dense_grad_out_grad);

  phi::GeneralBinaryGradInferMeta(meta_x, meta_grad_out, &meta_x_grad, &meta_grad_out_grad);

  std::vector<pir::Type> argument_outputs;
  pir::Type x_grad_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_x_grad.dtype()), dense_x_grad.dims(), dense_x_grad.layout(), dense_x_grad.lod(), dense_x_grad.offset());
  argument_outputs.push_back(x_grad_dense_tensor_type);

  pir::Type grad_out_grad_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_grad_out_grad.dtype()), dense_grad_out_grad.dims(), dense_grad_out_grad.layout(), dense_grad_out_grad.lod(), dense_grad_out_grad.offset());
  argument_outputs.push_back(grad_out_grad_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void PowDoubleGrad_Op::VerifySig() {}

void PowDoubleGrad_Op::InferMeta( phi::InferMetaContext *infer_meta ) {
  auto fn = PD_INFER_META(phi::GeneralBinaryGradInferMeta);
  fn(infer_meta);
}

phi::DataType PowDoubleGrad_Op::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: PowDoubleGrad_Op";
  


  return expected_kernel_dtype;
}

const char *PowGradOp::attributes_name[1] = { "y" };

OpInfoTuple PowGradOp::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("x", "paddle::dialect::DenseTensorType", false, false, false, true), paddle::dialect::OpInputInfo("out_grad", "paddle::dialect::DenseTensorType", false, false, false, true) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = { paddle::dialect::OpAttributeInfo("y", "paddle::dialect::ScalarAttribute", "float") };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("x_grad", "paddle::dialect::DenseTensorType", false, false) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("UnchangedInferMeta", {"x"}, "pow_grad", {"x", "out_grad", "y"}, {"out_grad"}, {}, {}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "pow_grad");
}

void PowGradOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, pir::Value out_grad_, float y) {
  VLOG(4) << "Start build PowGradOp";



  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_, out_grad_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_y = paddle::dialect::TransToIrAttribute(y, pir::IrContext::Instance());
  argument.AddAttribute("y", attr_y);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;
  paddle::dialect::DenseTensorType out_grad = out_grad_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)out_grad;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);
  paddle::dialect::IrTensor dense_x_grad;
  paddle::dialect::IrMetaTensor meta_x_grad(&dense_x_grad);

  phi::UnchangedInferMeta(meta_x, &meta_x_grad);

  std::vector<pir::Type> argument_outputs;
  pir::Type x_grad_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_x_grad.dtype()), dense_x_grad.dims(), dense_x_grad.layout(), dense_x_grad.lod(), dense_x_grad.offset());
  argument_outputs.push_back(x_grad_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void PowGradOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, pir::Value out_grad_, pir::AttributeMap attributes) {
  VLOG(4) << "Start build PowGradOp";


  IR_ENFORCE(
      attributes.find("y") != attributes.end(),
          "'y' Attribute is expected for PowGradOp. ");
  float y = attributes.at("y").dyn_cast<paddle::dialect::ScalarAttribute>().data().to<float>();


  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_, out_grad_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_y = paddle::dialect::TransToIrAttribute(y, pir::IrContext::Instance());
  argument.AddAttribute("y", attr_y);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;
  paddle::dialect::DenseTensorType out_grad = out_grad_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)out_grad;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);
  paddle::dialect::IrTensor dense_x_grad;
  paddle::dialect::IrMetaTensor meta_x_grad(&dense_x_grad);

  phi::UnchangedInferMeta(meta_x, &meta_x_grad);

  std::vector<pir::Type> argument_outputs;
  pir::Type x_grad_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_x_grad.dtype()), dense_x_grad.dims(), dense_x_grad.layout(), dense_x_grad.lod(), dense_x_grad.offset());
  argument_outputs.push_back(x_grad_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void PowGradOp::VerifySig() {}

void PowGradOp::InferMeta( phi::InferMetaContext *infer_meta ) {
  auto fn = PD_INFER_META(phi::UnchangedInferMeta);
  fn(infer_meta);
}

phi::DataType PowGradOp::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: PowGradOp";
  


  return expected_kernel_dtype;
}

const char *PowGrad_Op::attributes_name[1] = { "y" };

OpInfoTuple PowGrad_Op::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("x", "paddle::dialect::DenseTensorType", false, false, false, true), paddle::dialect::OpInputInfo("out_grad", "paddle::dialect::DenseTensorType", false, false, false, true) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = { paddle::dialect::OpAttributeInfo("y", "paddle::dialect::ScalarAttribute", "float") };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("x_grad", "paddle::dialect::DenseTensorType", false, false) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("UnchangedInferMeta", {"x"}, "pow_grad", {"x", "out_grad", "y"}, {"out_grad"}, {}, {{"x_grad", "out_grad"}}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "pow_grad");
}

void PowGrad_Op::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, pir::Value out_grad_, float y) {
  VLOG(4) << "Start build PowGrad_Op";



  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_, out_grad_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_y = paddle::dialect::TransToIrAttribute(y, pir::IrContext::Instance());
  argument.AddAttribute("y", attr_y);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;
  paddle::dialect::DenseTensorType out_grad = out_grad_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)out_grad;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);
  paddle::dialect::IrTensor dense_x_grad;
  paddle::dialect::IrMetaTensor meta_x_grad(&dense_x_grad);

  phi::UnchangedInferMeta(meta_x, &meta_x_grad);

  std::vector<pir::Type> argument_outputs;
  pir::Type x_grad_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_x_grad.dtype()), dense_x_grad.dims(), dense_x_grad.layout(), dense_x_grad.lod(), dense_x_grad.offset());
  argument_outputs.push_back(x_grad_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void PowGrad_Op::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, pir::Value out_grad_, pir::AttributeMap attributes) {
  VLOG(4) << "Start build PowGrad_Op";


  IR_ENFORCE(
      attributes.find("y") != attributes.end(),
          "'y' Attribute is expected for PowGrad_Op. ");
  float y = attributes.at("y").dyn_cast<paddle::dialect::ScalarAttribute>().data().to<float>();


  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_, out_grad_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_y = paddle::dialect::TransToIrAttribute(y, pir::IrContext::Instance());
  argument.AddAttribute("y", attr_y);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;
  paddle::dialect::DenseTensorType out_grad = out_grad_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)out_grad;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);
  paddle::dialect::IrTensor dense_x_grad;
  paddle::dialect::IrMetaTensor meta_x_grad(&dense_x_grad);

  phi::UnchangedInferMeta(meta_x, &meta_x_grad);

  std::vector<pir::Type> argument_outputs;
  pir::Type x_grad_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_x_grad.dtype()), dense_x_grad.dims(), dense_x_grad.layout(), dense_x_grad.lod(), dense_x_grad.offset());
  argument_outputs.push_back(x_grad_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void PowGrad_Op::VerifySig() {}

void PowGrad_Op::InferMeta( phi::InferMetaContext *infer_meta ) {
  auto fn = PD_INFER_META(phi::UnchangedInferMeta);
  fn(infer_meta);
}

phi::DataType PowGrad_Op::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: PowGrad_Op";
  


  return expected_kernel_dtype;
}

const char *PowTripleGradOp::attributes_name[1] = { "y" };

OpInfoTuple PowTripleGradOp::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("x", "paddle::dialect::DenseTensorType", false, false, false, false), paddle::dialect::OpInputInfo("grad_out", "paddle::dialect::DenseTensorType", false, false, false, false), paddle::dialect::OpInputInfo("grad_grad_x", "paddle::dialect::DenseTensorType", false, false, false, false), paddle::dialect::OpInputInfo("grad_x_grad", "paddle::dialect::DenseTensorType", false, false, false, false), paddle::dialect::OpInputInfo("grad_grad_out_grad", "paddle::dialect::DenseTensorType", true, false, false, false) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = { paddle::dialect::OpAttributeInfo("y", "paddle::dialect::ScalarAttribute", "float") };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("x_grad", "paddle::dialect::DenseTensorType", false, false), paddle::dialect::OpOutputInfo("grad_out_grad", "paddle::dialect::DenseTensorType", false, false), paddle::dialect::OpOutputInfo("grad_grad_x_grad", "paddle::dialect::DenseTensorType", false, false) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("GeneralTernaryGradInferMeta", {"x", "grad_out", "grad_grad_x"}, "pow_triple_grad", {"x", "grad_out", "grad_grad_x", "grad_x_grad", "grad_grad_out_grad", "y"}, {"x"}, {}, {}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "pow_triple_grad");
}

void PowTripleGradOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, pir::Value grad_out_, pir::Value grad_grad_x_, pir::Value grad_x_grad_, pir::Value grad_grad_out_grad_, float y) {
  VLOG(4) << "Start build PowTripleGradOp";



  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_, grad_out_, grad_grad_x_, grad_x_grad_, grad_grad_out_grad_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_y = paddle::dialect::TransToIrAttribute(y, pir::IrContext::Instance());
  argument.AddAttribute("y", attr_y);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;
  paddle::dialect::DenseTensorType grad_out = grad_out_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)grad_out;
  paddle::dialect::DenseTensorType grad_grad_x = grad_grad_x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)grad_grad_x;
  paddle::dialect::DenseTensorType grad_x_grad = grad_x_grad_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)grad_x_grad;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);

  VLOG(4) << "Builder construction  dense_grad_out";
  paddle::dialect::IrTensor ir_tensor_grad_out(paddle::dialect::TransToPhiDataType(grad_out.dtype()),
                                                      grad_out.dims(),
                                                      grad_out.data_layout(),
                                                      grad_out.lod(),
                                                      grad_out.offset());
  VLOG(4) << "Builder construction  meta_grad_out";
  paddle::dialect::IrMetaTensor meta_grad_out(&ir_tensor_grad_out);

  VLOG(4) << "Builder construction  dense_grad_grad_x";
  paddle::dialect::IrTensor ir_tensor_grad_grad_x(paddle::dialect::TransToPhiDataType(grad_grad_x.dtype()),
                                                      grad_grad_x.dims(),
                                                      grad_grad_x.data_layout(),
                                                      grad_grad_x.lod(),
                                                      grad_grad_x.offset());
  VLOG(4) << "Builder construction  meta_grad_grad_x";
  paddle::dialect::IrMetaTensor meta_grad_grad_x(&ir_tensor_grad_grad_x);
  paddle::dialect::IrTensor dense_x_grad;
  paddle::dialect::IrMetaTensor meta_x_grad(&dense_x_grad);
  paddle::dialect::IrTensor dense_grad_out_grad;
  paddle::dialect::IrMetaTensor meta_grad_out_grad(&dense_grad_out_grad);
  paddle::dialect::IrTensor dense_grad_grad_x_grad;
  paddle::dialect::IrMetaTensor meta_grad_grad_x_grad(&dense_grad_grad_x_grad);

  phi::GeneralTernaryGradInferMeta(meta_x, meta_grad_out, meta_grad_grad_x, &meta_x_grad, &meta_grad_out_grad, &meta_grad_grad_x_grad);

  std::vector<pir::Type> argument_outputs;
  pir::Type x_grad_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_x_grad.dtype()), dense_x_grad.dims(), dense_x_grad.layout(), dense_x_grad.lod(), dense_x_grad.offset());
  argument_outputs.push_back(x_grad_dense_tensor_type);

  pir::Type grad_out_grad_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_grad_out_grad.dtype()), dense_grad_out_grad.dims(), dense_grad_out_grad.layout(), dense_grad_out_grad.lod(), dense_grad_out_grad.offset());
  argument_outputs.push_back(grad_out_grad_dense_tensor_type);

  pir::Type grad_grad_x_grad_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_grad_grad_x_grad.dtype()), dense_grad_grad_x_grad.dims(), dense_grad_grad_x_grad.layout(), dense_grad_grad_x_grad.lod(), dense_grad_grad_x_grad.offset());
  argument_outputs.push_back(grad_grad_x_grad_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void PowTripleGradOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, pir::Value grad_out_, pir::Value grad_grad_x_, pir::Value grad_x_grad_, pir::Value grad_grad_out_grad_, pir::AttributeMap attributes) {
  VLOG(4) << "Start build PowTripleGradOp";


  IR_ENFORCE(
      attributes.find("y") != attributes.end(),
          "'y' Attribute is expected for PowTripleGradOp. ");
  float y = attributes.at("y").dyn_cast<paddle::dialect::ScalarAttribute>().data().to<float>();


  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_, grad_out_, grad_grad_x_, grad_x_grad_, grad_grad_out_grad_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_y = paddle::dialect::TransToIrAttribute(y, pir::IrContext::Instance());
  argument.AddAttribute("y", attr_y);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;
  paddle::dialect::DenseTensorType grad_out = grad_out_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)grad_out;
  paddle::dialect::DenseTensorType grad_grad_x = grad_grad_x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)grad_grad_x;
  paddle::dialect::DenseTensorType grad_x_grad = grad_x_grad_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)grad_x_grad;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);

  VLOG(4) << "Builder construction  dense_grad_out";
  paddle::dialect::IrTensor ir_tensor_grad_out(paddle::dialect::TransToPhiDataType(grad_out.dtype()),
                                                      grad_out.dims(),
                                                      grad_out.data_layout(),
                                                      grad_out.lod(),
                                                      grad_out.offset());
  VLOG(4) << "Builder construction  meta_grad_out";
  paddle::dialect::IrMetaTensor meta_grad_out(&ir_tensor_grad_out);

  VLOG(4) << "Builder construction  dense_grad_grad_x";
  paddle::dialect::IrTensor ir_tensor_grad_grad_x(paddle::dialect::TransToPhiDataType(grad_grad_x.dtype()),
                                                      grad_grad_x.dims(),
                                                      grad_grad_x.data_layout(),
                                                      grad_grad_x.lod(),
                                                      grad_grad_x.offset());
  VLOG(4) << "Builder construction  meta_grad_grad_x";
  paddle::dialect::IrMetaTensor meta_grad_grad_x(&ir_tensor_grad_grad_x);
  paddle::dialect::IrTensor dense_x_grad;
  paddle::dialect::IrMetaTensor meta_x_grad(&dense_x_grad);
  paddle::dialect::IrTensor dense_grad_out_grad;
  paddle::dialect::IrMetaTensor meta_grad_out_grad(&dense_grad_out_grad);
  paddle::dialect::IrTensor dense_grad_grad_x_grad;
  paddle::dialect::IrMetaTensor meta_grad_grad_x_grad(&dense_grad_grad_x_grad);

  phi::GeneralTernaryGradInferMeta(meta_x, meta_grad_out, meta_grad_grad_x, &meta_x_grad, &meta_grad_out_grad, &meta_grad_grad_x_grad);

  std::vector<pir::Type> argument_outputs;
  pir::Type x_grad_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_x_grad.dtype()), dense_x_grad.dims(), dense_x_grad.layout(), dense_x_grad.lod(), dense_x_grad.offset());
  argument_outputs.push_back(x_grad_dense_tensor_type);

  pir::Type grad_out_grad_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_grad_out_grad.dtype()), dense_grad_out_grad.dims(), dense_grad_out_grad.layout(), dense_grad_out_grad.lod(), dense_grad_out_grad.offset());
  argument_outputs.push_back(grad_out_grad_dense_tensor_type);

  pir::Type grad_grad_x_grad_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_grad_grad_x_grad.dtype()), dense_grad_grad_x_grad.dims(), dense_grad_grad_x_grad.layout(), dense_grad_grad_x_grad.lod(), dense_grad_grad_x_grad.offset());
  argument_outputs.push_back(grad_grad_x_grad_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void PowTripleGradOp::VerifySig() {}

void PowTripleGradOp::InferMeta( phi::InferMetaContext *infer_meta ) {
  auto fn = PD_INFER_META(phi::GeneralTernaryGradInferMeta);
  fn(infer_meta);
}

phi::DataType PowTripleGradOp::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: PowTripleGradOp";
  


  return expected_kernel_dtype;
}

const char *PreluGradOp::attributes_name[2] = { "data_format", "mode" };

OpInfoTuple PreluGradOp::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("x", "paddle::dialect::DenseTensorType", false, false, false, false), paddle::dialect::OpInputInfo("alpha", "paddle::dialect::DenseTensorType", false, false, false, false), paddle::dialect::OpInputInfo("out_grad", "paddle::dialect::DenseTensorType", false, false, false, false) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = { paddle::dialect::OpAttributeInfo("data_format", "pir::StrAttribute", ""), paddle::dialect::OpAttributeInfo("mode", "pir::StrAttribute", "") };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("x_grad", "paddle::dialect::DenseTensorType", false, false), paddle::dialect::OpOutputInfo("alpha_grad", "paddle::dialect::DenseTensorType", false, false) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("PreluGradInferMeta", {"x", "alpha"}, "prelu_grad", {"x", "alpha", "out_grad", "data_format", "mode"}, {"x"}, {}, {}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "prelu_grad");
}

void PreluGradOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, pir::Value alpha_, pir::Value out_grad_, const std::string& data_format, const std::string& mode) {
  VLOG(4) << "Start build PreluGradOp";



  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_, alpha_, out_grad_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_data_format = pir::StrAttribute::get(pir::IrContext::Instance(), data_format);
  argument.AddAttribute("data_format", attr_data_format);
  pir::Attribute attr_mode = pir::StrAttribute::get(pir::IrContext::Instance(), mode);
  argument.AddAttribute("mode", attr_mode);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;
  paddle::dialect::DenseTensorType alpha = alpha_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)alpha;
  paddle::dialect::DenseTensorType out_grad = out_grad_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)out_grad;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);

  VLOG(4) << "Builder construction  dense_alpha";
  paddle::dialect::IrTensor ir_tensor_alpha(paddle::dialect::TransToPhiDataType(alpha.dtype()),
                                                      alpha.dims(),
                                                      alpha.data_layout(),
                                                      alpha.lod(),
                                                      alpha.offset());
  VLOG(4) << "Builder construction  meta_alpha";
  paddle::dialect::IrMetaTensor meta_alpha(&ir_tensor_alpha);
  paddle::dialect::IrTensor dense_x_grad;
  paddle::dialect::IrMetaTensor meta_x_grad(&dense_x_grad);
  paddle::dialect::IrTensor dense_alpha_grad;
  paddle::dialect::IrMetaTensor meta_alpha_grad(&dense_alpha_grad);

  phi::PreluGradInferMeta(meta_x, meta_alpha, &meta_x_grad, &meta_alpha_grad);

  std::vector<pir::Type> argument_outputs;
  pir::Type x_grad_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_x_grad.dtype()), dense_x_grad.dims(), dense_x_grad.layout(), dense_x_grad.lod(), dense_x_grad.offset());
  argument_outputs.push_back(x_grad_dense_tensor_type);

  pir::Type alpha_grad_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_alpha_grad.dtype()), dense_alpha_grad.dims(), dense_alpha_grad.layout(), dense_alpha_grad.lod(), dense_alpha_grad.offset());
  argument_outputs.push_back(alpha_grad_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void PreluGradOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, pir::Value alpha_, pir::Value out_grad_, pir::AttributeMap attributes) {
  VLOG(4) << "Start build PreluGradOp";


  IR_ENFORCE(
      attributes.find("data_format") != attributes.end(),
          "'data_format' Attribute is expected for PreluGradOp. ");
  std::string data_format = attributes.at("data_format").dyn_cast<pir::StrAttribute>().AsString();

  IR_ENFORCE(
      attributes.find("mode") != attributes.end(),
          "'mode' Attribute is expected for PreluGradOp. ");
  std::string mode = attributes.at("mode").dyn_cast<pir::StrAttribute>().AsString();


  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_, alpha_, out_grad_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_data_format = pir::StrAttribute::get(pir::IrContext::Instance(), data_format);
  argument.AddAttribute("data_format", attr_data_format);
  pir::Attribute attr_mode = pir::StrAttribute::get(pir::IrContext::Instance(), mode);
  argument.AddAttribute("mode", attr_mode);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;
  paddle::dialect::DenseTensorType alpha = alpha_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)alpha;
  paddle::dialect::DenseTensorType out_grad = out_grad_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)out_grad;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);

  VLOG(4) << "Builder construction  dense_alpha";
  paddle::dialect::IrTensor ir_tensor_alpha(paddle::dialect::TransToPhiDataType(alpha.dtype()),
                                                      alpha.dims(),
                                                      alpha.data_layout(),
                                                      alpha.lod(),
                                                      alpha.offset());
  VLOG(4) << "Builder construction  meta_alpha";
  paddle::dialect::IrMetaTensor meta_alpha(&ir_tensor_alpha);
  paddle::dialect::IrTensor dense_x_grad;
  paddle::dialect::IrMetaTensor meta_x_grad(&dense_x_grad);
  paddle::dialect::IrTensor dense_alpha_grad;
  paddle::dialect::IrMetaTensor meta_alpha_grad(&dense_alpha_grad);

  phi::PreluGradInferMeta(meta_x, meta_alpha, &meta_x_grad, &meta_alpha_grad);

  std::vector<pir::Type> argument_outputs;
  pir::Type x_grad_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_x_grad.dtype()), dense_x_grad.dims(), dense_x_grad.layout(), dense_x_grad.lod(), dense_x_grad.offset());
  argument_outputs.push_back(x_grad_dense_tensor_type);

  pir::Type alpha_grad_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_alpha_grad.dtype()), dense_alpha_grad.dims(), dense_alpha_grad.layout(), dense_alpha_grad.lod(), dense_alpha_grad.offset());
  argument_outputs.push_back(alpha_grad_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void PreluGradOp::VerifySig() {}

void PreluGradOp::InferMeta( phi::InferMetaContext *infer_meta ) {
  auto fn = PD_INFER_META(phi::PreluGradInferMeta);
  fn(infer_meta);
}

phi::DataType PreluGradOp::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: PreluGradOp";
  


  return expected_kernel_dtype;
}

const char *PsroiPoolGradOp::attributes_name[4] = { "pooled_height", "pooled_width", "output_channels", "spatial_scale" };

OpInfoTuple PsroiPoolGradOp::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("x", "paddle::dialect::DenseTensorType", false, false, false, false), paddle::dialect::OpInputInfo("boxes", "paddle::dialect::DenseTensorType", false, false, false, false), paddle::dialect::OpInputInfo("boxes_num", "paddle::dialect::DenseTensorType", true, false, false, false), paddle::dialect::OpInputInfo("out_grad", "paddle::dialect::DenseTensorType", false, false, false, false) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = { paddle::dialect::OpAttributeInfo("pooled_height", "pir::Int32Attribute", ""), paddle::dialect::OpAttributeInfo("pooled_width", "pir::Int32Attribute", ""), paddle::dialect::OpAttributeInfo("output_channels", "pir::Int32Attribute", ""), paddle::dialect::OpAttributeInfo("spatial_scale", "pir::FloatAttribute", "") };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("x_grad", "paddle::dialect::DenseTensorType", false, false) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("GeneralUnaryGradInferMeta", {"x"}, "psroi_pool_grad", {"x", "boxes", "boxes_num", "out_grad", "pooled_height", "pooled_width", "output_channels", "spatial_scale"}, {"x"}, {}, {}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "psroi_pool_grad");
}

void PsroiPoolGradOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, pir::Value boxes_, pir::Value boxes_num_, pir::Value out_grad_, int pooled_height, int pooled_width, int output_channels, float spatial_scale) {
  VLOG(4) << "Start build PsroiPoolGradOp";



  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_, boxes_, boxes_num_, out_grad_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_pooled_height = pir::Int32Attribute::get(pir::IrContext::Instance(), pooled_height);
  argument.AddAttribute("pooled_height", attr_pooled_height);
  pir::Attribute attr_pooled_width = pir::Int32Attribute::get(pir::IrContext::Instance(), pooled_width);
  argument.AddAttribute("pooled_width", attr_pooled_width);
  pir::Attribute attr_output_channels = pir::Int32Attribute::get(pir::IrContext::Instance(), output_channels);
  argument.AddAttribute("output_channels", attr_output_channels);
  pir::Attribute attr_spatial_scale = pir::FloatAttribute::get(pir::IrContext::Instance(), spatial_scale);
  argument.AddAttribute("spatial_scale", attr_spatial_scale);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;
  paddle::dialect::DenseTensorType boxes = boxes_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)boxes;
  paddle::dialect::DenseTensorType out_grad = out_grad_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)out_grad;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);
  paddle::dialect::IrTensor dense_x_grad;
  paddle::dialect::IrMetaTensor meta_x_grad(&dense_x_grad);

  phi::GeneralUnaryGradInferMeta(meta_x, &meta_x_grad);

  std::vector<pir::Type> argument_outputs;
  pir::Type x_grad_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_x_grad.dtype()), dense_x_grad.dims(), dense_x_grad.layout(), dense_x_grad.lod(), dense_x_grad.offset());
  argument_outputs.push_back(x_grad_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void PsroiPoolGradOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, pir::Value boxes_, pir::Value boxes_num_, pir::Value out_grad_, pir::AttributeMap attributes) {
  VLOG(4) << "Start build PsroiPoolGradOp";


  IR_ENFORCE(
      attributes.find("pooled_height") != attributes.end(),
          "'pooled_height' Attribute is expected for PsroiPoolGradOp. ");
  int pooled_height = attributes.at("pooled_height").dyn_cast<pir::Int32Attribute>().data();

  IR_ENFORCE(
      attributes.find("pooled_width") != attributes.end(),
          "'pooled_width' Attribute is expected for PsroiPoolGradOp. ");
  int pooled_width = attributes.at("pooled_width").dyn_cast<pir::Int32Attribute>().data();

  IR_ENFORCE(
      attributes.find("output_channels") != attributes.end(),
          "'output_channels' Attribute is expected for PsroiPoolGradOp. ");
  int output_channels = attributes.at("output_channels").dyn_cast<pir::Int32Attribute>().data();

  IR_ENFORCE(
      attributes.find("spatial_scale") != attributes.end(),
          "'spatial_scale' Attribute is expected for PsroiPoolGradOp. ");
  float spatial_scale = attributes.at("spatial_scale").dyn_cast<pir::FloatAttribute>().data();


  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_, boxes_, boxes_num_, out_grad_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_pooled_height = pir::Int32Attribute::get(pir::IrContext::Instance(), pooled_height);
  argument.AddAttribute("pooled_height", attr_pooled_height);
  pir::Attribute attr_pooled_width = pir::Int32Attribute::get(pir::IrContext::Instance(), pooled_width);
  argument.AddAttribute("pooled_width", attr_pooled_width);
  pir::Attribute attr_output_channels = pir::Int32Attribute::get(pir::IrContext::Instance(), output_channels);
  argument.AddAttribute("output_channels", attr_output_channels);
  pir::Attribute attr_spatial_scale = pir::FloatAttribute::get(pir::IrContext::Instance(), spatial_scale);
  argument.AddAttribute("spatial_scale", attr_spatial_scale);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;
  paddle::dialect::DenseTensorType boxes = boxes_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)boxes;
  paddle::dialect::DenseTensorType out_grad = out_grad_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)out_grad;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);
  paddle::dialect::IrTensor dense_x_grad;
  paddle::dialect::IrMetaTensor meta_x_grad(&dense_x_grad);

  phi::GeneralUnaryGradInferMeta(meta_x, &meta_x_grad);

  std::vector<pir::Type> argument_outputs;
  pir::Type x_grad_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_x_grad.dtype()), dense_x_grad.dims(), dense_x_grad.layout(), dense_x_grad.lod(), dense_x_grad.offset());
  argument_outputs.push_back(x_grad_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void PsroiPoolGradOp::VerifySig() {}

void PsroiPoolGradOp::InferMeta( phi::InferMetaContext *infer_meta ) {
  auto fn = PD_INFER_META(phi::GeneralUnaryGradInferMeta);
  fn(infer_meta);
}

phi::DataType PsroiPoolGradOp::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: PsroiPoolGradOp";
  


  return expected_kernel_dtype;
}

const char *PutAlongAxisGradOp::attributes_name[3] = { "axis", "reduce", "include_self" };

OpInfoTuple PutAlongAxisGradOp::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("arr", "paddle::dialect::DenseTensorType", false, false, false, false), paddle::dialect::OpInputInfo("indices", "paddle::dialect::DenseTensorType", false, false, false, false), paddle::dialect::OpInputInfo("values", "paddle::dialect::DenseTensorType", false, false, false, false), paddle::dialect::OpInputInfo("out", "paddle::dialect::DenseTensorType", false, false, false, false), paddle::dialect::OpInputInfo("out_grad", "paddle::dialect::DenseTensorType", false, false, false, false) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = { paddle::dialect::OpAttributeInfo("axis", "pir::Int32Attribute", ""), paddle::dialect::OpAttributeInfo("reduce", "pir::StrAttribute", ""), paddle::dialect::OpAttributeInfo("include_self", "pir::BoolAttribute", "") };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("arr_grad", "paddle::dialect::DenseTensorType", false, false), paddle::dialect::OpOutputInfo("values_grad", "paddle::dialect::DenseTensorType", false, false) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("GeneralBinaryGradInferMeta", {"arr", "indices"}, "put_along_axis_grad", {"arr", "indices", "values", "out", "out_grad", "axis", "reduce", "include_self"}, {}, {}, {}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "put_along_axis_grad");
}

void PutAlongAxisGradOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value arr_, pir::Value indices_, pir::Value values_, pir::Value out_, pir::Value out_grad_, int axis, const std::string& reduce, bool include_self) {
  VLOG(4) << "Start build PutAlongAxisGradOp";



  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {arr_, indices_, values_, out_, out_grad_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_axis = pir::Int32Attribute::get(pir::IrContext::Instance(), axis);
  argument.AddAttribute("axis", attr_axis);
  pir::Attribute attr_reduce = pir::StrAttribute::get(pir::IrContext::Instance(), reduce);
  argument.AddAttribute("reduce", attr_reduce);
  pir::Attribute attr_include_self = pir::BoolAttribute::get(pir::IrContext::Instance(), include_self);
  argument.AddAttribute("include_self", attr_include_self);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType arr = arr_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)arr;
  paddle::dialect::DenseTensorType indices = indices_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)indices;
  paddle::dialect::DenseTensorType values = values_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)values;
  paddle::dialect::DenseTensorType out = out_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)out;
  paddle::dialect::DenseTensorType out_grad = out_grad_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)out_grad;

  VLOG(4) << "Builder construction  dense_arr";
  paddle::dialect::IrTensor ir_tensor_arr(paddle::dialect::TransToPhiDataType(arr.dtype()),
                                                      arr.dims(),
                                                      arr.data_layout(),
                                                      arr.lod(),
                                                      arr.offset());
  VLOG(4) << "Builder construction  meta_arr";
  paddle::dialect::IrMetaTensor meta_arr(&ir_tensor_arr);

  VLOG(4) << "Builder construction  dense_indices";
  paddle::dialect::IrTensor ir_tensor_indices(paddle::dialect::TransToPhiDataType(indices.dtype()),
                                                      indices.dims(),
                                                      indices.data_layout(),
                                                      indices.lod(),
                                                      indices.offset());
  VLOG(4) << "Builder construction  meta_indices";
  paddle::dialect::IrMetaTensor meta_indices(&ir_tensor_indices);
  paddle::dialect::IrTensor dense_arr_grad;
  paddle::dialect::IrMetaTensor meta_arr_grad(&dense_arr_grad);
  paddle::dialect::IrTensor dense_values_grad;
  paddle::dialect::IrMetaTensor meta_values_grad(&dense_values_grad);

  phi::GeneralBinaryGradInferMeta(meta_arr, meta_indices, &meta_arr_grad, &meta_values_grad);

  std::vector<pir::Type> argument_outputs;
  pir::Type arr_grad_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_arr_grad.dtype()), dense_arr_grad.dims(), dense_arr_grad.layout(), dense_arr_grad.lod(), dense_arr_grad.offset());
  argument_outputs.push_back(arr_grad_dense_tensor_type);

  pir::Type values_grad_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_values_grad.dtype()), dense_values_grad.dims(), dense_values_grad.layout(), dense_values_grad.lod(), dense_values_grad.offset());
  argument_outputs.push_back(values_grad_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void PutAlongAxisGradOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value arr_, pir::Value indices_, pir::Value values_, pir::Value out_, pir::Value out_grad_, pir::AttributeMap attributes) {
  VLOG(4) << "Start build PutAlongAxisGradOp";


  IR_ENFORCE(
      attributes.find("axis") != attributes.end(),
          "'axis' Attribute is expected for PutAlongAxisGradOp. ");
  int axis = attributes.at("axis").dyn_cast<pir::Int32Attribute>().data();

  IR_ENFORCE(
      attributes.find("reduce") != attributes.end(),
          "'reduce' Attribute is expected for PutAlongAxisGradOp. ");
  std::string reduce = attributes.at("reduce").dyn_cast<pir::StrAttribute>().AsString();

  IR_ENFORCE(
      attributes.find("include_self") != attributes.end(),
          "'include_self' Attribute is expected for PutAlongAxisGradOp. ");
  bool include_self = attributes.at("include_self").dyn_cast<pir::BoolAttribute>().data();


  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {arr_, indices_, values_, out_, out_grad_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_axis = pir::Int32Attribute::get(pir::IrContext::Instance(), axis);
  argument.AddAttribute("axis", attr_axis);
  pir::Attribute attr_reduce = pir::StrAttribute::get(pir::IrContext::Instance(), reduce);
  argument.AddAttribute("reduce", attr_reduce);
  pir::Attribute attr_include_self = pir::BoolAttribute::get(pir::IrContext::Instance(), include_self);
  argument.AddAttribute("include_self", attr_include_self);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType arr = arr_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)arr;
  paddle::dialect::DenseTensorType indices = indices_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)indices;
  paddle::dialect::DenseTensorType values = values_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)values;
  paddle::dialect::DenseTensorType out = out_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)out;
  paddle::dialect::DenseTensorType out_grad = out_grad_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)out_grad;

  VLOG(4) << "Builder construction  dense_arr";
  paddle::dialect::IrTensor ir_tensor_arr(paddle::dialect::TransToPhiDataType(arr.dtype()),
                                                      arr.dims(),
                                                      arr.data_layout(),
                                                      arr.lod(),
                                                      arr.offset());
  VLOG(4) << "Builder construction  meta_arr";
  paddle::dialect::IrMetaTensor meta_arr(&ir_tensor_arr);

  VLOG(4) << "Builder construction  dense_indices";
  paddle::dialect::IrTensor ir_tensor_indices(paddle::dialect::TransToPhiDataType(indices.dtype()),
                                                      indices.dims(),
                                                      indices.data_layout(),
                                                      indices.lod(),
                                                      indices.offset());
  VLOG(4) << "Builder construction  meta_indices";
  paddle::dialect::IrMetaTensor meta_indices(&ir_tensor_indices);
  paddle::dialect::IrTensor dense_arr_grad;
  paddle::dialect::IrMetaTensor meta_arr_grad(&dense_arr_grad);
  paddle::dialect::IrTensor dense_values_grad;
  paddle::dialect::IrMetaTensor meta_values_grad(&dense_values_grad);

  phi::GeneralBinaryGradInferMeta(meta_arr, meta_indices, &meta_arr_grad, &meta_values_grad);

  std::vector<pir::Type> argument_outputs;
  pir::Type arr_grad_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_arr_grad.dtype()), dense_arr_grad.dims(), dense_arr_grad.layout(), dense_arr_grad.lod(), dense_arr_grad.offset());
  argument_outputs.push_back(arr_grad_dense_tensor_type);

  pir::Type values_grad_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_values_grad.dtype()), dense_values_grad.dims(), dense_values_grad.layout(), dense_values_grad.lod(), dense_values_grad.offset());
  argument_outputs.push_back(values_grad_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void PutAlongAxisGradOp::VerifySig() {}

void PutAlongAxisGradOp::InferMeta( phi::InferMetaContext *infer_meta ) {
  auto fn = PD_INFER_META(phi::GeneralBinaryGradInferMeta);
  fn(infer_meta);
}

phi::DataType PutAlongAxisGradOp::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: PutAlongAxisGradOp";
  


  return expected_kernel_dtype;
}

const char *QrGradOp::attributes_name[1] = { "mode" };

OpInfoTuple QrGradOp::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("x", "paddle::dialect::DenseTensorType", false, false, false, false), paddle::dialect::OpInputInfo("q", "paddle::dialect::DenseTensorType", false, false, false, false), paddle::dialect::OpInputInfo("r", "paddle::dialect::DenseTensorType", false, false, false, false), paddle::dialect::OpInputInfo("q_grad", "paddle::dialect::DenseTensorType", false, false, false, false), paddle::dialect::OpInputInfo("r_grad", "paddle::dialect::DenseTensorType", false, false, false, false) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = { paddle::dialect::OpAttributeInfo("mode", "pir::StrAttribute", "") };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("x_grad", "paddle::dialect::DenseTensorType", false, false) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("UnchangedInferMeta", {"x"}, "qr_grad", {"x", "q", "r", "q_grad", "r_grad", "mode"}, {}, {}, {}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "qr_grad");
}

void QrGradOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, pir::Value q_, pir::Value r_, pir::Value q_grad_, pir::Value r_grad_, const std::string& mode) {
  VLOG(4) << "Start build QrGradOp";



  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_, q_, r_, q_grad_, r_grad_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_mode = pir::StrAttribute::get(pir::IrContext::Instance(), mode);
  argument.AddAttribute("mode", attr_mode);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;
  paddle::dialect::DenseTensorType q = q_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)q;
  paddle::dialect::DenseTensorType r = r_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)r;
  paddle::dialect::DenseTensorType q_grad = q_grad_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)q_grad;
  paddle::dialect::DenseTensorType r_grad = r_grad_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)r_grad;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);
  paddle::dialect::IrTensor dense_x_grad;
  paddle::dialect::IrMetaTensor meta_x_grad(&dense_x_grad);

  phi::UnchangedInferMeta(meta_x, &meta_x_grad);

  std::vector<pir::Type> argument_outputs;
  pir::Type x_grad_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_x_grad.dtype()), dense_x_grad.dims(), dense_x_grad.layout(), dense_x_grad.lod(), dense_x_grad.offset());
  argument_outputs.push_back(x_grad_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void QrGradOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, pir::Value q_, pir::Value r_, pir::Value q_grad_, pir::Value r_grad_, pir::AttributeMap attributes) {
  VLOG(4) << "Start build QrGradOp";


  IR_ENFORCE(
      attributes.find("mode") != attributes.end(),
          "'mode' Attribute is expected for QrGradOp. ");
  std::string mode = attributes.at("mode").dyn_cast<pir::StrAttribute>().AsString();


  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_, q_, r_, q_grad_, r_grad_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_mode = pir::StrAttribute::get(pir::IrContext::Instance(), mode);
  argument.AddAttribute("mode", attr_mode);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;
  paddle::dialect::DenseTensorType q = q_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)q;
  paddle::dialect::DenseTensorType r = r_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)r;
  paddle::dialect::DenseTensorType q_grad = q_grad_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)q_grad;
  paddle::dialect::DenseTensorType r_grad = r_grad_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)r_grad;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);
  paddle::dialect::IrTensor dense_x_grad;
  paddle::dialect::IrMetaTensor meta_x_grad(&dense_x_grad);

  phi::UnchangedInferMeta(meta_x, &meta_x_grad);

  std::vector<pir::Type> argument_outputs;
  pir::Type x_grad_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_x_grad.dtype()), dense_x_grad.dims(), dense_x_grad.layout(), dense_x_grad.lod(), dense_x_grad.offset());
  argument_outputs.push_back(x_grad_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void QrGradOp::VerifySig() {}

void QrGradOp::InferMeta( phi::InferMetaContext *infer_meta ) {
  auto fn = PD_INFER_META(phi::UnchangedInferMeta);
  fn(infer_meta);
}

phi::DataType QrGradOp::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: QrGradOp";
  


  return expected_kernel_dtype;
}

OpInfoTuple RealGradOp::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("out_grad", "paddle::dialect::DenseTensorType", false, false, false, false) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = {  };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("x_grad", "paddle::dialect::DenseTensorType", false, false) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("RealAndImagGradInferMeta", {"out_grad"}, "real_grad", {"out_grad"}, {"complex:out_grad"}, {}, {}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "real_grad");
}

void RealGradOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value out_grad_) {
  VLOG(4) << "Start build RealGradOp";



  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {out_grad_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType out_grad = out_grad_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)out_grad;

  VLOG(4) << "Builder construction  dense_out_grad";
  paddle::dialect::IrTensor ir_tensor_out_grad(paddle::dialect::TransToPhiDataType(out_grad.dtype()),
                                                      out_grad.dims(),
                                                      out_grad.data_layout(),
                                                      out_grad.lod(),
                                                      out_grad.offset());
  VLOG(4) << "Builder construction  meta_out_grad";
  paddle::dialect::IrMetaTensor meta_out_grad(&ir_tensor_out_grad);
  paddle::dialect::IrTensor dense_x_grad;
  paddle::dialect::IrMetaTensor meta_x_grad(&dense_x_grad);

  phi::RealAndImagGradInferMeta(meta_out_grad, &meta_x_grad);

  std::vector<pir::Type> argument_outputs;
  pir::Type x_grad_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_x_grad.dtype()), dense_x_grad.dims(), dense_x_grad.layout(), dense_x_grad.lod(), dense_x_grad.offset());
  argument_outputs.push_back(x_grad_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void RealGradOp::VerifySig() {}

void RealGradOp::InferMeta( phi::InferMetaContext *infer_meta ) {
  auto fn = PD_INFER_META(phi::RealAndImagGradInferMeta);
  fn(infer_meta);
}

phi::DataType RealGradOp::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: RealGradOp";
  


  return expected_kernel_dtype;
}

OpInfoTuple ReciprocalGradOp::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("out", "paddle::dialect::DenseTensorType", false, false, false, false), paddle::dialect::OpInputInfo("out_grad", "paddle::dialect::DenseTensorType", false, false, false, false) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = {  };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("x_grad", "paddle::dialect::DenseTensorType", false, false) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("UnchangedInferMeta", {"out"}, "reciprocal_grad", {"out", "out_grad"}, {}, {}, {}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "reciprocal_grad");
}

void ReciprocalGradOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value out_, pir::Value out_grad_) {
  VLOG(4) << "Start build ReciprocalGradOp";



  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {out_, out_grad_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType out = out_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)out;
  paddle::dialect::DenseTensorType out_grad = out_grad_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)out_grad;

  VLOG(4) << "Builder construction  dense_out";
  paddle::dialect::IrTensor ir_tensor_out(paddle::dialect::TransToPhiDataType(out.dtype()),
                                                      out.dims(),
                                                      out.data_layout(),
                                                      out.lod(),
                                                      out.offset());
  VLOG(4) << "Builder construction  meta_out";
  paddle::dialect::IrMetaTensor meta_out(&ir_tensor_out);
  paddle::dialect::IrTensor dense_x_grad;
  paddle::dialect::IrMetaTensor meta_x_grad(&dense_x_grad);

  phi::UnchangedInferMeta(meta_out, &meta_x_grad);

  std::vector<pir::Type> argument_outputs;
  pir::Type x_grad_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_x_grad.dtype()), dense_x_grad.dims(), dense_x_grad.layout(), dense_x_grad.lod(), dense_x_grad.offset());
  argument_outputs.push_back(x_grad_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void ReciprocalGradOp::VerifySig() {}

void ReciprocalGradOp::InferMeta( phi::InferMetaContext *infer_meta ) {
  auto fn = PD_INFER_META(phi::UnchangedInferMeta);
  fn(infer_meta);
}

phi::DataType ReciprocalGradOp::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: ReciprocalGradOp";
  


  return expected_kernel_dtype;
}

OpInfoTuple ReciprocalGrad_Op::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("out", "paddle::dialect::DenseTensorType", false, false, false, false), paddle::dialect::OpInputInfo("out_grad", "paddle::dialect::DenseTensorType", false, false, false, false) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = {  };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("x_grad", "paddle::dialect::DenseTensorType", false, false) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("UnchangedInferMeta", {"out"}, "reciprocal_grad", {"out", "out_grad"}, {}, {}, {{"x_grad", "out_grad"}}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "reciprocal_grad");
}

void ReciprocalGrad_Op::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value out_, pir::Value out_grad_) {
  VLOG(4) << "Start build ReciprocalGrad_Op";



  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {out_, out_grad_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType out = out_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)out;
  paddle::dialect::DenseTensorType out_grad = out_grad_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)out_grad;

  VLOG(4) << "Builder construction  dense_out";
  paddle::dialect::IrTensor ir_tensor_out(paddle::dialect::TransToPhiDataType(out.dtype()),
                                                      out.dims(),
                                                      out.data_layout(),
                                                      out.lod(),
                                                      out.offset());
  VLOG(4) << "Builder construction  meta_out";
  paddle::dialect::IrMetaTensor meta_out(&ir_tensor_out);
  paddle::dialect::IrTensor dense_x_grad;
  paddle::dialect::IrMetaTensor meta_x_grad(&dense_x_grad);

  phi::UnchangedInferMeta(meta_out, &meta_x_grad);

  std::vector<pir::Type> argument_outputs;
  pir::Type x_grad_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_x_grad.dtype()), dense_x_grad.dims(), dense_x_grad.layout(), dense_x_grad.lod(), dense_x_grad.offset());
  argument_outputs.push_back(x_grad_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void ReciprocalGrad_Op::VerifySig() {}

void ReciprocalGrad_Op::InferMeta( phi::InferMetaContext *infer_meta ) {
  auto fn = PD_INFER_META(phi::UnchangedInferMeta);
  fn(infer_meta);
}

phi::DataType ReciprocalGrad_Op::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: ReciprocalGrad_Op";
  


  return expected_kernel_dtype;
}

OpInfoTuple Relu6GradOp::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("out", "paddle::dialect::DenseTensorType", false, false, false, false), paddle::dialect::OpInputInfo("out_grad", "paddle::dialect::DenseTensorType", false, false, false, false) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = {  };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("x_grad", "paddle::dialect::DenseTensorType", false, false) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("UnchangedInferMeta", {"out"}, "relu6_grad", {"out", "out_grad"}, {}, {}, {}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "relu6_grad");
}

void Relu6GradOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value out_, pir::Value out_grad_) {
  VLOG(4) << "Start build Relu6GradOp";



  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {out_, out_grad_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType out = out_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)out;
  paddle::dialect::DenseTensorType out_grad = out_grad_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)out_grad;

  VLOG(4) << "Builder construction  dense_out";
  paddle::dialect::IrTensor ir_tensor_out(paddle::dialect::TransToPhiDataType(out.dtype()),
                                                      out.dims(),
                                                      out.data_layout(),
                                                      out.lod(),
                                                      out.offset());
  VLOG(4) << "Builder construction  meta_out";
  paddle::dialect::IrMetaTensor meta_out(&ir_tensor_out);
  paddle::dialect::IrTensor dense_x_grad;
  paddle::dialect::IrMetaTensor meta_x_grad(&dense_x_grad);

  phi::UnchangedInferMeta(meta_out, &meta_x_grad);

  std::vector<pir::Type> argument_outputs;
  pir::Type x_grad_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_x_grad.dtype()), dense_x_grad.dims(), dense_x_grad.layout(), dense_x_grad.lod(), dense_x_grad.offset());
  argument_outputs.push_back(x_grad_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void Relu6GradOp::VerifySig() {}

void Relu6GradOp::InferMeta( phi::InferMetaContext *infer_meta ) {
  auto fn = PD_INFER_META(phi::UnchangedInferMeta);
  fn(infer_meta);
}

phi::DataType Relu6GradOp::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: Relu6GradOp";
  


  return expected_kernel_dtype;
}

OpInfoTuple Relu6Grad_Op::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("out", "paddle::dialect::DenseTensorType", false, false, false, false), paddle::dialect::OpInputInfo("out_grad", "paddle::dialect::DenseTensorType", false, false, false, false) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = {  };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("x_grad", "paddle::dialect::DenseTensorType", false, false) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("UnchangedInferMeta", {"out"}, "relu6_grad", {"out", "out_grad"}, {}, {}, {{"x_grad", "out_grad"}}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "relu6_grad");
}

void Relu6Grad_Op::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value out_, pir::Value out_grad_) {
  VLOG(4) << "Start build Relu6Grad_Op";



  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {out_, out_grad_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType out = out_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)out;
  paddle::dialect::DenseTensorType out_grad = out_grad_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)out_grad;

  VLOG(4) << "Builder construction  dense_out";
  paddle::dialect::IrTensor ir_tensor_out(paddle::dialect::TransToPhiDataType(out.dtype()),
                                                      out.dims(),
                                                      out.data_layout(),
                                                      out.lod(),
                                                      out.offset());
  VLOG(4) << "Builder construction  meta_out";
  paddle::dialect::IrMetaTensor meta_out(&ir_tensor_out);
  paddle::dialect::IrTensor dense_x_grad;
  paddle::dialect::IrMetaTensor meta_x_grad(&dense_x_grad);

  phi::UnchangedInferMeta(meta_out, &meta_x_grad);

  std::vector<pir::Type> argument_outputs;
  pir::Type x_grad_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_x_grad.dtype()), dense_x_grad.dims(), dense_x_grad.layout(), dense_x_grad.lod(), dense_x_grad.offset());
  argument_outputs.push_back(x_grad_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void Relu6Grad_Op::VerifySig() {}

void Relu6Grad_Op::InferMeta( phi::InferMetaContext *infer_meta ) {
  auto fn = PD_INFER_META(phi::UnchangedInferMeta);
  fn(infer_meta);
}

phi::DataType Relu6Grad_Op::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: Relu6Grad_Op";
  


  return expected_kernel_dtype;
}

OpInfoTuple ReluDoubleGradOp::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("out", "paddle::dialect::DenseTensorType", false, false, false, false), paddle::dialect::OpInputInfo("grad_x_grad", "paddle::dialect::DenseTensorType", false, false, false, false) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = {  };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("grad_out_grad", "paddle::dialect::DenseTensorType", false, false) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("UnchangedInferMeta", {"out"}, "relu_double_grad", {"out", "grad_x_grad"}, {}, {}, {}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "relu_double_grad");
}

void ReluDoubleGradOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value out_, pir::Value grad_x_grad_) {
  VLOG(4) << "Start build ReluDoubleGradOp";



  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {out_, grad_x_grad_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType out = out_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)out;
  paddle::dialect::DenseTensorType grad_x_grad = grad_x_grad_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)grad_x_grad;

  VLOG(4) << "Builder construction  dense_out";
  paddle::dialect::IrTensor ir_tensor_out(paddle::dialect::TransToPhiDataType(out.dtype()),
                                                      out.dims(),
                                                      out.data_layout(),
                                                      out.lod(),
                                                      out.offset());
  VLOG(4) << "Builder construction  meta_out";
  paddle::dialect::IrMetaTensor meta_out(&ir_tensor_out);
  paddle::dialect::IrTensor dense_grad_out_grad;
  paddle::dialect::IrMetaTensor meta_grad_out_grad(&dense_grad_out_grad);

  phi::UnchangedInferMeta(meta_out, &meta_grad_out_grad);

  std::vector<pir::Type> argument_outputs;
  pir::Type grad_out_grad_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_grad_out_grad.dtype()), dense_grad_out_grad.dims(), dense_grad_out_grad.layout(), dense_grad_out_grad.lod(), dense_grad_out_grad.offset());
  argument_outputs.push_back(grad_out_grad_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void ReluDoubleGradOp::VerifySig() {}

void ReluDoubleGradOp::InferMeta( phi::InferMetaContext *infer_meta ) {
  auto fn = PD_INFER_META(phi::UnchangedInferMeta);
  fn(infer_meta);
}

phi::DataType ReluDoubleGradOp::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: ReluDoubleGradOp";
  


  return expected_kernel_dtype;
}

OpInfoTuple ReluDoubleGrad_Op::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("out", "paddle::dialect::DenseTensorType", false, false, false, false), paddle::dialect::OpInputInfo("grad_x_grad", "paddle::dialect::DenseTensorType", false, false, false, false) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = {  };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("grad_out_grad", "paddle::dialect::DenseTensorType", false, false) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("UnchangedInferMeta", {"out"}, "relu_double_grad", {"out", "grad_x_grad"}, {}, {}, {{"grad_out_grad", "grad_x_grad"}}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "relu_double_grad");
}

void ReluDoubleGrad_Op::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value out_, pir::Value grad_x_grad_) {
  VLOG(4) << "Start build ReluDoubleGrad_Op";



  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {out_, grad_x_grad_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType out = out_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)out;
  paddle::dialect::DenseTensorType grad_x_grad = grad_x_grad_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)grad_x_grad;

  VLOG(4) << "Builder construction  dense_out";
  paddle::dialect::IrTensor ir_tensor_out(paddle::dialect::TransToPhiDataType(out.dtype()),
                                                      out.dims(),
                                                      out.data_layout(),
                                                      out.lod(),
                                                      out.offset());
  VLOG(4) << "Builder construction  meta_out";
  paddle::dialect::IrMetaTensor meta_out(&ir_tensor_out);
  paddle::dialect::IrTensor dense_grad_out_grad;
  paddle::dialect::IrMetaTensor meta_grad_out_grad(&dense_grad_out_grad);

  phi::UnchangedInferMeta(meta_out, &meta_grad_out_grad);

  std::vector<pir::Type> argument_outputs;
  pir::Type grad_out_grad_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_grad_out_grad.dtype()), dense_grad_out_grad.dims(), dense_grad_out_grad.layout(), dense_grad_out_grad.lod(), dense_grad_out_grad.offset());
  argument_outputs.push_back(grad_out_grad_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void ReluDoubleGrad_Op::VerifySig() {}

void ReluDoubleGrad_Op::InferMeta( phi::InferMetaContext *infer_meta ) {
  auto fn = PD_INFER_META(phi::UnchangedInferMeta);
  fn(infer_meta);
}

phi::DataType ReluDoubleGrad_Op::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: ReluDoubleGrad_Op";
  


  return expected_kernel_dtype;
}

OpInfoTuple ReluGradOp::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("out", "paddle::dialect::DenseTensorType", false, false, false, false), paddle::dialect::OpInputInfo("out_grad", "paddle::dialect::DenseTensorType", false, false, false, true) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = {  };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("x_grad", "paddle::dialect::DenseTensorType", false, false) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("UnchangedInferMeta", {"out"}, "relu_grad", {"out", "out_grad"}, {}, {}, {}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "relu_grad");
}

void ReluGradOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value out_, pir::Value out_grad_) {
  VLOG(4) << "Start build ReluGradOp";



  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {out_, out_grad_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType out = out_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)out;
  paddle::dialect::DenseTensorType out_grad = out_grad_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)out_grad;

  VLOG(4) << "Builder construction  dense_out";
  paddle::dialect::IrTensor ir_tensor_out(paddle::dialect::TransToPhiDataType(out.dtype()),
                                                      out.dims(),
                                                      out.data_layout(),
                                                      out.lod(),
                                                      out.offset());
  VLOG(4) << "Builder construction  meta_out";
  paddle::dialect::IrMetaTensor meta_out(&ir_tensor_out);
  paddle::dialect::IrTensor dense_x_grad;
  paddle::dialect::IrMetaTensor meta_x_grad(&dense_x_grad);

  phi::UnchangedInferMeta(meta_out, &meta_x_grad);

  std::vector<pir::Type> argument_outputs;
  pir::Type x_grad_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_x_grad.dtype()), dense_x_grad.dims(), dense_x_grad.layout(), dense_x_grad.lod(), dense_x_grad.offset());
  argument_outputs.push_back(x_grad_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void ReluGradOp::VerifySig() {}

void ReluGradOp::InferMeta( phi::InferMetaContext *infer_meta ) {
  auto fn = PD_INFER_META(phi::UnchangedInferMeta);
  fn(infer_meta);
}

phi::DataType ReluGradOp::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: ReluGradOp";
  


  return expected_kernel_dtype;
}

OpInfoTuple ReluGrad_Op::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("out", "paddle::dialect::DenseTensorType", false, false, false, false), paddle::dialect::OpInputInfo("out_grad", "paddle::dialect::DenseTensorType", false, false, false, true) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = {  };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("x_grad", "paddle::dialect::DenseTensorType", false, false) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("UnchangedInferMeta", {"out"}, "relu_grad", {"out", "out_grad"}, {}, {}, {{"x_grad", "out_grad"}}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "relu_grad");
}

void ReluGrad_Op::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value out_, pir::Value out_grad_) {
  VLOG(4) << "Start build ReluGrad_Op";



  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {out_, out_grad_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType out = out_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)out;
  paddle::dialect::DenseTensorType out_grad = out_grad_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)out_grad;

  VLOG(4) << "Builder construction  dense_out";
  paddle::dialect::IrTensor ir_tensor_out(paddle::dialect::TransToPhiDataType(out.dtype()),
                                                      out.dims(),
                                                      out.data_layout(),
                                                      out.lod(),
                                                      out.offset());
  VLOG(4) << "Builder construction  meta_out";
  paddle::dialect::IrMetaTensor meta_out(&ir_tensor_out);
  paddle::dialect::IrTensor dense_x_grad;
  paddle::dialect::IrMetaTensor meta_x_grad(&dense_x_grad);

  phi::UnchangedInferMeta(meta_out, &meta_x_grad);

  std::vector<pir::Type> argument_outputs;
  pir::Type x_grad_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_x_grad.dtype()), dense_x_grad.dims(), dense_x_grad.layout(), dense_x_grad.lod(), dense_x_grad.offset());
  argument_outputs.push_back(x_grad_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void ReluGrad_Op::VerifySig() {}

void ReluGrad_Op::InferMeta( phi::InferMetaContext *infer_meta ) {
  auto fn = PD_INFER_META(phi::UnchangedInferMeta);
  fn(infer_meta);
}

phi::DataType ReluGrad_Op::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: ReluGrad_Op";
  


  return expected_kernel_dtype;
}

const char *RenormGradOp::attributes_name[3] = { "p", "axis", "max_norm" };

OpInfoTuple RenormGradOp::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("x", "paddle::dialect::DenseTensorType", false, false, false, false), paddle::dialect::OpInputInfo("out_grad", "paddle::dialect::DenseTensorType", false, false, false, false) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = { paddle::dialect::OpAttributeInfo("p", "pir::FloatAttribute", ""), paddle::dialect::OpAttributeInfo("axis", "pir::Int32Attribute", ""), paddle::dialect::OpAttributeInfo("max_norm", "pir::FloatAttribute", "") };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("x_grad", "paddle::dialect::DenseTensorType", false, false) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("UnchangedInferMeta", {"out_grad"}, "renorm_grad", {"x", "out_grad", "p", "axis", "max_norm"}, {}, {}, {}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "renorm_grad");
}

void RenormGradOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, pir::Value out_grad_, float p, int axis, float max_norm) {
  VLOG(4) << "Start build RenormGradOp";



  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_, out_grad_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_p = pir::FloatAttribute::get(pir::IrContext::Instance(), p);
  argument.AddAttribute("p", attr_p);
  pir::Attribute attr_axis = pir::Int32Attribute::get(pir::IrContext::Instance(), axis);
  argument.AddAttribute("axis", attr_axis);
  pir::Attribute attr_max_norm = pir::FloatAttribute::get(pir::IrContext::Instance(), max_norm);
  argument.AddAttribute("max_norm", attr_max_norm);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;
  paddle::dialect::DenseTensorType out_grad = out_grad_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)out_grad;

  VLOG(4) << "Builder construction  dense_out_grad";
  paddle::dialect::IrTensor ir_tensor_out_grad(paddle::dialect::TransToPhiDataType(out_grad.dtype()),
                                                      out_grad.dims(),
                                                      out_grad.data_layout(),
                                                      out_grad.lod(),
                                                      out_grad.offset());
  VLOG(4) << "Builder construction  meta_out_grad";
  paddle::dialect::IrMetaTensor meta_out_grad(&ir_tensor_out_grad);
  paddle::dialect::IrTensor dense_x_grad;
  paddle::dialect::IrMetaTensor meta_x_grad(&dense_x_grad);

  phi::UnchangedInferMeta(meta_out_grad, &meta_x_grad);

  std::vector<pir::Type> argument_outputs;
  pir::Type x_grad_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_x_grad.dtype()), dense_x_grad.dims(), dense_x_grad.layout(), dense_x_grad.lod(), dense_x_grad.offset());
  argument_outputs.push_back(x_grad_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void RenormGradOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, pir::Value out_grad_, pir::AttributeMap attributes) {
  VLOG(4) << "Start build RenormGradOp";


  IR_ENFORCE(
      attributes.find("p") != attributes.end(),
          "'p' Attribute is expected for RenormGradOp. ");
  float p = attributes.at("p").dyn_cast<pir::FloatAttribute>().data();

  IR_ENFORCE(
      attributes.find("axis") != attributes.end(),
          "'axis' Attribute is expected for RenormGradOp. ");
  int axis = attributes.at("axis").dyn_cast<pir::Int32Attribute>().data();

  IR_ENFORCE(
      attributes.find("max_norm") != attributes.end(),
          "'max_norm' Attribute is expected for RenormGradOp. ");
  float max_norm = attributes.at("max_norm").dyn_cast<pir::FloatAttribute>().data();


  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_, out_grad_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_p = pir::FloatAttribute::get(pir::IrContext::Instance(), p);
  argument.AddAttribute("p", attr_p);
  pir::Attribute attr_axis = pir::Int32Attribute::get(pir::IrContext::Instance(), axis);
  argument.AddAttribute("axis", attr_axis);
  pir::Attribute attr_max_norm = pir::FloatAttribute::get(pir::IrContext::Instance(), max_norm);
  argument.AddAttribute("max_norm", attr_max_norm);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;
  paddle::dialect::DenseTensorType out_grad = out_grad_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)out_grad;

  VLOG(4) << "Builder construction  dense_out_grad";
  paddle::dialect::IrTensor ir_tensor_out_grad(paddle::dialect::TransToPhiDataType(out_grad.dtype()),
                                                      out_grad.dims(),
                                                      out_grad.data_layout(),
                                                      out_grad.lod(),
                                                      out_grad.offset());
  VLOG(4) << "Builder construction  meta_out_grad";
  paddle::dialect::IrMetaTensor meta_out_grad(&ir_tensor_out_grad);
  paddle::dialect::IrTensor dense_x_grad;
  paddle::dialect::IrMetaTensor meta_x_grad(&dense_x_grad);

  phi::UnchangedInferMeta(meta_out_grad, &meta_x_grad);

  std::vector<pir::Type> argument_outputs;
  pir::Type x_grad_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_x_grad.dtype()), dense_x_grad.dims(), dense_x_grad.layout(), dense_x_grad.lod(), dense_x_grad.offset());
  argument_outputs.push_back(x_grad_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void RenormGradOp::VerifySig() {}

void RenormGradOp::InferMeta( phi::InferMetaContext *infer_meta ) {
  auto fn = PD_INFER_META(phi::UnchangedInferMeta);
  fn(infer_meta);
}

phi::DataType RenormGradOp::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: RenormGradOp";
  


  return expected_kernel_dtype;
}

OpInfoTuple ReverseGradOp::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("out_grad", "paddle::dialect::DenseTensorType", false, false, false, false), paddle::dialect::OpInputInfo("axis", "paddle::dialect::IntArrayAttribute", false, false, true, false) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = {  };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("x_grad", "paddle::dialect::DenseTensorType", false, false) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("", {""}, "", {""}, {}, {}, {}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "reverse_grad");
}

void ReverseGradOp::VerifySig() {}

phi::DataType ReverseGradOp::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: ReverseGradOp";
  


  return expected_kernel_dtype;
}

const char *RoiAlignGradOp::attributes_name[5] = { "pooled_height", "pooled_width", "spatial_scale", "sampling_ratio", "aligned" };

OpInfoTuple RoiAlignGradOp::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("x", "paddle::dialect::DenseTensorType", false, true, false, false), paddle::dialect::OpInputInfo("boxes", "paddle::dialect::DenseTensorType", false, false, false, false), paddle::dialect::OpInputInfo("boxes_num", "paddle::dialect::DenseTensorType", true, false, false, false), paddle::dialect::OpInputInfo("out_grad", "paddle::dialect::DenseTensorType", false, false, false, false) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = { paddle::dialect::OpAttributeInfo("pooled_height", "pir::Int32Attribute", ""), paddle::dialect::OpAttributeInfo("pooled_width", "pir::Int32Attribute", ""), paddle::dialect::OpAttributeInfo("spatial_scale", "pir::FloatAttribute", ""), paddle::dialect::OpAttributeInfo("sampling_ratio", "pir::Int32Attribute", ""), paddle::dialect::OpAttributeInfo("aligned", "pir::BoolAttribute", "") };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("x_grad", "paddle::dialect::DenseTensorType", false, false) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("UnchangedInferMeta", {"x"}, "roi_align_grad", {"x", "boxes", "boxes_num", "out_grad", "pooled_height", "pooled_width", "spatial_scale", "sampling_ratio", "aligned"}, {"boxes"}, {}, {}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "roi_align_grad");
}

void RoiAlignGradOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, pir::Value boxes_, pir::Value boxes_num_, pir::Value out_grad_, int pooled_height, int pooled_width, float spatial_scale, int sampling_ratio, bool aligned) {
  VLOG(4) << "Start build RoiAlignGradOp";



  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_, boxes_, boxes_num_, out_grad_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_pooled_height = pir::Int32Attribute::get(pir::IrContext::Instance(), pooled_height);
  argument.AddAttribute("pooled_height", attr_pooled_height);
  pir::Attribute attr_pooled_width = pir::Int32Attribute::get(pir::IrContext::Instance(), pooled_width);
  argument.AddAttribute("pooled_width", attr_pooled_width);
  pir::Attribute attr_spatial_scale = pir::FloatAttribute::get(pir::IrContext::Instance(), spatial_scale);
  argument.AddAttribute("spatial_scale", attr_spatial_scale);
  pir::Attribute attr_sampling_ratio = pir::Int32Attribute::get(pir::IrContext::Instance(), sampling_ratio);
  argument.AddAttribute("sampling_ratio", attr_sampling_ratio);
  pir::Attribute attr_aligned = pir::BoolAttribute::get(pir::IrContext::Instance(), aligned);
  argument.AddAttribute("aligned", attr_aligned);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;
  paddle::dialect::DenseTensorType boxes = boxes_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)boxes;
  paddle::dialect::DenseTensorType out_grad = out_grad_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)out_grad;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);
  paddle::dialect::IrTensor dense_x_grad;
  paddle::dialect::IrMetaTensor meta_x_grad(&dense_x_grad);

  phi::UnchangedInferMeta(meta_x, &meta_x_grad);

  std::vector<pir::Type> argument_outputs;
  pir::Type x_grad_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_x_grad.dtype()), dense_x_grad.dims(), dense_x_grad.layout(), dense_x_grad.lod(), dense_x_grad.offset());
  argument_outputs.push_back(x_grad_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void RoiAlignGradOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, pir::Value boxes_, pir::Value boxes_num_, pir::Value out_grad_, pir::AttributeMap attributes) {
  VLOG(4) << "Start build RoiAlignGradOp";


  IR_ENFORCE(
      attributes.find("pooled_height") != attributes.end(),
          "'pooled_height' Attribute is expected for RoiAlignGradOp. ");
  int pooled_height = attributes.at("pooled_height").dyn_cast<pir::Int32Attribute>().data();

  IR_ENFORCE(
      attributes.find("pooled_width") != attributes.end(),
          "'pooled_width' Attribute is expected for RoiAlignGradOp. ");
  int pooled_width = attributes.at("pooled_width").dyn_cast<pir::Int32Attribute>().data();

  IR_ENFORCE(
      attributes.find("spatial_scale") != attributes.end(),
          "'spatial_scale' Attribute is expected for RoiAlignGradOp. ");
  float spatial_scale = attributes.at("spatial_scale").dyn_cast<pir::FloatAttribute>().data();

  IR_ENFORCE(
      attributes.find("sampling_ratio") != attributes.end(),
          "'sampling_ratio' Attribute is expected for RoiAlignGradOp. ");
  int sampling_ratio = attributes.at("sampling_ratio").dyn_cast<pir::Int32Attribute>().data();

  IR_ENFORCE(
      attributes.find("aligned") != attributes.end(),
          "'aligned' Attribute is expected for RoiAlignGradOp. ");
  bool aligned = attributes.at("aligned").dyn_cast<pir::BoolAttribute>().data();


  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_, boxes_, boxes_num_, out_grad_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_pooled_height = pir::Int32Attribute::get(pir::IrContext::Instance(), pooled_height);
  argument.AddAttribute("pooled_height", attr_pooled_height);
  pir::Attribute attr_pooled_width = pir::Int32Attribute::get(pir::IrContext::Instance(), pooled_width);
  argument.AddAttribute("pooled_width", attr_pooled_width);
  pir::Attribute attr_spatial_scale = pir::FloatAttribute::get(pir::IrContext::Instance(), spatial_scale);
  argument.AddAttribute("spatial_scale", attr_spatial_scale);
  pir::Attribute attr_sampling_ratio = pir::Int32Attribute::get(pir::IrContext::Instance(), sampling_ratio);
  argument.AddAttribute("sampling_ratio", attr_sampling_ratio);
  pir::Attribute attr_aligned = pir::BoolAttribute::get(pir::IrContext::Instance(), aligned);
  argument.AddAttribute("aligned", attr_aligned);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;
  paddle::dialect::DenseTensorType boxes = boxes_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)boxes;
  paddle::dialect::DenseTensorType out_grad = out_grad_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)out_grad;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);
  paddle::dialect::IrTensor dense_x_grad;
  paddle::dialect::IrMetaTensor meta_x_grad(&dense_x_grad);

  phi::UnchangedInferMeta(meta_x, &meta_x_grad);

  std::vector<pir::Type> argument_outputs;
  pir::Type x_grad_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_x_grad.dtype()), dense_x_grad.dims(), dense_x_grad.layout(), dense_x_grad.lod(), dense_x_grad.offset());
  argument_outputs.push_back(x_grad_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void RoiAlignGradOp::VerifySig() {}

void RoiAlignGradOp::InferMeta( phi::InferMetaContext *infer_meta ) {
  auto fn = PD_INFER_META(phi::UnchangedInferMeta);
  fn(infer_meta);
}

phi::DataType RoiAlignGradOp::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: RoiAlignGradOp";
  


  return expected_kernel_dtype;
}

const char *RoiPoolGradOp::attributes_name[3] = { "pooled_height", "pooled_width", "spatial_scale" };

OpInfoTuple RoiPoolGradOp::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("x", "paddle::dialect::DenseTensorType", false, false, false, false), paddle::dialect::OpInputInfo("boxes", "paddle::dialect::DenseTensorType", false, false, false, false), paddle::dialect::OpInputInfo("boxes_num", "paddle::dialect::DenseTensorType", true, false, false, false), paddle::dialect::OpInputInfo("arg_max", "paddle::dialect::DenseTensorType", false, false, false, false), paddle::dialect::OpInputInfo("out_grad", "paddle::dialect::DenseTensorType", false, false, false, false) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = { paddle::dialect::OpAttributeInfo("pooled_height", "pir::Int32Attribute", ""), paddle::dialect::OpAttributeInfo("pooled_width", "pir::Int32Attribute", ""), paddle::dialect::OpAttributeInfo("spatial_scale", "pir::FloatAttribute", "") };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("x_grad", "paddle::dialect::DenseTensorType", false, false) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("UnchangedInferMeta", {"x"}, "roi_pool_grad", {"x", "boxes", "boxes_num", "arg_max", "out_grad", "pooled_height", "pooled_width", "spatial_scale"}, {"x"}, {}, {}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "roi_pool_grad");
}

void RoiPoolGradOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, pir::Value boxes_, pir::Value boxes_num_, pir::Value arg_max_, pir::Value out_grad_, int pooled_height, int pooled_width, float spatial_scale) {
  VLOG(4) << "Start build RoiPoolGradOp";



  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_, boxes_, boxes_num_, arg_max_, out_grad_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_pooled_height = pir::Int32Attribute::get(pir::IrContext::Instance(), pooled_height);
  argument.AddAttribute("pooled_height", attr_pooled_height);
  pir::Attribute attr_pooled_width = pir::Int32Attribute::get(pir::IrContext::Instance(), pooled_width);
  argument.AddAttribute("pooled_width", attr_pooled_width);
  pir::Attribute attr_spatial_scale = pir::FloatAttribute::get(pir::IrContext::Instance(), spatial_scale);
  argument.AddAttribute("spatial_scale", attr_spatial_scale);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;
  paddle::dialect::DenseTensorType boxes = boxes_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)boxes;
  paddle::dialect::DenseTensorType arg_max = arg_max_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)arg_max;
  paddle::dialect::DenseTensorType out_grad = out_grad_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)out_grad;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);
  paddle::dialect::IrTensor dense_x_grad;
  paddle::dialect::IrMetaTensor meta_x_grad(&dense_x_grad);

  phi::UnchangedInferMeta(meta_x, &meta_x_grad);

  std::vector<pir::Type> argument_outputs;
  pir::Type x_grad_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_x_grad.dtype()), dense_x_grad.dims(), dense_x_grad.layout(), dense_x_grad.lod(), dense_x_grad.offset());
  argument_outputs.push_back(x_grad_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void RoiPoolGradOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, pir::Value boxes_, pir::Value boxes_num_, pir::Value arg_max_, pir::Value out_grad_, pir::AttributeMap attributes) {
  VLOG(4) << "Start build RoiPoolGradOp";


  IR_ENFORCE(
      attributes.find("pooled_height") != attributes.end(),
          "'pooled_height' Attribute is expected for RoiPoolGradOp. ");
  int pooled_height = attributes.at("pooled_height").dyn_cast<pir::Int32Attribute>().data();

  IR_ENFORCE(
      attributes.find("pooled_width") != attributes.end(),
          "'pooled_width' Attribute is expected for RoiPoolGradOp. ");
  int pooled_width = attributes.at("pooled_width").dyn_cast<pir::Int32Attribute>().data();

  IR_ENFORCE(
      attributes.find("spatial_scale") != attributes.end(),
          "'spatial_scale' Attribute is expected for RoiPoolGradOp. ");
  float spatial_scale = attributes.at("spatial_scale").dyn_cast<pir::FloatAttribute>().data();


  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_, boxes_, boxes_num_, arg_max_, out_grad_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_pooled_height = pir::Int32Attribute::get(pir::IrContext::Instance(), pooled_height);
  argument.AddAttribute("pooled_height", attr_pooled_height);
  pir::Attribute attr_pooled_width = pir::Int32Attribute::get(pir::IrContext::Instance(), pooled_width);
  argument.AddAttribute("pooled_width", attr_pooled_width);
  pir::Attribute attr_spatial_scale = pir::FloatAttribute::get(pir::IrContext::Instance(), spatial_scale);
  argument.AddAttribute("spatial_scale", attr_spatial_scale);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;
  paddle::dialect::DenseTensorType boxes = boxes_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)boxes;
  paddle::dialect::DenseTensorType arg_max = arg_max_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)arg_max;
  paddle::dialect::DenseTensorType out_grad = out_grad_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)out_grad;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);
  paddle::dialect::IrTensor dense_x_grad;
  paddle::dialect::IrMetaTensor meta_x_grad(&dense_x_grad);

  phi::UnchangedInferMeta(meta_x, &meta_x_grad);

  std::vector<pir::Type> argument_outputs;
  pir::Type x_grad_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_x_grad.dtype()), dense_x_grad.dims(), dense_x_grad.layout(), dense_x_grad.lod(), dense_x_grad.offset());
  argument_outputs.push_back(x_grad_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void RoiPoolGradOp::VerifySig() {}

void RoiPoolGradOp::InferMeta( phi::InferMetaContext *infer_meta ) {
  auto fn = PD_INFER_META(phi::UnchangedInferMeta);
  fn(infer_meta);
}

phi::DataType RoiPoolGradOp::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: RoiPoolGradOp";
  


  return expected_kernel_dtype;
}

const char *RollGradOp::attributes_name[1] = { "axis" };

OpInfoTuple RollGradOp::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("x", "paddle::dialect::DenseTensorType", false, true, false, false), paddle::dialect::OpInputInfo("out_grad", "paddle::dialect::DenseTensorType", false, false, false, false), paddle::dialect::OpInputInfo("shifts", "paddle::dialect::IntArrayAttribute", false, false, true, false) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = { paddle::dialect::OpAttributeInfo("axis", "pir::ArrayAttribute<pir::Int64Attribute>", "") };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("x_grad", "paddle::dialect::DenseTensorType", false, false) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("UnchangedInferMeta", {"x"}, "roll_grad", {"x", "out_grad", "shifts", "axis"}, {"x"}, {}, {}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "roll_grad");
}

void RollGradOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, pir::Value out_grad_, const std::vector<int64_t>& shifts, const std::vector<int64_t>& axis) {
  VLOG(4) << "Start build RollGradOp";


  // Generate int_array mutable attribute: shifts
  paddle::dialect::FullIntArrayOp full_shifts_op = builder.Build<paddle::dialect::FullIntArrayOp>(shifts, phi::DataType::INT64, phi::CPUPlace());
  pir::OpResult shifts_ = full_shifts_op->result(0);
    
  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_, out_grad_, shifts_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  std::vector<pir::Attribute> vec_axis;
  for (size_t i = 0; i < static_cast<size_t>(axis.size()); i++) {
      pir::Attribute attr_axis = pir::Int64Attribute::get(pir::IrContext::Instance(), axis[i]);

    vec_axis.push_back(attr_axis);
  }
  pir::Attribute attr_axis = pir::ArrayAttribute::get(pir::IrContext::Instance(), vec_axis);
  argument.AddAttribute("axis", attr_axis);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;
  paddle::dialect::DenseTensorType out_grad = out_grad_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)out_grad;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);
  paddle::dialect::IrTensor dense_x_grad;
  paddle::dialect::IrMetaTensor meta_x_grad(&dense_x_grad);

  phi::UnchangedInferMeta(meta_x, &meta_x_grad);

  std::vector<pir::Type> argument_outputs;
  pir::Type x_grad_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_x_grad.dtype()), dense_x_grad.dims(), dense_x_grad.layout(), dense_x_grad.lod(), dense_x_grad.offset());
  argument_outputs.push_back(x_grad_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void RollGradOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, pir::Value out_grad_, pir::AttributeMap attributes) {
  VLOG(4) << "Start build RollGradOp";


  IR_ENFORCE(
      attributes.find("shifts") != attributes.end(),
          "'shifts' Attribute is expected for RollGradOp. ");
  std::vector<int64_t> shifts = attributes.at("shifts").dyn_cast<paddle::dialect::IntArrayAttribute>().data().GetData();

  IR_ENFORCE(
      attributes.find("axis") != attributes.end(),
          "'axis' Attribute is expected for RollGradOp. ");
  std::vector<int64_t> axis;
  for (size_t i = 0; i < attributes.at("axis").dyn_cast<pir::ArrayAttribute>().size(); i++) {
    axis.push_back(attributes.at("axis").dyn_cast<pir::ArrayAttribute>().at(i).dyn_cast<pir::Int64Attribute>().data());
  }

  // Generate int_array mutable attribute: shifts
  paddle::dialect::FullIntArrayOp full_shifts_op = builder.Build<paddle::dialect::FullIntArrayOp>(shifts, phi::DataType::INT64, phi::CPUPlace());
  pir::OpResult shifts_ = full_shifts_op->result(0);
    
  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_, out_grad_, shifts_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  std::vector<pir::Attribute> vec_axis;
  for (size_t i = 0; i < static_cast<size_t>(axis.size()); i++) {
      pir::Attribute attr_axis = pir::Int64Attribute::get(pir::IrContext::Instance(), axis[i]);

    vec_axis.push_back(attr_axis);
  }
  pir::Attribute attr_axis = pir::ArrayAttribute::get(pir::IrContext::Instance(), vec_axis);
  argument.AddAttribute("axis", attr_axis);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;
  paddle::dialect::DenseTensorType out_grad = out_grad_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)out_grad;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);
  paddle::dialect::IrTensor dense_x_grad;
  paddle::dialect::IrMetaTensor meta_x_grad(&dense_x_grad);

  phi::UnchangedInferMeta(meta_x, &meta_x_grad);

  std::vector<pir::Type> argument_outputs;
  pir::Type x_grad_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_x_grad.dtype()), dense_x_grad.dims(), dense_x_grad.layout(), dense_x_grad.lod(), dense_x_grad.offset());
  argument_outputs.push_back(x_grad_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void RollGradOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, pir::Value out_grad_, pir::Value shifts_, const std::vector<int64_t>& axis) {
  VLOG(4) << "Start build RollGradOp";



  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_, out_grad_, shifts_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  std::vector<pir::Attribute> vec_axis;
  for (size_t i = 0; i < static_cast<size_t>(axis.size()); i++) {
      pir::Attribute attr_axis = pir::Int64Attribute::get(pir::IrContext::Instance(), axis[i]);

    vec_axis.push_back(attr_axis);
  }
  pir::Attribute attr_axis = pir::ArrayAttribute::get(pir::IrContext::Instance(), vec_axis);
  argument.AddAttribute("axis", attr_axis);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;
  paddle::dialect::DenseTensorType out_grad = out_grad_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)out_grad;
  phi::IntArray shifts;
  if (shifts_.dyn_cast<pir::OpResult>() && shifts_.dyn_cast<pir::OpResult>().owner()->isa<paddle::dialect::FullIntArrayOp>()) {
    shifts = std::move(phi::IntArray(paddle::dialect::GetInt64Vector(
                          shifts_.dyn_cast<pir::OpResult>().owner()
                          ->dyn_cast<paddle::dialect::FullIntArrayOp>()
                          .attribute("value"))));
  } else if (shifts_.type().isa<pir::VectorType>()) {
    size_t shifts_size = shifts_.type().dyn_cast<pir::VectorType>().size();
    shifts = std::move(phi::IntArray(std::vector<int64_t>(shifts_size, -1)));
    shifts.SetFromTensor(true);
  } else if (shifts_.type().isa<paddle::dialect::DenseTensorType>()) {
    common::DDim shifts_dim = shifts_.type().dyn_cast<paddle::dialect::DenseTensorType>().dims();
    size_t shifts_size = common::product(shifts_dim);
    if (common::contain_unknown_dim(shifts_dim)) {
      shifts_size = 1;
    }
    shifts = std::move(phi::IntArray(std::vector<int64_t>(shifts_size, -1)));
    shifts.SetFromTensor(true);
  } else {
    PADDLE_THROW(phi::errors::Unimplemented("Only support VectorType or DenseTensorType"));
  }


  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);
  paddle::dialect::IrTensor dense_x_grad;
  paddle::dialect::IrMetaTensor meta_x_grad(&dense_x_grad);

  phi::UnchangedInferMeta(meta_x, &meta_x_grad);

  std::vector<pir::Type> argument_outputs;
  pir::Type x_grad_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_x_grad.dtype()), dense_x_grad.dims(), dense_x_grad.layout(), dense_x_grad.lod(), dense_x_grad.offset());
  argument_outputs.push_back(x_grad_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void RollGradOp::VerifySig() {}

void RollGradOp::InferMeta( phi::InferMetaContext *infer_meta ) {
  auto fn = PD_INFER_META(phi::UnchangedInferMeta);
  fn(infer_meta);
}

phi::DataType RollGradOp::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: RollGradOp";
  


  return expected_kernel_dtype;
}

OpInfoTuple RoundGradOp::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("out_grad", "paddle::dialect::DenseTensorType", false, false, false, false) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = {  };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("x_grad", "paddle::dialect::DenseTensorType", false, false) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("UnchangedInferMeta", {"out_grad"}, "round_grad", {"out_grad"}, {}, {}, {}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "round_grad");
}

void RoundGradOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value out_grad_) {
  VLOG(4) << "Start build RoundGradOp";



  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {out_grad_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType out_grad = out_grad_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)out_grad;

  VLOG(4) << "Builder construction  dense_out_grad";
  paddle::dialect::IrTensor ir_tensor_out_grad(paddle::dialect::TransToPhiDataType(out_grad.dtype()),
                                                      out_grad.dims(),
                                                      out_grad.data_layout(),
                                                      out_grad.lod(),
                                                      out_grad.offset());
  VLOG(4) << "Builder construction  meta_out_grad";
  paddle::dialect::IrMetaTensor meta_out_grad(&ir_tensor_out_grad);
  paddle::dialect::IrTensor dense_x_grad;
  paddle::dialect::IrMetaTensor meta_x_grad(&dense_x_grad);

  phi::UnchangedInferMeta(meta_out_grad, &meta_x_grad);

  std::vector<pir::Type> argument_outputs;
  pir::Type x_grad_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_x_grad.dtype()), dense_x_grad.dims(), dense_x_grad.layout(), dense_x_grad.lod(), dense_x_grad.offset());
  argument_outputs.push_back(x_grad_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void RoundGradOp::VerifySig() {}

void RoundGradOp::InferMeta( phi::InferMetaContext *infer_meta ) {
  auto fn = PD_INFER_META(phi::UnchangedInferMeta);
  fn(infer_meta);
}

phi::DataType RoundGradOp::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: RoundGradOp";
  


  return expected_kernel_dtype;
}

OpInfoTuple RoundGrad_Op::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("out_grad", "paddle::dialect::DenseTensorType", false, false, false, false) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = {  };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("x_grad", "paddle::dialect::DenseTensorType", false, false) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("UnchangedInferMeta", {"out_grad"}, "round_grad", {"out_grad"}, {}, {}, {{"x_grad", "out_grad"}}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "round_grad");
}

void RoundGrad_Op::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value out_grad_) {
  VLOG(4) << "Start build RoundGrad_Op";



  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {out_grad_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType out_grad = out_grad_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)out_grad;

  VLOG(4) << "Builder construction  dense_out_grad";
  paddle::dialect::IrTensor ir_tensor_out_grad(paddle::dialect::TransToPhiDataType(out_grad.dtype()),
                                                      out_grad.dims(),
                                                      out_grad.data_layout(),
                                                      out_grad.lod(),
                                                      out_grad.offset());
  VLOG(4) << "Builder construction  meta_out_grad";
  paddle::dialect::IrMetaTensor meta_out_grad(&ir_tensor_out_grad);
  paddle::dialect::IrTensor dense_x_grad;
  paddle::dialect::IrMetaTensor meta_x_grad(&dense_x_grad);

  phi::UnchangedInferMeta(meta_out_grad, &meta_x_grad);

  std::vector<pir::Type> argument_outputs;
  pir::Type x_grad_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_x_grad.dtype()), dense_x_grad.dims(), dense_x_grad.layout(), dense_x_grad.lod(), dense_x_grad.offset());
  argument_outputs.push_back(x_grad_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void RoundGrad_Op::VerifySig() {}

void RoundGrad_Op::InferMeta( phi::InferMetaContext *infer_meta ) {
  auto fn = PD_INFER_META(phi::UnchangedInferMeta);
  fn(infer_meta);
}

phi::DataType RoundGrad_Op::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: RoundGrad_Op";
  


  return expected_kernel_dtype;
}

OpInfoTuple RsqrtDoubleGradOp::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("out", "paddle::dialect::DenseTensorType", false, false, false, false), paddle::dialect::OpInputInfo("grad_x", "paddle::dialect::DenseTensorType", false, false, false, false), paddle::dialect::OpInputInfo("grad_x_grad", "paddle::dialect::DenseTensorType", false, false, false, false) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = {  };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("out_grad", "paddle::dialect::DenseTensorType", false, false), paddle::dialect::OpOutputInfo("grad_out_grad", "paddle::dialect::DenseTensorType", false, false) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("GeneralBinaryGradInferMeta", {"out", "out"}, "rsqrt_double_grad", {"out", "grad_x", "grad_x_grad"}, {}, {}, {}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "rsqrt_double_grad");
}

void RsqrtDoubleGradOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value out_, pir::Value grad_x_, pir::Value grad_x_grad_) {
  VLOG(4) << "Start build RsqrtDoubleGradOp";



  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {out_, grad_x_, grad_x_grad_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType out = out_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)out;
  paddle::dialect::DenseTensorType grad_x = grad_x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)grad_x;
  paddle::dialect::DenseTensorType grad_x_grad = grad_x_grad_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)grad_x_grad;

  VLOG(4) << "Builder construction  dense_out";
  paddle::dialect::IrTensor ir_tensor_out(paddle::dialect::TransToPhiDataType(out.dtype()),
                                                      out.dims(),
                                                      out.data_layout(),
                                                      out.lod(),
                                                      out.offset());
  VLOG(4) << "Builder construction  meta_out";
  paddle::dialect::IrMetaTensor meta_out(&ir_tensor_out);
  paddle::dialect::IrTensor dense_out_grad;
  paddle::dialect::IrMetaTensor meta_out_grad(&dense_out_grad);
  paddle::dialect::IrTensor dense_grad_out_grad;
  paddle::dialect::IrMetaTensor meta_grad_out_grad(&dense_grad_out_grad);

  phi::GeneralBinaryGradInferMeta(meta_out, meta_out, &meta_out_grad, &meta_grad_out_grad);

  std::vector<pir::Type> argument_outputs;
  pir::Type out_grad_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_out_grad.dtype()), dense_out_grad.dims(), dense_out_grad.layout(), dense_out_grad.lod(), dense_out_grad.offset());
  argument_outputs.push_back(out_grad_dense_tensor_type);

  pir::Type grad_out_grad_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_grad_out_grad.dtype()), dense_grad_out_grad.dims(), dense_grad_out_grad.layout(), dense_grad_out_grad.lod(), dense_grad_out_grad.offset());
  argument_outputs.push_back(grad_out_grad_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void RsqrtDoubleGradOp::VerifySig() {}

void RsqrtDoubleGradOp::InferMeta( phi::InferMetaContext *infer_meta ) {
  auto fn = PD_INFER_META(phi::GeneralBinaryGradInferMeta);
  fn(infer_meta);
}

phi::DataType RsqrtDoubleGradOp::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: RsqrtDoubleGradOp";
  


  return expected_kernel_dtype;
}

OpInfoTuple RsqrtDoubleGrad_Op::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("out", "paddle::dialect::DenseTensorType", false, false, false, false), paddle::dialect::OpInputInfo("grad_x", "paddle::dialect::DenseTensorType", false, false, false, false), paddle::dialect::OpInputInfo("grad_x_grad", "paddle::dialect::DenseTensorType", false, false, false, false) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = {  };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("out_grad", "paddle::dialect::DenseTensorType", false, false), paddle::dialect::OpOutputInfo("grad_out_grad", "paddle::dialect::DenseTensorType", false, false) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("GeneralBinaryGradInferMeta", {"out", "out"}, "rsqrt_double_grad", {"out", "grad_x", "grad_x_grad"}, {}, {}, {{"grad_out_grad", "grad_x_grad"}}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "rsqrt_double_grad");
}

void RsqrtDoubleGrad_Op::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value out_, pir::Value grad_x_, pir::Value grad_x_grad_) {
  VLOG(4) << "Start build RsqrtDoubleGrad_Op";



  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {out_, grad_x_, grad_x_grad_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType out = out_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)out;
  paddle::dialect::DenseTensorType grad_x = grad_x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)grad_x;
  paddle::dialect::DenseTensorType grad_x_grad = grad_x_grad_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)grad_x_grad;

  VLOG(4) << "Builder construction  dense_out";
  paddle::dialect::IrTensor ir_tensor_out(paddle::dialect::TransToPhiDataType(out.dtype()),
                                                      out.dims(),
                                                      out.data_layout(),
                                                      out.lod(),
                                                      out.offset());
  VLOG(4) << "Builder construction  meta_out";
  paddle::dialect::IrMetaTensor meta_out(&ir_tensor_out);
  paddle::dialect::IrTensor dense_out_grad;
  paddle::dialect::IrMetaTensor meta_out_grad(&dense_out_grad);
  paddle::dialect::IrTensor dense_grad_out_grad;
  paddle::dialect::IrMetaTensor meta_grad_out_grad(&dense_grad_out_grad);

  phi::GeneralBinaryGradInferMeta(meta_out, meta_out, &meta_out_grad, &meta_grad_out_grad);

  std::vector<pir::Type> argument_outputs;
  pir::Type out_grad_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_out_grad.dtype()), dense_out_grad.dims(), dense_out_grad.layout(), dense_out_grad.lod(), dense_out_grad.offset());
  argument_outputs.push_back(out_grad_dense_tensor_type);

  pir::Type grad_out_grad_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_grad_out_grad.dtype()), dense_grad_out_grad.dims(), dense_grad_out_grad.layout(), dense_grad_out_grad.lod(), dense_grad_out_grad.offset());
  argument_outputs.push_back(grad_out_grad_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void RsqrtDoubleGrad_Op::VerifySig() {}

void RsqrtDoubleGrad_Op::InferMeta( phi::InferMetaContext *infer_meta ) {
  auto fn = PD_INFER_META(phi::GeneralBinaryGradInferMeta);
  fn(infer_meta);
}

phi::DataType RsqrtDoubleGrad_Op::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: RsqrtDoubleGrad_Op";
  


  return expected_kernel_dtype;
}

OpInfoTuple RsqrtGradOp::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("out", "paddle::dialect::DenseTensorType", false, false, false, true), paddle::dialect::OpInputInfo("out_grad", "paddle::dialect::DenseTensorType", false, false, false, true) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = {  };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("x_grad", "paddle::dialect::DenseTensorType", false, false) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("UnchangedInferMeta", {"out"}, "rsqrt_grad", {"out", "out_grad"}, {}, {}, {}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "rsqrt_grad");
}

void RsqrtGradOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value out_, pir::Value out_grad_) {
  VLOG(4) << "Start build RsqrtGradOp";



  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {out_, out_grad_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType out = out_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)out;
  paddle::dialect::DenseTensorType out_grad = out_grad_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)out_grad;

  VLOG(4) << "Builder construction  dense_out";
  paddle::dialect::IrTensor ir_tensor_out(paddle::dialect::TransToPhiDataType(out.dtype()),
                                                      out.dims(),
                                                      out.data_layout(),
                                                      out.lod(),
                                                      out.offset());
  VLOG(4) << "Builder construction  meta_out";
  paddle::dialect::IrMetaTensor meta_out(&ir_tensor_out);
  paddle::dialect::IrTensor dense_x_grad;
  paddle::dialect::IrMetaTensor meta_x_grad(&dense_x_grad);

  phi::UnchangedInferMeta(meta_out, &meta_x_grad);

  std::vector<pir::Type> argument_outputs;
  pir::Type x_grad_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_x_grad.dtype()), dense_x_grad.dims(), dense_x_grad.layout(), dense_x_grad.lod(), dense_x_grad.offset());
  argument_outputs.push_back(x_grad_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void RsqrtGradOp::VerifySig() {}

void RsqrtGradOp::InferMeta( phi::InferMetaContext *infer_meta ) {
  auto fn = PD_INFER_META(phi::UnchangedInferMeta);
  fn(infer_meta);
}

phi::DataType RsqrtGradOp::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: RsqrtGradOp";
  


  return expected_kernel_dtype;
}

OpInfoTuple RsqrtGrad_Op::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("out", "paddle::dialect::DenseTensorType", false, false, false, true), paddle::dialect::OpInputInfo("out_grad", "paddle::dialect::DenseTensorType", false, false, false, true) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = {  };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("x_grad", "paddle::dialect::DenseTensorType", false, false) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("UnchangedInferMeta", {"out"}, "rsqrt_grad", {"out", "out_grad"}, {}, {}, {{"x_grad", "out_grad"}}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "rsqrt_grad");
}

void RsqrtGrad_Op::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value out_, pir::Value out_grad_) {
  VLOG(4) << "Start build RsqrtGrad_Op";



  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {out_, out_grad_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType out = out_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)out;
  paddle::dialect::DenseTensorType out_grad = out_grad_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)out_grad;

  VLOG(4) << "Builder construction  dense_out";
  paddle::dialect::IrTensor ir_tensor_out(paddle::dialect::TransToPhiDataType(out.dtype()),
                                                      out.dims(),
                                                      out.data_layout(),
                                                      out.lod(),
                                                      out.offset());
  VLOG(4) << "Builder construction  meta_out";
  paddle::dialect::IrMetaTensor meta_out(&ir_tensor_out);
  paddle::dialect::IrTensor dense_x_grad;
  paddle::dialect::IrMetaTensor meta_x_grad(&dense_x_grad);

  phi::UnchangedInferMeta(meta_out, &meta_x_grad);

  std::vector<pir::Type> argument_outputs;
  pir::Type x_grad_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_x_grad.dtype()), dense_x_grad.dims(), dense_x_grad.layout(), dense_x_grad.lod(), dense_x_grad.offset());
  argument_outputs.push_back(x_grad_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void RsqrtGrad_Op::VerifySig() {}

void RsqrtGrad_Op::InferMeta( phi::InferMetaContext *infer_meta ) {
  auto fn = PD_INFER_META(phi::UnchangedInferMeta);
  fn(infer_meta);
}

phi::DataType RsqrtGrad_Op::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: RsqrtGrad_Op";
  


  return expected_kernel_dtype;
}

OpInfoTuple ScaleGradOp::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("out_grad", "paddle::dialect::DenseTensorType", false, false, false, false), paddle::dialect::OpInputInfo("scale", "paddle::dialect::ScalarAttribute", false, false, true, false) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = {  };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("x_grad", "paddle::dialect::DenseTensorType", false, false) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("", {""}, "", {""}, {}, {}, {}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "scale_grad");
}

void ScaleGradOp::VerifySig() {}

phi::DataType ScaleGradOp::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: ScaleGradOp";
  


  return expected_kernel_dtype;
}

const char *ScatterGradOp::attributes_name[1] = { "overwrite" };

OpInfoTuple ScatterGradOp::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("index", "paddle::dialect::DenseTensorType", false, false, false, false), paddle::dialect::OpInputInfo("updates", "paddle::dialect::DenseTensorType", false, true, false, false), paddle::dialect::OpInputInfo("out_grad", "paddle::dialect::DenseTensorType", false, false, false, false) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = { paddle::dialect::OpAttributeInfo("overwrite", "pir::BoolAttribute", "") };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("x_grad", "paddle::dialect::DenseTensorType", false, false), paddle::dialect::OpOutputInfo("updates_grad", "paddle::dialect::DenseTensorType", false, false) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("ScatterGradInferMeta", {"index", "updates", "out_grad", "overwrite"}, "scatter_grad", {"index", "updates", "out_grad", "overwrite"}, {}, {}, {}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "scatter_grad");
}

void ScatterGradOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value index_, pir::Value updates_, pir::Value out_grad_, bool overwrite) {
  VLOG(4) << "Start build ScatterGradOp";



  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {index_, updates_, out_grad_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_overwrite = pir::BoolAttribute::get(pir::IrContext::Instance(), overwrite);
  argument.AddAttribute("overwrite", attr_overwrite);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType index = index_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)index;
  paddle::dialect::DenseTensorType updates = updates_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)updates;
  paddle::dialect::DenseTensorType out_grad = out_grad_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)out_grad;

  VLOG(4) << "Builder construction  dense_index";
  paddle::dialect::IrTensor ir_tensor_index(paddle::dialect::TransToPhiDataType(index.dtype()),
                                                      index.dims(),
                                                      index.data_layout(),
                                                      index.lod(),
                                                      index.offset());
  VLOG(4) << "Builder construction  meta_index";
  paddle::dialect::IrMetaTensor meta_index(&ir_tensor_index);

  VLOG(4) << "Builder construction  dense_updates";
  paddle::dialect::IrTensor ir_tensor_updates(paddle::dialect::TransToPhiDataType(updates.dtype()),
                                                      updates.dims(),
                                                      updates.data_layout(),
                                                      updates.lod(),
                                                      updates.offset());
  VLOG(4) << "Builder construction  meta_updates";
  paddle::dialect::IrMetaTensor meta_updates(&ir_tensor_updates);

  VLOG(4) << "Builder construction  dense_out_grad";
  paddle::dialect::IrTensor ir_tensor_out_grad(paddle::dialect::TransToPhiDataType(out_grad.dtype()),
                                                      out_grad.dims(),
                                                      out_grad.data_layout(),
                                                      out_grad.lod(),
                                                      out_grad.offset());
  VLOG(4) << "Builder construction  meta_out_grad";
  paddle::dialect::IrMetaTensor meta_out_grad(&ir_tensor_out_grad);
  paddle::dialect::IrTensor dense_x_grad;
  paddle::dialect::IrMetaTensor meta_x_grad(&dense_x_grad);
  paddle::dialect::IrTensor dense_updates_grad;
  paddle::dialect::IrMetaTensor meta_updates_grad(&dense_updates_grad);

  phi::ScatterGradInferMeta(meta_index, meta_updates, meta_out_grad, overwrite, &meta_x_grad, &meta_updates_grad);

  std::vector<pir::Type> argument_outputs;
  pir::Type x_grad_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_x_grad.dtype()), dense_x_grad.dims(), dense_x_grad.layout(), dense_x_grad.lod(), dense_x_grad.offset());
  argument_outputs.push_back(x_grad_dense_tensor_type);

  pir::Type updates_grad_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_updates_grad.dtype()), dense_updates_grad.dims(), dense_updates_grad.layout(), dense_updates_grad.lod(), dense_updates_grad.offset());
  argument_outputs.push_back(updates_grad_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void ScatterGradOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value index_, pir::Value updates_, pir::Value out_grad_, pir::AttributeMap attributes) {
  VLOG(4) << "Start build ScatterGradOp";


  IR_ENFORCE(
      attributes.find("overwrite") != attributes.end(),
          "'overwrite' Attribute is expected for ScatterGradOp. ");
  bool overwrite = attributes.at("overwrite").dyn_cast<pir::BoolAttribute>().data();


  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {index_, updates_, out_grad_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_overwrite = pir::BoolAttribute::get(pir::IrContext::Instance(), overwrite);
  argument.AddAttribute("overwrite", attr_overwrite);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType index = index_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)index;
  paddle::dialect::DenseTensorType updates = updates_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)updates;
  paddle::dialect::DenseTensorType out_grad = out_grad_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)out_grad;

  VLOG(4) << "Builder construction  dense_index";
  paddle::dialect::IrTensor ir_tensor_index(paddle::dialect::TransToPhiDataType(index.dtype()),
                                                      index.dims(),
                                                      index.data_layout(),
                                                      index.lod(),
                                                      index.offset());
  VLOG(4) << "Builder construction  meta_index";
  paddle::dialect::IrMetaTensor meta_index(&ir_tensor_index);

  VLOG(4) << "Builder construction  dense_updates";
  paddle::dialect::IrTensor ir_tensor_updates(paddle::dialect::TransToPhiDataType(updates.dtype()),
                                                      updates.dims(),
                                                      updates.data_layout(),
                                                      updates.lod(),
                                                      updates.offset());
  VLOG(4) << "Builder construction  meta_updates";
  paddle::dialect::IrMetaTensor meta_updates(&ir_tensor_updates);

  VLOG(4) << "Builder construction  dense_out_grad";
  paddle::dialect::IrTensor ir_tensor_out_grad(paddle::dialect::TransToPhiDataType(out_grad.dtype()),
                                                      out_grad.dims(),
                                                      out_grad.data_layout(),
                                                      out_grad.lod(),
                                                      out_grad.offset());
  VLOG(4) << "Builder construction  meta_out_grad";
  paddle::dialect::IrMetaTensor meta_out_grad(&ir_tensor_out_grad);
  paddle::dialect::IrTensor dense_x_grad;
  paddle::dialect::IrMetaTensor meta_x_grad(&dense_x_grad);
  paddle::dialect::IrTensor dense_updates_grad;
  paddle::dialect::IrMetaTensor meta_updates_grad(&dense_updates_grad);

  phi::ScatterGradInferMeta(meta_index, meta_updates, meta_out_grad, overwrite, &meta_x_grad, &meta_updates_grad);

  std::vector<pir::Type> argument_outputs;
  pir::Type x_grad_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_x_grad.dtype()), dense_x_grad.dims(), dense_x_grad.layout(), dense_x_grad.lod(), dense_x_grad.offset());
  argument_outputs.push_back(x_grad_dense_tensor_type);

  pir::Type updates_grad_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_updates_grad.dtype()), dense_updates_grad.dims(), dense_updates_grad.layout(), dense_updates_grad.lod(), dense_updates_grad.offset());
  argument_outputs.push_back(updates_grad_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void ScatterGradOp::VerifySig() {}

void ScatterGradOp::InferMeta( phi::InferMetaContext *infer_meta ) {
  auto fn = PD_INFER_META(phi::ScatterGradInferMeta);
  fn(infer_meta);
}

phi::DataType ScatterGradOp::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: ScatterGradOp";
  


  return expected_kernel_dtype;
}

OpInfoTuple ScatterNdAddGradOp::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("index", "paddle::dialect::DenseTensorType", false, false, false, false), paddle::dialect::OpInputInfo("updates", "paddle::dialect::DenseTensorType", false, true, false, false), paddle::dialect::OpInputInfo("out_grad", "paddle::dialect::DenseTensorType", false, false, false, false) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = {  };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("x_grad", "paddle::dialect::DenseTensorType", false, false), paddle::dialect::OpOutputInfo("updates_grad", "paddle::dialect::DenseTensorType", false, false) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("ScatterNdAddGradInferMeta", {"index", "updates", "out_grad"}, "scatter_nd_add_grad", {"index", "updates", "out_grad"}, {}, {}, {}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "scatter_nd_add_grad");
}

void ScatterNdAddGradOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value index_, pir::Value updates_, pir::Value out_grad_) {
  VLOG(4) << "Start build ScatterNdAddGradOp";



  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {index_, updates_, out_grad_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType index = index_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)index;
  paddle::dialect::DenseTensorType updates = updates_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)updates;
  paddle::dialect::DenseTensorType out_grad = out_grad_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)out_grad;

  VLOG(4) << "Builder construction  dense_index";
  paddle::dialect::IrTensor ir_tensor_index(paddle::dialect::TransToPhiDataType(index.dtype()),
                                                      index.dims(),
                                                      index.data_layout(),
                                                      index.lod(),
                                                      index.offset());
  VLOG(4) << "Builder construction  meta_index";
  paddle::dialect::IrMetaTensor meta_index(&ir_tensor_index);

  VLOG(4) << "Builder construction  dense_updates";
  paddle::dialect::IrTensor ir_tensor_updates(paddle::dialect::TransToPhiDataType(updates.dtype()),
                                                      updates.dims(),
                                                      updates.data_layout(),
                                                      updates.lod(),
                                                      updates.offset());
  VLOG(4) << "Builder construction  meta_updates";
  paddle::dialect::IrMetaTensor meta_updates(&ir_tensor_updates);

  VLOG(4) << "Builder construction  dense_out_grad";
  paddle::dialect::IrTensor ir_tensor_out_grad(paddle::dialect::TransToPhiDataType(out_grad.dtype()),
                                                      out_grad.dims(),
                                                      out_grad.data_layout(),
                                                      out_grad.lod(),
                                                      out_grad.offset());
  VLOG(4) << "Builder construction  meta_out_grad";
  paddle::dialect::IrMetaTensor meta_out_grad(&ir_tensor_out_grad);
  paddle::dialect::IrTensor dense_x_grad;
  paddle::dialect::IrMetaTensor meta_x_grad(&dense_x_grad);
  paddle::dialect::IrTensor dense_updates_grad;
  paddle::dialect::IrMetaTensor meta_updates_grad(&dense_updates_grad);

  phi::ScatterNdAddGradInferMeta(meta_index, meta_updates, meta_out_grad, &meta_x_grad, &meta_updates_grad);

  std::vector<pir::Type> argument_outputs;
  pir::Type x_grad_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_x_grad.dtype()), dense_x_grad.dims(), dense_x_grad.layout(), dense_x_grad.lod(), dense_x_grad.offset());
  argument_outputs.push_back(x_grad_dense_tensor_type);

  pir::Type updates_grad_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_updates_grad.dtype()), dense_updates_grad.dims(), dense_updates_grad.layout(), dense_updates_grad.lod(), dense_updates_grad.offset());
  argument_outputs.push_back(updates_grad_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void ScatterNdAddGradOp::VerifySig() {}

void ScatterNdAddGradOp::InferMeta( phi::InferMetaContext *infer_meta ) {
  auto fn = PD_INFER_META(phi::ScatterNdAddGradInferMeta);
  fn(infer_meta);
}

phi::DataType ScatterNdAddGradOp::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: ScatterNdAddGradOp";
  


  return expected_kernel_dtype;
}

const char *SegmentPoolGradOp::attributes_name[1] = { "pooltype" };

OpInfoTuple SegmentPoolGradOp::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("x", "paddle::dialect::DenseTensorType", false, false, false, false), paddle::dialect::OpInputInfo("segment_ids", "paddle::dialect::DenseTensorType", false, false, false, false), paddle::dialect::OpInputInfo("out", "paddle::dialect::DenseTensorType", false, false, false, false), paddle::dialect::OpInputInfo("summed_ids", "paddle::dialect::DenseTensorType", true, false, false, false), paddle::dialect::OpInputInfo("out_grad", "paddle::dialect::DenseTensorType", false, false, false, false) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = { paddle::dialect::OpAttributeInfo("pooltype", "pir::StrAttribute", "") };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("x_grad", "paddle::dialect::DenseTensorType", false, false) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("UnchangedInferMeta", {"x"}, "segment_pool_grad", {"x", "segment_ids", "out", "summed_ids", "out_grad", "pooltype"}, {"out_grad"}, {}, {}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "segment_pool_grad");
}

void SegmentPoolGradOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, pir::Value segment_ids_, pir::Value out_, pir::Value summed_ids_, pir::Value out_grad_, const std::string& pooltype) {
  VLOG(4) << "Start build SegmentPoolGradOp";



  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_, segment_ids_, out_, summed_ids_, out_grad_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_pooltype = pir::StrAttribute::get(pir::IrContext::Instance(), pooltype);
  argument.AddAttribute("pooltype", attr_pooltype);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;
  paddle::dialect::DenseTensorType segment_ids = segment_ids_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)segment_ids;
  paddle::dialect::DenseTensorType out = out_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)out;
  paddle::dialect::DenseTensorType out_grad = out_grad_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)out_grad;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);
  paddle::dialect::IrTensor dense_x_grad;
  paddle::dialect::IrMetaTensor meta_x_grad(&dense_x_grad);

  phi::UnchangedInferMeta(meta_x, &meta_x_grad);

  std::vector<pir::Type> argument_outputs;
  pir::Type x_grad_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_x_grad.dtype()), dense_x_grad.dims(), dense_x_grad.layout(), dense_x_grad.lod(), dense_x_grad.offset());
  argument_outputs.push_back(x_grad_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void SegmentPoolGradOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, pir::Value segment_ids_, pir::Value out_, pir::Value summed_ids_, pir::Value out_grad_, pir::AttributeMap attributes) {
  VLOG(4) << "Start build SegmentPoolGradOp";


  IR_ENFORCE(
      attributes.find("pooltype") != attributes.end(),
          "'pooltype' Attribute is expected for SegmentPoolGradOp. ");
  std::string pooltype = attributes.at("pooltype").dyn_cast<pir::StrAttribute>().AsString();


  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_, segment_ids_, out_, summed_ids_, out_grad_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_pooltype = pir::StrAttribute::get(pir::IrContext::Instance(), pooltype);
  argument.AddAttribute("pooltype", attr_pooltype);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;
  paddle::dialect::DenseTensorType segment_ids = segment_ids_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)segment_ids;
  paddle::dialect::DenseTensorType out = out_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)out;
  paddle::dialect::DenseTensorType out_grad = out_grad_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)out_grad;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);
  paddle::dialect::IrTensor dense_x_grad;
  paddle::dialect::IrMetaTensor meta_x_grad(&dense_x_grad);

  phi::UnchangedInferMeta(meta_x, &meta_x_grad);

  std::vector<pir::Type> argument_outputs;
  pir::Type x_grad_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_x_grad.dtype()), dense_x_grad.dims(), dense_x_grad.layout(), dense_x_grad.lod(), dense_x_grad.offset());
  argument_outputs.push_back(x_grad_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void SegmentPoolGradOp::VerifySig() {}

void SegmentPoolGradOp::InferMeta( phi::InferMetaContext *infer_meta ) {
  auto fn = PD_INFER_META(phi::UnchangedInferMeta);
  fn(infer_meta);
}

phi::DataType SegmentPoolGradOp::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: SegmentPoolGradOp";
  


  return expected_kernel_dtype;
}

const char *SeluGradOp::attributes_name[2] = { "scale", "alpha" };

OpInfoTuple SeluGradOp::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("out", "paddle::dialect::DenseTensorType", false, false, false, false), paddle::dialect::OpInputInfo("out_grad", "paddle::dialect::DenseTensorType", false, false, false, false) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = { paddle::dialect::OpAttributeInfo("scale", "pir::FloatAttribute", ""), paddle::dialect::OpAttributeInfo("alpha", "pir::FloatAttribute", "") };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("x_grad", "paddle::dialect::DenseTensorType", false, false) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("UnchangedInferMeta", {"out"}, "selu_grad", {"out", "out_grad", "scale", "alpha"}, {"out"}, {}, {}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "selu_grad");
}

void SeluGradOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value out_, pir::Value out_grad_, float scale, float alpha) {
  VLOG(4) << "Start build SeluGradOp";



  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {out_, out_grad_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_scale = pir::FloatAttribute::get(pir::IrContext::Instance(), scale);
  argument.AddAttribute("scale", attr_scale);
  pir::Attribute attr_alpha = pir::FloatAttribute::get(pir::IrContext::Instance(), alpha);
  argument.AddAttribute("alpha", attr_alpha);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType out = out_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)out;
  paddle::dialect::DenseTensorType out_grad = out_grad_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)out_grad;

  VLOG(4) << "Builder construction  dense_out";
  paddle::dialect::IrTensor ir_tensor_out(paddle::dialect::TransToPhiDataType(out.dtype()),
                                                      out.dims(),
                                                      out.data_layout(),
                                                      out.lod(),
                                                      out.offset());
  VLOG(4) << "Builder construction  meta_out";
  paddle::dialect::IrMetaTensor meta_out(&ir_tensor_out);
  paddle::dialect::IrTensor dense_x_grad;
  paddle::dialect::IrMetaTensor meta_x_grad(&dense_x_grad);

  phi::UnchangedInferMeta(meta_out, &meta_x_grad);

  std::vector<pir::Type> argument_outputs;
  pir::Type x_grad_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_x_grad.dtype()), dense_x_grad.dims(), dense_x_grad.layout(), dense_x_grad.lod(), dense_x_grad.offset());
  argument_outputs.push_back(x_grad_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void SeluGradOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value out_, pir::Value out_grad_, pir::AttributeMap attributes) {
  VLOG(4) << "Start build SeluGradOp";


  IR_ENFORCE(
      attributes.find("scale") != attributes.end(),
          "'scale' Attribute is expected for SeluGradOp. ");
  float scale = attributes.at("scale").dyn_cast<pir::FloatAttribute>().data();

  IR_ENFORCE(
      attributes.find("alpha") != attributes.end(),
          "'alpha' Attribute is expected for SeluGradOp. ");
  float alpha = attributes.at("alpha").dyn_cast<pir::FloatAttribute>().data();


  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {out_, out_grad_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_scale = pir::FloatAttribute::get(pir::IrContext::Instance(), scale);
  argument.AddAttribute("scale", attr_scale);
  pir::Attribute attr_alpha = pir::FloatAttribute::get(pir::IrContext::Instance(), alpha);
  argument.AddAttribute("alpha", attr_alpha);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType out = out_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)out;
  paddle::dialect::DenseTensorType out_grad = out_grad_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)out_grad;

  VLOG(4) << "Builder construction  dense_out";
  paddle::dialect::IrTensor ir_tensor_out(paddle::dialect::TransToPhiDataType(out.dtype()),
                                                      out.dims(),
                                                      out.data_layout(),
                                                      out.lod(),
                                                      out.offset());
  VLOG(4) << "Builder construction  meta_out";
  paddle::dialect::IrMetaTensor meta_out(&ir_tensor_out);
  paddle::dialect::IrTensor dense_x_grad;
  paddle::dialect::IrMetaTensor meta_x_grad(&dense_x_grad);

  phi::UnchangedInferMeta(meta_out, &meta_x_grad);

  std::vector<pir::Type> argument_outputs;
  pir::Type x_grad_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_x_grad.dtype()), dense_x_grad.dims(), dense_x_grad.layout(), dense_x_grad.lod(), dense_x_grad.offset());
  argument_outputs.push_back(x_grad_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void SeluGradOp::VerifySig() {}

void SeluGradOp::InferMeta( phi::InferMetaContext *infer_meta ) {
  auto fn = PD_INFER_META(phi::UnchangedInferMeta);
  fn(infer_meta);
}

phi::DataType SeluGradOp::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: SeluGradOp";
  


  return expected_kernel_dtype;
}

const char *SendURecvGradOp::attributes_name[1] = { "reduce_op" };

OpInfoTuple SendURecvGradOp::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("x", "paddle::dialect::DenseTensorType", false, false, false, false), paddle::dialect::OpInputInfo("src_index", "paddle::dialect::DenseTensorType", false, false, false, false), paddle::dialect::OpInputInfo("dst_index", "paddle::dialect::DenseTensorType", false, false, false, false), paddle::dialect::OpInputInfo("out", "paddle::dialect::DenseTensorType", true, false, false, false), paddle::dialect::OpInputInfo("dst_count", "paddle::dialect::DenseTensorType", true, false, false, false), paddle::dialect::OpInputInfo("out_grad", "paddle::dialect::DenseTensorType", false, false, false, false) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = { paddle::dialect::OpAttributeInfo("reduce_op", "pir::StrAttribute", "") };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("x_grad", "paddle::dialect::DenseTensorType", false, false) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("GeneralUnaryGradInferMeta", {"x"}, "send_u_recv_grad", {"x", "src_index", "dst_index", "out", "dst_count", "out_grad", "reduce_op"}, {"out_grad"}, {}, {}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "send_u_recv_grad");
}

void SendURecvGradOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, pir::Value src_index_, pir::Value dst_index_, pir::Value out_, pir::Value dst_count_, pir::Value out_grad_, const std::string& reduce_op) {
  VLOG(4) << "Start build SendURecvGradOp";



  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_, src_index_, dst_index_, out_, dst_count_, out_grad_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_reduce_op = pir::StrAttribute::get(pir::IrContext::Instance(), reduce_op);
  argument.AddAttribute("reduce_op", attr_reduce_op);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;
  paddle::dialect::DenseTensorType src_index = src_index_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)src_index;
  paddle::dialect::DenseTensorType dst_index = dst_index_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)dst_index;
  paddle::dialect::DenseTensorType out_grad = out_grad_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)out_grad;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);
  paddle::dialect::IrTensor dense_x_grad;
  paddle::dialect::IrMetaTensor meta_x_grad(&dense_x_grad);

  phi::GeneralUnaryGradInferMeta(meta_x, &meta_x_grad);

  std::vector<pir::Type> argument_outputs;
  pir::Type x_grad_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_x_grad.dtype()), dense_x_grad.dims(), dense_x_grad.layout(), dense_x_grad.lod(), dense_x_grad.offset());
  argument_outputs.push_back(x_grad_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void SendURecvGradOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, pir::Value src_index_, pir::Value dst_index_, pir::Value out_, pir::Value dst_count_, pir::Value out_grad_, pir::AttributeMap attributes) {
  VLOG(4) << "Start build SendURecvGradOp";


  IR_ENFORCE(
      attributes.find("reduce_op") != attributes.end(),
          "'reduce_op' Attribute is expected for SendURecvGradOp. ");
  std::string reduce_op = attributes.at("reduce_op").dyn_cast<pir::StrAttribute>().AsString();


  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_, src_index_, dst_index_, out_, dst_count_, out_grad_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_reduce_op = pir::StrAttribute::get(pir::IrContext::Instance(), reduce_op);
  argument.AddAttribute("reduce_op", attr_reduce_op);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;
  paddle::dialect::DenseTensorType src_index = src_index_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)src_index;
  paddle::dialect::DenseTensorType dst_index = dst_index_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)dst_index;
  paddle::dialect::DenseTensorType out_grad = out_grad_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)out_grad;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);
  paddle::dialect::IrTensor dense_x_grad;
  paddle::dialect::IrMetaTensor meta_x_grad(&dense_x_grad);

  phi::GeneralUnaryGradInferMeta(meta_x, &meta_x_grad);

  std::vector<pir::Type> argument_outputs;
  pir::Type x_grad_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_x_grad.dtype()), dense_x_grad.dims(), dense_x_grad.layout(), dense_x_grad.lod(), dense_x_grad.offset());
  argument_outputs.push_back(x_grad_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void SendURecvGradOp::VerifySig() {}

void SendURecvGradOp::InferMeta( phi::InferMetaContext *infer_meta ) {
  auto fn = PD_INFER_META(phi::GeneralUnaryGradInferMeta);
  fn(infer_meta);
}

phi::DataType SendURecvGradOp::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: SendURecvGradOp";
  


  return expected_kernel_dtype;
}

const char *SendUeRecvGradOp::attributes_name[2] = { "message_op", "reduce_op" };

OpInfoTuple SendUeRecvGradOp::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("x", "paddle::dialect::DenseTensorType", false, false, false, false), paddle::dialect::OpInputInfo("y", "paddle::dialect::DenseTensorType", false, false, false, false), paddle::dialect::OpInputInfo("src_index", "paddle::dialect::DenseTensorType", false, false, false, false), paddle::dialect::OpInputInfo("dst_index", "paddle::dialect::DenseTensorType", false, false, false, false), paddle::dialect::OpInputInfo("out", "paddle::dialect::DenseTensorType", true, false, false, false), paddle::dialect::OpInputInfo("dst_count", "paddle::dialect::DenseTensorType", true, false, false, false), paddle::dialect::OpInputInfo("out_grad", "paddle::dialect::DenseTensorType", false, false, false, false) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = { paddle::dialect::OpAttributeInfo("message_op", "pir::StrAttribute", ""), paddle::dialect::OpAttributeInfo("reduce_op", "pir::StrAttribute", "") };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("x_grad", "paddle::dialect::DenseTensorType", false, false), paddle::dialect::OpOutputInfo("y_grad", "paddle::dialect::DenseTensorType", false, false) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("GeneralBinaryGradInferMeta", {"x", "y"}, "send_ue_recv_grad", {"x", "y", "src_index", "dst_index", "out", "dst_count", "out_grad", "message_op", "reduce_op"}, {"out_grad"}, {}, {}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "send_ue_recv_grad");
}

void SendUeRecvGradOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, pir::Value y_, pir::Value src_index_, pir::Value dst_index_, pir::Value out_, pir::Value dst_count_, pir::Value out_grad_, const std::string& message_op, const std::string& reduce_op) {
  VLOG(4) << "Start build SendUeRecvGradOp";



  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_, y_, src_index_, dst_index_, out_, dst_count_, out_grad_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_message_op = pir::StrAttribute::get(pir::IrContext::Instance(), message_op);
  argument.AddAttribute("message_op", attr_message_op);
  pir::Attribute attr_reduce_op = pir::StrAttribute::get(pir::IrContext::Instance(), reduce_op);
  argument.AddAttribute("reduce_op", attr_reduce_op);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;
  paddle::dialect::DenseTensorType y = y_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)y;
  paddle::dialect::DenseTensorType src_index = src_index_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)src_index;
  paddle::dialect::DenseTensorType dst_index = dst_index_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)dst_index;
  paddle::dialect::DenseTensorType out_grad = out_grad_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)out_grad;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);

  VLOG(4) << "Builder construction  dense_y";
  paddle::dialect::IrTensor ir_tensor_y(paddle::dialect::TransToPhiDataType(y.dtype()),
                                                      y.dims(),
                                                      y.data_layout(),
                                                      y.lod(),
                                                      y.offset());
  VLOG(4) << "Builder construction  meta_y";
  paddle::dialect::IrMetaTensor meta_y(&ir_tensor_y);
  paddle::dialect::IrTensor dense_x_grad;
  paddle::dialect::IrMetaTensor meta_x_grad(&dense_x_grad);
  paddle::dialect::IrTensor dense_y_grad;
  paddle::dialect::IrMetaTensor meta_y_grad(&dense_y_grad);

  phi::GeneralBinaryGradInferMeta(meta_x, meta_y, &meta_x_grad, &meta_y_grad);

  std::vector<pir::Type> argument_outputs;
  pir::Type x_grad_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_x_grad.dtype()), dense_x_grad.dims(), dense_x_grad.layout(), dense_x_grad.lod(), dense_x_grad.offset());
  argument_outputs.push_back(x_grad_dense_tensor_type);

  pir::Type y_grad_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_y_grad.dtype()), dense_y_grad.dims(), dense_y_grad.layout(), dense_y_grad.lod(), dense_y_grad.offset());
  argument_outputs.push_back(y_grad_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void SendUeRecvGradOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, pir::Value y_, pir::Value src_index_, pir::Value dst_index_, pir::Value out_, pir::Value dst_count_, pir::Value out_grad_, pir::AttributeMap attributes) {
  VLOG(4) << "Start build SendUeRecvGradOp";


  IR_ENFORCE(
      attributes.find("message_op") != attributes.end(),
          "'message_op' Attribute is expected for SendUeRecvGradOp. ");
  std::string message_op = attributes.at("message_op").dyn_cast<pir::StrAttribute>().AsString();

  IR_ENFORCE(
      attributes.find("reduce_op") != attributes.end(),
          "'reduce_op' Attribute is expected for SendUeRecvGradOp. ");
  std::string reduce_op = attributes.at("reduce_op").dyn_cast<pir::StrAttribute>().AsString();


  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_, y_, src_index_, dst_index_, out_, dst_count_, out_grad_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_message_op = pir::StrAttribute::get(pir::IrContext::Instance(), message_op);
  argument.AddAttribute("message_op", attr_message_op);
  pir::Attribute attr_reduce_op = pir::StrAttribute::get(pir::IrContext::Instance(), reduce_op);
  argument.AddAttribute("reduce_op", attr_reduce_op);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;
  paddle::dialect::DenseTensorType y = y_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)y;
  paddle::dialect::DenseTensorType src_index = src_index_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)src_index;
  paddle::dialect::DenseTensorType dst_index = dst_index_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)dst_index;
  paddle::dialect::DenseTensorType out_grad = out_grad_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)out_grad;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);

  VLOG(4) << "Builder construction  dense_y";
  paddle::dialect::IrTensor ir_tensor_y(paddle::dialect::TransToPhiDataType(y.dtype()),
                                                      y.dims(),
                                                      y.data_layout(),
                                                      y.lod(),
                                                      y.offset());
  VLOG(4) << "Builder construction  meta_y";
  paddle::dialect::IrMetaTensor meta_y(&ir_tensor_y);
  paddle::dialect::IrTensor dense_x_grad;
  paddle::dialect::IrMetaTensor meta_x_grad(&dense_x_grad);
  paddle::dialect::IrTensor dense_y_grad;
  paddle::dialect::IrMetaTensor meta_y_grad(&dense_y_grad);

  phi::GeneralBinaryGradInferMeta(meta_x, meta_y, &meta_x_grad, &meta_y_grad);

  std::vector<pir::Type> argument_outputs;
  pir::Type x_grad_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_x_grad.dtype()), dense_x_grad.dims(), dense_x_grad.layout(), dense_x_grad.lod(), dense_x_grad.offset());
  argument_outputs.push_back(x_grad_dense_tensor_type);

  pir::Type y_grad_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_y_grad.dtype()), dense_y_grad.dims(), dense_y_grad.layout(), dense_y_grad.lod(), dense_y_grad.offset());
  argument_outputs.push_back(y_grad_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void SendUeRecvGradOp::VerifySig() {}

void SendUeRecvGradOp::InferMeta( phi::InferMetaContext *infer_meta ) {
  auto fn = PD_INFER_META(phi::GeneralBinaryGradInferMeta);
  fn(infer_meta);
}

phi::DataType SendUeRecvGradOp::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: SendUeRecvGradOp";
  


  return expected_kernel_dtype;
}

const char *SendUvGradOp::attributes_name[1] = { "message_op" };

OpInfoTuple SendUvGradOp::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("x", "paddle::dialect::DenseTensorType", false, false, false, false), paddle::dialect::OpInputInfo("y", "paddle::dialect::DenseTensorType", false, false, false, false), paddle::dialect::OpInputInfo("src_index", "paddle::dialect::DenseTensorType", false, false, false, false), paddle::dialect::OpInputInfo("dst_index", "paddle::dialect::DenseTensorType", false, false, false, false), paddle::dialect::OpInputInfo("out_grad", "paddle::dialect::DenseTensorType", false, false, false, false) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = { paddle::dialect::OpAttributeInfo("message_op", "pir::StrAttribute", "") };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("x_grad", "paddle::dialect::DenseTensorType", false, false), paddle::dialect::OpOutputInfo("y_grad", "paddle::dialect::DenseTensorType", false, false) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("GeneralBinaryGradInferMeta", {"x", "y"}, "send_uv_grad", {"x", "y", "src_index", "dst_index", "out_grad", "message_op"}, {"x"}, {}, {}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "send_uv_grad");
}

void SendUvGradOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, pir::Value y_, pir::Value src_index_, pir::Value dst_index_, pir::Value out_grad_, const std::string& message_op) {
  VLOG(4) << "Start build SendUvGradOp";



  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_, y_, src_index_, dst_index_, out_grad_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_message_op = pir::StrAttribute::get(pir::IrContext::Instance(), message_op);
  argument.AddAttribute("message_op", attr_message_op);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;
  paddle::dialect::DenseTensorType y = y_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)y;
  paddle::dialect::DenseTensorType src_index = src_index_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)src_index;
  paddle::dialect::DenseTensorType dst_index = dst_index_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)dst_index;
  paddle::dialect::DenseTensorType out_grad = out_grad_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)out_grad;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);

  VLOG(4) << "Builder construction  dense_y";
  paddle::dialect::IrTensor ir_tensor_y(paddle::dialect::TransToPhiDataType(y.dtype()),
                                                      y.dims(),
                                                      y.data_layout(),
                                                      y.lod(),
                                                      y.offset());
  VLOG(4) << "Builder construction  meta_y";
  paddle::dialect::IrMetaTensor meta_y(&ir_tensor_y);
  paddle::dialect::IrTensor dense_x_grad;
  paddle::dialect::IrMetaTensor meta_x_grad(&dense_x_grad);
  paddle::dialect::IrTensor dense_y_grad;
  paddle::dialect::IrMetaTensor meta_y_grad(&dense_y_grad);

  phi::GeneralBinaryGradInferMeta(meta_x, meta_y, &meta_x_grad, &meta_y_grad);

  std::vector<pir::Type> argument_outputs;
  pir::Type x_grad_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_x_grad.dtype()), dense_x_grad.dims(), dense_x_grad.layout(), dense_x_grad.lod(), dense_x_grad.offset());
  argument_outputs.push_back(x_grad_dense_tensor_type);

  pir::Type y_grad_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_y_grad.dtype()), dense_y_grad.dims(), dense_y_grad.layout(), dense_y_grad.lod(), dense_y_grad.offset());
  argument_outputs.push_back(y_grad_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void SendUvGradOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, pir::Value y_, pir::Value src_index_, pir::Value dst_index_, pir::Value out_grad_, pir::AttributeMap attributes) {
  VLOG(4) << "Start build SendUvGradOp";


  IR_ENFORCE(
      attributes.find("message_op") != attributes.end(),
          "'message_op' Attribute is expected for SendUvGradOp. ");
  std::string message_op = attributes.at("message_op").dyn_cast<pir::StrAttribute>().AsString();


  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_, y_, src_index_, dst_index_, out_grad_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_message_op = pir::StrAttribute::get(pir::IrContext::Instance(), message_op);
  argument.AddAttribute("message_op", attr_message_op);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;
  paddle::dialect::DenseTensorType y = y_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)y;
  paddle::dialect::DenseTensorType src_index = src_index_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)src_index;
  paddle::dialect::DenseTensorType dst_index = dst_index_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)dst_index;
  paddle::dialect::DenseTensorType out_grad = out_grad_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)out_grad;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);

  VLOG(4) << "Builder construction  dense_y";
  paddle::dialect::IrTensor ir_tensor_y(paddle::dialect::TransToPhiDataType(y.dtype()),
                                                      y.dims(),
                                                      y.data_layout(),
                                                      y.lod(),
                                                      y.offset());
  VLOG(4) << "Builder construction  meta_y";
  paddle::dialect::IrMetaTensor meta_y(&ir_tensor_y);
  paddle::dialect::IrTensor dense_x_grad;
  paddle::dialect::IrMetaTensor meta_x_grad(&dense_x_grad);
  paddle::dialect::IrTensor dense_y_grad;
  paddle::dialect::IrMetaTensor meta_y_grad(&dense_y_grad);

  phi::GeneralBinaryGradInferMeta(meta_x, meta_y, &meta_x_grad, &meta_y_grad);

  std::vector<pir::Type> argument_outputs;
  pir::Type x_grad_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_x_grad.dtype()), dense_x_grad.dims(), dense_x_grad.layout(), dense_x_grad.lod(), dense_x_grad.offset());
  argument_outputs.push_back(x_grad_dense_tensor_type);

  pir::Type y_grad_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_y_grad.dtype()), dense_y_grad.dims(), dense_y_grad.layout(), dense_y_grad.lod(), dense_y_grad.offset());
  argument_outputs.push_back(y_grad_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void SendUvGradOp::VerifySig() {}

void SendUvGradOp::InferMeta( phi::InferMetaContext *infer_meta ) {
  auto fn = PD_INFER_META(phi::GeneralBinaryGradInferMeta);
  fn(infer_meta);
}

phi::DataType SendUvGradOp::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: SendUvGradOp";
  


  return expected_kernel_dtype;
}

const char *SigmoidCrossEntropyWithLogitsGradOp::attributes_name[2] = { "normalize", "ignore_index" };

OpInfoTuple SigmoidCrossEntropyWithLogitsGradOp::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("x", "paddle::dialect::DenseTensorType", false, false, false, false), paddle::dialect::OpInputInfo("label", "paddle::dialect::DenseTensorType", false, false, false, false), paddle::dialect::OpInputInfo("pos_weight", "paddle::dialect::DenseTensorType", true, false, false, false), paddle::dialect::OpInputInfo("out_grad", "paddle::dialect::DenseTensorType", false, false, false, false) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = { paddle::dialect::OpAttributeInfo("normalize", "pir::BoolAttribute", ""), paddle::dialect::OpAttributeInfo("ignore_index", "pir::Int32Attribute", "") };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("x_grad", "paddle::dialect::DenseTensorType", false, false) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("UnchangedInferMeta", {"x"}, "sigmoid_cross_entropy_with_logits_grad", {"x", "label", "pos_weight", "out_grad", "normalize", "ignore_index"}, {}, {}, {}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "sigmoid_cross_entropy_with_logits_grad");
}

void SigmoidCrossEntropyWithLogitsGradOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, pir::Value label_, pir::Value pos_weight_, pir::Value out_grad_, bool normalize, int ignore_index) {
  VLOG(4) << "Start build SigmoidCrossEntropyWithLogitsGradOp";



  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_, label_, pos_weight_, out_grad_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_normalize = pir::BoolAttribute::get(pir::IrContext::Instance(), normalize);
  argument.AddAttribute("normalize", attr_normalize);
  pir::Attribute attr_ignore_index = pir::Int32Attribute::get(pir::IrContext::Instance(), ignore_index);
  argument.AddAttribute("ignore_index", attr_ignore_index);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;
  paddle::dialect::DenseTensorType label = label_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)label;
  paddle::dialect::DenseTensorType out_grad = out_grad_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)out_grad;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);
  paddle::dialect::IrTensor dense_x_grad;
  paddle::dialect::IrMetaTensor meta_x_grad(&dense_x_grad);

  phi::UnchangedInferMeta(meta_x, &meta_x_grad);

  std::vector<pir::Type> argument_outputs;
  pir::Type x_grad_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_x_grad.dtype()), dense_x_grad.dims(), dense_x_grad.layout(), dense_x_grad.lod(), dense_x_grad.offset());
  argument_outputs.push_back(x_grad_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void SigmoidCrossEntropyWithLogitsGradOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, pir::Value label_, pir::Value pos_weight_, pir::Value out_grad_, pir::AttributeMap attributes) {
  VLOG(4) << "Start build SigmoidCrossEntropyWithLogitsGradOp";


  IR_ENFORCE(
      attributes.find("normalize") != attributes.end(),
          "'normalize' Attribute is expected for SigmoidCrossEntropyWithLogitsGradOp. ");
  bool normalize = attributes.at("normalize").dyn_cast<pir::BoolAttribute>().data();

  IR_ENFORCE(
      attributes.find("ignore_index") != attributes.end(),
          "'ignore_index' Attribute is expected for SigmoidCrossEntropyWithLogitsGradOp. ");
  int ignore_index = attributes.at("ignore_index").dyn_cast<pir::Int32Attribute>().data();


  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_, label_, pos_weight_, out_grad_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_normalize = pir::BoolAttribute::get(pir::IrContext::Instance(), normalize);
  argument.AddAttribute("normalize", attr_normalize);
  pir::Attribute attr_ignore_index = pir::Int32Attribute::get(pir::IrContext::Instance(), ignore_index);
  argument.AddAttribute("ignore_index", attr_ignore_index);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;
  paddle::dialect::DenseTensorType label = label_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)label;
  paddle::dialect::DenseTensorType out_grad = out_grad_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)out_grad;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);
  paddle::dialect::IrTensor dense_x_grad;
  paddle::dialect::IrMetaTensor meta_x_grad(&dense_x_grad);

  phi::UnchangedInferMeta(meta_x, &meta_x_grad);

  std::vector<pir::Type> argument_outputs;
  pir::Type x_grad_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_x_grad.dtype()), dense_x_grad.dims(), dense_x_grad.layout(), dense_x_grad.lod(), dense_x_grad.offset());
  argument_outputs.push_back(x_grad_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void SigmoidCrossEntropyWithLogitsGradOp::VerifySig() {}

void SigmoidCrossEntropyWithLogitsGradOp::InferMeta( phi::InferMetaContext *infer_meta ) {
  auto fn = PD_INFER_META(phi::UnchangedInferMeta);
  fn(infer_meta);
}

phi::DataType SigmoidCrossEntropyWithLogitsGradOp::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: SigmoidCrossEntropyWithLogitsGradOp";
  


  return expected_kernel_dtype;
}

const char *SigmoidCrossEntropyWithLogitsGrad_Op::attributes_name[2] = { "normalize", "ignore_index" };

OpInfoTuple SigmoidCrossEntropyWithLogitsGrad_Op::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("x", "paddle::dialect::DenseTensorType", false, false, false, false), paddle::dialect::OpInputInfo("label", "paddle::dialect::DenseTensorType", false, false, false, false), paddle::dialect::OpInputInfo("pos_weight", "paddle::dialect::DenseTensorType", true, false, false, false), paddle::dialect::OpInputInfo("out_grad", "paddle::dialect::DenseTensorType", false, false, false, false) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = { paddle::dialect::OpAttributeInfo("normalize", "pir::BoolAttribute", ""), paddle::dialect::OpAttributeInfo("ignore_index", "pir::Int32Attribute", "") };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("x_grad", "paddle::dialect::DenseTensorType", false, false) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("UnchangedInferMeta", {"x"}, "sigmoid_cross_entropy_with_logits_grad", {"x", "label", "pos_weight", "out_grad", "normalize", "ignore_index"}, {}, {}, {{"x_grad", "out_grad"}}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "sigmoid_cross_entropy_with_logits_grad");
}

void SigmoidCrossEntropyWithLogitsGrad_Op::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, pir::Value label_, pir::Value pos_weight_, pir::Value out_grad_, bool normalize, int ignore_index) {
  VLOG(4) << "Start build SigmoidCrossEntropyWithLogitsGrad_Op";



  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_, label_, pos_weight_, out_grad_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_normalize = pir::BoolAttribute::get(pir::IrContext::Instance(), normalize);
  argument.AddAttribute("normalize", attr_normalize);
  pir::Attribute attr_ignore_index = pir::Int32Attribute::get(pir::IrContext::Instance(), ignore_index);
  argument.AddAttribute("ignore_index", attr_ignore_index);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;
  paddle::dialect::DenseTensorType label = label_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)label;
  paddle::dialect::DenseTensorType out_grad = out_grad_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)out_grad;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);
  paddle::dialect::IrTensor dense_x_grad;
  paddle::dialect::IrMetaTensor meta_x_grad(&dense_x_grad);

  phi::UnchangedInferMeta(meta_x, &meta_x_grad);

  std::vector<pir::Type> argument_outputs;
  pir::Type x_grad_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_x_grad.dtype()), dense_x_grad.dims(), dense_x_grad.layout(), dense_x_grad.lod(), dense_x_grad.offset());
  argument_outputs.push_back(x_grad_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void SigmoidCrossEntropyWithLogitsGrad_Op::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, pir::Value label_, pir::Value pos_weight_, pir::Value out_grad_, pir::AttributeMap attributes) {
  VLOG(4) << "Start build SigmoidCrossEntropyWithLogitsGrad_Op";


  IR_ENFORCE(
      attributes.find("normalize") != attributes.end(),
          "'normalize' Attribute is expected for SigmoidCrossEntropyWithLogitsGrad_Op. ");
  bool normalize = attributes.at("normalize").dyn_cast<pir::BoolAttribute>().data();

  IR_ENFORCE(
      attributes.find("ignore_index") != attributes.end(),
          "'ignore_index' Attribute is expected for SigmoidCrossEntropyWithLogitsGrad_Op. ");
  int ignore_index = attributes.at("ignore_index").dyn_cast<pir::Int32Attribute>().data();


  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_, label_, pos_weight_, out_grad_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_normalize = pir::BoolAttribute::get(pir::IrContext::Instance(), normalize);
  argument.AddAttribute("normalize", attr_normalize);
  pir::Attribute attr_ignore_index = pir::Int32Attribute::get(pir::IrContext::Instance(), ignore_index);
  argument.AddAttribute("ignore_index", attr_ignore_index);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;
  paddle::dialect::DenseTensorType label = label_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)label;
  paddle::dialect::DenseTensorType out_grad = out_grad_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)out_grad;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);
  paddle::dialect::IrTensor dense_x_grad;
  paddle::dialect::IrMetaTensor meta_x_grad(&dense_x_grad);

  phi::UnchangedInferMeta(meta_x, &meta_x_grad);

  std::vector<pir::Type> argument_outputs;
  pir::Type x_grad_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_x_grad.dtype()), dense_x_grad.dims(), dense_x_grad.layout(), dense_x_grad.lod(), dense_x_grad.offset());
  argument_outputs.push_back(x_grad_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void SigmoidCrossEntropyWithLogitsGrad_Op::VerifySig() {}

void SigmoidCrossEntropyWithLogitsGrad_Op::InferMeta( phi::InferMetaContext *infer_meta ) {
  auto fn = PD_INFER_META(phi::UnchangedInferMeta);
  fn(infer_meta);
}

phi::DataType SigmoidCrossEntropyWithLogitsGrad_Op::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: SigmoidCrossEntropyWithLogitsGrad_Op";
  


  return expected_kernel_dtype;
}

OpInfoTuple SigmoidDoubleGradOp::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("out", "paddle::dialect::DenseTensorType", false, false, false, true), paddle::dialect::OpInputInfo("fwd_grad_out", "paddle::dialect::DenseTensorType", false, false, false, true), paddle::dialect::OpInputInfo("grad_x_grad", "paddle::dialect::DenseTensorType", false, false, false, true) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = {  };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("out_grad", "paddle::dialect::DenseTensorType", false, false), paddle::dialect::OpOutputInfo("fwd_grad_out_grad", "paddle::dialect::DenseTensorType", false, false) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("GeneralBinaryGradInferMeta", {"out", "fwd_grad_out"}, "sigmoid_double_grad", {"out", "fwd_grad_out", "grad_x_grad"}, {}, {}, {}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "sigmoid_double_grad");
}

void SigmoidDoubleGradOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value out_, pir::Value fwd_grad_out_, pir::Value grad_x_grad_) {
  VLOG(4) << "Start build SigmoidDoubleGradOp";



  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {out_, fwd_grad_out_, grad_x_grad_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType out = out_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)out;
  paddle::dialect::DenseTensorType fwd_grad_out = fwd_grad_out_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)fwd_grad_out;
  paddle::dialect::DenseTensorType grad_x_grad = grad_x_grad_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)grad_x_grad;

  VLOG(4) << "Builder construction  dense_out";
  paddle::dialect::IrTensor ir_tensor_out(paddle::dialect::TransToPhiDataType(out.dtype()),
                                                      out.dims(),
                                                      out.data_layout(),
                                                      out.lod(),
                                                      out.offset());
  VLOG(4) << "Builder construction  meta_out";
  paddle::dialect::IrMetaTensor meta_out(&ir_tensor_out);

  VLOG(4) << "Builder construction  dense_fwd_grad_out";
  paddle::dialect::IrTensor ir_tensor_fwd_grad_out(paddle::dialect::TransToPhiDataType(fwd_grad_out.dtype()),
                                                      fwd_grad_out.dims(),
                                                      fwd_grad_out.data_layout(),
                                                      fwd_grad_out.lod(),
                                                      fwd_grad_out.offset());
  VLOG(4) << "Builder construction  meta_fwd_grad_out";
  paddle::dialect::IrMetaTensor meta_fwd_grad_out(&ir_tensor_fwd_grad_out);
  paddle::dialect::IrTensor dense_out_grad;
  paddle::dialect::IrMetaTensor meta_out_grad(&dense_out_grad);
  paddle::dialect::IrTensor dense_fwd_grad_out_grad;
  paddle::dialect::IrMetaTensor meta_fwd_grad_out_grad(&dense_fwd_grad_out_grad);

  phi::GeneralBinaryGradInferMeta(meta_out, meta_fwd_grad_out, &meta_out_grad, &meta_fwd_grad_out_grad);

  std::vector<pir::Type> argument_outputs;
  pir::Type out_grad_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_out_grad.dtype()), dense_out_grad.dims(), dense_out_grad.layout(), dense_out_grad.lod(), dense_out_grad.offset());
  argument_outputs.push_back(out_grad_dense_tensor_type);

  pir::Type fwd_grad_out_grad_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_fwd_grad_out_grad.dtype()), dense_fwd_grad_out_grad.dims(), dense_fwd_grad_out_grad.layout(), dense_fwd_grad_out_grad.lod(), dense_fwd_grad_out_grad.offset());
  argument_outputs.push_back(fwd_grad_out_grad_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void SigmoidDoubleGradOp::VerifySig() {}

void SigmoidDoubleGradOp::InferMeta( phi::InferMetaContext *infer_meta ) {
  auto fn = PD_INFER_META(phi::GeneralBinaryGradInferMeta);
  fn(infer_meta);
}

phi::DataType SigmoidDoubleGradOp::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: SigmoidDoubleGradOp";
  


  return expected_kernel_dtype;
}

OpInfoTuple SigmoidDoubleGrad_Op::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("out", "paddle::dialect::DenseTensorType", false, false, false, true), paddle::dialect::OpInputInfo("fwd_grad_out", "paddle::dialect::DenseTensorType", false, false, false, true), paddle::dialect::OpInputInfo("grad_x_grad", "paddle::dialect::DenseTensorType", false, false, false, true) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = {  };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("out_grad", "paddle::dialect::DenseTensorType", false, false), paddle::dialect::OpOutputInfo("fwd_grad_out_grad", "paddle::dialect::DenseTensorType", false, false) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("GeneralBinaryGradInferMeta", {"out", "fwd_grad_out"}, "sigmoid_double_grad", {"out", "fwd_grad_out", "grad_x_grad"}, {}, {}, {{"fwd_grad_out_grad", "grad_x_grad"}}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "sigmoid_double_grad");
}

void SigmoidDoubleGrad_Op::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value out_, pir::Value fwd_grad_out_, pir::Value grad_x_grad_) {
  VLOG(4) << "Start build SigmoidDoubleGrad_Op";



  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {out_, fwd_grad_out_, grad_x_grad_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType out = out_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)out;
  paddle::dialect::DenseTensorType fwd_grad_out = fwd_grad_out_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)fwd_grad_out;
  paddle::dialect::DenseTensorType grad_x_grad = grad_x_grad_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)grad_x_grad;

  VLOG(4) << "Builder construction  dense_out";
  paddle::dialect::IrTensor ir_tensor_out(paddle::dialect::TransToPhiDataType(out.dtype()),
                                                      out.dims(),
                                                      out.data_layout(),
                                                      out.lod(),
                                                      out.offset());
  VLOG(4) << "Builder construction  meta_out";
  paddle::dialect::IrMetaTensor meta_out(&ir_tensor_out);

  VLOG(4) << "Builder construction  dense_fwd_grad_out";
  paddle::dialect::IrTensor ir_tensor_fwd_grad_out(paddle::dialect::TransToPhiDataType(fwd_grad_out.dtype()),
                                                      fwd_grad_out.dims(),
                                                      fwd_grad_out.data_layout(),
                                                      fwd_grad_out.lod(),
                                                      fwd_grad_out.offset());
  VLOG(4) << "Builder construction  meta_fwd_grad_out";
  paddle::dialect::IrMetaTensor meta_fwd_grad_out(&ir_tensor_fwd_grad_out);
  paddle::dialect::IrTensor dense_out_grad;
  paddle::dialect::IrMetaTensor meta_out_grad(&dense_out_grad);
  paddle::dialect::IrTensor dense_fwd_grad_out_grad;
  paddle::dialect::IrMetaTensor meta_fwd_grad_out_grad(&dense_fwd_grad_out_grad);

  phi::GeneralBinaryGradInferMeta(meta_out, meta_fwd_grad_out, &meta_out_grad, &meta_fwd_grad_out_grad);

  std::vector<pir::Type> argument_outputs;
  pir::Type out_grad_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_out_grad.dtype()), dense_out_grad.dims(), dense_out_grad.layout(), dense_out_grad.lod(), dense_out_grad.offset());
  argument_outputs.push_back(out_grad_dense_tensor_type);

  pir::Type fwd_grad_out_grad_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_fwd_grad_out_grad.dtype()), dense_fwd_grad_out_grad.dims(), dense_fwd_grad_out_grad.layout(), dense_fwd_grad_out_grad.lod(), dense_fwd_grad_out_grad.offset());
  argument_outputs.push_back(fwd_grad_out_grad_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void SigmoidDoubleGrad_Op::VerifySig() {}

void SigmoidDoubleGrad_Op::InferMeta( phi::InferMetaContext *infer_meta ) {
  auto fn = PD_INFER_META(phi::GeneralBinaryGradInferMeta);
  fn(infer_meta);
}

phi::DataType SigmoidDoubleGrad_Op::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: SigmoidDoubleGrad_Op";
  


  return expected_kernel_dtype;
}

OpInfoTuple SigmoidGradOp::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("out", "paddle::dialect::DenseTensorType", false, false, false, true), paddle::dialect::OpInputInfo("out_grad", "paddle::dialect::DenseTensorType", false, false, false, true) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = {  };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("x_grad", "paddle::dialect::DenseTensorType", false, false) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("UnchangedInferMeta", {"out"}, "sigmoid_grad", {"out", "out_grad"}, {}, {}, {}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "sigmoid_grad");
}

void SigmoidGradOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value out_, pir::Value out_grad_) {
  VLOG(4) << "Start build SigmoidGradOp";



  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {out_, out_grad_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType out = out_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)out;
  paddle::dialect::DenseTensorType out_grad = out_grad_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)out_grad;

  VLOG(4) << "Builder construction  dense_out";
  paddle::dialect::IrTensor ir_tensor_out(paddle::dialect::TransToPhiDataType(out.dtype()),
                                                      out.dims(),
                                                      out.data_layout(),
                                                      out.lod(),
                                                      out.offset());
  VLOG(4) << "Builder construction  meta_out";
  paddle::dialect::IrMetaTensor meta_out(&ir_tensor_out);
  paddle::dialect::IrTensor dense_x_grad;
  paddle::dialect::IrMetaTensor meta_x_grad(&dense_x_grad);

  phi::UnchangedInferMeta(meta_out, &meta_x_grad);

  std::vector<pir::Type> argument_outputs;
  pir::Type x_grad_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_x_grad.dtype()), dense_x_grad.dims(), dense_x_grad.layout(), dense_x_grad.lod(), dense_x_grad.offset());
  argument_outputs.push_back(x_grad_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void SigmoidGradOp::VerifySig() {}

void SigmoidGradOp::InferMeta( phi::InferMetaContext *infer_meta ) {
  auto fn = PD_INFER_META(phi::UnchangedInferMeta);
  fn(infer_meta);
}

phi::DataType SigmoidGradOp::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: SigmoidGradOp";
  


  return expected_kernel_dtype;
}

OpInfoTuple SigmoidGrad_Op::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("out", "paddle::dialect::DenseTensorType", false, false, false, true), paddle::dialect::OpInputInfo("out_grad", "paddle::dialect::DenseTensorType", false, false, false, true) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = {  };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("x_grad", "paddle::dialect::DenseTensorType", false, false) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("UnchangedInferMeta", {"out"}, "sigmoid_grad", {"out", "out_grad"}, {}, {}, {{"x_grad", "out_grad"}}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "sigmoid_grad");
}

void SigmoidGrad_Op::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value out_, pir::Value out_grad_) {
  VLOG(4) << "Start build SigmoidGrad_Op";



  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {out_, out_grad_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType out = out_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)out;
  paddle::dialect::DenseTensorType out_grad = out_grad_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)out_grad;

  VLOG(4) << "Builder construction  dense_out";
  paddle::dialect::IrTensor ir_tensor_out(paddle::dialect::TransToPhiDataType(out.dtype()),
                                                      out.dims(),
                                                      out.data_layout(),
                                                      out.lod(),
                                                      out.offset());
  VLOG(4) << "Builder construction  meta_out";
  paddle::dialect::IrMetaTensor meta_out(&ir_tensor_out);
  paddle::dialect::IrTensor dense_x_grad;
  paddle::dialect::IrMetaTensor meta_x_grad(&dense_x_grad);

  phi::UnchangedInferMeta(meta_out, &meta_x_grad);

  std::vector<pir::Type> argument_outputs;
  pir::Type x_grad_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_x_grad.dtype()), dense_x_grad.dims(), dense_x_grad.layout(), dense_x_grad.lod(), dense_x_grad.offset());
  argument_outputs.push_back(x_grad_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void SigmoidGrad_Op::VerifySig() {}

void SigmoidGrad_Op::InferMeta( phi::InferMetaContext *infer_meta ) {
  auto fn = PD_INFER_META(phi::UnchangedInferMeta);
  fn(infer_meta);
}

phi::DataType SigmoidGrad_Op::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: SigmoidGrad_Op";
  


  return expected_kernel_dtype;
}

OpInfoTuple SigmoidTripleGradOp::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("out", "paddle::dialect::DenseTensorType", false, false, false, false), paddle::dialect::OpInputInfo("fwd_grad_out", "paddle::dialect::DenseTensorType", false, false, false, false), paddle::dialect::OpInputInfo("grad_grad_x", "paddle::dialect::DenseTensorType", false, false, false, false), paddle::dialect::OpInputInfo("grad_out_grad", "paddle::dialect::DenseTensorType", false, false, false, false), paddle::dialect::OpInputInfo("grad_grad_out_grad", "paddle::dialect::DenseTensorType", true, false, false, false) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = {  };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("out_grad", "paddle::dialect::DenseTensorType", false, false), paddle::dialect::OpOutputInfo("fwd_grad_out_grad", "paddle::dialect::DenseTensorType", false, false), paddle::dialect::OpOutputInfo("grad_grad_x_grad", "paddle::dialect::DenseTensorType", false, false) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("GeneralTernaryGradInferMeta", {"out", "fwd_grad_out", "grad_grad_x"}, "sigmoid_triple_grad", {"out", "fwd_grad_out", "grad_grad_x", "grad_out_grad", "grad_grad_out_grad"}, {}, {}, {}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "sigmoid_triple_grad");
}

void SigmoidTripleGradOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value out_, pir::Value fwd_grad_out_, pir::Value grad_grad_x_, pir::Value grad_out_grad_, pir::Value grad_grad_out_grad_) {
  VLOG(4) << "Start build SigmoidTripleGradOp";



  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {out_, fwd_grad_out_, grad_grad_x_, grad_out_grad_, grad_grad_out_grad_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType out = out_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)out;
  paddle::dialect::DenseTensorType fwd_grad_out = fwd_grad_out_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)fwd_grad_out;
  paddle::dialect::DenseTensorType grad_grad_x = grad_grad_x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)grad_grad_x;
  paddle::dialect::DenseTensorType grad_out_grad = grad_out_grad_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)grad_out_grad;

  VLOG(4) << "Builder construction  dense_out";
  paddle::dialect::IrTensor ir_tensor_out(paddle::dialect::TransToPhiDataType(out.dtype()),
                                                      out.dims(),
                                                      out.data_layout(),
                                                      out.lod(),
                                                      out.offset());
  VLOG(4) << "Builder construction  meta_out";
  paddle::dialect::IrMetaTensor meta_out(&ir_tensor_out);

  VLOG(4) << "Builder construction  dense_fwd_grad_out";
  paddle::dialect::IrTensor ir_tensor_fwd_grad_out(paddle::dialect::TransToPhiDataType(fwd_grad_out.dtype()),
                                                      fwd_grad_out.dims(),
                                                      fwd_grad_out.data_layout(),
                                                      fwd_grad_out.lod(),
                                                      fwd_grad_out.offset());
  VLOG(4) << "Builder construction  meta_fwd_grad_out";
  paddle::dialect::IrMetaTensor meta_fwd_grad_out(&ir_tensor_fwd_grad_out);

  VLOG(4) << "Builder construction  dense_grad_grad_x";
  paddle::dialect::IrTensor ir_tensor_grad_grad_x(paddle::dialect::TransToPhiDataType(grad_grad_x.dtype()),
                                                      grad_grad_x.dims(),
                                                      grad_grad_x.data_layout(),
                                                      grad_grad_x.lod(),
                                                      grad_grad_x.offset());
  VLOG(4) << "Builder construction  meta_grad_grad_x";
  paddle::dialect::IrMetaTensor meta_grad_grad_x(&ir_tensor_grad_grad_x);
  paddle::dialect::IrTensor dense_out_grad;
  paddle::dialect::IrMetaTensor meta_out_grad(&dense_out_grad);
  paddle::dialect::IrTensor dense_fwd_grad_out_grad;
  paddle::dialect::IrMetaTensor meta_fwd_grad_out_grad(&dense_fwd_grad_out_grad);
  paddle::dialect::IrTensor dense_grad_grad_x_grad;
  paddle::dialect::IrMetaTensor meta_grad_grad_x_grad(&dense_grad_grad_x_grad);

  phi::GeneralTernaryGradInferMeta(meta_out, meta_fwd_grad_out, meta_grad_grad_x, &meta_out_grad, &meta_fwd_grad_out_grad, &meta_grad_grad_x_grad);

  std::vector<pir::Type> argument_outputs;
  pir::Type out_grad_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_out_grad.dtype()), dense_out_grad.dims(), dense_out_grad.layout(), dense_out_grad.lod(), dense_out_grad.offset());
  argument_outputs.push_back(out_grad_dense_tensor_type);

  pir::Type fwd_grad_out_grad_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_fwd_grad_out_grad.dtype()), dense_fwd_grad_out_grad.dims(), dense_fwd_grad_out_grad.layout(), dense_fwd_grad_out_grad.lod(), dense_fwd_grad_out_grad.offset());
  argument_outputs.push_back(fwd_grad_out_grad_dense_tensor_type);

  pir::Type grad_grad_x_grad_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_grad_grad_x_grad.dtype()), dense_grad_grad_x_grad.dims(), dense_grad_grad_x_grad.layout(), dense_grad_grad_x_grad.lod(), dense_grad_grad_x_grad.offset());
  argument_outputs.push_back(grad_grad_x_grad_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void SigmoidTripleGradOp::VerifySig() {}

void SigmoidTripleGradOp::InferMeta( phi::InferMetaContext *infer_meta ) {
  auto fn = PD_INFER_META(phi::GeneralTernaryGradInferMeta);
  fn(infer_meta);
}

phi::DataType SigmoidTripleGradOp::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: SigmoidTripleGradOp";
  


  return expected_kernel_dtype;
}

OpInfoTuple SigmoidTripleGrad_Op::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("out", "paddle::dialect::DenseTensorType", false, false, false, false), paddle::dialect::OpInputInfo("fwd_grad_out", "paddle::dialect::DenseTensorType", false, false, false, false), paddle::dialect::OpInputInfo("grad_grad_x", "paddle::dialect::DenseTensorType", false, false, false, false), paddle::dialect::OpInputInfo("grad_out_grad", "paddle::dialect::DenseTensorType", false, false, false, false), paddle::dialect::OpInputInfo("grad_grad_out_grad", "paddle::dialect::DenseTensorType", true, false, false, false) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = {  };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("out_grad", "paddle::dialect::DenseTensorType", false, false), paddle::dialect::OpOutputInfo("fwd_grad_out_grad", "paddle::dialect::DenseTensorType", false, false), paddle::dialect::OpOutputInfo("grad_grad_x_grad", "paddle::dialect::DenseTensorType", false, false) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("GeneralTernaryGradInferMeta", {"out", "fwd_grad_out", "grad_grad_x"}, "sigmoid_triple_grad", {"out", "fwd_grad_out", "grad_grad_x", "grad_out_grad", "grad_grad_out_grad"}, {}, {}, {{"fwd_grad_out_grad", "grad_grad_x"}}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "sigmoid_triple_grad");
}

void SigmoidTripleGrad_Op::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value out_, pir::Value fwd_grad_out_, pir::Value grad_grad_x_, pir::Value grad_out_grad_, pir::Value grad_grad_out_grad_) {
  VLOG(4) << "Start build SigmoidTripleGrad_Op";



  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {out_, fwd_grad_out_, grad_grad_x_, grad_out_grad_, grad_grad_out_grad_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType out = out_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)out;
  paddle::dialect::DenseTensorType fwd_grad_out = fwd_grad_out_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)fwd_grad_out;
  paddle::dialect::DenseTensorType grad_grad_x = grad_grad_x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)grad_grad_x;
  paddle::dialect::DenseTensorType grad_out_grad = grad_out_grad_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)grad_out_grad;

  VLOG(4) << "Builder construction  dense_out";
  paddle::dialect::IrTensor ir_tensor_out(paddle::dialect::TransToPhiDataType(out.dtype()),
                                                      out.dims(),
                                                      out.data_layout(),
                                                      out.lod(),
                                                      out.offset());
  VLOG(4) << "Builder construction  meta_out";
  paddle::dialect::IrMetaTensor meta_out(&ir_tensor_out);

  VLOG(4) << "Builder construction  dense_fwd_grad_out";
  paddle::dialect::IrTensor ir_tensor_fwd_grad_out(paddle::dialect::TransToPhiDataType(fwd_grad_out.dtype()),
                                                      fwd_grad_out.dims(),
                                                      fwd_grad_out.data_layout(),
                                                      fwd_grad_out.lod(),
                                                      fwd_grad_out.offset());
  VLOG(4) << "Builder construction  meta_fwd_grad_out";
  paddle::dialect::IrMetaTensor meta_fwd_grad_out(&ir_tensor_fwd_grad_out);

  VLOG(4) << "Builder construction  dense_grad_grad_x";
  paddle::dialect::IrTensor ir_tensor_grad_grad_x(paddle::dialect::TransToPhiDataType(grad_grad_x.dtype()),
                                                      grad_grad_x.dims(),
                                                      grad_grad_x.data_layout(),
                                                      grad_grad_x.lod(),
                                                      grad_grad_x.offset());
  VLOG(4) << "Builder construction  meta_grad_grad_x";
  paddle::dialect::IrMetaTensor meta_grad_grad_x(&ir_tensor_grad_grad_x);
  paddle::dialect::IrTensor dense_out_grad;
  paddle::dialect::IrMetaTensor meta_out_grad(&dense_out_grad);
  paddle::dialect::IrTensor dense_fwd_grad_out_grad;
  paddle::dialect::IrMetaTensor meta_fwd_grad_out_grad(&dense_fwd_grad_out_grad);
  paddle::dialect::IrTensor dense_grad_grad_x_grad;
  paddle::dialect::IrMetaTensor meta_grad_grad_x_grad(&dense_grad_grad_x_grad);

  phi::GeneralTernaryGradInferMeta(meta_out, meta_fwd_grad_out, meta_grad_grad_x, &meta_out_grad, &meta_fwd_grad_out_grad, &meta_grad_grad_x_grad);

  std::vector<pir::Type> argument_outputs;
  pir::Type out_grad_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_out_grad.dtype()), dense_out_grad.dims(), dense_out_grad.layout(), dense_out_grad.lod(), dense_out_grad.offset());
  argument_outputs.push_back(out_grad_dense_tensor_type);

  pir::Type fwd_grad_out_grad_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_fwd_grad_out_grad.dtype()), dense_fwd_grad_out_grad.dims(), dense_fwd_grad_out_grad.layout(), dense_fwd_grad_out_grad.lod(), dense_fwd_grad_out_grad.offset());
  argument_outputs.push_back(fwd_grad_out_grad_dense_tensor_type);

  pir::Type grad_grad_x_grad_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_grad_grad_x_grad.dtype()), dense_grad_grad_x_grad.dims(), dense_grad_grad_x_grad.layout(), dense_grad_grad_x_grad.lod(), dense_grad_grad_x_grad.offset());
  argument_outputs.push_back(grad_grad_x_grad_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void SigmoidTripleGrad_Op::VerifySig() {}

void SigmoidTripleGrad_Op::InferMeta( phi::InferMetaContext *infer_meta ) {
  auto fn = PD_INFER_META(phi::GeneralTernaryGradInferMeta);
  fn(infer_meta);
}

phi::DataType SigmoidTripleGrad_Op::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: SigmoidTripleGrad_Op";
  


  return expected_kernel_dtype;
}

OpInfoTuple SignGradOp::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("out_grad", "paddle::dialect::DenseTensorType", false, false, false, false) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = {  };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("x_grad", "paddle::dialect::DenseTensorType", false, false) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("", {""}, "", {""}, {}, {}, {}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "sign_grad");
}

void SignGradOp::VerifySig() {}

phi::DataType SignGradOp::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: SignGradOp";
  


  return expected_kernel_dtype;
}

OpInfoTuple SiluGradOp::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("x", "paddle::dialect::DenseTensorType", false, false, false, true), paddle::dialect::OpInputInfo("out", "paddle::dialect::DenseTensorType", false, false, false, false), paddle::dialect::OpInputInfo("out_grad", "paddle::dialect::DenseTensorType", false, false, false, true) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = {  };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("x_grad", "paddle::dialect::DenseTensorType", false, false) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("UnchangedInferMeta", {"x"}, "silu_grad", {"x", "out", "out_grad"}, {}, {}, {}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "silu_grad");
}

void SiluGradOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, pir::Value out_, pir::Value out_grad_) {
  VLOG(4) << "Start build SiluGradOp";



  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_, out_, out_grad_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;
  paddle::dialect::DenseTensorType out = out_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)out;
  paddle::dialect::DenseTensorType out_grad = out_grad_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)out_grad;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);
  paddle::dialect::IrTensor dense_x_grad;
  paddle::dialect::IrMetaTensor meta_x_grad(&dense_x_grad);

  phi::UnchangedInferMeta(meta_x, &meta_x_grad);

  std::vector<pir::Type> argument_outputs;
  pir::Type x_grad_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_x_grad.dtype()), dense_x_grad.dims(), dense_x_grad.layout(), dense_x_grad.lod(), dense_x_grad.offset());
  argument_outputs.push_back(x_grad_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void SiluGradOp::VerifySig() {}

void SiluGradOp::InferMeta( phi::InferMetaContext *infer_meta ) {
  auto fn = PD_INFER_META(phi::UnchangedInferMeta);
  fn(infer_meta);
}

phi::DataType SiluGradOp::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: SiluGradOp";
  


  return expected_kernel_dtype;
}

OpInfoTuple SiluGrad_Op::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("x", "paddle::dialect::DenseTensorType", false, false, false, true), paddle::dialect::OpInputInfo("out", "paddle::dialect::DenseTensorType", false, false, false, false), paddle::dialect::OpInputInfo("out_grad", "paddle::dialect::DenseTensorType", false, false, false, true) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = {  };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("x_grad", "paddle::dialect::DenseTensorType", false, false) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("UnchangedInferMeta", {"x"}, "silu_grad", {"x", "out", "out_grad"}, {}, {}, {{"x_grad", "out_grad"}}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "silu_grad");
}

void SiluGrad_Op::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, pir::Value out_, pir::Value out_grad_) {
  VLOG(4) << "Start build SiluGrad_Op";



  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_, out_, out_grad_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;
  paddle::dialect::DenseTensorType out = out_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)out;
  paddle::dialect::DenseTensorType out_grad = out_grad_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)out_grad;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);
  paddle::dialect::IrTensor dense_x_grad;
  paddle::dialect::IrMetaTensor meta_x_grad(&dense_x_grad);

  phi::UnchangedInferMeta(meta_x, &meta_x_grad);

  std::vector<pir::Type> argument_outputs;
  pir::Type x_grad_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_x_grad.dtype()), dense_x_grad.dims(), dense_x_grad.layout(), dense_x_grad.lod(), dense_x_grad.offset());
  argument_outputs.push_back(x_grad_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void SiluGrad_Op::VerifySig() {}

void SiluGrad_Op::InferMeta( phi::InferMetaContext *infer_meta ) {
  auto fn = PD_INFER_META(phi::UnchangedInferMeta);
  fn(infer_meta);
}

phi::DataType SiluGrad_Op::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: SiluGrad_Op";
  


  return expected_kernel_dtype;
}

OpInfoTuple SinDoubleGradOp::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("x", "paddle::dialect::DenseTensorType", false, false, false, true), paddle::dialect::OpInputInfo("grad_out", "paddle::dialect::DenseTensorType", true, false, false, true), paddle::dialect::OpInputInfo("grad_x_grad", "paddle::dialect::DenseTensorType", false, false, false, true) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = {  };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("x_grad", "paddle::dialect::DenseTensorType", false, false), paddle::dialect::OpOutputInfo("grad_out_grad", "paddle::dialect::DenseTensorType", false, false) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("GeneralBinaryGradInferMeta", {"x", "x"}, "sin_double_grad", {"x", "grad_out", "grad_x_grad"}, {}, {}, {}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "sin_double_grad");
}

void SinDoubleGradOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, pir::Value grad_out_, pir::Value grad_x_grad_) {
  VLOG(4) << "Start build SinDoubleGradOp";



  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_, grad_out_, grad_x_grad_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;
  paddle::dialect::DenseTensorType grad_x_grad = grad_x_grad_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)grad_x_grad;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);
  paddle::dialect::IrTensor dense_x_grad;
  paddle::dialect::IrMetaTensor meta_x_grad(&dense_x_grad);
  paddle::dialect::IrTensor dense_grad_out_grad;
  paddle::dialect::IrMetaTensor meta_grad_out_grad(&dense_grad_out_grad);

  phi::GeneralBinaryGradInferMeta(meta_x, meta_x, &meta_x_grad, &meta_grad_out_grad);

  std::vector<pir::Type> argument_outputs;
  pir::Type x_grad_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_x_grad.dtype()), dense_x_grad.dims(), dense_x_grad.layout(), dense_x_grad.lod(), dense_x_grad.offset());
  argument_outputs.push_back(x_grad_dense_tensor_type);

  pir::Type grad_out_grad_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_grad_out_grad.dtype()), dense_grad_out_grad.dims(), dense_grad_out_grad.layout(), dense_grad_out_grad.lod(), dense_grad_out_grad.offset());
  argument_outputs.push_back(grad_out_grad_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void SinDoubleGradOp::VerifySig() {}

void SinDoubleGradOp::InferMeta( phi::InferMetaContext *infer_meta ) {
  auto fn = PD_INFER_META(phi::GeneralBinaryGradInferMeta);
  fn(infer_meta);
}

phi::DataType SinDoubleGradOp::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: SinDoubleGradOp";
  


  return expected_kernel_dtype;
}

OpInfoTuple SinDoubleGrad_Op::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("x", "paddle::dialect::DenseTensorType", false, false, false, true), paddle::dialect::OpInputInfo("grad_out", "paddle::dialect::DenseTensorType", true, false, false, true), paddle::dialect::OpInputInfo("grad_x_grad", "paddle::dialect::DenseTensorType", false, false, false, true) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = {  };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("x_grad", "paddle::dialect::DenseTensorType", false, false), paddle::dialect::OpOutputInfo("grad_out_grad", "paddle::dialect::DenseTensorType", false, false) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("GeneralBinaryGradInferMeta", {"x", "x"}, "sin_double_grad", {"x", "grad_out", "grad_x_grad"}, {}, {}, {{"grad_out_grad", "grad_x_grad"}}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "sin_double_grad");
}

void SinDoubleGrad_Op::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, pir::Value grad_out_, pir::Value grad_x_grad_) {
  VLOG(4) << "Start build SinDoubleGrad_Op";



  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_, grad_out_, grad_x_grad_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;
  paddle::dialect::DenseTensorType grad_x_grad = grad_x_grad_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)grad_x_grad;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);
  paddle::dialect::IrTensor dense_x_grad;
  paddle::dialect::IrMetaTensor meta_x_grad(&dense_x_grad);
  paddle::dialect::IrTensor dense_grad_out_grad;
  paddle::dialect::IrMetaTensor meta_grad_out_grad(&dense_grad_out_grad);

  phi::GeneralBinaryGradInferMeta(meta_x, meta_x, &meta_x_grad, &meta_grad_out_grad);

  std::vector<pir::Type> argument_outputs;
  pir::Type x_grad_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_x_grad.dtype()), dense_x_grad.dims(), dense_x_grad.layout(), dense_x_grad.lod(), dense_x_grad.offset());
  argument_outputs.push_back(x_grad_dense_tensor_type);

  pir::Type grad_out_grad_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_grad_out_grad.dtype()), dense_grad_out_grad.dims(), dense_grad_out_grad.layout(), dense_grad_out_grad.lod(), dense_grad_out_grad.offset());
  argument_outputs.push_back(grad_out_grad_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void SinDoubleGrad_Op::VerifySig() {}

void SinDoubleGrad_Op::InferMeta( phi::InferMetaContext *infer_meta ) {
  auto fn = PD_INFER_META(phi::GeneralBinaryGradInferMeta);
  fn(infer_meta);
}

phi::DataType SinDoubleGrad_Op::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: SinDoubleGrad_Op";
  


  return expected_kernel_dtype;
}

OpInfoTuple SinGradOp::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("x", "paddle::dialect::DenseTensorType", false, false, false, true), paddle::dialect::OpInputInfo("out_grad", "paddle::dialect::DenseTensorType", false, false, false, true) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = {  };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("x_grad", "paddle::dialect::DenseTensorType", false, false) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("UnchangedInferMeta", {"x"}, "sin_grad", {"x", "out_grad"}, {}, {}, {}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "sin_grad");
}

void SinGradOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, pir::Value out_grad_) {
  VLOG(4) << "Start build SinGradOp";



  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_, out_grad_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;
  paddle::dialect::DenseTensorType out_grad = out_grad_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)out_grad;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);
  paddle::dialect::IrTensor dense_x_grad;
  paddle::dialect::IrMetaTensor meta_x_grad(&dense_x_grad);

  phi::UnchangedInferMeta(meta_x, &meta_x_grad);

  std::vector<pir::Type> argument_outputs;
  pir::Type x_grad_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_x_grad.dtype()), dense_x_grad.dims(), dense_x_grad.layout(), dense_x_grad.lod(), dense_x_grad.offset());
  argument_outputs.push_back(x_grad_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void SinGradOp::VerifySig() {}

void SinGradOp::InferMeta( phi::InferMetaContext *infer_meta ) {
  auto fn = PD_INFER_META(phi::UnchangedInferMeta);
  fn(infer_meta);
}

phi::DataType SinGradOp::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: SinGradOp";
  


  return expected_kernel_dtype;
}

OpInfoTuple SinGrad_Op::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("x", "paddle::dialect::DenseTensorType", false, false, false, true), paddle::dialect::OpInputInfo("out_grad", "paddle::dialect::DenseTensorType", false, false, false, true) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = {  };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("x_grad", "paddle::dialect::DenseTensorType", false, false) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("UnchangedInferMeta", {"x"}, "sin_grad", {"x", "out_grad"}, {}, {}, {{"x_grad", "out_grad"}}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "sin_grad");
}

void SinGrad_Op::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, pir::Value out_grad_) {
  VLOG(4) << "Start build SinGrad_Op";



  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_, out_grad_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;
  paddle::dialect::DenseTensorType out_grad = out_grad_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)out_grad;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);
  paddle::dialect::IrTensor dense_x_grad;
  paddle::dialect::IrMetaTensor meta_x_grad(&dense_x_grad);

  phi::UnchangedInferMeta(meta_x, &meta_x_grad);

  std::vector<pir::Type> argument_outputs;
  pir::Type x_grad_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_x_grad.dtype()), dense_x_grad.dims(), dense_x_grad.layout(), dense_x_grad.lod(), dense_x_grad.offset());
  argument_outputs.push_back(x_grad_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void SinGrad_Op::VerifySig() {}

void SinGrad_Op::InferMeta( phi::InferMetaContext *infer_meta ) {
  auto fn = PD_INFER_META(phi::UnchangedInferMeta);
  fn(infer_meta);
}

phi::DataType SinGrad_Op::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: SinGrad_Op";
  


  return expected_kernel_dtype;
}

OpInfoTuple SinTripleGradOp::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("x", "paddle::dialect::DenseTensorType", false, false, false, false), paddle::dialect::OpInputInfo("grad_out_forward", "paddle::dialect::DenseTensorType", true, false, false, false), paddle::dialect::OpInputInfo("grad_x_grad_forward", "paddle::dialect::DenseTensorType", true, false, false, false), paddle::dialect::OpInputInfo("grad_x_grad", "paddle::dialect::DenseTensorType", false, false, false, false), paddle::dialect::OpInputInfo("grad_out_grad_grad", "paddle::dialect::DenseTensorType", true, false, false, false) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = {  };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("x_grad", "paddle::dialect::DenseTensorType", false, false), paddle::dialect::OpOutputInfo("grad_out_forward_grad", "paddle::dialect::DenseTensorType", false, false), paddle::dialect::OpOutputInfo("grad_x_grad_forward_grad", "paddle::dialect::DenseTensorType", false, false) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("GeneralTernaryGradInferMeta", {"x", "x", "grad_x_grad_forward"}, "sin_triple_grad", {"x", "grad_out_forward", "grad_x_grad_forward", "grad_x_grad", "grad_out_grad_grad"}, {}, {}, {}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "sin_triple_grad");
}

void SinTripleGradOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, pir::Value grad_out_forward_, pir::Value grad_x_grad_forward_, pir::Value grad_x_grad_, pir::Value grad_out_grad_grad_) {
  VLOG(4) << "Start build SinTripleGradOp";



  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_, grad_out_forward_, grad_x_grad_forward_, grad_x_grad_, grad_out_grad_grad_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;
  paddle::dialect::DenseTensorType grad_x_grad = grad_x_grad_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)grad_x_grad;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);

  paddle::dialect::IrMetaTensor meta_grad_x_grad_forward;
  paddle::dialect::IrTensor ir_tensor_grad_x_grad_forward;
  if (grad_x_grad_forward_.impl() != nullptr) {
    paddle::dialect::DenseTensorType grad_x_grad_forward = grad_x_grad_forward_.type().dyn_cast<paddle::dialect::DenseTensorType>();
    VLOG(4) << "Builder construction  dense_grad_x_grad_forward";
    ir_tensor_grad_x_grad_forward = paddle::dialect::IrTensor(paddle::dialect::TransToPhiDataType(grad_x_grad_forward.dtype()),
                                                        grad_x_grad_forward.dims(),
                                                        grad_x_grad_forward.data_layout(),
                                                        grad_x_grad_forward.lod(),
                                                        grad_x_grad_forward.offset());
    VLOG(4) << "Builder construction  meta_grad_x_grad_forward";
    meta_grad_x_grad_forward = paddle::dialect::IrMetaTensor(&ir_tensor_grad_x_grad_forward);
  }

  paddle::dialect::IrTensor dense_x_grad;
  paddle::dialect::IrMetaTensor meta_x_grad(&dense_x_grad);
  paddle::dialect::IrTensor dense_grad_out_forward_grad;
  paddle::dialect::IrMetaTensor meta_grad_out_forward_grad(&dense_grad_out_forward_grad);
  paddle::dialect::IrTensor dense_grad_x_grad_forward_grad;
  paddle::dialect::IrMetaTensor meta_grad_x_grad_forward_grad(&dense_grad_x_grad_forward_grad);

  phi::GeneralTernaryGradInferMeta(meta_x, meta_x, meta_grad_x_grad_forward, &meta_x_grad, &meta_grad_out_forward_grad, &meta_grad_x_grad_forward_grad);

  std::vector<pir::Type> argument_outputs;
  pir::Type x_grad_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_x_grad.dtype()), dense_x_grad.dims(), dense_x_grad.layout(), dense_x_grad.lod(), dense_x_grad.offset());
  argument_outputs.push_back(x_grad_dense_tensor_type);

  pir::Type grad_out_forward_grad_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_grad_out_forward_grad.dtype()), dense_grad_out_forward_grad.dims(), dense_grad_out_forward_grad.layout(), dense_grad_out_forward_grad.lod(), dense_grad_out_forward_grad.offset());
  argument_outputs.push_back(grad_out_forward_grad_dense_tensor_type);

  pir::Type grad_x_grad_forward_grad_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_grad_x_grad_forward_grad.dtype()), dense_grad_x_grad_forward_grad.dims(), dense_grad_x_grad_forward_grad.layout(), dense_grad_x_grad_forward_grad.lod(), dense_grad_x_grad_forward_grad.offset());
  argument_outputs.push_back(grad_x_grad_forward_grad_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void SinTripleGradOp::VerifySig() {}

void SinTripleGradOp::InferMeta( phi::InferMetaContext *infer_meta ) {
  auto fn = PD_INFER_META(phi::GeneralTernaryGradInferMeta);
  fn(infer_meta);
}

phi::DataType SinTripleGradOp::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: SinTripleGradOp";
  


  return expected_kernel_dtype;
}

OpInfoTuple SinTripleGrad_Op::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("x", "paddle::dialect::DenseTensorType", false, false, false, false), paddle::dialect::OpInputInfo("grad_out_forward", "paddle::dialect::DenseTensorType", true, false, false, false), paddle::dialect::OpInputInfo("grad_x_grad_forward", "paddle::dialect::DenseTensorType", true, false, false, false), paddle::dialect::OpInputInfo("grad_x_grad", "paddle::dialect::DenseTensorType", false, false, false, false), paddle::dialect::OpInputInfo("grad_out_grad_grad", "paddle::dialect::DenseTensorType", true, false, false, false) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = {  };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("x_grad", "paddle::dialect::DenseTensorType", false, false), paddle::dialect::OpOutputInfo("grad_out_forward_grad", "paddle::dialect::DenseTensorType", false, false), paddle::dialect::OpOutputInfo("grad_x_grad_forward_grad", "paddle::dialect::DenseTensorType", false, false) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("GeneralTernaryGradInferMeta", {"x", "x", "grad_x_grad_forward"}, "sin_triple_grad", {"x", "grad_out_forward", "grad_x_grad_forward", "grad_x_grad", "grad_out_grad_grad"}, {}, {}, {{"grad_out_forward_grad", "grad_x_grad_forward"}}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "sin_triple_grad");
}

void SinTripleGrad_Op::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, pir::Value grad_out_forward_, pir::Value grad_x_grad_forward_, pir::Value grad_x_grad_, pir::Value grad_out_grad_grad_) {
  VLOG(4) << "Start build SinTripleGrad_Op";



  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_, grad_out_forward_, grad_x_grad_forward_, grad_x_grad_, grad_out_grad_grad_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;
  paddle::dialect::DenseTensorType grad_x_grad = grad_x_grad_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)grad_x_grad;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);

  paddle::dialect::IrMetaTensor meta_grad_x_grad_forward;
  paddle::dialect::IrTensor ir_tensor_grad_x_grad_forward;
  if (grad_x_grad_forward_.impl() != nullptr) {
    paddle::dialect::DenseTensorType grad_x_grad_forward = grad_x_grad_forward_.type().dyn_cast<paddle::dialect::DenseTensorType>();
    VLOG(4) << "Builder construction  dense_grad_x_grad_forward";
    ir_tensor_grad_x_grad_forward = paddle::dialect::IrTensor(paddle::dialect::TransToPhiDataType(grad_x_grad_forward.dtype()),
                                                        grad_x_grad_forward.dims(),
                                                        grad_x_grad_forward.data_layout(),
                                                        grad_x_grad_forward.lod(),
                                                        grad_x_grad_forward.offset());
    VLOG(4) << "Builder construction  meta_grad_x_grad_forward";
    meta_grad_x_grad_forward = paddle::dialect::IrMetaTensor(&ir_tensor_grad_x_grad_forward);
  }

  paddle::dialect::IrTensor dense_x_grad;
  paddle::dialect::IrMetaTensor meta_x_grad(&dense_x_grad);
  paddle::dialect::IrTensor dense_grad_out_forward_grad;
  paddle::dialect::IrMetaTensor meta_grad_out_forward_grad(&dense_grad_out_forward_grad);
  paddle::dialect::IrTensor dense_grad_x_grad_forward_grad;
  paddle::dialect::IrMetaTensor meta_grad_x_grad_forward_grad(&dense_grad_x_grad_forward_grad);

  phi::GeneralTernaryGradInferMeta(meta_x, meta_x, meta_grad_x_grad_forward, &meta_x_grad, &meta_grad_out_forward_grad, &meta_grad_x_grad_forward_grad);

  std::vector<pir::Type> argument_outputs;
  pir::Type x_grad_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_x_grad.dtype()), dense_x_grad.dims(), dense_x_grad.layout(), dense_x_grad.lod(), dense_x_grad.offset());
  argument_outputs.push_back(x_grad_dense_tensor_type);

  pir::Type grad_out_forward_grad_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_grad_out_forward_grad.dtype()), dense_grad_out_forward_grad.dims(), dense_grad_out_forward_grad.layout(), dense_grad_out_forward_grad.lod(), dense_grad_out_forward_grad.offset());
  argument_outputs.push_back(grad_out_forward_grad_dense_tensor_type);

  pir::Type grad_x_grad_forward_grad_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_grad_x_grad_forward_grad.dtype()), dense_grad_x_grad_forward_grad.dims(), dense_grad_x_grad_forward_grad.layout(), dense_grad_x_grad_forward_grad.lod(), dense_grad_x_grad_forward_grad.offset());
  argument_outputs.push_back(grad_x_grad_forward_grad_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void SinTripleGrad_Op::VerifySig() {}

void SinTripleGrad_Op::InferMeta( phi::InferMetaContext *infer_meta ) {
  auto fn = PD_INFER_META(phi::GeneralTernaryGradInferMeta);
  fn(infer_meta);
}

phi::DataType SinTripleGrad_Op::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: SinTripleGrad_Op";
  


  return expected_kernel_dtype;
}

OpInfoTuple SinhGradOp::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("x", "paddle::dialect::DenseTensorType", false, false, false, false), paddle::dialect::OpInputInfo("out_grad", "paddle::dialect::DenseTensorType", false, false, false, false) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = {  };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("x_grad", "paddle::dialect::DenseTensorType", false, false) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("UnchangedInferMeta", {"x"}, "sinh_grad", {"x", "out_grad"}, {}, {}, {}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "sinh_grad");
}

void SinhGradOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, pir::Value out_grad_) {
  VLOG(4) << "Start build SinhGradOp";



  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_, out_grad_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;
  paddle::dialect::DenseTensorType out_grad = out_grad_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)out_grad;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);
  paddle::dialect::IrTensor dense_x_grad;
  paddle::dialect::IrMetaTensor meta_x_grad(&dense_x_grad);

  phi::UnchangedInferMeta(meta_x, &meta_x_grad);

  std::vector<pir::Type> argument_outputs;
  pir::Type x_grad_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_x_grad.dtype()), dense_x_grad.dims(), dense_x_grad.layout(), dense_x_grad.lod(), dense_x_grad.offset());
  argument_outputs.push_back(x_grad_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void SinhGradOp::VerifySig() {}

void SinhGradOp::InferMeta( phi::InferMetaContext *infer_meta ) {
  auto fn = PD_INFER_META(phi::UnchangedInferMeta);
  fn(infer_meta);
}

phi::DataType SinhGradOp::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: SinhGradOp";
  


  return expected_kernel_dtype;
}

OpInfoTuple SinhGrad_Op::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("x", "paddle::dialect::DenseTensorType", false, false, false, false), paddle::dialect::OpInputInfo("out_grad", "paddle::dialect::DenseTensorType", false, false, false, false) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = {  };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("x_grad", "paddle::dialect::DenseTensorType", false, false) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("UnchangedInferMeta", {"x"}, "sinh_grad", {"x", "out_grad"}, {}, {}, {{"x_grad", "out_grad"}}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "sinh_grad");
}

void SinhGrad_Op::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, pir::Value out_grad_) {
  VLOG(4) << "Start build SinhGrad_Op";



  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_, out_grad_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;
  paddle::dialect::DenseTensorType out_grad = out_grad_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)out_grad;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);
  paddle::dialect::IrTensor dense_x_grad;
  paddle::dialect::IrMetaTensor meta_x_grad(&dense_x_grad);

  phi::UnchangedInferMeta(meta_x, &meta_x_grad);

  std::vector<pir::Type> argument_outputs;
  pir::Type x_grad_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_x_grad.dtype()), dense_x_grad.dims(), dense_x_grad.layout(), dense_x_grad.lod(), dense_x_grad.offset());
  argument_outputs.push_back(x_grad_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void SinhGrad_Op::VerifySig() {}

void SinhGrad_Op::InferMeta( phi::InferMetaContext *infer_meta ) {
  auto fn = PD_INFER_META(phi::UnchangedInferMeta);
  fn(infer_meta);
}

phi::DataType SinhGrad_Op::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: SinhGrad_Op";
  


  return expected_kernel_dtype;
}

OpInfoTuple SlogdetGradOp::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("x", "paddle::dialect::DenseTensorType", false, false, false, false), paddle::dialect::OpInputInfo("out", "paddle::dialect::DenseTensorType", false, false, false, false), paddle::dialect::OpInputInfo("out_grad", "paddle::dialect::DenseTensorType", false, false, false, false) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = {  };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("x_grad", "paddle::dialect::DenseTensorType", false, false) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("GeneralUnaryGradInferMeta", {"x"}, "slogdet_grad", {"x", "out", "out_grad"}, {"out_grad"}, {}, {}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "slogdet_grad");
}

void SlogdetGradOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, pir::Value out_, pir::Value out_grad_) {
  VLOG(4) << "Start build SlogdetGradOp";



  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_, out_, out_grad_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;
  paddle::dialect::DenseTensorType out = out_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)out;
  paddle::dialect::DenseTensorType out_grad = out_grad_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)out_grad;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);
  paddle::dialect::IrTensor dense_x_grad;
  paddle::dialect::IrMetaTensor meta_x_grad(&dense_x_grad);

  phi::GeneralUnaryGradInferMeta(meta_x, &meta_x_grad);

  std::vector<pir::Type> argument_outputs;
  pir::Type x_grad_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_x_grad.dtype()), dense_x_grad.dims(), dense_x_grad.layout(), dense_x_grad.lod(), dense_x_grad.offset());
  argument_outputs.push_back(x_grad_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void SlogdetGradOp::VerifySig() {}

void SlogdetGradOp::InferMeta( phi::InferMetaContext *infer_meta ) {
  auto fn = PD_INFER_META(phi::GeneralUnaryGradInferMeta);
  fn(infer_meta);
}

phi::DataType SlogdetGradOp::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: SlogdetGradOp";
  


  return expected_kernel_dtype;
}

const char *SoftplusDoubleGradOp::attributes_name[2] = { "beta", "threshold" };

OpInfoTuple SoftplusDoubleGradOp::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("x", "paddle::dialect::DenseTensorType", false, false, false, false), paddle::dialect::OpInputInfo("grad_out", "paddle::dialect::DenseTensorType", false, false, false, false), paddle::dialect::OpInputInfo("grad_x_grad", "paddle::dialect::DenseTensorType", false, false, false, false) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = { paddle::dialect::OpAttributeInfo("beta", "pir::FloatAttribute", ""), paddle::dialect::OpAttributeInfo("threshold", "pir::FloatAttribute", "") };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("x_grad", "paddle::dialect::DenseTensorType", false, false), paddle::dialect::OpOutputInfo("grad_out_grad", "paddle::dialect::DenseTensorType", false, false) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("GeneralBinaryGradInferMeta", {"x", "x"}, "softplus_double_grad", {"x", "grad_out", "grad_x_grad", "beta", "threshold"}, {}, {}, {}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "softplus_double_grad");
}

void SoftplusDoubleGradOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, pir::Value grad_out_, pir::Value grad_x_grad_, float beta, float threshold) {
  VLOG(4) << "Start build SoftplusDoubleGradOp";



  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_, grad_out_, grad_x_grad_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_beta = pir::FloatAttribute::get(pir::IrContext::Instance(), beta);
  argument.AddAttribute("beta", attr_beta);
  pir::Attribute attr_threshold = pir::FloatAttribute::get(pir::IrContext::Instance(), threshold);
  argument.AddAttribute("threshold", attr_threshold);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;
  paddle::dialect::DenseTensorType grad_out = grad_out_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)grad_out;
  paddle::dialect::DenseTensorType grad_x_grad = grad_x_grad_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)grad_x_grad;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);
  paddle::dialect::IrTensor dense_x_grad;
  paddle::dialect::IrMetaTensor meta_x_grad(&dense_x_grad);
  paddle::dialect::IrTensor dense_grad_out_grad;
  paddle::dialect::IrMetaTensor meta_grad_out_grad(&dense_grad_out_grad);

  phi::GeneralBinaryGradInferMeta(meta_x, meta_x, &meta_x_grad, &meta_grad_out_grad);

  std::vector<pir::Type> argument_outputs;
  pir::Type x_grad_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_x_grad.dtype()), dense_x_grad.dims(), dense_x_grad.layout(), dense_x_grad.lod(), dense_x_grad.offset());
  argument_outputs.push_back(x_grad_dense_tensor_type);

  pir::Type grad_out_grad_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_grad_out_grad.dtype()), dense_grad_out_grad.dims(), dense_grad_out_grad.layout(), dense_grad_out_grad.lod(), dense_grad_out_grad.offset());
  argument_outputs.push_back(grad_out_grad_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void SoftplusDoubleGradOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, pir::Value grad_out_, pir::Value grad_x_grad_, pir::AttributeMap attributes) {
  VLOG(4) << "Start build SoftplusDoubleGradOp";


  IR_ENFORCE(
      attributes.find("beta") != attributes.end(),
          "'beta' Attribute is expected for SoftplusDoubleGradOp. ");
  float beta = attributes.at("beta").dyn_cast<pir::FloatAttribute>().data();

  IR_ENFORCE(
      attributes.find("threshold") != attributes.end(),
          "'threshold' Attribute is expected for SoftplusDoubleGradOp. ");
  float threshold = attributes.at("threshold").dyn_cast<pir::FloatAttribute>().data();


  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_, grad_out_, grad_x_grad_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_beta = pir::FloatAttribute::get(pir::IrContext::Instance(), beta);
  argument.AddAttribute("beta", attr_beta);
  pir::Attribute attr_threshold = pir::FloatAttribute::get(pir::IrContext::Instance(), threshold);
  argument.AddAttribute("threshold", attr_threshold);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;
  paddle::dialect::DenseTensorType grad_out = grad_out_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)grad_out;
  paddle::dialect::DenseTensorType grad_x_grad = grad_x_grad_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)grad_x_grad;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);
  paddle::dialect::IrTensor dense_x_grad;
  paddle::dialect::IrMetaTensor meta_x_grad(&dense_x_grad);
  paddle::dialect::IrTensor dense_grad_out_grad;
  paddle::dialect::IrMetaTensor meta_grad_out_grad(&dense_grad_out_grad);

  phi::GeneralBinaryGradInferMeta(meta_x, meta_x, &meta_x_grad, &meta_grad_out_grad);

  std::vector<pir::Type> argument_outputs;
  pir::Type x_grad_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_x_grad.dtype()), dense_x_grad.dims(), dense_x_grad.layout(), dense_x_grad.lod(), dense_x_grad.offset());
  argument_outputs.push_back(x_grad_dense_tensor_type);

  pir::Type grad_out_grad_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_grad_out_grad.dtype()), dense_grad_out_grad.dims(), dense_grad_out_grad.layout(), dense_grad_out_grad.lod(), dense_grad_out_grad.offset());
  argument_outputs.push_back(grad_out_grad_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void SoftplusDoubleGradOp::VerifySig() {}

void SoftplusDoubleGradOp::InferMeta( phi::InferMetaContext *infer_meta ) {
  auto fn = PD_INFER_META(phi::GeneralBinaryGradInferMeta);
  fn(infer_meta);
}

phi::DataType SoftplusDoubleGradOp::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: SoftplusDoubleGradOp";
  


  return expected_kernel_dtype;
}

const char *SoftplusDoubleGrad_Op::attributes_name[2] = { "beta", "threshold" };

OpInfoTuple SoftplusDoubleGrad_Op::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("x", "paddle::dialect::DenseTensorType", false, false, false, false), paddle::dialect::OpInputInfo("grad_out", "paddle::dialect::DenseTensorType", false, false, false, false), paddle::dialect::OpInputInfo("grad_x_grad", "paddle::dialect::DenseTensorType", false, false, false, false) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = { paddle::dialect::OpAttributeInfo("beta", "pir::FloatAttribute", ""), paddle::dialect::OpAttributeInfo("threshold", "pir::FloatAttribute", "") };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("x_grad", "paddle::dialect::DenseTensorType", false, false), paddle::dialect::OpOutputInfo("grad_out_grad", "paddle::dialect::DenseTensorType", false, false) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("GeneralBinaryGradInferMeta", {"x", "x"}, "softplus_double_grad", {"x", "grad_out", "grad_x_grad", "beta", "threshold"}, {}, {}, {{"grad_out_grad", "grad_x_grad"}}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "softplus_double_grad");
}

void SoftplusDoubleGrad_Op::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, pir::Value grad_out_, pir::Value grad_x_grad_, float beta, float threshold) {
  VLOG(4) << "Start build SoftplusDoubleGrad_Op";



  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_, grad_out_, grad_x_grad_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_beta = pir::FloatAttribute::get(pir::IrContext::Instance(), beta);
  argument.AddAttribute("beta", attr_beta);
  pir::Attribute attr_threshold = pir::FloatAttribute::get(pir::IrContext::Instance(), threshold);
  argument.AddAttribute("threshold", attr_threshold);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;
  paddle::dialect::DenseTensorType grad_out = grad_out_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)grad_out;
  paddle::dialect::DenseTensorType grad_x_grad = grad_x_grad_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)grad_x_grad;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);
  paddle::dialect::IrTensor dense_x_grad;
  paddle::dialect::IrMetaTensor meta_x_grad(&dense_x_grad);
  paddle::dialect::IrTensor dense_grad_out_grad;
  paddle::dialect::IrMetaTensor meta_grad_out_grad(&dense_grad_out_grad);

  phi::GeneralBinaryGradInferMeta(meta_x, meta_x, &meta_x_grad, &meta_grad_out_grad);

  std::vector<pir::Type> argument_outputs;
  pir::Type x_grad_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_x_grad.dtype()), dense_x_grad.dims(), dense_x_grad.layout(), dense_x_grad.lod(), dense_x_grad.offset());
  argument_outputs.push_back(x_grad_dense_tensor_type);

  pir::Type grad_out_grad_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_grad_out_grad.dtype()), dense_grad_out_grad.dims(), dense_grad_out_grad.layout(), dense_grad_out_grad.lod(), dense_grad_out_grad.offset());
  argument_outputs.push_back(grad_out_grad_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void SoftplusDoubleGrad_Op::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, pir::Value grad_out_, pir::Value grad_x_grad_, pir::AttributeMap attributes) {
  VLOG(4) << "Start build SoftplusDoubleGrad_Op";


  IR_ENFORCE(
      attributes.find("beta") != attributes.end(),
          "'beta' Attribute is expected for SoftplusDoubleGrad_Op. ");
  float beta = attributes.at("beta").dyn_cast<pir::FloatAttribute>().data();

  IR_ENFORCE(
      attributes.find("threshold") != attributes.end(),
          "'threshold' Attribute is expected for SoftplusDoubleGrad_Op. ");
  float threshold = attributes.at("threshold").dyn_cast<pir::FloatAttribute>().data();


  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_, grad_out_, grad_x_grad_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_beta = pir::FloatAttribute::get(pir::IrContext::Instance(), beta);
  argument.AddAttribute("beta", attr_beta);
  pir::Attribute attr_threshold = pir::FloatAttribute::get(pir::IrContext::Instance(), threshold);
  argument.AddAttribute("threshold", attr_threshold);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;
  paddle::dialect::DenseTensorType grad_out = grad_out_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)grad_out;
  paddle::dialect::DenseTensorType grad_x_grad = grad_x_grad_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)grad_x_grad;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);
  paddle::dialect::IrTensor dense_x_grad;
  paddle::dialect::IrMetaTensor meta_x_grad(&dense_x_grad);
  paddle::dialect::IrTensor dense_grad_out_grad;
  paddle::dialect::IrMetaTensor meta_grad_out_grad(&dense_grad_out_grad);

  phi::GeneralBinaryGradInferMeta(meta_x, meta_x, &meta_x_grad, &meta_grad_out_grad);

  std::vector<pir::Type> argument_outputs;
  pir::Type x_grad_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_x_grad.dtype()), dense_x_grad.dims(), dense_x_grad.layout(), dense_x_grad.lod(), dense_x_grad.offset());
  argument_outputs.push_back(x_grad_dense_tensor_type);

  pir::Type grad_out_grad_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_grad_out_grad.dtype()), dense_grad_out_grad.dims(), dense_grad_out_grad.layout(), dense_grad_out_grad.lod(), dense_grad_out_grad.offset());
  argument_outputs.push_back(grad_out_grad_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void SoftplusDoubleGrad_Op::VerifySig() {}

void SoftplusDoubleGrad_Op::InferMeta( phi::InferMetaContext *infer_meta ) {
  auto fn = PD_INFER_META(phi::GeneralBinaryGradInferMeta);
  fn(infer_meta);
}

phi::DataType SoftplusDoubleGrad_Op::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: SoftplusDoubleGrad_Op";
  


  return expected_kernel_dtype;
}

const char *SoftplusGradOp::attributes_name[2] = { "beta", "threshold" };

OpInfoTuple SoftplusGradOp::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("x", "paddle::dialect::DenseTensorType", false, false, false, true), paddle::dialect::OpInputInfo("out_grad", "paddle::dialect::DenseTensorType", false, false, false, true) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = { paddle::dialect::OpAttributeInfo("beta", "pir::FloatAttribute", ""), paddle::dialect::OpAttributeInfo("threshold", "pir::FloatAttribute", "") };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("x_grad", "paddle::dialect::DenseTensorType", false, false) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("UnchangedInferMeta", {"x"}, "softplus_grad", {"x", "out_grad", "beta", "threshold"}, {}, {}, {}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "softplus_grad");
}

void SoftplusGradOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, pir::Value out_grad_, float beta, float threshold) {
  VLOG(4) << "Start build SoftplusGradOp";



  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_, out_grad_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_beta = pir::FloatAttribute::get(pir::IrContext::Instance(), beta);
  argument.AddAttribute("beta", attr_beta);
  pir::Attribute attr_threshold = pir::FloatAttribute::get(pir::IrContext::Instance(), threshold);
  argument.AddAttribute("threshold", attr_threshold);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;
  paddle::dialect::DenseTensorType out_grad = out_grad_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)out_grad;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);
  paddle::dialect::IrTensor dense_x_grad;
  paddle::dialect::IrMetaTensor meta_x_grad(&dense_x_grad);

  phi::UnchangedInferMeta(meta_x, &meta_x_grad);

  std::vector<pir::Type> argument_outputs;
  pir::Type x_grad_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_x_grad.dtype()), dense_x_grad.dims(), dense_x_grad.layout(), dense_x_grad.lod(), dense_x_grad.offset());
  argument_outputs.push_back(x_grad_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void SoftplusGradOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, pir::Value out_grad_, pir::AttributeMap attributes) {
  VLOG(4) << "Start build SoftplusGradOp";


  IR_ENFORCE(
      attributes.find("beta") != attributes.end(),
          "'beta' Attribute is expected for SoftplusGradOp. ");
  float beta = attributes.at("beta").dyn_cast<pir::FloatAttribute>().data();

  IR_ENFORCE(
      attributes.find("threshold") != attributes.end(),
          "'threshold' Attribute is expected for SoftplusGradOp. ");
  float threshold = attributes.at("threshold").dyn_cast<pir::FloatAttribute>().data();


  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_, out_grad_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_beta = pir::FloatAttribute::get(pir::IrContext::Instance(), beta);
  argument.AddAttribute("beta", attr_beta);
  pir::Attribute attr_threshold = pir::FloatAttribute::get(pir::IrContext::Instance(), threshold);
  argument.AddAttribute("threshold", attr_threshold);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;
  paddle::dialect::DenseTensorType out_grad = out_grad_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)out_grad;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);
  paddle::dialect::IrTensor dense_x_grad;
  paddle::dialect::IrMetaTensor meta_x_grad(&dense_x_grad);

  phi::UnchangedInferMeta(meta_x, &meta_x_grad);

  std::vector<pir::Type> argument_outputs;
  pir::Type x_grad_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_x_grad.dtype()), dense_x_grad.dims(), dense_x_grad.layout(), dense_x_grad.lod(), dense_x_grad.offset());
  argument_outputs.push_back(x_grad_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void SoftplusGradOp::VerifySig() {}

void SoftplusGradOp::InferMeta( phi::InferMetaContext *infer_meta ) {
  auto fn = PD_INFER_META(phi::UnchangedInferMeta);
  fn(infer_meta);
}

phi::DataType SoftplusGradOp::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: SoftplusGradOp";
  


  return expected_kernel_dtype;
}

const char *SoftplusGrad_Op::attributes_name[2] = { "beta", "threshold" };

OpInfoTuple SoftplusGrad_Op::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("x", "paddle::dialect::DenseTensorType", false, false, false, true), paddle::dialect::OpInputInfo("out_grad", "paddle::dialect::DenseTensorType", false, false, false, true) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = { paddle::dialect::OpAttributeInfo("beta", "pir::FloatAttribute", ""), paddle::dialect::OpAttributeInfo("threshold", "pir::FloatAttribute", "") };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("x_grad", "paddle::dialect::DenseTensorType", false, false) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("UnchangedInferMeta", {"x"}, "softplus_grad", {"x", "out_grad", "beta", "threshold"}, {}, {}, {{"x_grad", "out_grad"}}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "softplus_grad");
}

void SoftplusGrad_Op::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, pir::Value out_grad_, float beta, float threshold) {
  VLOG(4) << "Start build SoftplusGrad_Op";



  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_, out_grad_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_beta = pir::FloatAttribute::get(pir::IrContext::Instance(), beta);
  argument.AddAttribute("beta", attr_beta);
  pir::Attribute attr_threshold = pir::FloatAttribute::get(pir::IrContext::Instance(), threshold);
  argument.AddAttribute("threshold", attr_threshold);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;
  paddle::dialect::DenseTensorType out_grad = out_grad_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)out_grad;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);
  paddle::dialect::IrTensor dense_x_grad;
  paddle::dialect::IrMetaTensor meta_x_grad(&dense_x_grad);

  phi::UnchangedInferMeta(meta_x, &meta_x_grad);

  std::vector<pir::Type> argument_outputs;
  pir::Type x_grad_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_x_grad.dtype()), dense_x_grad.dims(), dense_x_grad.layout(), dense_x_grad.lod(), dense_x_grad.offset());
  argument_outputs.push_back(x_grad_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void SoftplusGrad_Op::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, pir::Value out_grad_, pir::AttributeMap attributes) {
  VLOG(4) << "Start build SoftplusGrad_Op";


  IR_ENFORCE(
      attributes.find("beta") != attributes.end(),
          "'beta' Attribute is expected for SoftplusGrad_Op. ");
  float beta = attributes.at("beta").dyn_cast<pir::FloatAttribute>().data();

  IR_ENFORCE(
      attributes.find("threshold") != attributes.end(),
          "'threshold' Attribute is expected for SoftplusGrad_Op. ");
  float threshold = attributes.at("threshold").dyn_cast<pir::FloatAttribute>().data();


  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_, out_grad_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_beta = pir::FloatAttribute::get(pir::IrContext::Instance(), beta);
  argument.AddAttribute("beta", attr_beta);
  pir::Attribute attr_threshold = pir::FloatAttribute::get(pir::IrContext::Instance(), threshold);
  argument.AddAttribute("threshold", attr_threshold);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;
  paddle::dialect::DenseTensorType out_grad = out_grad_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)out_grad;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);
  paddle::dialect::IrTensor dense_x_grad;
  paddle::dialect::IrMetaTensor meta_x_grad(&dense_x_grad);

  phi::UnchangedInferMeta(meta_x, &meta_x_grad);

  std::vector<pir::Type> argument_outputs;
  pir::Type x_grad_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_x_grad.dtype()), dense_x_grad.dims(), dense_x_grad.layout(), dense_x_grad.lod(), dense_x_grad.offset());
  argument_outputs.push_back(x_grad_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void SoftplusGrad_Op::VerifySig() {}

void SoftplusGrad_Op::InferMeta( phi::InferMetaContext *infer_meta ) {
  auto fn = PD_INFER_META(phi::UnchangedInferMeta);
  fn(infer_meta);
}

phi::DataType SoftplusGrad_Op::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: SoftplusGrad_Op";
  


  return expected_kernel_dtype;
}

const char *SoftshrinkGradOp::attributes_name[1] = { "threshold" };

OpInfoTuple SoftshrinkGradOp::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("x", "paddle::dialect::DenseTensorType", false, false, false, false), paddle::dialect::OpInputInfo("out_grad", "paddle::dialect::DenseTensorType", false, false, false, false) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = { paddle::dialect::OpAttributeInfo("threshold", "pir::FloatAttribute", "") };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("x_grad", "paddle::dialect::DenseTensorType", false, false) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("UnchangedInferMeta", {"x"}, "softshrink_grad", {"x", "out_grad", "threshold"}, {}, {}, {}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "softshrink_grad");
}

void SoftshrinkGradOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, pir::Value out_grad_, float threshold) {
  VLOG(4) << "Start build SoftshrinkGradOp";



  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_, out_grad_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_threshold = pir::FloatAttribute::get(pir::IrContext::Instance(), threshold);
  argument.AddAttribute("threshold", attr_threshold);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;
  paddle::dialect::DenseTensorType out_grad = out_grad_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)out_grad;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);
  paddle::dialect::IrTensor dense_x_grad;
  paddle::dialect::IrMetaTensor meta_x_grad(&dense_x_grad);

  phi::UnchangedInferMeta(meta_x, &meta_x_grad);

  std::vector<pir::Type> argument_outputs;
  pir::Type x_grad_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_x_grad.dtype()), dense_x_grad.dims(), dense_x_grad.layout(), dense_x_grad.lod(), dense_x_grad.offset());
  argument_outputs.push_back(x_grad_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void SoftshrinkGradOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, pir::Value out_grad_, pir::AttributeMap attributes) {
  VLOG(4) << "Start build SoftshrinkGradOp";


  IR_ENFORCE(
      attributes.find("threshold") != attributes.end(),
          "'threshold' Attribute is expected for SoftshrinkGradOp. ");
  float threshold = attributes.at("threshold").dyn_cast<pir::FloatAttribute>().data();


  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_, out_grad_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_threshold = pir::FloatAttribute::get(pir::IrContext::Instance(), threshold);
  argument.AddAttribute("threshold", attr_threshold);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;
  paddle::dialect::DenseTensorType out_grad = out_grad_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)out_grad;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);
  paddle::dialect::IrTensor dense_x_grad;
  paddle::dialect::IrMetaTensor meta_x_grad(&dense_x_grad);

  phi::UnchangedInferMeta(meta_x, &meta_x_grad);

  std::vector<pir::Type> argument_outputs;
  pir::Type x_grad_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_x_grad.dtype()), dense_x_grad.dims(), dense_x_grad.layout(), dense_x_grad.lod(), dense_x_grad.offset());
  argument_outputs.push_back(x_grad_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void SoftshrinkGradOp::VerifySig() {}

void SoftshrinkGradOp::InferMeta( phi::InferMetaContext *infer_meta ) {
  auto fn = PD_INFER_META(phi::UnchangedInferMeta);
  fn(infer_meta);
}

phi::DataType SoftshrinkGradOp::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: SoftshrinkGradOp";
  


  return expected_kernel_dtype;
}

const char *SoftshrinkGrad_Op::attributes_name[1] = { "threshold" };

OpInfoTuple SoftshrinkGrad_Op::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("x", "paddle::dialect::DenseTensorType", false, false, false, false), paddle::dialect::OpInputInfo("out_grad", "paddle::dialect::DenseTensorType", false, false, false, false) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = { paddle::dialect::OpAttributeInfo("threshold", "pir::FloatAttribute", "") };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("x_grad", "paddle::dialect::DenseTensorType", false, false) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("UnchangedInferMeta", {"x"}, "softshrink_grad", {"x", "out_grad", "threshold"}, {}, {}, {{"x_grad", "out_grad"}}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "softshrink_grad");
}

void SoftshrinkGrad_Op::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, pir::Value out_grad_, float threshold) {
  VLOG(4) << "Start build SoftshrinkGrad_Op";



  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_, out_grad_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_threshold = pir::FloatAttribute::get(pir::IrContext::Instance(), threshold);
  argument.AddAttribute("threshold", attr_threshold);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;
  paddle::dialect::DenseTensorType out_grad = out_grad_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)out_grad;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);
  paddle::dialect::IrTensor dense_x_grad;
  paddle::dialect::IrMetaTensor meta_x_grad(&dense_x_grad);

  phi::UnchangedInferMeta(meta_x, &meta_x_grad);

  std::vector<pir::Type> argument_outputs;
  pir::Type x_grad_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_x_grad.dtype()), dense_x_grad.dims(), dense_x_grad.layout(), dense_x_grad.lod(), dense_x_grad.offset());
  argument_outputs.push_back(x_grad_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void SoftshrinkGrad_Op::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, pir::Value out_grad_, pir::AttributeMap attributes) {
  VLOG(4) << "Start build SoftshrinkGrad_Op";


  IR_ENFORCE(
      attributes.find("threshold") != attributes.end(),
          "'threshold' Attribute is expected for SoftshrinkGrad_Op. ");
  float threshold = attributes.at("threshold").dyn_cast<pir::FloatAttribute>().data();


  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_, out_grad_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_threshold = pir::FloatAttribute::get(pir::IrContext::Instance(), threshold);
  argument.AddAttribute("threshold", attr_threshold);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;
  paddle::dialect::DenseTensorType out_grad = out_grad_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)out_grad;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);
  paddle::dialect::IrTensor dense_x_grad;
  paddle::dialect::IrMetaTensor meta_x_grad(&dense_x_grad);

  phi::UnchangedInferMeta(meta_x, &meta_x_grad);

  std::vector<pir::Type> argument_outputs;
  pir::Type x_grad_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_x_grad.dtype()), dense_x_grad.dims(), dense_x_grad.layout(), dense_x_grad.lod(), dense_x_grad.offset());
  argument_outputs.push_back(x_grad_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void SoftshrinkGrad_Op::VerifySig() {}

void SoftshrinkGrad_Op::InferMeta( phi::InferMetaContext *infer_meta ) {
  auto fn = PD_INFER_META(phi::UnchangedInferMeta);
  fn(infer_meta);
}

phi::DataType SoftshrinkGrad_Op::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: SoftshrinkGrad_Op";
  


  return expected_kernel_dtype;
}

OpInfoTuple SoftsignGradOp::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("x", "paddle::dialect::DenseTensorType", false, false, false, false), paddle::dialect::OpInputInfo("out_grad", "paddle::dialect::DenseTensorType", false, false, false, false) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = {  };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("x_grad", "paddle::dialect::DenseTensorType", false, false) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("UnchangedInferMeta", {"x"}, "softsign_grad", {"x", "out_grad"}, {}, {}, {}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "softsign_grad");
}

void SoftsignGradOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, pir::Value out_grad_) {
  VLOG(4) << "Start build SoftsignGradOp";



  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_, out_grad_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;
  paddle::dialect::DenseTensorType out_grad = out_grad_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)out_grad;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);
  paddle::dialect::IrTensor dense_x_grad;
  paddle::dialect::IrMetaTensor meta_x_grad(&dense_x_grad);

  phi::UnchangedInferMeta(meta_x, &meta_x_grad);

  std::vector<pir::Type> argument_outputs;
  pir::Type x_grad_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_x_grad.dtype()), dense_x_grad.dims(), dense_x_grad.layout(), dense_x_grad.lod(), dense_x_grad.offset());
  argument_outputs.push_back(x_grad_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void SoftsignGradOp::VerifySig() {}

void SoftsignGradOp::InferMeta( phi::InferMetaContext *infer_meta ) {
  auto fn = PD_INFER_META(phi::UnchangedInferMeta);
  fn(infer_meta);
}

phi::DataType SoftsignGradOp::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: SoftsignGradOp";
  


  return expected_kernel_dtype;
}

OpInfoTuple SoftsignGrad_Op::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("x", "paddle::dialect::DenseTensorType", false, false, false, false), paddle::dialect::OpInputInfo("out_grad", "paddle::dialect::DenseTensorType", false, false, false, false) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = {  };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("x_grad", "paddle::dialect::DenseTensorType", false, false) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("UnchangedInferMeta", {"x"}, "softsign_grad", {"x", "out_grad"}, {}, {}, {{"x_grad", "out_grad"}}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "softsign_grad");
}

void SoftsignGrad_Op::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, pir::Value out_grad_) {
  VLOG(4) << "Start build SoftsignGrad_Op";



  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_, out_grad_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;
  paddle::dialect::DenseTensorType out_grad = out_grad_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)out_grad;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);
  paddle::dialect::IrTensor dense_x_grad;
  paddle::dialect::IrMetaTensor meta_x_grad(&dense_x_grad);

  phi::UnchangedInferMeta(meta_x, &meta_x_grad);

  std::vector<pir::Type> argument_outputs;
  pir::Type x_grad_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_x_grad.dtype()), dense_x_grad.dims(), dense_x_grad.layout(), dense_x_grad.lod(), dense_x_grad.offset());
  argument_outputs.push_back(x_grad_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void SoftsignGrad_Op::VerifySig() {}

void SoftsignGrad_Op::InferMeta( phi::InferMetaContext *infer_meta ) {
  auto fn = PD_INFER_META(phi::UnchangedInferMeta);
  fn(infer_meta);
}

phi::DataType SoftsignGrad_Op::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: SoftsignGrad_Op";
  


  return expected_kernel_dtype;
}

OpInfoTuple SolveGradOp::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("x", "paddle::dialect::DenseTensorType", false, false, false, false), paddle::dialect::OpInputInfo("y", "paddle::dialect::DenseTensorType", false, false, false, false), paddle::dialect::OpInputInfo("out", "paddle::dialect::DenseTensorType", false, false, false, false), paddle::dialect::OpInputInfo("out_grad", "paddle::dialect::DenseTensorType", false, false, false, false) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = {  };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("x_grad", "paddle::dialect::DenseTensorType", false, false), paddle::dialect::OpOutputInfo("y_grad", "paddle::dialect::DenseTensorType", false, false) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("GeneralBinaryGradInferMeta", {"x", "y"}, "solve_grad", {"x", "y", "out", "out_grad"}, {}, {}, {}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "solve_grad");
}

void SolveGradOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, pir::Value y_, pir::Value out_, pir::Value out_grad_) {
  VLOG(4) << "Start build SolveGradOp";



  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_, y_, out_, out_grad_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;
  paddle::dialect::DenseTensorType y = y_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)y;
  paddle::dialect::DenseTensorType out = out_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)out;
  paddle::dialect::DenseTensorType out_grad = out_grad_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)out_grad;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);

  VLOG(4) << "Builder construction  dense_y";
  paddle::dialect::IrTensor ir_tensor_y(paddle::dialect::TransToPhiDataType(y.dtype()),
                                                      y.dims(),
                                                      y.data_layout(),
                                                      y.lod(),
                                                      y.offset());
  VLOG(4) << "Builder construction  meta_y";
  paddle::dialect::IrMetaTensor meta_y(&ir_tensor_y);
  paddle::dialect::IrTensor dense_x_grad;
  paddle::dialect::IrMetaTensor meta_x_grad(&dense_x_grad);
  paddle::dialect::IrTensor dense_y_grad;
  paddle::dialect::IrMetaTensor meta_y_grad(&dense_y_grad);

  phi::GeneralBinaryGradInferMeta(meta_x, meta_y, &meta_x_grad, &meta_y_grad);

  std::vector<pir::Type> argument_outputs;
  pir::Type x_grad_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_x_grad.dtype()), dense_x_grad.dims(), dense_x_grad.layout(), dense_x_grad.lod(), dense_x_grad.offset());
  argument_outputs.push_back(x_grad_dense_tensor_type);

  pir::Type y_grad_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_y_grad.dtype()), dense_y_grad.dims(), dense_y_grad.layout(), dense_y_grad.lod(), dense_y_grad.offset());
  argument_outputs.push_back(y_grad_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void SolveGradOp::VerifySig() {}

void SolveGradOp::InferMeta( phi::InferMetaContext *infer_meta ) {
  auto fn = PD_INFER_META(phi::GeneralBinaryGradInferMeta);
  fn(infer_meta);
}

phi::DataType SolveGradOp::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: SolveGradOp";
  


  return expected_kernel_dtype;
}

const char *SpectralNormGradOp::attributes_name[3] = { "dim", "power_iters", "eps" };

OpInfoTuple SpectralNormGradOp::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("weight", "paddle::dialect::DenseTensorType", false, false, false, false), paddle::dialect::OpInputInfo("u", "paddle::dialect::DenseTensorType", false, false, false, false), paddle::dialect::OpInputInfo("v", "paddle::dialect::DenseTensorType", false, false, false, false), paddle::dialect::OpInputInfo("out_grad", "paddle::dialect::DenseTensorType", false, false, false, false) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = { paddle::dialect::OpAttributeInfo("dim", "pir::Int32Attribute", ""), paddle::dialect::OpAttributeInfo("power_iters", "pir::Int32Attribute", ""), paddle::dialect::OpAttributeInfo("eps", "pir::FloatAttribute", "") };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("weight_grad", "paddle::dialect::DenseTensorType", false, false) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("SpectralNormGradInferMeta", {"weight", "u", "v", "out_grad", "dim", "power_iters", "eps"}, "spectral_norm_grad", {"weight", "u", "v", "out_grad", "dim", "power_iters", "eps"}, {"weight"}, {}, {}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "spectral_norm_grad");
}

void SpectralNormGradOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value weight_, pir::Value u_, pir::Value v_, pir::Value out_grad_, int dim, int power_iters, float eps) {
  VLOG(4) << "Start build SpectralNormGradOp";



  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {weight_, u_, v_, out_grad_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_dim = pir::Int32Attribute::get(pir::IrContext::Instance(), dim);
  argument.AddAttribute("dim", attr_dim);
  pir::Attribute attr_power_iters = pir::Int32Attribute::get(pir::IrContext::Instance(), power_iters);
  argument.AddAttribute("power_iters", attr_power_iters);
  pir::Attribute attr_eps = pir::FloatAttribute::get(pir::IrContext::Instance(), eps);
  argument.AddAttribute("eps", attr_eps);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType weight = weight_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)weight;
  paddle::dialect::DenseTensorType u = u_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)u;
  paddle::dialect::DenseTensorType v = v_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)v;
  paddle::dialect::DenseTensorType out_grad = out_grad_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)out_grad;

  VLOG(4) << "Builder construction  dense_weight";
  paddle::dialect::IrTensor ir_tensor_weight(paddle::dialect::TransToPhiDataType(weight.dtype()),
                                                      weight.dims(),
                                                      weight.data_layout(),
                                                      weight.lod(),
                                                      weight.offset());
  VLOG(4) << "Builder construction  meta_weight";
  paddle::dialect::IrMetaTensor meta_weight(&ir_tensor_weight);

  VLOG(4) << "Builder construction  dense_u";
  paddle::dialect::IrTensor ir_tensor_u(paddle::dialect::TransToPhiDataType(u.dtype()),
                                                      u.dims(),
                                                      u.data_layout(),
                                                      u.lod(),
                                                      u.offset());
  VLOG(4) << "Builder construction  meta_u";
  paddle::dialect::IrMetaTensor meta_u(&ir_tensor_u);

  VLOG(4) << "Builder construction  dense_v";
  paddle::dialect::IrTensor ir_tensor_v(paddle::dialect::TransToPhiDataType(v.dtype()),
                                                      v.dims(),
                                                      v.data_layout(),
                                                      v.lod(),
                                                      v.offset());
  VLOG(4) << "Builder construction  meta_v";
  paddle::dialect::IrMetaTensor meta_v(&ir_tensor_v);

  VLOG(4) << "Builder construction  dense_out_grad";
  paddle::dialect::IrTensor ir_tensor_out_grad(paddle::dialect::TransToPhiDataType(out_grad.dtype()),
                                                      out_grad.dims(),
                                                      out_grad.data_layout(),
                                                      out_grad.lod(),
                                                      out_grad.offset());
  VLOG(4) << "Builder construction  meta_out_grad";
  paddle::dialect::IrMetaTensor meta_out_grad(&ir_tensor_out_grad);
  paddle::dialect::IrTensor dense_weight_grad;
  paddle::dialect::IrMetaTensor meta_weight_grad(&dense_weight_grad);

  phi::SpectralNormGradInferMeta(meta_weight, meta_u, meta_v, meta_out_grad, dim, power_iters, eps, &meta_weight_grad);

  std::vector<pir::Type> argument_outputs;
  pir::Type weight_grad_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_weight_grad.dtype()), dense_weight_grad.dims(), dense_weight_grad.layout(), dense_weight_grad.lod(), dense_weight_grad.offset());
  argument_outputs.push_back(weight_grad_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void SpectralNormGradOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value weight_, pir::Value u_, pir::Value v_, pir::Value out_grad_, pir::AttributeMap attributes) {
  VLOG(4) << "Start build SpectralNormGradOp";


  IR_ENFORCE(
      attributes.find("dim") != attributes.end(),
          "'dim' Attribute is expected for SpectralNormGradOp. ");
  int dim = attributes.at("dim").dyn_cast<pir::Int32Attribute>().data();

  IR_ENFORCE(
      attributes.find("power_iters") != attributes.end(),
          "'power_iters' Attribute is expected for SpectralNormGradOp. ");
  int power_iters = attributes.at("power_iters").dyn_cast<pir::Int32Attribute>().data();

  IR_ENFORCE(
      attributes.find("eps") != attributes.end(),
          "'eps' Attribute is expected for SpectralNormGradOp. ");
  float eps = attributes.at("eps").dyn_cast<pir::FloatAttribute>().data();


  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {weight_, u_, v_, out_grad_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_dim = pir::Int32Attribute::get(pir::IrContext::Instance(), dim);
  argument.AddAttribute("dim", attr_dim);
  pir::Attribute attr_power_iters = pir::Int32Attribute::get(pir::IrContext::Instance(), power_iters);
  argument.AddAttribute("power_iters", attr_power_iters);
  pir::Attribute attr_eps = pir::FloatAttribute::get(pir::IrContext::Instance(), eps);
  argument.AddAttribute("eps", attr_eps);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType weight = weight_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)weight;
  paddle::dialect::DenseTensorType u = u_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)u;
  paddle::dialect::DenseTensorType v = v_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)v;
  paddle::dialect::DenseTensorType out_grad = out_grad_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)out_grad;

  VLOG(4) << "Builder construction  dense_weight";
  paddle::dialect::IrTensor ir_tensor_weight(paddle::dialect::TransToPhiDataType(weight.dtype()),
                                                      weight.dims(),
                                                      weight.data_layout(),
                                                      weight.lod(),
                                                      weight.offset());
  VLOG(4) << "Builder construction  meta_weight";
  paddle::dialect::IrMetaTensor meta_weight(&ir_tensor_weight);

  VLOG(4) << "Builder construction  dense_u";
  paddle::dialect::IrTensor ir_tensor_u(paddle::dialect::TransToPhiDataType(u.dtype()),
                                                      u.dims(),
                                                      u.data_layout(),
                                                      u.lod(),
                                                      u.offset());
  VLOG(4) << "Builder construction  meta_u";
  paddle::dialect::IrMetaTensor meta_u(&ir_tensor_u);

  VLOG(4) << "Builder construction  dense_v";
  paddle::dialect::IrTensor ir_tensor_v(paddle::dialect::TransToPhiDataType(v.dtype()),
                                                      v.dims(),
                                                      v.data_layout(),
                                                      v.lod(),
                                                      v.offset());
  VLOG(4) << "Builder construction  meta_v";
  paddle::dialect::IrMetaTensor meta_v(&ir_tensor_v);

  VLOG(4) << "Builder construction  dense_out_grad";
  paddle::dialect::IrTensor ir_tensor_out_grad(paddle::dialect::TransToPhiDataType(out_grad.dtype()),
                                                      out_grad.dims(),
                                                      out_grad.data_layout(),
                                                      out_grad.lod(),
                                                      out_grad.offset());
  VLOG(4) << "Builder construction  meta_out_grad";
  paddle::dialect::IrMetaTensor meta_out_grad(&ir_tensor_out_grad);
  paddle::dialect::IrTensor dense_weight_grad;
  paddle::dialect::IrMetaTensor meta_weight_grad(&dense_weight_grad);

  phi::SpectralNormGradInferMeta(meta_weight, meta_u, meta_v, meta_out_grad, dim, power_iters, eps, &meta_weight_grad);

  std::vector<pir::Type> argument_outputs;
  pir::Type weight_grad_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_weight_grad.dtype()), dense_weight_grad.dims(), dense_weight_grad.layout(), dense_weight_grad.lod(), dense_weight_grad.offset());
  argument_outputs.push_back(weight_grad_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void SpectralNormGradOp::VerifySig() {}

void SpectralNormGradOp::InferMeta( phi::InferMetaContext *infer_meta ) {
  auto fn = PD_INFER_META(phi::SpectralNormGradInferMeta);
  fn(infer_meta);
}

phi::DataType SpectralNormGradOp::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: SpectralNormGradOp";
  


  return expected_kernel_dtype;
}

OpInfoTuple SqrtDoubleGradOp::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("out", "paddle::dialect::DenseTensorType", false, false, false, false), paddle::dialect::OpInputInfo("grad_x", "paddle::dialect::DenseTensorType", false, false, false, false), paddle::dialect::OpInputInfo("grad_x_grad", "paddle::dialect::DenseTensorType", false, false, false, false) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = {  };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("out_grad", "paddle::dialect::DenseTensorType", false, false), paddle::dialect::OpOutputInfo("grad_out_grad", "paddle::dialect::DenseTensorType", false, false) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("GeneralBinaryGradInferMeta", {"out", "out"}, "sqrt_double_grad", {"out", "grad_x", "grad_x_grad"}, {}, {}, {}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "sqrt_double_grad");
}

void SqrtDoubleGradOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value out_, pir::Value grad_x_, pir::Value grad_x_grad_) {
  VLOG(4) << "Start build SqrtDoubleGradOp";



  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {out_, grad_x_, grad_x_grad_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType out = out_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)out;
  paddle::dialect::DenseTensorType grad_x = grad_x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)grad_x;
  paddle::dialect::DenseTensorType grad_x_grad = grad_x_grad_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)grad_x_grad;

  VLOG(4) << "Builder construction  dense_out";
  paddle::dialect::IrTensor ir_tensor_out(paddle::dialect::TransToPhiDataType(out.dtype()),
                                                      out.dims(),
                                                      out.data_layout(),
                                                      out.lod(),
                                                      out.offset());
  VLOG(4) << "Builder construction  meta_out";
  paddle::dialect::IrMetaTensor meta_out(&ir_tensor_out);
  paddle::dialect::IrTensor dense_out_grad;
  paddle::dialect::IrMetaTensor meta_out_grad(&dense_out_grad);
  paddle::dialect::IrTensor dense_grad_out_grad;
  paddle::dialect::IrMetaTensor meta_grad_out_grad(&dense_grad_out_grad);

  phi::GeneralBinaryGradInferMeta(meta_out, meta_out, &meta_out_grad, &meta_grad_out_grad);

  std::vector<pir::Type> argument_outputs;
  pir::Type out_grad_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_out_grad.dtype()), dense_out_grad.dims(), dense_out_grad.layout(), dense_out_grad.lod(), dense_out_grad.offset());
  argument_outputs.push_back(out_grad_dense_tensor_type);

  pir::Type grad_out_grad_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_grad_out_grad.dtype()), dense_grad_out_grad.dims(), dense_grad_out_grad.layout(), dense_grad_out_grad.lod(), dense_grad_out_grad.offset());
  argument_outputs.push_back(grad_out_grad_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void SqrtDoubleGradOp::VerifySig() {}

void SqrtDoubleGradOp::InferMeta( phi::InferMetaContext *infer_meta ) {
  auto fn = PD_INFER_META(phi::GeneralBinaryGradInferMeta);
  fn(infer_meta);
}

phi::DataType SqrtDoubleGradOp::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: SqrtDoubleGradOp";
  


  return expected_kernel_dtype;
}

OpInfoTuple SqrtDoubleGrad_Op::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("out", "paddle::dialect::DenseTensorType", false, false, false, false), paddle::dialect::OpInputInfo("grad_x", "paddle::dialect::DenseTensorType", false, false, false, false), paddle::dialect::OpInputInfo("grad_x_grad", "paddle::dialect::DenseTensorType", false, false, false, false) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = {  };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("out_grad", "paddle::dialect::DenseTensorType", false, false), paddle::dialect::OpOutputInfo("grad_out_grad", "paddle::dialect::DenseTensorType", false, false) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("GeneralBinaryGradInferMeta", {"out", "out"}, "sqrt_double_grad", {"out", "grad_x", "grad_x_grad"}, {}, {}, {{"grad_out_grad", "grad_x_grad"}}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "sqrt_double_grad");
}

void SqrtDoubleGrad_Op::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value out_, pir::Value grad_x_, pir::Value grad_x_grad_) {
  VLOG(4) << "Start build SqrtDoubleGrad_Op";



  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {out_, grad_x_, grad_x_grad_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType out = out_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)out;
  paddle::dialect::DenseTensorType grad_x = grad_x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)grad_x;
  paddle::dialect::DenseTensorType grad_x_grad = grad_x_grad_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)grad_x_grad;

  VLOG(4) << "Builder construction  dense_out";
  paddle::dialect::IrTensor ir_tensor_out(paddle::dialect::TransToPhiDataType(out.dtype()),
                                                      out.dims(),
                                                      out.data_layout(),
                                                      out.lod(),
                                                      out.offset());
  VLOG(4) << "Builder construction  meta_out";
  paddle::dialect::IrMetaTensor meta_out(&ir_tensor_out);
  paddle::dialect::IrTensor dense_out_grad;
  paddle::dialect::IrMetaTensor meta_out_grad(&dense_out_grad);
  paddle::dialect::IrTensor dense_grad_out_grad;
  paddle::dialect::IrMetaTensor meta_grad_out_grad(&dense_grad_out_grad);

  phi::GeneralBinaryGradInferMeta(meta_out, meta_out, &meta_out_grad, &meta_grad_out_grad);

  std::vector<pir::Type> argument_outputs;
  pir::Type out_grad_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_out_grad.dtype()), dense_out_grad.dims(), dense_out_grad.layout(), dense_out_grad.lod(), dense_out_grad.offset());
  argument_outputs.push_back(out_grad_dense_tensor_type);

  pir::Type grad_out_grad_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_grad_out_grad.dtype()), dense_grad_out_grad.dims(), dense_grad_out_grad.layout(), dense_grad_out_grad.lod(), dense_grad_out_grad.offset());
  argument_outputs.push_back(grad_out_grad_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void SqrtDoubleGrad_Op::VerifySig() {}

void SqrtDoubleGrad_Op::InferMeta( phi::InferMetaContext *infer_meta ) {
  auto fn = PD_INFER_META(phi::GeneralBinaryGradInferMeta);
  fn(infer_meta);
}

phi::DataType SqrtDoubleGrad_Op::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: SqrtDoubleGrad_Op";
  


  return expected_kernel_dtype;
}

OpInfoTuple SqrtGradOp::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("out", "paddle::dialect::DenseTensorType", false, false, false, true), paddle::dialect::OpInputInfo("out_grad", "paddle::dialect::DenseTensorType", false, false, false, true) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = {  };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("x_grad", "paddle::dialect::DenseTensorType", false, false) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("UnchangedInferMeta", {"out"}, "sqrt_grad", {"out", "out_grad"}, {}, {}, {}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "sqrt_grad");
}

void SqrtGradOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value out_, pir::Value out_grad_) {
  VLOG(4) << "Start build SqrtGradOp";



  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {out_, out_grad_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType out = out_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)out;
  paddle::dialect::DenseTensorType out_grad = out_grad_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)out_grad;

  VLOG(4) << "Builder construction  dense_out";
  paddle::dialect::IrTensor ir_tensor_out(paddle::dialect::TransToPhiDataType(out.dtype()),
                                                      out.dims(),
                                                      out.data_layout(),
                                                      out.lod(),
                                                      out.offset());
  VLOG(4) << "Builder construction  meta_out";
  paddle::dialect::IrMetaTensor meta_out(&ir_tensor_out);
  paddle::dialect::IrTensor dense_x_grad;
  paddle::dialect::IrMetaTensor meta_x_grad(&dense_x_grad);

  phi::UnchangedInferMeta(meta_out, &meta_x_grad);

  std::vector<pir::Type> argument_outputs;
  pir::Type x_grad_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_x_grad.dtype()), dense_x_grad.dims(), dense_x_grad.layout(), dense_x_grad.lod(), dense_x_grad.offset());
  argument_outputs.push_back(x_grad_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void SqrtGradOp::VerifySig() {}

void SqrtGradOp::InferMeta( phi::InferMetaContext *infer_meta ) {
  auto fn = PD_INFER_META(phi::UnchangedInferMeta);
  fn(infer_meta);
}

phi::DataType SqrtGradOp::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: SqrtGradOp";
  


  return expected_kernel_dtype;
}

OpInfoTuple SqrtGrad_Op::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("out", "paddle::dialect::DenseTensorType", false, false, false, true), paddle::dialect::OpInputInfo("out_grad", "paddle::dialect::DenseTensorType", false, false, false, true) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = {  };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("x_grad", "paddle::dialect::DenseTensorType", false, false) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("UnchangedInferMeta", {"out"}, "sqrt_grad", {"out", "out_grad"}, {}, {}, {{"x_grad", "out_grad"}}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "sqrt_grad");
}

void SqrtGrad_Op::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value out_, pir::Value out_grad_) {
  VLOG(4) << "Start build SqrtGrad_Op";



  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {out_, out_grad_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType out = out_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)out;
  paddle::dialect::DenseTensorType out_grad = out_grad_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)out_grad;

  VLOG(4) << "Builder construction  dense_out";
  paddle::dialect::IrTensor ir_tensor_out(paddle::dialect::TransToPhiDataType(out.dtype()),
                                                      out.dims(),
                                                      out.data_layout(),
                                                      out.lod(),
                                                      out.offset());
  VLOG(4) << "Builder construction  meta_out";
  paddle::dialect::IrMetaTensor meta_out(&ir_tensor_out);
  paddle::dialect::IrTensor dense_x_grad;
  paddle::dialect::IrMetaTensor meta_x_grad(&dense_x_grad);

  phi::UnchangedInferMeta(meta_out, &meta_x_grad);

  std::vector<pir::Type> argument_outputs;
  pir::Type x_grad_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_x_grad.dtype()), dense_x_grad.dims(), dense_x_grad.layout(), dense_x_grad.lod(), dense_x_grad.offset());
  argument_outputs.push_back(x_grad_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void SqrtGrad_Op::VerifySig() {}

void SqrtGrad_Op::InferMeta( phi::InferMetaContext *infer_meta ) {
  auto fn = PD_INFER_META(phi::UnchangedInferMeta);
  fn(infer_meta);
}

phi::DataType SqrtGrad_Op::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: SqrtGrad_Op";
  


  return expected_kernel_dtype;
}

OpInfoTuple SquareDoubleGradOp::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("x", "paddle::dialect::DenseTensorType", false, false, false, false), paddle::dialect::OpInputInfo("grad_out", "paddle::dialect::DenseTensorType", false, false, false, false), paddle::dialect::OpInputInfo("grad_x_grad", "paddle::dialect::DenseTensorType", false, false, false, false) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = {  };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("x_grad", "paddle::dialect::DenseTensorType", false, false), paddle::dialect::OpOutputInfo("grad_out_grad", "paddle::dialect::DenseTensorType", false, false) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("GeneralBinaryGradInferMeta", {"x", "x"}, "square_double_grad", {"x", "grad_out", "grad_x_grad"}, {}, {}, {}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "square_double_grad");
}

void SquareDoubleGradOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, pir::Value grad_out_, pir::Value grad_x_grad_) {
  VLOG(4) << "Start build SquareDoubleGradOp";



  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_, grad_out_, grad_x_grad_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;
  paddle::dialect::DenseTensorType grad_out = grad_out_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)grad_out;
  paddle::dialect::DenseTensorType grad_x_grad = grad_x_grad_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)grad_x_grad;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);
  paddle::dialect::IrTensor dense_x_grad;
  paddle::dialect::IrMetaTensor meta_x_grad(&dense_x_grad);
  paddle::dialect::IrTensor dense_grad_out_grad;
  paddle::dialect::IrMetaTensor meta_grad_out_grad(&dense_grad_out_grad);

  phi::GeneralBinaryGradInferMeta(meta_x, meta_x, &meta_x_grad, &meta_grad_out_grad);

  std::vector<pir::Type> argument_outputs;
  pir::Type x_grad_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_x_grad.dtype()), dense_x_grad.dims(), dense_x_grad.layout(), dense_x_grad.lod(), dense_x_grad.offset());
  argument_outputs.push_back(x_grad_dense_tensor_type);

  pir::Type grad_out_grad_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_grad_out_grad.dtype()), dense_grad_out_grad.dims(), dense_grad_out_grad.layout(), dense_grad_out_grad.lod(), dense_grad_out_grad.offset());
  argument_outputs.push_back(grad_out_grad_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void SquareDoubleGradOp::VerifySig() {}

void SquareDoubleGradOp::InferMeta( phi::InferMetaContext *infer_meta ) {
  auto fn = PD_INFER_META(phi::GeneralBinaryGradInferMeta);
  fn(infer_meta);
}

phi::DataType SquareDoubleGradOp::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: SquareDoubleGradOp";
  


  return expected_kernel_dtype;
}

OpInfoTuple SquareDoubleGrad_Op::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("x", "paddle::dialect::DenseTensorType", false, false, false, false), paddle::dialect::OpInputInfo("grad_out", "paddle::dialect::DenseTensorType", false, false, false, false), paddle::dialect::OpInputInfo("grad_x_grad", "paddle::dialect::DenseTensorType", false, false, false, false) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = {  };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("x_grad", "paddle::dialect::DenseTensorType", false, false), paddle::dialect::OpOutputInfo("grad_out_grad", "paddle::dialect::DenseTensorType", false, false) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("GeneralBinaryGradInferMeta", {"x", "x"}, "square_double_grad", {"x", "grad_out", "grad_x_grad"}, {}, {}, {{"grad_out_grad", "grad_x_grad"}}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "square_double_grad");
}

void SquareDoubleGrad_Op::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, pir::Value grad_out_, pir::Value grad_x_grad_) {
  VLOG(4) << "Start build SquareDoubleGrad_Op";



  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_, grad_out_, grad_x_grad_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;
  paddle::dialect::DenseTensorType grad_out = grad_out_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)grad_out;
  paddle::dialect::DenseTensorType grad_x_grad = grad_x_grad_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)grad_x_grad;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);
  paddle::dialect::IrTensor dense_x_grad;
  paddle::dialect::IrMetaTensor meta_x_grad(&dense_x_grad);
  paddle::dialect::IrTensor dense_grad_out_grad;
  paddle::dialect::IrMetaTensor meta_grad_out_grad(&dense_grad_out_grad);

  phi::GeneralBinaryGradInferMeta(meta_x, meta_x, &meta_x_grad, &meta_grad_out_grad);

  std::vector<pir::Type> argument_outputs;
  pir::Type x_grad_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_x_grad.dtype()), dense_x_grad.dims(), dense_x_grad.layout(), dense_x_grad.lod(), dense_x_grad.offset());
  argument_outputs.push_back(x_grad_dense_tensor_type);

  pir::Type grad_out_grad_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_grad_out_grad.dtype()), dense_grad_out_grad.dims(), dense_grad_out_grad.layout(), dense_grad_out_grad.lod(), dense_grad_out_grad.offset());
  argument_outputs.push_back(grad_out_grad_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void SquareDoubleGrad_Op::VerifySig() {}

void SquareDoubleGrad_Op::InferMeta( phi::InferMetaContext *infer_meta ) {
  auto fn = PD_INFER_META(phi::GeneralBinaryGradInferMeta);
  fn(infer_meta);
}

phi::DataType SquareDoubleGrad_Op::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: SquareDoubleGrad_Op";
  


  return expected_kernel_dtype;
}

OpInfoTuple SquareGradOp::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("x", "paddle::dialect::DenseTensorType", false, false, false, true), paddle::dialect::OpInputInfo("out_grad", "paddle::dialect::DenseTensorType", false, false, false, true) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = {  };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("x_grad", "paddle::dialect::DenseTensorType", false, false) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("UnchangedInferMeta", {"x"}, "square_grad", {"x", "out_grad"}, {}, {}, {}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "square_grad");
}

void SquareGradOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, pir::Value out_grad_) {
  VLOG(4) << "Start build SquareGradOp";



  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_, out_grad_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;
  paddle::dialect::DenseTensorType out_grad = out_grad_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)out_grad;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);
  paddle::dialect::IrTensor dense_x_grad;
  paddle::dialect::IrMetaTensor meta_x_grad(&dense_x_grad);

  phi::UnchangedInferMeta(meta_x, &meta_x_grad);

  std::vector<pir::Type> argument_outputs;
  pir::Type x_grad_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_x_grad.dtype()), dense_x_grad.dims(), dense_x_grad.layout(), dense_x_grad.lod(), dense_x_grad.offset());
  argument_outputs.push_back(x_grad_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void SquareGradOp::VerifySig() {}

void SquareGradOp::InferMeta( phi::InferMetaContext *infer_meta ) {
  auto fn = PD_INFER_META(phi::UnchangedInferMeta);
  fn(infer_meta);
}

phi::DataType SquareGradOp::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: SquareGradOp";
  


  return expected_kernel_dtype;
}

OpInfoTuple SquareGrad_Op::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("x", "paddle::dialect::DenseTensorType", false, false, false, true), paddle::dialect::OpInputInfo("out_grad", "paddle::dialect::DenseTensorType", false, false, false, true) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = {  };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("x_grad", "paddle::dialect::DenseTensorType", false, false) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("UnchangedInferMeta", {"x"}, "square_grad", {"x", "out_grad"}, {}, {}, {{"x_grad", "out_grad"}}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "square_grad");
}

void SquareGrad_Op::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, pir::Value out_grad_) {
  VLOG(4) << "Start build SquareGrad_Op";



  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_, out_grad_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;
  paddle::dialect::DenseTensorType out_grad = out_grad_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)out_grad;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);
  paddle::dialect::IrTensor dense_x_grad;
  paddle::dialect::IrMetaTensor meta_x_grad(&dense_x_grad);

  phi::UnchangedInferMeta(meta_x, &meta_x_grad);

  std::vector<pir::Type> argument_outputs;
  pir::Type x_grad_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_x_grad.dtype()), dense_x_grad.dims(), dense_x_grad.layout(), dense_x_grad.lod(), dense_x_grad.offset());
  argument_outputs.push_back(x_grad_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void SquareGrad_Op::VerifySig() {}

void SquareGrad_Op::InferMeta( phi::InferMetaContext *infer_meta ) {
  auto fn = PD_INFER_META(phi::UnchangedInferMeta);
  fn(infer_meta);
}

phi::DataType SquareGrad_Op::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: SquareGrad_Op";
  


  return expected_kernel_dtype;
}

OpInfoTuple SquaredL2NormGradOp::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("x", "paddle::dialect::DenseTensorType", false, false, false, false), paddle::dialect::OpInputInfo("out_grad", "paddle::dialect::DenseTensorType", false, false, false, false) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = {  };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("x_grad", "paddle::dialect::DenseTensorType", false, false) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("UnchangedInferMeta", {"x"}, "squared_l2_norm_grad", {"x", "out_grad"}, {}, {}, {}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "squared_l2_norm_grad");
}

void SquaredL2NormGradOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, pir::Value out_grad_) {
  VLOG(4) << "Start build SquaredL2NormGradOp";



  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_, out_grad_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;
  paddle::dialect::DenseTensorType out_grad = out_grad_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)out_grad;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);
  paddle::dialect::IrTensor dense_x_grad;
  paddle::dialect::IrMetaTensor meta_x_grad(&dense_x_grad);

  phi::UnchangedInferMeta(meta_x, &meta_x_grad);

  std::vector<pir::Type> argument_outputs;
  pir::Type x_grad_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_x_grad.dtype()), dense_x_grad.dims(), dense_x_grad.layout(), dense_x_grad.lod(), dense_x_grad.offset());
  argument_outputs.push_back(x_grad_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void SquaredL2NormGradOp::VerifySig() {}

void SquaredL2NormGradOp::InferMeta( phi::InferMetaContext *infer_meta ) {
  auto fn = PD_INFER_META(phi::UnchangedInferMeta);
  fn(infer_meta);
}

phi::DataType SquaredL2NormGradOp::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: SquaredL2NormGradOp";
  


  return expected_kernel_dtype;
}

OpInfoTuple SqueezeDoubleGradOp::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("grad_x_grad", "paddle::dialect::DenseTensorType", false, false, false, false), paddle::dialect::OpInputInfo("axis", "paddle::dialect::IntArrayAttribute", false, false, true, false) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = {  };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("grad_out_grad", "paddle::dialect::DenseTensorType", false, false), paddle::dialect::OpOutputInfo("xshape", "paddle::dialect::DenseTensorType", true, true) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("", {""}, "", {""}, {}, {}, {}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "squeeze_double_grad");
}

void SqueezeDoubleGradOp::VerifySig() {}

phi::DataType SqueezeDoubleGradOp::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: SqueezeDoubleGradOp";
  


  return expected_kernel_dtype;
}

OpInfoTuple SqueezeGradOp::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("xshape", "paddle::dialect::DenseTensorType", false, false, false, false), paddle::dialect::OpInputInfo("out_grad", "paddle::dialect::DenseTensorType", false, false, false, true), paddle::dialect::OpInputInfo("axis", "paddle::dialect::IntArrayAttribute", false, false, true, false) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = {  };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("x_grad", "paddle::dialect::DenseTensorType", false, false) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("KernelWithXShapeInferMeta", {"xshape", "out_grad"}, "squeeze_grad", {"xshape", "out_grad", "axis"}, {"out_grad"}, {}, {}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "squeeze_grad");
}

void SqueezeGradOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value xshape_, pir::Value out_grad_, const std::vector<int64_t>& axis) {
  VLOG(4) << "Start build SqueezeGradOp";


  // Generate int_array mutable attribute: axis
  paddle::dialect::FullIntArrayOp full_axis_op = builder.Build<paddle::dialect::FullIntArrayOp>(axis, phi::DataType::INT64, phi::CPUPlace());
  pir::OpResult axis_ = full_axis_op->result(0);
    
  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {xshape_, out_grad_, axis_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType xshape = xshape_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)xshape;
  paddle::dialect::DenseTensorType out_grad = out_grad_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)out_grad;

  VLOG(4) << "Builder construction  dense_xshape";
  paddle::dialect::IrTensor ir_tensor_xshape(paddle::dialect::TransToPhiDataType(xshape.dtype()),
                                                      xshape.dims(),
                                                      xshape.data_layout(),
                                                      xshape.lod(),
                                                      xshape.offset());
  VLOG(4) << "Builder construction  meta_xshape";
  paddle::dialect::IrMetaTensor meta_xshape(&ir_tensor_xshape);

  VLOG(4) << "Builder construction  dense_out_grad";
  paddle::dialect::IrTensor ir_tensor_out_grad(paddle::dialect::TransToPhiDataType(out_grad.dtype()),
                                                      out_grad.dims(),
                                                      out_grad.data_layout(),
                                                      out_grad.lod(),
                                                      out_grad.offset());
  VLOG(4) << "Builder construction  meta_out_grad";
  paddle::dialect::IrMetaTensor meta_out_grad(&ir_tensor_out_grad);
  paddle::dialect::IrTensor dense_x_grad;
  paddle::dialect::IrMetaTensor meta_x_grad(&dense_x_grad);

  phi::KernelWithXShapeInferMeta(meta_xshape, meta_out_grad, &meta_x_grad);

  std::vector<pir::Type> argument_outputs;
  pir::Type x_grad_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_x_grad.dtype()), dense_x_grad.dims(), dense_x_grad.layout(), dense_x_grad.lod(), dense_x_grad.offset());
  argument_outputs.push_back(x_grad_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void SqueezeGradOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value xshape_, pir::Value out_grad_, pir::AttributeMap attributes) {
  VLOG(4) << "Start build SqueezeGradOp";


  IR_ENFORCE(
      attributes.find("axis") != attributes.end(),
          "'axis' Attribute is expected for SqueezeGradOp. ");
  std::vector<int64_t> axis = attributes.at("axis").dyn_cast<paddle::dialect::IntArrayAttribute>().data().GetData();

  // Generate int_array mutable attribute: axis
  paddle::dialect::FullIntArrayOp full_axis_op = builder.Build<paddle::dialect::FullIntArrayOp>(axis, phi::DataType::INT64, phi::CPUPlace());
  pir::OpResult axis_ = full_axis_op->result(0);
    
  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {xshape_, out_grad_, axis_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType xshape = xshape_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)xshape;
  paddle::dialect::DenseTensorType out_grad = out_grad_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)out_grad;

  VLOG(4) << "Builder construction  dense_xshape";
  paddle::dialect::IrTensor ir_tensor_xshape(paddle::dialect::TransToPhiDataType(xshape.dtype()),
                                                      xshape.dims(),
                                                      xshape.data_layout(),
                                                      xshape.lod(),
                                                      xshape.offset());
  VLOG(4) << "Builder construction  meta_xshape";
  paddle::dialect::IrMetaTensor meta_xshape(&ir_tensor_xshape);

  VLOG(4) << "Builder construction  dense_out_grad";
  paddle::dialect::IrTensor ir_tensor_out_grad(paddle::dialect::TransToPhiDataType(out_grad.dtype()),
                                                      out_grad.dims(),
                                                      out_grad.data_layout(),
                                                      out_grad.lod(),
                                                      out_grad.offset());
  VLOG(4) << "Builder construction  meta_out_grad";
  paddle::dialect::IrMetaTensor meta_out_grad(&ir_tensor_out_grad);
  paddle::dialect::IrTensor dense_x_grad;
  paddle::dialect::IrMetaTensor meta_x_grad(&dense_x_grad);

  phi::KernelWithXShapeInferMeta(meta_xshape, meta_out_grad, &meta_x_grad);

  std::vector<pir::Type> argument_outputs;
  pir::Type x_grad_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_x_grad.dtype()), dense_x_grad.dims(), dense_x_grad.layout(), dense_x_grad.lod(), dense_x_grad.offset());
  argument_outputs.push_back(x_grad_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void SqueezeGradOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value xshape_, pir::Value out_grad_, pir::Value axis_) {
  VLOG(4) << "Start build SqueezeGradOp";



  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {xshape_, out_grad_, axis_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType xshape = xshape_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)xshape;
  paddle::dialect::DenseTensorType out_grad = out_grad_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)out_grad;
  phi::IntArray axis;
  if (axis_.dyn_cast<pir::OpResult>() && axis_.dyn_cast<pir::OpResult>().owner()->isa<paddle::dialect::FullIntArrayOp>()) {
    axis = std::move(phi::IntArray(paddle::dialect::GetInt64Vector(
                          axis_.dyn_cast<pir::OpResult>().owner()
                          ->dyn_cast<paddle::dialect::FullIntArrayOp>()
                          .attribute("value"))));
  } else if (axis_.type().isa<pir::VectorType>()) {
    size_t axis_size = axis_.type().dyn_cast<pir::VectorType>().size();
    axis = std::move(phi::IntArray(std::vector<int64_t>(axis_size, -1)));
    axis.SetFromTensor(true);
  } else if (axis_.type().isa<paddle::dialect::DenseTensorType>()) {
    common::DDim axis_dim = axis_.type().dyn_cast<paddle::dialect::DenseTensorType>().dims();
    size_t axis_size = common::product(axis_dim);
    if (common::contain_unknown_dim(axis_dim)) {
      axis_size = 1;
    }
    axis = std::move(phi::IntArray(std::vector<int64_t>(axis_size, -1)));
    axis.SetFromTensor(true);
  } else {
    PADDLE_THROW(phi::errors::Unimplemented("Only support VectorType or DenseTensorType"));
  }


  VLOG(4) << "Builder construction  dense_xshape";
  paddle::dialect::IrTensor ir_tensor_xshape(paddle::dialect::TransToPhiDataType(xshape.dtype()),
                                                      xshape.dims(),
                                                      xshape.data_layout(),
                                                      xshape.lod(),
                                                      xshape.offset());
  VLOG(4) << "Builder construction  meta_xshape";
  paddle::dialect::IrMetaTensor meta_xshape(&ir_tensor_xshape);

  VLOG(4) << "Builder construction  dense_out_grad";
  paddle::dialect::IrTensor ir_tensor_out_grad(paddle::dialect::TransToPhiDataType(out_grad.dtype()),
                                                      out_grad.dims(),
                                                      out_grad.data_layout(),
                                                      out_grad.lod(),
                                                      out_grad.offset());
  VLOG(4) << "Builder construction  meta_out_grad";
  paddle::dialect::IrMetaTensor meta_out_grad(&ir_tensor_out_grad);
  paddle::dialect::IrTensor dense_x_grad;
  paddle::dialect::IrMetaTensor meta_x_grad(&dense_x_grad);

  phi::KernelWithXShapeInferMeta(meta_xshape, meta_out_grad, &meta_x_grad);

  std::vector<pir::Type> argument_outputs;
  pir::Type x_grad_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_x_grad.dtype()), dense_x_grad.dims(), dense_x_grad.layout(), dense_x_grad.lod(), dense_x_grad.offset());
  argument_outputs.push_back(x_grad_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void SqueezeGradOp::VerifySig() {}

void SqueezeGradOp::InferMeta( phi::InferMetaContext *infer_meta ) {
  auto fn = PD_INFER_META(phi::KernelWithXShapeInferMeta);
  fn(infer_meta);
}

phi::DataType SqueezeGradOp::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: SqueezeGradOp";
  


  return expected_kernel_dtype;
}

OpInfoTuple SqueezeGrad_Op::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("xshape", "paddle::dialect::DenseTensorType", false, false, false, false), paddle::dialect::OpInputInfo("out_grad", "paddle::dialect::DenseTensorType", false, false, false, true), paddle::dialect::OpInputInfo("axis", "paddle::dialect::IntArrayAttribute", false, false, true, false) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = {  };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("x_grad", "paddle::dialect::DenseTensorType", false, false) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("KernelWithXShapeInferMeta", {"xshape", "out_grad"}, "squeeze_grad", {"xshape", "out_grad", "axis"}, {"out_grad"}, {}, {{"x_grad", "out_grad"}}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "squeeze_grad");
}

void SqueezeGrad_Op::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value xshape_, pir::Value out_grad_, const std::vector<int64_t>& axis) {
  VLOG(4) << "Start build SqueezeGrad_Op";


  // Generate int_array mutable attribute: axis
  paddle::dialect::FullIntArrayOp full_axis_op = builder.Build<paddle::dialect::FullIntArrayOp>(axis, phi::DataType::INT64, phi::CPUPlace());
  pir::OpResult axis_ = full_axis_op->result(0);
    
  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {xshape_, out_grad_, axis_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType xshape = xshape_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)xshape;
  paddle::dialect::DenseTensorType out_grad = out_grad_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)out_grad;

  VLOG(4) << "Builder construction  dense_xshape";
  paddle::dialect::IrTensor ir_tensor_xshape(paddle::dialect::TransToPhiDataType(xshape.dtype()),
                                                      xshape.dims(),
                                                      xshape.data_layout(),
                                                      xshape.lod(),
                                                      xshape.offset());
  VLOG(4) << "Builder construction  meta_xshape";
  paddle::dialect::IrMetaTensor meta_xshape(&ir_tensor_xshape);

  VLOG(4) << "Builder construction  dense_out_grad";
  paddle::dialect::IrTensor ir_tensor_out_grad(paddle::dialect::TransToPhiDataType(out_grad.dtype()),
                                                      out_grad.dims(),
                                                      out_grad.data_layout(),
                                                      out_grad.lod(),
                                                      out_grad.offset());
  VLOG(4) << "Builder construction  meta_out_grad";
  paddle::dialect::IrMetaTensor meta_out_grad(&ir_tensor_out_grad);
  paddle::dialect::IrTensor dense_x_grad;
  paddle::dialect::IrMetaTensor meta_x_grad(&dense_x_grad);

  phi::KernelWithXShapeInferMeta(meta_xshape, meta_out_grad, &meta_x_grad);

  std::vector<pir::Type> argument_outputs;
  pir::Type x_grad_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_x_grad.dtype()), dense_x_grad.dims(), dense_x_grad.layout(), dense_x_grad.lod(), dense_x_grad.offset());
  argument_outputs.push_back(x_grad_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void SqueezeGrad_Op::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value xshape_, pir::Value out_grad_, pir::AttributeMap attributes) {
  VLOG(4) << "Start build SqueezeGrad_Op";


  IR_ENFORCE(
      attributes.find("axis") != attributes.end(),
          "'axis' Attribute is expected for SqueezeGrad_Op. ");
  std::vector<int64_t> axis = attributes.at("axis").dyn_cast<paddle::dialect::IntArrayAttribute>().data().GetData();

  // Generate int_array mutable attribute: axis
  paddle::dialect::FullIntArrayOp full_axis_op = builder.Build<paddle::dialect::FullIntArrayOp>(axis, phi::DataType::INT64, phi::CPUPlace());
  pir::OpResult axis_ = full_axis_op->result(0);
    
  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {xshape_, out_grad_, axis_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType xshape = xshape_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)xshape;
  paddle::dialect::DenseTensorType out_grad = out_grad_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)out_grad;

  VLOG(4) << "Builder construction  dense_xshape";
  paddle::dialect::IrTensor ir_tensor_xshape(paddle::dialect::TransToPhiDataType(xshape.dtype()),
                                                      xshape.dims(),
                                                      xshape.data_layout(),
                                                      xshape.lod(),
                                                      xshape.offset());
  VLOG(4) << "Builder construction  meta_xshape";
  paddle::dialect::IrMetaTensor meta_xshape(&ir_tensor_xshape);

  VLOG(4) << "Builder construction  dense_out_grad";
  paddle::dialect::IrTensor ir_tensor_out_grad(paddle::dialect::TransToPhiDataType(out_grad.dtype()),
                                                      out_grad.dims(),
                                                      out_grad.data_layout(),
                                                      out_grad.lod(),
                                                      out_grad.offset());
  VLOG(4) << "Builder construction  meta_out_grad";
  paddle::dialect::IrMetaTensor meta_out_grad(&ir_tensor_out_grad);
  paddle::dialect::IrTensor dense_x_grad;
  paddle::dialect::IrMetaTensor meta_x_grad(&dense_x_grad);

  phi::KernelWithXShapeInferMeta(meta_xshape, meta_out_grad, &meta_x_grad);

  std::vector<pir::Type> argument_outputs;
  pir::Type x_grad_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_x_grad.dtype()), dense_x_grad.dims(), dense_x_grad.layout(), dense_x_grad.lod(), dense_x_grad.offset());
  argument_outputs.push_back(x_grad_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void SqueezeGrad_Op::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value xshape_, pir::Value out_grad_, pir::Value axis_) {
  VLOG(4) << "Start build SqueezeGrad_Op";



  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {xshape_, out_grad_, axis_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType xshape = xshape_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)xshape;
  paddle::dialect::DenseTensorType out_grad = out_grad_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)out_grad;
  phi::IntArray axis;
  if (axis_.dyn_cast<pir::OpResult>() && axis_.dyn_cast<pir::OpResult>().owner()->isa<paddle::dialect::FullIntArrayOp>()) {
    axis = std::move(phi::IntArray(paddle::dialect::GetInt64Vector(
                          axis_.dyn_cast<pir::OpResult>().owner()
                          ->dyn_cast<paddle::dialect::FullIntArrayOp>()
                          .attribute("value"))));
  } else if (axis_.type().isa<pir::VectorType>()) {
    size_t axis_size = axis_.type().dyn_cast<pir::VectorType>().size();
    axis = std::move(phi::IntArray(std::vector<int64_t>(axis_size, -1)));
    axis.SetFromTensor(true);
  } else if (axis_.type().isa<paddle::dialect::DenseTensorType>()) {
    common::DDim axis_dim = axis_.type().dyn_cast<paddle::dialect::DenseTensorType>().dims();
    size_t axis_size = common::product(axis_dim);
    if (common::contain_unknown_dim(axis_dim)) {
      axis_size = 1;
    }
    axis = std::move(phi::IntArray(std::vector<int64_t>(axis_size, -1)));
    axis.SetFromTensor(true);
  } else {
    PADDLE_THROW(phi::errors::Unimplemented("Only support VectorType or DenseTensorType"));
  }


  VLOG(4) << "Builder construction  dense_xshape";
  paddle::dialect::IrTensor ir_tensor_xshape(paddle::dialect::TransToPhiDataType(xshape.dtype()),
                                                      xshape.dims(),
                                                      xshape.data_layout(),
                                                      xshape.lod(),
                                                      xshape.offset());
  VLOG(4) << "Builder construction  meta_xshape";
  paddle::dialect::IrMetaTensor meta_xshape(&ir_tensor_xshape);

  VLOG(4) << "Builder construction  dense_out_grad";
  paddle::dialect::IrTensor ir_tensor_out_grad(paddle::dialect::TransToPhiDataType(out_grad.dtype()),
                                                      out_grad.dims(),
                                                      out_grad.data_layout(),
                                                      out_grad.lod(),
                                                      out_grad.offset());
  VLOG(4) << "Builder construction  meta_out_grad";
  paddle::dialect::IrMetaTensor meta_out_grad(&ir_tensor_out_grad);
  paddle::dialect::IrTensor dense_x_grad;
  paddle::dialect::IrMetaTensor meta_x_grad(&dense_x_grad);

  phi::KernelWithXShapeInferMeta(meta_xshape, meta_out_grad, &meta_x_grad);

  std::vector<pir::Type> argument_outputs;
  pir::Type x_grad_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_x_grad.dtype()), dense_x_grad.dims(), dense_x_grad.layout(), dense_x_grad.lod(), dense_x_grad.offset());
  argument_outputs.push_back(x_grad_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void SqueezeGrad_Op::VerifySig() {}

void SqueezeGrad_Op::InferMeta( phi::InferMetaContext *infer_meta ) {
  auto fn = PD_INFER_META(phi::KernelWithXShapeInferMeta);
  fn(infer_meta);
}

phi::DataType SqueezeGrad_Op::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: SqueezeGrad_Op";
  


  return expected_kernel_dtype;
}

const char *StackGradOp::attributes_name[1] = { "axis" };

OpInfoTuple StackGradOp::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("x", "pir::VectorType<paddle::dialect::DenseTensorType>", false, true, false, false), paddle::dialect::OpInputInfo("out_grad", "paddle::dialect::DenseTensorType", false, false, false, false) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = { paddle::dialect::OpAttributeInfo("axis", "pir::Int32Attribute", "") };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("x_grad", "pir::VectorType<paddle::dialect::DenseTensorType>", false, false) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("StackGradInferMeta", {"out_grad", "axis"}, "stack_grad", {"out_grad", "axis"}, {"out_grad"}, {}, {}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "stack_grad");
}

void StackGradOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, pir::Value out_grad_, int axis) {
  VLOG(4) << "Start build StackGradOp";



  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_, out_grad_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_axis = pir::Int32Attribute::get(pir::IrContext::Instance(), axis);
  argument.AddAttribute("axis", attr_axis);

  VLOG(4) << "Builder construction outputs";
  pir::VectorType x = x_.type().dyn_cast<pir::VectorType>(); (void)x;
  paddle::dialect::DenseTensorType out_grad = out_grad_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)out_grad;

  VLOG(4) << "Builder construction  dense_out_grad";
  paddle::dialect::IrTensor ir_tensor_out_grad(paddle::dialect::TransToPhiDataType(out_grad.dtype()),
                                                      out_grad.dims(),
                                                      out_grad.data_layout(),
                                                      out_grad.lod(),
                                                      out_grad.offset());
  VLOG(4) << "Builder construction  meta_out_grad";
  paddle::dialect::IrMetaTensor meta_out_grad(&ir_tensor_out_grad);
  std::vector<paddle::dialect::IrTensor> vec_dense_x_grad((x.size()), paddle::dialect::IrTensor());
  std::vector<paddle::dialect::IrMetaTensor> vec_meta_x_grad;
  for (size_t i=0; i < static_cast<size_t>(x.size()); i++) {
    vec_meta_x_grad.push_back(paddle::dialect::IrMetaTensor(&vec_dense_x_grad[i]));
  }
  std::vector<phi::MetaTensor*> meta_x_grad;
  for (size_t i=0; i < static_cast<size_t>(vec_meta_x_grad.size()); i++) {
    meta_x_grad.push_back(&vec_meta_x_grad[i]);
  }

  phi::StackGradInferMeta(meta_out_grad, axis, meta_x_grad);

  std::vector<pir::Type> argument_outputs;
  std::vector<pir::Type> x_grad_types;
  for (size_t i=0; i < static_cast<size_t>(x.size()); i++) {
    x_grad_types.push_back(paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(vec_dense_x_grad[i].dtype()), vec_dense_x_grad[i].dims(), vec_dense_x_grad[i].layout(), vec_dense_x_grad[i].lod(), vec_dense_x_grad[i].offset()));
  }
  pir::Type x_grad_vector_type = pir::VectorType::get(pir::IrContext::Instance(), x_grad_types);
  argument_outputs.push_back(x_grad_vector_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void StackGradOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, pir::Value out_grad_, pir::AttributeMap attributes) {
  VLOG(4) << "Start build StackGradOp";


  IR_ENFORCE(
      attributes.find("axis") != attributes.end(),
          "'axis' Attribute is expected for StackGradOp. ");
  int axis = attributes.at("axis").dyn_cast<pir::Int32Attribute>().data();


  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_, out_grad_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_axis = pir::Int32Attribute::get(pir::IrContext::Instance(), axis);
  argument.AddAttribute("axis", attr_axis);

  VLOG(4) << "Builder construction outputs";
  pir::VectorType x = x_.type().dyn_cast<pir::VectorType>(); (void)x;
  paddle::dialect::DenseTensorType out_grad = out_grad_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)out_grad;

  VLOG(4) << "Builder construction  dense_out_grad";
  paddle::dialect::IrTensor ir_tensor_out_grad(paddle::dialect::TransToPhiDataType(out_grad.dtype()),
                                                      out_grad.dims(),
                                                      out_grad.data_layout(),
                                                      out_grad.lod(),
                                                      out_grad.offset());
  VLOG(4) << "Builder construction  meta_out_grad";
  paddle::dialect::IrMetaTensor meta_out_grad(&ir_tensor_out_grad);
  std::vector<paddle::dialect::IrTensor> vec_dense_x_grad((x.size()), paddle::dialect::IrTensor());
  std::vector<paddle::dialect::IrMetaTensor> vec_meta_x_grad;
  for (size_t i=0; i < static_cast<size_t>(x.size()); i++) {
    vec_meta_x_grad.push_back(paddle::dialect::IrMetaTensor(&vec_dense_x_grad[i]));
  }
  std::vector<phi::MetaTensor*> meta_x_grad;
  for (size_t i=0; i < static_cast<size_t>(vec_meta_x_grad.size()); i++) {
    meta_x_grad.push_back(&vec_meta_x_grad[i]);
  }

  phi::StackGradInferMeta(meta_out_grad, axis, meta_x_grad);

  std::vector<pir::Type> argument_outputs;
  std::vector<pir::Type> x_grad_types;
  for (size_t i=0; i < static_cast<size_t>(x.size()); i++) {
    x_grad_types.push_back(paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(vec_dense_x_grad[i].dtype()), vec_dense_x_grad[i].dims(), vec_dense_x_grad[i].layout(), vec_dense_x_grad[i].lod(), vec_dense_x_grad[i].offset()));
  }
  pir::Type x_grad_vector_type = pir::VectorType::get(pir::IrContext::Instance(), x_grad_types);
  argument_outputs.push_back(x_grad_vector_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void StackGradOp::VerifySig() {}

void StackGradOp::InferMeta( phi::InferMetaContext *infer_meta ) {
  auto fn = PD_INFER_META(phi::StackGradInferMeta);
  fn(infer_meta);
}

phi::DataType StackGradOp::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: StackGradOp";
  


  return expected_kernel_dtype;
}

const char *StanhGradOp::attributes_name[2] = { "scale_a", "scale_b" };

OpInfoTuple StanhGradOp::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("x", "paddle::dialect::DenseTensorType", false, false, false, false), paddle::dialect::OpInputInfo("out_grad", "paddle::dialect::DenseTensorType", false, false, false, false) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = { paddle::dialect::OpAttributeInfo("scale_a", "pir::FloatAttribute", ""), paddle::dialect::OpAttributeInfo("scale_b", "pir::FloatAttribute", "") };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("x_grad", "paddle::dialect::DenseTensorType", false, false) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("UnchangedInferMeta", {"x"}, "stanh_grad", {"x", "out_grad", "scale_a", "scale_b"}, {}, {}, {}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "stanh_grad");
}

void StanhGradOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, pir::Value out_grad_, float scale_a, float scale_b) {
  VLOG(4) << "Start build StanhGradOp";



  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_, out_grad_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_scale_a = pir::FloatAttribute::get(pir::IrContext::Instance(), scale_a);
  argument.AddAttribute("scale_a", attr_scale_a);
  pir::Attribute attr_scale_b = pir::FloatAttribute::get(pir::IrContext::Instance(), scale_b);
  argument.AddAttribute("scale_b", attr_scale_b);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;
  paddle::dialect::DenseTensorType out_grad = out_grad_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)out_grad;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);
  paddle::dialect::IrTensor dense_x_grad;
  paddle::dialect::IrMetaTensor meta_x_grad(&dense_x_grad);

  phi::UnchangedInferMeta(meta_x, &meta_x_grad);

  std::vector<pir::Type> argument_outputs;
  pir::Type x_grad_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_x_grad.dtype()), dense_x_grad.dims(), dense_x_grad.layout(), dense_x_grad.lod(), dense_x_grad.offset());
  argument_outputs.push_back(x_grad_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void StanhGradOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, pir::Value out_grad_, pir::AttributeMap attributes) {
  VLOG(4) << "Start build StanhGradOp";


  IR_ENFORCE(
      attributes.find("scale_a") != attributes.end(),
          "'scale_a' Attribute is expected for StanhGradOp. ");
  float scale_a = attributes.at("scale_a").dyn_cast<pir::FloatAttribute>().data();

  IR_ENFORCE(
      attributes.find("scale_b") != attributes.end(),
          "'scale_b' Attribute is expected for StanhGradOp. ");
  float scale_b = attributes.at("scale_b").dyn_cast<pir::FloatAttribute>().data();


  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_, out_grad_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_scale_a = pir::FloatAttribute::get(pir::IrContext::Instance(), scale_a);
  argument.AddAttribute("scale_a", attr_scale_a);
  pir::Attribute attr_scale_b = pir::FloatAttribute::get(pir::IrContext::Instance(), scale_b);
  argument.AddAttribute("scale_b", attr_scale_b);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;
  paddle::dialect::DenseTensorType out_grad = out_grad_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)out_grad;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);
  paddle::dialect::IrTensor dense_x_grad;
  paddle::dialect::IrMetaTensor meta_x_grad(&dense_x_grad);

  phi::UnchangedInferMeta(meta_x, &meta_x_grad);

  std::vector<pir::Type> argument_outputs;
  pir::Type x_grad_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_x_grad.dtype()), dense_x_grad.dims(), dense_x_grad.layout(), dense_x_grad.lod(), dense_x_grad.offset());
  argument_outputs.push_back(x_grad_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void StanhGradOp::VerifySig() {}

void StanhGradOp::InferMeta( phi::InferMetaContext *infer_meta ) {
  auto fn = PD_INFER_META(phi::UnchangedInferMeta);
  fn(infer_meta);
}

phi::DataType StanhGradOp::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: StanhGradOp";
  


  return expected_kernel_dtype;
}

const char *SvdGradOp::attributes_name[1] = { "full_matrices" };

OpInfoTuple SvdGradOp::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("x", "paddle::dialect::DenseTensorType", false, false, false, false), paddle::dialect::OpInputInfo("u", "paddle::dialect::DenseTensorType", false, false, false, false), paddle::dialect::OpInputInfo("vh", "paddle::dialect::DenseTensorType", false, false, false, false), paddle::dialect::OpInputInfo("s", "paddle::dialect::DenseTensorType", false, false, false, false), paddle::dialect::OpInputInfo("u_grad", "paddle::dialect::DenseTensorType", true, false, false, false), paddle::dialect::OpInputInfo("vh_grad", "paddle::dialect::DenseTensorType", true, false, false, false), paddle::dialect::OpInputInfo("s_grad", "paddle::dialect::DenseTensorType", true, false, false, false) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = { paddle::dialect::OpAttributeInfo("full_matrices", "pir::BoolAttribute", "") };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("x_grad", "paddle::dialect::DenseTensorType", false, false) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("UnchangedInferMeta", {"x"}, "svd_grad", {"x", "u", "vh", "s", "u_grad", "vh_grad", "s_grad", "full_matrices"}, {}, {}, {}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "svd_grad");
}

void SvdGradOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, pir::Value u_, pir::Value vh_, pir::Value s_, pir::Value u_grad_, pir::Value vh_grad_, pir::Value s_grad_, bool full_matrices) {
  VLOG(4) << "Start build SvdGradOp";



  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_, u_, vh_, s_, u_grad_, vh_grad_, s_grad_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_full_matrices = pir::BoolAttribute::get(pir::IrContext::Instance(), full_matrices);
  argument.AddAttribute("full_matrices", attr_full_matrices);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;
  paddle::dialect::DenseTensorType u = u_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)u;
  paddle::dialect::DenseTensorType vh = vh_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)vh;
  paddle::dialect::DenseTensorType s = s_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)s;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);
  paddle::dialect::IrTensor dense_x_grad;
  paddle::dialect::IrMetaTensor meta_x_grad(&dense_x_grad);

  phi::UnchangedInferMeta(meta_x, &meta_x_grad);

  std::vector<pir::Type> argument_outputs;
  pir::Type x_grad_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_x_grad.dtype()), dense_x_grad.dims(), dense_x_grad.layout(), dense_x_grad.lod(), dense_x_grad.offset());
  argument_outputs.push_back(x_grad_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void SvdGradOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, pir::Value u_, pir::Value vh_, pir::Value s_, pir::Value u_grad_, pir::Value vh_grad_, pir::Value s_grad_, pir::AttributeMap attributes) {
  VLOG(4) << "Start build SvdGradOp";


  IR_ENFORCE(
      attributes.find("full_matrices") != attributes.end(),
          "'full_matrices' Attribute is expected for SvdGradOp. ");
  bool full_matrices = attributes.at("full_matrices").dyn_cast<pir::BoolAttribute>().data();


  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_, u_, vh_, s_, u_grad_, vh_grad_, s_grad_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_full_matrices = pir::BoolAttribute::get(pir::IrContext::Instance(), full_matrices);
  argument.AddAttribute("full_matrices", attr_full_matrices);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;
  paddle::dialect::DenseTensorType u = u_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)u;
  paddle::dialect::DenseTensorType vh = vh_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)vh;
  paddle::dialect::DenseTensorType s = s_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)s;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);
  paddle::dialect::IrTensor dense_x_grad;
  paddle::dialect::IrMetaTensor meta_x_grad(&dense_x_grad);

  phi::UnchangedInferMeta(meta_x, &meta_x_grad);

  std::vector<pir::Type> argument_outputs;
  pir::Type x_grad_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_x_grad.dtype()), dense_x_grad.dims(), dense_x_grad.layout(), dense_x_grad.lod(), dense_x_grad.offset());
  argument_outputs.push_back(x_grad_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void SvdGradOp::VerifySig() {}

void SvdGradOp::InferMeta( phi::InferMetaContext *infer_meta ) {
  auto fn = PD_INFER_META(phi::UnchangedInferMeta);
  fn(infer_meta);
}

phi::DataType SvdGradOp::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: SvdGradOp";
  


  return expected_kernel_dtype;
}

const char *TakeAlongAxisGradOp::attributes_name[1] = { "axis" };

OpInfoTuple TakeAlongAxisGradOp::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("arr", "paddle::dialect::DenseTensorType", false, false, false, false), paddle::dialect::OpInputInfo("indices", "paddle::dialect::DenseTensorType", false, false, false, false), paddle::dialect::OpInputInfo("out_grad", "paddle::dialect::DenseTensorType", false, false, false, false) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = { paddle::dialect::OpAttributeInfo("axis", "pir::Int32Attribute", "") };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("arr_grad", "paddle::dialect::DenseTensorType", false, false) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("UnchangedInferMeta", {"arr"}, "take_along_axis_grad", {"arr", "indices", "out_grad", "axis"}, {}, {}, {}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "take_along_axis_grad");
}

void TakeAlongAxisGradOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value arr_, pir::Value indices_, pir::Value out_grad_, int axis) {
  VLOG(4) << "Start build TakeAlongAxisGradOp";



  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {arr_, indices_, out_grad_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_axis = pir::Int32Attribute::get(pir::IrContext::Instance(), axis);
  argument.AddAttribute("axis", attr_axis);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType arr = arr_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)arr;
  paddle::dialect::DenseTensorType indices = indices_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)indices;
  paddle::dialect::DenseTensorType out_grad = out_grad_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)out_grad;

  VLOG(4) << "Builder construction  dense_arr";
  paddle::dialect::IrTensor ir_tensor_arr(paddle::dialect::TransToPhiDataType(arr.dtype()),
                                                      arr.dims(),
                                                      arr.data_layout(),
                                                      arr.lod(),
                                                      arr.offset());
  VLOG(4) << "Builder construction  meta_arr";
  paddle::dialect::IrMetaTensor meta_arr(&ir_tensor_arr);
  paddle::dialect::IrTensor dense_arr_grad;
  paddle::dialect::IrMetaTensor meta_arr_grad(&dense_arr_grad);

  phi::UnchangedInferMeta(meta_arr, &meta_arr_grad);

  std::vector<pir::Type> argument_outputs;
  pir::Type arr_grad_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_arr_grad.dtype()), dense_arr_grad.dims(), dense_arr_grad.layout(), dense_arr_grad.lod(), dense_arr_grad.offset());
  argument_outputs.push_back(arr_grad_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void TakeAlongAxisGradOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value arr_, pir::Value indices_, pir::Value out_grad_, pir::AttributeMap attributes) {
  VLOG(4) << "Start build TakeAlongAxisGradOp";


  IR_ENFORCE(
      attributes.find("axis") != attributes.end(),
          "'axis' Attribute is expected for TakeAlongAxisGradOp. ");
  int axis = attributes.at("axis").dyn_cast<pir::Int32Attribute>().data();


  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {arr_, indices_, out_grad_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_axis = pir::Int32Attribute::get(pir::IrContext::Instance(), axis);
  argument.AddAttribute("axis", attr_axis);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType arr = arr_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)arr;
  paddle::dialect::DenseTensorType indices = indices_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)indices;
  paddle::dialect::DenseTensorType out_grad = out_grad_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)out_grad;

  VLOG(4) << "Builder construction  dense_arr";
  paddle::dialect::IrTensor ir_tensor_arr(paddle::dialect::TransToPhiDataType(arr.dtype()),
                                                      arr.dims(),
                                                      arr.data_layout(),
                                                      arr.lod(),
                                                      arr.offset());
  VLOG(4) << "Builder construction  meta_arr";
  paddle::dialect::IrMetaTensor meta_arr(&ir_tensor_arr);
  paddle::dialect::IrTensor dense_arr_grad;
  paddle::dialect::IrMetaTensor meta_arr_grad(&dense_arr_grad);

  phi::UnchangedInferMeta(meta_arr, &meta_arr_grad);

  std::vector<pir::Type> argument_outputs;
  pir::Type arr_grad_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_arr_grad.dtype()), dense_arr_grad.dims(), dense_arr_grad.layout(), dense_arr_grad.lod(), dense_arr_grad.offset());
  argument_outputs.push_back(arr_grad_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void TakeAlongAxisGradOp::VerifySig() {}

void TakeAlongAxisGradOp::InferMeta( phi::InferMetaContext *infer_meta ) {
  auto fn = PD_INFER_META(phi::UnchangedInferMeta);
  fn(infer_meta);
}

phi::DataType TakeAlongAxisGradOp::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: TakeAlongAxisGradOp";
  


  return expected_kernel_dtype;
}

OpInfoTuple TanGradOp::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("x", "paddle::dialect::DenseTensorType", false, false, false, false), paddle::dialect::OpInputInfo("out_grad", "paddle::dialect::DenseTensorType", false, false, false, false) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = {  };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("x_grad", "paddle::dialect::DenseTensorType", false, false) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("UnchangedInferMeta", {"x"}, "tan_grad", {"x", "out_grad"}, {}, {}, {}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "tan_grad");
}

void TanGradOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, pir::Value out_grad_) {
  VLOG(4) << "Start build TanGradOp";



  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_, out_grad_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;
  paddle::dialect::DenseTensorType out_grad = out_grad_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)out_grad;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);
  paddle::dialect::IrTensor dense_x_grad;
  paddle::dialect::IrMetaTensor meta_x_grad(&dense_x_grad);

  phi::UnchangedInferMeta(meta_x, &meta_x_grad);

  std::vector<pir::Type> argument_outputs;
  pir::Type x_grad_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_x_grad.dtype()), dense_x_grad.dims(), dense_x_grad.layout(), dense_x_grad.lod(), dense_x_grad.offset());
  argument_outputs.push_back(x_grad_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void TanGradOp::VerifySig() {}

void TanGradOp::InferMeta( phi::InferMetaContext *infer_meta ) {
  auto fn = PD_INFER_META(phi::UnchangedInferMeta);
  fn(infer_meta);
}

phi::DataType TanGradOp::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: TanGradOp";
  


  return expected_kernel_dtype;
}

OpInfoTuple TanGrad_Op::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("x", "paddle::dialect::DenseTensorType", false, false, false, false), paddle::dialect::OpInputInfo("out_grad", "paddle::dialect::DenseTensorType", false, false, false, false) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = {  };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("x_grad", "paddle::dialect::DenseTensorType", false, false) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("UnchangedInferMeta", {"x"}, "tan_grad", {"x", "out_grad"}, {}, {}, {{"x_grad", "out_grad"}}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "tan_grad");
}

void TanGrad_Op::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, pir::Value out_grad_) {
  VLOG(4) << "Start build TanGrad_Op";



  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_, out_grad_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;
  paddle::dialect::DenseTensorType out_grad = out_grad_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)out_grad;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);
  paddle::dialect::IrTensor dense_x_grad;
  paddle::dialect::IrMetaTensor meta_x_grad(&dense_x_grad);

  phi::UnchangedInferMeta(meta_x, &meta_x_grad);

  std::vector<pir::Type> argument_outputs;
  pir::Type x_grad_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_x_grad.dtype()), dense_x_grad.dims(), dense_x_grad.layout(), dense_x_grad.lod(), dense_x_grad.offset());
  argument_outputs.push_back(x_grad_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void TanGrad_Op::VerifySig() {}

void TanGrad_Op::InferMeta( phi::InferMetaContext *infer_meta ) {
  auto fn = PD_INFER_META(phi::UnchangedInferMeta);
  fn(infer_meta);
}

phi::DataType TanGrad_Op::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: TanGrad_Op";
  


  return expected_kernel_dtype;
}

OpInfoTuple TanhDoubleGradOp::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("out", "paddle::dialect::DenseTensorType", false, false, false, true), paddle::dialect::OpInputInfo("grad_out", "paddle::dialect::DenseTensorType", false, false, false, true), paddle::dialect::OpInputInfo("grad_x_grad", "paddle::dialect::DenseTensorType", false, false, false, true) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = {  };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("out_grad", "paddle::dialect::DenseTensorType", false, false), paddle::dialect::OpOutputInfo("grad_out_grad", "paddle::dialect::DenseTensorType", false, false) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("GeneralBinaryGradInferMeta", {"out", "out"}, "tanh_double_grad", {"out", "grad_out", "grad_x_grad"}, {}, {}, {}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "tanh_double_grad");
}

void TanhDoubleGradOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value out_, pir::Value grad_out_, pir::Value grad_x_grad_) {
  VLOG(4) << "Start build TanhDoubleGradOp";



  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {out_, grad_out_, grad_x_grad_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType out = out_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)out;
  paddle::dialect::DenseTensorType grad_out = grad_out_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)grad_out;
  paddle::dialect::DenseTensorType grad_x_grad = grad_x_grad_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)grad_x_grad;

  VLOG(4) << "Builder construction  dense_out";
  paddle::dialect::IrTensor ir_tensor_out(paddle::dialect::TransToPhiDataType(out.dtype()),
                                                      out.dims(),
                                                      out.data_layout(),
                                                      out.lod(),
                                                      out.offset());
  VLOG(4) << "Builder construction  meta_out";
  paddle::dialect::IrMetaTensor meta_out(&ir_tensor_out);
  paddle::dialect::IrTensor dense_out_grad;
  paddle::dialect::IrMetaTensor meta_out_grad(&dense_out_grad);
  paddle::dialect::IrTensor dense_grad_out_grad;
  paddle::dialect::IrMetaTensor meta_grad_out_grad(&dense_grad_out_grad);

  phi::GeneralBinaryGradInferMeta(meta_out, meta_out, &meta_out_grad, &meta_grad_out_grad);

  std::vector<pir::Type> argument_outputs;
  pir::Type out_grad_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_out_grad.dtype()), dense_out_grad.dims(), dense_out_grad.layout(), dense_out_grad.lod(), dense_out_grad.offset());
  argument_outputs.push_back(out_grad_dense_tensor_type);

  pir::Type grad_out_grad_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_grad_out_grad.dtype()), dense_grad_out_grad.dims(), dense_grad_out_grad.layout(), dense_grad_out_grad.lod(), dense_grad_out_grad.offset());
  argument_outputs.push_back(grad_out_grad_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void TanhDoubleGradOp::VerifySig() {}

void TanhDoubleGradOp::InferMeta( phi::InferMetaContext *infer_meta ) {
  auto fn = PD_INFER_META(phi::GeneralBinaryGradInferMeta);
  fn(infer_meta);
}

phi::DataType TanhDoubleGradOp::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: TanhDoubleGradOp";
  


  return expected_kernel_dtype;
}

OpInfoTuple TanhDoubleGrad_Op::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("out", "paddle::dialect::DenseTensorType", false, false, false, true), paddle::dialect::OpInputInfo("grad_out", "paddle::dialect::DenseTensorType", false, false, false, true), paddle::dialect::OpInputInfo("grad_x_grad", "paddle::dialect::DenseTensorType", false, false, false, true) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = {  };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("out_grad", "paddle::dialect::DenseTensorType", false, false), paddle::dialect::OpOutputInfo("grad_out_grad", "paddle::dialect::DenseTensorType", false, false) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("GeneralBinaryGradInferMeta", {"out", "out"}, "tanh_double_grad", {"out", "grad_out", "grad_x_grad"}, {}, {}, {{"grad_out_grad", "grad_x_grad"}}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "tanh_double_grad");
}

void TanhDoubleGrad_Op::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value out_, pir::Value grad_out_, pir::Value grad_x_grad_) {
  VLOG(4) << "Start build TanhDoubleGrad_Op";



  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {out_, grad_out_, grad_x_grad_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType out = out_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)out;
  paddle::dialect::DenseTensorType grad_out = grad_out_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)grad_out;
  paddle::dialect::DenseTensorType grad_x_grad = grad_x_grad_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)grad_x_grad;

  VLOG(4) << "Builder construction  dense_out";
  paddle::dialect::IrTensor ir_tensor_out(paddle::dialect::TransToPhiDataType(out.dtype()),
                                                      out.dims(),
                                                      out.data_layout(),
                                                      out.lod(),
                                                      out.offset());
  VLOG(4) << "Builder construction  meta_out";
  paddle::dialect::IrMetaTensor meta_out(&ir_tensor_out);
  paddle::dialect::IrTensor dense_out_grad;
  paddle::dialect::IrMetaTensor meta_out_grad(&dense_out_grad);
  paddle::dialect::IrTensor dense_grad_out_grad;
  paddle::dialect::IrMetaTensor meta_grad_out_grad(&dense_grad_out_grad);

  phi::GeneralBinaryGradInferMeta(meta_out, meta_out, &meta_out_grad, &meta_grad_out_grad);

  std::vector<pir::Type> argument_outputs;
  pir::Type out_grad_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_out_grad.dtype()), dense_out_grad.dims(), dense_out_grad.layout(), dense_out_grad.lod(), dense_out_grad.offset());
  argument_outputs.push_back(out_grad_dense_tensor_type);

  pir::Type grad_out_grad_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_grad_out_grad.dtype()), dense_grad_out_grad.dims(), dense_grad_out_grad.layout(), dense_grad_out_grad.lod(), dense_grad_out_grad.offset());
  argument_outputs.push_back(grad_out_grad_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void TanhDoubleGrad_Op::VerifySig() {}

void TanhDoubleGrad_Op::InferMeta( phi::InferMetaContext *infer_meta ) {
  auto fn = PD_INFER_META(phi::GeneralBinaryGradInferMeta);
  fn(infer_meta);
}

phi::DataType TanhDoubleGrad_Op::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: TanhDoubleGrad_Op";
  


  return expected_kernel_dtype;
}

OpInfoTuple TanhGradOp::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("out", "paddle::dialect::DenseTensorType", false, false, false, true), paddle::dialect::OpInputInfo("out_grad", "paddle::dialect::DenseTensorType", false, false, false, true) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = {  };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("x_grad", "paddle::dialect::DenseTensorType", false, false) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("UnchangedInferMeta", {"out"}, "tanh_grad", {"out", "out_grad"}, {}, {}, {}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "tanh_grad");
}

void TanhGradOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value out_, pir::Value out_grad_) {
  VLOG(4) << "Start build TanhGradOp";



  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {out_, out_grad_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType out = out_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)out;
  paddle::dialect::DenseTensorType out_grad = out_grad_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)out_grad;

  VLOG(4) << "Builder construction  dense_out";
  paddle::dialect::IrTensor ir_tensor_out(paddle::dialect::TransToPhiDataType(out.dtype()),
                                                      out.dims(),
                                                      out.data_layout(),
                                                      out.lod(),
                                                      out.offset());
  VLOG(4) << "Builder construction  meta_out";
  paddle::dialect::IrMetaTensor meta_out(&ir_tensor_out);
  paddle::dialect::IrTensor dense_x_grad;
  paddle::dialect::IrMetaTensor meta_x_grad(&dense_x_grad);

  phi::UnchangedInferMeta(meta_out, &meta_x_grad);

  std::vector<pir::Type> argument_outputs;
  pir::Type x_grad_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_x_grad.dtype()), dense_x_grad.dims(), dense_x_grad.layout(), dense_x_grad.lod(), dense_x_grad.offset());
  argument_outputs.push_back(x_grad_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void TanhGradOp::VerifySig() {}

void TanhGradOp::InferMeta( phi::InferMetaContext *infer_meta ) {
  auto fn = PD_INFER_META(phi::UnchangedInferMeta);
  fn(infer_meta);
}

phi::DataType TanhGradOp::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: TanhGradOp";
  


  return expected_kernel_dtype;
}

OpInfoTuple TanhGrad_Op::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("out", "paddle::dialect::DenseTensorType", false, false, false, true), paddle::dialect::OpInputInfo("out_grad", "paddle::dialect::DenseTensorType", false, false, false, true) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = {  };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("x_grad", "paddle::dialect::DenseTensorType", false, false) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("UnchangedInferMeta", {"out"}, "tanh_grad", {"out", "out_grad"}, {}, {}, {{"x_grad", "out_grad"}}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "tanh_grad");
}

void TanhGrad_Op::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value out_, pir::Value out_grad_) {
  VLOG(4) << "Start build TanhGrad_Op";



  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {out_, out_grad_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType out = out_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)out;
  paddle::dialect::DenseTensorType out_grad = out_grad_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)out_grad;

  VLOG(4) << "Builder construction  dense_out";
  paddle::dialect::IrTensor ir_tensor_out(paddle::dialect::TransToPhiDataType(out.dtype()),
                                                      out.dims(),
                                                      out.data_layout(),
                                                      out.lod(),
                                                      out.offset());
  VLOG(4) << "Builder construction  meta_out";
  paddle::dialect::IrMetaTensor meta_out(&ir_tensor_out);
  paddle::dialect::IrTensor dense_x_grad;
  paddle::dialect::IrMetaTensor meta_x_grad(&dense_x_grad);

  phi::UnchangedInferMeta(meta_out, &meta_x_grad);

  std::vector<pir::Type> argument_outputs;
  pir::Type x_grad_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_x_grad.dtype()), dense_x_grad.dims(), dense_x_grad.layout(), dense_x_grad.lod(), dense_x_grad.offset());
  argument_outputs.push_back(x_grad_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void TanhGrad_Op::VerifySig() {}

void TanhGrad_Op::InferMeta( phi::InferMetaContext *infer_meta ) {
  auto fn = PD_INFER_META(phi::UnchangedInferMeta);
  fn(infer_meta);
}

phi::DataType TanhGrad_Op::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: TanhGrad_Op";
  


  return expected_kernel_dtype;
}

OpInfoTuple TanhShrinkGradOp::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("x", "paddle::dialect::DenseTensorType", false, false, false, false), paddle::dialect::OpInputInfo("out_grad", "paddle::dialect::DenseTensorType", false, false, false, false) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = {  };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("x_grad", "paddle::dialect::DenseTensorType", false, false) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("UnchangedInferMeta", {"x"}, "tanh_shrink_grad", {"x", "out_grad"}, {}, {}, {}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "tanh_shrink_grad");
}

void TanhShrinkGradOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, pir::Value out_grad_) {
  VLOG(4) << "Start build TanhShrinkGradOp";



  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_, out_grad_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;
  paddle::dialect::DenseTensorType out_grad = out_grad_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)out_grad;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);
  paddle::dialect::IrTensor dense_x_grad;
  paddle::dialect::IrMetaTensor meta_x_grad(&dense_x_grad);

  phi::UnchangedInferMeta(meta_x, &meta_x_grad);

  std::vector<pir::Type> argument_outputs;
  pir::Type x_grad_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_x_grad.dtype()), dense_x_grad.dims(), dense_x_grad.layout(), dense_x_grad.lod(), dense_x_grad.offset());
  argument_outputs.push_back(x_grad_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void TanhShrinkGradOp::VerifySig() {}

void TanhShrinkGradOp::InferMeta( phi::InferMetaContext *infer_meta ) {
  auto fn = PD_INFER_META(phi::UnchangedInferMeta);
  fn(infer_meta);
}

phi::DataType TanhShrinkGradOp::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: TanhShrinkGradOp";
  


  return expected_kernel_dtype;
}

OpInfoTuple TanhShrinkGrad_Op::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("x", "paddle::dialect::DenseTensorType", false, false, false, false), paddle::dialect::OpInputInfo("out_grad", "paddle::dialect::DenseTensorType", false, false, false, false) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = {  };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("x_grad", "paddle::dialect::DenseTensorType", false, false) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("UnchangedInferMeta", {"x"}, "tanh_shrink_grad", {"x", "out_grad"}, {}, {}, {{"x_grad", "out_grad"}}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "tanh_shrink_grad");
}

void TanhShrinkGrad_Op::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, pir::Value out_grad_) {
  VLOG(4) << "Start build TanhShrinkGrad_Op";



  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_, out_grad_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;
  paddle::dialect::DenseTensorType out_grad = out_grad_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)out_grad;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);
  paddle::dialect::IrTensor dense_x_grad;
  paddle::dialect::IrMetaTensor meta_x_grad(&dense_x_grad);

  phi::UnchangedInferMeta(meta_x, &meta_x_grad);

  std::vector<pir::Type> argument_outputs;
  pir::Type x_grad_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_x_grad.dtype()), dense_x_grad.dims(), dense_x_grad.layout(), dense_x_grad.lod(), dense_x_grad.offset());
  argument_outputs.push_back(x_grad_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void TanhShrinkGrad_Op::VerifySig() {}

void TanhShrinkGrad_Op::InferMeta( phi::InferMetaContext *infer_meta ) {
  auto fn = PD_INFER_META(phi::UnchangedInferMeta);
  fn(infer_meta);
}

phi::DataType TanhShrinkGrad_Op::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: TanhShrinkGrad_Op";
  


  return expected_kernel_dtype;
}

OpInfoTuple TanhTripleGradOp::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("out", "paddle::dialect::DenseTensorType", false, false, false, false), paddle::dialect::OpInputInfo("grad_out_forward", "paddle::dialect::DenseTensorType", false, false, false, false), paddle::dialect::OpInputInfo("grad_x_grad_forward", "paddle::dialect::DenseTensorType", false, false, false, false), paddle::dialect::OpInputInfo("grad_out_new_grad", "paddle::dialect::DenseTensorType", true, false, false, false), paddle::dialect::OpInputInfo("grad_out_grad_grad", "paddle::dialect::DenseTensorType", true, false, false, false) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = {  };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("out_grad", "paddle::dialect::DenseTensorType", false, false), paddle::dialect::OpOutputInfo("grad_out_forward_grad", "paddle::dialect::DenseTensorType", false, false), paddle::dialect::OpOutputInfo("grad_x_grad_forward_grad", "paddle::dialect::DenseTensorType", false, false) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("GeneralTernaryGradInferMeta", {"out", "out", "grad_x_grad_forward"}, "tanh_triple_grad", {"out", "grad_out_forward", "grad_x_grad_forward", "grad_out_new_grad", "grad_out_grad_grad"}, {}, {}, {}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "tanh_triple_grad");
}

void TanhTripleGradOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value out_, pir::Value grad_out_forward_, pir::Value grad_x_grad_forward_, pir::Value grad_out_new_grad_, pir::Value grad_out_grad_grad_) {
  VLOG(4) << "Start build TanhTripleGradOp";



  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {out_, grad_out_forward_, grad_x_grad_forward_, grad_out_new_grad_, grad_out_grad_grad_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType out = out_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)out;
  paddle::dialect::DenseTensorType grad_out_forward = grad_out_forward_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)grad_out_forward;
  paddle::dialect::DenseTensorType grad_x_grad_forward = grad_x_grad_forward_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)grad_x_grad_forward;

  VLOG(4) << "Builder construction  dense_out";
  paddle::dialect::IrTensor ir_tensor_out(paddle::dialect::TransToPhiDataType(out.dtype()),
                                                      out.dims(),
                                                      out.data_layout(),
                                                      out.lod(),
                                                      out.offset());
  VLOG(4) << "Builder construction  meta_out";
  paddle::dialect::IrMetaTensor meta_out(&ir_tensor_out);

  VLOG(4) << "Builder construction  dense_grad_x_grad_forward";
  paddle::dialect::IrTensor ir_tensor_grad_x_grad_forward(paddle::dialect::TransToPhiDataType(grad_x_grad_forward.dtype()),
                                                      grad_x_grad_forward.dims(),
                                                      grad_x_grad_forward.data_layout(),
                                                      grad_x_grad_forward.lod(),
                                                      grad_x_grad_forward.offset());
  VLOG(4) << "Builder construction  meta_grad_x_grad_forward";
  paddle::dialect::IrMetaTensor meta_grad_x_grad_forward(&ir_tensor_grad_x_grad_forward);
  paddle::dialect::IrTensor dense_out_grad;
  paddle::dialect::IrMetaTensor meta_out_grad(&dense_out_grad);
  paddle::dialect::IrTensor dense_grad_out_forward_grad;
  paddle::dialect::IrMetaTensor meta_grad_out_forward_grad(&dense_grad_out_forward_grad);
  paddle::dialect::IrTensor dense_grad_x_grad_forward_grad;
  paddle::dialect::IrMetaTensor meta_grad_x_grad_forward_grad(&dense_grad_x_grad_forward_grad);

  phi::GeneralTernaryGradInferMeta(meta_out, meta_out, meta_grad_x_grad_forward, &meta_out_grad, &meta_grad_out_forward_grad, &meta_grad_x_grad_forward_grad);

  std::vector<pir::Type> argument_outputs;
  pir::Type out_grad_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_out_grad.dtype()), dense_out_grad.dims(), dense_out_grad.layout(), dense_out_grad.lod(), dense_out_grad.offset());
  argument_outputs.push_back(out_grad_dense_tensor_type);

  pir::Type grad_out_forward_grad_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_grad_out_forward_grad.dtype()), dense_grad_out_forward_grad.dims(), dense_grad_out_forward_grad.layout(), dense_grad_out_forward_grad.lod(), dense_grad_out_forward_grad.offset());
  argument_outputs.push_back(grad_out_forward_grad_dense_tensor_type);

  pir::Type grad_x_grad_forward_grad_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_grad_x_grad_forward_grad.dtype()), dense_grad_x_grad_forward_grad.dims(), dense_grad_x_grad_forward_grad.layout(), dense_grad_x_grad_forward_grad.lod(), dense_grad_x_grad_forward_grad.offset());
  argument_outputs.push_back(grad_x_grad_forward_grad_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void TanhTripleGradOp::VerifySig() {}

void TanhTripleGradOp::InferMeta( phi::InferMetaContext *infer_meta ) {
  auto fn = PD_INFER_META(phi::GeneralTernaryGradInferMeta);
  fn(infer_meta);
}

phi::DataType TanhTripleGradOp::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: TanhTripleGradOp";
  


  return expected_kernel_dtype;
}

OpInfoTuple TanhTripleGrad_Op::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("out", "paddle::dialect::DenseTensorType", false, false, false, false), paddle::dialect::OpInputInfo("grad_out_forward", "paddle::dialect::DenseTensorType", false, false, false, false), paddle::dialect::OpInputInfo("grad_x_grad_forward", "paddle::dialect::DenseTensorType", false, false, false, false), paddle::dialect::OpInputInfo("grad_out_new_grad", "paddle::dialect::DenseTensorType", true, false, false, false), paddle::dialect::OpInputInfo("grad_out_grad_grad", "paddle::dialect::DenseTensorType", true, false, false, false) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = {  };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("out_grad", "paddle::dialect::DenseTensorType", false, false), paddle::dialect::OpOutputInfo("grad_out_forward_grad", "paddle::dialect::DenseTensorType", false, false), paddle::dialect::OpOutputInfo("grad_x_grad_forward_grad", "paddle::dialect::DenseTensorType", false, false) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("GeneralTernaryGradInferMeta", {"out", "out", "grad_x_grad_forward"}, "tanh_triple_grad", {"out", "grad_out_forward", "grad_x_grad_forward", "grad_out_new_grad", "grad_out_grad_grad"}, {}, {}, {{"grad_out_forward_grad", "grad_x_grad_forward"}}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "tanh_triple_grad");
}

void TanhTripleGrad_Op::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value out_, pir::Value grad_out_forward_, pir::Value grad_x_grad_forward_, pir::Value grad_out_new_grad_, pir::Value grad_out_grad_grad_) {
  VLOG(4) << "Start build TanhTripleGrad_Op";



  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {out_, grad_out_forward_, grad_x_grad_forward_, grad_out_new_grad_, grad_out_grad_grad_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType out = out_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)out;
  paddle::dialect::DenseTensorType grad_out_forward = grad_out_forward_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)grad_out_forward;
  paddle::dialect::DenseTensorType grad_x_grad_forward = grad_x_grad_forward_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)grad_x_grad_forward;

  VLOG(4) << "Builder construction  dense_out";
  paddle::dialect::IrTensor ir_tensor_out(paddle::dialect::TransToPhiDataType(out.dtype()),
                                                      out.dims(),
                                                      out.data_layout(),
                                                      out.lod(),
                                                      out.offset());
  VLOG(4) << "Builder construction  meta_out";
  paddle::dialect::IrMetaTensor meta_out(&ir_tensor_out);

  VLOG(4) << "Builder construction  dense_grad_x_grad_forward";
  paddle::dialect::IrTensor ir_tensor_grad_x_grad_forward(paddle::dialect::TransToPhiDataType(grad_x_grad_forward.dtype()),
                                                      grad_x_grad_forward.dims(),
                                                      grad_x_grad_forward.data_layout(),
                                                      grad_x_grad_forward.lod(),
                                                      grad_x_grad_forward.offset());
  VLOG(4) << "Builder construction  meta_grad_x_grad_forward";
  paddle::dialect::IrMetaTensor meta_grad_x_grad_forward(&ir_tensor_grad_x_grad_forward);
  paddle::dialect::IrTensor dense_out_grad;
  paddle::dialect::IrMetaTensor meta_out_grad(&dense_out_grad);
  paddle::dialect::IrTensor dense_grad_out_forward_grad;
  paddle::dialect::IrMetaTensor meta_grad_out_forward_grad(&dense_grad_out_forward_grad);
  paddle::dialect::IrTensor dense_grad_x_grad_forward_grad;
  paddle::dialect::IrMetaTensor meta_grad_x_grad_forward_grad(&dense_grad_x_grad_forward_grad);

  phi::GeneralTernaryGradInferMeta(meta_out, meta_out, meta_grad_x_grad_forward, &meta_out_grad, &meta_grad_out_forward_grad, &meta_grad_x_grad_forward_grad);

  std::vector<pir::Type> argument_outputs;
  pir::Type out_grad_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_out_grad.dtype()), dense_out_grad.dims(), dense_out_grad.layout(), dense_out_grad.lod(), dense_out_grad.offset());
  argument_outputs.push_back(out_grad_dense_tensor_type);

  pir::Type grad_out_forward_grad_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_grad_out_forward_grad.dtype()), dense_grad_out_forward_grad.dims(), dense_grad_out_forward_grad.layout(), dense_grad_out_forward_grad.lod(), dense_grad_out_forward_grad.offset());
  argument_outputs.push_back(grad_out_forward_grad_dense_tensor_type);

  pir::Type grad_x_grad_forward_grad_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_grad_x_grad_forward_grad.dtype()), dense_grad_x_grad_forward_grad.dims(), dense_grad_x_grad_forward_grad.layout(), dense_grad_x_grad_forward_grad.lod(), dense_grad_x_grad_forward_grad.offset());
  argument_outputs.push_back(grad_x_grad_forward_grad_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void TanhTripleGrad_Op::VerifySig() {}

void TanhTripleGrad_Op::InferMeta( phi::InferMetaContext *infer_meta ) {
  auto fn = PD_INFER_META(phi::GeneralTernaryGradInferMeta);
  fn(infer_meta);
}

phi::DataType TanhTripleGrad_Op::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: TanhTripleGrad_Op";
  


  return expected_kernel_dtype;
}

const char *TemporalShiftGradOp::attributes_name[3] = { "seg_num", "shift_ratio", "data_format" };

OpInfoTuple TemporalShiftGradOp::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("out_grad", "paddle::dialect::DenseTensorType", false, false, false, false) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = { paddle::dialect::OpAttributeInfo("seg_num", "pir::Int32Attribute", ""), paddle::dialect::OpAttributeInfo("shift_ratio", "pir::FloatAttribute", ""), paddle::dialect::OpAttributeInfo("data_format", "pir::StrAttribute", "") };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("x_grad", "paddle::dialect::DenseTensorType", false, false) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("UnchangedInferMeta", {"out_grad"}, "temporal_shift_grad", {"out_grad", "seg_num", "shift_ratio", "data_format"}, {"out_grad"}, {}, {}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "temporal_shift_grad");
}

void TemporalShiftGradOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value out_grad_, int seg_num, float shift_ratio, const std::string& data_format) {
  VLOG(4) << "Start build TemporalShiftGradOp";



  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {out_grad_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_seg_num = pir::Int32Attribute::get(pir::IrContext::Instance(), seg_num);
  argument.AddAttribute("seg_num", attr_seg_num);
  pir::Attribute attr_shift_ratio = pir::FloatAttribute::get(pir::IrContext::Instance(), shift_ratio);
  argument.AddAttribute("shift_ratio", attr_shift_ratio);
  pir::Attribute attr_data_format = pir::StrAttribute::get(pir::IrContext::Instance(), data_format);
  argument.AddAttribute("data_format", attr_data_format);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType out_grad = out_grad_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)out_grad;

  VLOG(4) << "Builder construction  dense_out_grad";
  paddle::dialect::IrTensor ir_tensor_out_grad(paddle::dialect::TransToPhiDataType(out_grad.dtype()),
                                                      out_grad.dims(),
                                                      out_grad.data_layout(),
                                                      out_grad.lod(),
                                                      out_grad.offset());
  VLOG(4) << "Builder construction  meta_out_grad";
  paddle::dialect::IrMetaTensor meta_out_grad(&ir_tensor_out_grad);
  paddle::dialect::IrTensor dense_x_grad;
  paddle::dialect::IrMetaTensor meta_x_grad(&dense_x_grad);

  phi::UnchangedInferMeta(meta_out_grad, &meta_x_grad);

  std::vector<pir::Type> argument_outputs;
  pir::Type x_grad_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_x_grad.dtype()), dense_x_grad.dims(), dense_x_grad.layout(), dense_x_grad.lod(), dense_x_grad.offset());
  argument_outputs.push_back(x_grad_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void TemporalShiftGradOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value out_grad_, pir::AttributeMap attributes) {
  VLOG(4) << "Start build TemporalShiftGradOp";


  IR_ENFORCE(
      attributes.find("seg_num") != attributes.end(),
          "'seg_num' Attribute is expected for TemporalShiftGradOp. ");
  int seg_num = attributes.at("seg_num").dyn_cast<pir::Int32Attribute>().data();

  IR_ENFORCE(
      attributes.find("shift_ratio") != attributes.end(),
          "'shift_ratio' Attribute is expected for TemporalShiftGradOp. ");
  float shift_ratio = attributes.at("shift_ratio").dyn_cast<pir::FloatAttribute>().data();

  IR_ENFORCE(
      attributes.find("data_format") != attributes.end(),
          "'data_format' Attribute is expected for TemporalShiftGradOp. ");
  std::string data_format = attributes.at("data_format").dyn_cast<pir::StrAttribute>().AsString();


  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {out_grad_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_seg_num = pir::Int32Attribute::get(pir::IrContext::Instance(), seg_num);
  argument.AddAttribute("seg_num", attr_seg_num);
  pir::Attribute attr_shift_ratio = pir::FloatAttribute::get(pir::IrContext::Instance(), shift_ratio);
  argument.AddAttribute("shift_ratio", attr_shift_ratio);
  pir::Attribute attr_data_format = pir::StrAttribute::get(pir::IrContext::Instance(), data_format);
  argument.AddAttribute("data_format", attr_data_format);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType out_grad = out_grad_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)out_grad;

  VLOG(4) << "Builder construction  dense_out_grad";
  paddle::dialect::IrTensor ir_tensor_out_grad(paddle::dialect::TransToPhiDataType(out_grad.dtype()),
                                                      out_grad.dims(),
                                                      out_grad.data_layout(),
                                                      out_grad.lod(),
                                                      out_grad.offset());
  VLOG(4) << "Builder construction  meta_out_grad";
  paddle::dialect::IrMetaTensor meta_out_grad(&ir_tensor_out_grad);
  paddle::dialect::IrTensor dense_x_grad;
  paddle::dialect::IrMetaTensor meta_x_grad(&dense_x_grad);

  phi::UnchangedInferMeta(meta_out_grad, &meta_x_grad);

  std::vector<pir::Type> argument_outputs;
  pir::Type x_grad_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_x_grad.dtype()), dense_x_grad.dims(), dense_x_grad.layout(), dense_x_grad.lod(), dense_x_grad.offset());
  argument_outputs.push_back(x_grad_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void TemporalShiftGradOp::VerifySig() {}

void TemporalShiftGradOp::InferMeta( phi::InferMetaContext *infer_meta ) {
  auto fn = PD_INFER_META(phi::UnchangedInferMeta);
  fn(infer_meta);
}

phi::DataType TemporalShiftGradOp::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: TemporalShiftGradOp";
  


  return expected_kernel_dtype;
}

const char *TensorUnfoldGradOp::attributes_name[3] = { "axis", "size", "step" };

OpInfoTuple TensorUnfoldGradOp::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("input", "paddle::dialect::DenseTensorType", false, false, false, false), paddle::dialect::OpInputInfo("out_grad", "paddle::dialect::DenseTensorType", false, false, false, false) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = { paddle::dialect::OpAttributeInfo("axis", "pir::Int64Attribute", ""), paddle::dialect::OpAttributeInfo("size", "pir::Int64Attribute", ""), paddle::dialect::OpAttributeInfo("step", "pir::Int64Attribute", "") };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("input_grad", "paddle::dialect::DenseTensorType", false, false) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("StridedUnChangedInferMeta", {"input"}, "tensor_unfold_grad", {"input", "out_grad", "axis", "size", "step"}, {}, {}, {}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "tensor_unfold_grad");
}

void TensorUnfoldGradOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value input_, pir::Value out_grad_, int64_t axis, int64_t size, int64_t step) {
  VLOG(4) << "Start build TensorUnfoldGradOp";



  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {input_, out_grad_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_axis = pir::Int64Attribute::get(pir::IrContext::Instance(), axis);
  argument.AddAttribute("axis", attr_axis);
  pir::Attribute attr_size = pir::Int64Attribute::get(pir::IrContext::Instance(), size);
  argument.AddAttribute("size", attr_size);
  pir::Attribute attr_step = pir::Int64Attribute::get(pir::IrContext::Instance(), step);
  argument.AddAttribute("step", attr_step);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType input = input_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)input;
  paddle::dialect::DenseTensorType out_grad = out_grad_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)out_grad;

  VLOG(4) << "Builder construction  dense_input";
  paddle::dialect::IrTensor ir_tensor_input(paddle::dialect::TransToPhiDataType(input.dtype()),
                                                      input.dims(),
                                                      input.data_layout(),
                                                      input.lod(),
                                                      input.offset());
  VLOG(4) << "Builder construction  meta_input";
  paddle::dialect::IrMetaTensor meta_input(&ir_tensor_input);
  paddle::dialect::IrTensor dense_input_grad;
  paddle::dialect::IrMetaTensor meta_input_grad(&dense_input_grad);

  phi::StridedUnChangedInferMeta(meta_input, &meta_input_grad);

  std::vector<pir::Type> argument_outputs;
  pir::Type input_grad_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_input_grad.dtype()), dense_input_grad.dims(), dense_input_grad.layout(), dense_input_grad.lod(), dense_input_grad.offset());
  argument_outputs.push_back(input_grad_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void TensorUnfoldGradOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value input_, pir::Value out_grad_, pir::AttributeMap attributes) {
  VLOG(4) << "Start build TensorUnfoldGradOp";


  IR_ENFORCE(
      attributes.find("axis") != attributes.end(),
          "'axis' Attribute is expected for TensorUnfoldGradOp. ");
  int64_t axis = attributes.at("axis").dyn_cast<pir::Int64Attribute>().data();

  IR_ENFORCE(
      attributes.find("size") != attributes.end(),
          "'size' Attribute is expected for TensorUnfoldGradOp. ");
  int64_t size = attributes.at("size").dyn_cast<pir::Int64Attribute>().data();

  IR_ENFORCE(
      attributes.find("step") != attributes.end(),
          "'step' Attribute is expected for TensorUnfoldGradOp. ");
  int64_t step = attributes.at("step").dyn_cast<pir::Int64Attribute>().data();


  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {input_, out_grad_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_axis = pir::Int64Attribute::get(pir::IrContext::Instance(), axis);
  argument.AddAttribute("axis", attr_axis);
  pir::Attribute attr_size = pir::Int64Attribute::get(pir::IrContext::Instance(), size);
  argument.AddAttribute("size", attr_size);
  pir::Attribute attr_step = pir::Int64Attribute::get(pir::IrContext::Instance(), step);
  argument.AddAttribute("step", attr_step);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType input = input_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)input;
  paddle::dialect::DenseTensorType out_grad = out_grad_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)out_grad;

  VLOG(4) << "Builder construction  dense_input";
  paddle::dialect::IrTensor ir_tensor_input(paddle::dialect::TransToPhiDataType(input.dtype()),
                                                      input.dims(),
                                                      input.data_layout(),
                                                      input.lod(),
                                                      input.offset());
  VLOG(4) << "Builder construction  meta_input";
  paddle::dialect::IrMetaTensor meta_input(&ir_tensor_input);
  paddle::dialect::IrTensor dense_input_grad;
  paddle::dialect::IrMetaTensor meta_input_grad(&dense_input_grad);

  phi::StridedUnChangedInferMeta(meta_input, &meta_input_grad);

  std::vector<pir::Type> argument_outputs;
  pir::Type input_grad_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_input_grad.dtype()), dense_input_grad.dims(), dense_input_grad.layout(), dense_input_grad.lod(), dense_input_grad.offset());
  argument_outputs.push_back(input_grad_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void TensorUnfoldGradOp::VerifySig() {}

void TensorUnfoldGradOp::InferMeta( phi::InferMetaContext *infer_meta ) {
  auto fn = PD_INFER_META(phi::StridedUnChangedInferMeta);
  fn(infer_meta);
}

phi::DataType TensorUnfoldGradOp::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: TensorUnfoldGradOp";
  


  return expected_kernel_dtype;
}

const char *ThresholdedReluGradOp::attributes_name[1] = { "threshold" };

OpInfoTuple ThresholdedReluGradOp::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("x", "paddle::dialect::DenseTensorType", false, false, false, false), paddle::dialect::OpInputInfo("out_grad", "paddle::dialect::DenseTensorType", false, false, false, false) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = { paddle::dialect::OpAttributeInfo("threshold", "pir::FloatAttribute", "") };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("x_grad", "paddle::dialect::DenseTensorType", false, false) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("UnchangedInferMeta", {"x"}, "thresholded_relu_grad", {"x", "out_grad", "threshold"}, {}, {}, {}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "thresholded_relu_grad");
}

void ThresholdedReluGradOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, pir::Value out_grad_, float threshold) {
  VLOG(4) << "Start build ThresholdedReluGradOp";



  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_, out_grad_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_threshold = pir::FloatAttribute::get(pir::IrContext::Instance(), threshold);
  argument.AddAttribute("threshold", attr_threshold);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;
  paddle::dialect::DenseTensorType out_grad = out_grad_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)out_grad;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);
  paddle::dialect::IrTensor dense_x_grad;
  paddle::dialect::IrMetaTensor meta_x_grad(&dense_x_grad);

  phi::UnchangedInferMeta(meta_x, &meta_x_grad);

  std::vector<pir::Type> argument_outputs;
  pir::Type x_grad_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_x_grad.dtype()), dense_x_grad.dims(), dense_x_grad.layout(), dense_x_grad.lod(), dense_x_grad.offset());
  argument_outputs.push_back(x_grad_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void ThresholdedReluGradOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, pir::Value out_grad_, pir::AttributeMap attributes) {
  VLOG(4) << "Start build ThresholdedReluGradOp";


  IR_ENFORCE(
      attributes.find("threshold") != attributes.end(),
          "'threshold' Attribute is expected for ThresholdedReluGradOp. ");
  float threshold = attributes.at("threshold").dyn_cast<pir::FloatAttribute>().data();


  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_, out_grad_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_threshold = pir::FloatAttribute::get(pir::IrContext::Instance(), threshold);
  argument.AddAttribute("threshold", attr_threshold);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;
  paddle::dialect::DenseTensorType out_grad = out_grad_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)out_grad;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);
  paddle::dialect::IrTensor dense_x_grad;
  paddle::dialect::IrMetaTensor meta_x_grad(&dense_x_grad);

  phi::UnchangedInferMeta(meta_x, &meta_x_grad);

  std::vector<pir::Type> argument_outputs;
  pir::Type x_grad_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_x_grad.dtype()), dense_x_grad.dims(), dense_x_grad.layout(), dense_x_grad.lod(), dense_x_grad.offset());
  argument_outputs.push_back(x_grad_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void ThresholdedReluGradOp::VerifySig() {}

void ThresholdedReluGradOp::InferMeta( phi::InferMetaContext *infer_meta ) {
  auto fn = PD_INFER_META(phi::UnchangedInferMeta);
  fn(infer_meta);
}

phi::DataType ThresholdedReluGradOp::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: ThresholdedReluGradOp";
  


  return expected_kernel_dtype;
}

const char *ThresholdedReluGrad_Op::attributes_name[1] = { "threshold" };

OpInfoTuple ThresholdedReluGrad_Op::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("x", "paddle::dialect::DenseTensorType", false, false, false, false), paddle::dialect::OpInputInfo("out_grad", "paddle::dialect::DenseTensorType", false, false, false, false) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = { paddle::dialect::OpAttributeInfo("threshold", "pir::FloatAttribute", "") };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("x_grad", "paddle::dialect::DenseTensorType", false, false) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("UnchangedInferMeta", {"x"}, "thresholded_relu_grad", {"x", "out_grad", "threshold"}, {}, {}, {{"x_grad", "out_grad"}}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "thresholded_relu_grad");
}

void ThresholdedReluGrad_Op::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, pir::Value out_grad_, float threshold) {
  VLOG(4) << "Start build ThresholdedReluGrad_Op";



  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_, out_grad_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_threshold = pir::FloatAttribute::get(pir::IrContext::Instance(), threshold);
  argument.AddAttribute("threshold", attr_threshold);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;
  paddle::dialect::DenseTensorType out_grad = out_grad_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)out_grad;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);
  paddle::dialect::IrTensor dense_x_grad;
  paddle::dialect::IrMetaTensor meta_x_grad(&dense_x_grad);

  phi::UnchangedInferMeta(meta_x, &meta_x_grad);

  std::vector<pir::Type> argument_outputs;
  pir::Type x_grad_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_x_grad.dtype()), dense_x_grad.dims(), dense_x_grad.layout(), dense_x_grad.lod(), dense_x_grad.offset());
  argument_outputs.push_back(x_grad_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void ThresholdedReluGrad_Op::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, pir::Value out_grad_, pir::AttributeMap attributes) {
  VLOG(4) << "Start build ThresholdedReluGrad_Op";


  IR_ENFORCE(
      attributes.find("threshold") != attributes.end(),
          "'threshold' Attribute is expected for ThresholdedReluGrad_Op. ");
  float threshold = attributes.at("threshold").dyn_cast<pir::FloatAttribute>().data();


  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_, out_grad_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_threshold = pir::FloatAttribute::get(pir::IrContext::Instance(), threshold);
  argument.AddAttribute("threshold", attr_threshold);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;
  paddle::dialect::DenseTensorType out_grad = out_grad_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)out_grad;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);
  paddle::dialect::IrTensor dense_x_grad;
  paddle::dialect::IrMetaTensor meta_x_grad(&dense_x_grad);

  phi::UnchangedInferMeta(meta_x, &meta_x_grad);

  std::vector<pir::Type> argument_outputs;
  pir::Type x_grad_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_x_grad.dtype()), dense_x_grad.dims(), dense_x_grad.layout(), dense_x_grad.lod(), dense_x_grad.offset());
  argument_outputs.push_back(x_grad_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void ThresholdedReluGrad_Op::VerifySig() {}

void ThresholdedReluGrad_Op::InferMeta( phi::InferMetaContext *infer_meta ) {
  auto fn = PD_INFER_META(phi::UnchangedInferMeta);
  fn(infer_meta);
}

phi::DataType ThresholdedReluGrad_Op::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: ThresholdedReluGrad_Op";
  


  return expected_kernel_dtype;
}

const char *TopkGradOp::attributes_name[3] = { "axis", "largest", "sorted" };

OpInfoTuple TopkGradOp::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("x", "paddle::dialect::DenseTensorType", false, false, false, false), paddle::dialect::OpInputInfo("indices", "paddle::dialect::DenseTensorType", false, false, false, false), paddle::dialect::OpInputInfo("out_grad", "paddle::dialect::DenseTensorType", false, false, false, false), paddle::dialect::OpInputInfo("k", "paddle::dialect::ScalarAttribute", false, false, true, false) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = { paddle::dialect::OpAttributeInfo("axis", "pir::Int32Attribute", ""), paddle::dialect::OpAttributeInfo("largest", "pir::BoolAttribute", ""), paddle::dialect::OpAttributeInfo("sorted", "pir::BoolAttribute", "") };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("x_grad", "paddle::dialect::DenseTensorType", false, false) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("UnchangedInferMeta", {"x"}, "topk_grad", {"x", "indices", "out_grad", "k", "axis", "largest", "sorted"}, {"out_grad"}, {}, {}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "topk_grad");
}

void TopkGradOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, pir::Value indices_, pir::Value out_grad_, int k, int axis, bool largest, bool sorted) {
  VLOG(4) << "Start build TopkGradOp";


  // Generate scalar mutable attribute: k
  paddle::dialect::FullOp full_k_op = builder.Build<paddle::dialect::FullOp>(std::vector<int64_t>{1}, k, phi::DataType::INT32, phi::CPUPlace());
  pir::OpResult k_ = full_k_op->result(0);
    
  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_, indices_, out_grad_, k_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_axis = pir::Int32Attribute::get(pir::IrContext::Instance(), axis);
  argument.AddAttribute("axis", attr_axis);
  pir::Attribute attr_largest = pir::BoolAttribute::get(pir::IrContext::Instance(), largest);
  argument.AddAttribute("largest", attr_largest);
  pir::Attribute attr_sorted = pir::BoolAttribute::get(pir::IrContext::Instance(), sorted);
  argument.AddAttribute("sorted", attr_sorted);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;
  paddle::dialect::DenseTensorType indices = indices_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)indices;
  paddle::dialect::DenseTensorType out_grad = out_grad_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)out_grad;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);
  paddle::dialect::IrTensor dense_x_grad;
  paddle::dialect::IrMetaTensor meta_x_grad(&dense_x_grad);

  phi::UnchangedInferMeta(meta_x, &meta_x_grad);

  std::vector<pir::Type> argument_outputs;
  pir::Type x_grad_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_x_grad.dtype()), dense_x_grad.dims(), dense_x_grad.layout(), dense_x_grad.lod(), dense_x_grad.offset());
  argument_outputs.push_back(x_grad_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void TopkGradOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, pir::Value indices_, pir::Value out_grad_, pir::AttributeMap attributes) {
  VLOG(4) << "Start build TopkGradOp";


  IR_ENFORCE(
      attributes.find("k") != attributes.end(),
          "'k' Attribute is expected for TopkGradOp. ");
  int k = attributes.at("k").dyn_cast<paddle::dialect::ScalarAttribute>().data().to<int>();

  IR_ENFORCE(
      attributes.find("axis") != attributes.end(),
          "'axis' Attribute is expected for TopkGradOp. ");
  int axis = attributes.at("axis").dyn_cast<pir::Int32Attribute>().data();

  IR_ENFORCE(
      attributes.find("largest") != attributes.end(),
          "'largest' Attribute is expected for TopkGradOp. ");
  bool largest = attributes.at("largest").dyn_cast<pir::BoolAttribute>().data();

  IR_ENFORCE(
      attributes.find("sorted") != attributes.end(),
          "'sorted' Attribute is expected for TopkGradOp. ");
  bool sorted = attributes.at("sorted").dyn_cast<pir::BoolAttribute>().data();

  // Generate scalar mutable attribute: k
  paddle::dialect::FullOp full_k_op = builder.Build<paddle::dialect::FullOp>(std::vector<int64_t>{1}, k, phi::DataType::INT32, phi::CPUPlace());
  pir::OpResult k_ = full_k_op->result(0);
    
  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_, indices_, out_grad_, k_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_axis = pir::Int32Attribute::get(pir::IrContext::Instance(), axis);
  argument.AddAttribute("axis", attr_axis);
  pir::Attribute attr_largest = pir::BoolAttribute::get(pir::IrContext::Instance(), largest);
  argument.AddAttribute("largest", attr_largest);
  pir::Attribute attr_sorted = pir::BoolAttribute::get(pir::IrContext::Instance(), sorted);
  argument.AddAttribute("sorted", attr_sorted);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;
  paddle::dialect::DenseTensorType indices = indices_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)indices;
  paddle::dialect::DenseTensorType out_grad = out_grad_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)out_grad;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);
  paddle::dialect::IrTensor dense_x_grad;
  paddle::dialect::IrMetaTensor meta_x_grad(&dense_x_grad);

  phi::UnchangedInferMeta(meta_x, &meta_x_grad);

  std::vector<pir::Type> argument_outputs;
  pir::Type x_grad_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_x_grad.dtype()), dense_x_grad.dims(), dense_x_grad.layout(), dense_x_grad.lod(), dense_x_grad.offset());
  argument_outputs.push_back(x_grad_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void TopkGradOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, pir::Value indices_, pir::Value out_grad_, pir::Value k_, int axis, bool largest, bool sorted) {
  VLOG(4) << "Start build TopkGradOp";



  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_, indices_, out_grad_, k_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_axis = pir::Int32Attribute::get(pir::IrContext::Instance(), axis);
  argument.AddAttribute("axis", attr_axis);
  pir::Attribute attr_largest = pir::BoolAttribute::get(pir::IrContext::Instance(), largest);
  argument.AddAttribute("largest", attr_largest);
  pir::Attribute attr_sorted = pir::BoolAttribute::get(pir::IrContext::Instance(), sorted);
  argument.AddAttribute("sorted", attr_sorted);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;
  paddle::dialect::DenseTensorType indices = indices_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)indices;
  paddle::dialect::DenseTensorType out_grad = out_grad_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)out_grad;
  phi::Scalar k;
  if (k_.dyn_cast<pir::OpResult>() && k_.dyn_cast<pir::OpResult>().owner()->isa<paddle::dialect::FullOp>()) {
    k = std::move(phi::Scalar(k_.dyn_cast<pir::OpResult>().owner()
                                  ->dyn_cast<paddle::dialect::FullOp>()
                                  .attribute("value")
                                  .dyn_cast<paddle::dialect::ScalarAttribute>()
                                  .data()
                                  .to<int>()));
  }
  else {
    k = std::move(phi::Scalar(-1));
    k.SetFromTensor(true);
  }


  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);
  paddle::dialect::IrTensor dense_x_grad;
  paddle::dialect::IrMetaTensor meta_x_grad(&dense_x_grad);

  phi::UnchangedInferMeta(meta_x, &meta_x_grad);

  std::vector<pir::Type> argument_outputs;
  pir::Type x_grad_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_x_grad.dtype()), dense_x_grad.dims(), dense_x_grad.layout(), dense_x_grad.lod(), dense_x_grad.offset());
  argument_outputs.push_back(x_grad_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void TopkGradOp::VerifySig() {}

void TopkGradOp::InferMeta( phi::InferMetaContext *infer_meta ) {
  auto fn = PD_INFER_META(phi::UnchangedInferMeta);
  fn(infer_meta);
}

phi::DataType TopkGradOp::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: TopkGradOp";
  


  return expected_kernel_dtype;
}

const char *TraceGradOp::attributes_name[3] = { "offset", "axis1", "axis2" };

OpInfoTuple TraceGradOp::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("x", "paddle::dialect::DenseTensorType", false, true, false, false), paddle::dialect::OpInputInfo("out_grad", "paddle::dialect::DenseTensorType", false, false, false, false) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = { paddle::dialect::OpAttributeInfo("offset", "pir::Int32Attribute", ""), paddle::dialect::OpAttributeInfo("axis1", "pir::Int32Attribute", ""), paddle::dialect::OpAttributeInfo("axis2", "pir::Int32Attribute", "") };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("x_grad", "paddle::dialect::DenseTensorType", false, false) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("UnchangedInferMeta", {"x"}, "trace_grad", {"x", "out_grad", "offset", "axis1", "axis2"}, {"out_grad"}, {}, {}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "trace_grad");
}

void TraceGradOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, pir::Value out_grad_, int offset, int axis1, int axis2) {
  VLOG(4) << "Start build TraceGradOp";



  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_, out_grad_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_offset = pir::Int32Attribute::get(pir::IrContext::Instance(), offset);
  argument.AddAttribute("offset", attr_offset);
  pir::Attribute attr_axis1 = pir::Int32Attribute::get(pir::IrContext::Instance(), axis1);
  argument.AddAttribute("axis1", attr_axis1);
  pir::Attribute attr_axis2 = pir::Int32Attribute::get(pir::IrContext::Instance(), axis2);
  argument.AddAttribute("axis2", attr_axis2);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;
  paddle::dialect::DenseTensorType out_grad = out_grad_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)out_grad;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);
  paddle::dialect::IrTensor dense_x_grad;
  paddle::dialect::IrMetaTensor meta_x_grad(&dense_x_grad);

  phi::UnchangedInferMeta(meta_x, &meta_x_grad);

  std::vector<pir::Type> argument_outputs;
  pir::Type x_grad_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_x_grad.dtype()), dense_x_grad.dims(), dense_x_grad.layout(), dense_x_grad.lod(), dense_x_grad.offset());
  argument_outputs.push_back(x_grad_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void TraceGradOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, pir::Value out_grad_, pir::AttributeMap attributes) {
  VLOG(4) << "Start build TraceGradOp";


  IR_ENFORCE(
      attributes.find("offset") != attributes.end(),
          "'offset' Attribute is expected for TraceGradOp. ");
  int offset = attributes.at("offset").dyn_cast<pir::Int32Attribute>().data();

  IR_ENFORCE(
      attributes.find("axis1") != attributes.end(),
          "'axis1' Attribute is expected for TraceGradOp. ");
  int axis1 = attributes.at("axis1").dyn_cast<pir::Int32Attribute>().data();

  IR_ENFORCE(
      attributes.find("axis2") != attributes.end(),
          "'axis2' Attribute is expected for TraceGradOp. ");
  int axis2 = attributes.at("axis2").dyn_cast<pir::Int32Attribute>().data();


  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_, out_grad_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_offset = pir::Int32Attribute::get(pir::IrContext::Instance(), offset);
  argument.AddAttribute("offset", attr_offset);
  pir::Attribute attr_axis1 = pir::Int32Attribute::get(pir::IrContext::Instance(), axis1);
  argument.AddAttribute("axis1", attr_axis1);
  pir::Attribute attr_axis2 = pir::Int32Attribute::get(pir::IrContext::Instance(), axis2);
  argument.AddAttribute("axis2", attr_axis2);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;
  paddle::dialect::DenseTensorType out_grad = out_grad_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)out_grad;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);
  paddle::dialect::IrTensor dense_x_grad;
  paddle::dialect::IrMetaTensor meta_x_grad(&dense_x_grad);

  phi::UnchangedInferMeta(meta_x, &meta_x_grad);

  std::vector<pir::Type> argument_outputs;
  pir::Type x_grad_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_x_grad.dtype()), dense_x_grad.dims(), dense_x_grad.layout(), dense_x_grad.lod(), dense_x_grad.offset());
  argument_outputs.push_back(x_grad_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void TraceGradOp::VerifySig() {}

void TraceGradOp::InferMeta( phi::InferMetaContext *infer_meta ) {
  auto fn = PD_INFER_META(phi::UnchangedInferMeta);
  fn(infer_meta);
}

phi::DataType TraceGradOp::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: TraceGradOp";
  


  return expected_kernel_dtype;
}

const char *TriangularSolveGradOp::attributes_name[3] = { "upper", "transpose", "unitriangular" };

OpInfoTuple TriangularSolveGradOp::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("x", "paddle::dialect::DenseTensorType", false, false, false, false), paddle::dialect::OpInputInfo("y", "paddle::dialect::DenseTensorType", false, false, false, false), paddle::dialect::OpInputInfo("out", "paddle::dialect::DenseTensorType", false, false, false, false), paddle::dialect::OpInputInfo("out_grad", "paddle::dialect::DenseTensorType", false, false, false, false) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = { paddle::dialect::OpAttributeInfo("upper", "pir::BoolAttribute", ""), paddle::dialect::OpAttributeInfo("transpose", "pir::BoolAttribute", ""), paddle::dialect::OpAttributeInfo("unitriangular", "pir::BoolAttribute", "") };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("x_grad", "paddle::dialect::DenseTensorType", false, false), paddle::dialect::OpOutputInfo("y_grad", "paddle::dialect::DenseTensorType", false, false) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("GeneralBinaryGradInferMeta", {"x", "y"}, "triangular_solve_grad", {"x", "y", "out", "out_grad", "upper", "transpose", "unitriangular"}, {}, {}, {}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "triangular_solve_grad");
}

void TriangularSolveGradOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, pir::Value y_, pir::Value out_, pir::Value out_grad_, bool upper, bool transpose, bool unitriangular) {
  VLOG(4) << "Start build TriangularSolveGradOp";



  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_, y_, out_, out_grad_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_upper = pir::BoolAttribute::get(pir::IrContext::Instance(), upper);
  argument.AddAttribute("upper", attr_upper);
  pir::Attribute attr_transpose = pir::BoolAttribute::get(pir::IrContext::Instance(), transpose);
  argument.AddAttribute("transpose", attr_transpose);
  pir::Attribute attr_unitriangular = pir::BoolAttribute::get(pir::IrContext::Instance(), unitriangular);
  argument.AddAttribute("unitriangular", attr_unitriangular);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;
  paddle::dialect::DenseTensorType y = y_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)y;
  paddle::dialect::DenseTensorType out = out_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)out;
  paddle::dialect::DenseTensorType out_grad = out_grad_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)out_grad;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);

  VLOG(4) << "Builder construction  dense_y";
  paddle::dialect::IrTensor ir_tensor_y(paddle::dialect::TransToPhiDataType(y.dtype()),
                                                      y.dims(),
                                                      y.data_layout(),
                                                      y.lod(),
                                                      y.offset());
  VLOG(4) << "Builder construction  meta_y";
  paddle::dialect::IrMetaTensor meta_y(&ir_tensor_y);
  paddle::dialect::IrTensor dense_x_grad;
  paddle::dialect::IrMetaTensor meta_x_grad(&dense_x_grad);
  paddle::dialect::IrTensor dense_y_grad;
  paddle::dialect::IrMetaTensor meta_y_grad(&dense_y_grad);

  phi::GeneralBinaryGradInferMeta(meta_x, meta_y, &meta_x_grad, &meta_y_grad);

  std::vector<pir::Type> argument_outputs;
  pir::Type x_grad_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_x_grad.dtype()), dense_x_grad.dims(), dense_x_grad.layout(), dense_x_grad.lod(), dense_x_grad.offset());
  argument_outputs.push_back(x_grad_dense_tensor_type);

  pir::Type y_grad_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_y_grad.dtype()), dense_y_grad.dims(), dense_y_grad.layout(), dense_y_grad.lod(), dense_y_grad.offset());
  argument_outputs.push_back(y_grad_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void TriangularSolveGradOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, pir::Value y_, pir::Value out_, pir::Value out_grad_, pir::AttributeMap attributes) {
  VLOG(4) << "Start build TriangularSolveGradOp";


  IR_ENFORCE(
      attributes.find("upper") != attributes.end(),
          "'upper' Attribute is expected for TriangularSolveGradOp. ");
  bool upper = attributes.at("upper").dyn_cast<pir::BoolAttribute>().data();

  IR_ENFORCE(
      attributes.find("transpose") != attributes.end(),
          "'transpose' Attribute is expected for TriangularSolveGradOp. ");
  bool transpose = attributes.at("transpose").dyn_cast<pir::BoolAttribute>().data();

  IR_ENFORCE(
      attributes.find("unitriangular") != attributes.end(),
          "'unitriangular' Attribute is expected for TriangularSolveGradOp. ");
  bool unitriangular = attributes.at("unitriangular").dyn_cast<pir::BoolAttribute>().data();


  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_, y_, out_, out_grad_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_upper = pir::BoolAttribute::get(pir::IrContext::Instance(), upper);
  argument.AddAttribute("upper", attr_upper);
  pir::Attribute attr_transpose = pir::BoolAttribute::get(pir::IrContext::Instance(), transpose);
  argument.AddAttribute("transpose", attr_transpose);
  pir::Attribute attr_unitriangular = pir::BoolAttribute::get(pir::IrContext::Instance(), unitriangular);
  argument.AddAttribute("unitriangular", attr_unitriangular);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;
  paddle::dialect::DenseTensorType y = y_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)y;
  paddle::dialect::DenseTensorType out = out_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)out;
  paddle::dialect::DenseTensorType out_grad = out_grad_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)out_grad;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);

  VLOG(4) << "Builder construction  dense_y";
  paddle::dialect::IrTensor ir_tensor_y(paddle::dialect::TransToPhiDataType(y.dtype()),
                                                      y.dims(),
                                                      y.data_layout(),
                                                      y.lod(),
                                                      y.offset());
  VLOG(4) << "Builder construction  meta_y";
  paddle::dialect::IrMetaTensor meta_y(&ir_tensor_y);
  paddle::dialect::IrTensor dense_x_grad;
  paddle::dialect::IrMetaTensor meta_x_grad(&dense_x_grad);
  paddle::dialect::IrTensor dense_y_grad;
  paddle::dialect::IrMetaTensor meta_y_grad(&dense_y_grad);

  phi::GeneralBinaryGradInferMeta(meta_x, meta_y, &meta_x_grad, &meta_y_grad);

  std::vector<pir::Type> argument_outputs;
  pir::Type x_grad_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_x_grad.dtype()), dense_x_grad.dims(), dense_x_grad.layout(), dense_x_grad.lod(), dense_x_grad.offset());
  argument_outputs.push_back(x_grad_dense_tensor_type);

  pir::Type y_grad_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_y_grad.dtype()), dense_y_grad.dims(), dense_y_grad.layout(), dense_y_grad.lod(), dense_y_grad.offset());
  argument_outputs.push_back(y_grad_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void TriangularSolveGradOp::VerifySig() {}

void TriangularSolveGradOp::InferMeta( phi::InferMetaContext *infer_meta ) {
  auto fn = PD_INFER_META(phi::GeneralBinaryGradInferMeta);
  fn(infer_meta);
}

phi::DataType TriangularSolveGradOp::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: TriangularSolveGradOp";
  


  return expected_kernel_dtype;
}

const char *TrilinearInterpGradOp::attributes_name[8] = { "data_layout", "out_d", "out_h", "out_w", "scale", "interp_method", "align_corners", "align_mode" };

OpInfoTuple TrilinearInterpGradOp::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("x", "paddle::dialect::DenseTensorType", false, true, false, false), paddle::dialect::OpInputInfo("out_size", "paddle::dialect::DenseTensorType", true, false, false, false), paddle::dialect::OpInputInfo("size_tensor", "pir::VectorType<paddle::dialect::DenseTensorType>", true, false, false, false), paddle::dialect::OpInputInfo("scale_tensor", "paddle::dialect::DenseTensorType", true, false, false, false), paddle::dialect::OpInputInfo("output_grad", "paddle::dialect::DenseTensorType", false, false, false, false) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = { paddle::dialect::OpAttributeInfo("data_layout", "pir::StrAttribute", ""), paddle::dialect::OpAttributeInfo("out_d", "pir::Int32Attribute", ""), paddle::dialect::OpAttributeInfo("out_h", "pir::Int32Attribute", ""), paddle::dialect::OpAttributeInfo("out_w", "pir::Int32Attribute", ""), paddle::dialect::OpAttributeInfo("scale", "pir::ArrayAttribute<pir::FloatAttribute>", ""), paddle::dialect::OpAttributeInfo("interp_method", "pir::StrAttribute", ""), paddle::dialect::OpAttributeInfo("align_corners", "pir::BoolAttribute", ""), paddle::dialect::OpAttributeInfo("align_mode", "pir::Int32Attribute", "") };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("x_grad", "paddle::dialect::DenseTensorType", false, false) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("UnchangedInferMeta", {"x"}, "trilinear_interp_grad", {"x", "out_size", "size_tensor", "scale_tensor", "output_grad", "data_layout", "out_d", "out_h", "out_w", "scale", "interp_method", "align_corners", "align_mode"}, {"output_grad"}, {}, {}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "trilinear_interp_grad");
}

void TrilinearInterpGradOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, pir::Value out_size_, pir::Value size_tensor_, pir::Value scale_tensor_, pir::Value output_grad_, const std::string& data_layout, int out_d, int out_h, int out_w, const std::vector<float>& scale, const std::string& interp_method, bool align_corners, int align_mode) {
  VLOG(4) << "Start build TrilinearInterpGradOp";



  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_, out_size_, size_tensor_, scale_tensor_, output_grad_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_data_layout = pir::StrAttribute::get(pir::IrContext::Instance(), data_layout);
  argument.AddAttribute("data_layout", attr_data_layout);
  pir::Attribute attr_out_d = pir::Int32Attribute::get(pir::IrContext::Instance(), out_d);
  argument.AddAttribute("out_d", attr_out_d);
  pir::Attribute attr_out_h = pir::Int32Attribute::get(pir::IrContext::Instance(), out_h);
  argument.AddAttribute("out_h", attr_out_h);
  pir::Attribute attr_out_w = pir::Int32Attribute::get(pir::IrContext::Instance(), out_w);
  argument.AddAttribute("out_w", attr_out_w);
  std::vector<pir::Attribute> vec_scale;
  for (size_t i = 0; i < static_cast<size_t>(scale.size()); i++) {
      pir::Attribute attr_scale = pir::FloatAttribute::get(pir::IrContext::Instance(), scale[i]);

    vec_scale.push_back(attr_scale);
  }
  pir::Attribute attr_scale = pir::ArrayAttribute::get(pir::IrContext::Instance(), vec_scale);
  argument.AddAttribute("scale", attr_scale);
  pir::Attribute attr_interp_method = pir::StrAttribute::get(pir::IrContext::Instance(), interp_method);
  argument.AddAttribute("interp_method", attr_interp_method);
  pir::Attribute attr_align_corners = pir::BoolAttribute::get(pir::IrContext::Instance(), align_corners);
  argument.AddAttribute("align_corners", attr_align_corners);
  pir::Attribute attr_align_mode = pir::Int32Attribute::get(pir::IrContext::Instance(), align_mode);
  argument.AddAttribute("align_mode", attr_align_mode);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;
  paddle::dialect::DenseTensorType output_grad = output_grad_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)output_grad;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);
  paddle::dialect::IrTensor dense_x_grad;
  paddle::dialect::IrMetaTensor meta_x_grad(&dense_x_grad);

  phi::UnchangedInferMeta(meta_x, &meta_x_grad);

  std::vector<pir::Type> argument_outputs;
  pir::Type x_grad_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_x_grad.dtype()), dense_x_grad.dims(), dense_x_grad.layout(), dense_x_grad.lod(), dense_x_grad.offset());
  argument_outputs.push_back(x_grad_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void TrilinearInterpGradOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, pir::Value out_size_, pir::Value size_tensor_, pir::Value scale_tensor_, pir::Value output_grad_, pir::AttributeMap attributes) {
  VLOG(4) << "Start build TrilinearInterpGradOp";


  IR_ENFORCE(
      attributes.find("data_layout") != attributes.end(),
          "'data_layout' Attribute is expected for TrilinearInterpGradOp. ");
  std::string data_layout = attributes.at("data_layout").dyn_cast<pir::StrAttribute>().AsString();

  IR_ENFORCE(
      attributes.find("out_d") != attributes.end(),
          "'out_d' Attribute is expected for TrilinearInterpGradOp. ");
  int out_d = attributes.at("out_d").dyn_cast<pir::Int32Attribute>().data();

  IR_ENFORCE(
      attributes.find("out_h") != attributes.end(),
          "'out_h' Attribute is expected for TrilinearInterpGradOp. ");
  int out_h = attributes.at("out_h").dyn_cast<pir::Int32Attribute>().data();

  IR_ENFORCE(
      attributes.find("out_w") != attributes.end(),
          "'out_w' Attribute is expected for TrilinearInterpGradOp. ");
  int out_w = attributes.at("out_w").dyn_cast<pir::Int32Attribute>().data();

  IR_ENFORCE(
      attributes.find("scale") != attributes.end(),
          "'scale' Attribute is expected for TrilinearInterpGradOp. ");
  std::vector<float> scale;
  for (size_t i = 0; i < attributes.at("scale").dyn_cast<pir::ArrayAttribute>().size(); i++) {
    scale.push_back(attributes.at("scale").dyn_cast<pir::ArrayAttribute>().at(i).dyn_cast<pir::FloatAttribute>().data());
  }

  IR_ENFORCE(
      attributes.find("interp_method") != attributes.end(),
          "'interp_method' Attribute is expected for TrilinearInterpGradOp. ");
  std::string interp_method = attributes.at("interp_method").dyn_cast<pir::StrAttribute>().AsString();

  IR_ENFORCE(
      attributes.find("align_corners") != attributes.end(),
          "'align_corners' Attribute is expected for TrilinearInterpGradOp. ");
  bool align_corners = attributes.at("align_corners").dyn_cast<pir::BoolAttribute>().data();

  IR_ENFORCE(
      attributes.find("align_mode") != attributes.end(),
          "'align_mode' Attribute is expected for TrilinearInterpGradOp. ");
  int align_mode = attributes.at("align_mode").dyn_cast<pir::Int32Attribute>().data();


  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_, out_size_, size_tensor_, scale_tensor_, output_grad_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_data_layout = pir::StrAttribute::get(pir::IrContext::Instance(), data_layout);
  argument.AddAttribute("data_layout", attr_data_layout);
  pir::Attribute attr_out_d = pir::Int32Attribute::get(pir::IrContext::Instance(), out_d);
  argument.AddAttribute("out_d", attr_out_d);
  pir::Attribute attr_out_h = pir::Int32Attribute::get(pir::IrContext::Instance(), out_h);
  argument.AddAttribute("out_h", attr_out_h);
  pir::Attribute attr_out_w = pir::Int32Attribute::get(pir::IrContext::Instance(), out_w);
  argument.AddAttribute("out_w", attr_out_w);
  std::vector<pir::Attribute> vec_scale;
  for (size_t i = 0; i < static_cast<size_t>(scale.size()); i++) {
      pir::Attribute attr_scale = pir::FloatAttribute::get(pir::IrContext::Instance(), scale[i]);

    vec_scale.push_back(attr_scale);
  }
  pir::Attribute attr_scale = pir::ArrayAttribute::get(pir::IrContext::Instance(), vec_scale);
  argument.AddAttribute("scale", attr_scale);
  pir::Attribute attr_interp_method = pir::StrAttribute::get(pir::IrContext::Instance(), interp_method);
  argument.AddAttribute("interp_method", attr_interp_method);
  pir::Attribute attr_align_corners = pir::BoolAttribute::get(pir::IrContext::Instance(), align_corners);
  argument.AddAttribute("align_corners", attr_align_corners);
  pir::Attribute attr_align_mode = pir::Int32Attribute::get(pir::IrContext::Instance(), align_mode);
  argument.AddAttribute("align_mode", attr_align_mode);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;
  paddle::dialect::DenseTensorType output_grad = output_grad_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)output_grad;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);
  paddle::dialect::IrTensor dense_x_grad;
  paddle::dialect::IrMetaTensor meta_x_grad(&dense_x_grad);

  phi::UnchangedInferMeta(meta_x, &meta_x_grad);

  std::vector<pir::Type> argument_outputs;
  pir::Type x_grad_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_x_grad.dtype()), dense_x_grad.dims(), dense_x_grad.layout(), dense_x_grad.lod(), dense_x_grad.offset());
  argument_outputs.push_back(x_grad_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void TrilinearInterpGradOp::VerifySig() {}

void TrilinearInterpGradOp::InferMeta( phi::InferMetaContext *infer_meta ) {
  auto fn = PD_INFER_META(phi::UnchangedInferMeta);
  fn(infer_meta);
}

phi::DataType TrilinearInterpGradOp::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: TrilinearInterpGradOp";
  

  // deal skip data transform
  if (var_name == "out_size" || var_name == "size_tensor" || var_name == "scale_tensor"){
    return expected_kernel_dtype;
  }


  return expected_kernel_dtype;
}

OpInfoTuple TruncGradOp::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("out_grad", "paddle::dialect::DenseTensorType", false, false, false, false) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = {  };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("input_grad", "paddle::dialect::DenseTensorType", false, false) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("UnchangedInferMeta", {"out_grad"}, "trunc_grad", {"out_grad"}, {}, {}, {}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "trunc_grad");
}

void TruncGradOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value out_grad_) {
  VLOG(4) << "Start build TruncGradOp";



  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {out_grad_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType out_grad = out_grad_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)out_grad;

  VLOG(4) << "Builder construction  dense_out_grad";
  paddle::dialect::IrTensor ir_tensor_out_grad(paddle::dialect::TransToPhiDataType(out_grad.dtype()),
                                                      out_grad.dims(),
                                                      out_grad.data_layout(),
                                                      out_grad.lod(),
                                                      out_grad.offset());
  VLOG(4) << "Builder construction  meta_out_grad";
  paddle::dialect::IrMetaTensor meta_out_grad(&ir_tensor_out_grad);
  paddle::dialect::IrTensor dense_input_grad;
  paddle::dialect::IrMetaTensor meta_input_grad(&dense_input_grad);

  phi::UnchangedInferMeta(meta_out_grad, &meta_input_grad);

  std::vector<pir::Type> argument_outputs;
  pir::Type input_grad_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_input_grad.dtype()), dense_input_grad.dims(), dense_input_grad.layout(), dense_input_grad.lod(), dense_input_grad.offset());
  argument_outputs.push_back(input_grad_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void TruncGradOp::VerifySig() {}

void TruncGradOp::InferMeta( phi::InferMetaContext *infer_meta ) {
  auto fn = PD_INFER_META(phi::UnchangedInferMeta);
  fn(infer_meta);
}

phi::DataType TruncGradOp::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: TruncGradOp";
  


  return expected_kernel_dtype;
}

const char *UnbindGradOp::attributes_name[1] = { "axis" };

OpInfoTuple UnbindGradOp::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("out_grad", "pir::VectorType<paddle::dialect::DenseTensorType>", false, false, false, false) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = { paddle::dialect::OpAttributeInfo("axis", "pir::Int32Attribute", "") };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("input_grad", "paddle::dialect::DenseTensorType", false, false) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("", {""}, "", {""}, {}, {}, {}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "unbind_grad");
}

void UnbindGradOp::VerifySig() {}

phi::DataType UnbindGradOp::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: UnbindGradOp";
  


  return expected_kernel_dtype;
}

const char *UnfoldGradOp::attributes_name[4] = { "kernel_sizes", "strides", "paddings", "dilations" };

OpInfoTuple UnfoldGradOp::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("x", "paddle::dialect::DenseTensorType", false, true, false, false), paddle::dialect::OpInputInfo("out_grad", "paddle::dialect::DenseTensorType", false, false, false, false) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = { paddle::dialect::OpAttributeInfo("kernel_sizes", "pir::ArrayAttribute<pir::Int32Attribute>", ""), paddle::dialect::OpAttributeInfo("strides", "pir::ArrayAttribute<pir::Int32Attribute>", ""), paddle::dialect::OpAttributeInfo("paddings", "pir::ArrayAttribute<pir::Int32Attribute>", ""), paddle::dialect::OpAttributeInfo("dilations", "pir::ArrayAttribute<pir::Int32Attribute>", "") };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("x_grad", "paddle::dialect::DenseTensorType", false, false) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("UnchangedInferMeta", {"x"}, "unfold_grad", {"x", "out_grad", "kernel_sizes", "strides", "paddings", "dilations"}, {"out_grad"}, {}, {}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "unfold_grad");
}

void UnfoldGradOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, pir::Value out_grad_, const std::vector<int>& kernel_sizes, const std::vector<int>& strides, const std::vector<int>& paddings, const std::vector<int>& dilations) {
  VLOG(4) << "Start build UnfoldGradOp";



  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_, out_grad_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  std::vector<pir::Attribute> vec_kernel_sizes;
  for (size_t i = 0; i < static_cast<size_t>(kernel_sizes.size()); i++) {
      pir::Attribute attr_kernel_sizes = pir::Int32Attribute::get(pir::IrContext::Instance(), kernel_sizes[i]);

    vec_kernel_sizes.push_back(attr_kernel_sizes);
  }
  pir::Attribute attr_kernel_sizes = pir::ArrayAttribute::get(pir::IrContext::Instance(), vec_kernel_sizes);
  argument.AddAttribute("kernel_sizes", attr_kernel_sizes);
  std::vector<pir::Attribute> vec_strides;
  for (size_t i = 0; i < static_cast<size_t>(strides.size()); i++) {
      pir::Attribute attr_strides = pir::Int32Attribute::get(pir::IrContext::Instance(), strides[i]);

    vec_strides.push_back(attr_strides);
  }
  pir::Attribute attr_strides = pir::ArrayAttribute::get(pir::IrContext::Instance(), vec_strides);
  argument.AddAttribute("strides", attr_strides);
  std::vector<pir::Attribute> vec_paddings;
  for (size_t i = 0; i < static_cast<size_t>(paddings.size()); i++) {
      pir::Attribute attr_paddings = pir::Int32Attribute::get(pir::IrContext::Instance(), paddings[i]);

    vec_paddings.push_back(attr_paddings);
  }
  pir::Attribute attr_paddings = pir::ArrayAttribute::get(pir::IrContext::Instance(), vec_paddings);
  argument.AddAttribute("paddings", attr_paddings);
  std::vector<pir::Attribute> vec_dilations;
  for (size_t i = 0; i < static_cast<size_t>(dilations.size()); i++) {
      pir::Attribute attr_dilations = pir::Int32Attribute::get(pir::IrContext::Instance(), dilations[i]);

    vec_dilations.push_back(attr_dilations);
  }
  pir::Attribute attr_dilations = pir::ArrayAttribute::get(pir::IrContext::Instance(), vec_dilations);
  argument.AddAttribute("dilations", attr_dilations);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;
  paddle::dialect::DenseTensorType out_grad = out_grad_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)out_grad;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);
  paddle::dialect::IrTensor dense_x_grad;
  paddle::dialect::IrMetaTensor meta_x_grad(&dense_x_grad);

  phi::UnchangedInferMeta(meta_x, &meta_x_grad);

  std::vector<pir::Type> argument_outputs;
  pir::Type x_grad_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_x_grad.dtype()), dense_x_grad.dims(), dense_x_grad.layout(), dense_x_grad.lod(), dense_x_grad.offset());
  argument_outputs.push_back(x_grad_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void UnfoldGradOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, pir::Value out_grad_, pir::AttributeMap attributes) {
  VLOG(4) << "Start build UnfoldGradOp";


  IR_ENFORCE(
      attributes.find("kernel_sizes") != attributes.end(),
          "'kernel_sizes' Attribute is expected for UnfoldGradOp. ");
  std::vector<int> kernel_sizes;
  for (size_t i = 0; i < attributes.at("kernel_sizes").dyn_cast<pir::ArrayAttribute>().size(); i++) {
    kernel_sizes.push_back(attributes.at("kernel_sizes").dyn_cast<pir::ArrayAttribute>().at(i).dyn_cast<pir::Int32Attribute>().data());
  }

  IR_ENFORCE(
      attributes.find("strides") != attributes.end(),
          "'strides' Attribute is expected for UnfoldGradOp. ");
  std::vector<int> strides;
  for (size_t i = 0; i < attributes.at("strides").dyn_cast<pir::ArrayAttribute>().size(); i++) {
    strides.push_back(attributes.at("strides").dyn_cast<pir::ArrayAttribute>().at(i).dyn_cast<pir::Int32Attribute>().data());
  }

  IR_ENFORCE(
      attributes.find("paddings") != attributes.end(),
          "'paddings' Attribute is expected for UnfoldGradOp. ");
  std::vector<int> paddings;
  for (size_t i = 0; i < attributes.at("paddings").dyn_cast<pir::ArrayAttribute>().size(); i++) {
    paddings.push_back(attributes.at("paddings").dyn_cast<pir::ArrayAttribute>().at(i).dyn_cast<pir::Int32Attribute>().data());
  }

  IR_ENFORCE(
      attributes.find("dilations") != attributes.end(),
          "'dilations' Attribute is expected for UnfoldGradOp. ");
  std::vector<int> dilations;
  for (size_t i = 0; i < attributes.at("dilations").dyn_cast<pir::ArrayAttribute>().size(); i++) {
    dilations.push_back(attributes.at("dilations").dyn_cast<pir::ArrayAttribute>().at(i).dyn_cast<pir::Int32Attribute>().data());
  }


  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_, out_grad_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  std::vector<pir::Attribute> vec_kernel_sizes;
  for (size_t i = 0; i < static_cast<size_t>(kernel_sizes.size()); i++) {
      pir::Attribute attr_kernel_sizes = pir::Int32Attribute::get(pir::IrContext::Instance(), kernel_sizes[i]);

    vec_kernel_sizes.push_back(attr_kernel_sizes);
  }
  pir::Attribute attr_kernel_sizes = pir::ArrayAttribute::get(pir::IrContext::Instance(), vec_kernel_sizes);
  argument.AddAttribute("kernel_sizes", attr_kernel_sizes);
  std::vector<pir::Attribute> vec_strides;
  for (size_t i = 0; i < static_cast<size_t>(strides.size()); i++) {
      pir::Attribute attr_strides = pir::Int32Attribute::get(pir::IrContext::Instance(), strides[i]);

    vec_strides.push_back(attr_strides);
  }
  pir::Attribute attr_strides = pir::ArrayAttribute::get(pir::IrContext::Instance(), vec_strides);
  argument.AddAttribute("strides", attr_strides);
  std::vector<pir::Attribute> vec_paddings;
  for (size_t i = 0; i < static_cast<size_t>(paddings.size()); i++) {
      pir::Attribute attr_paddings = pir::Int32Attribute::get(pir::IrContext::Instance(), paddings[i]);

    vec_paddings.push_back(attr_paddings);
  }
  pir::Attribute attr_paddings = pir::ArrayAttribute::get(pir::IrContext::Instance(), vec_paddings);
  argument.AddAttribute("paddings", attr_paddings);
  std::vector<pir::Attribute> vec_dilations;
  for (size_t i = 0; i < static_cast<size_t>(dilations.size()); i++) {
      pir::Attribute attr_dilations = pir::Int32Attribute::get(pir::IrContext::Instance(), dilations[i]);

    vec_dilations.push_back(attr_dilations);
  }
  pir::Attribute attr_dilations = pir::ArrayAttribute::get(pir::IrContext::Instance(), vec_dilations);
  argument.AddAttribute("dilations", attr_dilations);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;
  paddle::dialect::DenseTensorType out_grad = out_grad_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)out_grad;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);
  paddle::dialect::IrTensor dense_x_grad;
  paddle::dialect::IrMetaTensor meta_x_grad(&dense_x_grad);

  phi::UnchangedInferMeta(meta_x, &meta_x_grad);

  std::vector<pir::Type> argument_outputs;
  pir::Type x_grad_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_x_grad.dtype()), dense_x_grad.dims(), dense_x_grad.layout(), dense_x_grad.lod(), dense_x_grad.offset());
  argument_outputs.push_back(x_grad_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void UnfoldGradOp::VerifySig() {}

void UnfoldGradOp::InferMeta( phi::InferMetaContext *infer_meta ) {
  auto fn = PD_INFER_META(phi::UnchangedInferMeta);
  fn(infer_meta);
}

phi::DataType UnfoldGradOp::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: UnfoldGradOp";
  


  return expected_kernel_dtype;
}

const char *UniformInplaceGradOp::attributes_name[6] = { "min", "max", "seed", "diag_num", "diag_step", "diag_val" };

OpInfoTuple UniformInplaceGradOp::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("out_grad", "paddle::dialect::DenseTensorType", false, false, false, false) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = { paddle::dialect::OpAttributeInfo("min", "pir::FloatAttribute", ""), paddle::dialect::OpAttributeInfo("max", "pir::FloatAttribute", ""), paddle::dialect::OpAttributeInfo("seed", "pir::Int32Attribute", ""), paddle::dialect::OpAttributeInfo("diag_num", "pir::Int32Attribute", ""), paddle::dialect::OpAttributeInfo("diag_step", "pir::Int32Attribute", ""), paddle::dialect::OpAttributeInfo("diag_val", "pir::FloatAttribute", "") };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("x_grad", "paddle::dialect::DenseTensorType", false, false) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("UniformRandomInplaceGradInferMeta", {"out_grad", "min", "max", "seed", "diag_num", "diag_step", "diag_val"}, "uniform_inplace_grad", {"out_grad", "min", "max", "seed", "diag_num", "diag_step", "diag_val"}, {}, {}, {}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "uniform_inplace_grad");
}

void UniformInplaceGradOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value out_grad_, float min, float max, int seed, int diag_num, int diag_step, float diag_val) {
  VLOG(4) << "Start build UniformInplaceGradOp";



  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {out_grad_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_min = pir::FloatAttribute::get(pir::IrContext::Instance(), min);
  argument.AddAttribute("min", attr_min);
  pir::Attribute attr_max = pir::FloatAttribute::get(pir::IrContext::Instance(), max);
  argument.AddAttribute("max", attr_max);
  pir::Attribute attr_seed = pir::Int32Attribute::get(pir::IrContext::Instance(), seed);
  argument.AddAttribute("seed", attr_seed);
  pir::Attribute attr_diag_num = pir::Int32Attribute::get(pir::IrContext::Instance(), diag_num);
  argument.AddAttribute("diag_num", attr_diag_num);
  pir::Attribute attr_diag_step = pir::Int32Attribute::get(pir::IrContext::Instance(), diag_step);
  argument.AddAttribute("diag_step", attr_diag_step);
  pir::Attribute attr_diag_val = pir::FloatAttribute::get(pir::IrContext::Instance(), diag_val);
  argument.AddAttribute("diag_val", attr_diag_val);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType out_grad = out_grad_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)out_grad;

  VLOG(4) << "Builder construction  dense_out_grad";
  paddle::dialect::IrTensor ir_tensor_out_grad(paddle::dialect::TransToPhiDataType(out_grad.dtype()),
                                                      out_grad.dims(),
                                                      out_grad.data_layout(),
                                                      out_grad.lod(),
                                                      out_grad.offset());
  VLOG(4) << "Builder construction  meta_out_grad";
  paddle::dialect::IrMetaTensor meta_out_grad(&ir_tensor_out_grad);
  paddle::dialect::IrTensor dense_x_grad;
  paddle::dialect::IrMetaTensor meta_x_grad(&dense_x_grad);

  phi::UniformRandomInplaceGradInferMeta(meta_out_grad, min, max, seed, diag_num, diag_step, diag_val, &meta_x_grad);

  std::vector<pir::Type> argument_outputs;
  pir::Type x_grad_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_x_grad.dtype()), dense_x_grad.dims(), dense_x_grad.layout(), dense_x_grad.lod(), dense_x_grad.offset());
  argument_outputs.push_back(x_grad_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void UniformInplaceGradOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value out_grad_, pir::AttributeMap attributes) {
  VLOG(4) << "Start build UniformInplaceGradOp";


  IR_ENFORCE(
      attributes.find("min") != attributes.end(),
          "'min' Attribute is expected for UniformInplaceGradOp. ");
  float min = attributes.at("min").dyn_cast<pir::FloatAttribute>().data();

  IR_ENFORCE(
      attributes.find("max") != attributes.end(),
          "'max' Attribute is expected for UniformInplaceGradOp. ");
  float max = attributes.at("max").dyn_cast<pir::FloatAttribute>().data();

  IR_ENFORCE(
      attributes.find("seed") != attributes.end(),
          "'seed' Attribute is expected for UniformInplaceGradOp. ");
  int seed = attributes.at("seed").dyn_cast<pir::Int32Attribute>().data();

  IR_ENFORCE(
      attributes.find("diag_num") != attributes.end(),
          "'diag_num' Attribute is expected for UniformInplaceGradOp. ");
  int diag_num = attributes.at("diag_num").dyn_cast<pir::Int32Attribute>().data();

  IR_ENFORCE(
      attributes.find("diag_step") != attributes.end(),
          "'diag_step' Attribute is expected for UniformInplaceGradOp. ");
  int diag_step = attributes.at("diag_step").dyn_cast<pir::Int32Attribute>().data();

  IR_ENFORCE(
      attributes.find("diag_val") != attributes.end(),
          "'diag_val' Attribute is expected for UniformInplaceGradOp. ");
  float diag_val = attributes.at("diag_val").dyn_cast<pir::FloatAttribute>().data();


  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {out_grad_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_min = pir::FloatAttribute::get(pir::IrContext::Instance(), min);
  argument.AddAttribute("min", attr_min);
  pir::Attribute attr_max = pir::FloatAttribute::get(pir::IrContext::Instance(), max);
  argument.AddAttribute("max", attr_max);
  pir::Attribute attr_seed = pir::Int32Attribute::get(pir::IrContext::Instance(), seed);
  argument.AddAttribute("seed", attr_seed);
  pir::Attribute attr_diag_num = pir::Int32Attribute::get(pir::IrContext::Instance(), diag_num);
  argument.AddAttribute("diag_num", attr_diag_num);
  pir::Attribute attr_diag_step = pir::Int32Attribute::get(pir::IrContext::Instance(), diag_step);
  argument.AddAttribute("diag_step", attr_diag_step);
  pir::Attribute attr_diag_val = pir::FloatAttribute::get(pir::IrContext::Instance(), diag_val);
  argument.AddAttribute("diag_val", attr_diag_val);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType out_grad = out_grad_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)out_grad;

  VLOG(4) << "Builder construction  dense_out_grad";
  paddle::dialect::IrTensor ir_tensor_out_grad(paddle::dialect::TransToPhiDataType(out_grad.dtype()),
                                                      out_grad.dims(),
                                                      out_grad.data_layout(),
                                                      out_grad.lod(),
                                                      out_grad.offset());
  VLOG(4) << "Builder construction  meta_out_grad";
  paddle::dialect::IrMetaTensor meta_out_grad(&ir_tensor_out_grad);
  paddle::dialect::IrTensor dense_x_grad;
  paddle::dialect::IrMetaTensor meta_x_grad(&dense_x_grad);

  phi::UniformRandomInplaceGradInferMeta(meta_out_grad, min, max, seed, diag_num, diag_step, diag_val, &meta_x_grad);

  std::vector<pir::Type> argument_outputs;
  pir::Type x_grad_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_x_grad.dtype()), dense_x_grad.dims(), dense_x_grad.layout(), dense_x_grad.lod(), dense_x_grad.offset());
  argument_outputs.push_back(x_grad_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void UniformInplaceGradOp::VerifySig() {}

void UniformInplaceGradOp::InferMeta( phi::InferMetaContext *infer_meta ) {
  auto fn = PD_INFER_META(phi::UniformRandomInplaceGradInferMeta);
  fn(infer_meta);
}

phi::DataType UniformInplaceGradOp::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: UniformInplaceGradOp";
  


  return expected_kernel_dtype;
}

const char *UniformInplaceGrad_Op::attributes_name[6] = { "min", "max", "seed", "diag_num", "diag_step", "diag_val" };

OpInfoTuple UniformInplaceGrad_Op::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("out_grad", "paddle::dialect::DenseTensorType", false, false, false, false) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = { paddle::dialect::OpAttributeInfo("min", "pir::FloatAttribute", ""), paddle::dialect::OpAttributeInfo("max", "pir::FloatAttribute", ""), paddle::dialect::OpAttributeInfo("seed", "pir::Int32Attribute", ""), paddle::dialect::OpAttributeInfo("diag_num", "pir::Int32Attribute", ""), paddle::dialect::OpAttributeInfo("diag_step", "pir::Int32Attribute", ""), paddle::dialect::OpAttributeInfo("diag_val", "pir::FloatAttribute", "") };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("x_grad", "paddle::dialect::DenseTensorType", false, false) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("UniformRandomInplaceGradInferMeta", {"out_grad", "min", "max", "seed", "diag_num", "diag_step", "diag_val"}, "uniform_inplace_grad", {"out_grad", "min", "max", "seed", "diag_num", "diag_step", "diag_val"}, {}, {}, {{"x_grad", "out_grad"}}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "uniform_inplace_grad");
}

void UniformInplaceGrad_Op::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value out_grad_, float min, float max, int seed, int diag_num, int diag_step, float diag_val) {
  VLOG(4) << "Start build UniformInplaceGrad_Op";



  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {out_grad_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_min = pir::FloatAttribute::get(pir::IrContext::Instance(), min);
  argument.AddAttribute("min", attr_min);
  pir::Attribute attr_max = pir::FloatAttribute::get(pir::IrContext::Instance(), max);
  argument.AddAttribute("max", attr_max);
  pir::Attribute attr_seed = pir::Int32Attribute::get(pir::IrContext::Instance(), seed);
  argument.AddAttribute("seed", attr_seed);
  pir::Attribute attr_diag_num = pir::Int32Attribute::get(pir::IrContext::Instance(), diag_num);
  argument.AddAttribute("diag_num", attr_diag_num);
  pir::Attribute attr_diag_step = pir::Int32Attribute::get(pir::IrContext::Instance(), diag_step);
  argument.AddAttribute("diag_step", attr_diag_step);
  pir::Attribute attr_diag_val = pir::FloatAttribute::get(pir::IrContext::Instance(), diag_val);
  argument.AddAttribute("diag_val", attr_diag_val);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType out_grad = out_grad_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)out_grad;

  VLOG(4) << "Builder construction  dense_out_grad";
  paddle::dialect::IrTensor ir_tensor_out_grad(paddle::dialect::TransToPhiDataType(out_grad.dtype()),
                                                      out_grad.dims(),
                                                      out_grad.data_layout(),
                                                      out_grad.lod(),
                                                      out_grad.offset());
  VLOG(4) << "Builder construction  meta_out_grad";
  paddle::dialect::IrMetaTensor meta_out_grad(&ir_tensor_out_grad);
  paddle::dialect::IrTensor dense_x_grad;
  paddle::dialect::IrMetaTensor meta_x_grad(&dense_x_grad);

  phi::UniformRandomInplaceGradInferMeta(meta_out_grad, min, max, seed, diag_num, diag_step, diag_val, &meta_x_grad);

  std::vector<pir::Type> argument_outputs;
  pir::Type x_grad_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_x_grad.dtype()), dense_x_grad.dims(), dense_x_grad.layout(), dense_x_grad.lod(), dense_x_grad.offset());
  argument_outputs.push_back(x_grad_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void UniformInplaceGrad_Op::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value out_grad_, pir::AttributeMap attributes) {
  VLOG(4) << "Start build UniformInplaceGrad_Op";


  IR_ENFORCE(
      attributes.find("min") != attributes.end(),
          "'min' Attribute is expected for UniformInplaceGrad_Op. ");
  float min = attributes.at("min").dyn_cast<pir::FloatAttribute>().data();

  IR_ENFORCE(
      attributes.find("max") != attributes.end(),
          "'max' Attribute is expected for UniformInplaceGrad_Op. ");
  float max = attributes.at("max").dyn_cast<pir::FloatAttribute>().data();

  IR_ENFORCE(
      attributes.find("seed") != attributes.end(),
          "'seed' Attribute is expected for UniformInplaceGrad_Op. ");
  int seed = attributes.at("seed").dyn_cast<pir::Int32Attribute>().data();

  IR_ENFORCE(
      attributes.find("diag_num") != attributes.end(),
          "'diag_num' Attribute is expected for UniformInplaceGrad_Op. ");
  int diag_num = attributes.at("diag_num").dyn_cast<pir::Int32Attribute>().data();

  IR_ENFORCE(
      attributes.find("diag_step") != attributes.end(),
          "'diag_step' Attribute is expected for UniformInplaceGrad_Op. ");
  int diag_step = attributes.at("diag_step").dyn_cast<pir::Int32Attribute>().data();

  IR_ENFORCE(
      attributes.find("diag_val") != attributes.end(),
          "'diag_val' Attribute is expected for UniformInplaceGrad_Op. ");
  float diag_val = attributes.at("diag_val").dyn_cast<pir::FloatAttribute>().data();


  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {out_grad_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_min = pir::FloatAttribute::get(pir::IrContext::Instance(), min);
  argument.AddAttribute("min", attr_min);
  pir::Attribute attr_max = pir::FloatAttribute::get(pir::IrContext::Instance(), max);
  argument.AddAttribute("max", attr_max);
  pir::Attribute attr_seed = pir::Int32Attribute::get(pir::IrContext::Instance(), seed);
  argument.AddAttribute("seed", attr_seed);
  pir::Attribute attr_diag_num = pir::Int32Attribute::get(pir::IrContext::Instance(), diag_num);
  argument.AddAttribute("diag_num", attr_diag_num);
  pir::Attribute attr_diag_step = pir::Int32Attribute::get(pir::IrContext::Instance(), diag_step);
  argument.AddAttribute("diag_step", attr_diag_step);
  pir::Attribute attr_diag_val = pir::FloatAttribute::get(pir::IrContext::Instance(), diag_val);
  argument.AddAttribute("diag_val", attr_diag_val);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType out_grad = out_grad_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)out_grad;

  VLOG(4) << "Builder construction  dense_out_grad";
  paddle::dialect::IrTensor ir_tensor_out_grad(paddle::dialect::TransToPhiDataType(out_grad.dtype()),
                                                      out_grad.dims(),
                                                      out_grad.data_layout(),
                                                      out_grad.lod(),
                                                      out_grad.offset());
  VLOG(4) << "Builder construction  meta_out_grad";
  paddle::dialect::IrMetaTensor meta_out_grad(&ir_tensor_out_grad);
  paddle::dialect::IrTensor dense_x_grad;
  paddle::dialect::IrMetaTensor meta_x_grad(&dense_x_grad);

  phi::UniformRandomInplaceGradInferMeta(meta_out_grad, min, max, seed, diag_num, diag_step, diag_val, &meta_x_grad);

  std::vector<pir::Type> argument_outputs;
  pir::Type x_grad_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_x_grad.dtype()), dense_x_grad.dims(), dense_x_grad.layout(), dense_x_grad.lod(), dense_x_grad.offset());
  argument_outputs.push_back(x_grad_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void UniformInplaceGrad_Op::VerifySig() {}

void UniformInplaceGrad_Op::InferMeta( phi::InferMetaContext *infer_meta ) {
  auto fn = PD_INFER_META(phi::UniformRandomInplaceGradInferMeta);
  fn(infer_meta);
}

phi::DataType UniformInplaceGrad_Op::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: UniformInplaceGrad_Op";
  


  return expected_kernel_dtype;
}

OpInfoTuple UnsqueezeDoubleGradOp::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("grad_x_grad", "paddle::dialect::DenseTensorType", false, false, false, false), paddle::dialect::OpInputInfo("axis", "paddle::dialect::IntArrayAttribute", false, false, true, false) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = {  };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("grad_out_grad", "paddle::dialect::DenseTensorType", false, false), paddle::dialect::OpOutputInfo("xshape", "paddle::dialect::DenseTensorType", true, true) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("", {""}, "", {""}, {}, {}, {}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "unsqueeze_double_grad");
}

void UnsqueezeDoubleGradOp::VerifySig() {}

phi::DataType UnsqueezeDoubleGradOp::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: UnsqueezeDoubleGradOp";
  


  return expected_kernel_dtype;
}

OpInfoTuple UnsqueezeGradOp::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("xshape", "paddle::dialect::DenseTensorType", false, false, false, false), paddle::dialect::OpInputInfo("out_grad", "paddle::dialect::DenseTensorType", false, false, false, true), paddle::dialect::OpInputInfo("axis", "paddle::dialect::IntArrayAttribute", false, false, true, false) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = {  };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("x_grad", "paddle::dialect::DenseTensorType", false, false) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("KernelWithXShapeInferMeta", {"xshape", "out_grad"}, "unsqueeze_grad", {"xshape", "out_grad"}, {"out_grad"}, {}, {}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "unsqueeze_grad");
}

void UnsqueezeGradOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value xshape_, pir::Value out_grad_, const std::vector<int64_t>& axis) {
  VLOG(4) << "Start build UnsqueezeGradOp";


  // Generate int_array mutable attribute: axis
  paddle::dialect::FullIntArrayOp full_axis_op = builder.Build<paddle::dialect::FullIntArrayOp>(axis, phi::DataType::INT64, phi::CPUPlace());
  pir::OpResult axis_ = full_axis_op->result(0);
    
  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {xshape_, out_grad_, axis_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType xshape = xshape_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)xshape;
  paddle::dialect::DenseTensorType out_grad = out_grad_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)out_grad;

  VLOG(4) << "Builder construction  dense_xshape";
  paddle::dialect::IrTensor ir_tensor_xshape(paddle::dialect::TransToPhiDataType(xshape.dtype()),
                                                      xshape.dims(),
                                                      xshape.data_layout(),
                                                      xshape.lod(),
                                                      xshape.offset());
  VLOG(4) << "Builder construction  meta_xshape";
  paddle::dialect::IrMetaTensor meta_xshape(&ir_tensor_xshape);

  VLOG(4) << "Builder construction  dense_out_grad";
  paddle::dialect::IrTensor ir_tensor_out_grad(paddle::dialect::TransToPhiDataType(out_grad.dtype()),
                                                      out_grad.dims(),
                                                      out_grad.data_layout(),
                                                      out_grad.lod(),
                                                      out_grad.offset());
  VLOG(4) << "Builder construction  meta_out_grad";
  paddle::dialect::IrMetaTensor meta_out_grad(&ir_tensor_out_grad);
  paddle::dialect::IrTensor dense_x_grad;
  paddle::dialect::IrMetaTensor meta_x_grad(&dense_x_grad);

  phi::KernelWithXShapeInferMeta(meta_xshape, meta_out_grad, &meta_x_grad);

  std::vector<pir::Type> argument_outputs;
  pir::Type x_grad_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_x_grad.dtype()), dense_x_grad.dims(), dense_x_grad.layout(), dense_x_grad.lod(), dense_x_grad.offset());
  argument_outputs.push_back(x_grad_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void UnsqueezeGradOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value xshape_, pir::Value out_grad_, pir::AttributeMap attributes) {
  VLOG(4) << "Start build UnsqueezeGradOp";


  IR_ENFORCE(
      attributes.find("axis") != attributes.end(),
          "'axis' Attribute is expected for UnsqueezeGradOp. ");
  std::vector<int64_t> axis = attributes.at("axis").dyn_cast<paddle::dialect::IntArrayAttribute>().data().GetData();

  // Generate int_array mutable attribute: axis
  paddle::dialect::FullIntArrayOp full_axis_op = builder.Build<paddle::dialect::FullIntArrayOp>(axis, phi::DataType::INT64, phi::CPUPlace());
  pir::OpResult axis_ = full_axis_op->result(0);
    
  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {xshape_, out_grad_, axis_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType xshape = xshape_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)xshape;
  paddle::dialect::DenseTensorType out_grad = out_grad_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)out_grad;

  VLOG(4) << "Builder construction  dense_xshape";
  paddle::dialect::IrTensor ir_tensor_xshape(paddle::dialect::TransToPhiDataType(xshape.dtype()),
                                                      xshape.dims(),
                                                      xshape.data_layout(),
                                                      xshape.lod(),
                                                      xshape.offset());
  VLOG(4) << "Builder construction  meta_xshape";
  paddle::dialect::IrMetaTensor meta_xshape(&ir_tensor_xshape);

  VLOG(4) << "Builder construction  dense_out_grad";
  paddle::dialect::IrTensor ir_tensor_out_grad(paddle::dialect::TransToPhiDataType(out_grad.dtype()),
                                                      out_grad.dims(),
                                                      out_grad.data_layout(),
                                                      out_grad.lod(),
                                                      out_grad.offset());
  VLOG(4) << "Builder construction  meta_out_grad";
  paddle::dialect::IrMetaTensor meta_out_grad(&ir_tensor_out_grad);
  paddle::dialect::IrTensor dense_x_grad;
  paddle::dialect::IrMetaTensor meta_x_grad(&dense_x_grad);

  phi::KernelWithXShapeInferMeta(meta_xshape, meta_out_grad, &meta_x_grad);

  std::vector<pir::Type> argument_outputs;
  pir::Type x_grad_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_x_grad.dtype()), dense_x_grad.dims(), dense_x_grad.layout(), dense_x_grad.lod(), dense_x_grad.offset());
  argument_outputs.push_back(x_grad_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void UnsqueezeGradOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value xshape_, pir::Value out_grad_, pir::Value axis_) {
  VLOG(4) << "Start build UnsqueezeGradOp";



  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {xshape_, out_grad_, axis_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType xshape = xshape_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)xshape;
  paddle::dialect::DenseTensorType out_grad = out_grad_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)out_grad;
  phi::IntArray axis;
  if (axis_.dyn_cast<pir::OpResult>() && axis_.dyn_cast<pir::OpResult>().owner()->isa<paddle::dialect::FullIntArrayOp>()) {
    axis = std::move(phi::IntArray(paddle::dialect::GetInt64Vector(
                          axis_.dyn_cast<pir::OpResult>().owner()
                          ->dyn_cast<paddle::dialect::FullIntArrayOp>()
                          .attribute("value"))));
  } else if (axis_.type().isa<pir::VectorType>()) {
    size_t axis_size = axis_.type().dyn_cast<pir::VectorType>().size();
    axis = std::move(phi::IntArray(std::vector<int64_t>(axis_size, -1)));
    axis.SetFromTensor(true);
  } else if (axis_.type().isa<paddle::dialect::DenseTensorType>()) {
    common::DDim axis_dim = axis_.type().dyn_cast<paddle::dialect::DenseTensorType>().dims();
    size_t axis_size = common::product(axis_dim);
    if (common::contain_unknown_dim(axis_dim)) {
      axis_size = 1;
    }
    axis = std::move(phi::IntArray(std::vector<int64_t>(axis_size, -1)));
    axis.SetFromTensor(true);
  } else {
    PADDLE_THROW(phi::errors::Unimplemented("Only support VectorType or DenseTensorType"));
  }


  VLOG(4) << "Builder construction  dense_xshape";
  paddle::dialect::IrTensor ir_tensor_xshape(paddle::dialect::TransToPhiDataType(xshape.dtype()),
                                                      xshape.dims(),
                                                      xshape.data_layout(),
                                                      xshape.lod(),
                                                      xshape.offset());
  VLOG(4) << "Builder construction  meta_xshape";
  paddle::dialect::IrMetaTensor meta_xshape(&ir_tensor_xshape);

  VLOG(4) << "Builder construction  dense_out_grad";
  paddle::dialect::IrTensor ir_tensor_out_grad(paddle::dialect::TransToPhiDataType(out_grad.dtype()),
                                                      out_grad.dims(),
                                                      out_grad.data_layout(),
                                                      out_grad.lod(),
                                                      out_grad.offset());
  VLOG(4) << "Builder construction  meta_out_grad";
  paddle::dialect::IrMetaTensor meta_out_grad(&ir_tensor_out_grad);
  paddle::dialect::IrTensor dense_x_grad;
  paddle::dialect::IrMetaTensor meta_x_grad(&dense_x_grad);

  phi::KernelWithXShapeInferMeta(meta_xshape, meta_out_grad, &meta_x_grad);

  std::vector<pir::Type> argument_outputs;
  pir::Type x_grad_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_x_grad.dtype()), dense_x_grad.dims(), dense_x_grad.layout(), dense_x_grad.lod(), dense_x_grad.offset());
  argument_outputs.push_back(x_grad_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void UnsqueezeGradOp::VerifySig() {}

void UnsqueezeGradOp::InferMeta( phi::InferMetaContext *infer_meta ) {
  auto fn = PD_INFER_META(phi::KernelWithXShapeInferMeta);
  fn(infer_meta);
}

phi::DataType UnsqueezeGradOp::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: UnsqueezeGradOp";
  


  return expected_kernel_dtype;
}

OpInfoTuple UnsqueezeGrad_Op::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("xshape", "paddle::dialect::DenseTensorType", false, false, false, false), paddle::dialect::OpInputInfo("out_grad", "paddle::dialect::DenseTensorType", false, false, false, true), paddle::dialect::OpInputInfo("axis", "paddle::dialect::IntArrayAttribute", false, false, true, false) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = {  };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("x_grad", "paddle::dialect::DenseTensorType", false, false) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("KernelWithXShapeInferMeta", {"xshape", "out_grad"}, "unsqueeze_grad", {"xshape", "out_grad"}, {"out_grad"}, {}, {{"x_grad", "out_grad"}}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "unsqueeze_grad");
}

void UnsqueezeGrad_Op::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value xshape_, pir::Value out_grad_, const std::vector<int64_t>& axis) {
  VLOG(4) << "Start build UnsqueezeGrad_Op";


  // Generate int_array mutable attribute: axis
  paddle::dialect::FullIntArrayOp full_axis_op = builder.Build<paddle::dialect::FullIntArrayOp>(axis, phi::DataType::INT64, phi::CPUPlace());
  pir::OpResult axis_ = full_axis_op->result(0);
    
  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {xshape_, out_grad_, axis_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType xshape = xshape_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)xshape;
  paddle::dialect::DenseTensorType out_grad = out_grad_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)out_grad;

  VLOG(4) << "Builder construction  dense_xshape";
  paddle::dialect::IrTensor ir_tensor_xshape(paddle::dialect::TransToPhiDataType(xshape.dtype()),
                                                      xshape.dims(),
                                                      xshape.data_layout(),
                                                      xshape.lod(),
                                                      xshape.offset());
  VLOG(4) << "Builder construction  meta_xshape";
  paddle::dialect::IrMetaTensor meta_xshape(&ir_tensor_xshape);

  VLOG(4) << "Builder construction  dense_out_grad";
  paddle::dialect::IrTensor ir_tensor_out_grad(paddle::dialect::TransToPhiDataType(out_grad.dtype()),
                                                      out_grad.dims(),
                                                      out_grad.data_layout(),
                                                      out_grad.lod(),
                                                      out_grad.offset());
  VLOG(4) << "Builder construction  meta_out_grad";
  paddle::dialect::IrMetaTensor meta_out_grad(&ir_tensor_out_grad);
  paddle::dialect::IrTensor dense_x_grad;
  paddle::dialect::IrMetaTensor meta_x_grad(&dense_x_grad);

  phi::KernelWithXShapeInferMeta(meta_xshape, meta_out_grad, &meta_x_grad);

  std::vector<pir::Type> argument_outputs;
  pir::Type x_grad_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_x_grad.dtype()), dense_x_grad.dims(), dense_x_grad.layout(), dense_x_grad.lod(), dense_x_grad.offset());
  argument_outputs.push_back(x_grad_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void UnsqueezeGrad_Op::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value xshape_, pir::Value out_grad_, pir::AttributeMap attributes) {
  VLOG(4) << "Start build UnsqueezeGrad_Op";


  IR_ENFORCE(
      attributes.find("axis") != attributes.end(),
          "'axis' Attribute is expected for UnsqueezeGrad_Op. ");
  std::vector<int64_t> axis = attributes.at("axis").dyn_cast<paddle::dialect::IntArrayAttribute>().data().GetData();

  // Generate int_array mutable attribute: axis
  paddle::dialect::FullIntArrayOp full_axis_op = builder.Build<paddle::dialect::FullIntArrayOp>(axis, phi::DataType::INT64, phi::CPUPlace());
  pir::OpResult axis_ = full_axis_op->result(0);
    
  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {xshape_, out_grad_, axis_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType xshape = xshape_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)xshape;
  paddle::dialect::DenseTensorType out_grad = out_grad_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)out_grad;

  VLOG(4) << "Builder construction  dense_xshape";
  paddle::dialect::IrTensor ir_tensor_xshape(paddle::dialect::TransToPhiDataType(xshape.dtype()),
                                                      xshape.dims(),
                                                      xshape.data_layout(),
                                                      xshape.lod(),
                                                      xshape.offset());
  VLOG(4) << "Builder construction  meta_xshape";
  paddle::dialect::IrMetaTensor meta_xshape(&ir_tensor_xshape);

  VLOG(4) << "Builder construction  dense_out_grad";
  paddle::dialect::IrTensor ir_tensor_out_grad(paddle::dialect::TransToPhiDataType(out_grad.dtype()),
                                                      out_grad.dims(),
                                                      out_grad.data_layout(),
                                                      out_grad.lod(),
                                                      out_grad.offset());
  VLOG(4) << "Builder construction  meta_out_grad";
  paddle::dialect::IrMetaTensor meta_out_grad(&ir_tensor_out_grad);
  paddle::dialect::IrTensor dense_x_grad;
  paddle::dialect::IrMetaTensor meta_x_grad(&dense_x_grad);

  phi::KernelWithXShapeInferMeta(meta_xshape, meta_out_grad, &meta_x_grad);

  std::vector<pir::Type> argument_outputs;
  pir::Type x_grad_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_x_grad.dtype()), dense_x_grad.dims(), dense_x_grad.layout(), dense_x_grad.lod(), dense_x_grad.offset());
  argument_outputs.push_back(x_grad_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void UnsqueezeGrad_Op::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value xshape_, pir::Value out_grad_, pir::Value axis_) {
  VLOG(4) << "Start build UnsqueezeGrad_Op";



  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {xshape_, out_grad_, axis_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType xshape = xshape_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)xshape;
  paddle::dialect::DenseTensorType out_grad = out_grad_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)out_grad;
  phi::IntArray axis;
  if (axis_.dyn_cast<pir::OpResult>() && axis_.dyn_cast<pir::OpResult>().owner()->isa<paddle::dialect::FullIntArrayOp>()) {
    axis = std::move(phi::IntArray(paddle::dialect::GetInt64Vector(
                          axis_.dyn_cast<pir::OpResult>().owner()
                          ->dyn_cast<paddle::dialect::FullIntArrayOp>()
                          .attribute("value"))));
  } else if (axis_.type().isa<pir::VectorType>()) {
    size_t axis_size = axis_.type().dyn_cast<pir::VectorType>().size();
    axis = std::move(phi::IntArray(std::vector<int64_t>(axis_size, -1)));
    axis.SetFromTensor(true);
  } else if (axis_.type().isa<paddle::dialect::DenseTensorType>()) {
    common::DDim axis_dim = axis_.type().dyn_cast<paddle::dialect::DenseTensorType>().dims();
    size_t axis_size = common::product(axis_dim);
    if (common::contain_unknown_dim(axis_dim)) {
      axis_size = 1;
    }
    axis = std::move(phi::IntArray(std::vector<int64_t>(axis_size, -1)));
    axis.SetFromTensor(true);
  } else {
    PADDLE_THROW(phi::errors::Unimplemented("Only support VectorType or DenseTensorType"));
  }


  VLOG(4) << "Builder construction  dense_xshape";
  paddle::dialect::IrTensor ir_tensor_xshape(paddle::dialect::TransToPhiDataType(xshape.dtype()),
                                                      xshape.dims(),
                                                      xshape.data_layout(),
                                                      xshape.lod(),
                                                      xshape.offset());
  VLOG(4) << "Builder construction  meta_xshape";
  paddle::dialect::IrMetaTensor meta_xshape(&ir_tensor_xshape);

  VLOG(4) << "Builder construction  dense_out_grad";
  paddle::dialect::IrTensor ir_tensor_out_grad(paddle::dialect::TransToPhiDataType(out_grad.dtype()),
                                                      out_grad.dims(),
                                                      out_grad.data_layout(),
                                                      out_grad.lod(),
                                                      out_grad.offset());
  VLOG(4) << "Builder construction  meta_out_grad";
  paddle::dialect::IrMetaTensor meta_out_grad(&ir_tensor_out_grad);
  paddle::dialect::IrTensor dense_x_grad;
  paddle::dialect::IrMetaTensor meta_x_grad(&dense_x_grad);

  phi::KernelWithXShapeInferMeta(meta_xshape, meta_out_grad, &meta_x_grad);

  std::vector<pir::Type> argument_outputs;
  pir::Type x_grad_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_x_grad.dtype()), dense_x_grad.dims(), dense_x_grad.layout(), dense_x_grad.lod(), dense_x_grad.offset());
  argument_outputs.push_back(x_grad_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void UnsqueezeGrad_Op::VerifySig() {}

void UnsqueezeGrad_Op::InferMeta( phi::InferMetaContext *infer_meta ) {
  auto fn = PD_INFER_META(phi::KernelWithXShapeInferMeta);
  fn(infer_meta);
}

phi::DataType UnsqueezeGrad_Op::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: UnsqueezeGrad_Op";
  


  return expected_kernel_dtype;
}

const char *UnstackGradOp::attributes_name[1] = { "axis" };

OpInfoTuple UnstackGradOp::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("out_grad", "pir::VectorType<paddle::dialect::DenseTensorType>", false, false, false, false) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = { paddle::dialect::OpAttributeInfo("axis", "pir::Int32Attribute", "") };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("x_grad", "paddle::dialect::DenseTensorType", false, false) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("UnStackGradInferMeta", {"out_grad", "axis"}, "unstack_grad", {"out_grad", "axis"}, {}, {}, {}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "unstack_grad");
}

void UnstackGradOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value out_grad_, int axis) {
  VLOG(4) << "Start build UnstackGradOp";



  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {out_grad_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_axis = pir::Int32Attribute::get(pir::IrContext::Instance(), axis);
  argument.AddAttribute("axis", attr_axis);

  VLOG(4) << "Builder construction outputs";
  pir::VectorType out_grad = out_grad_.type().dyn_cast<pir::VectorType>(); (void)out_grad;
  std::vector<paddle::dialect::IrTensor> vec_ir_tensor_out_grad;
  for (size_t i=0; i < static_cast<size_t>(out_grad.size()); i++) {
    vec_ir_tensor_out_grad.push_back(paddle::dialect::IrTensor(paddle::dialect::TransToPhiDataType(out_grad[i].dyn_cast<paddle::dialect::DenseTensorType>().dtype()),
                                                                     out_grad[i].dyn_cast<paddle::dialect::DenseTensorType>().dims(),
                                                                     out_grad[i].dyn_cast<paddle::dialect::DenseTensorType>().data_layout(),
                                                                     out_grad[i].dyn_cast<paddle::dialect::DenseTensorType>().lod(),
                                                                     out_grad[i].dyn_cast<paddle::dialect::DenseTensorType>().offset()));
  }
  std::vector<paddle::dialect::IrMetaTensor> vec_meta_out_grad;
  for (size_t i=0; i < vec_ir_tensor_out_grad.size(); i++) {
    vec_meta_out_grad.push_back(paddle::dialect::IrMetaTensor(&vec_ir_tensor_out_grad[i]));
  }

  std::vector<const phi::MetaTensor*> meta_out_grad;
  for (size_t i=0; i < static_cast<size_t>(vec_meta_out_grad.size()); i++) {
    meta_out_grad.push_back(&vec_meta_out_grad[i]);
  }
   paddle::dialect::IrTensor dense_x_grad;
  paddle::dialect::IrMetaTensor meta_x_grad(&dense_x_grad);

  phi::UnStackGradInferMeta(meta_out_grad, axis, &meta_x_grad);

  std::vector<pir::Type> argument_outputs;
  pir::Type x_grad_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_x_grad.dtype()), dense_x_grad.dims(), dense_x_grad.layout(), dense_x_grad.lod(), dense_x_grad.offset());
  argument_outputs.push_back(x_grad_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void UnstackGradOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value out_grad_, pir::AttributeMap attributes) {
  VLOG(4) << "Start build UnstackGradOp";


  IR_ENFORCE(
      attributes.find("axis") != attributes.end(),
          "'axis' Attribute is expected for UnstackGradOp. ");
  int axis = attributes.at("axis").dyn_cast<pir::Int32Attribute>().data();


  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {out_grad_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_axis = pir::Int32Attribute::get(pir::IrContext::Instance(), axis);
  argument.AddAttribute("axis", attr_axis);

  VLOG(4) << "Builder construction outputs";
  pir::VectorType out_grad = out_grad_.type().dyn_cast<pir::VectorType>(); (void)out_grad;
  std::vector<paddle::dialect::IrTensor> vec_ir_tensor_out_grad;
  for (size_t i=0; i < static_cast<size_t>(out_grad.size()); i++) {
    vec_ir_tensor_out_grad.push_back(paddle::dialect::IrTensor(paddle::dialect::TransToPhiDataType(out_grad[i].dyn_cast<paddle::dialect::DenseTensorType>().dtype()),
                                                                     out_grad[i].dyn_cast<paddle::dialect::DenseTensorType>().dims(),
                                                                     out_grad[i].dyn_cast<paddle::dialect::DenseTensorType>().data_layout(),
                                                                     out_grad[i].dyn_cast<paddle::dialect::DenseTensorType>().lod(),
                                                                     out_grad[i].dyn_cast<paddle::dialect::DenseTensorType>().offset()));
  }
  std::vector<paddle::dialect::IrMetaTensor> vec_meta_out_grad;
  for (size_t i=0; i < vec_ir_tensor_out_grad.size(); i++) {
    vec_meta_out_grad.push_back(paddle::dialect::IrMetaTensor(&vec_ir_tensor_out_grad[i]));
  }

  std::vector<const phi::MetaTensor*> meta_out_grad;
  for (size_t i=0; i < static_cast<size_t>(vec_meta_out_grad.size()); i++) {
    meta_out_grad.push_back(&vec_meta_out_grad[i]);
  }
   paddle::dialect::IrTensor dense_x_grad;
  paddle::dialect::IrMetaTensor meta_x_grad(&dense_x_grad);

  phi::UnStackGradInferMeta(meta_out_grad, axis, &meta_x_grad);

  std::vector<pir::Type> argument_outputs;
  pir::Type x_grad_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_x_grad.dtype()), dense_x_grad.dims(), dense_x_grad.layout(), dense_x_grad.lod(), dense_x_grad.offset());
  argument_outputs.push_back(x_grad_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void UnstackGradOp::VerifySig() {}

void UnstackGradOp::InferMeta( phi::InferMetaContext *infer_meta ) {
  auto fn = PD_INFER_META(phi::UnStackGradInferMeta);
  fn(infer_meta);
}

phi::DataType UnstackGradOp::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: UnstackGradOp";
  


  return expected_kernel_dtype;
}

const char *ViewDtypeGradOp::attributes_name[1] = { "dtype" };

OpInfoTuple ViewDtypeGradOp::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("input", "paddle::dialect::DenseTensorType", false, false, false, false), paddle::dialect::OpInputInfo("out_grad", "paddle::dialect::DenseTensorType", false, false, false, false) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = { paddle::dialect::OpAttributeInfo("dtype", "paddle::dialect::DataTypeAttribute", "") };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("input_grad", "paddle::dialect::DenseTensorType", false, false) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("StridedUnChangedInferMeta", {"input"}, "view_dtype_grad", {"input", "out_grad", "dtype"}, {"out_grad"}, {}, {}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "view_dtype_grad");
}

void ViewDtypeGradOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value input_, pir::Value out_grad_, phi::DataType dtype) {
  VLOG(4) << "Start build ViewDtypeGradOp";



  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {input_, out_grad_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_dtype = paddle::dialect::DataTypeAttribute::get(pir::IrContext::Instance(), dtype);
  argument.AddAttribute("dtype", attr_dtype);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType input = input_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)input;
  paddle::dialect::DenseTensorType out_grad = out_grad_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)out_grad;

  VLOG(4) << "Builder construction  dense_input";
  paddle::dialect::IrTensor ir_tensor_input(paddle::dialect::TransToPhiDataType(input.dtype()),
                                                      input.dims(),
                                                      input.data_layout(),
                                                      input.lod(),
                                                      input.offset());
  VLOG(4) << "Builder construction  meta_input";
  paddle::dialect::IrMetaTensor meta_input(&ir_tensor_input);
  paddle::dialect::IrTensor dense_input_grad;
  paddle::dialect::IrMetaTensor meta_input_grad(&dense_input_grad);

  phi::StridedUnChangedInferMeta(meta_input, &meta_input_grad);

  std::vector<pir::Type> argument_outputs;
  pir::Type input_grad_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_input_grad.dtype()), dense_input_grad.dims(), dense_input_grad.layout(), dense_input_grad.lod(), dense_input_grad.offset());
  argument_outputs.push_back(input_grad_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void ViewDtypeGradOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value input_, pir::Value out_grad_, pir::AttributeMap attributes) {
  VLOG(4) << "Start build ViewDtypeGradOp";


  IR_ENFORCE(
      attributes.find("dtype") != attributes.end(),
          "'dtype' Attribute is expected for ViewDtypeGradOp. ");
  phi::DataType dtype = attributes.at("dtype").dyn_cast<paddle::dialect::DataTypeAttribute>().data();


  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {input_, out_grad_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_dtype = paddle::dialect::DataTypeAttribute::get(pir::IrContext::Instance(), dtype);
  argument.AddAttribute("dtype", attr_dtype);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType input = input_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)input;
  paddle::dialect::DenseTensorType out_grad = out_grad_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)out_grad;

  VLOG(4) << "Builder construction  dense_input";
  paddle::dialect::IrTensor ir_tensor_input(paddle::dialect::TransToPhiDataType(input.dtype()),
                                                      input.dims(),
                                                      input.data_layout(),
                                                      input.lod(),
                                                      input.offset());
  VLOG(4) << "Builder construction  meta_input";
  paddle::dialect::IrMetaTensor meta_input(&ir_tensor_input);
  paddle::dialect::IrTensor dense_input_grad;
  paddle::dialect::IrMetaTensor meta_input_grad(&dense_input_grad);

  phi::StridedUnChangedInferMeta(meta_input, &meta_input_grad);

  std::vector<pir::Type> argument_outputs;
  pir::Type input_grad_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_input_grad.dtype()), dense_input_grad.dims(), dense_input_grad.layout(), dense_input_grad.lod(), dense_input_grad.offset());
  argument_outputs.push_back(input_grad_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void ViewDtypeGradOp::VerifySig() {}

void ViewDtypeGradOp::InferMeta( phi::InferMetaContext *infer_meta ) {
  auto fn = PD_INFER_META(phi::StridedUnChangedInferMeta);
  fn(infer_meta);
}

phi::DataType ViewDtypeGradOp::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: ViewDtypeGradOp";
  


  return expected_kernel_dtype;
}

const char *ViewShapeGradOp::attributes_name[1] = { "dims" };

OpInfoTuple ViewShapeGradOp::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("input", "paddle::dialect::DenseTensorType", false, false, false, false), paddle::dialect::OpInputInfo("out_grad", "paddle::dialect::DenseTensorType", false, false, false, false) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = { paddle::dialect::OpAttributeInfo("dims", "pir::ArrayAttribute<pir::Int64Attribute>", "") };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("input_grad", "paddle::dialect::DenseTensorType", false, false) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("StridedUnChangedInferMeta", {"input"}, "view_shape_grad", {"input", "out_grad", "dims"}, {}, {}, {}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "view_shape_grad");
}

void ViewShapeGradOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value input_, pir::Value out_grad_, const std::vector<int64_t>& dims) {
  VLOG(4) << "Start build ViewShapeGradOp";



  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {input_, out_grad_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  std::vector<pir::Attribute> vec_dims;
  for (size_t i = 0; i < static_cast<size_t>(dims.size()); i++) {
      pir::Attribute attr_dims = pir::Int64Attribute::get(pir::IrContext::Instance(), dims[i]);

    vec_dims.push_back(attr_dims);
  }
  pir::Attribute attr_dims = pir::ArrayAttribute::get(pir::IrContext::Instance(), vec_dims);
  argument.AddAttribute("dims", attr_dims);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType input = input_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)input;
  paddle::dialect::DenseTensorType out_grad = out_grad_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)out_grad;

  VLOG(4) << "Builder construction  dense_input";
  paddle::dialect::IrTensor ir_tensor_input(paddle::dialect::TransToPhiDataType(input.dtype()),
                                                      input.dims(),
                                                      input.data_layout(),
                                                      input.lod(),
                                                      input.offset());
  VLOG(4) << "Builder construction  meta_input";
  paddle::dialect::IrMetaTensor meta_input(&ir_tensor_input);
  paddle::dialect::IrTensor dense_input_grad;
  paddle::dialect::IrMetaTensor meta_input_grad(&dense_input_grad);

  phi::StridedUnChangedInferMeta(meta_input, &meta_input_grad);

  std::vector<pir::Type> argument_outputs;
  pir::Type input_grad_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_input_grad.dtype()), dense_input_grad.dims(), dense_input_grad.layout(), dense_input_grad.lod(), dense_input_grad.offset());
  argument_outputs.push_back(input_grad_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void ViewShapeGradOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value input_, pir::Value out_grad_, pir::AttributeMap attributes) {
  VLOG(4) << "Start build ViewShapeGradOp";


  IR_ENFORCE(
      attributes.find("dims") != attributes.end(),
          "'dims' Attribute is expected for ViewShapeGradOp. ");
  std::vector<int64_t> dims;
  for (size_t i = 0; i < attributes.at("dims").dyn_cast<pir::ArrayAttribute>().size(); i++) {
    dims.push_back(attributes.at("dims").dyn_cast<pir::ArrayAttribute>().at(i).dyn_cast<pir::Int64Attribute>().data());
  }


  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {input_, out_grad_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  std::vector<pir::Attribute> vec_dims;
  for (size_t i = 0; i < static_cast<size_t>(dims.size()); i++) {
      pir::Attribute attr_dims = pir::Int64Attribute::get(pir::IrContext::Instance(), dims[i]);

    vec_dims.push_back(attr_dims);
  }
  pir::Attribute attr_dims = pir::ArrayAttribute::get(pir::IrContext::Instance(), vec_dims);
  argument.AddAttribute("dims", attr_dims);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType input = input_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)input;
  paddle::dialect::DenseTensorType out_grad = out_grad_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)out_grad;

  VLOG(4) << "Builder construction  dense_input";
  paddle::dialect::IrTensor ir_tensor_input(paddle::dialect::TransToPhiDataType(input.dtype()),
                                                      input.dims(),
                                                      input.data_layout(),
                                                      input.lod(),
                                                      input.offset());
  VLOG(4) << "Builder construction  meta_input";
  paddle::dialect::IrMetaTensor meta_input(&ir_tensor_input);
  paddle::dialect::IrTensor dense_input_grad;
  paddle::dialect::IrMetaTensor meta_input_grad(&dense_input_grad);

  phi::StridedUnChangedInferMeta(meta_input, &meta_input_grad);

  std::vector<pir::Type> argument_outputs;
  pir::Type input_grad_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_input_grad.dtype()), dense_input_grad.dims(), dense_input_grad.layout(), dense_input_grad.lod(), dense_input_grad.offset());
  argument_outputs.push_back(input_grad_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void ViewShapeGradOp::VerifySig() {}

void ViewShapeGradOp::InferMeta( phi::InferMetaContext *infer_meta ) {
  auto fn = PD_INFER_META(phi::StridedUnChangedInferMeta);
  fn(infer_meta);
}

phi::DataType ViewShapeGradOp::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: ViewShapeGradOp";
  


  return expected_kernel_dtype;
}

const char *WarpctcGradOp::attributes_name[2] = { "blank", "norm_by_times" };

OpInfoTuple WarpctcGradOp::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("logits", "paddle::dialect::DenseTensorType", false, true, false, false), paddle::dialect::OpInputInfo("logits_length", "paddle::dialect::DenseTensorType", true, false, false, false), paddle::dialect::OpInputInfo("warpctcgrad", "paddle::dialect::DenseTensorType", false, false, false, false), paddle::dialect::OpInputInfo("loss_grad", "paddle::dialect::DenseTensorType", false, false, false, false) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = { paddle::dialect::OpAttributeInfo("blank", "pir::Int32Attribute", ""), paddle::dialect::OpAttributeInfo("norm_by_times", "pir::BoolAttribute", "") };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("logits_grad", "paddle::dialect::DenseTensorType", false, false) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("UnchangedInferMeta", {"logits"}, "warpctc_grad", {"logits", "logits_length", "warpctcgrad", "loss_grad", "blank", "norm_by_times"}, {"loss_grad"}, {}, {}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "warpctc_grad");
}

void WarpctcGradOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value logits_, pir::Value logits_length_, pir::Value warpctcgrad_, pir::Value loss_grad_, int blank, bool norm_by_times) {
  VLOG(4) << "Start build WarpctcGradOp";



  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {logits_, logits_length_, warpctcgrad_, loss_grad_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_blank = pir::Int32Attribute::get(pir::IrContext::Instance(), blank);
  argument.AddAttribute("blank", attr_blank);
  pir::Attribute attr_norm_by_times = pir::BoolAttribute::get(pir::IrContext::Instance(), norm_by_times);
  argument.AddAttribute("norm_by_times", attr_norm_by_times);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType logits = logits_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)logits;
  paddle::dialect::DenseTensorType warpctcgrad = warpctcgrad_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)warpctcgrad;
  paddle::dialect::DenseTensorType loss_grad = loss_grad_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)loss_grad;

  VLOG(4) << "Builder construction  dense_logits";
  paddle::dialect::IrTensor ir_tensor_logits(paddle::dialect::TransToPhiDataType(logits.dtype()),
                                                      logits.dims(),
                                                      logits.data_layout(),
                                                      logits.lod(),
                                                      logits.offset());
  VLOG(4) << "Builder construction  meta_logits";
  paddle::dialect::IrMetaTensor meta_logits(&ir_tensor_logits);
  paddle::dialect::IrTensor dense_logits_grad;
  paddle::dialect::IrMetaTensor meta_logits_grad(&dense_logits_grad);

  phi::UnchangedInferMeta(meta_logits, &meta_logits_grad);

  std::vector<pir::Type> argument_outputs;
  pir::Type logits_grad_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_logits_grad.dtype()), dense_logits_grad.dims(), dense_logits_grad.layout(), dense_logits_grad.lod(), dense_logits_grad.offset());
  argument_outputs.push_back(logits_grad_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void WarpctcGradOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value logits_, pir::Value logits_length_, pir::Value warpctcgrad_, pir::Value loss_grad_, pir::AttributeMap attributes) {
  VLOG(4) << "Start build WarpctcGradOp";


  IR_ENFORCE(
      attributes.find("blank") != attributes.end(),
          "'blank' Attribute is expected for WarpctcGradOp. ");
  int blank = attributes.at("blank").dyn_cast<pir::Int32Attribute>().data();

  IR_ENFORCE(
      attributes.find("norm_by_times") != attributes.end(),
          "'norm_by_times' Attribute is expected for WarpctcGradOp. ");
  bool norm_by_times = attributes.at("norm_by_times").dyn_cast<pir::BoolAttribute>().data();


  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {logits_, logits_length_, warpctcgrad_, loss_grad_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_blank = pir::Int32Attribute::get(pir::IrContext::Instance(), blank);
  argument.AddAttribute("blank", attr_blank);
  pir::Attribute attr_norm_by_times = pir::BoolAttribute::get(pir::IrContext::Instance(), norm_by_times);
  argument.AddAttribute("norm_by_times", attr_norm_by_times);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType logits = logits_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)logits;
  paddle::dialect::DenseTensorType warpctcgrad = warpctcgrad_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)warpctcgrad;
  paddle::dialect::DenseTensorType loss_grad = loss_grad_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)loss_grad;

  VLOG(4) << "Builder construction  dense_logits";
  paddle::dialect::IrTensor ir_tensor_logits(paddle::dialect::TransToPhiDataType(logits.dtype()),
                                                      logits.dims(),
                                                      logits.data_layout(),
                                                      logits.lod(),
                                                      logits.offset());
  VLOG(4) << "Builder construction  meta_logits";
  paddle::dialect::IrMetaTensor meta_logits(&ir_tensor_logits);
  paddle::dialect::IrTensor dense_logits_grad;
  paddle::dialect::IrMetaTensor meta_logits_grad(&dense_logits_grad);

  phi::UnchangedInferMeta(meta_logits, &meta_logits_grad);

  std::vector<pir::Type> argument_outputs;
  pir::Type logits_grad_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_logits_grad.dtype()), dense_logits_grad.dims(), dense_logits_grad.layout(), dense_logits_grad.lod(), dense_logits_grad.offset());
  argument_outputs.push_back(logits_grad_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void WarpctcGradOp::VerifySig() {}

void WarpctcGradOp::InferMeta( phi::InferMetaContext *infer_meta ) {
  auto fn = PD_INFER_META(phi::UnchangedInferMeta);
  fn(infer_meta);
}

phi::DataType WarpctcGradOp::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: WarpctcGradOp";
  


  return expected_kernel_dtype;
}

const char *WarprnntGradOp::attributes_name[2] = { "blank", "fastemit_lambda" };

OpInfoTuple WarprnntGradOp::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("input", "paddle::dialect::DenseTensorType", false, true, false, false), paddle::dialect::OpInputInfo("input_lengths", "paddle::dialect::DenseTensorType", false, false, false, false), paddle::dialect::OpInputInfo("warprnntgrad", "paddle::dialect::DenseTensorType", false, false, false, false), paddle::dialect::OpInputInfo("loss_grad", "paddle::dialect::DenseTensorType", false, false, false, false) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = { paddle::dialect::OpAttributeInfo("blank", "pir::Int32Attribute", ""), paddle::dialect::OpAttributeInfo("fastemit_lambda", "pir::FloatAttribute", "") };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("input_grad", "paddle::dialect::DenseTensorType", false, false) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("UnchangedInferMeta", {"input"}, "warprnnt_grad", {"input", "input_lengths", "warprnntgrad", "loss_grad", "blank", "fastemit_lambda"}, {}, {}, {}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "warprnnt_grad");
}

void WarprnntGradOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value input_, pir::Value input_lengths_, pir::Value warprnntgrad_, pir::Value loss_grad_, int blank, float fastemit_lambda) {
  VLOG(4) << "Start build WarprnntGradOp";



  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {input_, input_lengths_, warprnntgrad_, loss_grad_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_blank = pir::Int32Attribute::get(pir::IrContext::Instance(), blank);
  argument.AddAttribute("blank", attr_blank);
  pir::Attribute attr_fastemit_lambda = pir::FloatAttribute::get(pir::IrContext::Instance(), fastemit_lambda);
  argument.AddAttribute("fastemit_lambda", attr_fastemit_lambda);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType input = input_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)input;
  paddle::dialect::DenseTensorType input_lengths = input_lengths_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)input_lengths;
  paddle::dialect::DenseTensorType warprnntgrad = warprnntgrad_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)warprnntgrad;
  paddle::dialect::DenseTensorType loss_grad = loss_grad_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)loss_grad;

  VLOG(4) << "Builder construction  dense_input";
  paddle::dialect::IrTensor ir_tensor_input(paddle::dialect::TransToPhiDataType(input.dtype()),
                                                      input.dims(),
                                                      input.data_layout(),
                                                      input.lod(),
                                                      input.offset());
  VLOG(4) << "Builder construction  meta_input";
  paddle::dialect::IrMetaTensor meta_input(&ir_tensor_input);
  paddle::dialect::IrTensor dense_input_grad;
  paddle::dialect::IrMetaTensor meta_input_grad(&dense_input_grad);

  phi::UnchangedInferMeta(meta_input, &meta_input_grad);

  std::vector<pir::Type> argument_outputs;
  pir::Type input_grad_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_input_grad.dtype()), dense_input_grad.dims(), dense_input_grad.layout(), dense_input_grad.lod(), dense_input_grad.offset());
  argument_outputs.push_back(input_grad_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void WarprnntGradOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value input_, pir::Value input_lengths_, pir::Value warprnntgrad_, pir::Value loss_grad_, pir::AttributeMap attributes) {
  VLOG(4) << "Start build WarprnntGradOp";


  IR_ENFORCE(
      attributes.find("blank") != attributes.end(),
          "'blank' Attribute is expected for WarprnntGradOp. ");
  int blank = attributes.at("blank").dyn_cast<pir::Int32Attribute>().data();

  IR_ENFORCE(
      attributes.find("fastemit_lambda") != attributes.end(),
          "'fastemit_lambda' Attribute is expected for WarprnntGradOp. ");
  float fastemit_lambda = attributes.at("fastemit_lambda").dyn_cast<pir::FloatAttribute>().data();


  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {input_, input_lengths_, warprnntgrad_, loss_grad_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_blank = pir::Int32Attribute::get(pir::IrContext::Instance(), blank);
  argument.AddAttribute("blank", attr_blank);
  pir::Attribute attr_fastemit_lambda = pir::FloatAttribute::get(pir::IrContext::Instance(), fastemit_lambda);
  argument.AddAttribute("fastemit_lambda", attr_fastemit_lambda);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType input = input_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)input;
  paddle::dialect::DenseTensorType input_lengths = input_lengths_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)input_lengths;
  paddle::dialect::DenseTensorType warprnntgrad = warprnntgrad_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)warprnntgrad;
  paddle::dialect::DenseTensorType loss_grad = loss_grad_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)loss_grad;

  VLOG(4) << "Builder construction  dense_input";
  paddle::dialect::IrTensor ir_tensor_input(paddle::dialect::TransToPhiDataType(input.dtype()),
                                                      input.dims(),
                                                      input.data_layout(),
                                                      input.lod(),
                                                      input.offset());
  VLOG(4) << "Builder construction  meta_input";
  paddle::dialect::IrMetaTensor meta_input(&ir_tensor_input);
  paddle::dialect::IrTensor dense_input_grad;
  paddle::dialect::IrMetaTensor meta_input_grad(&dense_input_grad);

  phi::UnchangedInferMeta(meta_input, &meta_input_grad);

  std::vector<pir::Type> argument_outputs;
  pir::Type input_grad_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_input_grad.dtype()), dense_input_grad.dims(), dense_input_grad.layout(), dense_input_grad.lod(), dense_input_grad.offset());
  argument_outputs.push_back(input_grad_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void WarprnntGradOp::VerifySig() {}

void WarprnntGradOp::InferMeta( phi::InferMetaContext *infer_meta ) {
  auto fn = PD_INFER_META(phi::UnchangedInferMeta);
  fn(infer_meta);
}

phi::DataType WarprnntGradOp::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: WarprnntGradOp";
  


  return expected_kernel_dtype;
}

const char *WeightOnlyLinearGradOp::attributes_name[3] = { "weight_dtype", "arch", "group_size" };

OpInfoTuple WeightOnlyLinearGradOp::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("x", "paddle::dialect::DenseTensorType", false, true, false, false), paddle::dialect::OpInputInfo("weight", "paddle::dialect::DenseTensorType", false, false, false, false), paddle::dialect::OpInputInfo("bias", "paddle::dialect::DenseTensorType", true, false, false, false), paddle::dialect::OpInputInfo("weight_scale", "paddle::dialect::DenseTensorType", false, false, false, false), paddle::dialect::OpInputInfo("out_grad", "paddle::dialect::DenseTensorType", false, false, false, false) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = { paddle::dialect::OpAttributeInfo("weight_dtype", "pir::StrAttribute", ""), paddle::dialect::OpAttributeInfo("arch", "pir::Int32Attribute", ""), paddle::dialect::OpAttributeInfo("group_size", "pir::Int32Attribute", "") };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("x_grad", "paddle::dialect::DenseTensorType", false, false) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("WeightOnlyLinearGradInferMeta", {"x", "weight", "bias", "weight_scale", "out_grad", "weight_dtype", "arch", "group_size"}, "weight_only_linear_grad", {"x", "weight", "bias", "weight_scale", "out_grad", "weight_dtype", "arch", "group_size"}, {"out_grad"}, {}, {}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "weight_only_linear_grad");
}

void WeightOnlyLinearGradOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, pir::Value weight_, pir::Value bias_, pir::Value weight_scale_, pir::Value out_grad_, const std::string& weight_dtype, int arch, int group_size) {
  VLOG(4) << "Start build WeightOnlyLinearGradOp";



  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_, weight_, bias_, weight_scale_, out_grad_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_weight_dtype = pir::StrAttribute::get(pir::IrContext::Instance(), weight_dtype);
  argument.AddAttribute("weight_dtype", attr_weight_dtype);
  pir::Attribute attr_arch = pir::Int32Attribute::get(pir::IrContext::Instance(), arch);
  argument.AddAttribute("arch", attr_arch);
  pir::Attribute attr_group_size = pir::Int32Attribute::get(pir::IrContext::Instance(), group_size);
  argument.AddAttribute("group_size", attr_group_size);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;
  paddle::dialect::DenseTensorType weight = weight_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)weight;
  paddle::dialect::DenseTensorType weight_scale = weight_scale_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)weight_scale;
  paddle::dialect::DenseTensorType out_grad = out_grad_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)out_grad;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);

  VLOG(4) << "Builder construction  dense_weight";
  paddle::dialect::IrTensor ir_tensor_weight(paddle::dialect::TransToPhiDataType(weight.dtype()),
                                                      weight.dims(),
                                                      weight.data_layout(),
                                                      weight.lod(),
                                                      weight.offset());
  VLOG(4) << "Builder construction  meta_weight";
  paddle::dialect::IrMetaTensor meta_weight(&ir_tensor_weight);

  paddle::dialect::IrMetaTensor meta_bias;
  paddle::dialect::IrTensor ir_tensor_bias;
  if (bias_.impl() != nullptr) {
    paddle::dialect::DenseTensorType bias = bias_.type().dyn_cast<paddle::dialect::DenseTensorType>();
    VLOG(4) << "Builder construction  dense_bias";
    ir_tensor_bias = paddle::dialect::IrTensor(paddle::dialect::TransToPhiDataType(bias.dtype()),
                                                        bias.dims(),
                                                        bias.data_layout(),
                                                        bias.lod(),
                                                        bias.offset());
    VLOG(4) << "Builder construction  meta_bias";
    meta_bias = paddle::dialect::IrMetaTensor(&ir_tensor_bias);
  }


  VLOG(4) << "Builder construction  dense_weight_scale";
  paddle::dialect::IrTensor ir_tensor_weight_scale(paddle::dialect::TransToPhiDataType(weight_scale.dtype()),
                                                      weight_scale.dims(),
                                                      weight_scale.data_layout(),
                                                      weight_scale.lod(),
                                                      weight_scale.offset());
  VLOG(4) << "Builder construction  meta_weight_scale";
  paddle::dialect::IrMetaTensor meta_weight_scale(&ir_tensor_weight_scale);

  VLOG(4) << "Builder construction  dense_out_grad";
  paddle::dialect::IrTensor ir_tensor_out_grad(paddle::dialect::TransToPhiDataType(out_grad.dtype()),
                                                      out_grad.dims(),
                                                      out_grad.data_layout(),
                                                      out_grad.lod(),
                                                      out_grad.offset());
  VLOG(4) << "Builder construction  meta_out_grad";
  paddle::dialect::IrMetaTensor meta_out_grad(&ir_tensor_out_grad);
  paddle::dialect::IrTensor dense_x_grad;
  paddle::dialect::IrMetaTensor meta_x_grad(&dense_x_grad);

  phi::WeightOnlyLinearGradInferMeta(meta_x, meta_weight, meta_bias, meta_weight_scale, meta_out_grad, weight_dtype, arch, group_size, &meta_x_grad);

  std::vector<pir::Type> argument_outputs;
  pir::Type x_grad_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_x_grad.dtype()), dense_x_grad.dims(), dense_x_grad.layout(), dense_x_grad.lod(), dense_x_grad.offset());
  argument_outputs.push_back(x_grad_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void WeightOnlyLinearGradOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, pir::Value weight_, pir::Value bias_, pir::Value weight_scale_, pir::Value out_grad_, pir::AttributeMap attributes) {
  VLOG(4) << "Start build WeightOnlyLinearGradOp";


  IR_ENFORCE(
      attributes.find("weight_dtype") != attributes.end(),
          "'weight_dtype' Attribute is expected for WeightOnlyLinearGradOp. ");
  std::string weight_dtype = attributes.at("weight_dtype").dyn_cast<pir::StrAttribute>().AsString();

  IR_ENFORCE(
      attributes.find("arch") != attributes.end(),
          "'arch' Attribute is expected for WeightOnlyLinearGradOp. ");
  int arch = attributes.at("arch").dyn_cast<pir::Int32Attribute>().data();

  IR_ENFORCE(
      attributes.find("group_size") != attributes.end(),
          "'group_size' Attribute is expected for WeightOnlyLinearGradOp. ");
  int group_size = attributes.at("group_size").dyn_cast<pir::Int32Attribute>().data();


  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_, weight_, bias_, weight_scale_, out_grad_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_weight_dtype = pir::StrAttribute::get(pir::IrContext::Instance(), weight_dtype);
  argument.AddAttribute("weight_dtype", attr_weight_dtype);
  pir::Attribute attr_arch = pir::Int32Attribute::get(pir::IrContext::Instance(), arch);
  argument.AddAttribute("arch", attr_arch);
  pir::Attribute attr_group_size = pir::Int32Attribute::get(pir::IrContext::Instance(), group_size);
  argument.AddAttribute("group_size", attr_group_size);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;
  paddle::dialect::DenseTensorType weight = weight_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)weight;
  paddle::dialect::DenseTensorType weight_scale = weight_scale_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)weight_scale;
  paddle::dialect::DenseTensorType out_grad = out_grad_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)out_grad;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);

  VLOG(4) << "Builder construction  dense_weight";
  paddle::dialect::IrTensor ir_tensor_weight(paddle::dialect::TransToPhiDataType(weight.dtype()),
                                                      weight.dims(),
                                                      weight.data_layout(),
                                                      weight.lod(),
                                                      weight.offset());
  VLOG(4) << "Builder construction  meta_weight";
  paddle::dialect::IrMetaTensor meta_weight(&ir_tensor_weight);

  paddle::dialect::IrMetaTensor meta_bias;
  paddle::dialect::IrTensor ir_tensor_bias;
  if (bias_.impl() != nullptr) {
    paddle::dialect::DenseTensorType bias = bias_.type().dyn_cast<paddle::dialect::DenseTensorType>();
    VLOG(4) << "Builder construction  dense_bias";
    ir_tensor_bias = paddle::dialect::IrTensor(paddle::dialect::TransToPhiDataType(bias.dtype()),
                                                        bias.dims(),
                                                        bias.data_layout(),
                                                        bias.lod(),
                                                        bias.offset());
    VLOG(4) << "Builder construction  meta_bias";
    meta_bias = paddle::dialect::IrMetaTensor(&ir_tensor_bias);
  }


  VLOG(4) << "Builder construction  dense_weight_scale";
  paddle::dialect::IrTensor ir_tensor_weight_scale(paddle::dialect::TransToPhiDataType(weight_scale.dtype()),
                                                      weight_scale.dims(),
                                                      weight_scale.data_layout(),
                                                      weight_scale.lod(),
                                                      weight_scale.offset());
  VLOG(4) << "Builder construction  meta_weight_scale";
  paddle::dialect::IrMetaTensor meta_weight_scale(&ir_tensor_weight_scale);

  VLOG(4) << "Builder construction  dense_out_grad";
  paddle::dialect::IrTensor ir_tensor_out_grad(paddle::dialect::TransToPhiDataType(out_grad.dtype()),
                                                      out_grad.dims(),
                                                      out_grad.data_layout(),
                                                      out_grad.lod(),
                                                      out_grad.offset());
  VLOG(4) << "Builder construction  meta_out_grad";
  paddle::dialect::IrMetaTensor meta_out_grad(&ir_tensor_out_grad);
  paddle::dialect::IrTensor dense_x_grad;
  paddle::dialect::IrMetaTensor meta_x_grad(&dense_x_grad);

  phi::WeightOnlyLinearGradInferMeta(meta_x, meta_weight, meta_bias, meta_weight_scale, meta_out_grad, weight_dtype, arch, group_size, &meta_x_grad);

  std::vector<pir::Type> argument_outputs;
  pir::Type x_grad_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_x_grad.dtype()), dense_x_grad.dims(), dense_x_grad.layout(), dense_x_grad.lod(), dense_x_grad.offset());
  argument_outputs.push_back(x_grad_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void WeightOnlyLinearGradOp::VerifySig() {}

void WeightOnlyLinearGradOp::InferMeta( phi::InferMetaContext *infer_meta ) {
  auto fn = PD_INFER_META(phi::WeightOnlyLinearGradInferMeta);
  fn(infer_meta);
}

phi::DataType WeightOnlyLinearGradOp::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: WeightOnlyLinearGradOp";
  


  return expected_kernel_dtype;
}

OpInfoTuple WhereGradOp::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("condition", "paddle::dialect::DenseTensorType", false, false, false, false), paddle::dialect::OpInputInfo("x", "paddle::dialect::DenseTensorType", false, true, false, false), paddle::dialect::OpInputInfo("y", "paddle::dialect::DenseTensorType", false, true, false, false), paddle::dialect::OpInputInfo("out_grad", "paddle::dialect::DenseTensorType", false, false, false, false) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = {  };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("x_grad", "paddle::dialect::DenseTensorType", false, false), paddle::dialect::OpOutputInfo("y_grad", "paddle::dialect::DenseTensorType", false, false) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("GeneralBinaryGradInferMeta", {"x", "y"}, "where_grad", {"condition", "x", "y", "out_grad"}, {}, {}, {}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "where_grad");
}

void WhereGradOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value condition_, pir::Value x_, pir::Value y_, pir::Value out_grad_) {
  VLOG(4) << "Start build WhereGradOp";



  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {condition_, x_, y_, out_grad_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType condition = condition_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)condition;
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;
  paddle::dialect::DenseTensorType y = y_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)y;
  paddle::dialect::DenseTensorType out_grad = out_grad_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)out_grad;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);

  VLOG(4) << "Builder construction  dense_y";
  paddle::dialect::IrTensor ir_tensor_y(paddle::dialect::TransToPhiDataType(y.dtype()),
                                                      y.dims(),
                                                      y.data_layout(),
                                                      y.lod(),
                                                      y.offset());
  VLOG(4) << "Builder construction  meta_y";
  paddle::dialect::IrMetaTensor meta_y(&ir_tensor_y);
  paddle::dialect::IrTensor dense_x_grad;
  paddle::dialect::IrMetaTensor meta_x_grad(&dense_x_grad);
  paddle::dialect::IrTensor dense_y_grad;
  paddle::dialect::IrMetaTensor meta_y_grad(&dense_y_grad);

  phi::GeneralBinaryGradInferMeta(meta_x, meta_y, &meta_x_grad, &meta_y_grad);

  std::vector<pir::Type> argument_outputs;
  pir::Type x_grad_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_x_grad.dtype()), dense_x_grad.dims(), dense_x_grad.layout(), dense_x_grad.lod(), dense_x_grad.offset());
  argument_outputs.push_back(x_grad_dense_tensor_type);

  pir::Type y_grad_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_y_grad.dtype()), dense_y_grad.dims(), dense_y_grad.layout(), dense_y_grad.lod(), dense_y_grad.offset());
  argument_outputs.push_back(y_grad_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void WhereGradOp::VerifySig() {}

void WhereGradOp::InferMeta( phi::InferMetaContext *infer_meta ) {
  auto fn = PD_INFER_META(phi::GeneralBinaryGradInferMeta);
  fn(infer_meta);
}

phi::DataType WhereGradOp::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: WhereGradOp";
  


  return expected_kernel_dtype;
}

const char *YoloLossGradOp::attributes_name[7] = { "anchors", "anchor_mask", "class_num", "ignore_thresh", "downsample_ratio", "use_label_smooth", "scale_x_y" };

OpInfoTuple YoloLossGradOp::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("x", "paddle::dialect::DenseTensorType", false, false, false, false), paddle::dialect::OpInputInfo("gt_box", "paddle::dialect::DenseTensorType", false, false, false, false), paddle::dialect::OpInputInfo("gt_label", "paddle::dialect::DenseTensorType", false, false, false, false), paddle::dialect::OpInputInfo("gt_score", "paddle::dialect::DenseTensorType", true, false, false, false), paddle::dialect::OpInputInfo("objectness_mask", "paddle::dialect::DenseTensorType", false, false, false, false), paddle::dialect::OpInputInfo("gt_match_mask", "paddle::dialect::DenseTensorType", false, false, false, false), paddle::dialect::OpInputInfo("loss_grad", "paddle::dialect::DenseTensorType", false, false, false, false) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = { paddle::dialect::OpAttributeInfo("anchors", "pir::ArrayAttribute<pir::Int32Attribute>", ""), paddle::dialect::OpAttributeInfo("anchor_mask", "pir::ArrayAttribute<pir::Int32Attribute>", ""), paddle::dialect::OpAttributeInfo("class_num", "pir::Int32Attribute", ""), paddle::dialect::OpAttributeInfo("ignore_thresh", "pir::FloatAttribute", ""), paddle::dialect::OpAttributeInfo("downsample_ratio", "pir::Int32Attribute", ""), paddle::dialect::OpAttributeInfo("use_label_smooth", "pir::BoolAttribute", ""), paddle::dialect::OpAttributeInfo("scale_x_y", "pir::FloatAttribute", "") };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("x_grad", "paddle::dialect::DenseTensorType", false, false), paddle::dialect::OpOutputInfo("gt_box_grad", "paddle::dialect::DenseTensorType", false, false), paddle::dialect::OpOutputInfo("gt_label_grad", "paddle::dialect::DenseTensorType", false, false), paddle::dialect::OpOutputInfo("gt_score_grad", "paddle::dialect::DenseTensorType", false, false) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("YoloLossGradInferMeta", {"x", "gt_box", "gt_label", "gt_score", "objectness_mask", "gt_match_mask", "loss_grad", "anchors", "anchor_mask", "class_num", "ignore_thresh", "downsample_ratio", "use_label_smooth", "scale_x_y"}, "yolo_loss_grad", {"x", "gt_box", "gt_label", "gt_score", "objectness_mask", "gt_match_mask", "loss_grad", "anchors", "anchor_mask", "class_num", "ignore_thresh", "downsample_ratio", "use_label_smooth", "scale_x_y"}, {}, {}, {}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "yolo_loss_grad");
}

void YoloLossGradOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, pir::Value gt_box_, pir::Value gt_label_, pir::Value gt_score_, pir::Value objectness_mask_, pir::Value gt_match_mask_, pir::Value loss_grad_, const std::vector<int>& anchors, const std::vector<int>& anchor_mask, int class_num, float ignore_thresh, int downsample_ratio, bool use_label_smooth, float scale_x_y) {
  VLOG(4) << "Start build YoloLossGradOp";



  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_, gt_box_, gt_label_, gt_score_, objectness_mask_, gt_match_mask_, loss_grad_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  std::vector<pir::Attribute> vec_anchors;
  for (size_t i = 0; i < static_cast<size_t>(anchors.size()); i++) {
      pir::Attribute attr_anchors = pir::Int32Attribute::get(pir::IrContext::Instance(), anchors[i]);

    vec_anchors.push_back(attr_anchors);
  }
  pir::Attribute attr_anchors = pir::ArrayAttribute::get(pir::IrContext::Instance(), vec_anchors);
  argument.AddAttribute("anchors", attr_anchors);
  std::vector<pir::Attribute> vec_anchor_mask;
  for (size_t i = 0; i < static_cast<size_t>(anchor_mask.size()); i++) {
      pir::Attribute attr_anchor_mask = pir::Int32Attribute::get(pir::IrContext::Instance(), anchor_mask[i]);

    vec_anchor_mask.push_back(attr_anchor_mask);
  }
  pir::Attribute attr_anchor_mask = pir::ArrayAttribute::get(pir::IrContext::Instance(), vec_anchor_mask);
  argument.AddAttribute("anchor_mask", attr_anchor_mask);
  pir::Attribute attr_class_num = pir::Int32Attribute::get(pir::IrContext::Instance(), class_num);
  argument.AddAttribute("class_num", attr_class_num);
  pir::Attribute attr_ignore_thresh = pir::FloatAttribute::get(pir::IrContext::Instance(), ignore_thresh);
  argument.AddAttribute("ignore_thresh", attr_ignore_thresh);
  pir::Attribute attr_downsample_ratio = pir::Int32Attribute::get(pir::IrContext::Instance(), downsample_ratio);
  argument.AddAttribute("downsample_ratio", attr_downsample_ratio);
  pir::Attribute attr_use_label_smooth = pir::BoolAttribute::get(pir::IrContext::Instance(), use_label_smooth);
  argument.AddAttribute("use_label_smooth", attr_use_label_smooth);
  pir::Attribute attr_scale_x_y = pir::FloatAttribute::get(pir::IrContext::Instance(), scale_x_y);
  argument.AddAttribute("scale_x_y", attr_scale_x_y);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;
  paddle::dialect::DenseTensorType gt_box = gt_box_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)gt_box;
  paddle::dialect::DenseTensorType gt_label = gt_label_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)gt_label;
  paddle::dialect::DenseTensorType objectness_mask = objectness_mask_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)objectness_mask;
  paddle::dialect::DenseTensorType gt_match_mask = gt_match_mask_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)gt_match_mask;
  paddle::dialect::DenseTensorType loss_grad = loss_grad_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)loss_grad;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);

  VLOG(4) << "Builder construction  dense_gt_box";
  paddle::dialect::IrTensor ir_tensor_gt_box(paddle::dialect::TransToPhiDataType(gt_box.dtype()),
                                                      gt_box.dims(),
                                                      gt_box.data_layout(),
                                                      gt_box.lod(),
                                                      gt_box.offset());
  VLOG(4) << "Builder construction  meta_gt_box";
  paddle::dialect::IrMetaTensor meta_gt_box(&ir_tensor_gt_box);

  VLOG(4) << "Builder construction  dense_gt_label";
  paddle::dialect::IrTensor ir_tensor_gt_label(paddle::dialect::TransToPhiDataType(gt_label.dtype()),
                                                      gt_label.dims(),
                                                      gt_label.data_layout(),
                                                      gt_label.lod(),
                                                      gt_label.offset());
  VLOG(4) << "Builder construction  meta_gt_label";
  paddle::dialect::IrMetaTensor meta_gt_label(&ir_tensor_gt_label);

  paddle::dialect::IrMetaTensor meta_gt_score;
  paddle::dialect::IrTensor ir_tensor_gt_score;
  if (gt_score_.impl() != nullptr) {
    paddle::dialect::DenseTensorType gt_score = gt_score_.type().dyn_cast<paddle::dialect::DenseTensorType>();
    VLOG(4) << "Builder construction  dense_gt_score";
    ir_tensor_gt_score = paddle::dialect::IrTensor(paddle::dialect::TransToPhiDataType(gt_score.dtype()),
                                                        gt_score.dims(),
                                                        gt_score.data_layout(),
                                                        gt_score.lod(),
                                                        gt_score.offset());
    VLOG(4) << "Builder construction  meta_gt_score";
    meta_gt_score = paddle::dialect::IrMetaTensor(&ir_tensor_gt_score);
  }


  VLOG(4) << "Builder construction  dense_objectness_mask";
  paddle::dialect::IrTensor ir_tensor_objectness_mask(paddle::dialect::TransToPhiDataType(objectness_mask.dtype()),
                                                      objectness_mask.dims(),
                                                      objectness_mask.data_layout(),
                                                      objectness_mask.lod(),
                                                      objectness_mask.offset());
  VLOG(4) << "Builder construction  meta_objectness_mask";
  paddle::dialect::IrMetaTensor meta_objectness_mask(&ir_tensor_objectness_mask);

  VLOG(4) << "Builder construction  dense_gt_match_mask";
  paddle::dialect::IrTensor ir_tensor_gt_match_mask(paddle::dialect::TransToPhiDataType(gt_match_mask.dtype()),
                                                      gt_match_mask.dims(),
                                                      gt_match_mask.data_layout(),
                                                      gt_match_mask.lod(),
                                                      gt_match_mask.offset());
  VLOG(4) << "Builder construction  meta_gt_match_mask";
  paddle::dialect::IrMetaTensor meta_gt_match_mask(&ir_tensor_gt_match_mask);

  VLOG(4) << "Builder construction  dense_loss_grad";
  paddle::dialect::IrTensor ir_tensor_loss_grad(paddle::dialect::TransToPhiDataType(loss_grad.dtype()),
                                                      loss_grad.dims(),
                                                      loss_grad.data_layout(),
                                                      loss_grad.lod(),
                                                      loss_grad.offset());
  VLOG(4) << "Builder construction  meta_loss_grad";
  paddle::dialect::IrMetaTensor meta_loss_grad(&ir_tensor_loss_grad);
  paddle::dialect::IrTensor dense_x_grad;
  paddle::dialect::IrMetaTensor meta_x_grad(&dense_x_grad);
  paddle::dialect::IrTensor dense_gt_box_grad;
  paddle::dialect::IrMetaTensor meta_gt_box_grad(&dense_gt_box_grad);
  paddle::dialect::IrTensor dense_gt_label_grad;
  paddle::dialect::IrMetaTensor meta_gt_label_grad(&dense_gt_label_grad);
  paddle::dialect::IrTensor dense_gt_score_grad;
  paddle::dialect::IrMetaTensor meta_gt_score_grad(&dense_gt_score_grad);

  phi::YoloLossGradInferMeta(meta_x, meta_gt_box, meta_gt_label, meta_gt_score, meta_objectness_mask, meta_gt_match_mask, meta_loss_grad, anchors, anchor_mask, class_num, ignore_thresh, downsample_ratio, use_label_smooth, scale_x_y, &meta_x_grad, &meta_gt_box_grad, &meta_gt_label_grad, &meta_gt_score_grad);

  std::vector<pir::Type> argument_outputs;
  pir::Type x_grad_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_x_grad.dtype()), dense_x_grad.dims(), dense_x_grad.layout(), dense_x_grad.lod(), dense_x_grad.offset());
  argument_outputs.push_back(x_grad_dense_tensor_type);

  pir::Type gt_box_grad_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_gt_box_grad.dtype()), dense_gt_box_grad.dims(), dense_gt_box_grad.layout(), dense_gt_box_grad.lod(), dense_gt_box_grad.offset());
  argument_outputs.push_back(gt_box_grad_dense_tensor_type);

  pir::Type gt_label_grad_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_gt_label_grad.dtype()), dense_gt_label_grad.dims(), dense_gt_label_grad.layout(), dense_gt_label_grad.lod(), dense_gt_label_grad.offset());
  argument_outputs.push_back(gt_label_grad_dense_tensor_type);

  pir::Type gt_score_grad_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_gt_score_grad.dtype()), dense_gt_score_grad.dims(), dense_gt_score_grad.layout(), dense_gt_score_grad.lod(), dense_gt_score_grad.offset());
  argument_outputs.push_back(gt_score_grad_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void YoloLossGradOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, pir::Value gt_box_, pir::Value gt_label_, pir::Value gt_score_, pir::Value objectness_mask_, pir::Value gt_match_mask_, pir::Value loss_grad_, pir::AttributeMap attributes) {
  VLOG(4) << "Start build YoloLossGradOp";


  IR_ENFORCE(
      attributes.find("anchors") != attributes.end(),
          "'anchors' Attribute is expected for YoloLossGradOp. ");
  std::vector<int> anchors;
  for (size_t i = 0; i < attributes.at("anchors").dyn_cast<pir::ArrayAttribute>().size(); i++) {
    anchors.push_back(attributes.at("anchors").dyn_cast<pir::ArrayAttribute>().at(i).dyn_cast<pir::Int32Attribute>().data());
  }

  IR_ENFORCE(
      attributes.find("anchor_mask") != attributes.end(),
          "'anchor_mask' Attribute is expected for YoloLossGradOp. ");
  std::vector<int> anchor_mask;
  for (size_t i = 0; i < attributes.at("anchor_mask").dyn_cast<pir::ArrayAttribute>().size(); i++) {
    anchor_mask.push_back(attributes.at("anchor_mask").dyn_cast<pir::ArrayAttribute>().at(i).dyn_cast<pir::Int32Attribute>().data());
  }

  IR_ENFORCE(
      attributes.find("class_num") != attributes.end(),
          "'class_num' Attribute is expected for YoloLossGradOp. ");
  int class_num = attributes.at("class_num").dyn_cast<pir::Int32Attribute>().data();

  IR_ENFORCE(
      attributes.find("ignore_thresh") != attributes.end(),
          "'ignore_thresh' Attribute is expected for YoloLossGradOp. ");
  float ignore_thresh = attributes.at("ignore_thresh").dyn_cast<pir::FloatAttribute>().data();

  IR_ENFORCE(
      attributes.find("downsample_ratio") != attributes.end(),
          "'downsample_ratio' Attribute is expected for YoloLossGradOp. ");
  int downsample_ratio = attributes.at("downsample_ratio").dyn_cast<pir::Int32Attribute>().data();

  IR_ENFORCE(
      attributes.find("use_label_smooth") != attributes.end(),
          "'use_label_smooth' Attribute is expected for YoloLossGradOp. ");
  bool use_label_smooth = attributes.at("use_label_smooth").dyn_cast<pir::BoolAttribute>().data();

  IR_ENFORCE(
      attributes.find("scale_x_y") != attributes.end(),
          "'scale_x_y' Attribute is expected for YoloLossGradOp. ");
  float scale_x_y = attributes.at("scale_x_y").dyn_cast<pir::FloatAttribute>().data();


  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_, gt_box_, gt_label_, gt_score_, objectness_mask_, gt_match_mask_, loss_grad_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  std::vector<pir::Attribute> vec_anchors;
  for (size_t i = 0; i < static_cast<size_t>(anchors.size()); i++) {
      pir::Attribute attr_anchors = pir::Int32Attribute::get(pir::IrContext::Instance(), anchors[i]);

    vec_anchors.push_back(attr_anchors);
  }
  pir::Attribute attr_anchors = pir::ArrayAttribute::get(pir::IrContext::Instance(), vec_anchors);
  argument.AddAttribute("anchors", attr_anchors);
  std::vector<pir::Attribute> vec_anchor_mask;
  for (size_t i = 0; i < static_cast<size_t>(anchor_mask.size()); i++) {
      pir::Attribute attr_anchor_mask = pir::Int32Attribute::get(pir::IrContext::Instance(), anchor_mask[i]);

    vec_anchor_mask.push_back(attr_anchor_mask);
  }
  pir::Attribute attr_anchor_mask = pir::ArrayAttribute::get(pir::IrContext::Instance(), vec_anchor_mask);
  argument.AddAttribute("anchor_mask", attr_anchor_mask);
  pir::Attribute attr_class_num = pir::Int32Attribute::get(pir::IrContext::Instance(), class_num);
  argument.AddAttribute("class_num", attr_class_num);
  pir::Attribute attr_ignore_thresh = pir::FloatAttribute::get(pir::IrContext::Instance(), ignore_thresh);
  argument.AddAttribute("ignore_thresh", attr_ignore_thresh);
  pir::Attribute attr_downsample_ratio = pir::Int32Attribute::get(pir::IrContext::Instance(), downsample_ratio);
  argument.AddAttribute("downsample_ratio", attr_downsample_ratio);
  pir::Attribute attr_use_label_smooth = pir::BoolAttribute::get(pir::IrContext::Instance(), use_label_smooth);
  argument.AddAttribute("use_label_smooth", attr_use_label_smooth);
  pir::Attribute attr_scale_x_y = pir::FloatAttribute::get(pir::IrContext::Instance(), scale_x_y);
  argument.AddAttribute("scale_x_y", attr_scale_x_y);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;
  paddle::dialect::DenseTensorType gt_box = gt_box_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)gt_box;
  paddle::dialect::DenseTensorType gt_label = gt_label_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)gt_label;
  paddle::dialect::DenseTensorType objectness_mask = objectness_mask_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)objectness_mask;
  paddle::dialect::DenseTensorType gt_match_mask = gt_match_mask_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)gt_match_mask;
  paddle::dialect::DenseTensorType loss_grad = loss_grad_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)loss_grad;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);

  VLOG(4) << "Builder construction  dense_gt_box";
  paddle::dialect::IrTensor ir_tensor_gt_box(paddle::dialect::TransToPhiDataType(gt_box.dtype()),
                                                      gt_box.dims(),
                                                      gt_box.data_layout(),
                                                      gt_box.lod(),
                                                      gt_box.offset());
  VLOG(4) << "Builder construction  meta_gt_box";
  paddle::dialect::IrMetaTensor meta_gt_box(&ir_tensor_gt_box);

  VLOG(4) << "Builder construction  dense_gt_label";
  paddle::dialect::IrTensor ir_tensor_gt_label(paddle::dialect::TransToPhiDataType(gt_label.dtype()),
                                                      gt_label.dims(),
                                                      gt_label.data_layout(),
                                                      gt_label.lod(),
                                                      gt_label.offset());
  VLOG(4) << "Builder construction  meta_gt_label";
  paddle::dialect::IrMetaTensor meta_gt_label(&ir_tensor_gt_label);

  paddle::dialect::IrMetaTensor meta_gt_score;
  paddle::dialect::IrTensor ir_tensor_gt_score;
  if (gt_score_.impl() != nullptr) {
    paddle::dialect::DenseTensorType gt_score = gt_score_.type().dyn_cast<paddle::dialect::DenseTensorType>();
    VLOG(4) << "Builder construction  dense_gt_score";
    ir_tensor_gt_score = paddle::dialect::IrTensor(paddle::dialect::TransToPhiDataType(gt_score.dtype()),
                                                        gt_score.dims(),
                                                        gt_score.data_layout(),
                                                        gt_score.lod(),
                                                        gt_score.offset());
    VLOG(4) << "Builder construction  meta_gt_score";
    meta_gt_score = paddle::dialect::IrMetaTensor(&ir_tensor_gt_score);
  }


  VLOG(4) << "Builder construction  dense_objectness_mask";
  paddle::dialect::IrTensor ir_tensor_objectness_mask(paddle::dialect::TransToPhiDataType(objectness_mask.dtype()),
                                                      objectness_mask.dims(),
                                                      objectness_mask.data_layout(),
                                                      objectness_mask.lod(),
                                                      objectness_mask.offset());
  VLOG(4) << "Builder construction  meta_objectness_mask";
  paddle::dialect::IrMetaTensor meta_objectness_mask(&ir_tensor_objectness_mask);

  VLOG(4) << "Builder construction  dense_gt_match_mask";
  paddle::dialect::IrTensor ir_tensor_gt_match_mask(paddle::dialect::TransToPhiDataType(gt_match_mask.dtype()),
                                                      gt_match_mask.dims(),
                                                      gt_match_mask.data_layout(),
                                                      gt_match_mask.lod(),
                                                      gt_match_mask.offset());
  VLOG(4) << "Builder construction  meta_gt_match_mask";
  paddle::dialect::IrMetaTensor meta_gt_match_mask(&ir_tensor_gt_match_mask);

  VLOG(4) << "Builder construction  dense_loss_grad";
  paddle::dialect::IrTensor ir_tensor_loss_grad(paddle::dialect::TransToPhiDataType(loss_grad.dtype()),
                                                      loss_grad.dims(),
                                                      loss_grad.data_layout(),
                                                      loss_grad.lod(),
                                                      loss_grad.offset());
  VLOG(4) << "Builder construction  meta_loss_grad";
  paddle::dialect::IrMetaTensor meta_loss_grad(&ir_tensor_loss_grad);
  paddle::dialect::IrTensor dense_x_grad;
  paddle::dialect::IrMetaTensor meta_x_grad(&dense_x_grad);
  paddle::dialect::IrTensor dense_gt_box_grad;
  paddle::dialect::IrMetaTensor meta_gt_box_grad(&dense_gt_box_grad);
  paddle::dialect::IrTensor dense_gt_label_grad;
  paddle::dialect::IrMetaTensor meta_gt_label_grad(&dense_gt_label_grad);
  paddle::dialect::IrTensor dense_gt_score_grad;
  paddle::dialect::IrMetaTensor meta_gt_score_grad(&dense_gt_score_grad);

  phi::YoloLossGradInferMeta(meta_x, meta_gt_box, meta_gt_label, meta_gt_score, meta_objectness_mask, meta_gt_match_mask, meta_loss_grad, anchors, anchor_mask, class_num, ignore_thresh, downsample_ratio, use_label_smooth, scale_x_y, &meta_x_grad, &meta_gt_box_grad, &meta_gt_label_grad, &meta_gt_score_grad);

  std::vector<pir::Type> argument_outputs;
  pir::Type x_grad_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_x_grad.dtype()), dense_x_grad.dims(), dense_x_grad.layout(), dense_x_grad.lod(), dense_x_grad.offset());
  argument_outputs.push_back(x_grad_dense_tensor_type);

  pir::Type gt_box_grad_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_gt_box_grad.dtype()), dense_gt_box_grad.dims(), dense_gt_box_grad.layout(), dense_gt_box_grad.lod(), dense_gt_box_grad.offset());
  argument_outputs.push_back(gt_box_grad_dense_tensor_type);

  pir::Type gt_label_grad_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_gt_label_grad.dtype()), dense_gt_label_grad.dims(), dense_gt_label_grad.layout(), dense_gt_label_grad.lod(), dense_gt_label_grad.offset());
  argument_outputs.push_back(gt_label_grad_dense_tensor_type);

  pir::Type gt_score_grad_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_gt_score_grad.dtype()), dense_gt_score_grad.dims(), dense_gt_score_grad.layout(), dense_gt_score_grad.lod(), dense_gt_score_grad.offset());
  argument_outputs.push_back(gt_score_grad_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void YoloLossGradOp::VerifySig() {}

void YoloLossGradOp::InferMeta( phi::InferMetaContext *infer_meta ) {
  auto fn = PD_INFER_META(phi::YoloLossGradInferMeta);
  fn(infer_meta);
}

phi::DataType YoloLossGradOp::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: YoloLossGradOp";
  


  return expected_kernel_dtype;
}

OpInfoTuple SiluDoubleGradOp::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("x", "paddle::dialect::DenseTensorType", false, false, false, false), paddle::dialect::OpInputInfo("out", "paddle::dialect::DenseTensorType", false, false, false, false), paddle::dialect::OpInputInfo("grad_out", "paddle::dialect::DenseTensorType", false, false, false, false), paddle::dialect::OpInputInfo("grad_x_grad", "paddle::dialect::DenseTensorType", false, false, false, false) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = {  };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("x_grad", "paddle::dialect::DenseTensorType", false, false), paddle::dialect::OpOutputInfo("grad_out_grad", "paddle::dialect::DenseTensorType", false, false) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("", {""}, "", {""}, {}, {}, {}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "silu_double_grad");
}

void SiluDoubleGradOp::VerifySig() {}

phi::DataType SiluDoubleGradOp::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: SiluDoubleGradOp";
  


  return expected_kernel_dtype;
}

const char *Unpool3dGradOp::attributes_name[5] = { "ksize", "strides", "paddings", "output_size", "data_format" };

OpInfoTuple Unpool3dGradOp::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("x", "paddle::dialect::DenseTensorType", false, false, false, false), paddle::dialect::OpInputInfo("indices", "paddle::dialect::DenseTensorType", false, false, false, false), paddle::dialect::OpInputInfo("out", "paddle::dialect::DenseTensorType", false, false, false, false), paddle::dialect::OpInputInfo("out_grad", "paddle::dialect::DenseTensorType", false, false, false, false) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = { paddle::dialect::OpAttributeInfo("ksize", "pir::ArrayAttribute<pir::Int32Attribute>", ""), paddle::dialect::OpAttributeInfo("strides", "pir::ArrayAttribute<pir::Int32Attribute>", ""), paddle::dialect::OpAttributeInfo("paddings", "pir::ArrayAttribute<pir::Int32Attribute>", ""), paddle::dialect::OpAttributeInfo("output_size", "pir::ArrayAttribute<pir::Int32Attribute>", ""), paddle::dialect::OpAttributeInfo("data_format", "pir::StrAttribute", "") };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("x_grad", "paddle::dialect::DenseTensorType", false, false) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("UnchangedInferMeta", {"x"}, "unpool3d_grad", {"x", "indices", "out", "out_grad", "ksize", "strides", "paddings", "output_size", "data_format"}, {"x"}, {}, {}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "unpool3d_grad");
}

void Unpool3dGradOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, pir::Value indices_, pir::Value out_, pir::Value out_grad_, const std::vector<int>& ksize, const std::vector<int>& strides, const std::vector<int>& paddings, const std::vector<int>& output_size, const std::string& data_format) {
  VLOG(4) << "Start build Unpool3dGradOp";



  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_, indices_, out_, out_grad_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  std::vector<pir::Attribute> vec_ksize;
  for (size_t i = 0; i < static_cast<size_t>(ksize.size()); i++) {
      pir::Attribute attr_ksize = pir::Int32Attribute::get(pir::IrContext::Instance(), ksize[i]);

    vec_ksize.push_back(attr_ksize);
  }
  pir::Attribute attr_ksize = pir::ArrayAttribute::get(pir::IrContext::Instance(), vec_ksize);
  argument.AddAttribute("ksize", attr_ksize);
  std::vector<pir::Attribute> vec_strides;
  for (size_t i = 0; i < static_cast<size_t>(strides.size()); i++) {
      pir::Attribute attr_strides = pir::Int32Attribute::get(pir::IrContext::Instance(), strides[i]);

    vec_strides.push_back(attr_strides);
  }
  pir::Attribute attr_strides = pir::ArrayAttribute::get(pir::IrContext::Instance(), vec_strides);
  argument.AddAttribute("strides", attr_strides);
  std::vector<pir::Attribute> vec_paddings;
  for (size_t i = 0; i < static_cast<size_t>(paddings.size()); i++) {
      pir::Attribute attr_paddings = pir::Int32Attribute::get(pir::IrContext::Instance(), paddings[i]);

    vec_paddings.push_back(attr_paddings);
  }
  pir::Attribute attr_paddings = pir::ArrayAttribute::get(pir::IrContext::Instance(), vec_paddings);
  argument.AddAttribute("paddings", attr_paddings);
  std::vector<pir::Attribute> vec_output_size;
  for (size_t i = 0; i < static_cast<size_t>(output_size.size()); i++) {
      pir::Attribute attr_output_size = pir::Int32Attribute::get(pir::IrContext::Instance(), output_size[i]);

    vec_output_size.push_back(attr_output_size);
  }
  pir::Attribute attr_output_size = pir::ArrayAttribute::get(pir::IrContext::Instance(), vec_output_size);
  argument.AddAttribute("output_size", attr_output_size);
  pir::Attribute attr_data_format = pir::StrAttribute::get(pir::IrContext::Instance(), data_format);
  argument.AddAttribute("data_format", attr_data_format);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;
  paddle::dialect::DenseTensorType indices = indices_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)indices;
  paddle::dialect::DenseTensorType out = out_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)out;
  paddle::dialect::DenseTensorType out_grad = out_grad_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)out_grad;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);
  paddle::dialect::IrTensor dense_x_grad;
  paddle::dialect::IrMetaTensor meta_x_grad(&dense_x_grad);

  phi::UnchangedInferMeta(meta_x, &meta_x_grad);

  std::vector<pir::Type> argument_outputs;
  pir::Type x_grad_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_x_grad.dtype()), dense_x_grad.dims(), dense_x_grad.layout(), dense_x_grad.lod(), dense_x_grad.offset());
  argument_outputs.push_back(x_grad_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void Unpool3dGradOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, pir::Value indices_, pir::Value out_, pir::Value out_grad_, pir::AttributeMap attributes) {
  VLOG(4) << "Start build Unpool3dGradOp";


  IR_ENFORCE(
      attributes.find("ksize") != attributes.end(),
          "'ksize' Attribute is expected for Unpool3dGradOp. ");
  std::vector<int> ksize;
  for (size_t i = 0; i < attributes.at("ksize").dyn_cast<pir::ArrayAttribute>().size(); i++) {
    ksize.push_back(attributes.at("ksize").dyn_cast<pir::ArrayAttribute>().at(i).dyn_cast<pir::Int32Attribute>().data());
  }

  IR_ENFORCE(
      attributes.find("strides") != attributes.end(),
          "'strides' Attribute is expected for Unpool3dGradOp. ");
  std::vector<int> strides;
  for (size_t i = 0; i < attributes.at("strides").dyn_cast<pir::ArrayAttribute>().size(); i++) {
    strides.push_back(attributes.at("strides").dyn_cast<pir::ArrayAttribute>().at(i).dyn_cast<pir::Int32Attribute>().data());
  }

  IR_ENFORCE(
      attributes.find("paddings") != attributes.end(),
          "'paddings' Attribute is expected for Unpool3dGradOp. ");
  std::vector<int> paddings;
  for (size_t i = 0; i < attributes.at("paddings").dyn_cast<pir::ArrayAttribute>().size(); i++) {
    paddings.push_back(attributes.at("paddings").dyn_cast<pir::ArrayAttribute>().at(i).dyn_cast<pir::Int32Attribute>().data());
  }

  IR_ENFORCE(
      attributes.find("output_size") != attributes.end(),
          "'output_size' Attribute is expected for Unpool3dGradOp. ");
  std::vector<int> output_size;
  for (size_t i = 0; i < attributes.at("output_size").dyn_cast<pir::ArrayAttribute>().size(); i++) {
    output_size.push_back(attributes.at("output_size").dyn_cast<pir::ArrayAttribute>().at(i).dyn_cast<pir::Int32Attribute>().data());
  }

  IR_ENFORCE(
      attributes.find("data_format") != attributes.end(),
          "'data_format' Attribute is expected for Unpool3dGradOp. ");
  std::string data_format = attributes.at("data_format").dyn_cast<pir::StrAttribute>().AsString();


  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_, indices_, out_, out_grad_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  std::vector<pir::Attribute> vec_ksize;
  for (size_t i = 0; i < static_cast<size_t>(ksize.size()); i++) {
      pir::Attribute attr_ksize = pir::Int32Attribute::get(pir::IrContext::Instance(), ksize[i]);

    vec_ksize.push_back(attr_ksize);
  }
  pir::Attribute attr_ksize = pir::ArrayAttribute::get(pir::IrContext::Instance(), vec_ksize);
  argument.AddAttribute("ksize", attr_ksize);
  std::vector<pir::Attribute> vec_strides;
  for (size_t i = 0; i < static_cast<size_t>(strides.size()); i++) {
      pir::Attribute attr_strides = pir::Int32Attribute::get(pir::IrContext::Instance(), strides[i]);

    vec_strides.push_back(attr_strides);
  }
  pir::Attribute attr_strides = pir::ArrayAttribute::get(pir::IrContext::Instance(), vec_strides);
  argument.AddAttribute("strides", attr_strides);
  std::vector<pir::Attribute> vec_paddings;
  for (size_t i = 0; i < static_cast<size_t>(paddings.size()); i++) {
      pir::Attribute attr_paddings = pir::Int32Attribute::get(pir::IrContext::Instance(), paddings[i]);

    vec_paddings.push_back(attr_paddings);
  }
  pir::Attribute attr_paddings = pir::ArrayAttribute::get(pir::IrContext::Instance(), vec_paddings);
  argument.AddAttribute("paddings", attr_paddings);
  std::vector<pir::Attribute> vec_output_size;
  for (size_t i = 0; i < static_cast<size_t>(output_size.size()); i++) {
      pir::Attribute attr_output_size = pir::Int32Attribute::get(pir::IrContext::Instance(), output_size[i]);

    vec_output_size.push_back(attr_output_size);
  }
  pir::Attribute attr_output_size = pir::ArrayAttribute::get(pir::IrContext::Instance(), vec_output_size);
  argument.AddAttribute("output_size", attr_output_size);
  pir::Attribute attr_data_format = pir::StrAttribute::get(pir::IrContext::Instance(), data_format);
  argument.AddAttribute("data_format", attr_data_format);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;
  paddle::dialect::DenseTensorType indices = indices_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)indices;
  paddle::dialect::DenseTensorType out = out_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)out;
  paddle::dialect::DenseTensorType out_grad = out_grad_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)out_grad;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);
  paddle::dialect::IrTensor dense_x_grad;
  paddle::dialect::IrMetaTensor meta_x_grad(&dense_x_grad);

  phi::UnchangedInferMeta(meta_x, &meta_x_grad);

  std::vector<pir::Type> argument_outputs;
  pir::Type x_grad_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_x_grad.dtype()), dense_x_grad.dims(), dense_x_grad.layout(), dense_x_grad.lod(), dense_x_grad.offset());
  argument_outputs.push_back(x_grad_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void Unpool3dGradOp::VerifySig() {}

void Unpool3dGradOp::InferMeta( phi::InferMetaContext *infer_meta ) {
  auto fn = PD_INFER_META(phi::UnchangedInferMeta);
  fn(infer_meta);
}

phi::DataType Unpool3dGradOp::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: Unpool3dGradOp";
  


  return expected_kernel_dtype;
}

} // namespace dialect
} // namespace paddle


IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::AbsDoubleGradOp)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::AbsGradOp)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::AcosGradOp)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::AcosGrad_Op)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::AcoshGradOp)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::AcoshGrad_Op)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::AddmmGradOp)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::AffineGridGradOp)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::AngleGradOp)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::ArgsortGradOp)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::AsComplexGradOp)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::AsRealGradOp)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::AsStridedGradOp)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::AsinGradOp)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::AsinGrad_Op)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::AsinhGradOp)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::AsinhGrad_Op)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::Atan2GradOp)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::AtanGradOp)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::AtanGrad_Op)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::AtanhGradOp)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::AtanhGrad_Op)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::BceLossGradOp)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::BceLossGrad_Op)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::BicubicInterpGradOp)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::BilinearGradOp)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::BilinearInterpGradOp)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::BmmGradOp)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::BroadcastTensorsGradOp)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::CeilGradOp)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::CeilGrad_Op)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::CeluDoubleGradOp)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::CeluDoubleGrad_Op)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::CeluGradOp)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::CeluGrad_Op)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::CholeskyGradOp)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::CholeskySolveGradOp)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::ClipDoubleGradOp)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::ClipGradOp)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::ClipGrad_Op)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::ComplexGradOp)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::ConcatDoubleGradOp)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::ConcatGradOp)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::ConjGradOp)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::Conv2dGradOp)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::Conv2dGradGradOp)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::Conv3dDoubleGradOp)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::Conv3dGradOp)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::Conv3dTransposeGradOp)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::CosDoubleGradOp)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::CosDoubleGrad_Op)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::CosGradOp)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::CosGrad_Op)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::CosTripleGradOp)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::CosTripleGrad_Op)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::CoshGradOp)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::CoshGrad_Op)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::CropGradOp)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::CrossEntropyWithSoftmaxGradOp)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::CrossEntropyWithSoftmaxGrad_Op)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::CrossGradOp)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::CummaxGradOp)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::CumminGradOp)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::CumprodGradOp)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::CumsumGradOp)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::DepthwiseConv2dDoubleGradOp)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::DepthwiseConv2dGradOp)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::DetGradOp)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::DiagGradOp)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::DiagonalGradOp)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::DigammaGradOp)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::DistGradOp)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::DotGradOp)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::EigGradOp)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::EighGradOp)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::EigvalshGradOp)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::EluDoubleGradOp)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::EluDoubleGrad_Op)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::EluGradOp)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::EluGrad_Op)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::ErfGradOp)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::ErfinvGradOp)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::ExpGradOp)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::ExpGrad_Op)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::ExpandAsGradOp)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::ExpandDoubleGradOp)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::ExpandGradOp)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::Expm1GradOp)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::Expm1Grad_Op)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::FftC2cGradOp)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::FftC2rGradOp)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::FftR2cGradOp)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::FillDiagonalGradOp)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::FillDiagonalTensorGradOp)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::FillDiagonalTensorGrad_Op)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::FillGradOp)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::FillGrad_Op)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::FlashAttnGradOp)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::FlashAttnUnpaddedGradOp)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::FlattenGradOp)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::FlattenGrad_Op)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::FlipGradOp)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::FloorGradOp)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::FloorGrad_Op)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::FmaxGradOp)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::FminGradOp)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::FoldGradOp)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::FrameGradOp)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::GammalnGradOp)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::GatherGradOp)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::GatherNdGradOp)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::GaussianInplaceGradOp)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::GaussianInplaceGrad_Op)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::GeluGradOp)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::GridSampleGradOp)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::GroupNormGradOp)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::GroupNormGrad_Op)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::GumbelSoftmaxGradOp)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::HardshrinkGradOp)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::HardshrinkGrad_Op)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::HardsigmoidGradOp)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::HardsigmoidGrad_Op)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::HardtanhGradOp)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::HardtanhGrad_Op)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::HeavisideGradOp)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::HuberLossGradOp)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::I0GradOp)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::I0eGradOp)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::I1GradOp)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::I1eGradOp)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::IdentityLossGradOp)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::IdentityLossGrad_Op)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::ImagGradOp)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::IndexAddGradOp)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::IndexAddGrad_Op)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::IndexPutGradOp)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::IndexSampleGradOp)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::IndexSelectGradOp)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::IndexSelectStridedGradOp)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::InstanceNormDoubleGradOp)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::InstanceNormGradOp)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::InverseGradOp)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::KldivLossGradOp)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::KronGradOp)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::KthvalueGradOp)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::LabelSmoothGradOp)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::LayerNormGradOp)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::LeakyReluDoubleGradOp)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::LeakyReluDoubleGrad_Op)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::LeakyReluGradOp)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::LeakyReluGrad_Op)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::LerpGradOp)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::LgammaGradOp)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::LinearInterpGradOp)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::Log10GradOp)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::Log10Grad_Op)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::Log1pGradOp)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::Log1pGrad_Op)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::Log2GradOp)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::Log2Grad_Op)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::LogDoubleGradOp)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::LogDoubleGrad_Op)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::LogGradOp)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::LogGrad_Op)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::LogLossGradOp)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::LogSoftmaxGradOp)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::LogcumsumexpGradOp)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::LogitGradOp)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::LogsigmoidGradOp)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::LogsigmoidGrad_Op)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::LuGradOp)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::LuGrad_Op)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::LuUnpackGradOp)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::MarginCrossEntropyGradOp)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::MarginCrossEntropyGrad_Op)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::MaskedSelectGradOp)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::MatrixPowerGradOp)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::MaxPool2dWithIndexGradOp)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::MaxPool3dWithIndexGradOp)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::MaxoutGradOp)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::MeanAllGradOp)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::MemoryEfficientAttentionGradOp)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::MeshgridGradOp)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::ModeGradOp)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::MultiDotGradOp)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::MultiplexGradOp)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::MvGradOp)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::NanmedianGradOp)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::NearestInterpGradOp)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::NllLossGradOp)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::OverlapAddGradOp)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::PNormGradOp)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::Pad3dDoubleGradOp)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::Pad3dGradOp)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::PixelShuffleGradOp)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::PixelUnshuffleGradOp)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::PoissonGradOp)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::PolygammaGradOp)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::PowDoubleGradOp)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::PowDoubleGrad_Op)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::PowGradOp)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::PowGrad_Op)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::PowTripleGradOp)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::PreluGradOp)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::PsroiPoolGradOp)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::PutAlongAxisGradOp)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::QrGradOp)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::RealGradOp)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::ReciprocalGradOp)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::ReciprocalGrad_Op)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::Relu6GradOp)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::Relu6Grad_Op)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::ReluDoubleGradOp)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::ReluDoubleGrad_Op)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::ReluGradOp)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::ReluGrad_Op)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::RenormGradOp)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::ReverseGradOp)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::RoiAlignGradOp)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::RoiPoolGradOp)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::RollGradOp)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::RoundGradOp)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::RoundGrad_Op)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::RsqrtDoubleGradOp)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::RsqrtDoubleGrad_Op)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::RsqrtGradOp)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::RsqrtGrad_Op)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::ScaleGradOp)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::ScatterGradOp)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::ScatterNdAddGradOp)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::SegmentPoolGradOp)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::SeluGradOp)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::SendURecvGradOp)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::SendUeRecvGradOp)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::SendUvGradOp)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::SigmoidCrossEntropyWithLogitsGradOp)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::SigmoidCrossEntropyWithLogitsGrad_Op)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::SigmoidDoubleGradOp)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::SigmoidDoubleGrad_Op)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::SigmoidGradOp)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::SigmoidGrad_Op)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::SigmoidTripleGradOp)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::SigmoidTripleGrad_Op)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::SignGradOp)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::SiluGradOp)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::SiluGrad_Op)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::SinDoubleGradOp)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::SinDoubleGrad_Op)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::SinGradOp)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::SinGrad_Op)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::SinTripleGradOp)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::SinTripleGrad_Op)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::SinhGradOp)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::SinhGrad_Op)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::SlogdetGradOp)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::SoftplusDoubleGradOp)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::SoftplusDoubleGrad_Op)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::SoftplusGradOp)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::SoftplusGrad_Op)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::SoftshrinkGradOp)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::SoftshrinkGrad_Op)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::SoftsignGradOp)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::SoftsignGrad_Op)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::SolveGradOp)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::SpectralNormGradOp)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::SqrtDoubleGradOp)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::SqrtDoubleGrad_Op)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::SqrtGradOp)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::SqrtGrad_Op)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::SquareDoubleGradOp)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::SquareDoubleGrad_Op)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::SquareGradOp)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::SquareGrad_Op)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::SquaredL2NormGradOp)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::SqueezeDoubleGradOp)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::SqueezeGradOp)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::SqueezeGrad_Op)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::StackGradOp)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::StanhGradOp)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::SvdGradOp)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::TakeAlongAxisGradOp)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::TanGradOp)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::TanGrad_Op)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::TanhDoubleGradOp)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::TanhDoubleGrad_Op)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::TanhGradOp)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::TanhGrad_Op)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::TanhShrinkGradOp)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::TanhShrinkGrad_Op)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::TanhTripleGradOp)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::TanhTripleGrad_Op)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::TemporalShiftGradOp)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::TensorUnfoldGradOp)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::ThresholdedReluGradOp)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::ThresholdedReluGrad_Op)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::TopkGradOp)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::TraceGradOp)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::TriangularSolveGradOp)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::TrilinearInterpGradOp)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::TruncGradOp)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::UnbindGradOp)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::UnfoldGradOp)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::UniformInplaceGradOp)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::UniformInplaceGrad_Op)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::UnsqueezeDoubleGradOp)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::UnsqueezeGradOp)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::UnsqueezeGrad_Op)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::UnstackGradOp)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::ViewDtypeGradOp)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::ViewShapeGradOp)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::WarpctcGradOp)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::WarprnntGradOp)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::WeightOnlyLinearGradOp)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::WhereGradOp)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::YoloLossGradOp)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::SiluDoubleGradOp)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::Unpool3dGradOp)

