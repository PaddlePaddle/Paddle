// Auto Generated by decomp_gen.py, DO NOT EDIT!

#include "paddle/fluid/pir/dialect/operator/ir/op_attribute.h"
#include "paddle/fluid/pir/dialect/operator/ir/pd_op.h"
#include "paddle/fluid/pir/dialect/operator/utils/utils.h"
#include "paddle/fluid/primitive/composite/composite.h"
#include "paddle/fluid/primitive/type/lazy_tensor.h"
#include "paddle/phi/api/include/tensor.h"
#include "paddle/phi/common/int_array.h"
#include "paddle/pir/core/builtin_op.h"
#include "paddle/pir/core/op_base.h"

namespace paddle {
namespace dialect {
using IntArray = paddle::experimental::IntArray;

                                                                                                                
std::vector<std::vector<pir::OpResult>> GeluOp::Decomp(pir::Operation* op) {
  VLOG(4) << "Decomp call gelu's decomp interface begin";

  GeluOp op_obj = op->dyn_cast<GeluOp>();
  (void)op_obj;

  FLAGS_tensor_operants_mode = "static";

  VLOG(6) << "Decomp Prepare inputs of gelu";

  Tensor x(std::make_shared<primitive::LazyTensor>(op_obj.x()));

  VLOG(6) << "Decomp prepare attributes of gelu";
  bool approximate = op->attribute("approximate").dyn_cast<pir::BoolAttribute>().data();

  VLOG(6) << "Decomp call gelu's forward composite rule prepare";

  auto org_res = op->results();
  std::vector<std::vector<pir::OpResult>> res(org_res.size());

  VLOG(6) << "Decomp call gelu's forward composite rule begin";
  Tensor op_res = paddle::primitive::details::gelu_decomp<primitive::LazyTensor>(x, approximate);
  VLOG(6) << "Decomp call gelu's forward composite rule end";

  res[0].push_back(
    std::static_pointer_cast<primitive::LazyTensor>(op_res.impl())
        ->value()
        .dyn_cast<pir::OpResult>());

  VLOG(4) << "Decomp call gelu's decomp interface end";
  return res;
}

                         
std::vector<std::vector<pir::OpResult>> InstanceNormOp::Decomp(pir::Operation* op) {
  VLOG(4) << "Decomp call instance_norm's decomp interface begin";

  InstanceNormOp op_obj = op->dyn_cast<InstanceNormOp>();
  (void)op_obj;

  FLAGS_tensor_operants_mode = "static";

  VLOG(6) << "Decomp Prepare inputs of instance_norm";

  Tensor x(std::make_shared<primitive::LazyTensor>(op_obj.x()));
  paddle::optional<Tensor> scale;
  if (!IsEmptyValue(op_obj.scale())){
      scale = paddle::make_optional<Tensor>(Tensor(std::make_shared<primitive::LazyTensor>(op_obj.scale())));
  }
  paddle::optional<Tensor> bias;
  if (!IsEmptyValue(op_obj.bias())){
      bias = paddle::make_optional<Tensor>(Tensor(std::make_shared<primitive::LazyTensor>(op_obj.bias())));
  }

  VLOG(6) << "Decomp prepare attributes of instance_norm";
  float epsilon = op->attribute("epsilon").dyn_cast<pir::FloatAttribute>().data();

  VLOG(6) << "Decomp call instance_norm's forward composite rule prepare";

  auto org_res = op->results();
  std::vector<std::vector<pir::OpResult>> res(org_res.size());

  VLOG(6) << "Decomp call instance_norm's forward composite rule begin";
  std::tuple<Tensor, Tensor, Tensor> op_res = paddle::primitive::details::instance_norm_decomp<primitive::LazyTensor>(
          x, scale, bias, epsilon);
  VLOG(6) << "Decomp call instance_norm's forward composite rule end";
  res[0].push_back(std::static_pointer_cast<primitive::LazyTensor>(std::get<0>(op_res).impl())->value().dyn_cast<pir::OpResult>());
  res[1].push_back(std::static_pointer_cast<primitive::LazyTensor>(std::get<1>(op_res).impl())->value().dyn_cast<pir::OpResult>());
  res[2].push_back(std::static_pointer_cast<primitive::LazyTensor>(std::get<2>(op_res).impl())->value().dyn_cast<pir::OpResult>());

  VLOG(4) << "Decomp call instance_norm's decomp interface end";
  return res;
}

               
std::vector<std::vector<pir::OpResult>> LayerNormOp::Decomp(pir::Operation* op) {
  VLOG(4) << "Decomp call layer_norm's decomp interface begin";

  LayerNormOp op_obj = op->dyn_cast<LayerNormOp>();
  (void)op_obj;

  FLAGS_tensor_operants_mode = "static";

  VLOG(6) << "Decomp Prepare inputs of layer_norm";

  Tensor x(std::make_shared<primitive::LazyTensor>(op_obj.x()));
  paddle::optional<Tensor> scale;
  if (!IsEmptyValue(op_obj.scale())){
      scale = paddle::make_optional<Tensor>(Tensor(std::make_shared<primitive::LazyTensor>(op_obj.scale())));
  }
  paddle::optional<Tensor> bias;
  if (!IsEmptyValue(op_obj.bias())){
      bias = paddle::make_optional<Tensor>(Tensor(std::make_shared<primitive::LazyTensor>(op_obj.bias())));
  }

  VLOG(6) << "Decomp prepare attributes of layer_norm";
  float epsilon = op->attribute("epsilon").dyn_cast<pir::FloatAttribute>().data();
  int begin_norm_axis = op->attribute("begin_norm_axis").dyn_cast<pir::Int32Attribute>().data();

  VLOG(6) << "Decomp call layer_norm's forward composite rule prepare";

  auto org_res = op->results();
  std::vector<std::vector<pir::OpResult>> res(org_res.size());

  VLOG(6) << "Decomp call layer_norm's forward composite rule begin";
  std::tuple<Tensor, Tensor, Tensor> op_res = paddle::primitive::details::layer_norm_decomp<primitive::LazyTensor>(
          x, scale, bias, epsilon, begin_norm_axis);
  VLOG(6) << "Decomp call layer_norm's forward composite rule end";
  res[0].push_back(std::static_pointer_cast<primitive::LazyTensor>(std::get<0>(op_res).impl())->value().dyn_cast<pir::OpResult>());
  res[1].push_back(std::static_pointer_cast<primitive::LazyTensor>(std::get<1>(op_res).impl())->value().dyn_cast<pir::OpResult>());
  res[2].push_back(std::static_pointer_cast<primitive::LazyTensor>(std::get<2>(op_res).impl())->value().dyn_cast<pir::OpResult>());

  VLOG(4) << "Decomp call layer_norm's decomp interface end";
  return res;
}

    
std::vector<std::vector<pir::OpResult>> LeakyReluOp::Decomp(pir::Operation* op) {
  VLOG(4) << "Decomp call leaky_relu's decomp interface begin";

  LeakyReluOp op_obj = op->dyn_cast<LeakyReluOp>();
  (void)op_obj;

  FLAGS_tensor_operants_mode = "static";

  VLOG(6) << "Decomp Prepare inputs of leaky_relu";

  Tensor x(std::make_shared<primitive::LazyTensor>(op_obj.x()));

  VLOG(6) << "Decomp prepare attributes of leaky_relu";
  float negative_slope = op->attribute("negative_slope").dyn_cast<pir::FloatAttribute>().data();

  VLOG(6) << "Decomp call leaky_relu's forward composite rule prepare";

  auto org_res = op->results();
  std::vector<std::vector<pir::OpResult>> res(org_res.size());

  VLOG(6) << "Decomp call leaky_relu's forward composite rule begin";
  Tensor op_res = paddle::primitive::details::leaky_relu_decomp<primitive::LazyTensor>(x, negative_slope);
  VLOG(6) << "Decomp call leaky_relu's forward composite rule end";

  res[0].push_back(
    std::static_pointer_cast<primitive::LazyTensor>(op_res.impl())
        ->value()
        .dyn_cast<pir::OpResult>());

  VLOG(4) << "Decomp call leaky_relu's decomp interface end";
  return res;
}

                                                            
std::vector<std::vector<pir::OpResult>> PowOp::Decomp(pir::Operation* op) {
  VLOG(4) << "Decomp call pow's decomp interface begin";

  PowOp op_obj = op->dyn_cast<PowOp>();
  (void)op_obj;

  FLAGS_tensor_operants_mode = "static";

  VLOG(6) << "Decomp Prepare inputs of pow";

  Tensor x(std::make_shared<primitive::LazyTensor>(op_obj.x()));

  VLOG(6) << "Decomp prepare attributes of pow";
  Scalar y = op->attribute("y").dyn_cast<paddle::dialect::ScalarAttribute>().data();

  VLOG(6) << "Decomp call pow's forward composite rule prepare";

  auto org_res = op->results();
  std::vector<std::vector<pir::OpResult>> res(org_res.size());

  VLOG(6) << "Decomp call pow's forward composite rule begin";
  Tensor op_res = paddle::primitive::details::pow_decomp<primitive::LazyTensor>(x, y);
  VLOG(6) << "Decomp call pow's forward composite rule end";

  res[0].push_back(
    std::static_pointer_cast<primitive::LazyTensor>(op_res.impl())
        ->value()
        .dyn_cast<pir::OpResult>());

  VLOG(4) << "Decomp call pow's decomp interface end";
  return res;
}

            
std::vector<std::vector<pir::OpResult>> ReluOp::Decomp(pir::Operation* op) {
  VLOG(4) << "Decomp call relu's decomp interface begin";

  ReluOp op_obj = op->dyn_cast<ReluOp>();
  (void)op_obj;

  FLAGS_tensor_operants_mode = "static";

  VLOG(6) << "Decomp Prepare inputs of relu";

  Tensor x(std::make_shared<primitive::LazyTensor>(op_obj.x()));

  VLOG(6) << "Decomp prepare attributes of relu";

  VLOG(6) << "Decomp call relu's forward composite rule prepare";

  auto org_res = op->results();
  std::vector<std::vector<pir::OpResult>> res(org_res.size());

  VLOG(6) << "Decomp call relu's forward composite rule begin";
  Tensor op_res = paddle::primitive::details::relu_decomp<primitive::LazyTensor>(x);
  VLOG(6) << "Decomp call relu's forward composite rule end";

  res[0].push_back(
    std::static_pointer_cast<primitive::LazyTensor>(op_res.impl())
        ->value()
        .dyn_cast<pir::OpResult>());

  VLOG(4) << "Decomp call relu's decomp interface end";
  return res;
}

              
std::vector<std::vector<pir::OpResult>> RsqrtOp::Decomp(pir::Operation* op) {
  VLOG(4) << "Decomp call rsqrt's decomp interface begin";

  RsqrtOp op_obj = op->dyn_cast<RsqrtOp>();
  (void)op_obj;

  FLAGS_tensor_operants_mode = "static";

  VLOG(6) << "Decomp Prepare inputs of rsqrt";

  Tensor x(std::make_shared<primitive::LazyTensor>(op_obj.x()));

  VLOG(6) << "Decomp prepare attributes of rsqrt";

  VLOG(6) << "Decomp call rsqrt's forward composite rule prepare";

  auto org_res = op->results();
  std::vector<std::vector<pir::OpResult>> res(org_res.size());

  VLOG(6) << "Decomp call rsqrt's forward composite rule begin";
  Tensor op_res = paddle::primitive::details::rsqrt_decomp<primitive::LazyTensor>(x);
  VLOG(6) << "Decomp call rsqrt's forward composite rule end";

  res[0].push_back(
    std::static_pointer_cast<primitive::LazyTensor>(op_res.impl())
        ->value()
        .dyn_cast<pir::OpResult>());

  VLOG(4) << "Decomp call rsqrt's decomp interface end";
  return res;
}

                
std::vector<std::vector<pir::OpResult>> SigmoidOp::Decomp(pir::Operation* op) {
  VLOG(4) << "Decomp call sigmoid's decomp interface begin";

  SigmoidOp op_obj = op->dyn_cast<SigmoidOp>();
  (void)op_obj;

  FLAGS_tensor_operants_mode = "static";

  VLOG(6) << "Decomp Prepare inputs of sigmoid";

  Tensor x(std::make_shared<primitive::LazyTensor>(op_obj.x()));

  VLOG(6) << "Decomp prepare attributes of sigmoid";

  VLOG(6) << "Decomp call sigmoid's forward composite rule prepare";

  auto org_res = op->results();
  std::vector<std::vector<pir::OpResult>> res(org_res.size());

  VLOG(6) << "Decomp call sigmoid's forward composite rule begin";
  Tensor op_res = paddle::primitive::details::sigmoid_decomp<primitive::LazyTensor>(x);
  VLOG(6) << "Decomp call sigmoid's forward composite rule end";

  res[0].push_back(
    std::static_pointer_cast<primitive::LazyTensor>(op_res.impl())
        ->value()
        .dyn_cast<pir::OpResult>());

  VLOG(4) << "Decomp call sigmoid's decomp interface end";
  return res;
}

      
std::vector<std::vector<pir::OpResult>> SiluOp::Decomp(pir::Operation* op) {
  VLOG(4) << "Decomp call silu's decomp interface begin";

  SiluOp op_obj = op->dyn_cast<SiluOp>();
  (void)op_obj;

  FLAGS_tensor_operants_mode = "static";

  VLOG(6) << "Decomp Prepare inputs of silu";

  Tensor x(std::make_shared<primitive::LazyTensor>(op_obj.x()));

  VLOG(6) << "Decomp prepare attributes of silu";

  VLOG(6) << "Decomp call silu's forward composite rule prepare";

  auto org_res = op->results();
  std::vector<std::vector<pir::OpResult>> res(org_res.size());

  VLOG(6) << "Decomp call silu's forward composite rule begin";
  Tensor op_res = paddle::primitive::details::silu_decomp<primitive::LazyTensor>(x);
  VLOG(6) << "Decomp call silu's forward composite rule end";

  res[0].push_back(
    std::static_pointer_cast<primitive::LazyTensor>(op_res.impl())
        ->value()
        .dyn_cast<pir::OpResult>());

  VLOG(4) << "Decomp call silu's decomp interface end";
  return res;
}

            
std::vector<std::vector<pir::OpResult>> SqrtOp::Decomp(pir::Operation* op) {
  VLOG(4) << "Decomp call sqrt's decomp interface begin";

  SqrtOp op_obj = op->dyn_cast<SqrtOp>();
  (void)op_obj;

  FLAGS_tensor_operants_mode = "static";

  VLOG(6) << "Decomp Prepare inputs of sqrt";

  Tensor x(std::make_shared<primitive::LazyTensor>(op_obj.x()));

  VLOG(6) << "Decomp prepare attributes of sqrt";

  VLOG(6) << "Decomp call sqrt's forward composite rule prepare";

  auto org_res = op->results();
  std::vector<std::vector<pir::OpResult>> res(org_res.size());

  VLOG(6) << "Decomp call sqrt's forward composite rule begin";
  Tensor op_res = paddle::primitive::details::sqrt_decomp<primitive::LazyTensor>(x);
  VLOG(6) << "Decomp call sqrt's forward composite rule end";

  res[0].push_back(
    std::static_pointer_cast<primitive::LazyTensor>(op_res.impl())
        ->value()
        .dyn_cast<pir::OpResult>());

  VLOG(4) << "Decomp call sqrt's decomp interface end";
  return res;
}

      
std::vector<std::vector<pir::OpResult>> SqueezeOp::Decomp(pir::Operation* op) {
  VLOG(4) << "Decomp call squeeze's decomp interface begin";

  SqueezeOp op_obj = op->dyn_cast<SqueezeOp>();
  (void)op_obj;

  FLAGS_tensor_operants_mode = "static";

  VLOG(6) << "Decomp Prepare inputs of squeeze";

  Tensor x(std::make_shared<primitive::LazyTensor>(op_obj.x()));

  VLOG(6) << "Decomp prepare attributes of squeeze";

  Tensor axis_(std::make_shared<primitive::LazyTensor>(op_obj.axis()));

  auto* axis_define_op =
      std::static_pointer_cast<primitive::LazyTensor>(axis_.impl())
          ->value()
          .dyn_cast<pir::OpResult>()
          .owner();
  if (axis_define_op->name() != "pd_op.full_int_array") {
    PADDLE_THROW(
        platform::errors::Unimplemented("We don't support dynamic tensors "
                                        "attribute axis for squeeze decomposition "
                                        "for now. "));
  }
  IntArray axis = phi::IntArray(
      paddle::dialect::GetInt64Vector(axis_define_op->attribute("value")));


  VLOG(6) << "Decomp call squeeze's forward composite rule prepare";

  auto org_res = op->results();
  std::vector<std::vector<pir::OpResult>> res(org_res.size());

  VLOG(6) << "Decomp call squeeze's forward composite rule begin";
  std::tuple<Tensor, Tensor> op_res = paddle::primitive::details::squeeze_decomp<primitive::LazyTensor>(
          x, axis);
  VLOG(6) << "Decomp call squeeze's forward composite rule end";
  res[0].push_back(std::static_pointer_cast<primitive::LazyTensor>(std::get<0>(op_res).impl())->value().dyn_cast<pir::OpResult>());
  pir::OpResult xshape;
  res[1].push_back(xshape);

  VLOG(4) << "Decomp call squeeze's decomp interface end";
  return res;
}

    
std::vector<std::vector<pir::OpResult>> StackOp::Decomp(pir::Operation* op) {
  VLOG(4) << "Decomp call stack's decomp interface begin";

  StackOp op_obj = op->dyn_cast<StackOp>();
  (void)op_obj;

  FLAGS_tensor_operants_mode = "static";

  VLOG(6) << "Decomp Prepare inputs of stack";

  pir::CombineOp combine_op_obj_x =
    op_obj.x().dyn_cast<pir::OpResult>().owner()->dyn_cast<pir::CombineOp>();
  std::vector<Tensor> x;
  for (size_t idx = 0; idx < combine_op_obj_x.inputs().size(); idx++) {
      x.emplace_back(
          std::make_shared<primitive::LazyTensor>(combine_op_obj_x.inputs()[idx]));
  }

  VLOG(6) << "Decomp prepare attributes of stack";
  int axis = op->attribute("axis").dyn_cast<pir::Int32Attribute>().data();

  VLOG(6) << "Decomp call stack's forward composite rule prepare";

  auto org_res = op->results();
  std::vector<std::vector<pir::OpResult>> res(org_res.size());

  VLOG(6) << "Decomp call stack's forward composite rule begin";
  Tensor op_res = paddle::primitive::details::stack_decomp<primitive::LazyTensor>(x, axis);
  VLOG(6) << "Decomp call stack's forward composite rule end";

  res[0].push_back(
    std::static_pointer_cast<primitive::LazyTensor>(op_res.impl())
        ->value()
        .dyn_cast<pir::OpResult>());

  VLOG(4) << "Decomp call stack's decomp interface end";
  return res;
}

                         
std::vector<std::vector<pir::OpResult>> UnsqueezeOp::Decomp(pir::Operation* op) {
  VLOG(4) << "Decomp call unsqueeze's decomp interface begin";

  UnsqueezeOp op_obj = op->dyn_cast<UnsqueezeOp>();
  (void)op_obj;

  FLAGS_tensor_operants_mode = "static";

  VLOG(6) << "Decomp Prepare inputs of unsqueeze";

  Tensor x(std::make_shared<primitive::LazyTensor>(op_obj.x()));

  VLOG(6) << "Decomp prepare attributes of unsqueeze";

  Tensor axis_(std::make_shared<primitive::LazyTensor>(op_obj.axis()));

  auto* axis_define_op =
      std::static_pointer_cast<primitive::LazyTensor>(axis_.impl())
          ->value()
          .dyn_cast<pir::OpResult>()
          .owner();
  if (axis_define_op->name() != "pd_op.full_int_array") {
    PADDLE_THROW(
        platform::errors::Unimplemented("We don't support dynamic tensors "
                                        "attribute axis for unsqueeze decomposition "
                                        "for now. "));
  }
  IntArray axis = phi::IntArray(
      paddle::dialect::GetInt64Vector(axis_define_op->attribute("value")));


  VLOG(6) << "Decomp call unsqueeze's forward composite rule prepare";

  auto org_res = op->results();
  std::vector<std::vector<pir::OpResult>> res(org_res.size());

  VLOG(6) << "Decomp call unsqueeze's forward composite rule begin";
  std::tuple<Tensor, Tensor> op_res = paddle::primitive::details::unsqueeze_decomp<primitive::LazyTensor>(
          x, axis);
  VLOG(6) << "Decomp call unsqueeze's forward composite rule end";
  res[0].push_back(std::static_pointer_cast<primitive::LazyTensor>(std::get<0>(op_res).impl())->value().dyn_cast<pir::OpResult>());
  pir::OpResult xshape;
  res[1].push_back(xshape);

  VLOG(4) << "Decomp call unsqueeze's decomp interface end";
  return res;
}

                    
std::vector<std::vector<pir::OpResult>> AddNOp::Decomp(pir::Operation* op) {
  VLOG(4) << "Decomp call add_n's decomp interface begin";

  AddNOp op_obj = op->dyn_cast<AddNOp>();
  (void)op_obj;

  FLAGS_tensor_operants_mode = "static";

  VLOG(6) << "Decomp Prepare inputs of add_n";

  pir::CombineOp combine_op_obj_inputs =
    op_obj.inputs().dyn_cast<pir::OpResult>().owner()->dyn_cast<pir::CombineOp>();
  std::vector<Tensor> inputs;
  for (size_t idx = 0; idx < combine_op_obj_inputs.inputs().size(); idx++) {
      inputs.emplace_back(
          std::make_shared<primitive::LazyTensor>(combine_op_obj_inputs.inputs()[idx]));
  }

  VLOG(6) << "Decomp prepare attributes of add_n";

  VLOG(6) << "Decomp call add_n's forward composite rule prepare";

  auto org_res = op->results();
  std::vector<std::vector<pir::OpResult>> res(org_res.size());

  VLOG(6) << "Decomp call add_n's forward composite rule begin";
  Tensor op_res = paddle::primitive::details::add_n_decomp<primitive::LazyTensor>(inputs);
  VLOG(6) << "Decomp call add_n's forward composite rule end";

  res[0].push_back(
    std::static_pointer_cast<primitive::LazyTensor>(op_res.impl())
        ->value()
        .dyn_cast<pir::OpResult>());

  VLOG(4) << "Decomp call add_n's decomp interface end";
  return res;
}

                                      
std::vector<std::vector<pir::OpResult>> DropoutOp::Decomp(pir::Operation* op) {
  VLOG(4) << "Decomp call dropout's decomp interface begin";

  DropoutOp op_obj = op->dyn_cast<DropoutOp>();
  (void)op_obj;

  FLAGS_tensor_operants_mode = "static";

  VLOG(6) << "Decomp Prepare inputs of dropout";

  Tensor x(std::make_shared<primitive::LazyTensor>(op_obj.x()));
  paddle::optional<Tensor> seed_tensor;
  if (!IsEmptyValue(op_obj.seed_tensor())){
      seed_tensor = paddle::make_optional<Tensor>(Tensor(std::make_shared<primitive::LazyTensor>(op_obj.seed_tensor())));
  }

  VLOG(6) << "Decomp prepare attributes of dropout";
  Scalar p = op->attribute("p").dyn_cast<paddle::dialect::ScalarAttribute>().data();
  bool is_test = op->attribute("is_test").dyn_cast<pir::BoolAttribute>().data();
  const std::string& mode = op->attribute("mode").dyn_cast<pir::StrAttribute>().AsString();
  int seed = op->attribute("seed").dyn_cast<pir::Int32Attribute>().data();
  bool fix_seed = op->attribute("fix_seed").dyn_cast<pir::BoolAttribute>().data();

  VLOG(6) << "Decomp call dropout's forward composite rule prepare";

  auto org_res = op->results();
  std::vector<std::vector<pir::OpResult>> res(org_res.size());

  VLOG(6) << "Decomp call dropout's forward composite rule begin";
  std::tuple<Tensor, Tensor> op_res = paddle::primitive::details::dropout_decomp<primitive::LazyTensor>(
          x, seed_tensor, p, is_test, mode, seed, fix_seed);
  VLOG(6) << "Decomp call dropout's forward composite rule end";
  res[0].push_back(std::static_pointer_cast<primitive::LazyTensor>(std::get<0>(op_res).impl())->value().dyn_cast<pir::OpResult>());
  res[1].push_back(std::static_pointer_cast<primitive::LazyTensor>(std::get<1>(op_res).impl())->value().dyn_cast<pir::OpResult>());

  VLOG(4) << "Decomp call dropout's decomp interface end";
  return res;
}

                    
std::vector<std::vector<pir::OpResult>> FullLikeOp::Decomp(pir::Operation* op) {
  VLOG(4) << "Decomp call full_like's decomp interface begin";

  FullLikeOp op_obj = op->dyn_cast<FullLikeOp>();
  (void)op_obj;

  FLAGS_tensor_operants_mode = "static";

  VLOG(6) << "Decomp Prepare inputs of full_like";

  Tensor x(std::make_shared<primitive::LazyTensor>(op_obj.x()));

  VLOG(6) << "Decomp prepare attributes of full_like";

  Tensor value_(std::make_shared<primitive::LazyTensor>(op_obj.value()));

  auto* value_define_op =
      std::static_pointer_cast<primitive::LazyTensor>(value_.impl())
          ->value()
          .dyn_cast<pir::OpResult>()
          .owner();
  if (value_define_op->name() != "pd_op.full") {
    PADDLE_THROW(
        platform::errors::Unimplemented("We don't support dynamic tensors "
                                        "attribute value for full_like decomposition "
                                        "for now. "));
  }
  Scalar value = value_define_op->attribute("value").dyn_cast<paddle::dialect::ScalarAttribute>().data();

  DataType dtype = op->attribute("dtype").dyn_cast<paddle::dialect::DataTypeAttribute>().data();
  const Place& place = op->attribute("place").dyn_cast<paddle::dialect::PlaceAttribute>().data();

  VLOG(6) << "Decomp call full_like's forward composite rule prepare";

  auto org_res = op->results();
  std::vector<std::vector<pir::OpResult>> res(org_res.size());

  VLOG(6) << "Decomp call full_like's forward composite rule begin";
  Tensor op_res = paddle::primitive::details::full_like_decomp<primitive::LazyTensor>(x, value, dtype, place);
  VLOG(6) << "Decomp call full_like's forward composite rule end";

  res[0].push_back(
    std::static_pointer_cast<primitive::LazyTensor>(op_res.impl())
        ->value()
        .dyn_cast<pir::OpResult>());

  VLOG(4) << "Decomp call full_like's decomp interface end";
  return res;
}

                            
std::vector<std::vector<pir::OpResult>> MeanOp::Decomp(pir::Operation* op) {
  VLOG(4) << "Decomp call mean's decomp interface begin";

  MeanOp op_obj = op->dyn_cast<MeanOp>();
  (void)op_obj;

  FLAGS_tensor_operants_mode = "static";

  VLOG(6) << "Decomp Prepare inputs of mean";

  Tensor x(std::make_shared<primitive::LazyTensor>(op_obj.x()));

  VLOG(6) << "Decomp prepare attributes of mean";
  IntArray axis = op->attribute("axis").dyn_cast<paddle::dialect::IntArrayAttribute>().data();
  bool keepdim = op->attribute("keepdim").dyn_cast<pir::BoolAttribute>().data();

  VLOG(6) << "Decomp call mean's forward composite rule prepare";

  auto org_res = op->results();
  std::vector<std::vector<pir::OpResult>> res(org_res.size());

  VLOG(6) << "Decomp call mean's forward composite rule begin";
  Tensor op_res = paddle::primitive::details::mean_decomp<primitive::LazyTensor>(x, axis, keepdim);
  VLOG(6) << "Decomp call mean's forward composite rule end";

  res[0].push_back(
    std::static_pointer_cast<primitive::LazyTensor>(op_res.impl())
        ->value()
        .dyn_cast<pir::OpResult>());

  VLOG(4) << "Decomp call mean's decomp interface end";
  return res;
}

                                          
std::vector<std::vector<pir::OpResult>> SoftmaxOp::Decomp(pir::Operation* op) {
  VLOG(4) << "Decomp call softmax's decomp interface begin";

  SoftmaxOp op_obj = op->dyn_cast<SoftmaxOp>();
  (void)op_obj;

  FLAGS_tensor_operants_mode = "static";

  VLOG(6) << "Decomp Prepare inputs of softmax";

  Tensor x(std::make_shared<primitive::LazyTensor>(op_obj.x()));

  VLOG(6) << "Decomp prepare attributes of softmax";
  int axis = op->attribute("axis").dyn_cast<pir::Int32Attribute>().data();

  VLOG(6) << "Decomp call softmax's forward composite rule prepare";

  auto org_res = op->results();
  std::vector<std::vector<pir::OpResult>> res(org_res.size());

  VLOG(6) << "Decomp call softmax's forward composite rule begin";
  Tensor op_res = paddle::primitive::details::softmax_decomp<primitive::LazyTensor>(x, axis);
  VLOG(6) << "Decomp call softmax's forward composite rule end";

  res[0].push_back(
    std::static_pointer_cast<primitive::LazyTensor>(op_res.impl())
        ->value()
        .dyn_cast<pir::OpResult>());

  VLOG(4) << "Decomp call softmax's decomp interface end";
  return res;
}

                                  
}  // namespace dialect
}  // namespace paddle
