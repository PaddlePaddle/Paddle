// This file is generated by "paddle/fluid/pir/dialect/op_generator/op_gen.py"
#include "/home/aistudio/fix_op/Paddle/paddle/fluid/pir/dialect/operator/ir/pd_op.h"
#include "paddle/fluid/pir/dialect/operator/ir/op_type.h"
#include "paddle/fluid/pir/dialect/operator/ir/op_attribute.h"
#include "paddle/fluid/pir/dialect/operator/ir/ir_tensor.h"
#include "paddle/fluid/pir/dialect/operator/ir/ir_selected_rows.h"
#include "paddle/fluid/pir/dialect/operator/ir/ir_meta_tensor.h"
#include "paddle/pir/core/builtin_attribute.h"
#include "paddle/pir/core/builtin_type.h"
#include "paddle/pir/core/builtin_op.h"
#include "paddle/pir/core/ir_context.h"
#include "paddle/phi/core/enforce.h"
#include "paddle/phi/core/dense_tensor.h"
#include "paddle/phi/infermeta/binary.h"
#include "paddle/phi/infermeta/multiary.h"
#include "paddle/phi/infermeta/nullary.h"
#include "paddle/phi/infermeta/unary.h"
#include "paddle/phi/infermeta/ternary.h"
#include "paddle/phi/infermeta/backward.h"
#include "paddle/phi/infermeta/fusion.h"
#include "paddle/phi/api/lib/utils/allocator.h"
#include "paddle/fluid/primitive/rule/vjp/vjp.h"
#include "paddle/pir/core/op_base.h"

namespace paddle {
namespace dialect {

const char *FusedBiasDropoutResidualLayerNormGradOp::attributes_name[6] = { "dropout_rate", "is_test", "dropout_fix_seed", "dropout_seed", "dropout_implementation", "ln_epsilon" };

OpInfoTuple FusedBiasDropoutResidualLayerNormGradOp::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("y_grad", "paddle::dialect::DenseTensorType", false, false, false, false), paddle::dialect::OpInputInfo("x", "paddle::dialect::DenseTensorType", false, false, false, false), paddle::dialect::OpInputInfo("residual", "paddle::dialect::DenseTensorType", false, false, false, false), paddle::dialect::OpInputInfo("bias", "paddle::dialect::DenseTensorType", true, false, false, false), paddle::dialect::OpInputInfo("ln_scale", "paddle::dialect::DenseTensorType", true, false, false, false), paddle::dialect::OpInputInfo("ln_bias", "paddle::dialect::DenseTensorType", true, false, false, false), paddle::dialect::OpInputInfo("ln_mean", "paddle::dialect::DenseTensorType", false, false, false, false), paddle::dialect::OpInputInfo("ln_variance", "paddle::dialect::DenseTensorType", false, false, false, false), paddle::dialect::OpInputInfo("bias_dropout_residual_out", "paddle::dialect::DenseTensorType", false, false, false, false), paddle::dialect::OpInputInfo("dropout_mask_out", "paddle::dialect::DenseTensorType", false, false, false, false) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = { paddle::dialect::OpAttributeInfo("dropout_rate", "pir::FloatAttribute", ""), paddle::dialect::OpAttributeInfo("is_test", "pir::BoolAttribute", ""), paddle::dialect::OpAttributeInfo("dropout_fix_seed", "pir::BoolAttribute", ""), paddle::dialect::OpAttributeInfo("dropout_seed", "pir::Int32Attribute", ""), paddle::dialect::OpAttributeInfo("dropout_implementation", "pir::StrAttribute", ""), paddle::dialect::OpAttributeInfo("ln_epsilon", "pir::FloatAttribute", "") };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("x_grad", "paddle::dialect::DenseTensorType", false, false), paddle::dialect::OpOutputInfo("residual_grad", "paddle::dialect::DenseTensorType", false, false), paddle::dialect::OpOutputInfo("bias_grad", "paddle::dialect::DenseTensorType", true, false), paddle::dialect::OpOutputInfo("ln_scale_grad", "paddle::dialect::DenseTensorType", true, false), paddle::dialect::OpOutputInfo("ln_bias_grad", "paddle::dialect::DenseTensorType", true, false) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("FusedBiasDropoutResidualLnGradInferMeta", {"y_grad", "x", "residual", "bias", "ln_scale", "ln_bias", "ln_mean", "ln_variance", "bias_dropout_residual_out", "dropout_mask_out", "dropout_rate", "is_test", "dropout_fix_seed", "dropout_seed", "dropout_implementation", "ln_epsilon"}, "fused_bias_dropout_residual_layer_norm_grad", {"y_grad", "x", "residual", "bias", "ln_scale", "ln_bias", "ln_mean", "ln_variance", "bias_dropout_residual_out", "dropout_mask_out", "dropout_rate", "is_test", "dropout_fix_seed", "dropout_seed", "dropout_implementation", "ln_epsilon"}, {"y_grad"}, {}, {}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "fused_bias_dropout_residual_layer_norm_grad");
}

void FusedBiasDropoutResidualLayerNormGradOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value y_grad_, pir::Value x_, pir::Value residual_, pir::Value bias_, pir::Value ln_scale_, pir::Value ln_bias_, pir::Value ln_mean_, pir::Value ln_variance_, pir::Value bias_dropout_residual_out_, pir::Value dropout_mask_out_, float dropout_rate, bool is_test, bool dropout_fix_seed, int dropout_seed, const std::string& dropout_implementation, float ln_epsilon) {
  VLOG(4) << "Start build FusedBiasDropoutResidualLayerNormGradOp";



  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {y_grad_, x_, residual_, bias_, ln_scale_, ln_bias_, ln_mean_, ln_variance_, bias_dropout_residual_out_, dropout_mask_out_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_dropout_rate = pir::FloatAttribute::get(pir::IrContext::Instance(), dropout_rate);
  argument.AddAttribute("dropout_rate", attr_dropout_rate);
  pir::Attribute attr_is_test = pir::BoolAttribute::get(pir::IrContext::Instance(), is_test);
  argument.AddAttribute("is_test", attr_is_test);
  pir::Attribute attr_dropout_fix_seed = pir::BoolAttribute::get(pir::IrContext::Instance(), dropout_fix_seed);
  argument.AddAttribute("dropout_fix_seed", attr_dropout_fix_seed);
  pir::Attribute attr_dropout_seed = pir::Int32Attribute::get(pir::IrContext::Instance(), dropout_seed);
  argument.AddAttribute("dropout_seed", attr_dropout_seed);
  pir::Attribute attr_dropout_implementation = pir::StrAttribute::get(pir::IrContext::Instance(), dropout_implementation);
  argument.AddAttribute("dropout_implementation", attr_dropout_implementation);
  pir::Attribute attr_ln_epsilon = pir::FloatAttribute::get(pir::IrContext::Instance(), ln_epsilon);
  argument.AddAttribute("ln_epsilon", attr_ln_epsilon);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType y_grad = y_grad_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)y_grad;
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;
  paddle::dialect::DenseTensorType residual = residual_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)residual;
  paddle::dialect::DenseTensorType ln_mean = ln_mean_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)ln_mean;
  paddle::dialect::DenseTensorType ln_variance = ln_variance_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)ln_variance;
  paddle::dialect::DenseTensorType bias_dropout_residual_out = bias_dropout_residual_out_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)bias_dropout_residual_out;
  paddle::dialect::DenseTensorType dropout_mask_out = dropout_mask_out_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)dropout_mask_out;

  VLOG(4) << "Builder construction  dense_y_grad";
  paddle::dialect::IrTensor ir_tensor_y_grad(paddle::dialect::TransToPhiDataType(y_grad.dtype()),
                                                      y_grad.dims(),
                                                      y_grad.data_layout(),
                                                      y_grad.lod(),
                                                      y_grad.offset());
  VLOG(4) << "Builder construction  meta_y_grad";
  paddle::dialect::IrMetaTensor meta_y_grad(&ir_tensor_y_grad);

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);

  VLOG(4) << "Builder construction  dense_residual";
  paddle::dialect::IrTensor ir_tensor_residual(paddle::dialect::TransToPhiDataType(residual.dtype()),
                                                      residual.dims(),
                                                      residual.data_layout(),
                                                      residual.lod(),
                                                      residual.offset());
  VLOG(4) << "Builder construction  meta_residual";
  paddle::dialect::IrMetaTensor meta_residual(&ir_tensor_residual);

  paddle::dialect::IrMetaTensor meta_bias;
  paddle::dialect::IrTensor ir_tensor_bias;
  if (bias_.impl() != nullptr) {
    paddle::dialect::DenseTensorType bias = bias_.type().dyn_cast<paddle::dialect::DenseTensorType>();
    VLOG(4) << "Builder construction  dense_bias";
    ir_tensor_bias = paddle::dialect::IrTensor(paddle::dialect::TransToPhiDataType(bias.dtype()),
                                                        bias.dims(),
                                                        bias.data_layout(),
                                                        bias.lod(),
                                                        bias.offset());
    VLOG(4) << "Builder construction  meta_bias";
    meta_bias = paddle::dialect::IrMetaTensor(&ir_tensor_bias);
  }


  paddle::dialect::IrMetaTensor meta_ln_scale;
  paddle::dialect::IrTensor ir_tensor_ln_scale;
  if (ln_scale_.impl() != nullptr) {
    paddle::dialect::DenseTensorType ln_scale = ln_scale_.type().dyn_cast<paddle::dialect::DenseTensorType>();
    VLOG(4) << "Builder construction  dense_ln_scale";
    ir_tensor_ln_scale = paddle::dialect::IrTensor(paddle::dialect::TransToPhiDataType(ln_scale.dtype()),
                                                        ln_scale.dims(),
                                                        ln_scale.data_layout(),
                                                        ln_scale.lod(),
                                                        ln_scale.offset());
    VLOG(4) << "Builder construction  meta_ln_scale";
    meta_ln_scale = paddle::dialect::IrMetaTensor(&ir_tensor_ln_scale);
  }


  paddle::dialect::IrMetaTensor meta_ln_bias;
  paddle::dialect::IrTensor ir_tensor_ln_bias;
  if (ln_bias_.impl() != nullptr) {
    paddle::dialect::DenseTensorType ln_bias = ln_bias_.type().dyn_cast<paddle::dialect::DenseTensorType>();
    VLOG(4) << "Builder construction  dense_ln_bias";
    ir_tensor_ln_bias = paddle::dialect::IrTensor(paddle::dialect::TransToPhiDataType(ln_bias.dtype()),
                                                        ln_bias.dims(),
                                                        ln_bias.data_layout(),
                                                        ln_bias.lod(),
                                                        ln_bias.offset());
    VLOG(4) << "Builder construction  meta_ln_bias";
    meta_ln_bias = paddle::dialect::IrMetaTensor(&ir_tensor_ln_bias);
  }


  VLOG(4) << "Builder construction  dense_ln_mean";
  paddle::dialect::IrTensor ir_tensor_ln_mean(paddle::dialect::TransToPhiDataType(ln_mean.dtype()),
                                                      ln_mean.dims(),
                                                      ln_mean.data_layout(),
                                                      ln_mean.lod(),
                                                      ln_mean.offset());
  VLOG(4) << "Builder construction  meta_ln_mean";
  paddle::dialect::IrMetaTensor meta_ln_mean(&ir_tensor_ln_mean);

  VLOG(4) << "Builder construction  dense_ln_variance";
  paddle::dialect::IrTensor ir_tensor_ln_variance(paddle::dialect::TransToPhiDataType(ln_variance.dtype()),
                                                      ln_variance.dims(),
                                                      ln_variance.data_layout(),
                                                      ln_variance.lod(),
                                                      ln_variance.offset());
  VLOG(4) << "Builder construction  meta_ln_variance";
  paddle::dialect::IrMetaTensor meta_ln_variance(&ir_tensor_ln_variance);

  VLOG(4) << "Builder construction  dense_bias_dropout_residual_out";
  paddle::dialect::IrTensor ir_tensor_bias_dropout_residual_out(paddle::dialect::TransToPhiDataType(bias_dropout_residual_out.dtype()),
                                                      bias_dropout_residual_out.dims(),
                                                      bias_dropout_residual_out.data_layout(),
                                                      bias_dropout_residual_out.lod(),
                                                      bias_dropout_residual_out.offset());
  VLOG(4) << "Builder construction  meta_bias_dropout_residual_out";
  paddle::dialect::IrMetaTensor meta_bias_dropout_residual_out(&ir_tensor_bias_dropout_residual_out);

  VLOG(4) << "Builder construction  dense_dropout_mask_out";
  paddle::dialect::IrTensor ir_tensor_dropout_mask_out(paddle::dialect::TransToPhiDataType(dropout_mask_out.dtype()),
                                                      dropout_mask_out.dims(),
                                                      dropout_mask_out.data_layout(),
                                                      dropout_mask_out.lod(),
                                                      dropout_mask_out.offset());
  VLOG(4) << "Builder construction  meta_dropout_mask_out";
  paddle::dialect::IrMetaTensor meta_dropout_mask_out(&ir_tensor_dropout_mask_out);
  paddle::dialect::IrTensor dense_x_grad;
  paddle::dialect::IrMetaTensor meta_x_grad(&dense_x_grad);
  paddle::dialect::IrTensor dense_residual_grad;
  paddle::dialect::IrMetaTensor meta_residual_grad(&dense_residual_grad);
  paddle::dialect::IrTensor dense_bias_grad;
  paddle::dialect::IrMetaTensor meta_bias_grad(&dense_bias_grad);
  paddle::dialect::IrTensor dense_ln_scale_grad;
  paddle::dialect::IrMetaTensor meta_ln_scale_grad(&dense_ln_scale_grad);
  paddle::dialect::IrTensor dense_ln_bias_grad;
  paddle::dialect::IrMetaTensor meta_ln_bias_grad(&dense_ln_bias_grad);

  phi::FusedBiasDropoutResidualLnGradInferMeta(meta_y_grad, meta_x, meta_residual, meta_bias, meta_ln_scale, meta_ln_bias, meta_ln_mean, meta_ln_variance, meta_bias_dropout_residual_out, meta_dropout_mask_out, dropout_rate, is_test, dropout_fix_seed, dropout_seed, dropout_implementation, ln_epsilon, &meta_x_grad, &meta_residual_grad, &meta_bias_grad, &meta_ln_scale_grad, &meta_ln_bias_grad);

  std::vector<pir::Type> argument_outputs;
  pir::Type x_grad_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_x_grad.dtype()), dense_x_grad.dims(), dense_x_grad.layout(), dense_x_grad.lod(), dense_x_grad.offset());
  argument_outputs.push_back(x_grad_dense_tensor_type);

  pir::Type residual_grad_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_residual_grad.dtype()), dense_residual_grad.dims(), dense_residual_grad.layout(), dense_residual_grad.lod(), dense_residual_grad.offset());
  argument_outputs.push_back(residual_grad_dense_tensor_type);

  pir::Type bias_grad_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_bias_grad.dtype()), dense_bias_grad.dims(), dense_bias_grad.layout(), dense_bias_grad.lod(), dense_bias_grad.offset());
  argument_outputs.push_back(bias_grad_dense_tensor_type);

  pir::Type ln_scale_grad_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_ln_scale_grad.dtype()), dense_ln_scale_grad.dims(), dense_ln_scale_grad.layout(), dense_ln_scale_grad.lod(), dense_ln_scale_grad.offset());
  argument_outputs.push_back(ln_scale_grad_dense_tensor_type);

  pir::Type ln_bias_grad_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_ln_bias_grad.dtype()), dense_ln_bias_grad.dims(), dense_ln_bias_grad.layout(), dense_ln_bias_grad.lod(), dense_ln_bias_grad.offset());
  argument_outputs.push_back(ln_bias_grad_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void FusedBiasDropoutResidualLayerNormGradOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value y_grad_, pir::Value x_, pir::Value residual_, pir::Value bias_, pir::Value ln_scale_, pir::Value ln_bias_, pir::Value ln_mean_, pir::Value ln_variance_, pir::Value bias_dropout_residual_out_, pir::Value dropout_mask_out_, pir::AttributeMap attributes) {
  VLOG(4) << "Start build FusedBiasDropoutResidualLayerNormGradOp";


  IR_ENFORCE(
      attributes.find("dropout_rate") != attributes.end(),
          "'dropout_rate' Attribute is expected for FusedBiasDropoutResidualLayerNormGradOp. ");
  float dropout_rate = attributes.at("dropout_rate").dyn_cast<pir::FloatAttribute>().data();

  IR_ENFORCE(
      attributes.find("is_test") != attributes.end(),
          "'is_test' Attribute is expected for FusedBiasDropoutResidualLayerNormGradOp. ");
  bool is_test = attributes.at("is_test").dyn_cast<pir::BoolAttribute>().data();

  IR_ENFORCE(
      attributes.find("dropout_fix_seed") != attributes.end(),
          "'dropout_fix_seed' Attribute is expected for FusedBiasDropoutResidualLayerNormGradOp. ");
  bool dropout_fix_seed = attributes.at("dropout_fix_seed").dyn_cast<pir::BoolAttribute>().data();

  IR_ENFORCE(
      attributes.find("dropout_seed") != attributes.end(),
          "'dropout_seed' Attribute is expected for FusedBiasDropoutResidualLayerNormGradOp. ");
  int dropout_seed = attributes.at("dropout_seed").dyn_cast<pir::Int32Attribute>().data();

  IR_ENFORCE(
      attributes.find("dropout_implementation") != attributes.end(),
          "'dropout_implementation' Attribute is expected for FusedBiasDropoutResidualLayerNormGradOp. ");
  std::string dropout_implementation = attributes.at("dropout_implementation").dyn_cast<pir::StrAttribute>().AsString();

  IR_ENFORCE(
      attributes.find("ln_epsilon") != attributes.end(),
          "'ln_epsilon' Attribute is expected for FusedBiasDropoutResidualLayerNormGradOp. ");
  float ln_epsilon = attributes.at("ln_epsilon").dyn_cast<pir::FloatAttribute>().data();


  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {y_grad_, x_, residual_, bias_, ln_scale_, ln_bias_, ln_mean_, ln_variance_, bias_dropout_residual_out_, dropout_mask_out_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_dropout_rate = pir::FloatAttribute::get(pir::IrContext::Instance(), dropout_rate);
  argument.AddAttribute("dropout_rate", attr_dropout_rate);
  pir::Attribute attr_is_test = pir::BoolAttribute::get(pir::IrContext::Instance(), is_test);
  argument.AddAttribute("is_test", attr_is_test);
  pir::Attribute attr_dropout_fix_seed = pir::BoolAttribute::get(pir::IrContext::Instance(), dropout_fix_seed);
  argument.AddAttribute("dropout_fix_seed", attr_dropout_fix_seed);
  pir::Attribute attr_dropout_seed = pir::Int32Attribute::get(pir::IrContext::Instance(), dropout_seed);
  argument.AddAttribute("dropout_seed", attr_dropout_seed);
  pir::Attribute attr_dropout_implementation = pir::StrAttribute::get(pir::IrContext::Instance(), dropout_implementation);
  argument.AddAttribute("dropout_implementation", attr_dropout_implementation);
  pir::Attribute attr_ln_epsilon = pir::FloatAttribute::get(pir::IrContext::Instance(), ln_epsilon);
  argument.AddAttribute("ln_epsilon", attr_ln_epsilon);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType y_grad = y_grad_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)y_grad;
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;
  paddle::dialect::DenseTensorType residual = residual_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)residual;
  paddle::dialect::DenseTensorType ln_mean = ln_mean_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)ln_mean;
  paddle::dialect::DenseTensorType ln_variance = ln_variance_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)ln_variance;
  paddle::dialect::DenseTensorType bias_dropout_residual_out = bias_dropout_residual_out_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)bias_dropout_residual_out;
  paddle::dialect::DenseTensorType dropout_mask_out = dropout_mask_out_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)dropout_mask_out;

  VLOG(4) << "Builder construction  dense_y_grad";
  paddle::dialect::IrTensor ir_tensor_y_grad(paddle::dialect::TransToPhiDataType(y_grad.dtype()),
                                                      y_grad.dims(),
                                                      y_grad.data_layout(),
                                                      y_grad.lod(),
                                                      y_grad.offset());
  VLOG(4) << "Builder construction  meta_y_grad";
  paddle::dialect::IrMetaTensor meta_y_grad(&ir_tensor_y_grad);

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);

  VLOG(4) << "Builder construction  dense_residual";
  paddle::dialect::IrTensor ir_tensor_residual(paddle::dialect::TransToPhiDataType(residual.dtype()),
                                                      residual.dims(),
                                                      residual.data_layout(),
                                                      residual.lod(),
                                                      residual.offset());
  VLOG(4) << "Builder construction  meta_residual";
  paddle::dialect::IrMetaTensor meta_residual(&ir_tensor_residual);

  paddle::dialect::IrMetaTensor meta_bias;
  paddle::dialect::IrTensor ir_tensor_bias;
  if (bias_.impl() != nullptr) {
    paddle::dialect::DenseTensorType bias = bias_.type().dyn_cast<paddle::dialect::DenseTensorType>();
    VLOG(4) << "Builder construction  dense_bias";
    ir_tensor_bias = paddle::dialect::IrTensor(paddle::dialect::TransToPhiDataType(bias.dtype()),
                                                        bias.dims(),
                                                        bias.data_layout(),
                                                        bias.lod(),
                                                        bias.offset());
    VLOG(4) << "Builder construction  meta_bias";
    meta_bias = paddle::dialect::IrMetaTensor(&ir_tensor_bias);
  }


  paddle::dialect::IrMetaTensor meta_ln_scale;
  paddle::dialect::IrTensor ir_tensor_ln_scale;
  if (ln_scale_.impl() != nullptr) {
    paddle::dialect::DenseTensorType ln_scale = ln_scale_.type().dyn_cast<paddle::dialect::DenseTensorType>();
    VLOG(4) << "Builder construction  dense_ln_scale";
    ir_tensor_ln_scale = paddle::dialect::IrTensor(paddle::dialect::TransToPhiDataType(ln_scale.dtype()),
                                                        ln_scale.dims(),
                                                        ln_scale.data_layout(),
                                                        ln_scale.lod(),
                                                        ln_scale.offset());
    VLOG(4) << "Builder construction  meta_ln_scale";
    meta_ln_scale = paddle::dialect::IrMetaTensor(&ir_tensor_ln_scale);
  }


  paddle::dialect::IrMetaTensor meta_ln_bias;
  paddle::dialect::IrTensor ir_tensor_ln_bias;
  if (ln_bias_.impl() != nullptr) {
    paddle::dialect::DenseTensorType ln_bias = ln_bias_.type().dyn_cast<paddle::dialect::DenseTensorType>();
    VLOG(4) << "Builder construction  dense_ln_bias";
    ir_tensor_ln_bias = paddle::dialect::IrTensor(paddle::dialect::TransToPhiDataType(ln_bias.dtype()),
                                                        ln_bias.dims(),
                                                        ln_bias.data_layout(),
                                                        ln_bias.lod(),
                                                        ln_bias.offset());
    VLOG(4) << "Builder construction  meta_ln_bias";
    meta_ln_bias = paddle::dialect::IrMetaTensor(&ir_tensor_ln_bias);
  }


  VLOG(4) << "Builder construction  dense_ln_mean";
  paddle::dialect::IrTensor ir_tensor_ln_mean(paddle::dialect::TransToPhiDataType(ln_mean.dtype()),
                                                      ln_mean.dims(),
                                                      ln_mean.data_layout(),
                                                      ln_mean.lod(),
                                                      ln_mean.offset());
  VLOG(4) << "Builder construction  meta_ln_mean";
  paddle::dialect::IrMetaTensor meta_ln_mean(&ir_tensor_ln_mean);

  VLOG(4) << "Builder construction  dense_ln_variance";
  paddle::dialect::IrTensor ir_tensor_ln_variance(paddle::dialect::TransToPhiDataType(ln_variance.dtype()),
                                                      ln_variance.dims(),
                                                      ln_variance.data_layout(),
                                                      ln_variance.lod(),
                                                      ln_variance.offset());
  VLOG(4) << "Builder construction  meta_ln_variance";
  paddle::dialect::IrMetaTensor meta_ln_variance(&ir_tensor_ln_variance);

  VLOG(4) << "Builder construction  dense_bias_dropout_residual_out";
  paddle::dialect::IrTensor ir_tensor_bias_dropout_residual_out(paddle::dialect::TransToPhiDataType(bias_dropout_residual_out.dtype()),
                                                      bias_dropout_residual_out.dims(),
                                                      bias_dropout_residual_out.data_layout(),
                                                      bias_dropout_residual_out.lod(),
                                                      bias_dropout_residual_out.offset());
  VLOG(4) << "Builder construction  meta_bias_dropout_residual_out";
  paddle::dialect::IrMetaTensor meta_bias_dropout_residual_out(&ir_tensor_bias_dropout_residual_out);

  VLOG(4) << "Builder construction  dense_dropout_mask_out";
  paddle::dialect::IrTensor ir_tensor_dropout_mask_out(paddle::dialect::TransToPhiDataType(dropout_mask_out.dtype()),
                                                      dropout_mask_out.dims(),
                                                      dropout_mask_out.data_layout(),
                                                      dropout_mask_out.lod(),
                                                      dropout_mask_out.offset());
  VLOG(4) << "Builder construction  meta_dropout_mask_out";
  paddle::dialect::IrMetaTensor meta_dropout_mask_out(&ir_tensor_dropout_mask_out);
  paddle::dialect::IrTensor dense_x_grad;
  paddle::dialect::IrMetaTensor meta_x_grad(&dense_x_grad);
  paddle::dialect::IrTensor dense_residual_grad;
  paddle::dialect::IrMetaTensor meta_residual_grad(&dense_residual_grad);
  paddle::dialect::IrTensor dense_bias_grad;
  paddle::dialect::IrMetaTensor meta_bias_grad(&dense_bias_grad);
  paddle::dialect::IrTensor dense_ln_scale_grad;
  paddle::dialect::IrMetaTensor meta_ln_scale_grad(&dense_ln_scale_grad);
  paddle::dialect::IrTensor dense_ln_bias_grad;
  paddle::dialect::IrMetaTensor meta_ln_bias_grad(&dense_ln_bias_grad);

  phi::FusedBiasDropoutResidualLnGradInferMeta(meta_y_grad, meta_x, meta_residual, meta_bias, meta_ln_scale, meta_ln_bias, meta_ln_mean, meta_ln_variance, meta_bias_dropout_residual_out, meta_dropout_mask_out, dropout_rate, is_test, dropout_fix_seed, dropout_seed, dropout_implementation, ln_epsilon, &meta_x_grad, &meta_residual_grad, &meta_bias_grad, &meta_ln_scale_grad, &meta_ln_bias_grad);

  std::vector<pir::Type> argument_outputs;
  pir::Type x_grad_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_x_grad.dtype()), dense_x_grad.dims(), dense_x_grad.layout(), dense_x_grad.lod(), dense_x_grad.offset());
  argument_outputs.push_back(x_grad_dense_tensor_type);

  pir::Type residual_grad_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_residual_grad.dtype()), dense_residual_grad.dims(), dense_residual_grad.layout(), dense_residual_grad.lod(), dense_residual_grad.offset());
  argument_outputs.push_back(residual_grad_dense_tensor_type);

  pir::Type bias_grad_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_bias_grad.dtype()), dense_bias_grad.dims(), dense_bias_grad.layout(), dense_bias_grad.lod(), dense_bias_grad.offset());
  argument_outputs.push_back(bias_grad_dense_tensor_type);

  pir::Type ln_scale_grad_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_ln_scale_grad.dtype()), dense_ln_scale_grad.dims(), dense_ln_scale_grad.layout(), dense_ln_scale_grad.lod(), dense_ln_scale_grad.offset());
  argument_outputs.push_back(ln_scale_grad_dense_tensor_type);

  pir::Type ln_bias_grad_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_ln_bias_grad.dtype()), dense_ln_bias_grad.dims(), dense_ln_bias_grad.layout(), dense_ln_bias_grad.lod(), dense_ln_bias_grad.offset());
  argument_outputs.push_back(ln_bias_grad_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void FusedBiasDropoutResidualLayerNormGradOp::VerifySig() {}

void FusedBiasDropoutResidualLayerNormGradOp::InferMeta( phi::InferMetaContext *infer_meta ) {
  auto fn = PD_INFER_META(phi::FusedBiasDropoutResidualLnGradInferMeta);
  fn(infer_meta);
}

phi::DataType FusedBiasDropoutResidualLayerNormGradOp::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: FusedBiasDropoutResidualLayerNormGradOp";
  


  return expected_kernel_dtype;
}

const char *FusedDotProductAttentionGradOp::attributes_name[3] = { "scaling_factor", "dropout_probability", "is_causal_masking" };

OpInfoTuple FusedDotProductAttentionGradOp::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("q", "paddle::dialect::DenseTensorType", false, false, false, false), paddle::dialect::OpInputInfo("k", "paddle::dialect::DenseTensorType", false, false, false, false), paddle::dialect::OpInputInfo("v", "paddle::dialect::DenseTensorType", false, false, false, false), paddle::dialect::OpInputInfo("out", "paddle::dialect::DenseTensorType", false, false, false, false), paddle::dialect::OpInputInfo("softmax_out", "paddle::dialect::DenseTensorType", false, false, false, false), paddle::dialect::OpInputInfo("rng_state", "paddle::dialect::DenseTensorType", false, false, false, false), paddle::dialect::OpInputInfo("mask", "paddle::dialect::DenseTensorType", false, false, false, false), paddle::dialect::OpInputInfo("out_grad", "paddle::dialect::DenseTensorType", false, false, false, false) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = { paddle::dialect::OpAttributeInfo("scaling_factor", "pir::FloatAttribute", ""), paddle::dialect::OpAttributeInfo("dropout_probability", "pir::FloatAttribute", ""), paddle::dialect::OpAttributeInfo("is_causal_masking", "pir::BoolAttribute", "") };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("q_grad", "paddle::dialect::DenseTensorType", false, false), paddle::dialect::OpOutputInfo("k_grad", "paddle::dialect::DenseTensorType", false, false), paddle::dialect::OpOutputInfo("v_grad", "paddle::dialect::DenseTensorType", false, false) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("FusedDotProductAttentionGradInferMeta", {"q", "k", "v"}, "fused_dot_product_attention_grad", {"q", "k", "v", "out", "softmax_out", "rng_state", "mask", "out_grad", "scaling_factor", "dropout_probability", "is_causal_masking"}, {"q"}, {}, {}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "fused_dot_product_attention_grad");
}

void FusedDotProductAttentionGradOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value q_, pir::Value k_, pir::Value v_, pir::Value out_, pir::Value softmax_out_, pir::Value rng_state_, pir::Value mask_, pir::Value out_grad_, float scaling_factor, float dropout_probability, bool is_causal_masking) {
  VLOG(4) << "Start build FusedDotProductAttentionGradOp";



  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {q_, k_, v_, out_, softmax_out_, rng_state_, mask_, out_grad_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_scaling_factor = pir::FloatAttribute::get(pir::IrContext::Instance(), scaling_factor);
  argument.AddAttribute("scaling_factor", attr_scaling_factor);
  pir::Attribute attr_dropout_probability = pir::FloatAttribute::get(pir::IrContext::Instance(), dropout_probability);
  argument.AddAttribute("dropout_probability", attr_dropout_probability);
  pir::Attribute attr_is_causal_masking = pir::BoolAttribute::get(pir::IrContext::Instance(), is_causal_masking);
  argument.AddAttribute("is_causal_masking", attr_is_causal_masking);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType q = q_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)q;
  paddle::dialect::DenseTensorType k = k_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)k;
  paddle::dialect::DenseTensorType v = v_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)v;
  paddle::dialect::DenseTensorType out = out_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)out;
  paddle::dialect::DenseTensorType softmax_out = softmax_out_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)softmax_out;
  paddle::dialect::DenseTensorType rng_state = rng_state_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)rng_state;
  paddle::dialect::DenseTensorType mask = mask_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)mask;
  paddle::dialect::DenseTensorType out_grad = out_grad_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)out_grad;

  VLOG(4) << "Builder construction  dense_q";
  paddle::dialect::IrTensor ir_tensor_q(paddle::dialect::TransToPhiDataType(q.dtype()),
                                                      q.dims(),
                                                      q.data_layout(),
                                                      q.lod(),
                                                      q.offset());
  VLOG(4) << "Builder construction  meta_q";
  paddle::dialect::IrMetaTensor meta_q(&ir_tensor_q);

  VLOG(4) << "Builder construction  dense_k";
  paddle::dialect::IrTensor ir_tensor_k(paddle::dialect::TransToPhiDataType(k.dtype()),
                                                      k.dims(),
                                                      k.data_layout(),
                                                      k.lod(),
                                                      k.offset());
  VLOG(4) << "Builder construction  meta_k";
  paddle::dialect::IrMetaTensor meta_k(&ir_tensor_k);

  VLOG(4) << "Builder construction  dense_v";
  paddle::dialect::IrTensor ir_tensor_v(paddle::dialect::TransToPhiDataType(v.dtype()),
                                                      v.dims(),
                                                      v.data_layout(),
                                                      v.lod(),
                                                      v.offset());
  VLOG(4) << "Builder construction  meta_v";
  paddle::dialect::IrMetaTensor meta_v(&ir_tensor_v);
  paddle::dialect::IrTensor dense_q_grad;
  paddle::dialect::IrMetaTensor meta_q_grad(&dense_q_grad);
  paddle::dialect::IrTensor dense_k_grad;
  paddle::dialect::IrMetaTensor meta_k_grad(&dense_k_grad);
  paddle::dialect::IrTensor dense_v_grad;
  paddle::dialect::IrMetaTensor meta_v_grad(&dense_v_grad);

  phi::FusedDotProductAttentionGradInferMeta(meta_q, meta_k, meta_v, &meta_q_grad, &meta_k_grad, &meta_v_grad);

  std::vector<pir::Type> argument_outputs;
  pir::Type q_grad_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_q_grad.dtype()), dense_q_grad.dims(), dense_q_grad.layout(), dense_q_grad.lod(), dense_q_grad.offset());
  argument_outputs.push_back(q_grad_dense_tensor_type);

  pir::Type k_grad_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_k_grad.dtype()), dense_k_grad.dims(), dense_k_grad.layout(), dense_k_grad.lod(), dense_k_grad.offset());
  argument_outputs.push_back(k_grad_dense_tensor_type);

  pir::Type v_grad_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_v_grad.dtype()), dense_v_grad.dims(), dense_v_grad.layout(), dense_v_grad.lod(), dense_v_grad.offset());
  argument_outputs.push_back(v_grad_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void FusedDotProductAttentionGradOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value q_, pir::Value k_, pir::Value v_, pir::Value out_, pir::Value softmax_out_, pir::Value rng_state_, pir::Value mask_, pir::Value out_grad_, pir::AttributeMap attributes) {
  VLOG(4) << "Start build FusedDotProductAttentionGradOp";


  IR_ENFORCE(
      attributes.find("scaling_factor") != attributes.end(),
          "'scaling_factor' Attribute is expected for FusedDotProductAttentionGradOp. ");
  float scaling_factor = attributes.at("scaling_factor").dyn_cast<pir::FloatAttribute>().data();

  IR_ENFORCE(
      attributes.find("dropout_probability") != attributes.end(),
          "'dropout_probability' Attribute is expected for FusedDotProductAttentionGradOp. ");
  float dropout_probability = attributes.at("dropout_probability").dyn_cast<pir::FloatAttribute>().data();

  IR_ENFORCE(
      attributes.find("is_causal_masking") != attributes.end(),
          "'is_causal_masking' Attribute is expected for FusedDotProductAttentionGradOp. ");
  bool is_causal_masking = attributes.at("is_causal_masking").dyn_cast<pir::BoolAttribute>().data();


  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {q_, k_, v_, out_, softmax_out_, rng_state_, mask_, out_grad_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_scaling_factor = pir::FloatAttribute::get(pir::IrContext::Instance(), scaling_factor);
  argument.AddAttribute("scaling_factor", attr_scaling_factor);
  pir::Attribute attr_dropout_probability = pir::FloatAttribute::get(pir::IrContext::Instance(), dropout_probability);
  argument.AddAttribute("dropout_probability", attr_dropout_probability);
  pir::Attribute attr_is_causal_masking = pir::BoolAttribute::get(pir::IrContext::Instance(), is_causal_masking);
  argument.AddAttribute("is_causal_masking", attr_is_causal_masking);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType q = q_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)q;
  paddle::dialect::DenseTensorType k = k_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)k;
  paddle::dialect::DenseTensorType v = v_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)v;
  paddle::dialect::DenseTensorType out = out_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)out;
  paddle::dialect::DenseTensorType softmax_out = softmax_out_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)softmax_out;
  paddle::dialect::DenseTensorType rng_state = rng_state_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)rng_state;
  paddle::dialect::DenseTensorType mask = mask_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)mask;
  paddle::dialect::DenseTensorType out_grad = out_grad_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)out_grad;

  VLOG(4) << "Builder construction  dense_q";
  paddle::dialect::IrTensor ir_tensor_q(paddle::dialect::TransToPhiDataType(q.dtype()),
                                                      q.dims(),
                                                      q.data_layout(),
                                                      q.lod(),
                                                      q.offset());
  VLOG(4) << "Builder construction  meta_q";
  paddle::dialect::IrMetaTensor meta_q(&ir_tensor_q);

  VLOG(4) << "Builder construction  dense_k";
  paddle::dialect::IrTensor ir_tensor_k(paddle::dialect::TransToPhiDataType(k.dtype()),
                                                      k.dims(),
                                                      k.data_layout(),
                                                      k.lod(),
                                                      k.offset());
  VLOG(4) << "Builder construction  meta_k";
  paddle::dialect::IrMetaTensor meta_k(&ir_tensor_k);

  VLOG(4) << "Builder construction  dense_v";
  paddle::dialect::IrTensor ir_tensor_v(paddle::dialect::TransToPhiDataType(v.dtype()),
                                                      v.dims(),
                                                      v.data_layout(),
                                                      v.lod(),
                                                      v.offset());
  VLOG(4) << "Builder construction  meta_v";
  paddle::dialect::IrMetaTensor meta_v(&ir_tensor_v);
  paddle::dialect::IrTensor dense_q_grad;
  paddle::dialect::IrMetaTensor meta_q_grad(&dense_q_grad);
  paddle::dialect::IrTensor dense_k_grad;
  paddle::dialect::IrMetaTensor meta_k_grad(&dense_k_grad);
  paddle::dialect::IrTensor dense_v_grad;
  paddle::dialect::IrMetaTensor meta_v_grad(&dense_v_grad);

  phi::FusedDotProductAttentionGradInferMeta(meta_q, meta_k, meta_v, &meta_q_grad, &meta_k_grad, &meta_v_grad);

  std::vector<pir::Type> argument_outputs;
  pir::Type q_grad_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_q_grad.dtype()), dense_q_grad.dims(), dense_q_grad.layout(), dense_q_grad.lod(), dense_q_grad.offset());
  argument_outputs.push_back(q_grad_dense_tensor_type);

  pir::Type k_grad_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_k_grad.dtype()), dense_k_grad.dims(), dense_k_grad.layout(), dense_k_grad.lod(), dense_k_grad.offset());
  argument_outputs.push_back(k_grad_dense_tensor_type);

  pir::Type v_grad_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_v_grad.dtype()), dense_v_grad.dims(), dense_v_grad.layout(), dense_v_grad.lod(), dense_v_grad.offset());
  argument_outputs.push_back(v_grad_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void FusedDotProductAttentionGradOp::VerifySig() {}

void FusedDotProductAttentionGradOp::InferMeta( phi::InferMetaContext *infer_meta ) {
  auto fn = PD_INFER_META(phi::FusedDotProductAttentionGradInferMeta);
  fn(infer_meta);
}

phi::DataType FusedDotProductAttentionGradOp::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: FusedDotProductAttentionGradOp";
  


  return expected_kernel_dtype;
}

const char *FusedDropoutAddGradOp::attributes_name[4] = { "p", "is_test", "mode", "fix_seed" };

OpInfoTuple FusedDropoutAddGradOp::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("seed_offset", "paddle::dialect::DenseTensorType", false, false, false, false), paddle::dialect::OpInputInfo("out_grad", "paddle::dialect::DenseTensorType", false, false, false, false) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = { paddle::dialect::OpAttributeInfo("p", "paddle::dialect::ScalarAttribute", "float"), paddle::dialect::OpAttributeInfo("is_test", "pir::BoolAttribute", ""), paddle::dialect::OpAttributeInfo("mode", "pir::StrAttribute", ""), paddle::dialect::OpAttributeInfo("fix_seed", "pir::BoolAttribute", "") };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("x_grad", "paddle::dialect::DenseTensorType", false, false), paddle::dialect::OpOutputInfo("y_grad", "paddle::dialect::DenseTensorType", false, false) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("FusedDropoutAddGradInferMeta", {"seed_offset", "out_grad"}, "fused_dropout_add_grad", {"seed_offset", "out_grad", "p", "is_test", "mode", "fix_seed"}, {"out_grad"}, {}, {}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "fused_dropout_add_grad");
}

void FusedDropoutAddGradOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value seed_offset_, pir::Value out_grad_, float p, bool is_test, const std::string& mode, bool fix_seed) {
  VLOG(4) << "Start build FusedDropoutAddGradOp";



  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {seed_offset_, out_grad_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_p = paddle::dialect::TransToIrAttribute(p, pir::IrContext::Instance());
  argument.AddAttribute("p", attr_p);
  pir::Attribute attr_is_test = pir::BoolAttribute::get(pir::IrContext::Instance(), is_test);
  argument.AddAttribute("is_test", attr_is_test);
  pir::Attribute attr_mode = pir::StrAttribute::get(pir::IrContext::Instance(), mode);
  argument.AddAttribute("mode", attr_mode);
  pir::Attribute attr_fix_seed = pir::BoolAttribute::get(pir::IrContext::Instance(), fix_seed);
  argument.AddAttribute("fix_seed", attr_fix_seed);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType seed_offset = seed_offset_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)seed_offset;
  paddle::dialect::DenseTensorType out_grad = out_grad_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)out_grad;

  VLOG(4) << "Builder construction  dense_seed_offset";
  paddle::dialect::IrTensor ir_tensor_seed_offset(paddle::dialect::TransToPhiDataType(seed_offset.dtype()),
                                                      seed_offset.dims(),
                                                      seed_offset.data_layout(),
                                                      seed_offset.lod(),
                                                      seed_offset.offset());
  VLOG(4) << "Builder construction  meta_seed_offset";
  paddle::dialect::IrMetaTensor meta_seed_offset(&ir_tensor_seed_offset);

  VLOG(4) << "Builder construction  dense_out_grad";
  paddle::dialect::IrTensor ir_tensor_out_grad(paddle::dialect::TransToPhiDataType(out_grad.dtype()),
                                                      out_grad.dims(),
                                                      out_grad.data_layout(),
                                                      out_grad.lod(),
                                                      out_grad.offset());
  VLOG(4) << "Builder construction  meta_out_grad";
  paddle::dialect::IrMetaTensor meta_out_grad(&ir_tensor_out_grad);
  paddle::dialect::IrTensor dense_x_grad;
  paddle::dialect::IrMetaTensor meta_x_grad(&dense_x_grad);
  paddle::dialect::IrTensor dense_y_grad;
  paddle::dialect::IrMetaTensor meta_y_grad(&dense_y_grad);

  phi::FusedDropoutAddGradInferMeta(meta_seed_offset, meta_out_grad, &meta_x_grad, &meta_y_grad);

  std::vector<pir::Type> argument_outputs;
  pir::Type x_grad_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_x_grad.dtype()), dense_x_grad.dims(), dense_x_grad.layout(), dense_x_grad.lod(), dense_x_grad.offset());
  argument_outputs.push_back(x_grad_dense_tensor_type);

  pir::Type y_grad_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_y_grad.dtype()), dense_y_grad.dims(), dense_y_grad.layout(), dense_y_grad.lod(), dense_y_grad.offset());
  argument_outputs.push_back(y_grad_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void FusedDropoutAddGradOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value seed_offset_, pir::Value out_grad_, pir::AttributeMap attributes) {
  VLOG(4) << "Start build FusedDropoutAddGradOp";


  IR_ENFORCE(
      attributes.find("p") != attributes.end(),
          "'p' Attribute is expected for FusedDropoutAddGradOp. ");
  float p = attributes.at("p").dyn_cast<paddle::dialect::ScalarAttribute>().data().to<float>();

  IR_ENFORCE(
      attributes.find("is_test") != attributes.end(),
          "'is_test' Attribute is expected for FusedDropoutAddGradOp. ");
  bool is_test = attributes.at("is_test").dyn_cast<pir::BoolAttribute>().data();

  IR_ENFORCE(
      attributes.find("mode") != attributes.end(),
          "'mode' Attribute is expected for FusedDropoutAddGradOp. ");
  std::string mode = attributes.at("mode").dyn_cast<pir::StrAttribute>().AsString();

  IR_ENFORCE(
      attributes.find("fix_seed") != attributes.end(),
          "'fix_seed' Attribute is expected for FusedDropoutAddGradOp. ");
  bool fix_seed = attributes.at("fix_seed").dyn_cast<pir::BoolAttribute>().data();


  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {seed_offset_, out_grad_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_p = paddle::dialect::TransToIrAttribute(p, pir::IrContext::Instance());
  argument.AddAttribute("p", attr_p);
  pir::Attribute attr_is_test = pir::BoolAttribute::get(pir::IrContext::Instance(), is_test);
  argument.AddAttribute("is_test", attr_is_test);
  pir::Attribute attr_mode = pir::StrAttribute::get(pir::IrContext::Instance(), mode);
  argument.AddAttribute("mode", attr_mode);
  pir::Attribute attr_fix_seed = pir::BoolAttribute::get(pir::IrContext::Instance(), fix_seed);
  argument.AddAttribute("fix_seed", attr_fix_seed);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType seed_offset = seed_offset_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)seed_offset;
  paddle::dialect::DenseTensorType out_grad = out_grad_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)out_grad;

  VLOG(4) << "Builder construction  dense_seed_offset";
  paddle::dialect::IrTensor ir_tensor_seed_offset(paddle::dialect::TransToPhiDataType(seed_offset.dtype()),
                                                      seed_offset.dims(),
                                                      seed_offset.data_layout(),
                                                      seed_offset.lod(),
                                                      seed_offset.offset());
  VLOG(4) << "Builder construction  meta_seed_offset";
  paddle::dialect::IrMetaTensor meta_seed_offset(&ir_tensor_seed_offset);

  VLOG(4) << "Builder construction  dense_out_grad";
  paddle::dialect::IrTensor ir_tensor_out_grad(paddle::dialect::TransToPhiDataType(out_grad.dtype()),
                                                      out_grad.dims(),
                                                      out_grad.data_layout(),
                                                      out_grad.lod(),
                                                      out_grad.offset());
  VLOG(4) << "Builder construction  meta_out_grad";
  paddle::dialect::IrMetaTensor meta_out_grad(&ir_tensor_out_grad);
  paddle::dialect::IrTensor dense_x_grad;
  paddle::dialect::IrMetaTensor meta_x_grad(&dense_x_grad);
  paddle::dialect::IrTensor dense_y_grad;
  paddle::dialect::IrMetaTensor meta_y_grad(&dense_y_grad);

  phi::FusedDropoutAddGradInferMeta(meta_seed_offset, meta_out_grad, &meta_x_grad, &meta_y_grad);

  std::vector<pir::Type> argument_outputs;
  pir::Type x_grad_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_x_grad.dtype()), dense_x_grad.dims(), dense_x_grad.layout(), dense_x_grad.lod(), dense_x_grad.offset());
  argument_outputs.push_back(x_grad_dense_tensor_type);

  pir::Type y_grad_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_y_grad.dtype()), dense_y_grad.dims(), dense_y_grad.layout(), dense_y_grad.lod(), dense_y_grad.offset());
  argument_outputs.push_back(y_grad_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void FusedDropoutAddGradOp::VerifySig() {}

void FusedDropoutAddGradOp::InferMeta( phi::InferMetaContext *infer_meta ) {
  auto fn = PD_INFER_META(phi::FusedDropoutAddGradInferMeta);
  fn(infer_meta);
}

phi::DataType FusedDropoutAddGradOp::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: FusedDropoutAddGradOp";
  


  return expected_kernel_dtype;
}

const char *FusedRotaryPositionEmbeddingGradOp::attributes_name[1] = { "use_neox_rotary_style" };

OpInfoTuple FusedRotaryPositionEmbeddingGradOp::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("sin", "paddle::dialect::DenseTensorType", true, false, false, false), paddle::dialect::OpInputInfo("cos", "paddle::dialect::DenseTensorType", true, false, false, false), paddle::dialect::OpInputInfo("position_ids", "paddle::dialect::DenseTensorType", true, false, false, false), paddle::dialect::OpInputInfo("out_q_grad", "paddle::dialect::DenseTensorType", false, false, false, false), paddle::dialect::OpInputInfo("out_k_grad", "paddle::dialect::DenseTensorType", true, false, false, false), paddle::dialect::OpInputInfo("out_v_grad", "paddle::dialect::DenseTensorType", true, false, false, false) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = { paddle::dialect::OpAttributeInfo("use_neox_rotary_style", "pir::BoolAttribute", "") };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("q_grad", "paddle::dialect::DenseTensorType", false, false), paddle::dialect::OpOutputInfo("k_grad", "paddle::dialect::DenseTensorType", true, false), paddle::dialect::OpOutputInfo("v_grad", "paddle::dialect::DenseTensorType", true, false) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("FusedRopeGradInferMeta", {"sin", "cos", "position_ids", "out_q_grad", "out_k_grad", "out_v_grad", "use_neox_rotary_style"}, "fused_rotary_position_embedding_grad", {"sin", "cos", "position_ids", "out_q_grad", "out_k_grad", "out_v_grad", "use_neox_rotary_style"}, {"out_q_grad"}, {}, {}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "fused_rotary_position_embedding_grad");
}

void FusedRotaryPositionEmbeddingGradOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value sin_, pir::Value cos_, pir::Value position_ids_, pir::Value out_q_grad_, pir::Value out_k_grad_, pir::Value out_v_grad_, bool use_neox_rotary_style) {
  VLOG(4) << "Start build FusedRotaryPositionEmbeddingGradOp";



  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {sin_, cos_, position_ids_, out_q_grad_, out_k_grad_, out_v_grad_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_use_neox_rotary_style = pir::BoolAttribute::get(pir::IrContext::Instance(), use_neox_rotary_style);
  argument.AddAttribute("use_neox_rotary_style", attr_use_neox_rotary_style);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType out_q_grad = out_q_grad_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)out_q_grad;

  paddle::dialect::IrMetaTensor meta_sin;
  paddle::dialect::IrTensor ir_tensor_sin;
  if (sin_.impl() != nullptr) {
    paddle::dialect::DenseTensorType sin = sin_.type().dyn_cast<paddle::dialect::DenseTensorType>();
    VLOG(4) << "Builder construction  dense_sin";
    ir_tensor_sin = paddle::dialect::IrTensor(paddle::dialect::TransToPhiDataType(sin.dtype()),
                                                        sin.dims(),
                                                        sin.data_layout(),
                                                        sin.lod(),
                                                        sin.offset());
    VLOG(4) << "Builder construction  meta_sin";
    meta_sin = paddle::dialect::IrMetaTensor(&ir_tensor_sin);
  }


  paddle::dialect::IrMetaTensor meta_cos;
  paddle::dialect::IrTensor ir_tensor_cos;
  if (cos_.impl() != nullptr) {
    paddle::dialect::DenseTensorType cos = cos_.type().dyn_cast<paddle::dialect::DenseTensorType>();
    VLOG(4) << "Builder construction  dense_cos";
    ir_tensor_cos = paddle::dialect::IrTensor(paddle::dialect::TransToPhiDataType(cos.dtype()),
                                                        cos.dims(),
                                                        cos.data_layout(),
                                                        cos.lod(),
                                                        cos.offset());
    VLOG(4) << "Builder construction  meta_cos";
    meta_cos = paddle::dialect::IrMetaTensor(&ir_tensor_cos);
  }


  paddle::dialect::IrMetaTensor meta_position_ids;
  paddle::dialect::IrTensor ir_tensor_position_ids;
  if (position_ids_.impl() != nullptr) {
    paddle::dialect::DenseTensorType position_ids = position_ids_.type().dyn_cast<paddle::dialect::DenseTensorType>();
    VLOG(4) << "Builder construction  dense_position_ids";
    ir_tensor_position_ids = paddle::dialect::IrTensor(paddle::dialect::TransToPhiDataType(position_ids.dtype()),
                                                        position_ids.dims(),
                                                        position_ids.data_layout(),
                                                        position_ids.lod(),
                                                        position_ids.offset());
    VLOG(4) << "Builder construction  meta_position_ids";
    meta_position_ids = paddle::dialect::IrMetaTensor(&ir_tensor_position_ids);
  }


  VLOG(4) << "Builder construction  dense_out_q_grad";
  paddle::dialect::IrTensor ir_tensor_out_q_grad(paddle::dialect::TransToPhiDataType(out_q_grad.dtype()),
                                                      out_q_grad.dims(),
                                                      out_q_grad.data_layout(),
                                                      out_q_grad.lod(),
                                                      out_q_grad.offset());
  VLOG(4) << "Builder construction  meta_out_q_grad";
  paddle::dialect::IrMetaTensor meta_out_q_grad(&ir_tensor_out_q_grad);

  paddle::dialect::IrMetaTensor meta_out_k_grad;
  paddle::dialect::IrTensor ir_tensor_out_k_grad;
  if (out_k_grad_.impl() != nullptr) {
    paddle::dialect::DenseTensorType out_k_grad = out_k_grad_.type().dyn_cast<paddle::dialect::DenseTensorType>();
    VLOG(4) << "Builder construction  dense_out_k_grad";
    ir_tensor_out_k_grad = paddle::dialect::IrTensor(paddle::dialect::TransToPhiDataType(out_k_grad.dtype()),
                                                        out_k_grad.dims(),
                                                        out_k_grad.data_layout(),
                                                        out_k_grad.lod(),
                                                        out_k_grad.offset());
    VLOG(4) << "Builder construction  meta_out_k_grad";
    meta_out_k_grad = paddle::dialect::IrMetaTensor(&ir_tensor_out_k_grad);
  }


  paddle::dialect::IrMetaTensor meta_out_v_grad;
  paddle::dialect::IrTensor ir_tensor_out_v_grad;
  if (out_v_grad_.impl() != nullptr) {
    paddle::dialect::DenseTensorType out_v_grad = out_v_grad_.type().dyn_cast<paddle::dialect::DenseTensorType>();
    VLOG(4) << "Builder construction  dense_out_v_grad";
    ir_tensor_out_v_grad = paddle::dialect::IrTensor(paddle::dialect::TransToPhiDataType(out_v_grad.dtype()),
                                                        out_v_grad.dims(),
                                                        out_v_grad.data_layout(),
                                                        out_v_grad.lod(),
                                                        out_v_grad.offset());
    VLOG(4) << "Builder construction  meta_out_v_grad";
    meta_out_v_grad = paddle::dialect::IrMetaTensor(&ir_tensor_out_v_grad);
  }

  paddle::dialect::IrTensor dense_q_grad;
  paddle::dialect::IrMetaTensor meta_q_grad(&dense_q_grad);
  paddle::dialect::IrTensor dense_k_grad;
  paddle::dialect::IrMetaTensor meta_k_grad(&dense_k_grad);
  paddle::dialect::IrTensor dense_v_grad;
  paddle::dialect::IrMetaTensor meta_v_grad(&dense_v_grad);

  phi::FusedRopeGradInferMeta(meta_sin, meta_cos, meta_position_ids, meta_out_q_grad, meta_out_k_grad, meta_out_v_grad, use_neox_rotary_style, &meta_q_grad, &meta_k_grad, &meta_v_grad);

  std::vector<pir::Type> argument_outputs;
  pir::Type q_grad_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_q_grad.dtype()), dense_q_grad.dims(), dense_q_grad.layout(), dense_q_grad.lod(), dense_q_grad.offset());
  argument_outputs.push_back(q_grad_dense_tensor_type);

  pir::Type k_grad_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_k_grad.dtype()), dense_k_grad.dims(), dense_k_grad.layout(), dense_k_grad.lod(), dense_k_grad.offset());
  argument_outputs.push_back(k_grad_dense_tensor_type);

  pir::Type v_grad_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_v_grad.dtype()), dense_v_grad.dims(), dense_v_grad.layout(), dense_v_grad.lod(), dense_v_grad.offset());
  argument_outputs.push_back(v_grad_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void FusedRotaryPositionEmbeddingGradOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value sin_, pir::Value cos_, pir::Value position_ids_, pir::Value out_q_grad_, pir::Value out_k_grad_, pir::Value out_v_grad_, pir::AttributeMap attributes) {
  VLOG(4) << "Start build FusedRotaryPositionEmbeddingGradOp";


  IR_ENFORCE(
      attributes.find("use_neox_rotary_style") != attributes.end(),
          "'use_neox_rotary_style' Attribute is expected for FusedRotaryPositionEmbeddingGradOp. ");
  bool use_neox_rotary_style = attributes.at("use_neox_rotary_style").dyn_cast<pir::BoolAttribute>().data();


  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {sin_, cos_, position_ids_, out_q_grad_, out_k_grad_, out_v_grad_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_use_neox_rotary_style = pir::BoolAttribute::get(pir::IrContext::Instance(), use_neox_rotary_style);
  argument.AddAttribute("use_neox_rotary_style", attr_use_neox_rotary_style);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType out_q_grad = out_q_grad_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)out_q_grad;

  paddle::dialect::IrMetaTensor meta_sin;
  paddle::dialect::IrTensor ir_tensor_sin;
  if (sin_.impl() != nullptr) {
    paddle::dialect::DenseTensorType sin = sin_.type().dyn_cast<paddle::dialect::DenseTensorType>();
    VLOG(4) << "Builder construction  dense_sin";
    ir_tensor_sin = paddle::dialect::IrTensor(paddle::dialect::TransToPhiDataType(sin.dtype()),
                                                        sin.dims(),
                                                        sin.data_layout(),
                                                        sin.lod(),
                                                        sin.offset());
    VLOG(4) << "Builder construction  meta_sin";
    meta_sin = paddle::dialect::IrMetaTensor(&ir_tensor_sin);
  }


  paddle::dialect::IrMetaTensor meta_cos;
  paddle::dialect::IrTensor ir_tensor_cos;
  if (cos_.impl() != nullptr) {
    paddle::dialect::DenseTensorType cos = cos_.type().dyn_cast<paddle::dialect::DenseTensorType>();
    VLOG(4) << "Builder construction  dense_cos";
    ir_tensor_cos = paddle::dialect::IrTensor(paddle::dialect::TransToPhiDataType(cos.dtype()),
                                                        cos.dims(),
                                                        cos.data_layout(),
                                                        cos.lod(),
                                                        cos.offset());
    VLOG(4) << "Builder construction  meta_cos";
    meta_cos = paddle::dialect::IrMetaTensor(&ir_tensor_cos);
  }


  paddle::dialect::IrMetaTensor meta_position_ids;
  paddle::dialect::IrTensor ir_tensor_position_ids;
  if (position_ids_.impl() != nullptr) {
    paddle::dialect::DenseTensorType position_ids = position_ids_.type().dyn_cast<paddle::dialect::DenseTensorType>();
    VLOG(4) << "Builder construction  dense_position_ids";
    ir_tensor_position_ids = paddle::dialect::IrTensor(paddle::dialect::TransToPhiDataType(position_ids.dtype()),
                                                        position_ids.dims(),
                                                        position_ids.data_layout(),
                                                        position_ids.lod(),
                                                        position_ids.offset());
    VLOG(4) << "Builder construction  meta_position_ids";
    meta_position_ids = paddle::dialect::IrMetaTensor(&ir_tensor_position_ids);
  }


  VLOG(4) << "Builder construction  dense_out_q_grad";
  paddle::dialect::IrTensor ir_tensor_out_q_grad(paddle::dialect::TransToPhiDataType(out_q_grad.dtype()),
                                                      out_q_grad.dims(),
                                                      out_q_grad.data_layout(),
                                                      out_q_grad.lod(),
                                                      out_q_grad.offset());
  VLOG(4) << "Builder construction  meta_out_q_grad";
  paddle::dialect::IrMetaTensor meta_out_q_grad(&ir_tensor_out_q_grad);

  paddle::dialect::IrMetaTensor meta_out_k_grad;
  paddle::dialect::IrTensor ir_tensor_out_k_grad;
  if (out_k_grad_.impl() != nullptr) {
    paddle::dialect::DenseTensorType out_k_grad = out_k_grad_.type().dyn_cast<paddle::dialect::DenseTensorType>();
    VLOG(4) << "Builder construction  dense_out_k_grad";
    ir_tensor_out_k_grad = paddle::dialect::IrTensor(paddle::dialect::TransToPhiDataType(out_k_grad.dtype()),
                                                        out_k_grad.dims(),
                                                        out_k_grad.data_layout(),
                                                        out_k_grad.lod(),
                                                        out_k_grad.offset());
    VLOG(4) << "Builder construction  meta_out_k_grad";
    meta_out_k_grad = paddle::dialect::IrMetaTensor(&ir_tensor_out_k_grad);
  }


  paddle::dialect::IrMetaTensor meta_out_v_grad;
  paddle::dialect::IrTensor ir_tensor_out_v_grad;
  if (out_v_grad_.impl() != nullptr) {
    paddle::dialect::DenseTensorType out_v_grad = out_v_grad_.type().dyn_cast<paddle::dialect::DenseTensorType>();
    VLOG(4) << "Builder construction  dense_out_v_grad";
    ir_tensor_out_v_grad = paddle::dialect::IrTensor(paddle::dialect::TransToPhiDataType(out_v_grad.dtype()),
                                                        out_v_grad.dims(),
                                                        out_v_grad.data_layout(),
                                                        out_v_grad.lod(),
                                                        out_v_grad.offset());
    VLOG(4) << "Builder construction  meta_out_v_grad";
    meta_out_v_grad = paddle::dialect::IrMetaTensor(&ir_tensor_out_v_grad);
  }

  paddle::dialect::IrTensor dense_q_grad;
  paddle::dialect::IrMetaTensor meta_q_grad(&dense_q_grad);
  paddle::dialect::IrTensor dense_k_grad;
  paddle::dialect::IrMetaTensor meta_k_grad(&dense_k_grad);
  paddle::dialect::IrTensor dense_v_grad;
  paddle::dialect::IrMetaTensor meta_v_grad(&dense_v_grad);

  phi::FusedRopeGradInferMeta(meta_sin, meta_cos, meta_position_ids, meta_out_q_grad, meta_out_k_grad, meta_out_v_grad, use_neox_rotary_style, &meta_q_grad, &meta_k_grad, &meta_v_grad);

  std::vector<pir::Type> argument_outputs;
  pir::Type q_grad_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_q_grad.dtype()), dense_q_grad.dims(), dense_q_grad.layout(), dense_q_grad.lod(), dense_q_grad.offset());
  argument_outputs.push_back(q_grad_dense_tensor_type);

  pir::Type k_grad_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_k_grad.dtype()), dense_k_grad.dims(), dense_k_grad.layout(), dense_k_grad.lod(), dense_k_grad.offset());
  argument_outputs.push_back(k_grad_dense_tensor_type);

  pir::Type v_grad_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_v_grad.dtype()), dense_v_grad.dims(), dense_v_grad.layout(), dense_v_grad.lod(), dense_v_grad.offset());
  argument_outputs.push_back(v_grad_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void FusedRotaryPositionEmbeddingGradOp::VerifySig() {}

void FusedRotaryPositionEmbeddingGradOp::InferMeta( phi::InferMetaContext *infer_meta ) {
  auto fn = PD_INFER_META(phi::FusedRopeGradInferMeta);
  fn(infer_meta);
}

phi::DataType FusedRotaryPositionEmbeddingGradOp::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: FusedRotaryPositionEmbeddingGradOp";
  


  return expected_kernel_dtype;
}

const char *MaxPool2dV2GradOp::attributes_name[6] = { "kernel_size", "strides", "paddings", "data_format", "global_pooling", "adaptive" };

OpInfoTuple MaxPool2dV2GradOp::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("x", "paddle::dialect::DenseTensorType", false, false, false, false), paddle::dialect::OpInputInfo("out", "paddle::dialect::DenseTensorType", false, false, false, false), paddle::dialect::OpInputInfo("saved_idx", "paddle::dialect::DenseTensorType", false, false, false, false), paddle::dialect::OpInputInfo("out_grad", "paddle::dialect::DenseTensorType", false, false, false, false) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = { paddle::dialect::OpAttributeInfo("kernel_size", "pir::ArrayAttribute<pir::Int32Attribute>", ""), paddle::dialect::OpAttributeInfo("strides", "pir::ArrayAttribute<pir::Int32Attribute>", ""), paddle::dialect::OpAttributeInfo("paddings", "pir::ArrayAttribute<pir::Int32Attribute>", ""), paddle::dialect::OpAttributeInfo("data_format", "pir::StrAttribute", ""), paddle::dialect::OpAttributeInfo("global_pooling", "pir::BoolAttribute", ""), paddle::dialect::OpAttributeInfo("adaptive", "pir::BoolAttribute", "") };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("x_grad", "paddle::dialect::DenseTensorType", false, false) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("UnchangedInferMeta", {"x"}, "max_pool2d_v2_grad", {"x", "out", "saved_idx", "out_grad", "kernel_size", "strides", "paddings", "data_format", "global_pooling", "adaptive"}, {}, {}, {}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "max_pool2d_v2_grad");
}

void MaxPool2dV2GradOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, pir::Value out_, pir::Value saved_idx_, pir::Value out_grad_, const std::vector<int>& kernel_size, const std::vector<int>& strides, const std::vector<int>& paddings, const std::string& data_format, bool global_pooling, bool adaptive) {
  VLOG(4) << "Start build MaxPool2dV2GradOp";



  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_, out_, saved_idx_, out_grad_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  std::vector<pir::Attribute> vec_kernel_size;
  for (size_t i = 0; i < static_cast<size_t>(kernel_size.size()); i++) {
      pir::Attribute attr_kernel_size = pir::Int32Attribute::get(pir::IrContext::Instance(), kernel_size[i]);

    vec_kernel_size.push_back(attr_kernel_size);
  }
  pir::Attribute attr_kernel_size = pir::ArrayAttribute::get(pir::IrContext::Instance(), vec_kernel_size);
  argument.AddAttribute("kernel_size", attr_kernel_size);
  std::vector<pir::Attribute> vec_strides;
  for (size_t i = 0; i < static_cast<size_t>(strides.size()); i++) {
      pir::Attribute attr_strides = pir::Int32Attribute::get(pir::IrContext::Instance(), strides[i]);

    vec_strides.push_back(attr_strides);
  }
  pir::Attribute attr_strides = pir::ArrayAttribute::get(pir::IrContext::Instance(), vec_strides);
  argument.AddAttribute("strides", attr_strides);
  std::vector<pir::Attribute> vec_paddings;
  for (size_t i = 0; i < static_cast<size_t>(paddings.size()); i++) {
      pir::Attribute attr_paddings = pir::Int32Attribute::get(pir::IrContext::Instance(), paddings[i]);

    vec_paddings.push_back(attr_paddings);
  }
  pir::Attribute attr_paddings = pir::ArrayAttribute::get(pir::IrContext::Instance(), vec_paddings);
  argument.AddAttribute("paddings", attr_paddings);
  pir::Attribute attr_data_format = pir::StrAttribute::get(pir::IrContext::Instance(), data_format);
  argument.AddAttribute("data_format", attr_data_format);
  pir::Attribute attr_global_pooling = pir::BoolAttribute::get(pir::IrContext::Instance(), global_pooling);
  argument.AddAttribute("global_pooling", attr_global_pooling);
  pir::Attribute attr_adaptive = pir::BoolAttribute::get(pir::IrContext::Instance(), adaptive);
  argument.AddAttribute("adaptive", attr_adaptive);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;
  paddle::dialect::DenseTensorType out = out_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)out;
  paddle::dialect::DenseTensorType saved_idx = saved_idx_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)saved_idx;
  paddle::dialect::DenseTensorType out_grad = out_grad_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)out_grad;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);
  paddle::dialect::IrTensor dense_x_grad;
  paddle::dialect::IrMetaTensor meta_x_grad(&dense_x_grad);

  phi::UnchangedInferMeta(meta_x, &meta_x_grad);

  std::vector<pir::Type> argument_outputs;
  pir::Type x_grad_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_x_grad.dtype()), dense_x_grad.dims(), dense_x_grad.layout(), dense_x_grad.lod(), dense_x_grad.offset());
  argument_outputs.push_back(x_grad_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void MaxPool2dV2GradOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, pir::Value out_, pir::Value saved_idx_, pir::Value out_grad_, pir::AttributeMap attributes) {
  VLOG(4) << "Start build MaxPool2dV2GradOp";


  IR_ENFORCE(
      attributes.find("kernel_size") != attributes.end(),
          "'kernel_size' Attribute is expected for MaxPool2dV2GradOp. ");
  std::vector<int> kernel_size;
  for (size_t i = 0; i < attributes.at("kernel_size").dyn_cast<pir::ArrayAttribute>().size(); i++) {
    kernel_size.push_back(attributes.at("kernel_size").dyn_cast<pir::ArrayAttribute>().at(i).dyn_cast<pir::Int32Attribute>().data());
  }

  IR_ENFORCE(
      attributes.find("strides") != attributes.end(),
          "'strides' Attribute is expected for MaxPool2dV2GradOp. ");
  std::vector<int> strides;
  for (size_t i = 0; i < attributes.at("strides").dyn_cast<pir::ArrayAttribute>().size(); i++) {
    strides.push_back(attributes.at("strides").dyn_cast<pir::ArrayAttribute>().at(i).dyn_cast<pir::Int32Attribute>().data());
  }

  IR_ENFORCE(
      attributes.find("paddings") != attributes.end(),
          "'paddings' Attribute is expected for MaxPool2dV2GradOp. ");
  std::vector<int> paddings;
  for (size_t i = 0; i < attributes.at("paddings").dyn_cast<pir::ArrayAttribute>().size(); i++) {
    paddings.push_back(attributes.at("paddings").dyn_cast<pir::ArrayAttribute>().at(i).dyn_cast<pir::Int32Attribute>().data());
  }

  IR_ENFORCE(
      attributes.find("data_format") != attributes.end(),
          "'data_format' Attribute is expected for MaxPool2dV2GradOp. ");
  std::string data_format = attributes.at("data_format").dyn_cast<pir::StrAttribute>().AsString();

  IR_ENFORCE(
      attributes.find("global_pooling") != attributes.end(),
          "'global_pooling' Attribute is expected for MaxPool2dV2GradOp. ");
  bool global_pooling = attributes.at("global_pooling").dyn_cast<pir::BoolAttribute>().data();

  IR_ENFORCE(
      attributes.find("adaptive") != attributes.end(),
          "'adaptive' Attribute is expected for MaxPool2dV2GradOp. ");
  bool adaptive = attributes.at("adaptive").dyn_cast<pir::BoolAttribute>().data();


  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_, out_, saved_idx_, out_grad_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  std::vector<pir::Attribute> vec_kernel_size;
  for (size_t i = 0; i < static_cast<size_t>(kernel_size.size()); i++) {
      pir::Attribute attr_kernel_size = pir::Int32Attribute::get(pir::IrContext::Instance(), kernel_size[i]);

    vec_kernel_size.push_back(attr_kernel_size);
  }
  pir::Attribute attr_kernel_size = pir::ArrayAttribute::get(pir::IrContext::Instance(), vec_kernel_size);
  argument.AddAttribute("kernel_size", attr_kernel_size);
  std::vector<pir::Attribute> vec_strides;
  for (size_t i = 0; i < static_cast<size_t>(strides.size()); i++) {
      pir::Attribute attr_strides = pir::Int32Attribute::get(pir::IrContext::Instance(), strides[i]);

    vec_strides.push_back(attr_strides);
  }
  pir::Attribute attr_strides = pir::ArrayAttribute::get(pir::IrContext::Instance(), vec_strides);
  argument.AddAttribute("strides", attr_strides);
  std::vector<pir::Attribute> vec_paddings;
  for (size_t i = 0; i < static_cast<size_t>(paddings.size()); i++) {
      pir::Attribute attr_paddings = pir::Int32Attribute::get(pir::IrContext::Instance(), paddings[i]);

    vec_paddings.push_back(attr_paddings);
  }
  pir::Attribute attr_paddings = pir::ArrayAttribute::get(pir::IrContext::Instance(), vec_paddings);
  argument.AddAttribute("paddings", attr_paddings);
  pir::Attribute attr_data_format = pir::StrAttribute::get(pir::IrContext::Instance(), data_format);
  argument.AddAttribute("data_format", attr_data_format);
  pir::Attribute attr_global_pooling = pir::BoolAttribute::get(pir::IrContext::Instance(), global_pooling);
  argument.AddAttribute("global_pooling", attr_global_pooling);
  pir::Attribute attr_adaptive = pir::BoolAttribute::get(pir::IrContext::Instance(), adaptive);
  argument.AddAttribute("adaptive", attr_adaptive);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;
  paddle::dialect::DenseTensorType out = out_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)out;
  paddle::dialect::DenseTensorType saved_idx = saved_idx_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)saved_idx;
  paddle::dialect::DenseTensorType out_grad = out_grad_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)out_grad;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);
  paddle::dialect::IrTensor dense_x_grad;
  paddle::dialect::IrMetaTensor meta_x_grad(&dense_x_grad);

  phi::UnchangedInferMeta(meta_x, &meta_x_grad);

  std::vector<pir::Type> argument_outputs;
  pir::Type x_grad_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_x_grad.dtype()), dense_x_grad.dims(), dense_x_grad.layout(), dense_x_grad.lod(), dense_x_grad.offset());
  argument_outputs.push_back(x_grad_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void MaxPool2dV2GradOp::VerifySig() {}

void MaxPool2dV2GradOp::InferMeta( phi::InferMetaContext *infer_meta ) {
  auto fn = PD_INFER_META(phi::UnchangedInferMeta);
  fn(infer_meta);
}

phi::DataType MaxPool2dV2GradOp::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: MaxPool2dV2GradOp";
  


  return expected_kernel_dtype;
}

} // namespace dialect
} // namespace paddle


IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::FusedBiasDropoutResidualLayerNormGradOp)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::FusedDotProductAttentionGradOp)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::FusedDropoutAddGradOp)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::FusedRotaryPositionEmbeddingGradOp)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::MaxPool2dV2GradOp)

