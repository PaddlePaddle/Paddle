// This file is generated by "paddle/fluid/pir/dialect/op_generator/op_gen.py"
#include "paddle/fluid/pir/dialect/operator/ir/op_attribute.h"
#include "paddle/fluid/pir/dialect/operator/ir/pd_op.h"
#include "paddle/fluid/primitive/rule/vjp/vjp.h"
#include "paddle/fluid/primitive/type/lazy_tensor.h"
#include "paddle/pir/core/builtin_op.h"
#include "paddle/pir/core/op_base.h"
#include "paddle/phi/common/int_array.h"
#include "paddle/fluid/pir/dialect/operator/utils/utils.h"

namespace paddle {
namespace dialect {

std::vector<std::vector<pir::OpResult>> AbsOp::Vjp(pir::Operation* op, const std::vector<std::vector<pir::Value>>& inputs_, const std::vector<std::vector<pir::Value>>& outputs, const std::vector<std::vector<pir::Value>>& out_grads, const std::vector<std::vector<bool>>& stop_gradients){

    PADDLE_ENFORCE_EQ(
      inputs_.size(),
      1,
      platform::errors::InvalidArgument("abs op's inputs size should be 1, but now is %d.", inputs_.size()));
    PADDLE_ENFORCE_EQ(
      outputs.size(),
      1,
      platform::errors::InvalidArgument("abs op's outputs size should be 1, but now is %d.", outputs.size()));

    VLOG(6) << "Prepare inputs of abs_grad";

    Tensor x(std::make_shared<primitive::LazyTensor>(inputs_[0][0]));
    Tensor out_grad(std::make_shared<primitive::LazyTensor>(out_grads[0][0]));

    VLOG(6) << "Vjp prepare Prepare attributes of abs_grad";


    VLOG(6) << "Vjp prepare call abs's vjp inteface";

    std::vector<std::vector<Tensor>> tensor_res =
        primitive::abs_vjp(
        x, out_grad, stop_gradients);

    VLOG(6) << "Vjp prepare stop gradient of abs_grad";

    std::vector<std::vector<pir::OpResult>> res(tensor_res.size());
    for (size_t i = 0; i < tensor_res.size(); ++i) {
        res[i].resize(tensor_res[i].size());
        for (size_t j = 0; j < tensor_res[i].size(); ++j) {
            if(tensor_res[i][j].defined()){
                res[i][j] = std::static_pointer_cast<primitive::LazyTensor>(tensor_res[i][j].impl())->value().dyn_cast<pir::OpResult>();
            }
        }
    }
    return res;
}

std::vector<std::vector<pir::OpResult>> Abs_Op::Vjp(pir::Operation* op, const std::vector<std::vector<pir::Value>>& inputs_, const std::vector<std::vector<pir::Value>>& outputs, const std::vector<std::vector<pir::Value>>& out_grads, const std::vector<std::vector<bool>>& stop_gradients){

    PADDLE_ENFORCE_EQ(
      inputs_.size(),
      1,
      platform::errors::InvalidArgument("abs op's inputs size should be 1, but now is %d.", inputs_.size()));
    PADDLE_ENFORCE_EQ(
      outputs.size(),
      1,
      platform::errors::InvalidArgument("abs op's outputs size should be 1, but now is %d.", outputs.size()));

    VLOG(6) << "Prepare inputs of abs_grad";

    Tensor x(std::make_shared<primitive::LazyTensor>(inputs_[0][0]));
    Tensor out_grad(std::make_shared<primitive::LazyTensor>(out_grads[0][0]));

    VLOG(6) << "Vjp prepare Prepare attributes of abs_grad";


    VLOG(6) << "Vjp prepare call abs_'s vjp inteface";

    std::vector<std::vector<Tensor>> tensor_res =
        primitive::abs_vjp(
        x, out_grad, stop_gradients);

    VLOG(6) << "Vjp prepare stop gradient of abs_grad";

    std::vector<std::vector<pir::OpResult>> res(tensor_res.size());
    for (size_t i = 0; i < tensor_res.size(); ++i) {
        res[i].resize(tensor_res[i].size());
        for (size_t j = 0; j < tensor_res[i].size(); ++j) {
            if(tensor_res[i][j].defined()){
                res[i][j] = std::static_pointer_cast<primitive::LazyTensor>(tensor_res[i][j].impl())->value().dyn_cast<pir::OpResult>();
            }
        }
    }
    return res;
}

std::vector<std::vector<pir::OpResult>> AcosOp::Vjp(pir::Operation* op, const std::vector<std::vector<pir::Value>>& inputs_, const std::vector<std::vector<pir::Value>>& outputs, const std::vector<std::vector<pir::Value>>& out_grads, const std::vector<std::vector<bool>>& stop_gradients){

    PADDLE_ENFORCE_EQ(
      inputs_.size(),
      1,
      platform::errors::InvalidArgument("acos op's inputs size should be 1, but now is %d.", inputs_.size()));
    PADDLE_ENFORCE_EQ(
      outputs.size(),
      1,
      platform::errors::InvalidArgument("acos op's outputs size should be 1, but now is %d.", outputs.size()));

    VLOG(6) << "Prepare inputs of acos_grad";

    Tensor x(std::make_shared<primitive::LazyTensor>(inputs_[0][0]));
    Tensor out_grad(std::make_shared<primitive::LazyTensor>(out_grads[0][0]));

    VLOG(6) << "Vjp prepare Prepare attributes of acos_grad";


    VLOG(6) << "Vjp prepare call acos's vjp inteface";

    std::vector<std::vector<Tensor>> tensor_res =
        primitive::acos_vjp(
        x, out_grad, stop_gradients);

    VLOG(6) << "Vjp prepare stop gradient of acos_grad";

    std::vector<std::vector<pir::OpResult>> res(tensor_res.size());
    for (size_t i = 0; i < tensor_res.size(); ++i) {
        res[i].resize(tensor_res[i].size());
        for (size_t j = 0; j < tensor_res[i].size(); ++j) {
            if(tensor_res[i][j].defined()){
                res[i][j] = std::static_pointer_cast<primitive::LazyTensor>(tensor_res[i][j].impl())->value().dyn_cast<pir::OpResult>();
            }
        }
    }
    return res;
}

std::vector<std::vector<pir::OpResult>> Acos_Op::Vjp(pir::Operation* op, const std::vector<std::vector<pir::Value>>& inputs_, const std::vector<std::vector<pir::Value>>& outputs, const std::vector<std::vector<pir::Value>>& out_grads, const std::vector<std::vector<bool>>& stop_gradients){

    PADDLE_ENFORCE_EQ(
      inputs_.size(),
      1,
      platform::errors::InvalidArgument("acos op's inputs size should be 1, but now is %d.", inputs_.size()));
    PADDLE_ENFORCE_EQ(
      outputs.size(),
      1,
      platform::errors::InvalidArgument("acos op's outputs size should be 1, but now is %d.", outputs.size()));

    VLOG(6) << "Prepare inputs of acos_grad";

    Tensor x(std::make_shared<primitive::LazyTensor>(inputs_[0][0]));
    Tensor out_grad(std::make_shared<primitive::LazyTensor>(out_grads[0][0]));

    VLOG(6) << "Vjp prepare Prepare attributes of acos_grad";


    VLOG(6) << "Vjp prepare call acos_'s vjp inteface";

    std::vector<std::vector<Tensor>> tensor_res =
        primitive::acos_vjp(
        x, out_grad, stop_gradients);

    VLOG(6) << "Vjp prepare stop gradient of acos_grad";

    std::vector<std::vector<pir::OpResult>> res(tensor_res.size());
    for (size_t i = 0; i < tensor_res.size(); ++i) {
        res[i].resize(tensor_res[i].size());
        for (size_t j = 0; j < tensor_res[i].size(); ++j) {
            if(tensor_res[i][j].defined()){
                res[i][j] = std::static_pointer_cast<primitive::LazyTensor>(tensor_res[i][j].impl())->value().dyn_cast<pir::OpResult>();
            }
        }
    }
    return res;
}

std::vector<std::vector<pir::OpResult>> AcoshOp::Vjp(pir::Operation* op, const std::vector<std::vector<pir::Value>>& inputs_, const std::vector<std::vector<pir::Value>>& outputs, const std::vector<std::vector<pir::Value>>& out_grads, const std::vector<std::vector<bool>>& stop_gradients){

    PADDLE_ENFORCE_EQ(
      inputs_.size(),
      1,
      platform::errors::InvalidArgument("acosh op's inputs size should be 1, but now is %d.", inputs_.size()));
    PADDLE_ENFORCE_EQ(
      outputs.size(),
      1,
      platform::errors::InvalidArgument("acosh op's outputs size should be 1, but now is %d.", outputs.size()));

    VLOG(6) << "Prepare inputs of acosh_grad";

    Tensor x(std::make_shared<primitive::LazyTensor>(inputs_[0][0]));
    Tensor out_grad(std::make_shared<primitive::LazyTensor>(out_grads[0][0]));

    VLOG(6) << "Vjp prepare Prepare attributes of acosh_grad";


    VLOG(6) << "Vjp prepare call acosh's vjp inteface";

    std::vector<std::vector<Tensor>> tensor_res =
        primitive::acosh_vjp(
        x, out_grad, stop_gradients);

    VLOG(6) << "Vjp prepare stop gradient of acosh_grad";

    std::vector<std::vector<pir::OpResult>> res(tensor_res.size());
    for (size_t i = 0; i < tensor_res.size(); ++i) {
        res[i].resize(tensor_res[i].size());
        for (size_t j = 0; j < tensor_res[i].size(); ++j) {
            if(tensor_res[i][j].defined()){
                res[i][j] = std::static_pointer_cast<primitive::LazyTensor>(tensor_res[i][j].impl())->value().dyn_cast<pir::OpResult>();
            }
        }
    }
    return res;
}

std::vector<std::vector<pir::OpResult>> Acosh_Op::Vjp(pir::Operation* op, const std::vector<std::vector<pir::Value>>& inputs_, const std::vector<std::vector<pir::Value>>& outputs, const std::vector<std::vector<pir::Value>>& out_grads, const std::vector<std::vector<bool>>& stop_gradients){

    PADDLE_ENFORCE_EQ(
      inputs_.size(),
      1,
      platform::errors::InvalidArgument("acosh op's inputs size should be 1, but now is %d.", inputs_.size()));
    PADDLE_ENFORCE_EQ(
      outputs.size(),
      1,
      platform::errors::InvalidArgument("acosh op's outputs size should be 1, but now is %d.", outputs.size()));

    VLOG(6) << "Prepare inputs of acosh_grad";

    Tensor x(std::make_shared<primitive::LazyTensor>(inputs_[0][0]));
    Tensor out_grad(std::make_shared<primitive::LazyTensor>(out_grads[0][0]));

    VLOG(6) << "Vjp prepare Prepare attributes of acosh_grad";


    VLOG(6) << "Vjp prepare call acosh_'s vjp inteface";

    std::vector<std::vector<Tensor>> tensor_res =
        primitive::acosh_vjp(
        x, out_grad, stop_gradients);

    VLOG(6) << "Vjp prepare stop gradient of acosh_grad";

    std::vector<std::vector<pir::OpResult>> res(tensor_res.size());
    for (size_t i = 0; i < tensor_res.size(); ++i) {
        res[i].resize(tensor_res[i].size());
        for (size_t j = 0; j < tensor_res[i].size(); ++j) {
            if(tensor_res[i][j].defined()){
                res[i][j] = std::static_pointer_cast<primitive::LazyTensor>(tensor_res[i][j].impl())->value().dyn_cast<pir::OpResult>();
            }
        }
    }
    return res;
}

std::vector<std::vector<pir::OpResult>> AddmmOp::Vjp(pir::Operation* op, const std::vector<std::vector<pir::Value>>& inputs_, const std::vector<std::vector<pir::Value>>& outputs, const std::vector<std::vector<pir::Value>>& out_grads, const std::vector<std::vector<bool>>& stop_gradients){

    PADDLE_ENFORCE_EQ(
      inputs_.size(),
      3,
      platform::errors::InvalidArgument("addmm op's inputs size should be 3, but now is %d.", inputs_.size()));
    PADDLE_ENFORCE_EQ(
      outputs.size(),
      1,
      platform::errors::InvalidArgument("addmm op's outputs size should be 1, but now is %d.", outputs.size()));

    VLOG(6) << "Prepare inputs of addmm_grad";

    Tensor input(std::make_shared<primitive::LazyTensor>(inputs_[0][0]));
    Tensor x(std::make_shared<primitive::LazyTensor>(inputs_[1][0]));
    Tensor y(std::make_shared<primitive::LazyTensor>(inputs_[2][0]));
    Tensor out_grad(std::make_shared<primitive::LazyTensor>(out_grads[0][0]));

    VLOG(6) << "Vjp prepare Prepare attributes of addmm_grad";

    float alpha = op->attribute("alpha").dyn_cast<pir::FloatAttribute>().data();
    float beta = op->attribute("beta").dyn_cast<pir::FloatAttribute>().data();

    VLOG(6) << "Vjp prepare call addmm's vjp inteface";

    std::vector<std::vector<Tensor>> tensor_res =
        primitive::addmm_vjp(
        input, x, y, out_grad, alpha, beta, stop_gradients);

    VLOG(6) << "Vjp prepare stop gradient of addmm_grad";

    std::vector<std::vector<pir::OpResult>> res(tensor_res.size());
    for (size_t i = 0; i < tensor_res.size(); ++i) {
        res[i].resize(tensor_res[i].size());
        for (size_t j = 0; j < tensor_res[i].size(); ++j) {
            if(tensor_res[i][j].defined()){
                res[i][j] = std::static_pointer_cast<primitive::LazyTensor>(tensor_res[i][j].impl())->value().dyn_cast<pir::OpResult>();
            }
        }
    }
    return res;
}

std::vector<std::vector<pir::OpResult>> Addmm_Op::Vjp(pir::Operation* op, const std::vector<std::vector<pir::Value>>& inputs_, const std::vector<std::vector<pir::Value>>& outputs, const std::vector<std::vector<pir::Value>>& out_grads, const std::vector<std::vector<bool>>& stop_gradients){

    PADDLE_ENFORCE_EQ(
      inputs_.size(),
      3,
      platform::errors::InvalidArgument("addmm op's inputs size should be 3, but now is %d.", inputs_.size()));
    PADDLE_ENFORCE_EQ(
      outputs.size(),
      1,
      platform::errors::InvalidArgument("addmm op's outputs size should be 1, but now is %d.", outputs.size()));

    VLOG(6) << "Prepare inputs of addmm_grad";

    Tensor input(std::make_shared<primitive::LazyTensor>(inputs_[0][0]));
    Tensor x(std::make_shared<primitive::LazyTensor>(inputs_[1][0]));
    Tensor y(std::make_shared<primitive::LazyTensor>(inputs_[2][0]));
    Tensor out_grad(std::make_shared<primitive::LazyTensor>(out_grads[0][0]));

    VLOG(6) << "Vjp prepare Prepare attributes of addmm_grad";

    float alpha = op->attribute("alpha").dyn_cast<pir::FloatAttribute>().data();
    float beta = op->attribute("beta").dyn_cast<pir::FloatAttribute>().data();

    VLOG(6) << "Vjp prepare call addmm_'s vjp inteface";

    std::vector<std::vector<Tensor>> tensor_res =
        primitive::addmm_vjp(
        input, x, y, out_grad, alpha, beta, stop_gradients);

    VLOG(6) << "Vjp prepare stop gradient of addmm_grad";

    std::vector<std::vector<pir::OpResult>> res(tensor_res.size());
    for (size_t i = 0; i < tensor_res.size(); ++i) {
        res[i].resize(tensor_res[i].size());
        for (size_t j = 0; j < tensor_res[i].size(); ++j) {
            if(tensor_res[i][j].defined()){
                res[i][j] = std::static_pointer_cast<primitive::LazyTensor>(tensor_res[i][j].impl())->value().dyn_cast<pir::OpResult>();
            }
        }
    }
    return res;
}

std::vector<std::vector<pir::OpResult>> AffineGridOp::Vjp(pir::Operation* op, const std::vector<std::vector<pir::Value>>& inputs_, const std::vector<std::vector<pir::Value>>& outputs, const std::vector<std::vector<pir::Value>>& out_grads, const std::vector<std::vector<bool>>& stop_gradients){

    PADDLE_ENFORCE_EQ(
      inputs_.size(),
      2,
      platform::errors::InvalidArgument("affine_grid op's inputs size should be 2, but now is %d.", inputs_.size()));
    PADDLE_ENFORCE_EQ(
      outputs.size(),
      1,
      platform::errors::InvalidArgument("affine_grid op's outputs size should be 1, but now is %d.", outputs.size()));

    VLOG(6) << "Prepare inputs of affine_grid_grad";

    Tensor input(std::make_shared<primitive::LazyTensor>(inputs_[0][0]));
    Tensor output_grad(std::make_shared<primitive::LazyTensor>(out_grads[0][0]));

    VLOG(6) << "Vjp prepare Prepare attributes of affine_grid_grad";

    Tensor output_shape(std::make_shared<primitive::LazyTensor>(inputs_[1][0]));
    bool align_corners = op->attribute("align_corners").dyn_cast<pir::BoolAttribute>().data();

    VLOG(6) << "Vjp prepare call affine_grid's vjp inteface";

    std::vector<std::vector<Tensor>> tensor_res =
        primitive::affine_grid_vjp(
        input, output_grad, output_shape, align_corners, stop_gradients);

    VLOG(6) << "Vjp prepare stop gradient of affine_grid_grad";

    std::vector<std::vector<pir::OpResult>> res(tensor_res.size());
    for (size_t i = 0; i < tensor_res.size(); ++i) {
        res[i].resize(tensor_res[i].size());
        for (size_t j = 0; j < tensor_res[i].size(); ++j) {
            if(tensor_res[i][j].defined()){
                res[i][j] = std::static_pointer_cast<primitive::LazyTensor>(tensor_res[i][j].impl())->value().dyn_cast<pir::OpResult>();
            }
        }
    }
    return res;
}

std::vector<std::vector<pir::OpResult>> AngleOp::Vjp(pir::Operation* op, const std::vector<std::vector<pir::Value>>& inputs_, const std::vector<std::vector<pir::Value>>& outputs, const std::vector<std::vector<pir::Value>>& out_grads, const std::vector<std::vector<bool>>& stop_gradients){

    PADDLE_ENFORCE_EQ(
      inputs_.size(),
      1,
      platform::errors::InvalidArgument("angle op's inputs size should be 1, but now is %d.", inputs_.size()));
    PADDLE_ENFORCE_EQ(
      outputs.size(),
      1,
      platform::errors::InvalidArgument("angle op's outputs size should be 1, but now is %d.", outputs.size()));

    VLOG(6) << "Prepare inputs of angle_grad";

    Tensor x(std::make_shared<primitive::LazyTensor>(inputs_[0][0]));
    Tensor out_grad(std::make_shared<primitive::LazyTensor>(out_grads[0][0]));

    VLOG(6) << "Vjp prepare Prepare attributes of angle_grad";


    VLOG(6) << "Vjp prepare call angle's vjp inteface";

    std::vector<std::vector<Tensor>> tensor_res =
        primitive::angle_vjp(
        x, out_grad, stop_gradients);

    VLOG(6) << "Vjp prepare stop gradient of angle_grad";

    std::vector<std::vector<pir::OpResult>> res(tensor_res.size());
    for (size_t i = 0; i < tensor_res.size(); ++i) {
        res[i].resize(tensor_res[i].size());
        for (size_t j = 0; j < tensor_res[i].size(); ++j) {
            if(tensor_res[i][j].defined()){
                res[i][j] = std::static_pointer_cast<primitive::LazyTensor>(tensor_res[i][j].impl())->value().dyn_cast<pir::OpResult>();
            }
        }
    }
    return res;
}

std::vector<std::vector<pir::OpResult>> ArgsortOp::Vjp(pir::Operation* op, const std::vector<std::vector<pir::Value>>& inputs_, const std::vector<std::vector<pir::Value>>& outputs, const std::vector<std::vector<pir::Value>>& out_grads, const std::vector<std::vector<bool>>& stop_gradients){

    PADDLE_ENFORCE_EQ(
      inputs_.size(),
      1,
      platform::errors::InvalidArgument("argsort op's inputs size should be 1, but now is %d.", inputs_.size()));
    PADDLE_ENFORCE_EQ(
      outputs.size(),
      2,
      platform::errors::InvalidArgument("argsort op's outputs size should be 2, but now is %d.", outputs.size()));

    VLOG(6) << "Prepare inputs of argsort_grad";

    Tensor indices(std::make_shared<primitive::LazyTensor>(outputs[1][0]));
    Tensor x(std::make_shared<primitive::LazyTensor>(inputs_[0][0]));
    Tensor out_grad(std::make_shared<primitive::LazyTensor>(out_grads[0][0]));

    VLOG(6) << "Vjp prepare Prepare attributes of argsort_grad";

    int axis = op->attribute("axis").dyn_cast<pir::Int32Attribute>().data();
    bool descending = op->attribute("descending").dyn_cast<pir::BoolAttribute>().data();

    VLOG(6) << "Vjp prepare call argsort's vjp inteface";

    std::vector<std::vector<Tensor>> tensor_res =
        primitive::argsort_vjp(
        indices, x, out_grad, axis, descending, stop_gradients);

    VLOG(6) << "Vjp prepare stop gradient of argsort_grad";

    std::vector<std::vector<pir::OpResult>> res(tensor_res.size());
    for (size_t i = 0; i < tensor_res.size(); ++i) {
        res[i].resize(tensor_res[i].size());
        for (size_t j = 0; j < tensor_res[i].size(); ++j) {
            if(tensor_res[i][j].defined()){
                res[i][j] = std::static_pointer_cast<primitive::LazyTensor>(tensor_res[i][j].impl())->value().dyn_cast<pir::OpResult>();
            }
        }
    }
    return res;
}

std::vector<std::vector<pir::OpResult>> AsComplexOp::Vjp(pir::Operation* op, const std::vector<std::vector<pir::Value>>& inputs_, const std::vector<std::vector<pir::Value>>& outputs, const std::vector<std::vector<pir::Value>>& out_grads, const std::vector<std::vector<bool>>& stop_gradients){

    PADDLE_ENFORCE_EQ(
      inputs_.size(),
      1,
      platform::errors::InvalidArgument("as_complex op's inputs size should be 1, but now is %d.", inputs_.size()));
    PADDLE_ENFORCE_EQ(
      outputs.size(),
      1,
      platform::errors::InvalidArgument("as_complex op's outputs size should be 1, but now is %d.", outputs.size()));

    VLOG(6) << "Prepare inputs of as_complex_grad";

    Tensor out_grad(std::make_shared<primitive::LazyTensor>(out_grads[0][0]));

    VLOG(6) << "Vjp prepare Prepare attributes of as_complex_grad";


    VLOG(6) << "Vjp prepare call as_complex's vjp inteface";

    std::vector<std::vector<Tensor>> tensor_res =
        primitive::as_complex_vjp(
        out_grad, stop_gradients);

    VLOG(6) << "Vjp prepare stop gradient of as_complex_grad";

    std::vector<std::vector<pir::OpResult>> res(tensor_res.size());
    for (size_t i = 0; i < tensor_res.size(); ++i) {
        res[i].resize(tensor_res[i].size());
        for (size_t j = 0; j < tensor_res[i].size(); ++j) {
            if(tensor_res[i][j].defined()){
                res[i][j] = std::static_pointer_cast<primitive::LazyTensor>(tensor_res[i][j].impl())->value().dyn_cast<pir::OpResult>();
            }
        }
    }
    return res;
}

std::vector<std::vector<pir::OpResult>> AsRealOp::Vjp(pir::Operation* op, const std::vector<std::vector<pir::Value>>& inputs_, const std::vector<std::vector<pir::Value>>& outputs, const std::vector<std::vector<pir::Value>>& out_grads, const std::vector<std::vector<bool>>& stop_gradients){

    PADDLE_ENFORCE_EQ(
      inputs_.size(),
      1,
      platform::errors::InvalidArgument("as_real op's inputs size should be 1, but now is %d.", inputs_.size()));
    PADDLE_ENFORCE_EQ(
      outputs.size(),
      1,
      platform::errors::InvalidArgument("as_real op's outputs size should be 1, but now is %d.", outputs.size()));

    VLOG(6) << "Prepare inputs of as_real_grad";

    Tensor out_grad(std::make_shared<primitive::LazyTensor>(out_grads[0][0]));

    VLOG(6) << "Vjp prepare Prepare attributes of as_real_grad";


    VLOG(6) << "Vjp prepare call as_real's vjp inteface";

    std::vector<std::vector<Tensor>> tensor_res =
        primitive::as_real_vjp(
        out_grad, stop_gradients);

    VLOG(6) << "Vjp prepare stop gradient of as_real_grad";

    std::vector<std::vector<pir::OpResult>> res(tensor_res.size());
    for (size_t i = 0; i < tensor_res.size(); ++i) {
        res[i].resize(tensor_res[i].size());
        for (size_t j = 0; j < tensor_res[i].size(); ++j) {
            if(tensor_res[i][j].defined()){
                res[i][j] = std::static_pointer_cast<primitive::LazyTensor>(tensor_res[i][j].impl())->value().dyn_cast<pir::OpResult>();
            }
        }
    }
    return res;
}

std::vector<std::vector<pir::OpResult>> AsStridedOp::Vjp(pir::Operation* op, const std::vector<std::vector<pir::Value>>& inputs_, const std::vector<std::vector<pir::Value>>& outputs, const std::vector<std::vector<pir::Value>>& out_grads, const std::vector<std::vector<bool>>& stop_gradients){

    PADDLE_ENFORCE_EQ(
      inputs_.size(),
      1,
      platform::errors::InvalidArgument("as_strided op's inputs size should be 1, but now is %d.", inputs_.size()));
    PADDLE_ENFORCE_EQ(
      outputs.size(),
      1,
      platform::errors::InvalidArgument("as_strided op's outputs size should be 1, but now is %d.", outputs.size()));

    VLOG(6) << "Prepare inputs of as_strided_grad";

    Tensor input(std::make_shared<primitive::LazyTensor>(inputs_[0][0]));
    Tensor out_grad(std::make_shared<primitive::LazyTensor>(out_grads[0][0]));

    VLOG(6) << "Vjp prepare Prepare attributes of as_strided_grad";

    std::vector<int64_t> dims;
    for (size_t i = 0; i < op->attribute("dims").dyn_cast<pir::ArrayAttribute>().size(); i++) {
        dims.push_back(op->attribute("dims").dyn_cast<pir::ArrayAttribute>().at(i).dyn_cast<pir::Int64Attribute>().data());
    }
    std::vector<int64_t> stride;
    for (size_t i = 0; i < op->attribute("stride").dyn_cast<pir::ArrayAttribute>().size(); i++) {
        stride.push_back(op->attribute("stride").dyn_cast<pir::ArrayAttribute>().at(i).dyn_cast<pir::Int64Attribute>().data());
    }
    int64_t offset = op->attribute("offset").dyn_cast<pir::Int64Attribute>().data();

    VLOG(6) << "Vjp prepare call as_strided's vjp inteface";

    std::vector<std::vector<Tensor>> tensor_res =
        primitive::as_strided_vjp(
        input, out_grad, dims, stride, offset, stop_gradients);

    VLOG(6) << "Vjp prepare stop gradient of as_strided_grad";

    std::vector<std::vector<pir::OpResult>> res(tensor_res.size());
    for (size_t i = 0; i < tensor_res.size(); ++i) {
        res[i].resize(tensor_res[i].size());
        for (size_t j = 0; j < tensor_res[i].size(); ++j) {
            if(tensor_res[i][j].defined()){
                res[i][j] = std::static_pointer_cast<primitive::LazyTensor>(tensor_res[i][j].impl())->value().dyn_cast<pir::OpResult>();
            }
        }
    }
    return res;
}

std::vector<std::vector<pir::OpResult>> AsinOp::Vjp(pir::Operation* op, const std::vector<std::vector<pir::Value>>& inputs_, const std::vector<std::vector<pir::Value>>& outputs, const std::vector<std::vector<pir::Value>>& out_grads, const std::vector<std::vector<bool>>& stop_gradients){

    PADDLE_ENFORCE_EQ(
      inputs_.size(),
      1,
      platform::errors::InvalidArgument("asin op's inputs size should be 1, but now is %d.", inputs_.size()));
    PADDLE_ENFORCE_EQ(
      outputs.size(),
      1,
      platform::errors::InvalidArgument("asin op's outputs size should be 1, but now is %d.", outputs.size()));

    VLOG(6) << "Prepare inputs of asin_grad";

    Tensor x(std::make_shared<primitive::LazyTensor>(inputs_[0][0]));
    Tensor out_grad(std::make_shared<primitive::LazyTensor>(out_grads[0][0]));

    VLOG(6) << "Vjp prepare Prepare attributes of asin_grad";


    VLOG(6) << "Vjp prepare call asin's vjp inteface";

    std::vector<std::vector<Tensor>> tensor_res =
        primitive::asin_vjp(
        x, out_grad, stop_gradients);

    VLOG(6) << "Vjp prepare stop gradient of asin_grad";

    std::vector<std::vector<pir::OpResult>> res(tensor_res.size());
    for (size_t i = 0; i < tensor_res.size(); ++i) {
        res[i].resize(tensor_res[i].size());
        for (size_t j = 0; j < tensor_res[i].size(); ++j) {
            if(tensor_res[i][j].defined()){
                res[i][j] = std::static_pointer_cast<primitive::LazyTensor>(tensor_res[i][j].impl())->value().dyn_cast<pir::OpResult>();
            }
        }
    }
    return res;
}

std::vector<std::vector<pir::OpResult>> Asin_Op::Vjp(pir::Operation* op, const std::vector<std::vector<pir::Value>>& inputs_, const std::vector<std::vector<pir::Value>>& outputs, const std::vector<std::vector<pir::Value>>& out_grads, const std::vector<std::vector<bool>>& stop_gradients){

    PADDLE_ENFORCE_EQ(
      inputs_.size(),
      1,
      platform::errors::InvalidArgument("asin op's inputs size should be 1, but now is %d.", inputs_.size()));
    PADDLE_ENFORCE_EQ(
      outputs.size(),
      1,
      platform::errors::InvalidArgument("asin op's outputs size should be 1, but now is %d.", outputs.size()));

    VLOG(6) << "Prepare inputs of asin_grad";

    Tensor x(std::make_shared<primitive::LazyTensor>(inputs_[0][0]));
    Tensor out_grad(std::make_shared<primitive::LazyTensor>(out_grads[0][0]));

    VLOG(6) << "Vjp prepare Prepare attributes of asin_grad";


    VLOG(6) << "Vjp prepare call asin_'s vjp inteface";

    std::vector<std::vector<Tensor>> tensor_res =
        primitive::asin_vjp(
        x, out_grad, stop_gradients);

    VLOG(6) << "Vjp prepare stop gradient of asin_grad";

    std::vector<std::vector<pir::OpResult>> res(tensor_res.size());
    for (size_t i = 0; i < tensor_res.size(); ++i) {
        res[i].resize(tensor_res[i].size());
        for (size_t j = 0; j < tensor_res[i].size(); ++j) {
            if(tensor_res[i][j].defined()){
                res[i][j] = std::static_pointer_cast<primitive::LazyTensor>(tensor_res[i][j].impl())->value().dyn_cast<pir::OpResult>();
            }
        }
    }
    return res;
}

std::vector<std::vector<pir::OpResult>> AsinhOp::Vjp(pir::Operation* op, const std::vector<std::vector<pir::Value>>& inputs_, const std::vector<std::vector<pir::Value>>& outputs, const std::vector<std::vector<pir::Value>>& out_grads, const std::vector<std::vector<bool>>& stop_gradients){

    PADDLE_ENFORCE_EQ(
      inputs_.size(),
      1,
      platform::errors::InvalidArgument("asinh op's inputs size should be 1, but now is %d.", inputs_.size()));
    PADDLE_ENFORCE_EQ(
      outputs.size(),
      1,
      platform::errors::InvalidArgument("asinh op's outputs size should be 1, but now is %d.", outputs.size()));

    VLOG(6) << "Prepare inputs of asinh_grad";

    Tensor x(std::make_shared<primitive::LazyTensor>(inputs_[0][0]));
    Tensor out_grad(std::make_shared<primitive::LazyTensor>(out_grads[0][0]));

    VLOG(6) << "Vjp prepare Prepare attributes of asinh_grad";


    VLOG(6) << "Vjp prepare call asinh's vjp inteface";

    std::vector<std::vector<Tensor>> tensor_res =
        primitive::asinh_vjp(
        x, out_grad, stop_gradients);

    VLOG(6) << "Vjp prepare stop gradient of asinh_grad";

    std::vector<std::vector<pir::OpResult>> res(tensor_res.size());
    for (size_t i = 0; i < tensor_res.size(); ++i) {
        res[i].resize(tensor_res[i].size());
        for (size_t j = 0; j < tensor_res[i].size(); ++j) {
            if(tensor_res[i][j].defined()){
                res[i][j] = std::static_pointer_cast<primitive::LazyTensor>(tensor_res[i][j].impl())->value().dyn_cast<pir::OpResult>();
            }
        }
    }
    return res;
}

std::vector<std::vector<pir::OpResult>> Asinh_Op::Vjp(pir::Operation* op, const std::vector<std::vector<pir::Value>>& inputs_, const std::vector<std::vector<pir::Value>>& outputs, const std::vector<std::vector<pir::Value>>& out_grads, const std::vector<std::vector<bool>>& stop_gradients){

    PADDLE_ENFORCE_EQ(
      inputs_.size(),
      1,
      platform::errors::InvalidArgument("asinh op's inputs size should be 1, but now is %d.", inputs_.size()));
    PADDLE_ENFORCE_EQ(
      outputs.size(),
      1,
      platform::errors::InvalidArgument("asinh op's outputs size should be 1, but now is %d.", outputs.size()));

    VLOG(6) << "Prepare inputs of asinh_grad";

    Tensor x(std::make_shared<primitive::LazyTensor>(inputs_[0][0]));
    Tensor out_grad(std::make_shared<primitive::LazyTensor>(out_grads[0][0]));

    VLOG(6) << "Vjp prepare Prepare attributes of asinh_grad";


    VLOG(6) << "Vjp prepare call asinh_'s vjp inteface";

    std::vector<std::vector<Tensor>> tensor_res =
        primitive::asinh_vjp(
        x, out_grad, stop_gradients);

    VLOG(6) << "Vjp prepare stop gradient of asinh_grad";

    std::vector<std::vector<pir::OpResult>> res(tensor_res.size());
    for (size_t i = 0; i < tensor_res.size(); ++i) {
        res[i].resize(tensor_res[i].size());
        for (size_t j = 0; j < tensor_res[i].size(); ++j) {
            if(tensor_res[i][j].defined()){
                res[i][j] = std::static_pointer_cast<primitive::LazyTensor>(tensor_res[i][j].impl())->value().dyn_cast<pir::OpResult>();
            }
        }
    }
    return res;
}

std::vector<std::vector<pir::OpResult>> AtanOp::Vjp(pir::Operation* op, const std::vector<std::vector<pir::Value>>& inputs_, const std::vector<std::vector<pir::Value>>& outputs, const std::vector<std::vector<pir::Value>>& out_grads, const std::vector<std::vector<bool>>& stop_gradients){

    PADDLE_ENFORCE_EQ(
      inputs_.size(),
      1,
      platform::errors::InvalidArgument("atan op's inputs size should be 1, but now is %d.", inputs_.size()));
    PADDLE_ENFORCE_EQ(
      outputs.size(),
      1,
      platform::errors::InvalidArgument("atan op's outputs size should be 1, but now is %d.", outputs.size()));

    VLOG(6) << "Prepare inputs of atan_grad";

    Tensor x(std::make_shared<primitive::LazyTensor>(inputs_[0][0]));
    Tensor out_grad(std::make_shared<primitive::LazyTensor>(out_grads[0][0]));

    VLOG(6) << "Vjp prepare Prepare attributes of atan_grad";


    VLOG(6) << "Vjp prepare call atan's vjp inteface";

    std::vector<std::vector<Tensor>> tensor_res =
        primitive::atan_vjp(
        x, out_grad, stop_gradients);

    VLOG(6) << "Vjp prepare stop gradient of atan_grad";

    std::vector<std::vector<pir::OpResult>> res(tensor_res.size());
    for (size_t i = 0; i < tensor_res.size(); ++i) {
        res[i].resize(tensor_res[i].size());
        for (size_t j = 0; j < tensor_res[i].size(); ++j) {
            if(tensor_res[i][j].defined()){
                res[i][j] = std::static_pointer_cast<primitive::LazyTensor>(tensor_res[i][j].impl())->value().dyn_cast<pir::OpResult>();
            }
        }
    }
    return res;
}

std::vector<std::vector<pir::OpResult>> Atan_Op::Vjp(pir::Operation* op, const std::vector<std::vector<pir::Value>>& inputs_, const std::vector<std::vector<pir::Value>>& outputs, const std::vector<std::vector<pir::Value>>& out_grads, const std::vector<std::vector<bool>>& stop_gradients){

    PADDLE_ENFORCE_EQ(
      inputs_.size(),
      1,
      platform::errors::InvalidArgument("atan op's inputs size should be 1, but now is %d.", inputs_.size()));
    PADDLE_ENFORCE_EQ(
      outputs.size(),
      1,
      platform::errors::InvalidArgument("atan op's outputs size should be 1, but now is %d.", outputs.size()));

    VLOG(6) << "Prepare inputs of atan_grad";

    Tensor x(std::make_shared<primitive::LazyTensor>(inputs_[0][0]));
    Tensor out_grad(std::make_shared<primitive::LazyTensor>(out_grads[0][0]));

    VLOG(6) << "Vjp prepare Prepare attributes of atan_grad";


    VLOG(6) << "Vjp prepare call atan_'s vjp inteface";

    std::vector<std::vector<Tensor>> tensor_res =
        primitive::atan_vjp(
        x, out_grad, stop_gradients);

    VLOG(6) << "Vjp prepare stop gradient of atan_grad";

    std::vector<std::vector<pir::OpResult>> res(tensor_res.size());
    for (size_t i = 0; i < tensor_res.size(); ++i) {
        res[i].resize(tensor_res[i].size());
        for (size_t j = 0; j < tensor_res[i].size(); ++j) {
            if(tensor_res[i][j].defined()){
                res[i][j] = std::static_pointer_cast<primitive::LazyTensor>(tensor_res[i][j].impl())->value().dyn_cast<pir::OpResult>();
            }
        }
    }
    return res;
}

std::vector<std::vector<pir::OpResult>> Atan2Op::Vjp(pir::Operation* op, const std::vector<std::vector<pir::Value>>& inputs_, const std::vector<std::vector<pir::Value>>& outputs, const std::vector<std::vector<pir::Value>>& out_grads, const std::vector<std::vector<bool>>& stop_gradients){

    PADDLE_ENFORCE_EQ(
      inputs_.size(),
      2,
      platform::errors::InvalidArgument("atan2 op's inputs size should be 2, but now is %d.", inputs_.size()));
    PADDLE_ENFORCE_EQ(
      outputs.size(),
      1,
      platform::errors::InvalidArgument("atan2 op's outputs size should be 1, but now is %d.", outputs.size()));

    VLOG(6) << "Prepare inputs of atan2_grad";

    Tensor x(std::make_shared<primitive::LazyTensor>(inputs_[0][0]));
    Tensor y(std::make_shared<primitive::LazyTensor>(inputs_[1][0]));
    Tensor out_grad(std::make_shared<primitive::LazyTensor>(out_grads[0][0]));

    VLOG(6) << "Vjp prepare Prepare attributes of atan2_grad";


    VLOG(6) << "Vjp prepare call atan2's vjp inteface";

    std::vector<std::vector<Tensor>> tensor_res =
        primitive::atan2_vjp(
        x, y, out_grad, stop_gradients);

    VLOG(6) << "Vjp prepare stop gradient of atan2_grad";

    std::vector<std::vector<pir::OpResult>> res(tensor_res.size());
    for (size_t i = 0; i < tensor_res.size(); ++i) {
        res[i].resize(tensor_res[i].size());
        for (size_t j = 0; j < tensor_res[i].size(); ++j) {
            if(tensor_res[i][j].defined()){
                res[i][j] = std::static_pointer_cast<primitive::LazyTensor>(tensor_res[i][j].impl())->value().dyn_cast<pir::OpResult>();
            }
        }
    }
    return res;
}

std::vector<std::vector<pir::OpResult>> AtanhOp::Vjp(pir::Operation* op, const std::vector<std::vector<pir::Value>>& inputs_, const std::vector<std::vector<pir::Value>>& outputs, const std::vector<std::vector<pir::Value>>& out_grads, const std::vector<std::vector<bool>>& stop_gradients){

    PADDLE_ENFORCE_EQ(
      inputs_.size(),
      1,
      platform::errors::InvalidArgument("atanh op's inputs size should be 1, but now is %d.", inputs_.size()));
    PADDLE_ENFORCE_EQ(
      outputs.size(),
      1,
      platform::errors::InvalidArgument("atanh op's outputs size should be 1, but now is %d.", outputs.size()));

    VLOG(6) << "Prepare inputs of atanh_grad";

    Tensor x(std::make_shared<primitive::LazyTensor>(inputs_[0][0]));
    Tensor out_grad(std::make_shared<primitive::LazyTensor>(out_grads[0][0]));

    VLOG(6) << "Vjp prepare Prepare attributes of atanh_grad";


    VLOG(6) << "Vjp prepare call atanh's vjp inteface";

    std::vector<std::vector<Tensor>> tensor_res =
        primitive::atanh_vjp(
        x, out_grad, stop_gradients);

    VLOG(6) << "Vjp prepare stop gradient of atanh_grad";

    std::vector<std::vector<pir::OpResult>> res(tensor_res.size());
    for (size_t i = 0; i < tensor_res.size(); ++i) {
        res[i].resize(tensor_res[i].size());
        for (size_t j = 0; j < tensor_res[i].size(); ++j) {
            if(tensor_res[i][j].defined()){
                res[i][j] = std::static_pointer_cast<primitive::LazyTensor>(tensor_res[i][j].impl())->value().dyn_cast<pir::OpResult>();
            }
        }
    }
    return res;
}

std::vector<std::vector<pir::OpResult>> Atanh_Op::Vjp(pir::Operation* op, const std::vector<std::vector<pir::Value>>& inputs_, const std::vector<std::vector<pir::Value>>& outputs, const std::vector<std::vector<pir::Value>>& out_grads, const std::vector<std::vector<bool>>& stop_gradients){

    PADDLE_ENFORCE_EQ(
      inputs_.size(),
      1,
      platform::errors::InvalidArgument("atanh op's inputs size should be 1, but now is %d.", inputs_.size()));
    PADDLE_ENFORCE_EQ(
      outputs.size(),
      1,
      platform::errors::InvalidArgument("atanh op's outputs size should be 1, but now is %d.", outputs.size()));

    VLOG(6) << "Prepare inputs of atanh_grad";

    Tensor x(std::make_shared<primitive::LazyTensor>(inputs_[0][0]));
    Tensor out_grad(std::make_shared<primitive::LazyTensor>(out_grads[0][0]));

    VLOG(6) << "Vjp prepare Prepare attributes of atanh_grad";


    VLOG(6) << "Vjp prepare call atanh_'s vjp inteface";

    std::vector<std::vector<Tensor>> tensor_res =
        primitive::atanh_vjp(
        x, out_grad, stop_gradients);

    VLOG(6) << "Vjp prepare stop gradient of atanh_grad";

    std::vector<std::vector<pir::OpResult>> res(tensor_res.size());
    for (size_t i = 0; i < tensor_res.size(); ++i) {
        res[i].resize(tensor_res[i].size());
        for (size_t j = 0; j < tensor_res[i].size(); ++j) {
            if(tensor_res[i][j].defined()){
                res[i][j] = std::static_pointer_cast<primitive::LazyTensor>(tensor_res[i][j].impl())->value().dyn_cast<pir::OpResult>();
            }
        }
    }
    return res;
}

std::vector<std::vector<pir::OpResult>> BceLossOp::Vjp(pir::Operation* op, const std::vector<std::vector<pir::Value>>& inputs_, const std::vector<std::vector<pir::Value>>& outputs, const std::vector<std::vector<pir::Value>>& out_grads, const std::vector<std::vector<bool>>& stop_gradients){

    PADDLE_ENFORCE_EQ(
      inputs_.size(),
      2,
      platform::errors::InvalidArgument("bce_loss op's inputs size should be 2, but now is %d.", inputs_.size()));
    PADDLE_ENFORCE_EQ(
      outputs.size(),
      1,
      platform::errors::InvalidArgument("bce_loss op's outputs size should be 1, but now is %d.", outputs.size()));

    VLOG(6) << "Prepare inputs of bce_loss_grad";

    Tensor input(std::make_shared<primitive::LazyTensor>(inputs_[0][0]));
    Tensor label(std::make_shared<primitive::LazyTensor>(inputs_[1][0]));
    Tensor out_grad(std::make_shared<primitive::LazyTensor>(out_grads[0][0]));

    VLOG(6) << "Vjp prepare Prepare attributes of bce_loss_grad";


    VLOG(6) << "Vjp prepare call bce_loss's vjp inteface";

    std::vector<std::vector<Tensor>> tensor_res =
        primitive::bce_loss_vjp(
        input, label, out_grad, stop_gradients);

    VLOG(6) << "Vjp prepare stop gradient of bce_loss_grad";

    std::vector<std::vector<pir::OpResult>> res(tensor_res.size());
    for (size_t i = 0; i < tensor_res.size(); ++i) {
        res[i].resize(tensor_res[i].size());
        for (size_t j = 0; j < tensor_res[i].size(); ++j) {
            if(tensor_res[i][j].defined()){
                res[i][j] = std::static_pointer_cast<primitive::LazyTensor>(tensor_res[i][j].impl())->value().dyn_cast<pir::OpResult>();
            }
        }
    }
    return res;
}

std::vector<std::vector<pir::OpResult>> BceLoss_Op::Vjp(pir::Operation* op, const std::vector<std::vector<pir::Value>>& inputs_, const std::vector<std::vector<pir::Value>>& outputs, const std::vector<std::vector<pir::Value>>& out_grads, const std::vector<std::vector<bool>>& stop_gradients){

    PADDLE_ENFORCE_EQ(
      inputs_.size(),
      2,
      platform::errors::InvalidArgument("bce_loss op's inputs size should be 2, but now is %d.", inputs_.size()));
    PADDLE_ENFORCE_EQ(
      outputs.size(),
      1,
      platform::errors::InvalidArgument("bce_loss op's outputs size should be 1, but now is %d.", outputs.size()));

    VLOG(6) << "Prepare inputs of bce_loss_grad";

    Tensor input(std::make_shared<primitive::LazyTensor>(inputs_[0][0]));
    Tensor label(std::make_shared<primitive::LazyTensor>(inputs_[1][0]));
    Tensor out_grad(std::make_shared<primitive::LazyTensor>(out_grads[0][0]));

    VLOG(6) << "Vjp prepare Prepare attributes of bce_loss_grad";


    VLOG(6) << "Vjp prepare call bce_loss_'s vjp inteface";

    std::vector<std::vector<Tensor>> tensor_res =
        primitive::bce_loss_vjp(
        input, label, out_grad, stop_gradients);

    VLOG(6) << "Vjp prepare stop gradient of bce_loss_grad";

    std::vector<std::vector<pir::OpResult>> res(tensor_res.size());
    for (size_t i = 0; i < tensor_res.size(); ++i) {
        res[i].resize(tensor_res[i].size());
        for (size_t j = 0; j < tensor_res[i].size(); ++j) {
            if(tensor_res[i][j].defined()){
                res[i][j] = std::static_pointer_cast<primitive::LazyTensor>(tensor_res[i][j].impl())->value().dyn_cast<pir::OpResult>();
            }
        }
    }
    return res;
}

std::vector<std::vector<pir::OpResult>> BicubicInterpOp::Vjp(pir::Operation* op, const std::vector<std::vector<pir::Value>>& inputs_, const std::vector<std::vector<pir::Value>>& outputs, const std::vector<std::vector<pir::Value>>& out_grads, const std::vector<std::vector<bool>>& stop_gradients){

    PADDLE_ENFORCE_EQ(
      inputs_.size(),
      4,
      platform::errors::InvalidArgument("bicubic_interp op's inputs size should be 4, but now is %d.", inputs_.size()));
    PADDLE_ENFORCE_EQ(
      outputs.size(),
      1,
      platform::errors::InvalidArgument("bicubic_interp op's outputs size should be 1, but now is %d.", outputs.size()));

    VLOG(6) << "Prepare inputs of bicubic_interp_grad";

    Tensor x(std::make_shared<primitive::LazyTensor>(inputs_[0][0]));
    paddle::optional<Tensor> out_size;
    if (!IsEmptyValue(inputs_[1][0])){
        out_size = paddle::make_optional<Tensor>(Tensor(std::make_shared<primitive::LazyTensor>(inputs_[1][0])));
    }
    paddle::optional<std::vector<Tensor>> size_tensor;
    std::vector<Tensor> optional_size_tensor;
    if (!IsEmptyValue(inputs_[2][0])){
        for (size_t idx = 0; idx < inputs_[2].size(); idx++) {
            optional_size_tensor.emplace_back(
                std::make_shared<primitive::LazyTensor>(inputs_[2][idx]));
        }
        size_tensor = paddle::make_optional<std::vector<Tensor>>(optional_size_tensor);
    }
    paddle::optional<Tensor> scale_tensor;
    if (!IsEmptyValue(inputs_[3][0])){
        scale_tensor = paddle::make_optional<Tensor>(Tensor(std::make_shared<primitive::LazyTensor>(inputs_[3][0])));
    }
    Tensor output_grad(std::make_shared<primitive::LazyTensor>(out_grads[0][0]));

    VLOG(6) << "Vjp prepare Prepare attributes of bicubic_interp_grad";

    std::string data_layout = op->attribute("data_layout").dyn_cast<pir::StrAttribute>().AsString();
    int out_d = op->attribute("out_d").dyn_cast<pir::Int32Attribute>().data();
    int out_h = op->attribute("out_h").dyn_cast<pir::Int32Attribute>().data();
    int out_w = op->attribute("out_w").dyn_cast<pir::Int32Attribute>().data();
    std::vector<float> scale;
    for (size_t i = 0; i < op->attribute("scale").dyn_cast<pir::ArrayAttribute>().size(); i++) {
        scale.push_back(op->attribute("scale").dyn_cast<pir::ArrayAttribute>().at(i).dyn_cast<pir::FloatAttribute>().data());
    }
    std::string interp_method = op->attribute("interp_method").dyn_cast<pir::StrAttribute>().AsString();
    bool align_corners = op->attribute("align_corners").dyn_cast<pir::BoolAttribute>().data();
    int align_mode = op->attribute("align_mode").dyn_cast<pir::Int32Attribute>().data();

    VLOG(6) << "Vjp prepare call bicubic_interp's vjp inteface";

    std::vector<std::vector<Tensor>> tensor_res =
        primitive::bicubic_interp_vjp(
        x, out_size, size_tensor, scale_tensor, output_grad, data_layout, out_d, out_h, out_w, scale, interp_method, align_corners, align_mode, stop_gradients);

    VLOG(6) << "Vjp prepare stop gradient of bicubic_interp_grad";

    std::vector<std::vector<pir::OpResult>> res(tensor_res.size());
    for (size_t i = 0; i < tensor_res.size(); ++i) {
        res[i].resize(tensor_res[i].size());
        for (size_t j = 0; j < tensor_res[i].size(); ++j) {
            if(tensor_res[i][j].defined()){
                res[i][j] = std::static_pointer_cast<primitive::LazyTensor>(tensor_res[i][j].impl())->value().dyn_cast<pir::OpResult>();
            }
        }
    }
    return res;
}

std::vector<std::vector<pir::OpResult>> BilinearOp::Vjp(pir::Operation* op, const std::vector<std::vector<pir::Value>>& inputs_, const std::vector<std::vector<pir::Value>>& outputs, const std::vector<std::vector<pir::Value>>& out_grads, const std::vector<std::vector<bool>>& stop_gradients){

    PADDLE_ENFORCE_EQ(
      inputs_.size(),
      4,
      platform::errors::InvalidArgument("bilinear op's inputs size should be 4, but now is %d.", inputs_.size()));
    PADDLE_ENFORCE_EQ(
      outputs.size(),
      1,
      platform::errors::InvalidArgument("bilinear op's outputs size should be 1, but now is %d.", outputs.size()));

    VLOG(6) << "Prepare inputs of bilinear_grad";

    Tensor x(std::make_shared<primitive::LazyTensor>(inputs_[0][0]));
    Tensor y(std::make_shared<primitive::LazyTensor>(inputs_[1][0]));
    Tensor weight(std::make_shared<primitive::LazyTensor>(inputs_[2][0]));
    Tensor out_grad(std::make_shared<primitive::LazyTensor>(out_grads[0][0]));

    VLOG(6) << "Vjp prepare Prepare attributes of bilinear_grad";


    VLOG(6) << "Vjp prepare call bilinear's vjp inteface";

    std::vector<std::vector<Tensor>> tensor_res =
        primitive::bilinear_vjp(
        x, y, weight, out_grad, stop_gradients);

    VLOG(6) << "Vjp prepare stop gradient of bilinear_grad";

    std::vector<std::vector<pir::OpResult>> res(tensor_res.size());
    for (size_t i = 0; i < tensor_res.size(); ++i) {
        res[i].resize(tensor_res[i].size());
        for (size_t j = 0; j < tensor_res[i].size(); ++j) {
            if(tensor_res[i][j].defined()){
                res[i][j] = std::static_pointer_cast<primitive::LazyTensor>(tensor_res[i][j].impl())->value().dyn_cast<pir::OpResult>();
            }
        }
    }
    return res;
}

std::vector<std::vector<pir::OpResult>> BilinearInterpOp::Vjp(pir::Operation* op, const std::vector<std::vector<pir::Value>>& inputs_, const std::vector<std::vector<pir::Value>>& outputs, const std::vector<std::vector<pir::Value>>& out_grads, const std::vector<std::vector<bool>>& stop_gradients){

    PADDLE_ENFORCE_EQ(
      inputs_.size(),
      4,
      platform::errors::InvalidArgument("bilinear_interp op's inputs size should be 4, but now is %d.", inputs_.size()));
    PADDLE_ENFORCE_EQ(
      outputs.size(),
      1,
      platform::errors::InvalidArgument("bilinear_interp op's outputs size should be 1, but now is %d.", outputs.size()));

    VLOG(6) << "Prepare inputs of bilinear_interp_grad";

    Tensor x(std::make_shared<primitive::LazyTensor>(inputs_[0][0]));
    paddle::optional<Tensor> out_size;
    if (!IsEmptyValue(inputs_[1][0])){
        out_size = paddle::make_optional<Tensor>(Tensor(std::make_shared<primitive::LazyTensor>(inputs_[1][0])));
    }
    paddle::optional<std::vector<Tensor>> size_tensor;
    std::vector<Tensor> optional_size_tensor;
    if (!IsEmptyValue(inputs_[2][0])){
        for (size_t idx = 0; idx < inputs_[2].size(); idx++) {
            optional_size_tensor.emplace_back(
                std::make_shared<primitive::LazyTensor>(inputs_[2][idx]));
        }
        size_tensor = paddle::make_optional<std::vector<Tensor>>(optional_size_tensor);
    }
    paddle::optional<Tensor> scale_tensor;
    if (!IsEmptyValue(inputs_[3][0])){
        scale_tensor = paddle::make_optional<Tensor>(Tensor(std::make_shared<primitive::LazyTensor>(inputs_[3][0])));
    }
    Tensor output_grad(std::make_shared<primitive::LazyTensor>(out_grads[0][0]));

    VLOG(6) << "Vjp prepare Prepare attributes of bilinear_interp_grad";

    std::string data_layout = op->attribute("data_layout").dyn_cast<pir::StrAttribute>().AsString();
    int out_d = op->attribute("out_d").dyn_cast<pir::Int32Attribute>().data();
    int out_h = op->attribute("out_h").dyn_cast<pir::Int32Attribute>().data();
    int out_w = op->attribute("out_w").dyn_cast<pir::Int32Attribute>().data();
    std::vector<float> scale;
    for (size_t i = 0; i < op->attribute("scale").dyn_cast<pir::ArrayAttribute>().size(); i++) {
        scale.push_back(op->attribute("scale").dyn_cast<pir::ArrayAttribute>().at(i).dyn_cast<pir::FloatAttribute>().data());
    }
    std::string interp_method = op->attribute("interp_method").dyn_cast<pir::StrAttribute>().AsString();
    bool align_corners = op->attribute("align_corners").dyn_cast<pir::BoolAttribute>().data();
    int align_mode = op->attribute("align_mode").dyn_cast<pir::Int32Attribute>().data();

    VLOG(6) << "Vjp prepare call bilinear_interp's vjp inteface";

    std::vector<std::vector<Tensor>> tensor_res =
        primitive::bilinear_interp_vjp(
        x, out_size, size_tensor, scale_tensor, output_grad, data_layout, out_d, out_h, out_w, scale, interp_method, align_corners, align_mode, stop_gradients);

    VLOG(6) << "Vjp prepare stop gradient of bilinear_interp_grad";

    std::vector<std::vector<pir::OpResult>> res(tensor_res.size());
    for (size_t i = 0; i < tensor_res.size(); ++i) {
        res[i].resize(tensor_res[i].size());
        for (size_t j = 0; j < tensor_res[i].size(); ++j) {
            if(tensor_res[i][j].defined()){
                res[i][j] = std::static_pointer_cast<primitive::LazyTensor>(tensor_res[i][j].impl())->value().dyn_cast<pir::OpResult>();
            }
        }
    }
    return res;
}

std::vector<std::vector<pir::OpResult>> BmmOp::Vjp(pir::Operation* op, const std::vector<std::vector<pir::Value>>& inputs_, const std::vector<std::vector<pir::Value>>& outputs, const std::vector<std::vector<pir::Value>>& out_grads, const std::vector<std::vector<bool>>& stop_gradients){

    PADDLE_ENFORCE_EQ(
      inputs_.size(),
      2,
      platform::errors::InvalidArgument("bmm op's inputs size should be 2, but now is %d.", inputs_.size()));
    PADDLE_ENFORCE_EQ(
      outputs.size(),
      1,
      platform::errors::InvalidArgument("bmm op's outputs size should be 1, but now is %d.", outputs.size()));

    VLOG(6) << "Prepare inputs of bmm_grad";

    Tensor x(std::make_shared<primitive::LazyTensor>(inputs_[0][0]));
    Tensor y(std::make_shared<primitive::LazyTensor>(inputs_[1][0]));
    Tensor out_grad(std::make_shared<primitive::LazyTensor>(out_grads[0][0]));

    VLOG(6) << "Vjp prepare Prepare attributes of bmm_grad";


    VLOG(6) << "Vjp prepare call bmm's vjp inteface";

    std::vector<std::vector<Tensor>> tensor_res =
        primitive::bmm_vjp(
        x, y, out_grad, stop_gradients);

    VLOG(6) << "Vjp prepare stop gradient of bmm_grad";

    std::vector<std::vector<pir::OpResult>> res(tensor_res.size());
    for (size_t i = 0; i < tensor_res.size(); ++i) {
        res[i].resize(tensor_res[i].size());
        for (size_t j = 0; j < tensor_res[i].size(); ++j) {
            if(tensor_res[i][j].defined()){
                res[i][j] = std::static_pointer_cast<primitive::LazyTensor>(tensor_res[i][j].impl())->value().dyn_cast<pir::OpResult>();
            }
        }
    }
    return res;
}

std::vector<std::vector<pir::OpResult>> BroadcastTensorsOp::Vjp(pir::Operation* op, const std::vector<std::vector<pir::Value>>& inputs_, const std::vector<std::vector<pir::Value>>& outputs, const std::vector<std::vector<pir::Value>>& out_grads, const std::vector<std::vector<bool>>& stop_gradients){

    PADDLE_ENFORCE_EQ(
      inputs_.size(),
      1,
      platform::errors::InvalidArgument("broadcast_tensors op's inputs size should be 1, but now is %d.", inputs_.size()));
    PADDLE_ENFORCE_EQ(
      outputs.size(),
      1,
      platform::errors::InvalidArgument("broadcast_tensors op's outputs size should be 1, but now is %d.", outputs.size()));

    VLOG(6) << "Prepare inputs of broadcast_tensors_grad";

    std::vector<Tensor> input;
    for (size_t idx = 0; idx < inputs_[0].size(); idx++) {
        input.emplace_back(
            std::make_shared<primitive::LazyTensor>(inputs_[0][idx]));
    }
    std::vector<Tensor> out_grad;
    for (size_t idx = 0; idx < out_grads[0].size(); idx++) {
        out_grad.emplace_back(
            std::make_shared<primitive::LazyTensor>(out_grads[0][idx]));
    }

    VLOG(6) << "Vjp prepare Prepare attributes of broadcast_tensors_grad";


    VLOG(6) << "Vjp prepare call broadcast_tensors's vjp inteface";

    std::vector<std::vector<Tensor>> tensor_res =
        primitive::broadcast_tensors_vjp(
        input, out_grad, stop_gradients);

    VLOG(6) << "Vjp prepare stop gradient of broadcast_tensors_grad";

    std::vector<std::vector<pir::OpResult>> res(tensor_res.size());
    for (size_t i = 0; i < tensor_res.size(); ++i) {
        res[i].resize(tensor_res[i].size());
        for (size_t j = 0; j < tensor_res[i].size(); ++j) {
            if(tensor_res[i][j].defined()){
                res[i][j] = std::static_pointer_cast<primitive::LazyTensor>(tensor_res[i][j].impl())->value().dyn_cast<pir::OpResult>();
            }
        }
    }
    return res;
}

std::vector<std::vector<pir::OpResult>> CeilOp::Vjp(pir::Operation* op, const std::vector<std::vector<pir::Value>>& inputs_, const std::vector<std::vector<pir::Value>>& outputs, const std::vector<std::vector<pir::Value>>& out_grads, const std::vector<std::vector<bool>>& stop_gradients){

    PADDLE_ENFORCE_EQ(
      inputs_.size(),
      1,
      platform::errors::InvalidArgument("ceil op's inputs size should be 1, but now is %d.", inputs_.size()));
    PADDLE_ENFORCE_EQ(
      outputs.size(),
      1,
      platform::errors::InvalidArgument("ceil op's outputs size should be 1, but now is %d.", outputs.size()));

    VLOG(6) << "Prepare inputs of ceil_grad";

    Tensor out_grad(std::make_shared<primitive::LazyTensor>(out_grads[0][0]));

    VLOG(6) << "Vjp prepare Prepare attributes of ceil_grad";


    VLOG(6) << "Vjp prepare call ceil's vjp inteface";

    std::vector<std::vector<Tensor>> tensor_res =
        primitive::ceil_vjp(
        out_grad, stop_gradients);

    VLOG(6) << "Vjp prepare stop gradient of ceil_grad";

    std::vector<std::vector<pir::OpResult>> res(tensor_res.size());
    for (size_t i = 0; i < tensor_res.size(); ++i) {
        res[i].resize(tensor_res[i].size());
        for (size_t j = 0; j < tensor_res[i].size(); ++j) {
            if(tensor_res[i][j].defined()){
                res[i][j] = std::static_pointer_cast<primitive::LazyTensor>(tensor_res[i][j].impl())->value().dyn_cast<pir::OpResult>();
            }
        }
    }
    return res;
}

std::vector<std::vector<pir::OpResult>> Ceil_Op::Vjp(pir::Operation* op, const std::vector<std::vector<pir::Value>>& inputs_, const std::vector<std::vector<pir::Value>>& outputs, const std::vector<std::vector<pir::Value>>& out_grads, const std::vector<std::vector<bool>>& stop_gradients){

    PADDLE_ENFORCE_EQ(
      inputs_.size(),
      1,
      platform::errors::InvalidArgument("ceil op's inputs size should be 1, but now is %d.", inputs_.size()));
    PADDLE_ENFORCE_EQ(
      outputs.size(),
      1,
      platform::errors::InvalidArgument("ceil op's outputs size should be 1, but now is %d.", outputs.size()));

    VLOG(6) << "Prepare inputs of ceil_grad";

    Tensor out_grad(std::make_shared<primitive::LazyTensor>(out_grads[0][0]));

    VLOG(6) << "Vjp prepare Prepare attributes of ceil_grad";


    VLOG(6) << "Vjp prepare call ceil_'s vjp inteface";

    std::vector<std::vector<Tensor>> tensor_res =
        primitive::ceil_vjp(
        out_grad, stop_gradients);

    VLOG(6) << "Vjp prepare stop gradient of ceil_grad";

    std::vector<std::vector<pir::OpResult>> res(tensor_res.size());
    for (size_t i = 0; i < tensor_res.size(); ++i) {
        res[i].resize(tensor_res[i].size());
        for (size_t j = 0; j < tensor_res[i].size(); ++j) {
            if(tensor_res[i][j].defined()){
                res[i][j] = std::static_pointer_cast<primitive::LazyTensor>(tensor_res[i][j].impl())->value().dyn_cast<pir::OpResult>();
            }
        }
    }
    return res;
}

std::vector<std::vector<pir::OpResult>> CeluOp::Vjp(pir::Operation* op, const std::vector<std::vector<pir::Value>>& inputs_, const std::vector<std::vector<pir::Value>>& outputs, const std::vector<std::vector<pir::Value>>& out_grads, const std::vector<std::vector<bool>>& stop_gradients){

    PADDLE_ENFORCE_EQ(
      inputs_.size(),
      1,
      platform::errors::InvalidArgument("celu op's inputs size should be 1, but now is %d.", inputs_.size()));
    PADDLE_ENFORCE_EQ(
      outputs.size(),
      1,
      platform::errors::InvalidArgument("celu op's outputs size should be 1, but now is %d.", outputs.size()));

    VLOG(6) << "Prepare inputs of celu_grad";

    Tensor x(std::make_shared<primitive::LazyTensor>(inputs_[0][0]));
    Tensor out_grad(std::make_shared<primitive::LazyTensor>(out_grads[0][0]));

    VLOG(6) << "Vjp prepare Prepare attributes of celu_grad";

    float alpha = op->attribute("alpha").dyn_cast<pir::FloatAttribute>().data();

    VLOG(6) << "Vjp prepare call celu's vjp inteface";

    std::vector<std::vector<Tensor>> tensor_res =
        primitive::celu_vjp(
        x, out_grad, alpha, stop_gradients);

    VLOG(6) << "Vjp prepare stop gradient of celu_grad";

    std::vector<std::vector<pir::OpResult>> res(tensor_res.size());
    for (size_t i = 0; i < tensor_res.size(); ++i) {
        res[i].resize(tensor_res[i].size());
        for (size_t j = 0; j < tensor_res[i].size(); ++j) {
            if(tensor_res[i][j].defined()){
                res[i][j] = std::static_pointer_cast<primitive::LazyTensor>(tensor_res[i][j].impl())->value().dyn_cast<pir::OpResult>();
            }
        }
    }
    return res;
}

std::vector<std::vector<pir::OpResult>> CholeskyOp::Vjp(pir::Operation* op, const std::vector<std::vector<pir::Value>>& inputs_, const std::vector<std::vector<pir::Value>>& outputs, const std::vector<std::vector<pir::Value>>& out_grads, const std::vector<std::vector<bool>>& stop_gradients){

    PADDLE_ENFORCE_EQ(
      inputs_.size(),
      1,
      platform::errors::InvalidArgument("cholesky op's inputs size should be 1, but now is %d.", inputs_.size()));
    PADDLE_ENFORCE_EQ(
      outputs.size(),
      1,
      platform::errors::InvalidArgument("cholesky op's outputs size should be 1, but now is %d.", outputs.size()));

    VLOG(6) << "Prepare inputs of cholesky_grad";

    Tensor out(std::make_shared<primitive::LazyTensor>(outputs[0][0]));
    Tensor out_grad(std::make_shared<primitive::LazyTensor>(out_grads[0][0]));

    VLOG(6) << "Vjp prepare Prepare attributes of cholesky_grad";

    bool upper = op->attribute("upper").dyn_cast<pir::BoolAttribute>().data();

    VLOG(6) << "Vjp prepare call cholesky's vjp inteface";

    std::vector<std::vector<Tensor>> tensor_res =
        primitive::cholesky_vjp(
        out, out_grad, upper, stop_gradients);

    VLOG(6) << "Vjp prepare stop gradient of cholesky_grad";

    std::vector<std::vector<pir::OpResult>> res(tensor_res.size());
    for (size_t i = 0; i < tensor_res.size(); ++i) {
        res[i].resize(tensor_res[i].size());
        for (size_t j = 0; j < tensor_res[i].size(); ++j) {
            if(tensor_res[i][j].defined()){
                res[i][j] = std::static_pointer_cast<primitive::LazyTensor>(tensor_res[i][j].impl())->value().dyn_cast<pir::OpResult>();
            }
        }
    }
    return res;
}

std::vector<std::vector<pir::OpResult>> CholeskySolveOp::Vjp(pir::Operation* op, const std::vector<std::vector<pir::Value>>& inputs_, const std::vector<std::vector<pir::Value>>& outputs, const std::vector<std::vector<pir::Value>>& out_grads, const std::vector<std::vector<bool>>& stop_gradients){

    PADDLE_ENFORCE_EQ(
      inputs_.size(),
      2,
      platform::errors::InvalidArgument("cholesky_solve op's inputs size should be 2, but now is %d.", inputs_.size()));
    PADDLE_ENFORCE_EQ(
      outputs.size(),
      1,
      platform::errors::InvalidArgument("cholesky_solve op's outputs size should be 1, but now is %d.", outputs.size()));

    VLOG(6) << "Prepare inputs of cholesky_solve_grad";

    Tensor x(std::make_shared<primitive::LazyTensor>(inputs_[0][0]));
    Tensor y(std::make_shared<primitive::LazyTensor>(inputs_[1][0]));
    Tensor out(std::make_shared<primitive::LazyTensor>(outputs[0][0]));
    Tensor out_grad(std::make_shared<primitive::LazyTensor>(out_grads[0][0]));

    VLOG(6) << "Vjp prepare Prepare attributes of cholesky_solve_grad";

    bool upper = op->attribute("upper").dyn_cast<pir::BoolAttribute>().data();

    VLOG(6) << "Vjp prepare call cholesky_solve's vjp inteface";

    std::vector<std::vector<Tensor>> tensor_res =
        primitive::cholesky_solve_vjp(
        x, y, out, out_grad, upper, stop_gradients);

    VLOG(6) << "Vjp prepare stop gradient of cholesky_solve_grad";

    std::vector<std::vector<pir::OpResult>> res(tensor_res.size());
    for (size_t i = 0; i < tensor_res.size(); ++i) {
        res[i].resize(tensor_res[i].size());
        for (size_t j = 0; j < tensor_res[i].size(); ++j) {
            if(tensor_res[i][j].defined()){
                res[i][j] = std::static_pointer_cast<primitive::LazyTensor>(tensor_res[i][j].impl())->value().dyn_cast<pir::OpResult>();
            }
        }
    }
    return res;
}

std::vector<std::vector<pir::OpResult>> ClipOp::Vjp(pir::Operation* op, const std::vector<std::vector<pir::Value>>& inputs_, const std::vector<std::vector<pir::Value>>& outputs, const std::vector<std::vector<pir::Value>>& out_grads, const std::vector<std::vector<bool>>& stop_gradients){

    PADDLE_ENFORCE_EQ(
      inputs_.size(),
      3,
      platform::errors::InvalidArgument("clip op's inputs size should be 3, but now is %d.", inputs_.size()));
    PADDLE_ENFORCE_EQ(
      outputs.size(),
      1,
      platform::errors::InvalidArgument("clip op's outputs size should be 1, but now is %d.", outputs.size()));

    VLOG(6) << "Prepare inputs of clip_grad";

    Tensor x(std::make_shared<primitive::LazyTensor>(inputs_[0][0]));
    Tensor out_grad(std::make_shared<primitive::LazyTensor>(out_grads[0][0]));

    VLOG(6) << "Vjp prepare Prepare attributes of clip_grad";

    Tensor min(std::make_shared<primitive::LazyTensor>(inputs_[1][0]));
    Tensor max(std::make_shared<primitive::LazyTensor>(inputs_[2][0]));

    VLOG(6) << "Vjp prepare call clip's vjp inteface";

    std::vector<std::vector<Tensor>> tensor_res =
        primitive::clip_vjp(
        x, out_grad, min, max, stop_gradients);

    VLOG(6) << "Vjp prepare stop gradient of clip_grad";

    std::vector<std::vector<pir::OpResult>> res(tensor_res.size());
    for (size_t i = 0; i < tensor_res.size(); ++i) {
        res[i].resize(tensor_res[i].size());
        for (size_t j = 0; j < tensor_res[i].size(); ++j) {
            if(tensor_res[i][j].defined()){
                res[i][j] = std::static_pointer_cast<primitive::LazyTensor>(tensor_res[i][j].impl())->value().dyn_cast<pir::OpResult>();
            }
        }
    }
    return res;
}

std::vector<std::vector<pir::OpResult>> Clip_Op::Vjp(pir::Operation* op, const std::vector<std::vector<pir::Value>>& inputs_, const std::vector<std::vector<pir::Value>>& outputs, const std::vector<std::vector<pir::Value>>& out_grads, const std::vector<std::vector<bool>>& stop_gradients){

    PADDLE_ENFORCE_EQ(
      inputs_.size(),
      3,
      platform::errors::InvalidArgument("clip op's inputs size should be 3, but now is %d.", inputs_.size()));
    PADDLE_ENFORCE_EQ(
      outputs.size(),
      1,
      platform::errors::InvalidArgument("clip op's outputs size should be 1, but now is %d.", outputs.size()));

    VLOG(6) << "Prepare inputs of clip_grad";

    Tensor x(std::make_shared<primitive::LazyTensor>(inputs_[0][0]));
    Tensor out_grad(std::make_shared<primitive::LazyTensor>(out_grads[0][0]));

    VLOG(6) << "Vjp prepare Prepare attributes of clip_grad";

    Tensor min(std::make_shared<primitive::LazyTensor>(inputs_[1][0]));
    Tensor max(std::make_shared<primitive::LazyTensor>(inputs_[2][0]));

    VLOG(6) << "Vjp prepare call clip_'s vjp inteface";

    std::vector<std::vector<Tensor>> tensor_res =
        primitive::clip_vjp(
        x, out_grad, min, max, stop_gradients);

    VLOG(6) << "Vjp prepare stop gradient of clip_grad";

    std::vector<std::vector<pir::OpResult>> res(tensor_res.size());
    for (size_t i = 0; i < tensor_res.size(); ++i) {
        res[i].resize(tensor_res[i].size());
        for (size_t j = 0; j < tensor_res[i].size(); ++j) {
            if(tensor_res[i][j].defined()){
                res[i][j] = std::static_pointer_cast<primitive::LazyTensor>(tensor_res[i][j].impl())->value().dyn_cast<pir::OpResult>();
            }
        }
    }
    return res;
}

std::vector<std::vector<pir::OpResult>> ComplexOp::Vjp(pir::Operation* op, const std::vector<std::vector<pir::Value>>& inputs_, const std::vector<std::vector<pir::Value>>& outputs, const std::vector<std::vector<pir::Value>>& out_grads, const std::vector<std::vector<bool>>& stop_gradients){

    PADDLE_ENFORCE_EQ(
      inputs_.size(),
      2,
      platform::errors::InvalidArgument("complex op's inputs size should be 2, but now is %d.", inputs_.size()));
    PADDLE_ENFORCE_EQ(
      outputs.size(),
      1,
      platform::errors::InvalidArgument("complex op's outputs size should be 1, but now is %d.", outputs.size()));

    VLOG(6) << "Prepare inputs of complex_grad";

    Tensor real(std::make_shared<primitive::LazyTensor>(inputs_[0][0]));
    Tensor imag(std::make_shared<primitive::LazyTensor>(inputs_[1][0]));
    Tensor out_grad(std::make_shared<primitive::LazyTensor>(out_grads[0][0]));

    VLOG(6) << "Vjp prepare Prepare attributes of complex_grad";


    VLOG(6) << "Vjp prepare call complex's vjp inteface";

    std::vector<std::vector<Tensor>> tensor_res =
        primitive::complex_vjp(
        real, imag, out_grad, stop_gradients);

    VLOG(6) << "Vjp prepare stop gradient of complex_grad";

    std::vector<std::vector<pir::OpResult>> res(tensor_res.size());
    for (size_t i = 0; i < tensor_res.size(); ++i) {
        res[i].resize(tensor_res[i].size());
        for (size_t j = 0; j < tensor_res[i].size(); ++j) {
            if(tensor_res[i][j].defined()){
                res[i][j] = std::static_pointer_cast<primitive::LazyTensor>(tensor_res[i][j].impl())->value().dyn_cast<pir::OpResult>();
            }
        }
    }
    return res;
}

std::vector<std::vector<pir::OpResult>> ConcatOp::Vjp(pir::Operation* op, const std::vector<std::vector<pir::Value>>& inputs_, const std::vector<std::vector<pir::Value>>& outputs, const std::vector<std::vector<pir::Value>>& out_grads, const std::vector<std::vector<bool>>& stop_gradients){

    PADDLE_ENFORCE_EQ(
      inputs_.size(),
      2,
      platform::errors::InvalidArgument("concat op's inputs size should be 2, but now is %d.", inputs_.size()));
    PADDLE_ENFORCE_EQ(
      outputs.size(),
      1,
      platform::errors::InvalidArgument("concat op's outputs size should be 1, but now is %d.", outputs.size()));

    VLOG(6) << "Prepare inputs of concat_grad";

    std::vector<Tensor> x;
    for (size_t idx = 0; idx < inputs_[0].size(); idx++) {
        x.emplace_back(
            std::make_shared<primitive::LazyTensor>(inputs_[0][idx]));
    }
    Tensor out_grad(std::make_shared<primitive::LazyTensor>(out_grads[0][0]));

    VLOG(6) << "Vjp prepare Prepare attributes of concat_grad";

    Tensor axis(std::make_shared<primitive::LazyTensor>(inputs_[1][0]));

    VLOG(6) << "Vjp prepare call concat's vjp inteface";

    std::vector<std::vector<Tensor>> tensor_res =
        primitive::concat_vjp(
        x, out_grad, axis, stop_gradients);

    VLOG(6) << "Vjp prepare stop gradient of concat_grad";

    std::vector<std::vector<pir::OpResult>> res(tensor_res.size());
    for (size_t i = 0; i < tensor_res.size(); ++i) {
        res[i].resize(tensor_res[i].size());
        for (size_t j = 0; j < tensor_res[i].size(); ++j) {
            if(tensor_res[i][j].defined()){
                res[i][j] = std::static_pointer_cast<primitive::LazyTensor>(tensor_res[i][j].impl())->value().dyn_cast<pir::OpResult>();
            }
        }
    }
    return res;
}

std::vector<std::vector<pir::OpResult>> ConjOp::Vjp(pir::Operation* op, const std::vector<std::vector<pir::Value>>& inputs_, const std::vector<std::vector<pir::Value>>& outputs, const std::vector<std::vector<pir::Value>>& out_grads, const std::vector<std::vector<bool>>& stop_gradients){

    PADDLE_ENFORCE_EQ(
      inputs_.size(),
      1,
      platform::errors::InvalidArgument("conj op's inputs size should be 1, but now is %d.", inputs_.size()));
    PADDLE_ENFORCE_EQ(
      outputs.size(),
      1,
      platform::errors::InvalidArgument("conj op's outputs size should be 1, but now is %d.", outputs.size()));

    VLOG(6) << "Prepare inputs of conj_grad";

    Tensor out_grad(std::make_shared<primitive::LazyTensor>(out_grads[0][0]));

    VLOG(6) << "Vjp prepare Prepare attributes of conj_grad";


    VLOG(6) << "Vjp prepare call conj's vjp inteface";

    std::vector<std::vector<Tensor>> tensor_res =
        primitive::conj_vjp(
        out_grad, stop_gradients);

    VLOG(6) << "Vjp prepare stop gradient of conj_grad";

    std::vector<std::vector<pir::OpResult>> res(tensor_res.size());
    for (size_t i = 0; i < tensor_res.size(); ++i) {
        res[i].resize(tensor_res[i].size());
        for (size_t j = 0; j < tensor_res[i].size(); ++j) {
            if(tensor_res[i][j].defined()){
                res[i][j] = std::static_pointer_cast<primitive::LazyTensor>(tensor_res[i][j].impl())->value().dyn_cast<pir::OpResult>();
            }
        }
    }
    return res;
}

std::vector<std::vector<pir::OpResult>> Conv2dOp::Vjp(pir::Operation* op, const std::vector<std::vector<pir::Value>>& inputs_, const std::vector<std::vector<pir::Value>>& outputs, const std::vector<std::vector<pir::Value>>& out_grads, const std::vector<std::vector<bool>>& stop_gradients){

    PADDLE_ENFORCE_EQ(
      inputs_.size(),
      2,
      platform::errors::InvalidArgument("conv2d op's inputs size should be 2, but now is %d.", inputs_.size()));
    PADDLE_ENFORCE_EQ(
      outputs.size(),
      1,
      platform::errors::InvalidArgument("conv2d op's outputs size should be 1, but now is %d.", outputs.size()));

    VLOG(6) << "Prepare inputs of conv2d_grad";

    Tensor input(std::make_shared<primitive::LazyTensor>(inputs_[0][0]));
    Tensor filter(std::make_shared<primitive::LazyTensor>(inputs_[1][0]));
    Tensor out_grad(std::make_shared<primitive::LazyTensor>(out_grads[0][0]));

    VLOG(6) << "Vjp prepare Prepare attributes of conv2d_grad";

    std::vector<int> strides;
    for (size_t i = 0; i < op->attribute("strides").dyn_cast<pir::ArrayAttribute>().size(); i++) {
        strides.push_back(op->attribute("strides").dyn_cast<pir::ArrayAttribute>().at(i).dyn_cast<pir::Int32Attribute>().data());
    }
    std::vector<int> paddings;
    for (size_t i = 0; i < op->attribute("paddings").dyn_cast<pir::ArrayAttribute>().size(); i++) {
        paddings.push_back(op->attribute("paddings").dyn_cast<pir::ArrayAttribute>().at(i).dyn_cast<pir::Int32Attribute>().data());
    }
    std::string padding_algorithm = op->attribute("padding_algorithm").dyn_cast<pir::StrAttribute>().AsString();
    std::vector<int> dilations;
    for (size_t i = 0; i < op->attribute("dilations").dyn_cast<pir::ArrayAttribute>().size(); i++) {
        dilations.push_back(op->attribute("dilations").dyn_cast<pir::ArrayAttribute>().at(i).dyn_cast<pir::Int32Attribute>().data());
    }
    int groups = op->attribute("groups").dyn_cast<pir::Int32Attribute>().data();
    std::string data_format = op->attribute("data_format").dyn_cast<pir::StrAttribute>().AsString();

    VLOG(6) << "Vjp prepare call conv2d's vjp inteface";

    std::vector<std::vector<Tensor>> tensor_res =
        primitive::conv2d_vjp(
        input, filter, out_grad, strides, paddings, padding_algorithm, dilations, groups, data_format, stop_gradients);

    VLOG(6) << "Vjp prepare stop gradient of conv2d_grad";

    std::vector<std::vector<pir::OpResult>> res(tensor_res.size());
    for (size_t i = 0; i < tensor_res.size(); ++i) {
        res[i].resize(tensor_res[i].size());
        for (size_t j = 0; j < tensor_res[i].size(); ++j) {
            if(tensor_res[i][j].defined()){
                res[i][j] = std::static_pointer_cast<primitive::LazyTensor>(tensor_res[i][j].impl())->value().dyn_cast<pir::OpResult>();
            }
        }
    }
    return res;
}

std::vector<std::vector<pir::OpResult>> Conv3dOp::Vjp(pir::Operation* op, const std::vector<std::vector<pir::Value>>& inputs_, const std::vector<std::vector<pir::Value>>& outputs, const std::vector<std::vector<pir::Value>>& out_grads, const std::vector<std::vector<bool>>& stop_gradients){

    PADDLE_ENFORCE_EQ(
      inputs_.size(),
      2,
      platform::errors::InvalidArgument("conv3d op's inputs size should be 2, but now is %d.", inputs_.size()));
    PADDLE_ENFORCE_EQ(
      outputs.size(),
      1,
      platform::errors::InvalidArgument("conv3d op's outputs size should be 1, but now is %d.", outputs.size()));

    VLOG(6) << "Prepare inputs of conv3d_grad";

    Tensor input(std::make_shared<primitive::LazyTensor>(inputs_[0][0]));
    Tensor filter(std::make_shared<primitive::LazyTensor>(inputs_[1][0]));
    Tensor out_grad(std::make_shared<primitive::LazyTensor>(out_grads[0][0]));

    VLOG(6) << "Vjp prepare Prepare attributes of conv3d_grad";

    std::vector<int> strides;
    for (size_t i = 0; i < op->attribute("strides").dyn_cast<pir::ArrayAttribute>().size(); i++) {
        strides.push_back(op->attribute("strides").dyn_cast<pir::ArrayAttribute>().at(i).dyn_cast<pir::Int32Attribute>().data());
    }
    std::vector<int> paddings;
    for (size_t i = 0; i < op->attribute("paddings").dyn_cast<pir::ArrayAttribute>().size(); i++) {
        paddings.push_back(op->attribute("paddings").dyn_cast<pir::ArrayAttribute>().at(i).dyn_cast<pir::Int32Attribute>().data());
    }
    std::string padding_algorithm = op->attribute("padding_algorithm").dyn_cast<pir::StrAttribute>().AsString();
    int groups = op->attribute("groups").dyn_cast<pir::Int32Attribute>().data();
    std::vector<int> dilations;
    for (size_t i = 0; i < op->attribute("dilations").dyn_cast<pir::ArrayAttribute>().size(); i++) {
        dilations.push_back(op->attribute("dilations").dyn_cast<pir::ArrayAttribute>().at(i).dyn_cast<pir::Int32Attribute>().data());
    }
    std::string data_format = op->attribute("data_format").dyn_cast<pir::StrAttribute>().AsString();

    VLOG(6) << "Vjp prepare call conv3d's vjp inteface";

    std::vector<std::vector<Tensor>> tensor_res =
        primitive::conv3d_vjp(
        input, filter, out_grad, strides, paddings, padding_algorithm, groups, dilations, data_format, stop_gradients);

    VLOG(6) << "Vjp prepare stop gradient of conv3d_grad";

    std::vector<std::vector<pir::OpResult>> res(tensor_res.size());
    for (size_t i = 0; i < tensor_res.size(); ++i) {
        res[i].resize(tensor_res[i].size());
        for (size_t j = 0; j < tensor_res[i].size(); ++j) {
            if(tensor_res[i][j].defined()){
                res[i][j] = std::static_pointer_cast<primitive::LazyTensor>(tensor_res[i][j].impl())->value().dyn_cast<pir::OpResult>();
            }
        }
    }
    return res;
}

std::vector<std::vector<pir::OpResult>> Conv3dTransposeOp::Vjp(pir::Operation* op, const std::vector<std::vector<pir::Value>>& inputs_, const std::vector<std::vector<pir::Value>>& outputs, const std::vector<std::vector<pir::Value>>& out_grads, const std::vector<std::vector<bool>>& stop_gradients){

    PADDLE_ENFORCE_EQ(
      inputs_.size(),
      2,
      platform::errors::InvalidArgument("conv3d_transpose op's inputs size should be 2, but now is %d.", inputs_.size()));
    PADDLE_ENFORCE_EQ(
      outputs.size(),
      1,
      platform::errors::InvalidArgument("conv3d_transpose op's outputs size should be 1, but now is %d.", outputs.size()));

    VLOG(6) << "Prepare inputs of conv3d_transpose_grad";

    Tensor x(std::make_shared<primitive::LazyTensor>(inputs_[0][0]));
    Tensor filter(std::make_shared<primitive::LazyTensor>(inputs_[1][0]));
    Tensor out_grad(std::make_shared<primitive::LazyTensor>(out_grads[0][0]));

    VLOG(6) << "Vjp prepare Prepare attributes of conv3d_transpose_grad";

    std::vector<int> strides;
    for (size_t i = 0; i < op->attribute("strides").dyn_cast<pir::ArrayAttribute>().size(); i++) {
        strides.push_back(op->attribute("strides").dyn_cast<pir::ArrayAttribute>().at(i).dyn_cast<pir::Int32Attribute>().data());
    }
    std::vector<int> paddings;
    for (size_t i = 0; i < op->attribute("paddings").dyn_cast<pir::ArrayAttribute>().size(); i++) {
        paddings.push_back(op->attribute("paddings").dyn_cast<pir::ArrayAttribute>().at(i).dyn_cast<pir::Int32Attribute>().data());
    }
    std::vector<int> output_padding;
    for (size_t i = 0; i < op->attribute("output_padding").dyn_cast<pir::ArrayAttribute>().size(); i++) {
        output_padding.push_back(op->attribute("output_padding").dyn_cast<pir::ArrayAttribute>().at(i).dyn_cast<pir::Int32Attribute>().data());
    }
    std::vector<int> output_size;
    for (size_t i = 0; i < op->attribute("output_size").dyn_cast<pir::ArrayAttribute>().size(); i++) {
        output_size.push_back(op->attribute("output_size").dyn_cast<pir::ArrayAttribute>().at(i).dyn_cast<pir::Int32Attribute>().data());
    }
    std::string padding_algorithm = op->attribute("padding_algorithm").dyn_cast<pir::StrAttribute>().AsString();
    int groups = op->attribute("groups").dyn_cast<pir::Int32Attribute>().data();
    std::vector<int> dilations;
    for (size_t i = 0; i < op->attribute("dilations").dyn_cast<pir::ArrayAttribute>().size(); i++) {
        dilations.push_back(op->attribute("dilations").dyn_cast<pir::ArrayAttribute>().at(i).dyn_cast<pir::Int32Attribute>().data());
    }
    std::string data_format = op->attribute("data_format").dyn_cast<pir::StrAttribute>().AsString();

    VLOG(6) << "Vjp prepare call conv3d_transpose's vjp inteface";

    std::vector<std::vector<Tensor>> tensor_res =
        primitive::conv3d_transpose_vjp(
        x, filter, out_grad, strides, paddings, output_padding, output_size, padding_algorithm, groups, dilations, data_format, stop_gradients);

    VLOG(6) << "Vjp prepare stop gradient of conv3d_transpose_grad";

    std::vector<std::vector<pir::OpResult>> res(tensor_res.size());
    for (size_t i = 0; i < tensor_res.size(); ++i) {
        res[i].resize(tensor_res[i].size());
        for (size_t j = 0; j < tensor_res[i].size(); ++j) {
            if(tensor_res[i][j].defined()){
                res[i][j] = std::static_pointer_cast<primitive::LazyTensor>(tensor_res[i][j].impl())->value().dyn_cast<pir::OpResult>();
            }
        }
    }
    return res;
}

std::vector<std::vector<pir::OpResult>> CosOp::Vjp(pir::Operation* op, const std::vector<std::vector<pir::Value>>& inputs_, const std::vector<std::vector<pir::Value>>& outputs, const std::vector<std::vector<pir::Value>>& out_grads, const std::vector<std::vector<bool>>& stop_gradients){

    PADDLE_ENFORCE_EQ(
      inputs_.size(),
      1,
      platform::errors::InvalidArgument("cos op's inputs size should be 1, but now is %d.", inputs_.size()));
    PADDLE_ENFORCE_EQ(
      outputs.size(),
      1,
      platform::errors::InvalidArgument("cos op's outputs size should be 1, but now is %d.", outputs.size()));

    VLOG(6) << "Prepare inputs of cos_grad";

    Tensor x(std::make_shared<primitive::LazyTensor>(inputs_[0][0]));
    Tensor out_grad(std::make_shared<primitive::LazyTensor>(out_grads[0][0]));

    VLOG(6) << "Vjp prepare Prepare attributes of cos_grad";


    VLOG(6) << "Vjp prepare call cos's vjp inteface";

    std::vector<std::vector<Tensor>> tensor_res =
        primitive::cos_vjp(
        x, out_grad, stop_gradients);

    VLOG(6) << "Vjp prepare stop gradient of cos_grad";

    std::vector<std::vector<pir::OpResult>> res(tensor_res.size());
    for (size_t i = 0; i < tensor_res.size(); ++i) {
        res[i].resize(tensor_res[i].size());
        for (size_t j = 0; j < tensor_res[i].size(); ++j) {
            if(tensor_res[i][j].defined()){
                res[i][j] = std::static_pointer_cast<primitive::LazyTensor>(tensor_res[i][j].impl())->value().dyn_cast<pir::OpResult>();
            }
        }
    }
    return res;
}

std::vector<std::vector<pir::OpResult>> Cos_Op::Vjp(pir::Operation* op, const std::vector<std::vector<pir::Value>>& inputs_, const std::vector<std::vector<pir::Value>>& outputs, const std::vector<std::vector<pir::Value>>& out_grads, const std::vector<std::vector<bool>>& stop_gradients){

    PADDLE_ENFORCE_EQ(
      inputs_.size(),
      1,
      platform::errors::InvalidArgument("cos op's inputs size should be 1, but now is %d.", inputs_.size()));
    PADDLE_ENFORCE_EQ(
      outputs.size(),
      1,
      platform::errors::InvalidArgument("cos op's outputs size should be 1, but now is %d.", outputs.size()));

    VLOG(6) << "Prepare inputs of cos_grad";

    Tensor x(std::make_shared<primitive::LazyTensor>(inputs_[0][0]));
    Tensor out_grad(std::make_shared<primitive::LazyTensor>(out_grads[0][0]));

    VLOG(6) << "Vjp prepare Prepare attributes of cos_grad";


    VLOG(6) << "Vjp prepare call cos_'s vjp inteface";

    std::vector<std::vector<Tensor>> tensor_res =
        primitive::cos_vjp(
        x, out_grad, stop_gradients);

    VLOG(6) << "Vjp prepare stop gradient of cos_grad";

    std::vector<std::vector<pir::OpResult>> res(tensor_res.size());
    for (size_t i = 0; i < tensor_res.size(); ++i) {
        res[i].resize(tensor_res[i].size());
        for (size_t j = 0; j < tensor_res[i].size(); ++j) {
            if(tensor_res[i][j].defined()){
                res[i][j] = std::static_pointer_cast<primitive::LazyTensor>(tensor_res[i][j].impl())->value().dyn_cast<pir::OpResult>();
            }
        }
    }
    return res;
}

std::vector<std::vector<pir::OpResult>> CoshOp::Vjp(pir::Operation* op, const std::vector<std::vector<pir::Value>>& inputs_, const std::vector<std::vector<pir::Value>>& outputs, const std::vector<std::vector<pir::Value>>& out_grads, const std::vector<std::vector<bool>>& stop_gradients){

    PADDLE_ENFORCE_EQ(
      inputs_.size(),
      1,
      platform::errors::InvalidArgument("cosh op's inputs size should be 1, but now is %d.", inputs_.size()));
    PADDLE_ENFORCE_EQ(
      outputs.size(),
      1,
      platform::errors::InvalidArgument("cosh op's outputs size should be 1, but now is %d.", outputs.size()));

    VLOG(6) << "Prepare inputs of cosh_grad";

    Tensor x(std::make_shared<primitive::LazyTensor>(inputs_[0][0]));
    Tensor out_grad(std::make_shared<primitive::LazyTensor>(out_grads[0][0]));

    VLOG(6) << "Vjp prepare Prepare attributes of cosh_grad";


    VLOG(6) << "Vjp prepare call cosh's vjp inteface";

    std::vector<std::vector<Tensor>> tensor_res =
        primitive::cosh_vjp(
        x, out_grad, stop_gradients);

    VLOG(6) << "Vjp prepare stop gradient of cosh_grad";

    std::vector<std::vector<pir::OpResult>> res(tensor_res.size());
    for (size_t i = 0; i < tensor_res.size(); ++i) {
        res[i].resize(tensor_res[i].size());
        for (size_t j = 0; j < tensor_res[i].size(); ++j) {
            if(tensor_res[i][j].defined()){
                res[i][j] = std::static_pointer_cast<primitive::LazyTensor>(tensor_res[i][j].impl())->value().dyn_cast<pir::OpResult>();
            }
        }
    }
    return res;
}

std::vector<std::vector<pir::OpResult>> Cosh_Op::Vjp(pir::Operation* op, const std::vector<std::vector<pir::Value>>& inputs_, const std::vector<std::vector<pir::Value>>& outputs, const std::vector<std::vector<pir::Value>>& out_grads, const std::vector<std::vector<bool>>& stop_gradients){

    PADDLE_ENFORCE_EQ(
      inputs_.size(),
      1,
      platform::errors::InvalidArgument("cosh op's inputs size should be 1, but now is %d.", inputs_.size()));
    PADDLE_ENFORCE_EQ(
      outputs.size(),
      1,
      platform::errors::InvalidArgument("cosh op's outputs size should be 1, but now is %d.", outputs.size()));

    VLOG(6) << "Prepare inputs of cosh_grad";

    Tensor x(std::make_shared<primitive::LazyTensor>(inputs_[0][0]));
    Tensor out_grad(std::make_shared<primitive::LazyTensor>(out_grads[0][0]));

    VLOG(6) << "Vjp prepare Prepare attributes of cosh_grad";


    VLOG(6) << "Vjp prepare call cosh_'s vjp inteface";

    std::vector<std::vector<Tensor>> tensor_res =
        primitive::cosh_vjp(
        x, out_grad, stop_gradients);

    VLOG(6) << "Vjp prepare stop gradient of cosh_grad";

    std::vector<std::vector<pir::OpResult>> res(tensor_res.size());
    for (size_t i = 0; i < tensor_res.size(); ++i) {
        res[i].resize(tensor_res[i].size());
        for (size_t j = 0; j < tensor_res[i].size(); ++j) {
            if(tensor_res[i][j].defined()){
                res[i][j] = std::static_pointer_cast<primitive::LazyTensor>(tensor_res[i][j].impl())->value().dyn_cast<pir::OpResult>();
            }
        }
    }
    return res;
}

std::vector<std::vector<pir::OpResult>> CropOp::Vjp(pir::Operation* op, const std::vector<std::vector<pir::Value>>& inputs_, const std::vector<std::vector<pir::Value>>& outputs, const std::vector<std::vector<pir::Value>>& out_grads, const std::vector<std::vector<bool>>& stop_gradients){

    PADDLE_ENFORCE_EQ(
      inputs_.size(),
      3,
      platform::errors::InvalidArgument("crop op's inputs size should be 3, but now is %d.", inputs_.size()));
    PADDLE_ENFORCE_EQ(
      outputs.size(),
      1,
      platform::errors::InvalidArgument("crop op's outputs size should be 1, but now is %d.", outputs.size()));

    VLOG(6) << "Prepare inputs of crop_grad";

    Tensor x(std::make_shared<primitive::LazyTensor>(inputs_[0][0]));
    Tensor out_grad(std::make_shared<primitive::LazyTensor>(out_grads[0][0]));

    VLOG(6) << "Vjp prepare Prepare attributes of crop_grad";

    Tensor offsets(std::make_shared<primitive::LazyTensor>(inputs_[2][0]));

    VLOG(6) << "Vjp prepare call crop's vjp inteface";

    std::vector<std::vector<Tensor>> tensor_res =
        primitive::crop_vjp(
        x, out_grad, offsets, stop_gradients);

    VLOG(6) << "Vjp prepare stop gradient of crop_grad";

    std::vector<std::vector<pir::OpResult>> res(tensor_res.size());
    for (size_t i = 0; i < tensor_res.size(); ++i) {
        res[i].resize(tensor_res[i].size());
        for (size_t j = 0; j < tensor_res[i].size(); ++j) {
            if(tensor_res[i][j].defined()){
                res[i][j] = std::static_pointer_cast<primitive::LazyTensor>(tensor_res[i][j].impl())->value().dyn_cast<pir::OpResult>();
            }
        }
    }
    return res;
}

std::vector<std::vector<pir::OpResult>> CrossOp::Vjp(pir::Operation* op, const std::vector<std::vector<pir::Value>>& inputs_, const std::vector<std::vector<pir::Value>>& outputs, const std::vector<std::vector<pir::Value>>& out_grads, const std::vector<std::vector<bool>>& stop_gradients){

    PADDLE_ENFORCE_EQ(
      inputs_.size(),
      2,
      platform::errors::InvalidArgument("cross op's inputs size should be 2, but now is %d.", inputs_.size()));
    PADDLE_ENFORCE_EQ(
      outputs.size(),
      1,
      platform::errors::InvalidArgument("cross op's outputs size should be 1, but now is %d.", outputs.size()));

    VLOG(6) << "Prepare inputs of cross_grad";

    Tensor x(std::make_shared<primitive::LazyTensor>(inputs_[0][0]));
    Tensor y(std::make_shared<primitive::LazyTensor>(inputs_[1][0]));
    Tensor out_grad(std::make_shared<primitive::LazyTensor>(out_grads[0][0]));

    VLOG(6) << "Vjp prepare Prepare attributes of cross_grad";

    int axis = op->attribute("axis").dyn_cast<pir::Int32Attribute>().data();

    VLOG(6) << "Vjp prepare call cross's vjp inteface";

    std::vector<std::vector<Tensor>> tensor_res =
        primitive::cross_vjp(
        x, y, out_grad, axis, stop_gradients);

    VLOG(6) << "Vjp prepare stop gradient of cross_grad";

    std::vector<std::vector<pir::OpResult>> res(tensor_res.size());
    for (size_t i = 0; i < tensor_res.size(); ++i) {
        res[i].resize(tensor_res[i].size());
        for (size_t j = 0; j < tensor_res[i].size(); ++j) {
            if(tensor_res[i][j].defined()){
                res[i][j] = std::static_pointer_cast<primitive::LazyTensor>(tensor_res[i][j].impl())->value().dyn_cast<pir::OpResult>();
            }
        }
    }
    return res;
}

std::vector<std::vector<pir::OpResult>> CrossEntropyWithSoftmaxOp::Vjp(pir::Operation* op, const std::vector<std::vector<pir::Value>>& inputs_, const std::vector<std::vector<pir::Value>>& outputs, const std::vector<std::vector<pir::Value>>& out_grads, const std::vector<std::vector<bool>>& stop_gradients){

    PADDLE_ENFORCE_EQ(
      inputs_.size(),
      2,
      platform::errors::InvalidArgument("cross_entropy_with_softmax op's inputs size should be 2, but now is %d.", inputs_.size()));
    PADDLE_ENFORCE_EQ(
      outputs.size(),
      2,
      platform::errors::InvalidArgument("cross_entropy_with_softmax op's outputs size should be 2, but now is %d.", outputs.size()));

    VLOG(6) << "Prepare inputs of cross_entropy_with_softmax_grad";

    Tensor label(std::make_shared<primitive::LazyTensor>(inputs_[1][0]));
    Tensor softmax(std::make_shared<primitive::LazyTensor>(outputs[0][0]));
    Tensor loss_grad(std::make_shared<primitive::LazyTensor>(out_grads[1][0]));

    VLOG(6) << "Vjp prepare Prepare attributes of cross_entropy_with_softmax_grad";

    bool soft_label = op->attribute("soft_label").dyn_cast<pir::BoolAttribute>().data();
    bool use_softmax = op->attribute("use_softmax").dyn_cast<pir::BoolAttribute>().data();
    bool numeric_stable_mode = op->attribute("numeric_stable_mode").dyn_cast<pir::BoolAttribute>().data();
    int ignore_index = op->attribute("ignore_index").dyn_cast<pir::Int32Attribute>().data();
    int axis = op->attribute("axis").dyn_cast<pir::Int32Attribute>().data();

    VLOG(6) << "Vjp prepare call cross_entropy_with_softmax's vjp inteface";

    std::vector<std::vector<Tensor>> tensor_res =
        primitive::cross_entropy_with_softmax_vjp(
        label, softmax, loss_grad, soft_label, use_softmax, numeric_stable_mode, ignore_index, axis, stop_gradients);

    VLOG(6) << "Vjp prepare stop gradient of cross_entropy_with_softmax_grad";

    std::vector<std::vector<pir::OpResult>> res(tensor_res.size());
    for (size_t i = 0; i < tensor_res.size(); ++i) {
        res[i].resize(tensor_res[i].size());
        for (size_t j = 0; j < tensor_res[i].size(); ++j) {
            if(tensor_res[i][j].defined()){
                res[i][j] = std::static_pointer_cast<primitive::LazyTensor>(tensor_res[i][j].impl())->value().dyn_cast<pir::OpResult>();
            }
        }
    }
    return res;
}

std::vector<std::vector<pir::OpResult>> CrossEntropyWithSoftmax_Op::Vjp(pir::Operation* op, const std::vector<std::vector<pir::Value>>& inputs_, const std::vector<std::vector<pir::Value>>& outputs, const std::vector<std::vector<pir::Value>>& out_grads, const std::vector<std::vector<bool>>& stop_gradients){

    PADDLE_ENFORCE_EQ(
      inputs_.size(),
      2,
      platform::errors::InvalidArgument("cross_entropy_with_softmax op's inputs size should be 2, but now is %d.", inputs_.size()));
    PADDLE_ENFORCE_EQ(
      outputs.size(),
      2,
      platform::errors::InvalidArgument("cross_entropy_with_softmax op's outputs size should be 2, but now is %d.", outputs.size()));

    VLOG(6) << "Prepare inputs of cross_entropy_with_softmax_grad";

    Tensor label(std::make_shared<primitive::LazyTensor>(inputs_[1][0]));
    Tensor softmax(std::make_shared<primitive::LazyTensor>(outputs[0][0]));
    Tensor loss_grad(std::make_shared<primitive::LazyTensor>(out_grads[1][0]));

    VLOG(6) << "Vjp prepare Prepare attributes of cross_entropy_with_softmax_grad";

    bool soft_label = op->attribute("soft_label").dyn_cast<pir::BoolAttribute>().data();
    bool use_softmax = op->attribute("use_softmax").dyn_cast<pir::BoolAttribute>().data();
    bool numeric_stable_mode = op->attribute("numeric_stable_mode").dyn_cast<pir::BoolAttribute>().data();
    int ignore_index = op->attribute("ignore_index").dyn_cast<pir::Int32Attribute>().data();
    int axis = op->attribute("axis").dyn_cast<pir::Int32Attribute>().data();

    VLOG(6) << "Vjp prepare call cross_entropy_with_softmax_'s vjp inteface";

    std::vector<std::vector<Tensor>> tensor_res =
        primitive::cross_entropy_with_softmax_vjp(
        label, softmax, loss_grad, soft_label, use_softmax, numeric_stable_mode, ignore_index, axis, stop_gradients);

    VLOG(6) << "Vjp prepare stop gradient of cross_entropy_with_softmax_grad";

    std::vector<std::vector<pir::OpResult>> res(tensor_res.size());
    for (size_t i = 0; i < tensor_res.size(); ++i) {
        res[i].resize(tensor_res[i].size());
        for (size_t j = 0; j < tensor_res[i].size(); ++j) {
            if(tensor_res[i][j].defined()){
                res[i][j] = std::static_pointer_cast<primitive::LazyTensor>(tensor_res[i][j].impl())->value().dyn_cast<pir::OpResult>();
            }
        }
    }
    return res;
}

std::vector<std::vector<pir::OpResult>> CummaxOp::Vjp(pir::Operation* op, const std::vector<std::vector<pir::Value>>& inputs_, const std::vector<std::vector<pir::Value>>& outputs, const std::vector<std::vector<pir::Value>>& out_grads, const std::vector<std::vector<bool>>& stop_gradients){

    PADDLE_ENFORCE_EQ(
      inputs_.size(),
      1,
      platform::errors::InvalidArgument("cummax op's inputs size should be 1, but now is %d.", inputs_.size()));
    PADDLE_ENFORCE_EQ(
      outputs.size(),
      2,
      platform::errors::InvalidArgument("cummax op's outputs size should be 2, but now is %d.", outputs.size()));

    VLOG(6) << "Prepare inputs of cummax_grad";

    Tensor x(std::make_shared<primitive::LazyTensor>(inputs_[0][0]));
    Tensor indices(std::make_shared<primitive::LazyTensor>(outputs[1][0]));
    Tensor out_grad(std::make_shared<primitive::LazyTensor>(out_grads[0][0]));

    VLOG(6) << "Vjp prepare Prepare attributes of cummax_grad";

    int axis = op->attribute("axis").dyn_cast<pir::Int32Attribute>().data();
    phi::DataType dtype = op->attribute("dtype").dyn_cast<paddle::dialect::DataTypeAttribute>().data();

    VLOG(6) << "Vjp prepare call cummax's vjp inteface";

    std::vector<std::vector<Tensor>> tensor_res =
        primitive::cummax_vjp(
        x, indices, out_grad, axis, dtype, stop_gradients);

    VLOG(6) << "Vjp prepare stop gradient of cummax_grad";

    std::vector<std::vector<pir::OpResult>> res(tensor_res.size());
    for (size_t i = 0; i < tensor_res.size(); ++i) {
        res[i].resize(tensor_res[i].size());
        for (size_t j = 0; j < tensor_res[i].size(); ++j) {
            if(tensor_res[i][j].defined()){
                res[i][j] = std::static_pointer_cast<primitive::LazyTensor>(tensor_res[i][j].impl())->value().dyn_cast<pir::OpResult>();
            }
        }
    }
    return res;
}

std::vector<std::vector<pir::OpResult>> CumminOp::Vjp(pir::Operation* op, const std::vector<std::vector<pir::Value>>& inputs_, const std::vector<std::vector<pir::Value>>& outputs, const std::vector<std::vector<pir::Value>>& out_grads, const std::vector<std::vector<bool>>& stop_gradients){

    PADDLE_ENFORCE_EQ(
      inputs_.size(),
      1,
      platform::errors::InvalidArgument("cummin op's inputs size should be 1, but now is %d.", inputs_.size()));
    PADDLE_ENFORCE_EQ(
      outputs.size(),
      2,
      platform::errors::InvalidArgument("cummin op's outputs size should be 2, but now is %d.", outputs.size()));

    VLOG(6) << "Prepare inputs of cummin_grad";

    Tensor x(std::make_shared<primitive::LazyTensor>(inputs_[0][0]));
    Tensor indices(std::make_shared<primitive::LazyTensor>(outputs[1][0]));
    Tensor out_grad(std::make_shared<primitive::LazyTensor>(out_grads[0][0]));

    VLOG(6) << "Vjp prepare Prepare attributes of cummin_grad";

    int axis = op->attribute("axis").dyn_cast<pir::Int32Attribute>().data();
    phi::DataType dtype = op->attribute("dtype").dyn_cast<paddle::dialect::DataTypeAttribute>().data();

    VLOG(6) << "Vjp prepare call cummin's vjp inteface";

    std::vector<std::vector<Tensor>> tensor_res =
        primitive::cummin_vjp(
        x, indices, out_grad, axis, dtype, stop_gradients);

    VLOG(6) << "Vjp prepare stop gradient of cummin_grad";

    std::vector<std::vector<pir::OpResult>> res(tensor_res.size());
    for (size_t i = 0; i < tensor_res.size(); ++i) {
        res[i].resize(tensor_res[i].size());
        for (size_t j = 0; j < tensor_res[i].size(); ++j) {
            if(tensor_res[i][j].defined()){
                res[i][j] = std::static_pointer_cast<primitive::LazyTensor>(tensor_res[i][j].impl())->value().dyn_cast<pir::OpResult>();
            }
        }
    }
    return res;
}

std::vector<std::vector<pir::OpResult>> CumprodOp::Vjp(pir::Operation* op, const std::vector<std::vector<pir::Value>>& inputs_, const std::vector<std::vector<pir::Value>>& outputs, const std::vector<std::vector<pir::Value>>& out_grads, const std::vector<std::vector<bool>>& stop_gradients){

    PADDLE_ENFORCE_EQ(
      inputs_.size(),
      1,
      platform::errors::InvalidArgument("cumprod op's inputs size should be 1, but now is %d.", inputs_.size()));
    PADDLE_ENFORCE_EQ(
      outputs.size(),
      1,
      platform::errors::InvalidArgument("cumprod op's outputs size should be 1, but now is %d.", outputs.size()));

    VLOG(6) << "Prepare inputs of cumprod_grad";

    Tensor x(std::make_shared<primitive::LazyTensor>(inputs_[0][0]));
    Tensor out(std::make_shared<primitive::LazyTensor>(outputs[0][0]));
    Tensor out_grad(std::make_shared<primitive::LazyTensor>(out_grads[0][0]));

    VLOG(6) << "Vjp prepare Prepare attributes of cumprod_grad";

    int dim = op->attribute("dim").dyn_cast<pir::Int32Attribute>().data();

    VLOG(6) << "Vjp prepare call cumprod's vjp inteface";

    std::vector<std::vector<Tensor>> tensor_res =
        primitive::cumprod_vjp(
        x, out, out_grad, dim, stop_gradients);

    VLOG(6) << "Vjp prepare stop gradient of cumprod_grad";

    std::vector<std::vector<pir::OpResult>> res(tensor_res.size());
    for (size_t i = 0; i < tensor_res.size(); ++i) {
        res[i].resize(tensor_res[i].size());
        for (size_t j = 0; j < tensor_res[i].size(); ++j) {
            if(tensor_res[i][j].defined()){
                res[i][j] = std::static_pointer_cast<primitive::LazyTensor>(tensor_res[i][j].impl())->value().dyn_cast<pir::OpResult>();
            }
        }
    }
    return res;
}

std::vector<std::vector<pir::OpResult>> Cumprod_Op::Vjp(pir::Operation* op, const std::vector<std::vector<pir::Value>>& inputs_, const std::vector<std::vector<pir::Value>>& outputs, const std::vector<std::vector<pir::Value>>& out_grads, const std::vector<std::vector<bool>>& stop_gradients){

    PADDLE_ENFORCE_EQ(
      inputs_.size(),
      1,
      platform::errors::InvalidArgument("cumprod op's inputs size should be 1, but now is %d.", inputs_.size()));
    PADDLE_ENFORCE_EQ(
      outputs.size(),
      1,
      platform::errors::InvalidArgument("cumprod op's outputs size should be 1, but now is %d.", outputs.size()));

    VLOG(6) << "Prepare inputs of cumprod_grad";

    Tensor x(std::make_shared<primitive::LazyTensor>(inputs_[0][0]));
    Tensor out(std::make_shared<primitive::LazyTensor>(outputs[0][0]));
    Tensor out_grad(std::make_shared<primitive::LazyTensor>(out_grads[0][0]));

    VLOG(6) << "Vjp prepare Prepare attributes of cumprod_grad";

    int dim = op->attribute("dim").dyn_cast<pir::Int32Attribute>().data();

    VLOG(6) << "Vjp prepare call cumprod_'s vjp inteface";

    std::vector<std::vector<Tensor>> tensor_res =
        primitive::cumprod_vjp(
        x, out, out_grad, dim, stop_gradients);

    VLOG(6) << "Vjp prepare stop gradient of cumprod_grad";

    std::vector<std::vector<pir::OpResult>> res(tensor_res.size());
    for (size_t i = 0; i < tensor_res.size(); ++i) {
        res[i].resize(tensor_res[i].size());
        for (size_t j = 0; j < tensor_res[i].size(); ++j) {
            if(tensor_res[i][j].defined()){
                res[i][j] = std::static_pointer_cast<primitive::LazyTensor>(tensor_res[i][j].impl())->value().dyn_cast<pir::OpResult>();
            }
        }
    }
    return res;
}

std::vector<std::vector<pir::OpResult>> CumsumOp::Vjp(pir::Operation* op, const std::vector<std::vector<pir::Value>>& inputs_, const std::vector<std::vector<pir::Value>>& outputs, const std::vector<std::vector<pir::Value>>& out_grads, const std::vector<std::vector<bool>>& stop_gradients){

    PADDLE_ENFORCE_EQ(
      inputs_.size(),
      2,
      platform::errors::InvalidArgument("cumsum op's inputs size should be 2, but now is %d.", inputs_.size()));
    PADDLE_ENFORCE_EQ(
      outputs.size(),
      1,
      platform::errors::InvalidArgument("cumsum op's outputs size should be 1, but now is %d.", outputs.size()));

    VLOG(6) << "Prepare inputs of cumsum_grad";

    Tensor x(std::make_shared<primitive::LazyTensor>(inputs_[0][0]));
    Tensor out_grad(std::make_shared<primitive::LazyTensor>(out_grads[0][0]));

    VLOG(6) << "Vjp prepare Prepare attributes of cumsum_grad";

    Tensor axis(std::make_shared<primitive::LazyTensor>(inputs_[1][0]));
    bool flatten = op->attribute("flatten").dyn_cast<pir::BoolAttribute>().data();
    bool exclusive = op->attribute("exclusive").dyn_cast<pir::BoolAttribute>().data();
    bool reverse = op->attribute("reverse").dyn_cast<pir::BoolAttribute>().data();

    VLOG(6) << "Vjp prepare call cumsum's vjp inteface";

    std::vector<std::vector<Tensor>> tensor_res =
        primitive::cumsum_vjp(
        x, out_grad, axis, flatten, exclusive, reverse, stop_gradients);

    VLOG(6) << "Vjp prepare stop gradient of cumsum_grad";

    std::vector<std::vector<pir::OpResult>> res(tensor_res.size());
    for (size_t i = 0; i < tensor_res.size(); ++i) {
        res[i].resize(tensor_res[i].size());
        for (size_t j = 0; j < tensor_res[i].size(); ++j) {
            if(tensor_res[i][j].defined()){
                res[i][j] = std::static_pointer_cast<primitive::LazyTensor>(tensor_res[i][j].impl())->value().dyn_cast<pir::OpResult>();
            }
        }
    }
    return res;
}

std::vector<std::vector<pir::OpResult>> Cumsum_Op::Vjp(pir::Operation* op, const std::vector<std::vector<pir::Value>>& inputs_, const std::vector<std::vector<pir::Value>>& outputs, const std::vector<std::vector<pir::Value>>& out_grads, const std::vector<std::vector<bool>>& stop_gradients){

    PADDLE_ENFORCE_EQ(
      inputs_.size(),
      2,
      platform::errors::InvalidArgument("cumsum op's inputs size should be 2, but now is %d.", inputs_.size()));
    PADDLE_ENFORCE_EQ(
      outputs.size(),
      1,
      platform::errors::InvalidArgument("cumsum op's outputs size should be 1, but now is %d.", outputs.size()));

    VLOG(6) << "Prepare inputs of cumsum_grad";

    Tensor x(std::make_shared<primitive::LazyTensor>(inputs_[0][0]));
    Tensor out_grad(std::make_shared<primitive::LazyTensor>(out_grads[0][0]));

    VLOG(6) << "Vjp prepare Prepare attributes of cumsum_grad";

    Tensor axis(std::make_shared<primitive::LazyTensor>(inputs_[1][0]));
    bool flatten = op->attribute("flatten").dyn_cast<pir::BoolAttribute>().data();
    bool exclusive = op->attribute("exclusive").dyn_cast<pir::BoolAttribute>().data();
    bool reverse = op->attribute("reverse").dyn_cast<pir::BoolAttribute>().data();

    VLOG(6) << "Vjp prepare call cumsum_'s vjp inteface";

    std::vector<std::vector<Tensor>> tensor_res =
        primitive::cumsum_vjp(
        x, out_grad, axis, flatten, exclusive, reverse, stop_gradients);

    VLOG(6) << "Vjp prepare stop gradient of cumsum_grad";

    std::vector<std::vector<pir::OpResult>> res(tensor_res.size());
    for (size_t i = 0; i < tensor_res.size(); ++i) {
        res[i].resize(tensor_res[i].size());
        for (size_t j = 0; j < tensor_res[i].size(); ++j) {
            if(tensor_res[i][j].defined()){
                res[i][j] = std::static_pointer_cast<primitive::LazyTensor>(tensor_res[i][j].impl())->value().dyn_cast<pir::OpResult>();
            }
        }
    }
    return res;
}

std::vector<std::vector<pir::OpResult>> DepthwiseConv2dOp::Vjp(pir::Operation* op, const std::vector<std::vector<pir::Value>>& inputs_, const std::vector<std::vector<pir::Value>>& outputs, const std::vector<std::vector<pir::Value>>& out_grads, const std::vector<std::vector<bool>>& stop_gradients){

    PADDLE_ENFORCE_EQ(
      inputs_.size(),
      2,
      platform::errors::InvalidArgument("depthwise_conv2d op's inputs size should be 2, but now is %d.", inputs_.size()));
    PADDLE_ENFORCE_EQ(
      outputs.size(),
      1,
      platform::errors::InvalidArgument("depthwise_conv2d op's outputs size should be 1, but now is %d.", outputs.size()));

    VLOG(6) << "Prepare inputs of depthwise_conv2d_grad";

    Tensor input(std::make_shared<primitive::LazyTensor>(inputs_[0][0]));
    Tensor filter(std::make_shared<primitive::LazyTensor>(inputs_[1][0]));
    Tensor out_grad(std::make_shared<primitive::LazyTensor>(out_grads[0][0]));

    VLOG(6) << "Vjp prepare Prepare attributes of depthwise_conv2d_grad";

    std::vector<int> strides;
    for (size_t i = 0; i < op->attribute("strides").dyn_cast<pir::ArrayAttribute>().size(); i++) {
        strides.push_back(op->attribute("strides").dyn_cast<pir::ArrayAttribute>().at(i).dyn_cast<pir::Int32Attribute>().data());
    }
    std::vector<int> paddings;
    for (size_t i = 0; i < op->attribute("paddings").dyn_cast<pir::ArrayAttribute>().size(); i++) {
        paddings.push_back(op->attribute("paddings").dyn_cast<pir::ArrayAttribute>().at(i).dyn_cast<pir::Int32Attribute>().data());
    }
    std::string padding_algorithm = op->attribute("padding_algorithm").dyn_cast<pir::StrAttribute>().AsString();
    int groups = op->attribute("groups").dyn_cast<pir::Int32Attribute>().data();
    std::vector<int> dilations;
    for (size_t i = 0; i < op->attribute("dilations").dyn_cast<pir::ArrayAttribute>().size(); i++) {
        dilations.push_back(op->attribute("dilations").dyn_cast<pir::ArrayAttribute>().at(i).dyn_cast<pir::Int32Attribute>().data());
    }
    std::string data_format = op->attribute("data_format").dyn_cast<pir::StrAttribute>().AsString();

    VLOG(6) << "Vjp prepare call depthwise_conv2d's vjp inteface";

    std::vector<std::vector<Tensor>> tensor_res =
        primitive::depthwise_conv2d_vjp(
        input, filter, out_grad, strides, paddings, padding_algorithm, groups, dilations, data_format, stop_gradients);

    VLOG(6) << "Vjp prepare stop gradient of depthwise_conv2d_grad";

    std::vector<std::vector<pir::OpResult>> res(tensor_res.size());
    for (size_t i = 0; i < tensor_res.size(); ++i) {
        res[i].resize(tensor_res[i].size());
        for (size_t j = 0; j < tensor_res[i].size(); ++j) {
            if(tensor_res[i][j].defined()){
                res[i][j] = std::static_pointer_cast<primitive::LazyTensor>(tensor_res[i][j].impl())->value().dyn_cast<pir::OpResult>();
            }
        }
    }
    return res;
}

std::vector<std::vector<pir::OpResult>> DetOp::Vjp(pir::Operation* op, const std::vector<std::vector<pir::Value>>& inputs_, const std::vector<std::vector<pir::Value>>& outputs, const std::vector<std::vector<pir::Value>>& out_grads, const std::vector<std::vector<bool>>& stop_gradients){

    PADDLE_ENFORCE_EQ(
      inputs_.size(),
      1,
      platform::errors::InvalidArgument("det op's inputs size should be 1, but now is %d.", inputs_.size()));
    PADDLE_ENFORCE_EQ(
      outputs.size(),
      1,
      platform::errors::InvalidArgument("det op's outputs size should be 1, but now is %d.", outputs.size()));

    VLOG(6) << "Prepare inputs of det_grad";

    Tensor x(std::make_shared<primitive::LazyTensor>(inputs_[0][0]));
    Tensor out(std::make_shared<primitive::LazyTensor>(outputs[0][0]));
    Tensor out_grad(std::make_shared<primitive::LazyTensor>(out_grads[0][0]));

    VLOG(6) << "Vjp prepare Prepare attributes of det_grad";


    VLOG(6) << "Vjp prepare call det's vjp inteface";

    std::vector<std::vector<Tensor>> tensor_res =
        primitive::det_vjp(
        x, out, out_grad, stop_gradients);

    VLOG(6) << "Vjp prepare stop gradient of det_grad";

    std::vector<std::vector<pir::OpResult>> res(tensor_res.size());
    for (size_t i = 0; i < tensor_res.size(); ++i) {
        res[i].resize(tensor_res[i].size());
        for (size_t j = 0; j < tensor_res[i].size(); ++j) {
            if(tensor_res[i][j].defined()){
                res[i][j] = std::static_pointer_cast<primitive::LazyTensor>(tensor_res[i][j].impl())->value().dyn_cast<pir::OpResult>();
            }
        }
    }
    return res;
}

std::vector<std::vector<pir::OpResult>> DiagOp::Vjp(pir::Operation* op, const std::vector<std::vector<pir::Value>>& inputs_, const std::vector<std::vector<pir::Value>>& outputs, const std::vector<std::vector<pir::Value>>& out_grads, const std::vector<std::vector<bool>>& stop_gradients){

    PADDLE_ENFORCE_EQ(
      inputs_.size(),
      1,
      platform::errors::InvalidArgument("diag op's inputs size should be 1, but now is %d.", inputs_.size()));
    PADDLE_ENFORCE_EQ(
      outputs.size(),
      1,
      platform::errors::InvalidArgument("diag op's outputs size should be 1, but now is %d.", outputs.size()));

    VLOG(6) << "Prepare inputs of diag_grad";

    Tensor x(std::make_shared<primitive::LazyTensor>(inputs_[0][0]));
    Tensor out_grad(std::make_shared<primitive::LazyTensor>(out_grads[0][0]));

    VLOG(6) << "Vjp prepare Prepare attributes of diag_grad";

    int offset = op->attribute("offset").dyn_cast<pir::Int32Attribute>().data();

    VLOG(6) << "Vjp prepare call diag's vjp inteface";

    std::vector<std::vector<Tensor>> tensor_res =
        primitive::diag_vjp(
        x, out_grad, offset, stop_gradients);

    VLOG(6) << "Vjp prepare stop gradient of diag_grad";

    std::vector<std::vector<pir::OpResult>> res(tensor_res.size());
    for (size_t i = 0; i < tensor_res.size(); ++i) {
        res[i].resize(tensor_res[i].size());
        for (size_t j = 0; j < tensor_res[i].size(); ++j) {
            if(tensor_res[i][j].defined()){
                res[i][j] = std::static_pointer_cast<primitive::LazyTensor>(tensor_res[i][j].impl())->value().dyn_cast<pir::OpResult>();
            }
        }
    }
    return res;
}

std::vector<std::vector<pir::OpResult>> DiagonalOp::Vjp(pir::Operation* op, const std::vector<std::vector<pir::Value>>& inputs_, const std::vector<std::vector<pir::Value>>& outputs, const std::vector<std::vector<pir::Value>>& out_grads, const std::vector<std::vector<bool>>& stop_gradients){

    PADDLE_ENFORCE_EQ(
      inputs_.size(),
      1,
      platform::errors::InvalidArgument("diagonal op's inputs size should be 1, but now is %d.", inputs_.size()));
    PADDLE_ENFORCE_EQ(
      outputs.size(),
      1,
      platform::errors::InvalidArgument("diagonal op's outputs size should be 1, but now is %d.", outputs.size()));

    VLOG(6) << "Prepare inputs of diagonal_grad";

    Tensor x(std::make_shared<primitive::LazyTensor>(inputs_[0][0]));
    Tensor out_grad(std::make_shared<primitive::LazyTensor>(out_grads[0][0]));

    VLOG(6) << "Vjp prepare Prepare attributes of diagonal_grad";

    int offset = op->attribute("offset").dyn_cast<pir::Int32Attribute>().data();
    int axis1 = op->attribute("axis1").dyn_cast<pir::Int32Attribute>().data();
    int axis2 = op->attribute("axis2").dyn_cast<pir::Int32Attribute>().data();

    VLOG(6) << "Vjp prepare call diagonal's vjp inteface";

    std::vector<std::vector<Tensor>> tensor_res =
        primitive::diagonal_vjp(
        x, out_grad, offset, axis1, axis2, stop_gradients);

    VLOG(6) << "Vjp prepare stop gradient of diagonal_grad";

    std::vector<std::vector<pir::OpResult>> res(tensor_res.size());
    for (size_t i = 0; i < tensor_res.size(); ++i) {
        res[i].resize(tensor_res[i].size());
        for (size_t j = 0; j < tensor_res[i].size(); ++j) {
            if(tensor_res[i][j].defined()){
                res[i][j] = std::static_pointer_cast<primitive::LazyTensor>(tensor_res[i][j].impl())->value().dyn_cast<pir::OpResult>();
            }
        }
    }
    return res;
}

std::vector<std::vector<pir::OpResult>> DigammaOp::Vjp(pir::Operation* op, const std::vector<std::vector<pir::Value>>& inputs_, const std::vector<std::vector<pir::Value>>& outputs, const std::vector<std::vector<pir::Value>>& out_grads, const std::vector<std::vector<bool>>& stop_gradients){

    PADDLE_ENFORCE_EQ(
      inputs_.size(),
      1,
      platform::errors::InvalidArgument("digamma op's inputs size should be 1, but now is %d.", inputs_.size()));
    PADDLE_ENFORCE_EQ(
      outputs.size(),
      1,
      platform::errors::InvalidArgument("digamma op's outputs size should be 1, but now is %d.", outputs.size()));

    VLOG(6) << "Prepare inputs of digamma_grad";

    Tensor x(std::make_shared<primitive::LazyTensor>(inputs_[0][0]));
    Tensor out_grad(std::make_shared<primitive::LazyTensor>(out_grads[0][0]));

    VLOG(6) << "Vjp prepare Prepare attributes of digamma_grad";


    VLOG(6) << "Vjp prepare call digamma's vjp inteface";

    std::vector<std::vector<Tensor>> tensor_res =
        primitive::digamma_vjp(
        x, out_grad, stop_gradients);

    VLOG(6) << "Vjp prepare stop gradient of digamma_grad";

    std::vector<std::vector<pir::OpResult>> res(tensor_res.size());
    for (size_t i = 0; i < tensor_res.size(); ++i) {
        res[i].resize(tensor_res[i].size());
        for (size_t j = 0; j < tensor_res[i].size(); ++j) {
            if(tensor_res[i][j].defined()){
                res[i][j] = std::static_pointer_cast<primitive::LazyTensor>(tensor_res[i][j].impl())->value().dyn_cast<pir::OpResult>();
            }
        }
    }
    return res;
}

std::vector<std::vector<pir::OpResult>> Digamma_Op::Vjp(pir::Operation* op, const std::vector<std::vector<pir::Value>>& inputs_, const std::vector<std::vector<pir::Value>>& outputs, const std::vector<std::vector<pir::Value>>& out_grads, const std::vector<std::vector<bool>>& stop_gradients){

    PADDLE_ENFORCE_EQ(
      inputs_.size(),
      1,
      platform::errors::InvalidArgument("digamma op's inputs size should be 1, but now is %d.", inputs_.size()));
    PADDLE_ENFORCE_EQ(
      outputs.size(),
      1,
      platform::errors::InvalidArgument("digamma op's outputs size should be 1, but now is %d.", outputs.size()));

    VLOG(6) << "Prepare inputs of digamma_grad";

    Tensor x(std::make_shared<primitive::LazyTensor>(inputs_[0][0]));
    Tensor out_grad(std::make_shared<primitive::LazyTensor>(out_grads[0][0]));

    VLOG(6) << "Vjp prepare Prepare attributes of digamma_grad";


    VLOG(6) << "Vjp prepare call digamma_'s vjp inteface";

    std::vector<std::vector<Tensor>> tensor_res =
        primitive::digamma_vjp(
        x, out_grad, stop_gradients);

    VLOG(6) << "Vjp prepare stop gradient of digamma_grad";

    std::vector<std::vector<pir::OpResult>> res(tensor_res.size());
    for (size_t i = 0; i < tensor_res.size(); ++i) {
        res[i].resize(tensor_res[i].size());
        for (size_t j = 0; j < tensor_res[i].size(); ++j) {
            if(tensor_res[i][j].defined()){
                res[i][j] = std::static_pointer_cast<primitive::LazyTensor>(tensor_res[i][j].impl())->value().dyn_cast<pir::OpResult>();
            }
        }
    }
    return res;
}

std::vector<std::vector<pir::OpResult>> DistOp::Vjp(pir::Operation* op, const std::vector<std::vector<pir::Value>>& inputs_, const std::vector<std::vector<pir::Value>>& outputs, const std::vector<std::vector<pir::Value>>& out_grads, const std::vector<std::vector<bool>>& stop_gradients){

    PADDLE_ENFORCE_EQ(
      inputs_.size(),
      2,
      platform::errors::InvalidArgument("dist op's inputs size should be 2, but now is %d.", inputs_.size()));
    PADDLE_ENFORCE_EQ(
      outputs.size(),
      1,
      platform::errors::InvalidArgument("dist op's outputs size should be 1, but now is %d.", outputs.size()));

    VLOG(6) << "Prepare inputs of dist_grad";

    Tensor x(std::make_shared<primitive::LazyTensor>(inputs_[0][0]));
    Tensor y(std::make_shared<primitive::LazyTensor>(inputs_[1][0]));
    Tensor out(std::make_shared<primitive::LazyTensor>(outputs[0][0]));
    Tensor out_grad(std::make_shared<primitive::LazyTensor>(out_grads[0][0]));

    VLOG(6) << "Vjp prepare Prepare attributes of dist_grad";

    float p = op->attribute("p").dyn_cast<pir::FloatAttribute>().data();

    VLOG(6) << "Vjp prepare call dist's vjp inteface";

    std::vector<std::vector<Tensor>> tensor_res =
        primitive::dist_vjp(
        x, y, out, out_grad, p, stop_gradients);

    VLOG(6) << "Vjp prepare stop gradient of dist_grad";

    std::vector<std::vector<pir::OpResult>> res(tensor_res.size());
    for (size_t i = 0; i < tensor_res.size(); ++i) {
        res[i].resize(tensor_res[i].size());
        for (size_t j = 0; j < tensor_res[i].size(); ++j) {
            if(tensor_res[i][j].defined()){
                res[i][j] = std::static_pointer_cast<primitive::LazyTensor>(tensor_res[i][j].impl())->value().dyn_cast<pir::OpResult>();
            }
        }
    }
    return res;
}

std::vector<std::vector<pir::OpResult>> DotOp::Vjp(pir::Operation* op, const std::vector<std::vector<pir::Value>>& inputs_, const std::vector<std::vector<pir::Value>>& outputs, const std::vector<std::vector<pir::Value>>& out_grads, const std::vector<std::vector<bool>>& stop_gradients){

    PADDLE_ENFORCE_EQ(
      inputs_.size(),
      2,
      platform::errors::InvalidArgument("dot op's inputs size should be 2, but now is %d.", inputs_.size()));
    PADDLE_ENFORCE_EQ(
      outputs.size(),
      1,
      platform::errors::InvalidArgument("dot op's outputs size should be 1, but now is %d.", outputs.size()));

    VLOG(6) << "Prepare inputs of dot_grad";

    Tensor x(std::make_shared<primitive::LazyTensor>(inputs_[0][0]));
    Tensor y(std::make_shared<primitive::LazyTensor>(inputs_[1][0]));
    Tensor out_grad(std::make_shared<primitive::LazyTensor>(out_grads[0][0]));

    VLOG(6) << "Vjp prepare Prepare attributes of dot_grad";


    VLOG(6) << "Vjp prepare call dot's vjp inteface";

    std::vector<std::vector<Tensor>> tensor_res =
        primitive::dot_vjp(
        x, y, out_grad, stop_gradients);

    VLOG(6) << "Vjp prepare stop gradient of dot_grad";

    std::vector<std::vector<pir::OpResult>> res(tensor_res.size());
    for (size_t i = 0; i < tensor_res.size(); ++i) {
        res[i].resize(tensor_res[i].size());
        for (size_t j = 0; j < tensor_res[i].size(); ++j) {
            if(tensor_res[i][j].defined()){
                res[i][j] = std::static_pointer_cast<primitive::LazyTensor>(tensor_res[i][j].impl())->value().dyn_cast<pir::OpResult>();
            }
        }
    }
    return res;
}

std::vector<std::vector<pir::OpResult>> EigOp::Vjp(pir::Operation* op, const std::vector<std::vector<pir::Value>>& inputs_, const std::vector<std::vector<pir::Value>>& outputs, const std::vector<std::vector<pir::Value>>& out_grads, const std::vector<std::vector<bool>>& stop_gradients){

    PADDLE_ENFORCE_EQ(
      inputs_.size(),
      1,
      platform::errors::InvalidArgument("eig op's inputs size should be 1, but now is %d.", inputs_.size()));
    PADDLE_ENFORCE_EQ(
      outputs.size(),
      2,
      platform::errors::InvalidArgument("eig op's outputs size should be 2, but now is %d.", outputs.size()));

    VLOG(6) << "Prepare inputs of eig_grad";

    Tensor out_w(std::make_shared<primitive::LazyTensor>(outputs[0][0]));
    Tensor out_v(std::make_shared<primitive::LazyTensor>(outputs[1][0]));
    Tensor out_w_grad(std::make_shared<primitive::LazyTensor>(out_grads[0][0]));
    Tensor out_v_grad(std::make_shared<primitive::LazyTensor>(out_grads[1][0]));

    VLOG(6) << "Vjp prepare Prepare attributes of eig_grad";


    VLOG(6) << "Vjp prepare call eig's vjp inteface";

    std::vector<std::vector<Tensor>> tensor_res =
        primitive::eig_vjp(
        out_w, out_v, out_w_grad, out_v_grad, stop_gradients);

    VLOG(6) << "Vjp prepare stop gradient of eig_grad";

    std::vector<std::vector<pir::OpResult>> res(tensor_res.size());
    for (size_t i = 0; i < tensor_res.size(); ++i) {
        res[i].resize(tensor_res[i].size());
        for (size_t j = 0; j < tensor_res[i].size(); ++j) {
            if(tensor_res[i][j].defined()){
                res[i][j] = std::static_pointer_cast<primitive::LazyTensor>(tensor_res[i][j].impl())->value().dyn_cast<pir::OpResult>();
            }
        }
    }
    return res;
}

std::vector<std::vector<pir::OpResult>> EighOp::Vjp(pir::Operation* op, const std::vector<std::vector<pir::Value>>& inputs_, const std::vector<std::vector<pir::Value>>& outputs, const std::vector<std::vector<pir::Value>>& out_grads, const std::vector<std::vector<bool>>& stop_gradients){

    PADDLE_ENFORCE_EQ(
      inputs_.size(),
      1,
      platform::errors::InvalidArgument("eigh op's inputs size should be 1, but now is %d.", inputs_.size()));
    PADDLE_ENFORCE_EQ(
      outputs.size(),
      2,
      platform::errors::InvalidArgument("eigh op's outputs size should be 2, but now is %d.", outputs.size()));

    VLOG(6) << "Prepare inputs of eigh_grad";

    Tensor out_w(std::make_shared<primitive::LazyTensor>(outputs[0][0]));
    Tensor out_v(std::make_shared<primitive::LazyTensor>(outputs[1][0]));
    Tensor out_w_grad(std::make_shared<primitive::LazyTensor>(out_grads[0][0]));
    Tensor out_v_grad(std::make_shared<primitive::LazyTensor>(out_grads[1][0]));

    VLOG(6) << "Vjp prepare Prepare attributes of eigh_grad";


    VLOG(6) << "Vjp prepare call eigh's vjp inteface";

    std::vector<std::vector<Tensor>> tensor_res =
        primitive::eigh_vjp(
        out_w, out_v, out_w_grad, out_v_grad, stop_gradients);

    VLOG(6) << "Vjp prepare stop gradient of eigh_grad";

    std::vector<std::vector<pir::OpResult>> res(tensor_res.size());
    for (size_t i = 0; i < tensor_res.size(); ++i) {
        res[i].resize(tensor_res[i].size());
        for (size_t j = 0; j < tensor_res[i].size(); ++j) {
            if(tensor_res[i][j].defined()){
                res[i][j] = std::static_pointer_cast<primitive::LazyTensor>(tensor_res[i][j].impl())->value().dyn_cast<pir::OpResult>();
            }
        }
    }
    return res;
}

std::vector<std::vector<pir::OpResult>> EigvalshOp::Vjp(pir::Operation* op, const std::vector<std::vector<pir::Value>>& inputs_, const std::vector<std::vector<pir::Value>>& outputs, const std::vector<std::vector<pir::Value>>& out_grads, const std::vector<std::vector<bool>>& stop_gradients){

    PADDLE_ENFORCE_EQ(
      inputs_.size(),
      1,
      platform::errors::InvalidArgument("eigvalsh op's inputs size should be 1, but now is %d.", inputs_.size()));
    PADDLE_ENFORCE_EQ(
      outputs.size(),
      2,
      platform::errors::InvalidArgument("eigvalsh op's outputs size should be 2, but now is %d.", outputs.size()));

    VLOG(6) << "Prepare inputs of eigvalsh_grad";

    Tensor eigenvectors(std::make_shared<primitive::LazyTensor>(outputs[1][0]));
    Tensor eigenvalues_grad(std::make_shared<primitive::LazyTensor>(out_grads[0][0]));

    VLOG(6) << "Vjp prepare Prepare attributes of eigvalsh_grad";

    std::string uplo = op->attribute("uplo").dyn_cast<pir::StrAttribute>().AsString();
    bool is_test = op->attribute("is_test").dyn_cast<pir::BoolAttribute>().data();

    VLOG(6) << "Vjp prepare call eigvalsh's vjp inteface";

    std::vector<std::vector<Tensor>> tensor_res =
        primitive::eigvalsh_vjp(
        eigenvectors, eigenvalues_grad, uplo, is_test, stop_gradients);

    VLOG(6) << "Vjp prepare stop gradient of eigvalsh_grad";

    std::vector<std::vector<pir::OpResult>> res(tensor_res.size());
    for (size_t i = 0; i < tensor_res.size(); ++i) {
        res[i].resize(tensor_res[i].size());
        for (size_t j = 0; j < tensor_res[i].size(); ++j) {
            if(tensor_res[i][j].defined()){
                res[i][j] = std::static_pointer_cast<primitive::LazyTensor>(tensor_res[i][j].impl())->value().dyn_cast<pir::OpResult>();
            }
        }
    }
    return res;
}

std::vector<std::vector<pir::OpResult>> EluOp::Vjp(pir::Operation* op, const std::vector<std::vector<pir::Value>>& inputs_, const std::vector<std::vector<pir::Value>>& outputs, const std::vector<std::vector<pir::Value>>& out_grads, const std::vector<std::vector<bool>>& stop_gradients){

    PADDLE_ENFORCE_EQ(
      inputs_.size(),
      1,
      platform::errors::InvalidArgument("elu op's inputs size should be 1, but now is %d.", inputs_.size()));
    PADDLE_ENFORCE_EQ(
      outputs.size(),
      1,
      platform::errors::InvalidArgument("elu op's outputs size should be 1, but now is %d.", outputs.size()));

    VLOG(6) << "Prepare inputs of elu_grad";

    Tensor x(std::make_shared<primitive::LazyTensor>(inputs_[0][0]));
    Tensor out(std::make_shared<primitive::LazyTensor>(outputs[0][0]));
    Tensor out_grad(std::make_shared<primitive::LazyTensor>(out_grads[0][0]));

    VLOG(6) << "Vjp prepare Prepare attributes of elu_grad";

    float alpha = op->attribute("alpha").dyn_cast<pir::FloatAttribute>().data();

    VLOG(6) << "Vjp prepare call elu's vjp inteface";

    std::vector<std::vector<Tensor>> tensor_res =
        primitive::elu_vjp(
        x, out, out_grad, alpha, stop_gradients);

    VLOG(6) << "Vjp prepare stop gradient of elu_grad";

    std::vector<std::vector<pir::OpResult>> res(tensor_res.size());
    for (size_t i = 0; i < tensor_res.size(); ++i) {
        res[i].resize(tensor_res[i].size());
        for (size_t j = 0; j < tensor_res[i].size(); ++j) {
            if(tensor_res[i][j].defined()){
                res[i][j] = std::static_pointer_cast<primitive::LazyTensor>(tensor_res[i][j].impl())->value().dyn_cast<pir::OpResult>();
            }
        }
    }
    return res;
}

std::vector<std::vector<pir::OpResult>> Elu_Op::Vjp(pir::Operation* op, const std::vector<std::vector<pir::Value>>& inputs_, const std::vector<std::vector<pir::Value>>& outputs, const std::vector<std::vector<pir::Value>>& out_grads, const std::vector<std::vector<bool>>& stop_gradients){

    PADDLE_ENFORCE_EQ(
      inputs_.size(),
      1,
      platform::errors::InvalidArgument("elu op's inputs size should be 1, but now is %d.", inputs_.size()));
    PADDLE_ENFORCE_EQ(
      outputs.size(),
      1,
      platform::errors::InvalidArgument("elu op's outputs size should be 1, but now is %d.", outputs.size()));

    VLOG(6) << "Prepare inputs of elu_grad";

    Tensor x(std::make_shared<primitive::LazyTensor>(inputs_[0][0]));
    Tensor out(std::make_shared<primitive::LazyTensor>(outputs[0][0]));
    Tensor out_grad(std::make_shared<primitive::LazyTensor>(out_grads[0][0]));

    VLOG(6) << "Vjp prepare Prepare attributes of elu_grad";

    float alpha = op->attribute("alpha").dyn_cast<pir::FloatAttribute>().data();

    VLOG(6) << "Vjp prepare call elu_'s vjp inteface";

    std::vector<std::vector<Tensor>> tensor_res =
        primitive::elu_vjp(
        x, out, out_grad, alpha, stop_gradients);

    VLOG(6) << "Vjp prepare stop gradient of elu_grad";

    std::vector<std::vector<pir::OpResult>> res(tensor_res.size());
    for (size_t i = 0; i < tensor_res.size(); ++i) {
        res[i].resize(tensor_res[i].size());
        for (size_t j = 0; j < tensor_res[i].size(); ++j) {
            if(tensor_res[i][j].defined()){
                res[i][j] = std::static_pointer_cast<primitive::LazyTensor>(tensor_res[i][j].impl())->value().dyn_cast<pir::OpResult>();
            }
        }
    }
    return res;
}

std::vector<std::vector<pir::OpResult>> ErfOp::Vjp(pir::Operation* op, const std::vector<std::vector<pir::Value>>& inputs_, const std::vector<std::vector<pir::Value>>& outputs, const std::vector<std::vector<pir::Value>>& out_grads, const std::vector<std::vector<bool>>& stop_gradients){

    PADDLE_ENFORCE_EQ(
      inputs_.size(),
      1,
      platform::errors::InvalidArgument("erf op's inputs size should be 1, but now is %d.", inputs_.size()));
    PADDLE_ENFORCE_EQ(
      outputs.size(),
      1,
      platform::errors::InvalidArgument("erf op's outputs size should be 1, but now is %d.", outputs.size()));

    VLOG(6) << "Prepare inputs of erf_grad";

    Tensor x(std::make_shared<primitive::LazyTensor>(inputs_[0][0]));
    Tensor out_grad(std::make_shared<primitive::LazyTensor>(out_grads[0][0]));

    VLOG(6) << "Vjp prepare Prepare attributes of erf_grad";


    VLOG(6) << "Vjp prepare call erf's vjp inteface";

    std::vector<std::vector<Tensor>> tensor_res =
        primitive::erf_vjp(
        x, out_grad, stop_gradients);

    VLOG(6) << "Vjp prepare stop gradient of erf_grad";

    std::vector<std::vector<pir::OpResult>> res(tensor_res.size());
    for (size_t i = 0; i < tensor_res.size(); ++i) {
        res[i].resize(tensor_res[i].size());
        for (size_t j = 0; j < tensor_res[i].size(); ++j) {
            if(tensor_res[i][j].defined()){
                res[i][j] = std::static_pointer_cast<primitive::LazyTensor>(tensor_res[i][j].impl())->value().dyn_cast<pir::OpResult>();
            }
        }
    }
    return res;
}

std::vector<std::vector<pir::OpResult>> Erf_Op::Vjp(pir::Operation* op, const std::vector<std::vector<pir::Value>>& inputs_, const std::vector<std::vector<pir::Value>>& outputs, const std::vector<std::vector<pir::Value>>& out_grads, const std::vector<std::vector<bool>>& stop_gradients){

    PADDLE_ENFORCE_EQ(
      inputs_.size(),
      1,
      platform::errors::InvalidArgument("erf op's inputs size should be 1, but now is %d.", inputs_.size()));
    PADDLE_ENFORCE_EQ(
      outputs.size(),
      1,
      platform::errors::InvalidArgument("erf op's outputs size should be 1, but now is %d.", outputs.size()));

    VLOG(6) << "Prepare inputs of erf_grad";

    Tensor x(std::make_shared<primitive::LazyTensor>(inputs_[0][0]));
    Tensor out_grad(std::make_shared<primitive::LazyTensor>(out_grads[0][0]));

    VLOG(6) << "Vjp prepare Prepare attributes of erf_grad";


    VLOG(6) << "Vjp prepare call erf_'s vjp inteface";

    std::vector<std::vector<Tensor>> tensor_res =
        primitive::erf_vjp(
        x, out_grad, stop_gradients);

    VLOG(6) << "Vjp prepare stop gradient of erf_grad";

    std::vector<std::vector<pir::OpResult>> res(tensor_res.size());
    for (size_t i = 0; i < tensor_res.size(); ++i) {
        res[i].resize(tensor_res[i].size());
        for (size_t j = 0; j < tensor_res[i].size(); ++j) {
            if(tensor_res[i][j].defined()){
                res[i][j] = std::static_pointer_cast<primitive::LazyTensor>(tensor_res[i][j].impl())->value().dyn_cast<pir::OpResult>();
            }
        }
    }
    return res;
}

std::vector<std::vector<pir::OpResult>> ErfinvOp::Vjp(pir::Operation* op, const std::vector<std::vector<pir::Value>>& inputs_, const std::vector<std::vector<pir::Value>>& outputs, const std::vector<std::vector<pir::Value>>& out_grads, const std::vector<std::vector<bool>>& stop_gradients){

    PADDLE_ENFORCE_EQ(
      inputs_.size(),
      1,
      platform::errors::InvalidArgument("erfinv op's inputs size should be 1, but now is %d.", inputs_.size()));
    PADDLE_ENFORCE_EQ(
      outputs.size(),
      1,
      platform::errors::InvalidArgument("erfinv op's outputs size should be 1, but now is %d.", outputs.size()));

    VLOG(6) << "Prepare inputs of erfinv_grad";

    Tensor out(std::make_shared<primitive::LazyTensor>(outputs[0][0]));
    Tensor out_grad(std::make_shared<primitive::LazyTensor>(out_grads[0][0]));

    VLOG(6) << "Vjp prepare Prepare attributes of erfinv_grad";


    VLOG(6) << "Vjp prepare call erfinv's vjp inteface";

    std::vector<std::vector<Tensor>> tensor_res =
        primitive::erfinv_vjp(
        out, out_grad, stop_gradients);

    VLOG(6) << "Vjp prepare stop gradient of erfinv_grad";

    std::vector<std::vector<pir::OpResult>> res(tensor_res.size());
    for (size_t i = 0; i < tensor_res.size(); ++i) {
        res[i].resize(tensor_res[i].size());
        for (size_t j = 0; j < tensor_res[i].size(); ++j) {
            if(tensor_res[i][j].defined()){
                res[i][j] = std::static_pointer_cast<primitive::LazyTensor>(tensor_res[i][j].impl())->value().dyn_cast<pir::OpResult>();
            }
        }
    }
    return res;
}

std::vector<std::vector<pir::OpResult>> Erfinv_Op::Vjp(pir::Operation* op, const std::vector<std::vector<pir::Value>>& inputs_, const std::vector<std::vector<pir::Value>>& outputs, const std::vector<std::vector<pir::Value>>& out_grads, const std::vector<std::vector<bool>>& stop_gradients){

    PADDLE_ENFORCE_EQ(
      inputs_.size(),
      1,
      platform::errors::InvalidArgument("erfinv op's inputs size should be 1, but now is %d.", inputs_.size()));
    PADDLE_ENFORCE_EQ(
      outputs.size(),
      1,
      platform::errors::InvalidArgument("erfinv op's outputs size should be 1, but now is %d.", outputs.size()));

    VLOG(6) << "Prepare inputs of erfinv_grad";

    Tensor out(std::make_shared<primitive::LazyTensor>(outputs[0][0]));
    Tensor out_grad(std::make_shared<primitive::LazyTensor>(out_grads[0][0]));

    VLOG(6) << "Vjp prepare Prepare attributes of erfinv_grad";


    VLOG(6) << "Vjp prepare call erfinv_'s vjp inteface";

    std::vector<std::vector<Tensor>> tensor_res =
        primitive::erfinv_vjp(
        out, out_grad, stop_gradients);

    VLOG(6) << "Vjp prepare stop gradient of erfinv_grad";

    std::vector<std::vector<pir::OpResult>> res(tensor_res.size());
    for (size_t i = 0; i < tensor_res.size(); ++i) {
        res[i].resize(tensor_res[i].size());
        for (size_t j = 0; j < tensor_res[i].size(); ++j) {
            if(tensor_res[i][j].defined()){
                res[i][j] = std::static_pointer_cast<primitive::LazyTensor>(tensor_res[i][j].impl())->value().dyn_cast<pir::OpResult>();
            }
        }
    }
    return res;
}

std::vector<std::vector<pir::OpResult>> ExpOp::Vjp(pir::Operation* op, const std::vector<std::vector<pir::Value>>& inputs_, const std::vector<std::vector<pir::Value>>& outputs, const std::vector<std::vector<pir::Value>>& out_grads, const std::vector<std::vector<bool>>& stop_gradients){

    PADDLE_ENFORCE_EQ(
      inputs_.size(),
      1,
      platform::errors::InvalidArgument("exp op's inputs size should be 1, but now is %d.", inputs_.size()));
    PADDLE_ENFORCE_EQ(
      outputs.size(),
      1,
      platform::errors::InvalidArgument("exp op's outputs size should be 1, but now is %d.", outputs.size()));

    VLOG(6) << "Prepare inputs of exp_grad";

    Tensor out(std::make_shared<primitive::LazyTensor>(outputs[0][0]));
    Tensor out_grad(std::make_shared<primitive::LazyTensor>(out_grads[0][0]));

    VLOG(6) << "Vjp prepare Prepare attributes of exp_grad";


    VLOG(6) << "Vjp prepare call exp's vjp inteface";

    std::vector<std::vector<Tensor>> tensor_res =
        primitive::exp_vjp(
        out, out_grad, stop_gradients);

    VLOG(6) << "Vjp prepare stop gradient of exp_grad";

    std::vector<std::vector<pir::OpResult>> res(tensor_res.size());
    for (size_t i = 0; i < tensor_res.size(); ++i) {
        res[i].resize(tensor_res[i].size());
        for (size_t j = 0; j < tensor_res[i].size(); ++j) {
            if(tensor_res[i][j].defined()){
                res[i][j] = std::static_pointer_cast<primitive::LazyTensor>(tensor_res[i][j].impl())->value().dyn_cast<pir::OpResult>();
            }
        }
    }
    return res;
}

std::vector<std::vector<pir::OpResult>> Exp_Op::Vjp(pir::Operation* op, const std::vector<std::vector<pir::Value>>& inputs_, const std::vector<std::vector<pir::Value>>& outputs, const std::vector<std::vector<pir::Value>>& out_grads, const std::vector<std::vector<bool>>& stop_gradients){

    PADDLE_ENFORCE_EQ(
      inputs_.size(),
      1,
      platform::errors::InvalidArgument("exp op's inputs size should be 1, but now is %d.", inputs_.size()));
    PADDLE_ENFORCE_EQ(
      outputs.size(),
      1,
      platform::errors::InvalidArgument("exp op's outputs size should be 1, but now is %d.", outputs.size()));

    VLOG(6) << "Prepare inputs of exp_grad";

    Tensor out(std::make_shared<primitive::LazyTensor>(outputs[0][0]));
    Tensor out_grad(std::make_shared<primitive::LazyTensor>(out_grads[0][0]));

    VLOG(6) << "Vjp prepare Prepare attributes of exp_grad";


    VLOG(6) << "Vjp prepare call exp_'s vjp inteface";

    std::vector<std::vector<Tensor>> tensor_res =
        primitive::exp_vjp(
        out, out_grad, stop_gradients);

    VLOG(6) << "Vjp prepare stop gradient of exp_grad";

    std::vector<std::vector<pir::OpResult>> res(tensor_res.size());
    for (size_t i = 0; i < tensor_res.size(); ++i) {
        res[i].resize(tensor_res[i].size());
        for (size_t j = 0; j < tensor_res[i].size(); ++j) {
            if(tensor_res[i][j].defined()){
                res[i][j] = std::static_pointer_cast<primitive::LazyTensor>(tensor_res[i][j].impl())->value().dyn_cast<pir::OpResult>();
            }
        }
    }
    return res;
}

std::vector<std::vector<pir::OpResult>> ExpandAsOp::Vjp(pir::Operation* op, const std::vector<std::vector<pir::Value>>& inputs_, const std::vector<std::vector<pir::Value>>& outputs, const std::vector<std::vector<pir::Value>>& out_grads, const std::vector<std::vector<bool>>& stop_gradients){

    PADDLE_ENFORCE_EQ(
      inputs_.size(),
      2,
      platform::errors::InvalidArgument("expand_as op's inputs size should be 2, but now is %d.", inputs_.size()));
    PADDLE_ENFORCE_EQ(
      outputs.size(),
      1,
      platform::errors::InvalidArgument("expand_as op's outputs size should be 1, but now is %d.", outputs.size()));

    VLOG(6) << "Prepare inputs of expand_as_grad";

    Tensor x(std::make_shared<primitive::LazyTensor>(inputs_[0][0]));
    Tensor out_grad(std::make_shared<primitive::LazyTensor>(out_grads[0][0]));

    VLOG(6) << "Vjp prepare Prepare attributes of expand_as_grad";

    std::vector<int> target_shape;
    for (size_t i = 0; i < op->attribute("target_shape").dyn_cast<pir::ArrayAttribute>().size(); i++) {
        target_shape.push_back(op->attribute("target_shape").dyn_cast<pir::ArrayAttribute>().at(i).dyn_cast<pir::Int32Attribute>().data());
    }

    VLOG(6) << "Vjp prepare call expand_as's vjp inteface";

    std::vector<std::vector<Tensor>> tensor_res =
        primitive::expand_as_vjp(
        x, out_grad, target_shape, stop_gradients);

    VLOG(6) << "Vjp prepare stop gradient of expand_as_grad";

    std::vector<std::vector<pir::OpResult>> res(tensor_res.size());
    for (size_t i = 0; i < tensor_res.size(); ++i) {
        res[i].resize(tensor_res[i].size());
        for (size_t j = 0; j < tensor_res[i].size(); ++j) {
            if(tensor_res[i][j].defined()){
                res[i][j] = std::static_pointer_cast<primitive::LazyTensor>(tensor_res[i][j].impl())->value().dyn_cast<pir::OpResult>();
            }
        }
    }
    return res;
}

std::vector<std::vector<pir::OpResult>> Expm1Op::Vjp(pir::Operation* op, const std::vector<std::vector<pir::Value>>& inputs_, const std::vector<std::vector<pir::Value>>& outputs, const std::vector<std::vector<pir::Value>>& out_grads, const std::vector<std::vector<bool>>& stop_gradients){

    PADDLE_ENFORCE_EQ(
      inputs_.size(),
      1,
      platform::errors::InvalidArgument("expm1 op's inputs size should be 1, but now is %d.", inputs_.size()));
    PADDLE_ENFORCE_EQ(
      outputs.size(),
      1,
      platform::errors::InvalidArgument("expm1 op's outputs size should be 1, but now is %d.", outputs.size()));

    VLOG(6) << "Prepare inputs of expm1_grad";

    Tensor out(std::make_shared<primitive::LazyTensor>(outputs[0][0]));
    Tensor out_grad(std::make_shared<primitive::LazyTensor>(out_grads[0][0]));

    VLOG(6) << "Vjp prepare Prepare attributes of expm1_grad";


    VLOG(6) << "Vjp prepare call expm1's vjp inteface";

    std::vector<std::vector<Tensor>> tensor_res =
        primitive::expm1_vjp(
        out, out_grad, stop_gradients);

    VLOG(6) << "Vjp prepare stop gradient of expm1_grad";

    std::vector<std::vector<pir::OpResult>> res(tensor_res.size());
    for (size_t i = 0; i < tensor_res.size(); ++i) {
        res[i].resize(tensor_res[i].size());
        for (size_t j = 0; j < tensor_res[i].size(); ++j) {
            if(tensor_res[i][j].defined()){
                res[i][j] = std::static_pointer_cast<primitive::LazyTensor>(tensor_res[i][j].impl())->value().dyn_cast<pir::OpResult>();
            }
        }
    }
    return res;
}

std::vector<std::vector<pir::OpResult>> Expm1_Op::Vjp(pir::Operation* op, const std::vector<std::vector<pir::Value>>& inputs_, const std::vector<std::vector<pir::Value>>& outputs, const std::vector<std::vector<pir::Value>>& out_grads, const std::vector<std::vector<bool>>& stop_gradients){

    PADDLE_ENFORCE_EQ(
      inputs_.size(),
      1,
      platform::errors::InvalidArgument("expm1 op's inputs size should be 1, but now is %d.", inputs_.size()));
    PADDLE_ENFORCE_EQ(
      outputs.size(),
      1,
      platform::errors::InvalidArgument("expm1 op's outputs size should be 1, but now is %d.", outputs.size()));

    VLOG(6) << "Prepare inputs of expm1_grad";

    Tensor out(std::make_shared<primitive::LazyTensor>(outputs[0][0]));
    Tensor out_grad(std::make_shared<primitive::LazyTensor>(out_grads[0][0]));

    VLOG(6) << "Vjp prepare Prepare attributes of expm1_grad";


    VLOG(6) << "Vjp prepare call expm1_'s vjp inteface";

    std::vector<std::vector<Tensor>> tensor_res =
        primitive::expm1_vjp(
        out, out_grad, stop_gradients);

    VLOG(6) << "Vjp prepare stop gradient of expm1_grad";

    std::vector<std::vector<pir::OpResult>> res(tensor_res.size());
    for (size_t i = 0; i < tensor_res.size(); ++i) {
        res[i].resize(tensor_res[i].size());
        for (size_t j = 0; j < tensor_res[i].size(); ++j) {
            if(tensor_res[i][j].defined()){
                res[i][j] = std::static_pointer_cast<primitive::LazyTensor>(tensor_res[i][j].impl())->value().dyn_cast<pir::OpResult>();
            }
        }
    }
    return res;
}

std::vector<std::vector<pir::OpResult>> FftC2cOp::Vjp(pir::Operation* op, const std::vector<std::vector<pir::Value>>& inputs_, const std::vector<std::vector<pir::Value>>& outputs, const std::vector<std::vector<pir::Value>>& out_grads, const std::vector<std::vector<bool>>& stop_gradients){

    PADDLE_ENFORCE_EQ(
      inputs_.size(),
      1,
      platform::errors::InvalidArgument("fft_c2c op's inputs size should be 1, but now is %d.", inputs_.size()));
    PADDLE_ENFORCE_EQ(
      outputs.size(),
      1,
      platform::errors::InvalidArgument("fft_c2c op's outputs size should be 1, but now is %d.", outputs.size()));

    VLOG(6) << "Prepare inputs of fft_c2c_grad";

    Tensor out_grad(std::make_shared<primitive::LazyTensor>(out_grads[0][0]));

    VLOG(6) << "Vjp prepare Prepare attributes of fft_c2c_grad";

    std::vector<int64_t> axes;
    for (size_t i = 0; i < op->attribute("axes").dyn_cast<pir::ArrayAttribute>().size(); i++) {
        axes.push_back(op->attribute("axes").dyn_cast<pir::ArrayAttribute>().at(i).dyn_cast<pir::Int64Attribute>().data());
    }
    std::string normalization = op->attribute("normalization").dyn_cast<pir::StrAttribute>().AsString();
    bool forward = op->attribute("forward").dyn_cast<pir::BoolAttribute>().data();

    VLOG(6) << "Vjp prepare call fft_c2c's vjp inteface";

    std::vector<std::vector<Tensor>> tensor_res =
        primitive::fft_c2c_vjp(
        out_grad, axes, normalization, forward, stop_gradients);

    VLOG(6) << "Vjp prepare stop gradient of fft_c2c_grad";

    std::vector<std::vector<pir::OpResult>> res(tensor_res.size());
    for (size_t i = 0; i < tensor_res.size(); ++i) {
        res[i].resize(tensor_res[i].size());
        for (size_t j = 0; j < tensor_res[i].size(); ++j) {
            if(tensor_res[i][j].defined()){
                res[i][j] = std::static_pointer_cast<primitive::LazyTensor>(tensor_res[i][j].impl())->value().dyn_cast<pir::OpResult>();
            }
        }
    }
    return res;
}

std::vector<std::vector<pir::OpResult>> FftC2rOp::Vjp(pir::Operation* op, const std::vector<std::vector<pir::Value>>& inputs_, const std::vector<std::vector<pir::Value>>& outputs, const std::vector<std::vector<pir::Value>>& out_grads, const std::vector<std::vector<bool>>& stop_gradients){

    PADDLE_ENFORCE_EQ(
      inputs_.size(),
      1,
      platform::errors::InvalidArgument("fft_c2r op's inputs size should be 1, but now is %d.", inputs_.size()));
    PADDLE_ENFORCE_EQ(
      outputs.size(),
      1,
      platform::errors::InvalidArgument("fft_c2r op's outputs size should be 1, but now is %d.", outputs.size()));

    VLOG(6) << "Prepare inputs of fft_c2r_grad";

    Tensor out_grad(std::make_shared<primitive::LazyTensor>(out_grads[0][0]));

    VLOG(6) << "Vjp prepare Prepare attributes of fft_c2r_grad";

    std::vector<int64_t> axes;
    for (size_t i = 0; i < op->attribute("axes").dyn_cast<pir::ArrayAttribute>().size(); i++) {
        axes.push_back(op->attribute("axes").dyn_cast<pir::ArrayAttribute>().at(i).dyn_cast<pir::Int64Attribute>().data());
    }
    std::string normalization = op->attribute("normalization").dyn_cast<pir::StrAttribute>().AsString();
    bool forward = op->attribute("forward").dyn_cast<pir::BoolAttribute>().data();
    int64_t last_dim_size = op->attribute("last_dim_size").dyn_cast<pir::Int64Attribute>().data();

    VLOG(6) << "Vjp prepare call fft_c2r's vjp inteface";

    std::vector<std::vector<Tensor>> tensor_res =
        primitive::fft_c2r_vjp(
        out_grad, axes, normalization, forward, last_dim_size, stop_gradients);

    VLOG(6) << "Vjp prepare stop gradient of fft_c2r_grad";

    std::vector<std::vector<pir::OpResult>> res(tensor_res.size());
    for (size_t i = 0; i < tensor_res.size(); ++i) {
        res[i].resize(tensor_res[i].size());
        for (size_t j = 0; j < tensor_res[i].size(); ++j) {
            if(tensor_res[i][j].defined()){
                res[i][j] = std::static_pointer_cast<primitive::LazyTensor>(tensor_res[i][j].impl())->value().dyn_cast<pir::OpResult>();
            }
        }
    }
    return res;
}

std::vector<std::vector<pir::OpResult>> FftR2cOp::Vjp(pir::Operation* op, const std::vector<std::vector<pir::Value>>& inputs_, const std::vector<std::vector<pir::Value>>& outputs, const std::vector<std::vector<pir::Value>>& out_grads, const std::vector<std::vector<bool>>& stop_gradients){

    PADDLE_ENFORCE_EQ(
      inputs_.size(),
      1,
      platform::errors::InvalidArgument("fft_r2c op's inputs size should be 1, but now is %d.", inputs_.size()));
    PADDLE_ENFORCE_EQ(
      outputs.size(),
      1,
      platform::errors::InvalidArgument("fft_r2c op's outputs size should be 1, but now is %d.", outputs.size()));

    VLOG(6) << "Prepare inputs of fft_r2c_grad";

    Tensor x(std::make_shared<primitive::LazyTensor>(inputs_[0][0]));
    Tensor out_grad(std::make_shared<primitive::LazyTensor>(out_grads[0][0]));

    VLOG(6) << "Vjp prepare Prepare attributes of fft_r2c_grad";

    std::vector<int64_t> axes;
    for (size_t i = 0; i < op->attribute("axes").dyn_cast<pir::ArrayAttribute>().size(); i++) {
        axes.push_back(op->attribute("axes").dyn_cast<pir::ArrayAttribute>().at(i).dyn_cast<pir::Int64Attribute>().data());
    }
    std::string normalization = op->attribute("normalization").dyn_cast<pir::StrAttribute>().AsString();
    bool forward = op->attribute("forward").dyn_cast<pir::BoolAttribute>().data();
    bool onesided = op->attribute("onesided").dyn_cast<pir::BoolAttribute>().data();

    VLOG(6) << "Vjp prepare call fft_r2c's vjp inteface";

    std::vector<std::vector<Tensor>> tensor_res =
        primitive::fft_r2c_vjp(
        x, out_grad, axes, normalization, forward, onesided, stop_gradients);

    VLOG(6) << "Vjp prepare stop gradient of fft_r2c_grad";

    std::vector<std::vector<pir::OpResult>> res(tensor_res.size());
    for (size_t i = 0; i < tensor_res.size(); ++i) {
        res[i].resize(tensor_res[i].size());
        for (size_t j = 0; j < tensor_res[i].size(); ++j) {
            if(tensor_res[i][j].defined()){
                res[i][j] = std::static_pointer_cast<primitive::LazyTensor>(tensor_res[i][j].impl())->value().dyn_cast<pir::OpResult>();
            }
        }
    }
    return res;
}

std::vector<std::vector<pir::OpResult>> FillOp::Vjp(pir::Operation* op, const std::vector<std::vector<pir::Value>>& inputs_, const std::vector<std::vector<pir::Value>>& outputs, const std::vector<std::vector<pir::Value>>& out_grads, const std::vector<std::vector<bool>>& stop_gradients){

    PADDLE_ENFORCE_EQ(
      inputs_.size(),
      2,
      platform::errors::InvalidArgument("fill op's inputs size should be 2, but now is %d.", inputs_.size()));
    PADDLE_ENFORCE_EQ(
      outputs.size(),
      1,
      platform::errors::InvalidArgument("fill op's outputs size should be 1, but now is %d.", outputs.size()));

    VLOG(6) << "Prepare inputs of fill_grad";

    Tensor out_grad(std::make_shared<primitive::LazyTensor>(out_grads[0][0]));

    VLOG(6) << "Vjp prepare Prepare attributes of fill_grad";

    Tensor value(std::make_shared<primitive::LazyTensor>(inputs_[1][0]));

    VLOG(6) << "Vjp prepare call fill's vjp inteface";

    std::vector<std::vector<Tensor>> tensor_res =
        primitive::fill_vjp(
        out_grad, value, stop_gradients);

    VLOG(6) << "Vjp prepare stop gradient of fill_grad";

    std::vector<std::vector<pir::OpResult>> res(tensor_res.size());
    for (size_t i = 0; i < tensor_res.size(); ++i) {
        res[i].resize(tensor_res[i].size());
        for (size_t j = 0; j < tensor_res[i].size(); ++j) {
            if(tensor_res[i][j].defined()){
                res[i][j] = std::static_pointer_cast<primitive::LazyTensor>(tensor_res[i][j].impl())->value().dyn_cast<pir::OpResult>();
            }
        }
    }
    return res;
}

std::vector<std::vector<pir::OpResult>> Fill_Op::Vjp(pir::Operation* op, const std::vector<std::vector<pir::Value>>& inputs_, const std::vector<std::vector<pir::Value>>& outputs, const std::vector<std::vector<pir::Value>>& out_grads, const std::vector<std::vector<bool>>& stop_gradients){

    PADDLE_ENFORCE_EQ(
      inputs_.size(),
      2,
      platform::errors::InvalidArgument("fill op's inputs size should be 2, but now is %d.", inputs_.size()));
    PADDLE_ENFORCE_EQ(
      outputs.size(),
      1,
      platform::errors::InvalidArgument("fill op's outputs size should be 1, but now is %d.", outputs.size()));

    VLOG(6) << "Prepare inputs of fill_grad";

    Tensor out_grad(std::make_shared<primitive::LazyTensor>(out_grads[0][0]));

    VLOG(6) << "Vjp prepare Prepare attributes of fill_grad";

    Tensor value(std::make_shared<primitive::LazyTensor>(inputs_[1][0]));

    VLOG(6) << "Vjp prepare call fill_'s vjp inteface";

    std::vector<std::vector<Tensor>> tensor_res =
        primitive::fill_vjp(
        out_grad, value, stop_gradients);

    VLOG(6) << "Vjp prepare stop gradient of fill_grad";

    std::vector<std::vector<pir::OpResult>> res(tensor_res.size());
    for (size_t i = 0; i < tensor_res.size(); ++i) {
        res[i].resize(tensor_res[i].size());
        for (size_t j = 0; j < tensor_res[i].size(); ++j) {
            if(tensor_res[i][j].defined()){
                res[i][j] = std::static_pointer_cast<primitive::LazyTensor>(tensor_res[i][j].impl())->value().dyn_cast<pir::OpResult>();
            }
        }
    }
    return res;
}

std::vector<std::vector<pir::OpResult>> FillDiagonalOp::Vjp(pir::Operation* op, const std::vector<std::vector<pir::Value>>& inputs_, const std::vector<std::vector<pir::Value>>& outputs, const std::vector<std::vector<pir::Value>>& out_grads, const std::vector<std::vector<bool>>& stop_gradients){

    PADDLE_ENFORCE_EQ(
      inputs_.size(),
      1,
      platform::errors::InvalidArgument("fill_diagonal op's inputs size should be 1, but now is %d.", inputs_.size()));
    PADDLE_ENFORCE_EQ(
      outputs.size(),
      1,
      platform::errors::InvalidArgument("fill_diagonal op's outputs size should be 1, but now is %d.", outputs.size()));

    VLOG(6) << "Prepare inputs of fill_diagonal_grad";

    Tensor out_grad(std::make_shared<primitive::LazyTensor>(out_grads[0][0]));

    VLOG(6) << "Vjp prepare Prepare attributes of fill_diagonal_grad";

    float value = op->attribute("value").dyn_cast<pir::FloatAttribute>().data();
    int offset = op->attribute("offset").dyn_cast<pir::Int32Attribute>().data();
    bool wrap = op->attribute("wrap").dyn_cast<pir::BoolAttribute>().data();

    VLOG(6) << "Vjp prepare call fill_diagonal's vjp inteface";

    std::vector<std::vector<Tensor>> tensor_res =
        primitive::fill_diagonal_vjp(
        out_grad, value, offset, wrap, stop_gradients);

    VLOG(6) << "Vjp prepare stop gradient of fill_diagonal_grad";

    std::vector<std::vector<pir::OpResult>> res(tensor_res.size());
    for (size_t i = 0; i < tensor_res.size(); ++i) {
        res[i].resize(tensor_res[i].size());
        for (size_t j = 0; j < tensor_res[i].size(); ++j) {
            if(tensor_res[i][j].defined()){
                res[i][j] = std::static_pointer_cast<primitive::LazyTensor>(tensor_res[i][j].impl())->value().dyn_cast<pir::OpResult>();
            }
        }
    }
    return res;
}

std::vector<std::vector<pir::OpResult>> FillDiagonal_Op::Vjp(pir::Operation* op, const std::vector<std::vector<pir::Value>>& inputs_, const std::vector<std::vector<pir::Value>>& outputs, const std::vector<std::vector<pir::Value>>& out_grads, const std::vector<std::vector<bool>>& stop_gradients){

    PADDLE_ENFORCE_EQ(
      inputs_.size(),
      1,
      platform::errors::InvalidArgument("fill_diagonal op's inputs size should be 1, but now is %d.", inputs_.size()));
    PADDLE_ENFORCE_EQ(
      outputs.size(),
      1,
      platform::errors::InvalidArgument("fill_diagonal op's outputs size should be 1, but now is %d.", outputs.size()));

    VLOG(6) << "Prepare inputs of fill_diagonal_grad";

    Tensor out_grad(std::make_shared<primitive::LazyTensor>(out_grads[0][0]));

    VLOG(6) << "Vjp prepare Prepare attributes of fill_diagonal_grad";

    float value = op->attribute("value").dyn_cast<pir::FloatAttribute>().data();
    int offset = op->attribute("offset").dyn_cast<pir::Int32Attribute>().data();
    bool wrap = op->attribute("wrap").dyn_cast<pir::BoolAttribute>().data();

    VLOG(6) << "Vjp prepare call fill_diagonal_'s vjp inteface";

    std::vector<std::vector<Tensor>> tensor_res =
        primitive::fill_diagonal_vjp(
        out_grad, value, offset, wrap, stop_gradients);

    VLOG(6) << "Vjp prepare stop gradient of fill_diagonal_grad";

    std::vector<std::vector<pir::OpResult>> res(tensor_res.size());
    for (size_t i = 0; i < tensor_res.size(); ++i) {
        res[i].resize(tensor_res[i].size());
        for (size_t j = 0; j < tensor_res[i].size(); ++j) {
            if(tensor_res[i][j].defined()){
                res[i][j] = std::static_pointer_cast<primitive::LazyTensor>(tensor_res[i][j].impl())->value().dyn_cast<pir::OpResult>();
            }
        }
    }
    return res;
}

std::vector<std::vector<pir::OpResult>> FillDiagonalTensorOp::Vjp(pir::Operation* op, const std::vector<std::vector<pir::Value>>& inputs_, const std::vector<std::vector<pir::Value>>& outputs, const std::vector<std::vector<pir::Value>>& out_grads, const std::vector<std::vector<bool>>& stop_gradients){

    PADDLE_ENFORCE_EQ(
      inputs_.size(),
      2,
      platform::errors::InvalidArgument("fill_diagonal_tensor op's inputs size should be 2, but now is %d.", inputs_.size()));
    PADDLE_ENFORCE_EQ(
      outputs.size(),
      1,
      platform::errors::InvalidArgument("fill_diagonal_tensor op's outputs size should be 1, but now is %d.", outputs.size()));

    VLOG(6) << "Prepare inputs of fill_diagonal_tensor_grad";

    Tensor out_grad(std::make_shared<primitive::LazyTensor>(out_grads[0][0]));

    VLOG(6) << "Vjp prepare Prepare attributes of fill_diagonal_tensor_grad";

    int64_t offset = op->attribute("offset").dyn_cast<pir::Int64Attribute>().data();
    int dim1 = op->attribute("dim1").dyn_cast<pir::Int32Attribute>().data();
    int dim2 = op->attribute("dim2").dyn_cast<pir::Int32Attribute>().data();

    VLOG(6) << "Vjp prepare call fill_diagonal_tensor's vjp inteface";

    std::vector<std::vector<Tensor>> tensor_res =
        primitive::fill_diagonal_tensor_vjp(
        out_grad, offset, dim1, dim2, stop_gradients);

    VLOG(6) << "Vjp prepare stop gradient of fill_diagonal_tensor_grad";

    std::vector<std::vector<pir::OpResult>> res(tensor_res.size());
    for (size_t i = 0; i < tensor_res.size(); ++i) {
        res[i].resize(tensor_res[i].size());
        for (size_t j = 0; j < tensor_res[i].size(); ++j) {
            if(tensor_res[i][j].defined()){
                res[i][j] = std::static_pointer_cast<primitive::LazyTensor>(tensor_res[i][j].impl())->value().dyn_cast<pir::OpResult>();
            }
        }
    }
    return res;
}

std::vector<std::vector<pir::OpResult>> FillDiagonalTensor_Op::Vjp(pir::Operation* op, const std::vector<std::vector<pir::Value>>& inputs_, const std::vector<std::vector<pir::Value>>& outputs, const std::vector<std::vector<pir::Value>>& out_grads, const std::vector<std::vector<bool>>& stop_gradients){

    PADDLE_ENFORCE_EQ(
      inputs_.size(),
      2,
      platform::errors::InvalidArgument("fill_diagonal_tensor op's inputs size should be 2, but now is %d.", inputs_.size()));
    PADDLE_ENFORCE_EQ(
      outputs.size(),
      1,
      platform::errors::InvalidArgument("fill_diagonal_tensor op's outputs size should be 1, but now is %d.", outputs.size()));

    VLOG(6) << "Prepare inputs of fill_diagonal_tensor_grad";

    Tensor out_grad(std::make_shared<primitive::LazyTensor>(out_grads[0][0]));

    VLOG(6) << "Vjp prepare Prepare attributes of fill_diagonal_tensor_grad";

    int64_t offset = op->attribute("offset").dyn_cast<pir::Int64Attribute>().data();
    int dim1 = op->attribute("dim1").dyn_cast<pir::Int32Attribute>().data();
    int dim2 = op->attribute("dim2").dyn_cast<pir::Int32Attribute>().data();

    VLOG(6) << "Vjp prepare call fill_diagonal_tensor_'s vjp inteface";

    std::vector<std::vector<Tensor>> tensor_res =
        primitive::fill_diagonal_tensor_vjp(
        out_grad, offset, dim1, dim2, stop_gradients);

    VLOG(6) << "Vjp prepare stop gradient of fill_diagonal_tensor_grad";

    std::vector<std::vector<pir::OpResult>> res(tensor_res.size());
    for (size_t i = 0; i < tensor_res.size(); ++i) {
        res[i].resize(tensor_res[i].size());
        for (size_t j = 0; j < tensor_res[i].size(); ++j) {
            if(tensor_res[i][j].defined()){
                res[i][j] = std::static_pointer_cast<primitive::LazyTensor>(tensor_res[i][j].impl())->value().dyn_cast<pir::OpResult>();
            }
        }
    }
    return res;
}

std::vector<std::vector<pir::OpResult>> FlashAttnOp::Vjp(pir::Operation* op, const std::vector<std::vector<pir::Value>>& inputs_, const std::vector<std::vector<pir::Value>>& outputs, const std::vector<std::vector<pir::Value>>& out_grads, const std::vector<std::vector<bool>>& stop_gradients){

    PADDLE_ENFORCE_EQ(
      inputs_.size(),
      5,
      platform::errors::InvalidArgument("flash_attn op's inputs size should be 5, but now is %d.", inputs_.size()));
    PADDLE_ENFORCE_EQ(
      outputs.size(),
      4,
      platform::errors::InvalidArgument("flash_attn op's outputs size should be 4, but now is %d.", outputs.size()));

    VLOG(6) << "Prepare inputs of flash_attn_grad";

    Tensor q(std::make_shared<primitive::LazyTensor>(inputs_[0][0]));
    Tensor k(std::make_shared<primitive::LazyTensor>(inputs_[1][0]));
    Tensor v(std::make_shared<primitive::LazyTensor>(inputs_[2][0]));
    Tensor out(std::make_shared<primitive::LazyTensor>(outputs[0][0]));
    Tensor softmax_lse(std::make_shared<primitive::LazyTensor>(outputs[2][0]));
    Tensor seed_offset(std::make_shared<primitive::LazyTensor>(outputs[3][0]));
    paddle::optional<Tensor> attn_mask;
    if (!IsEmptyValue(inputs_[4][0])){
        attn_mask = paddle::make_optional<Tensor>(Tensor(std::make_shared<primitive::LazyTensor>(inputs_[4][0])));
    }
    Tensor out_grad(std::make_shared<primitive::LazyTensor>(out_grads[0][0]));

    VLOG(6) << "Vjp prepare Prepare attributes of flash_attn_grad";

    float dropout = op->attribute("dropout").dyn_cast<pir::FloatAttribute>().data();
    bool causal = op->attribute("causal").dyn_cast<pir::BoolAttribute>().data();

    VLOG(6) << "Vjp prepare call flash_attn's vjp inteface";

    std::vector<std::vector<Tensor>> tensor_res =
        primitive::flash_attn_vjp(
        q, k, v, out, softmax_lse, seed_offset, attn_mask, out_grad, dropout, causal, stop_gradients);

    VLOG(6) << "Vjp prepare stop gradient of flash_attn_grad";

    std::vector<std::vector<pir::OpResult>> res(tensor_res.size());
    for (size_t i = 0; i < tensor_res.size(); ++i) {
        res[i].resize(tensor_res[i].size());
        for (size_t j = 0; j < tensor_res[i].size(); ++j) {
            if(tensor_res[i][j].defined()){
                res[i][j] = std::static_pointer_cast<primitive::LazyTensor>(tensor_res[i][j].impl())->value().dyn_cast<pir::OpResult>();
            }
        }
    }
    return res;
}

std::vector<std::vector<pir::OpResult>> FlashAttnUnpaddedOp::Vjp(pir::Operation* op, const std::vector<std::vector<pir::Value>>& inputs_, const std::vector<std::vector<pir::Value>>& outputs, const std::vector<std::vector<pir::Value>>& out_grads, const std::vector<std::vector<bool>>& stop_gradients){

    PADDLE_ENFORCE_EQ(
      inputs_.size(),
      7,
      platform::errors::InvalidArgument("flash_attn_unpadded op's inputs size should be 7, but now is %d.", inputs_.size()));
    PADDLE_ENFORCE_EQ(
      outputs.size(),
      4,
      platform::errors::InvalidArgument("flash_attn_unpadded op's outputs size should be 4, but now is %d.", outputs.size()));

    VLOG(6) << "Prepare inputs of flash_attn_unpadded_grad";

    Tensor q(std::make_shared<primitive::LazyTensor>(inputs_[0][0]));
    Tensor k(std::make_shared<primitive::LazyTensor>(inputs_[1][0]));
    Tensor v(std::make_shared<primitive::LazyTensor>(inputs_[2][0]));
    Tensor cu_seqlens_q(std::make_shared<primitive::LazyTensor>(inputs_[3][0]));
    Tensor cu_seqlens_k(std::make_shared<primitive::LazyTensor>(inputs_[4][0]));
    Tensor out(std::make_shared<primitive::LazyTensor>(outputs[0][0]));
    Tensor softmax_lse(std::make_shared<primitive::LazyTensor>(outputs[2][0]));
    Tensor seed_offset(std::make_shared<primitive::LazyTensor>(outputs[3][0]));
    paddle::optional<Tensor> attn_mask;
    if (!IsEmptyValue(inputs_[6][0])){
        attn_mask = paddle::make_optional<Tensor>(Tensor(std::make_shared<primitive::LazyTensor>(inputs_[6][0])));
    }
    Tensor out_grad(std::make_shared<primitive::LazyTensor>(out_grads[0][0]));

    VLOG(6) << "Vjp prepare Prepare attributes of flash_attn_unpadded_grad";

    int64_t max_seqlen_q = op->attribute("max_seqlen_q").dyn_cast<pir::Int64Attribute>().data();
    int64_t max_seqlen_k = op->attribute("max_seqlen_k").dyn_cast<pir::Int64Attribute>().data();
    float scale = op->attribute("scale").dyn_cast<pir::FloatAttribute>().data();
    float dropout = op->attribute("dropout").dyn_cast<pir::FloatAttribute>().data();
    bool causal = op->attribute("causal").dyn_cast<pir::BoolAttribute>().data();

    VLOG(6) << "Vjp prepare call flash_attn_unpadded's vjp inteface";

    std::vector<std::vector<Tensor>> tensor_res =
        primitive::flash_attn_unpadded_vjp(
        q, k, v, cu_seqlens_q, cu_seqlens_k, out, softmax_lse, seed_offset, attn_mask, out_grad, max_seqlen_q, max_seqlen_k, scale, dropout, causal, stop_gradients);

    VLOG(6) << "Vjp prepare stop gradient of flash_attn_unpadded_grad";

    std::vector<std::vector<pir::OpResult>> res(tensor_res.size());
    for (size_t i = 0; i < tensor_res.size(); ++i) {
        res[i].resize(tensor_res[i].size());
        for (size_t j = 0; j < tensor_res[i].size(); ++j) {
            if(tensor_res[i][j].defined()){
                res[i][j] = std::static_pointer_cast<primitive::LazyTensor>(tensor_res[i][j].impl())->value().dyn_cast<pir::OpResult>();
            }
        }
    }
    return res;
}

std::vector<std::vector<pir::OpResult>> FlattenOp::Vjp(pir::Operation* op, const std::vector<std::vector<pir::Value>>& inputs_, const std::vector<std::vector<pir::Value>>& outputs, const std::vector<std::vector<pir::Value>>& out_grads, const std::vector<std::vector<bool>>& stop_gradients){

    PADDLE_ENFORCE_EQ(
      inputs_.size(),
      1,
      platform::errors::InvalidArgument("flatten op's inputs size should be 1, but now is %d.", inputs_.size()));
    PADDLE_ENFORCE_EQ(
      outputs.size(),
      2,
      platform::errors::InvalidArgument("flatten op's outputs size should be 2, but now is %d.", outputs.size()));

    VLOG(6) << "Prepare inputs of flatten_grad";

    Tensor xshape(std::make_shared<primitive::LazyTensor>(outputs[1][0]));
    Tensor out_grad(std::make_shared<primitive::LazyTensor>(out_grads[0][0]));

    VLOG(6) << "Vjp prepare Prepare attributes of flatten_grad";


    VLOG(6) << "Vjp prepare call flatten's vjp inteface";

    std::vector<std::vector<Tensor>> tensor_res =
        primitive::flatten_vjp(
        xshape, out_grad, stop_gradients);

    VLOG(6) << "Vjp prepare stop gradient of flatten_grad";

    std::vector<std::vector<pir::OpResult>> res(tensor_res.size());
    for (size_t i = 0; i < tensor_res.size(); ++i) {
        res[i].resize(tensor_res[i].size());
        for (size_t j = 0; j < tensor_res[i].size(); ++j) {
            if(tensor_res[i][j].defined()){
                res[i][j] = std::static_pointer_cast<primitive::LazyTensor>(tensor_res[i][j].impl())->value().dyn_cast<pir::OpResult>();
            }
        }
    }
    return res;
}

std::vector<std::vector<pir::OpResult>> Flatten_Op::Vjp(pir::Operation* op, const std::vector<std::vector<pir::Value>>& inputs_, const std::vector<std::vector<pir::Value>>& outputs, const std::vector<std::vector<pir::Value>>& out_grads, const std::vector<std::vector<bool>>& stop_gradients){

    PADDLE_ENFORCE_EQ(
      inputs_.size(),
      1,
      platform::errors::InvalidArgument("flatten op's inputs size should be 1, but now is %d.", inputs_.size()));
    PADDLE_ENFORCE_EQ(
      outputs.size(),
      2,
      platform::errors::InvalidArgument("flatten op's outputs size should be 2, but now is %d.", outputs.size()));

    VLOG(6) << "Prepare inputs of flatten_grad";

    Tensor xshape(std::make_shared<primitive::LazyTensor>(outputs[1][0]));
    Tensor out_grad(std::make_shared<primitive::LazyTensor>(out_grads[0][0]));

    VLOG(6) << "Vjp prepare Prepare attributes of flatten_grad";


    VLOG(6) << "Vjp prepare call flatten_'s vjp inteface";

    std::vector<std::vector<Tensor>> tensor_res =
        primitive::flatten_vjp(
        xshape, out_grad, stop_gradients);

    VLOG(6) << "Vjp prepare stop gradient of flatten_grad";

    std::vector<std::vector<pir::OpResult>> res(tensor_res.size());
    for (size_t i = 0; i < tensor_res.size(); ++i) {
        res[i].resize(tensor_res[i].size());
        for (size_t j = 0; j < tensor_res[i].size(); ++j) {
            if(tensor_res[i][j].defined()){
                res[i][j] = std::static_pointer_cast<primitive::LazyTensor>(tensor_res[i][j].impl())->value().dyn_cast<pir::OpResult>();
            }
        }
    }
    return res;
}

std::vector<std::vector<pir::OpResult>> FlipOp::Vjp(pir::Operation* op, const std::vector<std::vector<pir::Value>>& inputs_, const std::vector<std::vector<pir::Value>>& outputs, const std::vector<std::vector<pir::Value>>& out_grads, const std::vector<std::vector<bool>>& stop_gradients){

    PADDLE_ENFORCE_EQ(
      inputs_.size(),
      1,
      platform::errors::InvalidArgument("flip op's inputs size should be 1, but now is %d.", inputs_.size()));
    PADDLE_ENFORCE_EQ(
      outputs.size(),
      1,
      platform::errors::InvalidArgument("flip op's outputs size should be 1, but now is %d.", outputs.size()));

    VLOG(6) << "Prepare inputs of flip_grad";

    Tensor out_grad(std::make_shared<primitive::LazyTensor>(out_grads[0][0]));

    VLOG(6) << "Vjp prepare Prepare attributes of flip_grad";

    std::vector<int> axis;
    for (size_t i = 0; i < op->attribute("axis").dyn_cast<pir::ArrayAttribute>().size(); i++) {
        axis.push_back(op->attribute("axis").dyn_cast<pir::ArrayAttribute>().at(i).dyn_cast<pir::Int32Attribute>().data());
    }

    VLOG(6) << "Vjp prepare call flip's vjp inteface";

    std::vector<std::vector<Tensor>> tensor_res =
        primitive::flip_vjp(
        out_grad, axis, stop_gradients);

    VLOG(6) << "Vjp prepare stop gradient of flip_grad";

    std::vector<std::vector<pir::OpResult>> res(tensor_res.size());
    for (size_t i = 0; i < tensor_res.size(); ++i) {
        res[i].resize(tensor_res[i].size());
        for (size_t j = 0; j < tensor_res[i].size(); ++j) {
            if(tensor_res[i][j].defined()){
                res[i][j] = std::static_pointer_cast<primitive::LazyTensor>(tensor_res[i][j].impl())->value().dyn_cast<pir::OpResult>();
            }
        }
    }
    return res;
}

std::vector<std::vector<pir::OpResult>> FloorOp::Vjp(pir::Operation* op, const std::vector<std::vector<pir::Value>>& inputs_, const std::vector<std::vector<pir::Value>>& outputs, const std::vector<std::vector<pir::Value>>& out_grads, const std::vector<std::vector<bool>>& stop_gradients){

    PADDLE_ENFORCE_EQ(
      inputs_.size(),
      1,
      platform::errors::InvalidArgument("floor op's inputs size should be 1, but now is %d.", inputs_.size()));
    PADDLE_ENFORCE_EQ(
      outputs.size(),
      1,
      platform::errors::InvalidArgument("floor op's outputs size should be 1, but now is %d.", outputs.size()));

    VLOG(6) << "Prepare inputs of floor_grad";

    Tensor out_grad(std::make_shared<primitive::LazyTensor>(out_grads[0][0]));

    VLOG(6) << "Vjp prepare Prepare attributes of floor_grad";


    VLOG(6) << "Vjp prepare call floor's vjp inteface";

    std::vector<std::vector<Tensor>> tensor_res =
        primitive::floor_vjp(
        out_grad, stop_gradients);

    VLOG(6) << "Vjp prepare stop gradient of floor_grad";

    std::vector<std::vector<pir::OpResult>> res(tensor_res.size());
    for (size_t i = 0; i < tensor_res.size(); ++i) {
        res[i].resize(tensor_res[i].size());
        for (size_t j = 0; j < tensor_res[i].size(); ++j) {
            if(tensor_res[i][j].defined()){
                res[i][j] = std::static_pointer_cast<primitive::LazyTensor>(tensor_res[i][j].impl())->value().dyn_cast<pir::OpResult>();
            }
        }
    }
    return res;
}

std::vector<std::vector<pir::OpResult>> Floor_Op::Vjp(pir::Operation* op, const std::vector<std::vector<pir::Value>>& inputs_, const std::vector<std::vector<pir::Value>>& outputs, const std::vector<std::vector<pir::Value>>& out_grads, const std::vector<std::vector<bool>>& stop_gradients){

    PADDLE_ENFORCE_EQ(
      inputs_.size(),
      1,
      platform::errors::InvalidArgument("floor op's inputs size should be 1, but now is %d.", inputs_.size()));
    PADDLE_ENFORCE_EQ(
      outputs.size(),
      1,
      platform::errors::InvalidArgument("floor op's outputs size should be 1, but now is %d.", outputs.size()));

    VLOG(6) << "Prepare inputs of floor_grad";

    Tensor out_grad(std::make_shared<primitive::LazyTensor>(out_grads[0][0]));

    VLOG(6) << "Vjp prepare Prepare attributes of floor_grad";


    VLOG(6) << "Vjp prepare call floor_'s vjp inteface";

    std::vector<std::vector<Tensor>> tensor_res =
        primitive::floor_vjp(
        out_grad, stop_gradients);

    VLOG(6) << "Vjp prepare stop gradient of floor_grad";

    std::vector<std::vector<pir::OpResult>> res(tensor_res.size());
    for (size_t i = 0; i < tensor_res.size(); ++i) {
        res[i].resize(tensor_res[i].size());
        for (size_t j = 0; j < tensor_res[i].size(); ++j) {
            if(tensor_res[i][j].defined()){
                res[i][j] = std::static_pointer_cast<primitive::LazyTensor>(tensor_res[i][j].impl())->value().dyn_cast<pir::OpResult>();
            }
        }
    }
    return res;
}

std::vector<std::vector<pir::OpResult>> FmaxOp::Vjp(pir::Operation* op, const std::vector<std::vector<pir::Value>>& inputs_, const std::vector<std::vector<pir::Value>>& outputs, const std::vector<std::vector<pir::Value>>& out_grads, const std::vector<std::vector<bool>>& stop_gradients){

    PADDLE_ENFORCE_EQ(
      inputs_.size(),
      2,
      platform::errors::InvalidArgument("fmax op's inputs size should be 2, but now is %d.", inputs_.size()));
    PADDLE_ENFORCE_EQ(
      outputs.size(),
      1,
      platform::errors::InvalidArgument("fmax op's outputs size should be 1, but now is %d.", outputs.size()));

    VLOG(6) << "Prepare inputs of fmax_grad";

    Tensor x(std::make_shared<primitive::LazyTensor>(inputs_[0][0]));
    Tensor y(std::make_shared<primitive::LazyTensor>(inputs_[1][0]));
    Tensor out_grad(std::make_shared<primitive::LazyTensor>(out_grads[0][0]));

    VLOG(6) << "Vjp prepare Prepare attributes of fmax_grad";


    VLOG(6) << "Vjp prepare call fmax's vjp inteface";

    std::vector<std::vector<Tensor>> tensor_res =
        primitive::fmax_vjp(
        x, y, out_grad, stop_gradients);

    VLOG(6) << "Vjp prepare stop gradient of fmax_grad";

    std::vector<std::vector<pir::OpResult>> res(tensor_res.size());
    for (size_t i = 0; i < tensor_res.size(); ++i) {
        res[i].resize(tensor_res[i].size());
        for (size_t j = 0; j < tensor_res[i].size(); ++j) {
            if(tensor_res[i][j].defined()){
                res[i][j] = std::static_pointer_cast<primitive::LazyTensor>(tensor_res[i][j].impl())->value().dyn_cast<pir::OpResult>();
            }
        }
    }
    return res;
}

std::vector<std::vector<pir::OpResult>> FminOp::Vjp(pir::Operation* op, const std::vector<std::vector<pir::Value>>& inputs_, const std::vector<std::vector<pir::Value>>& outputs, const std::vector<std::vector<pir::Value>>& out_grads, const std::vector<std::vector<bool>>& stop_gradients){

    PADDLE_ENFORCE_EQ(
      inputs_.size(),
      2,
      platform::errors::InvalidArgument("fmin op's inputs size should be 2, but now is %d.", inputs_.size()));
    PADDLE_ENFORCE_EQ(
      outputs.size(),
      1,
      platform::errors::InvalidArgument("fmin op's outputs size should be 1, but now is %d.", outputs.size()));

    VLOG(6) << "Prepare inputs of fmin_grad";

    Tensor x(std::make_shared<primitive::LazyTensor>(inputs_[0][0]));
    Tensor y(std::make_shared<primitive::LazyTensor>(inputs_[1][0]));
    Tensor out_grad(std::make_shared<primitive::LazyTensor>(out_grads[0][0]));

    VLOG(6) << "Vjp prepare Prepare attributes of fmin_grad";


    VLOG(6) << "Vjp prepare call fmin's vjp inteface";

    std::vector<std::vector<Tensor>> tensor_res =
        primitive::fmin_vjp(
        x, y, out_grad, stop_gradients);

    VLOG(6) << "Vjp prepare stop gradient of fmin_grad";

    std::vector<std::vector<pir::OpResult>> res(tensor_res.size());
    for (size_t i = 0; i < tensor_res.size(); ++i) {
        res[i].resize(tensor_res[i].size());
        for (size_t j = 0; j < tensor_res[i].size(); ++j) {
            if(tensor_res[i][j].defined()){
                res[i][j] = std::static_pointer_cast<primitive::LazyTensor>(tensor_res[i][j].impl())->value().dyn_cast<pir::OpResult>();
            }
        }
    }
    return res;
}

std::vector<std::vector<pir::OpResult>> FoldOp::Vjp(pir::Operation* op, const std::vector<std::vector<pir::Value>>& inputs_, const std::vector<std::vector<pir::Value>>& outputs, const std::vector<std::vector<pir::Value>>& out_grads, const std::vector<std::vector<bool>>& stop_gradients){

    PADDLE_ENFORCE_EQ(
      inputs_.size(),
      1,
      platform::errors::InvalidArgument("fold op's inputs size should be 1, but now is %d.", inputs_.size()));
    PADDLE_ENFORCE_EQ(
      outputs.size(),
      1,
      platform::errors::InvalidArgument("fold op's outputs size should be 1, but now is %d.", outputs.size()));

    VLOG(6) << "Prepare inputs of fold_grad";

    Tensor x(std::make_shared<primitive::LazyTensor>(inputs_[0][0]));
    Tensor out_grad(std::make_shared<primitive::LazyTensor>(out_grads[0][0]));

    VLOG(6) << "Vjp prepare Prepare attributes of fold_grad";

    std::vector<int> output_sizes;
    for (size_t i = 0; i < op->attribute("output_sizes").dyn_cast<pir::ArrayAttribute>().size(); i++) {
        output_sizes.push_back(op->attribute("output_sizes").dyn_cast<pir::ArrayAttribute>().at(i).dyn_cast<pir::Int32Attribute>().data());
    }
    std::vector<int> kernel_sizes;
    for (size_t i = 0; i < op->attribute("kernel_sizes").dyn_cast<pir::ArrayAttribute>().size(); i++) {
        kernel_sizes.push_back(op->attribute("kernel_sizes").dyn_cast<pir::ArrayAttribute>().at(i).dyn_cast<pir::Int32Attribute>().data());
    }
    std::vector<int> strides;
    for (size_t i = 0; i < op->attribute("strides").dyn_cast<pir::ArrayAttribute>().size(); i++) {
        strides.push_back(op->attribute("strides").dyn_cast<pir::ArrayAttribute>().at(i).dyn_cast<pir::Int32Attribute>().data());
    }
    std::vector<int> paddings;
    for (size_t i = 0; i < op->attribute("paddings").dyn_cast<pir::ArrayAttribute>().size(); i++) {
        paddings.push_back(op->attribute("paddings").dyn_cast<pir::ArrayAttribute>().at(i).dyn_cast<pir::Int32Attribute>().data());
    }
    std::vector<int> dilations;
    for (size_t i = 0; i < op->attribute("dilations").dyn_cast<pir::ArrayAttribute>().size(); i++) {
        dilations.push_back(op->attribute("dilations").dyn_cast<pir::ArrayAttribute>().at(i).dyn_cast<pir::Int32Attribute>().data());
    }

    VLOG(6) << "Vjp prepare call fold's vjp inteface";

    std::vector<std::vector<Tensor>> tensor_res =
        primitive::fold_vjp(
        x, out_grad, output_sizes, kernel_sizes, strides, paddings, dilations, stop_gradients);

    VLOG(6) << "Vjp prepare stop gradient of fold_grad";

    std::vector<std::vector<pir::OpResult>> res(tensor_res.size());
    for (size_t i = 0; i < tensor_res.size(); ++i) {
        res[i].resize(tensor_res[i].size());
        for (size_t j = 0; j < tensor_res[i].size(); ++j) {
            if(tensor_res[i][j].defined()){
                res[i][j] = std::static_pointer_cast<primitive::LazyTensor>(tensor_res[i][j].impl())->value().dyn_cast<pir::OpResult>();
            }
        }
    }
    return res;
}

std::vector<std::vector<pir::OpResult>> FrameOp::Vjp(pir::Operation* op, const std::vector<std::vector<pir::Value>>& inputs_, const std::vector<std::vector<pir::Value>>& outputs, const std::vector<std::vector<pir::Value>>& out_grads, const std::vector<std::vector<bool>>& stop_gradients){

    PADDLE_ENFORCE_EQ(
      inputs_.size(),
      1,
      platform::errors::InvalidArgument("frame op's inputs size should be 1, but now is %d.", inputs_.size()));
    PADDLE_ENFORCE_EQ(
      outputs.size(),
      1,
      platform::errors::InvalidArgument("frame op's outputs size should be 1, but now is %d.", outputs.size()));

    VLOG(6) << "Prepare inputs of frame_grad";

    Tensor x(std::make_shared<primitive::LazyTensor>(inputs_[0][0]));
    Tensor out_grad(std::make_shared<primitive::LazyTensor>(out_grads[0][0]));

    VLOG(6) << "Vjp prepare Prepare attributes of frame_grad";

    int frame_length = op->attribute("frame_length").dyn_cast<pir::Int32Attribute>().data();
    int hop_length = op->attribute("hop_length").dyn_cast<pir::Int32Attribute>().data();
    int axis = op->attribute("axis").dyn_cast<pir::Int32Attribute>().data();

    VLOG(6) << "Vjp prepare call frame's vjp inteface";

    std::vector<std::vector<Tensor>> tensor_res =
        primitive::frame_vjp(
        x, out_grad, frame_length, hop_length, axis, stop_gradients);

    VLOG(6) << "Vjp prepare stop gradient of frame_grad";

    std::vector<std::vector<pir::OpResult>> res(tensor_res.size());
    for (size_t i = 0; i < tensor_res.size(); ++i) {
        res[i].resize(tensor_res[i].size());
        for (size_t j = 0; j < tensor_res[i].size(); ++j) {
            if(tensor_res[i][j].defined()){
                res[i][j] = std::static_pointer_cast<primitive::LazyTensor>(tensor_res[i][j].impl())->value().dyn_cast<pir::OpResult>();
            }
        }
    }
    return res;
}

std::vector<std::vector<pir::OpResult>> GammalnOp::Vjp(pir::Operation* op, const std::vector<std::vector<pir::Value>>& inputs_, const std::vector<std::vector<pir::Value>>& outputs, const std::vector<std::vector<pir::Value>>& out_grads, const std::vector<std::vector<bool>>& stop_gradients){

    PADDLE_ENFORCE_EQ(
      inputs_.size(),
      1,
      platform::errors::InvalidArgument("gammaln op's inputs size should be 1, but now is %d.", inputs_.size()));
    PADDLE_ENFORCE_EQ(
      outputs.size(),
      1,
      platform::errors::InvalidArgument("gammaln op's outputs size should be 1, but now is %d.", outputs.size()));

    VLOG(6) << "Prepare inputs of gammaln_grad";

    Tensor x(std::make_shared<primitive::LazyTensor>(inputs_[0][0]));
    Tensor out_grad(std::make_shared<primitive::LazyTensor>(out_grads[0][0]));

    VLOG(6) << "Vjp prepare Prepare attributes of gammaln_grad";


    VLOG(6) << "Vjp prepare call gammaln's vjp inteface";

    std::vector<std::vector<Tensor>> tensor_res =
        primitive::gammaln_vjp(
        x, out_grad, stop_gradients);

    VLOG(6) << "Vjp prepare stop gradient of gammaln_grad";

    std::vector<std::vector<pir::OpResult>> res(tensor_res.size());
    for (size_t i = 0; i < tensor_res.size(); ++i) {
        res[i].resize(tensor_res[i].size());
        for (size_t j = 0; j < tensor_res[i].size(); ++j) {
            if(tensor_res[i][j].defined()){
                res[i][j] = std::static_pointer_cast<primitive::LazyTensor>(tensor_res[i][j].impl())->value().dyn_cast<pir::OpResult>();
            }
        }
    }
    return res;
}

std::vector<std::vector<pir::OpResult>> Gammaln_Op::Vjp(pir::Operation* op, const std::vector<std::vector<pir::Value>>& inputs_, const std::vector<std::vector<pir::Value>>& outputs, const std::vector<std::vector<pir::Value>>& out_grads, const std::vector<std::vector<bool>>& stop_gradients){

    PADDLE_ENFORCE_EQ(
      inputs_.size(),
      1,
      platform::errors::InvalidArgument("gammaln op's inputs size should be 1, but now is %d.", inputs_.size()));
    PADDLE_ENFORCE_EQ(
      outputs.size(),
      1,
      platform::errors::InvalidArgument("gammaln op's outputs size should be 1, but now is %d.", outputs.size()));

    VLOG(6) << "Prepare inputs of gammaln_grad";

    Tensor x(std::make_shared<primitive::LazyTensor>(inputs_[0][0]));
    Tensor out_grad(std::make_shared<primitive::LazyTensor>(out_grads[0][0]));

    VLOG(6) << "Vjp prepare Prepare attributes of gammaln_grad";


    VLOG(6) << "Vjp prepare call gammaln_'s vjp inteface";

    std::vector<std::vector<Tensor>> tensor_res =
        primitive::gammaln_vjp(
        x, out_grad, stop_gradients);

    VLOG(6) << "Vjp prepare stop gradient of gammaln_grad";

    std::vector<std::vector<pir::OpResult>> res(tensor_res.size());
    for (size_t i = 0; i < tensor_res.size(); ++i) {
        res[i].resize(tensor_res[i].size());
        for (size_t j = 0; j < tensor_res[i].size(); ++j) {
            if(tensor_res[i][j].defined()){
                res[i][j] = std::static_pointer_cast<primitive::LazyTensor>(tensor_res[i][j].impl())->value().dyn_cast<pir::OpResult>();
            }
        }
    }
    return res;
}

std::vector<std::vector<pir::OpResult>> GatherOp::Vjp(pir::Operation* op, const std::vector<std::vector<pir::Value>>& inputs_, const std::vector<std::vector<pir::Value>>& outputs, const std::vector<std::vector<pir::Value>>& out_grads, const std::vector<std::vector<bool>>& stop_gradients){

    PADDLE_ENFORCE_EQ(
      inputs_.size(),
      3,
      platform::errors::InvalidArgument("gather op's inputs size should be 3, but now is %d.", inputs_.size()));
    PADDLE_ENFORCE_EQ(
      outputs.size(),
      1,
      platform::errors::InvalidArgument("gather op's outputs size should be 1, but now is %d.", outputs.size()));

    VLOG(6) << "Prepare inputs of gather_grad";

    Tensor x(std::make_shared<primitive::LazyTensor>(inputs_[0][0]));
    Tensor index(std::make_shared<primitive::LazyTensor>(inputs_[1][0]));
    Tensor out_grad(std::make_shared<primitive::LazyTensor>(out_grads[0][0]));

    VLOG(6) << "Vjp prepare Prepare attributes of gather_grad";

    Tensor axis(std::make_shared<primitive::LazyTensor>(inputs_[2][0]));

    VLOG(6) << "Vjp prepare call gather's vjp inteface";

    std::vector<std::vector<Tensor>> tensor_res =
        primitive::gather_vjp(
        x, index, out_grad, axis, stop_gradients);

    VLOG(6) << "Vjp prepare stop gradient of gather_grad";

    std::vector<std::vector<pir::OpResult>> res(tensor_res.size());
    for (size_t i = 0; i < tensor_res.size(); ++i) {
        res[i].resize(tensor_res[i].size());
        for (size_t j = 0; j < tensor_res[i].size(); ++j) {
            if(tensor_res[i][j].defined()){
                res[i][j] = std::static_pointer_cast<primitive::LazyTensor>(tensor_res[i][j].impl())->value().dyn_cast<pir::OpResult>();
            }
        }
    }
    return res;
}

std::vector<std::vector<pir::OpResult>> GatherNdOp::Vjp(pir::Operation* op, const std::vector<std::vector<pir::Value>>& inputs_, const std::vector<std::vector<pir::Value>>& outputs, const std::vector<std::vector<pir::Value>>& out_grads, const std::vector<std::vector<bool>>& stop_gradients){

    PADDLE_ENFORCE_EQ(
      inputs_.size(),
      2,
      platform::errors::InvalidArgument("gather_nd op's inputs size should be 2, but now is %d.", inputs_.size()));
    PADDLE_ENFORCE_EQ(
      outputs.size(),
      1,
      platform::errors::InvalidArgument("gather_nd op's outputs size should be 1, but now is %d.", outputs.size()));

    VLOG(6) << "Prepare inputs of gather_nd_grad";

    Tensor x(std::make_shared<primitive::LazyTensor>(inputs_[0][0]));
    Tensor index(std::make_shared<primitive::LazyTensor>(inputs_[1][0]));
    Tensor out_grad(std::make_shared<primitive::LazyTensor>(out_grads[0][0]));

    VLOG(6) << "Vjp prepare Prepare attributes of gather_nd_grad";


    VLOG(6) << "Vjp prepare call gather_nd's vjp inteface";

    std::vector<std::vector<Tensor>> tensor_res =
        primitive::gather_nd_vjp(
        x, index, out_grad, stop_gradients);

    VLOG(6) << "Vjp prepare stop gradient of gather_nd_grad";

    std::vector<std::vector<pir::OpResult>> res(tensor_res.size());
    for (size_t i = 0; i < tensor_res.size(); ++i) {
        res[i].resize(tensor_res[i].size());
        for (size_t j = 0; j < tensor_res[i].size(); ++j) {
            if(tensor_res[i][j].defined()){
                res[i][j] = std::static_pointer_cast<primitive::LazyTensor>(tensor_res[i][j].impl())->value().dyn_cast<pir::OpResult>();
            }
        }
    }
    return res;
}

std::vector<std::vector<pir::OpResult>> GaussianInplaceOp::Vjp(pir::Operation* op, const std::vector<std::vector<pir::Value>>& inputs_, const std::vector<std::vector<pir::Value>>& outputs, const std::vector<std::vector<pir::Value>>& out_grads, const std::vector<std::vector<bool>>& stop_gradients){

    PADDLE_ENFORCE_EQ(
      inputs_.size(),
      1,
      platform::errors::InvalidArgument("gaussian_inplace op's inputs size should be 1, but now is %d.", inputs_.size()));
    PADDLE_ENFORCE_EQ(
      outputs.size(),
      1,
      platform::errors::InvalidArgument("gaussian_inplace op's outputs size should be 1, but now is %d.", outputs.size()));

    VLOG(6) << "Prepare inputs of gaussian_inplace_grad";

    Tensor out_grad(std::make_shared<primitive::LazyTensor>(out_grads[0][0]));

    VLOG(6) << "Vjp prepare Prepare attributes of gaussian_inplace_grad";

    float mean = op->attribute("mean").dyn_cast<pir::FloatAttribute>().data();
    float std = op->attribute("std").dyn_cast<pir::FloatAttribute>().data();
    int seed = op->attribute("seed").dyn_cast<pir::Int32Attribute>().data();

    VLOG(6) << "Vjp prepare call gaussian_inplace's vjp inteface";

    std::vector<std::vector<Tensor>> tensor_res =
        primitive::gaussian_inplace_vjp(
        out_grad, mean, std, seed, stop_gradients);

    VLOG(6) << "Vjp prepare stop gradient of gaussian_inplace_grad";

    std::vector<std::vector<pir::OpResult>> res(tensor_res.size());
    for (size_t i = 0; i < tensor_res.size(); ++i) {
        res[i].resize(tensor_res[i].size());
        for (size_t j = 0; j < tensor_res[i].size(); ++j) {
            if(tensor_res[i][j].defined()){
                res[i][j] = std::static_pointer_cast<primitive::LazyTensor>(tensor_res[i][j].impl())->value().dyn_cast<pir::OpResult>();
            }
        }
    }
    return res;
}

std::vector<std::vector<pir::OpResult>> GaussianInplace_Op::Vjp(pir::Operation* op, const std::vector<std::vector<pir::Value>>& inputs_, const std::vector<std::vector<pir::Value>>& outputs, const std::vector<std::vector<pir::Value>>& out_grads, const std::vector<std::vector<bool>>& stop_gradients){

    PADDLE_ENFORCE_EQ(
      inputs_.size(),
      1,
      platform::errors::InvalidArgument("gaussian_inplace op's inputs size should be 1, but now is %d.", inputs_.size()));
    PADDLE_ENFORCE_EQ(
      outputs.size(),
      1,
      platform::errors::InvalidArgument("gaussian_inplace op's outputs size should be 1, but now is %d.", outputs.size()));

    VLOG(6) << "Prepare inputs of gaussian_inplace_grad";

    Tensor out_grad(std::make_shared<primitive::LazyTensor>(out_grads[0][0]));

    VLOG(6) << "Vjp prepare Prepare attributes of gaussian_inplace_grad";

    float mean = op->attribute("mean").dyn_cast<pir::FloatAttribute>().data();
    float std = op->attribute("std").dyn_cast<pir::FloatAttribute>().data();
    int seed = op->attribute("seed").dyn_cast<pir::Int32Attribute>().data();

    VLOG(6) << "Vjp prepare call gaussian_inplace_'s vjp inteface";

    std::vector<std::vector<Tensor>> tensor_res =
        primitive::gaussian_inplace_vjp(
        out_grad, mean, std, seed, stop_gradients);

    VLOG(6) << "Vjp prepare stop gradient of gaussian_inplace_grad";

    std::vector<std::vector<pir::OpResult>> res(tensor_res.size());
    for (size_t i = 0; i < tensor_res.size(); ++i) {
        res[i].resize(tensor_res[i].size());
        for (size_t j = 0; j < tensor_res[i].size(); ++j) {
            if(tensor_res[i][j].defined()){
                res[i][j] = std::static_pointer_cast<primitive::LazyTensor>(tensor_res[i][j].impl())->value().dyn_cast<pir::OpResult>();
            }
        }
    }
    return res;
}

std::vector<std::vector<pir::OpResult>> GeluOp::Vjp(pir::Operation* op, const std::vector<std::vector<pir::Value>>& inputs_, const std::vector<std::vector<pir::Value>>& outputs, const std::vector<std::vector<pir::Value>>& out_grads, const std::vector<std::vector<bool>>& stop_gradients){

    PADDLE_ENFORCE_EQ(
      inputs_.size(),
      1,
      platform::errors::InvalidArgument("gelu op's inputs size should be 1, but now is %d.", inputs_.size()));
    PADDLE_ENFORCE_EQ(
      outputs.size(),
      1,
      platform::errors::InvalidArgument("gelu op's outputs size should be 1, but now is %d.", outputs.size()));

    VLOG(6) << "Prepare inputs of gelu_grad";

    Tensor x(std::make_shared<primitive::LazyTensor>(inputs_[0][0]));
    Tensor out_grad(std::make_shared<primitive::LazyTensor>(out_grads[0][0]));

    VLOG(6) << "Vjp prepare Prepare attributes of gelu_grad";

    bool approximate = op->attribute("approximate").dyn_cast<pir::BoolAttribute>().data();

    VLOG(6) << "Vjp prepare call gelu's vjp inteface";

    std::vector<std::vector<Tensor>> tensor_res =
        primitive::gelu_vjp(
        x, out_grad, approximate, stop_gradients);

    VLOG(6) << "Vjp prepare stop gradient of gelu_grad";

    std::vector<std::vector<pir::OpResult>> res(tensor_res.size());
    for (size_t i = 0; i < tensor_res.size(); ++i) {
        res[i].resize(tensor_res[i].size());
        for (size_t j = 0; j < tensor_res[i].size(); ++j) {
            if(tensor_res[i][j].defined()){
                res[i][j] = std::static_pointer_cast<primitive::LazyTensor>(tensor_res[i][j].impl())->value().dyn_cast<pir::OpResult>();
            }
        }
    }
    return res;
}

std::vector<std::vector<pir::OpResult>> GridSampleOp::Vjp(pir::Operation* op, const std::vector<std::vector<pir::Value>>& inputs_, const std::vector<std::vector<pir::Value>>& outputs, const std::vector<std::vector<pir::Value>>& out_grads, const std::vector<std::vector<bool>>& stop_gradients){

    PADDLE_ENFORCE_EQ(
      inputs_.size(),
      2,
      platform::errors::InvalidArgument("grid_sample op's inputs size should be 2, but now is %d.", inputs_.size()));
    PADDLE_ENFORCE_EQ(
      outputs.size(),
      1,
      platform::errors::InvalidArgument("grid_sample op's outputs size should be 1, but now is %d.", outputs.size()));

    VLOG(6) << "Prepare inputs of grid_sample_grad";

    Tensor x(std::make_shared<primitive::LazyTensor>(inputs_[0][0]));
    Tensor grid(std::make_shared<primitive::LazyTensor>(inputs_[1][0]));
    Tensor out_grad(std::make_shared<primitive::LazyTensor>(out_grads[0][0]));

    VLOG(6) << "Vjp prepare Prepare attributes of grid_sample_grad";

    std::string mode = op->attribute("mode").dyn_cast<pir::StrAttribute>().AsString();
    std::string padding_mode = op->attribute("padding_mode").dyn_cast<pir::StrAttribute>().AsString();
    bool align_corners = op->attribute("align_corners").dyn_cast<pir::BoolAttribute>().data();

    VLOG(6) << "Vjp prepare call grid_sample's vjp inteface";

    std::vector<std::vector<Tensor>> tensor_res =
        primitive::grid_sample_vjp(
        x, grid, out_grad, mode, padding_mode, align_corners, stop_gradients);

    VLOG(6) << "Vjp prepare stop gradient of grid_sample_grad";

    std::vector<std::vector<pir::OpResult>> res(tensor_res.size());
    for (size_t i = 0; i < tensor_res.size(); ++i) {
        res[i].resize(tensor_res[i].size());
        for (size_t j = 0; j < tensor_res[i].size(); ++j) {
            if(tensor_res[i][j].defined()){
                res[i][j] = std::static_pointer_cast<primitive::LazyTensor>(tensor_res[i][j].impl())->value().dyn_cast<pir::OpResult>();
            }
        }
    }
    return res;
}

std::vector<std::vector<pir::OpResult>> GroupNormOp::Vjp(pir::Operation* op, const std::vector<std::vector<pir::Value>>& inputs_, const std::vector<std::vector<pir::Value>>& outputs, const std::vector<std::vector<pir::Value>>& out_grads, const std::vector<std::vector<bool>>& stop_gradients){

    PADDLE_ENFORCE_EQ(
      inputs_.size(),
      3,
      platform::errors::InvalidArgument("group_norm op's inputs size should be 3, but now is %d.", inputs_.size()));
    PADDLE_ENFORCE_EQ(
      outputs.size(),
      3,
      platform::errors::InvalidArgument("group_norm op's outputs size should be 3, but now is %d.", outputs.size()));

    VLOG(6) << "Prepare inputs of group_norm_grad";

    Tensor x(std::make_shared<primitive::LazyTensor>(inputs_[0][0]));
    paddle::optional<Tensor> scale;
    if (!IsEmptyValue(inputs_[1][0])){
        scale = paddle::make_optional<Tensor>(Tensor(std::make_shared<primitive::LazyTensor>(inputs_[1][0])));
    }
    paddle::optional<Tensor> bias;
    if (!IsEmptyValue(inputs_[2][0])){
        bias = paddle::make_optional<Tensor>(Tensor(std::make_shared<primitive::LazyTensor>(inputs_[2][0])));
    }
    Tensor y(std::make_shared<primitive::LazyTensor>(outputs[0][0]));
    Tensor mean(std::make_shared<primitive::LazyTensor>(outputs[1][0]));
    Tensor variance(std::make_shared<primitive::LazyTensor>(outputs[2][0]));
    Tensor y_grad(std::make_shared<primitive::LazyTensor>(out_grads[0][0]));

    VLOG(6) << "Vjp prepare Prepare attributes of group_norm_grad";

    float epsilon = op->attribute("epsilon").dyn_cast<pir::FloatAttribute>().data();
    int groups = op->attribute("groups").dyn_cast<pir::Int32Attribute>().data();
    std::string data_layout = op->attribute("data_layout").dyn_cast<pir::StrAttribute>().AsString();

    VLOG(6) << "Vjp prepare call group_norm's vjp inteface";

    std::vector<std::vector<Tensor>> tensor_res =
        primitive::group_norm_vjp(
        x, scale, bias, y, mean, variance, y_grad, epsilon, groups, data_layout, stop_gradients);

    VLOG(6) << "Vjp prepare stop gradient of group_norm_grad";

    std::vector<std::vector<pir::OpResult>> res(tensor_res.size());
    for (size_t i = 0; i < tensor_res.size(); ++i) {
        res[i].resize(tensor_res[i].size());
        for (size_t j = 0; j < tensor_res[i].size(); ++j) {
            if(tensor_res[i][j].defined()){
                res[i][j] = std::static_pointer_cast<primitive::LazyTensor>(tensor_res[i][j].impl())->value().dyn_cast<pir::OpResult>();
            }
        }
    }
    return res;
}

std::vector<std::vector<pir::OpResult>> GumbelSoftmaxOp::Vjp(pir::Operation* op, const std::vector<std::vector<pir::Value>>& inputs_, const std::vector<std::vector<pir::Value>>& outputs, const std::vector<std::vector<pir::Value>>& out_grads, const std::vector<std::vector<bool>>& stop_gradients){

    PADDLE_ENFORCE_EQ(
      inputs_.size(),
      1,
      platform::errors::InvalidArgument("gumbel_softmax op's inputs size should be 1, but now is %d.", inputs_.size()));
    PADDLE_ENFORCE_EQ(
      outputs.size(),
      1,
      platform::errors::InvalidArgument("gumbel_softmax op's outputs size should be 1, but now is %d.", outputs.size()));

    VLOG(6) << "Prepare inputs of gumbel_softmax_grad";

    Tensor out(std::make_shared<primitive::LazyTensor>(outputs[0][0]));
    Tensor out_grad(std::make_shared<primitive::LazyTensor>(out_grads[0][0]));

    VLOG(6) << "Vjp prepare Prepare attributes of gumbel_softmax_grad";

    int axis = op->attribute("axis").dyn_cast<pir::Int32Attribute>().data();

    VLOG(6) << "Vjp prepare call gumbel_softmax's vjp inteface";

    std::vector<std::vector<Tensor>> tensor_res =
        primitive::gumbel_softmax_vjp(
        out, out_grad, axis, stop_gradients);

    VLOG(6) << "Vjp prepare stop gradient of gumbel_softmax_grad";

    std::vector<std::vector<pir::OpResult>> res(tensor_res.size());
    for (size_t i = 0; i < tensor_res.size(); ++i) {
        res[i].resize(tensor_res[i].size());
        for (size_t j = 0; j < tensor_res[i].size(); ++j) {
            if(tensor_res[i][j].defined()){
                res[i][j] = std::static_pointer_cast<primitive::LazyTensor>(tensor_res[i][j].impl())->value().dyn_cast<pir::OpResult>();
            }
        }
    }
    return res;
}

std::vector<std::vector<pir::OpResult>> HardshrinkOp::Vjp(pir::Operation* op, const std::vector<std::vector<pir::Value>>& inputs_, const std::vector<std::vector<pir::Value>>& outputs, const std::vector<std::vector<pir::Value>>& out_grads, const std::vector<std::vector<bool>>& stop_gradients){

    PADDLE_ENFORCE_EQ(
      inputs_.size(),
      1,
      platform::errors::InvalidArgument("hardshrink op's inputs size should be 1, but now is %d.", inputs_.size()));
    PADDLE_ENFORCE_EQ(
      outputs.size(),
      1,
      platform::errors::InvalidArgument("hardshrink op's outputs size should be 1, but now is %d.", outputs.size()));

    VLOG(6) << "Prepare inputs of hardshrink_grad";

    Tensor x(std::make_shared<primitive::LazyTensor>(inputs_[0][0]));
    Tensor out_grad(std::make_shared<primitive::LazyTensor>(out_grads[0][0]));

    VLOG(6) << "Vjp prepare Prepare attributes of hardshrink_grad";

    float threshold = op->attribute("threshold").dyn_cast<pir::FloatAttribute>().data();

    VLOG(6) << "Vjp prepare call hardshrink's vjp inteface";

    std::vector<std::vector<Tensor>> tensor_res =
        primitive::hardshrink_vjp(
        x, out_grad, threshold, stop_gradients);

    VLOG(6) << "Vjp prepare stop gradient of hardshrink_grad";

    std::vector<std::vector<pir::OpResult>> res(tensor_res.size());
    for (size_t i = 0; i < tensor_res.size(); ++i) {
        res[i].resize(tensor_res[i].size());
        for (size_t j = 0; j < tensor_res[i].size(); ++j) {
            if(tensor_res[i][j].defined()){
                res[i][j] = std::static_pointer_cast<primitive::LazyTensor>(tensor_res[i][j].impl())->value().dyn_cast<pir::OpResult>();
            }
        }
    }
    return res;
}

std::vector<std::vector<pir::OpResult>> HardsigmoidOp::Vjp(pir::Operation* op, const std::vector<std::vector<pir::Value>>& inputs_, const std::vector<std::vector<pir::Value>>& outputs, const std::vector<std::vector<pir::Value>>& out_grads, const std::vector<std::vector<bool>>& stop_gradients){

    PADDLE_ENFORCE_EQ(
      inputs_.size(),
      1,
      platform::errors::InvalidArgument("hardsigmoid op's inputs size should be 1, but now is %d.", inputs_.size()));
    PADDLE_ENFORCE_EQ(
      outputs.size(),
      1,
      platform::errors::InvalidArgument("hardsigmoid op's outputs size should be 1, but now is %d.", outputs.size()));

    VLOG(6) << "Prepare inputs of hardsigmoid_grad";

    Tensor out(std::make_shared<primitive::LazyTensor>(outputs[0][0]));
    Tensor out_grad(std::make_shared<primitive::LazyTensor>(out_grads[0][0]));

    VLOG(6) << "Vjp prepare Prepare attributes of hardsigmoid_grad";

    float slope = op->attribute("slope").dyn_cast<pir::FloatAttribute>().data();
    float offset = op->attribute("offset").dyn_cast<pir::FloatAttribute>().data();

    VLOG(6) << "Vjp prepare call hardsigmoid's vjp inteface";

    std::vector<std::vector<Tensor>> tensor_res =
        primitive::hardsigmoid_vjp(
        out, out_grad, slope, offset, stop_gradients);

    VLOG(6) << "Vjp prepare stop gradient of hardsigmoid_grad";

    std::vector<std::vector<pir::OpResult>> res(tensor_res.size());
    for (size_t i = 0; i < tensor_res.size(); ++i) {
        res[i].resize(tensor_res[i].size());
        for (size_t j = 0; j < tensor_res[i].size(); ++j) {
            if(tensor_res[i][j].defined()){
                res[i][j] = std::static_pointer_cast<primitive::LazyTensor>(tensor_res[i][j].impl())->value().dyn_cast<pir::OpResult>();
            }
        }
    }
    return res;
}

std::vector<std::vector<pir::OpResult>> HardtanhOp::Vjp(pir::Operation* op, const std::vector<std::vector<pir::Value>>& inputs_, const std::vector<std::vector<pir::Value>>& outputs, const std::vector<std::vector<pir::Value>>& out_grads, const std::vector<std::vector<bool>>& stop_gradients){

    PADDLE_ENFORCE_EQ(
      inputs_.size(),
      1,
      platform::errors::InvalidArgument("hardtanh op's inputs size should be 1, but now is %d.", inputs_.size()));
    PADDLE_ENFORCE_EQ(
      outputs.size(),
      1,
      platform::errors::InvalidArgument("hardtanh op's outputs size should be 1, but now is %d.", outputs.size()));

    VLOG(6) << "Prepare inputs of hardtanh_grad";

    Tensor x(std::make_shared<primitive::LazyTensor>(inputs_[0][0]));
    Tensor out_grad(std::make_shared<primitive::LazyTensor>(out_grads[0][0]));

    VLOG(6) << "Vjp prepare Prepare attributes of hardtanh_grad";

    float t_min = op->attribute("t_min").dyn_cast<pir::FloatAttribute>().data();
    float t_max = op->attribute("t_max").dyn_cast<pir::FloatAttribute>().data();

    VLOG(6) << "Vjp prepare call hardtanh's vjp inteface";

    std::vector<std::vector<Tensor>> tensor_res =
        primitive::hardtanh_vjp(
        x, out_grad, t_min, t_max, stop_gradients);

    VLOG(6) << "Vjp prepare stop gradient of hardtanh_grad";

    std::vector<std::vector<pir::OpResult>> res(tensor_res.size());
    for (size_t i = 0; i < tensor_res.size(); ++i) {
        res[i].resize(tensor_res[i].size());
        for (size_t j = 0; j < tensor_res[i].size(); ++j) {
            if(tensor_res[i][j].defined()){
                res[i][j] = std::static_pointer_cast<primitive::LazyTensor>(tensor_res[i][j].impl())->value().dyn_cast<pir::OpResult>();
            }
        }
    }
    return res;
}

std::vector<std::vector<pir::OpResult>> Hardtanh_Op::Vjp(pir::Operation* op, const std::vector<std::vector<pir::Value>>& inputs_, const std::vector<std::vector<pir::Value>>& outputs, const std::vector<std::vector<pir::Value>>& out_grads, const std::vector<std::vector<bool>>& stop_gradients){

    PADDLE_ENFORCE_EQ(
      inputs_.size(),
      1,
      platform::errors::InvalidArgument("hardtanh op's inputs size should be 1, but now is %d.", inputs_.size()));
    PADDLE_ENFORCE_EQ(
      outputs.size(),
      1,
      platform::errors::InvalidArgument("hardtanh op's outputs size should be 1, but now is %d.", outputs.size()));

    VLOG(6) << "Prepare inputs of hardtanh_grad";

    Tensor x(std::make_shared<primitive::LazyTensor>(inputs_[0][0]));
    Tensor out_grad(std::make_shared<primitive::LazyTensor>(out_grads[0][0]));

    VLOG(6) << "Vjp prepare Prepare attributes of hardtanh_grad";

    float t_min = op->attribute("t_min").dyn_cast<pir::FloatAttribute>().data();
    float t_max = op->attribute("t_max").dyn_cast<pir::FloatAttribute>().data();

    VLOG(6) << "Vjp prepare call hardtanh_'s vjp inteface";

    std::vector<std::vector<Tensor>> tensor_res =
        primitive::hardtanh_vjp(
        x, out_grad, t_min, t_max, stop_gradients);

    VLOG(6) << "Vjp prepare stop gradient of hardtanh_grad";

    std::vector<std::vector<pir::OpResult>> res(tensor_res.size());
    for (size_t i = 0; i < tensor_res.size(); ++i) {
        res[i].resize(tensor_res[i].size());
        for (size_t j = 0; j < tensor_res[i].size(); ++j) {
            if(tensor_res[i][j].defined()){
                res[i][j] = std::static_pointer_cast<primitive::LazyTensor>(tensor_res[i][j].impl())->value().dyn_cast<pir::OpResult>();
            }
        }
    }
    return res;
}

std::vector<std::vector<pir::OpResult>> HeavisideOp::Vjp(pir::Operation* op, const std::vector<std::vector<pir::Value>>& inputs_, const std::vector<std::vector<pir::Value>>& outputs, const std::vector<std::vector<pir::Value>>& out_grads, const std::vector<std::vector<bool>>& stop_gradients){

    PADDLE_ENFORCE_EQ(
      inputs_.size(),
      2,
      platform::errors::InvalidArgument("heaviside op's inputs size should be 2, but now is %d.", inputs_.size()));
    PADDLE_ENFORCE_EQ(
      outputs.size(),
      1,
      platform::errors::InvalidArgument("heaviside op's outputs size should be 1, but now is %d.", outputs.size()));

    VLOG(6) << "Prepare inputs of heaviside_grad";

    Tensor x(std::make_shared<primitive::LazyTensor>(inputs_[0][0]));
    Tensor y(std::make_shared<primitive::LazyTensor>(inputs_[1][0]));
    Tensor out_grad(std::make_shared<primitive::LazyTensor>(out_grads[0][0]));

    VLOG(6) << "Vjp prepare Prepare attributes of heaviside_grad";


    VLOG(6) << "Vjp prepare call heaviside's vjp inteface";

    std::vector<std::vector<Tensor>> tensor_res =
        primitive::heaviside_vjp(
        x, y, out_grad, stop_gradients);

    VLOG(6) << "Vjp prepare stop gradient of heaviside_grad";

    std::vector<std::vector<pir::OpResult>> res(tensor_res.size());
    for (size_t i = 0; i < tensor_res.size(); ++i) {
        res[i].resize(tensor_res[i].size());
        for (size_t j = 0; j < tensor_res[i].size(); ++j) {
            if(tensor_res[i][j].defined()){
                res[i][j] = std::static_pointer_cast<primitive::LazyTensor>(tensor_res[i][j].impl())->value().dyn_cast<pir::OpResult>();
            }
        }
    }
    return res;
}

std::vector<std::vector<pir::OpResult>> HuberLossOp::Vjp(pir::Operation* op, const std::vector<std::vector<pir::Value>>& inputs_, const std::vector<std::vector<pir::Value>>& outputs, const std::vector<std::vector<pir::Value>>& out_grads, const std::vector<std::vector<bool>>& stop_gradients){

    PADDLE_ENFORCE_EQ(
      inputs_.size(),
      2,
      platform::errors::InvalidArgument("huber_loss op's inputs size should be 2, but now is %d.", inputs_.size()));
    PADDLE_ENFORCE_EQ(
      outputs.size(),
      2,
      platform::errors::InvalidArgument("huber_loss op's outputs size should be 2, but now is %d.", outputs.size()));

    VLOG(6) << "Prepare inputs of huber_loss_grad";

    Tensor residual(std::make_shared<primitive::LazyTensor>(outputs[1][0]));
    Tensor out_grad(std::make_shared<primitive::LazyTensor>(out_grads[0][0]));

    VLOG(6) << "Vjp prepare Prepare attributes of huber_loss_grad";

    float delta = op->attribute("delta").dyn_cast<pir::FloatAttribute>().data();

    VLOG(6) << "Vjp prepare call huber_loss's vjp inteface";

    std::vector<std::vector<Tensor>> tensor_res =
        primitive::huber_loss_vjp(
        residual, out_grad, delta, stop_gradients);

    VLOG(6) << "Vjp prepare stop gradient of huber_loss_grad";

    std::vector<std::vector<pir::OpResult>> res(tensor_res.size());
    for (size_t i = 0; i < tensor_res.size(); ++i) {
        res[i].resize(tensor_res[i].size());
        for (size_t j = 0; j < tensor_res[i].size(); ++j) {
            if(tensor_res[i][j].defined()){
                res[i][j] = std::static_pointer_cast<primitive::LazyTensor>(tensor_res[i][j].impl())->value().dyn_cast<pir::OpResult>();
            }
        }
    }
    return res;
}

std::vector<std::vector<pir::OpResult>> I0Op::Vjp(pir::Operation* op, const std::vector<std::vector<pir::Value>>& inputs_, const std::vector<std::vector<pir::Value>>& outputs, const std::vector<std::vector<pir::Value>>& out_grads, const std::vector<std::vector<bool>>& stop_gradients){

    PADDLE_ENFORCE_EQ(
      inputs_.size(),
      1,
      platform::errors::InvalidArgument("i0 op's inputs size should be 1, but now is %d.", inputs_.size()));
    PADDLE_ENFORCE_EQ(
      outputs.size(),
      1,
      platform::errors::InvalidArgument("i0 op's outputs size should be 1, but now is %d.", outputs.size()));

    VLOG(6) << "Prepare inputs of i0_grad";

    Tensor x(std::make_shared<primitive::LazyTensor>(inputs_[0][0]));
    Tensor out_grad(std::make_shared<primitive::LazyTensor>(out_grads[0][0]));

    VLOG(6) << "Vjp prepare Prepare attributes of i0_grad";


    VLOG(6) << "Vjp prepare call i0's vjp inteface";

    std::vector<std::vector<Tensor>> tensor_res =
        primitive::i0_vjp(
        x, out_grad, stop_gradients);

    VLOG(6) << "Vjp prepare stop gradient of i0_grad";

    std::vector<std::vector<pir::OpResult>> res(tensor_res.size());
    for (size_t i = 0; i < tensor_res.size(); ++i) {
        res[i].resize(tensor_res[i].size());
        for (size_t j = 0; j < tensor_res[i].size(); ++j) {
            if(tensor_res[i][j].defined()){
                res[i][j] = std::static_pointer_cast<primitive::LazyTensor>(tensor_res[i][j].impl())->value().dyn_cast<pir::OpResult>();
            }
        }
    }
    return res;
}

std::vector<std::vector<pir::OpResult>> I0_Op::Vjp(pir::Operation* op, const std::vector<std::vector<pir::Value>>& inputs_, const std::vector<std::vector<pir::Value>>& outputs, const std::vector<std::vector<pir::Value>>& out_grads, const std::vector<std::vector<bool>>& stop_gradients){

    PADDLE_ENFORCE_EQ(
      inputs_.size(),
      1,
      platform::errors::InvalidArgument("i0 op's inputs size should be 1, but now is %d.", inputs_.size()));
    PADDLE_ENFORCE_EQ(
      outputs.size(),
      1,
      platform::errors::InvalidArgument("i0 op's outputs size should be 1, but now is %d.", outputs.size()));

    VLOG(6) << "Prepare inputs of i0_grad";

    Tensor x(std::make_shared<primitive::LazyTensor>(inputs_[0][0]));
    Tensor out_grad(std::make_shared<primitive::LazyTensor>(out_grads[0][0]));

    VLOG(6) << "Vjp prepare Prepare attributes of i0_grad";


    VLOG(6) << "Vjp prepare call i0_'s vjp inteface";

    std::vector<std::vector<Tensor>> tensor_res =
        primitive::i0_vjp(
        x, out_grad, stop_gradients);

    VLOG(6) << "Vjp prepare stop gradient of i0_grad";

    std::vector<std::vector<pir::OpResult>> res(tensor_res.size());
    for (size_t i = 0; i < tensor_res.size(); ++i) {
        res[i].resize(tensor_res[i].size());
        for (size_t j = 0; j < tensor_res[i].size(); ++j) {
            if(tensor_res[i][j].defined()){
                res[i][j] = std::static_pointer_cast<primitive::LazyTensor>(tensor_res[i][j].impl())->value().dyn_cast<pir::OpResult>();
            }
        }
    }
    return res;
}

std::vector<std::vector<pir::OpResult>> I0eOp::Vjp(pir::Operation* op, const std::vector<std::vector<pir::Value>>& inputs_, const std::vector<std::vector<pir::Value>>& outputs, const std::vector<std::vector<pir::Value>>& out_grads, const std::vector<std::vector<bool>>& stop_gradients){

    PADDLE_ENFORCE_EQ(
      inputs_.size(),
      1,
      platform::errors::InvalidArgument("i0e op's inputs size should be 1, but now is %d.", inputs_.size()));
    PADDLE_ENFORCE_EQ(
      outputs.size(),
      1,
      platform::errors::InvalidArgument("i0e op's outputs size should be 1, but now is %d.", outputs.size()));

    VLOG(6) << "Prepare inputs of i0e_grad";

    Tensor x(std::make_shared<primitive::LazyTensor>(inputs_[0][0]));
    Tensor out(std::make_shared<primitive::LazyTensor>(outputs[0][0]));
    Tensor out_grad(std::make_shared<primitive::LazyTensor>(out_grads[0][0]));

    VLOG(6) << "Vjp prepare Prepare attributes of i0e_grad";


    VLOG(6) << "Vjp prepare call i0e's vjp inteface";

    std::vector<std::vector<Tensor>> tensor_res =
        primitive::i0e_vjp(
        x, out, out_grad, stop_gradients);

    VLOG(6) << "Vjp prepare stop gradient of i0e_grad";

    std::vector<std::vector<pir::OpResult>> res(tensor_res.size());
    for (size_t i = 0; i < tensor_res.size(); ++i) {
        res[i].resize(tensor_res[i].size());
        for (size_t j = 0; j < tensor_res[i].size(); ++j) {
            if(tensor_res[i][j].defined()){
                res[i][j] = std::static_pointer_cast<primitive::LazyTensor>(tensor_res[i][j].impl())->value().dyn_cast<pir::OpResult>();
            }
        }
    }
    return res;
}

std::vector<std::vector<pir::OpResult>> I1Op::Vjp(pir::Operation* op, const std::vector<std::vector<pir::Value>>& inputs_, const std::vector<std::vector<pir::Value>>& outputs, const std::vector<std::vector<pir::Value>>& out_grads, const std::vector<std::vector<bool>>& stop_gradients){

    PADDLE_ENFORCE_EQ(
      inputs_.size(),
      1,
      platform::errors::InvalidArgument("i1 op's inputs size should be 1, but now is %d.", inputs_.size()));
    PADDLE_ENFORCE_EQ(
      outputs.size(),
      1,
      platform::errors::InvalidArgument("i1 op's outputs size should be 1, but now is %d.", outputs.size()));

    VLOG(6) << "Prepare inputs of i1_grad";

    Tensor x(std::make_shared<primitive::LazyTensor>(inputs_[0][0]));
    Tensor out(std::make_shared<primitive::LazyTensor>(outputs[0][0]));
    Tensor out_grad(std::make_shared<primitive::LazyTensor>(out_grads[0][0]));

    VLOG(6) << "Vjp prepare Prepare attributes of i1_grad";


    VLOG(6) << "Vjp prepare call i1's vjp inteface";

    std::vector<std::vector<Tensor>> tensor_res =
        primitive::i1_vjp(
        x, out, out_grad, stop_gradients);

    VLOG(6) << "Vjp prepare stop gradient of i1_grad";

    std::vector<std::vector<pir::OpResult>> res(tensor_res.size());
    for (size_t i = 0; i < tensor_res.size(); ++i) {
        res[i].resize(tensor_res[i].size());
        for (size_t j = 0; j < tensor_res[i].size(); ++j) {
            if(tensor_res[i][j].defined()){
                res[i][j] = std::static_pointer_cast<primitive::LazyTensor>(tensor_res[i][j].impl())->value().dyn_cast<pir::OpResult>();
            }
        }
    }
    return res;
}

std::vector<std::vector<pir::OpResult>> I1eOp::Vjp(pir::Operation* op, const std::vector<std::vector<pir::Value>>& inputs_, const std::vector<std::vector<pir::Value>>& outputs, const std::vector<std::vector<pir::Value>>& out_grads, const std::vector<std::vector<bool>>& stop_gradients){

    PADDLE_ENFORCE_EQ(
      inputs_.size(),
      1,
      platform::errors::InvalidArgument("i1e op's inputs size should be 1, but now is %d.", inputs_.size()));
    PADDLE_ENFORCE_EQ(
      outputs.size(),
      1,
      platform::errors::InvalidArgument("i1e op's outputs size should be 1, but now is %d.", outputs.size()));

    VLOG(6) << "Prepare inputs of i1e_grad";

    Tensor x(std::make_shared<primitive::LazyTensor>(inputs_[0][0]));
    Tensor out(std::make_shared<primitive::LazyTensor>(outputs[0][0]));
    Tensor out_grad(std::make_shared<primitive::LazyTensor>(out_grads[0][0]));

    VLOG(6) << "Vjp prepare Prepare attributes of i1e_grad";


    VLOG(6) << "Vjp prepare call i1e's vjp inteface";

    std::vector<std::vector<Tensor>> tensor_res =
        primitive::i1e_vjp(
        x, out, out_grad, stop_gradients);

    VLOG(6) << "Vjp prepare stop gradient of i1e_grad";

    std::vector<std::vector<pir::OpResult>> res(tensor_res.size());
    for (size_t i = 0; i < tensor_res.size(); ++i) {
        res[i].resize(tensor_res[i].size());
        for (size_t j = 0; j < tensor_res[i].size(); ++j) {
            if(tensor_res[i][j].defined()){
                res[i][j] = std::static_pointer_cast<primitive::LazyTensor>(tensor_res[i][j].impl())->value().dyn_cast<pir::OpResult>();
            }
        }
    }
    return res;
}

std::vector<std::vector<pir::OpResult>> IdentityLossOp::Vjp(pir::Operation* op, const std::vector<std::vector<pir::Value>>& inputs_, const std::vector<std::vector<pir::Value>>& outputs, const std::vector<std::vector<pir::Value>>& out_grads, const std::vector<std::vector<bool>>& stop_gradients){

    PADDLE_ENFORCE_EQ(
      inputs_.size(),
      1,
      platform::errors::InvalidArgument("identity_loss op's inputs size should be 1, but now is %d.", inputs_.size()));
    PADDLE_ENFORCE_EQ(
      outputs.size(),
      1,
      platform::errors::InvalidArgument("identity_loss op's outputs size should be 1, but now is %d.", outputs.size()));

    VLOG(6) << "Prepare inputs of identity_loss_grad";

    Tensor x(std::make_shared<primitive::LazyTensor>(inputs_[0][0]));
    Tensor out_grad(std::make_shared<primitive::LazyTensor>(out_grads[0][0]));

    VLOG(6) << "Vjp prepare Prepare attributes of identity_loss_grad";

    int reduction = op->attribute("reduction").dyn_cast<pir::Int32Attribute>().data();

    VLOG(6) << "Vjp prepare call identity_loss's vjp inteface";

    std::vector<std::vector<Tensor>> tensor_res =
        primitive::identity_loss_vjp(
        x, out_grad, reduction, stop_gradients);

    VLOG(6) << "Vjp prepare stop gradient of identity_loss_grad";

    std::vector<std::vector<pir::OpResult>> res(tensor_res.size());
    for (size_t i = 0; i < tensor_res.size(); ++i) {
        res[i].resize(tensor_res[i].size());
        for (size_t j = 0; j < tensor_res[i].size(); ++j) {
            if(tensor_res[i][j].defined()){
                res[i][j] = std::static_pointer_cast<primitive::LazyTensor>(tensor_res[i][j].impl())->value().dyn_cast<pir::OpResult>();
            }
        }
    }
    return res;
}

std::vector<std::vector<pir::OpResult>> IdentityLoss_Op::Vjp(pir::Operation* op, const std::vector<std::vector<pir::Value>>& inputs_, const std::vector<std::vector<pir::Value>>& outputs, const std::vector<std::vector<pir::Value>>& out_grads, const std::vector<std::vector<bool>>& stop_gradients){

    PADDLE_ENFORCE_EQ(
      inputs_.size(),
      1,
      platform::errors::InvalidArgument("identity_loss op's inputs size should be 1, but now is %d.", inputs_.size()));
    PADDLE_ENFORCE_EQ(
      outputs.size(),
      1,
      platform::errors::InvalidArgument("identity_loss op's outputs size should be 1, but now is %d.", outputs.size()));

    VLOG(6) << "Prepare inputs of identity_loss_grad";

    Tensor x(std::make_shared<primitive::LazyTensor>(inputs_[0][0]));
    Tensor out_grad(std::make_shared<primitive::LazyTensor>(out_grads[0][0]));

    VLOG(6) << "Vjp prepare Prepare attributes of identity_loss_grad";

    int reduction = op->attribute("reduction").dyn_cast<pir::Int32Attribute>().data();

    VLOG(6) << "Vjp prepare call identity_loss_'s vjp inteface";

    std::vector<std::vector<Tensor>> tensor_res =
        primitive::identity_loss_vjp(
        x, out_grad, reduction, stop_gradients);

    VLOG(6) << "Vjp prepare stop gradient of identity_loss_grad";

    std::vector<std::vector<pir::OpResult>> res(tensor_res.size());
    for (size_t i = 0; i < tensor_res.size(); ++i) {
        res[i].resize(tensor_res[i].size());
        for (size_t j = 0; j < tensor_res[i].size(); ++j) {
            if(tensor_res[i][j].defined()){
                res[i][j] = std::static_pointer_cast<primitive::LazyTensor>(tensor_res[i][j].impl())->value().dyn_cast<pir::OpResult>();
            }
        }
    }
    return res;
}

std::vector<std::vector<pir::OpResult>> ImagOp::Vjp(pir::Operation* op, const std::vector<std::vector<pir::Value>>& inputs_, const std::vector<std::vector<pir::Value>>& outputs, const std::vector<std::vector<pir::Value>>& out_grads, const std::vector<std::vector<bool>>& stop_gradients){

    PADDLE_ENFORCE_EQ(
      inputs_.size(),
      1,
      platform::errors::InvalidArgument("imag op's inputs size should be 1, but now is %d.", inputs_.size()));
    PADDLE_ENFORCE_EQ(
      outputs.size(),
      1,
      platform::errors::InvalidArgument("imag op's outputs size should be 1, but now is %d.", outputs.size()));

    VLOG(6) << "Prepare inputs of imag_grad";

    Tensor out_grad(std::make_shared<primitive::LazyTensor>(out_grads[0][0]));

    VLOG(6) << "Vjp prepare Prepare attributes of imag_grad";


    VLOG(6) << "Vjp prepare call imag's vjp inteface";

    std::vector<std::vector<Tensor>> tensor_res =
        primitive::imag_vjp(
        out_grad, stop_gradients);

    VLOG(6) << "Vjp prepare stop gradient of imag_grad";

    std::vector<std::vector<pir::OpResult>> res(tensor_res.size());
    for (size_t i = 0; i < tensor_res.size(); ++i) {
        res[i].resize(tensor_res[i].size());
        for (size_t j = 0; j < tensor_res[i].size(); ++j) {
            if(tensor_res[i][j].defined()){
                res[i][j] = std::static_pointer_cast<primitive::LazyTensor>(tensor_res[i][j].impl())->value().dyn_cast<pir::OpResult>();
            }
        }
    }
    return res;
}

std::vector<std::vector<pir::OpResult>> IndexAddOp::Vjp(pir::Operation* op, const std::vector<std::vector<pir::Value>>& inputs_, const std::vector<std::vector<pir::Value>>& outputs, const std::vector<std::vector<pir::Value>>& out_grads, const std::vector<std::vector<bool>>& stop_gradients){

    PADDLE_ENFORCE_EQ(
      inputs_.size(),
      3,
      platform::errors::InvalidArgument("index_add op's inputs size should be 3, but now is %d.", inputs_.size()));
    PADDLE_ENFORCE_EQ(
      outputs.size(),
      1,
      platform::errors::InvalidArgument("index_add op's outputs size should be 1, but now is %d.", outputs.size()));

    VLOG(6) << "Prepare inputs of index_add_grad";

    Tensor index(std::make_shared<primitive::LazyTensor>(inputs_[1][0]));
    Tensor add_value(std::make_shared<primitive::LazyTensor>(inputs_[2][0]));
    Tensor out_grad(std::make_shared<primitive::LazyTensor>(out_grads[0][0]));

    VLOG(6) << "Vjp prepare Prepare attributes of index_add_grad";

    int axis = op->attribute("axis").dyn_cast<pir::Int32Attribute>().data();

    VLOG(6) << "Vjp prepare call index_add's vjp inteface";

    std::vector<std::vector<Tensor>> tensor_res =
        primitive::index_add_vjp(
        index, add_value, out_grad, axis, stop_gradients);

    VLOG(6) << "Vjp prepare stop gradient of index_add_grad";

    std::vector<std::vector<pir::OpResult>> res(tensor_res.size());
    for (size_t i = 0; i < tensor_res.size(); ++i) {
        res[i].resize(tensor_res[i].size());
        for (size_t j = 0; j < tensor_res[i].size(); ++j) {
            if(tensor_res[i][j].defined()){
                res[i][j] = std::static_pointer_cast<primitive::LazyTensor>(tensor_res[i][j].impl())->value().dyn_cast<pir::OpResult>();
            }
        }
    }
    return res;
}

std::vector<std::vector<pir::OpResult>> IndexAdd_Op::Vjp(pir::Operation* op, const std::vector<std::vector<pir::Value>>& inputs_, const std::vector<std::vector<pir::Value>>& outputs, const std::vector<std::vector<pir::Value>>& out_grads, const std::vector<std::vector<bool>>& stop_gradients){

    PADDLE_ENFORCE_EQ(
      inputs_.size(),
      3,
      platform::errors::InvalidArgument("index_add op's inputs size should be 3, but now is %d.", inputs_.size()));
    PADDLE_ENFORCE_EQ(
      outputs.size(),
      1,
      platform::errors::InvalidArgument("index_add op's outputs size should be 1, but now is %d.", outputs.size()));

    VLOG(6) << "Prepare inputs of index_add_grad";

    Tensor index(std::make_shared<primitive::LazyTensor>(inputs_[1][0]));
    Tensor add_value(std::make_shared<primitive::LazyTensor>(inputs_[2][0]));
    Tensor out_grad(std::make_shared<primitive::LazyTensor>(out_grads[0][0]));

    VLOG(6) << "Vjp prepare Prepare attributes of index_add_grad";

    int axis = op->attribute("axis").dyn_cast<pir::Int32Attribute>().data();

    VLOG(6) << "Vjp prepare call index_add_'s vjp inteface";

    std::vector<std::vector<Tensor>> tensor_res =
        primitive::index_add_vjp(
        index, add_value, out_grad, axis, stop_gradients);

    VLOG(6) << "Vjp prepare stop gradient of index_add_grad";

    std::vector<std::vector<pir::OpResult>> res(tensor_res.size());
    for (size_t i = 0; i < tensor_res.size(); ++i) {
        res[i].resize(tensor_res[i].size());
        for (size_t j = 0; j < tensor_res[i].size(); ++j) {
            if(tensor_res[i][j].defined()){
                res[i][j] = std::static_pointer_cast<primitive::LazyTensor>(tensor_res[i][j].impl())->value().dyn_cast<pir::OpResult>();
            }
        }
    }
    return res;
}

std::vector<std::vector<pir::OpResult>> IndexPutOp::Vjp(pir::Operation* op, const std::vector<std::vector<pir::Value>>& inputs_, const std::vector<std::vector<pir::Value>>& outputs, const std::vector<std::vector<pir::Value>>& out_grads, const std::vector<std::vector<bool>>& stop_gradients){

    PADDLE_ENFORCE_EQ(
      inputs_.size(),
      3,
      platform::errors::InvalidArgument("index_put op's inputs size should be 3, but now is %d.", inputs_.size()));
    PADDLE_ENFORCE_EQ(
      outputs.size(),
      1,
      platform::errors::InvalidArgument("index_put op's outputs size should be 1, but now is %d.", outputs.size()));

    VLOG(6) << "Prepare inputs of index_put_grad";

    Tensor x(std::make_shared<primitive::LazyTensor>(inputs_[0][0]));
    std::vector<Tensor> indices;
    for (size_t idx = 0; idx < inputs_[1].size(); idx++) {
        indices.emplace_back(
            std::make_shared<primitive::LazyTensor>(inputs_[1][idx]));
    }
    Tensor value(std::make_shared<primitive::LazyTensor>(inputs_[2][0]));
    Tensor out_grad(std::make_shared<primitive::LazyTensor>(out_grads[0][0]));

    VLOG(6) << "Vjp prepare Prepare attributes of index_put_grad";

    bool accumulate = op->attribute("accumulate").dyn_cast<pir::BoolAttribute>().data();

    VLOG(6) << "Vjp prepare call index_put's vjp inteface";

    std::vector<std::vector<Tensor>> tensor_res =
        primitive::index_put_vjp(
        x, indices, value, out_grad, accumulate, stop_gradients);

    VLOG(6) << "Vjp prepare stop gradient of index_put_grad";

    std::vector<std::vector<pir::OpResult>> res(tensor_res.size());
    for (size_t i = 0; i < tensor_res.size(); ++i) {
        res[i].resize(tensor_res[i].size());
        for (size_t j = 0; j < tensor_res[i].size(); ++j) {
            if(tensor_res[i][j].defined()){
                res[i][j] = std::static_pointer_cast<primitive::LazyTensor>(tensor_res[i][j].impl())->value().dyn_cast<pir::OpResult>();
            }
        }
    }
    return res;
}

std::vector<std::vector<pir::OpResult>> IndexPut_Op::Vjp(pir::Operation* op, const std::vector<std::vector<pir::Value>>& inputs_, const std::vector<std::vector<pir::Value>>& outputs, const std::vector<std::vector<pir::Value>>& out_grads, const std::vector<std::vector<bool>>& stop_gradients){

    PADDLE_ENFORCE_EQ(
      inputs_.size(),
      3,
      platform::errors::InvalidArgument("index_put op's inputs size should be 3, but now is %d.", inputs_.size()));
    PADDLE_ENFORCE_EQ(
      outputs.size(),
      1,
      platform::errors::InvalidArgument("index_put op's outputs size should be 1, but now is %d.", outputs.size()));

    VLOG(6) << "Prepare inputs of index_put_grad";

    Tensor x(std::make_shared<primitive::LazyTensor>(inputs_[0][0]));
    std::vector<Tensor> indices;
    for (size_t idx = 0; idx < inputs_[1].size(); idx++) {
        indices.emplace_back(
            std::make_shared<primitive::LazyTensor>(inputs_[1][idx]));
    }
    Tensor value(std::make_shared<primitive::LazyTensor>(inputs_[2][0]));
    Tensor out_grad(std::make_shared<primitive::LazyTensor>(out_grads[0][0]));

    VLOG(6) << "Vjp prepare Prepare attributes of index_put_grad";

    bool accumulate = op->attribute("accumulate").dyn_cast<pir::BoolAttribute>().data();

    VLOG(6) << "Vjp prepare call index_put_'s vjp inteface";

    std::vector<std::vector<Tensor>> tensor_res =
        primitive::index_put_vjp(
        x, indices, value, out_grad, accumulate, stop_gradients);

    VLOG(6) << "Vjp prepare stop gradient of index_put_grad";

    std::vector<std::vector<pir::OpResult>> res(tensor_res.size());
    for (size_t i = 0; i < tensor_res.size(); ++i) {
        res[i].resize(tensor_res[i].size());
        for (size_t j = 0; j < tensor_res[i].size(); ++j) {
            if(tensor_res[i][j].defined()){
                res[i][j] = std::static_pointer_cast<primitive::LazyTensor>(tensor_res[i][j].impl())->value().dyn_cast<pir::OpResult>();
            }
        }
    }
    return res;
}

std::vector<std::vector<pir::OpResult>> IndexSampleOp::Vjp(pir::Operation* op, const std::vector<std::vector<pir::Value>>& inputs_, const std::vector<std::vector<pir::Value>>& outputs, const std::vector<std::vector<pir::Value>>& out_grads, const std::vector<std::vector<bool>>& stop_gradients){

    PADDLE_ENFORCE_EQ(
      inputs_.size(),
      2,
      platform::errors::InvalidArgument("index_sample op's inputs size should be 2, but now is %d.", inputs_.size()));
    PADDLE_ENFORCE_EQ(
      outputs.size(),
      1,
      platform::errors::InvalidArgument("index_sample op's outputs size should be 1, but now is %d.", outputs.size()));

    VLOG(6) << "Prepare inputs of index_sample_grad";

    Tensor x(std::make_shared<primitive::LazyTensor>(inputs_[0][0]));
    Tensor index(std::make_shared<primitive::LazyTensor>(inputs_[1][0]));
    Tensor out_grad(std::make_shared<primitive::LazyTensor>(out_grads[0][0]));

    VLOG(6) << "Vjp prepare Prepare attributes of index_sample_grad";


    VLOG(6) << "Vjp prepare call index_sample's vjp inteface";

    std::vector<std::vector<Tensor>> tensor_res =
        primitive::index_sample_vjp(
        x, index, out_grad, stop_gradients);

    VLOG(6) << "Vjp prepare stop gradient of index_sample_grad";

    std::vector<std::vector<pir::OpResult>> res(tensor_res.size());
    for (size_t i = 0; i < tensor_res.size(); ++i) {
        res[i].resize(tensor_res[i].size());
        for (size_t j = 0; j < tensor_res[i].size(); ++j) {
            if(tensor_res[i][j].defined()){
                res[i][j] = std::static_pointer_cast<primitive::LazyTensor>(tensor_res[i][j].impl())->value().dyn_cast<pir::OpResult>();
            }
        }
    }
    return res;
}

std::vector<std::vector<pir::OpResult>> IndexSelectOp::Vjp(pir::Operation* op, const std::vector<std::vector<pir::Value>>& inputs_, const std::vector<std::vector<pir::Value>>& outputs, const std::vector<std::vector<pir::Value>>& out_grads, const std::vector<std::vector<bool>>& stop_gradients){

    PADDLE_ENFORCE_EQ(
      inputs_.size(),
      2,
      platform::errors::InvalidArgument("index_select op's inputs size should be 2, but now is %d.", inputs_.size()));
    PADDLE_ENFORCE_EQ(
      outputs.size(),
      1,
      platform::errors::InvalidArgument("index_select op's outputs size should be 1, but now is %d.", outputs.size()));

    VLOG(6) << "Prepare inputs of index_select_grad";

    Tensor x(std::make_shared<primitive::LazyTensor>(inputs_[0][0]));
    Tensor index(std::make_shared<primitive::LazyTensor>(inputs_[1][0]));
    Tensor out_grad(std::make_shared<primitive::LazyTensor>(out_grads[0][0]));

    VLOG(6) << "Vjp prepare Prepare attributes of index_select_grad";

    int axis = op->attribute("axis").dyn_cast<pir::Int32Attribute>().data();

    VLOG(6) << "Vjp prepare call index_select's vjp inteface";

    std::vector<std::vector<Tensor>> tensor_res =
        primitive::index_select_vjp(
        x, index, out_grad, axis, stop_gradients);

    VLOG(6) << "Vjp prepare stop gradient of index_select_grad";

    std::vector<std::vector<pir::OpResult>> res(tensor_res.size());
    for (size_t i = 0; i < tensor_res.size(); ++i) {
        res[i].resize(tensor_res[i].size());
        for (size_t j = 0; j < tensor_res[i].size(); ++j) {
            if(tensor_res[i][j].defined()){
                res[i][j] = std::static_pointer_cast<primitive::LazyTensor>(tensor_res[i][j].impl())->value().dyn_cast<pir::OpResult>();
            }
        }
    }
    return res;
}

std::vector<std::vector<pir::OpResult>> IndexSelectStridedOp::Vjp(pir::Operation* op, const std::vector<std::vector<pir::Value>>& inputs_, const std::vector<std::vector<pir::Value>>& outputs, const std::vector<std::vector<pir::Value>>& out_grads, const std::vector<std::vector<bool>>& stop_gradients){

    PADDLE_ENFORCE_EQ(
      inputs_.size(),
      1,
      platform::errors::InvalidArgument("index_select_strided op's inputs size should be 1, but now is %d.", inputs_.size()));
    PADDLE_ENFORCE_EQ(
      outputs.size(),
      1,
      platform::errors::InvalidArgument("index_select_strided op's outputs size should be 1, but now is %d.", outputs.size()));

    VLOG(6) << "Prepare inputs of index_select_strided_grad";

    Tensor x(std::make_shared<primitive::LazyTensor>(inputs_[0][0]));
    Tensor out_grad(std::make_shared<primitive::LazyTensor>(out_grads[0][0]));

    VLOG(6) << "Vjp prepare Prepare attributes of index_select_strided_grad";

    int64_t index = op->attribute("index").dyn_cast<pir::Int64Attribute>().data();
    int axis = op->attribute("axis").dyn_cast<pir::Int32Attribute>().data();

    VLOG(6) << "Vjp prepare call index_select_strided's vjp inteface";

    std::vector<std::vector<Tensor>> tensor_res =
        primitive::index_select_strided_vjp(
        x, out_grad, index, axis, stop_gradients);

    VLOG(6) << "Vjp prepare stop gradient of index_select_strided_grad";

    std::vector<std::vector<pir::OpResult>> res(tensor_res.size());
    for (size_t i = 0; i < tensor_res.size(); ++i) {
        res[i].resize(tensor_res[i].size());
        for (size_t j = 0; j < tensor_res[i].size(); ++j) {
            if(tensor_res[i][j].defined()){
                res[i][j] = std::static_pointer_cast<primitive::LazyTensor>(tensor_res[i][j].impl())->value().dyn_cast<pir::OpResult>();
            }
        }
    }
    return res;
}

std::vector<std::vector<pir::OpResult>> InstanceNormOp::Vjp(pir::Operation* op, const std::vector<std::vector<pir::Value>>& inputs_, const std::vector<std::vector<pir::Value>>& outputs, const std::vector<std::vector<pir::Value>>& out_grads, const std::vector<std::vector<bool>>& stop_gradients){

    PADDLE_ENFORCE_EQ(
      inputs_.size(),
      3,
      platform::errors::InvalidArgument("instance_norm op's inputs size should be 3, but now is %d.", inputs_.size()));
    PADDLE_ENFORCE_EQ(
      outputs.size(),
      3,
      platform::errors::InvalidArgument("instance_norm op's outputs size should be 3, but now is %d.", outputs.size()));

    VLOG(6) << "Prepare inputs of instance_norm_grad";

    Tensor x(std::make_shared<primitive::LazyTensor>(inputs_[0][0]));
    paddle::optional<Tensor> scale;
    if (!IsEmptyValue(inputs_[1][0])){
        scale = paddle::make_optional<Tensor>(Tensor(std::make_shared<primitive::LazyTensor>(inputs_[1][0])));
    }
    Tensor saved_mean(std::make_shared<primitive::LazyTensor>(outputs[1][0]));
    Tensor saved_variance(std::make_shared<primitive::LazyTensor>(outputs[2][0]));
    Tensor y_grad(std::make_shared<primitive::LazyTensor>(out_grads[0][0]));

    VLOG(6) << "Vjp prepare Prepare attributes of instance_norm_grad";

    float epsilon = op->attribute("epsilon").dyn_cast<pir::FloatAttribute>().data();

    VLOG(6) << "Vjp prepare call instance_norm's vjp inteface";

    std::vector<std::vector<Tensor>> tensor_res =
        primitive::instance_norm_vjp(
        x, scale, saved_mean, saved_variance, y_grad, epsilon, stop_gradients);

    VLOG(6) << "Vjp prepare stop gradient of instance_norm_grad";

    std::vector<std::vector<pir::OpResult>> res(tensor_res.size());
    for (size_t i = 0; i < tensor_res.size(); ++i) {
        res[i].resize(tensor_res[i].size());
        for (size_t j = 0; j < tensor_res[i].size(); ++j) {
            if(tensor_res[i][j].defined()){
                res[i][j] = std::static_pointer_cast<primitive::LazyTensor>(tensor_res[i][j].impl())->value().dyn_cast<pir::OpResult>();
            }
        }
    }
    return res;
}

std::vector<std::vector<pir::OpResult>> InverseOp::Vjp(pir::Operation* op, const std::vector<std::vector<pir::Value>>& inputs_, const std::vector<std::vector<pir::Value>>& outputs, const std::vector<std::vector<pir::Value>>& out_grads, const std::vector<std::vector<bool>>& stop_gradients){

    PADDLE_ENFORCE_EQ(
      inputs_.size(),
      1,
      platform::errors::InvalidArgument("inverse op's inputs size should be 1, but now is %d.", inputs_.size()));
    PADDLE_ENFORCE_EQ(
      outputs.size(),
      1,
      platform::errors::InvalidArgument("inverse op's outputs size should be 1, but now is %d.", outputs.size()));

    VLOG(6) << "Prepare inputs of inverse_grad";

    Tensor out(std::make_shared<primitive::LazyTensor>(outputs[0][0]));
    Tensor out_grad(std::make_shared<primitive::LazyTensor>(out_grads[0][0]));

    VLOG(6) << "Vjp prepare Prepare attributes of inverse_grad";


    VLOG(6) << "Vjp prepare call inverse's vjp inteface";

    std::vector<std::vector<Tensor>> tensor_res =
        primitive::inverse_vjp(
        out, out_grad, stop_gradients);

    VLOG(6) << "Vjp prepare stop gradient of inverse_grad";

    std::vector<std::vector<pir::OpResult>> res(tensor_res.size());
    for (size_t i = 0; i < tensor_res.size(); ++i) {
        res[i].resize(tensor_res[i].size());
        for (size_t j = 0; j < tensor_res[i].size(); ++j) {
            if(tensor_res[i][j].defined()){
                res[i][j] = std::static_pointer_cast<primitive::LazyTensor>(tensor_res[i][j].impl())->value().dyn_cast<pir::OpResult>();
            }
        }
    }
    return res;
}

std::vector<std::vector<pir::OpResult>> KldivLossOp::Vjp(pir::Operation* op, const std::vector<std::vector<pir::Value>>& inputs_, const std::vector<std::vector<pir::Value>>& outputs, const std::vector<std::vector<pir::Value>>& out_grads, const std::vector<std::vector<bool>>& stop_gradients){

    PADDLE_ENFORCE_EQ(
      inputs_.size(),
      2,
      platform::errors::InvalidArgument("kldiv_loss op's inputs size should be 2, but now is %d.", inputs_.size()));
    PADDLE_ENFORCE_EQ(
      outputs.size(),
      1,
      platform::errors::InvalidArgument("kldiv_loss op's outputs size should be 1, but now is %d.", outputs.size()));

    VLOG(6) << "Prepare inputs of kldiv_loss_grad";

    Tensor x(std::make_shared<primitive::LazyTensor>(inputs_[0][0]));
    Tensor label(std::make_shared<primitive::LazyTensor>(inputs_[1][0]));
    Tensor out_grad(std::make_shared<primitive::LazyTensor>(out_grads[0][0]));

    VLOG(6) << "Vjp prepare Prepare attributes of kldiv_loss_grad";

    std::string reduction = op->attribute("reduction").dyn_cast<pir::StrAttribute>().AsString();

    VLOG(6) << "Vjp prepare call kldiv_loss's vjp inteface";

    std::vector<std::vector<Tensor>> tensor_res =
        primitive::kldiv_loss_vjp(
        x, label, out_grad, reduction, stop_gradients);

    VLOG(6) << "Vjp prepare stop gradient of kldiv_loss_grad";

    std::vector<std::vector<pir::OpResult>> res(tensor_res.size());
    for (size_t i = 0; i < tensor_res.size(); ++i) {
        res[i].resize(tensor_res[i].size());
        for (size_t j = 0; j < tensor_res[i].size(); ++j) {
            if(tensor_res[i][j].defined()){
                res[i][j] = std::static_pointer_cast<primitive::LazyTensor>(tensor_res[i][j].impl())->value().dyn_cast<pir::OpResult>();
            }
        }
    }
    return res;
}

std::vector<std::vector<pir::OpResult>> KronOp::Vjp(pir::Operation* op, const std::vector<std::vector<pir::Value>>& inputs_, const std::vector<std::vector<pir::Value>>& outputs, const std::vector<std::vector<pir::Value>>& out_grads, const std::vector<std::vector<bool>>& stop_gradients){

    PADDLE_ENFORCE_EQ(
      inputs_.size(),
      2,
      platform::errors::InvalidArgument("kron op's inputs size should be 2, but now is %d.", inputs_.size()));
    PADDLE_ENFORCE_EQ(
      outputs.size(),
      1,
      platform::errors::InvalidArgument("kron op's outputs size should be 1, but now is %d.", outputs.size()));

    VLOG(6) << "Prepare inputs of kron_grad";

    Tensor x(std::make_shared<primitive::LazyTensor>(inputs_[0][0]));
    Tensor y(std::make_shared<primitive::LazyTensor>(inputs_[1][0]));
    Tensor out_grad(std::make_shared<primitive::LazyTensor>(out_grads[0][0]));

    VLOG(6) << "Vjp prepare Prepare attributes of kron_grad";


    VLOG(6) << "Vjp prepare call kron's vjp inteface";

    std::vector<std::vector<Tensor>> tensor_res =
        primitive::kron_vjp(
        x, y, out_grad, stop_gradients);

    VLOG(6) << "Vjp prepare stop gradient of kron_grad";

    std::vector<std::vector<pir::OpResult>> res(tensor_res.size());
    for (size_t i = 0; i < tensor_res.size(); ++i) {
        res[i].resize(tensor_res[i].size());
        for (size_t j = 0; j < tensor_res[i].size(); ++j) {
            if(tensor_res[i][j].defined()){
                res[i][j] = std::static_pointer_cast<primitive::LazyTensor>(tensor_res[i][j].impl())->value().dyn_cast<pir::OpResult>();
            }
        }
    }
    return res;
}

std::vector<std::vector<pir::OpResult>> KthvalueOp::Vjp(pir::Operation* op, const std::vector<std::vector<pir::Value>>& inputs_, const std::vector<std::vector<pir::Value>>& outputs, const std::vector<std::vector<pir::Value>>& out_grads, const std::vector<std::vector<bool>>& stop_gradients){

    PADDLE_ENFORCE_EQ(
      inputs_.size(),
      1,
      platform::errors::InvalidArgument("kthvalue op's inputs size should be 1, but now is %d.", inputs_.size()));
    PADDLE_ENFORCE_EQ(
      outputs.size(),
      2,
      platform::errors::InvalidArgument("kthvalue op's outputs size should be 2, but now is %d.", outputs.size()));

    VLOG(6) << "Prepare inputs of kthvalue_grad";

    Tensor x(std::make_shared<primitive::LazyTensor>(inputs_[0][0]));
    Tensor indices(std::make_shared<primitive::LazyTensor>(outputs[1][0]));
    Tensor out_grad(std::make_shared<primitive::LazyTensor>(out_grads[0][0]));

    VLOG(6) << "Vjp prepare Prepare attributes of kthvalue_grad";

    int k = op->attribute("k").dyn_cast<pir::Int32Attribute>().data();
    int axis = op->attribute("axis").dyn_cast<pir::Int32Attribute>().data();
    bool keepdim = op->attribute("keepdim").dyn_cast<pir::BoolAttribute>().data();

    VLOG(6) << "Vjp prepare call kthvalue's vjp inteface";

    std::vector<std::vector<Tensor>> tensor_res =
        primitive::kthvalue_vjp(
        x, indices, out_grad, k, axis, keepdim, stop_gradients);

    VLOG(6) << "Vjp prepare stop gradient of kthvalue_grad";

    std::vector<std::vector<pir::OpResult>> res(tensor_res.size());
    for (size_t i = 0; i < tensor_res.size(); ++i) {
        res[i].resize(tensor_res[i].size());
        for (size_t j = 0; j < tensor_res[i].size(); ++j) {
            if(tensor_res[i][j].defined()){
                res[i][j] = std::static_pointer_cast<primitive::LazyTensor>(tensor_res[i][j].impl())->value().dyn_cast<pir::OpResult>();
            }
        }
    }
    return res;
}

std::vector<std::vector<pir::OpResult>> LabelSmoothOp::Vjp(pir::Operation* op, const std::vector<std::vector<pir::Value>>& inputs_, const std::vector<std::vector<pir::Value>>& outputs, const std::vector<std::vector<pir::Value>>& out_grads, const std::vector<std::vector<bool>>& stop_gradients){

    PADDLE_ENFORCE_EQ(
      inputs_.size(),
      2,
      platform::errors::InvalidArgument("label_smooth op's inputs size should be 2, but now is %d.", inputs_.size()));
    PADDLE_ENFORCE_EQ(
      outputs.size(),
      1,
      platform::errors::InvalidArgument("label_smooth op's outputs size should be 1, but now is %d.", outputs.size()));

    VLOG(6) << "Prepare inputs of label_smooth_grad";

    Tensor out_grad(std::make_shared<primitive::LazyTensor>(out_grads[0][0]));

    VLOG(6) << "Vjp prepare Prepare attributes of label_smooth_grad";

    float epsilon = op->attribute("epsilon").dyn_cast<pir::FloatAttribute>().data();

    VLOG(6) << "Vjp prepare call label_smooth's vjp inteface";

    std::vector<std::vector<Tensor>> tensor_res =
        primitive::label_smooth_vjp(
        out_grad, epsilon, stop_gradients);

    VLOG(6) << "Vjp prepare stop gradient of label_smooth_grad";

    std::vector<std::vector<pir::OpResult>> res(tensor_res.size());
    for (size_t i = 0; i < tensor_res.size(); ++i) {
        res[i].resize(tensor_res[i].size());
        for (size_t j = 0; j < tensor_res[i].size(); ++j) {
            if(tensor_res[i][j].defined()){
                res[i][j] = std::static_pointer_cast<primitive::LazyTensor>(tensor_res[i][j].impl())->value().dyn_cast<pir::OpResult>();
            }
        }
    }
    return res;
}

std::vector<std::vector<pir::OpResult>> LayerNormOp::Vjp(pir::Operation* op, const std::vector<std::vector<pir::Value>>& inputs_, const std::vector<std::vector<pir::Value>>& outputs, const std::vector<std::vector<pir::Value>>& out_grads, const std::vector<std::vector<bool>>& stop_gradients){

    PADDLE_ENFORCE_EQ(
      inputs_.size(),
      3,
      platform::errors::InvalidArgument("layer_norm op's inputs size should be 3, but now is %d.", inputs_.size()));
    PADDLE_ENFORCE_EQ(
      outputs.size(),
      3,
      platform::errors::InvalidArgument("layer_norm op's outputs size should be 3, but now is %d.", outputs.size()));

    VLOG(6) << "Prepare inputs of layer_norm_grad";

    Tensor x(std::make_shared<primitive::LazyTensor>(inputs_[0][0]));
    paddle::optional<Tensor> scale;
    if (!IsEmptyValue(inputs_[1][0])){
        scale = paddle::make_optional<Tensor>(Tensor(std::make_shared<primitive::LazyTensor>(inputs_[1][0])));
    }
    paddle::optional<Tensor> bias;
    if (!IsEmptyValue(inputs_[2][0])){
        bias = paddle::make_optional<Tensor>(Tensor(std::make_shared<primitive::LazyTensor>(inputs_[2][0])));
    }
    Tensor mean(std::make_shared<primitive::LazyTensor>(outputs[1][0]));
    Tensor variance(std::make_shared<primitive::LazyTensor>(outputs[2][0]));
    Tensor out_grad(std::make_shared<primitive::LazyTensor>(out_grads[0][0]));

    VLOG(6) << "Vjp prepare Prepare attributes of layer_norm_grad";

    float epsilon = op->attribute("epsilon").dyn_cast<pir::FloatAttribute>().data();
    int begin_norm_axis = op->attribute("begin_norm_axis").dyn_cast<pir::Int32Attribute>().data();

    VLOG(6) << "Vjp prepare call layer_norm's vjp inteface";

    std::vector<std::vector<Tensor>> tensor_res =
        primitive::layer_norm_vjp(
        x, scale, bias, mean, variance, out_grad, epsilon, begin_norm_axis, stop_gradients);

    VLOG(6) << "Vjp prepare stop gradient of layer_norm_grad";

    std::vector<std::vector<pir::OpResult>> res(tensor_res.size());
    for (size_t i = 0; i < tensor_res.size(); ++i) {
        res[i].resize(tensor_res[i].size());
        for (size_t j = 0; j < tensor_res[i].size(); ++j) {
            if(tensor_res[i][j].defined()){
                res[i][j] = std::static_pointer_cast<primitive::LazyTensor>(tensor_res[i][j].impl())->value().dyn_cast<pir::OpResult>();
            }
        }
    }
    return res;
}

std::vector<std::vector<pir::OpResult>> LeakyReluOp::Vjp(pir::Operation* op, const std::vector<std::vector<pir::Value>>& inputs_, const std::vector<std::vector<pir::Value>>& outputs, const std::vector<std::vector<pir::Value>>& out_grads, const std::vector<std::vector<bool>>& stop_gradients){

    PADDLE_ENFORCE_EQ(
      inputs_.size(),
      1,
      platform::errors::InvalidArgument("leaky_relu op's inputs size should be 1, but now is %d.", inputs_.size()));
    PADDLE_ENFORCE_EQ(
      outputs.size(),
      1,
      platform::errors::InvalidArgument("leaky_relu op's outputs size should be 1, but now is %d.", outputs.size()));

    VLOG(6) << "Prepare inputs of leaky_relu_grad";

    Tensor x(std::make_shared<primitive::LazyTensor>(inputs_[0][0]));
    Tensor out_grad(std::make_shared<primitive::LazyTensor>(out_grads[0][0]));

    VLOG(6) << "Vjp prepare Prepare attributes of leaky_relu_grad";

    float negative_slope = op->attribute("negative_slope").dyn_cast<pir::FloatAttribute>().data();

    VLOG(6) << "Vjp prepare call leaky_relu's vjp inteface";

    std::vector<std::vector<Tensor>> tensor_res =
        primitive::leaky_relu_vjp(
        x, out_grad, negative_slope, stop_gradients);

    VLOG(6) << "Vjp prepare stop gradient of leaky_relu_grad";

    std::vector<std::vector<pir::OpResult>> res(tensor_res.size());
    for (size_t i = 0; i < tensor_res.size(); ++i) {
        res[i].resize(tensor_res[i].size());
        for (size_t j = 0; j < tensor_res[i].size(); ++j) {
            if(tensor_res[i][j].defined()){
                res[i][j] = std::static_pointer_cast<primitive::LazyTensor>(tensor_res[i][j].impl())->value().dyn_cast<pir::OpResult>();
            }
        }
    }
    return res;
}

std::vector<std::vector<pir::OpResult>> LeakyRelu_Op::Vjp(pir::Operation* op, const std::vector<std::vector<pir::Value>>& inputs_, const std::vector<std::vector<pir::Value>>& outputs, const std::vector<std::vector<pir::Value>>& out_grads, const std::vector<std::vector<bool>>& stop_gradients){

    PADDLE_ENFORCE_EQ(
      inputs_.size(),
      1,
      platform::errors::InvalidArgument("leaky_relu op's inputs size should be 1, but now is %d.", inputs_.size()));
    PADDLE_ENFORCE_EQ(
      outputs.size(),
      1,
      platform::errors::InvalidArgument("leaky_relu op's outputs size should be 1, but now is %d.", outputs.size()));

    VLOG(6) << "Prepare inputs of leaky_relu_grad";

    Tensor x(std::make_shared<primitive::LazyTensor>(inputs_[0][0]));
    Tensor out_grad(std::make_shared<primitive::LazyTensor>(out_grads[0][0]));

    VLOG(6) << "Vjp prepare Prepare attributes of leaky_relu_grad";

    float negative_slope = op->attribute("negative_slope").dyn_cast<pir::FloatAttribute>().data();

    VLOG(6) << "Vjp prepare call leaky_relu_'s vjp inteface";

    std::vector<std::vector<Tensor>> tensor_res =
        primitive::leaky_relu_vjp(
        x, out_grad, negative_slope, stop_gradients);

    VLOG(6) << "Vjp prepare stop gradient of leaky_relu_grad";

    std::vector<std::vector<pir::OpResult>> res(tensor_res.size());
    for (size_t i = 0; i < tensor_res.size(); ++i) {
        res[i].resize(tensor_res[i].size());
        for (size_t j = 0; j < tensor_res[i].size(); ++j) {
            if(tensor_res[i][j].defined()){
                res[i][j] = std::static_pointer_cast<primitive::LazyTensor>(tensor_res[i][j].impl())->value().dyn_cast<pir::OpResult>();
            }
        }
    }
    return res;
}

std::vector<std::vector<pir::OpResult>> LerpOp::Vjp(pir::Operation* op, const std::vector<std::vector<pir::Value>>& inputs_, const std::vector<std::vector<pir::Value>>& outputs, const std::vector<std::vector<pir::Value>>& out_grads, const std::vector<std::vector<bool>>& stop_gradients){

    PADDLE_ENFORCE_EQ(
      inputs_.size(),
      3,
      platform::errors::InvalidArgument("lerp op's inputs size should be 3, but now is %d.", inputs_.size()));
    PADDLE_ENFORCE_EQ(
      outputs.size(),
      1,
      platform::errors::InvalidArgument("lerp op's outputs size should be 1, but now is %d.", outputs.size()));

    VLOG(6) << "Prepare inputs of lerp_grad";

    Tensor x(std::make_shared<primitive::LazyTensor>(inputs_[0][0]));
    Tensor y(std::make_shared<primitive::LazyTensor>(inputs_[1][0]));
    Tensor weight(std::make_shared<primitive::LazyTensor>(inputs_[2][0]));
    Tensor out(std::make_shared<primitive::LazyTensor>(outputs[0][0]));
    Tensor out_grad(std::make_shared<primitive::LazyTensor>(out_grads[0][0]));

    VLOG(6) << "Vjp prepare Prepare attributes of lerp_grad";


    VLOG(6) << "Vjp prepare call lerp's vjp inteface";

    std::vector<std::vector<Tensor>> tensor_res =
        primitive::lerp_vjp(
        x, y, weight, out, out_grad, stop_gradients);

    VLOG(6) << "Vjp prepare stop gradient of lerp_grad";

    std::vector<std::vector<pir::OpResult>> res(tensor_res.size());
    for (size_t i = 0; i < tensor_res.size(); ++i) {
        res[i].resize(tensor_res[i].size());
        for (size_t j = 0; j < tensor_res[i].size(); ++j) {
            if(tensor_res[i][j].defined()){
                res[i][j] = std::static_pointer_cast<primitive::LazyTensor>(tensor_res[i][j].impl())->value().dyn_cast<pir::OpResult>();
            }
        }
    }
    return res;
}

std::vector<std::vector<pir::OpResult>> Lerp_Op::Vjp(pir::Operation* op, const std::vector<std::vector<pir::Value>>& inputs_, const std::vector<std::vector<pir::Value>>& outputs, const std::vector<std::vector<pir::Value>>& out_grads, const std::vector<std::vector<bool>>& stop_gradients){

    PADDLE_ENFORCE_EQ(
      inputs_.size(),
      3,
      platform::errors::InvalidArgument("lerp op's inputs size should be 3, but now is %d.", inputs_.size()));
    PADDLE_ENFORCE_EQ(
      outputs.size(),
      1,
      platform::errors::InvalidArgument("lerp op's outputs size should be 1, but now is %d.", outputs.size()));

    VLOG(6) << "Prepare inputs of lerp_grad";

    Tensor x(std::make_shared<primitive::LazyTensor>(inputs_[0][0]));
    Tensor y(std::make_shared<primitive::LazyTensor>(inputs_[1][0]));
    Tensor weight(std::make_shared<primitive::LazyTensor>(inputs_[2][0]));
    Tensor out(std::make_shared<primitive::LazyTensor>(outputs[0][0]));
    Tensor out_grad(std::make_shared<primitive::LazyTensor>(out_grads[0][0]));

    VLOG(6) << "Vjp prepare Prepare attributes of lerp_grad";


    VLOG(6) << "Vjp prepare call lerp_'s vjp inteface";

    std::vector<std::vector<Tensor>> tensor_res =
        primitive::lerp_vjp(
        x, y, weight, out, out_grad, stop_gradients);

    VLOG(6) << "Vjp prepare stop gradient of lerp_grad";

    std::vector<std::vector<pir::OpResult>> res(tensor_res.size());
    for (size_t i = 0; i < tensor_res.size(); ++i) {
        res[i].resize(tensor_res[i].size());
        for (size_t j = 0; j < tensor_res[i].size(); ++j) {
            if(tensor_res[i][j].defined()){
                res[i][j] = std::static_pointer_cast<primitive::LazyTensor>(tensor_res[i][j].impl())->value().dyn_cast<pir::OpResult>();
            }
        }
    }
    return res;
}

std::vector<std::vector<pir::OpResult>> LgammaOp::Vjp(pir::Operation* op, const std::vector<std::vector<pir::Value>>& inputs_, const std::vector<std::vector<pir::Value>>& outputs, const std::vector<std::vector<pir::Value>>& out_grads, const std::vector<std::vector<bool>>& stop_gradients){

    PADDLE_ENFORCE_EQ(
      inputs_.size(),
      1,
      platform::errors::InvalidArgument("lgamma op's inputs size should be 1, but now is %d.", inputs_.size()));
    PADDLE_ENFORCE_EQ(
      outputs.size(),
      1,
      platform::errors::InvalidArgument("lgamma op's outputs size should be 1, but now is %d.", outputs.size()));

    VLOG(6) << "Prepare inputs of lgamma_grad";

    Tensor x(std::make_shared<primitive::LazyTensor>(inputs_[0][0]));
    Tensor out_grad(std::make_shared<primitive::LazyTensor>(out_grads[0][0]));

    VLOG(6) << "Vjp prepare Prepare attributes of lgamma_grad";


    VLOG(6) << "Vjp prepare call lgamma's vjp inteface";

    std::vector<std::vector<Tensor>> tensor_res =
        primitive::lgamma_vjp(
        x, out_grad, stop_gradients);

    VLOG(6) << "Vjp prepare stop gradient of lgamma_grad";

    std::vector<std::vector<pir::OpResult>> res(tensor_res.size());
    for (size_t i = 0; i < tensor_res.size(); ++i) {
        res[i].resize(tensor_res[i].size());
        for (size_t j = 0; j < tensor_res[i].size(); ++j) {
            if(tensor_res[i][j].defined()){
                res[i][j] = std::static_pointer_cast<primitive::LazyTensor>(tensor_res[i][j].impl())->value().dyn_cast<pir::OpResult>();
            }
        }
    }
    return res;
}

std::vector<std::vector<pir::OpResult>> Lgamma_Op::Vjp(pir::Operation* op, const std::vector<std::vector<pir::Value>>& inputs_, const std::vector<std::vector<pir::Value>>& outputs, const std::vector<std::vector<pir::Value>>& out_grads, const std::vector<std::vector<bool>>& stop_gradients){

    PADDLE_ENFORCE_EQ(
      inputs_.size(),
      1,
      platform::errors::InvalidArgument("lgamma op's inputs size should be 1, but now is %d.", inputs_.size()));
    PADDLE_ENFORCE_EQ(
      outputs.size(),
      1,
      platform::errors::InvalidArgument("lgamma op's outputs size should be 1, but now is %d.", outputs.size()));

    VLOG(6) << "Prepare inputs of lgamma_grad";

    Tensor x(std::make_shared<primitive::LazyTensor>(inputs_[0][0]));
    Tensor out_grad(std::make_shared<primitive::LazyTensor>(out_grads[0][0]));

    VLOG(6) << "Vjp prepare Prepare attributes of lgamma_grad";


    VLOG(6) << "Vjp prepare call lgamma_'s vjp inteface";

    std::vector<std::vector<Tensor>> tensor_res =
        primitive::lgamma_vjp(
        x, out_grad, stop_gradients);

    VLOG(6) << "Vjp prepare stop gradient of lgamma_grad";

    std::vector<std::vector<pir::OpResult>> res(tensor_res.size());
    for (size_t i = 0; i < tensor_res.size(); ++i) {
        res[i].resize(tensor_res[i].size());
        for (size_t j = 0; j < tensor_res[i].size(); ++j) {
            if(tensor_res[i][j].defined()){
                res[i][j] = std::static_pointer_cast<primitive::LazyTensor>(tensor_res[i][j].impl())->value().dyn_cast<pir::OpResult>();
            }
        }
    }
    return res;
}

std::vector<std::vector<pir::OpResult>> LinearInterpOp::Vjp(pir::Operation* op, const std::vector<std::vector<pir::Value>>& inputs_, const std::vector<std::vector<pir::Value>>& outputs, const std::vector<std::vector<pir::Value>>& out_grads, const std::vector<std::vector<bool>>& stop_gradients){

    PADDLE_ENFORCE_EQ(
      inputs_.size(),
      4,
      platform::errors::InvalidArgument("linear_interp op's inputs size should be 4, but now is %d.", inputs_.size()));
    PADDLE_ENFORCE_EQ(
      outputs.size(),
      1,
      platform::errors::InvalidArgument("linear_interp op's outputs size should be 1, but now is %d.", outputs.size()));

    VLOG(6) << "Prepare inputs of linear_interp_grad";

    Tensor x(std::make_shared<primitive::LazyTensor>(inputs_[0][0]));
    paddle::optional<Tensor> out_size;
    if (!IsEmptyValue(inputs_[1][0])){
        out_size = paddle::make_optional<Tensor>(Tensor(std::make_shared<primitive::LazyTensor>(inputs_[1][0])));
    }
    paddle::optional<std::vector<Tensor>> size_tensor;
    std::vector<Tensor> optional_size_tensor;
    if (!IsEmptyValue(inputs_[2][0])){
        for (size_t idx = 0; idx < inputs_[2].size(); idx++) {
            optional_size_tensor.emplace_back(
                std::make_shared<primitive::LazyTensor>(inputs_[2][idx]));
        }
        size_tensor = paddle::make_optional<std::vector<Tensor>>(optional_size_tensor);
    }
    paddle::optional<Tensor> scale_tensor;
    if (!IsEmptyValue(inputs_[3][0])){
        scale_tensor = paddle::make_optional<Tensor>(Tensor(std::make_shared<primitive::LazyTensor>(inputs_[3][0])));
    }
    Tensor output_grad(std::make_shared<primitive::LazyTensor>(out_grads[0][0]));

    VLOG(6) << "Vjp prepare Prepare attributes of linear_interp_grad";

    std::string data_layout = op->attribute("data_layout").dyn_cast<pir::StrAttribute>().AsString();
    int out_d = op->attribute("out_d").dyn_cast<pir::Int32Attribute>().data();
    int out_h = op->attribute("out_h").dyn_cast<pir::Int32Attribute>().data();
    int out_w = op->attribute("out_w").dyn_cast<pir::Int32Attribute>().data();
    std::vector<float> scale;
    for (size_t i = 0; i < op->attribute("scale").dyn_cast<pir::ArrayAttribute>().size(); i++) {
        scale.push_back(op->attribute("scale").dyn_cast<pir::ArrayAttribute>().at(i).dyn_cast<pir::FloatAttribute>().data());
    }
    std::string interp_method = op->attribute("interp_method").dyn_cast<pir::StrAttribute>().AsString();
    bool align_corners = op->attribute("align_corners").dyn_cast<pir::BoolAttribute>().data();
    int align_mode = op->attribute("align_mode").dyn_cast<pir::Int32Attribute>().data();

    VLOG(6) << "Vjp prepare call linear_interp's vjp inteface";

    std::vector<std::vector<Tensor>> tensor_res =
        primitive::linear_interp_vjp(
        x, out_size, size_tensor, scale_tensor, output_grad, data_layout, out_d, out_h, out_w, scale, interp_method, align_corners, align_mode, stop_gradients);

    VLOG(6) << "Vjp prepare stop gradient of linear_interp_grad";

    std::vector<std::vector<pir::OpResult>> res(tensor_res.size());
    for (size_t i = 0; i < tensor_res.size(); ++i) {
        res[i].resize(tensor_res[i].size());
        for (size_t j = 0; j < tensor_res[i].size(); ++j) {
            if(tensor_res[i][j].defined()){
                res[i][j] = std::static_pointer_cast<primitive::LazyTensor>(tensor_res[i][j].impl())->value().dyn_cast<pir::OpResult>();
            }
        }
    }
    return res;
}

std::vector<std::vector<pir::OpResult>> LogOp::Vjp(pir::Operation* op, const std::vector<std::vector<pir::Value>>& inputs_, const std::vector<std::vector<pir::Value>>& outputs, const std::vector<std::vector<pir::Value>>& out_grads, const std::vector<std::vector<bool>>& stop_gradients){

    PADDLE_ENFORCE_EQ(
      inputs_.size(),
      1,
      platform::errors::InvalidArgument("log op's inputs size should be 1, but now is %d.", inputs_.size()));
    PADDLE_ENFORCE_EQ(
      outputs.size(),
      1,
      platform::errors::InvalidArgument("log op's outputs size should be 1, but now is %d.", outputs.size()));

    VLOG(6) << "Prepare inputs of log_grad";

    Tensor x(std::make_shared<primitive::LazyTensor>(inputs_[0][0]));
    Tensor out_grad(std::make_shared<primitive::LazyTensor>(out_grads[0][0]));

    VLOG(6) << "Vjp prepare Prepare attributes of log_grad";


    VLOG(6) << "Vjp prepare call log's vjp inteface";

    std::vector<std::vector<Tensor>> tensor_res =
        primitive::log_vjp(
        x, out_grad, stop_gradients);

    VLOG(6) << "Vjp prepare stop gradient of log_grad";

    std::vector<std::vector<pir::OpResult>> res(tensor_res.size());
    for (size_t i = 0; i < tensor_res.size(); ++i) {
        res[i].resize(tensor_res[i].size());
        for (size_t j = 0; j < tensor_res[i].size(); ++j) {
            if(tensor_res[i][j].defined()){
                res[i][j] = std::static_pointer_cast<primitive::LazyTensor>(tensor_res[i][j].impl())->value().dyn_cast<pir::OpResult>();
            }
        }
    }
    return res;
}

std::vector<std::vector<pir::OpResult>> Log_Op::Vjp(pir::Operation* op, const std::vector<std::vector<pir::Value>>& inputs_, const std::vector<std::vector<pir::Value>>& outputs, const std::vector<std::vector<pir::Value>>& out_grads, const std::vector<std::vector<bool>>& stop_gradients){

    PADDLE_ENFORCE_EQ(
      inputs_.size(),
      1,
      platform::errors::InvalidArgument("log op's inputs size should be 1, but now is %d.", inputs_.size()));
    PADDLE_ENFORCE_EQ(
      outputs.size(),
      1,
      platform::errors::InvalidArgument("log op's outputs size should be 1, but now is %d.", outputs.size()));

    VLOG(6) << "Prepare inputs of log_grad";

    Tensor x(std::make_shared<primitive::LazyTensor>(inputs_[0][0]));
    Tensor out_grad(std::make_shared<primitive::LazyTensor>(out_grads[0][0]));

    VLOG(6) << "Vjp prepare Prepare attributes of log_grad";


    VLOG(6) << "Vjp prepare call log_'s vjp inteface";

    std::vector<std::vector<Tensor>> tensor_res =
        primitive::log_vjp(
        x, out_grad, stop_gradients);

    VLOG(6) << "Vjp prepare stop gradient of log_grad";

    std::vector<std::vector<pir::OpResult>> res(tensor_res.size());
    for (size_t i = 0; i < tensor_res.size(); ++i) {
        res[i].resize(tensor_res[i].size());
        for (size_t j = 0; j < tensor_res[i].size(); ++j) {
            if(tensor_res[i][j].defined()){
                res[i][j] = std::static_pointer_cast<primitive::LazyTensor>(tensor_res[i][j].impl())->value().dyn_cast<pir::OpResult>();
            }
        }
    }
    return res;
}

std::vector<std::vector<pir::OpResult>> Log10Op::Vjp(pir::Operation* op, const std::vector<std::vector<pir::Value>>& inputs_, const std::vector<std::vector<pir::Value>>& outputs, const std::vector<std::vector<pir::Value>>& out_grads, const std::vector<std::vector<bool>>& stop_gradients){

    PADDLE_ENFORCE_EQ(
      inputs_.size(),
      1,
      platform::errors::InvalidArgument("log10 op's inputs size should be 1, but now is %d.", inputs_.size()));
    PADDLE_ENFORCE_EQ(
      outputs.size(),
      1,
      platform::errors::InvalidArgument("log10 op's outputs size should be 1, but now is %d.", outputs.size()));

    VLOG(6) << "Prepare inputs of log10_grad";

    Tensor x(std::make_shared<primitive::LazyTensor>(inputs_[0][0]));
    Tensor out_grad(std::make_shared<primitive::LazyTensor>(out_grads[0][0]));

    VLOG(6) << "Vjp prepare Prepare attributes of log10_grad";


    VLOG(6) << "Vjp prepare call log10's vjp inteface";

    std::vector<std::vector<Tensor>> tensor_res =
        primitive::log10_vjp(
        x, out_grad, stop_gradients);

    VLOG(6) << "Vjp prepare stop gradient of log10_grad";

    std::vector<std::vector<pir::OpResult>> res(tensor_res.size());
    for (size_t i = 0; i < tensor_res.size(); ++i) {
        res[i].resize(tensor_res[i].size());
        for (size_t j = 0; j < tensor_res[i].size(); ++j) {
            if(tensor_res[i][j].defined()){
                res[i][j] = std::static_pointer_cast<primitive::LazyTensor>(tensor_res[i][j].impl())->value().dyn_cast<pir::OpResult>();
            }
        }
    }
    return res;
}

std::vector<std::vector<pir::OpResult>> Log10_Op::Vjp(pir::Operation* op, const std::vector<std::vector<pir::Value>>& inputs_, const std::vector<std::vector<pir::Value>>& outputs, const std::vector<std::vector<pir::Value>>& out_grads, const std::vector<std::vector<bool>>& stop_gradients){

    PADDLE_ENFORCE_EQ(
      inputs_.size(),
      1,
      platform::errors::InvalidArgument("log10 op's inputs size should be 1, but now is %d.", inputs_.size()));
    PADDLE_ENFORCE_EQ(
      outputs.size(),
      1,
      platform::errors::InvalidArgument("log10 op's outputs size should be 1, but now is %d.", outputs.size()));

    VLOG(6) << "Prepare inputs of log10_grad";

    Tensor x(std::make_shared<primitive::LazyTensor>(inputs_[0][0]));
    Tensor out_grad(std::make_shared<primitive::LazyTensor>(out_grads[0][0]));

    VLOG(6) << "Vjp prepare Prepare attributes of log10_grad";


    VLOG(6) << "Vjp prepare call log10_'s vjp inteface";

    std::vector<std::vector<Tensor>> tensor_res =
        primitive::log10_vjp(
        x, out_grad, stop_gradients);

    VLOG(6) << "Vjp prepare stop gradient of log10_grad";

    std::vector<std::vector<pir::OpResult>> res(tensor_res.size());
    for (size_t i = 0; i < tensor_res.size(); ++i) {
        res[i].resize(tensor_res[i].size());
        for (size_t j = 0; j < tensor_res[i].size(); ++j) {
            if(tensor_res[i][j].defined()){
                res[i][j] = std::static_pointer_cast<primitive::LazyTensor>(tensor_res[i][j].impl())->value().dyn_cast<pir::OpResult>();
            }
        }
    }
    return res;
}

std::vector<std::vector<pir::OpResult>> Log1pOp::Vjp(pir::Operation* op, const std::vector<std::vector<pir::Value>>& inputs_, const std::vector<std::vector<pir::Value>>& outputs, const std::vector<std::vector<pir::Value>>& out_grads, const std::vector<std::vector<bool>>& stop_gradients){

    PADDLE_ENFORCE_EQ(
      inputs_.size(),
      1,
      platform::errors::InvalidArgument("log1p op's inputs size should be 1, but now is %d.", inputs_.size()));
    PADDLE_ENFORCE_EQ(
      outputs.size(),
      1,
      platform::errors::InvalidArgument("log1p op's outputs size should be 1, but now is %d.", outputs.size()));

    VLOG(6) << "Prepare inputs of log1p_grad";

    Tensor x(std::make_shared<primitive::LazyTensor>(inputs_[0][0]));
    Tensor out_grad(std::make_shared<primitive::LazyTensor>(out_grads[0][0]));

    VLOG(6) << "Vjp prepare Prepare attributes of log1p_grad";


    VLOG(6) << "Vjp prepare call log1p's vjp inteface";

    std::vector<std::vector<Tensor>> tensor_res =
        primitive::log1p_vjp(
        x, out_grad, stop_gradients);

    VLOG(6) << "Vjp prepare stop gradient of log1p_grad";

    std::vector<std::vector<pir::OpResult>> res(tensor_res.size());
    for (size_t i = 0; i < tensor_res.size(); ++i) {
        res[i].resize(tensor_res[i].size());
        for (size_t j = 0; j < tensor_res[i].size(); ++j) {
            if(tensor_res[i][j].defined()){
                res[i][j] = std::static_pointer_cast<primitive::LazyTensor>(tensor_res[i][j].impl())->value().dyn_cast<pir::OpResult>();
            }
        }
    }
    return res;
}

std::vector<std::vector<pir::OpResult>> Log1p_Op::Vjp(pir::Operation* op, const std::vector<std::vector<pir::Value>>& inputs_, const std::vector<std::vector<pir::Value>>& outputs, const std::vector<std::vector<pir::Value>>& out_grads, const std::vector<std::vector<bool>>& stop_gradients){

    PADDLE_ENFORCE_EQ(
      inputs_.size(),
      1,
      platform::errors::InvalidArgument("log1p op's inputs size should be 1, but now is %d.", inputs_.size()));
    PADDLE_ENFORCE_EQ(
      outputs.size(),
      1,
      platform::errors::InvalidArgument("log1p op's outputs size should be 1, but now is %d.", outputs.size()));

    VLOG(6) << "Prepare inputs of log1p_grad";

    Tensor x(std::make_shared<primitive::LazyTensor>(inputs_[0][0]));
    Tensor out_grad(std::make_shared<primitive::LazyTensor>(out_grads[0][0]));

    VLOG(6) << "Vjp prepare Prepare attributes of log1p_grad";


    VLOG(6) << "Vjp prepare call log1p_'s vjp inteface";

    std::vector<std::vector<Tensor>> tensor_res =
        primitive::log1p_vjp(
        x, out_grad, stop_gradients);

    VLOG(6) << "Vjp prepare stop gradient of log1p_grad";

    std::vector<std::vector<pir::OpResult>> res(tensor_res.size());
    for (size_t i = 0; i < tensor_res.size(); ++i) {
        res[i].resize(tensor_res[i].size());
        for (size_t j = 0; j < tensor_res[i].size(); ++j) {
            if(tensor_res[i][j].defined()){
                res[i][j] = std::static_pointer_cast<primitive::LazyTensor>(tensor_res[i][j].impl())->value().dyn_cast<pir::OpResult>();
            }
        }
    }
    return res;
}

std::vector<std::vector<pir::OpResult>> Log2Op::Vjp(pir::Operation* op, const std::vector<std::vector<pir::Value>>& inputs_, const std::vector<std::vector<pir::Value>>& outputs, const std::vector<std::vector<pir::Value>>& out_grads, const std::vector<std::vector<bool>>& stop_gradients){

    PADDLE_ENFORCE_EQ(
      inputs_.size(),
      1,
      platform::errors::InvalidArgument("log2 op's inputs size should be 1, but now is %d.", inputs_.size()));
    PADDLE_ENFORCE_EQ(
      outputs.size(),
      1,
      platform::errors::InvalidArgument("log2 op's outputs size should be 1, but now is %d.", outputs.size()));

    VLOG(6) << "Prepare inputs of log2_grad";

    Tensor x(std::make_shared<primitive::LazyTensor>(inputs_[0][0]));
    Tensor out_grad(std::make_shared<primitive::LazyTensor>(out_grads[0][0]));

    VLOG(6) << "Vjp prepare Prepare attributes of log2_grad";


    VLOG(6) << "Vjp prepare call log2's vjp inteface";

    std::vector<std::vector<Tensor>> tensor_res =
        primitive::log2_vjp(
        x, out_grad, stop_gradients);

    VLOG(6) << "Vjp prepare stop gradient of log2_grad";

    std::vector<std::vector<pir::OpResult>> res(tensor_res.size());
    for (size_t i = 0; i < tensor_res.size(); ++i) {
        res[i].resize(tensor_res[i].size());
        for (size_t j = 0; j < tensor_res[i].size(); ++j) {
            if(tensor_res[i][j].defined()){
                res[i][j] = std::static_pointer_cast<primitive::LazyTensor>(tensor_res[i][j].impl())->value().dyn_cast<pir::OpResult>();
            }
        }
    }
    return res;
}

std::vector<std::vector<pir::OpResult>> Log2_Op::Vjp(pir::Operation* op, const std::vector<std::vector<pir::Value>>& inputs_, const std::vector<std::vector<pir::Value>>& outputs, const std::vector<std::vector<pir::Value>>& out_grads, const std::vector<std::vector<bool>>& stop_gradients){

    PADDLE_ENFORCE_EQ(
      inputs_.size(),
      1,
      platform::errors::InvalidArgument("log2 op's inputs size should be 1, but now is %d.", inputs_.size()));
    PADDLE_ENFORCE_EQ(
      outputs.size(),
      1,
      platform::errors::InvalidArgument("log2 op's outputs size should be 1, but now is %d.", outputs.size()));

    VLOG(6) << "Prepare inputs of log2_grad";

    Tensor x(std::make_shared<primitive::LazyTensor>(inputs_[0][0]));
    Tensor out_grad(std::make_shared<primitive::LazyTensor>(out_grads[0][0]));

    VLOG(6) << "Vjp prepare Prepare attributes of log2_grad";


    VLOG(6) << "Vjp prepare call log2_'s vjp inteface";

    std::vector<std::vector<Tensor>> tensor_res =
        primitive::log2_vjp(
        x, out_grad, stop_gradients);

    VLOG(6) << "Vjp prepare stop gradient of log2_grad";

    std::vector<std::vector<pir::OpResult>> res(tensor_res.size());
    for (size_t i = 0; i < tensor_res.size(); ++i) {
        res[i].resize(tensor_res[i].size());
        for (size_t j = 0; j < tensor_res[i].size(); ++j) {
            if(tensor_res[i][j].defined()){
                res[i][j] = std::static_pointer_cast<primitive::LazyTensor>(tensor_res[i][j].impl())->value().dyn_cast<pir::OpResult>();
            }
        }
    }
    return res;
}

std::vector<std::vector<pir::OpResult>> LogLossOp::Vjp(pir::Operation* op, const std::vector<std::vector<pir::Value>>& inputs_, const std::vector<std::vector<pir::Value>>& outputs, const std::vector<std::vector<pir::Value>>& out_grads, const std::vector<std::vector<bool>>& stop_gradients){

    PADDLE_ENFORCE_EQ(
      inputs_.size(),
      2,
      platform::errors::InvalidArgument("log_loss op's inputs size should be 2, but now is %d.", inputs_.size()));
    PADDLE_ENFORCE_EQ(
      outputs.size(),
      1,
      platform::errors::InvalidArgument("log_loss op's outputs size should be 1, but now is %d.", outputs.size()));

    VLOG(6) << "Prepare inputs of log_loss_grad";

    Tensor input(std::make_shared<primitive::LazyTensor>(inputs_[0][0]));
    Tensor label(std::make_shared<primitive::LazyTensor>(inputs_[1][0]));
    Tensor out_grad(std::make_shared<primitive::LazyTensor>(out_grads[0][0]));

    VLOG(6) << "Vjp prepare Prepare attributes of log_loss_grad";

    float epsilon = op->attribute("epsilon").dyn_cast<pir::FloatAttribute>().data();

    VLOG(6) << "Vjp prepare call log_loss's vjp inteface";

    std::vector<std::vector<Tensor>> tensor_res =
        primitive::log_loss_vjp(
        input, label, out_grad, epsilon, stop_gradients);

    VLOG(6) << "Vjp prepare stop gradient of log_loss_grad";

    std::vector<std::vector<pir::OpResult>> res(tensor_res.size());
    for (size_t i = 0; i < tensor_res.size(); ++i) {
        res[i].resize(tensor_res[i].size());
        for (size_t j = 0; j < tensor_res[i].size(); ++j) {
            if(tensor_res[i][j].defined()){
                res[i][j] = std::static_pointer_cast<primitive::LazyTensor>(tensor_res[i][j].impl())->value().dyn_cast<pir::OpResult>();
            }
        }
    }
    return res;
}

std::vector<std::vector<pir::OpResult>> LogSoftmaxOp::Vjp(pir::Operation* op, const std::vector<std::vector<pir::Value>>& inputs_, const std::vector<std::vector<pir::Value>>& outputs, const std::vector<std::vector<pir::Value>>& out_grads, const std::vector<std::vector<bool>>& stop_gradients){

    PADDLE_ENFORCE_EQ(
      inputs_.size(),
      1,
      platform::errors::InvalidArgument("log_softmax op's inputs size should be 1, but now is %d.", inputs_.size()));
    PADDLE_ENFORCE_EQ(
      outputs.size(),
      1,
      platform::errors::InvalidArgument("log_softmax op's outputs size should be 1, but now is %d.", outputs.size()));

    VLOG(6) << "Prepare inputs of log_softmax_grad";

    Tensor out(std::make_shared<primitive::LazyTensor>(outputs[0][0]));
    Tensor out_grad(std::make_shared<primitive::LazyTensor>(out_grads[0][0]));

    VLOG(6) << "Vjp prepare Prepare attributes of log_softmax_grad";

    int axis = op->attribute("axis").dyn_cast<pir::Int32Attribute>().data();

    VLOG(6) << "Vjp prepare call log_softmax's vjp inteface";

    std::vector<std::vector<Tensor>> tensor_res =
        primitive::log_softmax_vjp(
        out, out_grad, axis, stop_gradients);

    VLOG(6) << "Vjp prepare stop gradient of log_softmax_grad";

    std::vector<std::vector<pir::OpResult>> res(tensor_res.size());
    for (size_t i = 0; i < tensor_res.size(); ++i) {
        res[i].resize(tensor_res[i].size());
        for (size_t j = 0; j < tensor_res[i].size(); ++j) {
            if(tensor_res[i][j].defined()){
                res[i][j] = std::static_pointer_cast<primitive::LazyTensor>(tensor_res[i][j].impl())->value().dyn_cast<pir::OpResult>();
            }
        }
    }
    return res;
}

std::vector<std::vector<pir::OpResult>> LogcumsumexpOp::Vjp(pir::Operation* op, const std::vector<std::vector<pir::Value>>& inputs_, const std::vector<std::vector<pir::Value>>& outputs, const std::vector<std::vector<pir::Value>>& out_grads, const std::vector<std::vector<bool>>& stop_gradients){

    PADDLE_ENFORCE_EQ(
      inputs_.size(),
      1,
      platform::errors::InvalidArgument("logcumsumexp op's inputs size should be 1, but now is %d.", inputs_.size()));
    PADDLE_ENFORCE_EQ(
      outputs.size(),
      1,
      platform::errors::InvalidArgument("logcumsumexp op's outputs size should be 1, but now is %d.", outputs.size()));

    VLOG(6) << "Prepare inputs of logcumsumexp_grad";

    Tensor x(std::make_shared<primitive::LazyTensor>(inputs_[0][0]));
    Tensor out(std::make_shared<primitive::LazyTensor>(outputs[0][0]));
    Tensor out_grad(std::make_shared<primitive::LazyTensor>(out_grads[0][0]));

    VLOG(6) << "Vjp prepare Prepare attributes of logcumsumexp_grad";

    int axis = op->attribute("axis").dyn_cast<pir::Int32Attribute>().data();
    bool flatten = op->attribute("flatten").dyn_cast<pir::BoolAttribute>().data();
    bool exclusive = op->attribute("exclusive").dyn_cast<pir::BoolAttribute>().data();
    bool reverse = op->attribute("reverse").dyn_cast<pir::BoolAttribute>().data();

    VLOG(6) << "Vjp prepare call logcumsumexp's vjp inteface";

    std::vector<std::vector<Tensor>> tensor_res =
        primitive::logcumsumexp_vjp(
        x, out, out_grad, axis, flatten, exclusive, reverse, stop_gradients);

    VLOG(6) << "Vjp prepare stop gradient of logcumsumexp_grad";

    std::vector<std::vector<pir::OpResult>> res(tensor_res.size());
    for (size_t i = 0; i < tensor_res.size(); ++i) {
        res[i].resize(tensor_res[i].size());
        for (size_t j = 0; j < tensor_res[i].size(); ++j) {
            if(tensor_res[i][j].defined()){
                res[i][j] = std::static_pointer_cast<primitive::LazyTensor>(tensor_res[i][j].impl())->value().dyn_cast<pir::OpResult>();
            }
        }
    }
    return res;
}

std::vector<std::vector<pir::OpResult>> LogitOp::Vjp(pir::Operation* op, const std::vector<std::vector<pir::Value>>& inputs_, const std::vector<std::vector<pir::Value>>& outputs, const std::vector<std::vector<pir::Value>>& out_grads, const std::vector<std::vector<bool>>& stop_gradients){

    PADDLE_ENFORCE_EQ(
      inputs_.size(),
      1,
      platform::errors::InvalidArgument("logit op's inputs size should be 1, but now is %d.", inputs_.size()));
    PADDLE_ENFORCE_EQ(
      outputs.size(),
      1,
      platform::errors::InvalidArgument("logit op's outputs size should be 1, but now is %d.", outputs.size()));

    VLOG(6) << "Prepare inputs of logit_grad";

    Tensor x(std::make_shared<primitive::LazyTensor>(inputs_[0][0]));
    Tensor out_grad(std::make_shared<primitive::LazyTensor>(out_grads[0][0]));

    VLOG(6) << "Vjp prepare Prepare attributes of logit_grad";

    float eps = op->attribute("eps").dyn_cast<pir::FloatAttribute>().data();

    VLOG(6) << "Vjp prepare call logit's vjp inteface";

    std::vector<std::vector<Tensor>> tensor_res =
        primitive::logit_vjp(
        x, out_grad, eps, stop_gradients);

    VLOG(6) << "Vjp prepare stop gradient of logit_grad";

    std::vector<std::vector<pir::OpResult>> res(tensor_res.size());
    for (size_t i = 0; i < tensor_res.size(); ++i) {
        res[i].resize(tensor_res[i].size());
        for (size_t j = 0; j < tensor_res[i].size(); ++j) {
            if(tensor_res[i][j].defined()){
                res[i][j] = std::static_pointer_cast<primitive::LazyTensor>(tensor_res[i][j].impl())->value().dyn_cast<pir::OpResult>();
            }
        }
    }
    return res;
}

std::vector<std::vector<pir::OpResult>> Logit_Op::Vjp(pir::Operation* op, const std::vector<std::vector<pir::Value>>& inputs_, const std::vector<std::vector<pir::Value>>& outputs, const std::vector<std::vector<pir::Value>>& out_grads, const std::vector<std::vector<bool>>& stop_gradients){

    PADDLE_ENFORCE_EQ(
      inputs_.size(),
      1,
      platform::errors::InvalidArgument("logit op's inputs size should be 1, but now is %d.", inputs_.size()));
    PADDLE_ENFORCE_EQ(
      outputs.size(),
      1,
      platform::errors::InvalidArgument("logit op's outputs size should be 1, but now is %d.", outputs.size()));

    VLOG(6) << "Prepare inputs of logit_grad";

    Tensor x(std::make_shared<primitive::LazyTensor>(inputs_[0][0]));
    Tensor out_grad(std::make_shared<primitive::LazyTensor>(out_grads[0][0]));

    VLOG(6) << "Vjp prepare Prepare attributes of logit_grad";

    float eps = op->attribute("eps").dyn_cast<pir::FloatAttribute>().data();

    VLOG(6) << "Vjp prepare call logit_'s vjp inteface";

    std::vector<std::vector<Tensor>> tensor_res =
        primitive::logit_vjp(
        x, out_grad, eps, stop_gradients);

    VLOG(6) << "Vjp prepare stop gradient of logit_grad";

    std::vector<std::vector<pir::OpResult>> res(tensor_res.size());
    for (size_t i = 0; i < tensor_res.size(); ++i) {
        res[i].resize(tensor_res[i].size());
        for (size_t j = 0; j < tensor_res[i].size(); ++j) {
            if(tensor_res[i][j].defined()){
                res[i][j] = std::static_pointer_cast<primitive::LazyTensor>(tensor_res[i][j].impl())->value().dyn_cast<pir::OpResult>();
            }
        }
    }
    return res;
}

std::vector<std::vector<pir::OpResult>> LogsigmoidOp::Vjp(pir::Operation* op, const std::vector<std::vector<pir::Value>>& inputs_, const std::vector<std::vector<pir::Value>>& outputs, const std::vector<std::vector<pir::Value>>& out_grads, const std::vector<std::vector<bool>>& stop_gradients){

    PADDLE_ENFORCE_EQ(
      inputs_.size(),
      1,
      platform::errors::InvalidArgument("logsigmoid op's inputs size should be 1, but now is %d.", inputs_.size()));
    PADDLE_ENFORCE_EQ(
      outputs.size(),
      1,
      platform::errors::InvalidArgument("logsigmoid op's outputs size should be 1, but now is %d.", outputs.size()));

    VLOG(6) << "Prepare inputs of logsigmoid_grad";

    Tensor x(std::make_shared<primitive::LazyTensor>(inputs_[0][0]));
    Tensor out_grad(std::make_shared<primitive::LazyTensor>(out_grads[0][0]));

    VLOG(6) << "Vjp prepare Prepare attributes of logsigmoid_grad";


    VLOG(6) << "Vjp prepare call logsigmoid's vjp inteface";

    std::vector<std::vector<Tensor>> tensor_res =
        primitive::logsigmoid_vjp(
        x, out_grad, stop_gradients);

    VLOG(6) << "Vjp prepare stop gradient of logsigmoid_grad";

    std::vector<std::vector<pir::OpResult>> res(tensor_res.size());
    for (size_t i = 0; i < tensor_res.size(); ++i) {
        res[i].resize(tensor_res[i].size());
        for (size_t j = 0; j < tensor_res[i].size(); ++j) {
            if(tensor_res[i][j].defined()){
                res[i][j] = std::static_pointer_cast<primitive::LazyTensor>(tensor_res[i][j].impl())->value().dyn_cast<pir::OpResult>();
            }
        }
    }
    return res;
}

std::vector<std::vector<pir::OpResult>> LuOp::Vjp(pir::Operation* op, const std::vector<std::vector<pir::Value>>& inputs_, const std::vector<std::vector<pir::Value>>& outputs, const std::vector<std::vector<pir::Value>>& out_grads, const std::vector<std::vector<bool>>& stop_gradients){

    PADDLE_ENFORCE_EQ(
      inputs_.size(),
      1,
      platform::errors::InvalidArgument("lu op's inputs size should be 1, but now is %d.", inputs_.size()));
    PADDLE_ENFORCE_EQ(
      outputs.size(),
      3,
      platform::errors::InvalidArgument("lu op's outputs size should be 3, but now is %d.", outputs.size()));

    VLOG(6) << "Prepare inputs of lu_grad";

    Tensor x(std::make_shared<primitive::LazyTensor>(inputs_[0][0]));
    Tensor out(std::make_shared<primitive::LazyTensor>(outputs[0][0]));
    Tensor pivots(std::make_shared<primitive::LazyTensor>(outputs[1][0]));
    Tensor out_grad(std::make_shared<primitive::LazyTensor>(out_grads[0][0]));

    VLOG(6) << "Vjp prepare Prepare attributes of lu_grad";

    bool pivot = op->attribute("pivot").dyn_cast<pir::BoolAttribute>().data();

    VLOG(6) << "Vjp prepare call lu's vjp inteface";

    std::vector<std::vector<Tensor>> tensor_res =
        primitive::lu_vjp(
        x, out, pivots, out_grad, pivot, stop_gradients);

    VLOG(6) << "Vjp prepare stop gradient of lu_grad";

    std::vector<std::vector<pir::OpResult>> res(tensor_res.size());
    for (size_t i = 0; i < tensor_res.size(); ++i) {
        res[i].resize(tensor_res[i].size());
        for (size_t j = 0; j < tensor_res[i].size(); ++j) {
            if(tensor_res[i][j].defined()){
                res[i][j] = std::static_pointer_cast<primitive::LazyTensor>(tensor_res[i][j].impl())->value().dyn_cast<pir::OpResult>();
            }
        }
    }
    return res;
}

std::vector<std::vector<pir::OpResult>> Lu_Op::Vjp(pir::Operation* op, const std::vector<std::vector<pir::Value>>& inputs_, const std::vector<std::vector<pir::Value>>& outputs, const std::vector<std::vector<pir::Value>>& out_grads, const std::vector<std::vector<bool>>& stop_gradients){

    PADDLE_ENFORCE_EQ(
      inputs_.size(),
      1,
      platform::errors::InvalidArgument("lu op's inputs size should be 1, but now is %d.", inputs_.size()));
    PADDLE_ENFORCE_EQ(
      outputs.size(),
      3,
      platform::errors::InvalidArgument("lu op's outputs size should be 3, but now is %d.", outputs.size()));

    VLOG(6) << "Prepare inputs of lu_grad";

    Tensor x(std::make_shared<primitive::LazyTensor>(inputs_[0][0]));
    Tensor out(std::make_shared<primitive::LazyTensor>(outputs[0][0]));
    Tensor pivots(std::make_shared<primitive::LazyTensor>(outputs[1][0]));
    Tensor out_grad(std::make_shared<primitive::LazyTensor>(out_grads[0][0]));

    VLOG(6) << "Vjp prepare Prepare attributes of lu_grad";

    bool pivot = op->attribute("pivot").dyn_cast<pir::BoolAttribute>().data();

    VLOG(6) << "Vjp prepare call lu_'s vjp inteface";

    std::vector<std::vector<Tensor>> tensor_res =
        primitive::lu_vjp(
        x, out, pivots, out_grad, pivot, stop_gradients);

    VLOG(6) << "Vjp prepare stop gradient of lu_grad";

    std::vector<std::vector<pir::OpResult>> res(tensor_res.size());
    for (size_t i = 0; i < tensor_res.size(); ++i) {
        res[i].resize(tensor_res[i].size());
        for (size_t j = 0; j < tensor_res[i].size(); ++j) {
            if(tensor_res[i][j].defined()){
                res[i][j] = std::static_pointer_cast<primitive::LazyTensor>(tensor_res[i][j].impl())->value().dyn_cast<pir::OpResult>();
            }
        }
    }
    return res;
}

std::vector<std::vector<pir::OpResult>> LuUnpackOp::Vjp(pir::Operation* op, const std::vector<std::vector<pir::Value>>& inputs_, const std::vector<std::vector<pir::Value>>& outputs, const std::vector<std::vector<pir::Value>>& out_grads, const std::vector<std::vector<bool>>& stop_gradients){

    PADDLE_ENFORCE_EQ(
      inputs_.size(),
      2,
      platform::errors::InvalidArgument("lu_unpack op's inputs size should be 2, but now is %d.", inputs_.size()));
    PADDLE_ENFORCE_EQ(
      outputs.size(),
      3,
      platform::errors::InvalidArgument("lu_unpack op's outputs size should be 3, but now is %d.", outputs.size()));

    VLOG(6) << "Prepare inputs of lu_unpack_grad";

    Tensor x(std::make_shared<primitive::LazyTensor>(inputs_[0][0]));
    Tensor y(std::make_shared<primitive::LazyTensor>(inputs_[1][0]));
    Tensor l(std::make_shared<primitive::LazyTensor>(outputs[1][0]));
    Tensor u(std::make_shared<primitive::LazyTensor>(outputs[2][0]));
    Tensor pmat(std::make_shared<primitive::LazyTensor>(outputs[0][0]));
    Tensor l_grad(std::make_shared<primitive::LazyTensor>(out_grads[1][0]));
    Tensor u_grad(std::make_shared<primitive::LazyTensor>(out_grads[2][0]));

    VLOG(6) << "Vjp prepare Prepare attributes of lu_unpack_grad";

    bool unpack_ludata = op->attribute("unpack_ludata").dyn_cast<pir::BoolAttribute>().data();
    bool unpack_pivots = op->attribute("unpack_pivots").dyn_cast<pir::BoolAttribute>().data();

    VLOG(6) << "Vjp prepare call lu_unpack's vjp inteface";

    std::vector<std::vector<Tensor>> tensor_res =
        primitive::lu_unpack_vjp(
        x, y, l, u, pmat, l_grad, u_grad, unpack_ludata, unpack_pivots, stop_gradients);

    VLOG(6) << "Vjp prepare stop gradient of lu_unpack_grad";

    std::vector<std::vector<pir::OpResult>> res(tensor_res.size());
    for (size_t i = 0; i < tensor_res.size(); ++i) {
        res[i].resize(tensor_res[i].size());
        for (size_t j = 0; j < tensor_res[i].size(); ++j) {
            if(tensor_res[i][j].defined()){
                res[i][j] = std::static_pointer_cast<primitive::LazyTensor>(tensor_res[i][j].impl())->value().dyn_cast<pir::OpResult>();
            }
        }
    }
    return res;
}

std::vector<std::vector<pir::OpResult>> MarginCrossEntropyOp::Vjp(pir::Operation* op, const std::vector<std::vector<pir::Value>>& inputs_, const std::vector<std::vector<pir::Value>>& outputs, const std::vector<std::vector<pir::Value>>& out_grads, const std::vector<std::vector<bool>>& stop_gradients){

    PADDLE_ENFORCE_EQ(
      inputs_.size(),
      2,
      platform::errors::InvalidArgument("margin_cross_entropy op's inputs size should be 2, but now is %d.", inputs_.size()));
    PADDLE_ENFORCE_EQ(
      outputs.size(),
      2,
      platform::errors::InvalidArgument("margin_cross_entropy op's outputs size should be 2, but now is %d.", outputs.size()));

    VLOG(6) << "Prepare inputs of margin_cross_entropy_grad";

    Tensor logits(std::make_shared<primitive::LazyTensor>(inputs_[0][0]));
    Tensor label(std::make_shared<primitive::LazyTensor>(inputs_[1][0]));
    Tensor softmax(std::make_shared<primitive::LazyTensor>(outputs[0][0]));
    Tensor loss_grad(std::make_shared<primitive::LazyTensor>(out_grads[1][0]));

    VLOG(6) << "Vjp prepare Prepare attributes of margin_cross_entropy_grad";

    bool return_softmax = op->attribute("return_softmax").dyn_cast<pir::BoolAttribute>().data();
    int ring_id = op->attribute("ring_id").dyn_cast<pir::Int32Attribute>().data();
    int rank = op->attribute("rank").dyn_cast<pir::Int32Attribute>().data();
    int nranks = op->attribute("nranks").dyn_cast<pir::Int32Attribute>().data();
    float margin1 = op->attribute("margin1").dyn_cast<pir::FloatAttribute>().data();
    float margin2 = op->attribute("margin2").dyn_cast<pir::FloatAttribute>().data();
    float margin3 = op->attribute("margin3").dyn_cast<pir::FloatAttribute>().data();
    float scale = op->attribute("scale").dyn_cast<pir::FloatAttribute>().data();

    VLOG(6) << "Vjp prepare call margin_cross_entropy's vjp inteface";

    std::vector<std::vector<Tensor>> tensor_res =
        primitive::margin_cross_entropy_vjp(
        logits, label, softmax, loss_grad, return_softmax, ring_id, rank, nranks, margin1, margin2, margin3, scale, stop_gradients);

    VLOG(6) << "Vjp prepare stop gradient of margin_cross_entropy_grad";

    std::vector<std::vector<pir::OpResult>> res(tensor_res.size());
    for (size_t i = 0; i < tensor_res.size(); ++i) {
        res[i].resize(tensor_res[i].size());
        for (size_t j = 0; j < tensor_res[i].size(); ++j) {
            if(tensor_res[i][j].defined()){
                res[i][j] = std::static_pointer_cast<primitive::LazyTensor>(tensor_res[i][j].impl())->value().dyn_cast<pir::OpResult>();
            }
        }
    }
    return res;
}

std::vector<std::vector<pir::OpResult>> MaskedSelectOp::Vjp(pir::Operation* op, const std::vector<std::vector<pir::Value>>& inputs_, const std::vector<std::vector<pir::Value>>& outputs, const std::vector<std::vector<pir::Value>>& out_grads, const std::vector<std::vector<bool>>& stop_gradients){

    PADDLE_ENFORCE_EQ(
      inputs_.size(),
      2,
      platform::errors::InvalidArgument("masked_select op's inputs size should be 2, but now is %d.", inputs_.size()));
    PADDLE_ENFORCE_EQ(
      outputs.size(),
      1,
      platform::errors::InvalidArgument("masked_select op's outputs size should be 1, but now is %d.", outputs.size()));

    VLOG(6) << "Prepare inputs of masked_select_grad";

    Tensor x(std::make_shared<primitive::LazyTensor>(inputs_[0][0]));
    Tensor mask(std::make_shared<primitive::LazyTensor>(inputs_[1][0]));
    Tensor out_grad(std::make_shared<primitive::LazyTensor>(out_grads[0][0]));

    VLOG(6) << "Vjp prepare Prepare attributes of masked_select_grad";


    VLOG(6) << "Vjp prepare call masked_select's vjp inteface";

    std::vector<std::vector<Tensor>> tensor_res =
        primitive::masked_select_vjp(
        x, mask, out_grad, stop_gradients);

    VLOG(6) << "Vjp prepare stop gradient of masked_select_grad";

    std::vector<std::vector<pir::OpResult>> res(tensor_res.size());
    for (size_t i = 0; i < tensor_res.size(); ++i) {
        res[i].resize(tensor_res[i].size());
        for (size_t j = 0; j < tensor_res[i].size(); ++j) {
            if(tensor_res[i][j].defined()){
                res[i][j] = std::static_pointer_cast<primitive::LazyTensor>(tensor_res[i][j].impl())->value().dyn_cast<pir::OpResult>();
            }
        }
    }
    return res;
}

std::vector<std::vector<pir::OpResult>> MatrixPowerOp::Vjp(pir::Operation* op, const std::vector<std::vector<pir::Value>>& inputs_, const std::vector<std::vector<pir::Value>>& outputs, const std::vector<std::vector<pir::Value>>& out_grads, const std::vector<std::vector<bool>>& stop_gradients){

    PADDLE_ENFORCE_EQ(
      inputs_.size(),
      1,
      platform::errors::InvalidArgument("matrix_power op's inputs size should be 1, but now is %d.", inputs_.size()));
    PADDLE_ENFORCE_EQ(
      outputs.size(),
      1,
      platform::errors::InvalidArgument("matrix_power op's outputs size should be 1, but now is %d.", outputs.size()));

    VLOG(6) << "Prepare inputs of matrix_power_grad";

    Tensor x(std::make_shared<primitive::LazyTensor>(inputs_[0][0]));
    Tensor out(std::make_shared<primitive::LazyTensor>(outputs[0][0]));
    Tensor out_grad(std::make_shared<primitive::LazyTensor>(out_grads[0][0]));

    VLOG(6) << "Vjp prepare Prepare attributes of matrix_power_grad";

    int n = op->attribute("n").dyn_cast<pir::Int32Attribute>().data();

    VLOG(6) << "Vjp prepare call matrix_power's vjp inteface";

    std::vector<std::vector<Tensor>> tensor_res =
        primitive::matrix_power_vjp(
        x, out, out_grad, n, stop_gradients);

    VLOG(6) << "Vjp prepare stop gradient of matrix_power_grad";

    std::vector<std::vector<pir::OpResult>> res(tensor_res.size());
    for (size_t i = 0; i < tensor_res.size(); ++i) {
        res[i].resize(tensor_res[i].size());
        for (size_t j = 0; j < tensor_res[i].size(); ++j) {
            if(tensor_res[i][j].defined()){
                res[i][j] = std::static_pointer_cast<primitive::LazyTensor>(tensor_res[i][j].impl())->value().dyn_cast<pir::OpResult>();
            }
        }
    }
    return res;
}

std::vector<std::vector<pir::OpResult>> MaxPool2dWithIndexOp::Vjp(pir::Operation* op, const std::vector<std::vector<pir::Value>>& inputs_, const std::vector<std::vector<pir::Value>>& outputs, const std::vector<std::vector<pir::Value>>& out_grads, const std::vector<std::vector<bool>>& stop_gradients){

    PADDLE_ENFORCE_EQ(
      inputs_.size(),
      1,
      platform::errors::InvalidArgument("max_pool2d_with_index op's inputs size should be 1, but now is %d.", inputs_.size()));
    PADDLE_ENFORCE_EQ(
      outputs.size(),
      2,
      platform::errors::InvalidArgument("max_pool2d_with_index op's outputs size should be 2, but now is %d.", outputs.size()));

    VLOG(6) << "Prepare inputs of max_pool2d_with_index_grad";

    Tensor x(std::make_shared<primitive::LazyTensor>(inputs_[0][0]));
    Tensor mask(std::make_shared<primitive::LazyTensor>(outputs[1][0]));
    Tensor out_grad(std::make_shared<primitive::LazyTensor>(out_grads[0][0]));

    VLOG(6) << "Vjp prepare Prepare attributes of max_pool2d_with_index_grad";

    std::vector<int> kernel_size;
    for (size_t i = 0; i < op->attribute("kernel_size").dyn_cast<pir::ArrayAttribute>().size(); i++) {
        kernel_size.push_back(op->attribute("kernel_size").dyn_cast<pir::ArrayAttribute>().at(i).dyn_cast<pir::Int32Attribute>().data());
    }
    std::vector<int> strides;
    for (size_t i = 0; i < op->attribute("strides").dyn_cast<pir::ArrayAttribute>().size(); i++) {
        strides.push_back(op->attribute("strides").dyn_cast<pir::ArrayAttribute>().at(i).dyn_cast<pir::Int32Attribute>().data());
    }
    std::vector<int> paddings;
    for (size_t i = 0; i < op->attribute("paddings").dyn_cast<pir::ArrayAttribute>().size(); i++) {
        paddings.push_back(op->attribute("paddings").dyn_cast<pir::ArrayAttribute>().at(i).dyn_cast<pir::Int32Attribute>().data());
    }
    bool global_pooling = op->attribute("global_pooling").dyn_cast<pir::BoolAttribute>().data();
    bool adaptive = op->attribute("adaptive").dyn_cast<pir::BoolAttribute>().data();

    VLOG(6) << "Vjp prepare call max_pool2d_with_index's vjp inteface";

    std::vector<std::vector<Tensor>> tensor_res =
        primitive::max_pool2d_with_index_vjp(
        x, mask, out_grad, kernel_size, strides, paddings, global_pooling, adaptive, stop_gradients);

    VLOG(6) << "Vjp prepare stop gradient of max_pool2d_with_index_grad";

    std::vector<std::vector<pir::OpResult>> res(tensor_res.size());
    for (size_t i = 0; i < tensor_res.size(); ++i) {
        res[i].resize(tensor_res[i].size());
        for (size_t j = 0; j < tensor_res[i].size(); ++j) {
            if(tensor_res[i][j].defined()){
                res[i][j] = std::static_pointer_cast<primitive::LazyTensor>(tensor_res[i][j].impl())->value().dyn_cast<pir::OpResult>();
            }
        }
    }
    return res;
}

std::vector<std::vector<pir::OpResult>> MaxPool3dWithIndexOp::Vjp(pir::Operation* op, const std::vector<std::vector<pir::Value>>& inputs_, const std::vector<std::vector<pir::Value>>& outputs, const std::vector<std::vector<pir::Value>>& out_grads, const std::vector<std::vector<bool>>& stop_gradients){

    PADDLE_ENFORCE_EQ(
      inputs_.size(),
      1,
      platform::errors::InvalidArgument("max_pool3d_with_index op's inputs size should be 1, but now is %d.", inputs_.size()));
    PADDLE_ENFORCE_EQ(
      outputs.size(),
      2,
      platform::errors::InvalidArgument("max_pool3d_with_index op's outputs size should be 2, but now is %d.", outputs.size()));

    VLOG(6) << "Prepare inputs of max_pool3d_with_index_grad";

    Tensor x(std::make_shared<primitive::LazyTensor>(inputs_[0][0]));
    Tensor mask(std::make_shared<primitive::LazyTensor>(outputs[1][0]));
    Tensor out_grad(std::make_shared<primitive::LazyTensor>(out_grads[0][0]));

    VLOG(6) << "Vjp prepare Prepare attributes of max_pool3d_with_index_grad";

    std::vector<int> kernel_size;
    for (size_t i = 0; i < op->attribute("kernel_size").dyn_cast<pir::ArrayAttribute>().size(); i++) {
        kernel_size.push_back(op->attribute("kernel_size").dyn_cast<pir::ArrayAttribute>().at(i).dyn_cast<pir::Int32Attribute>().data());
    }
    std::vector<int> strides;
    for (size_t i = 0; i < op->attribute("strides").dyn_cast<pir::ArrayAttribute>().size(); i++) {
        strides.push_back(op->attribute("strides").dyn_cast<pir::ArrayAttribute>().at(i).dyn_cast<pir::Int32Attribute>().data());
    }
    std::vector<int> paddings;
    for (size_t i = 0; i < op->attribute("paddings").dyn_cast<pir::ArrayAttribute>().size(); i++) {
        paddings.push_back(op->attribute("paddings").dyn_cast<pir::ArrayAttribute>().at(i).dyn_cast<pir::Int32Attribute>().data());
    }
    bool global_pooling = op->attribute("global_pooling").dyn_cast<pir::BoolAttribute>().data();
    bool adaptive = op->attribute("adaptive").dyn_cast<pir::BoolAttribute>().data();

    VLOG(6) << "Vjp prepare call max_pool3d_with_index's vjp inteface";

    std::vector<std::vector<Tensor>> tensor_res =
        primitive::max_pool3d_with_index_vjp(
        x, mask, out_grad, kernel_size, strides, paddings, global_pooling, adaptive, stop_gradients);

    VLOG(6) << "Vjp prepare stop gradient of max_pool3d_with_index_grad";

    std::vector<std::vector<pir::OpResult>> res(tensor_res.size());
    for (size_t i = 0; i < tensor_res.size(); ++i) {
        res[i].resize(tensor_res[i].size());
        for (size_t j = 0; j < tensor_res[i].size(); ++j) {
            if(tensor_res[i][j].defined()){
                res[i][j] = std::static_pointer_cast<primitive::LazyTensor>(tensor_res[i][j].impl())->value().dyn_cast<pir::OpResult>();
            }
        }
    }
    return res;
}

std::vector<std::vector<pir::OpResult>> MaxoutOp::Vjp(pir::Operation* op, const std::vector<std::vector<pir::Value>>& inputs_, const std::vector<std::vector<pir::Value>>& outputs, const std::vector<std::vector<pir::Value>>& out_grads, const std::vector<std::vector<bool>>& stop_gradients){

    PADDLE_ENFORCE_EQ(
      inputs_.size(),
      1,
      platform::errors::InvalidArgument("maxout op's inputs size should be 1, but now is %d.", inputs_.size()));
    PADDLE_ENFORCE_EQ(
      outputs.size(),
      1,
      platform::errors::InvalidArgument("maxout op's outputs size should be 1, but now is %d.", outputs.size()));

    VLOG(6) << "Prepare inputs of maxout_grad";

    Tensor x(std::make_shared<primitive::LazyTensor>(inputs_[0][0]));
    Tensor out(std::make_shared<primitive::LazyTensor>(outputs[0][0]));
    Tensor out_grad(std::make_shared<primitive::LazyTensor>(out_grads[0][0]));

    VLOG(6) << "Vjp prepare Prepare attributes of maxout_grad";

    int groups = op->attribute("groups").dyn_cast<pir::Int32Attribute>().data();
    int axis = op->attribute("axis").dyn_cast<pir::Int32Attribute>().data();

    VLOG(6) << "Vjp prepare call maxout's vjp inteface";

    std::vector<std::vector<Tensor>> tensor_res =
        primitive::maxout_vjp(
        x, out, out_grad, groups, axis, stop_gradients);

    VLOG(6) << "Vjp prepare stop gradient of maxout_grad";

    std::vector<std::vector<pir::OpResult>> res(tensor_res.size());
    for (size_t i = 0; i < tensor_res.size(); ++i) {
        res[i].resize(tensor_res[i].size());
        for (size_t j = 0; j < tensor_res[i].size(); ++j) {
            if(tensor_res[i][j].defined()){
                res[i][j] = std::static_pointer_cast<primitive::LazyTensor>(tensor_res[i][j].impl())->value().dyn_cast<pir::OpResult>();
            }
        }
    }
    return res;
}

std::vector<std::vector<pir::OpResult>> MeanAllOp::Vjp(pir::Operation* op, const std::vector<std::vector<pir::Value>>& inputs_, const std::vector<std::vector<pir::Value>>& outputs, const std::vector<std::vector<pir::Value>>& out_grads, const std::vector<std::vector<bool>>& stop_gradients){

    PADDLE_ENFORCE_EQ(
      inputs_.size(),
      1,
      platform::errors::InvalidArgument("mean_all op's inputs size should be 1, but now is %d.", inputs_.size()));
    PADDLE_ENFORCE_EQ(
      outputs.size(),
      1,
      platform::errors::InvalidArgument("mean_all op's outputs size should be 1, but now is %d.", outputs.size()));

    VLOG(6) << "Prepare inputs of mean_all_grad";

    Tensor x(std::make_shared<primitive::LazyTensor>(inputs_[0][0]));
    Tensor out_grad(std::make_shared<primitive::LazyTensor>(out_grads[0][0]));

    VLOG(6) << "Vjp prepare Prepare attributes of mean_all_grad";


    VLOG(6) << "Vjp prepare call mean_all's vjp inteface";

    std::vector<std::vector<Tensor>> tensor_res =
        primitive::mean_all_vjp(
        x, out_grad, stop_gradients);

    VLOG(6) << "Vjp prepare stop gradient of mean_all_grad";

    std::vector<std::vector<pir::OpResult>> res(tensor_res.size());
    for (size_t i = 0; i < tensor_res.size(); ++i) {
        res[i].resize(tensor_res[i].size());
        for (size_t j = 0; j < tensor_res[i].size(); ++j) {
            if(tensor_res[i][j].defined()){
                res[i][j] = std::static_pointer_cast<primitive::LazyTensor>(tensor_res[i][j].impl())->value().dyn_cast<pir::OpResult>();
            }
        }
    }
    return res;
}

std::vector<std::vector<pir::OpResult>> MemoryEfficientAttentionOp::Vjp(pir::Operation* op, const std::vector<std::vector<pir::Value>>& inputs_, const std::vector<std::vector<pir::Value>>& outputs, const std::vector<std::vector<pir::Value>>& out_grads, const std::vector<std::vector<bool>>& stop_gradients){

    PADDLE_ENFORCE_EQ(
      inputs_.size(),
      8,
      platform::errors::InvalidArgument("memory_efficient_attention op's inputs size should be 8, but now is %d.", inputs_.size()));
    PADDLE_ENFORCE_EQ(
      outputs.size(),
      3,
      platform::errors::InvalidArgument("memory_efficient_attention op's outputs size should be 3, but now is %d.", outputs.size()));

    VLOG(6) << "Prepare inputs of memory_efficient_attention_grad";

    Tensor query(std::make_shared<primitive::LazyTensor>(inputs_[0][0]));
    Tensor key(std::make_shared<primitive::LazyTensor>(inputs_[1][0]));
    Tensor value(std::make_shared<primitive::LazyTensor>(inputs_[2][0]));
    paddle::optional<Tensor> bias;
    if (!IsEmptyValue(inputs_[3][0])){
        bias = paddle::make_optional<Tensor>(Tensor(std::make_shared<primitive::LazyTensor>(inputs_[3][0])));
    }
    paddle::optional<Tensor> cu_seqlens_q;
    if (!IsEmptyValue(inputs_[4][0])){
        cu_seqlens_q = paddle::make_optional<Tensor>(Tensor(std::make_shared<primitive::LazyTensor>(inputs_[4][0])));
    }
    paddle::optional<Tensor> cu_seqlens_k;
    if (!IsEmptyValue(inputs_[5][0])){
        cu_seqlens_k = paddle::make_optional<Tensor>(Tensor(std::make_shared<primitive::LazyTensor>(inputs_[5][0])));
    }
    Tensor output(std::make_shared<primitive::LazyTensor>(outputs[0][0]));
    Tensor logsumexp(std::make_shared<primitive::LazyTensor>(outputs[1][0]));
    Tensor seed_and_offset(std::make_shared<primitive::LazyTensor>(outputs[2][0]));
    Tensor output_grad(std::make_shared<primitive::LazyTensor>(out_grads[0][0]));

    VLOG(6) << "Vjp prepare Prepare attributes of memory_efficient_attention_grad";

    phi::Scalar max_seqlen_q = op->attribute("max_seqlen_q").dyn_cast<paddle::dialect::ScalarAttribute>().data();
    phi::Scalar max_seqlen_k = op->attribute("max_seqlen_k").dyn_cast<paddle::dialect::ScalarAttribute>().data();
    bool causal = op->attribute("causal").dyn_cast<pir::BoolAttribute>().data();
    double dropout_p = op->attribute("dropout_p").dyn_cast<pir::DoubleAttribute>().data();
    float scale = op->attribute("scale").dyn_cast<pir::FloatAttribute>().data();

    VLOG(6) << "Vjp prepare call memory_efficient_attention's vjp inteface";

    std::vector<std::vector<Tensor>> tensor_res =
        primitive::memory_efficient_attention_vjp(
        query, key, value, bias, cu_seqlens_q, cu_seqlens_k, output, logsumexp, seed_and_offset, output_grad, max_seqlen_q, max_seqlen_k, causal, dropout_p, scale, stop_gradients);

    VLOG(6) << "Vjp prepare stop gradient of memory_efficient_attention_grad";

    std::vector<std::vector<pir::OpResult>> res(tensor_res.size());
    for (size_t i = 0; i < tensor_res.size(); ++i) {
        res[i].resize(tensor_res[i].size());
        for (size_t j = 0; j < tensor_res[i].size(); ++j) {
            if(tensor_res[i][j].defined()){
                res[i][j] = std::static_pointer_cast<primitive::LazyTensor>(tensor_res[i][j].impl())->value().dyn_cast<pir::OpResult>();
            }
        }
    }
    return res;
}

std::vector<std::vector<pir::OpResult>> MeshgridOp::Vjp(pir::Operation* op, const std::vector<std::vector<pir::Value>>& inputs_, const std::vector<std::vector<pir::Value>>& outputs, const std::vector<std::vector<pir::Value>>& out_grads, const std::vector<std::vector<bool>>& stop_gradients){

    PADDLE_ENFORCE_EQ(
      inputs_.size(),
      1,
      platform::errors::InvalidArgument("meshgrid op's inputs size should be 1, but now is %d.", inputs_.size()));
    PADDLE_ENFORCE_EQ(
      outputs.size(),
      1,
      platform::errors::InvalidArgument("meshgrid op's outputs size should be 1, but now is %d.", outputs.size()));

    VLOG(6) << "Prepare inputs of meshgrid_grad";

    std::vector<Tensor> inputs;
    for (size_t idx = 0; idx < inputs_[0].size(); idx++) {
        inputs.emplace_back(
            std::make_shared<primitive::LazyTensor>(inputs_[0][idx]));
    }
    std::vector<Tensor> outputs_grad;
    for (size_t idx = 0; idx < out_grads[0].size(); idx++) {
        outputs_grad.emplace_back(
            std::make_shared<primitive::LazyTensor>(out_grads[0][idx]));
    }

    VLOG(6) << "Vjp prepare Prepare attributes of meshgrid_grad";


    VLOG(6) << "Vjp prepare call meshgrid's vjp inteface";

    std::vector<std::vector<Tensor>> tensor_res =
        primitive::meshgrid_vjp(
        inputs, outputs_grad, stop_gradients);

    VLOG(6) << "Vjp prepare stop gradient of meshgrid_grad";

    std::vector<std::vector<pir::OpResult>> res(tensor_res.size());
    for (size_t i = 0; i < tensor_res.size(); ++i) {
        res[i].resize(tensor_res[i].size());
        for (size_t j = 0; j < tensor_res[i].size(); ++j) {
            if(tensor_res[i][j].defined()){
                res[i][j] = std::static_pointer_cast<primitive::LazyTensor>(tensor_res[i][j].impl())->value().dyn_cast<pir::OpResult>();
            }
        }
    }
    return res;
}

std::vector<std::vector<pir::OpResult>> ModeOp::Vjp(pir::Operation* op, const std::vector<std::vector<pir::Value>>& inputs_, const std::vector<std::vector<pir::Value>>& outputs, const std::vector<std::vector<pir::Value>>& out_grads, const std::vector<std::vector<bool>>& stop_gradients){

    PADDLE_ENFORCE_EQ(
      inputs_.size(),
      1,
      platform::errors::InvalidArgument("mode op's inputs size should be 1, but now is %d.", inputs_.size()));
    PADDLE_ENFORCE_EQ(
      outputs.size(),
      2,
      platform::errors::InvalidArgument("mode op's outputs size should be 2, but now is %d.", outputs.size()));

    VLOG(6) << "Prepare inputs of mode_grad";

    Tensor x(std::make_shared<primitive::LazyTensor>(inputs_[0][0]));
    Tensor indices(std::make_shared<primitive::LazyTensor>(outputs[1][0]));
    Tensor out_grad(std::make_shared<primitive::LazyTensor>(out_grads[0][0]));

    VLOG(6) << "Vjp prepare Prepare attributes of mode_grad";

    int axis = op->attribute("axis").dyn_cast<pir::Int32Attribute>().data();
    bool keepdim = op->attribute("keepdim").dyn_cast<pir::BoolAttribute>().data();

    VLOG(6) << "Vjp prepare call mode's vjp inteface";

    std::vector<std::vector<Tensor>> tensor_res =
        primitive::mode_vjp(
        x, indices, out_grad, axis, keepdim, stop_gradients);

    VLOG(6) << "Vjp prepare stop gradient of mode_grad";

    std::vector<std::vector<pir::OpResult>> res(tensor_res.size());
    for (size_t i = 0; i < tensor_res.size(); ++i) {
        res[i].resize(tensor_res[i].size());
        for (size_t j = 0; j < tensor_res[i].size(); ++j) {
            if(tensor_res[i][j].defined()){
                res[i][j] = std::static_pointer_cast<primitive::LazyTensor>(tensor_res[i][j].impl())->value().dyn_cast<pir::OpResult>();
            }
        }
    }
    return res;
}

std::vector<std::vector<pir::OpResult>> MultiDotOp::Vjp(pir::Operation* op, const std::vector<std::vector<pir::Value>>& inputs_, const std::vector<std::vector<pir::Value>>& outputs, const std::vector<std::vector<pir::Value>>& out_grads, const std::vector<std::vector<bool>>& stop_gradients){

    PADDLE_ENFORCE_EQ(
      inputs_.size(),
      1,
      platform::errors::InvalidArgument("multi_dot op's inputs size should be 1, but now is %d.", inputs_.size()));
    PADDLE_ENFORCE_EQ(
      outputs.size(),
      1,
      platform::errors::InvalidArgument("multi_dot op's outputs size should be 1, but now is %d.", outputs.size()));

    VLOG(6) << "Prepare inputs of multi_dot_grad";

    std::vector<Tensor> x;
    for (size_t idx = 0; idx < inputs_[0].size(); idx++) {
        x.emplace_back(
            std::make_shared<primitive::LazyTensor>(inputs_[0][idx]));
    }
    Tensor out_grad(std::make_shared<primitive::LazyTensor>(out_grads[0][0]));

    VLOG(6) << "Vjp prepare Prepare attributes of multi_dot_grad";


    VLOG(6) << "Vjp prepare call multi_dot's vjp inteface";

    std::vector<std::vector<Tensor>> tensor_res =
        primitive::multi_dot_vjp(
        x, out_grad, stop_gradients);

    VLOG(6) << "Vjp prepare stop gradient of multi_dot_grad";

    std::vector<std::vector<pir::OpResult>> res(tensor_res.size());
    for (size_t i = 0; i < tensor_res.size(); ++i) {
        res[i].resize(tensor_res[i].size());
        for (size_t j = 0; j < tensor_res[i].size(); ++j) {
            if(tensor_res[i][j].defined()){
                res[i][j] = std::static_pointer_cast<primitive::LazyTensor>(tensor_res[i][j].impl())->value().dyn_cast<pir::OpResult>();
            }
        }
    }
    return res;
}

std::vector<std::vector<pir::OpResult>> MultiplexOp::Vjp(pir::Operation* op, const std::vector<std::vector<pir::Value>>& inputs_, const std::vector<std::vector<pir::Value>>& outputs, const std::vector<std::vector<pir::Value>>& out_grads, const std::vector<std::vector<bool>>& stop_gradients){

    PADDLE_ENFORCE_EQ(
      inputs_.size(),
      2,
      platform::errors::InvalidArgument("multiplex op's inputs size should be 2, but now is %d.", inputs_.size()));
    PADDLE_ENFORCE_EQ(
      outputs.size(),
      1,
      platform::errors::InvalidArgument("multiplex op's outputs size should be 1, but now is %d.", outputs.size()));

    VLOG(6) << "Prepare inputs of multiplex_grad";

    std::vector<Tensor> inputs;
    for (size_t idx = 0; idx < inputs_[0].size(); idx++) {
        inputs.emplace_back(
            std::make_shared<primitive::LazyTensor>(inputs_[0][idx]));
    }
    Tensor index(std::make_shared<primitive::LazyTensor>(inputs_[1][0]));
    Tensor out_grad(std::make_shared<primitive::LazyTensor>(out_grads[0][0]));

    VLOG(6) << "Vjp prepare Prepare attributes of multiplex_grad";


    VLOG(6) << "Vjp prepare call multiplex's vjp inteface";

    std::vector<std::vector<Tensor>> tensor_res =
        primitive::multiplex_vjp(
        inputs, index, out_grad, stop_gradients);

    VLOG(6) << "Vjp prepare stop gradient of multiplex_grad";

    std::vector<std::vector<pir::OpResult>> res(tensor_res.size());
    for (size_t i = 0; i < tensor_res.size(); ++i) {
        res[i].resize(tensor_res[i].size());
        for (size_t j = 0; j < tensor_res[i].size(); ++j) {
            if(tensor_res[i][j].defined()){
                res[i][j] = std::static_pointer_cast<primitive::LazyTensor>(tensor_res[i][j].impl())->value().dyn_cast<pir::OpResult>();
            }
        }
    }
    return res;
}

std::vector<std::vector<pir::OpResult>> MvOp::Vjp(pir::Operation* op, const std::vector<std::vector<pir::Value>>& inputs_, const std::vector<std::vector<pir::Value>>& outputs, const std::vector<std::vector<pir::Value>>& out_grads, const std::vector<std::vector<bool>>& stop_gradients){

    PADDLE_ENFORCE_EQ(
      inputs_.size(),
      2,
      platform::errors::InvalidArgument("mv op's inputs size should be 2, but now is %d.", inputs_.size()));
    PADDLE_ENFORCE_EQ(
      outputs.size(),
      1,
      platform::errors::InvalidArgument("mv op's outputs size should be 1, but now is %d.", outputs.size()));

    VLOG(6) << "Prepare inputs of mv_grad";

    Tensor x(std::make_shared<primitive::LazyTensor>(inputs_[0][0]));
    Tensor vec(std::make_shared<primitive::LazyTensor>(inputs_[1][0]));
    Tensor out_grad(std::make_shared<primitive::LazyTensor>(out_grads[0][0]));

    VLOG(6) << "Vjp prepare Prepare attributes of mv_grad";


    VLOG(6) << "Vjp prepare call mv's vjp inteface";

    std::vector<std::vector<Tensor>> tensor_res =
        primitive::mv_vjp(
        x, vec, out_grad, stop_gradients);

    VLOG(6) << "Vjp prepare stop gradient of mv_grad";

    std::vector<std::vector<pir::OpResult>> res(tensor_res.size());
    for (size_t i = 0; i < tensor_res.size(); ++i) {
        res[i].resize(tensor_res[i].size());
        for (size_t j = 0; j < tensor_res[i].size(); ++j) {
            if(tensor_res[i][j].defined()){
                res[i][j] = std::static_pointer_cast<primitive::LazyTensor>(tensor_res[i][j].impl())->value().dyn_cast<pir::OpResult>();
            }
        }
    }
    return res;
}

std::vector<std::vector<pir::OpResult>> NanmedianOp::Vjp(pir::Operation* op, const std::vector<std::vector<pir::Value>>& inputs_, const std::vector<std::vector<pir::Value>>& outputs, const std::vector<std::vector<pir::Value>>& out_grads, const std::vector<std::vector<bool>>& stop_gradients){

    PADDLE_ENFORCE_EQ(
      inputs_.size(),
      1,
      platform::errors::InvalidArgument("nanmedian op's inputs size should be 1, but now is %d.", inputs_.size()));
    PADDLE_ENFORCE_EQ(
      outputs.size(),
      2,
      platform::errors::InvalidArgument("nanmedian op's outputs size should be 2, but now is %d.", outputs.size()));

    VLOG(6) << "Prepare inputs of nanmedian_grad";

    Tensor x(std::make_shared<primitive::LazyTensor>(inputs_[0][0]));
    Tensor medians(std::make_shared<primitive::LazyTensor>(outputs[1][0]));
    Tensor out_grad(std::make_shared<primitive::LazyTensor>(out_grads[0][0]));

    VLOG(6) << "Vjp prepare Prepare attributes of nanmedian_grad";

    phi::IntArray axis = op->attribute("axis").dyn_cast<paddle::dialect::IntArrayAttribute>().data();
    bool keepdim = op->attribute("keepdim").dyn_cast<pir::BoolAttribute>().data();

    VLOG(6) << "Vjp prepare call nanmedian's vjp inteface";

    std::vector<std::vector<Tensor>> tensor_res =
        primitive::nanmedian_vjp(
        x, medians, out_grad, axis, keepdim, stop_gradients);

    VLOG(6) << "Vjp prepare stop gradient of nanmedian_grad";

    std::vector<std::vector<pir::OpResult>> res(tensor_res.size());
    for (size_t i = 0; i < tensor_res.size(); ++i) {
        res[i].resize(tensor_res[i].size());
        for (size_t j = 0; j < tensor_res[i].size(); ++j) {
            if(tensor_res[i][j].defined()){
                res[i][j] = std::static_pointer_cast<primitive::LazyTensor>(tensor_res[i][j].impl())->value().dyn_cast<pir::OpResult>();
            }
        }
    }
    return res;
}

std::vector<std::vector<pir::OpResult>> NearestInterpOp::Vjp(pir::Operation* op, const std::vector<std::vector<pir::Value>>& inputs_, const std::vector<std::vector<pir::Value>>& outputs, const std::vector<std::vector<pir::Value>>& out_grads, const std::vector<std::vector<bool>>& stop_gradients){

    PADDLE_ENFORCE_EQ(
      inputs_.size(),
      4,
      platform::errors::InvalidArgument("nearest_interp op's inputs size should be 4, but now is %d.", inputs_.size()));
    PADDLE_ENFORCE_EQ(
      outputs.size(),
      1,
      platform::errors::InvalidArgument("nearest_interp op's outputs size should be 1, but now is %d.", outputs.size()));

    VLOG(6) << "Prepare inputs of nearest_interp_grad";

    Tensor x(std::make_shared<primitive::LazyTensor>(inputs_[0][0]));
    paddle::optional<Tensor> out_size;
    if (!IsEmptyValue(inputs_[1][0])){
        out_size = paddle::make_optional<Tensor>(Tensor(std::make_shared<primitive::LazyTensor>(inputs_[1][0])));
    }
    paddle::optional<std::vector<Tensor>> size_tensor;
    std::vector<Tensor> optional_size_tensor;
    if (!IsEmptyValue(inputs_[2][0])){
        for (size_t idx = 0; idx < inputs_[2].size(); idx++) {
            optional_size_tensor.emplace_back(
                std::make_shared<primitive::LazyTensor>(inputs_[2][idx]));
        }
        size_tensor = paddle::make_optional<std::vector<Tensor>>(optional_size_tensor);
    }
    paddle::optional<Tensor> scale_tensor;
    if (!IsEmptyValue(inputs_[3][0])){
        scale_tensor = paddle::make_optional<Tensor>(Tensor(std::make_shared<primitive::LazyTensor>(inputs_[3][0])));
    }
    Tensor output_grad(std::make_shared<primitive::LazyTensor>(out_grads[0][0]));

    VLOG(6) << "Vjp prepare Prepare attributes of nearest_interp_grad";

    std::string data_layout = op->attribute("data_layout").dyn_cast<pir::StrAttribute>().AsString();
    int out_d = op->attribute("out_d").dyn_cast<pir::Int32Attribute>().data();
    int out_h = op->attribute("out_h").dyn_cast<pir::Int32Attribute>().data();
    int out_w = op->attribute("out_w").dyn_cast<pir::Int32Attribute>().data();
    std::vector<float> scale;
    for (size_t i = 0; i < op->attribute("scale").dyn_cast<pir::ArrayAttribute>().size(); i++) {
        scale.push_back(op->attribute("scale").dyn_cast<pir::ArrayAttribute>().at(i).dyn_cast<pir::FloatAttribute>().data());
    }
    std::string interp_method = op->attribute("interp_method").dyn_cast<pir::StrAttribute>().AsString();
    bool align_corners = op->attribute("align_corners").dyn_cast<pir::BoolAttribute>().data();
    int align_mode = op->attribute("align_mode").dyn_cast<pir::Int32Attribute>().data();

    VLOG(6) << "Vjp prepare call nearest_interp's vjp inteface";

    std::vector<std::vector<Tensor>> tensor_res =
        primitive::nearest_interp_vjp(
        x, out_size, size_tensor, scale_tensor, output_grad, data_layout, out_d, out_h, out_w, scale, interp_method, align_corners, align_mode, stop_gradients);

    VLOG(6) << "Vjp prepare stop gradient of nearest_interp_grad";

    std::vector<std::vector<pir::OpResult>> res(tensor_res.size());
    for (size_t i = 0; i < tensor_res.size(); ++i) {
        res[i].resize(tensor_res[i].size());
        for (size_t j = 0; j < tensor_res[i].size(); ++j) {
            if(tensor_res[i][j].defined()){
                res[i][j] = std::static_pointer_cast<primitive::LazyTensor>(tensor_res[i][j].impl())->value().dyn_cast<pir::OpResult>();
            }
        }
    }
    return res;
}

std::vector<std::vector<pir::OpResult>> NllLossOp::Vjp(pir::Operation* op, const std::vector<std::vector<pir::Value>>& inputs_, const std::vector<std::vector<pir::Value>>& outputs, const std::vector<std::vector<pir::Value>>& out_grads, const std::vector<std::vector<bool>>& stop_gradients){

    PADDLE_ENFORCE_EQ(
      inputs_.size(),
      3,
      platform::errors::InvalidArgument("nll_loss op's inputs size should be 3, but now is %d.", inputs_.size()));
    PADDLE_ENFORCE_EQ(
      outputs.size(),
      2,
      platform::errors::InvalidArgument("nll_loss op's outputs size should be 2, but now is %d.", outputs.size()));

    VLOG(6) << "Prepare inputs of nll_loss_grad";

    Tensor input(std::make_shared<primitive::LazyTensor>(inputs_[0][0]));
    Tensor label(std::make_shared<primitive::LazyTensor>(inputs_[1][0]));
    paddle::optional<Tensor> weight;
    if (!IsEmptyValue(inputs_[2][0])){
        weight = paddle::make_optional<Tensor>(Tensor(std::make_shared<primitive::LazyTensor>(inputs_[2][0])));
    }
    Tensor total_weight(std::make_shared<primitive::LazyTensor>(outputs[1][0]));
    Tensor out_grad(std::make_shared<primitive::LazyTensor>(out_grads[0][0]));

    VLOG(6) << "Vjp prepare Prepare attributes of nll_loss_grad";

    int64_t ignore_index = op->attribute("ignore_index").dyn_cast<pir::Int64Attribute>().data();
    std::string reduction = op->attribute("reduction").dyn_cast<pir::StrAttribute>().AsString();

    VLOG(6) << "Vjp prepare call nll_loss's vjp inteface";

    std::vector<std::vector<Tensor>> tensor_res =
        primitive::nll_loss_vjp(
        input, label, weight, total_weight, out_grad, ignore_index, reduction, stop_gradients);

    VLOG(6) << "Vjp prepare stop gradient of nll_loss_grad";

    std::vector<std::vector<pir::OpResult>> res(tensor_res.size());
    for (size_t i = 0; i < tensor_res.size(); ++i) {
        res[i].resize(tensor_res[i].size());
        for (size_t j = 0; j < tensor_res[i].size(); ++j) {
            if(tensor_res[i][j].defined()){
                res[i][j] = std::static_pointer_cast<primitive::LazyTensor>(tensor_res[i][j].impl())->value().dyn_cast<pir::OpResult>();
            }
        }
    }
    return res;
}

std::vector<std::vector<pir::OpResult>> OverlapAddOp::Vjp(pir::Operation* op, const std::vector<std::vector<pir::Value>>& inputs_, const std::vector<std::vector<pir::Value>>& outputs, const std::vector<std::vector<pir::Value>>& out_grads, const std::vector<std::vector<bool>>& stop_gradients){

    PADDLE_ENFORCE_EQ(
      inputs_.size(),
      1,
      platform::errors::InvalidArgument("overlap_add op's inputs size should be 1, but now is %d.", inputs_.size()));
    PADDLE_ENFORCE_EQ(
      outputs.size(),
      1,
      platform::errors::InvalidArgument("overlap_add op's outputs size should be 1, but now is %d.", outputs.size()));

    VLOG(6) << "Prepare inputs of overlap_add_grad";

    Tensor x(std::make_shared<primitive::LazyTensor>(inputs_[0][0]));
    Tensor out_grad(std::make_shared<primitive::LazyTensor>(out_grads[0][0]));

    VLOG(6) << "Vjp prepare Prepare attributes of overlap_add_grad";

    int hop_length = op->attribute("hop_length").dyn_cast<pir::Int32Attribute>().data();
    int axis = op->attribute("axis").dyn_cast<pir::Int32Attribute>().data();

    VLOG(6) << "Vjp prepare call overlap_add's vjp inteface";

    std::vector<std::vector<Tensor>> tensor_res =
        primitive::overlap_add_vjp(
        x, out_grad, hop_length, axis, stop_gradients);

    VLOG(6) << "Vjp prepare stop gradient of overlap_add_grad";

    std::vector<std::vector<pir::OpResult>> res(tensor_res.size());
    for (size_t i = 0; i < tensor_res.size(); ++i) {
        res[i].resize(tensor_res[i].size());
        for (size_t j = 0; j < tensor_res[i].size(); ++j) {
            if(tensor_res[i][j].defined()){
                res[i][j] = std::static_pointer_cast<primitive::LazyTensor>(tensor_res[i][j].impl())->value().dyn_cast<pir::OpResult>();
            }
        }
    }
    return res;
}

std::vector<std::vector<pir::OpResult>> PNormOp::Vjp(pir::Operation* op, const std::vector<std::vector<pir::Value>>& inputs_, const std::vector<std::vector<pir::Value>>& outputs, const std::vector<std::vector<pir::Value>>& out_grads, const std::vector<std::vector<bool>>& stop_gradients){

    PADDLE_ENFORCE_EQ(
      inputs_.size(),
      1,
      platform::errors::InvalidArgument("p_norm op's inputs size should be 1, but now is %d.", inputs_.size()));
    PADDLE_ENFORCE_EQ(
      outputs.size(),
      1,
      platform::errors::InvalidArgument("p_norm op's outputs size should be 1, but now is %d.", outputs.size()));

    VLOG(6) << "Prepare inputs of p_norm_grad";

    Tensor x(std::make_shared<primitive::LazyTensor>(inputs_[0][0]));
    Tensor out(std::make_shared<primitive::LazyTensor>(outputs[0][0]));
    Tensor out_grad(std::make_shared<primitive::LazyTensor>(out_grads[0][0]));

    VLOG(6) << "Vjp prepare Prepare attributes of p_norm_grad";

    float porder = op->attribute("porder").dyn_cast<pir::FloatAttribute>().data();
    int axis = op->attribute("axis").dyn_cast<pir::Int32Attribute>().data();
    float epsilon = op->attribute("epsilon").dyn_cast<pir::FloatAttribute>().data();
    bool keepdim = op->attribute("keepdim").dyn_cast<pir::BoolAttribute>().data();
    bool asvector = op->attribute("asvector").dyn_cast<pir::BoolAttribute>().data();

    VLOG(6) << "Vjp prepare call p_norm's vjp inteface";

    std::vector<std::vector<Tensor>> tensor_res =
        primitive::p_norm_vjp(
        x, out, out_grad, porder, axis, epsilon, keepdim, asvector, stop_gradients);

    VLOG(6) << "Vjp prepare stop gradient of p_norm_grad";

    std::vector<std::vector<pir::OpResult>> res(tensor_res.size());
    for (size_t i = 0; i < tensor_res.size(); ++i) {
        res[i].resize(tensor_res[i].size());
        for (size_t j = 0; j < tensor_res[i].size(); ++j) {
            if(tensor_res[i][j].defined()){
                res[i][j] = std::static_pointer_cast<primitive::LazyTensor>(tensor_res[i][j].impl())->value().dyn_cast<pir::OpResult>();
            }
        }
    }
    return res;
}

std::vector<std::vector<pir::OpResult>> Pad3dOp::Vjp(pir::Operation* op, const std::vector<std::vector<pir::Value>>& inputs_, const std::vector<std::vector<pir::Value>>& outputs, const std::vector<std::vector<pir::Value>>& out_grads, const std::vector<std::vector<bool>>& stop_gradients){

    PADDLE_ENFORCE_EQ(
      inputs_.size(),
      2,
      platform::errors::InvalidArgument("pad3d op's inputs size should be 2, but now is %d.", inputs_.size()));
    PADDLE_ENFORCE_EQ(
      outputs.size(),
      1,
      platform::errors::InvalidArgument("pad3d op's outputs size should be 1, but now is %d.", outputs.size()));

    VLOG(6) << "Prepare inputs of pad3d_grad";

    Tensor x(std::make_shared<primitive::LazyTensor>(inputs_[0][0]));
    Tensor out_grad(std::make_shared<primitive::LazyTensor>(out_grads[0][0]));

    VLOG(6) << "Vjp prepare Prepare attributes of pad3d_grad";

    Tensor paddings(std::make_shared<primitive::LazyTensor>(inputs_[1][0]));
    std::string mode = op->attribute("mode").dyn_cast<pir::StrAttribute>().AsString();
    float pad_value = op->attribute("pad_value").dyn_cast<pir::FloatAttribute>().data();
    std::string data_format = op->attribute("data_format").dyn_cast<pir::StrAttribute>().AsString();

    VLOG(6) << "Vjp prepare call pad3d's vjp inteface";

    std::vector<std::vector<Tensor>> tensor_res =
        primitive::pad3d_vjp(
        x, out_grad, paddings, mode, pad_value, data_format, stop_gradients);

    VLOG(6) << "Vjp prepare stop gradient of pad3d_grad";

    std::vector<std::vector<pir::OpResult>> res(tensor_res.size());
    for (size_t i = 0; i < tensor_res.size(); ++i) {
        res[i].resize(tensor_res[i].size());
        for (size_t j = 0; j < tensor_res[i].size(); ++j) {
            if(tensor_res[i][j].defined()){
                res[i][j] = std::static_pointer_cast<primitive::LazyTensor>(tensor_res[i][j].impl())->value().dyn_cast<pir::OpResult>();
            }
        }
    }
    return res;
}

std::vector<std::vector<pir::OpResult>> PixelShuffleOp::Vjp(pir::Operation* op, const std::vector<std::vector<pir::Value>>& inputs_, const std::vector<std::vector<pir::Value>>& outputs, const std::vector<std::vector<pir::Value>>& out_grads, const std::vector<std::vector<bool>>& stop_gradients){

    PADDLE_ENFORCE_EQ(
      inputs_.size(),
      1,
      platform::errors::InvalidArgument("pixel_shuffle op's inputs size should be 1, but now is %d.", inputs_.size()));
    PADDLE_ENFORCE_EQ(
      outputs.size(),
      1,
      platform::errors::InvalidArgument("pixel_shuffle op's outputs size should be 1, but now is %d.", outputs.size()));

    VLOG(6) << "Prepare inputs of pixel_shuffle_grad";

    Tensor out_grad(std::make_shared<primitive::LazyTensor>(out_grads[0][0]));

    VLOG(6) << "Vjp prepare Prepare attributes of pixel_shuffle_grad";

    int upscale_factor = op->attribute("upscale_factor").dyn_cast<pir::Int32Attribute>().data();
    std::string data_format = op->attribute("data_format").dyn_cast<pir::StrAttribute>().AsString();

    VLOG(6) << "Vjp prepare call pixel_shuffle's vjp inteface";

    std::vector<std::vector<Tensor>> tensor_res =
        primitive::pixel_shuffle_vjp(
        out_grad, upscale_factor, data_format, stop_gradients);

    VLOG(6) << "Vjp prepare stop gradient of pixel_shuffle_grad";

    std::vector<std::vector<pir::OpResult>> res(tensor_res.size());
    for (size_t i = 0; i < tensor_res.size(); ++i) {
        res[i].resize(tensor_res[i].size());
        for (size_t j = 0; j < tensor_res[i].size(); ++j) {
            if(tensor_res[i][j].defined()){
                res[i][j] = std::static_pointer_cast<primitive::LazyTensor>(tensor_res[i][j].impl())->value().dyn_cast<pir::OpResult>();
            }
        }
    }
    return res;
}

std::vector<std::vector<pir::OpResult>> PixelUnshuffleOp::Vjp(pir::Operation* op, const std::vector<std::vector<pir::Value>>& inputs_, const std::vector<std::vector<pir::Value>>& outputs, const std::vector<std::vector<pir::Value>>& out_grads, const std::vector<std::vector<bool>>& stop_gradients){

    PADDLE_ENFORCE_EQ(
      inputs_.size(),
      1,
      platform::errors::InvalidArgument("pixel_unshuffle op's inputs size should be 1, but now is %d.", inputs_.size()));
    PADDLE_ENFORCE_EQ(
      outputs.size(),
      1,
      platform::errors::InvalidArgument("pixel_unshuffle op's outputs size should be 1, but now is %d.", outputs.size()));

    VLOG(6) << "Prepare inputs of pixel_unshuffle_grad";

    Tensor out_grad(std::make_shared<primitive::LazyTensor>(out_grads[0][0]));

    VLOG(6) << "Vjp prepare Prepare attributes of pixel_unshuffle_grad";

    int downscale_factor = op->attribute("downscale_factor").dyn_cast<pir::Int32Attribute>().data();
    std::string data_format = op->attribute("data_format").dyn_cast<pir::StrAttribute>().AsString();

    VLOG(6) << "Vjp prepare call pixel_unshuffle's vjp inteface";

    std::vector<std::vector<Tensor>> tensor_res =
        primitive::pixel_unshuffle_vjp(
        out_grad, downscale_factor, data_format, stop_gradients);

    VLOG(6) << "Vjp prepare stop gradient of pixel_unshuffle_grad";

    std::vector<std::vector<pir::OpResult>> res(tensor_res.size());
    for (size_t i = 0; i < tensor_res.size(); ++i) {
        res[i].resize(tensor_res[i].size());
        for (size_t j = 0; j < tensor_res[i].size(); ++j) {
            if(tensor_res[i][j].defined()){
                res[i][j] = std::static_pointer_cast<primitive::LazyTensor>(tensor_res[i][j].impl())->value().dyn_cast<pir::OpResult>();
            }
        }
    }
    return res;
}

std::vector<std::vector<pir::OpResult>> PoissonOp::Vjp(pir::Operation* op, const std::vector<std::vector<pir::Value>>& inputs_, const std::vector<std::vector<pir::Value>>& outputs, const std::vector<std::vector<pir::Value>>& out_grads, const std::vector<std::vector<bool>>& stop_gradients){

    PADDLE_ENFORCE_EQ(
      inputs_.size(),
      1,
      platform::errors::InvalidArgument("poisson op's inputs size should be 1, but now is %d.", inputs_.size()));
    PADDLE_ENFORCE_EQ(
      outputs.size(),
      1,
      platform::errors::InvalidArgument("poisson op's outputs size should be 1, but now is %d.", outputs.size()));

    VLOG(6) << "Prepare inputs of poisson_grad";

    Tensor out_grad(std::make_shared<primitive::LazyTensor>(out_grads[0][0]));

    VLOG(6) << "Vjp prepare Prepare attributes of poisson_grad";


    VLOG(6) << "Vjp prepare call poisson's vjp inteface";

    std::vector<std::vector<Tensor>> tensor_res =
        primitive::poisson_vjp(
        out_grad, stop_gradients);

    VLOG(6) << "Vjp prepare stop gradient of poisson_grad";

    std::vector<std::vector<pir::OpResult>> res(tensor_res.size());
    for (size_t i = 0; i < tensor_res.size(); ++i) {
        res[i].resize(tensor_res[i].size());
        for (size_t j = 0; j < tensor_res[i].size(); ++j) {
            if(tensor_res[i][j].defined()){
                res[i][j] = std::static_pointer_cast<primitive::LazyTensor>(tensor_res[i][j].impl())->value().dyn_cast<pir::OpResult>();
            }
        }
    }
    return res;
}

std::vector<std::vector<pir::OpResult>> PolygammaOp::Vjp(pir::Operation* op, const std::vector<std::vector<pir::Value>>& inputs_, const std::vector<std::vector<pir::Value>>& outputs, const std::vector<std::vector<pir::Value>>& out_grads, const std::vector<std::vector<bool>>& stop_gradients){

    PADDLE_ENFORCE_EQ(
      inputs_.size(),
      1,
      platform::errors::InvalidArgument("polygamma op's inputs size should be 1, but now is %d.", inputs_.size()));
    PADDLE_ENFORCE_EQ(
      outputs.size(),
      1,
      platform::errors::InvalidArgument("polygamma op's outputs size should be 1, but now is %d.", outputs.size()));

    VLOG(6) << "Prepare inputs of polygamma_grad";

    Tensor x(std::make_shared<primitive::LazyTensor>(inputs_[0][0]));
    Tensor out_grad(std::make_shared<primitive::LazyTensor>(out_grads[0][0]));

    VLOG(6) << "Vjp prepare Prepare attributes of polygamma_grad";

    int n = op->attribute("n").dyn_cast<pir::Int32Attribute>().data();

    VLOG(6) << "Vjp prepare call polygamma's vjp inteface";

    std::vector<std::vector<Tensor>> tensor_res =
        primitive::polygamma_vjp(
        x, out_grad, n, stop_gradients);

    VLOG(6) << "Vjp prepare stop gradient of polygamma_grad";

    std::vector<std::vector<pir::OpResult>> res(tensor_res.size());
    for (size_t i = 0; i < tensor_res.size(); ++i) {
        res[i].resize(tensor_res[i].size());
        for (size_t j = 0; j < tensor_res[i].size(); ++j) {
            if(tensor_res[i][j].defined()){
                res[i][j] = std::static_pointer_cast<primitive::LazyTensor>(tensor_res[i][j].impl())->value().dyn_cast<pir::OpResult>();
            }
        }
    }
    return res;
}

std::vector<std::vector<pir::OpResult>> Polygamma_Op::Vjp(pir::Operation* op, const std::vector<std::vector<pir::Value>>& inputs_, const std::vector<std::vector<pir::Value>>& outputs, const std::vector<std::vector<pir::Value>>& out_grads, const std::vector<std::vector<bool>>& stop_gradients){

    PADDLE_ENFORCE_EQ(
      inputs_.size(),
      1,
      platform::errors::InvalidArgument("polygamma op's inputs size should be 1, but now is %d.", inputs_.size()));
    PADDLE_ENFORCE_EQ(
      outputs.size(),
      1,
      platform::errors::InvalidArgument("polygamma op's outputs size should be 1, but now is %d.", outputs.size()));

    VLOG(6) << "Prepare inputs of polygamma_grad";

    Tensor x(std::make_shared<primitive::LazyTensor>(inputs_[0][0]));
    Tensor out_grad(std::make_shared<primitive::LazyTensor>(out_grads[0][0]));

    VLOG(6) << "Vjp prepare Prepare attributes of polygamma_grad";

    int n = op->attribute("n").dyn_cast<pir::Int32Attribute>().data();

    VLOG(6) << "Vjp prepare call polygamma_'s vjp inteface";

    std::vector<std::vector<Tensor>> tensor_res =
        primitive::polygamma_vjp(
        x, out_grad, n, stop_gradients);

    VLOG(6) << "Vjp prepare stop gradient of polygamma_grad";

    std::vector<std::vector<pir::OpResult>> res(tensor_res.size());
    for (size_t i = 0; i < tensor_res.size(); ++i) {
        res[i].resize(tensor_res[i].size());
        for (size_t j = 0; j < tensor_res[i].size(); ++j) {
            if(tensor_res[i][j].defined()){
                res[i][j] = std::static_pointer_cast<primitive::LazyTensor>(tensor_res[i][j].impl())->value().dyn_cast<pir::OpResult>();
            }
        }
    }
    return res;
}

std::vector<std::vector<pir::OpResult>> PowOp::Vjp(pir::Operation* op, const std::vector<std::vector<pir::Value>>& inputs_, const std::vector<std::vector<pir::Value>>& outputs, const std::vector<std::vector<pir::Value>>& out_grads, const std::vector<std::vector<bool>>& stop_gradients){

    PADDLE_ENFORCE_EQ(
      inputs_.size(),
      1,
      platform::errors::InvalidArgument("pow op's inputs size should be 1, but now is %d.", inputs_.size()));
    PADDLE_ENFORCE_EQ(
      outputs.size(),
      1,
      platform::errors::InvalidArgument("pow op's outputs size should be 1, but now is %d.", outputs.size()));

    VLOG(6) << "Prepare inputs of pow_grad";

    Tensor x(std::make_shared<primitive::LazyTensor>(inputs_[0][0]));
    Tensor out_grad(std::make_shared<primitive::LazyTensor>(out_grads[0][0]));

    VLOG(6) << "Vjp prepare Prepare attributes of pow_grad";

    phi::Scalar y = op->attribute("y").dyn_cast<paddle::dialect::ScalarAttribute>().data();

    VLOG(6) << "Vjp prepare call pow's vjp inteface";

    std::vector<std::vector<Tensor>> tensor_res =
        primitive::pow_vjp(
        x, out_grad, y, stop_gradients);

    VLOG(6) << "Vjp prepare stop gradient of pow_grad";

    std::vector<std::vector<pir::OpResult>> res(tensor_res.size());
    for (size_t i = 0; i < tensor_res.size(); ++i) {
        res[i].resize(tensor_res[i].size());
        for (size_t j = 0; j < tensor_res[i].size(); ++j) {
            if(tensor_res[i][j].defined()){
                res[i][j] = std::static_pointer_cast<primitive::LazyTensor>(tensor_res[i][j].impl())->value().dyn_cast<pir::OpResult>();
            }
        }
    }
    return res;
}

std::vector<std::vector<pir::OpResult>> Pow_Op::Vjp(pir::Operation* op, const std::vector<std::vector<pir::Value>>& inputs_, const std::vector<std::vector<pir::Value>>& outputs, const std::vector<std::vector<pir::Value>>& out_grads, const std::vector<std::vector<bool>>& stop_gradients){

    PADDLE_ENFORCE_EQ(
      inputs_.size(),
      1,
      platform::errors::InvalidArgument("pow op's inputs size should be 1, but now is %d.", inputs_.size()));
    PADDLE_ENFORCE_EQ(
      outputs.size(),
      1,
      platform::errors::InvalidArgument("pow op's outputs size should be 1, but now is %d.", outputs.size()));

    VLOG(6) << "Prepare inputs of pow_grad";

    Tensor x(std::make_shared<primitive::LazyTensor>(inputs_[0][0]));
    Tensor out_grad(std::make_shared<primitive::LazyTensor>(out_grads[0][0]));

    VLOG(6) << "Vjp prepare Prepare attributes of pow_grad";

    phi::Scalar y = op->attribute("y").dyn_cast<paddle::dialect::ScalarAttribute>().data();

    VLOG(6) << "Vjp prepare call pow_'s vjp inteface";

    std::vector<std::vector<Tensor>> tensor_res =
        primitive::pow_vjp(
        x, out_grad, y, stop_gradients);

    VLOG(6) << "Vjp prepare stop gradient of pow_grad";

    std::vector<std::vector<pir::OpResult>> res(tensor_res.size());
    for (size_t i = 0; i < tensor_res.size(); ++i) {
        res[i].resize(tensor_res[i].size());
        for (size_t j = 0; j < tensor_res[i].size(); ++j) {
            if(tensor_res[i][j].defined()){
                res[i][j] = std::static_pointer_cast<primitive::LazyTensor>(tensor_res[i][j].impl())->value().dyn_cast<pir::OpResult>();
            }
        }
    }
    return res;
}

std::vector<std::vector<pir::OpResult>> PreluOp::Vjp(pir::Operation* op, const std::vector<std::vector<pir::Value>>& inputs_, const std::vector<std::vector<pir::Value>>& outputs, const std::vector<std::vector<pir::Value>>& out_grads, const std::vector<std::vector<bool>>& stop_gradients){

    PADDLE_ENFORCE_EQ(
      inputs_.size(),
      2,
      platform::errors::InvalidArgument("prelu op's inputs size should be 2, but now is %d.", inputs_.size()));
    PADDLE_ENFORCE_EQ(
      outputs.size(),
      1,
      platform::errors::InvalidArgument("prelu op's outputs size should be 1, but now is %d.", outputs.size()));

    VLOG(6) << "Prepare inputs of prelu_grad";

    Tensor x(std::make_shared<primitive::LazyTensor>(inputs_[0][0]));
    Tensor alpha(std::make_shared<primitive::LazyTensor>(inputs_[1][0]));
    Tensor out_grad(std::make_shared<primitive::LazyTensor>(out_grads[0][0]));

    VLOG(6) << "Vjp prepare Prepare attributes of prelu_grad";

    std::string data_format = op->attribute("data_format").dyn_cast<pir::StrAttribute>().AsString();
    std::string mode = op->attribute("mode").dyn_cast<pir::StrAttribute>().AsString();

    VLOG(6) << "Vjp prepare call prelu's vjp inteface";

    std::vector<std::vector<Tensor>> tensor_res =
        primitive::prelu_vjp(
        x, alpha, out_grad, data_format, mode, stop_gradients);

    VLOG(6) << "Vjp prepare stop gradient of prelu_grad";

    std::vector<std::vector<pir::OpResult>> res(tensor_res.size());
    for (size_t i = 0; i < tensor_res.size(); ++i) {
        res[i].resize(tensor_res[i].size());
        for (size_t j = 0; j < tensor_res[i].size(); ++j) {
            if(tensor_res[i][j].defined()){
                res[i][j] = std::static_pointer_cast<primitive::LazyTensor>(tensor_res[i][j].impl())->value().dyn_cast<pir::OpResult>();
            }
        }
    }
    return res;
}

std::vector<std::vector<pir::OpResult>> PsroiPoolOp::Vjp(pir::Operation* op, const std::vector<std::vector<pir::Value>>& inputs_, const std::vector<std::vector<pir::Value>>& outputs, const std::vector<std::vector<pir::Value>>& out_grads, const std::vector<std::vector<bool>>& stop_gradients){

    PADDLE_ENFORCE_EQ(
      inputs_.size(),
      3,
      platform::errors::InvalidArgument("psroi_pool op's inputs size should be 3, but now is %d.", inputs_.size()));
    PADDLE_ENFORCE_EQ(
      outputs.size(),
      1,
      platform::errors::InvalidArgument("psroi_pool op's outputs size should be 1, but now is %d.", outputs.size()));

    VLOG(6) << "Prepare inputs of psroi_pool_grad";

    Tensor x(std::make_shared<primitive::LazyTensor>(inputs_[0][0]));
    Tensor boxes(std::make_shared<primitive::LazyTensor>(inputs_[1][0]));
    paddle::optional<Tensor> boxes_num;
    if (!IsEmptyValue(inputs_[2][0])){
        boxes_num = paddle::make_optional<Tensor>(Tensor(std::make_shared<primitive::LazyTensor>(inputs_[2][0])));
    }
    Tensor out_grad(std::make_shared<primitive::LazyTensor>(out_grads[0][0]));

    VLOG(6) << "Vjp prepare Prepare attributes of psroi_pool_grad";

    int pooled_height = op->attribute("pooled_height").dyn_cast<pir::Int32Attribute>().data();
    int pooled_width = op->attribute("pooled_width").dyn_cast<pir::Int32Attribute>().data();
    int output_channels = op->attribute("output_channels").dyn_cast<pir::Int32Attribute>().data();
    float spatial_scale = op->attribute("spatial_scale").dyn_cast<pir::FloatAttribute>().data();

    VLOG(6) << "Vjp prepare call psroi_pool's vjp inteface";

    std::vector<std::vector<Tensor>> tensor_res =
        primitive::psroi_pool_vjp(
        x, boxes, boxes_num, out_grad, pooled_height, pooled_width, output_channels, spatial_scale, stop_gradients);

    VLOG(6) << "Vjp prepare stop gradient of psroi_pool_grad";

    std::vector<std::vector<pir::OpResult>> res(tensor_res.size());
    for (size_t i = 0; i < tensor_res.size(); ++i) {
        res[i].resize(tensor_res[i].size());
        for (size_t j = 0; j < tensor_res[i].size(); ++j) {
            if(tensor_res[i][j].defined()){
                res[i][j] = std::static_pointer_cast<primitive::LazyTensor>(tensor_res[i][j].impl())->value().dyn_cast<pir::OpResult>();
            }
        }
    }
    return res;
}

std::vector<std::vector<pir::OpResult>> PutAlongAxisOp::Vjp(pir::Operation* op, const std::vector<std::vector<pir::Value>>& inputs_, const std::vector<std::vector<pir::Value>>& outputs, const std::vector<std::vector<pir::Value>>& out_grads, const std::vector<std::vector<bool>>& stop_gradients){

    PADDLE_ENFORCE_EQ(
      inputs_.size(),
      3,
      platform::errors::InvalidArgument("put_along_axis op's inputs size should be 3, but now is %d.", inputs_.size()));
    PADDLE_ENFORCE_EQ(
      outputs.size(),
      1,
      platform::errors::InvalidArgument("put_along_axis op's outputs size should be 1, but now is %d.", outputs.size()));

    VLOG(6) << "Prepare inputs of put_along_axis_grad";

    Tensor arr(std::make_shared<primitive::LazyTensor>(inputs_[0][0]));
    Tensor indices(std::make_shared<primitive::LazyTensor>(inputs_[1][0]));
    Tensor values(std::make_shared<primitive::LazyTensor>(inputs_[2][0]));
    Tensor out(std::make_shared<primitive::LazyTensor>(outputs[0][0]));
    Tensor out_grad(std::make_shared<primitive::LazyTensor>(out_grads[0][0]));

    VLOG(6) << "Vjp prepare Prepare attributes of put_along_axis_grad";

    int axis = op->attribute("axis").dyn_cast<pir::Int32Attribute>().data();
    std::string reduce = op->attribute("reduce").dyn_cast<pir::StrAttribute>().AsString();
    bool include_self = op->attribute("include_self").dyn_cast<pir::BoolAttribute>().data();

    VLOG(6) << "Vjp prepare call put_along_axis's vjp inteface";

    std::vector<std::vector<Tensor>> tensor_res =
        primitive::put_along_axis_vjp(
        arr, indices, values, out, out_grad, axis, reduce, include_self, stop_gradients);

    VLOG(6) << "Vjp prepare stop gradient of put_along_axis_grad";

    std::vector<std::vector<pir::OpResult>> res(tensor_res.size());
    for (size_t i = 0; i < tensor_res.size(); ++i) {
        res[i].resize(tensor_res[i].size());
        for (size_t j = 0; j < tensor_res[i].size(); ++j) {
            if(tensor_res[i][j].defined()){
                res[i][j] = std::static_pointer_cast<primitive::LazyTensor>(tensor_res[i][j].impl())->value().dyn_cast<pir::OpResult>();
            }
        }
    }
    return res;
}

std::vector<std::vector<pir::OpResult>> PutAlongAxis_Op::Vjp(pir::Operation* op, const std::vector<std::vector<pir::Value>>& inputs_, const std::vector<std::vector<pir::Value>>& outputs, const std::vector<std::vector<pir::Value>>& out_grads, const std::vector<std::vector<bool>>& stop_gradients){

    PADDLE_ENFORCE_EQ(
      inputs_.size(),
      3,
      platform::errors::InvalidArgument("put_along_axis op's inputs size should be 3, but now is %d.", inputs_.size()));
    PADDLE_ENFORCE_EQ(
      outputs.size(),
      1,
      platform::errors::InvalidArgument("put_along_axis op's outputs size should be 1, but now is %d.", outputs.size()));

    VLOG(6) << "Prepare inputs of put_along_axis_grad";

    Tensor arr(std::make_shared<primitive::LazyTensor>(inputs_[0][0]));
    Tensor indices(std::make_shared<primitive::LazyTensor>(inputs_[1][0]));
    Tensor values(std::make_shared<primitive::LazyTensor>(inputs_[2][0]));
    Tensor out(std::make_shared<primitive::LazyTensor>(outputs[0][0]));
    Tensor out_grad(std::make_shared<primitive::LazyTensor>(out_grads[0][0]));

    VLOG(6) << "Vjp prepare Prepare attributes of put_along_axis_grad";

    int axis = op->attribute("axis").dyn_cast<pir::Int32Attribute>().data();
    std::string reduce = op->attribute("reduce").dyn_cast<pir::StrAttribute>().AsString();
    bool include_self = op->attribute("include_self").dyn_cast<pir::BoolAttribute>().data();

    VLOG(6) << "Vjp prepare call put_along_axis_'s vjp inteface";

    std::vector<std::vector<Tensor>> tensor_res =
        primitive::put_along_axis_vjp(
        arr, indices, values, out, out_grad, axis, reduce, include_self, stop_gradients);

    VLOG(6) << "Vjp prepare stop gradient of put_along_axis_grad";

    std::vector<std::vector<pir::OpResult>> res(tensor_res.size());
    for (size_t i = 0; i < tensor_res.size(); ++i) {
        res[i].resize(tensor_res[i].size());
        for (size_t j = 0; j < tensor_res[i].size(); ++j) {
            if(tensor_res[i][j].defined()){
                res[i][j] = std::static_pointer_cast<primitive::LazyTensor>(tensor_res[i][j].impl())->value().dyn_cast<pir::OpResult>();
            }
        }
    }
    return res;
}

std::vector<std::vector<pir::OpResult>> QrOp::Vjp(pir::Operation* op, const std::vector<std::vector<pir::Value>>& inputs_, const std::vector<std::vector<pir::Value>>& outputs, const std::vector<std::vector<pir::Value>>& out_grads, const std::vector<std::vector<bool>>& stop_gradients){

    PADDLE_ENFORCE_EQ(
      inputs_.size(),
      1,
      platform::errors::InvalidArgument("qr op's inputs size should be 1, but now is %d.", inputs_.size()));
    PADDLE_ENFORCE_EQ(
      outputs.size(),
      2,
      platform::errors::InvalidArgument("qr op's outputs size should be 2, but now is %d.", outputs.size()));

    VLOG(6) << "Prepare inputs of qr_grad";

    Tensor x(std::make_shared<primitive::LazyTensor>(inputs_[0][0]));
    Tensor q(std::make_shared<primitive::LazyTensor>(outputs[0][0]));
    Tensor r(std::make_shared<primitive::LazyTensor>(outputs[1][0]));
    Tensor q_grad(std::make_shared<primitive::LazyTensor>(out_grads[0][0]));
    Tensor r_grad(std::make_shared<primitive::LazyTensor>(out_grads[1][0]));

    VLOG(6) << "Vjp prepare Prepare attributes of qr_grad";

    std::string mode = op->attribute("mode").dyn_cast<pir::StrAttribute>().AsString();

    VLOG(6) << "Vjp prepare call qr's vjp inteface";

    std::vector<std::vector<Tensor>> tensor_res =
        primitive::qr_vjp(
        x, q, r, q_grad, r_grad, mode, stop_gradients);

    VLOG(6) << "Vjp prepare stop gradient of qr_grad";

    std::vector<std::vector<pir::OpResult>> res(tensor_res.size());
    for (size_t i = 0; i < tensor_res.size(); ++i) {
        res[i].resize(tensor_res[i].size());
        for (size_t j = 0; j < tensor_res[i].size(); ++j) {
            if(tensor_res[i][j].defined()){
                res[i][j] = std::static_pointer_cast<primitive::LazyTensor>(tensor_res[i][j].impl())->value().dyn_cast<pir::OpResult>();
            }
        }
    }
    return res;
}

std::vector<std::vector<pir::OpResult>> RealOp::Vjp(pir::Operation* op, const std::vector<std::vector<pir::Value>>& inputs_, const std::vector<std::vector<pir::Value>>& outputs, const std::vector<std::vector<pir::Value>>& out_grads, const std::vector<std::vector<bool>>& stop_gradients){

    PADDLE_ENFORCE_EQ(
      inputs_.size(),
      1,
      platform::errors::InvalidArgument("real op's inputs size should be 1, but now is %d.", inputs_.size()));
    PADDLE_ENFORCE_EQ(
      outputs.size(),
      1,
      platform::errors::InvalidArgument("real op's outputs size should be 1, but now is %d.", outputs.size()));

    VLOG(6) << "Prepare inputs of real_grad";

    Tensor out_grad(std::make_shared<primitive::LazyTensor>(out_grads[0][0]));

    VLOG(6) << "Vjp prepare Prepare attributes of real_grad";


    VLOG(6) << "Vjp prepare call real's vjp inteface";

    std::vector<std::vector<Tensor>> tensor_res =
        primitive::real_vjp(
        out_grad, stop_gradients);

    VLOG(6) << "Vjp prepare stop gradient of real_grad";

    std::vector<std::vector<pir::OpResult>> res(tensor_res.size());
    for (size_t i = 0; i < tensor_res.size(); ++i) {
        res[i].resize(tensor_res[i].size());
        for (size_t j = 0; j < tensor_res[i].size(); ++j) {
            if(tensor_res[i][j].defined()){
                res[i][j] = std::static_pointer_cast<primitive::LazyTensor>(tensor_res[i][j].impl())->value().dyn_cast<pir::OpResult>();
            }
        }
    }
    return res;
}

std::vector<std::vector<pir::OpResult>> ReciprocalOp::Vjp(pir::Operation* op, const std::vector<std::vector<pir::Value>>& inputs_, const std::vector<std::vector<pir::Value>>& outputs, const std::vector<std::vector<pir::Value>>& out_grads, const std::vector<std::vector<bool>>& stop_gradients){

    PADDLE_ENFORCE_EQ(
      inputs_.size(),
      1,
      platform::errors::InvalidArgument("reciprocal op's inputs size should be 1, but now is %d.", inputs_.size()));
    PADDLE_ENFORCE_EQ(
      outputs.size(),
      1,
      platform::errors::InvalidArgument("reciprocal op's outputs size should be 1, but now is %d.", outputs.size()));

    VLOG(6) << "Prepare inputs of reciprocal_grad";

    Tensor out(std::make_shared<primitive::LazyTensor>(outputs[0][0]));
    Tensor out_grad(std::make_shared<primitive::LazyTensor>(out_grads[0][0]));

    VLOG(6) << "Vjp prepare Prepare attributes of reciprocal_grad";


    VLOG(6) << "Vjp prepare call reciprocal's vjp inteface";

    std::vector<std::vector<Tensor>> tensor_res =
        primitive::reciprocal_vjp(
        out, out_grad, stop_gradients);

    VLOG(6) << "Vjp prepare stop gradient of reciprocal_grad";

    std::vector<std::vector<pir::OpResult>> res(tensor_res.size());
    for (size_t i = 0; i < tensor_res.size(); ++i) {
        res[i].resize(tensor_res[i].size());
        for (size_t j = 0; j < tensor_res[i].size(); ++j) {
            if(tensor_res[i][j].defined()){
                res[i][j] = std::static_pointer_cast<primitive::LazyTensor>(tensor_res[i][j].impl())->value().dyn_cast<pir::OpResult>();
            }
        }
    }
    return res;
}

std::vector<std::vector<pir::OpResult>> Reciprocal_Op::Vjp(pir::Operation* op, const std::vector<std::vector<pir::Value>>& inputs_, const std::vector<std::vector<pir::Value>>& outputs, const std::vector<std::vector<pir::Value>>& out_grads, const std::vector<std::vector<bool>>& stop_gradients){

    PADDLE_ENFORCE_EQ(
      inputs_.size(),
      1,
      platform::errors::InvalidArgument("reciprocal op's inputs size should be 1, but now is %d.", inputs_.size()));
    PADDLE_ENFORCE_EQ(
      outputs.size(),
      1,
      platform::errors::InvalidArgument("reciprocal op's outputs size should be 1, but now is %d.", outputs.size()));

    VLOG(6) << "Prepare inputs of reciprocal_grad";

    Tensor out(std::make_shared<primitive::LazyTensor>(outputs[0][0]));
    Tensor out_grad(std::make_shared<primitive::LazyTensor>(out_grads[0][0]));

    VLOG(6) << "Vjp prepare Prepare attributes of reciprocal_grad";


    VLOG(6) << "Vjp prepare call reciprocal_'s vjp inteface";

    std::vector<std::vector<Tensor>> tensor_res =
        primitive::reciprocal_vjp(
        out, out_grad, stop_gradients);

    VLOG(6) << "Vjp prepare stop gradient of reciprocal_grad";

    std::vector<std::vector<pir::OpResult>> res(tensor_res.size());
    for (size_t i = 0; i < tensor_res.size(); ++i) {
        res[i].resize(tensor_res[i].size());
        for (size_t j = 0; j < tensor_res[i].size(); ++j) {
            if(tensor_res[i][j].defined()){
                res[i][j] = std::static_pointer_cast<primitive::LazyTensor>(tensor_res[i][j].impl())->value().dyn_cast<pir::OpResult>();
            }
        }
    }
    return res;
}

std::vector<std::vector<pir::OpResult>> ReluOp::Vjp(pir::Operation* op, const std::vector<std::vector<pir::Value>>& inputs_, const std::vector<std::vector<pir::Value>>& outputs, const std::vector<std::vector<pir::Value>>& out_grads, const std::vector<std::vector<bool>>& stop_gradients){

    PADDLE_ENFORCE_EQ(
      inputs_.size(),
      1,
      platform::errors::InvalidArgument("relu op's inputs size should be 1, but now is %d.", inputs_.size()));
    PADDLE_ENFORCE_EQ(
      outputs.size(),
      1,
      platform::errors::InvalidArgument("relu op's outputs size should be 1, but now is %d.", outputs.size()));

    VLOG(6) << "Prepare inputs of relu_grad";

    Tensor out(std::make_shared<primitive::LazyTensor>(outputs[0][0]));
    Tensor out_grad(std::make_shared<primitive::LazyTensor>(out_grads[0][0]));

    VLOG(6) << "Vjp prepare Prepare attributes of relu_grad";


    VLOG(6) << "Vjp prepare call relu's vjp inteface";

    std::vector<std::vector<Tensor>> tensor_res =
        primitive::relu_vjp(
        out, out_grad, stop_gradients);

    VLOG(6) << "Vjp prepare stop gradient of relu_grad";

    std::vector<std::vector<pir::OpResult>> res(tensor_res.size());
    for (size_t i = 0; i < tensor_res.size(); ++i) {
        res[i].resize(tensor_res[i].size());
        for (size_t j = 0; j < tensor_res[i].size(); ++j) {
            if(tensor_res[i][j].defined()){
                res[i][j] = std::static_pointer_cast<primitive::LazyTensor>(tensor_res[i][j].impl())->value().dyn_cast<pir::OpResult>();
            }
        }
    }
    return res;
}

std::vector<std::vector<pir::OpResult>> Relu_Op::Vjp(pir::Operation* op, const std::vector<std::vector<pir::Value>>& inputs_, const std::vector<std::vector<pir::Value>>& outputs, const std::vector<std::vector<pir::Value>>& out_grads, const std::vector<std::vector<bool>>& stop_gradients){

    PADDLE_ENFORCE_EQ(
      inputs_.size(),
      1,
      platform::errors::InvalidArgument("relu op's inputs size should be 1, but now is %d.", inputs_.size()));
    PADDLE_ENFORCE_EQ(
      outputs.size(),
      1,
      platform::errors::InvalidArgument("relu op's outputs size should be 1, but now is %d.", outputs.size()));

    VLOG(6) << "Prepare inputs of relu_grad";

    Tensor out(std::make_shared<primitive::LazyTensor>(outputs[0][0]));
    Tensor out_grad(std::make_shared<primitive::LazyTensor>(out_grads[0][0]));

    VLOG(6) << "Vjp prepare Prepare attributes of relu_grad";


    VLOG(6) << "Vjp prepare call relu_'s vjp inteface";

    std::vector<std::vector<Tensor>> tensor_res =
        primitive::relu_vjp(
        out, out_grad, stop_gradients);

    VLOG(6) << "Vjp prepare stop gradient of relu_grad";

    std::vector<std::vector<pir::OpResult>> res(tensor_res.size());
    for (size_t i = 0; i < tensor_res.size(); ++i) {
        res[i].resize(tensor_res[i].size());
        for (size_t j = 0; j < tensor_res[i].size(); ++j) {
            if(tensor_res[i][j].defined()){
                res[i][j] = std::static_pointer_cast<primitive::LazyTensor>(tensor_res[i][j].impl())->value().dyn_cast<pir::OpResult>();
            }
        }
    }
    return res;
}

std::vector<std::vector<pir::OpResult>> Relu6Op::Vjp(pir::Operation* op, const std::vector<std::vector<pir::Value>>& inputs_, const std::vector<std::vector<pir::Value>>& outputs, const std::vector<std::vector<pir::Value>>& out_grads, const std::vector<std::vector<bool>>& stop_gradients){

    PADDLE_ENFORCE_EQ(
      inputs_.size(),
      1,
      platform::errors::InvalidArgument("relu6 op's inputs size should be 1, but now is %d.", inputs_.size()));
    PADDLE_ENFORCE_EQ(
      outputs.size(),
      1,
      platform::errors::InvalidArgument("relu6 op's outputs size should be 1, but now is %d.", outputs.size()));

    VLOG(6) << "Prepare inputs of relu6_grad";

    Tensor out(std::make_shared<primitive::LazyTensor>(outputs[0][0]));
    Tensor out_grad(std::make_shared<primitive::LazyTensor>(out_grads[0][0]));

    VLOG(6) << "Vjp prepare Prepare attributes of relu6_grad";


    VLOG(6) << "Vjp prepare call relu6's vjp inteface";

    std::vector<std::vector<Tensor>> tensor_res =
        primitive::relu6_vjp(
        out, out_grad, stop_gradients);

    VLOG(6) << "Vjp prepare stop gradient of relu6_grad";

    std::vector<std::vector<pir::OpResult>> res(tensor_res.size());
    for (size_t i = 0; i < tensor_res.size(); ++i) {
        res[i].resize(tensor_res[i].size());
        for (size_t j = 0; j < tensor_res[i].size(); ++j) {
            if(tensor_res[i][j].defined()){
                res[i][j] = std::static_pointer_cast<primitive::LazyTensor>(tensor_res[i][j].impl())->value().dyn_cast<pir::OpResult>();
            }
        }
    }
    return res;
}

std::vector<std::vector<pir::OpResult>> RenormOp::Vjp(pir::Operation* op, const std::vector<std::vector<pir::Value>>& inputs_, const std::vector<std::vector<pir::Value>>& outputs, const std::vector<std::vector<pir::Value>>& out_grads, const std::vector<std::vector<bool>>& stop_gradients){

    PADDLE_ENFORCE_EQ(
      inputs_.size(),
      1,
      platform::errors::InvalidArgument("renorm op's inputs size should be 1, but now is %d.", inputs_.size()));
    PADDLE_ENFORCE_EQ(
      outputs.size(),
      1,
      platform::errors::InvalidArgument("renorm op's outputs size should be 1, but now is %d.", outputs.size()));

    VLOG(6) << "Prepare inputs of renorm_grad";

    Tensor x(std::make_shared<primitive::LazyTensor>(inputs_[0][0]));
    Tensor out_grad(std::make_shared<primitive::LazyTensor>(out_grads[0][0]));

    VLOG(6) << "Vjp prepare Prepare attributes of renorm_grad";

    float p = op->attribute("p").dyn_cast<pir::FloatAttribute>().data();
    int axis = op->attribute("axis").dyn_cast<pir::Int32Attribute>().data();
    float max_norm = op->attribute("max_norm").dyn_cast<pir::FloatAttribute>().data();

    VLOG(6) << "Vjp prepare call renorm's vjp inteface";

    std::vector<std::vector<Tensor>> tensor_res =
        primitive::renorm_vjp(
        x, out_grad, p, axis, max_norm, stop_gradients);

    VLOG(6) << "Vjp prepare stop gradient of renorm_grad";

    std::vector<std::vector<pir::OpResult>> res(tensor_res.size());
    for (size_t i = 0; i < tensor_res.size(); ++i) {
        res[i].resize(tensor_res[i].size());
        for (size_t j = 0; j < tensor_res[i].size(); ++j) {
            if(tensor_res[i][j].defined()){
                res[i][j] = std::static_pointer_cast<primitive::LazyTensor>(tensor_res[i][j].impl())->value().dyn_cast<pir::OpResult>();
            }
        }
    }
    return res;
}

std::vector<std::vector<pir::OpResult>> Renorm_Op::Vjp(pir::Operation* op, const std::vector<std::vector<pir::Value>>& inputs_, const std::vector<std::vector<pir::Value>>& outputs, const std::vector<std::vector<pir::Value>>& out_grads, const std::vector<std::vector<bool>>& stop_gradients){

    PADDLE_ENFORCE_EQ(
      inputs_.size(),
      1,
      platform::errors::InvalidArgument("renorm op's inputs size should be 1, but now is %d.", inputs_.size()));
    PADDLE_ENFORCE_EQ(
      outputs.size(),
      1,
      platform::errors::InvalidArgument("renorm op's outputs size should be 1, but now is %d.", outputs.size()));

    VLOG(6) << "Prepare inputs of renorm_grad";

    Tensor x(std::make_shared<primitive::LazyTensor>(inputs_[0][0]));
    Tensor out_grad(std::make_shared<primitive::LazyTensor>(out_grads[0][0]));

    VLOG(6) << "Vjp prepare Prepare attributes of renorm_grad";

    float p = op->attribute("p").dyn_cast<pir::FloatAttribute>().data();
    int axis = op->attribute("axis").dyn_cast<pir::Int32Attribute>().data();
    float max_norm = op->attribute("max_norm").dyn_cast<pir::FloatAttribute>().data();

    VLOG(6) << "Vjp prepare call renorm_'s vjp inteface";

    std::vector<std::vector<Tensor>> tensor_res =
        primitive::renorm_vjp(
        x, out_grad, p, axis, max_norm, stop_gradients);

    VLOG(6) << "Vjp prepare stop gradient of renorm_grad";

    std::vector<std::vector<pir::OpResult>> res(tensor_res.size());
    for (size_t i = 0; i < tensor_res.size(); ++i) {
        res[i].resize(tensor_res[i].size());
        for (size_t j = 0; j < tensor_res[i].size(); ++j) {
            if(tensor_res[i][j].defined()){
                res[i][j] = std::static_pointer_cast<primitive::LazyTensor>(tensor_res[i][j].impl())->value().dyn_cast<pir::OpResult>();
            }
        }
    }
    return res;
}

std::vector<std::vector<pir::OpResult>> ReverseOp::Vjp(pir::Operation* op, const std::vector<std::vector<pir::Value>>& inputs_, const std::vector<std::vector<pir::Value>>& outputs, const std::vector<std::vector<pir::Value>>& out_grads, const std::vector<std::vector<bool>>& stop_gradients){

    PADDLE_ENFORCE_EQ(
      inputs_.size(),
      2,
      platform::errors::InvalidArgument("reverse op's inputs size should be 2, but now is %d.", inputs_.size()));
    PADDLE_ENFORCE_EQ(
      outputs.size(),
      1,
      platform::errors::InvalidArgument("reverse op's outputs size should be 1, but now is %d.", outputs.size()));

    VLOG(6) << "Prepare inputs of reverse_grad";

    Tensor out_grad(std::make_shared<primitive::LazyTensor>(out_grads[0][0]));

    VLOG(6) << "Vjp prepare Prepare attributes of reverse_grad";

    Tensor axis(std::make_shared<primitive::LazyTensor>(inputs_[1][0]));

    VLOG(6) << "Vjp prepare call reverse's vjp inteface";

    std::vector<std::vector<Tensor>> tensor_res =
        primitive::reverse_vjp(
        out_grad, axis, stop_gradients);

    VLOG(6) << "Vjp prepare stop gradient of reverse_grad";

    std::vector<std::vector<pir::OpResult>> res(tensor_res.size());
    for (size_t i = 0; i < tensor_res.size(); ++i) {
        res[i].resize(tensor_res[i].size());
        for (size_t j = 0; j < tensor_res[i].size(); ++j) {
            if(tensor_res[i][j].defined()){
                res[i][j] = std::static_pointer_cast<primitive::LazyTensor>(tensor_res[i][j].impl())->value().dyn_cast<pir::OpResult>();
            }
        }
    }
    return res;
}

std::vector<std::vector<pir::OpResult>> RoiAlignOp::Vjp(pir::Operation* op, const std::vector<std::vector<pir::Value>>& inputs_, const std::vector<std::vector<pir::Value>>& outputs, const std::vector<std::vector<pir::Value>>& out_grads, const std::vector<std::vector<bool>>& stop_gradients){

    PADDLE_ENFORCE_EQ(
      inputs_.size(),
      3,
      platform::errors::InvalidArgument("roi_align op's inputs size should be 3, but now is %d.", inputs_.size()));
    PADDLE_ENFORCE_EQ(
      outputs.size(),
      1,
      platform::errors::InvalidArgument("roi_align op's outputs size should be 1, but now is %d.", outputs.size()));

    VLOG(6) << "Prepare inputs of roi_align_grad";

    Tensor x(std::make_shared<primitive::LazyTensor>(inputs_[0][0]));
    Tensor boxes(std::make_shared<primitive::LazyTensor>(inputs_[1][0]));
    paddle::optional<Tensor> boxes_num;
    if (!IsEmptyValue(inputs_[2][0])){
        boxes_num = paddle::make_optional<Tensor>(Tensor(std::make_shared<primitive::LazyTensor>(inputs_[2][0])));
    }
    Tensor out_grad(std::make_shared<primitive::LazyTensor>(out_grads[0][0]));

    VLOG(6) << "Vjp prepare Prepare attributes of roi_align_grad";

    int pooled_height = op->attribute("pooled_height").dyn_cast<pir::Int32Attribute>().data();
    int pooled_width = op->attribute("pooled_width").dyn_cast<pir::Int32Attribute>().data();
    float spatial_scale = op->attribute("spatial_scale").dyn_cast<pir::FloatAttribute>().data();
    int sampling_ratio = op->attribute("sampling_ratio").dyn_cast<pir::Int32Attribute>().data();
    bool aligned = op->attribute("aligned").dyn_cast<pir::BoolAttribute>().data();

    VLOG(6) << "Vjp prepare call roi_align's vjp inteface";

    std::vector<std::vector<Tensor>> tensor_res =
        primitive::roi_align_vjp(
        x, boxes, boxes_num, out_grad, pooled_height, pooled_width, spatial_scale, sampling_ratio, aligned, stop_gradients);

    VLOG(6) << "Vjp prepare stop gradient of roi_align_grad";

    std::vector<std::vector<pir::OpResult>> res(tensor_res.size());
    for (size_t i = 0; i < tensor_res.size(); ++i) {
        res[i].resize(tensor_res[i].size());
        for (size_t j = 0; j < tensor_res[i].size(); ++j) {
            if(tensor_res[i][j].defined()){
                res[i][j] = std::static_pointer_cast<primitive::LazyTensor>(tensor_res[i][j].impl())->value().dyn_cast<pir::OpResult>();
            }
        }
    }
    return res;
}

std::vector<std::vector<pir::OpResult>> RoiPoolOp::Vjp(pir::Operation* op, const std::vector<std::vector<pir::Value>>& inputs_, const std::vector<std::vector<pir::Value>>& outputs, const std::vector<std::vector<pir::Value>>& out_grads, const std::vector<std::vector<bool>>& stop_gradients){

    PADDLE_ENFORCE_EQ(
      inputs_.size(),
      3,
      platform::errors::InvalidArgument("roi_pool op's inputs size should be 3, but now is %d.", inputs_.size()));
    PADDLE_ENFORCE_EQ(
      outputs.size(),
      2,
      platform::errors::InvalidArgument("roi_pool op's outputs size should be 2, but now is %d.", outputs.size()));

    VLOG(6) << "Prepare inputs of roi_pool_grad";

    Tensor x(std::make_shared<primitive::LazyTensor>(inputs_[0][0]));
    Tensor boxes(std::make_shared<primitive::LazyTensor>(inputs_[1][0]));
    paddle::optional<Tensor> boxes_num;
    if (!IsEmptyValue(inputs_[2][0])){
        boxes_num = paddle::make_optional<Tensor>(Tensor(std::make_shared<primitive::LazyTensor>(inputs_[2][0])));
    }
    Tensor arg_max(std::make_shared<primitive::LazyTensor>(outputs[1][0]));
    Tensor out_grad(std::make_shared<primitive::LazyTensor>(out_grads[0][0]));

    VLOG(6) << "Vjp prepare Prepare attributes of roi_pool_grad";

    int pooled_height = op->attribute("pooled_height").dyn_cast<pir::Int32Attribute>().data();
    int pooled_width = op->attribute("pooled_width").dyn_cast<pir::Int32Attribute>().data();
    float spatial_scale = op->attribute("spatial_scale").dyn_cast<pir::FloatAttribute>().data();

    VLOG(6) << "Vjp prepare call roi_pool's vjp inteface";

    std::vector<std::vector<Tensor>> tensor_res =
        primitive::roi_pool_vjp(
        x, boxes, boxes_num, arg_max, out_grad, pooled_height, pooled_width, spatial_scale, stop_gradients);

    VLOG(6) << "Vjp prepare stop gradient of roi_pool_grad";

    std::vector<std::vector<pir::OpResult>> res(tensor_res.size());
    for (size_t i = 0; i < tensor_res.size(); ++i) {
        res[i].resize(tensor_res[i].size());
        for (size_t j = 0; j < tensor_res[i].size(); ++j) {
            if(tensor_res[i][j].defined()){
                res[i][j] = std::static_pointer_cast<primitive::LazyTensor>(tensor_res[i][j].impl())->value().dyn_cast<pir::OpResult>();
            }
        }
    }
    return res;
}

std::vector<std::vector<pir::OpResult>> RollOp::Vjp(pir::Operation* op, const std::vector<std::vector<pir::Value>>& inputs_, const std::vector<std::vector<pir::Value>>& outputs, const std::vector<std::vector<pir::Value>>& out_grads, const std::vector<std::vector<bool>>& stop_gradients){

    PADDLE_ENFORCE_EQ(
      inputs_.size(),
      2,
      platform::errors::InvalidArgument("roll op's inputs size should be 2, but now is %d.", inputs_.size()));
    PADDLE_ENFORCE_EQ(
      outputs.size(),
      1,
      platform::errors::InvalidArgument("roll op's outputs size should be 1, but now is %d.", outputs.size()));

    VLOG(6) << "Prepare inputs of roll_grad";

    Tensor x(std::make_shared<primitive::LazyTensor>(inputs_[0][0]));
    Tensor out_grad(std::make_shared<primitive::LazyTensor>(out_grads[0][0]));

    VLOG(6) << "Vjp prepare Prepare attributes of roll_grad";

    Tensor shifts(std::make_shared<primitive::LazyTensor>(inputs_[1][0]));
    std::vector<int64_t> axis;
    for (size_t i = 0; i < op->attribute("axis").dyn_cast<pir::ArrayAttribute>().size(); i++) {
        axis.push_back(op->attribute("axis").dyn_cast<pir::ArrayAttribute>().at(i).dyn_cast<pir::Int64Attribute>().data());
    }

    VLOG(6) << "Vjp prepare call roll's vjp inteface";

    std::vector<std::vector<Tensor>> tensor_res =
        primitive::roll_vjp(
        x, out_grad, shifts, axis, stop_gradients);

    VLOG(6) << "Vjp prepare stop gradient of roll_grad";

    std::vector<std::vector<pir::OpResult>> res(tensor_res.size());
    for (size_t i = 0; i < tensor_res.size(); ++i) {
        res[i].resize(tensor_res[i].size());
        for (size_t j = 0; j < tensor_res[i].size(); ++j) {
            if(tensor_res[i][j].defined()){
                res[i][j] = std::static_pointer_cast<primitive::LazyTensor>(tensor_res[i][j].impl())->value().dyn_cast<pir::OpResult>();
            }
        }
    }
    return res;
}

std::vector<std::vector<pir::OpResult>> RoundOp::Vjp(pir::Operation* op, const std::vector<std::vector<pir::Value>>& inputs_, const std::vector<std::vector<pir::Value>>& outputs, const std::vector<std::vector<pir::Value>>& out_grads, const std::vector<std::vector<bool>>& stop_gradients){

    PADDLE_ENFORCE_EQ(
      inputs_.size(),
      1,
      platform::errors::InvalidArgument("round op's inputs size should be 1, but now is %d.", inputs_.size()));
    PADDLE_ENFORCE_EQ(
      outputs.size(),
      1,
      platform::errors::InvalidArgument("round op's outputs size should be 1, but now is %d.", outputs.size()));

    VLOG(6) << "Prepare inputs of round_grad";

    Tensor out_grad(std::make_shared<primitive::LazyTensor>(out_grads[0][0]));

    VLOG(6) << "Vjp prepare Prepare attributes of round_grad";


    VLOG(6) << "Vjp prepare call round's vjp inteface";

    std::vector<std::vector<Tensor>> tensor_res =
        primitive::round_vjp(
        out_grad, stop_gradients);

    VLOG(6) << "Vjp prepare stop gradient of round_grad";

    std::vector<std::vector<pir::OpResult>> res(tensor_res.size());
    for (size_t i = 0; i < tensor_res.size(); ++i) {
        res[i].resize(tensor_res[i].size());
        for (size_t j = 0; j < tensor_res[i].size(); ++j) {
            if(tensor_res[i][j].defined()){
                res[i][j] = std::static_pointer_cast<primitive::LazyTensor>(tensor_res[i][j].impl())->value().dyn_cast<pir::OpResult>();
            }
        }
    }
    return res;
}

std::vector<std::vector<pir::OpResult>> Round_Op::Vjp(pir::Operation* op, const std::vector<std::vector<pir::Value>>& inputs_, const std::vector<std::vector<pir::Value>>& outputs, const std::vector<std::vector<pir::Value>>& out_grads, const std::vector<std::vector<bool>>& stop_gradients){

    PADDLE_ENFORCE_EQ(
      inputs_.size(),
      1,
      platform::errors::InvalidArgument("round op's inputs size should be 1, but now is %d.", inputs_.size()));
    PADDLE_ENFORCE_EQ(
      outputs.size(),
      1,
      platform::errors::InvalidArgument("round op's outputs size should be 1, but now is %d.", outputs.size()));

    VLOG(6) << "Prepare inputs of round_grad";

    Tensor out_grad(std::make_shared<primitive::LazyTensor>(out_grads[0][0]));

    VLOG(6) << "Vjp prepare Prepare attributes of round_grad";


    VLOG(6) << "Vjp prepare call round_'s vjp inteface";

    std::vector<std::vector<Tensor>> tensor_res =
        primitive::round_vjp(
        out_grad, stop_gradients);

    VLOG(6) << "Vjp prepare stop gradient of round_grad";

    std::vector<std::vector<pir::OpResult>> res(tensor_res.size());
    for (size_t i = 0; i < tensor_res.size(); ++i) {
        res[i].resize(tensor_res[i].size());
        for (size_t j = 0; j < tensor_res[i].size(); ++j) {
            if(tensor_res[i][j].defined()){
                res[i][j] = std::static_pointer_cast<primitive::LazyTensor>(tensor_res[i][j].impl())->value().dyn_cast<pir::OpResult>();
            }
        }
    }
    return res;
}

std::vector<std::vector<pir::OpResult>> RsqrtOp::Vjp(pir::Operation* op, const std::vector<std::vector<pir::Value>>& inputs_, const std::vector<std::vector<pir::Value>>& outputs, const std::vector<std::vector<pir::Value>>& out_grads, const std::vector<std::vector<bool>>& stop_gradients){

    PADDLE_ENFORCE_EQ(
      inputs_.size(),
      1,
      platform::errors::InvalidArgument("rsqrt op's inputs size should be 1, but now is %d.", inputs_.size()));
    PADDLE_ENFORCE_EQ(
      outputs.size(),
      1,
      platform::errors::InvalidArgument("rsqrt op's outputs size should be 1, but now is %d.", outputs.size()));

    VLOG(6) << "Prepare inputs of rsqrt_grad";

    Tensor out(std::make_shared<primitive::LazyTensor>(outputs[0][0]));
    Tensor out_grad(std::make_shared<primitive::LazyTensor>(out_grads[0][0]));

    VLOG(6) << "Vjp prepare Prepare attributes of rsqrt_grad";


    VLOG(6) << "Vjp prepare call rsqrt's vjp inteface";

    std::vector<std::vector<Tensor>> tensor_res =
        primitive::rsqrt_vjp(
        out, out_grad, stop_gradients);

    VLOG(6) << "Vjp prepare stop gradient of rsqrt_grad";

    std::vector<std::vector<pir::OpResult>> res(tensor_res.size());
    for (size_t i = 0; i < tensor_res.size(); ++i) {
        res[i].resize(tensor_res[i].size());
        for (size_t j = 0; j < tensor_res[i].size(); ++j) {
            if(tensor_res[i][j].defined()){
                res[i][j] = std::static_pointer_cast<primitive::LazyTensor>(tensor_res[i][j].impl())->value().dyn_cast<pir::OpResult>();
            }
        }
    }
    return res;
}

std::vector<std::vector<pir::OpResult>> Rsqrt_Op::Vjp(pir::Operation* op, const std::vector<std::vector<pir::Value>>& inputs_, const std::vector<std::vector<pir::Value>>& outputs, const std::vector<std::vector<pir::Value>>& out_grads, const std::vector<std::vector<bool>>& stop_gradients){

    PADDLE_ENFORCE_EQ(
      inputs_.size(),
      1,
      platform::errors::InvalidArgument("rsqrt op's inputs size should be 1, but now is %d.", inputs_.size()));
    PADDLE_ENFORCE_EQ(
      outputs.size(),
      1,
      platform::errors::InvalidArgument("rsqrt op's outputs size should be 1, but now is %d.", outputs.size()));

    VLOG(6) << "Prepare inputs of rsqrt_grad";

    Tensor out(std::make_shared<primitive::LazyTensor>(outputs[0][0]));
    Tensor out_grad(std::make_shared<primitive::LazyTensor>(out_grads[0][0]));

    VLOG(6) << "Vjp prepare Prepare attributes of rsqrt_grad";


    VLOG(6) << "Vjp prepare call rsqrt_'s vjp inteface";

    std::vector<std::vector<Tensor>> tensor_res =
        primitive::rsqrt_vjp(
        out, out_grad, stop_gradients);

    VLOG(6) << "Vjp prepare stop gradient of rsqrt_grad";

    std::vector<std::vector<pir::OpResult>> res(tensor_res.size());
    for (size_t i = 0; i < tensor_res.size(); ++i) {
        res[i].resize(tensor_res[i].size());
        for (size_t j = 0; j < tensor_res[i].size(); ++j) {
            if(tensor_res[i][j].defined()){
                res[i][j] = std::static_pointer_cast<primitive::LazyTensor>(tensor_res[i][j].impl())->value().dyn_cast<pir::OpResult>();
            }
        }
    }
    return res;
}

std::vector<std::vector<pir::OpResult>> ScaleOp::Vjp(pir::Operation* op, const std::vector<std::vector<pir::Value>>& inputs_, const std::vector<std::vector<pir::Value>>& outputs, const std::vector<std::vector<pir::Value>>& out_grads, const std::vector<std::vector<bool>>& stop_gradients){

    PADDLE_ENFORCE_EQ(
      inputs_.size(),
      2,
      platform::errors::InvalidArgument("scale op's inputs size should be 2, but now is %d.", inputs_.size()));
    PADDLE_ENFORCE_EQ(
      outputs.size(),
      1,
      platform::errors::InvalidArgument("scale op's outputs size should be 1, but now is %d.", outputs.size()));

    VLOG(6) << "Prepare inputs of scale_grad";

    Tensor out_grad(std::make_shared<primitive::LazyTensor>(out_grads[0][0]));

    VLOG(6) << "Vjp prepare Prepare attributes of scale_grad";

    Tensor scale(std::make_shared<primitive::LazyTensor>(inputs_[1][0]));

    VLOG(6) << "Vjp prepare call scale's vjp inteface";

    std::vector<std::vector<Tensor>> tensor_res =
        primitive::scale_vjp(
        out_grad, scale, stop_gradients);

    VLOG(6) << "Vjp prepare stop gradient of scale_grad";

    std::vector<std::vector<pir::OpResult>> res(tensor_res.size());
    for (size_t i = 0; i < tensor_res.size(); ++i) {
        res[i].resize(tensor_res[i].size());
        for (size_t j = 0; j < tensor_res[i].size(); ++j) {
            if(tensor_res[i][j].defined()){
                res[i][j] = std::static_pointer_cast<primitive::LazyTensor>(tensor_res[i][j].impl())->value().dyn_cast<pir::OpResult>();
            }
        }
    }
    return res;
}

std::vector<std::vector<pir::OpResult>> ScaleSrOp::Vjp(pir::Operation* op, const std::vector<std::vector<pir::Value>>& inputs_, const std::vector<std::vector<pir::Value>>& outputs, const std::vector<std::vector<pir::Value>>& out_grads, const std::vector<std::vector<bool>>& stop_gradients){

    PADDLE_ENFORCE_EQ(
      inputs_.size(),
      2,
      platform::errors::InvalidArgument("scale op's inputs size should be 2, but now is %d.", inputs_.size()));
    PADDLE_ENFORCE_EQ(
      outputs.size(),
      1,
      platform::errors::InvalidArgument("scale op's outputs size should be 1, but now is %d.", outputs.size()));

    VLOG(6) << "Prepare inputs of scale_grad";

    Tensor out_grad(std::make_shared<primitive::LazyTensor>(out_grads[0][0]));

    VLOG(6) << "Vjp prepare Prepare attributes of scale_grad";

    Tensor scale(std::make_shared<primitive::LazyTensor>(inputs_[1][0]));

    VLOG(6) << "Vjp prepare call scale's vjp inteface";

    std::vector<std::vector<Tensor>> tensor_res =
        primitive::scale_vjp(
        out_grad, scale, stop_gradients);

    VLOG(6) << "Vjp prepare stop gradient of scale_grad";

    std::vector<std::vector<pir::OpResult>> res(tensor_res.size());
    for (size_t i = 0; i < tensor_res.size(); ++i) {
        res[i].resize(tensor_res[i].size());
        for (size_t j = 0; j < tensor_res[i].size(); ++j) {
            if(tensor_res[i][j].defined()){
                res[i][j] = std::static_pointer_cast<primitive::LazyTensor>(tensor_res[i][j].impl())->value().dyn_cast<pir::OpResult>();
            }
        }
    }
    return res;
}

std::vector<std::vector<pir::OpResult>> Scale_Op::Vjp(pir::Operation* op, const std::vector<std::vector<pir::Value>>& inputs_, const std::vector<std::vector<pir::Value>>& outputs, const std::vector<std::vector<pir::Value>>& out_grads, const std::vector<std::vector<bool>>& stop_gradients){

    PADDLE_ENFORCE_EQ(
      inputs_.size(),
      2,
      platform::errors::InvalidArgument("scale op's inputs size should be 2, but now is %d.", inputs_.size()));
    PADDLE_ENFORCE_EQ(
      outputs.size(),
      1,
      platform::errors::InvalidArgument("scale op's outputs size should be 1, but now is %d.", outputs.size()));

    VLOG(6) << "Prepare inputs of scale_grad";

    Tensor out_grad(std::make_shared<primitive::LazyTensor>(out_grads[0][0]));

    VLOG(6) << "Vjp prepare Prepare attributes of scale_grad";

    Tensor scale(std::make_shared<primitive::LazyTensor>(inputs_[1][0]));

    VLOG(6) << "Vjp prepare call scale_'s vjp inteface";

    std::vector<std::vector<Tensor>> tensor_res =
        primitive::scale_vjp(
        out_grad, scale, stop_gradients);

    VLOG(6) << "Vjp prepare stop gradient of scale_grad";

    std::vector<std::vector<pir::OpResult>> res(tensor_res.size());
    for (size_t i = 0; i < tensor_res.size(); ++i) {
        res[i].resize(tensor_res[i].size());
        for (size_t j = 0; j < tensor_res[i].size(); ++j) {
            if(tensor_res[i][j].defined()){
                res[i][j] = std::static_pointer_cast<primitive::LazyTensor>(tensor_res[i][j].impl())->value().dyn_cast<pir::OpResult>();
            }
        }
    }
    return res;
}

std::vector<std::vector<pir::OpResult>> ScaleSr_Op::Vjp(pir::Operation* op, const std::vector<std::vector<pir::Value>>& inputs_, const std::vector<std::vector<pir::Value>>& outputs, const std::vector<std::vector<pir::Value>>& out_grads, const std::vector<std::vector<bool>>& stop_gradients){

    PADDLE_ENFORCE_EQ(
      inputs_.size(),
      2,
      platform::errors::InvalidArgument("scale op's inputs size should be 2, but now is %d.", inputs_.size()));
    PADDLE_ENFORCE_EQ(
      outputs.size(),
      1,
      platform::errors::InvalidArgument("scale op's outputs size should be 1, but now is %d.", outputs.size()));

    VLOG(6) << "Prepare inputs of scale_grad";

    Tensor out_grad(std::make_shared<primitive::LazyTensor>(out_grads[0][0]));

    VLOG(6) << "Vjp prepare Prepare attributes of scale_grad";

    Tensor scale(std::make_shared<primitive::LazyTensor>(inputs_[1][0]));

    VLOG(6) << "Vjp prepare call scale_'s vjp inteface";

    std::vector<std::vector<Tensor>> tensor_res =
        primitive::scale_vjp(
        out_grad, scale, stop_gradients);

    VLOG(6) << "Vjp prepare stop gradient of scale_grad";

    std::vector<std::vector<pir::OpResult>> res(tensor_res.size());
    for (size_t i = 0; i < tensor_res.size(); ++i) {
        res[i].resize(tensor_res[i].size());
        for (size_t j = 0; j < tensor_res[i].size(); ++j) {
            if(tensor_res[i][j].defined()){
                res[i][j] = std::static_pointer_cast<primitive::LazyTensor>(tensor_res[i][j].impl())->value().dyn_cast<pir::OpResult>();
            }
        }
    }
    return res;
}

std::vector<std::vector<pir::OpResult>> ScatterOp::Vjp(pir::Operation* op, const std::vector<std::vector<pir::Value>>& inputs_, const std::vector<std::vector<pir::Value>>& outputs, const std::vector<std::vector<pir::Value>>& out_grads, const std::vector<std::vector<bool>>& stop_gradients){

    PADDLE_ENFORCE_EQ(
      inputs_.size(),
      3,
      platform::errors::InvalidArgument("scatter op's inputs size should be 3, but now is %d.", inputs_.size()));
    PADDLE_ENFORCE_EQ(
      outputs.size(),
      1,
      platform::errors::InvalidArgument("scatter op's outputs size should be 1, but now is %d.", outputs.size()));

    VLOG(6) << "Prepare inputs of scatter_grad";

    Tensor index(std::make_shared<primitive::LazyTensor>(inputs_[1][0]));
    Tensor updates(std::make_shared<primitive::LazyTensor>(inputs_[2][0]));
    Tensor out_grad(std::make_shared<primitive::LazyTensor>(out_grads[0][0]));

    VLOG(6) << "Vjp prepare Prepare attributes of scatter_grad";

    bool overwrite = op->attribute("overwrite").dyn_cast<pir::BoolAttribute>().data();

    VLOG(6) << "Vjp prepare call scatter's vjp inteface";

    std::vector<std::vector<Tensor>> tensor_res =
        primitive::scatter_vjp(
        index, updates, out_grad, overwrite, stop_gradients);

    VLOG(6) << "Vjp prepare stop gradient of scatter_grad";

    std::vector<std::vector<pir::OpResult>> res(tensor_res.size());
    for (size_t i = 0; i < tensor_res.size(); ++i) {
        res[i].resize(tensor_res[i].size());
        for (size_t j = 0; j < tensor_res[i].size(); ++j) {
            if(tensor_res[i][j].defined()){
                res[i][j] = std::static_pointer_cast<primitive::LazyTensor>(tensor_res[i][j].impl())->value().dyn_cast<pir::OpResult>();
            }
        }
    }
    return res;
}

std::vector<std::vector<pir::OpResult>> Scatter_Op::Vjp(pir::Operation* op, const std::vector<std::vector<pir::Value>>& inputs_, const std::vector<std::vector<pir::Value>>& outputs, const std::vector<std::vector<pir::Value>>& out_grads, const std::vector<std::vector<bool>>& stop_gradients){

    PADDLE_ENFORCE_EQ(
      inputs_.size(),
      3,
      platform::errors::InvalidArgument("scatter op's inputs size should be 3, but now is %d.", inputs_.size()));
    PADDLE_ENFORCE_EQ(
      outputs.size(),
      1,
      platform::errors::InvalidArgument("scatter op's outputs size should be 1, but now is %d.", outputs.size()));

    VLOG(6) << "Prepare inputs of scatter_grad";

    Tensor index(std::make_shared<primitive::LazyTensor>(inputs_[1][0]));
    Tensor updates(std::make_shared<primitive::LazyTensor>(inputs_[2][0]));
    Tensor out_grad(std::make_shared<primitive::LazyTensor>(out_grads[0][0]));

    VLOG(6) << "Vjp prepare Prepare attributes of scatter_grad";

    bool overwrite = op->attribute("overwrite").dyn_cast<pir::BoolAttribute>().data();

    VLOG(6) << "Vjp prepare call scatter_'s vjp inteface";

    std::vector<std::vector<Tensor>> tensor_res =
        primitive::scatter_vjp(
        index, updates, out_grad, overwrite, stop_gradients);

    VLOG(6) << "Vjp prepare stop gradient of scatter_grad";

    std::vector<std::vector<pir::OpResult>> res(tensor_res.size());
    for (size_t i = 0; i < tensor_res.size(); ++i) {
        res[i].resize(tensor_res[i].size());
        for (size_t j = 0; j < tensor_res[i].size(); ++j) {
            if(tensor_res[i][j].defined()){
                res[i][j] = std::static_pointer_cast<primitive::LazyTensor>(tensor_res[i][j].impl())->value().dyn_cast<pir::OpResult>();
            }
        }
    }
    return res;
}

std::vector<std::vector<pir::OpResult>> ScatterNdAddOp::Vjp(pir::Operation* op, const std::vector<std::vector<pir::Value>>& inputs_, const std::vector<std::vector<pir::Value>>& outputs, const std::vector<std::vector<pir::Value>>& out_grads, const std::vector<std::vector<bool>>& stop_gradients){

    PADDLE_ENFORCE_EQ(
      inputs_.size(),
      3,
      platform::errors::InvalidArgument("scatter_nd_add op's inputs size should be 3, but now is %d.", inputs_.size()));
    PADDLE_ENFORCE_EQ(
      outputs.size(),
      1,
      platform::errors::InvalidArgument("scatter_nd_add op's outputs size should be 1, but now is %d.", outputs.size()));

    VLOG(6) << "Prepare inputs of scatter_nd_add_grad";

    Tensor index(std::make_shared<primitive::LazyTensor>(inputs_[1][0]));
    Tensor updates(std::make_shared<primitive::LazyTensor>(inputs_[2][0]));
    Tensor out_grad(std::make_shared<primitive::LazyTensor>(out_grads[0][0]));

    VLOG(6) << "Vjp prepare Prepare attributes of scatter_nd_add_grad";


    VLOG(6) << "Vjp prepare call scatter_nd_add's vjp inteface";

    std::vector<std::vector<Tensor>> tensor_res =
        primitive::scatter_nd_add_vjp(
        index, updates, out_grad, stop_gradients);

    VLOG(6) << "Vjp prepare stop gradient of scatter_nd_add_grad";

    std::vector<std::vector<pir::OpResult>> res(tensor_res.size());
    for (size_t i = 0; i < tensor_res.size(); ++i) {
        res[i].resize(tensor_res[i].size());
        for (size_t j = 0; j < tensor_res[i].size(); ++j) {
            if(tensor_res[i][j].defined()){
                res[i][j] = std::static_pointer_cast<primitive::LazyTensor>(tensor_res[i][j].impl())->value().dyn_cast<pir::OpResult>();
            }
        }
    }
    return res;
}

std::vector<std::vector<pir::OpResult>> SegmentPoolOp::Vjp(pir::Operation* op, const std::vector<std::vector<pir::Value>>& inputs_, const std::vector<std::vector<pir::Value>>& outputs, const std::vector<std::vector<pir::Value>>& out_grads, const std::vector<std::vector<bool>>& stop_gradients){

    PADDLE_ENFORCE_EQ(
      inputs_.size(),
      2,
      platform::errors::InvalidArgument("segment_pool op's inputs size should be 2, but now is %d.", inputs_.size()));
    PADDLE_ENFORCE_EQ(
      outputs.size(),
      2,
      platform::errors::InvalidArgument("segment_pool op's outputs size should be 2, but now is %d.", outputs.size()));

    VLOG(6) << "Prepare inputs of segment_pool_grad";

    Tensor x(std::make_shared<primitive::LazyTensor>(inputs_[0][0]));
    Tensor segment_ids(std::make_shared<primitive::LazyTensor>(inputs_[1][0]));
    Tensor out(std::make_shared<primitive::LazyTensor>(outputs[0][0]));
    paddle::optional<Tensor> summed_ids;
    if (!IsEmptyValue(outputs[1][0])){
        summed_ids = paddle::make_optional<Tensor>(Tensor(std::make_shared<primitive::LazyTensor>(outputs[1][0])));
    }
    Tensor out_grad(std::make_shared<primitive::LazyTensor>(out_grads[0][0]));

    VLOG(6) << "Vjp prepare Prepare attributes of segment_pool_grad";

    std::string pooltype = op->attribute("pooltype").dyn_cast<pir::StrAttribute>().AsString();

    VLOG(6) << "Vjp prepare call segment_pool's vjp inteface";

    std::vector<std::vector<Tensor>> tensor_res =
        primitive::segment_pool_vjp(
        x, segment_ids, out, summed_ids, out_grad, pooltype, stop_gradients);

    VLOG(6) << "Vjp prepare stop gradient of segment_pool_grad";

    std::vector<std::vector<pir::OpResult>> res(tensor_res.size());
    for (size_t i = 0; i < tensor_res.size(); ++i) {
        res[i].resize(tensor_res[i].size());
        for (size_t j = 0; j < tensor_res[i].size(); ++j) {
            if(tensor_res[i][j].defined()){
                res[i][j] = std::static_pointer_cast<primitive::LazyTensor>(tensor_res[i][j].impl())->value().dyn_cast<pir::OpResult>();
            }
        }
    }
    return res;
}

std::vector<std::vector<pir::OpResult>> SeluOp::Vjp(pir::Operation* op, const std::vector<std::vector<pir::Value>>& inputs_, const std::vector<std::vector<pir::Value>>& outputs, const std::vector<std::vector<pir::Value>>& out_grads, const std::vector<std::vector<bool>>& stop_gradients){

    PADDLE_ENFORCE_EQ(
      inputs_.size(),
      1,
      platform::errors::InvalidArgument("selu op's inputs size should be 1, but now is %d.", inputs_.size()));
    PADDLE_ENFORCE_EQ(
      outputs.size(),
      1,
      platform::errors::InvalidArgument("selu op's outputs size should be 1, but now is %d.", outputs.size()));

    VLOG(6) << "Prepare inputs of selu_grad";

    Tensor out(std::make_shared<primitive::LazyTensor>(outputs[0][0]));
    Tensor out_grad(std::make_shared<primitive::LazyTensor>(out_grads[0][0]));

    VLOG(6) << "Vjp prepare Prepare attributes of selu_grad";

    float scale = op->attribute("scale").dyn_cast<pir::FloatAttribute>().data();
    float alpha = op->attribute("alpha").dyn_cast<pir::FloatAttribute>().data();

    VLOG(6) << "Vjp prepare call selu's vjp inteface";

    std::vector<std::vector<Tensor>> tensor_res =
        primitive::selu_vjp(
        out, out_grad, scale, alpha, stop_gradients);

    VLOG(6) << "Vjp prepare stop gradient of selu_grad";

    std::vector<std::vector<pir::OpResult>> res(tensor_res.size());
    for (size_t i = 0; i < tensor_res.size(); ++i) {
        res[i].resize(tensor_res[i].size());
        for (size_t j = 0; j < tensor_res[i].size(); ++j) {
            if(tensor_res[i][j].defined()){
                res[i][j] = std::static_pointer_cast<primitive::LazyTensor>(tensor_res[i][j].impl())->value().dyn_cast<pir::OpResult>();
            }
        }
    }
    return res;
}

std::vector<std::vector<pir::OpResult>> SendURecvOp::Vjp(pir::Operation* op, const std::vector<std::vector<pir::Value>>& inputs_, const std::vector<std::vector<pir::Value>>& outputs, const std::vector<std::vector<pir::Value>>& out_grads, const std::vector<std::vector<bool>>& stop_gradients){

    PADDLE_ENFORCE_EQ(
      inputs_.size(),
      4,
      platform::errors::InvalidArgument("send_u_recv op's inputs size should be 4, but now is %d.", inputs_.size()));
    PADDLE_ENFORCE_EQ(
      outputs.size(),
      2,
      platform::errors::InvalidArgument("send_u_recv op's outputs size should be 2, but now is %d.", outputs.size()));

    VLOG(6) << "Prepare inputs of send_u_recv_grad";

    Tensor x(std::make_shared<primitive::LazyTensor>(inputs_[0][0]));
    Tensor src_index(std::make_shared<primitive::LazyTensor>(inputs_[1][0]));
    Tensor dst_index(std::make_shared<primitive::LazyTensor>(inputs_[2][0]));
    paddle::optional<Tensor> out;
    if (!IsEmptyValue(outputs[0][0])){
        out = paddle::make_optional<Tensor>(Tensor(std::make_shared<primitive::LazyTensor>(outputs[0][0])));
    }
    paddle::optional<Tensor> dst_count;
    if (!IsEmptyValue(outputs[1][0])){
        dst_count = paddle::make_optional<Tensor>(Tensor(std::make_shared<primitive::LazyTensor>(outputs[1][0])));
    }
    Tensor out_grad(std::make_shared<primitive::LazyTensor>(out_grads[0][0]));

    VLOG(6) << "Vjp prepare Prepare attributes of send_u_recv_grad";

    std::string reduce_op = op->attribute("reduce_op").dyn_cast<pir::StrAttribute>().AsString();

    VLOG(6) << "Vjp prepare call send_u_recv's vjp inteface";

    std::vector<std::vector<Tensor>> tensor_res =
        primitive::send_u_recv_vjp(
        x, src_index, dst_index, out, dst_count, out_grad, reduce_op, stop_gradients);

    VLOG(6) << "Vjp prepare stop gradient of send_u_recv_grad";

    std::vector<std::vector<pir::OpResult>> res(tensor_res.size());
    for (size_t i = 0; i < tensor_res.size(); ++i) {
        res[i].resize(tensor_res[i].size());
        for (size_t j = 0; j < tensor_res[i].size(); ++j) {
            if(tensor_res[i][j].defined()){
                res[i][j] = std::static_pointer_cast<primitive::LazyTensor>(tensor_res[i][j].impl())->value().dyn_cast<pir::OpResult>();
            }
        }
    }
    return res;
}

std::vector<std::vector<pir::OpResult>> SendUeRecvOp::Vjp(pir::Operation* op, const std::vector<std::vector<pir::Value>>& inputs_, const std::vector<std::vector<pir::Value>>& outputs, const std::vector<std::vector<pir::Value>>& out_grads, const std::vector<std::vector<bool>>& stop_gradients){

    PADDLE_ENFORCE_EQ(
      inputs_.size(),
      5,
      platform::errors::InvalidArgument("send_ue_recv op's inputs size should be 5, but now is %d.", inputs_.size()));
    PADDLE_ENFORCE_EQ(
      outputs.size(),
      2,
      platform::errors::InvalidArgument("send_ue_recv op's outputs size should be 2, but now is %d.", outputs.size()));

    VLOG(6) << "Prepare inputs of send_ue_recv_grad";

    Tensor x(std::make_shared<primitive::LazyTensor>(inputs_[0][0]));
    Tensor y(std::make_shared<primitive::LazyTensor>(inputs_[1][0]));
    Tensor src_index(std::make_shared<primitive::LazyTensor>(inputs_[2][0]));
    Tensor dst_index(std::make_shared<primitive::LazyTensor>(inputs_[3][0]));
    paddle::optional<Tensor> out;
    if (!IsEmptyValue(outputs[0][0])){
        out = paddle::make_optional<Tensor>(Tensor(std::make_shared<primitive::LazyTensor>(outputs[0][0])));
    }
    paddle::optional<Tensor> dst_count;
    if (!IsEmptyValue(outputs[1][0])){
        dst_count = paddle::make_optional<Tensor>(Tensor(std::make_shared<primitive::LazyTensor>(outputs[1][0])));
    }
    Tensor out_grad(std::make_shared<primitive::LazyTensor>(out_grads[0][0]));

    VLOG(6) << "Vjp prepare Prepare attributes of send_ue_recv_grad";

    std::string message_op = op->attribute("message_op").dyn_cast<pir::StrAttribute>().AsString();
    std::string reduce_op = op->attribute("reduce_op").dyn_cast<pir::StrAttribute>().AsString();

    VLOG(6) << "Vjp prepare call send_ue_recv's vjp inteface";

    std::vector<std::vector<Tensor>> tensor_res =
        primitive::send_ue_recv_vjp(
        x, y, src_index, dst_index, out, dst_count, out_grad, message_op, reduce_op, stop_gradients);

    VLOG(6) << "Vjp prepare stop gradient of send_ue_recv_grad";

    std::vector<std::vector<pir::OpResult>> res(tensor_res.size());
    for (size_t i = 0; i < tensor_res.size(); ++i) {
        res[i].resize(tensor_res[i].size());
        for (size_t j = 0; j < tensor_res[i].size(); ++j) {
            if(tensor_res[i][j].defined()){
                res[i][j] = std::static_pointer_cast<primitive::LazyTensor>(tensor_res[i][j].impl())->value().dyn_cast<pir::OpResult>();
            }
        }
    }
    return res;
}

std::vector<std::vector<pir::OpResult>> SendUvOp::Vjp(pir::Operation* op, const std::vector<std::vector<pir::Value>>& inputs_, const std::vector<std::vector<pir::Value>>& outputs, const std::vector<std::vector<pir::Value>>& out_grads, const std::vector<std::vector<bool>>& stop_gradients){

    PADDLE_ENFORCE_EQ(
      inputs_.size(),
      4,
      platform::errors::InvalidArgument("send_uv op's inputs size should be 4, but now is %d.", inputs_.size()));
    PADDLE_ENFORCE_EQ(
      outputs.size(),
      1,
      platform::errors::InvalidArgument("send_uv op's outputs size should be 1, but now is %d.", outputs.size()));

    VLOG(6) << "Prepare inputs of send_uv_grad";

    Tensor x(std::make_shared<primitive::LazyTensor>(inputs_[0][0]));
    Tensor y(std::make_shared<primitive::LazyTensor>(inputs_[1][0]));
    Tensor src_index(std::make_shared<primitive::LazyTensor>(inputs_[2][0]));
    Tensor dst_index(std::make_shared<primitive::LazyTensor>(inputs_[3][0]));
    Tensor out_grad(std::make_shared<primitive::LazyTensor>(out_grads[0][0]));

    VLOG(6) << "Vjp prepare Prepare attributes of send_uv_grad";

    std::string message_op = op->attribute("message_op").dyn_cast<pir::StrAttribute>().AsString();

    VLOG(6) << "Vjp prepare call send_uv's vjp inteface";

    std::vector<std::vector<Tensor>> tensor_res =
        primitive::send_uv_vjp(
        x, y, src_index, dst_index, out_grad, message_op, stop_gradients);

    VLOG(6) << "Vjp prepare stop gradient of send_uv_grad";

    std::vector<std::vector<pir::OpResult>> res(tensor_res.size());
    for (size_t i = 0; i < tensor_res.size(); ++i) {
        res[i].resize(tensor_res[i].size());
        for (size_t j = 0; j < tensor_res[i].size(); ++j) {
            if(tensor_res[i][j].defined()){
                res[i][j] = std::static_pointer_cast<primitive::LazyTensor>(tensor_res[i][j].impl())->value().dyn_cast<pir::OpResult>();
            }
        }
    }
    return res;
}

std::vector<std::vector<pir::OpResult>> SigmoidOp::Vjp(pir::Operation* op, const std::vector<std::vector<pir::Value>>& inputs_, const std::vector<std::vector<pir::Value>>& outputs, const std::vector<std::vector<pir::Value>>& out_grads, const std::vector<std::vector<bool>>& stop_gradients){

    PADDLE_ENFORCE_EQ(
      inputs_.size(),
      1,
      platform::errors::InvalidArgument("sigmoid op's inputs size should be 1, but now is %d.", inputs_.size()));
    PADDLE_ENFORCE_EQ(
      outputs.size(),
      1,
      platform::errors::InvalidArgument("sigmoid op's outputs size should be 1, but now is %d.", outputs.size()));

    VLOG(6) << "Prepare inputs of sigmoid_grad";

    Tensor out(std::make_shared<primitive::LazyTensor>(outputs[0][0]));
    Tensor out_grad(std::make_shared<primitive::LazyTensor>(out_grads[0][0]));

    VLOG(6) << "Vjp prepare Prepare attributes of sigmoid_grad";


    VLOG(6) << "Vjp prepare call sigmoid's vjp inteface";

    std::vector<std::vector<Tensor>> tensor_res =
        primitive::sigmoid_vjp(
        out, out_grad, stop_gradients);

    VLOG(6) << "Vjp prepare stop gradient of sigmoid_grad";

    std::vector<std::vector<pir::OpResult>> res(tensor_res.size());
    for (size_t i = 0; i < tensor_res.size(); ++i) {
        res[i].resize(tensor_res[i].size());
        for (size_t j = 0; j < tensor_res[i].size(); ++j) {
            if(tensor_res[i][j].defined()){
                res[i][j] = std::static_pointer_cast<primitive::LazyTensor>(tensor_res[i][j].impl())->value().dyn_cast<pir::OpResult>();
            }
        }
    }
    return res;
}

std::vector<std::vector<pir::OpResult>> Sigmoid_Op::Vjp(pir::Operation* op, const std::vector<std::vector<pir::Value>>& inputs_, const std::vector<std::vector<pir::Value>>& outputs, const std::vector<std::vector<pir::Value>>& out_grads, const std::vector<std::vector<bool>>& stop_gradients){

    PADDLE_ENFORCE_EQ(
      inputs_.size(),
      1,
      platform::errors::InvalidArgument("sigmoid op's inputs size should be 1, but now is %d.", inputs_.size()));
    PADDLE_ENFORCE_EQ(
      outputs.size(),
      1,
      platform::errors::InvalidArgument("sigmoid op's outputs size should be 1, but now is %d.", outputs.size()));

    VLOG(6) << "Prepare inputs of sigmoid_grad";

    Tensor out(std::make_shared<primitive::LazyTensor>(outputs[0][0]));
    Tensor out_grad(std::make_shared<primitive::LazyTensor>(out_grads[0][0]));

    VLOG(6) << "Vjp prepare Prepare attributes of sigmoid_grad";


    VLOG(6) << "Vjp prepare call sigmoid_'s vjp inteface";

    std::vector<std::vector<Tensor>> tensor_res =
        primitive::sigmoid_vjp(
        out, out_grad, stop_gradients);

    VLOG(6) << "Vjp prepare stop gradient of sigmoid_grad";

    std::vector<std::vector<pir::OpResult>> res(tensor_res.size());
    for (size_t i = 0; i < tensor_res.size(); ++i) {
        res[i].resize(tensor_res[i].size());
        for (size_t j = 0; j < tensor_res[i].size(); ++j) {
            if(tensor_res[i][j].defined()){
                res[i][j] = std::static_pointer_cast<primitive::LazyTensor>(tensor_res[i][j].impl())->value().dyn_cast<pir::OpResult>();
            }
        }
    }
    return res;
}

std::vector<std::vector<pir::OpResult>> SigmoidCrossEntropyWithLogitsOp::Vjp(pir::Operation* op, const std::vector<std::vector<pir::Value>>& inputs_, const std::vector<std::vector<pir::Value>>& outputs, const std::vector<std::vector<pir::Value>>& out_grads, const std::vector<std::vector<bool>>& stop_gradients){

    PADDLE_ENFORCE_EQ(
      inputs_.size(),
      3,
      platform::errors::InvalidArgument("sigmoid_cross_entropy_with_logits op's inputs size should be 3, but now is %d.", inputs_.size()));
    PADDLE_ENFORCE_EQ(
      outputs.size(),
      1,
      platform::errors::InvalidArgument("sigmoid_cross_entropy_with_logits op's outputs size should be 1, but now is %d.", outputs.size()));

    VLOG(6) << "Prepare inputs of sigmoid_cross_entropy_with_logits_grad";

    Tensor x(std::make_shared<primitive::LazyTensor>(inputs_[0][0]));
    Tensor label(std::make_shared<primitive::LazyTensor>(inputs_[1][0]));
    paddle::optional<Tensor> pos_weight;
    if (!IsEmptyValue(inputs_[2][0])){
        pos_weight = paddle::make_optional<Tensor>(Tensor(std::make_shared<primitive::LazyTensor>(inputs_[2][0])));
    }
    Tensor out_grad(std::make_shared<primitive::LazyTensor>(out_grads[0][0]));

    VLOG(6) << "Vjp prepare Prepare attributes of sigmoid_cross_entropy_with_logits_grad";

    bool normalize = op->attribute("normalize").dyn_cast<pir::BoolAttribute>().data();
    int ignore_index = op->attribute("ignore_index").dyn_cast<pir::Int32Attribute>().data();

    VLOG(6) << "Vjp prepare call sigmoid_cross_entropy_with_logits's vjp inteface";

    std::vector<std::vector<Tensor>> tensor_res =
        primitive::sigmoid_cross_entropy_with_logits_vjp(
        x, label, pos_weight, out_grad, normalize, ignore_index, stop_gradients);

    VLOG(6) << "Vjp prepare stop gradient of sigmoid_cross_entropy_with_logits_grad";

    std::vector<std::vector<pir::OpResult>> res(tensor_res.size());
    for (size_t i = 0; i < tensor_res.size(); ++i) {
        res[i].resize(tensor_res[i].size());
        for (size_t j = 0; j < tensor_res[i].size(); ++j) {
            if(tensor_res[i][j].defined()){
                res[i][j] = std::static_pointer_cast<primitive::LazyTensor>(tensor_res[i][j].impl())->value().dyn_cast<pir::OpResult>();
            }
        }
    }
    return res;
}

std::vector<std::vector<pir::OpResult>> SigmoidCrossEntropyWithLogits_Op::Vjp(pir::Operation* op, const std::vector<std::vector<pir::Value>>& inputs_, const std::vector<std::vector<pir::Value>>& outputs, const std::vector<std::vector<pir::Value>>& out_grads, const std::vector<std::vector<bool>>& stop_gradients){

    PADDLE_ENFORCE_EQ(
      inputs_.size(),
      3,
      platform::errors::InvalidArgument("sigmoid_cross_entropy_with_logits op's inputs size should be 3, but now is %d.", inputs_.size()));
    PADDLE_ENFORCE_EQ(
      outputs.size(),
      1,
      platform::errors::InvalidArgument("sigmoid_cross_entropy_with_logits op's outputs size should be 1, but now is %d.", outputs.size()));

    VLOG(6) << "Prepare inputs of sigmoid_cross_entropy_with_logits_grad";

    Tensor x(std::make_shared<primitive::LazyTensor>(inputs_[0][0]));
    Tensor label(std::make_shared<primitive::LazyTensor>(inputs_[1][0]));
    paddle::optional<Tensor> pos_weight;
    if (!IsEmptyValue(inputs_[2][0])){
        pos_weight = paddle::make_optional<Tensor>(Tensor(std::make_shared<primitive::LazyTensor>(inputs_[2][0])));
    }
    Tensor out_grad(std::make_shared<primitive::LazyTensor>(out_grads[0][0]));

    VLOG(6) << "Vjp prepare Prepare attributes of sigmoid_cross_entropy_with_logits_grad";

    bool normalize = op->attribute("normalize").dyn_cast<pir::BoolAttribute>().data();
    int ignore_index = op->attribute("ignore_index").dyn_cast<pir::Int32Attribute>().data();

    VLOG(6) << "Vjp prepare call sigmoid_cross_entropy_with_logits_'s vjp inteface";

    std::vector<std::vector<Tensor>> tensor_res =
        primitive::sigmoid_cross_entropy_with_logits_vjp(
        x, label, pos_weight, out_grad, normalize, ignore_index, stop_gradients);

    VLOG(6) << "Vjp prepare stop gradient of sigmoid_cross_entropy_with_logits_grad";

    std::vector<std::vector<pir::OpResult>> res(tensor_res.size());
    for (size_t i = 0; i < tensor_res.size(); ++i) {
        res[i].resize(tensor_res[i].size());
        for (size_t j = 0; j < tensor_res[i].size(); ++j) {
            if(tensor_res[i][j].defined()){
                res[i][j] = std::static_pointer_cast<primitive::LazyTensor>(tensor_res[i][j].impl())->value().dyn_cast<pir::OpResult>();
            }
        }
    }
    return res;
}

std::vector<std::vector<pir::OpResult>> SignOp::Vjp(pir::Operation* op, const std::vector<std::vector<pir::Value>>& inputs_, const std::vector<std::vector<pir::Value>>& outputs, const std::vector<std::vector<pir::Value>>& out_grads, const std::vector<std::vector<bool>>& stop_gradients){

    PADDLE_ENFORCE_EQ(
      inputs_.size(),
      1,
      platform::errors::InvalidArgument("sign op's inputs size should be 1, but now is %d.", inputs_.size()));
    PADDLE_ENFORCE_EQ(
      outputs.size(),
      1,
      platform::errors::InvalidArgument("sign op's outputs size should be 1, but now is %d.", outputs.size()));

    VLOG(6) << "Prepare inputs of sign_grad";

    Tensor out_grad(std::make_shared<primitive::LazyTensor>(out_grads[0][0]));

    VLOG(6) << "Vjp prepare Prepare attributes of sign_grad";


    VLOG(6) << "Vjp prepare call sign's vjp inteface";

    std::vector<std::vector<Tensor>> tensor_res =
        primitive::sign_vjp(
        out_grad, stop_gradients);

    VLOG(6) << "Vjp prepare stop gradient of sign_grad";

    std::vector<std::vector<pir::OpResult>> res(tensor_res.size());
    for (size_t i = 0; i < tensor_res.size(); ++i) {
        res[i].resize(tensor_res[i].size());
        for (size_t j = 0; j < tensor_res[i].size(); ++j) {
            if(tensor_res[i][j].defined()){
                res[i][j] = std::static_pointer_cast<primitive::LazyTensor>(tensor_res[i][j].impl())->value().dyn_cast<pir::OpResult>();
            }
        }
    }
    return res;
}

std::vector<std::vector<pir::OpResult>> SiluOp::Vjp(pir::Operation* op, const std::vector<std::vector<pir::Value>>& inputs_, const std::vector<std::vector<pir::Value>>& outputs, const std::vector<std::vector<pir::Value>>& out_grads, const std::vector<std::vector<bool>>& stop_gradients){

    PADDLE_ENFORCE_EQ(
      inputs_.size(),
      1,
      platform::errors::InvalidArgument("silu op's inputs size should be 1, but now is %d.", inputs_.size()));
    PADDLE_ENFORCE_EQ(
      outputs.size(),
      1,
      platform::errors::InvalidArgument("silu op's outputs size should be 1, but now is %d.", outputs.size()));

    VLOG(6) << "Prepare inputs of silu_grad";

    Tensor x(std::make_shared<primitive::LazyTensor>(inputs_[0][0]));
    Tensor out(std::make_shared<primitive::LazyTensor>(outputs[0][0]));
    Tensor out_grad(std::make_shared<primitive::LazyTensor>(out_grads[0][0]));

    VLOG(6) << "Vjp prepare Prepare attributes of silu_grad";


    VLOG(6) << "Vjp prepare call silu's vjp inteface";

    std::vector<std::vector<Tensor>> tensor_res =
        primitive::silu_vjp(
        x, out, out_grad, stop_gradients);

    VLOG(6) << "Vjp prepare stop gradient of silu_grad";

    std::vector<std::vector<pir::OpResult>> res(tensor_res.size());
    for (size_t i = 0; i < tensor_res.size(); ++i) {
        res[i].resize(tensor_res[i].size());
        for (size_t j = 0; j < tensor_res[i].size(); ++j) {
            if(tensor_res[i][j].defined()){
                res[i][j] = std::static_pointer_cast<primitive::LazyTensor>(tensor_res[i][j].impl())->value().dyn_cast<pir::OpResult>();
            }
        }
    }
    return res;
}

std::vector<std::vector<pir::OpResult>> SinOp::Vjp(pir::Operation* op, const std::vector<std::vector<pir::Value>>& inputs_, const std::vector<std::vector<pir::Value>>& outputs, const std::vector<std::vector<pir::Value>>& out_grads, const std::vector<std::vector<bool>>& stop_gradients){

    PADDLE_ENFORCE_EQ(
      inputs_.size(),
      1,
      platform::errors::InvalidArgument("sin op's inputs size should be 1, but now is %d.", inputs_.size()));
    PADDLE_ENFORCE_EQ(
      outputs.size(),
      1,
      platform::errors::InvalidArgument("sin op's outputs size should be 1, but now is %d.", outputs.size()));

    VLOG(6) << "Prepare inputs of sin_grad";

    Tensor x(std::make_shared<primitive::LazyTensor>(inputs_[0][0]));
    Tensor out_grad(std::make_shared<primitive::LazyTensor>(out_grads[0][0]));

    VLOG(6) << "Vjp prepare Prepare attributes of sin_grad";


    VLOG(6) << "Vjp prepare call sin's vjp inteface";

    std::vector<std::vector<Tensor>> tensor_res =
        primitive::sin_vjp(
        x, out_grad, stop_gradients);

    VLOG(6) << "Vjp prepare stop gradient of sin_grad";

    std::vector<std::vector<pir::OpResult>> res(tensor_res.size());
    for (size_t i = 0; i < tensor_res.size(); ++i) {
        res[i].resize(tensor_res[i].size());
        for (size_t j = 0; j < tensor_res[i].size(); ++j) {
            if(tensor_res[i][j].defined()){
                res[i][j] = std::static_pointer_cast<primitive::LazyTensor>(tensor_res[i][j].impl())->value().dyn_cast<pir::OpResult>();
            }
        }
    }
    return res;
}

std::vector<std::vector<pir::OpResult>> Sin_Op::Vjp(pir::Operation* op, const std::vector<std::vector<pir::Value>>& inputs_, const std::vector<std::vector<pir::Value>>& outputs, const std::vector<std::vector<pir::Value>>& out_grads, const std::vector<std::vector<bool>>& stop_gradients){

    PADDLE_ENFORCE_EQ(
      inputs_.size(),
      1,
      platform::errors::InvalidArgument("sin op's inputs size should be 1, but now is %d.", inputs_.size()));
    PADDLE_ENFORCE_EQ(
      outputs.size(),
      1,
      platform::errors::InvalidArgument("sin op's outputs size should be 1, but now is %d.", outputs.size()));

    VLOG(6) << "Prepare inputs of sin_grad";

    Tensor x(std::make_shared<primitive::LazyTensor>(inputs_[0][0]));
    Tensor out_grad(std::make_shared<primitive::LazyTensor>(out_grads[0][0]));

    VLOG(6) << "Vjp prepare Prepare attributes of sin_grad";


    VLOG(6) << "Vjp prepare call sin_'s vjp inteface";

    std::vector<std::vector<Tensor>> tensor_res =
        primitive::sin_vjp(
        x, out_grad, stop_gradients);

    VLOG(6) << "Vjp prepare stop gradient of sin_grad";

    std::vector<std::vector<pir::OpResult>> res(tensor_res.size());
    for (size_t i = 0; i < tensor_res.size(); ++i) {
        res[i].resize(tensor_res[i].size());
        for (size_t j = 0; j < tensor_res[i].size(); ++j) {
            if(tensor_res[i][j].defined()){
                res[i][j] = std::static_pointer_cast<primitive::LazyTensor>(tensor_res[i][j].impl())->value().dyn_cast<pir::OpResult>();
            }
        }
    }
    return res;
}

std::vector<std::vector<pir::OpResult>> SinhOp::Vjp(pir::Operation* op, const std::vector<std::vector<pir::Value>>& inputs_, const std::vector<std::vector<pir::Value>>& outputs, const std::vector<std::vector<pir::Value>>& out_grads, const std::vector<std::vector<bool>>& stop_gradients){

    PADDLE_ENFORCE_EQ(
      inputs_.size(),
      1,
      platform::errors::InvalidArgument("sinh op's inputs size should be 1, but now is %d.", inputs_.size()));
    PADDLE_ENFORCE_EQ(
      outputs.size(),
      1,
      platform::errors::InvalidArgument("sinh op's outputs size should be 1, but now is %d.", outputs.size()));

    VLOG(6) << "Prepare inputs of sinh_grad";

    Tensor x(std::make_shared<primitive::LazyTensor>(inputs_[0][0]));
    Tensor out_grad(std::make_shared<primitive::LazyTensor>(out_grads[0][0]));

    VLOG(6) << "Vjp prepare Prepare attributes of sinh_grad";


    VLOG(6) << "Vjp prepare call sinh's vjp inteface";

    std::vector<std::vector<Tensor>> tensor_res =
        primitive::sinh_vjp(
        x, out_grad, stop_gradients);

    VLOG(6) << "Vjp prepare stop gradient of sinh_grad";

    std::vector<std::vector<pir::OpResult>> res(tensor_res.size());
    for (size_t i = 0; i < tensor_res.size(); ++i) {
        res[i].resize(tensor_res[i].size());
        for (size_t j = 0; j < tensor_res[i].size(); ++j) {
            if(tensor_res[i][j].defined()){
                res[i][j] = std::static_pointer_cast<primitive::LazyTensor>(tensor_res[i][j].impl())->value().dyn_cast<pir::OpResult>();
            }
        }
    }
    return res;
}

std::vector<std::vector<pir::OpResult>> Sinh_Op::Vjp(pir::Operation* op, const std::vector<std::vector<pir::Value>>& inputs_, const std::vector<std::vector<pir::Value>>& outputs, const std::vector<std::vector<pir::Value>>& out_grads, const std::vector<std::vector<bool>>& stop_gradients){

    PADDLE_ENFORCE_EQ(
      inputs_.size(),
      1,
      platform::errors::InvalidArgument("sinh op's inputs size should be 1, but now is %d.", inputs_.size()));
    PADDLE_ENFORCE_EQ(
      outputs.size(),
      1,
      platform::errors::InvalidArgument("sinh op's outputs size should be 1, but now is %d.", outputs.size()));

    VLOG(6) << "Prepare inputs of sinh_grad";

    Tensor x(std::make_shared<primitive::LazyTensor>(inputs_[0][0]));
    Tensor out_grad(std::make_shared<primitive::LazyTensor>(out_grads[0][0]));

    VLOG(6) << "Vjp prepare Prepare attributes of sinh_grad";


    VLOG(6) << "Vjp prepare call sinh_'s vjp inteface";

    std::vector<std::vector<Tensor>> tensor_res =
        primitive::sinh_vjp(
        x, out_grad, stop_gradients);

    VLOG(6) << "Vjp prepare stop gradient of sinh_grad";

    std::vector<std::vector<pir::OpResult>> res(tensor_res.size());
    for (size_t i = 0; i < tensor_res.size(); ++i) {
        res[i].resize(tensor_res[i].size());
        for (size_t j = 0; j < tensor_res[i].size(); ++j) {
            if(tensor_res[i][j].defined()){
                res[i][j] = std::static_pointer_cast<primitive::LazyTensor>(tensor_res[i][j].impl())->value().dyn_cast<pir::OpResult>();
            }
        }
    }
    return res;
}

std::vector<std::vector<pir::OpResult>> SlogdetOp::Vjp(pir::Operation* op, const std::vector<std::vector<pir::Value>>& inputs_, const std::vector<std::vector<pir::Value>>& outputs, const std::vector<std::vector<pir::Value>>& out_grads, const std::vector<std::vector<bool>>& stop_gradients){

    PADDLE_ENFORCE_EQ(
      inputs_.size(),
      1,
      platform::errors::InvalidArgument("slogdet op's inputs size should be 1, but now is %d.", inputs_.size()));
    PADDLE_ENFORCE_EQ(
      outputs.size(),
      1,
      platform::errors::InvalidArgument("slogdet op's outputs size should be 1, but now is %d.", outputs.size()));

    VLOG(6) << "Prepare inputs of slogdet_grad";

    Tensor x(std::make_shared<primitive::LazyTensor>(inputs_[0][0]));
    Tensor out(std::make_shared<primitive::LazyTensor>(outputs[0][0]));
    Tensor out_grad(std::make_shared<primitive::LazyTensor>(out_grads[0][0]));

    VLOG(6) << "Vjp prepare Prepare attributes of slogdet_grad";


    VLOG(6) << "Vjp prepare call slogdet's vjp inteface";

    std::vector<std::vector<Tensor>> tensor_res =
        primitive::slogdet_vjp(
        x, out, out_grad, stop_gradients);

    VLOG(6) << "Vjp prepare stop gradient of slogdet_grad";

    std::vector<std::vector<pir::OpResult>> res(tensor_res.size());
    for (size_t i = 0; i < tensor_res.size(); ++i) {
        res[i].resize(tensor_res[i].size());
        for (size_t j = 0; j < tensor_res[i].size(); ++j) {
            if(tensor_res[i][j].defined()){
                res[i][j] = std::static_pointer_cast<primitive::LazyTensor>(tensor_res[i][j].impl())->value().dyn_cast<pir::OpResult>();
            }
        }
    }
    return res;
}

std::vector<std::vector<pir::OpResult>> SoftplusOp::Vjp(pir::Operation* op, const std::vector<std::vector<pir::Value>>& inputs_, const std::vector<std::vector<pir::Value>>& outputs, const std::vector<std::vector<pir::Value>>& out_grads, const std::vector<std::vector<bool>>& stop_gradients){

    PADDLE_ENFORCE_EQ(
      inputs_.size(),
      1,
      platform::errors::InvalidArgument("softplus op's inputs size should be 1, but now is %d.", inputs_.size()));
    PADDLE_ENFORCE_EQ(
      outputs.size(),
      1,
      platform::errors::InvalidArgument("softplus op's outputs size should be 1, but now is %d.", outputs.size()));

    VLOG(6) << "Prepare inputs of softplus_grad";

    Tensor x(std::make_shared<primitive::LazyTensor>(inputs_[0][0]));
    Tensor out_grad(std::make_shared<primitive::LazyTensor>(out_grads[0][0]));

    VLOG(6) << "Vjp prepare Prepare attributes of softplus_grad";

    float beta = op->attribute("beta").dyn_cast<pir::FloatAttribute>().data();
    float threshold = op->attribute("threshold").dyn_cast<pir::FloatAttribute>().data();

    VLOG(6) << "Vjp prepare call softplus's vjp inteface";

    std::vector<std::vector<Tensor>> tensor_res =
        primitive::softplus_vjp(
        x, out_grad, beta, threshold, stop_gradients);

    VLOG(6) << "Vjp prepare stop gradient of softplus_grad";

    std::vector<std::vector<pir::OpResult>> res(tensor_res.size());
    for (size_t i = 0; i < tensor_res.size(); ++i) {
        res[i].resize(tensor_res[i].size());
        for (size_t j = 0; j < tensor_res[i].size(); ++j) {
            if(tensor_res[i][j].defined()){
                res[i][j] = std::static_pointer_cast<primitive::LazyTensor>(tensor_res[i][j].impl())->value().dyn_cast<pir::OpResult>();
            }
        }
    }
    return res;
}

std::vector<std::vector<pir::OpResult>> SoftshrinkOp::Vjp(pir::Operation* op, const std::vector<std::vector<pir::Value>>& inputs_, const std::vector<std::vector<pir::Value>>& outputs, const std::vector<std::vector<pir::Value>>& out_grads, const std::vector<std::vector<bool>>& stop_gradients){

    PADDLE_ENFORCE_EQ(
      inputs_.size(),
      1,
      platform::errors::InvalidArgument("softshrink op's inputs size should be 1, but now is %d.", inputs_.size()));
    PADDLE_ENFORCE_EQ(
      outputs.size(),
      1,
      platform::errors::InvalidArgument("softshrink op's outputs size should be 1, but now is %d.", outputs.size()));

    VLOG(6) << "Prepare inputs of softshrink_grad";

    Tensor x(std::make_shared<primitive::LazyTensor>(inputs_[0][0]));
    Tensor out_grad(std::make_shared<primitive::LazyTensor>(out_grads[0][0]));

    VLOG(6) << "Vjp prepare Prepare attributes of softshrink_grad";

    float threshold = op->attribute("threshold").dyn_cast<pir::FloatAttribute>().data();

    VLOG(6) << "Vjp prepare call softshrink's vjp inteface";

    std::vector<std::vector<Tensor>> tensor_res =
        primitive::softshrink_vjp(
        x, out_grad, threshold, stop_gradients);

    VLOG(6) << "Vjp prepare stop gradient of softshrink_grad";

    std::vector<std::vector<pir::OpResult>> res(tensor_res.size());
    for (size_t i = 0; i < tensor_res.size(); ++i) {
        res[i].resize(tensor_res[i].size());
        for (size_t j = 0; j < tensor_res[i].size(); ++j) {
            if(tensor_res[i][j].defined()){
                res[i][j] = std::static_pointer_cast<primitive::LazyTensor>(tensor_res[i][j].impl())->value().dyn_cast<pir::OpResult>();
            }
        }
    }
    return res;
}

std::vector<std::vector<pir::OpResult>> SoftsignOp::Vjp(pir::Operation* op, const std::vector<std::vector<pir::Value>>& inputs_, const std::vector<std::vector<pir::Value>>& outputs, const std::vector<std::vector<pir::Value>>& out_grads, const std::vector<std::vector<bool>>& stop_gradients){

    PADDLE_ENFORCE_EQ(
      inputs_.size(),
      1,
      platform::errors::InvalidArgument("softsign op's inputs size should be 1, but now is %d.", inputs_.size()));
    PADDLE_ENFORCE_EQ(
      outputs.size(),
      1,
      platform::errors::InvalidArgument("softsign op's outputs size should be 1, but now is %d.", outputs.size()));

    VLOG(6) << "Prepare inputs of softsign_grad";

    Tensor x(std::make_shared<primitive::LazyTensor>(inputs_[0][0]));
    Tensor out_grad(std::make_shared<primitive::LazyTensor>(out_grads[0][0]));

    VLOG(6) << "Vjp prepare Prepare attributes of softsign_grad";


    VLOG(6) << "Vjp prepare call softsign's vjp inteface";

    std::vector<std::vector<Tensor>> tensor_res =
        primitive::softsign_vjp(
        x, out_grad, stop_gradients);

    VLOG(6) << "Vjp prepare stop gradient of softsign_grad";

    std::vector<std::vector<pir::OpResult>> res(tensor_res.size());
    for (size_t i = 0; i < tensor_res.size(); ++i) {
        res[i].resize(tensor_res[i].size());
        for (size_t j = 0; j < tensor_res[i].size(); ++j) {
            if(tensor_res[i][j].defined()){
                res[i][j] = std::static_pointer_cast<primitive::LazyTensor>(tensor_res[i][j].impl())->value().dyn_cast<pir::OpResult>();
            }
        }
    }
    return res;
}

std::vector<std::vector<pir::OpResult>> SolveOp::Vjp(pir::Operation* op, const std::vector<std::vector<pir::Value>>& inputs_, const std::vector<std::vector<pir::Value>>& outputs, const std::vector<std::vector<pir::Value>>& out_grads, const std::vector<std::vector<bool>>& stop_gradients){

    PADDLE_ENFORCE_EQ(
      inputs_.size(),
      2,
      platform::errors::InvalidArgument("solve op's inputs size should be 2, but now is %d.", inputs_.size()));
    PADDLE_ENFORCE_EQ(
      outputs.size(),
      1,
      platform::errors::InvalidArgument("solve op's outputs size should be 1, but now is %d.", outputs.size()));

    VLOG(6) << "Prepare inputs of solve_grad";

    Tensor x(std::make_shared<primitive::LazyTensor>(inputs_[0][0]));
    Tensor y(std::make_shared<primitive::LazyTensor>(inputs_[1][0]));
    Tensor out(std::make_shared<primitive::LazyTensor>(outputs[0][0]));
    Tensor out_grad(std::make_shared<primitive::LazyTensor>(out_grads[0][0]));

    VLOG(6) << "Vjp prepare Prepare attributes of solve_grad";


    VLOG(6) << "Vjp prepare call solve's vjp inteface";

    std::vector<std::vector<Tensor>> tensor_res =
        primitive::solve_vjp(
        x, y, out, out_grad, stop_gradients);

    VLOG(6) << "Vjp prepare stop gradient of solve_grad";

    std::vector<std::vector<pir::OpResult>> res(tensor_res.size());
    for (size_t i = 0; i < tensor_res.size(); ++i) {
        res[i].resize(tensor_res[i].size());
        for (size_t j = 0; j < tensor_res[i].size(); ++j) {
            if(tensor_res[i][j].defined()){
                res[i][j] = std::static_pointer_cast<primitive::LazyTensor>(tensor_res[i][j].impl())->value().dyn_cast<pir::OpResult>();
            }
        }
    }
    return res;
}

std::vector<std::vector<pir::OpResult>> SpectralNormOp::Vjp(pir::Operation* op, const std::vector<std::vector<pir::Value>>& inputs_, const std::vector<std::vector<pir::Value>>& outputs, const std::vector<std::vector<pir::Value>>& out_grads, const std::vector<std::vector<bool>>& stop_gradients){

    PADDLE_ENFORCE_EQ(
      inputs_.size(),
      3,
      platform::errors::InvalidArgument("spectral_norm op's inputs size should be 3, but now is %d.", inputs_.size()));
    PADDLE_ENFORCE_EQ(
      outputs.size(),
      1,
      platform::errors::InvalidArgument("spectral_norm op's outputs size should be 1, but now is %d.", outputs.size()));

    VLOG(6) << "Prepare inputs of spectral_norm_grad";

    Tensor weight(std::make_shared<primitive::LazyTensor>(inputs_[0][0]));
    Tensor u(std::make_shared<primitive::LazyTensor>(inputs_[1][0]));
    Tensor v(std::make_shared<primitive::LazyTensor>(inputs_[2][0]));
    Tensor out_grad(std::make_shared<primitive::LazyTensor>(out_grads[0][0]));

    VLOG(6) << "Vjp prepare Prepare attributes of spectral_norm_grad";

    int dim = op->attribute("dim").dyn_cast<pir::Int32Attribute>().data();
    int power_iters = op->attribute("power_iters").dyn_cast<pir::Int32Attribute>().data();
    float eps = op->attribute("eps").dyn_cast<pir::FloatAttribute>().data();

    VLOG(6) << "Vjp prepare call spectral_norm's vjp inteface";

    std::vector<std::vector<Tensor>> tensor_res =
        primitive::spectral_norm_vjp(
        weight, u, v, out_grad, dim, power_iters, eps, stop_gradients);

    VLOG(6) << "Vjp prepare stop gradient of spectral_norm_grad";

    std::vector<std::vector<pir::OpResult>> res(tensor_res.size());
    for (size_t i = 0; i < tensor_res.size(); ++i) {
        res[i].resize(tensor_res[i].size());
        for (size_t j = 0; j < tensor_res[i].size(); ++j) {
            if(tensor_res[i][j].defined()){
                res[i][j] = std::static_pointer_cast<primitive::LazyTensor>(tensor_res[i][j].impl())->value().dyn_cast<pir::OpResult>();
            }
        }
    }
    return res;
}

std::vector<std::vector<pir::OpResult>> SqrtOp::Vjp(pir::Operation* op, const std::vector<std::vector<pir::Value>>& inputs_, const std::vector<std::vector<pir::Value>>& outputs, const std::vector<std::vector<pir::Value>>& out_grads, const std::vector<std::vector<bool>>& stop_gradients){

    PADDLE_ENFORCE_EQ(
      inputs_.size(),
      1,
      platform::errors::InvalidArgument("sqrt op's inputs size should be 1, but now is %d.", inputs_.size()));
    PADDLE_ENFORCE_EQ(
      outputs.size(),
      1,
      platform::errors::InvalidArgument("sqrt op's outputs size should be 1, but now is %d.", outputs.size()));

    VLOG(6) << "Prepare inputs of sqrt_grad";

    Tensor out(std::make_shared<primitive::LazyTensor>(outputs[0][0]));
    Tensor out_grad(std::make_shared<primitive::LazyTensor>(out_grads[0][0]));

    VLOG(6) << "Vjp prepare Prepare attributes of sqrt_grad";


    VLOG(6) << "Vjp prepare call sqrt's vjp inteface";

    std::vector<std::vector<Tensor>> tensor_res =
        primitive::sqrt_vjp(
        out, out_grad, stop_gradients);

    VLOG(6) << "Vjp prepare stop gradient of sqrt_grad";

    std::vector<std::vector<pir::OpResult>> res(tensor_res.size());
    for (size_t i = 0; i < tensor_res.size(); ++i) {
        res[i].resize(tensor_res[i].size());
        for (size_t j = 0; j < tensor_res[i].size(); ++j) {
            if(tensor_res[i][j].defined()){
                res[i][j] = std::static_pointer_cast<primitive::LazyTensor>(tensor_res[i][j].impl())->value().dyn_cast<pir::OpResult>();
            }
        }
    }
    return res;
}

std::vector<std::vector<pir::OpResult>> SqrtSrOp::Vjp(pir::Operation* op, const std::vector<std::vector<pir::Value>>& inputs_, const std::vector<std::vector<pir::Value>>& outputs, const std::vector<std::vector<pir::Value>>& out_grads, const std::vector<std::vector<bool>>& stop_gradients){

    PADDLE_ENFORCE_EQ(
      inputs_.size(),
      1,
      platform::errors::InvalidArgument("sqrt op's inputs size should be 1, but now is %d.", inputs_.size()));
    PADDLE_ENFORCE_EQ(
      outputs.size(),
      1,
      platform::errors::InvalidArgument("sqrt op's outputs size should be 1, but now is %d.", outputs.size()));

    VLOG(6) << "Prepare inputs of sqrt_grad";

    Tensor out(std::make_shared<primitive::LazyTensor>(outputs[0][0]));
    Tensor out_grad(std::make_shared<primitive::LazyTensor>(out_grads[0][0]));

    VLOG(6) << "Vjp prepare Prepare attributes of sqrt_grad";


    VLOG(6) << "Vjp prepare call sqrt's vjp inteface";

    std::vector<std::vector<Tensor>> tensor_res =
        primitive::sqrt_vjp(
        out, out_grad, stop_gradients);

    VLOG(6) << "Vjp prepare stop gradient of sqrt_grad";

    std::vector<std::vector<pir::OpResult>> res(tensor_res.size());
    for (size_t i = 0; i < tensor_res.size(); ++i) {
        res[i].resize(tensor_res[i].size());
        for (size_t j = 0; j < tensor_res[i].size(); ++j) {
            if(tensor_res[i][j].defined()){
                res[i][j] = std::static_pointer_cast<primitive::LazyTensor>(tensor_res[i][j].impl())->value().dyn_cast<pir::OpResult>();
            }
        }
    }
    return res;
}

std::vector<std::vector<pir::OpResult>> Sqrt_Op::Vjp(pir::Operation* op, const std::vector<std::vector<pir::Value>>& inputs_, const std::vector<std::vector<pir::Value>>& outputs, const std::vector<std::vector<pir::Value>>& out_grads, const std::vector<std::vector<bool>>& stop_gradients){

    PADDLE_ENFORCE_EQ(
      inputs_.size(),
      1,
      platform::errors::InvalidArgument("sqrt op's inputs size should be 1, but now is %d.", inputs_.size()));
    PADDLE_ENFORCE_EQ(
      outputs.size(),
      1,
      platform::errors::InvalidArgument("sqrt op's outputs size should be 1, but now is %d.", outputs.size()));

    VLOG(6) << "Prepare inputs of sqrt_grad";

    Tensor out(std::make_shared<primitive::LazyTensor>(outputs[0][0]));
    Tensor out_grad(std::make_shared<primitive::LazyTensor>(out_grads[0][0]));

    VLOG(6) << "Vjp prepare Prepare attributes of sqrt_grad";


    VLOG(6) << "Vjp prepare call sqrt_'s vjp inteface";

    std::vector<std::vector<Tensor>> tensor_res =
        primitive::sqrt_vjp(
        out, out_grad, stop_gradients);

    VLOG(6) << "Vjp prepare stop gradient of sqrt_grad";

    std::vector<std::vector<pir::OpResult>> res(tensor_res.size());
    for (size_t i = 0; i < tensor_res.size(); ++i) {
        res[i].resize(tensor_res[i].size());
        for (size_t j = 0; j < tensor_res[i].size(); ++j) {
            if(tensor_res[i][j].defined()){
                res[i][j] = std::static_pointer_cast<primitive::LazyTensor>(tensor_res[i][j].impl())->value().dyn_cast<pir::OpResult>();
            }
        }
    }
    return res;
}

std::vector<std::vector<pir::OpResult>> SqrtSr_Op::Vjp(pir::Operation* op, const std::vector<std::vector<pir::Value>>& inputs_, const std::vector<std::vector<pir::Value>>& outputs, const std::vector<std::vector<pir::Value>>& out_grads, const std::vector<std::vector<bool>>& stop_gradients){

    PADDLE_ENFORCE_EQ(
      inputs_.size(),
      1,
      platform::errors::InvalidArgument("sqrt op's inputs size should be 1, but now is %d.", inputs_.size()));
    PADDLE_ENFORCE_EQ(
      outputs.size(),
      1,
      platform::errors::InvalidArgument("sqrt op's outputs size should be 1, but now is %d.", outputs.size()));

    VLOG(6) << "Prepare inputs of sqrt_grad";

    Tensor out(std::make_shared<primitive::LazyTensor>(outputs[0][0]));
    Tensor out_grad(std::make_shared<primitive::LazyTensor>(out_grads[0][0]));

    VLOG(6) << "Vjp prepare Prepare attributes of sqrt_grad";


    VLOG(6) << "Vjp prepare call sqrt_'s vjp inteface";

    std::vector<std::vector<Tensor>> tensor_res =
        primitive::sqrt_vjp(
        out, out_grad, stop_gradients);

    VLOG(6) << "Vjp prepare stop gradient of sqrt_grad";

    std::vector<std::vector<pir::OpResult>> res(tensor_res.size());
    for (size_t i = 0; i < tensor_res.size(); ++i) {
        res[i].resize(tensor_res[i].size());
        for (size_t j = 0; j < tensor_res[i].size(); ++j) {
            if(tensor_res[i][j].defined()){
                res[i][j] = std::static_pointer_cast<primitive::LazyTensor>(tensor_res[i][j].impl())->value().dyn_cast<pir::OpResult>();
            }
        }
    }
    return res;
}

std::vector<std::vector<pir::OpResult>> SquareOp::Vjp(pir::Operation* op, const std::vector<std::vector<pir::Value>>& inputs_, const std::vector<std::vector<pir::Value>>& outputs, const std::vector<std::vector<pir::Value>>& out_grads, const std::vector<std::vector<bool>>& stop_gradients){

    PADDLE_ENFORCE_EQ(
      inputs_.size(),
      1,
      platform::errors::InvalidArgument("square op's inputs size should be 1, but now is %d.", inputs_.size()));
    PADDLE_ENFORCE_EQ(
      outputs.size(),
      1,
      platform::errors::InvalidArgument("square op's outputs size should be 1, but now is %d.", outputs.size()));

    VLOG(6) << "Prepare inputs of square_grad";

    Tensor x(std::make_shared<primitive::LazyTensor>(inputs_[0][0]));
    Tensor out_grad(std::make_shared<primitive::LazyTensor>(out_grads[0][0]));

    VLOG(6) << "Vjp prepare Prepare attributes of square_grad";


    VLOG(6) << "Vjp prepare call square's vjp inteface";

    std::vector<std::vector<Tensor>> tensor_res =
        primitive::square_vjp(
        x, out_grad, stop_gradients);

    VLOG(6) << "Vjp prepare stop gradient of square_grad";

    std::vector<std::vector<pir::OpResult>> res(tensor_res.size());
    for (size_t i = 0; i < tensor_res.size(); ++i) {
        res[i].resize(tensor_res[i].size());
        for (size_t j = 0; j < tensor_res[i].size(); ++j) {
            if(tensor_res[i][j].defined()){
                res[i][j] = std::static_pointer_cast<primitive::LazyTensor>(tensor_res[i][j].impl())->value().dyn_cast<pir::OpResult>();
            }
        }
    }
    return res;
}

std::vector<std::vector<pir::OpResult>> SquareSrOp::Vjp(pir::Operation* op, const std::vector<std::vector<pir::Value>>& inputs_, const std::vector<std::vector<pir::Value>>& outputs, const std::vector<std::vector<pir::Value>>& out_grads, const std::vector<std::vector<bool>>& stop_gradients){

    PADDLE_ENFORCE_EQ(
      inputs_.size(),
      1,
      platform::errors::InvalidArgument("square op's inputs size should be 1, but now is %d.", inputs_.size()));
    PADDLE_ENFORCE_EQ(
      outputs.size(),
      1,
      platform::errors::InvalidArgument("square op's outputs size should be 1, but now is %d.", outputs.size()));

    VLOG(6) << "Prepare inputs of square_grad";

    Tensor x(std::make_shared<primitive::LazyTensor>(inputs_[0][0]));
    Tensor out_grad(std::make_shared<primitive::LazyTensor>(out_grads[0][0]));

    VLOG(6) << "Vjp prepare Prepare attributes of square_grad";


    VLOG(6) << "Vjp prepare call square's vjp inteface";

    std::vector<std::vector<Tensor>> tensor_res =
        primitive::square_vjp(
        x, out_grad, stop_gradients);

    VLOG(6) << "Vjp prepare stop gradient of square_grad";

    std::vector<std::vector<pir::OpResult>> res(tensor_res.size());
    for (size_t i = 0; i < tensor_res.size(); ++i) {
        res[i].resize(tensor_res[i].size());
        for (size_t j = 0; j < tensor_res[i].size(); ++j) {
            if(tensor_res[i][j].defined()){
                res[i][j] = std::static_pointer_cast<primitive::LazyTensor>(tensor_res[i][j].impl())->value().dyn_cast<pir::OpResult>();
            }
        }
    }
    return res;
}

std::vector<std::vector<pir::OpResult>> SquaredL2NormOp::Vjp(pir::Operation* op, const std::vector<std::vector<pir::Value>>& inputs_, const std::vector<std::vector<pir::Value>>& outputs, const std::vector<std::vector<pir::Value>>& out_grads, const std::vector<std::vector<bool>>& stop_gradients){

    PADDLE_ENFORCE_EQ(
      inputs_.size(),
      1,
      platform::errors::InvalidArgument("squared_l2_norm op's inputs size should be 1, but now is %d.", inputs_.size()));
    PADDLE_ENFORCE_EQ(
      outputs.size(),
      1,
      platform::errors::InvalidArgument("squared_l2_norm op's outputs size should be 1, but now is %d.", outputs.size()));

    VLOG(6) << "Prepare inputs of squared_l2_norm_grad";

    Tensor x(std::make_shared<primitive::LazyTensor>(inputs_[0][0]));
    Tensor out_grad(std::make_shared<primitive::LazyTensor>(out_grads[0][0]));

    VLOG(6) << "Vjp prepare Prepare attributes of squared_l2_norm_grad";


    VLOG(6) << "Vjp prepare call squared_l2_norm's vjp inteface";

    std::vector<std::vector<Tensor>> tensor_res =
        primitive::squared_l2_norm_vjp(
        x, out_grad, stop_gradients);

    VLOG(6) << "Vjp prepare stop gradient of squared_l2_norm_grad";

    std::vector<std::vector<pir::OpResult>> res(tensor_res.size());
    for (size_t i = 0; i < tensor_res.size(); ++i) {
        res[i].resize(tensor_res[i].size());
        for (size_t j = 0; j < tensor_res[i].size(); ++j) {
            if(tensor_res[i][j].defined()){
                res[i][j] = std::static_pointer_cast<primitive::LazyTensor>(tensor_res[i][j].impl())->value().dyn_cast<pir::OpResult>();
            }
        }
    }
    return res;
}

std::vector<std::vector<pir::OpResult>> SqueezeOp::Vjp(pir::Operation* op, const std::vector<std::vector<pir::Value>>& inputs_, const std::vector<std::vector<pir::Value>>& outputs, const std::vector<std::vector<pir::Value>>& out_grads, const std::vector<std::vector<bool>>& stop_gradients){

    PADDLE_ENFORCE_EQ(
      inputs_.size(),
      2,
      platform::errors::InvalidArgument("squeeze op's inputs size should be 2, but now is %d.", inputs_.size()));
    PADDLE_ENFORCE_EQ(
      outputs.size(),
      2,
      platform::errors::InvalidArgument("squeeze op's outputs size should be 2, but now is %d.", outputs.size()));

    VLOG(6) << "Prepare inputs of squeeze_grad";

    Tensor xshape(std::make_shared<primitive::LazyTensor>(outputs[1][0]));
    Tensor out_grad(std::make_shared<primitive::LazyTensor>(out_grads[0][0]));

    VLOG(6) << "Vjp prepare Prepare attributes of squeeze_grad";

    Tensor axis(std::make_shared<primitive::LazyTensor>(inputs_[1][0]));

    VLOG(6) << "Vjp prepare call squeeze's vjp inteface";

    std::vector<std::vector<Tensor>> tensor_res =
        primitive::squeeze_vjp(
        xshape, out_grad, axis, stop_gradients);

    VLOG(6) << "Vjp prepare stop gradient of squeeze_grad";

    std::vector<std::vector<pir::OpResult>> res(tensor_res.size());
    for (size_t i = 0; i < tensor_res.size(); ++i) {
        res[i].resize(tensor_res[i].size());
        for (size_t j = 0; j < tensor_res[i].size(); ++j) {
            if(tensor_res[i][j].defined()){
                res[i][j] = std::static_pointer_cast<primitive::LazyTensor>(tensor_res[i][j].impl())->value().dyn_cast<pir::OpResult>();
            }
        }
    }
    return res;
}

std::vector<std::vector<pir::OpResult>> Squeeze_Op::Vjp(pir::Operation* op, const std::vector<std::vector<pir::Value>>& inputs_, const std::vector<std::vector<pir::Value>>& outputs, const std::vector<std::vector<pir::Value>>& out_grads, const std::vector<std::vector<bool>>& stop_gradients){

    PADDLE_ENFORCE_EQ(
      inputs_.size(),
      2,
      platform::errors::InvalidArgument("squeeze op's inputs size should be 2, but now is %d.", inputs_.size()));
    PADDLE_ENFORCE_EQ(
      outputs.size(),
      2,
      platform::errors::InvalidArgument("squeeze op's outputs size should be 2, but now is %d.", outputs.size()));

    VLOG(6) << "Prepare inputs of squeeze_grad";

    Tensor xshape(std::make_shared<primitive::LazyTensor>(outputs[1][0]));
    Tensor out_grad(std::make_shared<primitive::LazyTensor>(out_grads[0][0]));

    VLOG(6) << "Vjp prepare Prepare attributes of squeeze_grad";

    Tensor axis(std::make_shared<primitive::LazyTensor>(inputs_[1][0]));

    VLOG(6) << "Vjp prepare call squeeze_'s vjp inteface";

    std::vector<std::vector<Tensor>> tensor_res =
        primitive::squeeze_vjp(
        xshape, out_grad, axis, stop_gradients);

    VLOG(6) << "Vjp prepare stop gradient of squeeze_grad";

    std::vector<std::vector<pir::OpResult>> res(tensor_res.size());
    for (size_t i = 0; i < tensor_res.size(); ++i) {
        res[i].resize(tensor_res[i].size());
        for (size_t j = 0; j < tensor_res[i].size(); ++j) {
            if(tensor_res[i][j].defined()){
                res[i][j] = std::static_pointer_cast<primitive::LazyTensor>(tensor_res[i][j].impl())->value().dyn_cast<pir::OpResult>();
            }
        }
    }
    return res;
}

std::vector<std::vector<pir::OpResult>> StackOp::Vjp(pir::Operation* op, const std::vector<std::vector<pir::Value>>& inputs_, const std::vector<std::vector<pir::Value>>& outputs, const std::vector<std::vector<pir::Value>>& out_grads, const std::vector<std::vector<bool>>& stop_gradients){

    PADDLE_ENFORCE_EQ(
      inputs_.size(),
      1,
      platform::errors::InvalidArgument("stack op's inputs size should be 1, but now is %d.", inputs_.size()));
    PADDLE_ENFORCE_EQ(
      outputs.size(),
      1,
      platform::errors::InvalidArgument("stack op's outputs size should be 1, but now is %d.", outputs.size()));

    VLOG(6) << "Prepare inputs of stack_grad";

    std::vector<Tensor> x;
    for (size_t idx = 0; idx < inputs_[0].size(); idx++) {
        x.emplace_back(
            std::make_shared<primitive::LazyTensor>(inputs_[0][idx]));
    }
    Tensor out_grad(std::make_shared<primitive::LazyTensor>(out_grads[0][0]));

    VLOG(6) << "Vjp prepare Prepare attributes of stack_grad";

    int axis = op->attribute("axis").dyn_cast<pir::Int32Attribute>().data();

    VLOG(6) << "Vjp prepare call stack's vjp inteface";

    std::vector<std::vector<Tensor>> tensor_res =
        primitive::stack_vjp(
        x, out_grad, axis, stop_gradients);

    VLOG(6) << "Vjp prepare stop gradient of stack_grad";

    std::vector<std::vector<pir::OpResult>> res(tensor_res.size());
    for (size_t i = 0; i < tensor_res.size(); ++i) {
        res[i].resize(tensor_res[i].size());
        for (size_t j = 0; j < tensor_res[i].size(); ++j) {
            if(tensor_res[i][j].defined()){
                res[i][j] = std::static_pointer_cast<primitive::LazyTensor>(tensor_res[i][j].impl())->value().dyn_cast<pir::OpResult>();
            }
        }
    }
    return res;
}

std::vector<std::vector<pir::OpResult>> StanhOp::Vjp(pir::Operation* op, const std::vector<std::vector<pir::Value>>& inputs_, const std::vector<std::vector<pir::Value>>& outputs, const std::vector<std::vector<pir::Value>>& out_grads, const std::vector<std::vector<bool>>& stop_gradients){

    PADDLE_ENFORCE_EQ(
      inputs_.size(),
      1,
      platform::errors::InvalidArgument("stanh op's inputs size should be 1, but now is %d.", inputs_.size()));
    PADDLE_ENFORCE_EQ(
      outputs.size(),
      1,
      platform::errors::InvalidArgument("stanh op's outputs size should be 1, but now is %d.", outputs.size()));

    VLOG(6) << "Prepare inputs of stanh_grad";

    Tensor x(std::make_shared<primitive::LazyTensor>(inputs_[0][0]));
    Tensor out_grad(std::make_shared<primitive::LazyTensor>(out_grads[0][0]));

    VLOG(6) << "Vjp prepare Prepare attributes of stanh_grad";

    float scale_a = op->attribute("scale_a").dyn_cast<pir::FloatAttribute>().data();
    float scale_b = op->attribute("scale_b").dyn_cast<pir::FloatAttribute>().data();

    VLOG(6) << "Vjp prepare call stanh's vjp inteface";

    std::vector<std::vector<Tensor>> tensor_res =
        primitive::stanh_vjp(
        x, out_grad, scale_a, scale_b, stop_gradients);

    VLOG(6) << "Vjp prepare stop gradient of stanh_grad";

    std::vector<std::vector<pir::OpResult>> res(tensor_res.size());
    for (size_t i = 0; i < tensor_res.size(); ++i) {
        res[i].resize(tensor_res[i].size());
        for (size_t j = 0; j < tensor_res[i].size(); ++j) {
            if(tensor_res[i][j].defined()){
                res[i][j] = std::static_pointer_cast<primitive::LazyTensor>(tensor_res[i][j].impl())->value().dyn_cast<pir::OpResult>();
            }
        }
    }
    return res;
}

std::vector<std::vector<pir::OpResult>> SvdOp::Vjp(pir::Operation* op, const std::vector<std::vector<pir::Value>>& inputs_, const std::vector<std::vector<pir::Value>>& outputs, const std::vector<std::vector<pir::Value>>& out_grads, const std::vector<std::vector<bool>>& stop_gradients){

    PADDLE_ENFORCE_EQ(
      inputs_.size(),
      1,
      platform::errors::InvalidArgument("svd op's inputs size should be 1, but now is %d.", inputs_.size()));
    PADDLE_ENFORCE_EQ(
      outputs.size(),
      3,
      platform::errors::InvalidArgument("svd op's outputs size should be 3, but now is %d.", outputs.size()));

    VLOG(6) << "Prepare inputs of svd_grad";

    Tensor x(std::make_shared<primitive::LazyTensor>(inputs_[0][0]));
    Tensor u(std::make_shared<primitive::LazyTensor>(outputs[0][0]));
    Tensor vh(std::make_shared<primitive::LazyTensor>(outputs[2][0]));
    Tensor s(std::make_shared<primitive::LazyTensor>(outputs[1][0]));
    paddle::optional<Tensor> u_grad;
    if (!IsEmptyValue(out_grads[0][0])){
        u_grad = paddle::make_optional<Tensor>(Tensor(std::make_shared<primitive::LazyTensor>(out_grads[0][0])));
    }
    paddle::optional<Tensor> vh_grad;
    if (!IsEmptyValue(out_grads[2][0])){
        vh_grad = paddle::make_optional<Tensor>(Tensor(std::make_shared<primitive::LazyTensor>(out_grads[2][0])));
    }
    paddle::optional<Tensor> s_grad;
    if (!IsEmptyValue(out_grads[1][0])){
        s_grad = paddle::make_optional<Tensor>(Tensor(std::make_shared<primitive::LazyTensor>(out_grads[1][0])));
    }

    VLOG(6) << "Vjp prepare Prepare attributes of svd_grad";

    bool full_matrices = op->attribute("full_matrices").dyn_cast<pir::BoolAttribute>().data();

    VLOG(6) << "Vjp prepare call svd's vjp inteface";

    std::vector<std::vector<Tensor>> tensor_res =
        primitive::svd_vjp(
        x, u, vh, s, u_grad, vh_grad, s_grad, full_matrices, stop_gradients);

    VLOG(6) << "Vjp prepare stop gradient of svd_grad";

    std::vector<std::vector<pir::OpResult>> res(tensor_res.size());
    for (size_t i = 0; i < tensor_res.size(); ++i) {
        res[i].resize(tensor_res[i].size());
        for (size_t j = 0; j < tensor_res[i].size(); ++j) {
            if(tensor_res[i][j].defined()){
                res[i][j] = std::static_pointer_cast<primitive::LazyTensor>(tensor_res[i][j].impl())->value().dyn_cast<pir::OpResult>();
            }
        }
    }
    return res;
}

std::vector<std::vector<pir::OpResult>> TakeAlongAxisOp::Vjp(pir::Operation* op, const std::vector<std::vector<pir::Value>>& inputs_, const std::vector<std::vector<pir::Value>>& outputs, const std::vector<std::vector<pir::Value>>& out_grads, const std::vector<std::vector<bool>>& stop_gradients){

    PADDLE_ENFORCE_EQ(
      inputs_.size(),
      2,
      platform::errors::InvalidArgument("take_along_axis op's inputs size should be 2, but now is %d.", inputs_.size()));
    PADDLE_ENFORCE_EQ(
      outputs.size(),
      1,
      platform::errors::InvalidArgument("take_along_axis op's outputs size should be 1, but now is %d.", outputs.size()));

    VLOG(6) << "Prepare inputs of take_along_axis_grad";

    Tensor arr(std::make_shared<primitive::LazyTensor>(inputs_[0][0]));
    Tensor indices(std::make_shared<primitive::LazyTensor>(inputs_[1][0]));
    Tensor out_grad(std::make_shared<primitive::LazyTensor>(out_grads[0][0]));

    VLOG(6) << "Vjp prepare Prepare attributes of take_along_axis_grad";

    int axis = op->attribute("axis").dyn_cast<pir::Int32Attribute>().data();

    VLOG(6) << "Vjp prepare call take_along_axis's vjp inteface";

    std::vector<std::vector<Tensor>> tensor_res =
        primitive::take_along_axis_vjp(
        arr, indices, out_grad, axis, stop_gradients);

    VLOG(6) << "Vjp prepare stop gradient of take_along_axis_grad";

    std::vector<std::vector<pir::OpResult>> res(tensor_res.size());
    for (size_t i = 0; i < tensor_res.size(); ++i) {
        res[i].resize(tensor_res[i].size());
        for (size_t j = 0; j < tensor_res[i].size(); ++j) {
            if(tensor_res[i][j].defined()){
                res[i][j] = std::static_pointer_cast<primitive::LazyTensor>(tensor_res[i][j].impl())->value().dyn_cast<pir::OpResult>();
            }
        }
    }
    return res;
}

std::vector<std::vector<pir::OpResult>> TanOp::Vjp(pir::Operation* op, const std::vector<std::vector<pir::Value>>& inputs_, const std::vector<std::vector<pir::Value>>& outputs, const std::vector<std::vector<pir::Value>>& out_grads, const std::vector<std::vector<bool>>& stop_gradients){

    PADDLE_ENFORCE_EQ(
      inputs_.size(),
      1,
      platform::errors::InvalidArgument("tan op's inputs size should be 1, but now is %d.", inputs_.size()));
    PADDLE_ENFORCE_EQ(
      outputs.size(),
      1,
      platform::errors::InvalidArgument("tan op's outputs size should be 1, but now is %d.", outputs.size()));

    VLOG(6) << "Prepare inputs of tan_grad";

    Tensor x(std::make_shared<primitive::LazyTensor>(inputs_[0][0]));
    Tensor out_grad(std::make_shared<primitive::LazyTensor>(out_grads[0][0]));

    VLOG(6) << "Vjp prepare Prepare attributes of tan_grad";


    VLOG(6) << "Vjp prepare call tan's vjp inteface";

    std::vector<std::vector<Tensor>> tensor_res =
        primitive::tan_vjp(
        x, out_grad, stop_gradients);

    VLOG(6) << "Vjp prepare stop gradient of tan_grad";

    std::vector<std::vector<pir::OpResult>> res(tensor_res.size());
    for (size_t i = 0; i < tensor_res.size(); ++i) {
        res[i].resize(tensor_res[i].size());
        for (size_t j = 0; j < tensor_res[i].size(); ++j) {
            if(tensor_res[i][j].defined()){
                res[i][j] = std::static_pointer_cast<primitive::LazyTensor>(tensor_res[i][j].impl())->value().dyn_cast<pir::OpResult>();
            }
        }
    }
    return res;
}

std::vector<std::vector<pir::OpResult>> Tan_Op::Vjp(pir::Operation* op, const std::vector<std::vector<pir::Value>>& inputs_, const std::vector<std::vector<pir::Value>>& outputs, const std::vector<std::vector<pir::Value>>& out_grads, const std::vector<std::vector<bool>>& stop_gradients){

    PADDLE_ENFORCE_EQ(
      inputs_.size(),
      1,
      platform::errors::InvalidArgument("tan op's inputs size should be 1, but now is %d.", inputs_.size()));
    PADDLE_ENFORCE_EQ(
      outputs.size(),
      1,
      platform::errors::InvalidArgument("tan op's outputs size should be 1, but now is %d.", outputs.size()));

    VLOG(6) << "Prepare inputs of tan_grad";

    Tensor x(std::make_shared<primitive::LazyTensor>(inputs_[0][0]));
    Tensor out_grad(std::make_shared<primitive::LazyTensor>(out_grads[0][0]));

    VLOG(6) << "Vjp prepare Prepare attributes of tan_grad";


    VLOG(6) << "Vjp prepare call tan_'s vjp inteface";

    std::vector<std::vector<Tensor>> tensor_res =
        primitive::tan_vjp(
        x, out_grad, stop_gradients);

    VLOG(6) << "Vjp prepare stop gradient of tan_grad";

    std::vector<std::vector<pir::OpResult>> res(tensor_res.size());
    for (size_t i = 0; i < tensor_res.size(); ++i) {
        res[i].resize(tensor_res[i].size());
        for (size_t j = 0; j < tensor_res[i].size(); ++j) {
            if(tensor_res[i][j].defined()){
                res[i][j] = std::static_pointer_cast<primitive::LazyTensor>(tensor_res[i][j].impl())->value().dyn_cast<pir::OpResult>();
            }
        }
    }
    return res;
}

std::vector<std::vector<pir::OpResult>> TanhOp::Vjp(pir::Operation* op, const std::vector<std::vector<pir::Value>>& inputs_, const std::vector<std::vector<pir::Value>>& outputs, const std::vector<std::vector<pir::Value>>& out_grads, const std::vector<std::vector<bool>>& stop_gradients){

    PADDLE_ENFORCE_EQ(
      inputs_.size(),
      1,
      platform::errors::InvalidArgument("tanh op's inputs size should be 1, but now is %d.", inputs_.size()));
    PADDLE_ENFORCE_EQ(
      outputs.size(),
      1,
      platform::errors::InvalidArgument("tanh op's outputs size should be 1, but now is %d.", outputs.size()));

    VLOG(6) << "Prepare inputs of tanh_grad";

    Tensor out(std::make_shared<primitive::LazyTensor>(outputs[0][0]));
    Tensor out_grad(std::make_shared<primitive::LazyTensor>(out_grads[0][0]));

    VLOG(6) << "Vjp prepare Prepare attributes of tanh_grad";


    VLOG(6) << "Vjp prepare call tanh's vjp inteface";

    std::vector<std::vector<Tensor>> tensor_res =
        primitive::tanh_vjp(
        out, out_grad, stop_gradients);

    VLOG(6) << "Vjp prepare stop gradient of tanh_grad";

    std::vector<std::vector<pir::OpResult>> res(tensor_res.size());
    for (size_t i = 0; i < tensor_res.size(); ++i) {
        res[i].resize(tensor_res[i].size());
        for (size_t j = 0; j < tensor_res[i].size(); ++j) {
            if(tensor_res[i][j].defined()){
                res[i][j] = std::static_pointer_cast<primitive::LazyTensor>(tensor_res[i][j].impl())->value().dyn_cast<pir::OpResult>();
            }
        }
    }
    return res;
}

std::vector<std::vector<pir::OpResult>> Tanh_Op::Vjp(pir::Operation* op, const std::vector<std::vector<pir::Value>>& inputs_, const std::vector<std::vector<pir::Value>>& outputs, const std::vector<std::vector<pir::Value>>& out_grads, const std::vector<std::vector<bool>>& stop_gradients){

    PADDLE_ENFORCE_EQ(
      inputs_.size(),
      1,
      platform::errors::InvalidArgument("tanh op's inputs size should be 1, but now is %d.", inputs_.size()));
    PADDLE_ENFORCE_EQ(
      outputs.size(),
      1,
      platform::errors::InvalidArgument("tanh op's outputs size should be 1, but now is %d.", outputs.size()));

    VLOG(6) << "Prepare inputs of tanh_grad";

    Tensor out(std::make_shared<primitive::LazyTensor>(outputs[0][0]));
    Tensor out_grad(std::make_shared<primitive::LazyTensor>(out_grads[0][0]));

    VLOG(6) << "Vjp prepare Prepare attributes of tanh_grad";


    VLOG(6) << "Vjp prepare call tanh_'s vjp inteface";

    std::vector<std::vector<Tensor>> tensor_res =
        primitive::tanh_vjp(
        out, out_grad, stop_gradients);

    VLOG(6) << "Vjp prepare stop gradient of tanh_grad";

    std::vector<std::vector<pir::OpResult>> res(tensor_res.size());
    for (size_t i = 0; i < tensor_res.size(); ++i) {
        res[i].resize(tensor_res[i].size());
        for (size_t j = 0; j < tensor_res[i].size(); ++j) {
            if(tensor_res[i][j].defined()){
                res[i][j] = std::static_pointer_cast<primitive::LazyTensor>(tensor_res[i][j].impl())->value().dyn_cast<pir::OpResult>();
            }
        }
    }
    return res;
}

std::vector<std::vector<pir::OpResult>> TanhShrinkOp::Vjp(pir::Operation* op, const std::vector<std::vector<pir::Value>>& inputs_, const std::vector<std::vector<pir::Value>>& outputs, const std::vector<std::vector<pir::Value>>& out_grads, const std::vector<std::vector<bool>>& stop_gradients){

    PADDLE_ENFORCE_EQ(
      inputs_.size(),
      1,
      platform::errors::InvalidArgument("tanh_shrink op's inputs size should be 1, but now is %d.", inputs_.size()));
    PADDLE_ENFORCE_EQ(
      outputs.size(),
      1,
      platform::errors::InvalidArgument("tanh_shrink op's outputs size should be 1, but now is %d.", outputs.size()));

    VLOG(6) << "Prepare inputs of tanh_shrink_grad";

    Tensor x(std::make_shared<primitive::LazyTensor>(inputs_[0][0]));
    Tensor out_grad(std::make_shared<primitive::LazyTensor>(out_grads[0][0]));

    VLOG(6) << "Vjp prepare Prepare attributes of tanh_shrink_grad";


    VLOG(6) << "Vjp prepare call tanh_shrink's vjp inteface";

    std::vector<std::vector<Tensor>> tensor_res =
        primitive::tanh_shrink_vjp(
        x, out_grad, stop_gradients);

    VLOG(6) << "Vjp prepare stop gradient of tanh_shrink_grad";

    std::vector<std::vector<pir::OpResult>> res(tensor_res.size());
    for (size_t i = 0; i < tensor_res.size(); ++i) {
        res[i].resize(tensor_res[i].size());
        for (size_t j = 0; j < tensor_res[i].size(); ++j) {
            if(tensor_res[i][j].defined()){
                res[i][j] = std::static_pointer_cast<primitive::LazyTensor>(tensor_res[i][j].impl())->value().dyn_cast<pir::OpResult>();
            }
        }
    }
    return res;
}

std::vector<std::vector<pir::OpResult>> TemporalShiftOp::Vjp(pir::Operation* op, const std::vector<std::vector<pir::Value>>& inputs_, const std::vector<std::vector<pir::Value>>& outputs, const std::vector<std::vector<pir::Value>>& out_grads, const std::vector<std::vector<bool>>& stop_gradients){

    PADDLE_ENFORCE_EQ(
      inputs_.size(),
      1,
      platform::errors::InvalidArgument("temporal_shift op's inputs size should be 1, but now is %d.", inputs_.size()));
    PADDLE_ENFORCE_EQ(
      outputs.size(),
      1,
      platform::errors::InvalidArgument("temporal_shift op's outputs size should be 1, but now is %d.", outputs.size()));

    VLOG(6) << "Prepare inputs of temporal_shift_grad";

    Tensor out_grad(std::make_shared<primitive::LazyTensor>(out_grads[0][0]));

    VLOG(6) << "Vjp prepare Prepare attributes of temporal_shift_grad";

    int seg_num = op->attribute("seg_num").dyn_cast<pir::Int32Attribute>().data();
    float shift_ratio = op->attribute("shift_ratio").dyn_cast<pir::FloatAttribute>().data();
    std::string data_format = op->attribute("data_format").dyn_cast<pir::StrAttribute>().AsString();

    VLOG(6) << "Vjp prepare call temporal_shift's vjp inteface";

    std::vector<std::vector<Tensor>> tensor_res =
        primitive::temporal_shift_vjp(
        out_grad, seg_num, shift_ratio, data_format, stop_gradients);

    VLOG(6) << "Vjp prepare stop gradient of temporal_shift_grad";

    std::vector<std::vector<pir::OpResult>> res(tensor_res.size());
    for (size_t i = 0; i < tensor_res.size(); ++i) {
        res[i].resize(tensor_res[i].size());
        for (size_t j = 0; j < tensor_res[i].size(); ++j) {
            if(tensor_res[i][j].defined()){
                res[i][j] = std::static_pointer_cast<primitive::LazyTensor>(tensor_res[i][j].impl())->value().dyn_cast<pir::OpResult>();
            }
        }
    }
    return res;
}

std::vector<std::vector<pir::OpResult>> TensorUnfoldOp::Vjp(pir::Operation* op, const std::vector<std::vector<pir::Value>>& inputs_, const std::vector<std::vector<pir::Value>>& outputs, const std::vector<std::vector<pir::Value>>& out_grads, const std::vector<std::vector<bool>>& stop_gradients){

    PADDLE_ENFORCE_EQ(
      inputs_.size(),
      1,
      platform::errors::InvalidArgument("tensor_unfold op's inputs size should be 1, but now is %d.", inputs_.size()));
    PADDLE_ENFORCE_EQ(
      outputs.size(),
      1,
      platform::errors::InvalidArgument("tensor_unfold op's outputs size should be 1, but now is %d.", outputs.size()));

    VLOG(6) << "Prepare inputs of tensor_unfold_grad";

    Tensor input(std::make_shared<primitive::LazyTensor>(inputs_[0][0]));
    Tensor out_grad(std::make_shared<primitive::LazyTensor>(out_grads[0][0]));

    VLOG(6) << "Vjp prepare Prepare attributes of tensor_unfold_grad";

    int64_t axis = op->attribute("axis").dyn_cast<pir::Int64Attribute>().data();
    int64_t size = op->attribute("size").dyn_cast<pir::Int64Attribute>().data();
    int64_t step = op->attribute("step").dyn_cast<pir::Int64Attribute>().data();

    VLOG(6) << "Vjp prepare call tensor_unfold's vjp inteface";

    std::vector<std::vector<Tensor>> tensor_res =
        primitive::tensor_unfold_vjp(
        input, out_grad, axis, size, step, stop_gradients);

    VLOG(6) << "Vjp prepare stop gradient of tensor_unfold_grad";

    std::vector<std::vector<pir::OpResult>> res(tensor_res.size());
    for (size_t i = 0; i < tensor_res.size(); ++i) {
        res[i].resize(tensor_res[i].size());
        for (size_t j = 0; j < tensor_res[i].size(); ++j) {
            if(tensor_res[i][j].defined()){
                res[i][j] = std::static_pointer_cast<primitive::LazyTensor>(tensor_res[i][j].impl())->value().dyn_cast<pir::OpResult>();
            }
        }
    }
    return res;
}

std::vector<std::vector<pir::OpResult>> ThresholdedReluOp::Vjp(pir::Operation* op, const std::vector<std::vector<pir::Value>>& inputs_, const std::vector<std::vector<pir::Value>>& outputs, const std::vector<std::vector<pir::Value>>& out_grads, const std::vector<std::vector<bool>>& stop_gradients){

    PADDLE_ENFORCE_EQ(
      inputs_.size(),
      1,
      platform::errors::InvalidArgument("thresholded_relu op's inputs size should be 1, but now is %d.", inputs_.size()));
    PADDLE_ENFORCE_EQ(
      outputs.size(),
      1,
      platform::errors::InvalidArgument("thresholded_relu op's outputs size should be 1, but now is %d.", outputs.size()));

    VLOG(6) << "Prepare inputs of thresholded_relu_grad";

    Tensor x(std::make_shared<primitive::LazyTensor>(inputs_[0][0]));
    Tensor out_grad(std::make_shared<primitive::LazyTensor>(out_grads[0][0]));

    VLOG(6) << "Vjp prepare Prepare attributes of thresholded_relu_grad";

    float threshold = op->attribute("threshold").dyn_cast<pir::FloatAttribute>().data();

    VLOG(6) << "Vjp prepare call thresholded_relu's vjp inteface";

    std::vector<std::vector<Tensor>> tensor_res =
        primitive::thresholded_relu_vjp(
        x, out_grad, threshold, stop_gradients);

    VLOG(6) << "Vjp prepare stop gradient of thresholded_relu_grad";

    std::vector<std::vector<pir::OpResult>> res(tensor_res.size());
    for (size_t i = 0; i < tensor_res.size(); ++i) {
        res[i].resize(tensor_res[i].size());
        for (size_t j = 0; j < tensor_res[i].size(); ++j) {
            if(tensor_res[i][j].defined()){
                res[i][j] = std::static_pointer_cast<primitive::LazyTensor>(tensor_res[i][j].impl())->value().dyn_cast<pir::OpResult>();
            }
        }
    }
    return res;
}

std::vector<std::vector<pir::OpResult>> ThresholdedRelu_Op::Vjp(pir::Operation* op, const std::vector<std::vector<pir::Value>>& inputs_, const std::vector<std::vector<pir::Value>>& outputs, const std::vector<std::vector<pir::Value>>& out_grads, const std::vector<std::vector<bool>>& stop_gradients){

    PADDLE_ENFORCE_EQ(
      inputs_.size(),
      1,
      platform::errors::InvalidArgument("thresholded_relu op's inputs size should be 1, but now is %d.", inputs_.size()));
    PADDLE_ENFORCE_EQ(
      outputs.size(),
      1,
      platform::errors::InvalidArgument("thresholded_relu op's outputs size should be 1, but now is %d.", outputs.size()));

    VLOG(6) << "Prepare inputs of thresholded_relu_grad";

    Tensor x(std::make_shared<primitive::LazyTensor>(inputs_[0][0]));
    Tensor out_grad(std::make_shared<primitive::LazyTensor>(out_grads[0][0]));

    VLOG(6) << "Vjp prepare Prepare attributes of thresholded_relu_grad";

    float threshold = op->attribute("threshold").dyn_cast<pir::FloatAttribute>().data();

    VLOG(6) << "Vjp prepare call thresholded_relu_'s vjp inteface";

    std::vector<std::vector<Tensor>> tensor_res =
        primitive::thresholded_relu_vjp(
        x, out_grad, threshold, stop_gradients);

    VLOG(6) << "Vjp prepare stop gradient of thresholded_relu_grad";

    std::vector<std::vector<pir::OpResult>> res(tensor_res.size());
    for (size_t i = 0; i < tensor_res.size(); ++i) {
        res[i].resize(tensor_res[i].size());
        for (size_t j = 0; j < tensor_res[i].size(); ++j) {
            if(tensor_res[i][j].defined()){
                res[i][j] = std::static_pointer_cast<primitive::LazyTensor>(tensor_res[i][j].impl())->value().dyn_cast<pir::OpResult>();
            }
        }
    }
    return res;
}

std::vector<std::vector<pir::OpResult>> TopkOp::Vjp(pir::Operation* op, const std::vector<std::vector<pir::Value>>& inputs_, const std::vector<std::vector<pir::Value>>& outputs, const std::vector<std::vector<pir::Value>>& out_grads, const std::vector<std::vector<bool>>& stop_gradients){

    PADDLE_ENFORCE_EQ(
      inputs_.size(),
      2,
      platform::errors::InvalidArgument("topk op's inputs size should be 2, but now is %d.", inputs_.size()));
    PADDLE_ENFORCE_EQ(
      outputs.size(),
      2,
      platform::errors::InvalidArgument("topk op's outputs size should be 2, but now is %d.", outputs.size()));

    VLOG(6) << "Prepare inputs of topk_grad";

    Tensor x(std::make_shared<primitive::LazyTensor>(inputs_[0][0]));
    Tensor indices(std::make_shared<primitive::LazyTensor>(outputs[1][0]));
    Tensor out_grad(std::make_shared<primitive::LazyTensor>(out_grads[0][0]));

    VLOG(6) << "Vjp prepare Prepare attributes of topk_grad";

    Tensor k(std::make_shared<primitive::LazyTensor>(inputs_[1][0]));
    int axis = op->attribute("axis").dyn_cast<pir::Int32Attribute>().data();
    bool largest = op->attribute("largest").dyn_cast<pir::BoolAttribute>().data();
    bool sorted = op->attribute("sorted").dyn_cast<pir::BoolAttribute>().data();

    VLOG(6) << "Vjp prepare call topk's vjp inteface";

    std::vector<std::vector<Tensor>> tensor_res =
        primitive::topk_vjp(
        x, indices, out_grad, k, axis, largest, sorted, stop_gradients);

    VLOG(6) << "Vjp prepare stop gradient of topk_grad";

    std::vector<std::vector<pir::OpResult>> res(tensor_res.size());
    for (size_t i = 0; i < tensor_res.size(); ++i) {
        res[i].resize(tensor_res[i].size());
        for (size_t j = 0; j < tensor_res[i].size(); ++j) {
            if(tensor_res[i][j].defined()){
                res[i][j] = std::static_pointer_cast<primitive::LazyTensor>(tensor_res[i][j].impl())->value().dyn_cast<pir::OpResult>();
            }
        }
    }
    return res;
}

std::vector<std::vector<pir::OpResult>> TraceOp::Vjp(pir::Operation* op, const std::vector<std::vector<pir::Value>>& inputs_, const std::vector<std::vector<pir::Value>>& outputs, const std::vector<std::vector<pir::Value>>& out_grads, const std::vector<std::vector<bool>>& stop_gradients){

    PADDLE_ENFORCE_EQ(
      inputs_.size(),
      1,
      platform::errors::InvalidArgument("trace op's inputs size should be 1, but now is %d.", inputs_.size()));
    PADDLE_ENFORCE_EQ(
      outputs.size(),
      1,
      platform::errors::InvalidArgument("trace op's outputs size should be 1, but now is %d.", outputs.size()));

    VLOG(6) << "Prepare inputs of trace_grad";

    Tensor x(std::make_shared<primitive::LazyTensor>(inputs_[0][0]));
    Tensor out_grad(std::make_shared<primitive::LazyTensor>(out_grads[0][0]));

    VLOG(6) << "Vjp prepare Prepare attributes of trace_grad";

    int offset = op->attribute("offset").dyn_cast<pir::Int32Attribute>().data();
    int axis1 = op->attribute("axis1").dyn_cast<pir::Int32Attribute>().data();
    int axis2 = op->attribute("axis2").dyn_cast<pir::Int32Attribute>().data();

    VLOG(6) << "Vjp prepare call trace's vjp inteface";

    std::vector<std::vector<Tensor>> tensor_res =
        primitive::trace_vjp(
        x, out_grad, offset, axis1, axis2, stop_gradients);

    VLOG(6) << "Vjp prepare stop gradient of trace_grad";

    std::vector<std::vector<pir::OpResult>> res(tensor_res.size());
    for (size_t i = 0; i < tensor_res.size(); ++i) {
        res[i].resize(tensor_res[i].size());
        for (size_t j = 0; j < tensor_res[i].size(); ++j) {
            if(tensor_res[i][j].defined()){
                res[i][j] = std::static_pointer_cast<primitive::LazyTensor>(tensor_res[i][j].impl())->value().dyn_cast<pir::OpResult>();
            }
        }
    }
    return res;
}

std::vector<std::vector<pir::OpResult>> TriangularSolveOp::Vjp(pir::Operation* op, const std::vector<std::vector<pir::Value>>& inputs_, const std::vector<std::vector<pir::Value>>& outputs, const std::vector<std::vector<pir::Value>>& out_grads, const std::vector<std::vector<bool>>& stop_gradients){

    PADDLE_ENFORCE_EQ(
      inputs_.size(),
      2,
      platform::errors::InvalidArgument("triangular_solve op's inputs size should be 2, but now is %d.", inputs_.size()));
    PADDLE_ENFORCE_EQ(
      outputs.size(),
      1,
      platform::errors::InvalidArgument("triangular_solve op's outputs size should be 1, but now is %d.", outputs.size()));

    VLOG(6) << "Prepare inputs of triangular_solve_grad";

    Tensor x(std::make_shared<primitive::LazyTensor>(inputs_[0][0]));
    Tensor y(std::make_shared<primitive::LazyTensor>(inputs_[1][0]));
    Tensor out(std::make_shared<primitive::LazyTensor>(outputs[0][0]));
    Tensor out_grad(std::make_shared<primitive::LazyTensor>(out_grads[0][0]));

    VLOG(6) << "Vjp prepare Prepare attributes of triangular_solve_grad";

    bool upper = op->attribute("upper").dyn_cast<pir::BoolAttribute>().data();
    bool transpose = op->attribute("transpose").dyn_cast<pir::BoolAttribute>().data();
    bool unitriangular = op->attribute("unitriangular").dyn_cast<pir::BoolAttribute>().data();

    VLOG(6) << "Vjp prepare call triangular_solve's vjp inteface";

    std::vector<std::vector<Tensor>> tensor_res =
        primitive::triangular_solve_vjp(
        x, y, out, out_grad, upper, transpose, unitriangular, stop_gradients);

    VLOG(6) << "Vjp prepare stop gradient of triangular_solve_grad";

    std::vector<std::vector<pir::OpResult>> res(tensor_res.size());
    for (size_t i = 0; i < tensor_res.size(); ++i) {
        res[i].resize(tensor_res[i].size());
        for (size_t j = 0; j < tensor_res[i].size(); ++j) {
            if(tensor_res[i][j].defined()){
                res[i][j] = std::static_pointer_cast<primitive::LazyTensor>(tensor_res[i][j].impl())->value().dyn_cast<pir::OpResult>();
            }
        }
    }
    return res;
}

std::vector<std::vector<pir::OpResult>> TrilinearInterpOp::Vjp(pir::Operation* op, const std::vector<std::vector<pir::Value>>& inputs_, const std::vector<std::vector<pir::Value>>& outputs, const std::vector<std::vector<pir::Value>>& out_grads, const std::vector<std::vector<bool>>& stop_gradients){

    PADDLE_ENFORCE_EQ(
      inputs_.size(),
      4,
      platform::errors::InvalidArgument("trilinear_interp op's inputs size should be 4, but now is %d.", inputs_.size()));
    PADDLE_ENFORCE_EQ(
      outputs.size(),
      1,
      platform::errors::InvalidArgument("trilinear_interp op's outputs size should be 1, but now is %d.", outputs.size()));

    VLOG(6) << "Prepare inputs of trilinear_interp_grad";

    Tensor x(std::make_shared<primitive::LazyTensor>(inputs_[0][0]));
    paddle::optional<Tensor> out_size;
    if (!IsEmptyValue(inputs_[1][0])){
        out_size = paddle::make_optional<Tensor>(Tensor(std::make_shared<primitive::LazyTensor>(inputs_[1][0])));
    }
    paddle::optional<std::vector<Tensor>> size_tensor;
    std::vector<Tensor> optional_size_tensor;
    if (!IsEmptyValue(inputs_[2][0])){
        for (size_t idx = 0; idx < inputs_[2].size(); idx++) {
            optional_size_tensor.emplace_back(
                std::make_shared<primitive::LazyTensor>(inputs_[2][idx]));
        }
        size_tensor = paddle::make_optional<std::vector<Tensor>>(optional_size_tensor);
    }
    paddle::optional<Tensor> scale_tensor;
    if (!IsEmptyValue(inputs_[3][0])){
        scale_tensor = paddle::make_optional<Tensor>(Tensor(std::make_shared<primitive::LazyTensor>(inputs_[3][0])));
    }
    Tensor output_grad(std::make_shared<primitive::LazyTensor>(out_grads[0][0]));

    VLOG(6) << "Vjp prepare Prepare attributes of trilinear_interp_grad";

    std::string data_layout = op->attribute("data_layout").dyn_cast<pir::StrAttribute>().AsString();
    int out_d = op->attribute("out_d").dyn_cast<pir::Int32Attribute>().data();
    int out_h = op->attribute("out_h").dyn_cast<pir::Int32Attribute>().data();
    int out_w = op->attribute("out_w").dyn_cast<pir::Int32Attribute>().data();
    std::vector<float> scale;
    for (size_t i = 0; i < op->attribute("scale").dyn_cast<pir::ArrayAttribute>().size(); i++) {
        scale.push_back(op->attribute("scale").dyn_cast<pir::ArrayAttribute>().at(i).dyn_cast<pir::FloatAttribute>().data());
    }
    std::string interp_method = op->attribute("interp_method").dyn_cast<pir::StrAttribute>().AsString();
    bool align_corners = op->attribute("align_corners").dyn_cast<pir::BoolAttribute>().data();
    int align_mode = op->attribute("align_mode").dyn_cast<pir::Int32Attribute>().data();

    VLOG(6) << "Vjp prepare call trilinear_interp's vjp inteface";

    std::vector<std::vector<Tensor>> tensor_res =
        primitive::trilinear_interp_vjp(
        x, out_size, size_tensor, scale_tensor, output_grad, data_layout, out_d, out_h, out_w, scale, interp_method, align_corners, align_mode, stop_gradients);

    VLOG(6) << "Vjp prepare stop gradient of trilinear_interp_grad";

    std::vector<std::vector<pir::OpResult>> res(tensor_res.size());
    for (size_t i = 0; i < tensor_res.size(); ++i) {
        res[i].resize(tensor_res[i].size());
        for (size_t j = 0; j < tensor_res[i].size(); ++j) {
            if(tensor_res[i][j].defined()){
                res[i][j] = std::static_pointer_cast<primitive::LazyTensor>(tensor_res[i][j].impl())->value().dyn_cast<pir::OpResult>();
            }
        }
    }
    return res;
}

std::vector<std::vector<pir::OpResult>> TruncOp::Vjp(pir::Operation* op, const std::vector<std::vector<pir::Value>>& inputs_, const std::vector<std::vector<pir::Value>>& outputs, const std::vector<std::vector<pir::Value>>& out_grads, const std::vector<std::vector<bool>>& stop_gradients){

    PADDLE_ENFORCE_EQ(
      inputs_.size(),
      1,
      platform::errors::InvalidArgument("trunc op's inputs size should be 1, but now is %d.", inputs_.size()));
    PADDLE_ENFORCE_EQ(
      outputs.size(),
      1,
      platform::errors::InvalidArgument("trunc op's outputs size should be 1, but now is %d.", outputs.size()));

    VLOG(6) << "Prepare inputs of trunc_grad";

    Tensor out_grad(std::make_shared<primitive::LazyTensor>(out_grads[0][0]));

    VLOG(6) << "Vjp prepare Prepare attributes of trunc_grad";


    VLOG(6) << "Vjp prepare call trunc's vjp inteface";

    std::vector<std::vector<Tensor>> tensor_res =
        primitive::trunc_vjp(
        out_grad, stop_gradients);

    VLOG(6) << "Vjp prepare stop gradient of trunc_grad";

    std::vector<std::vector<pir::OpResult>> res(tensor_res.size());
    for (size_t i = 0; i < tensor_res.size(); ++i) {
        res[i].resize(tensor_res[i].size());
        for (size_t j = 0; j < tensor_res[i].size(); ++j) {
            if(tensor_res[i][j].defined()){
                res[i][j] = std::static_pointer_cast<primitive::LazyTensor>(tensor_res[i][j].impl())->value().dyn_cast<pir::OpResult>();
            }
        }
    }
    return res;
}

std::vector<std::vector<pir::OpResult>> Trunc_Op::Vjp(pir::Operation* op, const std::vector<std::vector<pir::Value>>& inputs_, const std::vector<std::vector<pir::Value>>& outputs, const std::vector<std::vector<pir::Value>>& out_grads, const std::vector<std::vector<bool>>& stop_gradients){

    PADDLE_ENFORCE_EQ(
      inputs_.size(),
      1,
      platform::errors::InvalidArgument("trunc op's inputs size should be 1, but now is %d.", inputs_.size()));
    PADDLE_ENFORCE_EQ(
      outputs.size(),
      1,
      platform::errors::InvalidArgument("trunc op's outputs size should be 1, but now is %d.", outputs.size()));

    VLOG(6) << "Prepare inputs of trunc_grad";

    Tensor out_grad(std::make_shared<primitive::LazyTensor>(out_grads[0][0]));

    VLOG(6) << "Vjp prepare Prepare attributes of trunc_grad";


    VLOG(6) << "Vjp prepare call trunc_'s vjp inteface";

    std::vector<std::vector<Tensor>> tensor_res =
        primitive::trunc_vjp(
        out_grad, stop_gradients);

    VLOG(6) << "Vjp prepare stop gradient of trunc_grad";

    std::vector<std::vector<pir::OpResult>> res(tensor_res.size());
    for (size_t i = 0; i < tensor_res.size(); ++i) {
        res[i].resize(tensor_res[i].size());
        for (size_t j = 0; j < tensor_res[i].size(); ++j) {
            if(tensor_res[i][j].defined()){
                res[i][j] = std::static_pointer_cast<primitive::LazyTensor>(tensor_res[i][j].impl())->value().dyn_cast<pir::OpResult>();
            }
        }
    }
    return res;
}

std::vector<std::vector<pir::OpResult>> UnbindOp::Vjp(pir::Operation* op, const std::vector<std::vector<pir::Value>>& inputs_, const std::vector<std::vector<pir::Value>>& outputs, const std::vector<std::vector<pir::Value>>& out_grads, const std::vector<std::vector<bool>>& stop_gradients){

    PADDLE_ENFORCE_EQ(
      inputs_.size(),
      1,
      platform::errors::InvalidArgument("unbind op's inputs size should be 1, but now is %d.", inputs_.size()));
    PADDLE_ENFORCE_EQ(
      outputs.size(),
      1,
      platform::errors::InvalidArgument("unbind op's outputs size should be 1, but now is %d.", outputs.size()));

    VLOG(6) << "Prepare inputs of unbind_grad";

    std::vector<Tensor> out_grad;
    for (size_t idx = 0; idx < out_grads[0].size(); idx++) {
        out_grad.emplace_back(
            std::make_shared<primitive::LazyTensor>(out_grads[0][idx]));
    }

    VLOG(6) << "Vjp prepare Prepare attributes of unbind_grad";

    int axis = op->attribute("axis").dyn_cast<pir::Int32Attribute>().data();

    VLOG(6) << "Vjp prepare call unbind's vjp inteface";

    std::vector<std::vector<Tensor>> tensor_res =
        primitive::unbind_vjp(
        out_grad, axis, stop_gradients);

    VLOG(6) << "Vjp prepare stop gradient of unbind_grad";

    std::vector<std::vector<pir::OpResult>> res(tensor_res.size());
    for (size_t i = 0; i < tensor_res.size(); ++i) {
        res[i].resize(tensor_res[i].size());
        for (size_t j = 0; j < tensor_res[i].size(); ++j) {
            if(tensor_res[i][j].defined()){
                res[i][j] = std::static_pointer_cast<primitive::LazyTensor>(tensor_res[i][j].impl())->value().dyn_cast<pir::OpResult>();
            }
        }
    }
    return res;
}

std::vector<std::vector<pir::OpResult>> UnfoldOp::Vjp(pir::Operation* op, const std::vector<std::vector<pir::Value>>& inputs_, const std::vector<std::vector<pir::Value>>& outputs, const std::vector<std::vector<pir::Value>>& out_grads, const std::vector<std::vector<bool>>& stop_gradients){

    PADDLE_ENFORCE_EQ(
      inputs_.size(),
      1,
      platform::errors::InvalidArgument("unfold op's inputs size should be 1, but now is %d.", inputs_.size()));
    PADDLE_ENFORCE_EQ(
      outputs.size(),
      1,
      platform::errors::InvalidArgument("unfold op's outputs size should be 1, but now is %d.", outputs.size()));

    VLOG(6) << "Prepare inputs of unfold_grad";

    Tensor x(std::make_shared<primitive::LazyTensor>(inputs_[0][0]));
    Tensor out_grad(std::make_shared<primitive::LazyTensor>(out_grads[0][0]));

    VLOG(6) << "Vjp prepare Prepare attributes of unfold_grad";

    std::vector<int> kernel_sizes;
    for (size_t i = 0; i < op->attribute("kernel_sizes").dyn_cast<pir::ArrayAttribute>().size(); i++) {
        kernel_sizes.push_back(op->attribute("kernel_sizes").dyn_cast<pir::ArrayAttribute>().at(i).dyn_cast<pir::Int32Attribute>().data());
    }
    std::vector<int> strides;
    for (size_t i = 0; i < op->attribute("strides").dyn_cast<pir::ArrayAttribute>().size(); i++) {
        strides.push_back(op->attribute("strides").dyn_cast<pir::ArrayAttribute>().at(i).dyn_cast<pir::Int32Attribute>().data());
    }
    std::vector<int> paddings;
    for (size_t i = 0; i < op->attribute("paddings").dyn_cast<pir::ArrayAttribute>().size(); i++) {
        paddings.push_back(op->attribute("paddings").dyn_cast<pir::ArrayAttribute>().at(i).dyn_cast<pir::Int32Attribute>().data());
    }
    std::vector<int> dilations;
    for (size_t i = 0; i < op->attribute("dilations").dyn_cast<pir::ArrayAttribute>().size(); i++) {
        dilations.push_back(op->attribute("dilations").dyn_cast<pir::ArrayAttribute>().at(i).dyn_cast<pir::Int32Attribute>().data());
    }

    VLOG(6) << "Vjp prepare call unfold's vjp inteface";

    std::vector<std::vector<Tensor>> tensor_res =
        primitive::unfold_vjp(
        x, out_grad, kernel_sizes, strides, paddings, dilations, stop_gradients);

    VLOG(6) << "Vjp prepare stop gradient of unfold_grad";

    std::vector<std::vector<pir::OpResult>> res(tensor_res.size());
    for (size_t i = 0; i < tensor_res.size(); ++i) {
        res[i].resize(tensor_res[i].size());
        for (size_t j = 0; j < tensor_res[i].size(); ++j) {
            if(tensor_res[i][j].defined()){
                res[i][j] = std::static_pointer_cast<primitive::LazyTensor>(tensor_res[i][j].impl())->value().dyn_cast<pir::OpResult>();
            }
        }
    }
    return res;
}

std::vector<std::vector<pir::OpResult>> UniformInplaceOp::Vjp(pir::Operation* op, const std::vector<std::vector<pir::Value>>& inputs_, const std::vector<std::vector<pir::Value>>& outputs, const std::vector<std::vector<pir::Value>>& out_grads, const std::vector<std::vector<bool>>& stop_gradients){

    PADDLE_ENFORCE_EQ(
      inputs_.size(),
      1,
      platform::errors::InvalidArgument("uniform_inplace op's inputs size should be 1, but now is %d.", inputs_.size()));
    PADDLE_ENFORCE_EQ(
      outputs.size(),
      1,
      platform::errors::InvalidArgument("uniform_inplace op's outputs size should be 1, but now is %d.", outputs.size()));

    VLOG(6) << "Prepare inputs of uniform_inplace_grad";

    Tensor out_grad(std::make_shared<primitive::LazyTensor>(out_grads[0][0]));

    VLOG(6) << "Vjp prepare Prepare attributes of uniform_inplace_grad";

    float min = op->attribute("min").dyn_cast<pir::FloatAttribute>().data();
    float max = op->attribute("max").dyn_cast<pir::FloatAttribute>().data();
    int seed = op->attribute("seed").dyn_cast<pir::Int32Attribute>().data();
    int diag_num = op->attribute("diag_num").dyn_cast<pir::Int32Attribute>().data();
    int diag_step = op->attribute("diag_step").dyn_cast<pir::Int32Attribute>().data();
    float diag_val = op->attribute("diag_val").dyn_cast<pir::FloatAttribute>().data();

    VLOG(6) << "Vjp prepare call uniform_inplace's vjp inteface";

    std::vector<std::vector<Tensor>> tensor_res =
        primitive::uniform_inplace_vjp(
        out_grad, min, max, seed, diag_num, diag_step, diag_val, stop_gradients);

    VLOG(6) << "Vjp prepare stop gradient of uniform_inplace_grad";

    std::vector<std::vector<pir::OpResult>> res(tensor_res.size());
    for (size_t i = 0; i < tensor_res.size(); ++i) {
        res[i].resize(tensor_res[i].size());
        for (size_t j = 0; j < tensor_res[i].size(); ++j) {
            if(tensor_res[i][j].defined()){
                res[i][j] = std::static_pointer_cast<primitive::LazyTensor>(tensor_res[i][j].impl())->value().dyn_cast<pir::OpResult>();
            }
        }
    }
    return res;
}

std::vector<std::vector<pir::OpResult>> UniformInplace_Op::Vjp(pir::Operation* op, const std::vector<std::vector<pir::Value>>& inputs_, const std::vector<std::vector<pir::Value>>& outputs, const std::vector<std::vector<pir::Value>>& out_grads, const std::vector<std::vector<bool>>& stop_gradients){

    PADDLE_ENFORCE_EQ(
      inputs_.size(),
      1,
      platform::errors::InvalidArgument("uniform_inplace op's inputs size should be 1, but now is %d.", inputs_.size()));
    PADDLE_ENFORCE_EQ(
      outputs.size(),
      1,
      platform::errors::InvalidArgument("uniform_inplace op's outputs size should be 1, but now is %d.", outputs.size()));

    VLOG(6) << "Prepare inputs of uniform_inplace_grad";

    Tensor out_grad(std::make_shared<primitive::LazyTensor>(out_grads[0][0]));

    VLOG(6) << "Vjp prepare Prepare attributes of uniform_inplace_grad";

    float min = op->attribute("min").dyn_cast<pir::FloatAttribute>().data();
    float max = op->attribute("max").dyn_cast<pir::FloatAttribute>().data();
    int seed = op->attribute("seed").dyn_cast<pir::Int32Attribute>().data();
    int diag_num = op->attribute("diag_num").dyn_cast<pir::Int32Attribute>().data();
    int diag_step = op->attribute("diag_step").dyn_cast<pir::Int32Attribute>().data();
    float diag_val = op->attribute("diag_val").dyn_cast<pir::FloatAttribute>().data();

    VLOG(6) << "Vjp prepare call uniform_inplace_'s vjp inteface";

    std::vector<std::vector<Tensor>> tensor_res =
        primitive::uniform_inplace_vjp(
        out_grad, min, max, seed, diag_num, diag_step, diag_val, stop_gradients);

    VLOG(6) << "Vjp prepare stop gradient of uniform_inplace_grad";

    std::vector<std::vector<pir::OpResult>> res(tensor_res.size());
    for (size_t i = 0; i < tensor_res.size(); ++i) {
        res[i].resize(tensor_res[i].size());
        for (size_t j = 0; j < tensor_res[i].size(); ++j) {
            if(tensor_res[i][j].defined()){
                res[i][j] = std::static_pointer_cast<primitive::LazyTensor>(tensor_res[i][j].impl())->value().dyn_cast<pir::OpResult>();
            }
        }
    }
    return res;
}

std::vector<std::vector<pir::OpResult>> Unpool3dOp::Vjp(pir::Operation* op, const std::vector<std::vector<pir::Value>>& inputs_, const std::vector<std::vector<pir::Value>>& outputs, const std::vector<std::vector<pir::Value>>& out_grads, const std::vector<std::vector<bool>>& stop_gradients){

    PADDLE_ENFORCE_EQ(
      inputs_.size(),
      2,
      platform::errors::InvalidArgument("unpool3d op's inputs size should be 2, but now is %d.", inputs_.size()));
    PADDLE_ENFORCE_EQ(
      outputs.size(),
      1,
      platform::errors::InvalidArgument("unpool3d op's outputs size should be 1, but now is %d.", outputs.size()));

    VLOG(6) << "Prepare inputs of unpool3d_grad";

    Tensor x(std::make_shared<primitive::LazyTensor>(inputs_[0][0]));
    Tensor indices(std::make_shared<primitive::LazyTensor>(inputs_[1][0]));
    Tensor out(std::make_shared<primitive::LazyTensor>(outputs[0][0]));
    Tensor out_grad(std::make_shared<primitive::LazyTensor>(out_grads[0][0]));

    VLOG(6) << "Vjp prepare Prepare attributes of unpool3d_grad";

    std::vector<int> ksize;
    for (size_t i = 0; i < op->attribute("ksize").dyn_cast<pir::ArrayAttribute>().size(); i++) {
        ksize.push_back(op->attribute("ksize").dyn_cast<pir::ArrayAttribute>().at(i).dyn_cast<pir::Int32Attribute>().data());
    }
    std::vector<int> strides;
    for (size_t i = 0; i < op->attribute("strides").dyn_cast<pir::ArrayAttribute>().size(); i++) {
        strides.push_back(op->attribute("strides").dyn_cast<pir::ArrayAttribute>().at(i).dyn_cast<pir::Int32Attribute>().data());
    }
    std::vector<int> paddings;
    for (size_t i = 0; i < op->attribute("paddings").dyn_cast<pir::ArrayAttribute>().size(); i++) {
        paddings.push_back(op->attribute("paddings").dyn_cast<pir::ArrayAttribute>().at(i).dyn_cast<pir::Int32Attribute>().data());
    }
    std::vector<int> output_size;
    for (size_t i = 0; i < op->attribute("output_size").dyn_cast<pir::ArrayAttribute>().size(); i++) {
        output_size.push_back(op->attribute("output_size").dyn_cast<pir::ArrayAttribute>().at(i).dyn_cast<pir::Int32Attribute>().data());
    }
    std::string data_format = op->attribute("data_format").dyn_cast<pir::StrAttribute>().AsString();

    VLOG(6) << "Vjp prepare call unpool3d's vjp inteface";

    std::vector<std::vector<Tensor>> tensor_res =
        primitive::unpool3d_vjp(
        x, indices, out, out_grad, ksize, strides, paddings, output_size, data_format, stop_gradients);

    VLOG(6) << "Vjp prepare stop gradient of unpool3d_grad";

    std::vector<std::vector<pir::OpResult>> res(tensor_res.size());
    for (size_t i = 0; i < tensor_res.size(); ++i) {
        res[i].resize(tensor_res[i].size());
        for (size_t j = 0; j < tensor_res[i].size(); ++j) {
            if(tensor_res[i][j].defined()){
                res[i][j] = std::static_pointer_cast<primitive::LazyTensor>(tensor_res[i][j].impl())->value().dyn_cast<pir::OpResult>();
            }
        }
    }
    return res;
}

std::vector<std::vector<pir::OpResult>> UnsqueezeOp::Vjp(pir::Operation* op, const std::vector<std::vector<pir::Value>>& inputs_, const std::vector<std::vector<pir::Value>>& outputs, const std::vector<std::vector<pir::Value>>& out_grads, const std::vector<std::vector<bool>>& stop_gradients){

    PADDLE_ENFORCE_EQ(
      inputs_.size(),
      2,
      platform::errors::InvalidArgument("unsqueeze op's inputs size should be 2, but now is %d.", inputs_.size()));
    PADDLE_ENFORCE_EQ(
      outputs.size(),
      2,
      platform::errors::InvalidArgument("unsqueeze op's outputs size should be 2, but now is %d.", outputs.size()));

    VLOG(6) << "Prepare inputs of unsqueeze_grad";

    Tensor xshape(std::make_shared<primitive::LazyTensor>(outputs[1][0]));
    Tensor out_grad(std::make_shared<primitive::LazyTensor>(out_grads[0][0]));

    VLOG(6) << "Vjp prepare Prepare attributes of unsqueeze_grad";

    Tensor axis(std::make_shared<primitive::LazyTensor>(inputs_[1][0]));

    VLOG(6) << "Vjp prepare call unsqueeze's vjp inteface";

    std::vector<std::vector<Tensor>> tensor_res =
        primitive::unsqueeze_vjp(
        xshape, out_grad, axis, stop_gradients);

    VLOG(6) << "Vjp prepare stop gradient of unsqueeze_grad";

    std::vector<std::vector<pir::OpResult>> res(tensor_res.size());
    for (size_t i = 0; i < tensor_res.size(); ++i) {
        res[i].resize(tensor_res[i].size());
        for (size_t j = 0; j < tensor_res[i].size(); ++j) {
            if(tensor_res[i][j].defined()){
                res[i][j] = std::static_pointer_cast<primitive::LazyTensor>(tensor_res[i][j].impl())->value().dyn_cast<pir::OpResult>();
            }
        }
    }
    return res;
}

std::vector<std::vector<pir::OpResult>> Unsqueeze_Op::Vjp(pir::Operation* op, const std::vector<std::vector<pir::Value>>& inputs_, const std::vector<std::vector<pir::Value>>& outputs, const std::vector<std::vector<pir::Value>>& out_grads, const std::vector<std::vector<bool>>& stop_gradients){

    PADDLE_ENFORCE_EQ(
      inputs_.size(),
      2,
      platform::errors::InvalidArgument("unsqueeze op's inputs size should be 2, but now is %d.", inputs_.size()));
    PADDLE_ENFORCE_EQ(
      outputs.size(),
      2,
      platform::errors::InvalidArgument("unsqueeze op's outputs size should be 2, but now is %d.", outputs.size()));

    VLOG(6) << "Prepare inputs of unsqueeze_grad";

    Tensor xshape(std::make_shared<primitive::LazyTensor>(outputs[1][0]));
    Tensor out_grad(std::make_shared<primitive::LazyTensor>(out_grads[0][0]));

    VLOG(6) << "Vjp prepare Prepare attributes of unsqueeze_grad";

    Tensor axis(std::make_shared<primitive::LazyTensor>(inputs_[1][0]));

    VLOG(6) << "Vjp prepare call unsqueeze_'s vjp inteface";

    std::vector<std::vector<Tensor>> tensor_res =
        primitive::unsqueeze_vjp(
        xshape, out_grad, axis, stop_gradients);

    VLOG(6) << "Vjp prepare stop gradient of unsqueeze_grad";

    std::vector<std::vector<pir::OpResult>> res(tensor_res.size());
    for (size_t i = 0; i < tensor_res.size(); ++i) {
        res[i].resize(tensor_res[i].size());
        for (size_t j = 0; j < tensor_res[i].size(); ++j) {
            if(tensor_res[i][j].defined()){
                res[i][j] = std::static_pointer_cast<primitive::LazyTensor>(tensor_res[i][j].impl())->value().dyn_cast<pir::OpResult>();
            }
        }
    }
    return res;
}

std::vector<std::vector<pir::OpResult>> UnstackOp::Vjp(pir::Operation* op, const std::vector<std::vector<pir::Value>>& inputs_, const std::vector<std::vector<pir::Value>>& outputs, const std::vector<std::vector<pir::Value>>& out_grads, const std::vector<std::vector<bool>>& stop_gradients){

    PADDLE_ENFORCE_EQ(
      inputs_.size(),
      1,
      platform::errors::InvalidArgument("unstack op's inputs size should be 1, but now is %d.", inputs_.size()));
    PADDLE_ENFORCE_EQ(
      outputs.size(),
      1,
      platform::errors::InvalidArgument("unstack op's outputs size should be 1, but now is %d.", outputs.size()));

    VLOG(6) << "Prepare inputs of unstack_grad";

    std::vector<Tensor> out_grad;
    for (size_t idx = 0; idx < out_grads[0].size(); idx++) {
        out_grad.emplace_back(
            std::make_shared<primitive::LazyTensor>(out_grads[0][idx]));
    }

    VLOG(6) << "Vjp prepare Prepare attributes of unstack_grad";

    int axis = op->attribute("axis").dyn_cast<pir::Int32Attribute>().data();

    VLOG(6) << "Vjp prepare call unstack's vjp inteface";

    std::vector<std::vector<Tensor>> tensor_res =
        primitive::unstack_vjp(
        out_grad, axis, stop_gradients);

    VLOG(6) << "Vjp prepare stop gradient of unstack_grad";

    std::vector<std::vector<pir::OpResult>> res(tensor_res.size());
    for (size_t i = 0; i < tensor_res.size(); ++i) {
        res[i].resize(tensor_res[i].size());
        for (size_t j = 0; j < tensor_res[i].size(); ++j) {
            if(tensor_res[i][j].defined()){
                res[i][j] = std::static_pointer_cast<primitive::LazyTensor>(tensor_res[i][j].impl())->value().dyn_cast<pir::OpResult>();
            }
        }
    }
    return res;
}

std::vector<std::vector<pir::OpResult>> ViewDtypeOp::Vjp(pir::Operation* op, const std::vector<std::vector<pir::Value>>& inputs_, const std::vector<std::vector<pir::Value>>& outputs, const std::vector<std::vector<pir::Value>>& out_grads, const std::vector<std::vector<bool>>& stop_gradients){

    PADDLE_ENFORCE_EQ(
      inputs_.size(),
      1,
      platform::errors::InvalidArgument("view_dtype op's inputs size should be 1, but now is %d.", inputs_.size()));
    PADDLE_ENFORCE_EQ(
      outputs.size(),
      1,
      platform::errors::InvalidArgument("view_dtype op's outputs size should be 1, but now is %d.", outputs.size()));

    VLOG(6) << "Prepare inputs of view_dtype_grad";

    Tensor input(std::make_shared<primitive::LazyTensor>(inputs_[0][0]));
    Tensor out_grad(std::make_shared<primitive::LazyTensor>(out_grads[0][0]));

    VLOG(6) << "Vjp prepare Prepare attributes of view_dtype_grad";

    phi::DataType dtype = op->attribute("dtype").dyn_cast<paddle::dialect::DataTypeAttribute>().data();

    VLOG(6) << "Vjp prepare call view_dtype's vjp inteface";

    std::vector<std::vector<Tensor>> tensor_res =
        primitive::view_dtype_vjp(
        input, out_grad, dtype, stop_gradients);

    VLOG(6) << "Vjp prepare stop gradient of view_dtype_grad";

    std::vector<std::vector<pir::OpResult>> res(tensor_res.size());
    for (size_t i = 0; i < tensor_res.size(); ++i) {
        res[i].resize(tensor_res[i].size());
        for (size_t j = 0; j < tensor_res[i].size(); ++j) {
            if(tensor_res[i][j].defined()){
                res[i][j] = std::static_pointer_cast<primitive::LazyTensor>(tensor_res[i][j].impl())->value().dyn_cast<pir::OpResult>();
            }
        }
    }
    return res;
}

std::vector<std::vector<pir::OpResult>> ViewShapeOp::Vjp(pir::Operation* op, const std::vector<std::vector<pir::Value>>& inputs_, const std::vector<std::vector<pir::Value>>& outputs, const std::vector<std::vector<pir::Value>>& out_grads, const std::vector<std::vector<bool>>& stop_gradients){

    PADDLE_ENFORCE_EQ(
      inputs_.size(),
      1,
      platform::errors::InvalidArgument("view_shape op's inputs size should be 1, but now is %d.", inputs_.size()));
    PADDLE_ENFORCE_EQ(
      outputs.size(),
      1,
      platform::errors::InvalidArgument("view_shape op's outputs size should be 1, but now is %d.", outputs.size()));

    VLOG(6) << "Prepare inputs of view_shape_grad";

    Tensor input(std::make_shared<primitive::LazyTensor>(inputs_[0][0]));
    Tensor out_grad(std::make_shared<primitive::LazyTensor>(out_grads[0][0]));

    VLOG(6) << "Vjp prepare Prepare attributes of view_shape_grad";

    std::vector<int64_t> dims;
    for (size_t i = 0; i < op->attribute("dims").dyn_cast<pir::ArrayAttribute>().size(); i++) {
        dims.push_back(op->attribute("dims").dyn_cast<pir::ArrayAttribute>().at(i).dyn_cast<pir::Int64Attribute>().data());
    }

    VLOG(6) << "Vjp prepare call view_shape's vjp inteface";

    std::vector<std::vector<Tensor>> tensor_res =
        primitive::view_shape_vjp(
        input, out_grad, dims, stop_gradients);

    VLOG(6) << "Vjp prepare stop gradient of view_shape_grad";

    std::vector<std::vector<pir::OpResult>> res(tensor_res.size());
    for (size_t i = 0; i < tensor_res.size(); ++i) {
        res[i].resize(tensor_res[i].size());
        for (size_t j = 0; j < tensor_res[i].size(); ++j) {
            if(tensor_res[i][j].defined()){
                res[i][j] = std::static_pointer_cast<primitive::LazyTensor>(tensor_res[i][j].impl())->value().dyn_cast<pir::OpResult>();
            }
        }
    }
    return res;
}

std::vector<std::vector<pir::OpResult>> WarpctcOp::Vjp(pir::Operation* op, const std::vector<std::vector<pir::Value>>& inputs_, const std::vector<std::vector<pir::Value>>& outputs, const std::vector<std::vector<pir::Value>>& out_grads, const std::vector<std::vector<bool>>& stop_gradients){

    PADDLE_ENFORCE_EQ(
      inputs_.size(),
      4,
      platform::errors::InvalidArgument("warpctc op's inputs size should be 4, but now is %d.", inputs_.size()));
    PADDLE_ENFORCE_EQ(
      outputs.size(),
      2,
      platform::errors::InvalidArgument("warpctc op's outputs size should be 2, but now is %d.", outputs.size()));

    VLOG(6) << "Prepare inputs of warpctc_grad";

    Tensor logits(std::make_shared<primitive::LazyTensor>(inputs_[0][0]));
    paddle::optional<Tensor> logits_length;
    if (!IsEmptyValue(inputs_[2][0])){
        logits_length = paddle::make_optional<Tensor>(Tensor(std::make_shared<primitive::LazyTensor>(inputs_[2][0])));
    }
    Tensor warpctcgrad(std::make_shared<primitive::LazyTensor>(outputs[1][0]));
    Tensor loss_grad(std::make_shared<primitive::LazyTensor>(out_grads[0][0]));

    VLOG(6) << "Vjp prepare Prepare attributes of warpctc_grad";

    int blank = op->attribute("blank").dyn_cast<pir::Int32Attribute>().data();
    bool norm_by_times = op->attribute("norm_by_times").dyn_cast<pir::BoolAttribute>().data();

    VLOG(6) << "Vjp prepare call warpctc's vjp inteface";

    std::vector<std::vector<Tensor>> tensor_res =
        primitive::warpctc_vjp(
        logits, logits_length, warpctcgrad, loss_grad, blank, norm_by_times, stop_gradients);

    VLOG(6) << "Vjp prepare stop gradient of warpctc_grad";

    std::vector<std::vector<pir::OpResult>> res(tensor_res.size());
    for (size_t i = 0; i < tensor_res.size(); ++i) {
        res[i].resize(tensor_res[i].size());
        for (size_t j = 0; j < tensor_res[i].size(); ++j) {
            if(tensor_res[i][j].defined()){
                res[i][j] = std::static_pointer_cast<primitive::LazyTensor>(tensor_res[i][j].impl())->value().dyn_cast<pir::OpResult>();
            }
        }
    }
    return res;
}

std::vector<std::vector<pir::OpResult>> WarprnntOp::Vjp(pir::Operation* op, const std::vector<std::vector<pir::Value>>& inputs_, const std::vector<std::vector<pir::Value>>& outputs, const std::vector<std::vector<pir::Value>>& out_grads, const std::vector<std::vector<bool>>& stop_gradients){

    PADDLE_ENFORCE_EQ(
      inputs_.size(),
      4,
      platform::errors::InvalidArgument("warprnnt op's inputs size should be 4, but now is %d.", inputs_.size()));
    PADDLE_ENFORCE_EQ(
      outputs.size(),
      2,
      platform::errors::InvalidArgument("warprnnt op's outputs size should be 2, but now is %d.", outputs.size()));

    VLOG(6) << "Prepare inputs of warprnnt_grad";

    Tensor input(std::make_shared<primitive::LazyTensor>(inputs_[0][0]));
    Tensor input_lengths(std::make_shared<primitive::LazyTensor>(inputs_[2][0]));
    Tensor warprnntgrad(std::make_shared<primitive::LazyTensor>(outputs[1][0]));
    Tensor loss_grad(std::make_shared<primitive::LazyTensor>(out_grads[0][0]));

    VLOG(6) << "Vjp prepare Prepare attributes of warprnnt_grad";

    int blank = op->attribute("blank").dyn_cast<pir::Int32Attribute>().data();
    float fastemit_lambda = op->attribute("fastemit_lambda").dyn_cast<pir::FloatAttribute>().data();

    VLOG(6) << "Vjp prepare call warprnnt's vjp inteface";

    std::vector<std::vector<Tensor>> tensor_res =
        primitive::warprnnt_vjp(
        input, input_lengths, warprnntgrad, loss_grad, blank, fastemit_lambda, stop_gradients);

    VLOG(6) << "Vjp prepare stop gradient of warprnnt_grad";

    std::vector<std::vector<pir::OpResult>> res(tensor_res.size());
    for (size_t i = 0; i < tensor_res.size(); ++i) {
        res[i].resize(tensor_res[i].size());
        for (size_t j = 0; j < tensor_res[i].size(); ++j) {
            if(tensor_res[i][j].defined()){
                res[i][j] = std::static_pointer_cast<primitive::LazyTensor>(tensor_res[i][j].impl())->value().dyn_cast<pir::OpResult>();
            }
        }
    }
    return res;
}

std::vector<std::vector<pir::OpResult>> WeightOnlyLinearOp::Vjp(pir::Operation* op, const std::vector<std::vector<pir::Value>>& inputs_, const std::vector<std::vector<pir::Value>>& outputs, const std::vector<std::vector<pir::Value>>& out_grads, const std::vector<std::vector<bool>>& stop_gradients){

    PADDLE_ENFORCE_EQ(
      inputs_.size(),
      4,
      platform::errors::InvalidArgument("weight_only_linear op's inputs size should be 4, but now is %d.", inputs_.size()));
    PADDLE_ENFORCE_EQ(
      outputs.size(),
      1,
      platform::errors::InvalidArgument("weight_only_linear op's outputs size should be 1, but now is %d.", outputs.size()));

    VLOG(6) << "Prepare inputs of weight_only_linear_grad";

    Tensor x(std::make_shared<primitive::LazyTensor>(inputs_[0][0]));
    Tensor weight(std::make_shared<primitive::LazyTensor>(inputs_[1][0]));
    paddle::optional<Tensor> bias;
    if (!IsEmptyValue(inputs_[2][0])){
        bias = paddle::make_optional<Tensor>(Tensor(std::make_shared<primitive::LazyTensor>(inputs_[2][0])));
    }
    Tensor weight_scale(std::make_shared<primitive::LazyTensor>(inputs_[3][0]));
    Tensor out_grad(std::make_shared<primitive::LazyTensor>(out_grads[0][0]));

    VLOG(6) << "Vjp prepare Prepare attributes of weight_only_linear_grad";

    std::string weight_dtype = op->attribute("weight_dtype").dyn_cast<pir::StrAttribute>().AsString();
    int arch = op->attribute("arch").dyn_cast<pir::Int32Attribute>().data();
    int group_size = op->attribute("group_size").dyn_cast<pir::Int32Attribute>().data();

    VLOG(6) << "Vjp prepare call weight_only_linear's vjp inteface";

    std::vector<std::vector<Tensor>> tensor_res =
        primitive::weight_only_linear_vjp(
        x, weight, bias, weight_scale, out_grad, weight_dtype, arch, group_size, stop_gradients);

    VLOG(6) << "Vjp prepare stop gradient of weight_only_linear_grad";

    std::vector<std::vector<pir::OpResult>> res(tensor_res.size());
    for (size_t i = 0; i < tensor_res.size(); ++i) {
        res[i].resize(tensor_res[i].size());
        for (size_t j = 0; j < tensor_res[i].size(); ++j) {
            if(tensor_res[i][j].defined()){
                res[i][j] = std::static_pointer_cast<primitive::LazyTensor>(tensor_res[i][j].impl())->value().dyn_cast<pir::OpResult>();
            }
        }
    }
    return res;
}

std::vector<std::vector<pir::OpResult>> WhereOp::Vjp(pir::Operation* op, const std::vector<std::vector<pir::Value>>& inputs_, const std::vector<std::vector<pir::Value>>& outputs, const std::vector<std::vector<pir::Value>>& out_grads, const std::vector<std::vector<bool>>& stop_gradients){

    PADDLE_ENFORCE_EQ(
      inputs_.size(),
      3,
      platform::errors::InvalidArgument("where op's inputs size should be 3, but now is %d.", inputs_.size()));
    PADDLE_ENFORCE_EQ(
      outputs.size(),
      1,
      platform::errors::InvalidArgument("where op's outputs size should be 1, but now is %d.", outputs.size()));

    VLOG(6) << "Prepare inputs of where_grad";

    Tensor condition(std::make_shared<primitive::LazyTensor>(inputs_[0][0]));
    Tensor x(std::make_shared<primitive::LazyTensor>(inputs_[1][0]));
    Tensor y(std::make_shared<primitive::LazyTensor>(inputs_[2][0]));
    Tensor out_grad(std::make_shared<primitive::LazyTensor>(out_grads[0][0]));

    VLOG(6) << "Vjp prepare Prepare attributes of where_grad";


    VLOG(6) << "Vjp prepare call where's vjp inteface";

    std::vector<std::vector<Tensor>> tensor_res =
        primitive::where_vjp(
        condition, x, y, out_grad, stop_gradients);

    VLOG(6) << "Vjp prepare stop gradient of where_grad";

    std::vector<std::vector<pir::OpResult>> res(tensor_res.size());
    for (size_t i = 0; i < tensor_res.size(); ++i) {
        res[i].resize(tensor_res[i].size());
        for (size_t j = 0; j < tensor_res[i].size(); ++j) {
            if(tensor_res[i][j].defined()){
                res[i][j] = std::static_pointer_cast<primitive::LazyTensor>(tensor_res[i][j].impl())->value().dyn_cast<pir::OpResult>();
            }
        }
    }
    return res;
}

std::vector<std::vector<pir::OpResult>> Where_Op::Vjp(pir::Operation* op, const std::vector<std::vector<pir::Value>>& inputs_, const std::vector<std::vector<pir::Value>>& outputs, const std::vector<std::vector<pir::Value>>& out_grads, const std::vector<std::vector<bool>>& stop_gradients){

    PADDLE_ENFORCE_EQ(
      inputs_.size(),
      3,
      platform::errors::InvalidArgument("where op's inputs size should be 3, but now is %d.", inputs_.size()));
    PADDLE_ENFORCE_EQ(
      outputs.size(),
      1,
      platform::errors::InvalidArgument("where op's outputs size should be 1, but now is %d.", outputs.size()));

    VLOG(6) << "Prepare inputs of where_grad";

    Tensor condition(std::make_shared<primitive::LazyTensor>(inputs_[0][0]));
    Tensor x(std::make_shared<primitive::LazyTensor>(inputs_[1][0]));
    Tensor y(std::make_shared<primitive::LazyTensor>(inputs_[2][0]));
    Tensor out_grad(std::make_shared<primitive::LazyTensor>(out_grads[0][0]));

    VLOG(6) << "Vjp prepare Prepare attributes of where_grad";


    VLOG(6) << "Vjp prepare call where_'s vjp inteface";

    std::vector<std::vector<Tensor>> tensor_res =
        primitive::where_vjp(
        condition, x, y, out_grad, stop_gradients);

    VLOG(6) << "Vjp prepare stop gradient of where_grad";

    std::vector<std::vector<pir::OpResult>> res(tensor_res.size());
    for (size_t i = 0; i < tensor_res.size(); ++i) {
        res[i].resize(tensor_res[i].size());
        for (size_t j = 0; j < tensor_res[i].size(); ++j) {
            if(tensor_res[i][j].defined()){
                res[i][j] = std::static_pointer_cast<primitive::LazyTensor>(tensor_res[i][j].impl())->value().dyn_cast<pir::OpResult>();
            }
        }
    }
    return res;
}

std::vector<std::vector<pir::OpResult>> YoloLossOp::Vjp(pir::Operation* op, const std::vector<std::vector<pir::Value>>& inputs_, const std::vector<std::vector<pir::Value>>& outputs, const std::vector<std::vector<pir::Value>>& out_grads, const std::vector<std::vector<bool>>& stop_gradients){

    PADDLE_ENFORCE_EQ(
      inputs_.size(),
      4,
      platform::errors::InvalidArgument("yolo_loss op's inputs size should be 4, but now is %d.", inputs_.size()));
    PADDLE_ENFORCE_EQ(
      outputs.size(),
      3,
      platform::errors::InvalidArgument("yolo_loss op's outputs size should be 3, but now is %d.", outputs.size()));

    VLOG(6) << "Prepare inputs of yolo_loss_grad";

    Tensor x(std::make_shared<primitive::LazyTensor>(inputs_[0][0]));
    Tensor gt_box(std::make_shared<primitive::LazyTensor>(inputs_[1][0]));
    Tensor gt_label(std::make_shared<primitive::LazyTensor>(inputs_[2][0]));
    paddle::optional<Tensor> gt_score;
    if (!IsEmptyValue(inputs_[3][0])){
        gt_score = paddle::make_optional<Tensor>(Tensor(std::make_shared<primitive::LazyTensor>(inputs_[3][0])));
    }
    Tensor objectness_mask(std::make_shared<primitive::LazyTensor>(outputs[1][0]));
    Tensor gt_match_mask(std::make_shared<primitive::LazyTensor>(outputs[2][0]));
    Tensor loss_grad(std::make_shared<primitive::LazyTensor>(out_grads[0][0]));

    VLOG(6) << "Vjp prepare Prepare attributes of yolo_loss_grad";

    std::vector<int> anchors;
    for (size_t i = 0; i < op->attribute("anchors").dyn_cast<pir::ArrayAttribute>().size(); i++) {
        anchors.push_back(op->attribute("anchors").dyn_cast<pir::ArrayAttribute>().at(i).dyn_cast<pir::Int32Attribute>().data());
    }
    std::vector<int> anchor_mask;
    for (size_t i = 0; i < op->attribute("anchor_mask").dyn_cast<pir::ArrayAttribute>().size(); i++) {
        anchor_mask.push_back(op->attribute("anchor_mask").dyn_cast<pir::ArrayAttribute>().at(i).dyn_cast<pir::Int32Attribute>().data());
    }
    int class_num = op->attribute("class_num").dyn_cast<pir::Int32Attribute>().data();
    float ignore_thresh = op->attribute("ignore_thresh").dyn_cast<pir::FloatAttribute>().data();
    int downsample_ratio = op->attribute("downsample_ratio").dyn_cast<pir::Int32Attribute>().data();
    bool use_label_smooth = op->attribute("use_label_smooth").dyn_cast<pir::BoolAttribute>().data();
    float scale_x_y = op->attribute("scale_x_y").dyn_cast<pir::FloatAttribute>().data();

    VLOG(6) << "Vjp prepare call yolo_loss's vjp inteface";

    std::vector<std::vector<Tensor>> tensor_res =
        primitive::yolo_loss_vjp(
        x, gt_box, gt_label, gt_score, objectness_mask, gt_match_mask, loss_grad, anchors, anchor_mask, class_num, ignore_thresh, downsample_ratio, use_label_smooth, scale_x_y, stop_gradients);

    VLOG(6) << "Vjp prepare stop gradient of yolo_loss_grad";

    std::vector<std::vector<pir::OpResult>> res(tensor_res.size());
    for (size_t i = 0; i < tensor_res.size(); ++i) {
        res[i].resize(tensor_res[i].size());
        for (size_t j = 0; j < tensor_res[i].size(); ++j) {
            if(tensor_res[i][j].defined()){
                res[i][j] = std::static_pointer_cast<primitive::LazyTensor>(tensor_res[i][j].impl())->value().dyn_cast<pir::OpResult>();
            }
        }
    }
    return res;
}


std::vector<std::vector<pir::OpResult>> AbsGradOp::Vjp(pir::Operation* op, const std::vector<std::vector<pir::Value>>& inputs_, const std::vector<std::vector<pir::Value>>& outputs, const std::vector<std::vector<pir::Value>>& out_grads, const std::vector<std::vector<bool>>& stop_gradients){

    PADDLE_ENFORCE_EQ(
      inputs_.size(),
      2,
      platform::errors::InvalidArgument("abs_grad op's inputs size should be 2, but now is %d.", inputs_.size()));
    PADDLE_ENFORCE_EQ(
      outputs.size(),
      1,
      platform::errors::InvalidArgument("abs_grad op's outputs size should be 1, but now is %d.", outputs.size()));

    VLOG(6) << "Prepare inputs of abs_double_grad";

    Tensor x(std::make_shared<primitive::LazyTensor>(inputs_[0][0]));
    Tensor grad_x_grad(std::make_shared<primitive::LazyTensor>(out_grads[0][0]));

    VLOG(6) << "Vjp prepare Prepare attributes of abs_double_grad";


    VLOG(6) << "Vjp prepare call abs_grad's vjp inteface";

    std::vector<std::vector<Tensor>> tensor_res =
        primitive::abs_grad_vjp(
        x, grad_x_grad, stop_gradients);

    VLOG(6) << "Vjp prepare stop gradient of abs_double_grad";

    std::vector<std::vector<pir::OpResult>> res(tensor_res.size());
    for (size_t i = 0; i < tensor_res.size(); ++i) {
        res[i].resize(tensor_res[i].size());
        for (size_t j = 0; j < tensor_res[i].size(); ++j) {
            if(tensor_res[i][j].defined()){
                res[i][j] = std::static_pointer_cast<primitive::LazyTensor>(tensor_res[i][j].impl())->value().dyn_cast<pir::OpResult>();
            }
        }
    }
    return res;
}

std::vector<std::vector<pir::OpResult>> CeluGradOp::Vjp(pir::Operation* op, const std::vector<std::vector<pir::Value>>& inputs_, const std::vector<std::vector<pir::Value>>& outputs, const std::vector<std::vector<pir::Value>>& out_grads, const std::vector<std::vector<bool>>& stop_gradients){

    PADDLE_ENFORCE_EQ(
      inputs_.size(),
      2,
      platform::errors::InvalidArgument("celu_grad op's inputs size should be 2, but now is %d.", inputs_.size()));
    PADDLE_ENFORCE_EQ(
      outputs.size(),
      1,
      platform::errors::InvalidArgument("celu_grad op's outputs size should be 1, but now is %d.", outputs.size()));

    VLOG(6) << "Prepare inputs of celu_double_grad";

    Tensor x(std::make_shared<primitive::LazyTensor>(inputs_[0][0]));
    Tensor grad_out(std::make_shared<primitive::LazyTensor>(inputs_[1][0]));
    Tensor grad_x_grad(std::make_shared<primitive::LazyTensor>(out_grads[0][0]));

    VLOG(6) << "Vjp prepare Prepare attributes of celu_double_grad";

    float alpha = op->attribute("alpha").dyn_cast<pir::FloatAttribute>().data();

    VLOG(6) << "Vjp prepare call celu_grad's vjp inteface";

    std::vector<std::vector<Tensor>> tensor_res =
        primitive::celu_grad_vjp(
        x, grad_out, grad_x_grad, alpha, stop_gradients);

    VLOG(6) << "Vjp prepare stop gradient of celu_double_grad";

    std::vector<std::vector<pir::OpResult>> res(tensor_res.size());
    for (size_t i = 0; i < tensor_res.size(); ++i) {
        res[i].resize(tensor_res[i].size());
        for (size_t j = 0; j < tensor_res[i].size(); ++j) {
            if(tensor_res[i][j].defined()){
                res[i][j] = std::static_pointer_cast<primitive::LazyTensor>(tensor_res[i][j].impl())->value().dyn_cast<pir::OpResult>();
            }
        }
    }
    return res;
}

std::vector<std::vector<pir::OpResult>> CeluGrad_Op::Vjp(pir::Operation* op, const std::vector<std::vector<pir::Value>>& inputs_, const std::vector<std::vector<pir::Value>>& outputs, const std::vector<std::vector<pir::Value>>& out_grads, const std::vector<std::vector<bool>>& stop_gradients){

    PADDLE_ENFORCE_EQ(
      inputs_.size(),
      2,
      platform::errors::InvalidArgument("celu_grad op's inputs size should be 2, but now is %d.", inputs_.size()));
    PADDLE_ENFORCE_EQ(
      outputs.size(),
      1,
      platform::errors::InvalidArgument("celu_grad op's outputs size should be 1, but now is %d.", outputs.size()));

    VLOG(6) << "Prepare inputs of celu_double_grad";

    Tensor x(std::make_shared<primitive::LazyTensor>(inputs_[0][0]));
    Tensor grad_out(std::make_shared<primitive::LazyTensor>(inputs_[1][0]));
    Tensor grad_x_grad(std::make_shared<primitive::LazyTensor>(out_grads[0][0]));

    VLOG(6) << "Vjp prepare Prepare attributes of celu_double_grad";

    float alpha = op->attribute("alpha").dyn_cast<pir::FloatAttribute>().data();

    VLOG(6) << "Vjp prepare call celu_grad_'s vjp inteface";

    std::vector<std::vector<Tensor>> tensor_res =
        primitive::celu_grad_vjp(
        x, grad_out, grad_x_grad, alpha, stop_gradients);

    VLOG(6) << "Vjp prepare stop gradient of celu_double_grad";

    std::vector<std::vector<pir::OpResult>> res(tensor_res.size());
    for (size_t i = 0; i < tensor_res.size(); ++i) {
        res[i].resize(tensor_res[i].size());
        for (size_t j = 0; j < tensor_res[i].size(); ++j) {
            if(tensor_res[i][j].defined()){
                res[i][j] = std::static_pointer_cast<primitive::LazyTensor>(tensor_res[i][j].impl())->value().dyn_cast<pir::OpResult>();
            }
        }
    }
    return res;
}

std::vector<std::vector<pir::OpResult>> ClipGradOp::Vjp(pir::Operation* op, const std::vector<std::vector<pir::Value>>& inputs_, const std::vector<std::vector<pir::Value>>& outputs, const std::vector<std::vector<pir::Value>>& out_grads, const std::vector<std::vector<bool>>& stop_gradients){

    PADDLE_ENFORCE_EQ(
      inputs_.size(),
      4,
      platform::errors::InvalidArgument("clip_grad op's inputs size should be 4, but now is %d.", inputs_.size()));
    PADDLE_ENFORCE_EQ(
      outputs.size(),
      1,
      platform::errors::InvalidArgument("clip_grad op's outputs size should be 1, but now is %d.", outputs.size()));

    VLOG(6) << "Prepare inputs of clip_double_grad";

    Tensor x(std::make_shared<primitive::LazyTensor>(inputs_[0][0]));
    Tensor grad_x_grad(std::make_shared<primitive::LazyTensor>(out_grads[0][0]));

    VLOG(6) << "Vjp prepare Prepare attributes of clip_double_grad";

    Tensor min(std::make_shared<primitive::LazyTensor>(inputs_[2][0]));
    Tensor max(std::make_shared<primitive::LazyTensor>(inputs_[3][0]));

    VLOG(6) << "Vjp prepare call clip_grad's vjp inteface";

    std::vector<std::vector<Tensor>> tensor_res =
        primitive::clip_grad_vjp(
        x, grad_x_grad, min, max, stop_gradients);

    VLOG(6) << "Vjp prepare stop gradient of clip_double_grad";

    std::vector<std::vector<pir::OpResult>> res(tensor_res.size());
    for (size_t i = 0; i < tensor_res.size(); ++i) {
        res[i].resize(tensor_res[i].size());
        for (size_t j = 0; j < tensor_res[i].size(); ++j) {
            if(tensor_res[i][j].defined()){
                res[i][j] = std::static_pointer_cast<primitive::LazyTensor>(tensor_res[i][j].impl())->value().dyn_cast<pir::OpResult>();
            }
        }
    }
    return res;
}

std::vector<std::vector<pir::OpResult>> ClipGrad_Op::Vjp(pir::Operation* op, const std::vector<std::vector<pir::Value>>& inputs_, const std::vector<std::vector<pir::Value>>& outputs, const std::vector<std::vector<pir::Value>>& out_grads, const std::vector<std::vector<bool>>& stop_gradients){

    PADDLE_ENFORCE_EQ(
      inputs_.size(),
      4,
      platform::errors::InvalidArgument("clip_grad op's inputs size should be 4, but now is %d.", inputs_.size()));
    PADDLE_ENFORCE_EQ(
      outputs.size(),
      1,
      platform::errors::InvalidArgument("clip_grad op's outputs size should be 1, but now is %d.", outputs.size()));

    VLOG(6) << "Prepare inputs of clip_double_grad";

    Tensor x(std::make_shared<primitive::LazyTensor>(inputs_[0][0]));
    Tensor grad_x_grad(std::make_shared<primitive::LazyTensor>(out_grads[0][0]));

    VLOG(6) << "Vjp prepare Prepare attributes of clip_double_grad";

    Tensor min(std::make_shared<primitive::LazyTensor>(inputs_[2][0]));
    Tensor max(std::make_shared<primitive::LazyTensor>(inputs_[3][0]));

    VLOG(6) << "Vjp prepare call clip_grad_'s vjp inteface";

    std::vector<std::vector<Tensor>> tensor_res =
        primitive::clip_grad_vjp(
        x, grad_x_grad, min, max, stop_gradients);

    VLOG(6) << "Vjp prepare stop gradient of clip_double_grad";

    std::vector<std::vector<pir::OpResult>> res(tensor_res.size());
    for (size_t i = 0; i < tensor_res.size(); ++i) {
        res[i].resize(tensor_res[i].size());
        for (size_t j = 0; j < tensor_res[i].size(); ++j) {
            if(tensor_res[i][j].defined()){
                res[i][j] = std::static_pointer_cast<primitive::LazyTensor>(tensor_res[i][j].impl())->value().dyn_cast<pir::OpResult>();
            }
        }
    }
    return res;
}

std::vector<std::vector<pir::OpResult>> ConcatGradOp::Vjp(pir::Operation* op, const std::vector<std::vector<pir::Value>>& inputs_, const std::vector<std::vector<pir::Value>>& outputs, const std::vector<std::vector<pir::Value>>& out_grads, const std::vector<std::vector<bool>>& stop_gradients){

    PADDLE_ENFORCE_EQ(
      inputs_.size(),
      3,
      platform::errors::InvalidArgument("concat_grad op's inputs size should be 3, but now is %d.", inputs_.size()));
    PADDLE_ENFORCE_EQ(
      outputs.size(),
      1,
      platform::errors::InvalidArgument("concat_grad op's outputs size should be 1, but now is %d.", outputs.size()));

    VLOG(6) << "Prepare inputs of concat_double_grad";

    std::vector<Tensor> grad_x_grad;
    for (size_t idx = 0; idx < out_grads[0].size(); idx++) {
        grad_x_grad.emplace_back(
            std::make_shared<primitive::LazyTensor>(out_grads[0][idx]));
    }

    VLOG(6) << "Vjp prepare Prepare attributes of concat_double_grad";

    Tensor axis(std::make_shared<primitive::LazyTensor>(inputs_[2][0]));

    VLOG(6) << "Vjp prepare call concat_grad's vjp inteface";

    std::vector<std::vector<Tensor>> tensor_res =
        primitive::concat_grad_vjp(
        grad_x_grad, axis, stop_gradients);

    VLOG(6) << "Vjp prepare stop gradient of concat_double_grad";

    std::vector<std::vector<pir::OpResult>> res(tensor_res.size());
    for (size_t i = 0; i < tensor_res.size(); ++i) {
        res[i].resize(tensor_res[i].size());
        for (size_t j = 0; j < tensor_res[i].size(); ++j) {
            if(tensor_res[i][j].defined()){
                res[i][j] = std::static_pointer_cast<primitive::LazyTensor>(tensor_res[i][j].impl())->value().dyn_cast<pir::OpResult>();
            }
        }
    }
    return res;
}

std::vector<std::vector<pir::OpResult>> Conv2dGradOp::Vjp(pir::Operation* op, const std::vector<std::vector<pir::Value>>& inputs_, const std::vector<std::vector<pir::Value>>& outputs, const std::vector<std::vector<pir::Value>>& out_grads, const std::vector<std::vector<bool>>& stop_gradients){

    PADDLE_ENFORCE_EQ(
      inputs_.size(),
      3,
      platform::errors::InvalidArgument("conv2d_grad op's inputs size should be 3, but now is %d.", inputs_.size()));
    PADDLE_ENFORCE_EQ(
      outputs.size(),
      2,
      platform::errors::InvalidArgument("conv2d_grad op's outputs size should be 2, but now is %d.", outputs.size()));

    VLOG(6) << "Prepare inputs of conv2d_grad_grad";

    Tensor input(std::make_shared<primitive::LazyTensor>(inputs_[0][0]));
    Tensor filter(std::make_shared<primitive::LazyTensor>(inputs_[1][0]));
    Tensor grad_out(std::make_shared<primitive::LazyTensor>(inputs_[2][0]));
    paddle::optional<Tensor> grad_input_grad;
    if (!IsEmptyValue(out_grads[0][0])){
        grad_input_grad = paddle::make_optional<Tensor>(Tensor(std::make_shared<primitive::LazyTensor>(out_grads[0][0])));
    }
    paddle::optional<Tensor> grad_filter_grad;
    if (!IsEmptyValue(out_grads[1][0])){
        grad_filter_grad = paddle::make_optional<Tensor>(Tensor(std::make_shared<primitive::LazyTensor>(out_grads[1][0])));
    }

    VLOG(6) << "Vjp prepare Prepare attributes of conv2d_grad_grad";

    std::vector<int> strides;
    for (size_t i = 0; i < op->attribute("strides").dyn_cast<pir::ArrayAttribute>().size(); i++) {
        strides.push_back(op->attribute("strides").dyn_cast<pir::ArrayAttribute>().at(i).dyn_cast<pir::Int32Attribute>().data());
    }
    std::vector<int> paddings;
    for (size_t i = 0; i < op->attribute("paddings").dyn_cast<pir::ArrayAttribute>().size(); i++) {
        paddings.push_back(op->attribute("paddings").dyn_cast<pir::ArrayAttribute>().at(i).dyn_cast<pir::Int32Attribute>().data());
    }
    std::string padding_algorithm = op->attribute("padding_algorithm").dyn_cast<pir::StrAttribute>().AsString();
    std::vector<int> dilations;
    for (size_t i = 0; i < op->attribute("dilations").dyn_cast<pir::ArrayAttribute>().size(); i++) {
        dilations.push_back(op->attribute("dilations").dyn_cast<pir::ArrayAttribute>().at(i).dyn_cast<pir::Int32Attribute>().data());
    }
    int groups = op->attribute("groups").dyn_cast<pir::Int32Attribute>().data();
    std::string data_format = op->attribute("data_format").dyn_cast<pir::StrAttribute>().AsString();

    VLOG(6) << "Vjp prepare call conv2d_grad's vjp inteface";

    std::vector<std::vector<Tensor>> tensor_res =
        primitive::conv2d_grad_vjp(
        input, filter, grad_out, grad_input_grad, grad_filter_grad, strides, paddings, padding_algorithm, dilations, groups, data_format, stop_gradients);

    VLOG(6) << "Vjp prepare stop gradient of conv2d_grad_grad";

    std::vector<std::vector<pir::OpResult>> res(tensor_res.size());
    for (size_t i = 0; i < tensor_res.size(); ++i) {
        res[i].resize(tensor_res[i].size());
        for (size_t j = 0; j < tensor_res[i].size(); ++j) {
            if(tensor_res[i][j].defined()){
                res[i][j] = std::static_pointer_cast<primitive::LazyTensor>(tensor_res[i][j].impl())->value().dyn_cast<pir::OpResult>();
            }
        }
    }
    return res;
}

std::vector<std::vector<pir::OpResult>> Conv3dGradOp::Vjp(pir::Operation* op, const std::vector<std::vector<pir::Value>>& inputs_, const std::vector<std::vector<pir::Value>>& outputs, const std::vector<std::vector<pir::Value>>& out_grads, const std::vector<std::vector<bool>>& stop_gradients){

    PADDLE_ENFORCE_EQ(
      inputs_.size(),
      3,
      platform::errors::InvalidArgument("conv3d_grad op's inputs size should be 3, but now is %d.", inputs_.size()));
    PADDLE_ENFORCE_EQ(
      outputs.size(),
      2,
      platform::errors::InvalidArgument("conv3d_grad op's outputs size should be 2, but now is %d.", outputs.size()));

    VLOG(6) << "Prepare inputs of conv3d_double_grad";

    Tensor input(std::make_shared<primitive::LazyTensor>(inputs_[0][0]));
    Tensor filter(std::make_shared<primitive::LazyTensor>(inputs_[1][0]));
    Tensor grad_out(std::make_shared<primitive::LazyTensor>(inputs_[2][0]));
    paddle::optional<Tensor> grad_input_grad;
    if (!IsEmptyValue(out_grads[0][0])){
        grad_input_grad = paddle::make_optional<Tensor>(Tensor(std::make_shared<primitive::LazyTensor>(out_grads[0][0])));
    }
    paddle::optional<Tensor> grad_filter_grad;
    if (!IsEmptyValue(out_grads[1][0])){
        grad_filter_grad = paddle::make_optional<Tensor>(Tensor(std::make_shared<primitive::LazyTensor>(out_grads[1][0])));
    }

    VLOG(6) << "Vjp prepare Prepare attributes of conv3d_double_grad";

    std::vector<int> strides;
    for (size_t i = 0; i < op->attribute("strides").dyn_cast<pir::ArrayAttribute>().size(); i++) {
        strides.push_back(op->attribute("strides").dyn_cast<pir::ArrayAttribute>().at(i).dyn_cast<pir::Int32Attribute>().data());
    }
    std::vector<int> paddings;
    for (size_t i = 0; i < op->attribute("paddings").dyn_cast<pir::ArrayAttribute>().size(); i++) {
        paddings.push_back(op->attribute("paddings").dyn_cast<pir::ArrayAttribute>().at(i).dyn_cast<pir::Int32Attribute>().data());
    }
    std::string padding_algorithm = op->attribute("padding_algorithm").dyn_cast<pir::StrAttribute>().AsString();
    int groups = op->attribute("groups").dyn_cast<pir::Int32Attribute>().data();
    std::vector<int> dilations;
    for (size_t i = 0; i < op->attribute("dilations").dyn_cast<pir::ArrayAttribute>().size(); i++) {
        dilations.push_back(op->attribute("dilations").dyn_cast<pir::ArrayAttribute>().at(i).dyn_cast<pir::Int32Attribute>().data());
    }
    std::string data_format = op->attribute("data_format").dyn_cast<pir::StrAttribute>().AsString();

    VLOG(6) << "Vjp prepare call conv3d_grad's vjp inteface";

    std::vector<std::vector<Tensor>> tensor_res =
        primitive::conv3d_grad_vjp(
        input, filter, grad_out, grad_input_grad, grad_filter_grad, strides, paddings, padding_algorithm, groups, dilations, data_format, stop_gradients);

    VLOG(6) << "Vjp prepare stop gradient of conv3d_double_grad";

    std::vector<std::vector<pir::OpResult>> res(tensor_res.size());
    for (size_t i = 0; i < tensor_res.size(); ++i) {
        res[i].resize(tensor_res[i].size());
        for (size_t j = 0; j < tensor_res[i].size(); ++j) {
            if(tensor_res[i][j].defined()){
                res[i][j] = std::static_pointer_cast<primitive::LazyTensor>(tensor_res[i][j].impl())->value().dyn_cast<pir::OpResult>();
            }
        }
    }
    return res;
}

std::vector<std::vector<pir::OpResult>> CosDoubleGradOp::Vjp(pir::Operation* op, const std::vector<std::vector<pir::Value>>& inputs_, const std::vector<std::vector<pir::Value>>& outputs, const std::vector<std::vector<pir::Value>>& out_grads, const std::vector<std::vector<bool>>& stop_gradients){

    PADDLE_ENFORCE_EQ(
      inputs_.size(),
      3,
      platform::errors::InvalidArgument("cos_double_grad op's inputs size should be 3, but now is %d.", inputs_.size()));
    PADDLE_ENFORCE_EQ(
      outputs.size(),
      2,
      platform::errors::InvalidArgument("cos_double_grad op's outputs size should be 2, but now is %d.", outputs.size()));

    VLOG(6) << "Prepare inputs of cos_triple_grad";

    Tensor x(std::make_shared<primitive::LazyTensor>(inputs_[0][0]));
    paddle::optional<Tensor> grad_out_forward;
    if (!IsEmptyValue(inputs_[1][0])){
        grad_out_forward = paddle::make_optional<Tensor>(Tensor(std::make_shared<primitive::LazyTensor>(inputs_[1][0])));
    }
    paddle::optional<Tensor> grad_x_grad_forward;
    if (!IsEmptyValue(inputs_[2][0])){
        grad_x_grad_forward = paddle::make_optional<Tensor>(Tensor(std::make_shared<primitive::LazyTensor>(inputs_[2][0])));
    }
    Tensor grad_x_grad(std::make_shared<primitive::LazyTensor>(out_grads[0][0]));
    paddle::optional<Tensor> grad_out_grad_grad;
    if (!IsEmptyValue(out_grads[1][0])){
        grad_out_grad_grad = paddle::make_optional<Tensor>(Tensor(std::make_shared<primitive::LazyTensor>(out_grads[1][0])));
    }

    VLOG(6) << "Vjp prepare Prepare attributes of cos_triple_grad";


    VLOG(6) << "Vjp prepare call cos_double_grad's vjp inteface";

    std::vector<std::vector<Tensor>> tensor_res =
        primitive::cos_double_grad_vjp(
        x, grad_out_forward, grad_x_grad_forward, grad_x_grad, grad_out_grad_grad, stop_gradients);

    VLOG(6) << "Vjp prepare stop gradient of cos_triple_grad";

    std::vector<std::vector<pir::OpResult>> res(tensor_res.size());
    for (size_t i = 0; i < tensor_res.size(); ++i) {
        res[i].resize(tensor_res[i].size());
        for (size_t j = 0; j < tensor_res[i].size(); ++j) {
            if(tensor_res[i][j].defined()){
                res[i][j] = std::static_pointer_cast<primitive::LazyTensor>(tensor_res[i][j].impl())->value().dyn_cast<pir::OpResult>();
            }
        }
    }
    return res;
}

std::vector<std::vector<pir::OpResult>> CosDoubleGrad_Op::Vjp(pir::Operation* op, const std::vector<std::vector<pir::Value>>& inputs_, const std::vector<std::vector<pir::Value>>& outputs, const std::vector<std::vector<pir::Value>>& out_grads, const std::vector<std::vector<bool>>& stop_gradients){

    PADDLE_ENFORCE_EQ(
      inputs_.size(),
      3,
      platform::errors::InvalidArgument("cos_double_grad op's inputs size should be 3, but now is %d.", inputs_.size()));
    PADDLE_ENFORCE_EQ(
      outputs.size(),
      2,
      platform::errors::InvalidArgument("cos_double_grad op's outputs size should be 2, but now is %d.", outputs.size()));

    VLOG(6) << "Prepare inputs of cos_triple_grad";

    Tensor x(std::make_shared<primitive::LazyTensor>(inputs_[0][0]));
    paddle::optional<Tensor> grad_out_forward;
    if (!IsEmptyValue(inputs_[1][0])){
        grad_out_forward = paddle::make_optional<Tensor>(Tensor(std::make_shared<primitive::LazyTensor>(inputs_[1][0])));
    }
    paddle::optional<Tensor> grad_x_grad_forward;
    if (!IsEmptyValue(inputs_[2][0])){
        grad_x_grad_forward = paddle::make_optional<Tensor>(Tensor(std::make_shared<primitive::LazyTensor>(inputs_[2][0])));
    }
    Tensor grad_x_grad(std::make_shared<primitive::LazyTensor>(out_grads[0][0]));
    paddle::optional<Tensor> grad_out_grad_grad;
    if (!IsEmptyValue(out_grads[1][0])){
        grad_out_grad_grad = paddle::make_optional<Tensor>(Tensor(std::make_shared<primitive::LazyTensor>(out_grads[1][0])));
    }

    VLOG(6) << "Vjp prepare Prepare attributes of cos_triple_grad";


    VLOG(6) << "Vjp prepare call cos_double_grad_'s vjp inteface";

    std::vector<std::vector<Tensor>> tensor_res =
        primitive::cos_double_grad_vjp(
        x, grad_out_forward, grad_x_grad_forward, grad_x_grad, grad_out_grad_grad, stop_gradients);

    VLOG(6) << "Vjp prepare stop gradient of cos_triple_grad";

    std::vector<std::vector<pir::OpResult>> res(tensor_res.size());
    for (size_t i = 0; i < tensor_res.size(); ++i) {
        res[i].resize(tensor_res[i].size());
        for (size_t j = 0; j < tensor_res[i].size(); ++j) {
            if(tensor_res[i][j].defined()){
                res[i][j] = std::static_pointer_cast<primitive::LazyTensor>(tensor_res[i][j].impl())->value().dyn_cast<pir::OpResult>();
            }
        }
    }
    return res;
}

std::vector<std::vector<pir::OpResult>> CosGradOp::Vjp(pir::Operation* op, const std::vector<std::vector<pir::Value>>& inputs_, const std::vector<std::vector<pir::Value>>& outputs, const std::vector<std::vector<pir::Value>>& out_grads, const std::vector<std::vector<bool>>& stop_gradients){

    PADDLE_ENFORCE_EQ(
      inputs_.size(),
      2,
      platform::errors::InvalidArgument("cos_grad op's inputs size should be 2, but now is %d.", inputs_.size()));
    PADDLE_ENFORCE_EQ(
      outputs.size(),
      1,
      platform::errors::InvalidArgument("cos_grad op's outputs size should be 1, but now is %d.", outputs.size()));

    VLOG(6) << "Prepare inputs of cos_double_grad";

    Tensor x(std::make_shared<primitive::LazyTensor>(inputs_[0][0]));
    paddle::optional<Tensor> grad_out;
    if (!IsEmptyValue(inputs_[1][0])){
        grad_out = paddle::make_optional<Tensor>(Tensor(std::make_shared<primitive::LazyTensor>(inputs_[1][0])));
    }
    Tensor grad_x_grad(std::make_shared<primitive::LazyTensor>(out_grads[0][0]));

    VLOG(6) << "Vjp prepare Prepare attributes of cos_double_grad";


    VLOG(6) << "Vjp prepare call cos_grad's vjp inteface";

    std::vector<std::vector<Tensor>> tensor_res =
        primitive::cos_grad_vjp(
        x, grad_out, grad_x_grad, stop_gradients);

    VLOG(6) << "Vjp prepare stop gradient of cos_double_grad";

    std::vector<std::vector<pir::OpResult>> res(tensor_res.size());
    for (size_t i = 0; i < tensor_res.size(); ++i) {
        res[i].resize(tensor_res[i].size());
        for (size_t j = 0; j < tensor_res[i].size(); ++j) {
            if(tensor_res[i][j].defined()){
                res[i][j] = std::static_pointer_cast<primitive::LazyTensor>(tensor_res[i][j].impl())->value().dyn_cast<pir::OpResult>();
            }
        }
    }
    return res;
}

std::vector<std::vector<pir::OpResult>> CosGrad_Op::Vjp(pir::Operation* op, const std::vector<std::vector<pir::Value>>& inputs_, const std::vector<std::vector<pir::Value>>& outputs, const std::vector<std::vector<pir::Value>>& out_grads, const std::vector<std::vector<bool>>& stop_gradients){

    PADDLE_ENFORCE_EQ(
      inputs_.size(),
      2,
      platform::errors::InvalidArgument("cos_grad op's inputs size should be 2, but now is %d.", inputs_.size()));
    PADDLE_ENFORCE_EQ(
      outputs.size(),
      1,
      platform::errors::InvalidArgument("cos_grad op's outputs size should be 1, but now is %d.", outputs.size()));

    VLOG(6) << "Prepare inputs of cos_double_grad";

    Tensor x(std::make_shared<primitive::LazyTensor>(inputs_[0][0]));
    paddle::optional<Tensor> grad_out;
    if (!IsEmptyValue(inputs_[1][0])){
        grad_out = paddle::make_optional<Tensor>(Tensor(std::make_shared<primitive::LazyTensor>(inputs_[1][0])));
    }
    Tensor grad_x_grad(std::make_shared<primitive::LazyTensor>(out_grads[0][0]));

    VLOG(6) << "Vjp prepare Prepare attributes of cos_double_grad";


    VLOG(6) << "Vjp prepare call cos_grad_'s vjp inteface";

    std::vector<std::vector<Tensor>> tensor_res =
        primitive::cos_grad_vjp(
        x, grad_out, grad_x_grad, stop_gradients);

    VLOG(6) << "Vjp prepare stop gradient of cos_double_grad";

    std::vector<std::vector<pir::OpResult>> res(tensor_res.size());
    for (size_t i = 0; i < tensor_res.size(); ++i) {
        res[i].resize(tensor_res[i].size());
        for (size_t j = 0; j < tensor_res[i].size(); ++j) {
            if(tensor_res[i][j].defined()){
                res[i][j] = std::static_pointer_cast<primitive::LazyTensor>(tensor_res[i][j].impl())->value().dyn_cast<pir::OpResult>();
            }
        }
    }
    return res;
}

std::vector<std::vector<pir::OpResult>> DepthwiseConv2dGradOp::Vjp(pir::Operation* op, const std::vector<std::vector<pir::Value>>& inputs_, const std::vector<std::vector<pir::Value>>& outputs, const std::vector<std::vector<pir::Value>>& out_grads, const std::vector<std::vector<bool>>& stop_gradients){

    PADDLE_ENFORCE_EQ(
      inputs_.size(),
      3,
      platform::errors::InvalidArgument("depthwise_conv2d_grad op's inputs size should be 3, but now is %d.", inputs_.size()));
    PADDLE_ENFORCE_EQ(
      outputs.size(),
      2,
      platform::errors::InvalidArgument("depthwise_conv2d_grad op's outputs size should be 2, but now is %d.", outputs.size()));

    VLOG(6) << "Prepare inputs of depthwise_conv2d_double_grad";

    Tensor input(std::make_shared<primitive::LazyTensor>(inputs_[0][0]));
    Tensor filter(std::make_shared<primitive::LazyTensor>(inputs_[1][0]));
    Tensor grad_out(std::make_shared<primitive::LazyTensor>(inputs_[2][0]));
    paddle::optional<Tensor> grad_input_grad;
    if (!IsEmptyValue(out_grads[0][0])){
        grad_input_grad = paddle::make_optional<Tensor>(Tensor(std::make_shared<primitive::LazyTensor>(out_grads[0][0])));
    }
    paddle::optional<Tensor> grad_filter_grad;
    if (!IsEmptyValue(out_grads[1][0])){
        grad_filter_grad = paddle::make_optional<Tensor>(Tensor(std::make_shared<primitive::LazyTensor>(out_grads[1][0])));
    }

    VLOG(6) << "Vjp prepare Prepare attributes of depthwise_conv2d_double_grad";

    std::vector<int> strides;
    for (size_t i = 0; i < op->attribute("strides").dyn_cast<pir::ArrayAttribute>().size(); i++) {
        strides.push_back(op->attribute("strides").dyn_cast<pir::ArrayAttribute>().at(i).dyn_cast<pir::Int32Attribute>().data());
    }
    std::vector<int> paddings;
    for (size_t i = 0; i < op->attribute("paddings").dyn_cast<pir::ArrayAttribute>().size(); i++) {
        paddings.push_back(op->attribute("paddings").dyn_cast<pir::ArrayAttribute>().at(i).dyn_cast<pir::Int32Attribute>().data());
    }
    std::string padding_algorithm = op->attribute("padding_algorithm").dyn_cast<pir::StrAttribute>().AsString();
    int groups = op->attribute("groups").dyn_cast<pir::Int32Attribute>().data();
    std::vector<int> dilations;
    for (size_t i = 0; i < op->attribute("dilations").dyn_cast<pir::ArrayAttribute>().size(); i++) {
        dilations.push_back(op->attribute("dilations").dyn_cast<pir::ArrayAttribute>().at(i).dyn_cast<pir::Int32Attribute>().data());
    }
    std::string data_format = op->attribute("data_format").dyn_cast<pir::StrAttribute>().AsString();

    VLOG(6) << "Vjp prepare call depthwise_conv2d_grad's vjp inteface";

    std::vector<std::vector<Tensor>> tensor_res =
        primitive::depthwise_conv2d_grad_vjp(
        input, filter, grad_out, grad_input_grad, grad_filter_grad, strides, paddings, padding_algorithm, groups, dilations, data_format, stop_gradients);

    VLOG(6) << "Vjp prepare stop gradient of depthwise_conv2d_double_grad";

    std::vector<std::vector<pir::OpResult>> res(tensor_res.size());
    for (size_t i = 0; i < tensor_res.size(); ++i) {
        res[i].resize(tensor_res[i].size());
        for (size_t j = 0; j < tensor_res[i].size(); ++j) {
            if(tensor_res[i][j].defined()){
                res[i][j] = std::static_pointer_cast<primitive::LazyTensor>(tensor_res[i][j].impl())->value().dyn_cast<pir::OpResult>();
            }
        }
    }
    return res;
}

std::vector<std::vector<pir::OpResult>> EluGradOp::Vjp(pir::Operation* op, const std::vector<std::vector<pir::Value>>& inputs_, const std::vector<std::vector<pir::Value>>& outputs, const std::vector<std::vector<pir::Value>>& out_grads, const std::vector<std::vector<bool>>& stop_gradients){

    PADDLE_ENFORCE_EQ(
      inputs_.size(),
      3,
      platform::errors::InvalidArgument("elu_grad op's inputs size should be 3, but now is %d.", inputs_.size()));
    PADDLE_ENFORCE_EQ(
      outputs.size(),
      1,
      platform::errors::InvalidArgument("elu_grad op's outputs size should be 1, but now is %d.", outputs.size()));

    VLOG(6) << "Prepare inputs of elu_double_grad";

    Tensor x(std::make_shared<primitive::LazyTensor>(inputs_[0][0]));
    Tensor grad_out(std::make_shared<primitive::LazyTensor>(inputs_[2][0]));
    Tensor grad_x_grad(std::make_shared<primitive::LazyTensor>(out_grads[0][0]));

    VLOG(6) << "Vjp prepare Prepare attributes of elu_double_grad";

    float alpha = op->attribute("alpha").dyn_cast<pir::FloatAttribute>().data();

    VLOG(6) << "Vjp prepare call elu_grad's vjp inteface";

    std::vector<std::vector<Tensor>> tensor_res =
        primitive::elu_grad_vjp(
        x, grad_out, grad_x_grad, alpha, stop_gradients);

    VLOG(6) << "Vjp prepare stop gradient of elu_double_grad";

    std::vector<std::vector<pir::OpResult>> res(tensor_res.size());
    for (size_t i = 0; i < tensor_res.size(); ++i) {
        res[i].resize(tensor_res[i].size());
        for (size_t j = 0; j < tensor_res[i].size(); ++j) {
            if(tensor_res[i][j].defined()){
                res[i][j] = std::static_pointer_cast<primitive::LazyTensor>(tensor_res[i][j].impl())->value().dyn_cast<pir::OpResult>();
            }
        }
    }
    return res;
}

std::vector<std::vector<pir::OpResult>> EluGrad_Op::Vjp(pir::Operation* op, const std::vector<std::vector<pir::Value>>& inputs_, const std::vector<std::vector<pir::Value>>& outputs, const std::vector<std::vector<pir::Value>>& out_grads, const std::vector<std::vector<bool>>& stop_gradients){

    PADDLE_ENFORCE_EQ(
      inputs_.size(),
      3,
      platform::errors::InvalidArgument("elu_grad op's inputs size should be 3, but now is %d.", inputs_.size()));
    PADDLE_ENFORCE_EQ(
      outputs.size(),
      1,
      platform::errors::InvalidArgument("elu_grad op's outputs size should be 1, but now is %d.", outputs.size()));

    VLOG(6) << "Prepare inputs of elu_double_grad";

    Tensor x(std::make_shared<primitive::LazyTensor>(inputs_[0][0]));
    Tensor grad_out(std::make_shared<primitive::LazyTensor>(inputs_[2][0]));
    Tensor grad_x_grad(std::make_shared<primitive::LazyTensor>(out_grads[0][0]));

    VLOG(6) << "Vjp prepare Prepare attributes of elu_double_grad";

    float alpha = op->attribute("alpha").dyn_cast<pir::FloatAttribute>().data();

    VLOG(6) << "Vjp prepare call elu_grad_'s vjp inteface";

    std::vector<std::vector<Tensor>> tensor_res =
        primitive::elu_grad_vjp(
        x, grad_out, grad_x_grad, alpha, stop_gradients);

    VLOG(6) << "Vjp prepare stop gradient of elu_double_grad";

    std::vector<std::vector<pir::OpResult>> res(tensor_res.size());
    for (size_t i = 0; i < tensor_res.size(); ++i) {
        res[i].resize(tensor_res[i].size());
        for (size_t j = 0; j < tensor_res[i].size(); ++j) {
            if(tensor_res[i][j].defined()){
                res[i][j] = std::static_pointer_cast<primitive::LazyTensor>(tensor_res[i][j].impl())->value().dyn_cast<pir::OpResult>();
            }
        }
    }
    return res;
}

std::vector<std::vector<pir::OpResult>> ExpandGradOp::Vjp(pir::Operation* op, const std::vector<std::vector<pir::Value>>& inputs_, const std::vector<std::vector<pir::Value>>& outputs, const std::vector<std::vector<pir::Value>>& out_grads, const std::vector<std::vector<bool>>& stop_gradients){

    PADDLE_ENFORCE_EQ(
      inputs_.size(),
      3,
      platform::errors::InvalidArgument("expand_grad op's inputs size should be 3, but now is %d.", inputs_.size()));
    PADDLE_ENFORCE_EQ(
      outputs.size(),
      1,
      platform::errors::InvalidArgument("expand_grad op's outputs size should be 1, but now is %d.", outputs.size()));

    VLOG(6) << "Prepare inputs of expand_double_grad";

    Tensor grad_x_grad(std::make_shared<primitive::LazyTensor>(out_grads[0][0]));

    VLOG(6) << "Vjp prepare Prepare attributes of expand_double_grad";

    Tensor shape(std::make_shared<primitive::LazyTensor>(inputs_[2][0]));

    VLOG(6) << "Vjp prepare call expand_grad's vjp inteface";

    std::vector<std::vector<Tensor>> tensor_res =
        primitive::expand_grad_vjp(
        grad_x_grad, shape, stop_gradients);

    VLOG(6) << "Vjp prepare stop gradient of expand_double_grad";

    std::vector<std::vector<pir::OpResult>> res(tensor_res.size());
    for (size_t i = 0; i < tensor_res.size(); ++i) {
        res[i].resize(tensor_res[i].size());
        for (size_t j = 0; j < tensor_res[i].size(); ++j) {
            if(tensor_res[i][j].defined()){
                res[i][j] = std::static_pointer_cast<primitive::LazyTensor>(tensor_res[i][j].impl())->value().dyn_cast<pir::OpResult>();
            }
        }
    }
    return res;
}

std::vector<std::vector<pir::OpResult>> InstanceNormGradOp::Vjp(pir::Operation* op, const std::vector<std::vector<pir::Value>>& inputs_, const std::vector<std::vector<pir::Value>>& outputs, const std::vector<std::vector<pir::Value>>& out_grads, const std::vector<std::vector<bool>>& stop_gradients){

    PADDLE_ENFORCE_EQ(
      inputs_.size(),
      5,
      platform::errors::InvalidArgument("instance_norm_grad op's inputs size should be 5, but now is %d.", inputs_.size()));
    PADDLE_ENFORCE_EQ(
      outputs.size(),
      3,
      platform::errors::InvalidArgument("instance_norm_grad op's outputs size should be 3, but now is %d.", outputs.size()));

    VLOG(6) << "Prepare inputs of instance_norm_double_grad";

    Tensor x(std::make_shared<primitive::LazyTensor>(inputs_[0][0]));
    paddle::optional<Tensor> fwd_scale;
    if (!IsEmptyValue(inputs_[1][0])){
        fwd_scale = paddle::make_optional<Tensor>(Tensor(std::make_shared<primitive::LazyTensor>(inputs_[1][0])));
    }
    Tensor saved_mean(std::make_shared<primitive::LazyTensor>(inputs_[2][0]));
    Tensor saved_variance(std::make_shared<primitive::LazyTensor>(inputs_[3][0]));
    Tensor grad_y(std::make_shared<primitive::LazyTensor>(inputs_[4][0]));
    paddle::optional<Tensor> grad_x_grad;
    if (!IsEmptyValue(out_grads[0][0])){
        grad_x_grad = paddle::make_optional<Tensor>(Tensor(std::make_shared<primitive::LazyTensor>(out_grads[0][0])));
    }
    paddle::optional<Tensor> grad_scale_grad;
    if (!IsEmptyValue(out_grads[1][0])){
        grad_scale_grad = paddle::make_optional<Tensor>(Tensor(std::make_shared<primitive::LazyTensor>(out_grads[1][0])));
    }
    paddle::optional<Tensor> grad_bias_grad;
    if (!IsEmptyValue(out_grads[2][0])){
        grad_bias_grad = paddle::make_optional<Tensor>(Tensor(std::make_shared<primitive::LazyTensor>(out_grads[2][0])));
    }

    VLOG(6) << "Vjp prepare Prepare attributes of instance_norm_double_grad";

    float epsilon = op->attribute("epsilon").dyn_cast<pir::FloatAttribute>().data();

    VLOG(6) << "Vjp prepare call instance_norm_grad's vjp inteface";

    std::vector<std::vector<Tensor>> tensor_res =
        primitive::instance_norm_grad_vjp(
        x, fwd_scale, saved_mean, saved_variance, grad_y, grad_x_grad, grad_scale_grad, grad_bias_grad, epsilon, stop_gradients);

    VLOG(6) << "Vjp prepare stop gradient of instance_norm_double_grad";

    std::vector<std::vector<pir::OpResult>> res(tensor_res.size());
    for (size_t i = 0; i < tensor_res.size(); ++i) {
        res[i].resize(tensor_res[i].size());
        for (size_t j = 0; j < tensor_res[i].size(); ++j) {
            if(tensor_res[i][j].defined()){
                res[i][j] = std::static_pointer_cast<primitive::LazyTensor>(tensor_res[i][j].impl())->value().dyn_cast<pir::OpResult>();
            }
        }
    }
    return res;
}

std::vector<std::vector<pir::OpResult>> LeakyReluGradOp::Vjp(pir::Operation* op, const std::vector<std::vector<pir::Value>>& inputs_, const std::vector<std::vector<pir::Value>>& outputs, const std::vector<std::vector<pir::Value>>& out_grads, const std::vector<std::vector<bool>>& stop_gradients){

    PADDLE_ENFORCE_EQ(
      inputs_.size(),
      2,
      platform::errors::InvalidArgument("leaky_relu_grad op's inputs size should be 2, but now is %d.", inputs_.size()));
    PADDLE_ENFORCE_EQ(
      outputs.size(),
      1,
      platform::errors::InvalidArgument("leaky_relu_grad op's outputs size should be 1, but now is %d.", outputs.size()));

    VLOG(6) << "Prepare inputs of leaky_relu_double_grad";

    Tensor x(std::make_shared<primitive::LazyTensor>(inputs_[0][0]));
    Tensor grad_x_grad(std::make_shared<primitive::LazyTensor>(out_grads[0][0]));

    VLOG(6) << "Vjp prepare Prepare attributes of leaky_relu_double_grad";

    float negative_slope = op->attribute("negative_slope").dyn_cast<pir::FloatAttribute>().data();

    VLOG(6) << "Vjp prepare call leaky_relu_grad's vjp inteface";

    std::vector<std::vector<Tensor>> tensor_res =
        primitive::leaky_relu_grad_vjp(
        x, grad_x_grad, negative_slope, stop_gradients);

    VLOG(6) << "Vjp prepare stop gradient of leaky_relu_double_grad";

    std::vector<std::vector<pir::OpResult>> res(tensor_res.size());
    for (size_t i = 0; i < tensor_res.size(); ++i) {
        res[i].resize(tensor_res[i].size());
        for (size_t j = 0; j < tensor_res[i].size(); ++j) {
            if(tensor_res[i][j].defined()){
                res[i][j] = std::static_pointer_cast<primitive::LazyTensor>(tensor_res[i][j].impl())->value().dyn_cast<pir::OpResult>();
            }
        }
    }
    return res;
}

std::vector<std::vector<pir::OpResult>> LeakyReluGrad_Op::Vjp(pir::Operation* op, const std::vector<std::vector<pir::Value>>& inputs_, const std::vector<std::vector<pir::Value>>& outputs, const std::vector<std::vector<pir::Value>>& out_grads, const std::vector<std::vector<bool>>& stop_gradients){

    PADDLE_ENFORCE_EQ(
      inputs_.size(),
      2,
      platform::errors::InvalidArgument("leaky_relu_grad op's inputs size should be 2, but now is %d.", inputs_.size()));
    PADDLE_ENFORCE_EQ(
      outputs.size(),
      1,
      platform::errors::InvalidArgument("leaky_relu_grad op's outputs size should be 1, but now is %d.", outputs.size()));

    VLOG(6) << "Prepare inputs of leaky_relu_double_grad";

    Tensor x(std::make_shared<primitive::LazyTensor>(inputs_[0][0]));
    Tensor grad_x_grad(std::make_shared<primitive::LazyTensor>(out_grads[0][0]));

    VLOG(6) << "Vjp prepare Prepare attributes of leaky_relu_double_grad";

    float negative_slope = op->attribute("negative_slope").dyn_cast<pir::FloatAttribute>().data();

    VLOG(6) << "Vjp prepare call leaky_relu_grad_'s vjp inteface";

    std::vector<std::vector<Tensor>> tensor_res =
        primitive::leaky_relu_grad_vjp(
        x, grad_x_grad, negative_slope, stop_gradients);

    VLOG(6) << "Vjp prepare stop gradient of leaky_relu_double_grad";

    std::vector<std::vector<pir::OpResult>> res(tensor_res.size());
    for (size_t i = 0; i < tensor_res.size(); ++i) {
        res[i].resize(tensor_res[i].size());
        for (size_t j = 0; j < tensor_res[i].size(); ++j) {
            if(tensor_res[i][j].defined()){
                res[i][j] = std::static_pointer_cast<primitive::LazyTensor>(tensor_res[i][j].impl())->value().dyn_cast<pir::OpResult>();
            }
        }
    }
    return res;
}

std::vector<std::vector<pir::OpResult>> LogGradOp::Vjp(pir::Operation* op, const std::vector<std::vector<pir::Value>>& inputs_, const std::vector<std::vector<pir::Value>>& outputs, const std::vector<std::vector<pir::Value>>& out_grads, const std::vector<std::vector<bool>>& stop_gradients){

    PADDLE_ENFORCE_EQ(
      inputs_.size(),
      2,
      platform::errors::InvalidArgument("log_grad op's inputs size should be 2, but now is %d.", inputs_.size()));
    PADDLE_ENFORCE_EQ(
      outputs.size(),
      1,
      platform::errors::InvalidArgument("log_grad op's outputs size should be 1, but now is %d.", outputs.size()));

    VLOG(6) << "Prepare inputs of log_double_grad";

    Tensor x(std::make_shared<primitive::LazyTensor>(inputs_[0][0]));
    Tensor grad_out(std::make_shared<primitive::LazyTensor>(inputs_[1][0]));
    Tensor grad_x_grad(std::make_shared<primitive::LazyTensor>(out_grads[0][0]));

    VLOG(6) << "Vjp prepare Prepare attributes of log_double_grad";


    VLOG(6) << "Vjp prepare call log_grad's vjp inteface";

    std::vector<std::vector<Tensor>> tensor_res =
        primitive::log_grad_vjp(
        x, grad_out, grad_x_grad, stop_gradients);

    VLOG(6) << "Vjp prepare stop gradient of log_double_grad";

    std::vector<std::vector<pir::OpResult>> res(tensor_res.size());
    for (size_t i = 0; i < tensor_res.size(); ++i) {
        res[i].resize(tensor_res[i].size());
        for (size_t j = 0; j < tensor_res[i].size(); ++j) {
            if(tensor_res[i][j].defined()){
                res[i][j] = std::static_pointer_cast<primitive::LazyTensor>(tensor_res[i][j].impl())->value().dyn_cast<pir::OpResult>();
            }
        }
    }
    return res;
}

std::vector<std::vector<pir::OpResult>> LogGrad_Op::Vjp(pir::Operation* op, const std::vector<std::vector<pir::Value>>& inputs_, const std::vector<std::vector<pir::Value>>& outputs, const std::vector<std::vector<pir::Value>>& out_grads, const std::vector<std::vector<bool>>& stop_gradients){

    PADDLE_ENFORCE_EQ(
      inputs_.size(),
      2,
      platform::errors::InvalidArgument("log_grad op's inputs size should be 2, but now is %d.", inputs_.size()));
    PADDLE_ENFORCE_EQ(
      outputs.size(),
      1,
      platform::errors::InvalidArgument("log_grad op's outputs size should be 1, but now is %d.", outputs.size()));

    VLOG(6) << "Prepare inputs of log_double_grad";

    Tensor x(std::make_shared<primitive::LazyTensor>(inputs_[0][0]));
    Tensor grad_out(std::make_shared<primitive::LazyTensor>(inputs_[1][0]));
    Tensor grad_x_grad(std::make_shared<primitive::LazyTensor>(out_grads[0][0]));

    VLOG(6) << "Vjp prepare Prepare attributes of log_double_grad";


    VLOG(6) << "Vjp prepare call log_grad_'s vjp inteface";

    std::vector<std::vector<Tensor>> tensor_res =
        primitive::log_grad_vjp(
        x, grad_out, grad_x_grad, stop_gradients);

    VLOG(6) << "Vjp prepare stop gradient of log_double_grad";

    std::vector<std::vector<pir::OpResult>> res(tensor_res.size());
    for (size_t i = 0; i < tensor_res.size(); ++i) {
        res[i].resize(tensor_res[i].size());
        for (size_t j = 0; j < tensor_res[i].size(); ++j) {
            if(tensor_res[i][j].defined()){
                res[i][j] = std::static_pointer_cast<primitive::LazyTensor>(tensor_res[i][j].impl())->value().dyn_cast<pir::OpResult>();
            }
        }
    }
    return res;
}

std::vector<std::vector<pir::OpResult>> Pad3dGradOp::Vjp(pir::Operation* op, const std::vector<std::vector<pir::Value>>& inputs_, const std::vector<std::vector<pir::Value>>& outputs, const std::vector<std::vector<pir::Value>>& out_grads, const std::vector<std::vector<bool>>& stop_gradients){

    PADDLE_ENFORCE_EQ(
      inputs_.size(),
      3,
      platform::errors::InvalidArgument("pad3d_grad op's inputs size should be 3, but now is %d.", inputs_.size()));
    PADDLE_ENFORCE_EQ(
      outputs.size(),
      1,
      platform::errors::InvalidArgument("pad3d_grad op's outputs size should be 1, but now is %d.", outputs.size()));

    VLOG(6) << "Prepare inputs of pad3d_double_grad";

    Tensor grad_x_grad(std::make_shared<primitive::LazyTensor>(out_grads[0][0]));

    VLOG(6) << "Vjp prepare Prepare attributes of pad3d_double_grad";

    Tensor paddings(std::make_shared<primitive::LazyTensor>(inputs_[2][0]));
    std::string mode = op->attribute("mode").dyn_cast<pir::StrAttribute>().AsString();
    float pad_value = op->attribute("pad_value").dyn_cast<pir::FloatAttribute>().data();
    std::string data_format = op->attribute("data_format").dyn_cast<pir::StrAttribute>().AsString();

    VLOG(6) << "Vjp prepare call pad3d_grad's vjp inteface";

    std::vector<std::vector<Tensor>> tensor_res =
        primitive::pad3d_grad_vjp(
        grad_x_grad, paddings, mode, pad_value, data_format, stop_gradients);

    VLOG(6) << "Vjp prepare stop gradient of pad3d_double_grad";

    std::vector<std::vector<pir::OpResult>> res(tensor_res.size());
    for (size_t i = 0; i < tensor_res.size(); ++i) {
        res[i].resize(tensor_res[i].size());
        for (size_t j = 0; j < tensor_res[i].size(); ++j) {
            if(tensor_res[i][j].defined()){
                res[i][j] = std::static_pointer_cast<primitive::LazyTensor>(tensor_res[i][j].impl())->value().dyn_cast<pir::OpResult>();
            }
        }
    }
    return res;
}

std::vector<std::vector<pir::OpResult>> PowDoubleGradOp::Vjp(pir::Operation* op, const std::vector<std::vector<pir::Value>>& inputs_, const std::vector<std::vector<pir::Value>>& outputs, const std::vector<std::vector<pir::Value>>& out_grads, const std::vector<std::vector<bool>>& stop_gradients){

    PADDLE_ENFORCE_EQ(
      inputs_.size(),
      3,
      platform::errors::InvalidArgument("pow_double_grad op's inputs size should be 3, but now is %d.", inputs_.size()));
    PADDLE_ENFORCE_EQ(
      outputs.size(),
      2,
      platform::errors::InvalidArgument("pow_double_grad op's outputs size should be 2, but now is %d.", outputs.size()));

    VLOG(6) << "Prepare inputs of pow_triple_grad";

    Tensor x(std::make_shared<primitive::LazyTensor>(inputs_[0][0]));
    Tensor grad_out(std::make_shared<primitive::LazyTensor>(inputs_[1][0]));
    Tensor grad_grad_x(std::make_shared<primitive::LazyTensor>(inputs_[2][0]));
    Tensor grad_x_grad(std::make_shared<primitive::LazyTensor>(out_grads[0][0]));
    paddle::optional<Tensor> grad_grad_out_grad;
    if (!IsEmptyValue(out_grads[1][0])){
        grad_grad_out_grad = paddle::make_optional<Tensor>(Tensor(std::make_shared<primitive::LazyTensor>(out_grads[1][0])));
    }

    VLOG(6) << "Vjp prepare Prepare attributes of pow_triple_grad";

    phi::Scalar y = op->attribute("y").dyn_cast<paddle::dialect::ScalarAttribute>().data();

    VLOG(6) << "Vjp prepare call pow_double_grad's vjp inteface";

    std::vector<std::vector<Tensor>> tensor_res =
        primitive::pow_double_grad_vjp(
        x, grad_out, grad_grad_x, grad_x_grad, grad_grad_out_grad, y, stop_gradients);

    VLOG(6) << "Vjp prepare stop gradient of pow_triple_grad";

    std::vector<std::vector<pir::OpResult>> res(tensor_res.size());
    for (size_t i = 0; i < tensor_res.size(); ++i) {
        res[i].resize(tensor_res[i].size());
        for (size_t j = 0; j < tensor_res[i].size(); ++j) {
            if(tensor_res[i][j].defined()){
                res[i][j] = std::static_pointer_cast<primitive::LazyTensor>(tensor_res[i][j].impl())->value().dyn_cast<pir::OpResult>();
            }
        }
    }
    return res;
}

std::vector<std::vector<pir::OpResult>> PowDoubleGrad_Op::Vjp(pir::Operation* op, const std::vector<std::vector<pir::Value>>& inputs_, const std::vector<std::vector<pir::Value>>& outputs, const std::vector<std::vector<pir::Value>>& out_grads, const std::vector<std::vector<bool>>& stop_gradients){

    PADDLE_ENFORCE_EQ(
      inputs_.size(),
      3,
      platform::errors::InvalidArgument("pow_double_grad op's inputs size should be 3, but now is %d.", inputs_.size()));
    PADDLE_ENFORCE_EQ(
      outputs.size(),
      2,
      platform::errors::InvalidArgument("pow_double_grad op's outputs size should be 2, but now is %d.", outputs.size()));

    VLOG(6) << "Prepare inputs of pow_triple_grad";

    Tensor x(std::make_shared<primitive::LazyTensor>(inputs_[0][0]));
    Tensor grad_out(std::make_shared<primitive::LazyTensor>(inputs_[1][0]));
    Tensor grad_grad_x(std::make_shared<primitive::LazyTensor>(inputs_[2][0]));
    Tensor grad_x_grad(std::make_shared<primitive::LazyTensor>(out_grads[0][0]));
    paddle::optional<Tensor> grad_grad_out_grad;
    if (!IsEmptyValue(out_grads[1][0])){
        grad_grad_out_grad = paddle::make_optional<Tensor>(Tensor(std::make_shared<primitive::LazyTensor>(out_grads[1][0])));
    }

    VLOG(6) << "Vjp prepare Prepare attributes of pow_triple_grad";

    phi::Scalar y = op->attribute("y").dyn_cast<paddle::dialect::ScalarAttribute>().data();

    VLOG(6) << "Vjp prepare call pow_double_grad_'s vjp inteface";

    std::vector<std::vector<Tensor>> tensor_res =
        primitive::pow_double_grad_vjp(
        x, grad_out, grad_grad_x, grad_x_grad, grad_grad_out_grad, y, stop_gradients);

    VLOG(6) << "Vjp prepare stop gradient of pow_triple_grad";

    std::vector<std::vector<pir::OpResult>> res(tensor_res.size());
    for (size_t i = 0; i < tensor_res.size(); ++i) {
        res[i].resize(tensor_res[i].size());
        for (size_t j = 0; j < tensor_res[i].size(); ++j) {
            if(tensor_res[i][j].defined()){
                res[i][j] = std::static_pointer_cast<primitive::LazyTensor>(tensor_res[i][j].impl())->value().dyn_cast<pir::OpResult>();
            }
        }
    }
    return res;
}

std::vector<std::vector<pir::OpResult>> PowGradOp::Vjp(pir::Operation* op, const std::vector<std::vector<pir::Value>>& inputs_, const std::vector<std::vector<pir::Value>>& outputs, const std::vector<std::vector<pir::Value>>& out_grads, const std::vector<std::vector<bool>>& stop_gradients){

    PADDLE_ENFORCE_EQ(
      inputs_.size(),
      2,
      platform::errors::InvalidArgument("pow_grad op's inputs size should be 2, but now is %d.", inputs_.size()));
    PADDLE_ENFORCE_EQ(
      outputs.size(),
      1,
      platform::errors::InvalidArgument("pow_grad op's outputs size should be 1, but now is %d.", outputs.size()));

    VLOG(6) << "Prepare inputs of pow_double_grad";

    Tensor x(std::make_shared<primitive::LazyTensor>(inputs_[0][0]));
    Tensor grad_out(std::make_shared<primitive::LazyTensor>(inputs_[1][0]));
    Tensor grad_x_grad(std::make_shared<primitive::LazyTensor>(out_grads[0][0]));

    VLOG(6) << "Vjp prepare Prepare attributes of pow_double_grad";

    phi::Scalar y = op->attribute("y").dyn_cast<paddle::dialect::ScalarAttribute>().data();

    VLOG(6) << "Vjp prepare call pow_grad's vjp inteface";

    std::vector<std::vector<Tensor>> tensor_res =
        primitive::pow_grad_vjp(
        x, grad_out, grad_x_grad, y, stop_gradients);

    VLOG(6) << "Vjp prepare stop gradient of pow_double_grad";

    std::vector<std::vector<pir::OpResult>> res(tensor_res.size());
    for (size_t i = 0; i < tensor_res.size(); ++i) {
        res[i].resize(tensor_res[i].size());
        for (size_t j = 0; j < tensor_res[i].size(); ++j) {
            if(tensor_res[i][j].defined()){
                res[i][j] = std::static_pointer_cast<primitive::LazyTensor>(tensor_res[i][j].impl())->value().dyn_cast<pir::OpResult>();
            }
        }
    }
    return res;
}

std::vector<std::vector<pir::OpResult>> PowGrad_Op::Vjp(pir::Operation* op, const std::vector<std::vector<pir::Value>>& inputs_, const std::vector<std::vector<pir::Value>>& outputs, const std::vector<std::vector<pir::Value>>& out_grads, const std::vector<std::vector<bool>>& stop_gradients){

    PADDLE_ENFORCE_EQ(
      inputs_.size(),
      2,
      platform::errors::InvalidArgument("pow_grad op's inputs size should be 2, but now is %d.", inputs_.size()));
    PADDLE_ENFORCE_EQ(
      outputs.size(),
      1,
      platform::errors::InvalidArgument("pow_grad op's outputs size should be 1, but now is %d.", outputs.size()));

    VLOG(6) << "Prepare inputs of pow_double_grad";

    Tensor x(std::make_shared<primitive::LazyTensor>(inputs_[0][0]));
    Tensor grad_out(std::make_shared<primitive::LazyTensor>(inputs_[1][0]));
    Tensor grad_x_grad(std::make_shared<primitive::LazyTensor>(out_grads[0][0]));

    VLOG(6) << "Vjp prepare Prepare attributes of pow_double_grad";

    phi::Scalar y = op->attribute("y").dyn_cast<paddle::dialect::ScalarAttribute>().data();

    VLOG(6) << "Vjp prepare call pow_grad_'s vjp inteface";

    std::vector<std::vector<Tensor>> tensor_res =
        primitive::pow_grad_vjp(
        x, grad_out, grad_x_grad, y, stop_gradients);

    VLOG(6) << "Vjp prepare stop gradient of pow_double_grad";

    std::vector<std::vector<pir::OpResult>> res(tensor_res.size());
    for (size_t i = 0; i < tensor_res.size(); ++i) {
        res[i].resize(tensor_res[i].size());
        for (size_t j = 0; j < tensor_res[i].size(); ++j) {
            if(tensor_res[i][j].defined()){
                res[i][j] = std::static_pointer_cast<primitive::LazyTensor>(tensor_res[i][j].impl())->value().dyn_cast<pir::OpResult>();
            }
        }
    }
    return res;
}

std::vector<std::vector<pir::OpResult>> ReluGradOp::Vjp(pir::Operation* op, const std::vector<std::vector<pir::Value>>& inputs_, const std::vector<std::vector<pir::Value>>& outputs, const std::vector<std::vector<pir::Value>>& out_grads, const std::vector<std::vector<bool>>& stop_gradients){

    PADDLE_ENFORCE_EQ(
      inputs_.size(),
      2,
      platform::errors::InvalidArgument("relu_grad op's inputs size should be 2, but now is %d.", inputs_.size()));
    PADDLE_ENFORCE_EQ(
      outputs.size(),
      1,
      platform::errors::InvalidArgument("relu_grad op's outputs size should be 1, but now is %d.", outputs.size()));

    VLOG(6) << "Prepare inputs of relu_double_grad";

    Tensor out(std::make_shared<primitive::LazyTensor>(inputs_[0][0]));
    Tensor grad_x_grad(std::make_shared<primitive::LazyTensor>(out_grads[0][0]));

    VLOG(6) << "Vjp prepare Prepare attributes of relu_double_grad";


    VLOG(6) << "Vjp prepare call relu_grad's vjp inteface";

    std::vector<std::vector<Tensor>> tensor_res =
        primitive::relu_grad_vjp(
        out, grad_x_grad, stop_gradients);

    VLOG(6) << "Vjp prepare stop gradient of relu_double_grad";

    std::vector<std::vector<pir::OpResult>> res(tensor_res.size());
    for (size_t i = 0; i < tensor_res.size(); ++i) {
        res[i].resize(tensor_res[i].size());
        for (size_t j = 0; j < tensor_res[i].size(); ++j) {
            if(tensor_res[i][j].defined()){
                res[i][j] = std::static_pointer_cast<primitive::LazyTensor>(tensor_res[i][j].impl())->value().dyn_cast<pir::OpResult>();
            }
        }
    }
    return res;
}

std::vector<std::vector<pir::OpResult>> ReluGrad_Op::Vjp(pir::Operation* op, const std::vector<std::vector<pir::Value>>& inputs_, const std::vector<std::vector<pir::Value>>& outputs, const std::vector<std::vector<pir::Value>>& out_grads, const std::vector<std::vector<bool>>& stop_gradients){

    PADDLE_ENFORCE_EQ(
      inputs_.size(),
      2,
      platform::errors::InvalidArgument("relu_grad op's inputs size should be 2, but now is %d.", inputs_.size()));
    PADDLE_ENFORCE_EQ(
      outputs.size(),
      1,
      platform::errors::InvalidArgument("relu_grad op's outputs size should be 1, but now is %d.", outputs.size()));

    VLOG(6) << "Prepare inputs of relu_double_grad";

    Tensor out(std::make_shared<primitive::LazyTensor>(inputs_[0][0]));
    Tensor grad_x_grad(std::make_shared<primitive::LazyTensor>(out_grads[0][0]));

    VLOG(6) << "Vjp prepare Prepare attributes of relu_double_grad";


    VLOG(6) << "Vjp prepare call relu_grad_'s vjp inteface";

    std::vector<std::vector<Tensor>> tensor_res =
        primitive::relu_grad_vjp(
        out, grad_x_grad, stop_gradients);

    VLOG(6) << "Vjp prepare stop gradient of relu_double_grad";

    std::vector<std::vector<pir::OpResult>> res(tensor_res.size());
    for (size_t i = 0; i < tensor_res.size(); ++i) {
        res[i].resize(tensor_res[i].size());
        for (size_t j = 0; j < tensor_res[i].size(); ++j) {
            if(tensor_res[i][j].defined()){
                res[i][j] = std::static_pointer_cast<primitive::LazyTensor>(tensor_res[i][j].impl())->value().dyn_cast<pir::OpResult>();
            }
        }
    }
    return res;
}

std::vector<std::vector<pir::OpResult>> RsqrtGradOp::Vjp(pir::Operation* op, const std::vector<std::vector<pir::Value>>& inputs_, const std::vector<std::vector<pir::Value>>& outputs, const std::vector<std::vector<pir::Value>>& out_grads, const std::vector<std::vector<bool>>& stop_gradients){

    PADDLE_ENFORCE_EQ(
      inputs_.size(),
      2,
      platform::errors::InvalidArgument("rsqrt_grad op's inputs size should be 2, but now is %d.", inputs_.size()));
    PADDLE_ENFORCE_EQ(
      outputs.size(),
      1,
      platform::errors::InvalidArgument("rsqrt_grad op's outputs size should be 1, but now is %d.", outputs.size()));

    VLOG(6) << "Prepare inputs of rsqrt_double_grad";

    Tensor out(std::make_shared<primitive::LazyTensor>(inputs_[0][0]));
    Tensor grad_x(std::make_shared<primitive::LazyTensor>(outputs[0][0]));
    Tensor grad_x_grad(std::make_shared<primitive::LazyTensor>(out_grads[0][0]));

    VLOG(6) << "Vjp prepare Prepare attributes of rsqrt_double_grad";


    VLOG(6) << "Vjp prepare call rsqrt_grad's vjp inteface";

    std::vector<std::vector<Tensor>> tensor_res =
        primitive::rsqrt_grad_vjp(
        out, grad_x, grad_x_grad, stop_gradients);

    VLOG(6) << "Vjp prepare stop gradient of rsqrt_double_grad";

    std::vector<std::vector<pir::OpResult>> res(tensor_res.size());
    for (size_t i = 0; i < tensor_res.size(); ++i) {
        res[i].resize(tensor_res[i].size());
        for (size_t j = 0; j < tensor_res[i].size(); ++j) {
            if(tensor_res[i][j].defined()){
                res[i][j] = std::static_pointer_cast<primitive::LazyTensor>(tensor_res[i][j].impl())->value().dyn_cast<pir::OpResult>();
            }
        }
    }
    return res;
}

std::vector<std::vector<pir::OpResult>> RsqrtGrad_Op::Vjp(pir::Operation* op, const std::vector<std::vector<pir::Value>>& inputs_, const std::vector<std::vector<pir::Value>>& outputs, const std::vector<std::vector<pir::Value>>& out_grads, const std::vector<std::vector<bool>>& stop_gradients){

    PADDLE_ENFORCE_EQ(
      inputs_.size(),
      2,
      platform::errors::InvalidArgument("rsqrt_grad op's inputs size should be 2, but now is %d.", inputs_.size()));
    PADDLE_ENFORCE_EQ(
      outputs.size(),
      1,
      platform::errors::InvalidArgument("rsqrt_grad op's outputs size should be 1, but now is %d.", outputs.size()));

    VLOG(6) << "Prepare inputs of rsqrt_double_grad";

    Tensor out(std::make_shared<primitive::LazyTensor>(inputs_[0][0]));
    Tensor grad_x(std::make_shared<primitive::LazyTensor>(outputs[0][0]));
    Tensor grad_x_grad(std::make_shared<primitive::LazyTensor>(out_grads[0][0]));

    VLOG(6) << "Vjp prepare Prepare attributes of rsqrt_double_grad";


    VLOG(6) << "Vjp prepare call rsqrt_grad_'s vjp inteface";

    std::vector<std::vector<Tensor>> tensor_res =
        primitive::rsqrt_grad_vjp(
        out, grad_x, grad_x_grad, stop_gradients);

    VLOG(6) << "Vjp prepare stop gradient of rsqrt_double_grad";

    std::vector<std::vector<pir::OpResult>> res(tensor_res.size());
    for (size_t i = 0; i < tensor_res.size(); ++i) {
        res[i].resize(tensor_res[i].size());
        for (size_t j = 0; j < tensor_res[i].size(); ++j) {
            if(tensor_res[i][j].defined()){
                res[i][j] = std::static_pointer_cast<primitive::LazyTensor>(tensor_res[i][j].impl())->value().dyn_cast<pir::OpResult>();
            }
        }
    }
    return res;
}

std::vector<std::vector<pir::OpResult>> SigmoidDoubleGradOp::Vjp(pir::Operation* op, const std::vector<std::vector<pir::Value>>& inputs_, const std::vector<std::vector<pir::Value>>& outputs, const std::vector<std::vector<pir::Value>>& out_grads, const std::vector<std::vector<bool>>& stop_gradients){

    PADDLE_ENFORCE_EQ(
      inputs_.size(),
      3,
      platform::errors::InvalidArgument("sigmoid_double_grad op's inputs size should be 3, but now is %d.", inputs_.size()));
    PADDLE_ENFORCE_EQ(
      outputs.size(),
      2,
      platform::errors::InvalidArgument("sigmoid_double_grad op's outputs size should be 2, but now is %d.", outputs.size()));

    VLOG(6) << "Prepare inputs of sigmoid_triple_grad";

    Tensor out(std::make_shared<primitive::LazyTensor>(inputs_[0][0]));
    Tensor fwd_grad_out(std::make_shared<primitive::LazyTensor>(inputs_[1][0]));
    Tensor grad_grad_x(std::make_shared<primitive::LazyTensor>(inputs_[2][0]));
    Tensor grad_out_grad(std::make_shared<primitive::LazyTensor>(out_grads[0][0]));
    paddle::optional<Tensor> grad_grad_out_grad;
    if (!IsEmptyValue(out_grads[1][0])){
        grad_grad_out_grad = paddle::make_optional<Tensor>(Tensor(std::make_shared<primitive::LazyTensor>(out_grads[1][0])));
    }

    VLOG(6) << "Vjp prepare Prepare attributes of sigmoid_triple_grad";


    VLOG(6) << "Vjp prepare call sigmoid_double_grad's vjp inteface";

    std::vector<std::vector<Tensor>> tensor_res =
        primitive::sigmoid_double_grad_vjp(
        out, fwd_grad_out, grad_grad_x, grad_out_grad, grad_grad_out_grad, stop_gradients);

    VLOG(6) << "Vjp prepare stop gradient of sigmoid_triple_grad";

    std::vector<std::vector<pir::OpResult>> res(tensor_res.size());
    for (size_t i = 0; i < tensor_res.size(); ++i) {
        res[i].resize(tensor_res[i].size());
        for (size_t j = 0; j < tensor_res[i].size(); ++j) {
            if(tensor_res[i][j].defined()){
                res[i][j] = std::static_pointer_cast<primitive::LazyTensor>(tensor_res[i][j].impl())->value().dyn_cast<pir::OpResult>();
            }
        }
    }
    return res;
}

std::vector<std::vector<pir::OpResult>> SigmoidDoubleGrad_Op::Vjp(pir::Operation* op, const std::vector<std::vector<pir::Value>>& inputs_, const std::vector<std::vector<pir::Value>>& outputs, const std::vector<std::vector<pir::Value>>& out_grads, const std::vector<std::vector<bool>>& stop_gradients){

    PADDLE_ENFORCE_EQ(
      inputs_.size(),
      3,
      platform::errors::InvalidArgument("sigmoid_double_grad op's inputs size should be 3, but now is %d.", inputs_.size()));
    PADDLE_ENFORCE_EQ(
      outputs.size(),
      2,
      platform::errors::InvalidArgument("sigmoid_double_grad op's outputs size should be 2, but now is %d.", outputs.size()));

    VLOG(6) << "Prepare inputs of sigmoid_triple_grad";

    Tensor out(std::make_shared<primitive::LazyTensor>(inputs_[0][0]));
    Tensor fwd_grad_out(std::make_shared<primitive::LazyTensor>(inputs_[1][0]));
    Tensor grad_grad_x(std::make_shared<primitive::LazyTensor>(inputs_[2][0]));
    Tensor grad_out_grad(std::make_shared<primitive::LazyTensor>(out_grads[0][0]));
    paddle::optional<Tensor> grad_grad_out_grad;
    if (!IsEmptyValue(out_grads[1][0])){
        grad_grad_out_grad = paddle::make_optional<Tensor>(Tensor(std::make_shared<primitive::LazyTensor>(out_grads[1][0])));
    }

    VLOG(6) << "Vjp prepare Prepare attributes of sigmoid_triple_grad";


    VLOG(6) << "Vjp prepare call sigmoid_double_grad_'s vjp inteface";

    std::vector<std::vector<Tensor>> tensor_res =
        primitive::sigmoid_double_grad_vjp(
        out, fwd_grad_out, grad_grad_x, grad_out_grad, grad_grad_out_grad, stop_gradients);

    VLOG(6) << "Vjp prepare stop gradient of sigmoid_triple_grad";

    std::vector<std::vector<pir::OpResult>> res(tensor_res.size());
    for (size_t i = 0; i < tensor_res.size(); ++i) {
        res[i].resize(tensor_res[i].size());
        for (size_t j = 0; j < tensor_res[i].size(); ++j) {
            if(tensor_res[i][j].defined()){
                res[i][j] = std::static_pointer_cast<primitive::LazyTensor>(tensor_res[i][j].impl())->value().dyn_cast<pir::OpResult>();
            }
        }
    }
    return res;
}

std::vector<std::vector<pir::OpResult>> SigmoidGradOp::Vjp(pir::Operation* op, const std::vector<std::vector<pir::Value>>& inputs_, const std::vector<std::vector<pir::Value>>& outputs, const std::vector<std::vector<pir::Value>>& out_grads, const std::vector<std::vector<bool>>& stop_gradients){

    PADDLE_ENFORCE_EQ(
      inputs_.size(),
      2,
      platform::errors::InvalidArgument("sigmoid_grad op's inputs size should be 2, but now is %d.", inputs_.size()));
    PADDLE_ENFORCE_EQ(
      outputs.size(),
      1,
      platform::errors::InvalidArgument("sigmoid_grad op's outputs size should be 1, but now is %d.", outputs.size()));

    VLOG(6) << "Prepare inputs of sigmoid_double_grad";

    Tensor out(std::make_shared<primitive::LazyTensor>(inputs_[0][0]));
    Tensor fwd_grad_out(std::make_shared<primitive::LazyTensor>(inputs_[1][0]));
    Tensor grad_x_grad(std::make_shared<primitive::LazyTensor>(out_grads[0][0]));

    VLOG(6) << "Vjp prepare Prepare attributes of sigmoid_double_grad";


    VLOG(6) << "Vjp prepare call sigmoid_grad's vjp inteface";

    std::vector<std::vector<Tensor>> tensor_res =
        primitive::sigmoid_grad_vjp(
        out, fwd_grad_out, grad_x_grad, stop_gradients);

    VLOG(6) << "Vjp prepare stop gradient of sigmoid_double_grad";

    std::vector<std::vector<pir::OpResult>> res(tensor_res.size());
    for (size_t i = 0; i < tensor_res.size(); ++i) {
        res[i].resize(tensor_res[i].size());
        for (size_t j = 0; j < tensor_res[i].size(); ++j) {
            if(tensor_res[i][j].defined()){
                res[i][j] = std::static_pointer_cast<primitive::LazyTensor>(tensor_res[i][j].impl())->value().dyn_cast<pir::OpResult>();
            }
        }
    }
    return res;
}

std::vector<std::vector<pir::OpResult>> SigmoidGrad_Op::Vjp(pir::Operation* op, const std::vector<std::vector<pir::Value>>& inputs_, const std::vector<std::vector<pir::Value>>& outputs, const std::vector<std::vector<pir::Value>>& out_grads, const std::vector<std::vector<bool>>& stop_gradients){

    PADDLE_ENFORCE_EQ(
      inputs_.size(),
      2,
      platform::errors::InvalidArgument("sigmoid_grad op's inputs size should be 2, but now is %d.", inputs_.size()));
    PADDLE_ENFORCE_EQ(
      outputs.size(),
      1,
      platform::errors::InvalidArgument("sigmoid_grad op's outputs size should be 1, but now is %d.", outputs.size()));

    VLOG(6) << "Prepare inputs of sigmoid_double_grad";

    Tensor out(std::make_shared<primitive::LazyTensor>(inputs_[0][0]));
    Tensor fwd_grad_out(std::make_shared<primitive::LazyTensor>(inputs_[1][0]));
    Tensor grad_x_grad(std::make_shared<primitive::LazyTensor>(out_grads[0][0]));

    VLOG(6) << "Vjp prepare Prepare attributes of sigmoid_double_grad";


    VLOG(6) << "Vjp prepare call sigmoid_grad_'s vjp inteface";

    std::vector<std::vector<Tensor>> tensor_res =
        primitive::sigmoid_grad_vjp(
        out, fwd_grad_out, grad_x_grad, stop_gradients);

    VLOG(6) << "Vjp prepare stop gradient of sigmoid_double_grad";

    std::vector<std::vector<pir::OpResult>> res(tensor_res.size());
    for (size_t i = 0; i < tensor_res.size(); ++i) {
        res[i].resize(tensor_res[i].size());
        for (size_t j = 0; j < tensor_res[i].size(); ++j) {
            if(tensor_res[i][j].defined()){
                res[i][j] = std::static_pointer_cast<primitive::LazyTensor>(tensor_res[i][j].impl())->value().dyn_cast<pir::OpResult>();
            }
        }
    }
    return res;
}

std::vector<std::vector<pir::OpResult>> SinDoubleGradOp::Vjp(pir::Operation* op, const std::vector<std::vector<pir::Value>>& inputs_, const std::vector<std::vector<pir::Value>>& outputs, const std::vector<std::vector<pir::Value>>& out_grads, const std::vector<std::vector<bool>>& stop_gradients){

    PADDLE_ENFORCE_EQ(
      inputs_.size(),
      3,
      platform::errors::InvalidArgument("sin_double_grad op's inputs size should be 3, but now is %d.", inputs_.size()));
    PADDLE_ENFORCE_EQ(
      outputs.size(),
      2,
      platform::errors::InvalidArgument("sin_double_grad op's outputs size should be 2, but now is %d.", outputs.size()));

    VLOG(6) << "Prepare inputs of sin_triple_grad";

    Tensor x(std::make_shared<primitive::LazyTensor>(inputs_[0][0]));
    paddle::optional<Tensor> grad_out_forward;
    if (!IsEmptyValue(inputs_[1][0])){
        grad_out_forward = paddle::make_optional<Tensor>(Tensor(std::make_shared<primitive::LazyTensor>(inputs_[1][0])));
    }
    paddle::optional<Tensor> grad_x_grad_forward;
    if (!IsEmptyValue(inputs_[2][0])){
        grad_x_grad_forward = paddle::make_optional<Tensor>(Tensor(std::make_shared<primitive::LazyTensor>(inputs_[2][0])));
    }
    Tensor grad_x_grad(std::make_shared<primitive::LazyTensor>(out_grads[0][0]));
    paddle::optional<Tensor> grad_out_grad_grad;
    if (!IsEmptyValue(out_grads[1][0])){
        grad_out_grad_grad = paddle::make_optional<Tensor>(Tensor(std::make_shared<primitive::LazyTensor>(out_grads[1][0])));
    }

    VLOG(6) << "Vjp prepare Prepare attributes of sin_triple_grad";


    VLOG(6) << "Vjp prepare call sin_double_grad's vjp inteface";

    std::vector<std::vector<Tensor>> tensor_res =
        primitive::sin_double_grad_vjp(
        x, grad_out_forward, grad_x_grad_forward, grad_x_grad, grad_out_grad_grad, stop_gradients);

    VLOG(6) << "Vjp prepare stop gradient of sin_triple_grad";

    std::vector<std::vector<pir::OpResult>> res(tensor_res.size());
    for (size_t i = 0; i < tensor_res.size(); ++i) {
        res[i].resize(tensor_res[i].size());
        for (size_t j = 0; j < tensor_res[i].size(); ++j) {
            if(tensor_res[i][j].defined()){
                res[i][j] = std::static_pointer_cast<primitive::LazyTensor>(tensor_res[i][j].impl())->value().dyn_cast<pir::OpResult>();
            }
        }
    }
    return res;
}

std::vector<std::vector<pir::OpResult>> SinDoubleGrad_Op::Vjp(pir::Operation* op, const std::vector<std::vector<pir::Value>>& inputs_, const std::vector<std::vector<pir::Value>>& outputs, const std::vector<std::vector<pir::Value>>& out_grads, const std::vector<std::vector<bool>>& stop_gradients){

    PADDLE_ENFORCE_EQ(
      inputs_.size(),
      3,
      platform::errors::InvalidArgument("sin_double_grad op's inputs size should be 3, but now is %d.", inputs_.size()));
    PADDLE_ENFORCE_EQ(
      outputs.size(),
      2,
      platform::errors::InvalidArgument("sin_double_grad op's outputs size should be 2, but now is %d.", outputs.size()));

    VLOG(6) << "Prepare inputs of sin_triple_grad";

    Tensor x(std::make_shared<primitive::LazyTensor>(inputs_[0][0]));
    paddle::optional<Tensor> grad_out_forward;
    if (!IsEmptyValue(inputs_[1][0])){
        grad_out_forward = paddle::make_optional<Tensor>(Tensor(std::make_shared<primitive::LazyTensor>(inputs_[1][0])));
    }
    paddle::optional<Tensor> grad_x_grad_forward;
    if (!IsEmptyValue(inputs_[2][0])){
        grad_x_grad_forward = paddle::make_optional<Tensor>(Tensor(std::make_shared<primitive::LazyTensor>(inputs_[2][0])));
    }
    Tensor grad_x_grad(std::make_shared<primitive::LazyTensor>(out_grads[0][0]));
    paddle::optional<Tensor> grad_out_grad_grad;
    if (!IsEmptyValue(out_grads[1][0])){
        grad_out_grad_grad = paddle::make_optional<Tensor>(Tensor(std::make_shared<primitive::LazyTensor>(out_grads[1][0])));
    }

    VLOG(6) << "Vjp prepare Prepare attributes of sin_triple_grad";


    VLOG(6) << "Vjp prepare call sin_double_grad_'s vjp inteface";

    std::vector<std::vector<Tensor>> tensor_res =
        primitive::sin_double_grad_vjp(
        x, grad_out_forward, grad_x_grad_forward, grad_x_grad, grad_out_grad_grad, stop_gradients);

    VLOG(6) << "Vjp prepare stop gradient of sin_triple_grad";

    std::vector<std::vector<pir::OpResult>> res(tensor_res.size());
    for (size_t i = 0; i < tensor_res.size(); ++i) {
        res[i].resize(tensor_res[i].size());
        for (size_t j = 0; j < tensor_res[i].size(); ++j) {
            if(tensor_res[i][j].defined()){
                res[i][j] = std::static_pointer_cast<primitive::LazyTensor>(tensor_res[i][j].impl())->value().dyn_cast<pir::OpResult>();
            }
        }
    }
    return res;
}

std::vector<std::vector<pir::OpResult>> SinGradOp::Vjp(pir::Operation* op, const std::vector<std::vector<pir::Value>>& inputs_, const std::vector<std::vector<pir::Value>>& outputs, const std::vector<std::vector<pir::Value>>& out_grads, const std::vector<std::vector<bool>>& stop_gradients){

    PADDLE_ENFORCE_EQ(
      inputs_.size(),
      2,
      platform::errors::InvalidArgument("sin_grad op's inputs size should be 2, but now is %d.", inputs_.size()));
    PADDLE_ENFORCE_EQ(
      outputs.size(),
      1,
      platform::errors::InvalidArgument("sin_grad op's outputs size should be 1, but now is %d.", outputs.size()));

    VLOG(6) << "Prepare inputs of sin_double_grad";

    Tensor x(std::make_shared<primitive::LazyTensor>(inputs_[0][0]));
    paddle::optional<Tensor> grad_out;
    if (!IsEmptyValue(inputs_[1][0])){
        grad_out = paddle::make_optional<Tensor>(Tensor(std::make_shared<primitive::LazyTensor>(inputs_[1][0])));
    }
    Tensor grad_x_grad(std::make_shared<primitive::LazyTensor>(out_grads[0][0]));

    VLOG(6) << "Vjp prepare Prepare attributes of sin_double_grad";


    VLOG(6) << "Vjp prepare call sin_grad's vjp inteface";

    std::vector<std::vector<Tensor>> tensor_res =
        primitive::sin_grad_vjp(
        x, grad_out, grad_x_grad, stop_gradients);

    VLOG(6) << "Vjp prepare stop gradient of sin_double_grad";

    std::vector<std::vector<pir::OpResult>> res(tensor_res.size());
    for (size_t i = 0; i < tensor_res.size(); ++i) {
        res[i].resize(tensor_res[i].size());
        for (size_t j = 0; j < tensor_res[i].size(); ++j) {
            if(tensor_res[i][j].defined()){
                res[i][j] = std::static_pointer_cast<primitive::LazyTensor>(tensor_res[i][j].impl())->value().dyn_cast<pir::OpResult>();
            }
        }
    }
    return res;
}

std::vector<std::vector<pir::OpResult>> SinGrad_Op::Vjp(pir::Operation* op, const std::vector<std::vector<pir::Value>>& inputs_, const std::vector<std::vector<pir::Value>>& outputs, const std::vector<std::vector<pir::Value>>& out_grads, const std::vector<std::vector<bool>>& stop_gradients){

    PADDLE_ENFORCE_EQ(
      inputs_.size(),
      2,
      platform::errors::InvalidArgument("sin_grad op's inputs size should be 2, but now is %d.", inputs_.size()));
    PADDLE_ENFORCE_EQ(
      outputs.size(),
      1,
      platform::errors::InvalidArgument("sin_grad op's outputs size should be 1, but now is %d.", outputs.size()));

    VLOG(6) << "Prepare inputs of sin_double_grad";

    Tensor x(std::make_shared<primitive::LazyTensor>(inputs_[0][0]));
    paddle::optional<Tensor> grad_out;
    if (!IsEmptyValue(inputs_[1][0])){
        grad_out = paddle::make_optional<Tensor>(Tensor(std::make_shared<primitive::LazyTensor>(inputs_[1][0])));
    }
    Tensor grad_x_grad(std::make_shared<primitive::LazyTensor>(out_grads[0][0]));

    VLOG(6) << "Vjp prepare Prepare attributes of sin_double_grad";


    VLOG(6) << "Vjp prepare call sin_grad_'s vjp inteface";

    std::vector<std::vector<Tensor>> tensor_res =
        primitive::sin_grad_vjp(
        x, grad_out, grad_x_grad, stop_gradients);

    VLOG(6) << "Vjp prepare stop gradient of sin_double_grad";

    std::vector<std::vector<pir::OpResult>> res(tensor_res.size());
    for (size_t i = 0; i < tensor_res.size(); ++i) {
        res[i].resize(tensor_res[i].size());
        for (size_t j = 0; j < tensor_res[i].size(); ++j) {
            if(tensor_res[i][j].defined()){
                res[i][j] = std::static_pointer_cast<primitive::LazyTensor>(tensor_res[i][j].impl())->value().dyn_cast<pir::OpResult>();
            }
        }
    }
    return res;
}

std::vector<std::vector<pir::OpResult>> SoftplusGradOp::Vjp(pir::Operation* op, const std::vector<std::vector<pir::Value>>& inputs_, const std::vector<std::vector<pir::Value>>& outputs, const std::vector<std::vector<pir::Value>>& out_grads, const std::vector<std::vector<bool>>& stop_gradients){

    PADDLE_ENFORCE_EQ(
      inputs_.size(),
      2,
      platform::errors::InvalidArgument("softplus_grad op's inputs size should be 2, but now is %d.", inputs_.size()));
    PADDLE_ENFORCE_EQ(
      outputs.size(),
      1,
      platform::errors::InvalidArgument("softplus_grad op's outputs size should be 1, but now is %d.", outputs.size()));

    VLOG(6) << "Prepare inputs of softplus_double_grad";

    Tensor x(std::make_shared<primitive::LazyTensor>(inputs_[0][0]));
    Tensor grad_out(std::make_shared<primitive::LazyTensor>(inputs_[1][0]));
    Tensor grad_x_grad(std::make_shared<primitive::LazyTensor>(out_grads[0][0]));

    VLOG(6) << "Vjp prepare Prepare attributes of softplus_double_grad";

    float beta = op->attribute("beta").dyn_cast<pir::FloatAttribute>().data();
    float threshold = op->attribute("threshold").dyn_cast<pir::FloatAttribute>().data();

    VLOG(6) << "Vjp prepare call softplus_grad's vjp inteface";

    std::vector<std::vector<Tensor>> tensor_res =
        primitive::softplus_grad_vjp(
        x, grad_out, grad_x_grad, beta, threshold, stop_gradients);

    VLOG(6) << "Vjp prepare stop gradient of softplus_double_grad";

    std::vector<std::vector<pir::OpResult>> res(tensor_res.size());
    for (size_t i = 0; i < tensor_res.size(); ++i) {
        res[i].resize(tensor_res[i].size());
        for (size_t j = 0; j < tensor_res[i].size(); ++j) {
            if(tensor_res[i][j].defined()){
                res[i][j] = std::static_pointer_cast<primitive::LazyTensor>(tensor_res[i][j].impl())->value().dyn_cast<pir::OpResult>();
            }
        }
    }
    return res;
}

std::vector<std::vector<pir::OpResult>> SoftplusGrad_Op::Vjp(pir::Operation* op, const std::vector<std::vector<pir::Value>>& inputs_, const std::vector<std::vector<pir::Value>>& outputs, const std::vector<std::vector<pir::Value>>& out_grads, const std::vector<std::vector<bool>>& stop_gradients){

    PADDLE_ENFORCE_EQ(
      inputs_.size(),
      2,
      platform::errors::InvalidArgument("softplus_grad op's inputs size should be 2, but now is %d.", inputs_.size()));
    PADDLE_ENFORCE_EQ(
      outputs.size(),
      1,
      platform::errors::InvalidArgument("softplus_grad op's outputs size should be 1, but now is %d.", outputs.size()));

    VLOG(6) << "Prepare inputs of softplus_double_grad";

    Tensor x(std::make_shared<primitive::LazyTensor>(inputs_[0][0]));
    Tensor grad_out(std::make_shared<primitive::LazyTensor>(inputs_[1][0]));
    Tensor grad_x_grad(std::make_shared<primitive::LazyTensor>(out_grads[0][0]));

    VLOG(6) << "Vjp prepare Prepare attributes of softplus_double_grad";

    float beta = op->attribute("beta").dyn_cast<pir::FloatAttribute>().data();
    float threshold = op->attribute("threshold").dyn_cast<pir::FloatAttribute>().data();

    VLOG(6) << "Vjp prepare call softplus_grad_'s vjp inteface";

    std::vector<std::vector<Tensor>> tensor_res =
        primitive::softplus_grad_vjp(
        x, grad_out, grad_x_grad, beta, threshold, stop_gradients);

    VLOG(6) << "Vjp prepare stop gradient of softplus_double_grad";

    std::vector<std::vector<pir::OpResult>> res(tensor_res.size());
    for (size_t i = 0; i < tensor_res.size(); ++i) {
        res[i].resize(tensor_res[i].size());
        for (size_t j = 0; j < tensor_res[i].size(); ++j) {
            if(tensor_res[i][j].defined()){
                res[i][j] = std::static_pointer_cast<primitive::LazyTensor>(tensor_res[i][j].impl())->value().dyn_cast<pir::OpResult>();
            }
        }
    }
    return res;
}

std::vector<std::vector<pir::OpResult>> SqrtGradOp::Vjp(pir::Operation* op, const std::vector<std::vector<pir::Value>>& inputs_, const std::vector<std::vector<pir::Value>>& outputs, const std::vector<std::vector<pir::Value>>& out_grads, const std::vector<std::vector<bool>>& stop_gradients){

    PADDLE_ENFORCE_EQ(
      inputs_.size(),
      2,
      platform::errors::InvalidArgument("sqrt_grad op's inputs size should be 2, but now is %d.", inputs_.size()));
    PADDLE_ENFORCE_EQ(
      outputs.size(),
      1,
      platform::errors::InvalidArgument("sqrt_grad op's outputs size should be 1, but now is %d.", outputs.size()));

    VLOG(6) << "Prepare inputs of sqrt_double_grad";

    Tensor out(std::make_shared<primitive::LazyTensor>(inputs_[0][0]));
    Tensor grad_x(std::make_shared<primitive::LazyTensor>(outputs[0][0]));
    Tensor grad_x_grad(std::make_shared<primitive::LazyTensor>(out_grads[0][0]));

    VLOG(6) << "Vjp prepare Prepare attributes of sqrt_double_grad";


    VLOG(6) << "Vjp prepare call sqrt_grad's vjp inteface";

    std::vector<std::vector<Tensor>> tensor_res =
        primitive::sqrt_grad_vjp(
        out, grad_x, grad_x_grad, stop_gradients);

    VLOG(6) << "Vjp prepare stop gradient of sqrt_double_grad";

    std::vector<std::vector<pir::OpResult>> res(tensor_res.size());
    for (size_t i = 0; i < tensor_res.size(); ++i) {
        res[i].resize(tensor_res[i].size());
        for (size_t j = 0; j < tensor_res[i].size(); ++j) {
            if(tensor_res[i][j].defined()){
                res[i][j] = std::static_pointer_cast<primitive::LazyTensor>(tensor_res[i][j].impl())->value().dyn_cast<pir::OpResult>();
            }
        }
    }
    return res;
}

std::vector<std::vector<pir::OpResult>> SqrtGrad_Op::Vjp(pir::Operation* op, const std::vector<std::vector<pir::Value>>& inputs_, const std::vector<std::vector<pir::Value>>& outputs, const std::vector<std::vector<pir::Value>>& out_grads, const std::vector<std::vector<bool>>& stop_gradients){

    PADDLE_ENFORCE_EQ(
      inputs_.size(),
      2,
      platform::errors::InvalidArgument("sqrt_grad op's inputs size should be 2, but now is %d.", inputs_.size()));
    PADDLE_ENFORCE_EQ(
      outputs.size(),
      1,
      platform::errors::InvalidArgument("sqrt_grad op's outputs size should be 1, but now is %d.", outputs.size()));

    VLOG(6) << "Prepare inputs of sqrt_double_grad";

    Tensor out(std::make_shared<primitive::LazyTensor>(inputs_[0][0]));
    Tensor grad_x(std::make_shared<primitive::LazyTensor>(outputs[0][0]));
    Tensor grad_x_grad(std::make_shared<primitive::LazyTensor>(out_grads[0][0]));

    VLOG(6) << "Vjp prepare Prepare attributes of sqrt_double_grad";


    VLOG(6) << "Vjp prepare call sqrt_grad_'s vjp inteface";

    std::vector<std::vector<Tensor>> tensor_res =
        primitive::sqrt_grad_vjp(
        out, grad_x, grad_x_grad, stop_gradients);

    VLOG(6) << "Vjp prepare stop gradient of sqrt_double_grad";

    std::vector<std::vector<pir::OpResult>> res(tensor_res.size());
    for (size_t i = 0; i < tensor_res.size(); ++i) {
        res[i].resize(tensor_res[i].size());
        for (size_t j = 0; j < tensor_res[i].size(); ++j) {
            if(tensor_res[i][j].defined()){
                res[i][j] = std::static_pointer_cast<primitive::LazyTensor>(tensor_res[i][j].impl())->value().dyn_cast<pir::OpResult>();
            }
        }
    }
    return res;
}

std::vector<std::vector<pir::OpResult>> SquareGradOp::Vjp(pir::Operation* op, const std::vector<std::vector<pir::Value>>& inputs_, const std::vector<std::vector<pir::Value>>& outputs, const std::vector<std::vector<pir::Value>>& out_grads, const std::vector<std::vector<bool>>& stop_gradients){

    PADDLE_ENFORCE_EQ(
      inputs_.size(),
      2,
      platform::errors::InvalidArgument("square_grad op's inputs size should be 2, but now is %d.", inputs_.size()));
    PADDLE_ENFORCE_EQ(
      outputs.size(),
      1,
      platform::errors::InvalidArgument("square_grad op's outputs size should be 1, but now is %d.", outputs.size()));

    VLOG(6) << "Prepare inputs of square_double_grad";

    Tensor x(std::make_shared<primitive::LazyTensor>(inputs_[0][0]));
    Tensor grad_out(std::make_shared<primitive::LazyTensor>(inputs_[1][0]));
    Tensor grad_x_grad(std::make_shared<primitive::LazyTensor>(out_grads[0][0]));

    VLOG(6) << "Vjp prepare Prepare attributes of square_double_grad";


    VLOG(6) << "Vjp prepare call square_grad's vjp inteface";

    std::vector<std::vector<Tensor>> tensor_res =
        primitive::square_grad_vjp(
        x, grad_out, grad_x_grad, stop_gradients);

    VLOG(6) << "Vjp prepare stop gradient of square_double_grad";

    std::vector<std::vector<pir::OpResult>> res(tensor_res.size());
    for (size_t i = 0; i < tensor_res.size(); ++i) {
        res[i].resize(tensor_res[i].size());
        for (size_t j = 0; j < tensor_res[i].size(); ++j) {
            if(tensor_res[i][j].defined()){
                res[i][j] = std::static_pointer_cast<primitive::LazyTensor>(tensor_res[i][j].impl())->value().dyn_cast<pir::OpResult>();
            }
        }
    }
    return res;
}

std::vector<std::vector<pir::OpResult>> SquareGrad_Op::Vjp(pir::Operation* op, const std::vector<std::vector<pir::Value>>& inputs_, const std::vector<std::vector<pir::Value>>& outputs, const std::vector<std::vector<pir::Value>>& out_grads, const std::vector<std::vector<bool>>& stop_gradients){

    PADDLE_ENFORCE_EQ(
      inputs_.size(),
      2,
      platform::errors::InvalidArgument("square_grad op's inputs size should be 2, but now is %d.", inputs_.size()));
    PADDLE_ENFORCE_EQ(
      outputs.size(),
      1,
      platform::errors::InvalidArgument("square_grad op's outputs size should be 1, but now is %d.", outputs.size()));

    VLOG(6) << "Prepare inputs of square_double_grad";

    Tensor x(std::make_shared<primitive::LazyTensor>(inputs_[0][0]));
    Tensor grad_out(std::make_shared<primitive::LazyTensor>(inputs_[1][0]));
    Tensor grad_x_grad(std::make_shared<primitive::LazyTensor>(out_grads[0][0]));

    VLOG(6) << "Vjp prepare Prepare attributes of square_double_grad";


    VLOG(6) << "Vjp prepare call square_grad_'s vjp inteface";

    std::vector<std::vector<Tensor>> tensor_res =
        primitive::square_grad_vjp(
        x, grad_out, grad_x_grad, stop_gradients);

    VLOG(6) << "Vjp prepare stop gradient of square_double_grad";

    std::vector<std::vector<pir::OpResult>> res(tensor_res.size());
    for (size_t i = 0; i < tensor_res.size(); ++i) {
        res[i].resize(tensor_res[i].size());
        for (size_t j = 0; j < tensor_res[i].size(); ++j) {
            if(tensor_res[i][j].defined()){
                res[i][j] = std::static_pointer_cast<primitive::LazyTensor>(tensor_res[i][j].impl())->value().dyn_cast<pir::OpResult>();
            }
        }
    }
    return res;
}

std::vector<std::vector<pir::OpResult>> SqueezeGradOp::Vjp(pir::Operation* op, const std::vector<std::vector<pir::Value>>& inputs_, const std::vector<std::vector<pir::Value>>& outputs, const std::vector<std::vector<pir::Value>>& out_grads, const std::vector<std::vector<bool>>& stop_gradients){

    PADDLE_ENFORCE_EQ(
      inputs_.size(),
      3,
      platform::errors::InvalidArgument("squeeze_grad op's inputs size should be 3, but now is %d.", inputs_.size()));
    PADDLE_ENFORCE_EQ(
      outputs.size(),
      1,
      platform::errors::InvalidArgument("squeeze_grad op's outputs size should be 1, but now is %d.", outputs.size()));

    VLOG(6) << "Prepare inputs of squeeze_double_grad";

    Tensor grad_x_grad(std::make_shared<primitive::LazyTensor>(out_grads[0][0]));

    VLOG(6) << "Vjp prepare Prepare attributes of squeeze_double_grad";

    Tensor axis(std::make_shared<primitive::LazyTensor>(inputs_[2][0]));

    VLOG(6) << "Vjp prepare call squeeze_grad's vjp inteface";

    std::vector<std::vector<Tensor>> tensor_res =
        primitive::squeeze_grad_vjp(
        grad_x_grad, axis, stop_gradients);

    VLOG(6) << "Vjp prepare stop gradient of squeeze_double_grad";

    std::vector<std::vector<pir::OpResult>> res(tensor_res.size());
    for (size_t i = 0; i < tensor_res.size(); ++i) {
        res[i].resize(tensor_res[i].size());
        for (size_t j = 0; j < tensor_res[i].size(); ++j) {
            if(tensor_res[i][j].defined()){
                res[i][j] = std::static_pointer_cast<primitive::LazyTensor>(tensor_res[i][j].impl())->value().dyn_cast<pir::OpResult>();
            }
        }
    }
    return res;
}

std::vector<std::vector<pir::OpResult>> SqueezeGrad_Op::Vjp(pir::Operation* op, const std::vector<std::vector<pir::Value>>& inputs_, const std::vector<std::vector<pir::Value>>& outputs, const std::vector<std::vector<pir::Value>>& out_grads, const std::vector<std::vector<bool>>& stop_gradients){

    PADDLE_ENFORCE_EQ(
      inputs_.size(),
      3,
      platform::errors::InvalidArgument("squeeze_grad op's inputs size should be 3, but now is %d.", inputs_.size()));
    PADDLE_ENFORCE_EQ(
      outputs.size(),
      1,
      platform::errors::InvalidArgument("squeeze_grad op's outputs size should be 1, but now is %d.", outputs.size()));

    VLOG(6) << "Prepare inputs of squeeze_double_grad";

    Tensor grad_x_grad(std::make_shared<primitive::LazyTensor>(out_grads[0][0]));

    VLOG(6) << "Vjp prepare Prepare attributes of squeeze_double_grad";

    Tensor axis(std::make_shared<primitive::LazyTensor>(inputs_[2][0]));

    VLOG(6) << "Vjp prepare call squeeze_grad_'s vjp inteface";

    std::vector<std::vector<Tensor>> tensor_res =
        primitive::squeeze_grad_vjp(
        grad_x_grad, axis, stop_gradients);

    VLOG(6) << "Vjp prepare stop gradient of squeeze_double_grad";

    std::vector<std::vector<pir::OpResult>> res(tensor_res.size());
    for (size_t i = 0; i < tensor_res.size(); ++i) {
        res[i].resize(tensor_res[i].size());
        for (size_t j = 0; j < tensor_res[i].size(); ++j) {
            if(tensor_res[i][j].defined()){
                res[i][j] = std::static_pointer_cast<primitive::LazyTensor>(tensor_res[i][j].impl())->value().dyn_cast<pir::OpResult>();
            }
        }
    }
    return res;
}

std::vector<std::vector<pir::OpResult>> TanhDoubleGradOp::Vjp(pir::Operation* op, const std::vector<std::vector<pir::Value>>& inputs_, const std::vector<std::vector<pir::Value>>& outputs, const std::vector<std::vector<pir::Value>>& out_grads, const std::vector<std::vector<bool>>& stop_gradients){

    PADDLE_ENFORCE_EQ(
      inputs_.size(),
      3,
      platform::errors::InvalidArgument("tanh_double_grad op's inputs size should be 3, but now is %d.", inputs_.size()));
    PADDLE_ENFORCE_EQ(
      outputs.size(),
      2,
      platform::errors::InvalidArgument("tanh_double_grad op's outputs size should be 2, but now is %d.", outputs.size()));

    VLOG(6) << "Prepare inputs of tanh_triple_grad";

    Tensor out(std::make_shared<primitive::LazyTensor>(inputs_[0][0]));
    Tensor grad_out_forward(std::make_shared<primitive::LazyTensor>(inputs_[1][0]));
    Tensor grad_x_grad_forward(std::make_shared<primitive::LazyTensor>(inputs_[2][0]));
    paddle::optional<Tensor> grad_out_new_grad;
    if (!IsEmptyValue(out_grads[0][0])){
        grad_out_new_grad = paddle::make_optional<Tensor>(Tensor(std::make_shared<primitive::LazyTensor>(out_grads[0][0])));
    }
    paddle::optional<Tensor> grad_out_grad_grad;
    if (!IsEmptyValue(out_grads[1][0])){
        grad_out_grad_grad = paddle::make_optional<Tensor>(Tensor(std::make_shared<primitive::LazyTensor>(out_grads[1][0])));
    }

    VLOG(6) << "Vjp prepare Prepare attributes of tanh_triple_grad";


    VLOG(6) << "Vjp prepare call tanh_double_grad's vjp inteface";

    std::vector<std::vector<Tensor>> tensor_res =
        primitive::tanh_double_grad_vjp(
        out, grad_out_forward, grad_x_grad_forward, grad_out_new_grad, grad_out_grad_grad, stop_gradients);

    VLOG(6) << "Vjp prepare stop gradient of tanh_triple_grad";

    std::vector<std::vector<pir::OpResult>> res(tensor_res.size());
    for (size_t i = 0; i < tensor_res.size(); ++i) {
        res[i].resize(tensor_res[i].size());
        for (size_t j = 0; j < tensor_res[i].size(); ++j) {
            if(tensor_res[i][j].defined()){
                res[i][j] = std::static_pointer_cast<primitive::LazyTensor>(tensor_res[i][j].impl())->value().dyn_cast<pir::OpResult>();
            }
        }
    }
    return res;
}

std::vector<std::vector<pir::OpResult>> TanhDoubleGrad_Op::Vjp(pir::Operation* op, const std::vector<std::vector<pir::Value>>& inputs_, const std::vector<std::vector<pir::Value>>& outputs, const std::vector<std::vector<pir::Value>>& out_grads, const std::vector<std::vector<bool>>& stop_gradients){

    PADDLE_ENFORCE_EQ(
      inputs_.size(),
      3,
      platform::errors::InvalidArgument("tanh_double_grad op's inputs size should be 3, but now is %d.", inputs_.size()));
    PADDLE_ENFORCE_EQ(
      outputs.size(),
      2,
      platform::errors::InvalidArgument("tanh_double_grad op's outputs size should be 2, but now is %d.", outputs.size()));

    VLOG(6) << "Prepare inputs of tanh_triple_grad";

    Tensor out(std::make_shared<primitive::LazyTensor>(inputs_[0][0]));
    Tensor grad_out_forward(std::make_shared<primitive::LazyTensor>(inputs_[1][0]));
    Tensor grad_x_grad_forward(std::make_shared<primitive::LazyTensor>(inputs_[2][0]));
    paddle::optional<Tensor> grad_out_new_grad;
    if (!IsEmptyValue(out_grads[0][0])){
        grad_out_new_grad = paddle::make_optional<Tensor>(Tensor(std::make_shared<primitive::LazyTensor>(out_grads[0][0])));
    }
    paddle::optional<Tensor> grad_out_grad_grad;
    if (!IsEmptyValue(out_grads[1][0])){
        grad_out_grad_grad = paddle::make_optional<Tensor>(Tensor(std::make_shared<primitive::LazyTensor>(out_grads[1][0])));
    }

    VLOG(6) << "Vjp prepare Prepare attributes of tanh_triple_grad";


    VLOG(6) << "Vjp prepare call tanh_double_grad_'s vjp inteface";

    std::vector<std::vector<Tensor>> tensor_res =
        primitive::tanh_double_grad_vjp(
        out, grad_out_forward, grad_x_grad_forward, grad_out_new_grad, grad_out_grad_grad, stop_gradients);

    VLOG(6) << "Vjp prepare stop gradient of tanh_triple_grad";

    std::vector<std::vector<pir::OpResult>> res(tensor_res.size());
    for (size_t i = 0; i < tensor_res.size(); ++i) {
        res[i].resize(tensor_res[i].size());
        for (size_t j = 0; j < tensor_res[i].size(); ++j) {
            if(tensor_res[i][j].defined()){
                res[i][j] = std::static_pointer_cast<primitive::LazyTensor>(tensor_res[i][j].impl())->value().dyn_cast<pir::OpResult>();
            }
        }
    }
    return res;
}

std::vector<std::vector<pir::OpResult>> TanhGradOp::Vjp(pir::Operation* op, const std::vector<std::vector<pir::Value>>& inputs_, const std::vector<std::vector<pir::Value>>& outputs, const std::vector<std::vector<pir::Value>>& out_grads, const std::vector<std::vector<bool>>& stop_gradients){

    PADDLE_ENFORCE_EQ(
      inputs_.size(),
      2,
      platform::errors::InvalidArgument("tanh_grad op's inputs size should be 2, but now is %d.", inputs_.size()));
    PADDLE_ENFORCE_EQ(
      outputs.size(),
      1,
      platform::errors::InvalidArgument("tanh_grad op's outputs size should be 1, but now is %d.", outputs.size()));

    VLOG(6) << "Prepare inputs of tanh_double_grad";

    Tensor out(std::make_shared<primitive::LazyTensor>(inputs_[0][0]));
    Tensor grad_out(std::make_shared<primitive::LazyTensor>(inputs_[1][0]));
    Tensor grad_x_grad(std::make_shared<primitive::LazyTensor>(out_grads[0][0]));

    VLOG(6) << "Vjp prepare Prepare attributes of tanh_double_grad";


    VLOG(6) << "Vjp prepare call tanh_grad's vjp inteface";

    std::vector<std::vector<Tensor>> tensor_res =
        primitive::tanh_grad_vjp(
        out, grad_out, grad_x_grad, stop_gradients);

    VLOG(6) << "Vjp prepare stop gradient of tanh_double_grad";

    std::vector<std::vector<pir::OpResult>> res(tensor_res.size());
    for (size_t i = 0; i < tensor_res.size(); ++i) {
        res[i].resize(tensor_res[i].size());
        for (size_t j = 0; j < tensor_res[i].size(); ++j) {
            if(tensor_res[i][j].defined()){
                res[i][j] = std::static_pointer_cast<primitive::LazyTensor>(tensor_res[i][j].impl())->value().dyn_cast<pir::OpResult>();
            }
        }
    }
    return res;
}

std::vector<std::vector<pir::OpResult>> TanhGrad_Op::Vjp(pir::Operation* op, const std::vector<std::vector<pir::Value>>& inputs_, const std::vector<std::vector<pir::Value>>& outputs, const std::vector<std::vector<pir::Value>>& out_grads, const std::vector<std::vector<bool>>& stop_gradients){

    PADDLE_ENFORCE_EQ(
      inputs_.size(),
      2,
      platform::errors::InvalidArgument("tanh_grad op's inputs size should be 2, but now is %d.", inputs_.size()));
    PADDLE_ENFORCE_EQ(
      outputs.size(),
      1,
      platform::errors::InvalidArgument("tanh_grad op's outputs size should be 1, but now is %d.", outputs.size()));

    VLOG(6) << "Prepare inputs of tanh_double_grad";

    Tensor out(std::make_shared<primitive::LazyTensor>(inputs_[0][0]));
    Tensor grad_out(std::make_shared<primitive::LazyTensor>(inputs_[1][0]));
    Tensor grad_x_grad(std::make_shared<primitive::LazyTensor>(out_grads[0][0]));

    VLOG(6) << "Vjp prepare Prepare attributes of tanh_double_grad";


    VLOG(6) << "Vjp prepare call tanh_grad_'s vjp inteface";

    std::vector<std::vector<Tensor>> tensor_res =
        primitive::tanh_grad_vjp(
        out, grad_out, grad_x_grad, stop_gradients);

    VLOG(6) << "Vjp prepare stop gradient of tanh_double_grad";

    std::vector<std::vector<pir::OpResult>> res(tensor_res.size());
    for (size_t i = 0; i < tensor_res.size(); ++i) {
        res[i].resize(tensor_res[i].size());
        for (size_t j = 0; j < tensor_res[i].size(); ++j) {
            if(tensor_res[i][j].defined()){
                res[i][j] = std::static_pointer_cast<primitive::LazyTensor>(tensor_res[i][j].impl())->value().dyn_cast<pir::OpResult>();
            }
        }
    }
    return res;
}

std::vector<std::vector<pir::OpResult>> UnsqueezeGradOp::Vjp(pir::Operation* op, const std::vector<std::vector<pir::Value>>& inputs_, const std::vector<std::vector<pir::Value>>& outputs, const std::vector<std::vector<pir::Value>>& out_grads, const std::vector<std::vector<bool>>& stop_gradients){

    PADDLE_ENFORCE_EQ(
      inputs_.size(),
      3,
      platform::errors::InvalidArgument("unsqueeze_grad op's inputs size should be 3, but now is %d.", inputs_.size()));
    PADDLE_ENFORCE_EQ(
      outputs.size(),
      1,
      platform::errors::InvalidArgument("unsqueeze_grad op's outputs size should be 1, but now is %d.", outputs.size()));

    VLOG(6) << "Prepare inputs of unsqueeze_double_grad";

    Tensor grad_x_grad(std::make_shared<primitive::LazyTensor>(out_grads[0][0]));

    VLOG(6) << "Vjp prepare Prepare attributes of unsqueeze_double_grad";

    Tensor axis(std::make_shared<primitive::LazyTensor>(inputs_[2][0]));

    VLOG(6) << "Vjp prepare call unsqueeze_grad's vjp inteface";

    std::vector<std::vector<Tensor>> tensor_res =
        primitive::unsqueeze_grad_vjp(
        grad_x_grad, axis, stop_gradients);

    VLOG(6) << "Vjp prepare stop gradient of unsqueeze_double_grad";

    std::vector<std::vector<pir::OpResult>> res(tensor_res.size());
    for (size_t i = 0; i < tensor_res.size(); ++i) {
        res[i].resize(tensor_res[i].size());
        for (size_t j = 0; j < tensor_res[i].size(); ++j) {
            if(tensor_res[i][j].defined()){
                res[i][j] = std::static_pointer_cast<primitive::LazyTensor>(tensor_res[i][j].impl())->value().dyn_cast<pir::OpResult>();
            }
        }
    }
    return res;
}

std::vector<std::vector<pir::OpResult>> UnsqueezeGrad_Op::Vjp(pir::Operation* op, const std::vector<std::vector<pir::Value>>& inputs_, const std::vector<std::vector<pir::Value>>& outputs, const std::vector<std::vector<pir::Value>>& out_grads, const std::vector<std::vector<bool>>& stop_gradients){

    PADDLE_ENFORCE_EQ(
      inputs_.size(),
      3,
      platform::errors::InvalidArgument("unsqueeze_grad op's inputs size should be 3, but now is %d.", inputs_.size()));
    PADDLE_ENFORCE_EQ(
      outputs.size(),
      1,
      platform::errors::InvalidArgument("unsqueeze_grad op's outputs size should be 1, but now is %d.", outputs.size()));

    VLOG(6) << "Prepare inputs of unsqueeze_double_grad";

    Tensor grad_x_grad(std::make_shared<primitive::LazyTensor>(out_grads[0][0]));

    VLOG(6) << "Vjp prepare Prepare attributes of unsqueeze_double_grad";

    Tensor axis(std::make_shared<primitive::LazyTensor>(inputs_[2][0]));

    VLOG(6) << "Vjp prepare call unsqueeze_grad_'s vjp inteface";

    std::vector<std::vector<Tensor>> tensor_res =
        primitive::unsqueeze_grad_vjp(
        grad_x_grad, axis, stop_gradients);

    VLOG(6) << "Vjp prepare stop gradient of unsqueeze_double_grad";

    std::vector<std::vector<pir::OpResult>> res(tensor_res.size());
    for (size_t i = 0; i < tensor_res.size(); ++i) {
        res[i].resize(tensor_res[i].size());
        for (size_t j = 0; j < tensor_res[i].size(); ++j) {
            if(tensor_res[i][j].defined()){
                res[i][j] = std::static_pointer_cast<primitive::LazyTensor>(tensor_res[i][j].impl())->value().dyn_cast<pir::OpResult>();
            }
        }
    }
    return res;
}




std::vector<std::vector<pir::OpResult>> AddOp::Vjp(pir::Operation* op, const std::vector<std::vector<pir::Value>>& inputs_, const std::vector<std::vector<pir::Value>>& outputs, const std::vector<std::vector<pir::Value>>& out_grads, const std::vector<std::vector<bool>>& stop_gradients){

    PADDLE_ENFORCE_EQ(
      inputs_.size(),
      2,
      platform::errors::InvalidArgument("add op's inputs size should be 2, but now is %d.", inputs_.size()));
    PADDLE_ENFORCE_EQ(
      outputs.size(),
      1,
      platform::errors::InvalidArgument("add op's outputs size should be 1, but now is %d.", outputs.size()));

    VLOG(6) << "Prepare inputs of add_grad";

    Tensor x(std::make_shared<primitive::LazyTensor>(inputs_[0][0]));
    Tensor y(std::make_shared<primitive::LazyTensor>(inputs_[1][0]));
    Tensor out_grad(std::make_shared<primitive::LazyTensor>(out_grads[0][0]));

    VLOG(6) << "Vjp prepare Prepare attributes of add_grad";

    int axis = -1;

    VLOG(6) << "Vjp prepare call add's vjp inteface";

    std::vector<std::vector<Tensor>> tensor_res =
        primitive::add_vjp(
        x, y, out_grad, axis, stop_gradients);

    VLOG(6) << "Vjp prepare stop gradient of add_grad";

    std::vector<std::vector<pir::OpResult>> res(tensor_res.size());
    for (size_t i = 0; i < tensor_res.size(); ++i) {
        res[i].resize(tensor_res[i].size());
        for (size_t j = 0; j < tensor_res[i].size(); ++j) {
            if(tensor_res[i][j].defined()){
                res[i][j] = std::static_pointer_cast<primitive::LazyTensor>(tensor_res[i][j].impl())->value().dyn_cast<pir::OpResult>();
            }
        }
    }
    return res;
}

std::vector<std::vector<pir::OpResult>> Add_Op::Vjp(pir::Operation* op, const std::vector<std::vector<pir::Value>>& inputs_, const std::vector<std::vector<pir::Value>>& outputs, const std::vector<std::vector<pir::Value>>& out_grads, const std::vector<std::vector<bool>>& stop_gradients){

    PADDLE_ENFORCE_EQ(
      inputs_.size(),
      2,
      platform::errors::InvalidArgument("add op's inputs size should be 2, but now is %d.", inputs_.size()));
    PADDLE_ENFORCE_EQ(
      outputs.size(),
      1,
      platform::errors::InvalidArgument("add op's outputs size should be 1, but now is %d.", outputs.size()));

    VLOG(6) << "Prepare inputs of add_grad";

    Tensor x(std::make_shared<primitive::LazyTensor>(inputs_[0][0]));
    Tensor y(std::make_shared<primitive::LazyTensor>(inputs_[1][0]));
    Tensor out_grad(std::make_shared<primitive::LazyTensor>(out_grads[0][0]));

    VLOG(6) << "Vjp prepare Prepare attributes of add_grad";

    int axis = -1;

    VLOG(6) << "Vjp prepare call add_'s vjp inteface";

    std::vector<std::vector<Tensor>> tensor_res =
        primitive::add_vjp(
        x, y, out_grad, axis, stop_gradients);

    VLOG(6) << "Vjp prepare stop gradient of add_grad";

    std::vector<std::vector<pir::OpResult>> res(tensor_res.size());
    for (size_t i = 0; i < tensor_res.size(); ++i) {
        res[i].resize(tensor_res[i].size());
        for (size_t j = 0; j < tensor_res[i].size(); ++j) {
            if(tensor_res[i][j].defined()){
                res[i][j] = std::static_pointer_cast<primitive::LazyTensor>(tensor_res[i][j].impl())->value().dyn_cast<pir::OpResult>();
            }
        }
    }
    return res;
}

std::vector<std::vector<pir::OpResult>> AmaxOp::Vjp(pir::Operation* op, const std::vector<std::vector<pir::Value>>& inputs_, const std::vector<std::vector<pir::Value>>& outputs, const std::vector<std::vector<pir::Value>>& out_grads, const std::vector<std::vector<bool>>& stop_gradients){

    PADDLE_ENFORCE_EQ(
      inputs_.size(),
      1,
      platform::errors::InvalidArgument("amax op's inputs size should be 1, but now is %d.", inputs_.size()));
    PADDLE_ENFORCE_EQ(
      outputs.size(),
      1,
      platform::errors::InvalidArgument("amax op's outputs size should be 1, but now is %d.", outputs.size()));

    VLOG(6) << "Prepare inputs of amax_grad";

    Tensor x(std::make_shared<primitive::LazyTensor>(inputs_[0][0]));
    Tensor out(std::make_shared<primitive::LazyTensor>(outputs[0][0]));
    Tensor out_grad(std::make_shared<primitive::LazyTensor>(out_grads[0][0]));

    VLOG(6) << "Vjp prepare Prepare attributes of amax_grad";

    std::vector<int64_t> axis;
    for (size_t i = 0; i < op->attribute("axis").dyn_cast<pir::ArrayAttribute>().size(); i++) {
        axis.push_back(op->attribute("axis").dyn_cast<pir::ArrayAttribute>().at(i).dyn_cast<pir::Int64Attribute>().data());
    }
    bool keepdim = op->attribute("keepdim").dyn_cast<pir::BoolAttribute>().data();
    bool reduce_all = false;

    VLOG(6) << "Vjp prepare call amax's vjp inteface";

    std::vector<std::vector<Tensor>> tensor_res =
        primitive::amax_vjp(
        x, out, out_grad, axis, keepdim, reduce_all, stop_gradients);

    VLOG(6) << "Vjp prepare stop gradient of amax_grad";

    std::vector<std::vector<pir::OpResult>> res(tensor_res.size());
    for (size_t i = 0; i < tensor_res.size(); ++i) {
        res[i].resize(tensor_res[i].size());
        for (size_t j = 0; j < tensor_res[i].size(); ++j) {
            if(tensor_res[i][j].defined()){
                res[i][j] = std::static_pointer_cast<primitive::LazyTensor>(tensor_res[i][j].impl())->value().dyn_cast<pir::OpResult>();
            }
        }
    }
    return res;
}

std::vector<std::vector<pir::OpResult>> AminOp::Vjp(pir::Operation* op, const std::vector<std::vector<pir::Value>>& inputs_, const std::vector<std::vector<pir::Value>>& outputs, const std::vector<std::vector<pir::Value>>& out_grads, const std::vector<std::vector<bool>>& stop_gradients){

    PADDLE_ENFORCE_EQ(
      inputs_.size(),
      1,
      platform::errors::InvalidArgument("amin op's inputs size should be 1, but now is %d.", inputs_.size()));
    PADDLE_ENFORCE_EQ(
      outputs.size(),
      1,
      platform::errors::InvalidArgument("amin op's outputs size should be 1, but now is %d.", outputs.size()));

    VLOG(6) << "Prepare inputs of amin_grad";

    Tensor x(std::make_shared<primitive::LazyTensor>(inputs_[0][0]));
    Tensor out(std::make_shared<primitive::LazyTensor>(outputs[0][0]));
    Tensor out_grad(std::make_shared<primitive::LazyTensor>(out_grads[0][0]));

    VLOG(6) << "Vjp prepare Prepare attributes of amin_grad";

    std::vector<int64_t> axis;
    for (size_t i = 0; i < op->attribute("axis").dyn_cast<pir::ArrayAttribute>().size(); i++) {
        axis.push_back(op->attribute("axis").dyn_cast<pir::ArrayAttribute>().at(i).dyn_cast<pir::Int64Attribute>().data());
    }
    bool keepdim = op->attribute("keepdim").dyn_cast<pir::BoolAttribute>().data();
    bool reduce_all = false;

    VLOG(6) << "Vjp prepare call amin's vjp inteface";

    std::vector<std::vector<Tensor>> tensor_res =
        primitive::amin_vjp(
        x, out, out_grad, axis, keepdim, reduce_all, stop_gradients);

    VLOG(6) << "Vjp prepare stop gradient of amin_grad";

    std::vector<std::vector<pir::OpResult>> res(tensor_res.size());
    for (size_t i = 0; i < tensor_res.size(); ++i) {
        res[i].resize(tensor_res[i].size());
        for (size_t j = 0; j < tensor_res[i].size(); ++j) {
            if(tensor_res[i][j].defined()){
                res[i][j] = std::static_pointer_cast<primitive::LazyTensor>(tensor_res[i][j].impl())->value().dyn_cast<pir::OpResult>();
            }
        }
    }
    return res;
}

std::vector<std::vector<pir::OpResult>> AssignOp::Vjp(pir::Operation* op, const std::vector<std::vector<pir::Value>>& inputs_, const std::vector<std::vector<pir::Value>>& outputs, const std::vector<std::vector<pir::Value>>& out_grads, const std::vector<std::vector<bool>>& stop_gradients){

    PADDLE_ENFORCE_EQ(
      inputs_.size(),
      1,
      platform::errors::InvalidArgument("assign op's inputs size should be 1, but now is %d.", inputs_.size()));
    PADDLE_ENFORCE_EQ(
      outputs.size(),
      1,
      platform::errors::InvalidArgument("assign op's outputs size should be 1, but now is %d.", outputs.size()));

    VLOG(6) << "Prepare inputs of assign_grad";

    Tensor out_grad(std::make_shared<primitive::LazyTensor>(out_grads[0][0]));

    VLOG(6) << "Vjp prepare Prepare attributes of assign_grad";


    VLOG(6) << "Vjp prepare call assign's vjp inteface";

    std::vector<std::vector<Tensor>> tensor_res =
        primitive::assign_vjp(
        out_grad, stop_gradients);

    VLOG(6) << "Vjp prepare stop gradient of assign_grad";

    std::vector<std::vector<pir::OpResult>> res(tensor_res.size());
    for (size_t i = 0; i < tensor_res.size(); ++i) {
        res[i].resize(tensor_res[i].size());
        for (size_t j = 0; j < tensor_res[i].size(); ++j) {
            if(tensor_res[i][j].defined()){
                res[i][j] = std::static_pointer_cast<primitive::LazyTensor>(tensor_res[i][j].impl())->value().dyn_cast<pir::OpResult>();
            }
        }
    }
    return res;
}

std::vector<std::vector<pir::OpResult>> Assign_Op::Vjp(pir::Operation* op, const std::vector<std::vector<pir::Value>>& inputs_, const std::vector<std::vector<pir::Value>>& outputs, const std::vector<std::vector<pir::Value>>& out_grads, const std::vector<std::vector<bool>>& stop_gradients){

    PADDLE_ENFORCE_EQ(
      inputs_.size(),
      1,
      platform::errors::InvalidArgument("assign op's inputs size should be 1, but now is %d.", inputs_.size()));
    PADDLE_ENFORCE_EQ(
      outputs.size(),
      1,
      platform::errors::InvalidArgument("assign op's outputs size should be 1, but now is %d.", outputs.size()));

    VLOG(6) << "Prepare inputs of assign_grad";

    Tensor out_grad(std::make_shared<primitive::LazyTensor>(out_grads[0][0]));

    VLOG(6) << "Vjp prepare Prepare attributes of assign_grad";


    VLOG(6) << "Vjp prepare call assign_'s vjp inteface";

    std::vector<std::vector<Tensor>> tensor_res =
        primitive::assign_vjp(
        out_grad, stop_gradients);

    VLOG(6) << "Vjp prepare stop gradient of assign_grad";

    std::vector<std::vector<pir::OpResult>> res(tensor_res.size());
    for (size_t i = 0; i < tensor_res.size(); ++i) {
        res[i].resize(tensor_res[i].size());
        for (size_t j = 0; j < tensor_res[i].size(); ++j) {
            if(tensor_res[i][j].defined()){
                res[i][j] = std::static_pointer_cast<primitive::LazyTensor>(tensor_res[i][j].impl())->value().dyn_cast<pir::OpResult>();
            }
        }
    }
    return res;
}

std::vector<std::vector<pir::OpResult>> AssignOut_Op::Vjp(pir::Operation* op, const std::vector<std::vector<pir::Value>>& inputs_, const std::vector<std::vector<pir::Value>>& outputs, const std::vector<std::vector<pir::Value>>& out_grads, const std::vector<std::vector<bool>>& stop_gradients){

    PADDLE_ENFORCE_EQ(
      inputs_.size(),
      2,
      platform::errors::InvalidArgument("assign_out_ op's inputs size should be 2, but now is %d.", inputs_.size()));
    PADDLE_ENFORCE_EQ(
      outputs.size(),
      1,
      platform::errors::InvalidArgument("assign_out_ op's outputs size should be 1, but now is %d.", outputs.size()));

    VLOG(6) << "Prepare inputs of assign_out__grad";

    Tensor out_grad(std::make_shared<primitive::LazyTensor>(out_grads[0][0]));

    VLOG(6) << "Vjp prepare Prepare attributes of assign_out__grad";


    VLOG(6) << "Vjp prepare call assign_out_'s vjp inteface";

    std::vector<std::vector<Tensor>> tensor_res =
        primitive::assign_out__vjp(
        out_grad, stop_gradients);

    VLOG(6) << "Vjp prepare stop gradient of assign_out__grad";

    std::vector<std::vector<pir::OpResult>> res(tensor_res.size());
    for (size_t i = 0; i < tensor_res.size(); ++i) {
        res[i].resize(tensor_res[i].size());
        for (size_t j = 0; j < tensor_res[i].size(); ++j) {
            if(tensor_res[i][j].defined()){
                res[i][j] = std::static_pointer_cast<primitive::LazyTensor>(tensor_res[i][j].impl())->value().dyn_cast<pir::OpResult>();
            }
        }
    }
    return res;
}

std::vector<std::vector<pir::OpResult>> BatchNormOp::Vjp(pir::Operation* op, const std::vector<std::vector<pir::Value>>& inputs_, const std::vector<std::vector<pir::Value>>& outputs, const std::vector<std::vector<pir::Value>>& out_grads, const std::vector<std::vector<bool>>& stop_gradients){

    PADDLE_ENFORCE_EQ(
      inputs_.size(),
      5,
      platform::errors::InvalidArgument("batch_norm op's inputs size should be 5, but now is %d.", inputs_.size()));
    PADDLE_ENFORCE_EQ(
      outputs.size(),
      6,
      platform::errors::InvalidArgument("batch_norm op's outputs size should be 6, but now is %d.", outputs.size()));

    VLOG(6) << "Prepare inputs of batch_norm_grad";

    Tensor x(std::make_shared<primitive::LazyTensor>(inputs_[0][0]));
    paddle::optional<Tensor> scale;
    if (!IsEmptyValue(inputs_[3][0])){
        scale = paddle::make_optional<Tensor>(Tensor(std::make_shared<primitive::LazyTensor>(inputs_[3][0])));
    }
    paddle::optional<Tensor> bias;
    if (!IsEmptyValue(inputs_[4][0])){
        bias = paddle::make_optional<Tensor>(Tensor(std::make_shared<primitive::LazyTensor>(inputs_[4][0])));
    }
    paddle::optional<Tensor> mean_out;
    if (!IsEmptyValue(outputs[1][0])){
        mean_out = paddle::make_optional<Tensor>(Tensor(std::make_shared<primitive::LazyTensor>(outputs[1][0])));
    }
    paddle::optional<Tensor> variance_out;
    if (!IsEmptyValue(outputs[2][0])){
        variance_out = paddle::make_optional<Tensor>(Tensor(std::make_shared<primitive::LazyTensor>(outputs[2][0])));
    }
    Tensor saved_mean(std::make_shared<primitive::LazyTensor>(outputs[3][0]));
    Tensor saved_variance(std::make_shared<primitive::LazyTensor>(outputs[4][0]));
    paddle::optional<Tensor> reserve_space;
    if (!IsEmptyValue(outputs[5][0])){
        reserve_space = paddle::make_optional<Tensor>(Tensor(std::make_shared<primitive::LazyTensor>(outputs[5][0])));
    }
    Tensor out_grad(std::make_shared<primitive::LazyTensor>(out_grads[0][0]));

    VLOG(6) << "Vjp prepare Prepare attributes of batch_norm_grad";

    float momentum = op->attribute("momentum").dyn_cast<pir::FloatAttribute>().data();
    float epsilon = op->attribute("epsilon").dyn_cast<pir::FloatAttribute>().data();
    std::string data_layout = op->attribute("data_layout").dyn_cast<pir::StrAttribute>().AsString();
    bool is_test = op->attribute("is_test").dyn_cast<pir::BoolAttribute>().data();
    bool use_global_stats = op->attribute("use_global_stats").dyn_cast<pir::BoolAttribute>().data();
    bool trainable_statistics = op->attribute("trainable_statistics").dyn_cast<pir::BoolAttribute>().data();

    VLOG(6) << "Vjp prepare call batch_norm's vjp inteface";

    std::vector<std::vector<Tensor>> tensor_res =
        primitive::batch_norm_vjp(
        x, scale, bias, mean_out, variance_out, saved_mean, saved_variance, reserve_space, out_grad, momentum, epsilon, data_layout, is_test, use_global_stats, trainable_statistics, stop_gradients);

    VLOG(6) << "Vjp prepare stop gradient of batch_norm_grad";

    std::vector<std::vector<pir::OpResult>> res(tensor_res.size());
    for (size_t i = 0; i < tensor_res.size(); ++i) {
        res[i].resize(tensor_res[i].size());
        for (size_t j = 0; j < tensor_res[i].size(); ++j) {
            if(tensor_res[i][j].defined()){
                res[i][j] = std::static_pointer_cast<primitive::LazyTensor>(tensor_res[i][j].impl())->value().dyn_cast<pir::OpResult>();
            }
        }
    }
    return res;
}

std::vector<std::vector<pir::OpResult>> BatchNorm_Op::Vjp(pir::Operation* op, const std::vector<std::vector<pir::Value>>& inputs_, const std::vector<std::vector<pir::Value>>& outputs, const std::vector<std::vector<pir::Value>>& out_grads, const std::vector<std::vector<bool>>& stop_gradients){

    PADDLE_ENFORCE_EQ(
      inputs_.size(),
      5,
      platform::errors::InvalidArgument("batch_norm op's inputs size should be 5, but now is %d.", inputs_.size()));
    PADDLE_ENFORCE_EQ(
      outputs.size(),
      6,
      platform::errors::InvalidArgument("batch_norm op's outputs size should be 6, but now is %d.", outputs.size()));

    VLOG(6) << "Prepare inputs of batch_norm_grad";

    Tensor x(std::make_shared<primitive::LazyTensor>(inputs_[0][0]));
    paddle::optional<Tensor> scale;
    if (!IsEmptyValue(inputs_[3][0])){
        scale = paddle::make_optional<Tensor>(Tensor(std::make_shared<primitive::LazyTensor>(inputs_[3][0])));
    }
    paddle::optional<Tensor> bias;
    if (!IsEmptyValue(inputs_[4][0])){
        bias = paddle::make_optional<Tensor>(Tensor(std::make_shared<primitive::LazyTensor>(inputs_[4][0])));
    }
    paddle::optional<Tensor> mean_out;
    if (!IsEmptyValue(outputs[1][0])){
        mean_out = paddle::make_optional<Tensor>(Tensor(std::make_shared<primitive::LazyTensor>(outputs[1][0])));
    }
    paddle::optional<Tensor> variance_out;
    if (!IsEmptyValue(outputs[2][0])){
        variance_out = paddle::make_optional<Tensor>(Tensor(std::make_shared<primitive::LazyTensor>(outputs[2][0])));
    }
    Tensor saved_mean(std::make_shared<primitive::LazyTensor>(outputs[3][0]));
    Tensor saved_variance(std::make_shared<primitive::LazyTensor>(outputs[4][0]));
    paddle::optional<Tensor> reserve_space;
    if (!IsEmptyValue(outputs[5][0])){
        reserve_space = paddle::make_optional<Tensor>(Tensor(std::make_shared<primitive::LazyTensor>(outputs[5][0])));
    }
    Tensor out_grad(std::make_shared<primitive::LazyTensor>(out_grads[0][0]));

    VLOG(6) << "Vjp prepare Prepare attributes of batch_norm_grad";

    float momentum = op->attribute("momentum").dyn_cast<pir::FloatAttribute>().data();
    float epsilon = op->attribute("epsilon").dyn_cast<pir::FloatAttribute>().data();
    std::string data_layout = op->attribute("data_layout").dyn_cast<pir::StrAttribute>().AsString();
    bool is_test = op->attribute("is_test").dyn_cast<pir::BoolAttribute>().data();
    bool use_global_stats = op->attribute("use_global_stats").dyn_cast<pir::BoolAttribute>().data();
    bool trainable_statistics = op->attribute("trainable_statistics").dyn_cast<pir::BoolAttribute>().data();

    VLOG(6) << "Vjp prepare call batch_norm_'s vjp inteface";

    std::vector<std::vector<Tensor>> tensor_res =
        primitive::batch_norm_vjp(
        x, scale, bias, mean_out, variance_out, saved_mean, saved_variance, reserve_space, out_grad, momentum, epsilon, data_layout, is_test, use_global_stats, trainable_statistics, stop_gradients);

    VLOG(6) << "Vjp prepare stop gradient of batch_norm_grad";

    std::vector<std::vector<pir::OpResult>> res(tensor_res.size());
    for (size_t i = 0; i < tensor_res.size(); ++i) {
        res[i].resize(tensor_res[i].size());
        for (size_t j = 0; j < tensor_res[i].size(); ++j) {
            if(tensor_res[i][j].defined()){
                res[i][j] = std::static_pointer_cast<primitive::LazyTensor>(tensor_res[i][j].impl())->value().dyn_cast<pir::OpResult>();
            }
        }
    }
    return res;
}

std::vector<std::vector<pir::OpResult>> CEmbeddingOp::Vjp(pir::Operation* op, const std::vector<std::vector<pir::Value>>& inputs_, const std::vector<std::vector<pir::Value>>& outputs, const std::vector<std::vector<pir::Value>>& out_grads, const std::vector<std::vector<bool>>& stop_gradients){

    PADDLE_ENFORCE_EQ(
      inputs_.size(),
      2,
      platform::errors::InvalidArgument("c_embedding op's inputs size should be 2, but now is %d.", inputs_.size()));
    PADDLE_ENFORCE_EQ(
      outputs.size(),
      1,
      platform::errors::InvalidArgument("c_embedding op's outputs size should be 1, but now is %d.", outputs.size()));

    VLOG(6) << "Prepare inputs of c_embedding_grad";

    Tensor weight(std::make_shared<primitive::LazyTensor>(inputs_[0][0]));
    Tensor x(std::make_shared<primitive::LazyTensor>(inputs_[1][0]));
    Tensor out_grad(std::make_shared<primitive::LazyTensor>(out_grads[0][0]));

    VLOG(6) << "Vjp prepare Prepare attributes of c_embedding_grad";

    int64_t start_index = op->attribute("start_index").dyn_cast<pir::Int64Attribute>().data();

    VLOG(6) << "Vjp prepare call c_embedding's vjp inteface";

    std::vector<std::vector<Tensor>> tensor_res =
        primitive::c_embedding_vjp(
        weight, x, out_grad, start_index, stop_gradients);

    VLOG(6) << "Vjp prepare stop gradient of c_embedding_grad";

    std::vector<std::vector<pir::OpResult>> res(tensor_res.size());
    for (size_t i = 0; i < tensor_res.size(); ++i) {
        res[i].resize(tensor_res[i].size());
        for (size_t j = 0; j < tensor_res[i].size(); ++j) {
            if(tensor_res[i][j].defined()){
                res[i][j] = std::static_pointer_cast<primitive::LazyTensor>(tensor_res[i][j].impl())->value().dyn_cast<pir::OpResult>();
            }
        }
    }
    return res;
}

std::vector<std::vector<pir::OpResult>> CastOp::Vjp(pir::Operation* op, const std::vector<std::vector<pir::Value>>& inputs_, const std::vector<std::vector<pir::Value>>& outputs, const std::vector<std::vector<pir::Value>>& out_grads, const std::vector<std::vector<bool>>& stop_gradients){

    PADDLE_ENFORCE_EQ(
      inputs_.size(),
      1,
      platform::errors::InvalidArgument("cast op's inputs size should be 1, but now is %d.", inputs_.size()));
    PADDLE_ENFORCE_EQ(
      outputs.size(),
      1,
      platform::errors::InvalidArgument("cast op's outputs size should be 1, but now is %d.", outputs.size()));

    VLOG(6) << "Prepare inputs of cast_grad";

    Tensor x(std::make_shared<primitive::LazyTensor>(inputs_[0][0]));
    Tensor out_grad(std::make_shared<primitive::LazyTensor>(out_grads[0][0]));

    VLOG(6) << "Vjp prepare Prepare attributes of cast_grad";


    VLOG(6) << "Vjp prepare call cast's vjp inteface";

    std::vector<std::vector<Tensor>> tensor_res =
        primitive::cast_vjp(
        x, out_grad, stop_gradients);

    VLOG(6) << "Vjp prepare stop gradient of cast_grad";

    std::vector<std::vector<pir::OpResult>> res(tensor_res.size());
    for (size_t i = 0; i < tensor_res.size(); ++i) {
        res[i].resize(tensor_res[i].size());
        for (size_t j = 0; j < tensor_res[i].size(); ++j) {
            if(tensor_res[i][j].defined()){
                res[i][j] = std::static_pointer_cast<primitive::LazyTensor>(tensor_res[i][j].impl())->value().dyn_cast<pir::OpResult>();
            }
        }
    }
    return res;
}

std::vector<std::vector<pir::OpResult>> Cast_Op::Vjp(pir::Operation* op, const std::vector<std::vector<pir::Value>>& inputs_, const std::vector<std::vector<pir::Value>>& outputs, const std::vector<std::vector<pir::Value>>& out_grads, const std::vector<std::vector<bool>>& stop_gradients){

    PADDLE_ENFORCE_EQ(
      inputs_.size(),
      1,
      platform::errors::InvalidArgument("cast op's inputs size should be 1, but now is %d.", inputs_.size()));
    PADDLE_ENFORCE_EQ(
      outputs.size(),
      1,
      platform::errors::InvalidArgument("cast op's outputs size should be 1, but now is %d.", outputs.size()));

    VLOG(6) << "Prepare inputs of cast_grad";

    Tensor x(std::make_shared<primitive::LazyTensor>(inputs_[0][0]));
    Tensor out_grad(std::make_shared<primitive::LazyTensor>(out_grads[0][0]));

    VLOG(6) << "Vjp prepare Prepare attributes of cast_grad";


    VLOG(6) << "Vjp prepare call cast_'s vjp inteface";

    std::vector<std::vector<Tensor>> tensor_res =
        primitive::cast_vjp(
        x, out_grad, stop_gradients);

    VLOG(6) << "Vjp prepare stop gradient of cast_grad";

    std::vector<std::vector<pir::OpResult>> res(tensor_res.size());
    for (size_t i = 0; i < tensor_res.size(); ++i) {
        res[i].resize(tensor_res[i].size());
        for (size_t j = 0; j < tensor_res[i].size(); ++j) {
            if(tensor_res[i][j].defined()){
                res[i][j] = std::static_pointer_cast<primitive::LazyTensor>(tensor_res[i][j].impl())->value().dyn_cast<pir::OpResult>();
            }
        }
    }
    return res;
}

std::vector<std::vector<pir::OpResult>> ChannelShuffleOp::Vjp(pir::Operation* op, const std::vector<std::vector<pir::Value>>& inputs_, const std::vector<std::vector<pir::Value>>& outputs, const std::vector<std::vector<pir::Value>>& out_grads, const std::vector<std::vector<bool>>& stop_gradients){

    PADDLE_ENFORCE_EQ(
      inputs_.size(),
      1,
      platform::errors::InvalidArgument("channel_shuffle op's inputs size should be 1, but now is %d.", inputs_.size()));
    PADDLE_ENFORCE_EQ(
      outputs.size(),
      1,
      platform::errors::InvalidArgument("channel_shuffle op's outputs size should be 1, but now is %d.", outputs.size()));

    VLOG(6) << "Prepare inputs of channel_shuffle_grad";

    Tensor out_grad(std::make_shared<primitive::LazyTensor>(out_grads[0][0]));

    VLOG(6) << "Vjp prepare Prepare attributes of channel_shuffle_grad";

    int groups = op->attribute("groups").dyn_cast<pir::Int32Attribute>().data();
    std::string data_format = op->attribute("data_format").dyn_cast<pir::StrAttribute>().AsString();

    VLOG(6) << "Vjp prepare call channel_shuffle's vjp inteface";

    std::vector<std::vector<Tensor>> tensor_res =
        primitive::channel_shuffle_vjp(
        out_grad, groups, data_format, stop_gradients);

    VLOG(6) << "Vjp prepare stop gradient of channel_shuffle_grad";

    std::vector<std::vector<pir::OpResult>> res(tensor_res.size());
    for (size_t i = 0; i < tensor_res.size(); ++i) {
        res[i].resize(tensor_res[i].size());
        for (size_t j = 0; j < tensor_res[i].size(); ++j) {
            if(tensor_res[i][j].defined()){
                res[i][j] = std::static_pointer_cast<primitive::LazyTensor>(tensor_res[i][j].impl())->value().dyn_cast<pir::OpResult>();
            }
        }
    }
    return res;
}

std::vector<std::vector<pir::OpResult>> Conv2dTransposeOp::Vjp(pir::Operation* op, const std::vector<std::vector<pir::Value>>& inputs_, const std::vector<std::vector<pir::Value>>& outputs, const std::vector<std::vector<pir::Value>>& out_grads, const std::vector<std::vector<bool>>& stop_gradients){

    PADDLE_ENFORCE_EQ(
      inputs_.size(),
      3,
      platform::errors::InvalidArgument("conv2d_transpose op's inputs size should be 3, but now is %d.", inputs_.size()));
    PADDLE_ENFORCE_EQ(
      outputs.size(),
      1,
      platform::errors::InvalidArgument("conv2d_transpose op's outputs size should be 1, but now is %d.", outputs.size()));

    VLOG(6) << "Prepare inputs of conv2d_transpose_grad";

    Tensor x(std::make_shared<primitive::LazyTensor>(inputs_[0][0]));
    Tensor filter(std::make_shared<primitive::LazyTensor>(inputs_[1][0]));
    Tensor out_grad(std::make_shared<primitive::LazyTensor>(out_grads[0][0]));

    VLOG(6) << "Vjp prepare Prepare attributes of conv2d_transpose_grad";

    std::vector<int> strides;
    for (size_t i = 0; i < op->attribute("strides").dyn_cast<pir::ArrayAttribute>().size(); i++) {
        strides.push_back(op->attribute("strides").dyn_cast<pir::ArrayAttribute>().at(i).dyn_cast<pir::Int32Attribute>().data());
    }
    std::vector<int> paddings;
    for (size_t i = 0; i < op->attribute("paddings").dyn_cast<pir::ArrayAttribute>().size(); i++) {
        paddings.push_back(op->attribute("paddings").dyn_cast<pir::ArrayAttribute>().at(i).dyn_cast<pir::Int32Attribute>().data());
    }
    std::vector<int> output_padding;
    for (size_t i = 0; i < op->attribute("output_padding").dyn_cast<pir::ArrayAttribute>().size(); i++) {
        output_padding.push_back(op->attribute("output_padding").dyn_cast<pir::ArrayAttribute>().at(i).dyn_cast<pir::Int32Attribute>().data());
    }
    Tensor output_size(std::make_shared<primitive::LazyTensor>(inputs_[2][0]));
    std::string padding_algorithm = op->attribute("padding_algorithm").dyn_cast<pir::StrAttribute>().AsString();
    int groups = op->attribute("groups").dyn_cast<pir::Int32Attribute>().data();
    std::vector<int> dilations;
    for (size_t i = 0; i < op->attribute("dilations").dyn_cast<pir::ArrayAttribute>().size(); i++) {
        dilations.push_back(op->attribute("dilations").dyn_cast<pir::ArrayAttribute>().at(i).dyn_cast<pir::Int32Attribute>().data());
    }
    std::string data_format = op->attribute("data_format").dyn_cast<pir::StrAttribute>().AsString();

    VLOG(6) << "Vjp prepare call conv2d_transpose's vjp inteface";

    std::vector<std::vector<Tensor>> tensor_res =
        primitive::conv2d_transpose_vjp(
        x, filter, out_grad, output_size, strides, paddings, output_padding, padding_algorithm, groups, dilations, data_format, stop_gradients);

    VLOG(6) << "Vjp prepare stop gradient of conv2d_transpose_grad";

    std::vector<std::vector<pir::OpResult>> res(tensor_res.size());
    for (size_t i = 0; i < tensor_res.size(); ++i) {
        res[i].resize(tensor_res[i].size());
        for (size_t j = 0; j < tensor_res[i].size(); ++j) {
            if(tensor_res[i][j].defined()){
                res[i][j] = std::static_pointer_cast<primitive::LazyTensor>(tensor_res[i][j].impl())->value().dyn_cast<pir::OpResult>();
            }
        }
    }
    return res;
}

std::vector<std::vector<pir::OpResult>> DeformableConvOp::Vjp(pir::Operation* op, const std::vector<std::vector<pir::Value>>& inputs_, const std::vector<std::vector<pir::Value>>& outputs, const std::vector<std::vector<pir::Value>>& out_grads, const std::vector<std::vector<bool>>& stop_gradients){

    PADDLE_ENFORCE_EQ(
      inputs_.size(),
      4,
      platform::errors::InvalidArgument("deformable_conv op's inputs size should be 4, but now is %d.", inputs_.size()));
    PADDLE_ENFORCE_EQ(
      outputs.size(),
      1,
      platform::errors::InvalidArgument("deformable_conv op's outputs size should be 1, but now is %d.", outputs.size()));

    VLOG(6) << "Prepare inputs of deformable_conv_grad";

    Tensor x(std::make_shared<primitive::LazyTensor>(inputs_[0][0]));
    Tensor offset(std::make_shared<primitive::LazyTensor>(inputs_[1][0]));
    Tensor filter(std::make_shared<primitive::LazyTensor>(inputs_[2][0]));
    paddle::optional<Tensor> mask;
    if (!IsEmptyValue(inputs_[3][0])){
        mask = paddle::make_optional<Tensor>(Tensor(std::make_shared<primitive::LazyTensor>(inputs_[3][0])));
    }
    Tensor out_grad(std::make_shared<primitive::LazyTensor>(out_grads[0][0]));

    VLOG(6) << "Vjp prepare Prepare attributes of deformable_conv_grad";

    std::vector<int> strides;
    for (size_t i = 0; i < op->attribute("strides").dyn_cast<pir::ArrayAttribute>().size(); i++) {
        strides.push_back(op->attribute("strides").dyn_cast<pir::ArrayAttribute>().at(i).dyn_cast<pir::Int32Attribute>().data());
    }
    std::vector<int> paddings;
    for (size_t i = 0; i < op->attribute("paddings").dyn_cast<pir::ArrayAttribute>().size(); i++) {
        paddings.push_back(op->attribute("paddings").dyn_cast<pir::ArrayAttribute>().at(i).dyn_cast<pir::Int32Attribute>().data());
    }
    std::vector<int> dilations;
    for (size_t i = 0; i < op->attribute("dilations").dyn_cast<pir::ArrayAttribute>().size(); i++) {
        dilations.push_back(op->attribute("dilations").dyn_cast<pir::ArrayAttribute>().at(i).dyn_cast<pir::Int32Attribute>().data());
    }
    int deformable_groups = op->attribute("deformable_groups").dyn_cast<pir::Int32Attribute>().data();
    int groups = op->attribute("groups").dyn_cast<pir::Int32Attribute>().data();
    int im2col_step = op->attribute("im2col_step").dyn_cast<pir::Int32Attribute>().data();

    VLOG(6) << "Vjp prepare call deformable_conv's vjp inteface";

    std::vector<std::vector<Tensor>> tensor_res =
        primitive::deformable_conv_vjp(
        x, offset, filter, mask, out_grad, strides, paddings, dilations, deformable_groups, groups, im2col_step, stop_gradients);

    VLOG(6) << "Vjp prepare stop gradient of deformable_conv_grad";

    std::vector<std::vector<pir::OpResult>> res(tensor_res.size());
    for (size_t i = 0; i < tensor_res.size(); ++i) {
        res[i].resize(tensor_res[i].size());
        for (size_t j = 0; j < tensor_res[i].size(); ++j) {
            if(tensor_res[i][j].defined()){
                res[i][j] = std::static_pointer_cast<primitive::LazyTensor>(tensor_res[i][j].impl())->value().dyn_cast<pir::OpResult>();
            }
        }
    }
    return res;
}

std::vector<std::vector<pir::OpResult>> DepthwiseConv2dTransposeOp::Vjp(pir::Operation* op, const std::vector<std::vector<pir::Value>>& inputs_, const std::vector<std::vector<pir::Value>>& outputs, const std::vector<std::vector<pir::Value>>& out_grads, const std::vector<std::vector<bool>>& stop_gradients){

    PADDLE_ENFORCE_EQ(
      inputs_.size(),
      3,
      platform::errors::InvalidArgument("depthwise_conv2d_transpose op's inputs size should be 3, but now is %d.", inputs_.size()));
    PADDLE_ENFORCE_EQ(
      outputs.size(),
      1,
      platform::errors::InvalidArgument("depthwise_conv2d_transpose op's outputs size should be 1, but now is %d.", outputs.size()));

    VLOG(6) << "Prepare inputs of depthwise_conv2d_transpose_grad";

    Tensor x(std::make_shared<primitive::LazyTensor>(inputs_[0][0]));
    Tensor filter(std::make_shared<primitive::LazyTensor>(inputs_[1][0]));
    Tensor out_grad(std::make_shared<primitive::LazyTensor>(out_grads[0][0]));

    VLOG(6) << "Vjp prepare Prepare attributes of depthwise_conv2d_transpose_grad";

    std::vector<int> strides;
    for (size_t i = 0; i < op->attribute("strides").dyn_cast<pir::ArrayAttribute>().size(); i++) {
        strides.push_back(op->attribute("strides").dyn_cast<pir::ArrayAttribute>().at(i).dyn_cast<pir::Int32Attribute>().data());
    }
    std::vector<int> paddings;
    for (size_t i = 0; i < op->attribute("paddings").dyn_cast<pir::ArrayAttribute>().size(); i++) {
        paddings.push_back(op->attribute("paddings").dyn_cast<pir::ArrayAttribute>().at(i).dyn_cast<pir::Int32Attribute>().data());
    }
    std::vector<int> output_padding;
    for (size_t i = 0; i < op->attribute("output_padding").dyn_cast<pir::ArrayAttribute>().size(); i++) {
        output_padding.push_back(op->attribute("output_padding").dyn_cast<pir::ArrayAttribute>().at(i).dyn_cast<pir::Int32Attribute>().data());
    }
    Tensor output_size(std::make_shared<primitive::LazyTensor>(inputs_[2][0]));
    std::string padding_algorithm = op->attribute("padding_algorithm").dyn_cast<pir::StrAttribute>().AsString();
    int groups = op->attribute("groups").dyn_cast<pir::Int32Attribute>().data();
    std::vector<int> dilations;
    for (size_t i = 0; i < op->attribute("dilations").dyn_cast<pir::ArrayAttribute>().size(); i++) {
        dilations.push_back(op->attribute("dilations").dyn_cast<pir::ArrayAttribute>().at(i).dyn_cast<pir::Int32Attribute>().data());
    }
    std::string data_format = op->attribute("data_format").dyn_cast<pir::StrAttribute>().AsString();

    VLOG(6) << "Vjp prepare call depthwise_conv2d_transpose's vjp inteface";

    std::vector<std::vector<Tensor>> tensor_res =
        primitive::depthwise_conv2d_transpose_vjp(
        x, filter, out_grad, output_size, strides, paddings, output_padding, padding_algorithm, groups, dilations, data_format, stop_gradients);

    VLOG(6) << "Vjp prepare stop gradient of depthwise_conv2d_transpose_grad";

    std::vector<std::vector<pir::OpResult>> res(tensor_res.size());
    for (size_t i = 0; i < tensor_res.size(); ++i) {
        res[i].resize(tensor_res[i].size());
        for (size_t j = 0; j < tensor_res[i].size(); ++j) {
            if(tensor_res[i][j].defined()){
                res[i][j] = std::static_pointer_cast<primitive::LazyTensor>(tensor_res[i][j].impl())->value().dyn_cast<pir::OpResult>();
            }
        }
    }
    return res;
}

std::vector<std::vector<pir::OpResult>> DisableCheckModelNanInfOp::Vjp(pir::Operation* op, const std::vector<std::vector<pir::Value>>& inputs_, const std::vector<std::vector<pir::Value>>& outputs, const std::vector<std::vector<pir::Value>>& out_grads, const std::vector<std::vector<bool>>& stop_gradients){

    PADDLE_ENFORCE_EQ(
      inputs_.size(),
      1,
      platform::errors::InvalidArgument("disable_check_model_nan_inf op's inputs size should be 1, but now is %d.", inputs_.size()));
    PADDLE_ENFORCE_EQ(
      outputs.size(),
      1,
      platform::errors::InvalidArgument("disable_check_model_nan_inf op's outputs size should be 1, but now is %d.", outputs.size()));

    VLOG(6) << "Prepare inputs of disable_check_model_nan_inf_grad";

    Tensor out_grad(std::make_shared<primitive::LazyTensor>(out_grads[0][0]));

    VLOG(6) << "Vjp prepare Prepare attributes of disable_check_model_nan_inf_grad";

    int unsetflag = 1;

    VLOG(6) << "Vjp prepare call disable_check_model_nan_inf's vjp inteface";

    std::vector<std::vector<Tensor>> tensor_res =
        primitive::disable_check_model_nan_inf_vjp(
        out_grad, unsetflag, stop_gradients);

    VLOG(6) << "Vjp prepare stop gradient of disable_check_model_nan_inf_grad";

    std::vector<std::vector<pir::OpResult>> res(tensor_res.size());
    for (size_t i = 0; i < tensor_res.size(); ++i) {
        res[i].resize(tensor_res[i].size());
        for (size_t j = 0; j < tensor_res[i].size(); ++j) {
            if(tensor_res[i][j].defined()){
                res[i][j] = std::static_pointer_cast<primitive::LazyTensor>(tensor_res[i][j].impl())->value().dyn_cast<pir::OpResult>();
            }
        }
    }
    return res;
}

std::vector<std::vector<pir::OpResult>> DivideOp::Vjp(pir::Operation* op, const std::vector<std::vector<pir::Value>>& inputs_, const std::vector<std::vector<pir::Value>>& outputs, const std::vector<std::vector<pir::Value>>& out_grads, const std::vector<std::vector<bool>>& stop_gradients){

    PADDLE_ENFORCE_EQ(
      inputs_.size(),
      2,
      platform::errors::InvalidArgument("divide op's inputs size should be 2, but now is %d.", inputs_.size()));
    PADDLE_ENFORCE_EQ(
      outputs.size(),
      1,
      platform::errors::InvalidArgument("divide op's outputs size should be 1, but now is %d.", outputs.size()));

    VLOG(6) << "Prepare inputs of divide_grad";

    Tensor x(std::make_shared<primitive::LazyTensor>(inputs_[0][0]));
    Tensor y(std::make_shared<primitive::LazyTensor>(inputs_[1][0]));
    Tensor out(std::make_shared<primitive::LazyTensor>(outputs[0][0]));
    Tensor out_grad(std::make_shared<primitive::LazyTensor>(out_grads[0][0]));

    VLOG(6) << "Vjp prepare Prepare attributes of divide_grad";

    int axis = -1;

    VLOG(6) << "Vjp prepare call divide's vjp inteface";

    std::vector<std::vector<Tensor>> tensor_res =
        primitive::divide_vjp(
        x, y, out, out_grad, axis, stop_gradients);

    VLOG(6) << "Vjp prepare stop gradient of divide_grad";

    std::vector<std::vector<pir::OpResult>> res(tensor_res.size());
    for (size_t i = 0; i < tensor_res.size(); ++i) {
        res[i].resize(tensor_res[i].size());
        for (size_t j = 0; j < tensor_res[i].size(); ++j) {
            if(tensor_res[i][j].defined()){
                res[i][j] = std::static_pointer_cast<primitive::LazyTensor>(tensor_res[i][j].impl())->value().dyn_cast<pir::OpResult>();
            }
        }
    }
    return res;
}

std::vector<std::vector<pir::OpResult>> Divide_Op::Vjp(pir::Operation* op, const std::vector<std::vector<pir::Value>>& inputs_, const std::vector<std::vector<pir::Value>>& outputs, const std::vector<std::vector<pir::Value>>& out_grads, const std::vector<std::vector<bool>>& stop_gradients){

    PADDLE_ENFORCE_EQ(
      inputs_.size(),
      2,
      platform::errors::InvalidArgument("divide op's inputs size should be 2, but now is %d.", inputs_.size()));
    PADDLE_ENFORCE_EQ(
      outputs.size(),
      1,
      platform::errors::InvalidArgument("divide op's outputs size should be 1, but now is %d.", outputs.size()));

    VLOG(6) << "Prepare inputs of divide_grad";

    Tensor x(std::make_shared<primitive::LazyTensor>(inputs_[0][0]));
    Tensor y(std::make_shared<primitive::LazyTensor>(inputs_[1][0]));
    Tensor out(std::make_shared<primitive::LazyTensor>(outputs[0][0]));
    Tensor out_grad(std::make_shared<primitive::LazyTensor>(out_grads[0][0]));

    VLOG(6) << "Vjp prepare Prepare attributes of divide_grad";

    int axis = -1;

    VLOG(6) << "Vjp prepare call divide_'s vjp inteface";

    std::vector<std::vector<Tensor>> tensor_res =
        primitive::divide_vjp(
        x, y, out, out_grad, axis, stop_gradients);

    VLOG(6) << "Vjp prepare stop gradient of divide_grad";

    std::vector<std::vector<pir::OpResult>> res(tensor_res.size());
    for (size_t i = 0; i < tensor_res.size(); ++i) {
        res[i].resize(tensor_res[i].size());
        for (size_t j = 0; j < tensor_res[i].size(); ++j) {
            if(tensor_res[i][j].defined()){
                res[i][j] = std::static_pointer_cast<primitive::LazyTensor>(tensor_res[i][j].impl())->value().dyn_cast<pir::OpResult>();
            }
        }
    }
    return res;
}

std::vector<std::vector<pir::OpResult>> DropoutOp::Vjp(pir::Operation* op, const std::vector<std::vector<pir::Value>>& inputs_, const std::vector<std::vector<pir::Value>>& outputs, const std::vector<std::vector<pir::Value>>& out_grads, const std::vector<std::vector<bool>>& stop_gradients){

    PADDLE_ENFORCE_EQ(
      inputs_.size(),
      2,
      platform::errors::InvalidArgument("dropout op's inputs size should be 2, but now is %d.", inputs_.size()));
    PADDLE_ENFORCE_EQ(
      outputs.size(),
      2,
      platform::errors::InvalidArgument("dropout op's outputs size should be 2, but now is %d.", outputs.size()));

    VLOG(6) << "Prepare inputs of dropout_grad";

    Tensor mask(std::make_shared<primitive::LazyTensor>(outputs[1][0]));
    Tensor out_grad(std::make_shared<primitive::LazyTensor>(out_grads[0][0]));

    VLOG(6) << "Vjp prepare Prepare attributes of dropout_grad";

    phi::Scalar p = op->attribute("p").dyn_cast<paddle::dialect::ScalarAttribute>().data();
    bool is_test = op->attribute("is_test").dyn_cast<pir::BoolAttribute>().data();
    std::string mode = op->attribute("mode").dyn_cast<pir::StrAttribute>().AsString();

    VLOG(6) << "Vjp prepare call dropout's vjp inteface";

    std::vector<std::vector<Tensor>> tensor_res =
        primitive::dropout_vjp(
        mask, out_grad, p, is_test, mode, stop_gradients);

    VLOG(6) << "Vjp prepare stop gradient of dropout_grad";

    std::vector<std::vector<pir::OpResult>> res(tensor_res.size());
    for (size_t i = 0; i < tensor_res.size(); ++i) {
        res[i].resize(tensor_res[i].size());
        for (size_t j = 0; j < tensor_res[i].size(); ++j) {
            if(tensor_res[i][j].defined()){
                res[i][j] = std::static_pointer_cast<primitive::LazyTensor>(tensor_res[i][j].impl())->value().dyn_cast<pir::OpResult>();
            }
        }
    }
    return res;
}

std::vector<std::vector<pir::OpResult>> EinsumOp::Vjp(pir::Operation* op, const std::vector<std::vector<pir::Value>>& inputs_, const std::vector<std::vector<pir::Value>>& outputs, const std::vector<std::vector<pir::Value>>& out_grads, const std::vector<std::vector<bool>>& stop_gradients){

    PADDLE_ENFORCE_EQ(
      inputs_.size(),
      1,
      platform::errors::InvalidArgument("einsum op's inputs size should be 1, but now is %d.", inputs_.size()));
    PADDLE_ENFORCE_EQ(
      outputs.size(),
      3,
      platform::errors::InvalidArgument("einsum op's outputs size should be 3, but now is %d.", outputs.size()));

    VLOG(6) << "Prepare inputs of einsum_grad";

    std::vector<Tensor> x_shape;
    for (size_t idx = 0; idx < outputs[2].size(); idx++) {
        x_shape.emplace_back(
            std::make_shared<primitive::LazyTensor>(outputs[2][idx]));
    }
    std::vector<Tensor> inner_cache;
    for (size_t idx = 0; idx < outputs[1].size(); idx++) {
        inner_cache.emplace_back(
            std::make_shared<primitive::LazyTensor>(outputs[1][idx]));
    }
    Tensor out_grad(std::make_shared<primitive::LazyTensor>(out_grads[0][0]));

    VLOG(6) << "Vjp prepare Prepare attributes of einsum_grad";

    std::string equation = op->attribute("equation").dyn_cast<pir::StrAttribute>().AsString();

    VLOG(6) << "Vjp prepare call einsum's vjp inteface";

    std::vector<std::vector<Tensor>> tensor_res =
        primitive::einsum_vjp(
        x_shape, inner_cache, out_grad, equation, stop_gradients);

    VLOG(6) << "Vjp prepare stop gradient of einsum_grad";

    std::vector<std::vector<pir::OpResult>> res(tensor_res.size());
    for (size_t i = 0; i < tensor_res.size(); ++i) {
        res[i].resize(tensor_res[i].size());
        for (size_t j = 0; j < tensor_res[i].size(); ++j) {
            if(tensor_res[i][j].defined()){
                res[i][j] = std::static_pointer_cast<primitive::LazyTensor>(tensor_res[i][j].impl())->value().dyn_cast<pir::OpResult>();
            }
        }
    }
    return res;
}

std::vector<std::vector<pir::OpResult>> ElementwisePowOp::Vjp(pir::Operation* op, const std::vector<std::vector<pir::Value>>& inputs_, const std::vector<std::vector<pir::Value>>& outputs, const std::vector<std::vector<pir::Value>>& out_grads, const std::vector<std::vector<bool>>& stop_gradients){

    PADDLE_ENFORCE_EQ(
      inputs_.size(),
      2,
      platform::errors::InvalidArgument("elementwise_pow op's inputs size should be 2, but now is %d.", inputs_.size()));
    PADDLE_ENFORCE_EQ(
      outputs.size(),
      1,
      platform::errors::InvalidArgument("elementwise_pow op's outputs size should be 1, but now is %d.", outputs.size()));

    VLOG(6) << "Prepare inputs of elementwise_pow_grad";

    Tensor x(std::make_shared<primitive::LazyTensor>(inputs_[0][0]));
    Tensor y(std::make_shared<primitive::LazyTensor>(inputs_[1][0]));
    Tensor out_grad(std::make_shared<primitive::LazyTensor>(out_grads[0][0]));

    VLOG(6) << "Vjp prepare Prepare attributes of elementwise_pow_grad";


    VLOG(6) << "Vjp prepare call elementwise_pow's vjp inteface";

    std::vector<std::vector<Tensor>> tensor_res =
        primitive::elementwise_pow_vjp(
        x, y, out_grad, stop_gradients);

    VLOG(6) << "Vjp prepare stop gradient of elementwise_pow_grad";

    std::vector<std::vector<pir::OpResult>> res(tensor_res.size());
    for (size_t i = 0; i < tensor_res.size(); ++i) {
        res[i].resize(tensor_res[i].size());
        for (size_t j = 0; j < tensor_res[i].size(); ++j) {
            if(tensor_res[i][j].defined()){
                res[i][j] = std::static_pointer_cast<primitive::LazyTensor>(tensor_res[i][j].impl())->value().dyn_cast<pir::OpResult>();
            }
        }
    }
    return res;
}

std::vector<std::vector<pir::OpResult>> EmbeddingOp::Vjp(pir::Operation* op, const std::vector<std::vector<pir::Value>>& inputs_, const std::vector<std::vector<pir::Value>>& outputs, const std::vector<std::vector<pir::Value>>& out_grads, const std::vector<std::vector<bool>>& stop_gradients){

    PADDLE_ENFORCE_EQ(
      inputs_.size(),
      2,
      platform::errors::InvalidArgument("embedding op's inputs size should be 2, but now is %d.", inputs_.size()));
    PADDLE_ENFORCE_EQ(
      outputs.size(),
      1,
      platform::errors::InvalidArgument("embedding op's outputs size should be 1, but now is %d.", outputs.size()));

    VLOG(6) << "Prepare inputs of embedding_grad";

    Tensor x(std::make_shared<primitive::LazyTensor>(inputs_[0][0]));
    Tensor weight(std::make_shared<primitive::LazyTensor>(inputs_[1][0]));
    Tensor out_grad(std::make_shared<primitive::LazyTensor>(out_grads[0][0]));

    VLOG(6) << "Vjp prepare Prepare attributes of embedding_grad";

    int64_t padding_idx = op->attribute("padding_idx").dyn_cast<pir::Int64Attribute>().data();

    VLOG(6) << "Vjp prepare call embedding's vjp inteface";

    std::vector<std::vector<Tensor>> tensor_res =
        primitive::embedding_vjp(
        x, weight, out_grad, padding_idx, stop_gradients);

    VLOG(6) << "Vjp prepare stop gradient of embedding_grad";

    std::vector<std::vector<pir::OpResult>> res(tensor_res.size());
    for (size_t i = 0; i < tensor_res.size(); ++i) {
        res[i].resize(tensor_res[i].size());
        for (size_t j = 0; j < tensor_res[i].size(); ++j) {
            if(tensor_res[i][j].defined()){
                res[i][j] = std::static_pointer_cast<primitive::LazyTensor>(tensor_res[i][j].impl())->value().dyn_cast<pir::OpResult>();
            }
        }
    }
    return res;
}

std::vector<std::vector<pir::OpResult>> SparseWeightEmbeddingOp::Vjp(pir::Operation* op, const std::vector<std::vector<pir::Value>>& inputs_, const std::vector<std::vector<pir::Value>>& outputs, const std::vector<std::vector<pir::Value>>& out_grads, const std::vector<std::vector<bool>>& stop_gradients){

    PADDLE_ENFORCE_EQ(
      inputs_.size(),
      2,
      platform::errors::InvalidArgument("embedding op's inputs size should be 2, but now is %d.", inputs_.size()));
    PADDLE_ENFORCE_EQ(
      outputs.size(),
      1,
      platform::errors::InvalidArgument("embedding op's outputs size should be 1, but now is %d.", outputs.size()));

    VLOG(6) << "Prepare inputs of embedding_grad";

    Tensor x(std::make_shared<primitive::LazyTensor>(inputs_[0][0]));
    Tensor weight(std::make_shared<primitive::LazyTensor>(inputs_[1][0]));
    Tensor out_grad(std::make_shared<primitive::LazyTensor>(out_grads[0][0]));

    VLOG(6) << "Vjp prepare Prepare attributes of embedding_grad";

    int64_t padding_idx = op->attribute("padding_idx").dyn_cast<pir::Int64Attribute>().data();

    VLOG(6) << "Vjp prepare call embedding's vjp inteface";

    std::vector<std::vector<Tensor>> tensor_res =
        primitive::embedding_vjp(
        x, weight, out_grad, padding_idx, stop_gradients);

    VLOG(6) << "Vjp prepare stop gradient of embedding_grad";

    std::vector<std::vector<pir::OpResult>> res(tensor_res.size());
    for (size_t i = 0; i < tensor_res.size(); ++i) {
        res[i].resize(tensor_res[i].size());
        for (size_t j = 0; j < tensor_res[i].size(); ++j) {
            if(tensor_res[i][j].defined()){
                res[i][j] = std::static_pointer_cast<primitive::LazyTensor>(tensor_res[i][j].impl())->value().dyn_cast<pir::OpResult>();
            }
        }
    }
    return res;
}

std::vector<std::vector<pir::OpResult>> EnableCheckModelNanInfOp::Vjp(pir::Operation* op, const std::vector<std::vector<pir::Value>>& inputs_, const std::vector<std::vector<pir::Value>>& outputs, const std::vector<std::vector<pir::Value>>& out_grads, const std::vector<std::vector<bool>>& stop_gradients){

    PADDLE_ENFORCE_EQ(
      inputs_.size(),
      1,
      platform::errors::InvalidArgument("enable_check_model_nan_inf op's inputs size should be 1, but now is %d.", inputs_.size()));
    PADDLE_ENFORCE_EQ(
      outputs.size(),
      1,
      platform::errors::InvalidArgument("enable_check_model_nan_inf op's outputs size should be 1, but now is %d.", outputs.size()));

    VLOG(6) << "Prepare inputs of enable_check_model_nan_inf_grad";

    Tensor out_grad(std::make_shared<primitive::LazyTensor>(out_grads[0][0]));

    VLOG(6) << "Vjp prepare Prepare attributes of enable_check_model_nan_inf_grad";

    int unsetflag = 0;

    VLOG(6) << "Vjp prepare call enable_check_model_nan_inf's vjp inteface";

    std::vector<std::vector<Tensor>> tensor_res =
        primitive::enable_check_model_nan_inf_vjp(
        out_grad, unsetflag, stop_gradients);

    VLOG(6) << "Vjp prepare stop gradient of enable_check_model_nan_inf_grad";

    std::vector<std::vector<pir::OpResult>> res(tensor_res.size());
    for (size_t i = 0; i < tensor_res.size(); ++i) {
        res[i].resize(tensor_res[i].size());
        for (size_t j = 0; j < tensor_res[i].size(); ++j) {
            if(tensor_res[i][j].defined()){
                res[i][j] = std::static_pointer_cast<primitive::LazyTensor>(tensor_res[i][j].impl())->value().dyn_cast<pir::OpResult>();
            }
        }
    }
    return res;
}

std::vector<std::vector<pir::OpResult>> Exponential_Op::Vjp(pir::Operation* op, const std::vector<std::vector<pir::Value>>& inputs_, const std::vector<std::vector<pir::Value>>& outputs, const std::vector<std::vector<pir::Value>>& out_grads, const std::vector<std::vector<bool>>& stop_gradients){

    PADDLE_ENFORCE_EQ(
      inputs_.size(),
      1,
      platform::errors::InvalidArgument("exponential_ op's inputs size should be 1, but now is %d.", inputs_.size()));
    PADDLE_ENFORCE_EQ(
      outputs.size(),
      1,
      platform::errors::InvalidArgument("exponential_ op's outputs size should be 1, but now is %d.", outputs.size()));

    VLOG(6) << "Prepare inputs of exponential__grad";

    Tensor out_grad(std::make_shared<primitive::LazyTensor>(out_grads[0][0]));

    VLOG(6) << "Vjp prepare Prepare attributes of exponential__grad";


    VLOG(6) << "Vjp prepare call exponential_'s vjp inteface";

    std::vector<std::vector<Tensor>> tensor_res =
        primitive::exponential__vjp(
        out_grad, stop_gradients);

    VLOG(6) << "Vjp prepare stop gradient of exponential__grad";

    std::vector<std::vector<pir::OpResult>> res(tensor_res.size());
    for (size_t i = 0; i < tensor_res.size(); ++i) {
        res[i].resize(tensor_res[i].size());
        for (size_t j = 0; j < tensor_res[i].size(); ++j) {
            if(tensor_res[i][j].defined()){
                res[i][j] = std::static_pointer_cast<primitive::LazyTensor>(tensor_res[i][j].impl())->value().dyn_cast<pir::OpResult>();
            }
        }
    }
    return res;
}

std::vector<std::vector<pir::OpResult>> FrobeniusNormOp::Vjp(pir::Operation* op, const std::vector<std::vector<pir::Value>>& inputs_, const std::vector<std::vector<pir::Value>>& outputs, const std::vector<std::vector<pir::Value>>& out_grads, const std::vector<std::vector<bool>>& stop_gradients){

    PADDLE_ENFORCE_EQ(
      inputs_.size(),
      2,
      platform::errors::InvalidArgument("frobenius_norm op's inputs size should be 2, but now is %d.", inputs_.size()));
    PADDLE_ENFORCE_EQ(
      outputs.size(),
      1,
      platform::errors::InvalidArgument("frobenius_norm op's outputs size should be 1, but now is %d.", outputs.size()));

    VLOG(6) << "Prepare inputs of frobenius_norm_grad";

    Tensor x(std::make_shared<primitive::LazyTensor>(inputs_[0][0]));
    Tensor out(std::make_shared<primitive::LazyTensor>(outputs[0][0]));
    Tensor out_grad(std::make_shared<primitive::LazyTensor>(out_grads[0][0]));

    VLOG(6) << "Vjp prepare Prepare attributes of frobenius_norm_grad";

    Tensor axis(std::make_shared<primitive::LazyTensor>(inputs_[1][0]));
    bool keep_dim = op->attribute("keep_dim").dyn_cast<pir::BoolAttribute>().data();
    bool reduce_all = op->attribute("reduce_all").dyn_cast<pir::BoolAttribute>().data();

    VLOG(6) << "Vjp prepare call frobenius_norm's vjp inteface";

    std::vector<std::vector<Tensor>> tensor_res =
        primitive::frobenius_norm_vjp(
        x, out, out_grad, axis, keep_dim, reduce_all, stop_gradients);

    VLOG(6) << "Vjp prepare stop gradient of frobenius_norm_grad";

    std::vector<std::vector<pir::OpResult>> res(tensor_res.size());
    for (size_t i = 0; i < tensor_res.size(); ++i) {
        res[i].resize(tensor_res[i].size());
        for (size_t j = 0; j < tensor_res[i].size(); ++j) {
            if(tensor_res[i][j].defined()){
                res[i][j] = std::static_pointer_cast<primitive::LazyTensor>(tensor_res[i][j].impl())->value().dyn_cast<pir::OpResult>();
            }
        }
    }
    return res;
}

std::vector<std::vector<pir::OpResult>> FusedBatchNormActOp::Vjp(pir::Operation* op, const std::vector<std::vector<pir::Value>>& inputs_, const std::vector<std::vector<pir::Value>>& outputs, const std::vector<std::vector<pir::Value>>& out_grads, const std::vector<std::vector<bool>>& stop_gradients){

    PADDLE_ENFORCE_EQ(
      inputs_.size(),
      5,
      platform::errors::InvalidArgument("fused_batch_norm_act op's inputs size should be 5, but now is %d.", inputs_.size()));
    PADDLE_ENFORCE_EQ(
      outputs.size(),
      6,
      platform::errors::InvalidArgument("fused_batch_norm_act op's outputs size should be 6, but now is %d.", outputs.size()));

    VLOG(6) << "Prepare inputs of fused_batch_norm_act_grad";

    Tensor x(std::make_shared<primitive::LazyTensor>(inputs_[0][0]));
    Tensor scale(std::make_shared<primitive::LazyTensor>(inputs_[1][0]));
    Tensor bias(std::make_shared<primitive::LazyTensor>(inputs_[2][0]));
    Tensor out(std::make_shared<primitive::LazyTensor>(outputs[0][0]));
    Tensor saved_mean(std::make_shared<primitive::LazyTensor>(outputs[3][0]));
    Tensor saved_variance(std::make_shared<primitive::LazyTensor>(outputs[4][0]));
    paddle::optional<Tensor> reserve_space;
    if (!IsEmptyValue(outputs[5][0])){
        reserve_space = paddle::make_optional<Tensor>(Tensor(std::make_shared<primitive::LazyTensor>(outputs[5][0])));
    }
    Tensor out_grad(std::make_shared<primitive::LazyTensor>(out_grads[0][0]));

    VLOG(6) << "Vjp prepare Prepare attributes of fused_batch_norm_act_grad";

    float momentum = op->attribute("momentum").dyn_cast<pir::FloatAttribute>().data();
    float epsilon = op->attribute("epsilon").dyn_cast<pir::FloatAttribute>().data();
    std::string act_type = op->attribute("act_type").dyn_cast<pir::StrAttribute>().AsString();

    VLOG(6) << "Vjp prepare call fused_batch_norm_act's vjp inteface";

    std::vector<std::vector<Tensor>> tensor_res =
        primitive::fused_batch_norm_act_vjp(
        x, scale, bias, out, saved_mean, saved_variance, reserve_space, out_grad, momentum, epsilon, act_type, stop_gradients);

    VLOG(6) << "Vjp prepare stop gradient of fused_batch_norm_act_grad";

    std::vector<std::vector<pir::OpResult>> res(tensor_res.size());
    for (size_t i = 0; i < tensor_res.size(); ++i) {
        res[i].resize(tensor_res[i].size());
        for (size_t j = 0; j < tensor_res[i].size(); ++j) {
            if(tensor_res[i][j].defined()){
                res[i][j] = std::static_pointer_cast<primitive::LazyTensor>(tensor_res[i][j].impl())->value().dyn_cast<pir::OpResult>();
            }
        }
    }
    return res;
}

std::vector<std::vector<pir::OpResult>> FusedBatchNormAct_Op::Vjp(pir::Operation* op, const std::vector<std::vector<pir::Value>>& inputs_, const std::vector<std::vector<pir::Value>>& outputs, const std::vector<std::vector<pir::Value>>& out_grads, const std::vector<std::vector<bool>>& stop_gradients){

    PADDLE_ENFORCE_EQ(
      inputs_.size(),
      5,
      platform::errors::InvalidArgument("fused_batch_norm_act op's inputs size should be 5, but now is %d.", inputs_.size()));
    PADDLE_ENFORCE_EQ(
      outputs.size(),
      6,
      platform::errors::InvalidArgument("fused_batch_norm_act op's outputs size should be 6, but now is %d.", outputs.size()));

    VLOG(6) << "Prepare inputs of fused_batch_norm_act_grad";

    Tensor x(std::make_shared<primitive::LazyTensor>(inputs_[0][0]));
    Tensor scale(std::make_shared<primitive::LazyTensor>(inputs_[1][0]));
    Tensor bias(std::make_shared<primitive::LazyTensor>(inputs_[2][0]));
    Tensor out(std::make_shared<primitive::LazyTensor>(outputs[0][0]));
    Tensor saved_mean(std::make_shared<primitive::LazyTensor>(outputs[3][0]));
    Tensor saved_variance(std::make_shared<primitive::LazyTensor>(outputs[4][0]));
    paddle::optional<Tensor> reserve_space;
    if (!IsEmptyValue(outputs[5][0])){
        reserve_space = paddle::make_optional<Tensor>(Tensor(std::make_shared<primitive::LazyTensor>(outputs[5][0])));
    }
    Tensor out_grad(std::make_shared<primitive::LazyTensor>(out_grads[0][0]));

    VLOG(6) << "Vjp prepare Prepare attributes of fused_batch_norm_act_grad";

    float momentum = op->attribute("momentum").dyn_cast<pir::FloatAttribute>().data();
    float epsilon = op->attribute("epsilon").dyn_cast<pir::FloatAttribute>().data();
    std::string act_type = op->attribute("act_type").dyn_cast<pir::StrAttribute>().AsString();

    VLOG(6) << "Vjp prepare call fused_batch_norm_act_'s vjp inteface";

    std::vector<std::vector<Tensor>> tensor_res =
        primitive::fused_batch_norm_act_vjp(
        x, scale, bias, out, saved_mean, saved_variance, reserve_space, out_grad, momentum, epsilon, act_type, stop_gradients);

    VLOG(6) << "Vjp prepare stop gradient of fused_batch_norm_act_grad";

    std::vector<std::vector<pir::OpResult>> res(tensor_res.size());
    for (size_t i = 0; i < tensor_res.size(); ++i) {
        res[i].resize(tensor_res[i].size());
        for (size_t j = 0; j < tensor_res[i].size(); ++j) {
            if(tensor_res[i][j].defined()){
                res[i][j] = std::static_pointer_cast<primitive::LazyTensor>(tensor_res[i][j].impl())->value().dyn_cast<pir::OpResult>();
            }
        }
    }
    return res;
}

std::vector<std::vector<pir::OpResult>> FusedBnAddActivationOp::Vjp(pir::Operation* op, const std::vector<std::vector<pir::Value>>& inputs_, const std::vector<std::vector<pir::Value>>& outputs, const std::vector<std::vector<pir::Value>>& out_grads, const std::vector<std::vector<bool>>& stop_gradients){

    PADDLE_ENFORCE_EQ(
      inputs_.size(),
      6,
      platform::errors::InvalidArgument("fused_bn_add_activation op's inputs size should be 6, but now is %d.", inputs_.size()));
    PADDLE_ENFORCE_EQ(
      outputs.size(),
      6,
      platform::errors::InvalidArgument("fused_bn_add_activation op's outputs size should be 6, but now is %d.", outputs.size()));

    VLOG(6) << "Prepare inputs of fused_bn_add_activation_grad";

    Tensor x(std::make_shared<primitive::LazyTensor>(inputs_[0][0]));
    Tensor scale(std::make_shared<primitive::LazyTensor>(inputs_[2][0]));
    Tensor bias(std::make_shared<primitive::LazyTensor>(inputs_[3][0]));
    Tensor out(std::make_shared<primitive::LazyTensor>(outputs[0][0]));
    Tensor saved_mean(std::make_shared<primitive::LazyTensor>(outputs[3][0]));
    Tensor saved_variance(std::make_shared<primitive::LazyTensor>(outputs[4][0]));
    paddle::optional<Tensor> reserve_space;
    if (!IsEmptyValue(outputs[5][0])){
        reserve_space = paddle::make_optional<Tensor>(Tensor(std::make_shared<primitive::LazyTensor>(outputs[5][0])));
    }
    Tensor out_grad(std::make_shared<primitive::LazyTensor>(out_grads[0][0]));

    VLOG(6) << "Vjp prepare Prepare attributes of fused_bn_add_activation_grad";

    float momentum = op->attribute("momentum").dyn_cast<pir::FloatAttribute>().data();
    float epsilon = op->attribute("epsilon").dyn_cast<pir::FloatAttribute>().data();
    std::string act_type = op->attribute("act_type").dyn_cast<pir::StrAttribute>().AsString();

    VLOG(6) << "Vjp prepare call fused_bn_add_activation's vjp inteface";

    std::vector<std::vector<Tensor>> tensor_res =
        primitive::fused_bn_add_activation_vjp(
        x, scale, bias, out, saved_mean, saved_variance, reserve_space, out_grad, momentum, epsilon, act_type, stop_gradients);

    VLOG(6) << "Vjp prepare stop gradient of fused_bn_add_activation_grad";

    std::vector<std::vector<pir::OpResult>> res(tensor_res.size());
    for (size_t i = 0; i < tensor_res.size(); ++i) {
        res[i].resize(tensor_res[i].size());
        for (size_t j = 0; j < tensor_res[i].size(); ++j) {
            if(tensor_res[i][j].defined()){
                res[i][j] = std::static_pointer_cast<primitive::LazyTensor>(tensor_res[i][j].impl())->value().dyn_cast<pir::OpResult>();
            }
        }
    }
    return res;
}

std::vector<std::vector<pir::OpResult>> FusedBnAddActivation_Op::Vjp(pir::Operation* op, const std::vector<std::vector<pir::Value>>& inputs_, const std::vector<std::vector<pir::Value>>& outputs, const std::vector<std::vector<pir::Value>>& out_grads, const std::vector<std::vector<bool>>& stop_gradients){

    PADDLE_ENFORCE_EQ(
      inputs_.size(),
      6,
      platform::errors::InvalidArgument("fused_bn_add_activation op's inputs size should be 6, but now is %d.", inputs_.size()));
    PADDLE_ENFORCE_EQ(
      outputs.size(),
      6,
      platform::errors::InvalidArgument("fused_bn_add_activation op's outputs size should be 6, but now is %d.", outputs.size()));

    VLOG(6) << "Prepare inputs of fused_bn_add_activation_grad";

    Tensor x(std::make_shared<primitive::LazyTensor>(inputs_[0][0]));
    Tensor scale(std::make_shared<primitive::LazyTensor>(inputs_[2][0]));
    Tensor bias(std::make_shared<primitive::LazyTensor>(inputs_[3][0]));
    Tensor out(std::make_shared<primitive::LazyTensor>(outputs[0][0]));
    Tensor saved_mean(std::make_shared<primitive::LazyTensor>(outputs[3][0]));
    Tensor saved_variance(std::make_shared<primitive::LazyTensor>(outputs[4][0]));
    paddle::optional<Tensor> reserve_space;
    if (!IsEmptyValue(outputs[5][0])){
        reserve_space = paddle::make_optional<Tensor>(Tensor(std::make_shared<primitive::LazyTensor>(outputs[5][0])));
    }
    Tensor out_grad(std::make_shared<primitive::LazyTensor>(out_grads[0][0]));

    VLOG(6) << "Vjp prepare Prepare attributes of fused_bn_add_activation_grad";

    float momentum = op->attribute("momentum").dyn_cast<pir::FloatAttribute>().data();
    float epsilon = op->attribute("epsilon").dyn_cast<pir::FloatAttribute>().data();
    std::string act_type = op->attribute("act_type").dyn_cast<pir::StrAttribute>().AsString();

    VLOG(6) << "Vjp prepare call fused_bn_add_activation_'s vjp inteface";

    std::vector<std::vector<Tensor>> tensor_res =
        primitive::fused_bn_add_activation_vjp(
        x, scale, bias, out, saved_mean, saved_variance, reserve_space, out_grad, momentum, epsilon, act_type, stop_gradients);

    VLOG(6) << "Vjp prepare stop gradient of fused_bn_add_activation_grad";

    std::vector<std::vector<pir::OpResult>> res(tensor_res.size());
    for (size_t i = 0; i < tensor_res.size(); ++i) {
        res[i].resize(tensor_res[i].size());
        for (size_t j = 0; j < tensor_res[i].size(); ++j) {
            if(tensor_res[i][j].defined()){
                res[i][j] = std::static_pointer_cast<primitive::LazyTensor>(tensor_res[i][j].impl())->value().dyn_cast<pir::OpResult>();
            }
        }
    }
    return res;
}

std::vector<std::vector<pir::OpResult>> FusedSoftmaxMaskUpperTriangleOp::Vjp(pir::Operation* op, const std::vector<std::vector<pir::Value>>& inputs_, const std::vector<std::vector<pir::Value>>& outputs, const std::vector<std::vector<pir::Value>>& out_grads, const std::vector<std::vector<bool>>& stop_gradients){

    PADDLE_ENFORCE_EQ(
      inputs_.size(),
      1,
      platform::errors::InvalidArgument("fused_softmax_mask_upper_triangle op's inputs size should be 1, but now is %d.", inputs_.size()));
    PADDLE_ENFORCE_EQ(
      outputs.size(),
      1,
      platform::errors::InvalidArgument("fused_softmax_mask_upper_triangle op's outputs size should be 1, but now is %d.", outputs.size()));

    VLOG(6) << "Prepare inputs of fused_softmax_mask_upper_triangle_grad";

    Tensor Out(std::make_shared<primitive::LazyTensor>(outputs[0][0]));
    Tensor Out_grad(std::make_shared<primitive::LazyTensor>(out_grads[0][0]));

    VLOG(6) << "Vjp prepare Prepare attributes of fused_softmax_mask_upper_triangle_grad";


    VLOG(6) << "Vjp prepare call fused_softmax_mask_upper_triangle's vjp inteface";

    std::vector<std::vector<Tensor>> tensor_res =
        primitive::fused_softmax_mask_upper_triangle_vjp(
        Out, Out_grad, stop_gradients);

    VLOG(6) << "Vjp prepare stop gradient of fused_softmax_mask_upper_triangle_grad";

    std::vector<std::vector<pir::OpResult>> res(tensor_res.size());
    for (size_t i = 0; i < tensor_res.size(); ++i) {
        res[i].resize(tensor_res[i].size());
        for (size_t j = 0; j < tensor_res[i].size(); ++j) {
            if(tensor_res[i][j].defined()){
                res[i][j] = std::static_pointer_cast<primitive::LazyTensor>(tensor_res[i][j].impl())->value().dyn_cast<pir::OpResult>();
            }
        }
    }
    return res;
}

std::vector<std::vector<pir::OpResult>> HardswishOp::Vjp(pir::Operation* op, const std::vector<std::vector<pir::Value>>& inputs_, const std::vector<std::vector<pir::Value>>& outputs, const std::vector<std::vector<pir::Value>>& out_grads, const std::vector<std::vector<bool>>& stop_gradients){

    PADDLE_ENFORCE_EQ(
      inputs_.size(),
      1,
      platform::errors::InvalidArgument("hardswish op's inputs size should be 1, but now is %d.", inputs_.size()));
    PADDLE_ENFORCE_EQ(
      outputs.size(),
      1,
      platform::errors::InvalidArgument("hardswish op's outputs size should be 1, but now is %d.", outputs.size()));

    VLOG(6) << "Prepare inputs of hardswish_grad";

    Tensor x(std::make_shared<primitive::LazyTensor>(inputs_[0][0]));
    Tensor out_grad(std::make_shared<primitive::LazyTensor>(out_grads[0][0]));

    VLOG(6) << "Vjp prepare Prepare attributes of hardswish_grad";


    VLOG(6) << "Vjp prepare call hardswish's vjp inteface";

    std::vector<std::vector<Tensor>> tensor_res =
        primitive::hardswish_vjp(
        x, out_grad, stop_gradients);

    VLOG(6) << "Vjp prepare stop gradient of hardswish_grad";

    std::vector<std::vector<pir::OpResult>> res(tensor_res.size());
    for (size_t i = 0; i < tensor_res.size(); ++i) {
        res[i].resize(tensor_res[i].size());
        for (size_t j = 0; j < tensor_res[i].size(); ++j) {
            if(tensor_res[i][j].defined()){
                res[i][j] = std::static_pointer_cast<primitive::LazyTensor>(tensor_res[i][j].impl())->value().dyn_cast<pir::OpResult>();
            }
        }
    }
    return res;
}

std::vector<std::vector<pir::OpResult>> HsigmoidLossOp::Vjp(pir::Operation* op, const std::vector<std::vector<pir::Value>>& inputs_, const std::vector<std::vector<pir::Value>>& outputs, const std::vector<std::vector<pir::Value>>& out_grads, const std::vector<std::vector<bool>>& stop_gradients){

    PADDLE_ENFORCE_EQ(
      inputs_.size(),
      6,
      platform::errors::InvalidArgument("hsigmoid_loss op's inputs size should be 6, but now is %d.", inputs_.size()));
    PADDLE_ENFORCE_EQ(
      outputs.size(),
      3,
      platform::errors::InvalidArgument("hsigmoid_loss op's outputs size should be 3, but now is %d.", outputs.size()));

    VLOG(6) << "Prepare inputs of hsigmoid_loss_grad";

    Tensor x(std::make_shared<primitive::LazyTensor>(inputs_[0][0]));
    Tensor w(std::make_shared<primitive::LazyTensor>(inputs_[2][0]));
    Tensor label(std::make_shared<primitive::LazyTensor>(inputs_[1][0]));
    paddle::optional<Tensor> path;
    if (!IsEmptyValue(inputs_[4][0])){
        path = paddle::make_optional<Tensor>(Tensor(std::make_shared<primitive::LazyTensor>(inputs_[4][0])));
    }
    paddle::optional<Tensor> code;
    if (!IsEmptyValue(inputs_[5][0])){
        code = paddle::make_optional<Tensor>(Tensor(std::make_shared<primitive::LazyTensor>(inputs_[5][0])));
    }
    paddle::optional<Tensor> bias;
    if (!IsEmptyValue(inputs_[3][0])){
        bias = paddle::make_optional<Tensor>(Tensor(std::make_shared<primitive::LazyTensor>(inputs_[3][0])));
    }
    Tensor pre_out(std::make_shared<primitive::LazyTensor>(outputs[1][0]));
    Tensor out_grad(std::make_shared<primitive::LazyTensor>(out_grads[0][0]));

    VLOG(6) << "Vjp prepare Prepare attributes of hsigmoid_loss_grad";

    int num_classes = op->attribute("num_classes").dyn_cast<pir::Int32Attribute>().data();
    bool is_sparse = op->attribute("is_sparse").dyn_cast<pir::BoolAttribute>().data();

    VLOG(6) << "Vjp prepare call hsigmoid_loss's vjp inteface";

    std::vector<std::vector<Tensor>> tensor_res =
        primitive::hsigmoid_loss_vjp(
        x, w, label, path, code, bias, pre_out, out_grad, num_classes, is_sparse, stop_gradients);

    VLOG(6) << "Vjp prepare stop gradient of hsigmoid_loss_grad";

    std::vector<std::vector<pir::OpResult>> res(tensor_res.size());
    for (size_t i = 0; i < tensor_res.size(); ++i) {
        res[i].resize(tensor_res[i].size());
        for (size_t j = 0; j < tensor_res[i].size(); ++j) {
            if(tensor_res[i][j].defined()){
                res[i][j] = std::static_pointer_cast<primitive::LazyTensor>(tensor_res[i][j].impl())->value().dyn_cast<pir::OpResult>();
            }
        }
    }
    return res;
}

std::vector<std::vector<pir::OpResult>> LogsumexpOp::Vjp(pir::Operation* op, const std::vector<std::vector<pir::Value>>& inputs_, const std::vector<std::vector<pir::Value>>& outputs, const std::vector<std::vector<pir::Value>>& out_grads, const std::vector<std::vector<bool>>& stop_gradients){

    PADDLE_ENFORCE_EQ(
      inputs_.size(),
      1,
      platform::errors::InvalidArgument("logsumexp op's inputs size should be 1, but now is %d.", inputs_.size()));
    PADDLE_ENFORCE_EQ(
      outputs.size(),
      1,
      platform::errors::InvalidArgument("logsumexp op's outputs size should be 1, but now is %d.", outputs.size()));

    VLOG(6) << "Prepare inputs of logsumexp_grad";

    Tensor x(std::make_shared<primitive::LazyTensor>(inputs_[0][0]));
    Tensor out(std::make_shared<primitive::LazyTensor>(outputs[0][0]));
    Tensor out_grad(std::make_shared<primitive::LazyTensor>(out_grads[0][0]));

    VLOG(6) << "Vjp prepare Prepare attributes of logsumexp_grad";

    std::vector<int64_t> axis;
    for (size_t i = 0; i < op->attribute("axis").dyn_cast<pir::ArrayAttribute>().size(); i++) {
        axis.push_back(op->attribute("axis").dyn_cast<pir::ArrayAttribute>().at(i).dyn_cast<pir::Int64Attribute>().data());
    }
    bool keepdim = op->attribute("keepdim").dyn_cast<pir::BoolAttribute>().data();
    bool reduce_all = op->attribute("reduce_all").dyn_cast<pir::BoolAttribute>().data();

    VLOG(6) << "Vjp prepare call logsumexp's vjp inteface";

    std::vector<std::vector<Tensor>> tensor_res =
        primitive::logsumexp_vjp(
        x, out, out_grad, axis, keepdim, reduce_all, stop_gradients);

    VLOG(6) << "Vjp prepare stop gradient of logsumexp_grad";

    std::vector<std::vector<pir::OpResult>> res(tensor_res.size());
    for (size_t i = 0; i < tensor_res.size(); ++i) {
        res[i].resize(tensor_res[i].size());
        for (size_t j = 0; j < tensor_res[i].size(); ++j) {
            if(tensor_res[i][j].defined()){
                res[i][j] = std::static_pointer_cast<primitive::LazyTensor>(tensor_res[i][j].impl())->value().dyn_cast<pir::OpResult>();
            }
        }
    }
    return res;
}

std::vector<std::vector<pir::OpResult>> MatmulOp::Vjp(pir::Operation* op, const std::vector<std::vector<pir::Value>>& inputs_, const std::vector<std::vector<pir::Value>>& outputs, const std::vector<std::vector<pir::Value>>& out_grads, const std::vector<std::vector<bool>>& stop_gradients){

    PADDLE_ENFORCE_EQ(
      inputs_.size(),
      2,
      platform::errors::InvalidArgument("matmul op's inputs size should be 2, but now is %d.", inputs_.size()));
    PADDLE_ENFORCE_EQ(
      outputs.size(),
      1,
      platform::errors::InvalidArgument("matmul op's outputs size should be 1, but now is %d.", outputs.size()));

    VLOG(6) << "Prepare inputs of matmul_grad";

    Tensor x(std::make_shared<primitive::LazyTensor>(inputs_[0][0]));
    Tensor y(std::make_shared<primitive::LazyTensor>(inputs_[1][0]));
    Tensor out_grad(std::make_shared<primitive::LazyTensor>(out_grads[0][0]));

    VLOG(6) << "Vjp prepare Prepare attributes of matmul_grad";

    bool transpose_x = op->attribute("transpose_x").dyn_cast<pir::BoolAttribute>().data();
    bool transpose_y = op->attribute("transpose_y").dyn_cast<pir::BoolAttribute>().data();

    VLOG(6) << "Vjp prepare call matmul's vjp inteface";

    std::vector<std::vector<Tensor>> tensor_res =
        primitive::matmul_vjp(
        x, y, out_grad, transpose_x, transpose_y, stop_gradients);

    VLOG(6) << "Vjp prepare stop gradient of matmul_grad";

    std::vector<std::vector<pir::OpResult>> res(tensor_res.size());
    for (size_t i = 0; i < tensor_res.size(); ++i) {
        res[i].resize(tensor_res[i].size());
        for (size_t j = 0; j < tensor_res[i].size(); ++j) {
            if(tensor_res[i][j].defined()){
                res[i][j] = std::static_pointer_cast<primitive::LazyTensor>(tensor_res[i][j].impl())->value().dyn_cast<pir::OpResult>();
            }
        }
    }
    return res;
}

std::vector<std::vector<pir::OpResult>> MaxOp::Vjp(pir::Operation* op, const std::vector<std::vector<pir::Value>>& inputs_, const std::vector<std::vector<pir::Value>>& outputs, const std::vector<std::vector<pir::Value>>& out_grads, const std::vector<std::vector<bool>>& stop_gradients){

    PADDLE_ENFORCE_EQ(
      inputs_.size(),
      2,
      platform::errors::InvalidArgument("max op's inputs size should be 2, but now is %d.", inputs_.size()));
    PADDLE_ENFORCE_EQ(
      outputs.size(),
      1,
      platform::errors::InvalidArgument("max op's outputs size should be 1, but now is %d.", outputs.size()));

    VLOG(6) << "Prepare inputs of max_grad";

    Tensor x(std::make_shared<primitive::LazyTensor>(inputs_[0][0]));
    Tensor out(std::make_shared<primitive::LazyTensor>(outputs[0][0]));
    Tensor out_grad(std::make_shared<primitive::LazyTensor>(out_grads[0][0]));

    VLOG(6) << "Vjp prepare Prepare attributes of max_grad";

    Tensor axis(std::make_shared<primitive::LazyTensor>(inputs_[1][0]));
    bool keepdim = op->attribute("keepdim").dyn_cast<pir::BoolAttribute>().data();
    bool reduce_all = false;

    VLOG(6) << "Vjp prepare call max's vjp inteface";

    std::vector<std::vector<Tensor>> tensor_res =
        primitive::max_vjp(
        x, out, out_grad, axis, keepdim, reduce_all, stop_gradients);

    VLOG(6) << "Vjp prepare stop gradient of max_grad";

    std::vector<std::vector<pir::OpResult>> res(tensor_res.size());
    for (size_t i = 0; i < tensor_res.size(); ++i) {
        res[i].resize(tensor_res[i].size());
        for (size_t j = 0; j < tensor_res[i].size(); ++j) {
            if(tensor_res[i][j].defined()){
                res[i][j] = std::static_pointer_cast<primitive::LazyTensor>(tensor_res[i][j].impl())->value().dyn_cast<pir::OpResult>();
            }
        }
    }
    return res;
}

std::vector<std::vector<pir::OpResult>> MaximumOp::Vjp(pir::Operation* op, const std::vector<std::vector<pir::Value>>& inputs_, const std::vector<std::vector<pir::Value>>& outputs, const std::vector<std::vector<pir::Value>>& out_grads, const std::vector<std::vector<bool>>& stop_gradients){

    PADDLE_ENFORCE_EQ(
      inputs_.size(),
      2,
      platform::errors::InvalidArgument("maximum op's inputs size should be 2, but now is %d.", inputs_.size()));
    PADDLE_ENFORCE_EQ(
      outputs.size(),
      1,
      platform::errors::InvalidArgument("maximum op's outputs size should be 1, but now is %d.", outputs.size()));

    VLOG(6) << "Prepare inputs of maximum_grad";

    Tensor x(std::make_shared<primitive::LazyTensor>(inputs_[0][0]));
    Tensor y(std::make_shared<primitive::LazyTensor>(inputs_[1][0]));
    Tensor out_grad(std::make_shared<primitive::LazyTensor>(out_grads[0][0]));

    VLOG(6) << "Vjp prepare Prepare attributes of maximum_grad";


    VLOG(6) << "Vjp prepare call maximum's vjp inteface";

    std::vector<std::vector<Tensor>> tensor_res =
        primitive::maximum_vjp(
        x, y, out_grad, stop_gradients);

    VLOG(6) << "Vjp prepare stop gradient of maximum_grad";

    std::vector<std::vector<pir::OpResult>> res(tensor_res.size());
    for (size_t i = 0; i < tensor_res.size(); ++i) {
        res[i].resize(tensor_res[i].size());
        for (size_t j = 0; j < tensor_res[i].size(); ++j) {
            if(tensor_res[i][j].defined()){
                res[i][j] = std::static_pointer_cast<primitive::LazyTensor>(tensor_res[i][j].impl())->value().dyn_cast<pir::OpResult>();
            }
        }
    }
    return res;
}

std::vector<std::vector<pir::OpResult>> MeanOp::Vjp(pir::Operation* op, const std::vector<std::vector<pir::Value>>& inputs_, const std::vector<std::vector<pir::Value>>& outputs, const std::vector<std::vector<pir::Value>>& out_grads, const std::vector<std::vector<bool>>& stop_gradients){

    PADDLE_ENFORCE_EQ(
      inputs_.size(),
      1,
      platform::errors::InvalidArgument("mean op's inputs size should be 1, but now is %d.", inputs_.size()));
    PADDLE_ENFORCE_EQ(
      outputs.size(),
      1,
      platform::errors::InvalidArgument("mean op's outputs size should be 1, but now is %d.", outputs.size()));

    VLOG(6) << "Prepare inputs of mean_grad";

    Tensor x(std::make_shared<primitive::LazyTensor>(inputs_[0][0]));
    Tensor out_grad(std::make_shared<primitive::LazyTensor>(out_grads[0][0]));

    VLOG(6) << "Vjp prepare Prepare attributes of mean_grad";

    phi::IntArray axis = op->attribute("axis").dyn_cast<paddle::dialect::IntArrayAttribute>().data();
    bool keepdim = op->attribute("keepdim").dyn_cast<pir::BoolAttribute>().data();
    bool reduce_all = false;

    VLOG(6) << "Vjp prepare call mean's vjp inteface";

    std::vector<std::vector<Tensor>> tensor_res =
        primitive::mean_vjp(
        x, out_grad, axis, keepdim, reduce_all, stop_gradients);

    VLOG(6) << "Vjp prepare stop gradient of mean_grad";

    std::vector<std::vector<pir::OpResult>> res(tensor_res.size());
    for (size_t i = 0; i < tensor_res.size(); ++i) {
        res[i].resize(tensor_res[i].size());
        for (size_t j = 0; j < tensor_res[i].size(); ++j) {
            if(tensor_res[i][j].defined()){
                res[i][j] = std::static_pointer_cast<primitive::LazyTensor>(tensor_res[i][j].impl())->value().dyn_cast<pir::OpResult>();
            }
        }
    }
    return res;
}

std::vector<std::vector<pir::OpResult>> MinOp::Vjp(pir::Operation* op, const std::vector<std::vector<pir::Value>>& inputs_, const std::vector<std::vector<pir::Value>>& outputs, const std::vector<std::vector<pir::Value>>& out_grads, const std::vector<std::vector<bool>>& stop_gradients){

    PADDLE_ENFORCE_EQ(
      inputs_.size(),
      2,
      platform::errors::InvalidArgument("min op's inputs size should be 2, but now is %d.", inputs_.size()));
    PADDLE_ENFORCE_EQ(
      outputs.size(),
      1,
      platform::errors::InvalidArgument("min op's outputs size should be 1, but now is %d.", outputs.size()));

    VLOG(6) << "Prepare inputs of min_grad";

    Tensor x(std::make_shared<primitive::LazyTensor>(inputs_[0][0]));
    Tensor out(std::make_shared<primitive::LazyTensor>(outputs[0][0]));
    Tensor out_grad(std::make_shared<primitive::LazyTensor>(out_grads[0][0]));

    VLOG(6) << "Vjp prepare Prepare attributes of min_grad";

    Tensor axis(std::make_shared<primitive::LazyTensor>(inputs_[1][0]));
    bool keepdim = op->attribute("keepdim").dyn_cast<pir::BoolAttribute>().data();
    bool reduce_all = false;

    VLOG(6) << "Vjp prepare call min's vjp inteface";

    std::vector<std::vector<Tensor>> tensor_res =
        primitive::min_vjp(
        x, out, out_grad, axis, keepdim, reduce_all, stop_gradients);

    VLOG(6) << "Vjp prepare stop gradient of min_grad";

    std::vector<std::vector<pir::OpResult>> res(tensor_res.size());
    for (size_t i = 0; i < tensor_res.size(); ++i) {
        res[i].resize(tensor_res[i].size());
        for (size_t j = 0; j < tensor_res[i].size(); ++j) {
            if(tensor_res[i][j].defined()){
                res[i][j] = std::static_pointer_cast<primitive::LazyTensor>(tensor_res[i][j].impl())->value().dyn_cast<pir::OpResult>();
            }
        }
    }
    return res;
}

std::vector<std::vector<pir::OpResult>> MinimumOp::Vjp(pir::Operation* op, const std::vector<std::vector<pir::Value>>& inputs_, const std::vector<std::vector<pir::Value>>& outputs, const std::vector<std::vector<pir::Value>>& out_grads, const std::vector<std::vector<bool>>& stop_gradients){

    PADDLE_ENFORCE_EQ(
      inputs_.size(),
      2,
      platform::errors::InvalidArgument("minimum op's inputs size should be 2, but now is %d.", inputs_.size()));
    PADDLE_ENFORCE_EQ(
      outputs.size(),
      1,
      platform::errors::InvalidArgument("minimum op's outputs size should be 1, but now is %d.", outputs.size()));

    VLOG(6) << "Prepare inputs of minimum_grad";

    Tensor x(std::make_shared<primitive::LazyTensor>(inputs_[0][0]));
    Tensor y(std::make_shared<primitive::LazyTensor>(inputs_[1][0]));
    Tensor out_grad(std::make_shared<primitive::LazyTensor>(out_grads[0][0]));

    VLOG(6) << "Vjp prepare Prepare attributes of minimum_grad";


    VLOG(6) << "Vjp prepare call minimum's vjp inteface";

    std::vector<std::vector<Tensor>> tensor_res =
        primitive::minimum_vjp(
        x, y, out_grad, stop_gradients);

    VLOG(6) << "Vjp prepare stop gradient of minimum_grad";

    std::vector<std::vector<pir::OpResult>> res(tensor_res.size());
    for (size_t i = 0; i < tensor_res.size(); ++i) {
        res[i].resize(tensor_res[i].size());
        for (size_t j = 0; j < tensor_res[i].size(); ++j) {
            if(tensor_res[i][j].defined()){
                res[i][j] = std::static_pointer_cast<primitive::LazyTensor>(tensor_res[i][j].impl())->value().dyn_cast<pir::OpResult>();
            }
        }
    }
    return res;
}

std::vector<std::vector<pir::OpResult>> MishOp::Vjp(pir::Operation* op, const std::vector<std::vector<pir::Value>>& inputs_, const std::vector<std::vector<pir::Value>>& outputs, const std::vector<std::vector<pir::Value>>& out_grads, const std::vector<std::vector<bool>>& stop_gradients){

    PADDLE_ENFORCE_EQ(
      inputs_.size(),
      1,
      platform::errors::InvalidArgument("mish op's inputs size should be 1, but now is %d.", inputs_.size()));
    PADDLE_ENFORCE_EQ(
      outputs.size(),
      1,
      platform::errors::InvalidArgument("mish op's outputs size should be 1, but now is %d.", outputs.size()));

    VLOG(6) << "Prepare inputs of mish_grad";

    Tensor x(std::make_shared<primitive::LazyTensor>(inputs_[0][0]));
    Tensor out_grad(std::make_shared<primitive::LazyTensor>(out_grads[0][0]));

    VLOG(6) << "Vjp prepare Prepare attributes of mish_grad";

    float lambda = op->attribute("lambda").dyn_cast<pir::FloatAttribute>().data();

    VLOG(6) << "Vjp prepare call mish's vjp inteface";

    std::vector<std::vector<Tensor>> tensor_res =
        primitive::mish_vjp(
        x, out_grad, lambda, stop_gradients);

    VLOG(6) << "Vjp prepare stop gradient of mish_grad";

    std::vector<std::vector<pir::OpResult>> res(tensor_res.size());
    for (size_t i = 0; i < tensor_res.size(); ++i) {
        res[i].resize(tensor_res[i].size());
        for (size_t j = 0; j < tensor_res[i].size(); ++j) {
            if(tensor_res[i][j].defined()){
                res[i][j] = std::static_pointer_cast<primitive::LazyTensor>(tensor_res[i][j].impl())->value().dyn_cast<pir::OpResult>();
            }
        }
    }
    return res;
}

std::vector<std::vector<pir::OpResult>> MultiplyOp::Vjp(pir::Operation* op, const std::vector<std::vector<pir::Value>>& inputs_, const std::vector<std::vector<pir::Value>>& outputs, const std::vector<std::vector<pir::Value>>& out_grads, const std::vector<std::vector<bool>>& stop_gradients){

    PADDLE_ENFORCE_EQ(
      inputs_.size(),
      2,
      platform::errors::InvalidArgument("multiply op's inputs size should be 2, but now is %d.", inputs_.size()));
    PADDLE_ENFORCE_EQ(
      outputs.size(),
      1,
      platform::errors::InvalidArgument("multiply op's outputs size should be 1, but now is %d.", outputs.size()));

    VLOG(6) << "Prepare inputs of multiply_grad";

    Tensor x(std::make_shared<primitive::LazyTensor>(inputs_[0][0]));
    Tensor y(std::make_shared<primitive::LazyTensor>(inputs_[1][0]));
    Tensor out_grad(std::make_shared<primitive::LazyTensor>(out_grads[0][0]));

    VLOG(6) << "Vjp prepare Prepare attributes of multiply_grad";

    int axis = -1;

    VLOG(6) << "Vjp prepare call multiply's vjp inteface";

    std::vector<std::vector<Tensor>> tensor_res =
        primitive::multiply_vjp(
        x, y, out_grad, axis, stop_gradients);

    VLOG(6) << "Vjp prepare stop gradient of multiply_grad";

    std::vector<std::vector<pir::OpResult>> res(tensor_res.size());
    for (size_t i = 0; i < tensor_res.size(); ++i) {
        res[i].resize(tensor_res[i].size());
        for (size_t j = 0; j < tensor_res[i].size(); ++j) {
            if(tensor_res[i][j].defined()){
                res[i][j] = std::static_pointer_cast<primitive::LazyTensor>(tensor_res[i][j].impl())->value().dyn_cast<pir::OpResult>();
            }
        }
    }
    return res;
}

std::vector<std::vector<pir::OpResult>> MultiplySrOp::Vjp(pir::Operation* op, const std::vector<std::vector<pir::Value>>& inputs_, const std::vector<std::vector<pir::Value>>& outputs, const std::vector<std::vector<pir::Value>>& out_grads, const std::vector<std::vector<bool>>& stop_gradients){

    PADDLE_ENFORCE_EQ(
      inputs_.size(),
      2,
      platform::errors::InvalidArgument("multiply op's inputs size should be 2, but now is %d.", inputs_.size()));
    PADDLE_ENFORCE_EQ(
      outputs.size(),
      1,
      platform::errors::InvalidArgument("multiply op's outputs size should be 1, but now is %d.", outputs.size()));

    VLOG(6) << "Prepare inputs of multiply_grad";

    Tensor x(std::make_shared<primitive::LazyTensor>(inputs_[0][0]));
    Tensor y(std::make_shared<primitive::LazyTensor>(inputs_[1][0]));
    Tensor out_grad(std::make_shared<primitive::LazyTensor>(out_grads[0][0]));

    VLOG(6) << "Vjp prepare Prepare attributes of multiply_grad";

    int axis = -1;

    VLOG(6) << "Vjp prepare call multiply's vjp inteface";

    std::vector<std::vector<Tensor>> tensor_res =
        primitive::multiply_vjp(
        x, y, out_grad, axis, stop_gradients);

    VLOG(6) << "Vjp prepare stop gradient of multiply_grad";

    std::vector<std::vector<pir::OpResult>> res(tensor_res.size());
    for (size_t i = 0; i < tensor_res.size(); ++i) {
        res[i].resize(tensor_res[i].size());
        for (size_t j = 0; j < tensor_res[i].size(); ++j) {
            if(tensor_res[i][j].defined()){
                res[i][j] = std::static_pointer_cast<primitive::LazyTensor>(tensor_res[i][j].impl())->value().dyn_cast<pir::OpResult>();
            }
        }
    }
    return res;
}

std::vector<std::vector<pir::OpResult>> Multiply_Op::Vjp(pir::Operation* op, const std::vector<std::vector<pir::Value>>& inputs_, const std::vector<std::vector<pir::Value>>& outputs, const std::vector<std::vector<pir::Value>>& out_grads, const std::vector<std::vector<bool>>& stop_gradients){

    PADDLE_ENFORCE_EQ(
      inputs_.size(),
      2,
      platform::errors::InvalidArgument("multiply op's inputs size should be 2, but now is %d.", inputs_.size()));
    PADDLE_ENFORCE_EQ(
      outputs.size(),
      1,
      platform::errors::InvalidArgument("multiply op's outputs size should be 1, but now is %d.", outputs.size()));

    VLOG(6) << "Prepare inputs of multiply_grad";

    Tensor x(std::make_shared<primitive::LazyTensor>(inputs_[0][0]));
    Tensor y(std::make_shared<primitive::LazyTensor>(inputs_[1][0]));
    Tensor out_grad(std::make_shared<primitive::LazyTensor>(out_grads[0][0]));

    VLOG(6) << "Vjp prepare Prepare attributes of multiply_grad";

    int axis = -1;

    VLOG(6) << "Vjp prepare call multiply_'s vjp inteface";

    std::vector<std::vector<Tensor>> tensor_res =
        primitive::multiply_vjp(
        x, y, out_grad, axis, stop_gradients);

    VLOG(6) << "Vjp prepare stop gradient of multiply_grad";

    std::vector<std::vector<pir::OpResult>> res(tensor_res.size());
    for (size_t i = 0; i < tensor_res.size(); ++i) {
        res[i].resize(tensor_res[i].size());
        for (size_t j = 0; j < tensor_res[i].size(); ++j) {
            if(tensor_res[i][j].defined()){
                res[i][j] = std::static_pointer_cast<primitive::LazyTensor>(tensor_res[i][j].impl())->value().dyn_cast<pir::OpResult>();
            }
        }
    }
    return res;
}

std::vector<std::vector<pir::OpResult>> MultiplySr_Op::Vjp(pir::Operation* op, const std::vector<std::vector<pir::Value>>& inputs_, const std::vector<std::vector<pir::Value>>& outputs, const std::vector<std::vector<pir::Value>>& out_grads, const std::vector<std::vector<bool>>& stop_gradients){

    PADDLE_ENFORCE_EQ(
      inputs_.size(),
      2,
      platform::errors::InvalidArgument("multiply op's inputs size should be 2, but now is %d.", inputs_.size()));
    PADDLE_ENFORCE_EQ(
      outputs.size(),
      1,
      platform::errors::InvalidArgument("multiply op's outputs size should be 1, but now is %d.", outputs.size()));

    VLOG(6) << "Prepare inputs of multiply_grad";

    Tensor x(std::make_shared<primitive::LazyTensor>(inputs_[0][0]));
    Tensor y(std::make_shared<primitive::LazyTensor>(inputs_[1][0]));
    Tensor out_grad(std::make_shared<primitive::LazyTensor>(out_grads[0][0]));

    VLOG(6) << "Vjp prepare Prepare attributes of multiply_grad";

    int axis = -1;

    VLOG(6) << "Vjp prepare call multiply_'s vjp inteface";

    std::vector<std::vector<Tensor>> tensor_res =
        primitive::multiply_vjp(
        x, y, out_grad, axis, stop_gradients);

    VLOG(6) << "Vjp prepare stop gradient of multiply_grad";

    std::vector<std::vector<pir::OpResult>> res(tensor_res.size());
    for (size_t i = 0; i < tensor_res.size(); ++i) {
        res[i].resize(tensor_res[i].size());
        for (size_t j = 0; j < tensor_res[i].size(); ++j) {
            if(tensor_res[i][j].defined()){
                res[i][j] = std::static_pointer_cast<primitive::LazyTensor>(tensor_res[i][j].impl())->value().dyn_cast<pir::OpResult>();
            }
        }
    }
    return res;
}

std::vector<std::vector<pir::OpResult>> NormOp::Vjp(pir::Operation* op, const std::vector<std::vector<pir::Value>>& inputs_, const std::vector<std::vector<pir::Value>>& outputs, const std::vector<std::vector<pir::Value>>& out_grads, const std::vector<std::vector<bool>>& stop_gradients){

    PADDLE_ENFORCE_EQ(
      inputs_.size(),
      1,
      platform::errors::InvalidArgument("norm op's inputs size should be 1, but now is %d.", inputs_.size()));
    PADDLE_ENFORCE_EQ(
      outputs.size(),
      2,
      platform::errors::InvalidArgument("norm op's outputs size should be 2, but now is %d.", outputs.size()));

    VLOG(6) << "Prepare inputs of norm_grad";

    Tensor x(std::make_shared<primitive::LazyTensor>(inputs_[0][0]));
    Tensor norm(std::make_shared<primitive::LazyTensor>(outputs[1][0]));
    Tensor out_grad(std::make_shared<primitive::LazyTensor>(out_grads[0][0]));

    VLOG(6) << "Vjp prepare Prepare attributes of norm_grad";

    int axis = op->attribute("axis").dyn_cast<pir::Int32Attribute>().data();
    float epsilon = op->attribute("epsilon").dyn_cast<pir::FloatAttribute>().data();
    bool is_test = op->attribute("is_test").dyn_cast<pir::BoolAttribute>().data();

    VLOG(6) << "Vjp prepare call norm's vjp inteface";

    std::vector<std::vector<Tensor>> tensor_res =
        primitive::norm_vjp(
        x, norm, out_grad, axis, epsilon, is_test, stop_gradients);

    VLOG(6) << "Vjp prepare stop gradient of norm_grad";

    std::vector<std::vector<pir::OpResult>> res(tensor_res.size());
    for (size_t i = 0; i < tensor_res.size(); ++i) {
        res[i].resize(tensor_res[i].size());
        for (size_t j = 0; j < tensor_res[i].size(); ++j) {
            if(tensor_res[i][j].defined()){
                res[i][j] = std::static_pointer_cast<primitive::LazyTensor>(tensor_res[i][j].impl())->value().dyn_cast<pir::OpResult>();
            }
        }
    }
    return res;
}

std::vector<std::vector<pir::OpResult>> PadOp::Vjp(pir::Operation* op, const std::vector<std::vector<pir::Value>>& inputs_, const std::vector<std::vector<pir::Value>>& outputs, const std::vector<std::vector<pir::Value>>& out_grads, const std::vector<std::vector<bool>>& stop_gradients){

    PADDLE_ENFORCE_EQ(
      inputs_.size(),
      2,
      platform::errors::InvalidArgument("pad op's inputs size should be 2, but now is %d.", inputs_.size()));
    PADDLE_ENFORCE_EQ(
      outputs.size(),
      1,
      platform::errors::InvalidArgument("pad op's outputs size should be 1, but now is %d.", outputs.size()));

    VLOG(6) << "Prepare inputs of pad_grad";

    Tensor x(std::make_shared<primitive::LazyTensor>(inputs_[0][0]));
    Tensor out_grad(std::make_shared<primitive::LazyTensor>(out_grads[0][0]));

    VLOG(6) << "Vjp prepare Prepare attributes of pad_grad";

    std::vector<int> paddings;
    for (size_t i = 0; i < op->attribute("paddings").dyn_cast<pir::ArrayAttribute>().size(); i++) {
        paddings.push_back(op->attribute("paddings").dyn_cast<pir::ArrayAttribute>().at(i).dyn_cast<pir::Int32Attribute>().data());
    }
    Tensor pad_value(std::make_shared<primitive::LazyTensor>(inputs_[1][0]));

    VLOG(6) << "Vjp prepare call pad's vjp inteface";

    std::vector<std::vector<Tensor>> tensor_res =
        primitive::pad_vjp(
        x, out_grad, pad_value, paddings, stop_gradients);

    VLOG(6) << "Vjp prepare stop gradient of pad_grad";

    std::vector<std::vector<pir::OpResult>> res(tensor_res.size());
    for (size_t i = 0; i < tensor_res.size(); ++i) {
        res[i].resize(tensor_res[i].size());
        for (size_t j = 0; j < tensor_res[i].size(); ++j) {
            if(tensor_res[i][j].defined()){
                res[i][j] = std::static_pointer_cast<primitive::LazyTensor>(tensor_res[i][j].impl())->value().dyn_cast<pir::OpResult>();
            }
        }
    }
    return res;
}

std::vector<std::vector<pir::OpResult>> Pool2dOp::Vjp(pir::Operation* op, const std::vector<std::vector<pir::Value>>& inputs_, const std::vector<std::vector<pir::Value>>& outputs, const std::vector<std::vector<pir::Value>>& out_grads, const std::vector<std::vector<bool>>& stop_gradients){

    PADDLE_ENFORCE_EQ(
      inputs_.size(),
      2,
      platform::errors::InvalidArgument("pool2d op's inputs size should be 2, but now is %d.", inputs_.size()));
    PADDLE_ENFORCE_EQ(
      outputs.size(),
      1,
      platform::errors::InvalidArgument("pool2d op's outputs size should be 1, but now is %d.", outputs.size()));

    VLOG(6) << "Prepare inputs of pool2d_grad";

    Tensor x(std::make_shared<primitive::LazyTensor>(inputs_[0][0]));
    Tensor out(std::make_shared<primitive::LazyTensor>(outputs[0][0]));
    Tensor out_grad(std::make_shared<primitive::LazyTensor>(out_grads[0][0]));

    VLOG(6) << "Vjp prepare Prepare attributes of pool2d_grad";

    Tensor kernel_size(std::make_shared<primitive::LazyTensor>(inputs_[1][0]));
    std::vector<int> strides;
    for (size_t i = 0; i < op->attribute("strides").dyn_cast<pir::ArrayAttribute>().size(); i++) {
        strides.push_back(op->attribute("strides").dyn_cast<pir::ArrayAttribute>().at(i).dyn_cast<pir::Int32Attribute>().data());
    }
    std::vector<int> paddings;
    for (size_t i = 0; i < op->attribute("paddings").dyn_cast<pir::ArrayAttribute>().size(); i++) {
        paddings.push_back(op->attribute("paddings").dyn_cast<pir::ArrayAttribute>().at(i).dyn_cast<pir::Int32Attribute>().data());
    }
    bool ceil_mode = op->attribute("ceil_mode").dyn_cast<pir::BoolAttribute>().data();
    bool exclusive = op->attribute("exclusive").dyn_cast<pir::BoolAttribute>().data();
    std::string data_format = op->attribute("data_format").dyn_cast<pir::StrAttribute>().AsString();
    std::string pooling_type = op->attribute("pooling_type").dyn_cast<pir::StrAttribute>().AsString();
    bool global_pooling = op->attribute("global_pooling").dyn_cast<pir::BoolAttribute>().data();
    bool adaptive = op->attribute("adaptive").dyn_cast<pir::BoolAttribute>().data();
    std::string padding_algorithm = op->attribute("padding_algorithm").dyn_cast<pir::StrAttribute>().AsString();

    VLOG(6) << "Vjp prepare call pool2d's vjp inteface";

    std::vector<std::vector<Tensor>> tensor_res =
        primitive::pool2d_vjp(
        x, out, out_grad, kernel_size, strides, paddings, ceil_mode, exclusive, data_format, pooling_type, global_pooling, adaptive, padding_algorithm, stop_gradients);

    VLOG(6) << "Vjp prepare stop gradient of pool2d_grad";

    std::vector<std::vector<pir::OpResult>> res(tensor_res.size());
    for (size_t i = 0; i < tensor_res.size(); ++i) {
        res[i].resize(tensor_res[i].size());
        for (size_t j = 0; j < tensor_res[i].size(); ++j) {
            if(tensor_res[i][j].defined()){
                res[i][j] = std::static_pointer_cast<primitive::LazyTensor>(tensor_res[i][j].impl())->value().dyn_cast<pir::OpResult>();
            }
        }
    }
    return res;
}

std::vector<std::vector<pir::OpResult>> Pool3dOp::Vjp(pir::Operation* op, const std::vector<std::vector<pir::Value>>& inputs_, const std::vector<std::vector<pir::Value>>& outputs, const std::vector<std::vector<pir::Value>>& out_grads, const std::vector<std::vector<bool>>& stop_gradients){

    PADDLE_ENFORCE_EQ(
      inputs_.size(),
      1,
      platform::errors::InvalidArgument("pool3d op's inputs size should be 1, but now is %d.", inputs_.size()));
    PADDLE_ENFORCE_EQ(
      outputs.size(),
      1,
      platform::errors::InvalidArgument("pool3d op's outputs size should be 1, but now is %d.", outputs.size()));

    VLOG(6) << "Prepare inputs of pool3d_grad";

    Tensor x(std::make_shared<primitive::LazyTensor>(inputs_[0][0]));
    Tensor out(std::make_shared<primitive::LazyTensor>(outputs[0][0]));
    Tensor out_grad(std::make_shared<primitive::LazyTensor>(out_grads[0][0]));

    VLOG(6) << "Vjp prepare Prepare attributes of pool3d_grad";

    std::vector<int> kernel_size;
    for (size_t i = 0; i < op->attribute("kernel_size").dyn_cast<pir::ArrayAttribute>().size(); i++) {
        kernel_size.push_back(op->attribute("kernel_size").dyn_cast<pir::ArrayAttribute>().at(i).dyn_cast<pir::Int32Attribute>().data());
    }
    std::vector<int> strides;
    for (size_t i = 0; i < op->attribute("strides").dyn_cast<pir::ArrayAttribute>().size(); i++) {
        strides.push_back(op->attribute("strides").dyn_cast<pir::ArrayAttribute>().at(i).dyn_cast<pir::Int32Attribute>().data());
    }
    std::vector<int> paddings;
    for (size_t i = 0; i < op->attribute("paddings").dyn_cast<pir::ArrayAttribute>().size(); i++) {
        paddings.push_back(op->attribute("paddings").dyn_cast<pir::ArrayAttribute>().at(i).dyn_cast<pir::Int32Attribute>().data());
    }
    bool ceil_mode = op->attribute("ceil_mode").dyn_cast<pir::BoolAttribute>().data();
    bool exclusive = op->attribute("exclusive").dyn_cast<pir::BoolAttribute>().data();
    std::string data_format = op->attribute("data_format").dyn_cast<pir::StrAttribute>().AsString();
    std::string pooling_type = op->attribute("pooling_type").dyn_cast<pir::StrAttribute>().AsString();
    bool global_pooling = op->attribute("global_pooling").dyn_cast<pir::BoolAttribute>().data();
    bool adaptive = op->attribute("adaptive").dyn_cast<pir::BoolAttribute>().data();
    std::string padding_algorithm = op->attribute("padding_algorithm").dyn_cast<pir::StrAttribute>().AsString();

    VLOG(6) << "Vjp prepare call pool3d's vjp inteface";

    std::vector<std::vector<Tensor>> tensor_res =
        primitive::pool3d_vjp(
        x, out, out_grad, kernel_size, strides, paddings, ceil_mode, exclusive, data_format, pooling_type, global_pooling, adaptive, padding_algorithm, stop_gradients);

    VLOG(6) << "Vjp prepare stop gradient of pool3d_grad";

    std::vector<std::vector<pir::OpResult>> res(tensor_res.size());
    for (size_t i = 0; i < tensor_res.size(); ++i) {
        res[i].resize(tensor_res[i].size());
        for (size_t j = 0; j < tensor_res[i].size(); ++j) {
            if(tensor_res[i][j].defined()){
                res[i][j] = std::static_pointer_cast<primitive::LazyTensor>(tensor_res[i][j].impl())->value().dyn_cast<pir::OpResult>();
            }
        }
    }
    return res;
}

std::vector<std::vector<pir::OpResult>> ProdOp::Vjp(pir::Operation* op, const std::vector<std::vector<pir::Value>>& inputs_, const std::vector<std::vector<pir::Value>>& outputs, const std::vector<std::vector<pir::Value>>& out_grads, const std::vector<std::vector<bool>>& stop_gradients){

    PADDLE_ENFORCE_EQ(
      inputs_.size(),
      2,
      platform::errors::InvalidArgument("prod op's inputs size should be 2, but now is %d.", inputs_.size()));
    PADDLE_ENFORCE_EQ(
      outputs.size(),
      1,
      platform::errors::InvalidArgument("prod op's outputs size should be 1, but now is %d.", outputs.size()));

    VLOG(6) << "Prepare inputs of prod_grad";

    Tensor x(std::make_shared<primitive::LazyTensor>(inputs_[0][0]));
    Tensor out(std::make_shared<primitive::LazyTensor>(outputs[0][0]));
    Tensor out_grad(std::make_shared<primitive::LazyTensor>(out_grads[0][0]));

    VLOG(6) << "Vjp prepare Prepare attributes of prod_grad";

    Tensor dims(std::make_shared<primitive::LazyTensor>(inputs_[1][0]));
    bool keep_dim = op->attribute("keep_dim").dyn_cast<pir::BoolAttribute>().data();
    bool reduce_all = op->attribute("reduce_all").dyn_cast<pir::BoolAttribute>().data();

    VLOG(6) << "Vjp prepare call prod's vjp inteface";

    std::vector<std::vector<Tensor>> tensor_res =
        primitive::prod_vjp(
        x, out, out_grad, dims, keep_dim, reduce_all, stop_gradients);

    VLOG(6) << "Vjp prepare stop gradient of prod_grad";

    std::vector<std::vector<pir::OpResult>> res(tensor_res.size());
    for (size_t i = 0; i < tensor_res.size(); ++i) {
        res[i].resize(tensor_res[i].size());
        for (size_t j = 0; j < tensor_res[i].size(); ++j) {
            if(tensor_res[i][j].defined()){
                res[i][j] = std::static_pointer_cast<primitive::LazyTensor>(tensor_res[i][j].impl())->value().dyn_cast<pir::OpResult>();
            }
        }
    }
    return res;
}

std::vector<std::vector<pir::OpResult>> RepeatInterleaveOp::Vjp(pir::Operation* op, const std::vector<std::vector<pir::Value>>& inputs_, const std::vector<std::vector<pir::Value>>& outputs, const std::vector<std::vector<pir::Value>>& out_grads, const std::vector<std::vector<bool>>& stop_gradients){

    PADDLE_ENFORCE_EQ(
      inputs_.size(),
      1,
      platform::errors::InvalidArgument("repeat_interleave op's inputs size should be 1, but now is %d.", inputs_.size()));
    PADDLE_ENFORCE_EQ(
      outputs.size(),
      1,
      platform::errors::InvalidArgument("repeat_interleave op's outputs size should be 1, but now is %d.", outputs.size()));

    VLOG(6) << "Prepare inputs of repeat_interleave_grad";

    Tensor x(std::make_shared<primitive::LazyTensor>(inputs_[0][0]));
    Tensor out_grad(std::make_shared<primitive::LazyTensor>(out_grads[0][0]));

    VLOG(6) << "Vjp prepare Prepare attributes of repeat_interleave_grad";

    int repeats = op->attribute("repeats").dyn_cast<pir::Int32Attribute>().data();
    int axis = op->attribute("axis").dyn_cast<pir::Int32Attribute>().data();

    VLOG(6) << "Vjp prepare call repeat_interleave's vjp inteface";

    std::vector<std::vector<Tensor>> tensor_res =
        primitive::repeat_interleave_vjp(
        x, out_grad, repeats, axis, stop_gradients);

    VLOG(6) << "Vjp prepare stop gradient of repeat_interleave_grad";

    std::vector<std::vector<pir::OpResult>> res(tensor_res.size());
    for (size_t i = 0; i < tensor_res.size(); ++i) {
        res[i].resize(tensor_res[i].size());
        for (size_t j = 0; j < tensor_res[i].size(); ++j) {
            if(tensor_res[i][j].defined()){
                res[i][j] = std::static_pointer_cast<primitive::LazyTensor>(tensor_res[i][j].impl())->value().dyn_cast<pir::OpResult>();
            }
        }
    }
    return res;
}

std::vector<std::vector<pir::OpResult>> RepeatInterleaveWithTensorIndexOp::Vjp(pir::Operation* op, const std::vector<std::vector<pir::Value>>& inputs_, const std::vector<std::vector<pir::Value>>& outputs, const std::vector<std::vector<pir::Value>>& out_grads, const std::vector<std::vector<bool>>& stop_gradients){

    PADDLE_ENFORCE_EQ(
      inputs_.size(),
      2,
      platform::errors::InvalidArgument("repeat_interleave_with_tensor_index op's inputs size should be 2, but now is %d.", inputs_.size()));
    PADDLE_ENFORCE_EQ(
      outputs.size(),
      1,
      platform::errors::InvalidArgument("repeat_interleave_with_tensor_index op's outputs size should be 1, but now is %d.", outputs.size()));

    VLOG(6) << "Prepare inputs of repeat_interleave_with_tensor_index_grad";

    Tensor x(std::make_shared<primitive::LazyTensor>(inputs_[0][0]));
    Tensor repeats(std::make_shared<primitive::LazyTensor>(inputs_[1][0]));
    Tensor out_grad(std::make_shared<primitive::LazyTensor>(out_grads[0][0]));

    VLOG(6) << "Vjp prepare Prepare attributes of repeat_interleave_with_tensor_index_grad";

    int axis = op->attribute("axis").dyn_cast<pir::Int32Attribute>().data();

    VLOG(6) << "Vjp prepare call repeat_interleave_with_tensor_index's vjp inteface";

    std::vector<std::vector<Tensor>> tensor_res =
        primitive::repeat_interleave_with_tensor_index_vjp(
        x, repeats, out_grad, axis, stop_gradients);

    VLOG(6) << "Vjp prepare stop gradient of repeat_interleave_with_tensor_index_grad";

    std::vector<std::vector<pir::OpResult>> res(tensor_res.size());
    for (size_t i = 0; i < tensor_res.size(); ++i) {
        res[i].resize(tensor_res[i].size());
        for (size_t j = 0; j < tensor_res[i].size(); ++j) {
            if(tensor_res[i][j].defined()){
                res[i][j] = std::static_pointer_cast<primitive::LazyTensor>(tensor_res[i][j].impl())->value().dyn_cast<pir::OpResult>();
            }
        }
    }
    return res;
}

std::vector<std::vector<pir::OpResult>> ReshapeOp::Vjp(pir::Operation* op, const std::vector<std::vector<pir::Value>>& inputs_, const std::vector<std::vector<pir::Value>>& outputs, const std::vector<std::vector<pir::Value>>& out_grads, const std::vector<std::vector<bool>>& stop_gradients){

    PADDLE_ENFORCE_EQ(
      inputs_.size(),
      2,
      platform::errors::InvalidArgument("reshape op's inputs size should be 2, but now is %d.", inputs_.size()));
    PADDLE_ENFORCE_EQ(
      outputs.size(),
      2,
      platform::errors::InvalidArgument("reshape op's outputs size should be 2, but now is %d.", outputs.size()));

    VLOG(6) << "Prepare inputs of reshape_grad";

    Tensor xshape(std::make_shared<primitive::LazyTensor>(outputs[1][0]));
    Tensor out_grad(std::make_shared<primitive::LazyTensor>(out_grads[0][0]));

    VLOG(6) << "Vjp prepare Prepare attributes of reshape_grad";


    VLOG(6) << "Vjp prepare call reshape's vjp inteface";

    std::vector<std::vector<Tensor>> tensor_res =
        primitive::reshape_vjp(
        xshape, out_grad, stop_gradients);

    VLOG(6) << "Vjp prepare stop gradient of reshape_grad";

    std::vector<std::vector<pir::OpResult>> res(tensor_res.size());
    for (size_t i = 0; i < tensor_res.size(); ++i) {
        res[i].resize(tensor_res[i].size());
        for (size_t j = 0; j < tensor_res[i].size(); ++j) {
            if(tensor_res[i][j].defined()){
                res[i][j] = std::static_pointer_cast<primitive::LazyTensor>(tensor_res[i][j].impl())->value().dyn_cast<pir::OpResult>();
            }
        }
    }
    return res;
}

std::vector<std::vector<pir::OpResult>> Reshape_Op::Vjp(pir::Operation* op, const std::vector<std::vector<pir::Value>>& inputs_, const std::vector<std::vector<pir::Value>>& outputs, const std::vector<std::vector<pir::Value>>& out_grads, const std::vector<std::vector<bool>>& stop_gradients){

    PADDLE_ENFORCE_EQ(
      inputs_.size(),
      2,
      platform::errors::InvalidArgument("reshape op's inputs size should be 2, but now is %d.", inputs_.size()));
    PADDLE_ENFORCE_EQ(
      outputs.size(),
      2,
      platform::errors::InvalidArgument("reshape op's outputs size should be 2, but now is %d.", outputs.size()));

    VLOG(6) << "Prepare inputs of reshape_grad";

    Tensor xshape(std::make_shared<primitive::LazyTensor>(outputs[1][0]));
    Tensor out_grad(std::make_shared<primitive::LazyTensor>(out_grads[0][0]));

    VLOG(6) << "Vjp prepare Prepare attributes of reshape_grad";


    VLOG(6) << "Vjp prepare call reshape_'s vjp inteface";

    std::vector<std::vector<Tensor>> tensor_res =
        primitive::reshape_vjp(
        xshape, out_grad, stop_gradients);

    VLOG(6) << "Vjp prepare stop gradient of reshape_grad";

    std::vector<std::vector<pir::OpResult>> res(tensor_res.size());
    for (size_t i = 0; i < tensor_res.size(); ++i) {
        res[i].resize(tensor_res[i].size());
        for (size_t j = 0; j < tensor_res[i].size(); ++j) {
            if(tensor_res[i][j].defined()){
                res[i][j] = std::static_pointer_cast<primitive::LazyTensor>(tensor_res[i][j].impl())->value().dyn_cast<pir::OpResult>();
            }
        }
    }
    return res;
}

std::vector<std::vector<pir::OpResult>> RnnOp::Vjp(pir::Operation* op, const std::vector<std::vector<pir::Value>>& inputs_, const std::vector<std::vector<pir::Value>>& outputs, const std::vector<std::vector<pir::Value>>& out_grads, const std::vector<std::vector<bool>>& stop_gradients){

    PADDLE_ENFORCE_EQ(
      inputs_.size(),
      5,
      platform::errors::InvalidArgument("rnn op's inputs size should be 5, but now is %d.", inputs_.size()));
    PADDLE_ENFORCE_EQ(
      outputs.size(),
      4,
      platform::errors::InvalidArgument("rnn op's outputs size should be 4, but now is %d.", outputs.size()));

    VLOG(6) << "Prepare inputs of rnn_grad";

    Tensor x(std::make_shared<primitive::LazyTensor>(inputs_[0][0]));
    std::vector<Tensor> pre_state;
    for (size_t idx = 0; idx < inputs_[1].size(); idx++) {
        pre_state.emplace_back(
            std::make_shared<primitive::LazyTensor>(inputs_[1][idx]));
    }
    std::vector<Tensor> weight_list;
    for (size_t idx = 0; idx < inputs_[2].size(); idx++) {
        weight_list.emplace_back(
            std::make_shared<primitive::LazyTensor>(inputs_[2][idx]));
    }
    paddle::optional<Tensor> sequence_length;
    if (!IsEmptyValue(inputs_[3][0])){
        sequence_length = paddle::make_optional<Tensor>(Tensor(std::make_shared<primitive::LazyTensor>(inputs_[3][0])));
    }
    Tensor out(std::make_shared<primitive::LazyTensor>(outputs[0][0]));
    Tensor dropout_state_out(std::make_shared<primitive::LazyTensor>(outputs[1][0]));
    Tensor reserve(std::make_shared<primitive::LazyTensor>(outputs[3][0]));
    Tensor out_grad(std::make_shared<primitive::LazyTensor>(out_grads[0][0]));
    std::vector<Tensor> state_grad;
    for (size_t idx = 0; idx < out_grads[2].size(); idx++) {
        state_grad.emplace_back(
            std::make_shared<primitive::LazyTensor>(out_grads[2][idx]));
    }

    VLOG(6) << "Vjp prepare Prepare attributes of rnn_grad";

    float dropout_prob = op->attribute("dropout_prob").dyn_cast<pir::FloatAttribute>().data();
    bool is_bidirec = op->attribute("is_bidirec").dyn_cast<pir::BoolAttribute>().data();
    int input_size = op->attribute("input_size").dyn_cast<pir::Int32Attribute>().data();
    int hidden_size = op->attribute("hidden_size").dyn_cast<pir::Int32Attribute>().data();
    int num_layers = op->attribute("num_layers").dyn_cast<pir::Int32Attribute>().data();
    std::string mode = op->attribute("mode").dyn_cast<pir::StrAttribute>().AsString();
    int seed = op->attribute("seed").dyn_cast<pir::Int32Attribute>().data();
    bool is_test = op->attribute("is_test").dyn_cast<pir::BoolAttribute>().data();

    VLOG(6) << "Vjp prepare call rnn's vjp inteface";

    std::vector<std::vector<Tensor>> tensor_res =
        primitive::rnn_vjp(
        x, pre_state, weight_list, sequence_length, out, dropout_state_out, reserve, out_grad, state_grad, dropout_prob, is_bidirec, input_size, hidden_size, num_layers, mode, seed, is_test, stop_gradients);

    VLOG(6) << "Vjp prepare stop gradient of rnn_grad";

    std::vector<std::vector<pir::OpResult>> res(tensor_res.size());
    for (size_t i = 0; i < tensor_res.size(); ++i) {
        res[i].resize(tensor_res[i].size());
        for (size_t j = 0; j < tensor_res[i].size(); ++j) {
            if(tensor_res[i][j].defined()){
                res[i][j] = std::static_pointer_cast<primitive::LazyTensor>(tensor_res[i][j].impl())->value().dyn_cast<pir::OpResult>();
            }
        }
    }
    return res;
}

std::vector<std::vector<pir::OpResult>> Rnn_Op::Vjp(pir::Operation* op, const std::vector<std::vector<pir::Value>>& inputs_, const std::vector<std::vector<pir::Value>>& outputs, const std::vector<std::vector<pir::Value>>& out_grads, const std::vector<std::vector<bool>>& stop_gradients){

    PADDLE_ENFORCE_EQ(
      inputs_.size(),
      5,
      platform::errors::InvalidArgument("rnn op's inputs size should be 5, but now is %d.", inputs_.size()));
    PADDLE_ENFORCE_EQ(
      outputs.size(),
      4,
      platform::errors::InvalidArgument("rnn op's outputs size should be 4, but now is %d.", outputs.size()));

    VLOG(6) << "Prepare inputs of rnn_grad";

    Tensor x(std::make_shared<primitive::LazyTensor>(inputs_[0][0]));
    std::vector<Tensor> pre_state;
    for (size_t idx = 0; idx < inputs_[1].size(); idx++) {
        pre_state.emplace_back(
            std::make_shared<primitive::LazyTensor>(inputs_[1][idx]));
    }
    std::vector<Tensor> weight_list;
    for (size_t idx = 0; idx < inputs_[2].size(); idx++) {
        weight_list.emplace_back(
            std::make_shared<primitive::LazyTensor>(inputs_[2][idx]));
    }
    paddle::optional<Tensor> sequence_length;
    if (!IsEmptyValue(inputs_[3][0])){
        sequence_length = paddle::make_optional<Tensor>(Tensor(std::make_shared<primitive::LazyTensor>(inputs_[3][0])));
    }
    Tensor out(std::make_shared<primitive::LazyTensor>(outputs[0][0]));
    Tensor dropout_state_out(std::make_shared<primitive::LazyTensor>(outputs[1][0]));
    Tensor reserve(std::make_shared<primitive::LazyTensor>(outputs[3][0]));
    Tensor out_grad(std::make_shared<primitive::LazyTensor>(out_grads[0][0]));
    std::vector<Tensor> state_grad;
    for (size_t idx = 0; idx < out_grads[2].size(); idx++) {
        state_grad.emplace_back(
            std::make_shared<primitive::LazyTensor>(out_grads[2][idx]));
    }

    VLOG(6) << "Vjp prepare Prepare attributes of rnn_grad";

    float dropout_prob = op->attribute("dropout_prob").dyn_cast<pir::FloatAttribute>().data();
    bool is_bidirec = op->attribute("is_bidirec").dyn_cast<pir::BoolAttribute>().data();
    int input_size = op->attribute("input_size").dyn_cast<pir::Int32Attribute>().data();
    int hidden_size = op->attribute("hidden_size").dyn_cast<pir::Int32Attribute>().data();
    int num_layers = op->attribute("num_layers").dyn_cast<pir::Int32Attribute>().data();
    std::string mode = op->attribute("mode").dyn_cast<pir::StrAttribute>().AsString();
    int seed = op->attribute("seed").dyn_cast<pir::Int32Attribute>().data();
    bool is_test = op->attribute("is_test").dyn_cast<pir::BoolAttribute>().data();

    VLOG(6) << "Vjp prepare call rnn_'s vjp inteface";

    std::vector<std::vector<Tensor>> tensor_res =
        primitive::rnn_vjp(
        x, pre_state, weight_list, sequence_length, out, dropout_state_out, reserve, out_grad, state_grad, dropout_prob, is_bidirec, input_size, hidden_size, num_layers, mode, seed, is_test, stop_gradients);

    VLOG(6) << "Vjp prepare stop gradient of rnn_grad";

    std::vector<std::vector<pir::OpResult>> res(tensor_res.size());
    for (size_t i = 0; i < tensor_res.size(); ++i) {
        res[i].resize(tensor_res[i].size());
        for (size_t j = 0; j < tensor_res[i].size(); ++j) {
            if(tensor_res[i][j].defined()){
                res[i][j] = std::static_pointer_cast<primitive::LazyTensor>(tensor_res[i][j].impl())->value().dyn_cast<pir::OpResult>();
            }
        }
    }
    return res;
}

std::vector<std::vector<pir::OpResult>> RreluOp::Vjp(pir::Operation* op, const std::vector<std::vector<pir::Value>>& inputs_, const std::vector<std::vector<pir::Value>>& outputs, const std::vector<std::vector<pir::Value>>& out_grads, const std::vector<std::vector<bool>>& stop_gradients){

    PADDLE_ENFORCE_EQ(
      inputs_.size(),
      1,
      platform::errors::InvalidArgument("rrelu op's inputs size should be 1, but now is %d.", inputs_.size()));
    PADDLE_ENFORCE_EQ(
      outputs.size(),
      2,
      platform::errors::InvalidArgument("rrelu op's outputs size should be 2, but now is %d.", outputs.size()));

    VLOG(6) << "Prepare inputs of rrelu_grad";

    Tensor x(std::make_shared<primitive::LazyTensor>(inputs_[0][0]));
    Tensor noise(std::make_shared<primitive::LazyTensor>(outputs[1][0]));
    Tensor out_grad(std::make_shared<primitive::LazyTensor>(out_grads[0][0]));

    VLOG(6) << "Vjp prepare Prepare attributes of rrelu_grad";


    VLOG(6) << "Vjp prepare call rrelu's vjp inteface";

    std::vector<std::vector<Tensor>> tensor_res =
        primitive::rrelu_vjp(
        x, noise, out_grad, stop_gradients);

    VLOG(6) << "Vjp prepare stop gradient of rrelu_grad";

    std::vector<std::vector<pir::OpResult>> res(tensor_res.size());
    for (size_t i = 0; i < tensor_res.size(); ++i) {
        res[i].resize(tensor_res[i].size());
        for (size_t j = 0; j < tensor_res[i].size(); ++j) {
            if(tensor_res[i][j].defined()){
                res[i][j] = std::static_pointer_cast<primitive::LazyTensor>(tensor_res[i][j].impl())->value().dyn_cast<pir::OpResult>();
            }
        }
    }
    return res;
}

std::vector<std::vector<pir::OpResult>> SetValueOp::Vjp(pir::Operation* op, const std::vector<std::vector<pir::Value>>& inputs_, const std::vector<std::vector<pir::Value>>& outputs, const std::vector<std::vector<pir::Value>>& out_grads, const std::vector<std::vector<bool>>& stop_gradients){

    PADDLE_ENFORCE_EQ(
      inputs_.size(),
      4,
      platform::errors::InvalidArgument("set_value op's inputs size should be 4, but now is %d.", inputs_.size()));
    PADDLE_ENFORCE_EQ(
      outputs.size(),
      1,
      platform::errors::InvalidArgument("set_value op's outputs size should be 1, but now is %d.", outputs.size()));

    VLOG(6) << "Prepare inputs of set_value_grad";

    Tensor out_grad(std::make_shared<primitive::LazyTensor>(out_grads[0][0]));

    VLOG(6) << "Vjp prepare Prepare attributes of set_value_grad";


    VLOG(6) << "Vjp prepare call set_value's vjp inteface";

    std::vector<std::vector<Tensor>> tensor_res =
        primitive::set_value_vjp(
        out_grad, stop_gradients);

    VLOG(6) << "Vjp prepare stop gradient of set_value_grad";

    std::vector<std::vector<pir::OpResult>> res(tensor_res.size());
    for (size_t i = 0; i < tensor_res.size(); ++i) {
        res[i].resize(tensor_res[i].size());
        for (size_t j = 0; j < tensor_res[i].size(); ++j) {
            if(tensor_res[i][j].defined()){
                res[i][j] = std::static_pointer_cast<primitive::LazyTensor>(tensor_res[i][j].impl())->value().dyn_cast<pir::OpResult>();
            }
        }
    }
    return res;
}

std::vector<std::vector<pir::OpResult>> SetValue_Op::Vjp(pir::Operation* op, const std::vector<std::vector<pir::Value>>& inputs_, const std::vector<std::vector<pir::Value>>& outputs, const std::vector<std::vector<pir::Value>>& out_grads, const std::vector<std::vector<bool>>& stop_gradients){

    PADDLE_ENFORCE_EQ(
      inputs_.size(),
      4,
      platform::errors::InvalidArgument("set_value op's inputs size should be 4, but now is %d.", inputs_.size()));
    PADDLE_ENFORCE_EQ(
      outputs.size(),
      1,
      platform::errors::InvalidArgument("set_value op's outputs size should be 1, but now is %d.", outputs.size()));

    VLOG(6) << "Prepare inputs of set_value_grad";

    Tensor out_grad(std::make_shared<primitive::LazyTensor>(out_grads[0][0]));

    VLOG(6) << "Vjp prepare Prepare attributes of set_value_grad";


    VLOG(6) << "Vjp prepare call set_value_'s vjp inteface";

    std::vector<std::vector<Tensor>> tensor_res =
        primitive::set_value_vjp(
        out_grad, stop_gradients);

    VLOG(6) << "Vjp prepare stop gradient of set_value_grad";

    std::vector<std::vector<pir::OpResult>> res(tensor_res.size());
    for (size_t i = 0; i < tensor_res.size(); ++i) {
        res[i].resize(tensor_res[i].size());
        for (size_t j = 0; j < tensor_res[i].size(); ++j) {
            if(tensor_res[i][j].defined()){
                res[i][j] = std::static_pointer_cast<primitive::LazyTensor>(tensor_res[i][j].impl())->value().dyn_cast<pir::OpResult>();
            }
        }
    }
    return res;
}

std::vector<std::vector<pir::OpResult>> SetValueWithTensorOp::Vjp(pir::Operation* op, const std::vector<std::vector<pir::Value>>& inputs_, const std::vector<std::vector<pir::Value>>& outputs, const std::vector<std::vector<pir::Value>>& out_grads, const std::vector<std::vector<bool>>& stop_gradients){

    PADDLE_ENFORCE_EQ(
      inputs_.size(),
      5,
      platform::errors::InvalidArgument("set_value_with_tensor op's inputs size should be 5, but now is %d.", inputs_.size()));
    PADDLE_ENFORCE_EQ(
      outputs.size(),
      1,
      platform::errors::InvalidArgument("set_value_with_tensor op's outputs size should be 1, but now is %d.", outputs.size()));

    VLOG(6) << "Prepare inputs of set_value_with_tensor_grad";

    Tensor values(std::make_shared<primitive::LazyTensor>(inputs_[1][0]));
    Tensor out_grad(std::make_shared<primitive::LazyTensor>(out_grads[0][0]));

    VLOG(6) << "Vjp prepare Prepare attributes of set_value_with_tensor_grad";

    Tensor starts(std::make_shared<primitive::LazyTensor>(inputs_[2][0]));
    Tensor ends(std::make_shared<primitive::LazyTensor>(inputs_[3][0]));
    Tensor steps(std::make_shared<primitive::LazyTensor>(inputs_[4][0]));
    std::vector<int64_t> axes;
    for (size_t i = 0; i < op->attribute("axes").dyn_cast<pir::ArrayAttribute>().size(); i++) {
        axes.push_back(op->attribute("axes").dyn_cast<pir::ArrayAttribute>().at(i).dyn_cast<pir::Int64Attribute>().data());
    }
    std::vector<int64_t> decrease_axes;
    for (size_t i = 0; i < op->attribute("decrease_axes").dyn_cast<pir::ArrayAttribute>().size(); i++) {
        decrease_axes.push_back(op->attribute("decrease_axes").dyn_cast<pir::ArrayAttribute>().at(i).dyn_cast<pir::Int64Attribute>().data());
    }
    std::vector<int64_t> none_axes;
    for (size_t i = 0; i < op->attribute("none_axes").dyn_cast<pir::ArrayAttribute>().size(); i++) {
        none_axes.push_back(op->attribute("none_axes").dyn_cast<pir::ArrayAttribute>().at(i).dyn_cast<pir::Int64Attribute>().data());
    }

    VLOG(6) << "Vjp prepare call set_value_with_tensor's vjp inteface";

    std::vector<std::vector<Tensor>> tensor_res =
        primitive::set_value_with_tensor_vjp(
        values, out_grad, starts, ends, steps, axes, decrease_axes, none_axes, stop_gradients);

    VLOG(6) << "Vjp prepare stop gradient of set_value_with_tensor_grad";

    std::vector<std::vector<pir::OpResult>> res(tensor_res.size());
    for (size_t i = 0; i < tensor_res.size(); ++i) {
        res[i].resize(tensor_res[i].size());
        for (size_t j = 0; j < tensor_res[i].size(); ++j) {
            if(tensor_res[i][j].defined()){
                res[i][j] = std::static_pointer_cast<primitive::LazyTensor>(tensor_res[i][j].impl())->value().dyn_cast<pir::OpResult>();
            }
        }
    }
    return res;
}

std::vector<std::vector<pir::OpResult>> SetValueWithTensor_Op::Vjp(pir::Operation* op, const std::vector<std::vector<pir::Value>>& inputs_, const std::vector<std::vector<pir::Value>>& outputs, const std::vector<std::vector<pir::Value>>& out_grads, const std::vector<std::vector<bool>>& stop_gradients){

    PADDLE_ENFORCE_EQ(
      inputs_.size(),
      5,
      platform::errors::InvalidArgument("set_value_with_tensor op's inputs size should be 5, but now is %d.", inputs_.size()));
    PADDLE_ENFORCE_EQ(
      outputs.size(),
      1,
      platform::errors::InvalidArgument("set_value_with_tensor op's outputs size should be 1, but now is %d.", outputs.size()));

    VLOG(6) << "Prepare inputs of set_value_with_tensor_grad";

    Tensor values(std::make_shared<primitive::LazyTensor>(inputs_[1][0]));
    Tensor out_grad(std::make_shared<primitive::LazyTensor>(out_grads[0][0]));

    VLOG(6) << "Vjp prepare Prepare attributes of set_value_with_tensor_grad";

    Tensor starts(std::make_shared<primitive::LazyTensor>(inputs_[2][0]));
    Tensor ends(std::make_shared<primitive::LazyTensor>(inputs_[3][0]));
    Tensor steps(std::make_shared<primitive::LazyTensor>(inputs_[4][0]));
    std::vector<int64_t> axes;
    for (size_t i = 0; i < op->attribute("axes").dyn_cast<pir::ArrayAttribute>().size(); i++) {
        axes.push_back(op->attribute("axes").dyn_cast<pir::ArrayAttribute>().at(i).dyn_cast<pir::Int64Attribute>().data());
    }
    std::vector<int64_t> decrease_axes;
    for (size_t i = 0; i < op->attribute("decrease_axes").dyn_cast<pir::ArrayAttribute>().size(); i++) {
        decrease_axes.push_back(op->attribute("decrease_axes").dyn_cast<pir::ArrayAttribute>().at(i).dyn_cast<pir::Int64Attribute>().data());
    }
    std::vector<int64_t> none_axes;
    for (size_t i = 0; i < op->attribute("none_axes").dyn_cast<pir::ArrayAttribute>().size(); i++) {
        none_axes.push_back(op->attribute("none_axes").dyn_cast<pir::ArrayAttribute>().at(i).dyn_cast<pir::Int64Attribute>().data());
    }

    VLOG(6) << "Vjp prepare call set_value_with_tensor_'s vjp inteface";

    std::vector<std::vector<Tensor>> tensor_res =
        primitive::set_value_with_tensor_vjp(
        values, out_grad, starts, ends, steps, axes, decrease_axes, none_axes, stop_gradients);

    VLOG(6) << "Vjp prepare stop gradient of set_value_with_tensor_grad";

    std::vector<std::vector<pir::OpResult>> res(tensor_res.size());
    for (size_t i = 0; i < tensor_res.size(); ++i) {
        res[i].resize(tensor_res[i].size());
        for (size_t j = 0; j < tensor_res[i].size(); ++j) {
            if(tensor_res[i][j].defined()){
                res[i][j] = std::static_pointer_cast<primitive::LazyTensor>(tensor_res[i][j].impl())->value().dyn_cast<pir::OpResult>();
            }
        }
    }
    return res;
}

std::vector<std::vector<pir::OpResult>> ShuffleBatchOp::Vjp(pir::Operation* op, const std::vector<std::vector<pir::Value>>& inputs_, const std::vector<std::vector<pir::Value>>& outputs, const std::vector<std::vector<pir::Value>>& out_grads, const std::vector<std::vector<bool>>& stop_gradients){

    PADDLE_ENFORCE_EQ(
      inputs_.size(),
      2,
      platform::errors::InvalidArgument("shuffle_batch op's inputs size should be 2, but now is %d.", inputs_.size()));
    PADDLE_ENFORCE_EQ(
      outputs.size(),
      3,
      platform::errors::InvalidArgument("shuffle_batch op's outputs size should be 3, but now is %d.", outputs.size()));

    VLOG(6) << "Prepare inputs of shuffle_batch_grad";

    Tensor shuffle_idx(std::make_shared<primitive::LazyTensor>(outputs[1][0]));
    Tensor out_grad(std::make_shared<primitive::LazyTensor>(out_grads[0][0]));

    VLOG(6) << "Vjp prepare Prepare attributes of shuffle_batch_grad";

    int startup_seed = op->attribute("startup_seed").dyn_cast<pir::Int32Attribute>().data();

    VLOG(6) << "Vjp prepare call shuffle_batch's vjp inteface";

    std::vector<std::vector<Tensor>> tensor_res =
        primitive::shuffle_batch_vjp(
        shuffle_idx, out_grad, startup_seed, stop_gradients);

    VLOG(6) << "Vjp prepare stop gradient of shuffle_batch_grad";

    std::vector<std::vector<pir::OpResult>> res(tensor_res.size());
    for (size_t i = 0; i < tensor_res.size(); ++i) {
        res[i].resize(tensor_res[i].size());
        for (size_t j = 0; j < tensor_res[i].size(); ++j) {
            if(tensor_res[i][j].defined()){
                res[i][j] = std::static_pointer_cast<primitive::LazyTensor>(tensor_res[i][j].impl())->value().dyn_cast<pir::OpResult>();
            }
        }
    }
    return res;
}

std::vector<std::vector<pir::OpResult>> SliceOp::Vjp(pir::Operation* op, const std::vector<std::vector<pir::Value>>& inputs_, const std::vector<std::vector<pir::Value>>& outputs, const std::vector<std::vector<pir::Value>>& out_grads, const std::vector<std::vector<bool>>& stop_gradients){

    PADDLE_ENFORCE_EQ(
      inputs_.size(),
      3,
      platform::errors::InvalidArgument("slice op's inputs size should be 3, but now is %d.", inputs_.size()));
    PADDLE_ENFORCE_EQ(
      outputs.size(),
      1,
      platform::errors::InvalidArgument("slice op's outputs size should be 1, but now is %d.", outputs.size()));

    VLOG(6) << "Prepare inputs of slice_grad";

    Tensor input(std::make_shared<primitive::LazyTensor>(inputs_[0][0]));
    Tensor out_grad(std::make_shared<primitive::LazyTensor>(out_grads[0][0]));

    VLOG(6) << "Vjp prepare Prepare attributes of slice_grad";

    std::vector<int64_t> axes;
    for (size_t i = 0; i < op->attribute("axes").dyn_cast<pir::ArrayAttribute>().size(); i++) {
        axes.push_back(op->attribute("axes").dyn_cast<pir::ArrayAttribute>().at(i).dyn_cast<pir::Int64Attribute>().data());
    }
    Tensor starts(std::make_shared<primitive::LazyTensor>(inputs_[1][0]));
    Tensor ends(std::make_shared<primitive::LazyTensor>(inputs_[2][0]));
    std::vector<int64_t> infer_flags;
    for (size_t i = 0; i < op->attribute("infer_flags").dyn_cast<pir::ArrayAttribute>().size(); i++) {
        infer_flags.push_back(op->attribute("infer_flags").dyn_cast<pir::ArrayAttribute>().at(i).dyn_cast<pir::Int64Attribute>().data());
    }
    std::vector<int64_t> decrease_axis;
    for (size_t i = 0; i < op->attribute("decrease_axis").dyn_cast<pir::ArrayAttribute>().size(); i++) {
        decrease_axis.push_back(op->attribute("decrease_axis").dyn_cast<pir::ArrayAttribute>().at(i).dyn_cast<pir::Int64Attribute>().data());
    }

    VLOG(6) << "Vjp prepare call slice's vjp inteface";

    std::vector<std::vector<Tensor>> tensor_res =
        primitive::slice_vjp(
        input, out_grad, starts, ends, axes, infer_flags, decrease_axis, stop_gradients);

    VLOG(6) << "Vjp prepare stop gradient of slice_grad";

    std::vector<std::vector<pir::OpResult>> res(tensor_res.size());
    for (size_t i = 0; i < tensor_res.size(); ++i) {
        res[i].resize(tensor_res[i].size());
        for (size_t j = 0; j < tensor_res[i].size(); ++j) {
            if(tensor_res[i][j].defined()){
                res[i][j] = std::static_pointer_cast<primitive::LazyTensor>(tensor_res[i][j].impl())->value().dyn_cast<pir::OpResult>();
            }
        }
    }
    return res;
}

std::vector<std::vector<pir::OpResult>> SoftReluOp::Vjp(pir::Operation* op, const std::vector<std::vector<pir::Value>>& inputs_, const std::vector<std::vector<pir::Value>>& outputs, const std::vector<std::vector<pir::Value>>& out_grads, const std::vector<std::vector<bool>>& stop_gradients){

    PADDLE_ENFORCE_EQ(
      inputs_.size(),
      1,
      platform::errors::InvalidArgument("soft_relu op's inputs size should be 1, but now is %d.", inputs_.size()));
    PADDLE_ENFORCE_EQ(
      outputs.size(),
      1,
      platform::errors::InvalidArgument("soft_relu op's outputs size should be 1, but now is %d.", outputs.size()));

    VLOG(6) << "Prepare inputs of soft_relu_grad";

    Tensor out(std::make_shared<primitive::LazyTensor>(outputs[0][0]));
    Tensor out_grad(std::make_shared<primitive::LazyTensor>(out_grads[0][0]));

    VLOG(6) << "Vjp prepare Prepare attributes of soft_relu_grad";

    float threshold = op->attribute("threshold").dyn_cast<pir::FloatAttribute>().data();

    VLOG(6) << "Vjp prepare call soft_relu's vjp inteface";

    std::vector<std::vector<Tensor>> tensor_res =
        primitive::soft_relu_vjp(
        out, out_grad, threshold, stop_gradients);

    VLOG(6) << "Vjp prepare stop gradient of soft_relu_grad";

    std::vector<std::vector<pir::OpResult>> res(tensor_res.size());
    for (size_t i = 0; i < tensor_res.size(); ++i) {
        res[i].resize(tensor_res[i].size());
        for (size_t j = 0; j < tensor_res[i].size(); ++j) {
            if(tensor_res[i][j].defined()){
                res[i][j] = std::static_pointer_cast<primitive::LazyTensor>(tensor_res[i][j].impl())->value().dyn_cast<pir::OpResult>();
            }
        }
    }
    return res;
}

std::vector<std::vector<pir::OpResult>> SoftmaxOp::Vjp(pir::Operation* op, const std::vector<std::vector<pir::Value>>& inputs_, const std::vector<std::vector<pir::Value>>& outputs, const std::vector<std::vector<pir::Value>>& out_grads, const std::vector<std::vector<bool>>& stop_gradients){

    PADDLE_ENFORCE_EQ(
      inputs_.size(),
      1,
      platform::errors::InvalidArgument("softmax op's inputs size should be 1, but now is %d.", inputs_.size()));
    PADDLE_ENFORCE_EQ(
      outputs.size(),
      1,
      platform::errors::InvalidArgument("softmax op's outputs size should be 1, but now is %d.", outputs.size()));

    VLOG(6) << "Prepare inputs of softmax_grad";

    Tensor out(std::make_shared<primitive::LazyTensor>(outputs[0][0]));
    Tensor out_grad(std::make_shared<primitive::LazyTensor>(out_grads[0][0]));

    VLOG(6) << "Vjp prepare Prepare attributes of softmax_grad";

    int axis = op->attribute("axis").dyn_cast<pir::Int32Attribute>().data();

    VLOG(6) << "Vjp prepare call softmax's vjp inteface";

    std::vector<std::vector<Tensor>> tensor_res =
        primitive::softmax_vjp(
        out, out_grad, axis, stop_gradients);

    VLOG(6) << "Vjp prepare stop gradient of softmax_grad";

    std::vector<std::vector<pir::OpResult>> res(tensor_res.size());
    for (size_t i = 0; i < tensor_res.size(); ++i) {
        res[i].resize(tensor_res[i].size());
        for (size_t j = 0; j < tensor_res[i].size(); ++j) {
            if(tensor_res[i][j].defined()){
                res[i][j] = std::static_pointer_cast<primitive::LazyTensor>(tensor_res[i][j].impl())->value().dyn_cast<pir::OpResult>();
            }
        }
    }
    return res;
}

std::vector<std::vector<pir::OpResult>> Softmax_Op::Vjp(pir::Operation* op, const std::vector<std::vector<pir::Value>>& inputs_, const std::vector<std::vector<pir::Value>>& outputs, const std::vector<std::vector<pir::Value>>& out_grads, const std::vector<std::vector<bool>>& stop_gradients){

    PADDLE_ENFORCE_EQ(
      inputs_.size(),
      1,
      platform::errors::InvalidArgument("softmax op's inputs size should be 1, but now is %d.", inputs_.size()));
    PADDLE_ENFORCE_EQ(
      outputs.size(),
      1,
      platform::errors::InvalidArgument("softmax op's outputs size should be 1, but now is %d.", outputs.size()));

    VLOG(6) << "Prepare inputs of softmax_grad";

    Tensor out(std::make_shared<primitive::LazyTensor>(outputs[0][0]));
    Tensor out_grad(std::make_shared<primitive::LazyTensor>(out_grads[0][0]));

    VLOG(6) << "Vjp prepare Prepare attributes of softmax_grad";

    int axis = op->attribute("axis").dyn_cast<pir::Int32Attribute>().data();

    VLOG(6) << "Vjp prepare call softmax_'s vjp inteface";

    std::vector<std::vector<Tensor>> tensor_res =
        primitive::softmax_vjp(
        out, out_grad, axis, stop_gradients);

    VLOG(6) << "Vjp prepare stop gradient of softmax_grad";

    std::vector<std::vector<pir::OpResult>> res(tensor_res.size());
    for (size_t i = 0; i < tensor_res.size(); ++i) {
        res[i].resize(tensor_res[i].size());
        for (size_t j = 0; j < tensor_res[i].size(); ++j) {
            if(tensor_res[i][j].defined()){
                res[i][j] = std::static_pointer_cast<primitive::LazyTensor>(tensor_res[i][j].impl())->value().dyn_cast<pir::OpResult>();
            }
        }
    }
    return res;
}

std::vector<std::vector<pir::OpResult>> SplitOp::Vjp(pir::Operation* op, const std::vector<std::vector<pir::Value>>& inputs_, const std::vector<std::vector<pir::Value>>& outputs, const std::vector<std::vector<pir::Value>>& out_grads, const std::vector<std::vector<bool>>& stop_gradients){

    PADDLE_ENFORCE_EQ(
      inputs_.size(),
      3,
      platform::errors::InvalidArgument("split op's inputs size should be 3, but now is %d.", inputs_.size()));
    PADDLE_ENFORCE_EQ(
      outputs.size(),
      1,
      platform::errors::InvalidArgument("split op's outputs size should be 1, but now is %d.", outputs.size()));

    VLOG(6) << "Prepare inputs of split_grad";

    std::vector<Tensor> out_grad;
    for (size_t idx = 0; idx < out_grads[0].size(); idx++) {
        out_grad.emplace_back(
            std::make_shared<primitive::LazyTensor>(out_grads[0][idx]));
    }

    VLOG(6) << "Vjp prepare Prepare attributes of split_grad";

    Tensor axis(std::make_shared<primitive::LazyTensor>(inputs_[2][0]));

    VLOG(6) << "Vjp prepare call split's vjp inteface";

    std::vector<std::vector<Tensor>> tensor_res =
        primitive::split_vjp(
        out_grad, axis, stop_gradients);

    VLOG(6) << "Vjp prepare stop gradient of split_grad";

    std::vector<std::vector<pir::OpResult>> res(tensor_res.size());
    for (size_t i = 0; i < tensor_res.size(); ++i) {
        res[i].resize(tensor_res[i].size());
        for (size_t j = 0; j < tensor_res[i].size(); ++j) {
            if(tensor_res[i][j].defined()){
                res[i][j] = std::static_pointer_cast<primitive::LazyTensor>(tensor_res[i][j].impl())->value().dyn_cast<pir::OpResult>();
            }
        }
    }
    return res;
}

std::vector<std::vector<pir::OpResult>> SplitWithNumOp::Vjp(pir::Operation* op, const std::vector<std::vector<pir::Value>>& inputs_, const std::vector<std::vector<pir::Value>>& outputs, const std::vector<std::vector<pir::Value>>& out_grads, const std::vector<std::vector<bool>>& stop_gradients){

    PADDLE_ENFORCE_EQ(
      inputs_.size(),
      2,
      platform::errors::InvalidArgument("split_with_num op's inputs size should be 2, but now is %d.", inputs_.size()));
    PADDLE_ENFORCE_EQ(
      outputs.size(),
      1,
      platform::errors::InvalidArgument("split_with_num op's outputs size should be 1, but now is %d.", outputs.size()));

    VLOG(6) << "Prepare inputs of split_with_num_grad";

    std::vector<Tensor> out_grad;
    for (size_t idx = 0; idx < out_grads[0].size(); idx++) {
        out_grad.emplace_back(
            std::make_shared<primitive::LazyTensor>(out_grads[0][idx]));
    }

    VLOG(6) << "Vjp prepare Prepare attributes of split_with_num_grad";

    Tensor axis(std::make_shared<primitive::LazyTensor>(inputs_[1][0]));

    VLOG(6) << "Vjp prepare call split_with_num's vjp inteface";

    std::vector<std::vector<Tensor>> tensor_res =
        primitive::split_with_num_vjp(
        out_grad, axis, stop_gradients);

    VLOG(6) << "Vjp prepare stop gradient of split_with_num_grad";

    std::vector<std::vector<pir::OpResult>> res(tensor_res.size());
    for (size_t i = 0; i < tensor_res.size(); ++i) {
        res[i].resize(tensor_res[i].size());
        for (size_t j = 0; j < tensor_res[i].size(); ++j) {
            if(tensor_res[i][j].defined()){
                res[i][j] = std::static_pointer_cast<primitive::LazyTensor>(tensor_res[i][j].impl())->value().dyn_cast<pir::OpResult>();
            }
        }
    }
    return res;
}

std::vector<std::vector<pir::OpResult>> StridedSliceOp::Vjp(pir::Operation* op, const std::vector<std::vector<pir::Value>>& inputs_, const std::vector<std::vector<pir::Value>>& outputs, const std::vector<std::vector<pir::Value>>& out_grads, const std::vector<std::vector<bool>>& stop_gradients){

    PADDLE_ENFORCE_EQ(
      inputs_.size(),
      4,
      platform::errors::InvalidArgument("strided_slice op's inputs size should be 4, but now is %d.", inputs_.size()));
    PADDLE_ENFORCE_EQ(
      outputs.size(),
      1,
      platform::errors::InvalidArgument("strided_slice op's outputs size should be 1, but now is %d.", outputs.size()));

    VLOG(6) << "Prepare inputs of strided_slice_grad";

    Tensor x(std::make_shared<primitive::LazyTensor>(inputs_[0][0]));
    Tensor out_grad(std::make_shared<primitive::LazyTensor>(out_grads[0][0]));

    VLOG(6) << "Vjp prepare Prepare attributes of strided_slice_grad";

    std::vector<int> axes;
    for (size_t i = 0; i < op->attribute("axes").dyn_cast<pir::ArrayAttribute>().size(); i++) {
        axes.push_back(op->attribute("axes").dyn_cast<pir::ArrayAttribute>().at(i).dyn_cast<pir::Int32Attribute>().data());
    }
    Tensor starts(std::make_shared<primitive::LazyTensor>(inputs_[1][0]));
    Tensor ends(std::make_shared<primitive::LazyTensor>(inputs_[2][0]));
    Tensor strides(std::make_shared<primitive::LazyTensor>(inputs_[3][0]));

    VLOG(6) << "Vjp prepare call strided_slice's vjp inteface";

    std::vector<std::vector<Tensor>> tensor_res =
        primitive::strided_slice_vjp(
        x, out_grad, starts, ends, strides, axes, stop_gradients);

    VLOG(6) << "Vjp prepare stop gradient of strided_slice_grad";

    std::vector<std::vector<pir::OpResult>> res(tensor_res.size());
    for (size_t i = 0; i < tensor_res.size(); ++i) {
        res[i].resize(tensor_res[i].size());
        for (size_t j = 0; j < tensor_res[i].size(); ++j) {
            if(tensor_res[i][j].defined()){
                res[i][j] = std::static_pointer_cast<primitive::LazyTensor>(tensor_res[i][j].impl())->value().dyn_cast<pir::OpResult>();
            }
        }
    }
    return res;
}

std::vector<std::vector<pir::OpResult>> SubtractOp::Vjp(pir::Operation* op, const std::vector<std::vector<pir::Value>>& inputs_, const std::vector<std::vector<pir::Value>>& outputs, const std::vector<std::vector<pir::Value>>& out_grads, const std::vector<std::vector<bool>>& stop_gradients){

    PADDLE_ENFORCE_EQ(
      inputs_.size(),
      2,
      platform::errors::InvalidArgument("subtract op's inputs size should be 2, but now is %d.", inputs_.size()));
    PADDLE_ENFORCE_EQ(
      outputs.size(),
      1,
      platform::errors::InvalidArgument("subtract op's outputs size should be 1, but now is %d.", outputs.size()));

    VLOG(6) << "Prepare inputs of subtract_grad";

    Tensor x(std::make_shared<primitive::LazyTensor>(inputs_[0][0]));
    Tensor y(std::make_shared<primitive::LazyTensor>(inputs_[1][0]));
    Tensor out_grad(std::make_shared<primitive::LazyTensor>(out_grads[0][0]));

    VLOG(6) << "Vjp prepare Prepare attributes of subtract_grad";

    int axis = -1;

    VLOG(6) << "Vjp prepare call subtract's vjp inteface";

    std::vector<std::vector<Tensor>> tensor_res =
        primitive::subtract_vjp(
        x, y, out_grad, axis, stop_gradients);

    VLOG(6) << "Vjp prepare stop gradient of subtract_grad";

    std::vector<std::vector<pir::OpResult>> res(tensor_res.size());
    for (size_t i = 0; i < tensor_res.size(); ++i) {
        res[i].resize(tensor_res[i].size());
        for (size_t j = 0; j < tensor_res[i].size(); ++j) {
            if(tensor_res[i][j].defined()){
                res[i][j] = std::static_pointer_cast<primitive::LazyTensor>(tensor_res[i][j].impl())->value().dyn_cast<pir::OpResult>();
            }
        }
    }
    return res;
}

std::vector<std::vector<pir::OpResult>> Subtract_Op::Vjp(pir::Operation* op, const std::vector<std::vector<pir::Value>>& inputs_, const std::vector<std::vector<pir::Value>>& outputs, const std::vector<std::vector<pir::Value>>& out_grads, const std::vector<std::vector<bool>>& stop_gradients){

    PADDLE_ENFORCE_EQ(
      inputs_.size(),
      2,
      platform::errors::InvalidArgument("subtract op's inputs size should be 2, but now is %d.", inputs_.size()));
    PADDLE_ENFORCE_EQ(
      outputs.size(),
      1,
      platform::errors::InvalidArgument("subtract op's outputs size should be 1, but now is %d.", outputs.size()));

    VLOG(6) << "Prepare inputs of subtract_grad";

    Tensor x(std::make_shared<primitive::LazyTensor>(inputs_[0][0]));
    Tensor y(std::make_shared<primitive::LazyTensor>(inputs_[1][0]));
    Tensor out_grad(std::make_shared<primitive::LazyTensor>(out_grads[0][0]));

    VLOG(6) << "Vjp prepare Prepare attributes of subtract_grad";

    int axis = -1;

    VLOG(6) << "Vjp prepare call subtract_'s vjp inteface";

    std::vector<std::vector<Tensor>> tensor_res =
        primitive::subtract_vjp(
        x, y, out_grad, axis, stop_gradients);

    VLOG(6) << "Vjp prepare stop gradient of subtract_grad";

    std::vector<std::vector<pir::OpResult>> res(tensor_res.size());
    for (size_t i = 0; i < tensor_res.size(); ++i) {
        res[i].resize(tensor_res[i].size());
        for (size_t j = 0; j < tensor_res[i].size(); ++j) {
            if(tensor_res[i][j].defined()){
                res[i][j] = std::static_pointer_cast<primitive::LazyTensor>(tensor_res[i][j].impl())->value().dyn_cast<pir::OpResult>();
            }
        }
    }
    return res;
}

std::vector<std::vector<pir::OpResult>> SumOp::Vjp(pir::Operation* op, const std::vector<std::vector<pir::Value>>& inputs_, const std::vector<std::vector<pir::Value>>& outputs, const std::vector<std::vector<pir::Value>>& out_grads, const std::vector<std::vector<bool>>& stop_gradients){

    PADDLE_ENFORCE_EQ(
      inputs_.size(),
      2,
      platform::errors::InvalidArgument("sum op's inputs size should be 2, but now is %d.", inputs_.size()));
    PADDLE_ENFORCE_EQ(
      outputs.size(),
      1,
      platform::errors::InvalidArgument("sum op's outputs size should be 1, but now is %d.", outputs.size()));

    VLOG(6) << "Prepare inputs of sum_grad";

    Tensor x(std::make_shared<primitive::LazyTensor>(inputs_[0][0]));
    Tensor out_grad(std::make_shared<primitive::LazyTensor>(out_grads[0][0]));

    VLOG(6) << "Vjp prepare Prepare attributes of sum_grad";

    Tensor axis(std::make_shared<primitive::LazyTensor>(inputs_[1][0]));
    bool keepdim = op->attribute("keepdim").dyn_cast<pir::BoolAttribute>().data();
    bool reduce_all = false;

    VLOG(6) << "Vjp prepare call sum's vjp inteface";

    std::vector<std::vector<Tensor>> tensor_res =
        primitive::sum_vjp(
        x, out_grad, axis, keepdim, reduce_all, stop_gradients);

    VLOG(6) << "Vjp prepare stop gradient of sum_grad";

    std::vector<std::vector<pir::OpResult>> res(tensor_res.size());
    for (size_t i = 0; i < tensor_res.size(); ++i) {
        res[i].resize(tensor_res[i].size());
        for (size_t j = 0; j < tensor_res[i].size(); ++j) {
            if(tensor_res[i][j].defined()){
                res[i][j] = std::static_pointer_cast<primitive::LazyTensor>(tensor_res[i][j].impl())->value().dyn_cast<pir::OpResult>();
            }
        }
    }
    return res;
}

std::vector<std::vector<pir::OpResult>> SwishOp::Vjp(pir::Operation* op, const std::vector<std::vector<pir::Value>>& inputs_, const std::vector<std::vector<pir::Value>>& outputs, const std::vector<std::vector<pir::Value>>& out_grads, const std::vector<std::vector<bool>>& stop_gradients){

    PADDLE_ENFORCE_EQ(
      inputs_.size(),
      1,
      platform::errors::InvalidArgument("swish op's inputs size should be 1, but now is %d.", inputs_.size()));
    PADDLE_ENFORCE_EQ(
      outputs.size(),
      1,
      platform::errors::InvalidArgument("swish op's outputs size should be 1, but now is %d.", outputs.size()));

    VLOG(6) << "Prepare inputs of swish_grad";

    Tensor x(std::make_shared<primitive::LazyTensor>(inputs_[0][0]));
    Tensor out_grad(std::make_shared<primitive::LazyTensor>(out_grads[0][0]));

    VLOG(6) << "Vjp prepare Prepare attributes of swish_grad";


    VLOG(6) << "Vjp prepare call swish's vjp inteface";

    std::vector<std::vector<Tensor>> tensor_res =
        primitive::swish_vjp(
        x, out_grad, stop_gradients);

    VLOG(6) << "Vjp prepare stop gradient of swish_grad";

    std::vector<std::vector<pir::OpResult>> res(tensor_res.size());
    for (size_t i = 0; i < tensor_res.size(); ++i) {
        res[i].resize(tensor_res[i].size());
        for (size_t j = 0; j < tensor_res[i].size(); ++j) {
            if(tensor_res[i][j].defined()){
                res[i][j] = std::static_pointer_cast<primitive::LazyTensor>(tensor_res[i][j].impl())->value().dyn_cast<pir::OpResult>();
            }
        }
    }
    return res;
}

std::vector<std::vector<pir::OpResult>> SyncBatchNorm_Op::Vjp(pir::Operation* op, const std::vector<std::vector<pir::Value>>& inputs_, const std::vector<std::vector<pir::Value>>& outputs, const std::vector<std::vector<pir::Value>>& out_grads, const std::vector<std::vector<bool>>& stop_gradients){

    PADDLE_ENFORCE_EQ(
      inputs_.size(),
      5,
      platform::errors::InvalidArgument("sync_batch_norm_ op's inputs size should be 5, but now is %d.", inputs_.size()));
    PADDLE_ENFORCE_EQ(
      outputs.size(),
      6,
      platform::errors::InvalidArgument("sync_batch_norm_ op's outputs size should be 6, but now is %d.", outputs.size()));

    VLOG(6) << "Prepare inputs of sync_batch_norm_grad";

    Tensor x(std::make_shared<primitive::LazyTensor>(inputs_[0][0]));
    Tensor scale(std::make_shared<primitive::LazyTensor>(inputs_[3][0]));
    Tensor bias(std::make_shared<primitive::LazyTensor>(inputs_[4][0]));
    Tensor saved_mean(std::make_shared<primitive::LazyTensor>(outputs[3][0]));
    Tensor saved_variance(std::make_shared<primitive::LazyTensor>(outputs[4][0]));
    paddle::optional<Tensor> reserve_space;
    if (!IsEmptyValue(outputs[5][0])){
        reserve_space = paddle::make_optional<Tensor>(Tensor(std::make_shared<primitive::LazyTensor>(outputs[5][0])));
    }
    Tensor out_grad(std::make_shared<primitive::LazyTensor>(out_grads[0][0]));

    VLOG(6) << "Vjp prepare Prepare attributes of sync_batch_norm_grad";

    float momentum = op->attribute("momentum").dyn_cast<pir::FloatAttribute>().data();
    float epsilon = op->attribute("epsilon").dyn_cast<pir::FloatAttribute>().data();
    std::string data_layout = op->attribute("data_layout").dyn_cast<pir::StrAttribute>().AsString();
    bool is_test = op->attribute("is_test").dyn_cast<pir::BoolAttribute>().data();
    bool use_global_stats = op->attribute("use_global_stats").dyn_cast<pir::BoolAttribute>().data();
    bool trainable_statistics = op->attribute("trainable_statistics").dyn_cast<pir::BoolAttribute>().data();

    VLOG(6) << "Vjp prepare call sync_batch_norm_'s vjp inteface";

    std::vector<std::vector<Tensor>> tensor_res =
        primitive::sync_batch_norm__vjp(
        x, scale, bias, saved_mean, saved_variance, reserve_space, out_grad, momentum, epsilon, data_layout, is_test, use_global_stats, trainable_statistics, stop_gradients);

    VLOG(6) << "Vjp prepare stop gradient of sync_batch_norm_grad";

    std::vector<std::vector<pir::OpResult>> res(tensor_res.size());
    for (size_t i = 0; i < tensor_res.size(); ++i) {
        res[i].resize(tensor_res[i].size());
        for (size_t j = 0; j < tensor_res[i].size(); ++j) {
            if(tensor_res[i][j].defined()){
                res[i][j] = std::static_pointer_cast<primitive::LazyTensor>(tensor_res[i][j].impl())->value().dyn_cast<pir::OpResult>();
            }
        }
    }
    return res;
}

std::vector<std::vector<pir::OpResult>> TileOp::Vjp(pir::Operation* op, const std::vector<std::vector<pir::Value>>& inputs_, const std::vector<std::vector<pir::Value>>& outputs, const std::vector<std::vector<pir::Value>>& out_grads, const std::vector<std::vector<bool>>& stop_gradients){

    PADDLE_ENFORCE_EQ(
      inputs_.size(),
      2,
      platform::errors::InvalidArgument("tile op's inputs size should be 2, but now is %d.", inputs_.size()));
    PADDLE_ENFORCE_EQ(
      outputs.size(),
      1,
      platform::errors::InvalidArgument("tile op's outputs size should be 1, but now is %d.", outputs.size()));

    VLOG(6) << "Prepare inputs of tile_grad";

    Tensor x(std::make_shared<primitive::LazyTensor>(inputs_[0][0]));
    Tensor out_grad(std::make_shared<primitive::LazyTensor>(out_grads[0][0]));

    VLOG(6) << "Vjp prepare Prepare attributes of tile_grad";

    Tensor repeat_times(std::make_shared<primitive::LazyTensor>(inputs_[1][0]));

    VLOG(6) << "Vjp prepare call tile's vjp inteface";

    std::vector<std::vector<Tensor>> tensor_res =
        primitive::tile_vjp(
        x, out_grad, repeat_times, stop_gradients);

    VLOG(6) << "Vjp prepare stop gradient of tile_grad";

    std::vector<std::vector<pir::OpResult>> res(tensor_res.size());
    for (size_t i = 0; i < tensor_res.size(); ++i) {
        res[i].resize(tensor_res[i].size());
        for (size_t j = 0; j < tensor_res[i].size(); ++j) {
            if(tensor_res[i][j].defined()){
                res[i][j] = std::static_pointer_cast<primitive::LazyTensor>(tensor_res[i][j].impl())->value().dyn_cast<pir::OpResult>();
            }
        }
    }
    return res;
}

std::vector<std::vector<pir::OpResult>> TransLayoutOp::Vjp(pir::Operation* op, const std::vector<std::vector<pir::Value>>& inputs_, const std::vector<std::vector<pir::Value>>& outputs, const std::vector<std::vector<pir::Value>>& out_grads, const std::vector<std::vector<bool>>& stop_gradients){

    PADDLE_ENFORCE_EQ(
      inputs_.size(),
      1,
      platform::errors::InvalidArgument("trans_layout op's inputs size should be 1, but now is %d.", inputs_.size()));
    PADDLE_ENFORCE_EQ(
      outputs.size(),
      1,
      platform::errors::InvalidArgument("trans_layout op's outputs size should be 1, but now is %d.", outputs.size()));

    VLOG(6) << "Prepare inputs of trans_layout_grad";

    Tensor x(std::make_shared<primitive::LazyTensor>(inputs_[0][0]));
    Tensor out_grad(std::make_shared<primitive::LazyTensor>(out_grads[0][0]));

    VLOG(6) << "Vjp prepare Prepare attributes of trans_layout_grad";

    std::vector<int> perm;
    for (size_t i = 0; i < op->attribute("perm").dyn_cast<pir::ArrayAttribute>().size(); i++) {
        perm.push_back(op->attribute("perm").dyn_cast<pir::ArrayAttribute>().at(i).dyn_cast<pir::Int32Attribute>().data());
    }

    VLOG(6) << "Vjp prepare call trans_layout's vjp inteface";

    std::vector<std::vector<Tensor>> tensor_res =
        primitive::trans_layout_vjp(
        x, out_grad, perm, stop_gradients);

    VLOG(6) << "Vjp prepare stop gradient of trans_layout_grad";

    std::vector<std::vector<pir::OpResult>> res(tensor_res.size());
    for (size_t i = 0; i < tensor_res.size(); ++i) {
        res[i].resize(tensor_res[i].size());
        for (size_t j = 0; j < tensor_res[i].size(); ++j) {
            if(tensor_res[i][j].defined()){
                res[i][j] = std::static_pointer_cast<primitive::LazyTensor>(tensor_res[i][j].impl())->value().dyn_cast<pir::OpResult>();
            }
        }
    }
    return res;
}

std::vector<std::vector<pir::OpResult>> TransposeOp::Vjp(pir::Operation* op, const std::vector<std::vector<pir::Value>>& inputs_, const std::vector<std::vector<pir::Value>>& outputs, const std::vector<std::vector<pir::Value>>& out_grads, const std::vector<std::vector<bool>>& stop_gradients){

    PADDLE_ENFORCE_EQ(
      inputs_.size(),
      1,
      platform::errors::InvalidArgument("transpose op's inputs size should be 1, but now is %d.", inputs_.size()));
    PADDLE_ENFORCE_EQ(
      outputs.size(),
      1,
      platform::errors::InvalidArgument("transpose op's outputs size should be 1, but now is %d.", outputs.size()));

    VLOG(6) << "Prepare inputs of transpose_grad";

    Tensor out_grad(std::make_shared<primitive::LazyTensor>(out_grads[0][0]));

    VLOG(6) << "Vjp prepare Prepare attributes of transpose_grad";

    std::vector<int> perm;
    for (size_t i = 0; i < op->attribute("perm").dyn_cast<pir::ArrayAttribute>().size(); i++) {
        perm.push_back(op->attribute("perm").dyn_cast<pir::ArrayAttribute>().at(i).dyn_cast<pir::Int32Attribute>().data());
    }

    VLOG(6) << "Vjp prepare call transpose's vjp inteface";

    std::vector<std::vector<Tensor>> tensor_res =
        primitive::transpose_vjp(
        out_grad, perm, stop_gradients);

    VLOG(6) << "Vjp prepare stop gradient of transpose_grad";

    std::vector<std::vector<pir::OpResult>> res(tensor_res.size());
    for (size_t i = 0; i < tensor_res.size(); ++i) {
        res[i].resize(tensor_res[i].size());
        for (size_t j = 0; j < tensor_res[i].size(); ++j) {
            if(tensor_res[i][j].defined()){
                res[i][j] = std::static_pointer_cast<primitive::LazyTensor>(tensor_res[i][j].impl())->value().dyn_cast<pir::OpResult>();
            }
        }
    }
    return res;
}

std::vector<std::vector<pir::OpResult>> Transpose_Op::Vjp(pir::Operation* op, const std::vector<std::vector<pir::Value>>& inputs_, const std::vector<std::vector<pir::Value>>& outputs, const std::vector<std::vector<pir::Value>>& out_grads, const std::vector<std::vector<bool>>& stop_gradients){

    PADDLE_ENFORCE_EQ(
      inputs_.size(),
      1,
      platform::errors::InvalidArgument("transpose op's inputs size should be 1, but now is %d.", inputs_.size()));
    PADDLE_ENFORCE_EQ(
      outputs.size(),
      1,
      platform::errors::InvalidArgument("transpose op's outputs size should be 1, but now is %d.", outputs.size()));

    VLOG(6) << "Prepare inputs of transpose_grad";

    Tensor out_grad(std::make_shared<primitive::LazyTensor>(out_grads[0][0]));

    VLOG(6) << "Vjp prepare Prepare attributes of transpose_grad";

    std::vector<int> perm;
    for (size_t i = 0; i < op->attribute("perm").dyn_cast<pir::ArrayAttribute>().size(); i++) {
        perm.push_back(op->attribute("perm").dyn_cast<pir::ArrayAttribute>().at(i).dyn_cast<pir::Int32Attribute>().data());
    }

    VLOG(6) << "Vjp prepare call transpose_'s vjp inteface";

    std::vector<std::vector<Tensor>> tensor_res =
        primitive::transpose_vjp(
        out_grad, perm, stop_gradients);

    VLOG(6) << "Vjp prepare stop gradient of transpose_grad";

    std::vector<std::vector<pir::OpResult>> res(tensor_res.size());
    for (size_t i = 0; i < tensor_res.size(); ++i) {
        res[i].resize(tensor_res[i].size());
        for (size_t j = 0; j < tensor_res[i].size(); ++j) {
            if(tensor_res[i][j].defined()){
                res[i][j] = std::static_pointer_cast<primitive::LazyTensor>(tensor_res[i][j].impl())->value().dyn_cast<pir::OpResult>();
            }
        }
    }
    return res;
}

std::vector<std::vector<pir::OpResult>> TrilOp::Vjp(pir::Operation* op, const std::vector<std::vector<pir::Value>>& inputs_, const std::vector<std::vector<pir::Value>>& outputs, const std::vector<std::vector<pir::Value>>& out_grads, const std::vector<std::vector<bool>>& stop_gradients){

    PADDLE_ENFORCE_EQ(
      inputs_.size(),
      1,
      platform::errors::InvalidArgument("tril op's inputs size should be 1, but now is %d.", inputs_.size()));
    PADDLE_ENFORCE_EQ(
      outputs.size(),
      1,
      platform::errors::InvalidArgument("tril op's outputs size should be 1, but now is %d.", outputs.size()));

    VLOG(6) << "Prepare inputs of tril_grad";

    Tensor out_grad(std::make_shared<primitive::LazyTensor>(out_grads[0][0]));

    VLOG(6) << "Vjp prepare Prepare attributes of tril_grad";

    int diagonal = op->attribute("diagonal").dyn_cast<pir::Int32Attribute>().data();

    VLOG(6) << "Vjp prepare call tril's vjp inteface";

    std::vector<std::vector<Tensor>> tensor_res =
        primitive::tril_vjp(
        out_grad, diagonal, stop_gradients);

    VLOG(6) << "Vjp prepare stop gradient of tril_grad";

    std::vector<std::vector<pir::OpResult>> res(tensor_res.size());
    for (size_t i = 0; i < tensor_res.size(); ++i) {
        res[i].resize(tensor_res[i].size());
        for (size_t j = 0; j < tensor_res[i].size(); ++j) {
            if(tensor_res[i][j].defined()){
                res[i][j] = std::static_pointer_cast<primitive::LazyTensor>(tensor_res[i][j].impl())->value().dyn_cast<pir::OpResult>();
            }
        }
    }
    return res;
}

std::vector<std::vector<pir::OpResult>> Tril_Op::Vjp(pir::Operation* op, const std::vector<std::vector<pir::Value>>& inputs_, const std::vector<std::vector<pir::Value>>& outputs, const std::vector<std::vector<pir::Value>>& out_grads, const std::vector<std::vector<bool>>& stop_gradients){

    PADDLE_ENFORCE_EQ(
      inputs_.size(),
      1,
      platform::errors::InvalidArgument("tril op's inputs size should be 1, but now is %d.", inputs_.size()));
    PADDLE_ENFORCE_EQ(
      outputs.size(),
      1,
      platform::errors::InvalidArgument("tril op's outputs size should be 1, but now is %d.", outputs.size()));

    VLOG(6) << "Prepare inputs of tril_grad";

    Tensor out_grad(std::make_shared<primitive::LazyTensor>(out_grads[0][0]));

    VLOG(6) << "Vjp prepare Prepare attributes of tril_grad";

    int diagonal = op->attribute("diagonal").dyn_cast<pir::Int32Attribute>().data();

    VLOG(6) << "Vjp prepare call tril_'s vjp inteface";

    std::vector<std::vector<Tensor>> tensor_res =
        primitive::tril_vjp(
        out_grad, diagonal, stop_gradients);

    VLOG(6) << "Vjp prepare stop gradient of tril_grad";

    std::vector<std::vector<pir::OpResult>> res(tensor_res.size());
    for (size_t i = 0; i < tensor_res.size(); ++i) {
        res[i].resize(tensor_res[i].size());
        for (size_t j = 0; j < tensor_res[i].size(); ++j) {
            if(tensor_res[i][j].defined()){
                res[i][j] = std::static_pointer_cast<primitive::LazyTensor>(tensor_res[i][j].impl())->value().dyn_cast<pir::OpResult>();
            }
        }
    }
    return res;
}

std::vector<std::vector<pir::OpResult>> TriuOp::Vjp(pir::Operation* op, const std::vector<std::vector<pir::Value>>& inputs_, const std::vector<std::vector<pir::Value>>& outputs, const std::vector<std::vector<pir::Value>>& out_grads, const std::vector<std::vector<bool>>& stop_gradients){

    PADDLE_ENFORCE_EQ(
      inputs_.size(),
      1,
      platform::errors::InvalidArgument("triu op's inputs size should be 1, but now is %d.", inputs_.size()));
    PADDLE_ENFORCE_EQ(
      outputs.size(),
      1,
      platform::errors::InvalidArgument("triu op's outputs size should be 1, but now is %d.", outputs.size()));

    VLOG(6) << "Prepare inputs of triu_grad";

    Tensor out_grad(std::make_shared<primitive::LazyTensor>(out_grads[0][0]));

    VLOG(6) << "Vjp prepare Prepare attributes of triu_grad";

    int diagonal = op->attribute("diagonal").dyn_cast<pir::Int32Attribute>().data();

    VLOG(6) << "Vjp prepare call triu's vjp inteface";

    std::vector<std::vector<Tensor>> tensor_res =
        primitive::triu_vjp(
        out_grad, diagonal, stop_gradients);

    VLOG(6) << "Vjp prepare stop gradient of triu_grad";

    std::vector<std::vector<pir::OpResult>> res(tensor_res.size());
    for (size_t i = 0; i < tensor_res.size(); ++i) {
        res[i].resize(tensor_res[i].size());
        for (size_t j = 0; j < tensor_res[i].size(); ++j) {
            if(tensor_res[i][j].defined()){
                res[i][j] = std::static_pointer_cast<primitive::LazyTensor>(tensor_res[i][j].impl())->value().dyn_cast<pir::OpResult>();
            }
        }
    }
    return res;
}

std::vector<std::vector<pir::OpResult>> Triu_Op::Vjp(pir::Operation* op, const std::vector<std::vector<pir::Value>>& inputs_, const std::vector<std::vector<pir::Value>>& outputs, const std::vector<std::vector<pir::Value>>& out_grads, const std::vector<std::vector<bool>>& stop_gradients){

    PADDLE_ENFORCE_EQ(
      inputs_.size(),
      1,
      platform::errors::InvalidArgument("triu op's inputs size should be 1, but now is %d.", inputs_.size()));
    PADDLE_ENFORCE_EQ(
      outputs.size(),
      1,
      platform::errors::InvalidArgument("triu op's outputs size should be 1, but now is %d.", outputs.size()));

    VLOG(6) << "Prepare inputs of triu_grad";

    Tensor out_grad(std::make_shared<primitive::LazyTensor>(out_grads[0][0]));

    VLOG(6) << "Vjp prepare Prepare attributes of triu_grad";

    int diagonal = op->attribute("diagonal").dyn_cast<pir::Int32Attribute>().data();

    VLOG(6) << "Vjp prepare call triu_'s vjp inteface";

    std::vector<std::vector<Tensor>> tensor_res =
        primitive::triu_vjp(
        out_grad, diagonal, stop_gradients);

    VLOG(6) << "Vjp prepare stop gradient of triu_grad";

    std::vector<std::vector<pir::OpResult>> res(tensor_res.size());
    for (size_t i = 0; i < tensor_res.size(); ++i) {
        res[i].resize(tensor_res[i].size());
        for (size_t j = 0; j < tensor_res[i].size(); ++j) {
            if(tensor_res[i][j].defined()){
                res[i][j] = std::static_pointer_cast<primitive::LazyTensor>(tensor_res[i][j].impl())->value().dyn_cast<pir::OpResult>();
            }
        }
    }
    return res;
}

std::vector<std::vector<pir::OpResult>> UnpoolOp::Vjp(pir::Operation* op, const std::vector<std::vector<pir::Value>>& inputs_, const std::vector<std::vector<pir::Value>>& outputs, const std::vector<std::vector<pir::Value>>& out_grads, const std::vector<std::vector<bool>>& stop_gradients){

    PADDLE_ENFORCE_EQ(
      inputs_.size(),
      3,
      platform::errors::InvalidArgument("unpool op's inputs size should be 3, but now is %d.", inputs_.size()));
    PADDLE_ENFORCE_EQ(
      outputs.size(),
      1,
      platform::errors::InvalidArgument("unpool op's outputs size should be 1, but now is %d.", outputs.size()));

    VLOG(6) << "Prepare inputs of unpool_grad";

    Tensor x(std::make_shared<primitive::LazyTensor>(inputs_[0][0]));
    Tensor indices(std::make_shared<primitive::LazyTensor>(inputs_[1][0]));
    Tensor out(std::make_shared<primitive::LazyTensor>(outputs[0][0]));
    Tensor out_grad(std::make_shared<primitive::LazyTensor>(out_grads[0][0]));

    VLOG(6) << "Vjp prepare Prepare attributes of unpool_grad";

    std::vector<int> ksize;
    for (size_t i = 0; i < op->attribute("ksize").dyn_cast<pir::ArrayAttribute>().size(); i++) {
        ksize.push_back(op->attribute("ksize").dyn_cast<pir::ArrayAttribute>().at(i).dyn_cast<pir::Int32Attribute>().data());
    }
    std::vector<int> strides;
    for (size_t i = 0; i < op->attribute("strides").dyn_cast<pir::ArrayAttribute>().size(); i++) {
        strides.push_back(op->attribute("strides").dyn_cast<pir::ArrayAttribute>().at(i).dyn_cast<pir::Int32Attribute>().data());
    }
    std::vector<int> padding;
    for (size_t i = 0; i < op->attribute("padding").dyn_cast<pir::ArrayAttribute>().size(); i++) {
        padding.push_back(op->attribute("padding").dyn_cast<pir::ArrayAttribute>().at(i).dyn_cast<pir::Int32Attribute>().data());
    }
    Tensor output_size(std::make_shared<primitive::LazyTensor>(inputs_[2][0]));
    std::string data_format = op->attribute("data_format").dyn_cast<pir::StrAttribute>().AsString();

    VLOG(6) << "Vjp prepare call unpool's vjp inteface";

    std::vector<std::vector<Tensor>> tensor_res =
        primitive::unpool_vjp(
        x, indices, out, out_grad, output_size, ksize, strides, padding, data_format, stop_gradients);

    VLOG(6) << "Vjp prepare stop gradient of unpool_grad";

    std::vector<std::vector<pir::OpResult>> res(tensor_res.size());
    for (size_t i = 0; i < tensor_res.size(); ++i) {
        res[i].resize(tensor_res[i].size());
        for (size_t j = 0; j < tensor_res[i].size(); ++j) {
            if(tensor_res[i][j].defined()){
                res[i][j] = std::static_pointer_cast<primitive::LazyTensor>(tensor_res[i][j].impl())->value().dyn_cast<pir::OpResult>();
            }
        }
    }
    return res;
}

std::vector<std::vector<pir::OpResult>> CSoftmaxWithCrossEntropyOp::Vjp(pir::Operation* op, const std::vector<std::vector<pir::Value>>& inputs_, const std::vector<std::vector<pir::Value>>& outputs, const std::vector<std::vector<pir::Value>>& out_grads, const std::vector<std::vector<bool>>& stop_gradients){

    PADDLE_ENFORCE_EQ(
      inputs_.size(),
      2,
      platform::errors::InvalidArgument("c_softmax_with_cross_entropy op's inputs size should be 2, but now is %d.", inputs_.size()));
    PADDLE_ENFORCE_EQ(
      outputs.size(),
      2,
      platform::errors::InvalidArgument("c_softmax_with_cross_entropy op's outputs size should be 2, but now is %d.", outputs.size()));

    VLOG(6) << "Prepare inputs of c_softmax_with_cross_entropy_grad";

    Tensor softmax(std::make_shared<primitive::LazyTensor>(outputs[0][0]));
    Tensor label(std::make_shared<primitive::LazyTensor>(inputs_[1][0]));
    Tensor loss_grad(std::make_shared<primitive::LazyTensor>(out_grads[1][0]));

    VLOG(6) << "Vjp prepare Prepare attributes of c_softmax_with_cross_entropy_grad";

    int64_t ignore_index = op->attribute("ignore_index").dyn_cast<pir::Int64Attribute>().data();
    int ring_id = op->attribute("ring_id").dyn_cast<pir::Int32Attribute>().data();
    int rank = op->attribute("rank").dyn_cast<pir::Int32Attribute>().data();
    int nranks = op->attribute("nranks").dyn_cast<pir::Int32Attribute>().data();

    VLOG(6) << "Vjp prepare call c_softmax_with_cross_entropy's vjp inteface";

    std::vector<std::vector<Tensor>> tensor_res =
        primitive::c_softmax_with_cross_entropy_vjp(
        softmax, label, loss_grad, ignore_index, ring_id, rank, nranks, stop_gradients);

    VLOG(6) << "Vjp prepare stop gradient of c_softmax_with_cross_entropy_grad";

    std::vector<std::vector<pir::OpResult>> res(tensor_res.size());
    for (size_t i = 0; i < tensor_res.size(); ++i) {
        res[i].resize(tensor_res[i].size());
        for (size_t j = 0; j < tensor_res[i].size(); ++j) {
            if(tensor_res[i][j].defined()){
                res[i][j] = std::static_pointer_cast<primitive::LazyTensor>(tensor_res[i][j].impl())->value().dyn_cast<pir::OpResult>();
            }
        }
    }
    return res;
}

std::vector<std::vector<pir::OpResult>> FusedAttentionOp::Vjp(pir::Operation* op, const std::vector<std::vector<pir::Value>>& inputs_, const std::vector<std::vector<pir::Value>>& outputs, const std::vector<std::vector<pir::Value>>& out_grads, const std::vector<std::vector<bool>>& stop_gradients){

    PADDLE_ENFORCE_EQ(
      inputs_.size(),
      11,
      platform::errors::InvalidArgument("fused_attention op's inputs size should be 11, but now is %d.", inputs_.size()));
    PADDLE_ENFORCE_EQ(
      outputs.size(),
      20,
      platform::errors::InvalidArgument("fused_attention op's outputs size should be 20, but now is %d.", outputs.size()));

    VLOG(6) << "Prepare inputs of fused_attention_grad";

    Tensor out_grad(std::make_shared<primitive::LazyTensor>(out_grads[19][0]));
    Tensor x(std::make_shared<primitive::LazyTensor>(inputs_[0][0]));
    Tensor qkv_weight(std::make_shared<primitive::LazyTensor>(inputs_[3][0]));
    paddle::optional<Tensor> qkv_bias;
    if (!IsEmptyValue(inputs_[4][0])){
        qkv_bias = paddle::make_optional<Tensor>(Tensor(std::make_shared<primitive::LazyTensor>(inputs_[4][0])));
    }
    paddle::optional<Tensor> qkv_bias_out;
    if (!IsEmptyValue(outputs[4][0])){
        qkv_bias_out = paddle::make_optional<Tensor>(Tensor(std::make_shared<primitive::LazyTensor>(outputs[4][0])));
    }
    paddle::optional<Tensor> src_mask;
    if (!IsEmptyValue(inputs_[6][0])){
        src_mask = paddle::make_optional<Tensor>(Tensor(std::make_shared<primitive::LazyTensor>(inputs_[6][0])));
    }
    paddle::optional<Tensor> src_mask_out;
    if (!IsEmptyValue(outputs[11][0])){
        src_mask_out = paddle::make_optional<Tensor>(Tensor(std::make_shared<primitive::LazyTensor>(outputs[11][0])));
    }
    Tensor out_linear_weight(std::make_shared<primitive::LazyTensor>(inputs_[7][0]));
    paddle::optional<Tensor> out_linear_bias;
    if (!IsEmptyValue(inputs_[8][0])){
        out_linear_bias = paddle::make_optional<Tensor>(Tensor(std::make_shared<primitive::LazyTensor>(inputs_[8][0])));
    }
    paddle::optional<Tensor> ln_scale;
    if (!IsEmptyValue(inputs_[1][0])){
        ln_scale = paddle::make_optional<Tensor>(Tensor(std::make_shared<primitive::LazyTensor>(inputs_[1][0])));
    }
    paddle::optional<Tensor> ln_bias;
    if (!IsEmptyValue(inputs_[2][0])){
        ln_bias = paddle::make_optional<Tensor>(Tensor(std::make_shared<primitive::LazyTensor>(inputs_[2][0])));
    }
    paddle::optional<Tensor> ln_scale_2;
    if (!IsEmptyValue(inputs_[9][0])){
        ln_scale_2 = paddle::make_optional<Tensor>(Tensor(std::make_shared<primitive::LazyTensor>(inputs_[9][0])));
    }
    paddle::optional<Tensor> ln_bias_2;
    if (!IsEmptyValue(inputs_[10][0])){
        ln_bias_2 = paddle::make_optional<Tensor>(Tensor(std::make_shared<primitive::LazyTensor>(inputs_[10][0])));
    }
    paddle::optional<Tensor> ln_out;
    if (!IsEmptyValue(outputs[2][0])){
        ln_out = paddle::make_optional<Tensor>(Tensor(std::make_shared<primitive::LazyTensor>(outputs[2][0])));
    }
    paddle::optional<Tensor> ln_mean;
    if (!IsEmptyValue(outputs[0][0])){
        ln_mean = paddle::make_optional<Tensor>(Tensor(std::make_shared<primitive::LazyTensor>(outputs[0][0])));
    }
    paddle::optional<Tensor> ln_var;
    if (!IsEmptyValue(outputs[1][0])){
        ln_var = paddle::make_optional<Tensor>(Tensor(std::make_shared<primitive::LazyTensor>(outputs[1][0])));
    }
    paddle::optional<Tensor> ln_mean_2;
    if (!IsEmptyValue(outputs[15][0])){
        ln_mean_2 = paddle::make_optional<Tensor>(Tensor(std::make_shared<primitive::LazyTensor>(outputs[15][0])));
    }
    paddle::optional<Tensor> ln_var_2;
    if (!IsEmptyValue(outputs[16][0])){
        ln_var_2 = paddle::make_optional<Tensor>(Tensor(std::make_shared<primitive::LazyTensor>(outputs[16][0])));
    }
    paddle::optional<Tensor> bias_dropout_residual_out;
    if (!IsEmptyValue(outputs[17][0])){
        bias_dropout_residual_out = paddle::make_optional<Tensor>(Tensor(std::make_shared<primitive::LazyTensor>(outputs[17][0])));
    }
    Tensor qkv_out(std::make_shared<primitive::LazyTensor>(outputs[3][0]));
    Tensor transpose_out_2(std::make_shared<primitive::LazyTensor>(outputs[5][0]));
    Tensor qk_out(std::make_shared<primitive::LazyTensor>(outputs[6][0]));
    Tensor qktv_out(std::make_shared<primitive::LazyTensor>(outputs[7][0]));
    Tensor softmax_out(std::make_shared<primitive::LazyTensor>(outputs[8][0]));
    Tensor attn_dropout_mask_out(std::make_shared<primitive::LazyTensor>(outputs[9][0]));
    Tensor attn_dropout_out(std::make_shared<primitive::LazyTensor>(outputs[10][0]));
    Tensor fmha_out(std::make_shared<primitive::LazyTensor>(outputs[12][0]));
    Tensor out_linear_out(std::make_shared<primitive::LazyTensor>(outputs[13][0]));
    Tensor dropout_mask_out(std::make_shared<primitive::LazyTensor>(outputs[14][0]));

    VLOG(6) << "Vjp prepare Prepare attributes of fused_attention_grad";

    int num_heads = op->attribute("num_heads").dyn_cast<pir::Int32Attribute>().data();
    bool transpose_qkv_wb = op->attribute("transpose_qkv_wb").dyn_cast<pir::BoolAttribute>().data();
    bool pre_layer_norm = op->attribute("pre_layer_norm").dyn_cast<pir::BoolAttribute>().data();
    float epsilon = op->attribute("epsilon").dyn_cast<pir::FloatAttribute>().data();
    float attn_dropout_rate = op->attribute("attn_dropout_rate").dyn_cast<pir::FloatAttribute>().data();
    bool is_test = op->attribute("is_test").dyn_cast<pir::BoolAttribute>().data();
    bool attn_dropout_fix_seed = op->attribute("attn_dropout_fix_seed").dyn_cast<pir::BoolAttribute>().data();
    int attn_dropout_seed = op->attribute("attn_dropout_seed").dyn_cast<pir::Int32Attribute>().data();
    std::string attn_dropout_implementation = op->attribute("attn_dropout_implementation").dyn_cast<pir::StrAttribute>().AsString();
    float dropout_rate = op->attribute("dropout_rate").dyn_cast<pir::FloatAttribute>().data();
    bool dropout_fix_seed = op->attribute("dropout_fix_seed").dyn_cast<pir::BoolAttribute>().data();
    int dropout_seed = op->attribute("dropout_seed").dyn_cast<pir::Int32Attribute>().data();
    std::string dropout_implementation = op->attribute("dropout_implementation").dyn_cast<pir::StrAttribute>().AsString();
    float ln_epsilon = op->attribute("ln_epsilon").dyn_cast<pir::FloatAttribute>().data();
    bool add_residual = op->attribute("add_residual").dyn_cast<pir::BoolAttribute>().data();
    int ring_id = op->attribute("ring_id").dyn_cast<pir::Int32Attribute>().data();

    VLOG(6) << "Vjp prepare call fused_attention's vjp inteface";

    std::vector<std::vector<Tensor>> tensor_res =
        primitive::fused_attention_vjp(
        out_grad, x, qkv_weight, qkv_bias, qkv_bias_out, src_mask, src_mask_out, out_linear_weight, out_linear_bias, ln_scale, ln_bias, ln_scale_2, ln_bias_2, ln_out, ln_mean, ln_var, ln_mean_2, ln_var_2, bias_dropout_residual_out, qkv_out, transpose_out_2, qk_out, qktv_out, softmax_out, attn_dropout_mask_out, attn_dropout_out, fmha_out, out_linear_out, dropout_mask_out, num_heads, transpose_qkv_wb, pre_layer_norm, epsilon, attn_dropout_rate, is_test, attn_dropout_fix_seed, attn_dropout_seed, attn_dropout_implementation, dropout_rate, dropout_fix_seed, dropout_seed, dropout_implementation, ln_epsilon, add_residual, ring_id, stop_gradients);

    VLOG(6) << "Vjp prepare stop gradient of fused_attention_grad";

    std::vector<std::vector<pir::OpResult>> res(tensor_res.size());
    for (size_t i = 0; i < tensor_res.size(); ++i) {
        res[i].resize(tensor_res[i].size());
        for (size_t j = 0; j < tensor_res[i].size(); ++j) {
            if(tensor_res[i][j].defined()){
                res[i][j] = std::static_pointer_cast<primitive::LazyTensor>(tensor_res[i][j].impl())->value().dyn_cast<pir::OpResult>();
            }
        }
    }
    return res;
}

std::vector<std::vector<pir::OpResult>> FusedElemwiseAddActivationOp::Vjp(pir::Operation* op, const std::vector<std::vector<pir::Value>>& inputs_, const std::vector<std::vector<pir::Value>>& outputs, const std::vector<std::vector<pir::Value>>& out_grads, const std::vector<std::vector<bool>>& stop_gradients){

    PADDLE_ENFORCE_EQ(
      inputs_.size(),
      2,
      platform::errors::InvalidArgument("fused_elemwise_add_activation op's inputs size should be 2, but now is %d.", inputs_.size()));
    PADDLE_ENFORCE_EQ(
      outputs.size(),
      2,
      platform::errors::InvalidArgument("fused_elemwise_add_activation op's outputs size should be 2, but now is %d.", outputs.size()));

    VLOG(6) << "Prepare inputs of fused_elemwise_add_activation_grad";

    paddle::optional<Tensor> x;
    if (!IsEmptyValue(inputs_[0][0])){
        x = paddle::make_optional<Tensor>(Tensor(std::make_shared<primitive::LazyTensor>(inputs_[0][0])));
    }
    Tensor y(std::make_shared<primitive::LazyTensor>(inputs_[1][0]));
    Tensor out(std::make_shared<primitive::LazyTensor>(outputs[0][0]));
    paddle::optional<Tensor> intermediate_out;
    if (!IsEmptyValue(outputs[1][0])){
        intermediate_out = paddle::make_optional<Tensor>(Tensor(std::make_shared<primitive::LazyTensor>(outputs[1][0])));
    }
    Tensor out_grad(std::make_shared<primitive::LazyTensor>(out_grads[0][0]));

    VLOG(6) << "Vjp prepare Prepare attributes of fused_elemwise_add_activation_grad";

    std::vector<std::string> functor_list;
    for (size_t i = 0; i < op->attribute("functor_list").dyn_cast<pir::ArrayAttribute>().size(); i++) {
        functor_list.push_back(op->attribute("functor_list").dyn_cast<pir::ArrayAttribute>().at(i).dyn_cast<pir::StrAttribute>().AsString());
    }
    float scale = op->attribute("scale").dyn_cast<pir::FloatAttribute>().data();
    int axis = op->attribute("axis").dyn_cast<pir::Int32Attribute>().data();
    bool save_intermediate_out = op->attribute("save_intermediate_out").dyn_cast<pir::BoolAttribute>().data();

    VLOG(6) << "Vjp prepare call fused_elemwise_add_activation's vjp inteface";

    std::vector<std::vector<Tensor>> tensor_res =
        primitive::fused_elemwise_add_activation_vjp(
        x, y, out, intermediate_out, out_grad, functor_list, scale, axis, save_intermediate_out, stop_gradients);

    VLOG(6) << "Vjp prepare stop gradient of fused_elemwise_add_activation_grad";

    std::vector<std::vector<pir::OpResult>> res(tensor_res.size());
    for (size_t i = 0; i < tensor_res.size(); ++i) {
        res[i].resize(tensor_res[i].size());
        for (size_t j = 0; j < tensor_res[i].size(); ++j) {
            if(tensor_res[i][j].defined()){
                res[i][j] = std::static_pointer_cast<primitive::LazyTensor>(tensor_res[i][j].impl())->value().dyn_cast<pir::OpResult>();
            }
        }
    }
    return res;
}

std::vector<std::vector<pir::OpResult>> FusedFeedforwardOp::Vjp(pir::Operation* op, const std::vector<std::vector<pir::Value>>& inputs_, const std::vector<std::vector<pir::Value>>& outputs, const std::vector<std::vector<pir::Value>>& out_grads, const std::vector<std::vector<bool>>& stop_gradients){

    PADDLE_ENFORCE_EQ(
      inputs_.size(),
      11,
      platform::errors::InvalidArgument("fused_feedforward op's inputs size should be 11, but now is %d.", inputs_.size()));
    PADDLE_ENFORCE_EQ(
      outputs.size(),
      11,
      platform::errors::InvalidArgument("fused_feedforward op's outputs size should be 11, but now is %d.", outputs.size()));

    VLOG(6) << "Prepare inputs of fused_feedforward_grad";

    Tensor out_grad(std::make_shared<primitive::LazyTensor>(out_grads[0][0]));
    Tensor x(std::make_shared<primitive::LazyTensor>(inputs_[0][0]));
    Tensor linear1_weight(std::make_shared<primitive::LazyTensor>(inputs_[3][0]));
    paddle::optional<Tensor> linear1_bias;
    if (!IsEmptyValue(inputs_[4][0])){
        linear1_bias = paddle::make_optional<Tensor>(Tensor(std::make_shared<primitive::LazyTensor>(inputs_[4][0])));
    }
    Tensor linear2_weight(std::make_shared<primitive::LazyTensor>(inputs_[5][0]));
    Tensor dropout1_mask(std::make_shared<primitive::LazyTensor>(outputs[1][0]));
    Tensor dropout2_mask(std::make_shared<primitive::LazyTensor>(outputs[2][0]));
    Tensor linear1_out(std::make_shared<primitive::LazyTensor>(outputs[7][0]));
    Tensor dropout1_out(std::make_shared<primitive::LazyTensor>(outputs[9][0]));
    paddle::optional<Tensor> dropout2_out;
    if (!IsEmptyValue(outputs[10][0])){
        dropout2_out = paddle::make_optional<Tensor>(Tensor(std::make_shared<primitive::LazyTensor>(outputs[10][0])));
    }
    paddle::optional<Tensor> ln1_scale;
    if (!IsEmptyValue(inputs_[7][0])){
        ln1_scale = paddle::make_optional<Tensor>(Tensor(std::make_shared<primitive::LazyTensor>(inputs_[7][0])));
    }
    paddle::optional<Tensor> ln1_bias;
    if (!IsEmptyValue(inputs_[8][0])){
        ln1_bias = paddle::make_optional<Tensor>(Tensor(std::make_shared<primitive::LazyTensor>(inputs_[8][0])));
    }
    paddle::optional<Tensor> ln1_out;
    if (!IsEmptyValue(outputs[8][0])){
        ln1_out = paddle::make_optional<Tensor>(Tensor(std::make_shared<primitive::LazyTensor>(outputs[8][0])));
    }
    paddle::optional<Tensor> ln1_mean;
    if (!IsEmptyValue(outputs[3][0])){
        ln1_mean = paddle::make_optional<Tensor>(Tensor(std::make_shared<primitive::LazyTensor>(outputs[3][0])));
    }
    paddle::optional<Tensor> ln1_variance;
    if (!IsEmptyValue(outputs[4][0])){
        ln1_variance = paddle::make_optional<Tensor>(Tensor(std::make_shared<primitive::LazyTensor>(outputs[4][0])));
    }
    paddle::optional<Tensor> ln2_scale;
    if (!IsEmptyValue(inputs_[9][0])){
        ln2_scale = paddle::make_optional<Tensor>(Tensor(std::make_shared<primitive::LazyTensor>(inputs_[9][0])));
    }
    paddle::optional<Tensor> ln2_bias;
    if (!IsEmptyValue(inputs_[10][0])){
        ln2_bias = paddle::make_optional<Tensor>(Tensor(std::make_shared<primitive::LazyTensor>(inputs_[10][0])));
    }
    paddle::optional<Tensor> ln2_mean;
    if (!IsEmptyValue(outputs[5][0])){
        ln2_mean = paddle::make_optional<Tensor>(Tensor(std::make_shared<primitive::LazyTensor>(outputs[5][0])));
    }
    paddle::optional<Tensor> ln2_variance;
    if (!IsEmptyValue(outputs[6][0])){
        ln2_variance = paddle::make_optional<Tensor>(Tensor(std::make_shared<primitive::LazyTensor>(outputs[6][0])));
    }
    paddle::optional<Tensor> linear2_bias;
    if (!IsEmptyValue(inputs_[6][0])){
        linear2_bias = paddle::make_optional<Tensor>(Tensor(std::make_shared<primitive::LazyTensor>(inputs_[6][0])));
    }

    VLOG(6) << "Vjp prepare Prepare attributes of fused_feedforward_grad";

    bool pre_layer_norm = op->attribute("pre_layer_norm").dyn_cast<pir::BoolAttribute>().data();
    float ln1_epsilon = op->attribute("ln1_epsilon").dyn_cast<pir::FloatAttribute>().data();
    float ln2_epsilon = op->attribute("ln2_epsilon").dyn_cast<pir::FloatAttribute>().data();
    std::string act_method = op->attribute("act_method").dyn_cast<pir::StrAttribute>().AsString();
    float dropout1_prob = op->attribute("dropout1_prob").dyn_cast<pir::FloatAttribute>().data();
    float dropout2_prob = op->attribute("dropout2_prob").dyn_cast<pir::FloatAttribute>().data();
    std::string dropout1_implementation = op->attribute("dropout1_implementation").dyn_cast<pir::StrAttribute>().AsString();
    std::string dropout2_implementation = op->attribute("dropout2_implementation").dyn_cast<pir::StrAttribute>().AsString();
    bool is_test = op->attribute("is_test").dyn_cast<pir::BoolAttribute>().data();
    bool dropout1_fix_seed = op->attribute("dropout1_fix_seed").dyn_cast<pir::BoolAttribute>().data();
    bool dropout2_fix_seed = op->attribute("dropout2_fix_seed").dyn_cast<pir::BoolAttribute>().data();
    int dropout1_seed_val = op->attribute("dropout1_seed_val").dyn_cast<pir::Int32Attribute>().data();
    int dropout2_seed_val = op->attribute("dropout2_seed_val").dyn_cast<pir::Int32Attribute>().data();
    bool add_residual = op->attribute("add_residual").dyn_cast<pir::BoolAttribute>().data();
    int ring_id = op->attribute("ring_id").dyn_cast<pir::Int32Attribute>().data();

    VLOG(6) << "Vjp prepare call fused_feedforward's vjp inteface";

    std::vector<std::vector<Tensor>> tensor_res =
        primitive::fused_feedforward_vjp(
        out_grad, x, linear1_weight, linear1_bias, linear2_weight, dropout1_mask, dropout2_mask, linear1_out, dropout1_out, dropout2_out, ln1_scale, ln1_bias, ln1_out, ln1_mean, ln1_variance, ln2_scale, ln2_bias, ln2_mean, ln2_variance, linear2_bias, pre_layer_norm, ln1_epsilon, ln2_epsilon, act_method, dropout1_prob, dropout2_prob, dropout1_implementation, dropout2_implementation, is_test, dropout1_fix_seed, dropout2_fix_seed, dropout1_seed_val, dropout2_seed_val, add_residual, ring_id, stop_gradients);

    VLOG(6) << "Vjp prepare stop gradient of fused_feedforward_grad";

    std::vector<std::vector<pir::OpResult>> res(tensor_res.size());
    for (size_t i = 0; i < tensor_res.size(); ++i) {
        res[i].resize(tensor_res[i].size());
        for (size_t j = 0; j < tensor_res[i].size(); ++j) {
            if(tensor_res[i][j].defined()){
                res[i][j] = std::static_pointer_cast<primitive::LazyTensor>(tensor_res[i][j].impl())->value().dyn_cast<pir::OpResult>();
            }
        }
    }
    return res;
}

std::vector<std::vector<pir::OpResult>> NceOp::Vjp(pir::Operation* op, const std::vector<std::vector<pir::Value>>& inputs_, const std::vector<std::vector<pir::Value>>& outputs, const std::vector<std::vector<pir::Value>>& out_grads, const std::vector<std::vector<bool>>& stop_gradients){

    PADDLE_ENFORCE_EQ(
      inputs_.size(),
      8,
      platform::errors::InvalidArgument("nce op's inputs size should be 8, but now is %d.", inputs_.size()));
    PADDLE_ENFORCE_EQ(
      outputs.size(),
      3,
      platform::errors::InvalidArgument("nce op's outputs size should be 3, but now is %d.", outputs.size()));

    VLOG(6) << "Prepare inputs of nce_grad";

    Tensor input(std::make_shared<primitive::LazyTensor>(inputs_[0][0]));
    Tensor label(std::make_shared<primitive::LazyTensor>(inputs_[1][0]));
    paddle::optional<Tensor> bias;
    if (!IsEmptyValue(inputs_[3][0])){
        bias = paddle::make_optional<Tensor>(Tensor(std::make_shared<primitive::LazyTensor>(inputs_[3][0])));
    }
    Tensor weight(std::make_shared<primitive::LazyTensor>(inputs_[2][0]));
    Tensor sample_logits(std::make_shared<primitive::LazyTensor>(outputs[1][0]));
    Tensor sample_labels(std::make_shared<primitive::LazyTensor>(outputs[2][0]));
    paddle::optional<Tensor> sample_weight;
    if (!IsEmptyValue(inputs_[4][0])){
        sample_weight = paddle::make_optional<Tensor>(Tensor(std::make_shared<primitive::LazyTensor>(inputs_[4][0])));
    }
    paddle::optional<Tensor> custom_dist_probs;
    if (!IsEmptyValue(inputs_[5][0])){
        custom_dist_probs = paddle::make_optional<Tensor>(Tensor(std::make_shared<primitive::LazyTensor>(inputs_[5][0])));
    }
    paddle::optional<Tensor> custom_dist_alias;
    if (!IsEmptyValue(inputs_[6][0])){
        custom_dist_alias = paddle::make_optional<Tensor>(Tensor(std::make_shared<primitive::LazyTensor>(inputs_[6][0])));
    }
    paddle::optional<Tensor> custom_dist_alias_probs;
    if (!IsEmptyValue(inputs_[7][0])){
        custom_dist_alias_probs = paddle::make_optional<Tensor>(Tensor(std::make_shared<primitive::LazyTensor>(inputs_[7][0])));
    }
    Tensor cost_grad(std::make_shared<primitive::LazyTensor>(out_grads[0][0]));

    VLOG(6) << "Vjp prepare Prepare attributes of nce_grad";

    int num_total_classes = op->attribute("num_total_classes").dyn_cast<pir::Int32Attribute>().data();
    std::vector<int> custom_neg_classes;
    for (size_t i = 0; i < op->attribute("custom_neg_classes").dyn_cast<pir::ArrayAttribute>().size(); i++) {
        custom_neg_classes.push_back(op->attribute("custom_neg_classes").dyn_cast<pir::ArrayAttribute>().at(i).dyn_cast<pir::Int32Attribute>().data());
    }
    int num_neg_samples = op->attribute("num_neg_samples").dyn_cast<pir::Int32Attribute>().data();
    int sampler = op->attribute("sampler").dyn_cast<pir::Int32Attribute>().data();
    int seed = op->attribute("seed").dyn_cast<pir::Int32Attribute>().data();
    bool is_sparse = op->attribute("is_sparse").dyn_cast<pir::BoolAttribute>().data();
    bool remote_prefetch = op->attribute("remote_prefetch").dyn_cast<pir::BoolAttribute>().data();
    bool is_test = op->attribute("is_test").dyn_cast<pir::BoolAttribute>().data();

    VLOG(6) << "Vjp prepare call nce's vjp inteface";

    std::vector<std::vector<Tensor>> tensor_res =
        primitive::nce_vjp(
        input, label, bias, weight, sample_logits, sample_labels, sample_weight, custom_dist_probs, custom_dist_alias, custom_dist_alias_probs, cost_grad, num_total_classes, custom_neg_classes, num_neg_samples, sampler, seed, is_sparse, remote_prefetch, is_test, stop_gradients);

    VLOG(6) << "Vjp prepare stop gradient of nce_grad";

    std::vector<std::vector<pir::OpResult>> res(tensor_res.size());
    for (size_t i = 0; i < tensor_res.size(); ++i) {
        res[i].resize(tensor_res[i].size());
        for (size_t j = 0; j < tensor_res[i].size(); ++j) {
            if(tensor_res[i][j].defined()){
                res[i][j] = std::static_pointer_cast<primitive::LazyTensor>(tensor_res[i][j].impl())->value().dyn_cast<pir::OpResult>();
            }
        }
    }
    return res;
}

std::vector<std::vector<pir::OpResult>> MatchMatrixTensorOp::Vjp(pir::Operation* op, const std::vector<std::vector<pir::Value>>& inputs_, const std::vector<std::vector<pir::Value>>& outputs, const std::vector<std::vector<pir::Value>>& out_grads, const std::vector<std::vector<bool>>& stop_gradients){

    PADDLE_ENFORCE_EQ(
      inputs_.size(),
      3,
      platform::errors::InvalidArgument("match_matrix_tensor op's inputs size should be 3, but now is %d.", inputs_.size()));
    PADDLE_ENFORCE_EQ(
      outputs.size(),
      2,
      platform::errors::InvalidArgument("match_matrix_tensor op's outputs size should be 2, but now is %d.", outputs.size()));

    VLOG(6) << "Prepare inputs of match_matrix_tensor_grad";

    Tensor x(std::make_shared<primitive::LazyTensor>(inputs_[0][0]));
    Tensor y(std::make_shared<primitive::LazyTensor>(inputs_[1][0]));
    Tensor w(std::make_shared<primitive::LazyTensor>(inputs_[2][0]));
    Tensor tmp(std::make_shared<primitive::LazyTensor>(outputs[1][0]));
    Tensor out_grad(std::make_shared<primitive::LazyTensor>(out_grads[0][0]));

    VLOG(6) << "Vjp prepare Prepare attributes of match_matrix_tensor_grad";

    int dim_t = op->attribute("dim_t").dyn_cast<pir::Int32Attribute>().data();

    VLOG(6) << "Vjp prepare call match_matrix_tensor's vjp inteface";

    std::vector<std::vector<Tensor>> tensor_res =
        primitive::match_matrix_tensor_vjp(
        x, y, w, tmp, out_grad, dim_t, stop_gradients);

    VLOG(6) << "Vjp prepare stop gradient of match_matrix_tensor_grad";

    std::vector<std::vector<pir::OpResult>> res(tensor_res.size());
    for (size_t i = 0; i < tensor_res.size(); ++i) {
        res[i].resize(tensor_res[i].size());
        for (size_t j = 0; j < tensor_res[i].size(); ++j) {
            if(tensor_res[i][j].defined()){
                res[i][j] = std::static_pointer_cast<primitive::LazyTensor>(tensor_res[i][j].impl())->value().dyn_cast<pir::OpResult>();
            }
        }
    }
    return res;
}


std::vector<std::vector<pir::OpResult>> AddDoubleGradOp::Vjp(pir::Operation* op, const std::vector<std::vector<pir::Value>>& inputs_, const std::vector<std::vector<pir::Value>>& outputs, const std::vector<std::vector<pir::Value>>& out_grads, const std::vector<std::vector<bool>>& stop_gradients){

    PADDLE_ENFORCE_EQ(
      inputs_.size(),
      4,
      platform::errors::InvalidArgument("add_double_grad op's inputs size should be 4, but now is %d.", inputs_.size()));
    PADDLE_ENFORCE_EQ(
      outputs.size(),
      1,
      platform::errors::InvalidArgument("add_double_grad op's outputs size should be 1, but now is %d.", outputs.size()));

    VLOG(6) << "Prepare inputs of add_triple_grad";

    Tensor grad_grad_x(std::make_shared<primitive::LazyTensor>(inputs_[2][0]));
    Tensor grad_grad_y(std::make_shared<primitive::LazyTensor>(inputs_[3][0]));
    Tensor grad_grad_out_grad(std::make_shared<primitive::LazyTensor>(out_grads[0][0]));

    VLOG(6) << "Vjp prepare Prepare attributes of add_triple_grad";

    int axis = op->attribute("axis").dyn_cast<pir::Int32Attribute>().data();

    VLOG(6) << "Vjp prepare call add_double_grad's vjp inteface";

    std::vector<std::vector<Tensor>> tensor_res =
        primitive::add_double_grad_vjp(
        grad_grad_x, grad_grad_y, grad_grad_out_grad, axis, stop_gradients);

    VLOG(6) << "Vjp prepare stop gradient of add_triple_grad";

    std::vector<std::vector<pir::OpResult>> res(tensor_res.size());
    for (size_t i = 0; i < tensor_res.size(); ++i) {
        res[i].resize(tensor_res[i].size());
        for (size_t j = 0; j < tensor_res[i].size(); ++j) {
            if(tensor_res[i][j].defined()){
                res[i][j] = std::static_pointer_cast<primitive::LazyTensor>(tensor_res[i][j].impl())->value().dyn_cast<pir::OpResult>();
            }
        }
    }
    return res;
}

std::vector<std::vector<pir::OpResult>> AddDoubleGrad_Op::Vjp(pir::Operation* op, const std::vector<std::vector<pir::Value>>& inputs_, const std::vector<std::vector<pir::Value>>& outputs, const std::vector<std::vector<pir::Value>>& out_grads, const std::vector<std::vector<bool>>& stop_gradients){

    PADDLE_ENFORCE_EQ(
      inputs_.size(),
      4,
      platform::errors::InvalidArgument("add_double_grad op's inputs size should be 4, but now is %d.", inputs_.size()));
    PADDLE_ENFORCE_EQ(
      outputs.size(),
      1,
      platform::errors::InvalidArgument("add_double_grad op's outputs size should be 1, but now is %d.", outputs.size()));

    VLOG(6) << "Prepare inputs of add_triple_grad";

    Tensor grad_grad_x(std::make_shared<primitive::LazyTensor>(inputs_[2][0]));
    Tensor grad_grad_y(std::make_shared<primitive::LazyTensor>(inputs_[3][0]));
    Tensor grad_grad_out_grad(std::make_shared<primitive::LazyTensor>(out_grads[0][0]));

    VLOG(6) << "Vjp prepare Prepare attributes of add_triple_grad";

    int axis = op->attribute("axis").dyn_cast<pir::Int32Attribute>().data();

    VLOG(6) << "Vjp prepare call add_double_grad_'s vjp inteface";

    std::vector<std::vector<Tensor>> tensor_res =
        primitive::add_double_grad_vjp(
        grad_grad_x, grad_grad_y, grad_grad_out_grad, axis, stop_gradients);

    VLOG(6) << "Vjp prepare stop gradient of add_triple_grad";

    std::vector<std::vector<pir::OpResult>> res(tensor_res.size());
    for (size_t i = 0; i < tensor_res.size(); ++i) {
        res[i].resize(tensor_res[i].size());
        for (size_t j = 0; j < tensor_res[i].size(); ++j) {
            if(tensor_res[i][j].defined()){
                res[i][j] = std::static_pointer_cast<primitive::LazyTensor>(tensor_res[i][j].impl())->value().dyn_cast<pir::OpResult>();
            }
        }
    }
    return res;
}

std::vector<std::vector<pir::OpResult>> AddGradOp::Vjp(pir::Operation* op, const std::vector<std::vector<pir::Value>>& inputs_, const std::vector<std::vector<pir::Value>>& outputs, const std::vector<std::vector<pir::Value>>& out_grads, const std::vector<std::vector<bool>>& stop_gradients){

    PADDLE_ENFORCE_EQ(
      inputs_.size(),
      3,
      platform::errors::InvalidArgument("add_grad op's inputs size should be 3, but now is %d.", inputs_.size()));
    PADDLE_ENFORCE_EQ(
      outputs.size(),
      2,
      platform::errors::InvalidArgument("add_grad op's outputs size should be 2, but now is %d.", outputs.size()));

    VLOG(6) << "Prepare inputs of add_double_grad";

    Tensor y(std::make_shared<primitive::LazyTensor>(inputs_[1][0]));
    Tensor grad_out(std::make_shared<primitive::LazyTensor>(inputs_[2][0]));
    paddle::optional<Tensor> grad_x_grad;
    if (!IsEmptyValue(out_grads[0][0])){
        grad_x_grad = paddle::make_optional<Tensor>(Tensor(std::make_shared<primitive::LazyTensor>(out_grads[0][0])));
    }
    paddle::optional<Tensor> grad_y_grad;
    if (!IsEmptyValue(out_grads[1][0])){
        grad_y_grad = paddle::make_optional<Tensor>(Tensor(std::make_shared<primitive::LazyTensor>(out_grads[1][0])));
    }

    VLOG(6) << "Vjp prepare Prepare attributes of add_double_grad";

    int axis = op->attribute("axis").dyn_cast<pir::Int32Attribute>().data();

    VLOG(6) << "Vjp prepare call add_grad's vjp inteface";

    std::vector<std::vector<Tensor>> tensor_res =
        primitive::add_grad_vjp(
        y, grad_out, grad_x_grad, grad_y_grad, axis, stop_gradients);

    VLOG(6) << "Vjp prepare stop gradient of add_double_grad";

    std::vector<std::vector<pir::OpResult>> res(tensor_res.size());
    for (size_t i = 0; i < tensor_res.size(); ++i) {
        res[i].resize(tensor_res[i].size());
        for (size_t j = 0; j < tensor_res[i].size(); ++j) {
            if(tensor_res[i][j].defined()){
                res[i][j] = std::static_pointer_cast<primitive::LazyTensor>(tensor_res[i][j].impl())->value().dyn_cast<pir::OpResult>();
            }
        }
    }
    return res;
}

std::vector<std::vector<pir::OpResult>> AddGrad_Op::Vjp(pir::Operation* op, const std::vector<std::vector<pir::Value>>& inputs_, const std::vector<std::vector<pir::Value>>& outputs, const std::vector<std::vector<pir::Value>>& out_grads, const std::vector<std::vector<bool>>& stop_gradients){

    PADDLE_ENFORCE_EQ(
      inputs_.size(),
      3,
      platform::errors::InvalidArgument("add_grad op's inputs size should be 3, but now is %d.", inputs_.size()));
    PADDLE_ENFORCE_EQ(
      outputs.size(),
      2,
      platform::errors::InvalidArgument("add_grad op's outputs size should be 2, but now is %d.", outputs.size()));

    VLOG(6) << "Prepare inputs of add_double_grad";

    Tensor y(std::make_shared<primitive::LazyTensor>(inputs_[1][0]));
    Tensor grad_out(std::make_shared<primitive::LazyTensor>(inputs_[2][0]));
    paddle::optional<Tensor> grad_x_grad;
    if (!IsEmptyValue(out_grads[0][0])){
        grad_x_grad = paddle::make_optional<Tensor>(Tensor(std::make_shared<primitive::LazyTensor>(out_grads[0][0])));
    }
    paddle::optional<Tensor> grad_y_grad;
    if (!IsEmptyValue(out_grads[1][0])){
        grad_y_grad = paddle::make_optional<Tensor>(Tensor(std::make_shared<primitive::LazyTensor>(out_grads[1][0])));
    }

    VLOG(6) << "Vjp prepare Prepare attributes of add_double_grad";

    int axis = op->attribute("axis").dyn_cast<pir::Int32Attribute>().data();

    VLOG(6) << "Vjp prepare call add_grad_'s vjp inteface";

    std::vector<std::vector<Tensor>> tensor_res =
        primitive::add_grad_vjp(
        y, grad_out, grad_x_grad, grad_y_grad, axis, stop_gradients);

    VLOG(6) << "Vjp prepare stop gradient of add_double_grad";

    std::vector<std::vector<pir::OpResult>> res(tensor_res.size());
    for (size_t i = 0; i < tensor_res.size(); ++i) {
        res[i].resize(tensor_res[i].size());
        for (size_t j = 0; j < tensor_res[i].size(); ++j) {
            if(tensor_res[i][j].defined()){
                res[i][j] = std::static_pointer_cast<primitive::LazyTensor>(tensor_res[i][j].impl())->value().dyn_cast<pir::OpResult>();
            }
        }
    }
    return res;
}

std::vector<std::vector<pir::OpResult>> BatchNormGradOp::Vjp(pir::Operation* op, const std::vector<std::vector<pir::Value>>& inputs_, const std::vector<std::vector<pir::Value>>& outputs, const std::vector<std::vector<pir::Value>>& out_grads, const std::vector<std::vector<bool>>& stop_gradients){

    PADDLE_ENFORCE_EQ(
      inputs_.size(),
      9,
      platform::errors::InvalidArgument("batch_norm_grad op's inputs size should be 9, but now is %d.", inputs_.size()));
    PADDLE_ENFORCE_EQ(
      outputs.size(),
      3,
      platform::errors::InvalidArgument("batch_norm_grad op's outputs size should be 3, but now is %d.", outputs.size()));

    VLOG(6) << "Prepare inputs of batch_norm_double_grad";

    Tensor x(std::make_shared<primitive::LazyTensor>(inputs_[0][0]));
    paddle::optional<Tensor> scale;
    if (!IsEmptyValue(inputs_[1][0])){
        scale = paddle::make_optional<Tensor>(Tensor(std::make_shared<primitive::LazyTensor>(inputs_[1][0])));
    }
    paddle::optional<Tensor> out_mean;
    if (!IsEmptyValue(inputs_[3][0])){
        out_mean = paddle::make_optional<Tensor>(Tensor(std::make_shared<primitive::LazyTensor>(inputs_[3][0])));
    }
    paddle::optional<Tensor> out_variance;
    if (!IsEmptyValue(inputs_[4][0])){
        out_variance = paddle::make_optional<Tensor>(Tensor(std::make_shared<primitive::LazyTensor>(inputs_[4][0])));
    }
    Tensor saved_mean(std::make_shared<primitive::LazyTensor>(inputs_[5][0]));
    Tensor saved_variance(std::make_shared<primitive::LazyTensor>(inputs_[6][0]));
    Tensor grad_out(std::make_shared<primitive::LazyTensor>(inputs_[8][0]));
    paddle::optional<Tensor> grad_x_grad;
    if (!IsEmptyValue(out_grads[0][0])){
        grad_x_grad = paddle::make_optional<Tensor>(Tensor(std::make_shared<primitive::LazyTensor>(out_grads[0][0])));
    }
    paddle::optional<Tensor> grad_scale_grad;
    if (!IsEmptyValue(out_grads[1][0])){
        grad_scale_grad = paddle::make_optional<Tensor>(Tensor(std::make_shared<primitive::LazyTensor>(out_grads[1][0])));
    }
    paddle::optional<Tensor> grad_bias_grad;
    if (!IsEmptyValue(out_grads[2][0])){
        grad_bias_grad = paddle::make_optional<Tensor>(Tensor(std::make_shared<primitive::LazyTensor>(out_grads[2][0])));
    }

    VLOG(6) << "Vjp prepare Prepare attributes of batch_norm_double_grad";

    float momentum = op->attribute("momentum").dyn_cast<pir::FloatAttribute>().data();
    float epsilon = op->attribute("epsilon").dyn_cast<pir::FloatAttribute>().data();
    std::string data_layout = op->attribute("data_layout").dyn_cast<pir::StrAttribute>().AsString();
    bool is_test = op->attribute("is_test").dyn_cast<pir::BoolAttribute>().data();
    bool use_global_stats = op->attribute("use_global_stats").dyn_cast<pir::BoolAttribute>().data();
    bool trainable_statistics = op->attribute("trainable_statistics").dyn_cast<pir::BoolAttribute>().data();

    VLOG(6) << "Vjp prepare call batch_norm_grad's vjp inteface";

    std::vector<std::vector<Tensor>> tensor_res =
        primitive::batch_norm_grad_vjp(
        x, scale, out_mean, out_variance, saved_mean, saved_variance, grad_out, grad_x_grad, grad_scale_grad, grad_bias_grad, momentum, epsilon, data_layout, is_test, use_global_stats, trainable_statistics, stop_gradients);

    VLOG(6) << "Vjp prepare stop gradient of batch_norm_double_grad";

    std::vector<std::vector<pir::OpResult>> res(tensor_res.size());
    for (size_t i = 0; i < tensor_res.size(); ++i) {
        res[i].resize(tensor_res[i].size());
        for (size_t j = 0; j < tensor_res[i].size(); ++j) {
            if(tensor_res[i][j].defined()){
                res[i][j] = std::static_pointer_cast<primitive::LazyTensor>(tensor_res[i][j].impl())->value().dyn_cast<pir::OpResult>();
            }
        }
    }
    return res;
}

std::vector<std::vector<pir::OpResult>> Conv2dTransposeGradOp::Vjp(pir::Operation* op, const std::vector<std::vector<pir::Value>>& inputs_, const std::vector<std::vector<pir::Value>>& outputs, const std::vector<std::vector<pir::Value>>& out_grads, const std::vector<std::vector<bool>>& stop_gradients){

    PADDLE_ENFORCE_EQ(
      inputs_.size(),
      4,
      platform::errors::InvalidArgument("conv2d_transpose_grad op's inputs size should be 4, but now is %d.", inputs_.size()));
    PADDLE_ENFORCE_EQ(
      outputs.size(),
      2,
      platform::errors::InvalidArgument("conv2d_transpose_grad op's outputs size should be 2, but now is %d.", outputs.size()));

    VLOG(6) << "Prepare inputs of conv2d_transpose_double_grad";

    Tensor x(std::make_shared<primitive::LazyTensor>(inputs_[0][0]));
    Tensor filter(std::make_shared<primitive::LazyTensor>(inputs_[1][0]));
    Tensor grad_out(std::make_shared<primitive::LazyTensor>(inputs_[2][0]));
    Tensor grad_x_grad(std::make_shared<primitive::LazyTensor>(out_grads[0][0]));
    Tensor grad_filter_grad(std::make_shared<primitive::LazyTensor>(out_grads[1][0]));

    VLOG(6) << "Vjp prepare Prepare attributes of conv2d_transpose_double_grad";

    std::vector<int> strides;
    for (size_t i = 0; i < op->attribute("strides").dyn_cast<pir::ArrayAttribute>().size(); i++) {
        strides.push_back(op->attribute("strides").dyn_cast<pir::ArrayAttribute>().at(i).dyn_cast<pir::Int32Attribute>().data());
    }
    std::vector<int> paddings;
    for (size_t i = 0; i < op->attribute("paddings").dyn_cast<pir::ArrayAttribute>().size(); i++) {
        paddings.push_back(op->attribute("paddings").dyn_cast<pir::ArrayAttribute>().at(i).dyn_cast<pir::Int32Attribute>().data());
    }
    std::vector<int> output_padding;
    for (size_t i = 0; i < op->attribute("output_padding").dyn_cast<pir::ArrayAttribute>().size(); i++) {
        output_padding.push_back(op->attribute("output_padding").dyn_cast<pir::ArrayAttribute>().at(i).dyn_cast<pir::Int32Attribute>().data());
    }
    Tensor output_size(std::make_shared<primitive::LazyTensor>(inputs_[3][0]));
    std::string padding_algorithm = op->attribute("padding_algorithm").dyn_cast<pir::StrAttribute>().AsString();
    int groups = op->attribute("groups").dyn_cast<pir::Int32Attribute>().data();
    std::vector<int> dilations;
    for (size_t i = 0; i < op->attribute("dilations").dyn_cast<pir::ArrayAttribute>().size(); i++) {
        dilations.push_back(op->attribute("dilations").dyn_cast<pir::ArrayAttribute>().at(i).dyn_cast<pir::Int32Attribute>().data());
    }
    std::string data_format = op->attribute("data_format").dyn_cast<pir::StrAttribute>().AsString();

    VLOG(6) << "Vjp prepare call conv2d_transpose_grad's vjp inteface";

    std::vector<std::vector<Tensor>> tensor_res =
        primitive::conv2d_transpose_grad_vjp(
        x, filter, grad_out, grad_x_grad, grad_filter_grad, output_size, strides, paddings, output_padding, padding_algorithm, groups, dilations, data_format, stop_gradients);

    VLOG(6) << "Vjp prepare stop gradient of conv2d_transpose_double_grad";

    std::vector<std::vector<pir::OpResult>> res(tensor_res.size());
    for (size_t i = 0; i < tensor_res.size(); ++i) {
        res[i].resize(tensor_res[i].size());
        for (size_t j = 0; j < tensor_res[i].size(); ++j) {
            if(tensor_res[i][j].defined()){
                res[i][j] = std::static_pointer_cast<primitive::LazyTensor>(tensor_res[i][j].impl())->value().dyn_cast<pir::OpResult>();
            }
        }
    }
    return res;
}

std::vector<std::vector<pir::OpResult>> DivideGradOp::Vjp(pir::Operation* op, const std::vector<std::vector<pir::Value>>& inputs_, const std::vector<std::vector<pir::Value>>& outputs, const std::vector<std::vector<pir::Value>>& out_grads, const std::vector<std::vector<bool>>& stop_gradients){

    PADDLE_ENFORCE_EQ(
      inputs_.size(),
      4,
      platform::errors::InvalidArgument("divide_grad op's inputs size should be 4, but now is %d.", inputs_.size()));
    PADDLE_ENFORCE_EQ(
      outputs.size(),
      2,
      platform::errors::InvalidArgument("divide_grad op's outputs size should be 2, but now is %d.", outputs.size()));

    VLOG(6) << "Prepare inputs of divide_double_grad";

    Tensor y(std::make_shared<primitive::LazyTensor>(inputs_[1][0]));
    Tensor out(std::make_shared<primitive::LazyTensor>(inputs_[2][0]));
    Tensor grad_x(std::make_shared<primitive::LazyTensor>(outputs[0][0]));
    paddle::optional<Tensor> grad_x_grad;
    if (!IsEmptyValue(out_grads[0][0])){
        grad_x_grad = paddle::make_optional<Tensor>(Tensor(std::make_shared<primitive::LazyTensor>(out_grads[0][0])));
    }
    paddle::optional<Tensor> grad_y_grad;
    if (!IsEmptyValue(out_grads[1][0])){
        grad_y_grad = paddle::make_optional<Tensor>(Tensor(std::make_shared<primitive::LazyTensor>(out_grads[1][0])));
    }

    VLOG(6) << "Vjp prepare Prepare attributes of divide_double_grad";

    int axis = op->attribute("axis").dyn_cast<pir::Int32Attribute>().data();

    VLOG(6) << "Vjp prepare call divide_grad's vjp inteface";

    std::vector<std::vector<Tensor>> tensor_res =
        primitive::divide_grad_vjp(
        y, out, grad_x, grad_x_grad, grad_y_grad, axis, stop_gradients);

    VLOG(6) << "Vjp prepare stop gradient of divide_double_grad";

    std::vector<std::vector<pir::OpResult>> res(tensor_res.size());
    for (size_t i = 0; i < tensor_res.size(); ++i) {
        res[i].resize(tensor_res[i].size());
        for (size_t j = 0; j < tensor_res[i].size(); ++j) {
            if(tensor_res[i][j].defined()){
                res[i][j] = std::static_pointer_cast<primitive::LazyTensor>(tensor_res[i][j].impl())->value().dyn_cast<pir::OpResult>();
            }
        }
    }
    return res;
}

std::vector<std::vector<pir::OpResult>> MatmulGradOp::Vjp(pir::Operation* op, const std::vector<std::vector<pir::Value>>& inputs_, const std::vector<std::vector<pir::Value>>& outputs, const std::vector<std::vector<pir::Value>>& out_grads, const std::vector<std::vector<bool>>& stop_gradients){

    PADDLE_ENFORCE_EQ(
      inputs_.size(),
      3,
      platform::errors::InvalidArgument("matmul_grad op's inputs size should be 3, but now is %d.", inputs_.size()));
    PADDLE_ENFORCE_EQ(
      outputs.size(),
      2,
      platform::errors::InvalidArgument("matmul_grad op's outputs size should be 2, but now is %d.", outputs.size()));

    VLOG(6) << "Prepare inputs of matmul_double_grad";

    Tensor x(std::make_shared<primitive::LazyTensor>(inputs_[0][0]));
    Tensor y(std::make_shared<primitive::LazyTensor>(inputs_[1][0]));
    Tensor grad_out(std::make_shared<primitive::LazyTensor>(inputs_[2][0]));
    paddle::optional<Tensor> grad_x_grad;
    if (!IsEmptyValue(out_grads[0][0])){
        grad_x_grad = paddle::make_optional<Tensor>(Tensor(std::make_shared<primitive::LazyTensor>(out_grads[0][0])));
    }
    paddle::optional<Tensor> grad_y_grad;
    if (!IsEmptyValue(out_grads[1][0])){
        grad_y_grad = paddle::make_optional<Tensor>(Tensor(std::make_shared<primitive::LazyTensor>(out_grads[1][0])));
    }

    VLOG(6) << "Vjp prepare Prepare attributes of matmul_double_grad";

    bool transpose_x = op->attribute("transpose_x").dyn_cast<pir::BoolAttribute>().data();
    bool transpose_y = op->attribute("transpose_y").dyn_cast<pir::BoolAttribute>().data();

    VLOG(6) << "Vjp prepare call matmul_grad's vjp inteface";

    std::vector<std::vector<Tensor>> tensor_res =
        primitive::matmul_grad_vjp(
        x, y, grad_out, grad_x_grad, grad_y_grad, transpose_x, transpose_y, stop_gradients);

    VLOG(6) << "Vjp prepare stop gradient of matmul_double_grad";

    std::vector<std::vector<pir::OpResult>> res(tensor_res.size());
    for (size_t i = 0; i < tensor_res.size(); ++i) {
        res[i].resize(tensor_res[i].size());
        for (size_t j = 0; j < tensor_res[i].size(); ++j) {
            if(tensor_res[i][j].defined()){
                res[i][j] = std::static_pointer_cast<primitive::LazyTensor>(tensor_res[i][j].impl())->value().dyn_cast<pir::OpResult>();
            }
        }
    }
    return res;
}

std::vector<std::vector<pir::OpResult>> MeanGradOp::Vjp(pir::Operation* op, const std::vector<std::vector<pir::Value>>& inputs_, const std::vector<std::vector<pir::Value>>& outputs, const std::vector<std::vector<pir::Value>>& out_grads, const std::vector<std::vector<bool>>& stop_gradients){

    PADDLE_ENFORCE_EQ(
      inputs_.size(),
      2,
      platform::errors::InvalidArgument("mean_grad op's inputs size should be 2, but now is %d.", inputs_.size()));
    PADDLE_ENFORCE_EQ(
      outputs.size(),
      1,
      platform::errors::InvalidArgument("mean_grad op's outputs size should be 1, but now is %d.", outputs.size()));

    VLOG(6) << "Prepare inputs of mean_double_grad";

    Tensor grad_x_grad(std::make_shared<primitive::LazyTensor>(out_grads[0][0]));

    VLOG(6) << "Vjp prepare Prepare attributes of mean_double_grad";

    phi::IntArray axis = op->attribute("axis").dyn_cast<paddle::dialect::IntArrayAttribute>().data();
    bool keepdim = op->attribute("keepdim").dyn_cast<pir::BoolAttribute>().data();

    VLOG(6) << "Vjp prepare call mean_grad's vjp inteface";

    std::vector<std::vector<Tensor>> tensor_res =
        primitive::mean_grad_vjp(
        grad_x_grad, axis, keepdim, stop_gradients);

    VLOG(6) << "Vjp prepare stop gradient of mean_double_grad";

    std::vector<std::vector<pir::OpResult>> res(tensor_res.size());
    for (size_t i = 0; i < tensor_res.size(); ++i) {
        res[i].resize(tensor_res[i].size());
        for (size_t j = 0; j < tensor_res[i].size(); ++j) {
            if(tensor_res[i][j].defined()){
                res[i][j] = std::static_pointer_cast<primitive::LazyTensor>(tensor_res[i][j].impl())->value().dyn_cast<pir::OpResult>();
            }
        }
    }
    return res;
}

std::vector<std::vector<pir::OpResult>> MultiplyDoubleGradOp::Vjp(pir::Operation* op, const std::vector<std::vector<pir::Value>>& inputs_, const std::vector<std::vector<pir::Value>>& outputs, const std::vector<std::vector<pir::Value>>& out_grads, const std::vector<std::vector<bool>>& stop_gradients){

    PADDLE_ENFORCE_EQ(
      inputs_.size(),
      5,
      platform::errors::InvalidArgument("multiply_double_grad op's inputs size should be 5, but now is %d.", inputs_.size()));
    PADDLE_ENFORCE_EQ(
      outputs.size(),
      3,
      platform::errors::InvalidArgument("multiply_double_grad op's outputs size should be 3, but now is %d.", outputs.size()));

    VLOG(6) << "Prepare inputs of multiply_triple_grad";

    Tensor x(std::make_shared<primitive::LazyTensor>(inputs_[0][0]));
    Tensor y(std::make_shared<primitive::LazyTensor>(inputs_[1][0]));
    Tensor fwd_grad_out(std::make_shared<primitive::LazyTensor>(inputs_[2][0]));
    paddle::optional<Tensor> fwd_grad_grad_x;
    if (!IsEmptyValue(inputs_[3][0])){
        fwd_grad_grad_x = paddle::make_optional<Tensor>(Tensor(std::make_shared<primitive::LazyTensor>(inputs_[3][0])));
    }
    paddle::optional<Tensor> fwd_grad_grad_y;
    if (!IsEmptyValue(inputs_[4][0])){
        fwd_grad_grad_y = paddle::make_optional<Tensor>(Tensor(std::make_shared<primitive::LazyTensor>(inputs_[4][0])));
    }
    paddle::optional<Tensor> grad_x_grad;
    if (!IsEmptyValue(out_grads[0][0])){
        grad_x_grad = paddle::make_optional<Tensor>(Tensor(std::make_shared<primitive::LazyTensor>(out_grads[0][0])));
    }
    paddle::optional<Tensor> grad_y_grad;
    if (!IsEmptyValue(out_grads[1][0])){
        grad_y_grad = paddle::make_optional<Tensor>(Tensor(std::make_shared<primitive::LazyTensor>(out_grads[1][0])));
    }
    paddle::optional<Tensor> grad_grad_out_grad;
    if (!IsEmptyValue(out_grads[2][0])){
        grad_grad_out_grad = paddle::make_optional<Tensor>(Tensor(std::make_shared<primitive::LazyTensor>(out_grads[2][0])));
    }

    VLOG(6) << "Vjp prepare Prepare attributes of multiply_triple_grad";

    int axis = op->attribute("axis").dyn_cast<pir::Int32Attribute>().data();

    VLOG(6) << "Vjp prepare call multiply_double_grad's vjp inteface";

    std::vector<std::vector<Tensor>> tensor_res =
        primitive::multiply_double_grad_vjp(
        x, y, fwd_grad_out, fwd_grad_grad_x, fwd_grad_grad_y, grad_x_grad, grad_y_grad, grad_grad_out_grad, axis, stop_gradients);

    VLOG(6) << "Vjp prepare stop gradient of multiply_triple_grad";

    std::vector<std::vector<pir::OpResult>> res(tensor_res.size());
    for (size_t i = 0; i < tensor_res.size(); ++i) {
        res[i].resize(tensor_res[i].size());
        for (size_t j = 0; j < tensor_res[i].size(); ++j) {
            if(tensor_res[i][j].defined()){
                res[i][j] = std::static_pointer_cast<primitive::LazyTensor>(tensor_res[i][j].impl())->value().dyn_cast<pir::OpResult>();
            }
        }
    }
    return res;
}

std::vector<std::vector<pir::OpResult>> MultiplyDoubleGrad_Op::Vjp(pir::Operation* op, const std::vector<std::vector<pir::Value>>& inputs_, const std::vector<std::vector<pir::Value>>& outputs, const std::vector<std::vector<pir::Value>>& out_grads, const std::vector<std::vector<bool>>& stop_gradients){

    PADDLE_ENFORCE_EQ(
      inputs_.size(),
      5,
      platform::errors::InvalidArgument("multiply_double_grad op's inputs size should be 5, but now is %d.", inputs_.size()));
    PADDLE_ENFORCE_EQ(
      outputs.size(),
      3,
      platform::errors::InvalidArgument("multiply_double_grad op's outputs size should be 3, but now is %d.", outputs.size()));

    VLOG(6) << "Prepare inputs of multiply_triple_grad";

    Tensor x(std::make_shared<primitive::LazyTensor>(inputs_[0][0]));
    Tensor y(std::make_shared<primitive::LazyTensor>(inputs_[1][0]));
    Tensor fwd_grad_out(std::make_shared<primitive::LazyTensor>(inputs_[2][0]));
    paddle::optional<Tensor> fwd_grad_grad_x;
    if (!IsEmptyValue(inputs_[3][0])){
        fwd_grad_grad_x = paddle::make_optional<Tensor>(Tensor(std::make_shared<primitive::LazyTensor>(inputs_[3][0])));
    }
    paddle::optional<Tensor> fwd_grad_grad_y;
    if (!IsEmptyValue(inputs_[4][0])){
        fwd_grad_grad_y = paddle::make_optional<Tensor>(Tensor(std::make_shared<primitive::LazyTensor>(inputs_[4][0])));
    }
    paddle::optional<Tensor> grad_x_grad;
    if (!IsEmptyValue(out_grads[0][0])){
        grad_x_grad = paddle::make_optional<Tensor>(Tensor(std::make_shared<primitive::LazyTensor>(out_grads[0][0])));
    }
    paddle::optional<Tensor> grad_y_grad;
    if (!IsEmptyValue(out_grads[1][0])){
        grad_y_grad = paddle::make_optional<Tensor>(Tensor(std::make_shared<primitive::LazyTensor>(out_grads[1][0])));
    }
    paddle::optional<Tensor> grad_grad_out_grad;
    if (!IsEmptyValue(out_grads[2][0])){
        grad_grad_out_grad = paddle::make_optional<Tensor>(Tensor(std::make_shared<primitive::LazyTensor>(out_grads[2][0])));
    }

    VLOG(6) << "Vjp prepare Prepare attributes of multiply_triple_grad";

    int axis = op->attribute("axis").dyn_cast<pir::Int32Attribute>().data();

    VLOG(6) << "Vjp prepare call multiply_double_grad_'s vjp inteface";

    std::vector<std::vector<Tensor>> tensor_res =
        primitive::multiply_double_grad_vjp(
        x, y, fwd_grad_out, fwd_grad_grad_x, fwd_grad_grad_y, grad_x_grad, grad_y_grad, grad_grad_out_grad, axis, stop_gradients);

    VLOG(6) << "Vjp prepare stop gradient of multiply_triple_grad";

    std::vector<std::vector<pir::OpResult>> res(tensor_res.size());
    for (size_t i = 0; i < tensor_res.size(); ++i) {
        res[i].resize(tensor_res[i].size());
        for (size_t j = 0; j < tensor_res[i].size(); ++j) {
            if(tensor_res[i][j].defined()){
                res[i][j] = std::static_pointer_cast<primitive::LazyTensor>(tensor_res[i][j].impl())->value().dyn_cast<pir::OpResult>();
            }
        }
    }
    return res;
}

std::vector<std::vector<pir::OpResult>> MultiplyGradOp::Vjp(pir::Operation* op, const std::vector<std::vector<pir::Value>>& inputs_, const std::vector<std::vector<pir::Value>>& outputs, const std::vector<std::vector<pir::Value>>& out_grads, const std::vector<std::vector<bool>>& stop_gradients){

    PADDLE_ENFORCE_EQ(
      inputs_.size(),
      3,
      platform::errors::InvalidArgument("multiply_grad op's inputs size should be 3, but now is %d.", inputs_.size()));
    PADDLE_ENFORCE_EQ(
      outputs.size(),
      2,
      platform::errors::InvalidArgument("multiply_grad op's outputs size should be 2, but now is %d.", outputs.size()));

    VLOG(6) << "Prepare inputs of multiply_double_grad";

    Tensor x(std::make_shared<primitive::LazyTensor>(inputs_[0][0]));
    Tensor y(std::make_shared<primitive::LazyTensor>(inputs_[1][0]));
    Tensor grad_out(std::make_shared<primitive::LazyTensor>(inputs_[2][0]));
    paddle::optional<Tensor> grad_x_grad;
    if (!IsEmptyValue(out_grads[0][0])){
        grad_x_grad = paddle::make_optional<Tensor>(Tensor(std::make_shared<primitive::LazyTensor>(out_grads[0][0])));
    }
    paddle::optional<Tensor> grad_y_grad;
    if (!IsEmptyValue(out_grads[1][0])){
        grad_y_grad = paddle::make_optional<Tensor>(Tensor(std::make_shared<primitive::LazyTensor>(out_grads[1][0])));
    }

    VLOG(6) << "Vjp prepare Prepare attributes of multiply_double_grad";

    int axis = op->attribute("axis").dyn_cast<pir::Int32Attribute>().data();

    VLOG(6) << "Vjp prepare call multiply_grad's vjp inteface";

    std::vector<std::vector<Tensor>> tensor_res =
        primitive::multiply_grad_vjp(
        x, y, grad_out, grad_x_grad, grad_y_grad, axis, stop_gradients);

    VLOG(6) << "Vjp prepare stop gradient of multiply_double_grad";

    std::vector<std::vector<pir::OpResult>> res(tensor_res.size());
    for (size_t i = 0; i < tensor_res.size(); ++i) {
        res[i].resize(tensor_res[i].size());
        for (size_t j = 0; j < tensor_res[i].size(); ++j) {
            if(tensor_res[i][j].defined()){
                res[i][j] = std::static_pointer_cast<primitive::LazyTensor>(tensor_res[i][j].impl())->value().dyn_cast<pir::OpResult>();
            }
        }
    }
    return res;
}

std::vector<std::vector<pir::OpResult>> PadGradOp::Vjp(pir::Operation* op, const std::vector<std::vector<pir::Value>>& inputs_, const std::vector<std::vector<pir::Value>>& outputs, const std::vector<std::vector<pir::Value>>& out_grads, const std::vector<std::vector<bool>>& stop_gradients){

    PADDLE_ENFORCE_EQ(
      inputs_.size(),
      3,
      platform::errors::InvalidArgument("pad_grad op's inputs size should be 3, but now is %d.", inputs_.size()));
    PADDLE_ENFORCE_EQ(
      outputs.size(),
      1,
      platform::errors::InvalidArgument("pad_grad op's outputs size should be 1, but now is %d.", outputs.size()));

    VLOG(6) << "Prepare inputs of pad_double_grad";

    Tensor grad_x_grad(std::make_shared<primitive::LazyTensor>(out_grads[0][0]));

    VLOG(6) << "Vjp prepare Prepare attributes of pad_double_grad";

    std::vector<int> paddings;
    for (size_t i = 0; i < op->attribute("paddings").dyn_cast<pir::ArrayAttribute>().size(); i++) {
        paddings.push_back(op->attribute("paddings").dyn_cast<pir::ArrayAttribute>().at(i).dyn_cast<pir::Int32Attribute>().data());
    }
    Tensor pad_value(std::make_shared<primitive::LazyTensor>(inputs_[2][0]));

    VLOG(6) << "Vjp prepare call pad_grad's vjp inteface";

    std::vector<std::vector<Tensor>> tensor_res =
        primitive::pad_grad_vjp(
        grad_x_grad, pad_value, paddings, stop_gradients);

    VLOG(6) << "Vjp prepare stop gradient of pad_double_grad";

    std::vector<std::vector<pir::OpResult>> res(tensor_res.size());
    for (size_t i = 0; i < tensor_res.size(); ++i) {
        res[i].resize(tensor_res[i].size());
        for (size_t j = 0; j < tensor_res[i].size(); ++j) {
            if(tensor_res[i][j].defined()){
                res[i][j] = std::static_pointer_cast<primitive::LazyTensor>(tensor_res[i][j].impl())->value().dyn_cast<pir::OpResult>();
            }
        }
    }
    return res;
}

std::vector<std::vector<pir::OpResult>> Pool2dGradOp::Vjp(pir::Operation* op, const std::vector<std::vector<pir::Value>>& inputs_, const std::vector<std::vector<pir::Value>>& outputs, const std::vector<std::vector<pir::Value>>& out_grads, const std::vector<std::vector<bool>>& stop_gradients){

    PADDLE_ENFORCE_EQ(
      inputs_.size(),
      4,
      platform::errors::InvalidArgument("pool2d_grad op's inputs size should be 4, but now is %d.", inputs_.size()));
    PADDLE_ENFORCE_EQ(
      outputs.size(),
      1,
      platform::errors::InvalidArgument("pool2d_grad op's outputs size should be 1, but now is %d.", outputs.size()));

    VLOG(6) << "Prepare inputs of pool2d_double_grad";

    Tensor x(std::make_shared<primitive::LazyTensor>(inputs_[0][0]));
    Tensor grad_x_grad(std::make_shared<primitive::LazyTensor>(out_grads[0][0]));

    VLOG(6) << "Vjp prepare Prepare attributes of pool2d_double_grad";

    Tensor kernel_size(std::make_shared<primitive::LazyTensor>(inputs_[3][0]));
    std::vector<int> strides;
    for (size_t i = 0; i < op->attribute("strides").dyn_cast<pir::ArrayAttribute>().size(); i++) {
        strides.push_back(op->attribute("strides").dyn_cast<pir::ArrayAttribute>().at(i).dyn_cast<pir::Int32Attribute>().data());
    }
    std::vector<int> paddings;
    for (size_t i = 0; i < op->attribute("paddings").dyn_cast<pir::ArrayAttribute>().size(); i++) {
        paddings.push_back(op->attribute("paddings").dyn_cast<pir::ArrayAttribute>().at(i).dyn_cast<pir::Int32Attribute>().data());
    }
    bool ceil_mode = op->attribute("ceil_mode").dyn_cast<pir::BoolAttribute>().data();
    bool exclusive = op->attribute("exclusive").dyn_cast<pir::BoolAttribute>().data();
    std::string data_format = op->attribute("data_format").dyn_cast<pir::StrAttribute>().AsString();
    std::string pooling_type = op->attribute("pooling_type").dyn_cast<pir::StrAttribute>().AsString();
    bool global_pooling = op->attribute("global_pooling").dyn_cast<pir::BoolAttribute>().data();
    bool adaptive = op->attribute("adaptive").dyn_cast<pir::BoolAttribute>().data();
    std::string padding_algorithm = op->attribute("padding_algorithm").dyn_cast<pir::StrAttribute>().AsString();

    VLOG(6) << "Vjp prepare call pool2d_grad's vjp inteface";

    std::vector<std::vector<Tensor>> tensor_res =
        primitive::pool2d_grad_vjp(
        x, grad_x_grad, kernel_size, strides, paddings, ceil_mode, exclusive, data_format, pooling_type, global_pooling, adaptive, padding_algorithm, stop_gradients);

    VLOG(6) << "Vjp prepare stop gradient of pool2d_double_grad";

    std::vector<std::vector<pir::OpResult>> res(tensor_res.size());
    for (size_t i = 0; i < tensor_res.size(); ++i) {
        res[i].resize(tensor_res[i].size());
        for (size_t j = 0; j < tensor_res[i].size(); ++j) {
            if(tensor_res[i][j].defined()){
                res[i][j] = std::static_pointer_cast<primitive::LazyTensor>(tensor_res[i][j].impl())->value().dyn_cast<pir::OpResult>();
            }
        }
    }
    return res;
}

std::vector<std::vector<pir::OpResult>> ReshapeGradOp::Vjp(pir::Operation* op, const std::vector<std::vector<pir::Value>>& inputs_, const std::vector<std::vector<pir::Value>>& outputs, const std::vector<std::vector<pir::Value>>& out_grads, const std::vector<std::vector<bool>>& stop_gradients){

    PADDLE_ENFORCE_EQ(
      inputs_.size(),
      2,
      platform::errors::InvalidArgument("reshape_grad op's inputs size should be 2, but now is %d.", inputs_.size()));
    PADDLE_ENFORCE_EQ(
      outputs.size(),
      1,
      platform::errors::InvalidArgument("reshape_grad op's outputs size should be 1, but now is %d.", outputs.size()));

    VLOG(6) << "Prepare inputs of reshape_double_grad";

    Tensor grad_out(std::make_shared<primitive::LazyTensor>(inputs_[1][0]));
    Tensor grad_x_grad(std::make_shared<primitive::LazyTensor>(out_grads[0][0]));

    VLOG(6) << "Vjp prepare Prepare attributes of reshape_double_grad";


    VLOG(6) << "Vjp prepare call reshape_grad's vjp inteface";

    std::vector<std::vector<Tensor>> tensor_res =
        primitive::reshape_grad_vjp(
        grad_out, grad_x_grad, stop_gradients);

    VLOG(6) << "Vjp prepare stop gradient of reshape_double_grad";

    std::vector<std::vector<pir::OpResult>> res(tensor_res.size());
    for (size_t i = 0; i < tensor_res.size(); ++i) {
        res[i].resize(tensor_res[i].size());
        for (size_t j = 0; j < tensor_res[i].size(); ++j) {
            if(tensor_res[i][j].defined()){
                res[i][j] = std::static_pointer_cast<primitive::LazyTensor>(tensor_res[i][j].impl())->value().dyn_cast<pir::OpResult>();
            }
        }
    }
    return res;
}

std::vector<std::vector<pir::OpResult>> ReshapeGrad_Op::Vjp(pir::Operation* op, const std::vector<std::vector<pir::Value>>& inputs_, const std::vector<std::vector<pir::Value>>& outputs, const std::vector<std::vector<pir::Value>>& out_grads, const std::vector<std::vector<bool>>& stop_gradients){

    PADDLE_ENFORCE_EQ(
      inputs_.size(),
      2,
      platform::errors::InvalidArgument("reshape_grad op's inputs size should be 2, but now is %d.", inputs_.size()));
    PADDLE_ENFORCE_EQ(
      outputs.size(),
      1,
      platform::errors::InvalidArgument("reshape_grad op's outputs size should be 1, but now is %d.", outputs.size()));

    VLOG(6) << "Prepare inputs of reshape_double_grad";

    Tensor grad_out(std::make_shared<primitive::LazyTensor>(inputs_[1][0]));
    Tensor grad_x_grad(std::make_shared<primitive::LazyTensor>(out_grads[0][0]));

    VLOG(6) << "Vjp prepare Prepare attributes of reshape_double_grad";


    VLOG(6) << "Vjp prepare call reshape_grad_'s vjp inteface";

    std::vector<std::vector<Tensor>> tensor_res =
        primitive::reshape_grad_vjp(
        grad_out, grad_x_grad, stop_gradients);

    VLOG(6) << "Vjp prepare stop gradient of reshape_double_grad";

    std::vector<std::vector<pir::OpResult>> res(tensor_res.size());
    for (size_t i = 0; i < tensor_res.size(); ++i) {
        res[i].resize(tensor_res[i].size());
        for (size_t j = 0; j < tensor_res[i].size(); ++j) {
            if(tensor_res[i][j].defined()){
                res[i][j] = std::static_pointer_cast<primitive::LazyTensor>(tensor_res[i][j].impl())->value().dyn_cast<pir::OpResult>();
            }
        }
    }
    return res;
}

std::vector<std::vector<pir::OpResult>> SliceGradOp::Vjp(pir::Operation* op, const std::vector<std::vector<pir::Value>>& inputs_, const std::vector<std::vector<pir::Value>>& outputs, const std::vector<std::vector<pir::Value>>& out_grads, const std::vector<std::vector<bool>>& stop_gradients){

    PADDLE_ENFORCE_EQ(
      inputs_.size(),
      4,
      platform::errors::InvalidArgument("slice_grad op's inputs size should be 4, but now is %d.", inputs_.size()));
    PADDLE_ENFORCE_EQ(
      outputs.size(),
      1,
      platform::errors::InvalidArgument("slice_grad op's outputs size should be 1, but now is %d.", outputs.size()));

    VLOG(6) << "Prepare inputs of slice_double_grad";

    Tensor grad_input_grad(std::make_shared<primitive::LazyTensor>(out_grads[0][0]));

    VLOG(6) << "Vjp prepare Prepare attributes of slice_double_grad";

    std::vector<int64_t> axes;
    for (size_t i = 0; i < op->attribute("axes").dyn_cast<pir::ArrayAttribute>().size(); i++) {
        axes.push_back(op->attribute("axes").dyn_cast<pir::ArrayAttribute>().at(i).dyn_cast<pir::Int64Attribute>().data());
    }
    Tensor starts(std::make_shared<primitive::LazyTensor>(inputs_[2][0]));
    Tensor ends(std::make_shared<primitive::LazyTensor>(inputs_[3][0]));
    std::vector<int64_t> infer_flags;
    for (size_t i = 0; i < op->attribute("infer_flags").dyn_cast<pir::ArrayAttribute>().size(); i++) {
        infer_flags.push_back(op->attribute("infer_flags").dyn_cast<pir::ArrayAttribute>().at(i).dyn_cast<pir::Int64Attribute>().data());
    }
    std::vector<int64_t> decrease_axis;
    for (size_t i = 0; i < op->attribute("decrease_axis").dyn_cast<pir::ArrayAttribute>().size(); i++) {
        decrease_axis.push_back(op->attribute("decrease_axis").dyn_cast<pir::ArrayAttribute>().at(i).dyn_cast<pir::Int64Attribute>().data());
    }

    VLOG(6) << "Vjp prepare call slice_grad's vjp inteface";

    std::vector<std::vector<Tensor>> tensor_res =
        primitive::slice_grad_vjp(
        grad_input_grad, starts, ends, axes, infer_flags, decrease_axis, stop_gradients);

    VLOG(6) << "Vjp prepare stop gradient of slice_double_grad";

    std::vector<std::vector<pir::OpResult>> res(tensor_res.size());
    for (size_t i = 0; i < tensor_res.size(); ++i) {
        res[i].resize(tensor_res[i].size());
        for (size_t j = 0; j < tensor_res[i].size(); ++j) {
            if(tensor_res[i][j].defined()){
                res[i][j] = std::static_pointer_cast<primitive::LazyTensor>(tensor_res[i][j].impl())->value().dyn_cast<pir::OpResult>();
            }
        }
    }
    return res;
}

std::vector<std::vector<pir::OpResult>> SubtractGradOp::Vjp(pir::Operation* op, const std::vector<std::vector<pir::Value>>& inputs_, const std::vector<std::vector<pir::Value>>& outputs, const std::vector<std::vector<pir::Value>>& out_grads, const std::vector<std::vector<bool>>& stop_gradients){

    PADDLE_ENFORCE_EQ(
      inputs_.size(),
      3,
      platform::errors::InvalidArgument("subtract_grad op's inputs size should be 3, but now is %d.", inputs_.size()));
    PADDLE_ENFORCE_EQ(
      outputs.size(),
      2,
      platform::errors::InvalidArgument("subtract_grad op's outputs size should be 2, but now is %d.", outputs.size()));

    VLOG(6) << "Prepare inputs of subtract_double_grad";

    Tensor y(std::make_shared<primitive::LazyTensor>(inputs_[1][0]));
    Tensor grad_out(std::make_shared<primitive::LazyTensor>(inputs_[2][0]));
    paddle::optional<Tensor> grad_x_grad;
    if (!IsEmptyValue(out_grads[0][0])){
        grad_x_grad = paddle::make_optional<Tensor>(Tensor(std::make_shared<primitive::LazyTensor>(out_grads[0][0])));
    }
    paddle::optional<Tensor> grad_y_grad;
    if (!IsEmptyValue(out_grads[1][0])){
        grad_y_grad = paddle::make_optional<Tensor>(Tensor(std::make_shared<primitive::LazyTensor>(out_grads[1][0])));
    }

    VLOG(6) << "Vjp prepare Prepare attributes of subtract_double_grad";

    int axis = op->attribute("axis").dyn_cast<pir::Int32Attribute>().data();

    VLOG(6) << "Vjp prepare call subtract_grad's vjp inteface";

    std::vector<std::vector<Tensor>> tensor_res =
        primitive::subtract_grad_vjp(
        y, grad_out, grad_x_grad, grad_y_grad, axis, stop_gradients);

    VLOG(6) << "Vjp prepare stop gradient of subtract_double_grad";

    std::vector<std::vector<pir::OpResult>> res(tensor_res.size());
    for (size_t i = 0; i < tensor_res.size(); ++i) {
        res[i].resize(tensor_res[i].size());
        for (size_t j = 0; j < tensor_res[i].size(); ++j) {
            if(tensor_res[i][j].defined()){
                res[i][j] = std::static_pointer_cast<primitive::LazyTensor>(tensor_res[i][j].impl())->value().dyn_cast<pir::OpResult>();
            }
        }
    }
    return res;
}

std::vector<std::vector<pir::OpResult>> SubtractGrad_Op::Vjp(pir::Operation* op, const std::vector<std::vector<pir::Value>>& inputs_, const std::vector<std::vector<pir::Value>>& outputs, const std::vector<std::vector<pir::Value>>& out_grads, const std::vector<std::vector<bool>>& stop_gradients){

    PADDLE_ENFORCE_EQ(
      inputs_.size(),
      3,
      platform::errors::InvalidArgument("subtract_grad op's inputs size should be 3, but now is %d.", inputs_.size()));
    PADDLE_ENFORCE_EQ(
      outputs.size(),
      2,
      platform::errors::InvalidArgument("subtract_grad op's outputs size should be 2, but now is %d.", outputs.size()));

    VLOG(6) << "Prepare inputs of subtract_double_grad";

    Tensor y(std::make_shared<primitive::LazyTensor>(inputs_[1][0]));
    Tensor grad_out(std::make_shared<primitive::LazyTensor>(inputs_[2][0]));
    paddle::optional<Tensor> grad_x_grad;
    if (!IsEmptyValue(out_grads[0][0])){
        grad_x_grad = paddle::make_optional<Tensor>(Tensor(std::make_shared<primitive::LazyTensor>(out_grads[0][0])));
    }
    paddle::optional<Tensor> grad_y_grad;
    if (!IsEmptyValue(out_grads[1][0])){
        grad_y_grad = paddle::make_optional<Tensor>(Tensor(std::make_shared<primitive::LazyTensor>(out_grads[1][0])));
    }

    VLOG(6) << "Vjp prepare Prepare attributes of subtract_double_grad";

    int axis = op->attribute("axis").dyn_cast<pir::Int32Attribute>().data();

    VLOG(6) << "Vjp prepare call subtract_grad_'s vjp inteface";

    std::vector<std::vector<Tensor>> tensor_res =
        primitive::subtract_grad_vjp(
        y, grad_out, grad_x_grad, grad_y_grad, axis, stop_gradients);

    VLOG(6) << "Vjp prepare stop gradient of subtract_double_grad";

    std::vector<std::vector<pir::OpResult>> res(tensor_res.size());
    for (size_t i = 0; i < tensor_res.size(); ++i) {
        res[i].resize(tensor_res[i].size());
        for (size_t j = 0; j < tensor_res[i].size(); ++j) {
            if(tensor_res[i][j].defined()){
                res[i][j] = std::static_pointer_cast<primitive::LazyTensor>(tensor_res[i][j].impl())->value().dyn_cast<pir::OpResult>();
            }
        }
    }
    return res;
}

std::vector<std::vector<pir::OpResult>> SumGradOp::Vjp(pir::Operation* op, const std::vector<std::vector<pir::Value>>& inputs_, const std::vector<std::vector<pir::Value>>& outputs, const std::vector<std::vector<pir::Value>>& out_grads, const std::vector<std::vector<bool>>& stop_gradients){

    PADDLE_ENFORCE_EQ(
      inputs_.size(),
      3,
      platform::errors::InvalidArgument("sum_grad op's inputs size should be 3, but now is %d.", inputs_.size()));
    PADDLE_ENFORCE_EQ(
      outputs.size(),
      1,
      platform::errors::InvalidArgument("sum_grad op's outputs size should be 1, but now is %d.", outputs.size()));

    VLOG(6) << "Prepare inputs of sum_double_grad";

    Tensor grad_x_grad(std::make_shared<primitive::LazyTensor>(out_grads[0][0]));

    VLOG(6) << "Vjp prepare Prepare attributes of sum_double_grad";

    Tensor axis(std::make_shared<primitive::LazyTensor>(inputs_[2][0]));
    bool keepdim = op->attribute("keepdim").dyn_cast<pir::BoolAttribute>().data();

    VLOG(6) << "Vjp prepare call sum_grad's vjp inteface";

    std::vector<std::vector<Tensor>> tensor_res =
        primitive::sum_grad_vjp(
        grad_x_grad, axis, keepdim, stop_gradients);

    VLOG(6) << "Vjp prepare stop gradient of sum_double_grad";

    std::vector<std::vector<pir::OpResult>> res(tensor_res.size());
    for (size_t i = 0; i < tensor_res.size(); ++i) {
        res[i].resize(tensor_res[i].size());
        for (size_t j = 0; j < tensor_res[i].size(); ++j) {
            if(tensor_res[i][j].defined()){
                res[i][j] = std::static_pointer_cast<primitive::LazyTensor>(tensor_res[i][j].impl())->value().dyn_cast<pir::OpResult>();
            }
        }
    }
    return res;
}

std::vector<std::vector<pir::OpResult>> TileGradOp::Vjp(pir::Operation* op, const std::vector<std::vector<pir::Value>>& inputs_, const std::vector<std::vector<pir::Value>>& outputs, const std::vector<std::vector<pir::Value>>& out_grads, const std::vector<std::vector<bool>>& stop_gradients){

    PADDLE_ENFORCE_EQ(
      inputs_.size(),
      3,
      platform::errors::InvalidArgument("tile_grad op's inputs size should be 3, but now is %d.", inputs_.size()));
    PADDLE_ENFORCE_EQ(
      outputs.size(),
      1,
      platform::errors::InvalidArgument("tile_grad op's outputs size should be 1, but now is %d.", outputs.size()));

    VLOG(6) << "Prepare inputs of tile_double_grad";

    Tensor grad_x_grad(std::make_shared<primitive::LazyTensor>(out_grads[0][0]));

    VLOG(6) << "Vjp prepare Prepare attributes of tile_double_grad";

    Tensor repeat_times(std::make_shared<primitive::LazyTensor>(inputs_[2][0]));

    VLOG(6) << "Vjp prepare call tile_grad's vjp inteface";

    std::vector<std::vector<Tensor>> tensor_res =
        primitive::tile_grad_vjp(
        grad_x_grad, repeat_times, stop_gradients);

    VLOG(6) << "Vjp prepare stop gradient of tile_double_grad";

    std::vector<std::vector<pir::OpResult>> res(tensor_res.size());
    for (size_t i = 0; i < tensor_res.size(); ++i) {
        res[i].resize(tensor_res[i].size());
        for (size_t j = 0; j < tensor_res[i].size(); ++j) {
            if(tensor_res[i][j].defined()){
                res[i][j] = std::static_pointer_cast<primitive::LazyTensor>(tensor_res[i][j].impl())->value().dyn_cast<pir::OpResult>();
            }
        }
    }
    return res;
}

std::vector<std::vector<pir::OpResult>> TransposeGradOp::Vjp(pir::Operation* op, const std::vector<std::vector<pir::Value>>& inputs_, const std::vector<std::vector<pir::Value>>& outputs, const std::vector<std::vector<pir::Value>>& out_grads, const std::vector<std::vector<bool>>& stop_gradients){

    PADDLE_ENFORCE_EQ(
      inputs_.size(),
      1,
      platform::errors::InvalidArgument("transpose_grad op's inputs size should be 1, but now is %d.", inputs_.size()));
    PADDLE_ENFORCE_EQ(
      outputs.size(),
      1,
      platform::errors::InvalidArgument("transpose_grad op's outputs size should be 1, but now is %d.", outputs.size()));

    VLOG(6) << "Prepare inputs of transpose_double_grad";

    Tensor grad_x_grad(std::make_shared<primitive::LazyTensor>(out_grads[0][0]));

    VLOG(6) << "Vjp prepare Prepare attributes of transpose_double_grad";

    std::vector<int> perm;
    for (size_t i = 0; i < op->attribute("perm").dyn_cast<pir::ArrayAttribute>().size(); i++) {
        perm.push_back(op->attribute("perm").dyn_cast<pir::ArrayAttribute>().at(i).dyn_cast<pir::Int32Attribute>().data());
    }

    VLOG(6) << "Vjp prepare call transpose_grad's vjp inteface";

    std::vector<std::vector<Tensor>> tensor_res =
        primitive::transpose_grad_vjp(
        grad_x_grad, perm, stop_gradients);

    VLOG(6) << "Vjp prepare stop gradient of transpose_double_grad";

    std::vector<std::vector<pir::OpResult>> res(tensor_res.size());
    for (size_t i = 0; i < tensor_res.size(); ++i) {
        res[i].resize(tensor_res[i].size());
        for (size_t j = 0; j < tensor_res[i].size(); ++j) {
            if(tensor_res[i][j].defined()){
                res[i][j] = std::static_pointer_cast<primitive::LazyTensor>(tensor_res[i][j].impl())->value().dyn_cast<pir::OpResult>();
            }
        }
    }
    return res;
}


}  // namespace dialect
}  // namespace paddle
